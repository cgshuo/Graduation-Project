 Community detection is an important task for social net-works, which helps us understand the functional modules on the whole network. Among different community detec-tion methods based on graph structures, modularity-based methods are very popular recently, but suffer a well-known resolution limit problem. This paper connects modularity-based methods with correlation analysis by subtly reformat-ting their math formulas and investigates how to fully make use of correlation analysis to change the objective function of modularity-based methods, which provides a more natural and effective way to solve the resolution limit problem. In addition, a novel theoretical analysis on the upper bound of different objective functions helps us understand their bias to different community sizes, and experiments are conducted on both real life and simulated data to validate our findings. H.2.8 [ DATABASE MANAGEMENT ]: Database Ap-plications  X  Data Mining community detection; correlation analysis; modularity; lever-age; likelihood ratio
The modern science of graphs has significantly helped us understand complex systems. One important feature of graphs is community structure where nodes in the same community have a higher chance to be connected to each other than that of nodes in different communities. Such communities can be considered as relatively independent components and play a role in the system. Community de-tection, which attempts to identify the modules by using the graph topology, has a long history in sociology, biol-ogy, and computer science where systems are often repre-sented as graphs. The first research on community detection was made by Weiss and Jacobson [39] to study the work-ing relationships between members of a government agency. Nowadays, there are many different community detection methods, such as spectral-based methods [24], density-based methods [26], modularity-based methods [9, 35], divisive methods [19], statistical-inference-based methods [28], etc. Generally speaking, their progress can be categorized into the following three procedures: (1) feature selection (2) ob-jective function (3) search procedure.

Feature selection selects relevant features, and removes irrelevant noisy information. Spectral-based methods [24] use the eigenvectors of the adjacency matrix for community detection. The Laplacian is by far the most used matrix in spectral-based methods. Though no unique matrix is ex-actly called the graph Laplacian [24], one commonly used Laplacian is calculated as follows. Given the adjacency ma-trix W of the graph G , we calculate the matrix D where the diagonal element d ii is equal to n j =1 ( w ij ) and non-diagonal elements are 0. The Laplacian matrix L is equal to D  X  W . We choose k eigenvectors corresponding to the k smallest eigenvalues to transform the original adjacency matrix, and then apply the traditional clustering methods like K-mean [25] on the transformed matrix. The spectral-based methods are popular because the change of representation induced by eigenvectors makes the community structure more obvious.
Objective function is the function to express our goal in mathematical terms. No matter how the community is de-fined, the commonly accepted goal for community detection can be boiled down to two objectives: (1) More connections are inside each community. (2) Fewer connections are across different communities. Since there are two objectives, people have proposed many different methods to strike a different balance between them. There are various kinds of objective functions for community detection [1]. For example, density-based methods [26] try to find the communities within which nodes are tightly connected with each other. We define the internal-community density  X  int ( S ) of the subgraph S as the ratio of the number of internal edges of S to the number of all possible internal edges, i.e.  X  int ( S )= k int ( S ) k int ( S ) is the number of internal edges of S . Similarly, the be a community, we expect large  X  int ( S )andsmall  X  ext Searching for the best tradeoff between  X  int ( S )and  X  ext is the goal of density-based methods. A simple way of doing that is to maximize the sum of the difference  X  int ( S ) [26]. Another example is modularity-based methods which search the partition that maximizes the modularity function [9]. Although the modularity function is originally intro-duced as a stopping criterion for hierarchical methods [19], it has rapidly become an essential element of many cluster-ing methods by searching the partitions that maximize it.
Search procedure is the way we search the optimal so-lution according to the objective function. However, for the most objective functions, finding the optimal value is very slow. For example, it has been proved that modularity op-timization is an NP-complete problem [7]. Therefore, many different heuristic search algorithms, including greedy search [9, 38], simulated annealing [27], extremal optimization [6, 14], and genetic algorithms [31], are used.

According to different survey studies [16, 40] on commu-nity detection, modularity-based methods are considered as one classical type of methods. Despite the vast amount of expert endeavor spent on different optimization techniques to maximize the modularity function, there is little analysis on the modularity function itself, and the most analysis on the modularity function is related to the calculation of the edge probability [17] or the extension to complicated net-works [3]. In addition, modularity-based methods suffer the well-known resolution limit problem [22]. This problem also cannot be solved by multi-resolution methods [33] as two concurrent biases, the tendency to merge small communi-ties and to split large communities, are introduced.
The modularity function [9] searches for partitions that the actual number of edges is larger than the expected num-ber of edges inside communities under the assumption of random partition, while correlation analysis on itemset min-ing [13, 18, 36] searches for itemsets that occur more than the expected occurrence if items are independent from each other. Since both modularity on community detection and correlation analysis on itemset mining search for patterns that actually happens more than what is expected under the assumption of independence, this paper builds the connec-tion between the modularity function and correlation mea-sures, and changes the modularity function from the correla-tion perspective, through which the resolution limit problem of the original modularity function can be solved and per-formance can be improved. The focus in this paper is on the improvements over the modularity function from the corre-lation perspective. In order to have a fair comparison within the framework of correlation, the original modularity func-tion is treated as a baseline for the performance evaluation rather than other commonly used objective functions such as density [26] and conductance [21]. Although the modu-larity function can be combined with fuzzy logic for over-lapping community detection [29], we limit our performance evaluation to non-overlapping community detection in this paper. The rest of the paper is organized as follows. Section 2 introduces the basic notation of correlation analysis and modularity-based community detection. We build the con-nection between correlation analysis and modularity-based community detection by subtly reformatting their math for-mulas, and introduce a novel theoretical analysis on the bias of different correlation measures by analyzing their upper bounds in Section 3. Experiments on both real life and sim-ulated datasets are conducted to test different methods in Section 4. Finally, we draw a conclusion and point out future directions in Section 5.
Since we try to improve modularity-based methods from the correlation analysis perspective, the basic concepts of correlation and modularity-based community detection will be introduced first before connecting them.
Most of the correlation analysis in the data mining area is conducted on the context of itemset mining. Therefore, we follow the routine to introduce the basic concept of correla-tion. Given an itemset S = { I 1 ,I 2 , ..., I m } with m items in a dataset with sample size n , the true probability is tp = P ( S ), the expected proba bility under the assu mption of indepen-dence among items is ep = m i =1 P ( I i ). Many functions have been proposed to measure correlation [12, 13, 18, 36]. Here, we only introduce four typical correlation measures, Simplified  X  2 , Probability Ratio, Leverage, and Likelihood Ratio, which are derived from t he simple statistical theory and enough for generality.
The  X  2 test is arguably the most popular statistical check for correlation, and is specifically designed for use with cate-gorical data. It is calculated as  X  2 = i j ( r ij  X  E ( r If an itemset contains m items, 2 m cells in the contingency table must be considered for the above  X  2 statistic. The computation of the statistic itself is intractable for high-dimensional data. However, we can still use the basic idea behind  X  2 to create Simplified  X  2 [20]:  X  2 =( r  X  E ( r )) i.e., n  X  ( tp  X  ep ) 2 /ep ,wherethecell r corresponds to the exact itemset S and n is the total number of records. Since Simplified  X  2 is more computationally desirable, we only discuss the properties and experimental results of Simpli-fied  X  2 . The value of Simplified  X  2 is always larger than 0 and cannot differentiate positive from negative correlation. Therefore, we take advantage of the comparison between tp and ep .If tp &gt; ep , it is a positive correlation. Then Simpli-correlation. Then Simplified  X  2 is equal to  X  n  X  ( tp  X  This transformed Simplified  X  2 is mathematically favorable. Larger positive numbers indicate stronger positive correla-tion, 0 indicates no correlation, and larger (in magnitude) negative numbers indicate stronger negative correlation.
Probability Ratio (also known as Lift or Interest Factor) [8] is the ratio of an itemset X  X  true probability to its ex-pected probability under the assumption of independence. It is calculated as follows: P robabilityRatio ( S )= tp/ep . This measure is straightforward and means how many times the itemset S happens more than expected. However, this measure might not be a good correlation measure to use. The problem is that it favors the itemsets containing a large number of items rather than significant trends in the data. An itemset S with higher occurrence and low Probability Ratio may be more interesting than an alternative itemset S with low occurrence and high Probability Ratio. Intro-duced by Piatesky-Shapiro [30], Leverage ( S )= tp  X  ep .It measures the difference between the true probability of an itemset S and its expected probability if all the items in S are independent from each other. Since ep is always no less than 0, Leverage ( S ) can never be bigger than tp . Therefore, Leverage is biased to high-occurrence itemsets.
Likelihood Ratio is similar to a statistical test based on the loglikelihood ratio described by Dunning [15]. We take the ratio of the likelihood under our hypothesis of independence to the likelihood of the best  X  X xplanation X  overall. To apply the likelihood ratio test as a correlation measure, we use the binomial distribution Pr ( p, o, n )= n o p o (1  X  p ) ( n  X  o ) where p is the probability of a given itemset S , o is the occurrence of the itemset S ,and n is the total number of transactions. Given our assumption of independence of all items, we predict that each trial has a probability of suc-cess ep . Therefore, the chance for us to observe o out n transactions contain S is Pr ( ep, o, n ) under the assumption of independence. However, the best possible explanation for the single trial probability is tp instead of ep according to the observed data. In order to measure to what extent our assumption of item independence was violated in practice, we comparing the null hypothesis of independence with the best possible explanation. Formally, the Likelihood Ratio in this case is LikelihoodRatio ( S )= Pr ( tp, o, n ) /P r ( ep, o, n ). The Likelihood Ratio strikes a balance between the Proba-bility Ratio and the actual occurrence o .Itfavorsitemsets with both high Probability Ratio and high occurrence. For the itemsets containing a small number of items, their oc-currence tends to be high, but the Probability Ratio tends to be low, while, for the itemsets containing a large number of items, their Probability Ratio tends to be high, but the actual occurrence tends to be low. Likelihood Ratio favors middle-sized itemsets which can strike a balance between the Probability Ratio and the actual occurrence. The nu-merator of the Likelihood Ratio is the maximal likelihood of the real situation, so the Likelihood Ratio is always larger than 1 and cannot differentiate positive from negative cor-relation. Therefore, we conduct the similar transformation we do for Simplified  X  2 by comparing tp with ep .
The modularity function has several variants, but these variants share the same idea. Without the loss of general-ity, we introduce the original modularity-based method [9]. Given a graph with n nodes and m links represented by the adjacency matrix W , the expected number of edges falling between two nodes i and j is k i  X  k j / (2 m ) under the assump-tion of independence where k i is the degree of node i .The It is the sum of the difference between the actual number of edges and the expected number of edges over all the pairs of nodes in the same community.  X  ( v i ,v j )istheKronecker delta function whose value is equal to 1 if v i and v j are in the same community and 0 otherwise. Initially, each node is the only member of its own community. The original al-gorithm iteratively joins the two communities that increase the modularity most in the current round. The original al-gorithm will stop if the best merge cannot further increase modularity.
In this section, we subtly transform the modularity func-tion and connect it with correlation measures. Given a par-tition with l groups { G 1 ,G 2 , ..., G l } for the graph G with n nodes and m links, the modularity Q is 1 2 m ij ( w ij  X  2 m )  X   X  ( v i ,v j ). For the node v q in the group G p the number of the nodes in the group G p that connect to v . The partial modularity Q p , which all the nodes in the group G p contribute to the overall modularity function, is Therefore, Q
It is easy to calculate that the total number of links in-side G p is i  X  G p k int i / 2 and the total number of links in the graph G is m . If we randomly select a link from the graph G , the probability of the link inside G p is i  X  G p ilarly, the probability of the link with at least one end inside G graph G . If the partition with l groups { G 1 ,G 2 , ..., G the graph G is totally random, the probability of the link with the other end inside G p from the links with one end partition with l groups { G 1 ,G 2 , ..., G l } for the graph G ,if we randomly select a link from the graph G , the true prob-ability of the link being inside G p , tp ,is i  X  G p expected probability of the link being inside G p under the as-sumption of independent partition, ep ,is i  X  G p Therefore, the partial modularity function Q p can be rewrit-ten as: Q p = tp  X  ep . By comparing the correlation measure Leverage ( S )= tp  X  ep , we can see the modularity function shares the same idea with the correlation measure Leverage. Since the other correlation measures are also functions of tp and ep , we can change the partial modularity function Q p using the formula of other correlation measures. In the rest of the paper, instead of using the term modularity, we use Simplified  X  2 , Probability Ratio, Leverage, and Likelihood Ratio referring to the corresponding changed partial modu-larity function Q p , and Leverage is the original modularity community detection method.
The performance differences among different correlation measures have been recognized since 2004 by two very influ-ential papers [18, 36] in the data mining area. They catego-rized measures according to their different property satisfac-tion. By categorizing measures, users only need to check the performance of the typical measure in each category instead of all the possible measures. However, two measures can still generate different results even if they satisfy the same set of properties. Instead, one recent paper [37] categorized mea-sures directly according to their final result similarity. No matter how categorizing measures, the fundamental ques-tion is still not answered. If there is a difference between results of two measures, what is the difference? In this sec-tion, we provide a novel way to understand the performance difference by analyzing the upper bound of different partial modularity functions Q p inferred from different correlation measures. Simplified  X  2 , Probability Ratio, Leverage, and Likelihood Ratio all satisfy the third correlation property proposed by Piatesky-Shapiro [30]: The correlation Measure M monotonically decreases with the increase of ep when tp remains the same. According to t he above correlation prop-erty, the measures reach their upper bound when tp is fixed and ep reaches its lower bound.

Theorem 1. Simplified  X  2 , Probability Ratio, Leverage, and Likelihood Ratio monotonically decreases with the in-crease of ep when tp remains the same.

Proof. Simplified  X  2 : When tp  X  ep ,  X  2 = n  X  ( tp  X  ep ) 2 /ep . If we consider Simplified  X  2 as a function of ep , then  X  2 = n  X  ( ep 2  X  tp 2 ) /ep 2 .Since0  X  ep  X  tp  X  1, ep 2  X  tp 2 . Therefore,  X  2 ( S )  X  0. Similarly, when tp &lt; ep ,  X  2 =  X  n  X  ( tp  X  ep ) 2 /ep and  X  2 =  X  n  X  ( ep 2  X  tp 2 In all, Simplified  X  2 decreases with the increase of ep . Probability Ratio: When tp is fixed, Probability Ratio, tp/ep , decreases with the increase of ep .
 Leverage: When tp is fixed, Leverage, tp  X  ep , decreases with the increase of ep .
 Likelihood Ratio: When tp &gt; ep , ln ( LR ( S )) = n  X  tp  X  ( ln ( tp )  X  ln ( ep ))
If we consider ln ( LR ( S )) as a function of ep ,then Since tp &gt; ep ,then ln ( LR ( S )) &lt; 0. In other words, Likelihood Ratio decreases with the increase of ep when tp &gt; ep . Similarly, when tp &lt; ep , we can prove Likelihood Ratio decreases with the increase of ep . In all, Likelihood Ratio decreases with the increase of ep . Figure 1: Upper bounds of different measures for a single community
Given a partition with l groups { G 1 ,G 2 , ..., G l } for the graph G , the true probability of a link being inside G p when tp for the group G p is fixed, the lowest possible value for ep is tp 2 .When ep = tp 2 , the measures reach their upper bound. Figure 1 shows the upper bounds of the various mea-sures with respect to different tp for a single community. It is easy to see that different measures favor groups within dif-ferent tp ranges. The upper bound of Simplified  X  2 increases to 1 and that of Probability Rat io increase to infinity when tp is close to 0, which means they favor extremely small groups rather than large groups. Leverage and Likelihood Ratio reach their highest upper bound when tp is between 0 and 1. According to the graph, Leverage does not favor the group which contains more than half of the edges in the graph since its upper bound starts to decrease even when the group size increases. Similarly, Likelihood Ratio does not favor the group which contains more than roughly one quarter of the edges in the graph. In all, Probability Ratio favors the smallest groups, followed by Simplified  X  2 ,Like-lihood Ratio, and Leverage.
Most prior research on modularity-based methods sets modularity as their objective function and uses different op-timization techniques to search for the partition that gen-erates the highest value. In this paper, instead of explor-ing better optimization techniques for the same objective function, we change the objective function and study what kinds of difference the various objective functions make. In order to conduct the fair comparison for different objective functions, we choose greedy search, the simplest optimiza-tion technique, which is also used by the original modularity method and generates reasonably good results [9]. Initially, each node is the only member of its own community. The algorithm iteratively join the two communities that increase the objective function most in the current round. The algo-rithm will stop if the best merge cannot further increase the objective function. In this section, we conduct experiments on both real life and simulated datasets. The two real life datasets include Karate Club [41] and College Football [19], as shown in Figure 2 1 .Aswemen-tioned in Section 1, we only conduct the performance evalua-tion on non-overlapping community detection. The existing large datasets with ground truth that we can find are all for overlapping community detection, so we can only do the performance evaluation on large datasets of simulated data. The karate dataset contains friendships between 34 members of a karate club at a US university in the 1970s. There was a disagreement between the administrator and the instruc-tor in the club, which resulted in two communities in this graph. The football dataset records games between Division IA colleges during regular season Fall 2000. There were 115 teams in 12 different conferences.

With regard to graph simulation, the first related work is called the planted L-partition model [10]. The model simu-lates a graph with n = g  X  l nodes in l groups with g nodes each. Nodes of the same group are linked with a probability p internal , whereas nodes of different groups are linked with cluster edge density exceeds the inter-cluster edge density. Then the graph has a community structure which is quite in-tuitive. However, by using the planted L-partition model, all nodes have approximately the same degree and all commu-nities have exactly the same size. In the real social network data, degree distributions are usually skewed, with many nodes with low degree and a few nodes with high degree. A similar heterogeneity is also observed in the distribution of community size. Recently, Lancichinetti et al. [23] intro-duced a very popular LFR model. They assume that the All the network visualization in this paper is visualized by Pajek Software distributions of degree and community size are power laws with  X  1 and  X  2 respectively. Each node shares a fraction of 1  X  u of its edges with the nodes in the same community and a fraction of u with the nodes in the other communities, where u is the mixing parameter. Since the LFR model is much more realistic, we will use the graphs generated by this model to test our algorithms. The simulation procedure is as follows:
However, the constraint used in the LFR model to assign the internal degree of each node in the second step is prob-lematic because the condition imposed by a fixed u cannot guarantee p internal &gt;p external which must be satisfied for a community structure. For a node A in a community with n nodes in a graph with n nodes, u must be smaller than 1  X  n /n to guarantee p internal &gt;p external . Therefore, we use the following constraint to assign the internal degree of each node in the second step: p internal =  X   X  p external  X  is the ratio to control the community structure and must be greater than 1.

There are 8 parameters related to the LFR simulation model: the total number of nodes, the minimal node degree, the maximal node degree, the power law parameter for node degree, the minimal community size, the maximal commu-nity size, the power law parameter for community size, and the ratio  X  for community structure. In order to avoid bias to different objective functions, we choose greedy search, the simplest optimization technique. However, greedy search can only handle 2,000 nodes within a reasonable amount of time. Therefore, we also use a fast unfolding search tech-nique [5] to handle the network with more than 1 million nodes, which is closer to the real world network size.
Given the number of nodes i s 2,000, we co nduct 9 sets of experiments and the parameter values are shown in Table 1. In order to check the performance difference on different community sizes and tightness of community structure, we only change the minimal community size and the ratio  X  for community structure to generate different graphs. When the minimal community size is 5, there are a lot of small com-munities, some mid-size communities, and a few large com-munities, while the graph only contains large communities when the minimal community size is 100. The community structure is fuzzy when  X  is 5, while it is clear when  X  is 20. How the  X  affects the community structure is shown in Figure 3. Given the number of nodes is 300, 000, we co nduct a similar set of 9 experiments and the parameter values are also shown in Table 1.
After finding communities in a given graph, we need to compare our search results with the  X  X ctual communities X  (the ground truth). For two partitions X =( X 1 ,X 2 , ..., X and Y =( Y 1 ,Y 2 , ..., Y n y ) of a graph, X is determined by the algorithm with n x communities and Y is the ground truth with n y communities. We need a criterion to mea-sure how similar the partition result of the algorithm is to the partition we hope to find. Many different measures, such as Normalized Mutual Information [11], Jaccard, Rand Index [32], and F-measure [34], have been proposed, and they can be divided into three categories: pair counting, community matching, and information theory [2]. Since different measures have different bias, we show the exper-imental results on different measures, but focus our analy-sis on the most widely accepted measure, Normalized Mu-tual Information. First, it calculates Mutual Information: amount of information by which our knowledge about the community in one partition increases when we are told what the community in the other partition is. The minimum of MI is 0 if the X partition is random with respect to the Y partition. However, given a partition Y , all partitions derived from Y by further partitioning have the same mu-tual information with Y , even though they are different from each other. In this case, the mutual information is equal to the entropy H ( Y )=  X  j P ( Y j ) log ( P ( Y j )). To avoid that, Danon et al. [11] proposed the normalized mutual informa-reaches its maximal value 1 if X partition is identical to Y partition.
The result on real life datasets is shown in Table 2 2 .The best method for Karate dataset is Leverage. The result is consistent with our theoretical analysis. The karate dataset only contains two large communities, and Leverage is the method having the most bias to large communities. Both Simplified  X  2 and Likelihood Ratio are good for the football dataset. This dataset contains 12 almost equal size com-
RI: Rand Index; DNC: the detected number of communi-ties; ANC: the actual number of communities;  X  2 : Simplified  X  ; PR: Probability Ratio; and LR: Likelihood Ratio munities. According to Figure 1, Likelihood Ratio has the bias to middle-size communities, and Simplified  X  2 has the bias, but not the extreme bias, to small communities. It is why Simplified  X  2 and Likelihood Ratio work better on the football dataset.
For each parameter setting, we generate the graph 10 times to test each method. We calculate the average value for each evaluation measure shown in Table 3 and 4. Since the result of 2,000 nodes is very similar to that of 300,000 nodes, we focus our analysis on the result of 2,000 nodes. In addition, different evaluation measures provide different information. Since NMI is the most widely-accepted mea-sure, we focus our discussion on NMI in the following. The average NMI and the average number of partitioned commu-nities generated by each method for each parameter setting are shown in Figures 4 -7.
 Figure 4 shows the NMI achieved by each method and Figure 5 shows the number of partitioned communities when fixing the minimal community size and changing the ratio  X  . No matter what the minimal community size and the ratio  X  are, the NMI and the number of partitioned communi-ties for both Simplified  X  2 and Probability Ratio are almost the same. They always detect more than 500 communities. Since the total number of nodes is 2,000, most of the com-munities they detect contain 2 or 3 nodes. That supports our observation in Section 3 that Simplified  X  2 and Proba-bility Ratio favor small size communities. No matter what the minimal community size is, both Leverage and Likeli-hood Ratio achieve better NMI when the community struc-ture becomes clearer. Only when the community structure is clear and the whole graph only contains large commu-nities, the NMI of Leverage is better than that of Likeli-hood Ratio. Leverage has more bias towards large commu-nities than Likelihood Ratio according to our upper bound analysis; therefore, we are expecting Leverage is better than Likelihood Ratio when the graph only contains large com-munities. In practice, social networks contain a lot of small communities; therefore, Likelihood Ratio is better in the common case. The number of partitioned communities by Leverage is almost the same no matter how we change the minimal community size and the ratio  X  , while the number of partitioned communities by Likelihood Ratio get closer to the ground truth when the community structure becomes clearer. Another interesting observation related to Leverage is that its NMI is very low when the minimal community size is large under the fuzzy community structure. Even under the fuzzy community structure, Leverage detects the same number of large communities. Such a partition assigns many nodes in different real communities to the same par-titions, which results in the low NMI. Generally speaking, the partition generated by Likelihood Ratio is better and more adaptive to the different types of graphs than that of Leverage.
 Figure 6 shows the NMI achieved by each method and Figure 7 shows the number of partitioned communities when fixing the ratio  X  and changing the minimal community size. Since both Simplified  X  2 and Probability Ratio favor small-size communities, their NMI decreases with the increase of the minimal community size no matter the community struc-ture is fuzzy or clear. The NMI of Likelihood Ratio decreases with the increase of the minimal community size when the community structure is not clear. However, when the com-munity structure is clear, Likelihood Ratio achieves almost the same performance with the increase of the minimal com-munity size. The NMI of Leverage always increases with the increase of the minimal community size since it has the bias to large communities.
In this paper, we connect modularity-based methods with correlation analysis by subtly reformatting their math for-mulas, and make smart use of different correlation measures to change the objective function of modularity-based meth-ods. A novel theoretical analysis on upper bounds is con-ducted to analyze the bias of different objective functions and the bias is validated by our experiments. Using the widely-accepted Normalized Mutual Information to compare the partitions determined by the algorithm with the ground truth, Likelihood Ratio is better and more robust. However, different measures can be used for different purposes. For example, Probability Ratio can be used if we want to fairly partition the students in the class into small groups for class projects, and we might use Leverage to find relatively large groups for marketing campaigns. As shown above, our find-ing provides a more natural a nd effective way to solve the resolution limit problem of the original modularity function by modifying it through different correlation measures. In the future, we will investigate more correlation measures, and test performance differences for detecting overlapping communities. [1] H.Almeida,D.O.G.Neto,W.M.Jr.,andM.J.Zaki.
 [2] E. Amig  X  o, J. Gonzalo, J. Artiles, and F. Verdejo. A [3] A. Arenas, A. Fern  X  A  X  l  X  A  X cndez, and S. G  X  A [4] J. P. Bagrow. Evaluating local community methods in [5] V. D. Blondel, J.-L. Guillaume, R. Lambiotte, and [6] S. Boettcher and A. G. Percus. Extremal optimization [7] U. Brandes, D. Delling, M. Gaertler, R. Gorke, [8] S. Brin, R. Motwani, J. D. Ullman, and S. Tsur. [9] A. Clauset, M. E. J. Newman, and C. Moore. Finding [10] A. Condon and R. M. Karp. Algorithms for graph [11] L. Danon, A. D. Guilera, J. Duch, and A. Arenas. [12] L. Duan and W. N. Street. Finding maximal [13] L. Duan and W. N. Street. Selecting the right [14] J. Duch and A. Arenas. Community detection in [15] T. Dunning. Accurate methods for the statistics of [16] S. Fortunato. Community detection in graphs. Physics [17] M. Gaertler, R. G  X  orke, and D. Wagner.
 [18] L. Geng and H. J. Hamilton. Interestingness measures [19] M. Girvan and M. E. J. Newman. Community [20] C. Jermaine. Finding the most interesting correlations [21] R. Kannan, S. Vempala, and A. Veta. On [22] A. Lancichinetti and S. Fortunato. Limits of [23] A. Lancichinetti, S. Fortunato, and F. Radicchi. [24] U. Luxburg. A tutorial on spectral clustering. [25] J. B. MacQueen. Some methods for classification and [26] S. Mancoridis, B. S. Mitchell, and C. Rorres. Using [27] C. P. Massen and J. P. K. Doye. Identifying [28] M. E. J. Newman. Community detection and graph [29] V. Nicosia, G. Mangioni, V. Carchiolo, and [30] G. Piatetsky-Shapiro. Discovery, Analysis, and [31] C. Pizzu ti. Community detection in social networks [32] W. M. Rand. Objective Criteria for the Evaluation of [33] J. Reichardt and S. Bornholdt. Statistical mechanics [34] C. V. Rijsbergen. Foundation of Evaluation. Journal [35] H. Shiokawa, Y. Fujiwara, and M. Onizuka. Fast [36] P.-N. Tan, V. Kumar, and J. Srivastava. Selecting the [37] C. Tew, C. Giraud-Carrier, K. Tanner, and S. Burton. [38] K. Wakita and T. Tsurumi. Finding community [39] R. S. Weiss and E. Jacobson. A method for the [40] J. Xie, S. Kelley, and B. K. SZYMANSKI.
 [41] W. W. Zachary. An information flow model for
