 Multi-document summarization aims to distill the most rep-resentative information from a set of documents to generate a summary. Given a set of documents as input, most of exist-ing multi-document summarization approaches utilize differ-ent sentence selection techniques to extract a set of sentences from the document set as the summary. The submodularity hidden in textual-unit similarity motivates us to incorporate this property into our solution to multi-document summa-rization tasks. In this poster, we propose a new principled and versatile framework for different multi-document sum-marization tasks using the submodular function [8]. Categories and Subject Descriptors: H.3.3 [Informa-tion Storage and Retrieval]: Information Search and Re-trieval General Terms: Algorithms, Performance Keywords: Submodularity, Summarization, Framework
Typical multi-document summarization tasks include generic and query-focused summarization. Recently, new summa-rization tasks such as update summarization [1] and com-parative summarization [10] have also been proposed.
Lin et al. [5] proposed attacking generic summarization problem using submodularity along with a greedy algorithm. In this paper, we propose a new framework for M ulti-document S ummarization using the S ubmodular F unction (MSSF) which extends the submodularity to not only generic sum-marization, but also query-focused, update, and compar-ative summarization, and applies an improved greedy al-gorithm proposed by Minoux [6] (which is faster than the greedy algorithm in Lin et al. [5]) to generate summaries.
In this section, first of all, the definition of submodularity is given since it is the core of MSSF. Then we explore how to map the summarization problem into a budgeted maximum coverage problem which is based on submodularity. Finally, the submodular functions for various summarization tasks performed by MSSF are presented.
Let E be a finite set and f be a real valued nondecreasing function defined on the subsets of E that satisfies
Table 3: The brief description of the data sets. where E is the whole sentence set, and sim ( s i , s j ) is the similarity between the textual units s i and s j (the typical textual unit is sentence). Note that the first component of Eq.(2) is for information coverage and the second com-ponent is for redundancy removal. Both components are submodular, thus f ( S ) is also submodular, since the linear combination of submodular functions is closed. The goal of multi-document summarization is to generate a summary which provides the largest possible quality within the bud-get. Hence, multi-document summarization problem can be modeled as a budgeted maximum coverage problem.
Eq.(2) has presented the submodular function for generic summarization task, thus we can define more submodular functions for different summarization tasks. All the sub-modular functions are shown in Table 1, and descriptions of the notations are given in Table 2. Notice that f G can be presented by the functions for generic summarization. As we have obtained corresponding submodular function for each summarization task, the algorithm by Minoux [6] is utilized for extracting summaries which have the quality close to optimal.
Lin et al. [5] have already showed the feasibility of the generic summarization using submodularity. Thus, we con-ducted experiments on three new summarization tasks to evaluate our proposed MSSF based on submodular function.
Table 3 shows the characteristics of all the datasets used for our experiments. All the tasks, except the comparative summarization, are evaluated by ROUGE, an widely used evaluation toolkit for document summarization.

Table 4: Results on query-focused summarization.
For the query-focused summarization task, we com-pared our method with some widely used and recently pub-lished methods: SNMF [9], Qs-MRF [11], and Wiki [7]. The empirical results are reported in Table 4. Our method achieves the best result.

For the update summarization task, Table 5 shows the comparative experimental results,  X  X AC Best X  and  X  X AC Median X  represent the best and median results from the participants of the TAC08 summarization track in the two tasks respectively according to the TAC08 report [1]. The experimental results demonstrate that MSSF leads to the competitive performance for update summarization.
For comparative summarization , we use the top three largest clusters of documents from TDT2 corpora to gener-
