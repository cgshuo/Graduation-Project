
RolandHausser (Friedrich-Alexander-Universit  X  at Erlangen-N  X  urnberg)
Springer, 2006, xii+365 pp; hardbound, ISBN 3-540-35476-X/978-3-540-35476-5, Reviewed by Markus Egg University of Groningen
The work presented in this book is motivated by the goal of applying linguistic theory-building to the concrete needs of potential linguistic applications such as question answering, dialogue systems, and machine translation. To pursue this goal, a translation of linguistic theory into a framework of  X  X ractical linguistics X  is suggested. Database
Semantics (DBS) is presented as a first step towards such a framework. It models the communication between cognitive agents, which can be used, for example, to imple-ment the communicative abilities of a cognitive robot.
 lends itself to an account of both language processing and language production (think-ing is added as a separate component, which refers to inferencing on stored information, and activating content to be verbalized). As such an underlying format, it can be used to describe linguistic as well as extralinguistic content (to represent utterances and the con-text, respectively). Being explicitly designed for practical applications, DBS deliberately ignores linguistic phenomena considered irrelevant for these (e.g., quantifier scope). outline the range of constructions covered by DBS so far, and specify fragments that can be processed or produced in the framework of DBS. There is also an appendix with two sections on the treatment of word-order variation in DBS and on the global architecture of DBS systems, and a glossary.
 ply to DBS. These principles include incrementality (input is to be processed successively as it comes in, which yields an analysis for incomplete as well as complete chunks of input; the syntactic basis for this strategy is Left-Associative Grammar [Hausser 1992]), surface orientation (no empty categories), and a focus on communication (description for-malisms must be able to handle turn-taking, i.e., language processing and production). is presented in detail. It is implemented as a non-recursive data structure, that is, a list of feature structures called proplets (usually, one per word the values of specific features. 2 For example, subcategorizing elements ( X  X unctors X ) have features whose values indicate their arguments and the other way around. expressions. Although it does abstract away from purely syntactic phenomena such as word order and diatheses, it still preserves much syntactic structure, for example, in its representation of modification and of elliptical expressions. Semantics proper is encoded within proplets (except those for deictic expressions and proper names) by defining a concept as the value of their  X  X ore attribute. X  described in terms of proplet sets that are linked by feature value coindexation; the only difference to proplet sets for the modeling of linguistic content is that proplet sets for context do not comprise explicit pointers to specific words.
 move between them straightforward, which is crucial for the proposed analysis of language processing and production: Language processing consists of deriving lists of proplets (including the coindexations between proplet values) from utterances and storing them in the context representation, which is modeled as a database. Language production consists of the activation of such lists of proplets from this database and their translation into utterances.
 their description in DBS. The first class is called  X  X unctor-argument structure X  and cov-ers the relations between subcategorizing elements and their arguments and modifica-tion. This includes sentential arguments, subordinate clauses, and relative clauses. The second class consists of coordination phenomena, ranging from simple coordination on the word or phrase level to gapping and right-node raising. The last class is cases of coreference. A wide range of these cases is represented in DBS, including even Bach X 
Peters sentences (where there are two NPs that constitute anaphors whose antecedent is the respective other NP). The DBS framework is used to formulate a version of the
Langacker X  X oss condition dating back to Langacker (1969) and Ross (1969): Pronouns can precede a coreferential NP only if they are part of a clause that is embedded within the clause of the NP.
 processing and production perspective, the last one only from the processing perspec-tive. The first fragment prepares the ground by illustrating how the approach handles extremely simple texts consisting of intransitive present-tense sentences whose NP is a proper name. The second fragment extends the coverage to pronouns, complex NPs (Det-Adj*-N), and transitive and ditransitive verbs in simple and complex tenses.
Finally, the third fragment offers a treatment of intensifiers ( very, rather ) and adverbials, and an outlook on a syntactically underspecified approach to modifier attachment ambiguities. The fragments are described in terms of  X  X rammars, X  which specify start and end states (in terms of the first and the last proplet of a list to be processed or verbalized) and a set of rules. The rules are ordered in that every rule is accompanied by a set of potential successors, and in that rules to start and to end a derivation with are specified.
 application to the fragments is described thoroughly, which makes it easy to understand and evaluate DBS. The underlying perspective on linguistic theory-building and the theory of communication of which DBS is a part are also explicated clearly. The formal details of the analysis are presented carefully. A remaining point of dispute is in my view the set of readings of sentences where several PPs have more than one attachment possibility (Chapter 15.1).
 proposed analysis and competing approaches. This shows up in specific parts of the analysis X  X or example, in the discussion of coreference in Chapter 10, which does not 312 integrate previous work that formulates constraints on potential coreferences in terms of syntactic constellations such as c-or o-command (e.g., Pollard and Sag 1994; Reuland 2006), and in the treatment of quantifier scope and scope ambiguity in Chapter 6 (as opposed to, e.g., the papers in van Deemter and Peters [1996]). But even more important, it would have been interesting to hear more about the way in which DBS compares to other approaches whose goal is the application of linguistic theory-building to concrete needs of potential linguistic applications. Although the completion of the manuscript admittedly antedates much of the ongoing work in the field (e.g., the application of deep linguistic processing in the analysis of biomedical and other scientific texts), a comparison of DBS to wide-coverage systems such as the LinGO English Resource
Grammar (Copestake and Flickinger 2000) (including also related activities such as the development of Robust Minimal Recursion Semantics [Copestake 2007]) or Alpino (analysis of unrestricted Dutch texts [Bouma, van Noord, and Malouf 2001]) would have been a welcome complementation to the presentation of DBS in the book. References
 StevenAbney (University of Michigan) Boca Raton, FL: Chapman &amp; Hall / CRC (Computer science and data analysis series, edited by David Madigan et al.), 2007, xi+308 pp; hardbound, ISBN 978-1-58488-559-7, $79.95,  X 44.99 Reviewed by Vincent Ng University of Texas at Dallas Semi-supervised learning is by no means an unfamiliar concept to natural language processing researchers. Labeled data has been used to improve unsupervised parameter estimation procedures such as the EM algorithm and its variants since the beginning of the statistical revolution in NLP (e.g., Pereira and Schabes 1992). Unlabeled data has also been used to improve supervised learning procedures, the most notable ex-amples being the successful applications of self-training and co-training to word sense disambiguation (Yarowsky 1995) and named entity classification (Collins and Singer 1999).
 typically discussed in introductory machine learning texts (e.g., Mitchell 1997; Alpaydin 2004) or NLP texts (e.g., Manning and Sch  X  utze 1999; Jurafsky and Martin 2000). 1 Consequently, to learn about semi-supervised learning research, one has to consult the machine-learning literature. This can be a daunting task for NLP researchers who have little background in machine learning. Steven Abney X  X  book Semisupervised Learning for Computational Linguistics is targeted precisely at such researchers, aiming to provide them with a  X  X road and accessible presentation X  of topics in semi-supervised learning. According to the preamble, the reader is assumed to have taken only an introductory course in NLP  X  X hat include[s] statistical methods X  X oncretely the material contained in Jurafsky and Martin (2000) and Manning and Sch  X  utze (1999). X  Nonetheless, I agree with the author that any NLP researcher who has a solid background in machine learning is ready to  X  X ackle the primary literature on semisupervised learning, and will probably not find this book particularly useful X  (page 11).
 who have little background in machine learning. In particular, of the 12 chapters in the book, three are devoted to preparatory material, including: a brief introduc-tion to machine learning, basic unconstrained and constrained optimization tech-niques (e.g., gradient descent and the method of Lagrange multipliers), and relevant linear-algebra concepts (e.g., eigenvalues, eigenvectors, matrix and vector norms, diagonalization). The remaining chapters focus roughly on six types of semi-supervised learning methods: 2 lack relevant mathematical background, each theory and algorithm is presented in a rigorous manner.
 disparate ideas. As mentioned earlier, it shows that many semi-supervised learners can in fact be viewed as self-training; also, the description of the connection between spectral clustering and other semi-supervised learners is insightful.
 co-training: Although the algorithm is presented in Chapter 2, its theoretical underpin-nings are not described until Chapter 9. This enables the reader to see its applications in NLP (in Chapter 3) before going through the mathematics, which could be important for researchers who are linguistically but not mathematically oriented. Another reason 450 is that the preparatory material is presented on a need-to-know basis. This allows the discussion of algorithmic ideas as soon as the reader grasps the relevant fundamentals. For instance, function optimization and basic linear algebra concepts are presented in separate chapters, with the latter being deferred to Chapter 11, right before the discussion of spectral clustering in Chapter 12.
 application to NLP problems, the same is not true for the remaining semi-supervised learners described in the book. The reader is often left to imagine the potential NLP applications of these learners, and as a consequence is unable to gain an understanding of the state of the art of semi-supervised learning for NLP. In fact, given its scarcity of NLP applications, the book perhaps does not merit its current title. It does have a rich bibliography on semi-supervised learning for NLP, but most of the references are not cited in the text.
 supervised learners. For instance, the author does not mention that in practice it is not easy to choose k in k -means clustering, merely describing k as a parameter of the clustering algorithm. As another example, when introducing the EM algorithm, the author applies it to a generative model that can be expressed in exponential form, without acknowledging that one of the most difficult issues surrounding the application of EM concerns the design of the right generative model given the data. The lack of NLP applications in the book has unfortunately enabled the author to sidestep these practical issues. On a related note, one can hardly find any discussions of the strengths and weaknesses of the semi-supervised learners in the book. This could leave the reader without the ability to choose the best learner for a given NLP problem, and is probably another undesirable consequence of the book X  X  reluctance to discuss NLP applications. lems effectively limits the scope of the book. One consequence of this decision is that the reader may not be able to apply the EM algorithm to train a hidden Markov model for solving sequence-learning problems as basic as part-of-speech tagging upon completion of this book. Given the recent surge of interest in structure prediction in the NLP community, and the fact that co-training and semi-supervised EM have been applied to structure-prediction problems such as statistical parsing and part-of-speech tagging, the book X  X  sole focus on classification problem is perhaps one of its weaknesses. of the capability of an algorithm. For instance, the reader might think that spectral methods can be applied only to binary classification tasks, owing to the book X  X  exclusive focus on such tasks in its discussion of spectral clustering. Similarly for the treatment of support vector machines: The reader may get the impression that SVMs cannot be used to learn non-linear functions, as the discussion of kernels is deliberately omitted due to their irrelevance to transductive learning. Although it is important to keep the presentation focused, I believe that the author could easily have removed potential con-fusions by explicitly stating the full capability of an algorithm and referring the reader to the relevant papers for details.
 there is currently a need for a broad and accessible reference to this area of research. Abney X  X  book serves this purpose in spite of the aforementioned weaknesses, and I believe that it is a useful starting point for any non X  X achine-learning experts who intend to apply semi-supervised learning techniques to their research. As someone who has some prior knowledge of semi-supervised learning, I still find this book insightful: It reveals deep connections among apparently disparate ideas. If I were to teach a course on semi-supervised learning for NLP, I would undoubtedly use this book as a primary reference.
 References

J  X  orgTiedemann (Uppsala University) Morgan &amp; Claypool (Synthesis Lectures on Human Language Technologies, edited by
Graeme Hirst, volume 14), 2011, 153 pp; paperbound, ISBN 978-1-60845-510-2, $45.00; e-book, ISBN 978-1-60815-511-9, $30.00 or by subscription Reviewed by Michel Simard National Research Council Canada
Bitext alignment techniques are at the heart of the revolution that swept through the fieldofmachinetranslation(MT)twodecadesago.Fromtheearlyattemptsofpioneers like Kay and R  X  oscheisen (1993), to the recent success of [ enter name of your current favorite alignment method or researcher ], the growth of translation alignment is so tightly intertwined with that of statistical machine translation that it is actually difficult (and most likely futile) to try to establish which was most instrumental to which. From the beginning,researchers havebeenattractedbythealignmentproblem,notonlybecause of its essential link to MT and the numerous other applications that could be derived, but also because bitext alignment seems like a  X  X eat X  problem: one around which a researcher can easily wrap his or her head until a clean solution emerges. out, and all these  X  X ossible X  links in publicly available reference alignments are wit-nesses to the fact that translation alignment is not a perfectly well defined problem, and that explaining the subtle ways in which identical concepts are rendered across languages by drawing lines between words is an oversimplification, to say the least. processing literature. After going through the motivations in Chapter 1, he introduces the basic concepts and terminology in Chapter 2, discusses various alignment types, modelsandsearchprocedures,andpresentsthefundamentalsofalignmentevaluation.
Of particular interest is the discussion on the crucial role played by the segmentation of the text into the units on which the alignment will operate. The process of collecting and structuring parallel corpora is briefly outlined in Chapter 3.
 ment. Following a more-or-less historical line, he first covers approaches based on surface features (sentence lengths, alignment type), ` a la Gale and Church (1993), then methods relying on lexical resources, before looking at combined and resource-specific methods.
 sizeableportionofthebook(aboutathird).Halfofthischapterisdevotedtogenerative translationmodels(essentially,theIBMmodels),theotherhalftovariousdiscriminative models. Finally, Chapter 6 rapidly surveys phrase and tree alignment models. vastfield,whichhasseenthepublicationofhundreds(thousands?)ofpublicationsover the last 20 years or so. The author clearly knows his stuff, and manages to structure its presentation in a logical and intuitive manner. The exposition X  X  clarity sometimes suffers,however,fromtheauthor X  X obviousdesirenottomissanything.Manyresearch avenuesarejustalludedto,andevenfundamentalnotionsaresometimespresentedina sketchymanner.Forinstance,thetopicthatclearlygetsthemostelaboratepresentation is the IBM models, with 15 pages; yet it is unlikely that a reader new to this field will manage to extract more than a general intuition about them.
 just  X  X amiliarity with basic concepts X  of machine learning. The means and methods of machine learning have become so ubiquitous in computational linguistics that we tend to forget how fundamental they are: the cycle of training and testing, feature engineer-ing, the debates of generative versus discriminative modeling, supervised versus semi-supervised versus unsupervised learning, and so forth. All of this is part of our daily routine and has taken over our way of viewing the field.
 overview of the state-of-the-art in text-translation alignment. It reads like a roadmap of theworkaccomplishedmorethanatruetravelguide.Assuch,itisrichwithpointersto the many variations on methods and approaches to the problem. It will most likely be ofinteresttothebi-curiousamongus:graduatestudentsandresearcherswho,although already familiar with computational linguistics, may feelattracted to the other text. References

NissimFrancez  X  andShulyWintner  X  ( Technion X  X srael Institute of Technology and  X  University of Haifa)
Cambridge University Press, 2012, xii+312 pp; hardbound, ISBN 978-1-107-01417-6, $95.00 Reviewed by
Tracy Holloway King eBay Inc.

FrancezandWintner X  X textbookonunificationgrammarsisaimedatstudentsinterested in computational linguistics who are at the advanced undergraduate or introductory graduate level. As stated by the authors, the book assumes a solid introductory course in syntax and a year of undergraduate mathematics, but no programming experience;
I agree with this assessment. The book will also be of interest to anyone working with unification grammars X  X or example, an HPSG or LFG theoretician or grammar engineer,whowantstounderstandmoreaboutthemathematical underpinnings ofthe systems they are working on; even without a strong mathematics background, large portions of the book will be accessible for motivated readers with previous unification grammar experience.
 of the book, the material is made accessible by its presentation style. Concepts are introduced in stages, with frequent references to earlier concepts and sections. Each concept is introduced in prose, in relevant proofs, and by examples tied to linguistic issues,therebyreinforcingthematerial.Exercisesareprovidedthroughouteachchapter sothatreaderscanchecktheirunderstandingastheyworkthroughthematerial;many of theexercises have answers provided intheback of thebook. Another strength of the book is the detailed further-reading section at the end of each chapter: These provide historical background as well as an introduction to more-advanced topics. index.
 syntax of natural languages (e.g., parts of speech, subcategorization, control, long-distance dependencies, and coordination). There is also an overview of formal lan-guagesandcontext-freegrammars.Thesearethenlinkedbyadiscussionofsomeofthe arguments against natural languages being context-free. Building on this discussion, mildly context-sensitive languages are introduced. This chapter will serve as a review of relevant concepts for most readers.
 tending context-free grammars to express linguistic information, using examples from agreement as motivation. The connection between feature graphs, feature structures, abstract feature structures, and attribute-value matrices is presented in detail. Special attention is paid to reentrancies and cycles, two key issues in the formal and practical understandingoffeaturestructures.Numerousgraphicalexamplesillustratetheformal proofsandhelptoprovidetheintuitionbehindtheconceptsandhowtheyrelatetoone another.
 introduced as the mechanism to combine the information in two compatible feature structures. Again, graphical examples provide an intuitive view into the formalisms introduced. A simple, destructive algorithm for unification is introduced and linked to the formal definitions (computational aspects of unification grammars are discussed in detail in Chapter 6). Generalization is briefly discussed as the dual of unification. feature structures are extended to multirooted feature structures and then combined with unification to form unification grammars. Grammar rules and derivations, along with the lexicon, are defined. Comparisons to context-free grammars and their limita-tions are provided to further exemplify the formal power of unification grammars. are then used to account for a variety of linguistic phenomena, including traditional  X  X ovement X  X henomena,bystartingwithasimpleunificationgrammarforafragment of English and gradually extending it. Examples are provided of where the grammar engineer must choose among different ways to formulate the grammar rules within theunificationgrammarformalismandhowthesedifferentchoicescanreflectdifferent linguistic generalizations as well as have different computational costs. solid overview of computational complexity and then discusses how unification gram-mars fit into the picture. Issues with recognition and parsing are discussed. Examples areworkedoutindetail(e.g.,showingrelevantdottedrules)forcontext-freegrammars and are then extended to unification grammars. This chapter could be skipped by those focused on theoretical and formal aspects, but it provides a practical view of the repercussions of the formal issues introduced earlier and is directly relevant for computational linguists.
 used in the book, a summary of preliminary mathematical notions, and solutions to selected exercises.
 especially in programs that have a grammar engineering track or that want to build on a strong formal language program. I also particularly recommend it for those working with unification grammars, especially with implementations of such grammars.

SimoneTeufel (University of Cambridge)
Stanford, CA :CSLI Publications (CSLI Studies in Computational Linguistics), 2010, xii+518 pp; hardbound, ISBN 978-1-57586-555-3, $70.00; paperbound, ISBN 978-1-57586-556-0, $32.50 Reviewed by Robert E. Mercer University of Western Ontario
Discourse models have received significant attention in the computational linguistics community with some important connections to the non-computational discourse com-munity. More recently, the importance of discourse annotation has increased as models generated with supervised machine learning techniques are being used to annotate text automatically. A primary area for annotation is science. The theme of Teufel X  X  book is an important contribution in these areas :discourse models, annotation schemes, and applications.

It extends Teufel X  X  Ph.D. thesis (Teufel 2000) with a decade of new work and updated references. The book is content-rich and meticulously written. In addition to presenting
Teufel X  X  discourse model, it also works as a good entry point into discourse models and annotation. Because each chapter is structured with background, new material, and a summary, each chapter can be read somewhat independently. Cross-references to other parts of the book are carefully included where warranted. This structure lends itself to using the book as a reference for each of the subtopics or as an introduction to the subject area as a whole, suitable as a textbook.
 mental assumptions and hypotheses. The fundamental assumptions arise from three observations that she has made regarding the literature. Scientific discourse contains descriptions of positive and negative states, contains references to others X  contributions, and is the result of a rhetorical game intended to promote one X  X  contribution. Chap-ter 2, on information retrieval and citation indexes, and Chapter 3, on summarization, provide the motivation for the main theme of the book :These two information-based endeavors can be enhanced with automated tools that incorporate an understanding of the rhetorical aspects of science writing.
  X  X ew Types of Information Access, X  introduces two new techniques, rhetorical extracts and citation maps, that are suggested as information navigation methods enhanced by knowledge of the discourse that contains the information being accessed. Rhetorical extracts are snippets that can be tailored to user expertise and navigation task. Citation maps are interactive citation indexes that have their citation links augmented with rhetorical or sentiment information.
 are used in the research described throughout the book :computational linguistics, chemistry, genetics, cardiology, and agriculture. The chapter focuses primarily on the computational linguistics corpus, on which most of the results in the book are based. S CI XML, Teufel X  X  markup language for science articles, is described.

Model (KCDM). Teufel gives reasons why the traditional discourse models are aban-doned in favor of her new model. In addition to it being a shallow method, she points out the important aspects of KCDM (compared to Rhetorical Structure Theory) :It is text-type-specific (scientific articles); no world knowledge is required; it has global (top-down) not local (bottom-up) relations; it is non-hierarchical (citation and summarization applications do not require a rich hierarchical structure).
 Claim Attribution (KCA), Citation Function Classification (CFC), and Argumentative
Zoning (AZ). The background and purpose of the schemes are carefully laid out. The annotation guidelines (coding manuals) are given in Appendix C.
 the quality of the annotation scheme using agreement among the annotators as a proxy for this measure. A good discussion of the measures of annotator agreement opens the chapter, followed by a detailed analysis of the four studies. Three of the four studies used three annotators, the other used 18 annotators. All studies used the computational linguistics corpus.
 tations of AZ, KCA, and CFC that are described in Chapter 11. Chapter 9 provides a comprehensive discussion of the various embodiments of meta-discourse, the text that concerns itself with the dialogue between the author and the reader rather than content-bearing text. Chapter 10 discusses the computable surface features that capture the important aspects of meta-discourse that are used by the automatic annotation methods.
Chapter 11 then introduces the reader to the standard supervised machine-learning methodology used to generate the statistical models that implement the automatic
AZ, KCA, and CFC annotators. Chapter 12 presents gold-standard, and extrinsic and subjective evaluations of these automatic methods. The gold standard is the human-annotated computational linguistics articles and the extrinsic task is rhetorical extracts. sults were based on the computational linguistics corpus. This chapter considers the disciplines of chemistry, computer science, biology, astrophysics, and legal texts. Two issues surface :the need to modify the original KCDM slightly, and the move from an absolutely domain-knowledge-free annotation to one which includes some high-level facts about research practices in the discipline.
 support tools for scientific writing, automatic review generation, scientific summary generation that moves beyond simple sentence extraction methods and summaries of multiple scientific documents, as well as integration of automatic AZ into a large-scale digital library. Chapter 15 provides the conclusion. In the first section it recapitulates the main themes of the book. This section also nicely serves as an introduction to the book, if so desired. Section 2 lists a number of areas that could lead to an improved automatic system.
 the annotation guidelines, and a catalog of lexical items and patterns useful in the discourse setting.
 modeling and annotation, and provides an important body of work to which other 444 researchers can add or compare their work. I think it is important to keep in mind the following few points while reading the book :First, Teufel comments that she is interested in a discourse model for the experimental sciences, yet her focus for much of the book is a corpus of computational linguistics papers. Also, the discourse model pro-posed is based on knowledge claims and rhetorical moves. This catholic view of what is science and the narrow view of structure may surprise some readers given the title of the book. Next, some of the fundamental decisions regarding the discourse model are heavily influenced by the requirements of the two motivational topics, leading one to question the full generality of the discourse model. As well, the range of rhetoric in science writing may be broader than anticipated by Teufel X  X  model X  X or example, the style found in the geology discipline is more cumulative than critical (Heather Graves, personal communication). And finally, some researchers (White 2010) argue that the domain-knowledge-free annotation dictum, although loosened slightly by Teufel, may need to be further relaxed in order to produce a more accurate gold standard, regardless of the automatic system X  X  access to the same domain knowledge.
 References
 JanvanEijck  X  andChristinaUnger  X  ( CWI, Amsterdam and Utrecht University;  X  University of Bielefeld) Cambridge: Cambridge University Press, 2010, xv+405 pp ;hardbound, ISBN 978-0-521-76030-0, $99.00 ;paperbound, ISBN 978-0-521-75760-7, $40.00 Reviewed by Robin Cooper University of Gothenburg There has been a recent intensification of interest in  X  X emantics X  in computational linguistics. I write the word in scare quotes because there are very different views of what computational semantics is. Broadly, it divides into the view that word meaning can be modeled in distributional terms and the view that meaning is to be viewed in terms of model theory of the kind employed in formal semantics deriving from the seminal work of Richard Montague (1974). This book is firmly placed in the latter logic-based semantics camp.
 are three basic texts you can go to: Blackburn and Bos (2005), which uses Prolog ;Bird, Klein, and Loper (2009, chapter 10), which covers the essential aspects of Blackburn and Bos using Python within the popular Natural Language Toolkit (NLTK) ;and the present book, which uses the functional programming language Haskell. All three of these references will teach you both semantics and the required programming skills at the same time. So how do you choose between them? ming language, seems like a natural choice for logic-based semantics and, indeed, as Blackburn and Bos show, it provides support for writing concise and transparent code which is close to the kind of formalism used in theoretical logic-based semantics. It is not without its problems, however. Prolog variables are associated with unification-based binding, which is not the same as the kind of binding by quantifiers and operators that is used in logic. A second problem is that Prolog is a relation-based language and does not have a direct implementation of functions that return a value. Formal semantics building on Montague X  X  work makes heavy use of the  X  -calculus with  X  -expressions denoting functions, and semantic composition is largely defined in terms of function-argument application. Blackburn and Bos implement a version of the  X  -calculus in Prolog but this leads to a third problem: Formal semantics uses a typed version of the  X  -calculus, yet standard Prolog is not a typed language (apart from making basic distinctions between integers, lists, etc.). Python, being object-oriented, allows a partial solution to the typing problem and it also has functions. It is a very flexible language and allows transparent coding of semantic formalisms by its powerful string processing. But coding semantic formalisms in terms of strings, although providing a great deal of flexibility, does not give you the feeling that the language is providing support for or insight into the logical operations that are being performed.
 based on the  X  -calculus, with a strict typing system that requires the programmer programming language (in the sense of being based on functions that take arguments and return values) it comes with a proper logical notion of variable binding rather than the unification variety. Furthermore, it has static typing, which means that it checks your program for type errors at compile time. This means that if you have made an error you do not have to wait until it shows up in some particular example at run time (perhaps after you have delivered your system to your client). The core of this language then provides exactly the tools that the logical semanticist needs. Here the semanticist is getting real support from the language, not just the flexibility to implement what she needs. A few words of caution are appropriate, however. If the type system that you want to implement does not exactly match the type system of the programming lan-guage, you may well be better off doing your implementation in a non-typed language so that the two type systems do not interfere with each other, though how you feel about this issue depends very much on your programming background and preferences.
Van Eijck and Unger show, in my view magnificently, that Haskell X  X  type system does match the type system used in the classical approach to formal semantics. Some might feel that there is a slight cost, in that direct implementation in Haskell involves using formal semantics. Haskell X  X  syntax is extremely slick and concise. For example, it seems to follow the philosophy:  X  X on X  X  put any kind of brackets around expressions, when you could simply write them next to each other and the context will determine what that means. X  This can be initially unnerving to linguists used to more florid notations with an element of redundancy.
 logic-based computational semantics can be connected to off-the-shelf first-order the-orem provers and model builders such as Prover9 and Mace and showing how this connection can be exploited in semantics and discourse processing. This work has also been incorporated into the NLTK version of semantics. Although you will find discus-sion of inference in van Eijck and Unger X  X  book, you will not find accounts of how to connect to external theorem provers. This is probably a principled decision. If you learn semantics from the Blackburn and Bos or NLTK texts you might be forgiven for coming to the conclusion that the main aim of using the  X  -calculus in linguistic semantics is to get a compositional treatment that will always allow you to reduce a first-order formula that can be sent off to a fast first-order theorem prover or model builder and you may just miss their occasional references to the fact that not all natural language sentences correspond to first-order formulae. This means that you will not be able to adequately treat sentences containing certain generalized quantifiers such as most in most customers prefer this product or intensional verbs such as want in I want to go to Chicago .Van
Eijck and Unger, on the other hand, have a fairly detailed discussion of generalized quantifiers in Chapter 7 and devote the whole of Chapter 8 to intensionality. Although it would, of course, be a challenge to cover all the major results of formal semantics within a single introductory textbook on computational semantics, at the end of this book one has the impression that we have the tools we need to go further, whereas with the other introductions we have the impression that computational semantics deals with expressions of natural language whose interpretations can be represented in first-order logic.
 logical setting. Chapter 1 deals with the formal, logical approach to natural language, and Chapter 2 with the tools used in functional programming. Chapter 3 provides a practical introduction to the parts of the programming language Haskell that you need to know in order to be able to follow the book. Chapters 4 and 5 deal, respectively, with 448 syntax and semantics for fragments. The intention here was, I imagine, to introduce things gently by starting with the syntax of games rather than of languages. I was not sure that this was the most useful way to introduce things for linguists. It was not entirely clear to me what the syntax of a game was meant to represent (the game itself?, the language you use when playing the game or when describing the game?) and this was not helped by having the semantics coming in the next chapter after discussing the syntax of a fragment of English, propositional logic, predicate logic, and an extension of predicate logic with function symbols. Chapter 6 deals with model checking with predicate logic, and finally with Chapter 7 we get down to some serious compositional semantics for natural language. The remaining chapters build on this and deal with more advanced topics: intensionality in Chapter 8, parsing in Chapter 9, quantifier scope in Chapter 10, continuation semantics in Chapter 11, discourse representation in Chapter 12, and communication and information in Chapter 13. If you make it to the end, the Afterword tells you to treat yourself to a beer as a reward.
 programmers and provides basically all you need to know if you are coming from one of these areas and do not know about the other two. This is a major achievement and practically everything you need is there, clearly and concisely expressed. The learning curve is steep, however, and there is a great deal of complex material to digest. If you are on your own without a sympathetic teacher, you might feel that you have earned something stronger than a beer by the time you get to the end.
 introductory textbook to computational semantics that every serious student of the field should study, it represents a mature major research contribution demonstrat-ing the close relationship between classical formal semantics and modern functional programming.
 References
 AntalvandenBosch  X  andGosseBouma  X  (editors) ( Tilburg University and  X  University of Groningen) Berlin: Springer (Theory and Applications of Natural Language Processing series, edited by Eduard Hovy), 2011, xii+279 pp; hardbound, ISBN 978-3-642-17524-4, $124.00; e-book, ISBN 978-3-642-17525-1; paperbound, $24.95 or  X  24.95 to members of subscribing institutions Reviewed by Constantin Or  X  asan University of Wolverhampton Processing and presentation of multimodal information was one of the important di-rections pursued by researchers in the areas related to information processing and management in the first decade of this century (Stock and Zancanaro 2005; Maragos, Potamianos, and Gros 2008; Lalanne et al. 2009). The Interactive Multimodal Informa-tion eXtraction (IMIX) Programme, a research program that ran between 2004 and 2009 and was funded by the Netherlands Organisation for Scientific Research (NWO), ad-hered to this direction of research. This book contains a collection of articles describing research carried out in the IMIX Programme. Given the large scale of the program, the book covers only parts of it, arguably the most important ones: question answering, (spoken) dialogue systems, and human X  X achine interaction.
 the IMIX Programme and the demonstrator developed by it. The main purpose of the program was to bring together research groups from the Netherlands to build an interactive multimodal question answering (QA) system that is able to answer gen-eral encyclopedic medical questions. The IMIX Programme funded seven individual projects that worked in a common field and contributed to a common demonstrator. The fact that these projects ran largely independently is also apparent from the book because there are few links between its chapters.
  X  X he IMIX Demonstrator X  (Dennis Hofs, Boris van Schooten, and Rieks op den Akker). The demonstrator showed users a fully functional system and allowed them to ask questions using text, speech, and gestures. The answers produced by the system were presented in the form of text, speech, or images, and could be used in follow-up ques-tions. The article features a detailed description of the architecture, as well as several screenshots and diagrams; these can be useful to researchers who want to find out more about the demonstrator. In addition to the technical details, there is an interesting discussion about the role of demonstrators in large projects and problems that need to be addressed when building them. I think this brief discussion could be very useful for anyone involved in a medium or large project that includes several research groups and needs to build demonstrators.
 interaction discussed in this book. First, the Vidiam (DIAlogue Management and the VIsual channel) project is described in the article entitled  X  X orpus-Based Develop-ment of a Dialogue Manager for Multimodal Question Answering X  (Van Schooten and
Op den Akker). In addition to the corpora built in the project and the dialog manager developed on the basis of these corpora, the article also contains a very good discussion about how it is possible to integrate a dialog manager with a QA engine as a way of developing an interactive QA system. I am not aware of any other articles that contain all the information presented here in one place and in such detail. The second article in this part,  X  X ultidimensional Dialogue Management X  (Simon Keizer, Harry Bunt, and
Volha Petukhova), is more theoretical and presents a dialog manager built using the framework of Dynamic Interpretation Theory (Bunt 2000) which is able to both interpret and generate utterances using dialog acts. The article also presents briefly the way in which this dialog manager was integrated in the IMIX demonstrator.
 part of the book:  X  X using Text, Speech, and Images. X  Both articles in this part present work done in the IMOGEN (Interactive Multimodal Output GENeration) project, 1 one of the subprojects embedded in the IMIX Programme that focused on producing mul-timodal presentations that combine text, speech, and graphics. Only the first article focuses on the multimodal aspect of the project, however. The other one discusses only text processing. The article  X  X xperiments in Multimodal Information Presentation X  (Charlotte van Hooijdonk, Wauter Bosma, Emiel Krahmer, Alfons Maes, and Mari  X  et
Theune) presents three experiments for finding the appropriate way of combining text and images when answering questions from the medical domain. In one of these experiments, the multimodal answers are produced automatically. The other article,  X  X ext-to-Text Generation for Question Answering X  (Bosma, Erwin Marsi, Krahmer, and
Theune), discusses sentence fusion and could fit very well in a book dedicated to text summarization, as the method presented there is tested not only on data specific to IMIX, but also on the DUC 2005 data. 2 swering. X  It contains five articles, none of which describe a full QA system. Instead, as the title suggests, they focus on various ways of processing texts that can help with answering questions. One common feature of these articles is that they describe methods to extract entities or relations between entities from texts. Most of the articles also briefly discuss how this information is used in QA systems.
 in computational linguistics, but when they were proposed a few years ago many of them were rather innovative. For brevity, I give only a succinct indication of the methods presented in the articles.  X  X utomatic Extraction of Medical Term Variants from Multilingual Parallel Translations X  (Lonneke van der Plas, J  X  org Tiedemann, and
Ismail Fahmi) describes how to acquire medical terms and their variants from parallel corpora.  X  X elation Extraction for Open and Closed Domain Question Answering X  (Bouma, Fahmi, and Jori Mur) shows how it is possible to extract relations between entities using dependency paths in a large collection of newspaper articles and in a much smaller and closed domain corpus of medical documents. A sequence label-ing method for entity recognition is presented in the article  X  X onstraint-Satisfaction Inference for Entity Recognition X  (Sander Canisius, Antal van den Bosch, and Walter
Daelemans). Large newspaper corpora and the Web are used in  X  X xtraction of Hyper-nymy Information from Texts X  (Erik Tjong Kim Sang, Katja Hofmann, and Maarten de
Rijke) to determine hypernymy relations between entities. The last article in the fourth 452 part,  X  X owards a Discourse-Driven Taxonomic Inference Model X  (Piroska Lendvai) looks at how the structure of discourse can be used for knowledge discovery from encyclopedic texts. All the articles are well written and could be very interesting for researchers working on information extraction.
 review panel of IMIX (Eduard Hovy, Jon Oberlander, and Norbert Reithinger) who give a very good overview of the project, providing information that is not covered in any other article of the book. For example, it expands on the multimodal research carried out in the program and presents some details from the point of view of project management. An objective evaluation of the overall program is also included.
 the research presented here is rather old. The IMIX Programme effectively ended in 2008, so it can be argued that most of the articles refer to work that is more than 5 years old. The authors of the epilogue praise the researchers involved in IMIX for the large number of publications they produced. This means that most of the information presented in the book was already published in one form or another somewhere else. Despite this, the book compiles in one place information about the IMIX Program which otherwise could take a while to collect.
 modal question answering . Each of these topics is presented individually, but with the exception of the article about the IMIX demonstrator, they are not discussed as a whole. I was particularly disappointed by how little space was dedicated to multimodal processing.
 or less stand-alone. To achieve this, they all present brief background information about the IMIX Programme. Despite the extra space required for it and the overlap between the information presented in the articles, this is not necessarily bad because it means that researchers who do not have the time to read the whole book can focus on only the articles that are most relevant for them.
 processing of Dutch texts. Researchers in question answering, dialog processing, and information extraction would also benefit from the book.
 References

ReinhardK  X  ohler (Trier University) Berlin and Boston: De Gruyter Mouton (Quantitative Linguistics series, edited by
Reinhard K  X  ohler, Gabriel Altmann, and Peter Grzybek, volume 65), 2012, x+224 pp, hardbound, ISBN 978-3-11-027219-2, e 99.95, $140.00 Reviewed by
Chunshan Xu  X  and Haitao Liu  X  Anhui University of Architecture,  X  Zhejiang University
Quantitative linguistics (QL) is a discipline of linguistics, that, using real texts, studies languages with quantitative mathematical approaches, aiming to precisely describe and explain, with a system of mathematical laws, the operation and development of language systems. Later in this review, we will address the relationship between QL and computational linguistics. Quantitative Synta xAnalysis is a recent work on QL by
Reinhard K  X  ohler that not only provides a comprehensive introduction to the work of QL on the syntactic level, but also sketches the theoretical grounds, the research paradigm, and the ultimate goals of quantitative linguistics in general. enables language users to code structures instead of ideas as wholes. A text embodies a complex cognitive formation and meets several basic requirements in human communi-cation, which implies that language is not autonomous, but a dynamic communicative system used by human beings. Hence, the ultimate understanding and explanation of syntax (and the whole language system) depends on usage-based investigation of the cognitive basis and the functional requirements of language, which is somewhat neglected in many mainstream syntactic studies. Even in those cases where explanatory power is acknowledged as the ultimate goal of linguistic investigation, the necessary knowledge is still required as to what a scientific linguistic theory is and how such a theory may be built. So far, it is rare for quantitative means to be used in syntactic study. One reason is that many syntacticians are too addicted to the enshrined tradi-tional paradigms that have been proven to be somewhat inadequate when it comes to processing real texts. And that is why in computational linguistics,  X  X evout executors of the belief in strictly formal methods as opposed to statistical ones do not have any chance to succeed X  (page 4).
 begins with an explanation of the difference between quantitative linguistics and the formal branches of linguistics that have once been widely used in computational linguistics: QL is concerned with the quantitative properties important for under-standing the development and the operation of linguistic system, whereas the formal branches of linguistics use only qualitative mathematical means and formal logics to model structural properties of language, overlooking, in most cases, the aspects of systems that exceed structure, viz., functions, dynamics, and processes. K  X  ohler points out that the successes of modern natural sciences (the exact, testable statements, the precise predictions, and the copious applications) all derive from their instruments and their advanced models. This implies that these instruments and models, of which the quantitative parts of mathematics (probability theory and statistics, function theory, differential equations) are indispensable ingredients, are worth integrating into lin-guistics, which is the aim of QL.
 the important works of quantitative syntactic analysis. In the first section, K  X  ohler gives a long list of the important syntactic units and properties defined within the frameworks of both phrase structure syntax and dependency syntax; this reflects the fact that researchers in both fields have been engaged in some fruitful quantitative studies. In Section 3.2, he defines quantitation of syntactic concepts as counting the objects under study, because syntactic analysis investigates only discrete objects. Sec-tion 3.4 is a detailed review of the important works on various syntactic phenom-ena within the frameworks of both phrase structure syntax and dependency syntax, including sentence length, probabilistic grammars and probabilistic parsing, Markov chains, Frumkina X  X  law on the syntactic level, distribution of dependency distance, and distribution of dependency types, and so on. These quantitative models, which have been empirically corroborated with real texts, or sometimes treebanks and dictionaries (of various languages), can be linguistically, cognitively, or functionally interpreted X  a rare achievement in the past statistical investigations of language. Apart from the models concerning probabilistic grammars and Markov chains, which have already been widely used in computational linguistics, there are some other works that may also have practical applications in various fields. For example, the mathematical model of sentence length, which describes the probability of neighboring length classes as a function of the probability of the first of the two given classes, may contribute to practical applications such as text classification and the measurement of text compre-hensibility, and so forth. The frequency studies of word and syntactic constructions have obtained many results useful for language teaching, the construction of parsing algorithms, and estimation of effort of (automatic) rule learning, and more. The syntactic studies on Frumkina X  X  law, which is concerned with the number of text blocks with x occurrences of a given syntactic element or category, may benefit certain types of com-putational text processing if specific constructions or categories can be differentiated and found automatically by their particular distributions. One advantage of QL is that all its findings are mathematically formulated and linguistically interpreted, which at least makes it possible to be used in constructing models necessary for computational linguistics.
 he believes that  X  X here is not yet any elaborated linguistic theory in the sense of the philosophy of science X  (page 21). Building such a theory begins with  X  X lausible hy-potheses, X  which may become laws when sufficiently attested and may then be further integrated into a coherent system. This is the process of setting up a scientific theory, as succinctly summarized in the title of this chapter:  X  X ypotheses, Laws, and Theories. X  theory, the process in which  X  X lausible hypotheses X  are deduced, interpreted, and empirically attested before finally becoming laws. In Section 2, K  X  ohler introduces the foundation of his synergetic linguistics, which views language as a dynamic, self-organizing, and self-regulating system where the so-called enslaving principle and order parameters are the crucial elements. On this basis, the author builds a synergetic syn-tactic model in Section 4.2.7 with certain modeling principles. Within the framework of phrase-structure syntax, eight properties of syntactic constructions and four inventories are chosen to build this model, which are linked together by laws resulting from the verified hypotheses and subject to the regulation of some order parameters. 696 depends heavily on real texts and mathematical tools. Therefore we believe it is worth-while to briefly clarify the differences and the relations between QL and corpus lin-guistics, on one hand, and between QL and computational linguistics, on the other hand. In comparison with quantitative linguistics, corpus linguistics is in fact more of a research methodology rather than an independent linguistic discipline, reflecting a shift of focus from competence to performance, from introspection to empirical study. This makes both the common ground and the difference between corpus linguistics and QL, which aims to quantitatively and mathematically explore, on the basis of real texts and treebanks, the fundamental laws governing the structure and evolution of language, and integrate them into a systematic theory capable of explanation and prediction. ture of natural languages from a formal, mathematical, and computational point of view. Compared with QL, CL seems to be more interested in research that can have direct applications in such fields as language understanding, language generation, machine translation, and so forth, than in the explanation of language structure, op-eration, and evolution. Traditionally, the mathematical models used in CL are derived from the linguistic theories via the process of formalization. Due to the limitations of the qualitative mathematical models, however, the statistical models, most of which so far fail linguistic interpretations, have recently become dominant in the field of computational linguistics. That is perhaps why Shuly Wintner (2009, page 641), in a  X  X ast Words X  article in this journal, called for  X  X he return of linguistics to computational linguistics, X  implying that the advances in the field of CL may ultimately lie in the advances in the understanding of language itself.
 discipline in which many works are heavily oriented towards engineering and weakly grounded in linguistics. The formal linguistic models seem now outshone by the purely statistical paradigms, as the result of their inadequacy in processing real-world lan-guages. Of course, this means no renouncement of the value of the traditional formal models. But it is obvious that considerable updating and enrichment are necessary for these formal models if they are to play significant roles in the future. In this regard,
QL, which has a solid linguistic foundation, may help by providing quantitative cues (which are linguistically interpretable) to improve the performance in NLP, as has been illustrated in the case of probabilistic grammar that ingeniously integrates quantitative, statistical devices into qualitative linguistic models. QL has provided some useful mod-els for CL. And it is reasonable to believe that it will continue to do so in the future, though some of its achievements seem now not ready to be directly used in CL. tion from computational linguistics, it is potentially a proper answer to Wintner X  X  call for the return of linguistics to computational linguistics. CL aims to replicate in comput-ers the patterns of human language behavior, whereas QL endeavors to mathematically and quantitatively reveal the laws and the principles that govern human language behavior X  X his is a relation between theory and practice. The success of a scientific discipline is usually based on precise models that are well-grounded in profound under-standing of the object of study. Computational linguistics is no exception. Two features of QL are hence noteworthy. One is that it aims to explain, within a certain linguistic framework, the operation and the evolution of language by uncovering the systematic cognitive and functional regulations that underlie human languages. The other is that it tries to mathematically model, with systematic and precise quantitative laws, these regulations and the resulting mechanism of language. In view of these two features, we believe that the success of QL will somehow and somewhat boost the studies in the field of CL and that the communication between QL and CL is and will be not only possible but also mutually beneficial. This is why we hold that it is worthwhile to recommend this book to researchers in the field of computational linguistics, a book presenting a panorama of QL in general and discoveries on the syntactic level in particular. References
 ManfredStede University of Potsdam
Morgan &amp; Claypool (Synthesis Lectures on Human Language Technologies, edited by Graeme Hirst, volume 15), 2011, ix+155 pp; paperbound, ISBN 978-1-60845-734-2, $40.00; ebook, ISBN 978-1-60845-735-9, $30.00 or by subscription Reviewed by Bonnie Webber University of Edinburgh
Discourse is coming in from the cold. After years of being ignored by researchers in other areas of computational linguistics and language technology, many of these same researchers are beginning to think that their own work could benefit from treating text as more than just a bag of sentences. That is, they are beginning to think that discourse offers some low-hanging fruit X  X chievable improvements in system performance that exploit either aspects of text structure or the context that text establishes and uses for efficient referring and/or predicational expressions. new zeitgeist andprovidesanintroductiontodiscourseforresearchersincomputational linguistics or language technology with little or no background in the area. This clear and timely monograph consists of a brief introduction to discourse, a meaty chapter on each of the three aspects of discourse processing that hold most promise for language technology, and a brief conclusion on where discourse research might go in the future. I will go through the three major chapters, and then make some general remarks. Chapter2
Chapter 2 addresses two distinct types of large-scale discourse structure: structure that follows from a text belonging to a particular genre, and structure that follows from the topic (or topic mix) of a text. The genre of a text affects features such as style and register. What is relevant here is structure that genre may confer on a text. Stede suggests that some, but not all, texts inherit large-scale structure from their genre, calling some unstructured ,some structured ,andsome semi-structured .Asareader,I did not find this distinction useful, because all text that belongs to a genre seems to get some large-scale structure from it. On the other hand, all or part of this structure might simply not be manifest in the kind of lexico-syntactic features that automated systemsregularlyrelyonfortextsegmentation.Asacaseinpoint,althoughStedeoffers the text Suffering (used as a running example throughout the book) as an example of unstructured text, like other instances of Comments in the Ta lk o f t h e To w n section of the New Yorker magazine, its large-scale structure comprises a  X  X ook X  aimed at getting the reader X  X  attention, followed by a short essay that concludes with a serious point.
Although ways of attracting a reader X  X  attention may not have specific lexico-syntactic features, it might still be possible to recognize the transition between  X  X ook X  and essay, and essay structure itself is what ETS X  X  eRater system (Burstein and Chodorow 2010) aims to recognize and evaluate.
 and of film reviews. Here researchers have already shown that language technologies suchasinformationextractionandsentimentanalysisbenefitfromtakingsuchstructure into account, so this is entirely appropriate for the book X  X  target audience. More on genre-based functional structure and its use in producing structured biomedical abstracts can be found in the recent survey of research on discourse structure and language technology by Webber, Egg, and Kordoni (2012).
 with patterns of topics. Such structure is often found in expository writing such as encyclopedia articles and travel pieces. Here, changing patterns of content words corre-late well with changes in topic, rendering them useful for the many approaches to text segmentation that are well-described in this half of the chapter. Because the discussion hereofprobabilisticmodelsfortopicsegmentationisrathershort,thereaderwhowants to know more should consult the excellent survey of topic segmentation methods by Purver (2011).
 Chapter3
Chapter 3, entitled Coreference Resolution , addresses more than this, dealing with the resolution of other expressions whose reduction is licensed by the discourse context, such as bridging reference and  X  X ther X  reference, which Halliday and Hasan (1976) call comparative reference because it occurs with comparative forms such as  X  X arger fish X  and  X  X  more impressive poodle, X  as well as with  X  X ther, X   X  X nother, X  and  X  X uch. X 
Stede justifies inclusion of this chapter for two reasons X  X he close connection between coreferenceresolutionandtopicsegmentationandthebenefitstotextanalysisprovided by having its pronouns resolved. But another reason must be the link mentioned earlier between text and context: Discourse creates the context in which context-reduced expressions make sense, so it falls naturally within the tasks of discourse processing to resolve them, either through modeling context explicitly or through the use of proxies.
 their forms and their functions. This is followed by an important section on corpus annotation(Section3.2),includedbecause(asStedenotes)whathasbeenannotatedand why it has been annotated strongly determines what expressions are resolved and how.
This section identifies many of the problems in coreference annotation that have been raised in the literature, but recognizes that research has to make use of the resources that exist and not just the resources it wants. Several of these are indicated at the end of the section, reminding one that it would have been useful to have some pointers in
Chapter 2 to corpora available for genre-based segmentation (such as Liakata X  X  ART corpus) 1 or for topic-based segmentation.
 entity-basedcoherence(Section3.3)andthendiscusseshowtoidentifywhenapronoun or definite noun phrase should be treated as anaphoric (Section 3.4) as groundwork for discussion of anaphora resolution (Sections 3.5 X 3.7). Missing from the discussion of detecting non-anaphoric (pleonastic) pronouns is mention of Bergsma X  X  recent system
NADA for doing this (Bergsma and Yarowsky 2011). 2 918 nominal anaphora (Section 3.5) and then supervised machine learning methods for anaphora resolution (Section 3.6). The latter follows the structure (albeit not the con-tent) of Ng X  X  survey (2010), in discussing mention-pair models , and then entity-mention models . Whereas Ng then discusses ranking models , including his cluster ranker (Rahman and Ng 2009), which is conceptually similar to the Lappin and Leass (1994) approach described in Section 3.5, Stede discusses a range of more recent models, most of which are subsequent to Ng X  X  survey.
 known problems in doing so. A good complement to this is Byron X  X  too-little-known discussion of problems in the consistent reporting of such results (Byron 2001). Chap-ter 3 concludes with a section on Recent Trends , which would also have been useful in Chapter 2.
 Chapter4
The fourth and longest chapter deals with semantic or pragmatically oriented coherence relations that hold between adjacent text spans or discourse units . Whereas the previous two chapters were essentially theory-neutral, the presentation in Chapter 4 largely reflects the perspective of Rhetorica lStructure Theory (Mann and Thompson 1988). RST takes a text to be a sequence of elementary discourse units that comprise the leaves of a tree structure of coherence relations between recursively defined discourse units. RST also assumes that one of the arguments to a coherence relation may be more important to the speaker X  X  purpose than the other, calling the former the nucleus and the latter, the satellite .
 sectionthatexplainsandmotivatescoherencerelations,eachsubsequentsectionconsid-ers the next task in an RST analysis X  X egmenting a text into elementary discourse units (Section 4.2), recognizing which (adjacent) units stand in a coherence relation and what (single) relation holds between them (Section 4.3), and finally, inducing the overall tree structure of coherence relations that hold between recursively defined discourse units (Section 4.4). All these tasks are well described, both from a theoretical perspective and in terms of automated procedures for carrying them out. Coverage of relevant work is very high.
 morerecentworkonidentifyingcoherencerelationsdoesnotfallwithintheframework of RST, and thus doesn X  X  adhere to several of its assumptions X  X n particular, that a text is divisible into a covering sequence of elementary discourse units, that only one relation can hold between discourse units, that the arguments to a coherence relation must be adjacent, that one argument to a coherence relation may intrinsically convey information that is more important to the speaker X  X  purpose than the other, and that coherence relations impose an overall tree structure on a text in terms of recursively defined discourse units.
 its  X  X omewhat modest annotations X  (page 126), the discussion is framed in terms of
RST tasks, whereas the assumptions underlying the Penn Discourse TreeBank reflect its concerns with a quite different set of tasks involved in recognizing coherence relations.
The first task requires finding evidence for a coherence relation (in the form of a discourseconnectivesuchasacoordinatingorsubordinatingconjunctionoradiscourse adverbial,orintheformofsentenceadjacency)andthendetermining(1)iftheevidence does indeed signal a coherence relation, given that evidence is often ambiguous; (2) if it does, what constitutes its arguments; and (3) what is its sense. Although Chapter 4 covers some of this work (Dinesh et al. 2005; Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler and Nenkova 2009; Prasad, Joshi, and Webber 2010), its appearance withinthecontext of adiscussion of RST-tasks may lead tosome confusion. ing coherence relations, including problems with associating a large text span with a single recursive structure of coherence relations and problems with inter-annotator agreement.
 Summary
For its intended audience, this monograph will serve as a compact, readable intro-duction to the subject of discourse processing. The relevant phenomena are presented clearly, as are many of the computational methods for dealing with them. What readers won X  X  get is criteria for choosing among the methods or an understanding of what each method is good for. This problem may reflect the absence of comparable performance results and useful error analyses in the original publications, however.
 cessing, and pointers to more of the resources available to researchers interested in discourse structure. This is where the additional resources I have mentioned may prove complementary.

Some monographs in the series have one, whereas others (like this one) don X  X . Because the series appears in both electronic and physical format, one could excuse the former not having an explicit index, since in most cases, one can get away with the basic search facility in the Adobe Reader. Nothing similar is available for the nicely sized physical monographs. Their authors should be strongly encouraged to provide them.
 References 920
