 The Internet has become one of the biggest storages of information in the world. More than 92% of all newly created informati on is stored and accessible via electronic media, Internet being the prominent one [1]. At the same time we come across a great physiological well-being and social involvem ent [2]. One of the causes, indicated by researches, is the information overload [3]. 1.1 Vision Many attempts have been made in order to enhance user experience of the Internet, especially of the World Wide Web. Those include: information retrieval and filtering popular personal information filtering syst ems; information extraction systems, robust, but focusing on atomic information, and hardly understandable by users without a degree in computer science. Other technologies, such as RSS (RDF Site Summary), trying to utilize content of the World Wide Web, have also been proposed. Goal of the mentioned technologies is to provide Internet users with information of their interest in a very compact form, without cluttering the view. The problem with technologies like RSS is however a requirement of investing additional efforts in preparing Web content. Additionally, RSS feeds are slowly evolving from streams of compact information to streams of URLs pointing to traditional webpages, without any additional information (like abstracts of the pages). The challenge of current World Wide Web is to provide users with a system that will limit information presented to the users to only relevant one, with ease of use similar to those of search engines, robustness of information extraction systems, but providing more complex information ( content ), similar to one originally provided by RSS feeds, a system which will not require any additional work in terms of annotating Web content or using new protocols, simply using the existing Web. 1.2 Research Challenges The main research challenge described in this paper is to answer the question, least as robust as similar information extraction methods and at the same time easy to use. The latter challenge  X  ease of use  X  is achieved when the system does not require users to be accustomed with any programming languages or to be able to create and robustness  X  can be measured with su ccess ratio of extraction procedure and compared to other existing extraction methods, which is discussed in the paper. 1.3 Contribution use system for building user content extraction expressions and integrating the extracted content. At first sight, building such a system should be straightforward. However there are a number of difficulties that are discussed in this paper. The prototypical system, addressing these difficulties, has been implemented. It is based on technologies used in information retrieval and information extraction, addressing technologies used in XML and HTML document processing. It is also inspired by the works in webpage clipping and positioning. The system fulfills the following tasks: it allows users to point interesting content on webpages, which should then afterwards users to construct a personal portal (therefore the system name  X  myPortal) out of mind, so that whenever content on selected pages changes, the user will always get the interesting one, and whenever other radical changes (for instance structure of the document) are introduced, the system will maintain high effectiveness of content extraction. 
The second contribution is the preliminary evaluation of the robustness of the proposed extraction and integration method in comparison to other state-of-the-art and widely adopted extraction technique. We are currently continuing the evaluation, complete results of which will be published in later stages of the research. 1.4 Structure concepts addressed in this work, namely content extraction and integration. Section requirements, outline of the process, and problems encountered. Section four presents some examples of using the new method. In section five we discuss potential future directions of the work. The paper concludes with sections six and seven, where contribution, possible usage scenarios of the method, and information about supporter of the research are shortly presented. Several attempts have been made in order to improve user experience while browsing the Web by limiting the displayed content only to the most relevant one. Information accessing the source document, it may be processed using information extraction methods to get very specific information or using Web page clipping tools to limit the extracted content, perhaps due to perceiving it as a simple task. The most challenging problem nowadays is to provide users with robust tools for content extraction, which will prove usable in everyday Internet usage. 2.1 Content Extraction Content extraction is understood as extracting complex, semantically and visually distinguishable information, such as paragraphs or whole articles from the Web. It borrows from information extraction methods used in the World Wide Web environment, and especially from Web data extraction methods. The most comprehensible survey of Web data extraction tools has been provided by Laender et al. [4], there are however other ones, also relevant to our study. 
The WebViews system [5] is a GUI system that allows users to record a sequence generated. The system is limited to extracting data from tables. IEPAD [6] is a system used to automatically extract repetitive subsequences of pages (such as search IEPAD uses PAT trees to identify repetitive substructures and is prone to specific types of changes in subsequent substructures (for instance changing attributes of HTML tags, additional symbols between tags). Annotea [7], on the other hand, is a system designed not for content extraction, but for its annotation. The work provides a description of an approach of addressing specific parts of HTML documents. The authors present the method on XML documents, implicitly assuming that the conversion from HTML to XML representation has been done. As the authors point themselves, the method is very sensitive to changes in the document, which makes it usable only in addressing content of static documents. eShopMonitor [8] is a complex system providing tools for monitoring content of Web sites. It consists of three components: crawling system, which retrieves interesting webpages; miner, allowing which executes queries on extracted data an d then provides user with consolidated results. The miner uses XPath expressions to represent interesting data. ANDES (A Nifty Data Extraction Systems) [9] extracts structured data using XHTML and XSLT technologies. The author of this system decomposes the extraction problem into five sub-problems: website navigation, data extraction, hyperlink synthesis, structure synthesis, data mapping, and data integration. WysiWyg Web Wrapper Factory generating retrieval rules and a declarative language for building extraction rules. W4F uses a proprietary language, making it hard to integrate with other extraction systems. WebL [11] is a data extraction language. It is possible to represent complex queries (such as recursive paths and regular expressions) with it, however the language provides very limited means to address XML documents, particularly it doesn X  X  support XSLT templates and XPath expressions. Chen, Ma, and Zhang [12] propose a system that clips and positions webpages in order to display them properly on small form factor devices. They use heuristics in order to identify potentially interesting content. Their clipping methods, according to a set of 10 X 000 analyzed HTML pages, behaves perfectly (no problems in page analysis and splitting) in around 55% of documents. Out of remaining 45%, some 35% percent documents cause problems in page splitting, and the final 10% generates errors in both page analysis and splitting. Other possibly interesting systems include: WIDL [13], Ariadne [14], Garlic [15], TSIMMIS [16], XWRAP [17], and Informia [18]. It is important to note, that none of the mentioned systems was designed explicitly to extract previously defined content from dynamically changing webpages. 2.2 Content Integration Content integration is a concept that has not been very well researched so far. There term content integration to refer to the integration of operational information across enterprises [19]. The authors understand content as semi-structured and unstructured information, and content integration deals with sharing such information. Stonebraker and Hellerstein describe a Cohera Content Integration System, helpful in content integration, which consists of the following components: Cohera Connect, providing browsing and DOM based wrapper generation functionality, Cohera Workbench for content syndication, and Cohera Integrate for distributed query processing. 
There are currently no publicly known appr oaches that allow building the so called  X  X ntelligent portals X . Simon and Shaffer [20] enumerate nine providers of integration services, however only two of them are actually providing services, the other ones are not operating. One provider is giving access to information integrated from limited (such as bank accounts). A careful examination reveals that both of them use extraction rules tailored to specific content providers. This approach limits application of the proposed tools. The users cannot actually build personal portals, they can only choose from information blocks prepared in advance. In order to reach the goal described in introductory section (high relevance of information, ease of use, robustness, providing content (as opposed to information), similar to RSS feeds, not requiring any additional work on server side), we have divided the goal into three problems: navigation, content extraction, and content integration. Although the three problems were addressed independently, implemented solutions to them constitute one consistent system, named myPortal. 3.1 Navigation simplest possible manner. Therefore, it has been implemented as a standard Web browser. The first difficulty is maintaining the context of user navigation. During navigation some information about its context is stored on the client machine, other tasks that require some data persistence between browser calls to the remote website. As opposed to WebViews [5], myPortal doesn X  X  record complete sequence of navigation to a desired webpage. However, it stores all relevant information in the context of navigation (client side session or other cookies, HTTP request headers information, and HTTP variables with corresponding values  X  POST and GET data). Therefore when navigating for instance to an e-store, all personalization information is used in myPortal as well (no need to log in to the store etc.). 
While navigating, the users have an option to formulate their information needs (in a form of queries). Some of the systems described in Section 2 require usage of complex languages, providing only limited support for the user. In our system this has been solved by a simple point-and-click interface. Move of the mouse pointer over a document can be reflected on the respective nodes in the document DOM tree. This information can be used further to highlight content selected by the user by drawing a border over it (or highlighting selection in any other way). 
During myPortal configuration phase, a user is required to navigate to relevant webpages and point and click at the interesting content. This results in automatic generation of a query to the selected element. 3.2 Content Extraction Content extraction process uses queries generated during the navigation phase. The information about context is used to accurately access the webpage contents. In the prototypical implementation we used existing XML technologies to extract relevant content. User queries are translated into XPath expressions over the document tree. The source HTML document is converted to the XML form. Evaluating XPath expression on such prepared document return s a part of the document tree (or a set of elements), the desired content blocks. This approach poses some difficulties: 1. Numerous HTML documents are inconsistent with HTML specification so that the conversion to XML is not straightforward and it is hard to construct a proper DOM tree from them. Luckily there are tools for cleaning not well-formed documents (we adopted HTML Tidy for this task). 2. The dynamic structure of webpages, that are generated with scripts, allows assuming that the document will maintain its conformance with HTML standards over time and invariable structure. This is however not always the case. During our research we encountered examples of websites that did not hold this condition. 
This may be a result of either configuration errors in the cluster of Web servers serving the website, or a countermeasure against content theft. In the former case 
HTML cleaning usually solves the problem. The latter is dealt with relative XPath queries that we use for content extraction. 3. The webpage structure may change over time as a result of new layout or redesign. 
In case of dramatic change and renaming of content sections, all pattern based content methods fail (which is also the case in our system). Only methods based on text analysis and similarity measures searching for relevant content could deliver appropriate content. They are however prone to returning not relevant answers due to their inherent  X  X uzziness X . 4. The webpage structure may change slightly due to introduction of advertisements or repositioning certain layout elements. This problem is addressed in our approach with relative XPath expressions, which do not require the whole document structure to remain unchanged. The document tree, for this method to work, should remain unchanged only locally  X  around the selected content element. This is the main feature that provides improved robustness of our approach in comparison to other methods of content extraction. 5. The query generated from user input does not give unique answers. This problem occurs when the structure of a webpage contains numerous duplicate structures, and can be dealt with simple query reformulation or displaying multiple results (both possible in our prototype). 
Content extraction process is invisible to the user in myPortal and occurs while user wants to refresh the myPortal view on the selected websites. 3.3 Content Integration portal, specifying its layout (3), later they assign previously prepared queries to be executed and results in filling the empty portal with content. 
Currently we encounter two shortcomings of the method, they should however not be treated as research challenges. First, myPortal doesn X  X  yet support cascading style sheets (CSS). Due to their distribution (in di fferent files, in HTML document headers, in tag attributes) and style inheritance. It is not straightforward to collect all formatting information. However, this should be regarded as a programming task, not requiring sophisticated research. The second problem is related to portal layout, which is currently using HTML tables. It may happen that one of extracted content will include some large elements (for instance very wide horizontal banners), which will distort the layout (cells with large elements will dominate the portal). One solution to that is using IFRAMES with content inside cells. However, this may in turn require users to scroll content inside cells, and may in result be considered a shortcoming as well. 
Figure 3 shows a sample extraction and integration result in myPortal. Two used to formulate myPortal queries. All content around the results (most of the content on both pages) has been discarded when defining queries. The figure shows a portal integrating results from two websites. numerous. They may include news integration from multiple sources in order to be able to compare different sources. Accessibility applications (for instance speech enabled browsing) may benefit from getting rid of clutter surrounding relevant enabled and very easy to accomplish using myPortal application. Content extraction technologies could ease the maintenance of corporate Web portals that currently require tedious and costly work to maintain numerous views for separate user groups, or demand heavy personalization software to be applied. Another possible application is content clipping for viewing on small form factor devices. 
The sheer number of solutions that aim at information integration, such as RSS feeds, adoption of ontologies and content annotation, and even Web services which What is really necessary is a simple solution, since there is already a wealth of approaches that did not receive deserved a ttention, probably due to their complexity. We define extraction robustness as immunity to document structure changes and ability to extract desired content with the same query evaluated against subsequent versions of the same dynamic webpage. The robustness can be measured with average number of successful content extractions against the number of page versions. 
We conducted preliminary research on thr ee selected main Web portal pages. The 2004. They were subsequently evaluated against all the remaining occurrences of each page for the whole 2004. We used two methods of extraction. The first one, traditional and widely adopted, defines query as an XPath expression over the DOM one, the proposed relative addressing method, also uses XPath expression which spans relatively from the user selected reference element as opposed to DOM root node. 
These preliminary examinations made on several hundred historical webpages show that myPortal method is robust in 99,68% of analyzed webpages, while most commonly used XPath with absolute addressing shows robustness of 73,16%. The average values were weighted according to a number of tested pages for each website. Due to very small sample data presented here, we expect that the results may change (worsen). However, current state of analysis (after analyzing several thousand pages) confirms superiority of XPath relative method in Web content extraction. 
The increased robustness of the relative XPath method is achieved through its immunity to  X  X mall X  document structure changes (like advertisement introduction or content repositioning) that likely occur on Web portals. Currently we are working on comparing the t echnologies used in myPortal to other popular ones (absolute XPath expressions, wrappers based on rules or stiff HTML expressions). myPortal is using a modified method of extracting data, and preliminary research results show that the method is more robust than other, currently used, one. Since there are no standard ways of assessing efficiency of content extraction (as for example those used in Message Understanding Conferences in information extraction or Text Retrieval Evaluation Conference in information retrieval), the preparation phase for empirical evaluation is very demanding. We are currently collecting Web documents to perform more extensive robustness tests which hopefully will confirm the preliminary results. 
Other directions in future work include preparing a time enabled content extraction and integration, providing different portal layouts and content in different moments (for example news in the morning, financial data in the afternoon etc.), and suggesting documents. We have presented an approach to building an easy to use system for robust extraction and integration of Web content. The myPortal application fulfills the requirements stated in the first section of the paper, proving that such an application similar way to this envisioned in the Semantic Web concept. 
The contribution of the work is in presenting a system enabling new methods of browsing the Internet, clipping webpages to improve relevance of browsed content and to improve user experience in mobile, small screen, devices. We have also shown ways of integrating the content that may facilitate business use of the Internet. Preliminary research results indicate that the system is more robust than current approaches in Web data extraction. This research project has been supported by a Marie Curie Transfer of Knowledge Fellowship of the European Community's Sixth Framework Programme under contract number MTKD-CT-2004-509766 (enIRaF) 
