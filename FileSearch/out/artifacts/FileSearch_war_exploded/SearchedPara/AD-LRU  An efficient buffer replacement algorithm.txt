 1. Introduction 1.1. Problem Statement In recent years, flash memory greatly gained acceptance in various embedded computing systems and portable devices such as
PDAs (personal digital assistants), HPCs (handheld PCs), PMPs (portable multimedia players), and mobile phones because of low cost, volumetric capacity, shock resistance, low-power consumption, and non-volatile properties [1,2,3] . Among the two types of flash memory, NAND and NOR, the NAND flash memory is more often used for mass-storage devices. just use the term flash memory or flash to indicate the NAND flash memory .

However, flash memory has many properties differing from magnetic disk, e.g. write-once, block erasure, asymmetric read/ write speed and limited block erase count. Flash memory usually consists of many blocks and each block contains a fixed set of pages [5] . Read/write operations are performed on page granularity, whereas erase operations use block granularity. Write/ erase operations are relatively slow compared to read operations. Typically, write operations are about ten times slower than read operations, and erase operations are about ten times slower than write operations [6] . Data in a page cannot be updated in-place, i.e., when some data in a page has to be modified, the entire page must be written into a free page slot and the old page content has to be invalidated. Hence, flash always requires out-of-place updates . Furthermore, updating a page will cause costly erase operations performed by some garbage collection policy [7] , in case that no enough free pages exist in flash memory. Hence, increasing the number of writes will accompany even more erase operations, as shown in previous experimental studies [6] .
Since flash memory has become a serious disk alternative, traditional DBMSs should support flash storage devices and provide efficient techniques to cope with flash I/O properties. Among those techniques, DBMS buffering has first received much attention from the research community because of its effectiveness in reducing I/O latencies and thus improving the overall DBMS perfor-mance. Traditional (magnetic-disk-based) buffering algorithms do not consider the differing I/O latencies of flash memory, so their straight adoption would result in poor buffering performance and would demote the development of flash-based DBMSs [8] . The use of flash memory requires new buffer replacement policies considering not only buffer hit ratios but also replacement costs incurring when a dirty page has to be propagated to flash memory to make room for a requested page currently not in the buffer. As a consequence, a replacement policy should minimize the number of write and erase operations on flash memory and, at the same time, avoid to worsen the hit ratio which otherwise would lead to additional read operations.

The above flash challenges of DBMS buffering are not met by traditional buffer replacement algorithms. Most of them focus on hit-ratio improvement alone, but not on write costs caused by the replacement process. Recently, LRU-WSR [6] and CFLRU [8] were proposed as the new buffering algorithms for flash-based DBMSs. These algorithms favor to first evict clean pages from the buffer so that the number of writes incurring for replacements can be reduced. Of course, this is a very important idea
However, CFLRU and LRU-WSR do not exploit the frequency of page references, which will result in an increase of both write count [9] , i.e., the buffer will be totally polluted with sequentially referenced pages when some kind of scan operation is performed. 1.2. Our contributions
In this article, we present an efficient buffer replacement policy for flash-based DBMSs, called AD-LRU (Adaptive Double LRU), of our article can be summarized as follows: (1) We present the novel AD-LRU algorithm for the buffer management of flash-based DBMSs (see Section 3), which not only (2) We run experiments both in a flash simulation environment and in a real DBMS to evaluate the efficiency of AD-LRU 1.3. A brief outline of the paper concepts of the AD-LRU approach to flash DBMS buffering. Section 4 describes the details about the experiments and the performance evaluation results. Finally, Section 5 concludes the article and outlines our future work. 2. Related work
In this section, we briefly introduce flash storage systems (see Section 2.1) and then review the replacement algorithms for magnetic-disk-based (see Section 2.2) and those for flash-based DBMSs (see Section 2.3). 2.1. Flash memory and fl ash storage system
Flash memory is a type of EEPROM, which was invented by Intel and Toshiba in 1980s. Unlike magnetic disks, flash memory does not support update in-place, i.e., previous data must be first erased before a write can be initiated to the same place. As another important property of flash memory, three types of operations can be executed: read, write, and erase. In contrast, for both device types.

Compared to magnetic disks, flash memory has the following special properties: (1) It has no mechanical latency, i.e., seek time and rotational delay are not present. (2) It uses an out-of-place update mechanism, because update in-place as used for magnetic disks would be too costly. (3) Read/write/erase operations on flash memory have different latencies. While reads are fastest, erase operations are slowest. (4) Flash memory has a limited erase count, i.e., typically 100,000 for SLC-based NAND flash memory. Read/write operations
NAND flash memory can be categorized into two types, which are Single-Level-Cell (SLC) and Multi-Level-Cell (MLC) flash memory. SLC stores one bit in a memory cell, while MLC represents two or more bits in a memory cell. SLC is superior to MLC both in read/write performance and durability. On the other hand, MLC can provide larger capacity with lower price than SLC [11] . However, the special features of SLC/MLC are usually transparent to file and database systems, because most flash-based storage devices use a flash translation layer (FTL) [12,13] to cope with the special features of flash memory, which maps logical page addresses from the file system to physical page addresses used in flash memory devices. FTL is very useful because it enables a traditional DBMS to run on flash disks without any changes to its kernel. In other words, A DBMS-level algorithm, e.g., buffer management, is independent of the fundamental SLC or MLC flash chips, due to the FTL layer.

Fig. 1 shows the typical architecture of a flash-based storage system using FTL. It indicates that the file system regards flash disks as block devices. Page rewrites and in-place updates can be logically done at the file system layer. However, updating a layer is helpful to reduce the number of physical write and erase operations. 2.2. Traditional buffer replacement algorithms
Buffer management is one of the key issues in DBMSs. Typically, we assume a two-level storage system: main memory and between the two levels. When a page is requested from modules of upper layers, the buffer manager has to read it from secondary ment. In such a scheme, the quality of buffer replacement decisions contributes as the most important factor to buffer management performance.
 performance. Many algorithms have been proposed so far, either based on the recency or frequency property of page references. Among them, the best-known ones are LRU, CLOCK [14] , LRU-2 [15] , and ARC [9] .

LRU always evicts the least-recently-used page from an LRU queue used to organize the buffer pages ordered by time of their last reference. It always replaces the page found at the LRU position. An important advantage of LRU is its constant runtime complexity. Furthermore, LRU is known for its good performance in case of reference patterns having high temporal locality, i.e., currently referenced pages have a high re-reference probability in the near future. But LRU also has severe disadvantages. First, it only considers the recency of page references and does not exploit the frequency of references. Second, it is not scan-probability.

CLOCK uses a reference bit which is set to 1 whenever the page is referenced [14] . Furthermore, it organizes the buffer pages as a circle, which guides the page inspection when a victim is searched. When a page currently inspected has a 1 in the referenced considerreferencefrequency.Moreover, itis not scan-resistant,too. The improved CLOCKalgorithm, calledGCLOCK, can betailored to workload characteristics by adding a reference count to each buffer page and using page-type-specific weights (as parameters). But even worse than LRU [17] .

LRU-2 is an improvement of LRU, as it captures both recency and approximate frequency of references [15] . It considers the two most-recent references of each page to determine the victim for replacement. Though LRU-2 tends to have a better hit ratio than LRU, its runtime complexity is higher than that of LRU. LRU-2 has to maintain the history of reference in a priority queue, which is controlled by a tunable parameter called Correlated Information Period (CIP). Unfortunately, the (manual) choice of the CIP parameter crucially affects the LRU-2 performance [9] . Experiments have revealed that no single parameter setting works universally well for all buffer sizes and differing workloads.

There are other replacement algorithms that consider both recency and frequency [9] ,suchasLRFU [18] ,2Q [19] , and FBR [20] . However, they have similar shortcoming as LRU-2, i.e., they all are not self-tuning for different buffer sizes and workloads.

The ARC algorithm is an adaptive buffer replacement algorithm [9] , which utilizes both frequency and recency. Moreover, ARC is scan-resistant and has proven a superior performance than other replacement algorithms. It maintains two LRU lists: L
The L 1 list stores the pages which are referenced only once, while the L algorithm adapts the sizes of the two lists to make the replacement algorithm to suit different buffer sizes and workloads.
All the traditional algorithms mentioned above do not consider the asymmetric I/O properties of flash memory. Yet, reducing the write/erase count for buffer management of flash-based DBMSs is not only useful to improve the runtime, but also helpful to extend the life cycle of flash memory. Hence, buffer replacement algorithms focusing on the avoidance of write operations have been investigated in recent years. 2.3. Buffer replacement algorithms for fl ash-based DBMSs
To our knowledge, CFLRU is the first algorithm designed for flash-based DBMSs [8] . It modified the LRU policy by introducing a clean-first window W , which starts from the LRU position and contains the least-recently-used w no clean page is found in W , CFLRU acts according to the LRU policy. However, the following problems occur: (1) Its clean-first window size has to be tuned to the current workload and can not suit differing workloads. For this reason, [8] (3) It has to search the clean-first window during each page replacement, which brings additional runtime costs. (4) Just like LRU, CFLRU is also not scan-resistant and does not exploit frequency of references.
 Based on the CFLRU policy, Yoo et al. presented a number of upgrades to the CFLRU algorithm, called CFLRU/C, CFLRU/E, and
DL-CFLRU/E [21] . These algorithms explored strategies for evicting pages based on lowest write reference count, lowest block erase count, frequency of access and wear-leveling degree. In the CFLRU/C and CFLRU/E algorithms, the buffer list structure is the same as that of CFLRU, the least-recently-used clean page is selected as the victim within the pre-specified window of the
LRU list. If there is no clean page within the window, CFLRU/C evicts the dirty page with the lowest write reference count, while CFLRU/E evicts the dirty page with the lowest block erase count. DL-CFLRU/E maintains two LRU lists called clean page E evicts the dirty page with the lowest block erase count within the window of the dirty page list.

Unlike CFLRU, CFLRU/C only delays to flush dirty pages with a high write reference count, which can effectively reduce the number of write operations and hence the number of erase operations to some extent. However, it does not consider the refer-ence frequency, just like the original CFLRU algorithm. The CFLRU/E and DL-CFLRU/E algorithms need to know exactly the erase count of each block, which is usually hidden in the FTL layer and can not be obtained by upper-layered buffer replacement algorithms. Therefore, the CFLRU/E and DL-CFLRU/E algorithms do not fit for DBMS applications.

CFDC [22] and CASA [23] are two more improvements of the CFLRU algorithm. CFDC improves the efficiency of the buffer man-by flash disks more efficiently than random writes. CASA improves CFLRU by automatically adjusting the size of buffer portions allocated for clean pages and dirty pages according to the storage device's read/write cost ratio.

The LRU-WSR policy in [6] considers the cold/hot property of dirty pages, which is not tackled by the CFLRU algorithm. LRU-WSR long as in the case of CFLRU.

LRU-WSR has a high dependency on the write locality of workloads. It shows poor performance in case of low write locality which may cause dirty pages to be quickly evicted. The LRU-WSR policy also does not capture the frequency of references, which may degrade the hit ratio. Like LRU, LRU-WSR is also not scan-resistant.
 The LIRS-WSR algorithm [24] is an improvement of LIRS [25] so that it can suit the requirements of flash-based DBMSs.
However, LIRS-WSR has the same limitation as CFLRU and LRU-WSR, because it is not self-tuning, too, and hardly considers the reference frequency. Frequently referenced pages may be evicted before a cold dirty page, because a dirty page is always put on the top of the LIRS stack, irrespective of its reference frequency. Moreover, LIRS-WSR needs additional buffer space, because it has to maintain historical reference information for those pages that were referenced previously, but are currently not in the buffer.

The CCF-LRU algorithm proposed in [26] used two LRU queues, a cold clean queue and a mixed queue, to maintain buffer pages. The cold clean queue stores those cold (first-referenced) clean pages, while the mixed queue stores dirty pages or hot as LRU-WSR to select a dirty page from the mixed queue. This algorithm focused on the reference frequency of clean pages and has little consideration of the reference frequency of dirty pages. Besides, the CCF-LRU algorithm has no techniques to control the length of the cold clean queue, which will lead to frequent evictions of recently-read pages in the cold clean queue and
AD-LRU algorithm takes into account both the reference frequency of clean pages and dirty pages and has a new mechanism to control the length of the cold queue to avoid a drop in the hit ratio. Our experimental results also demonstrate that AD-LRU has better performance than CCF-LRU. 3. The AD-LRU algorithm
In this section, we present the basic idea and details of AD-LRU. 3.1. Basic idea For buffer replacement algorithms of flash-based DBMSs, we need to consider not only the hit ratio but also the write costs.
It has been proven that the idea to evict clean pages first reduces the number of write operations and, thus, the overall run-time [8] . However, traditional algorithms such as LRU will not always evict a clean page. On the other hand, the flash-aware algorithms CFLRU and LRU-WSR likely evict hot clean pages from the buffer, because they both use the clean-first strategy and do not take the page reference frequency into account. In such cases, the hit ratio may degrade as confirmed by previous ex-periments [8,6] .

Therefore, we explicitly enhance the traditional LRU policy by frequency considerations and first evict least-recently-and least-frequently-used clean pages to reduce the write count during the replacement. Our aim is to reduce the write costs of the buffer replacement algorithm while keeping a high hit ratio. Table 1 shows a simple description of the current buffer replacement algorithms for flash-based DBMSs. It also presents the motivation of our AD-LRU algorithm, which tries to integrate the properties of recency, frequency, and cleanness into the buffer replacement policy.

The AD-LRU concepts can be summarized as follows: (1) We use two LRU queues to capture both the recency and frequency of page references, among which one cold LRU queue (2) The sizes of the double LRU queues are dynamically adjusted according to the changes in the reference patterns. We increase (3) During the eviction procedure, we first select the least-recently-used clean page from the cold LRU queue as the victim, for As illustrated by Fig. 2 , the double LRU queues of AD-LRU separate the cold and hot pages in the buffer, where the cold and hot
LRU queues contain c and h pages, respectively. The values of c and, at the same time, h are dynamically adjusted according to the replacements in it, because pages requested from external stores always arrive at the cold queue and, in turn, we always select a victim from this queue at first.
 or, if FC is null, we select a dirty page based on the second-chance policy. 3.2. AD-LRU page eviction The page fetching algorithm is characterized as follows (see Algorithm AD-LRU_fetch ). If the requested page is found in the hot hot queue, thereby automatically reduce the cold queue, and move the page to the MRU position in the hot queue (line 6 to line 12). If a page miss occurs and the buffer has free space, we increase the size of the cold queue and put the fetched page into the more than min _ lc pages, we evict the victim from the cold queue (line 22), otherwise, we evict it from the hot queue and reduce will be used in the SelectVictim routine to determine the victim based on the second-chance policy.
The algorithm SelectVictim first selects the FC page (least-recently-used clean page) from the LRU queue as the victim. If no clean pages exist in the queue, it selects a dirty page using the second-chance policy. The referenced bit of the buffer page is continued until the first page with referenced bit having 0 is located, which is then returned as the result.
Fig. 3 shows an example of how the SelectVictim algorithm works. In this example, we suppose that there are six pages in the buffer and the buffer is full. When the buffer manager receives a new page reference, our AD-LRU algorithm will choose page 4 as the victim, as shown in Fig. 3 . On the other side, Fig. 4 shows the victims selected by the CFLRU algorithm and the LRU-WSR al-gorithm. Compared with CFLRU, which selects the hot clean page 2 in the example, AD-LRU will choose a cold clean page so as to achieve a higher hit ratio. Compared with LRU-WSR, which selects the cold dirty page 2, AD-LRU has fewer write operations, because it avoids to evict dirty pages.

The AD-LRU algorithm has the same overall goal as CFLRU and LRU-WSR; however, it is designed in a new way. The differences between AD-LRU and those approaches are listed as follows: (1) AD-LRU considers reference frequency, an important property of reference patterns, which is more or less ignored by (2) Cold-dirty pages may reside in the buffer under CFLRU for an overly long period, whereas AD-LRU purges the buffer from 4) AD-LRU is scan-resistant, a property missing in CFLRU and LRU-WSR. Under AD-LRU, a scan only influences the cold queue 3.3. Considerations on checkpointing
The concept of checkpointing requires a DBMS to periodically or incrementally force dirty pages out to non-volatile storage for fast recovery, regardless of their access frequency or access recency. To create a checkpoint at a safe place , earlier solutions flushed all modified buffer pages thereby achieving a transaction-consistent or action-consistent firewall for redo recovery on disk. Such direct checkpoints are not practical anymore, because given large database buffer sizes they would repeatedly imply the checkpoint are written to the log. The logs can help to determine which pages containing committed data were actually in the buffer at the moment of a crash, with two or three write operations [28] . As a consequence, fuzzy checkpointing does not require the hit ratio of a buffer replacement algorithm, because hot dirty pages will remain in the buffer after being committed into non-compared with other algorithms such as CFLRU and LRU-WSR. 4. Performance evaluation
In this section, we compare AD-LRU with five competitor algorithms, i.e., LRU, CFLRU, LRU-WSR, CFLRU/C, and CCF-LRU. We do not compare the CFLRU/E and DL-CFLRU/E algorithms in our experiment, because those two algorithms need to know the erase count of blocks in flash disks, which can not be realized in a uppe r-layered buffer manager. We perform the experiments both in a simulation environment and in a real DBMS, where different types of workloads are applied. The simulation experiment is performed to test the hit ratio and write count of each algorithm, whereas the DBMS-based experiment aims at the comparison of the overall runtime. 4.1. Simulation experiment 4.1.1. Experiment setup The simulation experiments are conducted based on a flash memory simulation platform, called Flash-DBSim [29,30] . Flash-DBSim is a reusable and reconfigurable framework for simulation-based evaluation of algorithms on flash disks, as shown in
Fig. 5 . The VFD module is a software layer that simulates the actual flash memory devices. Its most important function module is to provide virtual flash memory using DRAM or even magnetic disks. It also provides manipulating operations over the virtual flash memory, such as page reads, page writes, and block erases. The MTD module maintains a list of different virtual flash devices, which enables us to easily manipulate different types of flash devices, e.g., NAND, NOR, or even hybrid flash disks.
The FTL module simulates the virtual flash memory as a block device, so that the upper-layer applications can access the virtual flash memory via block-level interfaces. The FTL module employs the EE-Greedy algorithm [31] in the garbage collection part and uses the threshold for wear-levelling proposed in [32] . In one word, Flash-DBSim can be regarded as a reconfigurable SSD (solid refer to the MICRON MT29F4G08AAA flash chip in the Flash-DBSim. The detailed parameters of the selected flash chips are listed in Table 2 . 4.1.2. Workloads
We use four types of synthetic traces in the simulation experiment, i.e., random trace, read-most trace (e.g., of decision support among a totality of N pages, P i , is given by the following expression:
Here,  X  = loga / logb and H N s is the N th harmonic number of order s , namely 1 0.2, the distribution means that 80% of the references deal with the most active 20% of the pages. Such a referential locality is to a set of pages whose numbers range from 0 to 49,999. The total number of page references in the Zipf trace is set to 500,000 in order to obtain a good approximation, while the page numbers still fall in [0, 49999]. Tables 3 to 6 show the details concerning these workloads. 4.1.3. Parameter setup
Parameter w of the CFLRU algorithm is set to 0.5, which means half of the buffer is used as clean-first window. As mentioned entire buffer space to store dirty pages. So a middle value 0.5 is reasonable to conduct the comparison between our algorithm and 1 MB to nearly 36 MB.

For each of the algorithms, we ran the traces shown in Tables 3 to 6 and compared the hit ratios. For the Zipf trace, AD-LRU achieved the best hit ratios, as shown in Fig. 6 . For the other traces, the hit ratios of all the algorithms are comparable (thus not shown), due to the (randomly generated) uniform distribution of the page references. Even for such page references, AD-LRU still outperforms the competitors in terms of write count and runtime, as shown in the following sections. 4.1.4. Write count
Fig. 7 a to d shows the number of pages propagated to flash memory. We obtained these results by counting the number of to get the exact write counts. LRU generates the largest number of write operations in all cases, because it has no provisions to reduce the number of writes to flash memory. While CFLRU first replaces clean pages and keeps dirty pages for the longest time among all algorithms, it has the second smallest write count. As shown in all the four figures, AD-LRU has the smallest write count. The reason is that it divides all the buffer pages into a hot LRU queue and a cold queue, and first selects the least-recently-used clean pages from the cold queue.
 In Fig. 7 b, AD-LRU tends to remain a stable write count when the buffer size is over 20 MB: we obtained 8,973 in our experiment. replacement algorithm. 4.1.5. Erase count
Fig. 8 illustrates the erase counts for all algorithms considered w.r.t. the Zipf trace listed in Table 7 . The erase behavior is mostly influenced by garbage collection and wearleveling. In the Flash-DBSim environment, we use the garbage collection algorithm of [31] and the wear-leveling algorithm of [32] . We do not compare the erase counts for the traces random , read-most , and write-most , because only few erase operations are performed when running those traces, owing to the small number of pages referenced. As Fig. 8 shows, the erase counts of the buffer replacement algorithms are nearly proportional to the write counts shown in Fig. 7 d. 4.2. DBMS-based experiment
In the simulation experiment, we assume that each flash read or write has the same latency, but in real SSD-based systems, we
DBMS to demonstrate the superior performance of our algorithm. In the DBMS-based experiment, we concentrate on the comparison of algorithms. The comparison of hit ratios has been studied in the si mulation experiment and we will not make further discussions. 4.2.1. Experiment setup
The DBMS-based experiments are performed on the XTC database engine [35] . XTC is strictly designed in accordance to the well-known five-layer database architecture proven for relational DBMS implementations, so our experiments are also meaningful to the relational DBMS environment. To better explain our results, we have only used its two bottom-most layers in our Although designed for XML data management, the processing behavior of these two XTC layers is very close to that of a relational DBMS.

The test computer has an AMD Athlon Dual Core Processor, 512 MB of main memory, is running Ubuntu Linux with kernel version disk we used in the experiments is a 32 GB SLC-based Super Talent DuraDrive FSD32GC35M SSD. As discussed in Section 2.1, our algorithm does not rely on the SSD type.

In our experiments, we deactivated the file-system prefetching and the I/O scheduling for the flash disk and emptied the Linux each execution. Furthermore, we sequentially read and wrote a 512 MB file (of irrelevant data) from and onto the flash disk before executing each algorithm.

We run our DBMS-based experiment over two types of traces. The first trace is a Zipf trace with a larger number of references (1,000,000) as well as a larger page space (100,000) than that used in the simulation experiment. The second one is a one-hour
OLTP trace of a real bank system, which has also been used in LRU-2 [15] and ARC [9] . This trace contains 607,391 page references to a CODASYL database with a total size of 20 GB. The parameter w of CFLRU is set to 0.5, and the parameter min _ lc of AD-LRU is set to 0.1 for all traces. The buffer page size is 2048 bytes.

In the DBMS-based experiment, we will measure the write count and runtime of each buffer replacement algorithm mentioned in the simulation experiment. Here, the runtime of a buffer replacement algorithm is defined as follows:
Runtime=CPU time for manipulating various data structures+flash read time for missed pages+flash write time (assuming an underlying FTL) for dirty victims.

Here, flash write time contributes most to the total runtime, because flash write operations need more time than flash read operations and CPU operations. Meanwhile, flash write time has a high dependence on the underlying FTL algorithm. Although different FTL algorithms may be used in different SSDs, our goal is to compare the performance of buffer replacement algorithms under the same underlying FTL algorithm. Note that current SSD manufacturers do not report many details about the internal design of their SSDs, such as what FTL algorithm is implemented inside the SSDs. 4.2.2. Zipf trace experiment
Table 7 shows the details about the Zipf trace used in the DBMS-based experiment. The write count and runtime of the four complexity. But due to its largest write count among all the algorithms, it exhibits the worst overall runtime. The CPU runtime of
AD-LRU is comparable to CFLRU, LRU-WSR, CFLRU/C, and CCF-LRU, because they all need search time to locate the victim for replacement. However, since the AD-LRU algorithm has the lowest write count, it has the best overall runtime.
As a result, AD-LRU reduced the number of writes under the Zipf trace compared to LRU, CFLRU, and LRU-WSR by about 23%, 17%, and 21%, respectively. In addition, our algorithm decreased the r untime by about 21%, 16%, and 20% over LRU, CFLRU, and LRU-WSR. 4.2.3. OLTP trace experiment
Table 8 describes the real OLTP trace used in the DBMS-based experiment. Fig. 10 a and b shows the write count and runtime of the experiment. AD-LRU is comparable with CCF-LRU for write count, but superior to all the other four competitor algorithms throughout the spectrum of buffer sizes, both for write count and runtime. Note that the runtime of AD-LRU is still superior to the CCF-LRU algorithm, due to the higher hit ratio of AD-LRU. The performance of CFLRU is surprisingly better than that of
LRU-WSR. We also noted this in the previous experiment over the 1000 k -100 k Zipf trace. This result is somewhat different from what was reported in the LRU-WSR paper. The reason is that the parameter w of CFLRU is set to 0.5 in our experiment, write operations while accompanied with an increasing read count. 4.3. Impact of the parameter min _lc
The only parameter in the AD-LRU algorithm is min _ lc , which refers to the minimal size of the cold LRU queue. The min _ lc parameter is used to avoid the frequent replacement of recently-referenced pages in the cold LRU queue. This will occur if the cold queue is very small. On the other hand, it is very likely to get a cold queue containing only one page if we do not take any control, because AD-LRU will always choose the pages in the cold queue as victims. By setting up a minimal size of the cold queue, we have a better chance to avoid this situation. For example, suppose that the current cold queue contains c pages, a new page is read, and the buffer is full. If min _ lc is set to c , we will go to replace pages from the hot LRU queue and shows the final results. Surprisingly, we find the min _ lc parameter has little impact on the random, read-most, and write-most traces. The reason is that the page references are uniformly distributed in these three traces and there is no clear distinc-tion between hot and cold pages. However, when the reference pattern is skewed, the best case appears when min _ lc =0.1.If min _ lc is much larger than 0.1, there are probably more replacements in the hot LRU queue. On the other hand, if min _ lc is less than 0.1, most replacements will occur in the cold queue. Moreover, more dirty pages will be evicted from the cold queue, since its size is so small that we have little chance to find a clean page in it.

It is somehow difficult to determine the optimal value of min _ lc for all kinds of workloads. Currently, we have not developed a the four kinds of traces, it shows that to set min _ lc =0.1 is an acceptable choice.
 4.4. Scan resistance
To examine the scan resistance of AD-LRU, we performed a custom-tailored experiment, where the baseline workload consists of a Zipf trace and fifty scans. The Zipf trace contains 100,000 references to 40,000 pages whose page numbers range from 0 to 39,999, and the fifty scans are restricted to 10,000 pages with page numbers ranging from 40,000 to 49,999. Each scan contains a fixed number of read operations and all the scans are distributed uniformly among the Zipf trace. We change the length of scan, which refers to the number of continuous operations in a scan, to examine the performance of LRU, CFLRU, LRU-WSR, and AD-LRU. The buffer size in the experiment is 4096 MB, the parameter min _ lc of AD-LRU is 0.1, and the parameter w of CFLRU is 0.5.
The hit ratios and write counts are shown in Fig. 12 a and b, respectively. While AD-LRU always maintains the highest hit ratio when using different scan lengths, it keeps a relatively stable write count. In contrast, LRU, CFLRU, CFLRU/C, and LRU-WSR, all imply a considerable increase of the write count when the scan length is increased. The CCF-LRU algorithm also has a stable writecount,becauseofitstwo-LRU-queuemechanism.Insummary,AD-LRUisalsosuperiortoLRU,CFLRU,CFLRU/C,andLRU-WSR in terms of scan resilience. Although CCF-LRU has similar scan resistance, it creates worse write counts than AD-LRU. When lots of page references from scans are present, the AD-LRU algorithm clearly adheres to its performance objectives much better than its competitors. 4.5. Device sensitivity study
SSDs are usually regarded as black-boxes. It has been experimentally demonstrated that the I/O performance of SSDs differs from that of flash chips [36] , due to varying internal mechanisms such as address mapping and wear leveling. As a consequence, various SSD types have differing I/O performance. For example, the Intel-X25-M SSD has a sustained read speed of up to 250 MB/s, while the maximum read speed of the Mtron MSP SATA7525 SSD is only 130 MB/s.
 various SSD types to test the runtime performance of the AD-LRU algorithm and its competitors to validate the AD-LRU applicability for typical SSDs. Besides the previously used Super Talent SSD designed a couple of years ago and now considered as a low-end SSD, we include two additional SSDs: The SLC-based Mtron MSP SATA7525 SSD is selected as a middle-class SSD. In addition, the MLC-based Intel-X25-M (SSDSA2MH160G1) has high I/O bandwidth and stands for typical high-end SSDs. Therefore, those three SSDs are representative for a large spectrum of SSD types.

We re-executed the DBMS-based experiment over the 1000 k -100 k Zipf trace (as shown in Table 7 ) to measure the runtime cost. The results are shown in Fig. 13 . The runtime values substantially differ when using different SSDs, owing to their varying of AD-LRU are not device dependent. 4.6. Buffer-size impact on CPU usage
AD-LRU maintains two LRU queues to reduce the writes to SSD. However, the CPU time to manipulate queues may increase with growing buffer sizes. To examine the CPU time overhead of AD-LRU as well as its competitors, we generated two new traces.
One of them follows the Zipf distribution and the other follows the self-similar distribution. Both of them contain much more page requests than the previously introduced traces. The reference locality of both traces follows the 80 has 10 million random page requests, addressing 1 million pages (simulating a database of 2 GB) with 50% of the requests being read-only. The self-similar trace consists of 100 million page requests, 20% of them are read-only, and randomly addresses 10 million database pages, which correspond to a database of 20 GB.

We ran the traces for all the considered algorithms in the real DBMS environment, where we measured the overall execution time and the time spent doing IO. Then we derived the CPU portion of the execution time, as an indication of the CPU usage and the time complexity of the algorithms. For comparison, we also included the 1000 k -100 k Zipf trace introduced in the previous sections. The buffer size was always set to 5% of the database size, namely, 10 MB for the 1000 k -100 k trace, 100 MB for the 10-million Zipf trace, and 1 GB for the self-similar trace.
 Table 9 shows the total runtime together with the CPU portion for the three traces. As indicates by these performance figures, AD-LRU always had the best performance. At the same time, the CPU usage remained relatively stable for all tested algorithms.
Database applications are typically IO-bound, and this is also the case in our experiments. 5. Conclusions
Flash memory has become an alternative to magnetic disks, which brings new challenges to traditional DBMSs. To efficiently support the characteristics of flash storage devices, traditional buffering approaches need to be revised to take into account the imbalanced I/O property of flash memory. In the recent three years, people have tried to present new buffer replacement policies for flash-based DBMSs. However, as the experimental results in our study show, the overall performance of those algorithms are not as optimal as we expect.

In this article, we proposed AD-LRU, a new efficient buffer replacement algorithm for flash-based DBMSs. The new algorithm captures both the frequency and recency of page references by using double LRU queues to classify all the buffer pages into a hot set and a cold set. It also uses an adaptive mechanism to make the sizes of the two LRU queues suitable for different reference patterns. We use different traces, including a real trace and some simulated traces, to evaluate the performance of the AD-LRU algorithm and also to compare it to the three competitor algorithms: LRU, CFLRU, and LRU-WSR. The experimental results show that in most cases the AD-LRU algorithm outperforms all competitors w.r.t. hit ratio, write count, and overall runtime.
Based on our experimental study, we draw the following conclusions: (1) The LRU algorithm has the worst performance in each case. It shows that traditional algorithms will not work well in (2) The performance of buffer management in flash-based DBMSs is dominated by the number of write operations, given the (3) While the runtime in a simulation environment is much different from that in real SSD-based systems, it is still reasonable (4) It should be a good choice in flash-based DBMSs to first evict clean pages from the buffer. However, an additional effort has (5) Our AD-LRU algorithm exhibits superior performance behavior than other methods proposed so far. It has a lower number of
Next we will implement our algorithm in a relational database engine, e.g., PostgreSQL or BerkeleyDB, and perform further performance evaluations using standard benchmarks [37] . Another future work will be focused on using more than two queues to organize buffer pages. Basically, more queues will introduce more manipulation operations on data structures as well as more additional overhead to adjust queues and control their lengths. However, it may be helpful to improve hit ratio and reduce write operations for the buffer management in flash-based DBMSs, because different types of frequency can be supported by using more queues. Finally, Flash-DBSim used in our current experiments is only able to simulate the behavior of flash chips.
As reported in [36] , SSDs behave much different from flash chips. Thus, it will be one of our future works to make the outcome of Flash-DBSim more similar to real SSDs and to test the robustness of the AD-LRU algorithm under different SSD types. architecture with flash as a caching layer between the RAM-based main memory buffer and the storage layer based on magnetic disks [38] , or in a key-value storage system consisting of an array of flash-based storage nodes [39] .
 Acknowledgments
We are grateful to anonymous referees for the valuable comments that greatly improved this article, and to Gerhard Weikum for providing the real OLTP trace. This research is partially supported by the National Science Foundation of China (No. 60833005 and No. 61073039), the German Research Foundation, and the Carl Zeiss Foundation.

References
