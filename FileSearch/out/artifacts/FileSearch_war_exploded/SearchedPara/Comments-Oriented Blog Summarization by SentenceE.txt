 Much existing research on blogs focused on posts only, ignor-ing their comments. Our user study conducted on summa-rizing blog posts, however, showed that reading comments does change one X  X  understanding about blog posts. In this research, we aim to extract representative sentences from ablogpostthatbestrepresentthetopicsdiscussedamong its comments. The proposed solution first derives repre-sentative words from comments and then selects sentences containing representative words. The representativeness of words is measured using ReQuT (i.e., Re ader, Qu otation, and T opic). Evaluated on human labeled sentences, ReQuT together with summation-based sentence selection showed promising results.
 H.3.3 [ Information Search and Retrieval ]: Information filtering; H.3.1 [ Content Analysis and Indexing ]: Ab-stracting methods Experimentation Blog, Comments, Sentence Selection, ReQuT
Entries of blogs, also known as blog posts, often contain comments from blog readers. A recent study on blog conver-sation showed that readers treat comments associated with a post as an inherent part of the post [2]. However, exist-ing research largely ignore comments by focusing on blog posts only. To find out whether the reading of comments would change a reader X  X  understanding about the post, we This research is partially supported by grant SUG7/06, Nanyang Technological University, Singapore.
 Copyright 2007 ACM 978-1-59593-803-9/07/0011 ... $ 5.00.
Figure 1: Comments-oriented blog summarization conducted a user study on summarizing blog posts by la-beling representative sentences in those posts. Significant differences between the sentences labeled before and after reading comments were observed.

In this research, we therefore focus on the problem of comments-oriented blog post summarization. The task is tosummarizeablogpostbyextractingrepresentativesen-tences from the post using information hidden in its com-ments. The extracted sentences represent the topics pre-sented in the post that are captured by its readers (i.e., com-menters). Many applications would benefit from comments-oriented summarization, such as blog search, blog presenta-tion, reader feedback, and others.

Given a blog post and its comments, our solution con-sists of three modules (see Figure 1): sentence detection splits blog post content into sentences; word representative-ness measure weighs words appearing in comments; and sen-tence selection computes a representativeness score for each sentence based on representativeness of its contained words. In this paper, we mainly focus on the last two modules. For word representativeness measure, we evaluate binary, comment frequency, term frequency, and ReQuT ,where Re-QuT measures the representativeness of a word from three aspects including Re ader, Qu otation, and T opic. To select sentences, we propose a summation-based sentence selection method. Together with ReQuT , the proposed sentence se-lection method performed well in our experiments evaluated on manually labeled sentences.

The rest of this paper is organized as follows. Section 2 surveys related work. We formally define the research prob-lem in Section 3. The ReQuT model and the sentence se-lectionmethodaregiveninSection4. Afterpresentingour user study and experiments in Section 5, we conclude the paper in Section 6.
Blogs have received much attention from researchers in re-cent years. Various studies have been conducted including blog posts tagging, spam blog post detection, and opinion mining, to name a few. Nevertheless, very few studies on blog comments and blog post summarization have been re-ported. In a recent study, Mishne and Glance reported that 28% of the collected 36,044 blogs contain comments from readers [6]. Among all blog posts containing comments, an average of 6.3 comments per post was observed. They also reported that comments contributed to the improvement of recall in blog search.

Zhou et al viewed a blog post as a summary of online news articles it linked to, with added personal opinions [9]. A summary is generated by deleting sentences from the blog post that are not relevant to its linked news articles. Com-ments associated with blog posts were however not used. The problem of comments-oriented blog summarization is quite related to the problem of identifying most commented sentences reported in [3]. Comments are represented and clustered using feature vectors, and a human expert is in-volved to select the clusters of interest. Sentences in blog post are scored and selected using comments in the selected clusters. Our solution, however, differs in two major aspects. First, we do not model comments using feature vectors. Sec-ond, our solution is topic neutral and does not involve user judgement.

Sun et al used LSA and Luhn X  X  sentence selection methods to generate Web page summaries using clickthrough data [8]. In their work, clickthrough data is believed to provide some human understanding about Web pages. This is similar to our problem setting where comments of a post are utilized in summarizing the blog post.
The problem of comments-oriented blog summarization is formally defined as follows:
Definition 1. Given a blog post P consisting of a set of sentences P = { s 1 ,s 2 ,...,s n } and the set of comments C = { c 1 ,c 2 ,...,c } associated with P , the task of comments-oriented blog summarization is to extract a subset of sen-tences from P , denoted by S r ( S r  X  P ), that best represents the discussion in C .

Given the problem, one straightforward approach is to compute a representativeness score for each sentence s i ,de-noted by Rep ( s i ), and select sentences with representative-ness scores above a given threshold 1 . As a sentence consists of a set of words, s i = { w 1 ,w 2 ,...,w m } ,onecanderive Rep ( s i ) using representativeness scores of all words con-tained in s i .

Intuitively, word representativeness can be measured by counting the number of occurrences of a word in comments, such as the following three schemes.
A threshold could be defined based on the number of sen-tences to be selected.

All three measures are simple statistics on comment con-tent. Binary captures minimum information; CF and TF capture slightly more. Other information available in com-ments that could be very useful are ignored, e.g., authors of comments, quotations among comments and so on. More-over, all three measures suffer from spam comments. For instance, a blog reader (or even the blogger himself) could intentionally write comments containing certain words in order to boost their representativeness, and hence to affect the summary generated. This calls for a measure that could capture more information from comments (besides content) and is less sensitive to spam.
A comment, other than its content, is often associated with an author, a time-stamp, and even a permalink. A comment author is also known as a blog reader in this paper. We state three common observations on how comments may link to each other. These observations provide us guidelines on measuring word representativeness.

Observation 1. A reader often mentions another reader X  X  name to indicate that the current comment is a reply to pre-vious comment(s) posted by the mentioned reader. A reader may mention multiple readers in one comment.

Observation 2. A comment may contain quoted sentences from one or more comments to reply these comments or con-tinue the discussion.

Observation 3. Discussion in comments often branches into several topics and a set of comments are linked together by sharing the same topic.
Based on the three observations, we believe that a word is representative if it is written by authoritative readers, appears in widely quoted comments, and represents hotly discussed topics.

With Observation 1, given the full set of comments to a blog, we construct a directed reader graph G R :=( V R , E Each node r a  X  V R is a reader, and an edge e R ( r b ,r exists if r b mentions r a in one of r b  X  X  comments. The weight on an edge, W R ( r b ,r a ), is the ratio between the number of times r b mention r a against all times r b mention other readers (including r a ). We compute reader authority using a PageRank [1] like algorithm, shown in Equation 1, where |R| denotes the total number of readers of the blog, and d is the damping factor as in PageRank.
 RM ( w k )= The reader measure of a word w k , denoted by RM ( w k ), is giveninEquation2,where tf ( w k ,c i ) is the term frequency of word w k in comment c i ,and c i  X  r a means that c i is authored by reader r a .

With Observation 2, for the set of comments associated with each blog post, we construct a directed acyclic quo-tation graph G Q := ( V Q ,E Q ). Each node c i  X  V Q comment, and an edge ( c j ,c i )  X  E Q indicates c j quoted sentences from c i . The weight on an edge, W Q ( c j ,c over the number of comments that c j ever quoted.
We derive the quotation degree D ( c i ) of a comment c i using Equation 3. A comment that is not quoted by any other comment receives a quotation degree of 1 / |C| where |C| is the number of comments associated with the given post.
 The quotation measure of a word w k , denoted by QM ( w k is given in Equation 4 where w k  X  c i means that word w k appears in comment c i .

With Observation 3, given the set of comments associ-ated with each blog post, we group these comments into topic cluster s using a Single-Pass Incremental Clustering al-gorithm presented in [7].

We believe that a hotly discussed topic has a large number of comments all close to the topic cluster centroid. Thus we have Equation 5 to compute the importance of a topic cluster, where | c i | is the length of comment c i in number of words, C is the set of comments, and sim ( c i ,t u )isthe cosine similarity between comment c i and the centroid of topic cluster t u .
 Equation 6 defines the topic measure of a word w k , denoted by TM ( w k ). In this equation, c i  X  t u denotes comment c is clustered into topic cluster t u .
The representativeness score of a word Rep ( w k )isthe combination of reader-, quotation-and topic-measures in ReQuT model, shown in Figure 2. The three measures are first normalized independently based on their correspond-ing maximum values and then combined linearly to derive Rep ( w k ) using Equation 7. In this equation  X  ,  X  and  X  are the coefficients (0  X   X ,  X ,  X   X  1 . 0and  X  +  X  +  X  =1 . 0).
Rep ( w k )=  X   X  RM ( w k )+  X   X  QM ( w k )+  X   X  TM ( w k As both readers and bloggers have no control on authority measure and very minimum control on quotation and topic measure, we argue that ReQuT is less sensitive to spam com-ments.
Two sentence selection methods are evaluated in our ex-periments, namely Density-based selection and Summation-based selection.

Density-based selection (DBS) was proposed to rank and select sentences in question answering [5]. Given a set of weighted keywords representing a question, a sentence is scored using Equation 8, where K is the total number of keywords contained in s i , Score ( w j )isthescoreofkeyword w ,and distance ( w j ,w j +1 ) is the number of non-keywords (including stopwords) between the two adjacent keywords w j and w j +1 in s i . We adopted DBS in our problem by treating words appearing in comments as keywords and the rest non-keywords.
 Score ( s i )= 1
Summation-based selection (SBS), proposed in this pa-per, gives a higher representativeness score to a sentence if it contains more representati ve words. Nevertheless, SBS does not favor long sentences by considering the number of words in a sentence (see Equation 9). In this equation, | is the length of sentence s i in number of words (including stopwords), and  X  (  X &gt; 0) is a parameter to flexibly control the contribution of a word X  X  representativeness score.
To the best of our knowledge, no similar user study has been conducted before; hence there is no benchmark dataset. We collected data from two famous blogs, i.e., Cosmic Vari-ance 2 and IEBlog 3 , both having relatively large readership and being widely commented. The former has more loyal but fewer readers with very diverse topics covered in posts; while the latter has less loyal but more readers, with topics mainly in Web development. Table 1 reports statistics of data collected 4 .
With 10 posts randomly picked up from each of the two blogs and 3 human summarizers recruited from final-year Computer Engineering students, we conducted a user study on the impact of reading comments. Our hypothesis is that one X  X  understanding about a blog post does not change after he or she read the comments associated with the post.
The user study was conducted in two phrases. In the first phrase, we provided 3 summarizers the 20 blog posts with-out comments and asked them to select approximately 30% of sentences from each post as its summary. The selected sentences served as a labeled dataset known as Reference-Set 1, or RefSet-1 for short. In the second phrase, 3 human http://cosmicvariance.com http://blog.msdn.com/ie
Note that  X  X ingback X  and  X  X rackback X  comments are ex-cluded in our dataset Parameter CosmicVariance IEBlog Number of blog posts 1114 364 Number of readers 2904 9490 Average post length 508.8 376.4 Average comments per post 22.1 66.8 summarizers were provided the nearly 1000 comments as-sociated with the 20 posts, and were asked to read both the posts and their comments, and again to summarize the posts by labeling approximately 30% of the sentences from each post. We name the second set of selected sentences Reference-Set 2, or RefSet-2 .

We computed the level of peer-agreement for each pair of human summarizers. The averaged peer-agreement level in RefSet-1 and RefSet-2 are 37.8% and 32.6% respectively.
For each human summarizer, we computed the level of self-agreement shown in Table 2. Self-agreement level is de-fined by the percentage of sentences labeled in both reference sets against sentences in RefSet-1 bythesamesummarizer. Recall our hypothesis is that one does not change his/her understanding about a blog post after reading comments, the expected level of self-agreement is 100% for every sum-marizer. The observed much lower self-agreement level is significant enough to invalidate our hypothesis 5 .Thatis, reading comments does change one X  X  understanding about blog posts.
As the sentences are labeled after reading comments, RefSet-2 was used to evaluate the two sentence selection methods with four word representativeness measures. We adopted R-Precision and NDCG (see [4]) as performance metrics. In NDCG ,the Relevance Level of a sentence is defined by the number of human summarizers labeled that sentence in RefSet-2 . The reported results in Table 3 are averaged over all posts.

In our experiments, the similarity threshold in clustering comments was empirically set to 0 . 4; and parameter  X  in SBS was set to 0 . 2. The three coefficients  X  ,  X  ,and  X  in combining reader-, quotation-, and topic-measures were all 0 . 33.
 As shown in Table 3, with either R-Precision or NDCG, SBS achieved better performance than DBS over all four word representativeness measures. For SBS method, ReQuT performed the best among the four word measures. Nev-ertheless, ReQuT together with SBS was not significantly better than other combinations according to our significance test. The possible reasons are: (i) the dataset is small and (ii) there is almost no spam comment in our dataset.
The much lower self-agreement level on IEblog (compared with CosmicVariance) could be due to the fact that IEblog posts contain much more comments than that from Cosmic-Variance.

Based on the findings in our user study that reading com-ments does affect one X  X  understanding about a blog post (and probably other kind of Web objects), we define the problem of comments-oriented blog post summarization. Our proposed solution measures word representativeness using informationhiddenincomments,andthenselectssentences based on the representativeness of the words contained in sentences. Using human labeled sentences, we evaluated two sentence selection methods with four word represen-tativeness measures. Among the latter, ReQuT gives the flexibility to measure word representativeness through three aspects, reader, quotation and topic. To study the impact of the three aspects in ReQuT is part of our future work. [1] S. Brin and L. Page. The anatomy of a large-scale [2] A. de Moor and L. Efimova. An argumentation analysis [3] J.-Y. Delort. Identifying commented passages of [4] K. Jrvelin and J. Keklinen. IR evaluation methods for [5] G. G. Lee, J. Seo, S. Lee, H. Jung, B.-H. Cho, C. Lee, [6] G. Mishne and N. Glance. Leave a reply: An analysis of [7] D. Shen, Q. Yang, J.-T. Sun, and Z. Chen. Thread [8] J.-T. Sun, D. Shen, H.-J. Zeng, Q. Yang, Y. Lu, and [9] L. Zhou and E. Hovy. On the summarization of
