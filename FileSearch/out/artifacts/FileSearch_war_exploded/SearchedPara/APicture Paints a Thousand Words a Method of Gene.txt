 Manual timelines have greatly helped us to keep pace with the big world. In this paper, we introduce a novel solu-tion which generates image-text timelines for news events based on Evolutionary Image-Text Summarization, which is an important and challenging problem. We first extract image X  X  semantic information under translation model, and then fuse the high quality images with text timeline under an image assignment algorithm which can optimize the global coordination of the final timeline. The experimental results show that news readers can receive more satisfaction from the image-text timelines we generate.
 H.4 [ Information Systems Applications ]: Miscellaneous Algorithms, Experimentation Image-text, Cross-modality, Summarization, Timeline
With the rapid spreading of Web2.0, the news reports consist of various types of information, especially text infor-mation and image information. Since the social rhythm is quickening, news consumers need a more concise and con-venient way to perceive directly. But given such large texts and images collection related to a specified news subject (i.e., Occupy Wall Street ), readers usually want to simply find out the current progresses, causes and effects.
Benefitting from the researches on automatic multi-document summarization, we can generate a description for a specific Corre sponding author Support by NSFC under Grant No. 61073081 event. However, almost all of the existing work focuses on one specific modality, text or image. Analysis and consoli-dation of inter-modality are academically novel. Illustrated summary can give a more concise presentation for an event.
Timeline could be a good display form with abbreviated, informative reorganization for faster and better browsing. The websites usually spend a lot of human cost on provid-ing image-text timelines. Learning from manual timelines, we can draw two requisite features for a text-image time-line. First, from inner-modality view, both texts or images should present valuable information in one day and keep a coherent temporal sequence among different dates. Second, from inter-modality view, texts and images should talk un-der the same event and informatively replenish each other. Studying on how to combine these two kinds of resources is of practical significance.

In this paper, we introduce a novel solution for the au-tomatic image-text evolutionary summarization problem to meet the challenges mentioned above. For the challenge to inner-modality , we generate text timeline under a trans-temporal framework while keeping a good data individual-ity and correlativeness. For the challenge to inter-modality , we propose a model employing GlobalOptimizedImage-Assignment method to enrich the text timeline with the high quality images. The experimental results conducted on the real data from 6 famous news websites show that our approach can outperform the rivals effectively.
To our best knowledge, this paper is one of the few innova-tive researches that steps into cross-modality summarization field, though there are kinks of research focusing on summa-rization of single domain. Great efforts have been made to generate text timeline but they ignore the important role of images[3]. Contrariwise, some focus on visual modality[11].
To mine the latent knowledge of image, visual features are described as visual words in [6] and a probabilistic model is proposed based on the assumption that images and their co-occurring textual data are generated by mixtures of la-tent topics. Keyphrases usually denote the semantic fea-tures of image, Feng et al. operate over the output of a probabilistic image annotation model that preprocesses the pictures and suggests keywords to describe their content[5]. Image retrieval has also been used to enrich text content, i.e., Agrawal et al. propose techniques for finding images from the web and attach to a text section for augmenting[1].
Unlike previous studies, our method can combine modal-ities of text and image in a harmony.
To reveal the perceptive relevance between images and texts, we first model the semantic features of images.
Images will be preprocessed so that they can be repre-sented by word-like units. Local image descriptors are com-puted using the SIFT algorithm[7] and subsequently quan-tized into a discrete set of visual words via a clustering algo-rithm such as K-means. Formally, each image is expressed in a bag-of-words format vector [ vw 1 , vw 2 , . . . , vw vw i = n only if the image has n regions denoted by v i . Then we manage to translate visual words into textual words through statistical translation model. In our dataset, most news images have news texts around them. On an intuitive level, the text contexts can tell what these images are talk-ing about, and thus well define the semantic content of the images. Given a set of news webpages X  html docs, VIPS[2] and WebKit 1 rendering tool help us to segment pages and pick those text blocks whose coordinates are neighboring to each specified image. We treat these text contents as image X  X  semantic annotations. The model now works with a bag-of-words representation and treats each image-annotation pair as a single document consisting of textual and visual words. Given a visual word vw and a textual annotation word w , based on their co-occurrences we estimate P ( w j vw ) using maximum likelihood estimation with Laplace smoothing[4]: where vw i , w i denote the frequency of v w and w is the i th image-annotation pair, n denotes the number of pairs in training set and V denotes the visual vocabulary size.  X  is the smooth parameter which makes our model insensitive to data noise and is fixed to be 1. Then given an unknown image we can translate it with its visual words into a set of semantic features, which in practical form is an integral probability distribution of textual words.
Intuitively, a required summary needs to be informational rich with both global biased and local biased view. That means the target summary should be correlative with sen-tences from today as well as neighboring dates, which will lead to significant temporal continuity. ETTS [12] offers a good process and we extend its work to generate text time-lines. We conduct a temporal proximity projection proce-dure to achieve global biased characteristic.

To generate the component summary on date t , besides considering the sentences with timestamp t , we also emphat-ically regard the sentences with different timestamps. All sentences in the collection are projected onto the time hori-zon of t to construct a global affinity graph with Gaussian kernels  X ( X  t )[8], where  X  t = j t t  X  j is the distance between the pending date t and neighboring date t  X  :
P arameter  X  will be tuned in later experiments, which controls the spreading of kernel curves. After DivRank[9] is applied, we can estimate the local rank as well as the global ht tp://www.webkit.org rank of each sentence and then calculate the merged rank with linear weighting, which can be proved to be optimal. Thus the text timeline is generated.

We will assign images to the given text timeline. Formally, given the image and sentence collection I , S partitioned by the timestamp set T , I t = f I i j 1 i j I t jg , and S f
S i j 1 i j S t jg , where S i and I i is a sentence and an image with the timestamp t 2 T . For a specific date t , we com-pute the target affinity matrices M = ( m ij ) | I | X | S | the semantic relevance between image and sentence. Since our translation model can offer a semantic feature for every image, presented as I i ! f p ( w 1 j i ) , p i ( w 2 j i ) , . . . , p where p ( w j i ) denotes the semantic transition probability of image I i to word w and W is the vocabulary shared by both modalities. We can calculate the semantic similarity sim ( I i , S j ) between image I i and sentence S j :  X  ( w ) is the translation probability from image I i to word w with isf weighting.  X  ( w ) denotes the tf isf weighting of word w , and tf ( w j i ) is the term frequency of word w in S isf is the inverse-sentence-frequency to diminish the weight of common words and increase the discrimination: wh ere N w is the number of sentences or images contain-ing word w . m ij that denotes the relevance between image I and sentence S j can be derived as follow. And like the handling of sentences, temporal weighting is added to the similarity.
Note that normalization is employed on the matrix to make the sum of each row equal to 1. In general, the opti-mal setting of  X  may vary according to the dataset because sentences and images would have wider semantic scope on news timeline, so that a higher value of  X  is required. And vice versa.

Each image I i calculates its worthiness score R ( I i ) based on the ordered sentences. Specifically, given text summary of date t which has size of n t , image I i  X  X  score R ( I be estimated as follow:
Now we conduct a optimal image assignment method as the final step of our summarization work. On an intu-itive level, this problem admits a natural greedy algorithm. For every date t , all images (include today X  X  and neighbor-ing days X ) are sort by their scores R ( I i j t ), then more valu-able images are picked out to make up summary for date t and no longer selectable for the other dates. But con-sider the following situation. On two specific dates t 1 and t , ( I i , R ( I i j t )) means that image I i  X  X  score is pose we have t 1 f ( I 1 , 0 . 4) , ( I 2 , 0 . 4) , ( I f But an optimal assignment is t 1 f I 1 , I 3 g and t 2 f with a total score of 1 . 5. Therefore the image assignment problem needs an optimization solution. We present an al-gorithm to satisfy the optimization efficiency as well as com-putation feasibility and proof it later. The following is the statement of the optimization problem: s.t.
Here,  X  ti is an indicator variable that takes value 1 if image i is selected for date t and 0 otherwise. Eq.2 captures this binary constraint. Eq.3 ensures that the number of images assigned to one day is at most, and furthermore, enforces that each image is assigned to at most one day. The op-timization objective (Eq.1) is the global score of the final timeline, defined as the sum of valuable scores over all dates of which the images assigned to. Thus the goal of the op-timization is to compute the binary variables  X  ti such that the total score for the whole timeline is maximized.
We can prove the GlobalOptimizedImageAssignment can be solved optimally in polynomial time by showing an efficient reduction from our image assignment to the Max-imum Weighted Bipartite Matching problem[10], which ad-mits an efficient polynomial time solution and can be solved optimally in O ( nm ( n + m )) time (the numbers of edges and nodes of are supposed to be m and n ). Due to the limited space we don X  X  present the reduction process.
We randomly choose 8 news subjects with special cov-erage and handcrafted timelines by editors from 6 selected news websites: New York Times , BBC , CNN , China Daily , Reuters and Yahoo! News . The manually-edited timelines are employed as our golden reference standards to evaluate the proposed systems empirically. Table 1 shows the details.
There are no acknowledged metrics to judge image-text timeline X  X  quality, so we build an evaluation system which includes two golden indexes, QDCG and SDCG . We em-ploy five volunteers as the news readers to rate the quality of images showed to them in timelines with three levels, judg-ing if the images fit the text well. The levels range from 1 to 3 where level 3 denotes the best. To evaluate the whole time-line with the index, we estimate average quality discounted cumulative gain (QDCG) scores as follows: where rate Q ( i j t ) denotes the rating level of the i th image in date t  X  X  timeline. The main idea about the usage of loga-rithm is that more important sentences pick more valuable images. What X  X  more, in this way images and sentences can be well organized to help reading. Besides judging the im-age quality, we are concerned with the attitude of readers to the inserted images. So the judges quantize how the images influence their reading experience with three grades at the same time. An image may get high grade if it does well in complementing the information as well as increasing reading enjoyment. We also estimate average satisfaction discounted cumulative gain (SDCG) as follows:
The unit of timestamp is one day. The upper limit K t of the number of picked images are fixed to 8 for every t 2 T after considering datasets X  size and browsing experience. We tentatively set  X  = 10, and tune them later. Since the image-text summarization work is novel among current researches, it becomes a challenge to highlight our method X  X  effective-ness. We focus on the following image-text summarization algorithms as the baselines.

Randomly assignment . The method randomly picks and assigns the images with the specified timestamp.
Image retrieval assignment . The method runs text summarization first. Based on the sentences in the summa-rization, we use COMITY [1] to mine images from web after forming key phrases for each day. Then the most relevant images are assigned to the text timeline.
 Greedy assignment . This approach shares the same R ( I i j t ) for each image as our method, but the images with high scores are selected by the first date and removed di-rectly from the candidate list for the other dates. The same strategy is applied to the following dates.
Figure 1 shows that our ranking framework can outper-form the others. Randomly assignment has the worst per-formance as expected. Retrieval-based gets a poor score of SDCG due to that it picks many discordant images when taking no textual context influence into account. It X  X  worth noting that Greedy assignment gets a much better result than the previous two and we can verify the superiority of translation model and worthiness score estimating.
Further more, we can see that our assignment approach defeats Greedy by a significant upgrade. This result verifies th e effectiveness of our GlobalOptimizedImageAssign-ment algorithm. Note that our Optimized method serves the last five datasets more better. Other methods play espe-cially poorly on those datasets which contain less burstiness, as Optimized is still maintaining high quality. Now we can draw an inspiring conclusion. With modeling the tempo-ral continuity, cross-modality and maximize assigning score at the same time, our method can get the most satisfying timelines of summaries over the other methods.

The key parameter  X  measures the temporal projection in-fluence from global candidates to local candidates and hence the size of neighbors. Figure 2 shows that the events with high burstiness prefer a smaller value to dominate the influ-ence from neighbors. In the opposite we get converse results. We can see that the summarization method to some extent is sensitive to the data. Developing a parameter adaptive framework can be one of our future work.

At the end, we analyze our generated timelines. We notice that for all datasets we can highlight the material facts of events and keep information diversity at the same time. Ta-ble 2 shows two-days timeline (discontinuously) of  X  X ccupy Wall Street X  as the sample output.
We study the feasibility of automatically generating time-lines of image-text summaries and present a solution for this problem. Our method exploits the internal relationships be-tween image and text under our translation model. We gen-erate text timeline with time individuality and correlative-Table 2: Part timeline of \Occupy Wall Street" nes s. Finally we raise up an image assignment algorithm to optimize global coordination as well as gain panoramic harmony. Experimental results show that our method can successfully generate satisfying image-text timelines. We ad-ditionally analyze some interesting findings about data de-pendency. In the future, we plan to fuse different modalities in a unified ranking framework so that they can improve each other. [1] R. Agrawal, S. Gollapudi, A. Kannan, and [2] D. Cai, S. Yu, J. Wen, and W. Ma. Vips: a visionbased [3] M. Chandra, V. Gupta, and S. Paul. A statistical [4] S. Chen and J. Goodman. An empirical study of [5] Y. Feng and M. Lapata. How many words is a picture [6] Y. Feng and M. Lapata. Topic models for image [7] D. Lowe. Object recognition from local scale-invariant [8] Y. Lv and C. Zhai. Positional language models for [9] Q. Mei, J. Guo, and D. Radev. Divrank: the interplay [10] C. Papadimitriou and K. Steiglitz. Combinatorial [11] L. Tan, Y. Song, S. Liu, and L. Xie. Imagehive: [12] R. Yan, L. Kong, C. Huang, X. Wan, X. Li, and
