 Exploratory search systems are designed to help people with ill-defined information needs. People X  X  information need tends to be ill-defined when they engage in a com-plex task. Exploratory search systems can be categorized into two groups based on accessibility to an entire document collection. When one has an access to an entire doc-ument collection, some level of content analysis can be performed and a set of common properties can be identified. These common properties are then used to organize the collection for users to explore. Examples of this type of exploratory search systems are Flamenco [9] and mSpace [7]. Another type of exploratory search systems does not assume an access to an entire collection. Therefore, support is often given to users dy-namically based on their search history and some analysis of search results. Examples of this type of exploratory search systems are AspecTiles [2], Aspectual browser [8], and Slice n X  Dicer [3]. This paper proposes a new interaction model for the latter type of exploratory search systems, and studies its e ff ect on task performance, information seeking behavior, and user perceptions. 1.1 Grid-Based Interaction Model The interaction model proposed in this paper was derived from two lines of research in interactive information retrieval (IIR). One is an instance finding task that has been studied in a series of Interactive Tracks in TREC [1]. In this task, searchers were asked to find instances of events, achievements, countries, technologies that matched a partic-ular condition. The idea was to go beyond a conventional document retrieval which was often not su ffi cient to study interactive aspects in IR. For example, searchers received no reward to find duplicated instances in th e task. A key element of this task was that it used instances as an axis of informatio n seeking process. Inst ances can be a generic yet powerful property to search, organize, an d analyze a given topic and its information space. For example, Barack Obama, David C ameron, and Hu Jintao are an instance of world leaders. Similarly, iOS, Android, and Windows Mobile are an instance of mo-bile operating systems. Instances tend to co-occur in relevant documents since they are often contextually related. Therefore, we consider that the notion of instances plays an important role in supporting exploratory search tasks.

Another line of research is called aspectual search [2,8]. Aspectual search empha-sizes to find aspects to complete an exploratory search task. For example, writing a biography of a world leader requires to find and select what aspects of the leader should be included. Making a summary of a large event like Olympics also depends on ex-ploration of potential angles. These aspects or angles can be relatively simple such as time and location in some cases. However, when a topic becomes complex, determining appropriate aspects is not trivial. Again, a key element of this search was that it used aspects as an axis of information seeking p rocess. Aspects can be seen as a property of instances. For example, age, education, and political agenda are an aspect of world leaders. Similarly, price, required memory, and hardware compatibility might be an as-pect of mobile operating systems. If instan ces are a vertical axis, aspects can be seen as a horizontal axis in exploration and organization of a search space.

As can be seen, these two lines of research are closely related, and thus, can be integrated into a single model. This was our motivation and we call it a grid-based interaction model . Existing studies did not make a clear distinction of the two kinds of notion. In this sense, our model was an extension of the research which looked at instance finding and aspect finding. An importa nt consideration here is that searchers are unlikely to find instances and aspects at t he beginning of search. Instead, they are more likely to discover these elements of a given topic as they make progress in a search task. Therefore, we need an interaction model that can support exploration of a search space using the notion of aspects and instances.

We consider that an expression of information needs is crucial for supporting ex-ploratory search, and thus, devised a syntax to formulate a grid-based query as shown in Figure 1. Dimensions in a query space are separated by a bar sign ( | ), which means that this syntax can represent as many dimensions as needed. However, in this paper, we only consider two dimensions. All the terms placed before the bar sign represent the first di-mension while those after the bar sign represent the second dimension. Keywords in in-dividual dimensions are separated by a comma (,). When more than one keyword is used between commas, such term will be taken as a phrase. For example, the query,  X  X lack, white | cat, dog X  , will represent four search queries such as  X  X lack cat X ,  X  X lack dog X ,  X  X hite cat X , and  X  X hite dog X . The query,  X  X avid cameron, barack obama | approval rate X  , will yield two queries such as &lt;  X  X avid cameron X   X  X pproval rate X  &gt; and &lt;  X  X arack obama X   X  X pproval rate X  &gt; . As can be seen, the syntax is simple and intu-itive. A user study suggests that participants did not have a major problem to operate a search task with the proposed syntax. To link with the notions of instances and aspects described earlier, the first query had aspects first then instances, while the second query had instances first then aspects. Therefor e, the proposed syntax does not explicitly de-fine the order of dimensions expressed in the query. It is up to a designer (or possibly user) of search systems. The next section des cribes a grid search interface which can be seen as one possible case of implementing the proposed query syntax.
 A search interface was developed based on the interaction model discussed above. The interface is composed of three main ar eas, namely, Query Box (1), Document List (3), and Grid (4), as shown in Figure 2. When a user submits a query in Query Box using the proposed syntax, both Document List and Grid are shown with search results. There is a History button next to Query Bo x which allows users to revisit and rerun previous queries submitted in a search session.

The Grid area consists of Labels (5) and Cells (6). Labels are derived from individ-ual terms given in Query Box. Given that a user submits a query Blair, Clinton | Middle East Peace, Japanese Economy to the interface, terms on the left part of a query (i.e., Blair and Clinton) are placed as a row label (Green Line), while terms on the right part (i.e., Middle East Peace and J apanese Economy) are placed as a column label (Red Line). Moreover, the terms in Query Box and Labels are synchronized, and users can edit either of them to reformulate the query. Double-clicking a label allows a user to change the keywords. Add Button (7) allows a user to append a new label to the grid. Full-text is shown in a pop-up window when the title of the result is clicked from a cell or document list. Cells (6) show search results of the grid-based queries as shown in Figure 3. Each cell shows the res ult of a particular combination of terms derived from the query. Taking our earlier example query, Cell 1 shows a search re-sult of a query, &lt; Blair  X  X iddle East Peace  X  &gt; ,Cell2showsthatof &lt; Blair  X  X apanese Economy X  &gt; , and so forth. As a consequence, one grid presents the results of four sub-queries in this case. Grid does not have to be n  X  n , and can be n  X  m . Furthermore, the order of rows and columns can be changed by dragging a label.

When a cell is selected, Document List shows search results of a particular sub-query (e.g., &lt; Blair  X  X apanese Economy X  &gt; ). When multiple cells are selected, Document List shows search results by merging selected individual cells. Currently, the merged rank-ing is based on redundancy and round-robin. In other words, those documents that are commonly retrieved by sub-queries are ranked higher than those are retrieved once. Tie-documents are ranked in a round-robin manner across the retrieved documents of individual sub-queries. When a query is submitted or reformulated in Query Box or via Labels, Document List shows a set of retrieved documents by merging the results of all sub-queries. This is equivalent to select all cells in Grid. A more e ff ective way to generate the document list is of our future work. The evaluation of the proposed search interface was carried out as a user study in the framework of NTCIR-9 VisEx Task 1 . Since the detail of experimental design is given in the overview paper of VisEx Task [5], this section only gives a brief summary of how a user study were carried out. It should be noted that a team who participated in VisEx Task will be referred to as participants , while people who participated in the user study as a subject will be referred to as subjects in the rest of this paper.

The task required participants to develop a user interface that can support either an event collection task or trend summarization task or both. An example of the event collection task is to find as many instances of airplane crashes that occurred in Asia as possible. An example of the trend summariza tion task is to find as many information that describes a prime minister X  X  approval rate as possible. As can be seen, both tasks were exploratory and required a number of iterations of search to complete them. Subjects were given up to 50 minutes to complete the tasks. A baseline system was provided by the organizer and all systems used t he API of a common backend search engine. Subjects were given an instruction (6 slides ) of how to use the systems and a training session to familiarize with tasks and behavior of the experimental system. In the system instruction, an emphasis was made to enc ourage people to organize a problem space using a n  X  m notion such as people  X  year , place  X  event ,and things  X  attributes . This section presents results of our preliminary analysis on the two exploratory tasks of VisEx. It should be noted that our analysis is intentionally qualitative due to the experimental design. Therefore, no statisti cal test is performed unless otherwise stated. Finally, please note that we use the term nugget [6] to refer to a unit of information required to collect in individual tasks, which might be di ff erent from traditional use. Task Performance. Both tasks asked subjects to collect as many relevant nuggets as possible within allocated time. Therefore, an overall task performance can be measured by the number of nuggets found. The results are shown in Table 1. If you look at the third row of Table 1, the performance was compara ble between the baseline and experimental systems in the former task, while the di ff erence between the two systems was found to be larger in the latter task. A similar trend was found in the maximum number of nuggets found by subjects. A grand average number of nuggets found by subjects was 7.5 (SD: 3.1) and 7.2 (SD: 2.9) for Event collection task and Trend summarization task, respectively (not shown in the table).

The next analysis looked at whether or not a particular topic was easy (or di ffi cult), or a particular subject performed really well (or bad). The results are shown in Figure 4. For the topic breakdown of the number of nuggets, the X axis represents Topic 1 to 4 of the two task while the Y axis represents the number of nuggets found. Each topic line has five data points which correspond to five subjects. A horizontal bar is the median value of the five data points. Please note that Topic 1 of Event collection task has nothing to do with Topic 1 of Trend summarization task. Similarly, for the subject breakdown, the X axis represents Subject 1 to 5, and data points are the number of nuggets found by each subject across four topics . Again, Subject 1 in Event collection task is a di ff erent person from Subject 1 in Trend su mmarization task. As for the topic breakdown, no obvious pattern was observed from the analysis. Topic 3 of Event collec-tion task seems to have the best performance in both systems, but the data range is large, so this is not necessarily the easiest topic. Subjects with the baseline system appear to struggle in Topic 2 of Event collection task while Topic 1 appears to be the most dif-ficult one by subjects of the experimental system. In Trend summarization task, fewer noticeable pattern was observed. As for the subject breakdown, it appears that subjects X  performance varies over the system groups as well as the task groups. All groups seem to have a good performer and poor performer, some had a large di ff erence across topics, while others had a similar performance.
 Finally, we looked at whether or not we had order e ff ects on subjects X  performance. There were at least two reasons for us to investigate the e ff ect. First, since this was the first time for subjects to carry out a search task with the grid search interface, their per-formance might increase as they got used to the system. Second, since individual tasks lasted 50 minutes, we suspected that there might be fatigue e ff ect on their performance towards later topics. Pearson X  X  correlation shows a significant correlation ( p  X  . 003) be-tween the number of nuggets and topic order, but coe ffi cient was r = . 44 which means the contribution ratio is below 20%. Therefore, an order e ff ect appears to be weak.
To summarize the results of task perform ance, subjects with the experimental sys-tem appear to achieve a comparable performance to the baseline system in Event col-lection task, while there seems to be some f actor in the experiment system that caused a performance loss in Trend summarizatio n task. These observations did not seem to be influenced by a particular topic, subject, nor order, although their interaction e ff ect might exist. The following sections look at information seeking behavior and subjective assessments to gain a further insight into these results. Information Seeking Behavior. An advantage of the proposed query syntax was that it allowed a user to express a complex search space as they made progress in exploratory search tasks. Therefore, we first looked at how subjects formulated and reformulated their queries during the VisEx tasks. Query history of the best performing session (i.e., the session with the largest number of nuggets found) using the experimental system was shown in Table 1(a) and Table 1(b) for Event collection task and Trend summariza-tion task, respectively. Queries are a translation from the original language used in the experiment (i.e., Japanese).

Table 1(a) shows the queries for the topic which asked for event information (e.g., time, location, people) about nuclear tests all over the world. The first query appears to try to retrieve documents that includes a te xt like  X  X arried out a nuclear test X . In fact, the subject found several nuggets from this query. Then, except Query #5, the rest is a combination of  X  X uclear test X  and a country name. In other words, the grid did not really expand as the task developed. The number of nuggets found by those queries was mixed. More importantly, those queries with country names could have been ex-pressed as nuclear test | France, North Korea, Russia, UK, China .Table 1(b) shows the queries for the topic which asked for trend information about the price of crude oil and regular gasoline in Dubai. This topic was more complex than the pre-vious example, since it explicitly asked for three dimensions: time, price, and oil type. Using the proposed syntax, users would have to somehow divide them into a set of two dimensions. Such attempt can be found in Query #4 and #11 where the subject focused on oil type and year. However, it seems that the results were not satisfactory without a location in the query, and thus, other queries contains Dubai in either side of the query.
The last analysis on information seeking behavior examined to what extent subjects accessed external sources to complete tasks. In the VisEx tasks, subjects were allowed to access external sources to support their tasks, although none of the nuggets found in external sources counted in performance measures. The results are as follows. There were 10 (Baseline) and 5 (Experimental) subjects who answered yes to the question in Event collection task, while there were 3 (Baseline) and 4 (Experimental) in Trend summarization task. As can be seen, there was a noticeable di ff erence between the two systems in Event collection task while th e frequency was comparable in Trend sum-marization task. Given that an overall task performance of Event collection task was comparable between the two systems, subj ects with the baseline system appeared to need more support outside the given system than the experimental system. The moti-vation for accessing external sources varied. The post-search questionnaire established that subjects sought for a detail of a particul ar event, definition of a technical term, or information needed to judge relevance of nuggets.

To summarize the results of information s eeking behavior, we have observed cases where the proposed syntax could be e ff ective to organize a search space. There were cases where subjects organized a query space two-dimensionally (e.g., oil types  X  year in Table 1(b)). However, the frequency of e ff ective use of the proposed syntax was relative low. As for relevance judgements on document surrogates, the experimental system appeared to increase a chance of finding nuggets in click-through documents in Event collection task. Finally, subjects tended to need an access to external sources in the baseline system when compared to the experimental system.
 User perceptions. The last part of our analysis looked at subjects X  perceptions on tasks and systems. Subjective assessments were captured by a 7-point Likert scale where subjects indicated a degree of agreement to a given statement. For example, a statement was  X  X he task I performed was complex X  and the scale were Strongly Agree (1), to Either (4), to Strongly Disagree (7). Task perceptions were captured after every topics, while system perceptions and interaction p erceptions were captured after all topics.
The results of task perceptions are shown in Table 3. The numbers in Table 3 are a median of corresponding data. As can be seen, we did not observe a large di ff erence in any aspects of task perceptions. However, a topic-breakdown of the data (not shown) suggests that the di ff erence in Satisfaction in Trend summarization task is likely to be due to Topic 3 and 4. On the other hand, the di ff erence in the percep tion of time (You had su ffi cient time to complete a task) seems to be consistent over all topics in both tasks. The di ff erence in the perception of resources (You found a su ffi cient amount of news articles to complete a task) is likely to be due to Topic 1 and 3 in Event collection task. Little pattern was observed in the rest of questions.

Another question we asked in the post-search questionnaire was whether or not sub-jects discovered new knowledge which was somehow unexpected, during the task. The results are as follows. There were 1 (Baseline) and 16 (Experimental) subjects answered yes to the question in Event collection task, while there were 10 (Baseline) and 8 (Experimental) in Trend su mmarization task. As can be seen, there was a noticeable di ff erence between the two systems in Event collection task, while the number was comparable in Trend summari zation task. Examples of discovery reported by subjects include an association between two events, varied amount of information across coun-tries, lack or bias of information in news articles, as well as topics themselves. Finally, the results of subjects X  perceptions of the systems will be briefly reported. In Event col-lection task, subjects appeared to find the baseline system easier to learn and to operate than the experimental system. There was a clear trend in the assessments of function-ality. Additional comments suggest that subj ects wanted an ability to submit a standard query, to move to next 10 results within a cell, and to sort documents by date. Response speed seems to be acceptable. All subjects seemed to feel some level of frustration during the task, but the variance was much larger in the experimental system than the baseline.

To summarize user perceptions, we did not observe a large di ff erence between the two systems in terms of the perception of tasks although some values varied over topics. However, with the experimental system, subjects tended to encounter new knowledge during the tasks when compared to the baseline system. As for the perception of sys-tems, subjects seemed to find it more familiar to the baseline system than the experi-mental system. Subjects expressed several features that they would like to have in the experimental system, which can be considered for further development. A new grid-based interaction model was proposed, and the performance of the grid-based search interface was measured by three aspects in our study: task performance, information seeking behavio r, and subjective assessments. This section first summarizes the major findings of the experiment and discuss their implications for further research and development of the grid -based interaction model.

The overall task performance of the two sy stems was comparab le in Event collec-tion task. However, we observed some positive signals in the experiment. Based on the frequency of external source access and of disc overing new knowledge, the grid-based interface might facilitate the exploration o f a document collection through analytical search process during the tasks. It might be possible that the performance of the base-line system was actually due to a frequent access to external sources. We need more analysis to exploit this aspect. We also obtained ideas to improve the current implemen-tation of the grid-based interaction model. The query analysis of the best performing session suggests that there were cases where the proposed query syntax can be e ff ective to represent a complex search space. Howeve r, subjects did not appear to take advan-tage of the syntax. Since our tasks were not simple like home page finding, it is possible that subjects were focusing on finding nuggets than e ff ectively leveraging the potential benefit of the syntax. Thus, more examples should be given in the instruction, and a step-by-step tutorial might be needed in a training session. A comparison of query for-mulation process between the two systems should give us a better idea of how exactly such instruction should be formed.

The overall task performance of the experimental system appeared to be lower than the baseline system in Trend summarization task. We did not observe a particular topic or subject strongly a ff ected the average performance. Furthermore, several aspects such as the successful click-through rate, frequency of accessing external sources, and fre-quency of discovering new knowledge seem to be comparable between the two systems. We speculate that information needs often fo rmulated in Trend summarization task re-quire more than two dimensions to express, which was not supported well in the current implementation of the interf ace. We intentionally limited the dimension size to two, but this could cause an extra e ff ort to subjects to divide a search space to a set of two dimen-sions. This was exemplified in the query reformulation of the best session in this task. In short, this task was more complex than the current search interface was designed to sup-port. It is technically possible to expand th e proposed query syntax to accept more than two dimensions. However, this would require further consideration regarding how to present search results in a way that they make sense to searchers. More fundamentally, we need to study how to guide searchers to divide high-dimensional complex search into sub spaces, and how to support such tactics using the grid-based interaction model. These are all interesting research questions to pursue as future work.

