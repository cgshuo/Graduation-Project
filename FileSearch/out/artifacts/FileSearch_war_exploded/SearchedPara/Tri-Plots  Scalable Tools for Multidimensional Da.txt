 We focus on the problem of finding patterns across two large, multidimensional datasets. For example, given feature vec-tors of healthy and of non-healthy patients, we want to an-swer the following questions: Are the two clouds of points separable? What is the smallest/laxgest pair-wise distance across the two datasets? Which of the two clouds does a new point (feature vector) come from? 
We propose a new tool, the tri-plot, and its generalization, the pq-plot, which help us answer the above questions. We provide a set of rules on how to interpret a tri-plot, and we apply these rules on synthetic and real datasets. We also show how to use our tool for classification, when traditional methods (nearest neighbor, classification trees) may fail. 
The automatic discovery of meaningful patterns and rela-tionships hidden in vast repositories of raw information has become an issue of great importance. Multimedia systems for satellite images, medical data and banking information *An extended/color version of the paper is available at h~;tp ://www. db. cs. cmu. edu/Pubs/Lib/kdd01triplot/ tThis research has been partially funded by FAPESP (S X o Paulo State Foundation for Research Support -Brazil), un-der grants 98/05556/5 and 98/0559-7), and CNPq (Brazilian National Council for Science and Technology Development) under grants 52.1685/98-6 and 860.068/00-7. SThis material is based upon work supported by the Na-tional Science Foundation under Grants No. DMS-9873442, IIS-9817496, IIS-9910606, IIS-9988876, LIS 9720374, IIS-0083148, IIS-0113089, and by the Defense Advanced Re-search Projects Agency under Contracts No. N66001-97-C-8517 and N66001-00-1-8936. Additional funding was pro-vided by donations from Intel. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation, DARPA, or other funding parties. requires prior specific permission and/or a fee. KDD 01 San Francisco CA USA 
Copyright ACM 2001 1-58113-391-x/01/08...$5.00 are some examples of prolific data sources. Many of these data are inherently multi-dimensional. It is often difficult to summarize a large number of attributes by extracting a few essential features. Moreover, many methods proposed in the literature suffer from the dimensionality curse and are impractical to apply directly. Thus, dealing efficiently with high-dimensional data is a challenge for researchers in the database field [26, 5]. Things become worse when more than one datasets are involved. 
We propose a method for exploring the relationship be-tween two multidimensional datasets, by summarizing the information about their relative position. Our method re-quires only a single pass on the data and scales linearly with the number of dimensions. sets, find rules about their relative placement in space: Q1 Do the datasets come from the same distribution? Q2 Do they repel each other? Q3 Are they close or far away? Q4 Are they separable? Q5 For a given, unlabelled point, which of the two sets does it come from (if any)? 
In the following section, we will briefly discuss the related work on data mining techniques and describe the datasets we used in our experiments. We then introduce the cross-cloud plots and explain their properties. Based on these, we present a set of practicM rules which allow us to analyze two clouds of points. Finally, we describe the algorithm for generating the plots. 
There has been a tremendous amount of work on data mining during the past years. Many techniques have been developed that have Mlowed the discovery of various trends, relations and characteristics with large amounts of data [16, 6]. Detailed surveys can be found in [7] and [13]. Also, [11] contains an insightful discussion of the overall process of knowledge discovery in databases (KDD) as well as a com-prehensive overview of methods, problems, and their inher-ent characteristics. 
In the field of spatial data mining [9] much recent work has focused on clustering and the discovery of local trends and characterizations. Scalable Mgorithms for extracting clusters from large collections of spatial data are presented in [19] and [18]. The authors also combine this with the extraction of characteristics based on non-spatial attributes by using both spatial dominant and non-spatial dominant approaches (depending on whether the cluster discovery is performed initially or on subsets derived using non-spa-tial attributes). A general framework for discovering trends and characterizations among neighborhoods of data-points 
DBMS and utilizes neighborhood-relationship graphs which are traversed to perform a number of operations. Addition-ally, scalable clustering algorithms are included [1, 25, 24, 12]. The work on fractals and box-counting plots is related: [3] used the correlation fractal dimension of a dataset to es-timate the selectivity of nearest-neighbor queries; [10] gave formulas for the selectivity of spatial joins across two point-sets. [4] analyze the performance of nearest-neighbor queries, eventually using the fractal dimension. More remote work on fractals includes [21], [15], [2]. Almost all of these pa-pers use fast, linear (or O(NlogN)) algorithms, based on the box-counting method. We also use a similar approach for our tri-plots. Visualization techniques for large amounts of multidimen-sional data have also been developed. The work described in [17] presents a visualization method which utilizes views of the data around reference points and effectively reduces the amount of information to be displayed in a way that affects various characteristics of the data (eg. shape and location of clusters, etc.) in a controlled manner. There has also been significant work on data mining in non-spatial, multidimensional databases. Recent work on a general framework that incorporates a number of algorithms is presented in [14]. The authors introduce a general query language and demonstrate its application on the discovery of a large variety of association rules which satisfy the anti-monotonicity property. However, none of the above methods can answer all the questions, Q1 to Q5, which we posed in the previous section. 
The method proposed in this paper can answer such ques-tions. To find a solution for the given problem, we move away from association rules and focus on the spatial rela-tionships between two multidimensional datasets. We applied our method on several datasets, both synthetic and real. The former are used to build intuition, and the latter to validate our techniques. The synthetic datasets are always normalized to a unit hypercube and they may be translated, rotated and/or scaled in the experiments. The datasets are described in table 1. Our approach relies on a novel method that allows fast summarization of the distribution of distances between points from two sets A and B. Table 2 presents the symbols used in this paper. Consider a grid with cells of side r and let 
CA,, (Cs,i) be the number of points from set A (B) in the i-th cell. The cell grid partitions the minimum bounding box of both datasets. The cross-function CrossfA,B(r,p , q) is defined as follows: DEFINITION 1. Given two data sets A and B in the same Symbol Definition NA (or Ns) No. of points in dataset A (or B) Cross Self A WA Ws 
CA,, (CB,~) Count of type A (B) points in the i-th cell n No. of dimensions (embedding dimensionality) 
D2 Correlation fractal dimension ~,un Est. minimum distance between two points ~ma~ Est. maximum distance between two points n-dimensional space, we define the cross-function of order Typically, we plot the cross-function in log-log scales, after some normalization. The normalization factor scales the plot, maximizing the information presented: DEFINITION 2. Given two data sets A and B (with NA and NB points) in the same n-dimensional space, we define the cross-cloud plot as the plot of CrossA,s(r,p,q) = log(N~ N q) The cross-function has several desirable properties: PROPERTY 1. For p = q = 1, the cross-function is pro-portional to the count of A-B pairs within distance r. That is, Proof. Using Schuster's lemma [23]. This is an important property. For p = q = 1, the cross-cloud plot gives the cumulative distribution function of the pairwise distances between the two "clouds" A and B [10]. 
Because of its importance, we will use p = q = 1 as the de-fault values. We will also omit the subscripts A, B from the cross-cloud plot when it is clear which datasets are involved. That is, PROPERTY 2. The cross-function includes the correlation integral as a special case when we apply it to the same dataset (i.e., A-B). Proof. From the definition of correlation integral [22]. The correlation integral gives the correlation fractal di-mension D2 of a dataset A, if it is self-similar. Since the above property is very important, we shall give the self cross-cloud plots a special name: Line Points along a line segment, randomly chosen. Circumference Points along a circle, randomly chosen. Square Points on a 2D manifold, randomly generated. Cube Points in a 3D manifold, randomly generated. Iris Galaxy 
DEFINITION 3. The self-plot of a given dataset A is the plot of In order to avoid artifacts that self-pairs generate, self-plots do not count self-pairs, by definition. Moreover, minor pairs ((Pl,P2) and (P2,pl)) are counted only once. 
PROPERTY 3. If A is self similar, then the self-plot of A is linear and its slope is its intrinsic dimensionality (corre-lation fractal dimension, D2 ). Proof. See [3]. 
We are now ready to define our two main tools, the tri-plot and the pq-plot. 
DEFINITION 4. The tri-plot of two datasets, A and B, is the graph which contains the cross-plot Cross(r) and the nor-malized self-plots for each dataset (SelfA(r) q-log(NA/Ns) and Self B(r ) + log(YB/NA)). The normalization factors, log(NA/NB) and log(NB/NA), perform only translation, preserving the steepness of the graphs. In this paper, for every tri-plot we present the three graphs with the same color pattern: the cross-plot is pre-sented in blue lines with diamonds, SelfA in green lines with crosses and SelfB in red lines with squares. We also show the slope (or steepness) of the fitted lines. 
DEFINITION 5. The pq-plot of two datasets, A and B, is the graph of the three cross-cloud plots: CrOSSA,B(r), CrOSSA,B(r, 1, k), and CrossA,B(r, k, 1) for large values of Fig. 1 shows the tri-plot and pq-plot for the Line and Sierpinsky datasets. Notice that, although the Cross() is almost always linear (fig. la), this is not necessarily true for the Cross(r, 1, k) and Cross(r, k, 1) (in fig. lb, k = 10). Figure 1: Sierpinsky and Line datasets: (a) the tri-plot, (b) the pq-plot. The cross-plots are presented in blue with diamonds, the self-and weighted-Sierpinsky plots in green with crosses, and the self-and weighted-Line in red with squares. 
DEFINITION 6. The steepness of a plot is its slope, as de-termined by fitting a line with least-squares regression. The tri-plots allow us to determine the relationship between the two datasets. If they are self-similar (ie. both their self-plots are linear for a meaningful range of radii), their slopes can be used in the comparisons that follow. However, the proposed analysis can be applied even to datasets which are not self-similar (ie. do not have linear self-plots). Thus, we will in use the terms steepness and similarity (as defined above). The pq-plot is used in a further analysis step. Its use is more subtle and is discussed in section 4.3. 
This section shows how to "read" the cross-cloud plots and take advantage of the tri-and pq-plots, without any extra calculations on the datasets. 3.1.1 Properties of the self-plots 
PROPERTY 4. The first radius for which the count-of-pairs is not zero in the self-plot provides an accurate estimate, ~,u,~, of the minimum distance between any two points. 
Figure 2: Measurements obtained from self-plots: (a) Line, and (b) Super-cluster datasets. 
Figure 3: Example of a tri-plot indicating where to find meaningful information. The cross-plot is always in blue with diamonds, SelfA in green with crosses and Selfs in red with squares. of-pair increases (being constant for larger radii) provides an accurate estimate, ~maz, of the maximum distance between any two points. We also refer to this distance as the dataset diameter. 
Fig. 2 illustrates the above properties. The lower row of 
The slope, which is D2, is equal to 1, as expected (since this is the intrinsic dimensionality of a line). The ~,m~ and ~mo~ estimates are also indicated. plot has a plateau from radius ~,~in to ~mo~ (see fig. 2). 
Whenever the self-plot is piecewise linear, the dataset has characteristic scales. Plateaus are of particular interest; these occur when the dataset is not homogeneous. From the endpoints of the plateau, we can accurately estimate the maximum cluster diameter, ~cdmo=, and characteristic sepa-ration between clusters, ~s~vc. This occurs in the self-plot of the Super-cluster dataset (fig. 2b). 3.1.2 Properties of the cross-cloud plot is a randomly generated set of 6,000 points from a line (y = xO/x, y E [0, 1]), and dataset B is a Sierpinsky triangle with 6,561 points. These two datasets where chosen to highlight some interesting plot properties. These are discussed in the following (see also fig. 3). can be accurately estimated as the smallest radius which has a non-zero value in the cross-cloud function. the datasets (or, the maximum surrounding diameter) can be accurately estimated as the greatest radius before the plot turns fiat. part for very small radii, there are duplicate points across both datasets. 
All the previous estimates can be obtained with a single pro-cessing pass over both datasets to count grid occupancies, without explicitly computing any distances. always greater than or equal to that of the steepest self-plot. define some terms: mation law (eg. "line," "square," "sierpinsky"). (highly) overlapping minimum bounding boxes. position and orientation. We use these three terms when comparing two datasets. 
Two datasets can have the same shape but different place-ment (eg. two non-collinear lines). Two datasets have the same shape but different placement, if the one can be ob-tained from the other through aflfine transformations. Also, two datasets with the same intrinsic dimensionality can have different shapes (eg. a line and a circle -both have D2 = 1). mary) to analyze and classify the relationship between two datasets. From the tri-plots we can get information about the intrinsic structure and the global relationship between the datasets. 
Rule 1 (identical). If both datasets are identical, then all plots of a tri-plot are similar (SelfA ~-, Sells ~ Cross). In this case, the three graphs will be on top of each other. 
This means that the intrinsic dimensionality, shape as well as placement of both datasets are the same. This may be because one dataset is a subset of the other, or both are sam-ples from a bigger one. Fig. 4 shows the tri-plots for (a) two has steepness comparable to that of SelfA Figure 5 is much steeper than both SelfA and Sells Figure 6 
SelfA or SelfB have the same steepness Figure 7 has steepness comparable to that of SelfA Figure 8 ..... "*" ..... "~'~'~'*" ..... l" ~" ~" .... ..z .~.."~~ ' ....... : ....... : Figure 4: Rule 1 -The two datasets have the same shape and placement: (a) Two superimposed lines (all plots have slope ~ 1, (b) Two superimposed Sierpinsky triangles (all plots have slopes ~ 1.64 log 3/log 2, (c) Two superimposed squares (all plots have slopes ~ 2. All datasets are in 2D space, and the axes of all tri-plots are in log-log scale. lines with different number of objects, (b) two Sierpinsky triangles, and (c) two coplanar squares in 3D. All datasets in fig. 4 are in a 2D manifold. In all these examples, both datasets have the same shape and placement but different number of points. have the same intrinsic dimensionality, but different place-ment, then their steepness is similar (SelfA ..~ Self B), Cross is only moderately steeper than both. bSarther anal-ysis using the pq-plot can indicate whether the datasets are separable or not and, if separable, to what extent. Examples are intersecting lines, intersecting planes, or two Sierpinsky datasets with one rotated over the other (see fig. 5a, 5b and 5c, respectively). Rule 3 (disjoint datasets), if the datasets are disjoint, then Cross is much steeper than both SelfA and Sells not matter whether the latter are similar or not). For two intersecting datasets, the Cross steepness will not be so far from the steepness of their self-plots. However, if the is much steeper than both SelfA and Sells , it means that the minimum distance between points from the datasets is bigger than the average distance of the nearest neighbors of points in both data.sets, so the datasets are disjoint. In Figure 5: Rule 2 -The two datasets have the same intrinsic dimensionality, but different placements: (a) Two intersecting circumferences in 2D space, (b) A line crossing a circumference in 2D space, (c) Two piercing planes in 3D space. The upper row shows the tri-plots with the axes in log-log scale. The lower row shows the corresponding datasets in their respective spaces. fact, this case leads to the conclusion that both datasets are well-defined clusters, hence they should be separable by traditional clustering techniques. Examples of this situation are non-intersecting lines, squares far apart, or a Sierpinsky triangle and a plane which is not coplanar with the Sierpin-sky's supporting plane (see fig. 6a to 6c). All datasets are in 3D space. Notice that the self-plots have the expected slopes, but the cross-plots have very high steepness (18, 13 and 26 respectively). Rule4(sub-manifold). Without loss of generality, let be the steepest of SelfA and Sells. If dataset B is a sub-manifold of dataset A, the self-plots do not have similar steepness (Self A ~ Self B ) and the Cross is equal Remember that the steepness of the Cross cannot be smMler than the steepness of SelfA or SelfB. Therefore, if the steep-188 Cross ~ SdfA), then the other graph (in this case 
In the previous section we described the rules, using syn-
The pq-plot allows us to further examine the relation-
Even if a CrossA,s(r,p,q) plot with p -~ 1 -~ q happens log(NA  X  NB)/log(N~  X  N~), both the leftmost and right;-most points in all pq-plots coincide. According to equa-tion 1, if a particular CA,~ (or CB,~) in the calculation of CrossA,s(r,p,q) is zero for a given radius r in a given re-gion of the space, the corresponding CB,~ (or CA,S) contribute to the total for this particular radius. The result will be a flat region in this part of the curve. Otherwise, if there is a regular distribution of distances over a continuous part of the curve, the resulting curve will exhibit a linear shape. Sudden rises in a plot indicate a large growth of counts starting at that radius. Hence, the two shapes in the curves of the cross-cloud plots that are worth looking for are: the linear parts, and the regions where the curves are flat. 
The cross-cloud plots, CrOSSA,B(r, k, 1), and CrOSSA,B(r, with k &gt;&gt; 1 (which we have named WA and WB because they are 'weighted'), can be generated for any value of k. However, increasing k only increases the distortions on the plot, without giving any extra information. Thus, we picked k = 10. Each conclusion is valid for the range of radii which presents specific behavior. Next, we discuss two represen-tative situations, using pairs of synthetic datasets and com-paring the obtained tri-plots and pq-plots. 
Fig. 10 compares two pairs of datasets: circumference-circumference and line-circumference. This illustrates the situation stated by Rule 2: the two datasets are similar (SeYA ..~ SelfB and Cross steepness is less or equal than the steepness of SelfA plus the steepness of SelfB ). say anything else about the datasets. However, in fig. 10b the three graphs are on top of each other. This means that both datasets have the same behavior under weighted calculation (Cross(r, 1, 10) and Cross(r, 10, 1)). Thus, both datasets have the same shape. On the other hand, the be-havior of the pq-plot in fig. 10e shows that the datasets have different shapes, as well as how they are correlated within specific radii ranges (Region I and II on the plots). 
In this section we proposed the rules to analyze the tri-plots and the pq-plots using easily understandable synthetic datasets in 2D and 3D spaces. However, the same conclu-sions should apply for real datasets in any multi-dimensional how to describe the relationship between the attributes and to know if they are correlated. Nonetheless, our proposed analysis can indicate not only the existence of correlations~ but also how "tight" they are. This analysis can also pro-vide evidence of how separable the datasets are, as well as if it is possible to classify points as belonging to one or to the other dataset. 
Due to space limitations, we present pq-plots only for some 
Figure 8: Rule 5 -The datasets come from different placements: (a) a line and a Sierpinsky triangle in 2D space, (b) a line piercing a square in 3D space, (c) a plane and an intersecting Sierpinsky triangle in 3D space. The upper row shows the tri-plots in log-log scale. The lower row shows the corresponding datasets in their respective space. " " " (~l) " ";-' ' ..... ( X )" " ~'' (0 
Figure 10: pq-plots for two pairs of datasets: (a) the tri-plot of two intersecting circumferences (as shown in (c)), (b) the pq-plot of the two circumferences, (d) the tri-plot of a line intersecting a circumference (as shown in (f)), and (e) the pq-plot of the line and the circumference. of the real datasets(fig. 11). Fig. lla shows the pq-plot for the Galaxy datasets. For the highlighted range, there is a distinct separation between the datasets. Besides confirm-ing that the two galaxy types indeed repel each other, the pq-plots show that there are few clusters consisting only of 'exp' galaxies (although there are clusters including points of both datasets also only with 'dev' points). Outside the high-lighted range, the sets are almost identical. As expected, fig. llb confirms that the Democrat and Republican datasets are separable, since the weighted plots have completely op-posite behaviors. Fig. 11c shows the pq-plot of the California-water and 
California-political datasets. In this plot, there are four ranges with distinct behaviors. Range I corresponds to very small distances, so these distances are probably less than the resolution of the measurements; therefore they are not meaningful. Ranges II and III are where the real distances are meaningful. The sudden fall to the left of the wWater-plot in range II means that there are very few points in the political dataset at distances below this range from points in the water dataset. This indicates a kind of "repulsion" of points from both datasets for these small distances. In range III, both datasets have approximately the same be-havior. Range IV is almost flat for all plots, meaning that there are almost no more pairs within this distance range. 
In fact, the "almost flat" part of the graph is due to a few outliers in the dataset. 
So far we have shown how to use the tri-plots to answer questions Q1-Q5. In this section we illustrate the power of cross-cloud plots in another setting: membership testing and classification (Q5). Fig. 12 illustrates the following situation: 
We have two datasets, A (20 points along a line) and B (900 points in a 'tight' square). A new point (indicated by '?') arrives. Which set, if any, does it belong to? 
Visually, the new point ('?') should belong to the Line20 set. However, nearest neighbors or decision-tree classifiers would put it into the square: the new point has ,,~ 900 Figure 11: pq-plots for real datasets: (a) Galaxy, (b) Democrat and Republican, (c) California-water and 
California-political. The upper row shows the tri-plots and the lower row the corresponding pq-plots. 
The axes are in log-log scales. 'Square' neighbors, before even the first 'Line20' neighbor comes along! 
We propose a method that exploits cross-cloud plots to correctly classify the new point ('?'). The new point is treated as a singleton dataset and its cross-plots are com-paved to the self-plots of each candidate set. In this partic-ular case, we compare the steepness of CrossLine,Point and classify the new point accordingly. Notice that the plots in fig. 12b are more similar to each other (almost equal steep-we conclude that the new point ('?') belongs to the Line20 dataset, despite what k-neavset neighbor classification would say! 
The full details of the classification method are the topic of ongoing research. This is yet another application of the cross-cloud technique. 
Figure 12: Classifying a point as either belonging to a sparse line or to a dense square, using the cross-cloud method: (a) spatial placement of the incoming 
Figure 13: Left -Wall-clock time (in seconds) needed to generate the tri-plots for varyingly sized datasets. The blue graph represents the time for 2D datasets, the green graph for 8D datasets and the red graph for 16D datasets. Right -Wall-clock time (in seconds) needed to generate the Tri-plots versus the dimensionality of the datasets, for three differ-ent dataset sizes (100,000, 200,000 and 300,000). algorithm presented in appendix A. This is based on box-counting and is an extension of [3, 10]. arbitrarily large datasets, and arbitrarily high dimensions. 
This is rarely true for other spatial data mining methods in the literature. The algorithm to generate the pq-plots is very similar to the algorithm in appendix A, except we construct 
WA and Ws (instead of SelfA and SelfB ) plots. 
O(NA +Ns). If we want I points in each cross-cloud plot (ie. number of grid sizes), then the complexity of our algorithm is O((NB + NA)  X  1  X  n), where n is the embedding dimen-sionality. Fig. 13 shows the wall-clock time required to pro-cess datasets on a Pentium II machine running NT4.0. The datasets on the left graph have varying numbers of points in 2, 8 and 16-dimensional spaces, and we used 20 grids for each dataset. For the right graph, we used datasets with 100,000, 200,000 and 300,000 points and dimensions 2 to 40. 
The execution time is indeed linear on the total number of points, as well as on the dimensionality of the datasets. The algorithm does not suffer from the dimensionality curse. and maintain counts of each non-empty grid ceil. These counts can be kept in any data structure (hash tables, quad-trees, etc). spatial data mining across two n-dimensional datasets. We have shown that our tool has all the necessary properties: 
The experiments on real datasets show that our tool finds patterns that no other known method can. We believe that our cross-cloud plot is a powerful tool for spatial data mining and that we have just seen only the beginning of its potential uses. [1] R. Agrawal, J. Gherke, D. Gunopoulos, and [2] D. Barbar X  and P. Chen. Using the fractal dimension [3] A. Belussi and C. Faloutsos. Estimating the selectivity [4] S. Berchtold, C. B6hm, D. A. Keim, and H.-P. [5] S. Berchtold, C. BShm, and H.-P. Kriegel. The [6] S. Chaudhuri. Data mining and database systems: [7] M.-S. Chen, J. Han, and P. S. Yu. Data mining -an [8] M. Ester, A. Frommelt, H.-P. Kriegel, and J. Sander. [9] M. Ester, H.-P. Kriegel, and J. Sander. Spatial data [10] C. Faloutsos, B. Seeger, C. Traina Jr., and A. Traina. [11] U. M. Fayyad. Mining databases -towards algorithms [12] U. M. Fayyad, C. Reina, and P. S. Bradley. [13] V. Ganti, J. Gehrke, and R. Ramakrishnan. Mining [14] J. Han, L. V. S. Lakshmanan, and R. T. Ng. [15] C. Traina Jr., A. Traina, L. Wu, and C. Faloutsos. [16] R. J. Bayardo Jr., R. Agrawal, and D. Gunopulos. [17] D. A. Keim and H.-P. Kriegel. Possibilities and limits [18] E. M. Knorr and R. T. Ng. Finding aggregate [19] R. T. Ng and J. Han. Efficient and effective clustering [20] Bureau of Census. Tiger/line preeensus files: 1990 [21] B.-U. Pagel, F. Korn, and C. Faloutsos. Deflating the [22] M. Schroeder. Fractals, Chaos, Power Laws. W.H. [23] H. G. Schuster. Deterministic Chaos. VCH Publisher, [24] G. Sheikholeslami, S. Chatterjee, and A. Zhang. [25] M. L. Tian Zhang, R. Ramakrishnan. Birch: An [26] R. Weber, H.-J. Schek, and S. Blott. A quantitative Given two datasets A and B (with cardinalities NA and NB) in a n-dimensional space, we generate the tri-plot (ie. CrOSSA,B, Self A and Self B plots): 1 -For each point p of datasets A and B: 2 -Compute the sum of product occupancies: 3 -Print the tri-plot: The number F of non-empty cells in each grid does not 
