 Hema S. Koppula hema@cs.cornell.edu Ashutosh Saxena asaxena@cs.cornell.edu Computer Science Department, Cornell University, Ithaca, NY 14853 USA Being able to detect which activity is being performed as well as being able to anticipate what is going to happen next in an environment is important for ap-plication domains such as robotics and surveillance. In a typical environment, we have humans interact-ing with the objects and performing a sequence of ac-tivities. Recently, inexpensive RGB-D cameras (such as Microsoft Kinect, see Figure 1) have enabled re-searchers to model such rich spatio-temporal interac-tions in the 3D scene for learning complex human ac-tivities. For example, Koppula, Gupta and Saxena (2013) (KGS) used a conditional random field (CRF), trained with max-margin methods, to model the rich spatio-temporal relations between the objects and hu-mans in the scene.
 However, in previous works, emphasis has been on modeling the relations between components in the scene (human pose, objects, etc.), and performing learning and inference given the spatio-temporal struc-ture of the model (i.e., for a given CRF structure in the case of KGS). However, it is quite challenging to estimate this structure because of two reasons. First, an activity comprises several sub-activities, of vary-ing temporal length, performed in a sequence. This results in an ambiguity in the temporal segmentation and thus a single graph structure may not explain the activity well. Second, there can be several possible graph structures when we are reasoning about activi-ties in the future (i.e., when the goal is to anticipate future activities, different from just detecting the past activities). Multiple spatio-temporal graphs are possi-ble in these cases and we need to reason over them in our learning algorithm.
 In our work, we start by using a CRF to model the sub-activities and affordances of the objects, how they change over time, and how they relate to each other. In detail, we have two kinds of nodes: object and sub-activity nodes. The edges in the graph model the pair-wise relations among interacting nodes, namely the object X  X bject interactions, object X  X ub-activity inter-actions, and the temporal interactions (see Figure 1). This model is built with each spatio-temporal segment being a node. Figure 2 shows two possible graph struc-tures for an activity with two objects. We then reason about the possible graph structures for both past and future activities. The key idea is to first sample a few segmentations that are close to the ground-truth seg-mentation using our CRF model instantiated with a subset of features, and then explore the space of seg-mentation by making merge and split moves to create new segmentations. We do so by approximating the graph with only additive features, which lends to effi-cient dynamic programming.
 In extensive experiments over 120 activity videos col-lected from four subjects, we showed that our ap-proach outperforms the state-of-the-art results both in the tasks of activity and affordance detection. We achieved an accuracy of 85.4% for affordance, 70.3% for sub-activity labeling and 83.1% for high-level ac-tivities respectively for detection. Furthermore, we obtain an accuracy of 67.2% and 49.6% for anticipat-ing affordances and sub-activities respectively in fu-ture time-frames. There has been a considerable amount of previous work on detection of human activities from still im-ages as well as videos (e.g., Maji et al., 2011; Yang et al., 2010; Xing et al., 2008; Ryoo, 2011; Hoai &amp; De la Torre, 2012a). Similar to our setting some recent works have shown that modeling the mutual context between human poses and objects (either the category label or affordance label, Jiang et al. 2012a) is use-ful for activity detection (Gupta et al., 2009; Yao &amp; Fei-Fei, 2010; Delaitre et al., 2011; Prest et al., 2012; Koppula et al., 2013).
 Recent availability of inexpensive RGB-D sensors has enabled significant improvement in scene modeling (Koppula et al., 2011; Anand et al., 2012; Jiang et al., 2012b; 2013; Jia et al., 2013; Jiang &amp; Saxena, 2013) and estimation of human poses (Shotton et al., 2012; Ly et al., 2012). This, together with depth infor-mation, has enabled some recent works (Sung et al., 2011; Zhang &amp; Parker, 2011; Ni et al., 2011; Sung et al., 2012) to obtain good action recognition perfor-mance. However, these methods only address detec-tion over small periods of time, where temporal seg-mentation (and thus knowledge of the spatio-temporal graph structure) is not a big problem. KGS (Kop-pula et al., 2013) proposed a model to jointly predict sub-activities and object affordances by taking into ac-count both spatio-temporal interactions between hu-man poses and objects over longer time periods. How-ever, KGS found that not knowing the graph structure (i.e., the correct temporal segmentation) decreased the performance significantly. This is because the bound-ary between two sub-activities is often not very clear, as people often start performing the next sub-activity before finishing the current sub-activity. We compare our proposed method with theirs and show consider-able improvement over their state-of-the-art results. In activity detection from 2D videos, much previous work has focussed on short video clips, assuming that temporal segmentation has been done apriori. Some recent effort in recognizing actions from longer video sequences take an event detection approach (Ke et al., 2007; Simon et al., 2010; Nguyen et al., 2009), where they evaluate a classifier function at many different segments of the video and then predict the event pres-ence in segments. Similarly, change point detection methods (Xuan &amp; Murphy, 2007; Harchaoui et al., 2008) work by performing a sequence of change-point analysis in a sliding window along the time dimension. However, these methods only detect local boundaries and tend to over-segment complex actions which often contain many changes in local motion statistics. Some previous works consider joint segmentation and recognition by defining dynamical models based on kinematics (Oh et al., 2008; Fox et al., 2009), but these works do not model the complex human-object inter-actions. Hoai et al. (2011) and Hoai &amp; De la Torre (2012b) do not consider temporal context and only perform activity classification and clustering respec-tively. In related work, Ion et al. (2011) consider the problem 2D image segmentation. They sample seg-mentations of images for labeling using an Incremen-tal Saddle Point estimation procedure which require good initial samples. In contrast, our application re-quires modeling of the temporal context (as compared to just spatial). This work is closer to KGS, where they also sample the segmentation space. However, in our approach we use a discriminative approach, where we model the energy function as composed of an additive and a non-additive term. This allows us to efficiently sample the potential graph structures.
 One important application of our approach is in antic-ipating future activities, where reasoning over future possible graph structures becomes important. Antic-ipating future activities has gained attention only re-cently (Kitani et al., 2012; Koppula &amp; Saxena, 2013). Kitani et al. (2012) proposed a Markov decision pro-cess to obtain a distribution over possible human navi-gation trajectories in 2D from visual data. Koppula &amp; Saxena (2013) addressed the problem of anticipating human activities at a fine-grained level of how humans interact with objects in more complex activities such as microwaving food or taking medicine . They repre-sent the distribution of the possible futures with a set of particles that are obtained by augmenting the CRF structure of KGS with sampled future nodes. However, they do not reason about the possible graph structures for the past. In our work, we show that sampling the spatio-temporal structure, in addition to sampling the future nodes, results in better anticipation.
 In terms of learning algorithms, probabilistic graphical models are a workhorse of machine learning and have been applied to a variety of applications. Frameworks such as HMMs (Hongeng &amp; Nevatia, 2003; Natarajan &amp; Nevatia, 2007), DBNs (Gong &amp; Xiang, 2003), CRFs (Quattoni et al., 2007; Sminchisescu et al., 2005; Kop-pula et al., 2013), and semi-CRFs (Sarawagi &amp; Cohen, 2004) have been previously used to model the tempo-ral structure of videos and text. While most previous works maintain their template graph structure over time, in our work, new graph structures are possible. Works on semi-Markov models (Sarawagi &amp; Cohen, 2004; Shi et al., 2011) are quite related to our work as they address the problem of finding the segmenta-tion along with labeling. However, these methods are limited since they are only efficient for feature maps that are additive in nature. We build upon these ideas where we use the additive feature map as only a close approximation to the graph structure and then explore the space of likely graph structure by designing moves. We show that this improves performance while being computationally efficient. In our setting, our algorithm observes a scene contain-ing a human and objects for time t in the past, and our goal is to detect activities in the observed past and also anticipate the future activities for time d . Following KGS, we discretize time to the frames of the video 1 and group the frames into temporal segments, where each temporal segment spans a set of contiguous frames cor-responding to a single sub-activity. Therefore, at time  X  t  X  we have observed  X  t  X  frames of the activity that are grouped into  X  k  X  temporal segments. (Figure 2 shows two temporal segments for the past.) We model the spatio-temporal structure of an activity using a conditional random field, illustrated in Fig-ure 1. For the past t frames, we know the nodes of the CRF but we do not know the temporal segmen-tation, i.e., which frame level nodes are connected to each of the segment level node. The node labels are also unknown. For the future d frames, we do not even know the nodes in the graph X  X here maybe different number of objects being interacted with depending on which sub-activity is performed in the future. Our goal is to explore different possible past and future graph structures (i.e., sub-activities, human poses, object lo-cations and affordances). We will do so by augment-ing the graph in time with potential object nodes, and sampling several possible graph structures.
 We first describe the CRF modeling given a fixed graph structure for t observed frames. Let, s  X  { 1 ,..,N } denote a temporal segment and y = ( y 1 ,..., y N ) denote the labeling we are interested in finding, where y s is the set of sub-activity and ob-ject affordance labels for the temporal segment s . Our input is a set of features  X ( x ) extracted from the seg-mented 3D video. The prediction  X  y is computed as the argmax of an energy function E ( y |  X ( x ); w ) that is parameterized by weights w . This energy is expressed over a graph, G = ( V , E ) (as il-lustrated in Figure 1), and consists of two terms X  X ode terms and edge terms. Each of these terms comprises the label, appropriate features, and weights. Let y k i be a binary variable representing the node i having label k , where K is the set of labels. Let  X  n ( i ) and  X  e ( i,j ) be the node and edge feature maps respectively. (De-pending on the node and edge being used, the appro-priate subset of features and class labels are used.) We thus write the energy function as: Note that the graph has two types of nodes: sub-activity and object nodes, and it has four types of edges: object X  X bject edges, object X  X ub-activity edges, object X  X bject temporal edges, and sub-activity X  X ub-activity temporal edges. Our energy function is a sum of six types of potentials that define the energy of a particular assignment of sub-activity and object affor-dance labels to the sequence of segments in the given video: E ( y |  X ( x ); w ) = E o + E a + E oo + E oa + E t However, we have written it compactly in Eq. (2). Learning and Inference . As the structure of the graph is fully known, we learn the parameters of the energy function in Eq. (2) by using the cutting plane method (Joachims et al., 2009).
 However, during inference we only know the nodes in the graph but not the temporal segmentation, i.e., the structure of the graph in terms of the edges connecting frame level nodes to the segment level label nodes. We could search for the best labeling over all possible segmentations, but this is very intractable because our feature maps contain non-additive features (that are important and are described in the next sub-section). 3.1. Features: Additive and Non-Additive We categorize the features into two sets: additive fea-tures,  X  A ( x ), and non-additive features,  X  NA ( x ). We compute the additive features for a set of frames cor-responding to a temporal segment by adding the fea-ture values for the frames belonging to the temporal segment. Examples of the additive features include distance moved and vertical displacement of an object within a temporal segment. The features that do not satisfy this property are referred to as the non-additive features, for example, maximum and minimum dis-tances between two objects. As we discuss in the next section, additive features allow efficient joint segmen-tation and labeling by using dynamic programming, but may not be expressive enough.
 Non-additive features sometimes provide very useful cues for discriminating the sub-activity and affordance classes. For example, consider discriminating cleaning sub-activity from a moving sub-activity: here the total distance moved could be similar (an additive feature), however, the minimum and maximum distance moved being small may be strong indicator of the activity be-ing cleaning . In fact, when compared to our model learned using only the additive features, the model learned with both additive and non-additive features improves macro precision and recall by 5% and 10.1% for labeling object affordance respectively and by 3.7% and 6.2% for labeling sub-activities respectively. In detail, we use the same features as described by KGS. These features include the node feature maps  X  ( i ) and  X  a ( j ) for object node i and sub-activity node j respectively, and edge feature maps  X  t ( i,j ) capturing the relations between various nodes. The object node feature map,  X  o ( i ), includes the ( x,y,z ) coordinates of the object X  X  centroid, the coordinates of the object X  X  bounding box and transformation matrix w.r.t. to the previous frame computed at the middle frame of the temporal segment, the total displacement and the to-tal distance moved by the object X  X  centroid in the set of frames belonging to the temporal segment. The sub-activity node feature map,  X  a ( j ), gives a vector of features computed using the noisy human skeleton poses obtained from running Openni X  X  skeleton tracker on the RGBD video. We compute the above described location (relative to the subject X  X  head location) and distance features for each the upper-skeleton joints ex-cluding the elbow joints (neck, torso, left shoulder, left palm, right shoulder and right palm).
 The edge feature maps,  X  t ( i,j ), include relative geo-metric features such as the difference in ( x,y,z ) coor-dinates of the object centroids and skeleton joint lo-cations and the distance between them. In addition to computing these values at the first, middle and last frames of the temporal segment, we also consider the min and max of their values across all frames in the temporal segment to capture the relative motion in-formation. The temporal relational features capture the change across temporal segments and we use the vertical change in position and the distance between corresponding object and joint locations. We perform cumulative binning of all the feature values into 10 bins for each feature.
 Efficient Inference with Additive Features . We express the feature set,  X ( x ), as the concatenation of the additive and non-additive feature sets,  X  A ( x ) and  X 
NA ( x ) respectively. Therefore, by rearranging the terms in Eq. (2), the energy function can written as:
E ( y |  X ( x ); w ) = E ( y |  X  A ( x ); w ) + E ( y |  X  NA We perform efficient inference for the energy term E ( y |  X  A ( x ); w ) by formulating it as a dynamic pro-gram (see Eq. (3)). In detail, let L denote the max length of a temporal segment, i denote the frame in-dex, s denote the temporal segment spanning frames ( i  X  l ) to i , and ( s  X  1) denote the previous segment. We write the energy function in a recursive form as: Here,  X  A n ( s ) and  X  A e ( s  X  1 ,s ) denote the additive fea-ture maps and can be efficiently computed by using the concept of integral images. 2 The best segmentation then corresponds to the path traced by max a V ( t,a ), where t is the number of video frames.
 Using E ( y |  X  A ( x ); w ), we find the top-k scored seg-mentations and then evaluate them using the full model E ( y |  X ( x ); w ) in order to obtain more accurate labelings.
 Merge and Split Moves. The segmentations generated by the approximate energy function, E ( y |  X  A ( x ); w ), are often very close to the given ground-truth segmentations. However, since the en-ergy function used is only approximate, it sometimes tends to over-segment or miss the boundary by a few frames. In order to obtain a representative set of seg-mentation samples, we also perform random merge and split moves over these segmentations, and con-sider them for evaluating with the full model as well. A merge move randomly selects a boundary and re-moves it, and a split move randomly chooses a frame in a segment and creates a boundary. Heuristic Segmentations . There is a lot of infor-mation present in the video which can be utilized for the purpose of temporal segmentation. For example, smooth movement of the skeleton joints usually rep-resent a single sub-activity and the sudden changes in the direction or speed of motion indicate sub-activity boundaries. Therefore, we incorporate such informa-tion in performing temporal segmentation of the activ-ities. In detail, we use the multiple segmentation hy-potheses proposed by KGS. These include graph based segmentation method proposed by (Felzenszwalb &amp; Huttenlocher, 2004) adapted to temporally segment the videos. The sum of the Euclidean distances be-tween the skeleton joints and the rate of change of the Euclidean distance are used as the edge weights for two heuristic segmentations respectively. By vary-ing the thresholds, different temporal segmentations of the given activity can be obtained. In addition to the graph based segmentation methods, we also use the uniform segmentation method which considers a set of continuous frames of fixed size as the temporal segment. There are two parameters for this method: the segment size and the offset (the size of the first seg-ment). However, these methods often over-segment a sub-activity, and each segmentation would result in a different graph structure for our CRF modeling. We generate multiple graph structures for various val-ues of the parameters for the above mentioned meth-ods and obtain the predicted labels for each using Eq. (1). We obtain the final labeling over the segments by either using the second-step learning method pre-sented in KGS, or by performing voting and taking the label predicted by majority of the sampled graph structures (our experiments follow the latter). 4.1. Anticipating and Sampling Future Nodes For anticipating the next d time, we augment the CRF structure with d frame nodes as proposed in (Kop-pula &amp; Saxena, 2013). Sampling this graph comprises three steps. First, we need to sample the possible sub-activity and the object nodes involved. Second, for this segment-level graph, we sample the frame-level nodes, starting with possible end-points of the physi-cal location of the objects and human hands. Third, given the end locations, we sample a variety of tra-jectories. This sampling procedure gives us various possible future nodes and their spatial relations. We add these to the observed nodes and sample various possible temporal segmentations as described above. In the first step, we sample the object affordances and activities based on a discrete distribution generated from the training data. This distribution is based on the object type (e.g., cup, bowl, etc.) and object X  X  cur-rent position with respect to the human in the scene (e.g., in contact with the human hand, etc.). For ex-ample, if a human is holding an object of type  X  X up X  placed on the table, then the affordances drinkable and movable with their corresponding sub-activities ( drinking and moving respectively) have equal proba-bility, with all others being 0.
 In the second step, given the sampled affordances and sub-activity, we sample the corresponding object loca-tions and human poses for the d anticipated frames. In order to have meaningful object locations and hu-man poses we take the following approach. We model object affordances using a potential function based on how the object is being interacted with, when the cor-responding affordance is active. The affordance poten-tial has the form  X  o = Q i  X  dist i Q j  X  ori j , where  X  is the i th distance potential and  X  ori j is the j th relative angular potential. We model each distance potential with a Gaussian distribution and each relative angu-lar potential with a von Mises distribution and find the parameters from the training data. We can now score the points in the 3D space using the potential function, whose value represents the strength of the particular affordance at that location. Therefore, for the chosen affordance, we sample the object X  X  most likely future location using the affordance potentials.
 In the third step, for every sampled target object location, we generate a set possible trajectories fol-lowing which the object can be moved form its cur-rent location to the sampled target location. We use parametrized cubic equations, in particular B  X ezier curves, to generate human hand like motions (Faraway et al., 2007). We estimate the control points of the B  X ezier curves from the training data.
 Sampling Augmented Graphs. In order to sample the future graphs, we augment our observed frames with these sampled future frames, and then sample the augmented graph structures (for different tempo-ral segmentations) as described in Section 4. We then use the energy function in Eq. (2) to obtain the best labeling for each sampled graph. The node potentials score how well the features extracted from the antici-pated object locations and human poses match the af-fordance and sub-activity labels respectively, and the edge potentials score how likely are the anticipated sub-activities and affordances likely to follow the ob-served ones. Therefore, the value of the energy func-tion provides a ranking over the sampled augmented graphs for the future time. For obtaining anticipa-tion metrics, we report the highest ranked one (and compare it with what was actually performed in the future). Data. We test our model on the Cornell Activity Dataset-120 (CAD-120) (Koppula et al., 2013). It con-tains 120 3D videos of four different subjects perform-ing 10 high-level activities, where each high-level activ-ity was performed three times with different objects. It contains a total of 61,585 total 3D video frames. The activities have a long sequence of sub-activities, which vary from subject to subject significantly in terms of length of the sub-activities, order of the sub-activities as well as in the way they executed the task. The dataset is labeled with both sub-activity and object af-fordance labels. The high-level activities are: { making cereal , taking medicine , stacking objects , unstacking objects , microwaving food , picking objects , cleaning ob-jects , taking food , arranging objects , having a meal } . The sub-activity labels are: { reaching , moving , pour-ing , eating , drinking , opening , placing , closing , scrub-bing , null } and affordance labels are: { reachable , mov-able , pourable , pourto , containable , drinkable , openable , placeable , closable , scrubbable , scrubber , stationary } . Evaluation: For comparison, we follow the same train-test split described in KGS and train our model on activities performed by three subjects and test on activities of a new subject . We report the results ob-tained by 4-fold cross validation by averaging across the folds. We consider the overall micro accuracy (P/R), macro precision and macro recall of the de-tected sub-activities, affordances and overall activity. Micro accuracy is the percentage of correctly classified labels. Macro precision and recall are the averages of precision and recall respectively for all classes. We also computed these metrics for the anticipated sub-activity and affordances.
 Detection Results. Table 1 shows the performance of our proposed approach on object affordance, sub-activity and high-level activity labeling for past activi-ties. Rows 3-5 show the performance for the case where ground-truth temporal segmentation is provided and rows 6-9 show the performance for the different meth-ods when no temporal segmentation is provided. With known graph structure, the model using the the full set of features (row 4) outperforms the model which uses only the additive features (row 5): macro precision and recall improve by 5% and 10.1% for labeling ob-ject affordance respectively and by 3.7% and 6.2% for labeling sub-activities respectively. This shows that additive features bring us close, but not quite, to the optimal graph structure.
 When the graph structure is not known, the perfor-mance drops significantly. Our graph sampling ap-proach based on the additive energy function (row 6) achieves 83.6% and 71.5% micro precision for label-ing object affordance and sub-activities, respectively. This is improved by sampling additional graph struc-tures based on the Split and Merge moves (row 7). Finally, when combining these segmentations with the other heuristically generated segmentations presented by KGS, our method obtains the best performance (row 9) and significantly improves the previous state-of-the-art (KGS, row 8).
 Figure 3 shows the confusion matrix for labeling af-fordances, sub-activities and high-level activities us-ing our method (row 9). We can see that there is a strong diagonal with a few errors such as pouring mis-classified as moving , and picking objects misclassified as having a meal . Figure 4 shows the labeling out-put of the different methods. The bottom-most row show the ground-truth segmentation, top-most row is the labeling obtained when the graph structure is pro-vided, followed by three heuristically generated seg-mentations. The fifth row is the segmentation gen-erated by our sampling approach and the sixth and seventh rows are the labeling obtained by combining the multiple segmentations using a simple max-voting and by the multi-segmentation learning of KGS. Note that some sub-activity boundaries are more ambigu-ous (high variance among different methods) than the others. Our method has an end-to-end (including fea-ture computation cost) frame rate of 4.3 frames/sec compared to 16.0 frames/sec of KGS.
 Anticipation Results. Table 2 shows the frame-level metrics for anticipating the sub-activity and ob-ject affordance labels for 3 seconds in the future on the CAD-120 dataset. We compare our anticipation method against the following baselines: 1. Chance. Labels are chosen at random. 2. Nearest Neighbor. It first finds an example from chance 8.3 (0.1) 8.3 (0.1) 8.3 (0.1) 10.0 (0.1) 10.0 (0.1) 10.0 (0.1) 3. Co-occurrence Method. The transition probabil-4. Our Method -segment . Our method that only 5. Koppula &amp; Saxena (2013) . Our method that sam-Our method outperforms all the baseline algorithms. Sampling future frame level nodes in addition to just sampling the segment level nodes (row 4) increases the accuracy of affordance and sub-activity anticipation by 6.6% and 13.4% respectively. Our method of sampling the whole graph structure (row 6) achieves the best performance with an increase in macro precision and recall over the best baseline results (row 5)  X  1.9% and 4.3% for anticipating object affordances and 2.5% and 2.3% for anticipating sub-activities, respectively. This shows that sampling the graph structures enables us to reason about the spatial and temporal interactions that can happen in the future, which is essential to obtain good anticipation performance.
 Figure 5 shows how the macro F1 scores changes with the anticipated time. The average duration of a sub-activity in the CAD-120 dataset is around 3.6 seconds, therefore, an anticipation duration of 10 seconds is over two to three sub-activities. With the increase in anticipation time, the performance of the others ap-proach that of a random chance baseline. Our method outperforms the baselines for all anticipation times and its performance declines gracefully with increase in the anticipation time. The code and videos are available at: http://pr.cs.cornell.edu/anticipation/ . In this paper, we considered the task of detecting the past human activities as well as anticipating the fu-ture human activities using object affordances. In the task of detection, most previous works assume ground-truth temporal segmentation is known or simply use some heuristic methods. Since modeling human activ-ities requires rich modeling of the spatio-temporal re-lations, fixing the temporal segmentation limited the expressive power of CRF-based model in the previ-ous works. In this work, we proposed a method to first obtain potential graph structures that are close to the ground-truth ones by approximating the graph with only additive features. We then designed moves to explore the space of likely graph structures. Our approach also enabled us to anticipate the future ac-tivities where considering multiple possible graphs is critical. In the recently growing field of RGB-D vi-sion, our work thus shows a considerable advance by improving the state-of-the-art results on both the de-tection and anticipation tasks.
 We thank Jerry Yeh and Yun Jiang for useful discus-sions. This research was funded in part by ARO award W911NF-12-1-0267, and by Microsoft Faculty Fellow-ship and NSF Career Award to one of us (Saxena).
