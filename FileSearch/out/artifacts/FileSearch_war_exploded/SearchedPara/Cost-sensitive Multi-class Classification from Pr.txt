 Deirdre B. O X  X rien deirdre@google.com Google Inc., Mountain View, CA Maya R. Gupta gupta@ee.washington.edu University Of Washington, Seattle, WA Robert M. Gray rmgray@stanford.edu Stanford University, Stanford, CA Many classifiers first estimate class probabilities  X  p k for each class k  X  X  1 , . . . , K } , then classify a test sam-ple x as the class  X  y ( x ) that minimizes the expected misclassification costs: cost matrix c , and the cost of classifying as class i when the true class is j . We define the function g to use as short-hand for this minimization.
 Such probability-based classifiers can be interpreted as mapping each test sample to a point on the  X  p -simplex, where each corner of the simplex has  X  p j = 1 for some j , and  X  p i = 0 for all i 6 = j ; and that the cost matrix c induces a partitioning of the  X  p -simplex into regions assigned to each of the K classes. However, the proba-bility estimation can suffer from systematic errors, e.g. oversmoothing the estimate towards class prior prob-abilities. The main contribution of this paper is an analytic and experimental investigation of how chang-ing the partitioning of the  X  p -simplex can reduce the effect of such systematic errors on classification loss, analogous to ROC analysis for two-class classification. First, we discuss systematic probability estimation er-rors and show how these errors can cause classification errors. Then in Section 3 we review methods to reduce the effect of such errors. In Section 4 we establish prop-erties that describe how changing c affects the class-partitioning of the  X  p -simplex. In Section 5, we propose learning a partitioning of the  X  p -simplex that seeks to minimize the empirical misclassification costs for the given c , and we provide experimental evidence of the effectiveness of our approach in Section 6. Friedman uses the term oversmoothing for cases where the probability estimates are systematically smoothed towards the class prior probabilities, and undersmooth-ing for cases where the class probability estimates produced are too confident, such as 1-NN (Friedman, 1997). Other systematic errors in the probability es-timates can occur; Niculescu-Mizil and Caruana have documented the systematic errors introduced by var-ious methods of probability estimation for two-class classification (Niculescu-Mizil &amp; Caruana, 2005). Here we provide illustrative examples of over-and under-smoothing in multiclass tasks.
 2.1. A Naive Bayes X  Example Consider a three-class problem with discrete features. We estimate class probabilities for test samples using naive Bayes (Hastie et al., 2001). The three classes are equally likely, and the feature vector consists of two identical copies of the same feature. Thus the naive Bayes X  assumption of independent features is clearly violated. Figure 1(a) shows pairings of a true class probability (marked with an attached circle) and the associated naive Bayes X  estimated probability (marked with a triangle). The incorrect feature-independence assumption undersmooths the probability estimates, pushing them towards the edges of the simplex. When an estimated probability and the corresponding true probability fall in the same class partition, the un-dersmoothing does not cause any classification error. When the line attaching a triangle to a circle crosses a class partition line, a classification error occurs. One sees that undersmoothing does not cause errors given the 0/1 cost matrix (dashed lines), but causes many errors given an asymmetric cost matrix (solid lines). 2.2. A k -NN Example Let N be the number of training samples. Then for the k -NN classifier as the number of nearest neigh-bors k  X  N , the probability estimates are smoothed towards the class prior probabilities. Figure 1(b) illus-trates an extreme example: k = 2000, N = 3000, and the samples are drawn iid and with equal probability from one of three class-conditional normal distribu-tions. The oversmoothing does not cause errors given the 0/1 cost matrix (dashed lines), but causes many errors given an asymmetric cost matrix (solid lines). Approaches to deal with the systematic errors in prob-ability estimation can be analyzed in terms of the clas-sification rule given in (1). Such approaches gener-ally either change the partitioning of the  X  p -simplex, or change the probability estimates. 3.1. Related Work in Two-class Classification For the two-class case, the  X  p -simplex is a line segment from  X  p 1 ( x ) = 0 to  X  p 1 ( x ) = 1, and a scalar threshold t partitions the two class regions. The optimal threshold Classification errors can be reduced by changing the class-partitioning by specifying a threshold t that re-duces the effect of systematic errors of the class prob-ability estimates. The most common approach uses the receiver operating characteristic (ROC) curves (Egan, 1975; Hanley &amp; McNeil, 1982). An ROC curve plots estimates of the probabilities P  X  Y | Y (2 | 2) versus P
Y | Y (2 | 1) for thresholds t, 0  X  t  X  1, where the es-timates are derived from training or validation data. For a given cost matrix the desired point on the ROC curve is chosen and the associated threshold t is used for the classifier (Noe, 1983; Provost &amp; Fawcett, 2001). Other methods fix the threshold at the theoretical op-timal t ? given by (2), and seek to improve classification by improving the probability estimates. Friedman con-sidered adding a scalar a to the probability estimates for class 1 (Friedman, 1997); it is easy to show that this method is equivalent to using a threshold  X  t = t ?  X  a . Zadrozny and Elkan use monotonic functions of the probability estimate  X  p 1 to give a calibrated estimate and show great improvements in cost-sensitive classi-fication when the calibrated probability estimates are used in place of the original estimates (Zadrozny &amp; Elkan, 2001; Zadrozny &amp; Elkan, 2002). One of their approaches builds on Platt X  X  earlier work to transform support vector machine (SVM) scores into probability estimates using a sigmoid function (Platt, 2000). The same approach can be applied to probability estimates rather than SVM scores. Zadrozny and Elkan propose two other approaches to perform the calibration: bin-ning and pair-adjacent violators .
 Binning takes the probability estimates obtained using cross-validation, orders these values and then groups them into B bins so that there are an equal num-ber of samples in each bin (Zadrozny &amp; Elkan, 2001). The upper and lower boundaries of each bin are de-termined, and for any test sample with a probability estimate falling in bin b , the updated probability esti-mate for class 1 is given by the fraction of validation samples in bin b that belong to class 1.
 Pair-adjacent violators (PAV) monotonically trans-form the probability estimates using isotonic regres-sion (Ayer et al., 1955). It has been shown that apply-ing threshold t ? from (2) to the calibrated probability estimates obtained using PAV is equivalent to using a threshold chosen by ROC analysis on the original probabilities (O X  X rien, 2006). 3.2. Related Work in Multi-class Classification Zadrozny and Elkan extended their two-class solutions to multi-class problems by breaking the classification task into a number of binary classification tasks and using error correcting output codes (ECOC) to obtain multi-class probability estimates (Zadrozny &amp; Elkan, 2002).
 Other methods seek to extend the ROC thresholding approach to K -class classification. Instead of choos-ing a scalar threshold t , a partition of the ( K  X  1)-dimensional  X  p -simplex must be specified. Mossman proposed a method for three class tasks using a very re-strictive partitioning of the simplex (Mossman, 1999):  X  y ( x ) = Lachiche and Flach proposed an alternative to the minimum expected misclassification cost assignment of (1) (Lachiche &amp; Flach, 2003): where the w  X  i are chosen by minimizing costs on the training set: and x n , y n are the n th training sample and its asso-ciated class label. We refer to (4) and (5) as the LF method . Mossman X  X  method and the LF method can both be viewed as learning a new partitioning for the  X  p -simplex. In Section 5, we show how these parti-tions can be achieved by using different cost matrices in equation (1).
 MetaCost is a wrapper method that can be used with any classification algorithm and reduces the variance of the probability estimates by bootstrapping (Domin-gos, 1999). MetaCost reduces the variance of proba-bility estimates, but is not designed to overcome sys-tematic probability estimation errors. In this section we establish how different changes in the cost matrix affect the class partitioning of the  X  p -simplex enacted by (1). In Section 5 we use these properties to propose a new method to reduce the ef-fect of systematic errors in probability estimation. For a particular cost matrix c , and any two classes i, k that are adjacent in the partition of the  X  p -simplex, the partition-boundary between them is described by the hyperplane, We restrict attention to cost matrices where the cost of correct assignment is always less than the cost of incorrect assignment, that is, c j | j &lt; c i | j ,  X  i 6 = j . Property 1: For any  X  p , the assigned class is the same for cost matrices c and  X c for any scalar  X  . Proof: The minimization in (1) is unaffected by replacing c i | j by  X c i | j ,  X  i, j .
 Property 2: If the cost matrix c is full rank, then there is a point  X  e where all two class boundaries as described by (6) intersect, and  X  e may occur outside the probability simplex. We term  X  e the  X  X qual risk point X  .
 Proof: If c is full rank then the solution is  X  e = c  X  1 1 / k c  X  1 1 k 1 that will solve (6) for all classes. How-ever, it can happen that k c  X  1 1 k 1 = 0, in which case the equal risk point can be said to be at infinity. If c is not full rank there may still be an unique equal risk point. In general,  X  e must solve  X   X   X   X  If the above matrix is full rank then there is a unique point solution for  X  e , otherwise the above system is un-derdetermined and there is a hyperplane of solutions, or the above system can be overdetermined and there is no solution.
 Property 3: Adding a constant to all costs for a par-ticular true class (equivalently, adding a constant to each term in any column of the cost matrix) does not affect the assignment.
 Proof: The minimization in (1) is unaffected by Property 4: The class-partitioning produced by any cost matrix c can equivalently be produced by some cost Proof: Follows directly from Property 3.
 Property 5: (See Fig. 2b) Adding a constant  X  to the cost of assignment to class i irrespective of the true class (that is, adding  X  to each term in row i of a cost matrix), produces a new class-partitioning with partition boundaries parallel to those of the original. Proof: This change only affects the two-class boundary equations specified by (6) for class i and each class k : Thus the new boundary between class i and k is parallel to the original boundary between class i and k . To maintain c j | j &lt; c i | j ,  X  i 6 = j requires Property 6: (See Fig. 2c) Scaling all costs where the true class is ` by a positive constant  X  (that is, mul-tiplying column ` of c by  X  ) moves an equal risk point along the line joining it to the corner of the simplex where  X  p ` = 1 . The intersections of the class bound-aries with the  X  p ` = 0 plane are unchanged. Proof: To prove that the new equal risk point  X   X  e is on the line joining the original equal risk point  X  e and the corner of the simplex where  X  p ` = 1, we show that there exists a constant  X  such that for all j , and where I is the indicator function. From (6), multiplying column ` by  X  , and requiring equal risk for classes ` and k : First note that if  X  e ` = 0, then  X  e remains an equal risk point for the transformed cost matrix. Otherwise, for any  X  e , we write  X  e j = s j  X  e ` . Comparing (6) and (8), where (11) follows from (10) since components of  X   X  e and  X  e both sum to 1. This establishes (7) for ` . Lastly, by setting  X  p ` = 0 in equation (6), it is evident that changes in c i | ` and c j | ` will not effect the inter-sections of the class boundaries with the  X  p ` = 0 plane. Property 7: (See Fig. 2d) Scaling all costs where the assigned class is i by a positive constant  X  (that is, multiplying all elements in row i by  X  ) moves an equal risk point  X  e along the hyper-plane where all classes but class i have equal expected misclassification costs. Proof: Let  X  c j | k = c j | k for all j 6 = i , and let  X  c Then the equal risk point  X   X  e produced by  X  c solves the same set of class boundary equations (6) specifying  X  e , except Because the constraints specifying that the other classes have equal misclassification costs still apply, the new equal risk point  X   X  e must occur along the hyper-plane specified by that subset of the constraints. We propose changing the class partitions so that the class-partitioning corrects for the systematic error in the probability estimates. Changing the partition of the multi-class  X  p -simplex is analogous to the two-class practice of changing the threshold on  X  p 1 ( x ). We split the cost matrix X  X  role into two separate enti-ties: a partition matrix (which partitions the  X  p -simplex with linear boundaries), and the misclassification cost matrix that specifies how misclassification errors are to be scored. From now on we will use the term partition matrix for the former role, and the term cost matrix only for the latter role.
 Given a training set { ( x 1 , y 1 ) , ( x 2 , y 2 ) , ..., ( x where x n has class probability estimate vector  X  p ( x n we propose to use a partition matrix a ? that solves where the function g in (12) is defined in (1). To avoid the issue of overfitting in learning a partition matrix, we restrict the partition to have linear boundaries that are parallel to the original decision boundaries pro-duced by the cost matrix (see Fig. 2b). We have also considered learning partition matrices with differ-ent constraints, including partition matrices with all K 2  X  K free parameters, partition matrices that are column-multiply modifications of the original cost ma-trix (see Fig. 2c), and row-multiply modifications (see Fig. 2d). We found these different constraints resulted in similar performance, with the parallel partitioning working consistently well (O X  X rien, 2006).
 By Property 4 stated in Section 4, without loss of gen-erality we consider only partition matrices a where a i | i = 0 and a i | j &gt; 0 for i 6 = j . From Property 5, adding  X  i to row i of the cost matrix will yield a par-tition matrix with partition boundaries parallel to the partition boundaries induced by c . To maintain the requirement that a i | i = 0 we subtract the same con-stant from column i without affecting the partition boundaries (Property 3). Thus given the cost matrix c , the partition matrix is a where a i | j = c i | j +  X  This approach requires learning the parameters  X  i for i = 1 , . . . , K .
 Related methods can also be viewed as applications of (12) but with different restrictions on a . Mossman X  X  method for three classes (Mossman, 1999) implicitly requires a to be of the form where the  X  terms are those used in (3) and L  X  1. The LF method (Lachiche &amp; Flach, 2003) is equivalent to choosing a partition matrix a such that a i | j = w j z where z i | j is the 0-1 cost matrix and w j is the weight used in equations (4) and (5). Thus the LF method is equal to a column-multiply of a 0-1 cost matrix (see Property 5 stated in Section 4, and Fig 2c), whereas our proposed method enacts a parallel shift based on the actual cost matrix c . 5.1. Optimizing the Partition Matrix We learn the new partition matrix by minimizing an empirical loss calculated on a validation set of labeled samples using a greedy approach. The new partition matrix a is initialized to the cost matrix c . Then each free parameter is updated in turn. Let a denote the current partition matrix, and the new partition matrix j 6 = i . Suppose  X  i =  X  X  X  and interpret  X  a as a cost matrix  X  then there would be an infinitely negative cost to assigning a sample as class i , and thus every training sample would be assigned to class i . Sup-pose one increased  X  i from negative infinity. For dif-ferent values of  X  i it would become more cost-effective to classify each of the training samples as a different class, call this classification choice g i ( X  p ( x n )), where note the changepoint value  X  for  X  i &lt;  X  in training sample n would be assigned to class i and for  X  i &gt;  X  in training sample n would be assigned to class g i ( X  p ( x We find these N changepoints  X  in for n = 1 , . . . , N , by solving the N equations, Re-order the training data by their changepoints, so that { x k , y k } denotes the training point with the k th largest changepoint. Then select N  X  where, N  X  = arg min Note that  X  iN  X  to  X  iN  X  +1 defines the range of  X  i that would yield the empirical cost given in (14); we set the parameter  X  i to be the geometric mean of  X  iN  X  and  X  for all j, k 6 = i , and so  X  i is clipped to satisfy these conditions. In addition, if  X  iN  X  &lt; 0 &lt;  X  iN  X  +1 , then  X  i is set equal to 0, or equivalently  X  a is set equal to a . Each class X  X  partition matrix parameter is adjusted in this manner once, and the parameters are updated in order of class size from most populous to least. Pre-liminary experiments provided evidence that multiple passes through the parameters did not improve the fi-nal classification performance, and that performance was fairly robust to the parameter ordering. Experiments with UCI benchmark datasets compare the proposed parallel-partition matrix method to MetaCost (MC) (Domingos, 1999) and to the LF method (Lachiche &amp; Flach, 2003).
 For two-class problems the proposed partition matrix methods are equivalent to ROC analysis and therefore only multi-class problems are considered here. There are two basic variants of MetaCost: the first variant is that the probability estimates are based on training samples not including the sample, while the other variant is that all-inclusive estimates are made. The results reported here are the better of the two variants for each dataset.
 Randomized ten-fold cross-validation was done 100 times for each method and each dataset. In the cross-validation, 1 / 10 of the data was set aside as test data. For MetaCost, 100 resamples were generated using the remaining nine folds. For the proposed methods and the LF method the remaining nine folds were subject to a nine-fold cross-validation so that 8 / 10 of the data (eight folds) were used to estimate the probabilities for each of the nine folds. Then the partition matrix a and LF parameters were estimated using the nine folds X  probability estimates. Finally, the learned cost-sensitive classifier was applied to the withheld 1 / 10 of the test data.
 Experiments were done with two different probability estimation methods. For datasets with discrete fea-tures, multinomial naive Bayes was used with Lapla-cian correction for estimating probabilities. Any con-tinuous features used with naive Bayes were quan-tized to 21 values. For datasets with continuous features, regularized quadratic discriminant analysis (QDA) (Friedman, 1989) was used. Each class X  X  esti-mated covariance matrix was regularized as, where  X   X  ML is the maximum likelihood estimate of the full covariance matrix,  X   X  is the pooled maximum like-lihood estimate, d is the dimensionality of the feature vector, and  X  and  X  were increased from zero until the condition number of  X   X  was less than 10 6 .
 Experiments were run with two different cost scenar-ios. In practical situations it is often the rare events that are of greatest interest, and therefore the cost of misclassifying samples from rare classes is higher. To simulate this situation, we set c i | i = 0 and, where N i is the number of training samples labeled class i . For the second set of experiments, we set c i | i 0, and each element c j | k for j 6 = k was drawn randomly and uniformly from [1 , 10]. 6.1. Discussion of Results Results are presented in Tables 1 and 2 in terms of the mean increase in performance over the baseline of us-ing the partitioning induced by the cost matrix. The datasets in the results tables are ordered by increas-ing geometric-mean class size for the training set. The results show that MetaCost did not consistently im-prove over the baseline. The LF method performed better and usually improved performance, but caused large increases in error in two cases: image segmenta-tion with QDA, and dermatology with naive Bayes. In contrast, learning a new parallel partitioning showed a mean improvement of 10 . 8% for the rarity-based cost matrix and a mean improvement of 8 . 9% with the ran-dom cost matrix.
 The LF method and proposed partition-learning method are designed to correct for systematic errors in the probability estimates. Such systematic errors can be interpreted as a bias that can cause the classifier to be wrong in the same way on average over many training sets. Thus, we expected to see a greater in-crease in performance for the LF method and proposed partition-learning method for larger datasets (further down in the tables) because performance given a large training sets is more likely to suffer from problems of bias than estimation variance. With smaller datasets, estimation variance is generally a larger concern, and the bias reduction offered by the LF and proposed method may not be very helpful. In addition, for small datasets it is harder to learn the systematic error from only a few training samples, and there is an increased risk of overfitting the learned parameters.
 The datasets iris and dermatology had very low mis-classification loss for the original probability estimates. For these datasets there was not much improvement possible, and we hypothesize that the methods that learned parameters were likely to overfit to small im-provements in the training data.
 We used a greedy optimization approach to learn the new partition matrix for our method, but in some cases this leads only to a locally optimal solution. The LF method also uses a greedy search. However, Deng et al. X  X  results (Deng et al., 2006) show that improving the optimization of the LF objective can lead to an improvement in results. Similarly, we hypothesize that finding a globally optimal solution would also lead to an improvement in costs. We analyzed how changes in the cost matrix affect the partitioning of the  X  p -simplex due to the cost ma-trix. Based on this analysis, we explored correcting for systematic probability estimation errors by learning a partitioning of the  X  p -simplex that minimizes empirical misclassification costs on the training set. To reduce overfitting, we only considered partitionings parallel to the original partitioning induced by the cost matrix. Experiments with two standard classifiers showed that this post-processing worked best when the number of training samples per class is relatively large, and when the estimation error with the original cost matrix is large.
 The authors thank Jerome Friedman and Richard Ol-shen for helpful discussions during the course of this research. This research was supported in part by the United States National Science Foundation Grant CCR-0073050.
 Ayer, M., Brunk, H., Ewing, G., Reid, W., &amp; Silver-man, E. (1955). An empirical distribution function for sampling with incomplete information. Annals of Mathematical Statistics , 4 , 641 X 647.
 Deng, K., Bourke, C., Scott, S., &amp; Vinodchandran,
N. (2006). New algorithms for optimizing multi-class classifiers with ROC surfaces. Proc. of the ICML 2006 Workshop on ROC Analysis in Machine Learning . Pittsburgh, USA.
 Domingos, P. (1999). Metacost: A general method for making classifiers cost-sensitive. Proc. of 5th In-ternational Conference on Knowledge Discovery and Data Mining (pp. 155 X 164). San Diego, CA.
 Egan, J. (1975). Signal detection theory and ROC-analysis . New York: Academic Press.
 Friedman, J. H. (1989). Regularized discriminant anal-ysis. Journal of the American Statistical Associa-tion , 84 , 165 X 175.
 Friedman, J. H. (1997). On bias, variance, 0/1-loss, and the curse-of-dimensionality. Data Mining and Knowledge Discovery , 1 , 55 X 77.
 Hanley, J., &amp; McNeil, B. (1982). The meaning and use of the area under a receiver operating characteristic (ROC) curve. Radiology , 143 , 29 X 36.
 Hastie, T., Tibshirani, R., &amp; Friedman, J. (2001). The elements of statistical learning . New York: Springer-Verlag.
 Lachiche, N., &amp; Flach, P. (2003). Improving accuracy and cost of two-class and multi-class probabilistic classifiers using ROC curves. Proc. of 20th Interna-tional Conference on Machine Learning (pp. 416 X  423). Washington DC.
 Mossman, D. (1999). Three-way ROCs. Medical De-cision Making , 19 , 78 X 98.
 Niculescu-Mizil, A., &amp; Caruana, R. (2005). Predicting good probabilities with supervised learning. Proc. of 22nd International Conference on Machine Learn-ing .
 Noe, D. (1983). Selecting a diagnostic study X  X  cutoff value by using its receiver operating characteristic curve. Clinical Chemistry , 29 , 571 X 2.
 O X  X rien, D. B. (2006). Cost-sensitive performance of probability-estimation based classifiers: analysis and practice . Doctoral dissertation, Stanford University. Platt, J. (2000). Probabilistic outputs for support vec-tor machines and comparison to regularized likeli-hood methods. Advances in Large Margin Classi-fiers (pp. 61 X 74).
 Provost, F., &amp; Fawcett, T. (2001). Robust classifica-tion for imprecise environments. Machine Learning , 42 , 203  X  231.
 Zadrozny, B., &amp; Elkan, C. (2001). Obtaining cali-brated probability estimates from decision trees and na  X  X ve Bayesian classifiers. Proc. of 18th Interna-tional Conference on Machine Learning (pp. 609 X  616). Morgan Kaufmann Publishers, Inc.
 Zadrozny, B., &amp; Elkan, C. (2002). Transforming clas-sifier scores into accurate multiclass probability es-timates. Proc. of 8th International Conference on
Knowledge Discovery and Data Mining (pp. 694 X 
