 Using eye-tracking, we investigate how searchers interact with Web search engines which get affected by nonsensical results. We conduct a user survey to choose  X  X tupid X  com-ponents for our laboratory experiment and explore the most conspicuous ones. This research provides insights about searchers X  interactions with different kinds of  X  X tupid" search components, such as organic search results, vertical results, ads and automatic misspell correction. We investigate the influence of each class of  X  X tupid X  components on users X  at-titude to a search engine. We found that sticking in memory of the impression about the  X  X tupidity X  of the search engine depended on whether the users were finally satisfied with their searches, or did not find the answer. Experimental results show that classes of  X  X tupid X  components can be dif-ferentiated by their influence on users X  attitude. The most negative impression is caused by word losses, word colloca-tion breaks and inappropriate misspell corrections. Categories and Subject Descriptors: H.3.3 [Informa-tion Storage &amp; Retrieval]: Information Search &amp; Retrieval Keywords: online evaluation; gaze tracking; user study  X  X tupids X  are search components that appear to be non-sensical (extremely irrelevant) with respect to a given query and make a searcher disoriented and frustrated. A searcher may encounter  X  X tupids X  in the scope of different search components such as organic search results, vertical search results, search ads, automatic misspell corrections, auto-matic query suggestions or related queries. The concept of  X  X tupids X  was first introduced in [9]. The authors sug-gested a way of detecting  X  X tupid X  documents in the organic search results. In this paper, we investigate the influence of  X  X tupids X  on users X  behavior in different search components, not only in organic results, using eye-tracking.

A number of studies used eye-tracking tools to understand how searchers examine search results in detail. Granka et al. [6] provided first insights into how users browse the first  X  Now at Saint Petersburg State University c  X  page of organic search results. This research was contin-ued in [7]. Buscher et al. investigated how people interact with Web search engine results supplemented with ads and related queries of varying quality [4]. Dumais et al. used the eye-tracking methodology to provide a detailed analy-sis of the patterns of user attention to search engine result pages consisting of organic results, ads and related searches [5]. Arguello et al. conducted a user study to investigate the  X  X pill-over X  effect between vertical and organic results [1, 2]. They show that users are more likely to interact with web results when blended vertical results (images, commercial) are more consistent with the actual query intent. In [10], the presence of following biases is demonstrated: examina-tion bias for multimedia vertical results, trust bias of result lists with vertical results and a higher probability of result revisitation for vertical results. However, no attention has been paid to the problem of the influence of  X  X tupid X  results on searchers.

We conducted a qualitative laboratory eye-tracking based research of several classes of  X  X tupids X . First, to be sure that chosen  X  X tupids X  components are indeed nonsensical accord-ing to the majority of users, we organized a user survey to come up with a classification of the most notorious types of  X  X tupids X . That enabled us to study some distinct classes in our research and make generalizations not only for any  X  X tupids X  on average, but for the components from the same class of  X  X tupids X .

Further, by monitoring search behaviour of a user group with eye-tracking, we compared 5 artificial search engines, each of them having  X  X tupids X  in only one of the following components: organic search results (2 classes of  X  X tupids X ), vertical results, misspell corrections and search ads.
The goals of our research were the following. First, we aimed to investigate how searchers interact with  X  X tupid X  components of different classes. Second, we wanted to ana-lyze how different classes of  X  X tupids X  affect users X  attitude to a search engine.

The remainder of this paper is organized as follows. We describe the classification of "stupid" search components in Section 2. In Section 3 we describe the above-mentioned lab-oratory eye-tracking experiment and analyze its outcomes. Section 4 concludes the paper with the summary of the ma-jor findings.
To set up a controlled eye-tracking experiment with differ-ent classes of  X  X tupids X  it was necessary to systematize them. As defined in [9],  X  X tupids X  are such  X  X xtremely X  irrelevant results which, in contrast to  X  X oderately X  irrelevant docu-ments, do not contain any information on the topic of the Off the topic advertisement 12.2% red Wrong language of documents 7.32%  X  X  Missing misspell corrections 7.32%  X  X  Table 1: Classes of  X  X tupids X  from different search engines, sorted by the percentage of respondents mentioned them and by colors of experimental sys-tems with such  X  X tupid X  components ( X  X  X  were not in the experiment) user query and are also completely out of the query X  X  subject area. Meanwhile,  X  X tupids X  might mean something different for each user, as the above definition is very subjective.
To determine if there are any distinguishable classes in  X  X tupid X  components, we took a poll. Forty-one paid experts (35% females, 65% males) ranging in age from 26 to 66 years old with a diverse range of backgrounds and education levels were asked to respond to some questions. All of them were experienced at Web search and all were familiar with several different search engines.

The survey consisted of three questions: Every of 41 experts noted at least one case when they en-countered search components which can be named as  X  X tupid X . We systematized all examples and identified several classes of  X  X tupid X  components. The classification results are pre-sented in Table 1.

The most noticeable classes of  X  X tupids X  are:  X  X ord losses X  and  X  X ord collocation breaks X . Almost a half of the experts mentioned cases, which we named  X  X ord losses X , as an ex-tremely frustrating situation. In this case,  X  X tupidity X  of a search engine lies in the fact that some query words can be completely ignored and that is why users get very unex-pected results. Often digits, special characters, prepositions and, what is more critical, query specifications are lost. It means that users may get information about entirely dif-ferent persons, program versions, addresses, organizations, etc. (e.g. a user may receive documents related to books by Stephen Fry in response to the query  X  X tephen king books X  or iPhone 4s price offers instead of iPhone 6 Plus prices).  X  X ord collocation breaks X  is another class of  X  X tupids X  frequently occurring in our survey results. If users get search results with all the query words in each of them but shuf-fled and scattered over the documents, it is also a  X  X tupid X  that one third of experts get easily frustrated with (e.g. one might get a page about both Pamela Anderson and Gillian Flynn in response to the  X  X illian anderson X  query). Wrong automatic misspell corrections spoiled interaction with search engines for 29.3% of the experts. For example, there are wrong corrections of queries in other languages, rare words/ names and terms, wrong grammar forms.  X  X nappropriate verticals X  are regarded as  X  X tupids X  too: when users get search results blended with unexpected irrelevant videos, images or, for example, weather forecast verticals in re-sponse to the query  X  X isten to online radio Monte-Carlo X . In this case they cannot explain how and why the search engine could return such nonsensical search result. Some other small classes were mentioned: wrong region in local searches, wrong language of search results, wrong sugges-tions, missing misspell corrections, duplicated documents.
The experts added  X omments to some answers describing their feelings and further actions they undertook after they had seen  X  X tupid X  components on a search engine result page (SERP). Such comments described those feelings as annoy-ance, indifference or reaction to the situation with humor. The above-mentioned classes of  X  X tupid X  results made some experts make more thoroughly formulated queries, whereas others preferred to switch either to specialized search ser-vices or just other web search engines. The experts X  an-swers to our questions provided us with information not only about the classes of  X  X tupids X  that can be distinguished by users, but also about different possible reactions, which often appeared to be quite emotionally strong. This fact stresses the importance of closely studying the behavior of users fac-ing  X  X tupid X  results, what can be done in the scope of a controlled experiment. So, to organize such an experiment, we selected those classes of  X  X tupids X  that were mentioned in the majority of the respondents examples. Among them there were two classes that seemed to be very similar at the first glance ( X  X ord losses X  and  X  X ord collocation breaks X ). However, many users mentioned both of them separately in their answers. That is why one of the subtasks of this re-search was to define the difference in users X  attitude towards the two classes.
We designed a qualitative eye-tracking experiment in which participants were asked to complete a number of given search tasks using a web search engine. We were interested if users pay attention to  X  X tupids X  in search results and how it in-fluences their attitude to search engines in general. We used eye-tracking as an instrument to obtain detailed information about the users X  visual attention patterns. It is common for eye-tracking studies to take sequences of eye movements as a proxy for visual attention. Thus, eye-tracking and par-ticipants X  comments during the experiment can provide the data leading us to valuable insights about how users interact with  X  X tupid X  components in search results pages.
Next, we describe the experimental setup, the search tasks used, the participants and the experimental procedure in Section 3.1. We describe the observations and experimental results in Section 3.2.
For each of five selected classes of  X  X tupids X  we created an artificial search system. One additional system was used to illustrate the procedure and give participants the opportu-nity to get used to the systems. All the systems were iden-tical in design and differed only by color. The distribution of classes by systems is presented in Table 1. An example of the start page is presented in Figure 1. All experimental systems were implemented using Yandex Search API.
We prepared 43 search tasks for the experiment. Every system in the experiment had its own set of tasks (from 6 to 11 tasks).
Each task had a description what the participants should be looking for. To be able to compare the participants X  be-havior on SERPs with and without  X  X tupids X , we  X  X poiled X  SERPs only in half of the tasks. In order to make the initial SERPs comparable across participants, we provided them with an initial query for each task. Some examples of task descriptions and the corresponding initial task queries are given in Table 2.

Ten eye-tracking study participants[3] were hired by a re-cruiting company. None of them were involved in the clas-sification poll from the Section 2. All participants (ranged in age between 22 and 58 years) were familiar with several different search engines.

After a short introduction to the study and eye-tracker calibration, the participants completed a couple of prac-tical tasks in test system to get themselves familiar with search systems. Then they started the experiment. For each task we provided the participants with a written description and the corresponding initial query. After reading a query and its description aloud, the participants typed the query into the search box and pressed the search button to begin searching. The first SERP was always a locally stored, static page containing generated results with  X  X tupid X  components for  X  X poiled X  tasks and ordinary ranking for  X  X lean X  tasks. Modified search results were prepared for several variations of initial queries in case of the participants X  misprints. All the participants were asked to think aloud during browsing and to comment on their actions and their line of reasoning. The participants were free to interact with search results as they wished after getting the initial SERP. They could click links, view the next page of results, or reformulate the query. The combination of an initial fixed SERP and the full search functionality provided good balance between experimental control and search realism for a laboratory study.
To complete the task, the users had to either 1) navigate to a relevant web page and announce the finding of a solu-tion, or 2) decide that an appropriate page cannot be found and stop searching. Then they were assigned with the next task.

We had time limitations for each participant. So each of them performed 20 tasks only in 3 of 5 experimental search systems. Every system was used by 6 participants in a dif-ferent order (Latin Square design [8] was used) to avoid the perception biases and to ensure there X  X  a better balanced experimental design.

Having found solutions for all the tasks in one system, the participants were to fill in a questionnaire covering their web search experience in completed system and their feelings during the study. The experiment took about one hour per participant.
Our main goals were to study how searchers interact with  X  X tupid X  components of different types and to determine whether such results have any influence on users X  attitude to search engines. During the eye-tracking experiment, the participants were asked to comment on their actions aloud, thus making it possible for the observers to summarize the outspoken negative reactions to the search results caused by the presence of  X  X tupids X .

It was observed that even  X  X tupids X  from the same class can stick in users X  minds to a varying degree. It appeared that sticking in memory depends on whether the users were finally satisfied with their searches, or did not find the an-swer and stopped searching. In case of success, searchers could forget about the  X  X tupids X  they had seen. In case of an unsuccessful search,  X  X tupids X  reinforced the negative ex-perience and transferred the frustration accumulated during one search session to the whole system.

It was found from eye-tracking results, that if  X  X tupids X  were in the components where users do not normally ex-pect to find the answer in the first place (inappropriate ver-ticals/advertisement), the searchers paid no attention and skipped such components. It was particularly noticeable in the system with blended inappropriate vertical results (green system).

For example, the participants tried to find a definition of the word "precedent" in law dictionaries within one of the experiments X  tasks. According to eye-tracking data, they skipped unexpected and highly irrelevant image vertical re-sults, just because it was clearly impossible to find appropri-ate answers there, and then carefully examined other search results.

The similar situation was observed in the red system track-ing results. If the participants got a SERP with search ads on the right side, they paid no attention to them. Mean-while, if such ads were situated at the top of the SERP, searchers had to examine them. However, they perceived that component only as a block of sponsored links and not as a potential source of the information they needed regard-less of whether it was  X  X tupid X  or not.

In the blue and garnet systems ( X  X ord losses X  and  X  X ord collocation breaks X ) participants examined results X  snippets and titles very carefully, looking for the indications of the presence of the query terms in them. It took a lot of time and caused a negative reaction as they failed to find what they were looking for. However, when  X  X tupids X  were blended below relevant documents in search results, the users could not notice that the corresponding SERPS were spoiled.
One of the additional research questions was: do  X  X ord losses X  differ from  X  X ord collocation breaks X  in the way they affect user behavior? The participants named different reasons for unsuccessful searches in the system with such  X  X tupids X . In case of  X  X ord losses X , searchers blamed the search engine for poor results and it adversely affected users X  feelings about their searches in the blue system. Whereas, when users received search results in the garnet system ( X  X ord collocation breaks X ), they were upset, but found themselves guilty in those system fails. If searchers saw all query words in the snippets, that made them feel that sys-tem  X  X ried to do its best X , however something in the query formulation was incorrect. In this case, searchers tried to improve this situation with reformulations and more precise queries. Those eye-tracking results give us an explanation why the experts responded to the  X  X tupids X  classification poll, regarded  X  X ord losses X  and  X  X ord collocation breaks X  as two different classes.

There were two classes of  X  X tupid X  components in the experimental system with inappropriate automatic misspell corrections (violet system). The participants got search re-Table 3: Participant perceptions of the system used. Ratings ranges for Goodness and Association from 1 to 5 and from 1 to 3 respectively. Higher is more positive. sults with suggestions for corrections or search results af-fected by automatic misspell correction. In the first case, suggestions were perceived as a part of the search results X  binding, so participants did not pay attention to them. In the second case, the results changed by the automatic mis-spell correction caused increase in negative participants X  re-action. Searchers were upset and annoyed as well as in the  X  X ord losses X  system, because they were forced to spend a lot of time searching for the solution. So, we observed that  X  X tupids X  had an influence on the searchers, but it was weaker than in the case of  X  X ord losses X  and stronger than in the search results in other experimental systems.
Observations made via analyzing the eye-tracking log con-form with the results of the survey, filled out by the partic-ipants after the experiment completion.

When all the tasks in each search system were complete, the participants filled out a survey covering the following questions about each system they used: Q1: In your opinion, how good or bad was the search en-gine? (a five-point scale: very good, somewhat good, neither bad nor good, somewhat bad, very bad) (Goodness). Q2: Continue the phrase  X  X sers searching in this search en-gine ... X  (open answer) (Association).
 Answers to Q2 were transformed into a three-point scale: negative association, neutral association and positive asso-ciation. Some examples for Q2:  X ...get first-hand informa-tion X  (Positive),  X ...spend too much time to get information they need X  (Negative)
Table 3 shows the average answer ratings for each ques-tion. The participants perceived the red engine more posi-tively than the blue one. So, the worst  X  X tupid X  components were  X  X ord losses X ,  X  X nappropriate misspell corrections X  and  X  X ord collocation brakes X . It coincides with the most con-spicuous classes of  X  X tupids X  from the survey summarized in Table 1. We also found out that searchers not only no-ticed stupid components, but also remembered them after the completion of the tasks and transferred that negative attitude to the whole system.
In this paper, we presented the results of a qualitative eye-tracking study to characterize how  X  X tupid X  components influence users X  visual attention to different components of SERPs and to assess the searchers X  attitude to the search en-gines affected by  X  X tupids X . We summarize our observations of  X  X tupids X  influence on searchers as follows: the answer whether the impression about the  X  X tupidity X  of a search engine will stick in one X  X  memory depends on whether the users were finally satisfied with their searches, or did not find the answer. As consistent with the user survey we found that among such  X  X tupid X  components as organic search re-sults, vertical results, ads and automatic misspell correction,  X  X ord losses X  left the most negative impression, since it in-duced the most negative attitude towards the whole system.  X  X nappropriate misspell corrections X  and  X  X ord collocation breaks X  were the next in the list of the most frustrating system failures.

This research represents the first step in understanding how  X  X tupid X  components of different types influence search interactions. In future work we plan to analyze the influ-ence of  X  X tupids X  in a large-scale experiment on federated searches based on our findings in this paper. We also plan to look at  X  X ord losses X  more meticulously to find out which missing part of a query has the worst influence on a user. [1] J. Arguello and R. Capra. The effect of aggregated [2] J. Arguello and R. Capra. The effects of vertical rank [3] A. Aula, R. M. Khan, Z. Guan, P. Fontes, and [4] G. Buscher, S. T. Dumais, and E. Cutrell. The good, [5] S. T. Dumais, G. Buscher, and E. Cutrell. Individual [6] L. A. Granka, T. Joachims, and G. Gay. Eye-tracking [7] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and [8] D. Kelly. Methods for evaluating interactive [9] A. Lomakina, N. Povarov, and P. Serdyukov. Web [10] C. Wang, Y. Liu, M. Zhang, S. Ma, M. Zheng, J. Qian,
