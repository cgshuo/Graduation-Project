 looks for a set of features on which the response variable most depends. contrast to measures such as mutual information.
 apply our measure in experiments on synthetic and real data. The probability law of a random variable X is denoted by P R ( T ) , respecti vely . 2.1 Dependence measur es with normalized cross-co variance operators k (A-1) E [ k This assumption ensures H H for all f 2H and positi ve. The operator mar ginals and the correlation; that is, there exists a unique bounded operator V to 1. We call V While the operator V correlation analysis [1] uses the lar gest eigen value of V Suppose we have another random variable Z on Z and RKHS ( H to (A-1). We then define the normalized conditional cross-co variance oper ator , for measuring the conditional dependence of X and Y given Z , where V similarly to Eq. (2). The operator V where tional covariance matrix C The operator used, X ?? Y j Z is equi valent to  X  Noting that the conditions measures. Recall that an operator A : H thonormal systems (CONSs) f [13 ]). For a Hilbert-Schmidt operator A , the Hilbert-Schmidt (HS) norm k A k Pro vided that V It is easy to pro vide empirical estimates of the measures. Let ( X where " from Eq. (3). The HS norm of the finite rank operator b V ( n ) G and so on, and define R and R where the extended variables are used for ^ I CON D Cholesk y decomposition [17] of rank r , the comple xity to compute ^ I CON D 2.2 Infer ence on probabilities by characteristic kernels [8, 1]. We now discuss a unified class of kernels for inference on probabilities. assumption (A-1). The mean element of X on H is defined by the unique element m that h m denote m The kernel k is said to be char acteristic 1 if the map M E
X P [ f ( X )] = E X Q [ f ( X )] The notion of a characteristic kernel is an analogy to the characteristic function E which is the expectation of the Fourier kernel k E
P [ k ( u;X )] = E Q [ k ( u;X )] well-kno wn property of the characteristic function that E probability P on ( X ; B ) , the kernel k is char acteristic. Proof. Assume m function f 2H and c 2 R such that j E and k be a kernel of the form k ( x;y ) = ( x y ) . If for any 2 R m ther e exists R Theor em 3. (i) Assume (A-1) for the kernels. If the product k (ii) Denote  X  X = ( X;Z ) and k  X  char acteristic kernel on ( X Z ) Y , and H From the abo ve results, we can guarantee that V of 2.3 Kernel-fr ee integral expr ession of the measur es probability E z ] E [ A ( X ) j Z = z ] dP Z ( z ) for A 2B X and B 2B Y . Theor em 4. Let P density functions p L wher e p As a special case of Z = ; , we have Hilbert-Schmidt under (A-1), there exist CONSs f the eigenfunctions of Let I X and ~ With the notations ~ P By a similar argument, the second and third term of the expansion are rewritten as The expression of Eq. (9) can be compared with the mutual information, is infeasible if the joint space has even a moderate number of dimensions. 2.4 Consistency of the measur es I in HS norm, pro vided that V Theor em 5. Assume that V constant " In particular , ^ I N OCCO 2.5 Choice of kernels As with all empirical measures, the sample estimates ^ I N OCCO mixture of 2 distrib utions with variance Var lim [ nHSIC ] = 2 k the next section we see that the method gives a reasonable result for ^ I N OCCO various degrees of dependence. The test randomly permutes the order of Y samples independent of ( X For the evaluation of ^ I CON D of data, and the sample f ( X We perform permutation tests with ^ I N OCCO explicit estimation of the densities. Since ^ I N OCCO into bins. Figure 1 sho ws the values of ^ I N OCCO of ^ the chosen parameters seems reasonable, because the range of Y is split into two small regions. variables X and Y are four dimensional: the components X heuristic method [8] which chooses as the median of pairwise distances of the data. (a) Plot of H  X  enon map (b) X Figure 2: Chaotic time series. (a,b): examples of data. (c,d) examples of ^ I CON D the threshholds of the permutation test with significance level 5% (black  X + X ). The proposed ^ I N OCCO Ne xt, we apply ^ I CON D is a cause of Y for &gt; 0 , but there is no opposite causality , i.e., X In Table 3, it is remarkable that ^ I CON D times non-causality was accepted out of 100 tests is sho wn. pendence more strongly . on a sum of the log arithm of the eigen values of V
