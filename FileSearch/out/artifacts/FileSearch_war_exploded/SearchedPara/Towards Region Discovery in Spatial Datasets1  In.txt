 Wei Ding 1 , , Rachsuda Jiamthapthaksin 1 , Rachana Parmar 1 , Dan Jiang 1 , The goal of spatial data mining [1,2,3] is to automate the extraction of interest-ing and useful patterns that are not explic itly represented in spatial datasets. Of particular interests to scientists are the techniques capable of finding sci-entifically meaningful regions as they have many immediate applications in geoscience, medical science, and social s cience; e.g., detect ion of earthquake hotspots, disease zones, and criminal locations. An ultimate goal for region discovery is to provide search-engine-style capabilities to scientists in a highly automated fashion. Developing such a sy stem faces the following challenges. First, the system must be able to find regions of arbitrary shape at differ-ent levels of resolution. Second, the sys tem needs to provide suitable, plug-in measures of interestingness to instruct discovery algorithms what they should seek for. Third, the identified regions s hould be properly ranked by relevance. Fourth, the system must be able to accommodate discrepancies in various formats of spatial datasets. In par-ticular, the discrepancy between con-tinuous and discrete datasets poses a challenge, because existing data min-ing techniques are not designed to op-erate on a mixture of continuous and discrete datasets. Fifth, it is desirable for the framework to provide pruning and other sophisticated search strate-gies as the goal is to seek for interest-ing, highly ranked regions.

This paper presents a novel region discovery framework (see Fig. 1) geared to-wards finding scientifically interesting places in spatial datasets. We view region discovery as a clustering problem in which an externally given fitness function has to be maximized. The framework adapts four representative clustering algo-rithms, exemplifying prototype-based, grid-based, density-based, and agglomer-ative clustering algorithms for the task of region discovery. The fitness function combines contributions of interestingness from constituent clusters and can be customized by domain experts. The framework allows for plug-in fitness functions to support a variety of region discovery applications correspondent to different domain interests.
 Relevant Work. Many studies have been conducted in region discovery. These most relevant to our work are region-oriented clustering techniques and hotspot discovery. In our previous work, we have discussed a region discovery method that was restricted to one categorical attribute [4,5]. The integrated framework introduced in this paper is generalized to be applicable to both continuous and discrete datasets. The framework allows for various plug-in fitness functions and extends our work to the field of feature-based hotspot discovery (see Section 2). [1] introduces a  X  X egion oriented X  clustering algorithm to select regions to satisfy certain condition such as density. This approach uses statistical information instead of a fitness function to evaluate a cluster.

Hotspots are object clusters with resp ect to spatial coord inates. Detection of hotspots using variable resolution approach [6] was investigated in order to minimize the effects of spatial superposition. In [7] a region growing method for hotspot discovery was described, which selects seed points first and then grows clusters from these seed points by adding neighbor points as long as a density threshold condition is satisfied. Definition of hotspots was extended in [8] using circular zones for multiple variables.
 Contributions. This paper presents a highly generic framework for region dis-covery in spatial datasets. We custom ize our discovery framework to accom-modate raster, continuous, and categorical datasets. This involves finding a suitable object stru cture, suitable preprocessing te chniques, a family of reward-based fitness functions for various measures of interestingness, and a collection of clustering algorithms. We systematically evaluate a wide range of representative clustering algorithms to determine when and which type of clustering techniques are more suitable for region discovery. We apply our framework to a real-world case study concerning gr ound ice on Mars and successfully find scientifically interesting places. Region Discovery Framework. Our region discovery method employs a reward-based evaluation scheme that evaluates the quality of the discovered regions. Given a set of regions R = { r 1 ,...,r k } identified from a spatial dataset O = { o 1 ,...,o n } , the fitness of R is defined as the sum of the rewards obtained from each region r j ( j =1 ...k ): where i ( r j ) is the interestingness measure of region r j  X  X quantitybasedon domain interest to reflect the degree to which the region is  X  X ewsworthy X . The framework seeks for a set of regions R such that the sum of rewards over all of its of the fitness nonlinearly with respect to the number of objects in O belonging to the region r j . A region reward is proportional to its interestingness, but given two regions with the same value of inte restingness, a larger region receives a higher reward to reflect a preference given to larger regions.

We employ clustering algorithms for region discovery. A region is a contiguous subspace that contains a set of spatial objects: for each pair of objects belonging to the same region, there always exists a path within this region that connects them. We search for regions r 1 ,...,r k such that: 1. r i  X  r j =  X  ,i = j . The regions are disjoint. 2. R = { r 1 ,...,r k } maximizes q ( R ). 3. r 1  X  ...  X  r k  X  O . The generated regions are not required to be exhaustive 4. r 1 ,...,r k are ranked based on their rewar d values. Regions that receive no Preprocessing. Preprocessing techniques are introduced to facilitate the appli-cation of the framework to heterogeneous datasets. Given a collection of raster, categorical, and continuous datasets with a common spatial extent, the raster Step 1. Dataset Integration. Categorical datasets are converted into a con-Step 2. Dataset Unification. A single unified spatial dataset is created by Step 3. Dataset Normalization. Finally, continuous variables are normal-Measure of Interestingness. The fitness function q ( R ) (Eqn. 1) allows a function of interestingness to be defined based on different domain interests. In our previous work, we have defined fitness functions to search risk zones of earthquakes [4] and volcanoes [5] with respect to a single categorical attribute. In this paper, we define feature-based hotspots as localized regions where contin-uous non-spatial features of objects attain together the values from the wings of their respective distributions. Hen ce our feature-based hotspots are places where multiple, potentially globally uncorrelated attributes happen to attain extreme values. We then introduce a new interestingness function i on the top of the generic dataset O : given set of continuous features A = { A 1 , ..., A q } the interestingness of an object o  X  O is measured as follows: 0 are clustered in feature-based hotspots where the features in A happen to attain extreme values X  X easured as products of z-scores.

We then extend the definition of interestingness to regions: the interestingness of a region r is the absolute value of the average interestingness of the objects belonging to it: In Eqn. 3 the interestingness threshold z th is introduced to weed out regions with i ( r ) close to 0, which prevents clustering solutions from containing only large clusters of low interestingness.
 Clustering Algorithms. Our regional discovery framework relies on reward-based fitness functions. Consequently, clustering algorithms embedded in the framework, have to allow for plug-in fitness functions. However, the use of fitness function is quite uncommon in clustering, although a few exceptions exist, e.g., CHAMELEON [10]. Furthermore, region discovery is different from traditional clustering as it gears to find interesting places with respect to a given measure of interestingness. Consequently, existing clustering techniques need to be mod-ified extensively for the task of region discovery. The proposed region discovery framework adapts a family of prototype-based, agglomerative, density-based, and grid-based clustering approaches. We give a brief survey of these algorithms in this section.
 Prototype-based Clustering Algorithms. Prototype-based clustering algo-rithms first seek for a set of representativ es; clusters are then created by assigning objects in the dataset to the closest representatives. We introduce a modifica-tion of the PAM algorithm [11] which we call SPAM (Supervised PAM). SPAM starts its search with a random set of k representatives, and then greedily re-places representatives with n on-representatives as long as q ( R )improves.SPAM requires the number of clusters, k , as an input parameter. Fig. 3a illustrates the application of SPAM to a supervised clustering task in which purity of clusters with respect to the instances of two cla sses has to be maximized. SPAM correctly separates cluster A from cluster B because the fitness value would be decreased if the two clusters were merged, while the traditional PAM algorithm will merge the two clusters because they are in close proximity.
 Agglomerative Algorithms. Due to the fact that prototype-based algorithms construct clusters using nearest neighbor queries, the shape of clusters iden-tified are limited to convex polygons (Voronoi cells). Interesting regions, and in particular, hotspots, may not be restricted to convex shapes. Agglomerative clustering algorithms are capable of yielding solutions with clusters of arbitrary shape by constructing unions of small convex polygons. We adapt the MOSAIC algorithm [5] that takes a set of small convex clusters as its input and greed-ily merges neighboring clusters as long as q ( R ) improves. In our experiments the inputs are generated by the SPAM algorithm. Gabriel graphs [12] are used to determine which clusters are neighbors. The number of clusters, k ,isthen implicitly determined by the clustering a lgorithm itself. Fig. 3b illustrates that MOSAIC identifies 9 clusters (4 of them are in non-convex shape) from the 95 small convex cluster s generated by SPAM.
 Density-Based Algorithms. Density-based algorithms construct clusters from an overall density function . We adapt the SCDE (Supervised Clustering Using Density Estimation) algorithm [13] to search feature-based hotspots. Each object o in O is assigned a value of i ( A, o ) (see Eqn. 2). The influence function of object o , f Gauss ( p, o ), is defined as the product of i ( A, o ) and a Gaussian kernel: The parameter  X  determines how quickly the influence of o on p decreases as the distance between o and p increases. The density function,  X  ( p )atpoint p is then computed as: Unlike traditional density estimation techniques, which only consider the spa-tial distance between data points, our density estimation approach additionally considers the influence of the interestingness i ( A, o ). SCDE uses a hill climbing approach to compute local maxima and local minima of the density function  X  . These locales act as cluster attractors; c lusters are formed by associating objects in O with the attractors. The number of clusters, k , is implicitly determined by the parameter  X  . Fig. 3c illustrates an example in which SCDE identifies 9 re-gions that are associated with maxima (in red) and minima (in blue) of the depicted density function on the right.
 Grid-based Algorithms. SCMRG (Supervised Clustering using Multi-Resolution Grids) [4] is a hierarchical, grid-based method that utilizes a di-visive, top-down search. The spatial space of the dataset is partitioned into grid cells. Each grid cell at a higher level is partitioned further into smaller cells at the lower level, and this process continues as long as the sum of the rewards of the lower level cells q ( R ) is not decreased. The regions returned by SCMRG are combination of grid cells obtained at different level of resolution. The number of clusters, k , is calculated by the algorithm itself. Fig. 3d illustrates that SCMRG drills down 3 levels and identifies 2 clusters (the rest of cells are discarded as outliers due to low interestingness). Dataset Description and Preprocessing. We systematically evaluate our region discovery framework on spatial distribution of ground ice on Mars. Mars is at the center of the solar system exploration efforts. Finding scientifically interesting places where shallow and deep ice abundances coincide provides im-portant insight into the history of water on Mars. Shallow ice locatedinthe shallow subsurface of Mars, within an upper 1 meter, is obtained remotely from orbit by the gamma-ray spectrometer [14] (see Fig. 4a, shallow ice in 5 o  X  5 o resolution). A spatial distribution of deep ice , up to the depth of a few kilo-meters, can be inferred from spatial distribution of rampart craters [15] (see Fig. 4b, distribution of 7559 rampart craters restricted to the spatial extent de-fined by the shallow ice raster). Rampart craters, which constitute about 20% of all the 35927 craters on Mars, are surrounded by ejecta that have patterns like splashes and are thought to form in locations once rich in subsurface ice. Locally-defined relative abundance of rampart craters can be considered a proxy for the abundance of deep ice.

Using the preprocessing procedure outlined in Section 2 we construct a generic dinate of each rampart crater, z di denotes the z-score of deep ice and z si denotes the z-score of shallow ice. The value softhesetwofeaturesatlocation p are computed using a 5 o  X  5 o moving window wrapped around p . The shallow ice feature is an average of shallow-ice abundances as measured at locations of ob-jects within the window, and the deep-ice feature is a ratio of rampart to all the craters located within the window.
 Region Discovery Results. SPAM, MOSAIC, SCDE, and SCMRG clustering algorithms are used to find feature-based hotspots where extreme values of deep ice and shallow ice co-locate on Mars. The algorithms have been developed in our open source project Cougar 2 Java Library for Machine Learning and Data Mining Algorithms [16]. In the experiments, the clustering algorithms maximize the following fitness function q ( R ) X  X eealsoEqn1: For the purpose of simplification, we will use z for i ( { z di ,z si } ,r )intherestofthe paper. In the experiments, the interestingness threshold is set to be z th =0 . 15 and two different  X  values are used:  X  =1 . 01 is used for finding stronger hotspots characterized by higher values of z even if the sizes are small, and  X  =1 . 2for identifying larger but likely weaker hot spots. Table 1 summarizes the experimen-tal results. Fig. 4c shows the correspondent clustering results using  X  =1 . 01. And Fig. 4d demonstrates that larger (but weaker) hotspots are identified for  X  =1 . 2. Objects (craters) are col or-coded according to the z values of clus-ters to which they belong. The hotspots are in the locations where objects are coded by either deep red or deep blue colors. In the red-coded hotspots the two variables have values from the same-side wings of their distributions (high-high or low-low). In the blue-coded hotspots the two variables have values from the opposite-side wings of their distributions (high-low or low-high).

Which clustering algorithm produces the best region discovery results? In the rest of section, we evaluate the four c lustering algorithms with respect to statistical measures, algorithmic consideration, shape analysis, and scientific contributions.
 Statistical Measures. Table 1 is divided into four sections. The first section reports on the overall properties of clustering solutions: the parameters used by the clustering algorithms, the total reward and the number of regions discovered. The remaining three sections report on statistics of three different properties: region size, its reward on the population of the constituent regions, and the square root of the interestingness of regions. The SPAM algorithm requires an input parameter k , which is chosen to be a value that is of the same order of magnitude as the values of k yielded by the SCMRG and SCDE algorithms. Due to its agglomerative character the MOSAIC algorithm always produces a significantly smaller number of clusters regardless of the size of its input provided by the SPAM clustering solution. Thus the MOSAIC is separated from the other solutions in the table.

To seek for feature-based hotspots of sh allow ice and deep ice, the solution that receives high value of q ( R ) and provides more clusters with the highest values of  X  z is the most suitable. This is the solution having a large value of skewness for the reward and bility distribution, as the large value of skewness indicates existence of hotspots (more extreme values). In addition a suitable solution has larger values of the mean and the standard deviation for the reward and cate existence of stronger hotspots. The analysis of Table 1 indicates that SCDE and SCMRG algorithms are more suitable to discovery hotspots with higher values in z . Furthermore, we are interested in evaluating the search capability, how the top n regions are selected by the four algorithms. Fig. 5a illustrates percentile for the value of interetingness z . Fig. 5b depicts the average value of interestingness per cluster with resp ect to the top 10 largest regions. We ob-serve that SCDE can pinpoint stronger hotspots in smaller size (e.g., size =4 and z =5 . 95), while MOSAIC is the better algorithm for larger hotspots with relatively higher value of interestingness (e.g., size = 2096 and z =1 . 38). Algorithmic Considerations. As determined by the nature of the algorithm, SCDE and SCMRG algorithms support the notion of outliers  X  both algorithms evaluate and prune low-interest regions (outliers) dynamically during the search procedure. Outliers crea te an overhead for MOSAIC and SPAM because both algorithms are forced to create clusters to separate non-reward regions (outliers) from reward regions. Assigning outliers to a reward region in proximity is not an alternative because this would lead to a significant drop in the interestingness value and therefore to a signifi cant drop in total rewards.

The computer used in our experiments is Intel(R) Xeon, CPU 3.2GHz, 1GB of RAM. In the experiments of  X  =1 . 01 the SCDE algorithm takes  X  500 s to complete, whereas the SCMRG takes  X  3 . 5 s , the SPAM takes  X  50000 s ,andthe MOSAIC took  X  155000 s . Thus, the SCMRG algorithm is significantly faster than the other clustering algorithms and, on this basis, it could be a suitable candidate to searching for hotspots in a very large dataset with limited time. Shape Analysis. As depicted in Fig. 4, in con-trast to SPAM whose shapes are limited to convex polygons, and SCMRG whose shapes are limited to unions of grid-cells, MOSAIC and SCDE can find arbitrary-shaped clusters. The SCMRG algorithm only produces good solutions for small values of  X  , as larger values of  X  lead to the formation of large, boxy segments that are not effective in isolating the hotspots. In addition, the figure on the right depicts the area of Acidalia Plantia on Mars (centered at  X  X  X  15 o longitude,  X  40 o lati-tude). MOSAIC and SCDE have done a good job in finding non-convex shape clusters. Moveover, notice that both algorithms can discover interesting regions inside other regions  X  red-coded region s (high-high or low-low) are successfully identified inside the blue-coded regions (low-high or high-low). It thus makes the hotspots even  X  X otter X  when excluding inside regions from an outside region. Scientific Contributions. Although the global correlation between the shal-low ice and deep ice variables is only  X  0 . 14434  X  suggesting the absence of a global linear relationship  X  our region discovery framework has found a number of local regions where extreme values of both variables co-locate. Our results in-dicate that there are several regions on Mars that show a strong anti-collocation between shallow and deep ice (in blue), but there are only few regions on Mars where shallow and deep ground ice co-locate (in red). This suggests that shallow ice and deep ice have been deposited at different geological times on Mars. These places need to be further studied by the domain experts to find what particular set of geological circumsta nces led to their existence. This paper presents a novel region discovery framework for identifying the feature-based hotspots in spatial datasets. We have evaluated the framework with a real-world case study of spatial distribution of ground ice on Mar. Empirical sta-tistical evaluation was developed to compare the different clustering solutions for their effectiveness in locat ing hotspots. The results reveal that the density-based SCDE algorithm outperforms other algorithms inasmuch as it discovers more re-gions with higher interestingness, the grid-based SCMRG algorithm can provide acceptable solutions within limited time, while the agglomerative MOSAIC clus-tering algorithm performs best on larger hotspots of arbitrary shape. Further-more, our region discovery algorithms have identified several interesting places on Mars that will be further studied in the application domain.
 The work is supported in part by the National Science Foundation under Grant IIS-0430208. A portion of this research wa s conducted at the Lunar and Plane-tary Institute, which is operated by the USRA under contract CAN-NCC5-679 with NASA.

