 School of Computer Science and Technology, Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, Liaoning, China School of Innovation and Experiment, Dalian University of Technology, Dalian, Liaoning, China 1. Introduction
Agrawal [1] developed the first algorithm (Apriori) for mining complete frequent itemsets from trans-actional datasets in 1994. Since then, new algorithms have been continuously proposed, such as FP-Growth [11], COFI [8], MAFIA [5], Pincer-search [19], CLOSET [21], CHARM [28] and so on. These approaches can be classified into two categories: level-wise approaches and pattern-growth approaches. The Apriori algorithm [1] is a classical level-wise approach, and the FP-Growth algorithm [11] is a classical pattern-growth approach. Frequent itemset mining approaches have been successfully adopted in many application domains. However, these approaches only consider each item in a transactional database as a 0/1 value; they do not consider the difference in significance between items. The utility of an itemset is a measure of this difference in terms of a weight value, such as quantity or profit, or a world stream data.

To address this issue, Yao et al. [26] proposed the High utility itemsets mining model, and defined of utilities of itemset X in all transactions containing itemset X .Anitemset X is a high utility itemset if u ( X ) is not less than the user-specified minimum utility value .
 For static transactional databases, there are many algorithms, such as UMining and UMining_H [27], Two-Phase [20], IIDS [17], an approximation approach for mining high utility itemsets [12], CTU-Mine [9], IHUP [2], UP-Growth [24], and HUP-Growth [18]. In 2010, Ahmed et al. [3] proposed two algorithms for mining high utility itemsets in sequence databases. However, data stream is more common been an important research issue in data mining to mine over data streams. A commonly used approach in mining on continuous data streams is the sliding window filtering method [4,6,7,14 X 16,22,23,25].The existing algorithms [4,15,25] are for mining high utility itemsets from data streams.
 The first algorithm for mining temporal high utility itemsets over data streams is based on Two-Phase and is called THUI [25]; it is a level-wise approach, and may generate some false candidates as well as lose some high utility itemsets. Reference [ 15] also proposes a level-wise approach; it uses two data structures bit-vector and TID-lists to quickly scan dataset. The algorithm in [4] is based on tree structure; it maintains transactional itemsets on a tree, and generates candidates by the tree. Though it has achieved better performance in terms of time, it still generates candidate itemsets, and need two scans of dataset. Since data streams are continuous and unbounded, one important task in data mining a novel tree structure, called UT-Tree (Utility on Tail Tree), and an algorithm, called HUM-UT (High Utility itemsets Mining based on UT-Tree), for mining high utility itemsets from data streams based on the sliding window filtering method. HUM-UT can find the high utility itemsets without generating candidate itemsets, and need only one scan of dataset. Experiments show that our algorithm outperforms the state-of-the-art algorithm HUPMS [4] in terms of time and space.

The remainder of the paper is organized as follows: Section 2 defines the relevant terms and discusses the related work; Section 3 describes the construction of UT-Tree and its updating mechanism, and introduces our HUM-UT algorithm; Section 4 shows the experimental results of HUM-UT vs. HUPMS under varied conditions; Sectio n 5 concludes this study. 2. Background and related work 2.1. Definitions ( i = { T 1 ,T 2 ,...,T transactional itemset T d be q ( i r ,T d ) .
 We adopt definitions similar to those presented in the previous works [1,2,20,24,26].
 by For example, in Tables 1 and 2, u ( { d } ,T 1 )=3  X  8=24 .
 Definition 2. The utility of an itemset X in T d is denoted as u ( X,T d ) and is defined by [2,20,24] For example, u ( { de } ,T 1 )= u ( { d } ,T 1 )+ u ( { e } ,T 1 ) = 24 + 40 = 64 .
 by For example, u ( { de } )= u ( { de } ,T 1 )+ u ( { de } ,T 8 ) = 64 + 70 = 134 .
 by For example, tu ( T d )= u ( { d } ,T 1 )+ u ( { e } ,T 1 )+ u ( { f } ,T 1 ) = 24 + 40 + 1 = 65 . and is defined by For example, Ttu ( DB )= 8 i =1 tu ( T i ) = 65 + 30 + 52 + 56 + 38 + 74 + 37 + 103 = 455 . Definition 6. The minimum utility threshold  X  is a user given threshold as a percentage value of total  X  as follows: utility value .

Mining high utility itemsets from a transactional database means to discover all itemsets whose utility values are not less than the minimum utility value .
 by It is the sum of the transactional utilities of all transactional itemsets containing X . For example, twu ( { de } )= tu ( T 1 )+ tu ( T 8 ) = 65 + 103 = 168 .
 The downward closure property remains valid with transactional wei ghted utilization .
 minimum utility value . We also describe a candidate as promising and a non-candidate as unpromising . Definition 10. The support number ( sn )ofanitemset X is the number of transactional itemsets contain-ing X . 2.2. Related work 2.2.1. Level-wise and pattern-growth approaches
The idea of level-wise approach is to iteratively generate the candidate ( k +1 )-itemsets from combi-nations of the frequent k -itemsets ( k 1 ), and calculate their support numbers by scanning database. Its erate candidate itemsets in each iteration. The Apriori algorithm [1] is a classical level-wise approach, and it is the first algorithm for mining frequent itemsets.

Pattern-growth approach is also an iteration process. FP-Growth is a classical pattern-growth algo-rithm; it finds all frequent items under the condition of k -itemset X ( k 1 ), and generates frequent ( k +1 )-itemsets by the union of each one of those frequent items with X . It first finds the frequent 1-itemsets by scanning the database once, and maintains all transactional itemsets on a tree with a second scan of database. It will generate a conditional sub-tree for each frequent itemset X . Thus it will find all frequent itmes under the condition of X by scanning this conditional sub-tree. FP-Growth uses a combination of the vertical and horizontal database layout to store transactional itemsets in a compact tree structure, and every item has a linked list going through all transactions that contain that item. 2.2.2. High utility itemsets mining
Yao et al. [26] proposed the mathematical model and definitions for high utility itemsets mining in 2004. The authors estimated an expected utility value to determine whether an itemset should be a can-didate itemset for high utility itemsets. However, the number of candidates may approach to the number of all combinations of all items when the minimum utility value is very small and the database contains many long transactions. Later, they proposed two new algorithms, UMining_H and UMining [27], in 2006. UMining_H adopts a heuristic method for pruning. UMining employs the strategy of utility upper bound to prune. However, these two algorithms may lose some high utility itemsets, meanwhile generate excessive candidates.

In 2005, Liu et al. proposed a classical algorithm called Two-Phase [20] for mining high utility item-sets. Two-Phase is based on the transaction-weighted-utilization ( twu ) model: an itemset can be consid-model maintains a twu downward closure property: if an itemset is not a candidate, its supersets are not method; second, identify the high utility itemsets fro m the candidates by an additional database scan. Two-Phase outperforms the algorithm in paper [26]. However, it still generates excessive candidates.
To reduce the number of the candidates generated by the Two-Phase algorithm, in 2007, Erwin et al. [9] proposed a tree-based algorithm called CTU-Mine. It outperforms Two-Phase only in dense databases when the minimum utility value is very small.

Li et al. proposed an isolated items discarding strategy (IIDS) to reduce the number of candidates, and applied this strategy to two existing algorithms, resulting two new algorithms called FUM and DCG + [17], respectively. These new algorithms outperform their original algorithms as well as Two-Phase. However, their shortcoming is that they need multiple database scans and generate candidates. In 2011, the HUP-Growth algorithm [18] was proposed to overcome the problem of Two-Phase. HUP-Growth is more efficient than Two-Phase, but it still generates a lot of combinations of items.
In 2009, Ahmed et al. proposed a tree-based data structure, called IHUP-tree, to maintain the infor-mation of transactional itemsets using one scan of database. They then propose an IHUP algorithm to mine candidates for high utility itemsets from the IHUP-tree without additional scan of the database [2]. The IHUP algorithm consists of three steps: first, construct the IHUP-tree; second, generate candidates; last, find high utility itemsets from candidates. IHUP algorithm achieves better performance than IIDS and Two-Phase in terms of time and space.

In 2010, Tseng et al. proposed the UP-Growth algorithm [24] to reduce the number of candidates generated by IHUP. UP-Growth outperforms IHUP substantially in terms of execution time through reducing the number of candidates, and has better performance when the database contains lots of long transactions. The authors mainly adopted four strategies: first, discard global low utility items; second, by these four strategies, thus the number of candidates is effectively reduced.

Though both IHUP [2] and UP-Growth [24] maintain transactional itemsets in a tree, the utility of each item in each transactional itemset cannot be retrieved from the tree. This is an important reason why it must generate candidate itemsets. 2.2.3. Mining high utility itemsets from data streams
In 2006, Tseng et al. [25] proposed the first algorithm, called THUI, for mining temporal high utility itemsets over data streams. The key idea is to integrate the Two-Phase algorithm [20] and the sliding-window filtering method [13] to mine temporal high utility itemsets over data streams. The mining process on a window is performed when it is needed. A window contains a fixed number of batches, of the window, and then finds all candidate 2-itemsets for the first 2 batches in the window by one scan of the second batch of data, and so on; lastly, find all candidate 2-itemsets of the whole window. After finish processing the whole window, remove the most obsolete data, and append newly come data. The THUI algorithm generates all candidates using candidate 2-itemsets , and reduces the number of scans on the original database. In its experiments, an operation of mining for high utility itemsets was done in every window. THUI outperforms Two-Phase in terms of running time, but it may generate some false candidates as well as lose some true candidates.
In 2008, Li et al. [15] proposed two algorithms based on sliding-window filtering method, MHUI-BIT and MHUI-TID, for mining high utility itemsets from data streams. In order to quickly calculate the utility values of ite mset in each scan of database, MHUI-BIT and MHUI-TID use the data structure bit-vector and TID-lists to maintain information of distinct items in a window respectively. To mine high utility itemsets from the window, the two algorithms a dopt the level-wise approach. Their experiments show that the two algorithms outperform THUI in terms of running time.

In 2010, Ahmed et al. [4] proposed a new algorithm, called HUPMS, for mining high utility itemsets over data streams base on sliding-window filtering method using tree data structure. First, it maintains transactional information on a tree by one scan of dataset, and removes obsolete data and adds new data when new data slip into the window. Second, it generates all candidates for high utility itemsets by the tree. Last, it finds high utility itemsets from the candidates by one additional scan of dataset of the window. This method reduces the number of candidates. Its experiments show that it has achieved a better performance in terms of running time. The reason why it must generate candidates is that the to the tree. This motivates us to construct a special tree structure to maintain transactional itemsets as conditional sub-trees. 3. Proposed method
Our proposed method is called HUM-UT (High Utility itemsets Mining based on UT-Tree). It consists of three main steps: (1) construct a UT-Tree (Utility on Tail Tree); (2) remove obsolete data and update new data; (3) mine high utility itemsets from the current window. 3.1. Construction of a UT-Tree When X is added to a tree T in this order, the node that represents the tail item is called a tail-node of X . Thus, all leaf nodes on T are tail-nodes. The node that does not represent the tail item is called an the nodes  X  X  X  and  X  X  X  are tail-nodes of T 4 and T 7 in Table 1, respectively.

On a UT-Tree, there are two types of nodes: one is inner-node , which consists of the following fields: of the utility values of all items in the tail-itemset of the tail-node).

The construction of a UT-Tree is as follows: (1) arrange the items of each transactional itemset in lexicographic order; (2) add the sorted itemset into a UT-Tree, and the utility values of all items of an itemset are stored as a list on its tail-node.

For example, consider the transactional database of Tables 1 and 2. Here each window contains 3 batches of data, and each batch contains 2 transactional itemsets. Initially, the UT-Tree is created with only a root. After first batch of data is added to the tree, as shown in Fig. 1(a), nodes  X  X  X  and  X  X  X  are tail-nodes, and  X  X 24,40,1}  X  on node  X  X  X  represents t he utility value of each node on the path  X  X oot-d-e-f X . Because a window contains 3 batches, there are 3 lists ( X  X } X ) on each tail-node.  X  X [1] X ,  X  X [2] X  and  X  X [3] X  in the Global Header Table maintain the twu value of each item for the 3 batches in a window, respectively. The tree in Fig. 1(a) is considered as a global tree. The tail node table maintains pointers of tail-nodes of each batch and is used for deleting obsolete data.

Now we start add the second batch of data. The search space is increasing along with the growing of the tree (as transactional itemsets are added continuously). In order to construct a UT-Tree quickly, we use the following strategy: (1) construct a sub UT-Tree for each batch data; (2) merge the sub UT-Tree with the global UT-Tree. For the example of Fig. 1, when the second batch is retrieved, construct a sub-tree T 2, as shown in Fig. 1(b), and then merge the sub-tree with the global UT-Tree, as shown in Fig. 1(c). All tail-nodes of each batch on the global tree are stored to the tail node table when the sub-tree is merged with the global UT-Tree. Figure 1(d) is the UT-Tree after the first three batches of data are added.

When the UT-Tree contains 3 batches of data, an operation of mining can be done if it is needed. And after processing the whole window, remove obsolete data from the tree and append new data to it. 3.2. Removing obsolete data and updating new data
The tail node table maintains all tail-nodes of each batch of the current window. When obsolete data are discarded, we can find all tail-nodes of the obsolete batch through searching the tail node table For example, as shown in Fig. 1(d), when the obsolete data are removed from the tree, there are two two nodes, meanwhile these nodes have no children, thus we can remove these two tail-nodes from the Use the same way to process the parents of the deleted nodes. When all obsolete data are removed, the corresponding information is also removed from the global twu table and the tail node table. For example Fig. 2(a).

After removing the obsolete data, next step is to append a new batch of data (that is, the fourth batch) tree, as shown in Fig. 2(c). Note that to improve algorithm efficiency, we use modulus operation (in-stead of shifting left corresponding utility information) when we update new utility information to the corresponding position on the global tree ,the tail node table ,andthe global twu table . 3.3. Mining high utility items ets from the current window
The procedure of mining HUIs (high utility itemsets) from the current window is shown in Fig. 3. 3.3.1. Main steps of mining HUIs Step 1: Calculate the minimum utility value .
 Step 2: Create a header table for the global tree.
 Step 3: Add an attached list to each leaf node.
 Step 4: Calculate twu and utility value o f each item in the header table.
 Step 5: Create a prefix-tree and a header table for the base-itemset .
 Step 6: Process the prefix tree.
 3.3.2. An example of mining HUIs from a data window
Consider the example of Tables 1 and 2, the following is the detailed process of mining high utility itemsets for the second window.

Step 1: By the global twu table and the minimum utility threshold , we can calculate the minimum
Step 2: Add an Utility_cache to each leaf node, as shown in Fig. 4(a). Step 3: Process each item beginning from the last one in the header table in Fig. 4(a): 4. Experimental evaluation
In this section, we evaluated the performance of our proposed algorithm HUM-UT and performed transactional dataset T10.I4.D100K . The dataset retail contains the retail market basket data from an anonymous Belgian retail store, with 88,162 transactions and 16,470 distinct items; its mean transaction size is 10.3, and the degree of sparse is 0.06% (10.3/16470). The dataset Connect consists of 67,557 the degree of dense is 33.33% (43/129). As for dataset T10.I4.D100K , its name describes the dataset X  X  parameters: T is the average size of transactions; I is the average size of maximal potential frequent itemsets; D is the total number of transactions; it is a sparse dataset, and the degree of sparse is 1.15% tional itemset. We adopted the method of randomly generating the quantity values, as proposed in [2,9, been generated by a log-normal distribution like the existing works [2,9,17,20], assigning a utility value for each item randomly, ranging from 0.01 to 10.00. Figures 5 X 7 show the external utility distribution of 870 distinct items of the dataset T10.I4.D100K , 16470 distinct items of the dataset retail , and 129 distinct items of the dataset Connect using a log-normal distribution, respectively.

Since the HUPMS algorithm outperforms the other algorithms [15,25] for mining high utility itemsets over data streams in terms of running time and memory, we compare the performance of the HUM-UT algorithm with only the HUPMS algorithm. The configuration of the testing platform is as follows: Windows 7 operating system, 3G Memory, Intel(R) Core(TM) i5-2300 CPU @ 2.80 GHz; all algorithms were written in Java programming language (Java heap si ze is 2 G). The original transactional datasets were obtained from FIMI Repository [10]. The runnable programs and testing datasets used in our experiments are available at: http://code .google.com/p/ ut-tree/downloads/list.

We evaluated the performance of our algorithm in terms of execution time under varied minimum utility threshold, varied window-size, and varied batch-size. 4.1. Execution time under varied minimum utility threshold
In this section, we evaluated the execution time of our algorithm under varied minimum utility thresh-old . Here each window consists of 3 batches, and each batch consists of 10 K transactional itemsets. Mining was performed on 6 consecutive windows for retail , 8 consecutive windows for T10.I4.D100K and 4 consecutive windows for Connect .
 Table 3 shows the number of total candidates and trees generated by two algorithms on retail and T10.I4.D100K during the implementation process. Because the HUPMS calculates an overestimated twu value of an itemset to judge whether a sub tree should be created for this itemset while HUM-UT can calculate an actual twu value of an itemset, HUPMS generates excessive candidates and trees during the implementation process. This is reason why HUPMS may need much memory and time. Figures 8 and 9 show the running time comparison of HUM-UT and HUPMS on the dataset retail and T10.I4.D100K . From Table 3, Figs 8 and 9, it is obvious that our proposed algorithm outperforms HUPMS in terms of time and space.

Figure 10 shows the running time of HUM-UT on the dataset Connect . Because HUPMS is out of memory when the minimum utility threshold is less than 36%, there is no running time diagram for HUPMS in Fig. 10.

Table 4 shows the numbers of total trees generated by HUM-UT on Connect. The number of frequent itemsts increases drastically (from 181754 to 493109) when the minimum utility value is down to 30% from 31%; this is the reason why we see a quick increasing of running time in Fig. 10.

From the above results we can see that our algorithm (HUM-UT) outperforms HUPMS in terms of time and memory, and the running time of HUM-UT increases more slowly with the decreasing of the minimum utility threshold . 4.2. Execution time under varied window-size
In this section, we evaluated the execution time of our algorithm under varied number of batches in a window. The number of batches in a window ranges from 2 up to 5. Each batch consists of 10 K transactional itemsets, and the minimum utility threshold is 1.0%. A mining operation was performed on each window. Figure 11 shows the running time comparison of HUM-UT and HUPMS on the dataset retail under different number of batches in a window. Figure 12 shows the running time comparison of HUM-UT and HUPMS on the dataset T10.I4.D100K under different number of batches in a window. From these results we can see that the running time of algorithm HUM-UT outperforms HUPMS sig-nificantly in running time and space, and it is stable when the number of batches in a window increases. The reason is that HUPMS not only generates more candidates, but also more trees than HUM-UT. 4.3. Execution time under varied batch-size
In this section, we evaluated the performance of our algorithm under varied batch-size; the number of transactions of a batch ranges from 5 K up to 25 K. Each window consists of 3 batches and the minimum utility threshold is 1.0%. Same as the other tests, a mining operation was performed on each window. Figures 13 and 14 show the running time comparison of HUM-UT and HUPMS on dataset retail and dataset T10.I4.D100K respectively. These results show that our algorithm HUM-UT outperforms HUPMS significantly in terms of running time. Moreover, the running time of HUM-UT is stable under different batch-sizes, while the running time of HUPMS increases quickly when the batch-size decreases. Summarizing these experimental results, we can conclude that not only our algorithm outperforms HUPMS in execution time and memory, but also its performance is more stable under varied minimum utility thresholds, varied window-size, and varied batch-size.
 5. Conclusions
In this paper, we propose an efficient and effective algorithm, named HUM-UT, for mining high utility itemsets over transactional data streams based on sliding window, and a data structure, named UT-Tree, for maintaining utility information of transactional itemsets. UT-Tree is constructed with one scan of the database, and contains a fixed number of transactional itemsets. The HUM-UT algorithm mines high utility itemsets from the tr ee without addi tional scan of database. Experimental results show that the performance of HUM-UT outperforms that of the state-of-the-art HUPMS for data streams, in terms of time and space, under different experimental conditions, including varied minimum utility thresholds, varied window-size, and varied batch-size on real sparse dataset, synthetic sparse dataset and real dense dataset. Moreover, the time performance of our algorithm is stable under these different experimental conditions.
 Acknowledgments
This work is supported by National Natural Science Foundation of P.R. China (Grant No. 61173163, 51105052), Program for New Century Excellent Talents in University (Grant No. NCET-09-0251), and Liaoning Provincial Natural Science Foundation of China (Grant No. 201102037).
 References
