 Adam Coates acoates@cs.stanford.edu Pieter Abbeel pabbeel@cs.stanford.edu Andrew Y. Ng ang@cs.stanford.edu Many tasks in robotics can be described as a trajectory that the robot should follow. Unfortunately, specify-ing the desired trajectory and building an appropriate model for the robot dynamics along that trajectory are often non-trivial tasks. For example, when asked to describe the trajectory that a helicopter should follow to perform an aerobatic flip, one would have to spec-ify a trajectory that (i) corresponds to the aerobatic flip task, and (ii) is consistent with the helicopter X  X  dy-namics. The latter requires (iii) an accurate helicopter dynamics model for all of the flight regimes encoun-tered in the vicinity of the trajectory. These coupled tasks are non-trivial for systems with complex dynam-ics, such as helicopters. Failing to adequately address these points leads to a significantly more difficult con-trol problem.
 In the apprenticeship learning setting, where an ex-pert is available, rather than relying on a hand-engineered target trajectory, one can instead have the expert demonstrate the desired trajectory. The expert demonstration yields both a desired trajectory for the robot to follow, as well as data to build a dynamics model in the vicinity of this trajectory. Unfortunately, perfect demonstrations can be hard (if not impossible) to obtain. However, repeated expert demonstrations are often suboptimal in different ways, suggesting that a large number of suboptimal expert demonstrations could implicitly encode the ideal trajectory the subop-timal expert is trying to demonstrate.
 In this paper we propose an algorithm that ap-proximately extracts this implicitly encoded opti-mal demonstration from multiple suboptimal expert demonstrations, and then builds a model of the dy-namics in the vicinity of this trajectory suitable for high-performance control. In doing so, the algorithm learns a target trajectory and a model that allows the robot to not only mimic the behavior of the expert but even perform significantly better.
 Properly extracting the underlying ideal trajectory from a set of suboptimal trajectories requires a signifi-cantly more sophisticated approach than merely aver-aging the states observed at each time-step. A simple arithmetic average of the states would result in a tra-jectory that does not even obey the constraints of the dynamics model. Also, in practice, each of the demon-strations will occur at different rates so that attempt-ing to combine states from the same time-step in each trajectory will not work properly.
 We propose a generative model that describes the ex-pert demonstrations as noisy observations of the unob-served, intended target trajectory, where each demon-stration is possibly warped along the time axis. We present an EM algorithm X  X hich uses a (extended) Kalman smoother and an efficient dynamic program-ming algorithm to perform the E-step X  X o both infer the unobserved, intended target trajectory and a time-alignment of all the demonstrations. The time-aligned demonstrations provide the appropriate data to learn good local models in the vicinity of the trajectory X  such trajectory-specific local models tend to greatly improve control performance.
 Our algorithm allows one to easily incorporate prior knowledge to further improve the quality of the learned trajectory. For example, for a helicopter performing in-place flips, it is known that the helicopter can be roughly centered around the same position over the entire sequence of flips. Our algorithm incorporates this prior knowledge, and successfully factors out the position drift in the expert demonstrations. We apply our algorithm to learn trajectories and dy-namics models for aerobatic flight with a remote con-trolled helicopter. Our experimental results show that (i) our algorithm successfully extracts a good trajec-tory from the multiple sub-optimal demonstrations, and (ii) the resulting flight performance significantly extends the state of the art in aerobatic helicopter flight (Abbeel et al., 2007; Gavrilets et al., 2002). Most importantly, our resulting controllers are the first to perform as well, and often even better, than our ex-pert pilot.
 We posted movies of our autonomous helicopter flights at: The remainder of this paper is organized as follows: Section 2 presents our generative model for (multi-ple) suboptimal demonstrations; Section 3 describes our trajectory learning algorithm in detail; Section 4 describes our local model learning algorithm; Section 5 describes our helicopter platform and experimental re-sults; Section 6 discusses related work. 2.1. Basic Generative Model We are given M demonstration trajectories of length N k , for k = 0 ..M  X  1. Each trajectory is a sequence of states, s k j , and control inputs, u k j , composed into a single state vector: Our goal is to estimate a  X  X idden X  target trajectory of length T , denoted similarly: We use the following notation: y = { y k j | j = 0 ..N k  X  1 , k = 0 ..M  X  1 } , z = { z t | t = 0 ..T  X  1 } , and similarly for other indexed variables.
 The generative model for the ideal trajectory is given by an initial state distribution z 0  X  X  (  X  0 ,  X  0 ) and an approximate model of the dynamics The dynamics model does not need to be particularly accurate X  X n our experiments, we use a single generic model learned from a large corpus of data that is not specific to the trajectory we want to perform. In our experiments (Section 5) we provide some concrete ex-amples showing how accurately the generic model cap-tures the true dynamics for our helicopter. 1 Our generative model represents each demonstration as a set of independent  X  X bservations X  of the hidden, ideal trajectory z . Specifically, our model assumes Here  X  k j is the time index in the hidden trajectory to which the observation y k j is mapped. The noise term in the observation equation captures both inaccuracy in estimating the observed trajectories from sensor data, as well as errors in the maneuver that are the result of the human pilot X  X  imperfect demonstration. 2 The time indices  X  k j are unobserved, and our model To accommodate small, gradual shifts in time between the hidden and observed trajectories, our model as-sumes the observed trajectories are subsampled ver-sions of the hidden trajectory. We found that hav-ing a hidden trajectory length equal to twice the average length of the demonstrations, i.e., T = Figure 1 depicts the graphical model corresponding to our basic generative model. Note that each observa-tion y k j depends on the hidden trajectory X  X  state at pends on all states in the hidden trajectory that it could be associated with. 2.2. Extensions to the Generative Model Thus far we have assumed that the expert demon-strations are misaligned copies of the ideal trajectory merely corrupted by Gaussian noise. Listgarten et al. have used this same basic generative model (for the case where f (  X  ) is the identity function) to align speech signals and biological data (Listgarten, 2006; Listgarten et al., 2005). We now augment the basic model to account for other sources of error which are important for modeling and control. 2.2.1. Learning Local Model Parameters For many systems, we can substantially improve our modeling accuracy by using a time-varying model f t (  X  ) that is specific to the vicinity of the intended trajectory at each time t . We express f t as our  X  X rude X  model, f , augmented with a bias term 3 ,  X   X  t : To regularize our model, we assume that  X   X  t changes We incorporate the bias into our observation model for each of the observed state transitions, and mod-eling this as a direct observation of the  X  X rue X  model bias corrupted by Gaussian noise. The result of this modification is that the ideal trajectory must not only look similar to the demonstration trajectories, but it must also obey a dynamics model which includes those errors consistently observed in the demonstrations. 2.2.2. Factoring out Demonstration Drift It is often difficult, even for an expert pilot, during aerobatic maneuvers to keep the helicopter centered around a fixed position. The recorded position tra-jectory will often drift around unintentionally. Since these position errors are highly correlated, they are not explained well by the Gaussian noise term in our observation model.
 To capture such slow drift in the demonstrated trajec-tories, we augment the latent trajectory X  X  state with a  X  X rift X  vector  X  k t for each time t and each demonstrated trajectory k . We model the drift as a zero-mean ran-dom walk with (relatively) small variance. The state observations are now noisy measurements of z t +  X  k t rather than merely z t . 2.2.3. Incorporating Prior Knowledge Even though it might be hard to specify the complete ideal trajectory in state space, we might still have prior knowledge about the trajectory. Hence, we introduce additional observations  X  t =  X  ( z t ) corresponding to our prior knowledge about the ideal trajectory at time t . The function  X  ( z t ) computes some features of the hidden state z t and our expert supplies the value  X  t that this feature should take. For example, for the case of a helicopter performing an in-place flip, we use an observation that corresponds to our expert pilot X  X  knowledge that the helicopter should stay at a fixed position while it is flipping. We assume that these ob-servations may be corrupted by Gaussian noise, where the variance of the noise expresses our confidence in the accuracy of the expert X  X  advice. In the case of the flip, the variance expresses our knowledge that it is, in fact, impossible to flip perfectly in-place and that the actual position of the helicopter may vary slightly from the position given by the expert.
 Incorporating prior knowledge of this kind can greatly enhance the learned ideal trajectory. We give more detailed examples in Section 5. 2.2.4. Model Summary In summary, we have the following generative model: random variables with respective covariance matrices  X  ties for  X  k j are defined by Eqs. (3, 4) with parameters d , d k 2 , d k 3 (collectively denoted d ). Our learning algorithm automatically finds the time-alignment indexes  X  , the time-index transition prob-abilities d , and the covariance matrices  X  ( ) by (ap-proximately) maximizing the joint likelihood of the observed trajectories y and the observed prior knowl-edge about the ideal trajectory  X  , while marginalizing out over the unobserved, intended trajectory z . Con-cretely, our algorithm (approximately) solves Then, once our algorithm has found  X  , d ,  X  ( ) , it finds the most likely hidden trajectory, namely the trajec-tory z that maximizes the joint likelihood of the ob-served trajectories y and the observed prior knowledge about the ideal trajectory  X  for the learned parameters  X  The joint optimization in Eq. (11) is difficult because (as can be seen in Figure 1) the lack of knowledge of the time-alignment index variables  X  introduces a very large set of dependencies between all the variables. However, when  X  is known, the optimization problem in Eq. (11) greatly simplifies thanks to context spe-cific independencies (Boutilier et al., 1996). When  X  is fixed, we obtain a model such as the one shown in Figure 2. In this model we can directly estimate the multinomial parameters d in closed form; and we have a standard HMM parameter learning problem for the covariances  X  ( ) , which can be solved using the EM al-gorithm (Dempster et al., 1977) X  X ften referred to as Baum-Welch in the context of HMMs. Concretely, for our setting, the EM algorithm X  X  E-step computes the pairwise marginals over sequential hidden state vari-ables by running a (extended) Kalman smoother; the M-step then uses these marginals to update the covari-ances  X  ( ) .
 To also optimize over the time-indexing variables  X  , we propose an alternating optimization procedure. For fixed  X  ( ) and d , and for fixed z , we can find the opti-mal time-indexing variables  X  using dynamic program-ming over the time-index assignments for each demon-stration independently. The dynamic programming al-gorithm to find  X  is known in the speech recognition literature as dynamic time warping (Sakoe &amp; Chiba, 1978) and in the biological sequence alignment litera-ture as the Needleman-Wunsch algorithm (Needleman &amp; Wunsch, 1970). The fixed z we use, is the one that maximizes the likelihood of the observations for the current setting of parameters  X  , d ,  X  ( ) . 5 In practice, rather than alternating between complete optimizations over  X  ( ) , d and  X  , we only partially op-timize over  X  ( ) , running only one iteration of the EM algorithm.
 We provide the complete details of our algorithm in the full paper (Coates et al., 2008). For complex dynamical systems, the state z t used in the dynamics model often does not correspond to the  X  X omplete state X  of the system, since the latter could involve large numbers of previous states or unob-served variables that make modeling difficult. 6 How-ever, when we only seek to model the system dynamics along a specific trajectory, knowledge of both z t and how far we are along that trajectory is often sufficient to accurately predict the next state z t +1 . Once the alignments between the demonstrations are computed by our trajectory learning algorithm, we can use the time aligned demonstration data to learn a se-quence of trajectory-specific models. The time indices of the aligned demonstrations now accurately associate the demonstration data points with locations along the learned trajectory, allowing us to build models for the state at time t using the appropriate corresponding data from the demonstration trajectories. 7 To construct an accurate nonlinear model to predict z t +1 from z t , using the aligned data, one could use lo-cally weighted linear regression (Atkeson et al., 1997), where a linear model is learned based on a weighted dataset. Data points from our aligned demonstrations that are nearer to the current time index along the trajectory, t , and nearer the current state, z t , would be weighted more highly than data far away. While this allows us to build a more accurate model from our time-aligned data, the weighted regression must be done online, since the weights depend on the cur-rent state, z t . For performance reasons 8 this may often be impractical. Thus, we weight data only based on the time index, and learn a parametric model in the re-maining variables (which, in our experiments, has the same form as the global  X  X rude X  model, f (  X  )). Con-cretely, when estimating the model for the dynamics at time t , we weight a data point at time t  X  by: 9 where  X  is a bandwidth parameter. Typical values for  X  are between one and two seconds in our experiments. Since the weights for the data points now only depend on the time index, we can precompute all models f t (  X  ) along the entire trajectory. The ability to precompute the models is a feature crucial to our control algorithm, which relies heavily on fast simulation. 5.1. Experimental Setup To test our algorithm, we had our expert helicopter pilot fly our XCell Tempest helicopter (Figure 3), which can perform professional, competition-level ma-neuvers. 10 We collected multiple demonstrations from our expert for a variety of aerobatic trajectories: continuous in-place flips and rolls, a continuous tail-down  X  X ic toc, X  and an airshow, which consists of the following maneu-vers in rapid sequence: split-S, snap roll, stall-turn, loop, loop with pirouette, stall-turn with pirouette,  X  X urricane X  (fast backward funnel), knife-edge, flips and rolls, tic-toc and inverted hover.
 The (crude) helicopter dynamics f (  X  ) is constructed using the method of Abbeel et al. (2006a). 11 The helicopter dynamics model predicts linear and angular accelerations as a function of current state and inputs. The next state is then obtained by integrating forward in time using the standard rigid-body equations. In the trajectory learning algorithm, we have bias terms  X   X  t for each of the predicted accelerations. We use the state-drift variables,  X  k t , for position only. For the flips, rolls, and tic-tocs we incorporated our prior knowledge that the helicopter should stay in place. We added a measurement of the form: where p (  X  ) is a function that returns the position co-ordinates of z t , and  X  (  X  0 ) is a diagonal covariance ma-trix. This measurement X  X hich is a direct observation of the pilot X  X  intended trajectory X  X s similar to advice given to a novice human pilot to describe the desired maneuver: A good flip, roll, or tic-toc trajectory stays close to the same position.
 We also used additional advice in the airshow to in-dicate that the vertical loops, stall-turns and split-S should all lie in a single vertical plane; that the hurri-canes should lie in a horizontal plane and that a good knife-edge stays in a vertical plane. These measure-ments take the form: where, again, p ( z t ) returns the position coordinates of z . N is a vector normal to the plane of the maneu-ver, c is a constant, and  X  (  X  1 ) is a diagonal covariance matrix. 5.2. Trajectory Learning Results Figure 4(a) shows the horizontal and vertical position of the helicopter during the two loops flown during the airshow. The colored lines show the expert pi-lot X  X  demonstrations. The black dotted line shows the inferred ideal path produced by our algorithm. The loops are more rounded and more consistent in the in-ferred ideal path. We did not incorporate any prior knowledge to this extent. Figure 4(b) shows a top-down view of the same demonstrations and inferred trajectory. The prior successfully encouraged the in-ferred trajectory to lie in a vertical plane, while obey-ing the system dynamics.
 Figure 4(c) shows one of the bias terms, namely the model prediction errors for the Z-axis acceleration of the helicopter computed from the demonstrations, be-fore time-alignment. Figure 4(d) shows the result after alignment (in color) as well as the inferred acceleration error (black dotted). We see that the unaligned bias measurements allude to errors approximately in the -1G to -2G range for the first 40 seconds of the airshow (a period that involves high-G maneuvering that is not predicted accurately by the  X  X rude X  model). However, only the aligned biases precisely show the magnitudes and locations of these errors along the trajectory. The alignment allows us to build our ideal trajectory based upon a much more accurate model that is tailored to match the dynamics observed in the demonstrations. Results for other maneuvers and state variables are similar. At the URL provided in the introduction we posted movies which simultaneously replay the differ-ent demonstrations, before alignment and after align-ment. The movies visualize the alignment results in many state dimensions simultaneously. 5.3. Flight Results After constructing the idealized trajectory and models using our algorithm, we attempted to fly the trajectory on the actual helicopter.
 Our helicopter uses a receding-horizon differential dy-namic programming (DDP) controller (Jacobson &amp; Mayne, 1970). DDP approximately solves general con-tinuous state-space optimal control problems by taking advantage of the fact that optimal control problems with linear dynamics and a quadratic reward function (known as linear quadratic regulator (LQR) problems) can be solved efficiently. It is well-known that the so-lution to the (time-varying, finite horizon) LQR prob-lem is a sequence of linear feedback controllers. In short, DDP iteratively approximates the general con-trol problem with LQR problems until convergence, re-sulting in a sequence of linear feedback controllers that are approximately optimal. In the receding-horizon al-gorithm, we not only run DDP initially to design the sequence of controllers, but also re-run DDP during control execution at every time step and recompute the optimal controller over a fixed-length time interval (the horizon), assuming the precomputed controller and cost-to-go are correct after this horizon. As described in Section 4, our algorithm outputs a sequence of learned local parametric models, each of the form described by Abbeel et al. (2006a). Our implementation linearizes these models on the fly with a 2 second horizon (at 20Hz). Our reward function penalizes error from the target trajectory, s  X  t , as well as deviation from the desired controls, u  X  t , and the First we compare our results with the previous state-of-the-art in aerobatic helicopter flight, namely the in-place rolls and flips of Abbeel et al. (2007). That work used hand-specified target trajectories and a sin-gle nonlinear model for the entire trajectory. Figure 5(a) shows the Y-Z position 12 and the collec-tive (thrust) control inputs for the in-place rolls for both their controller and ours. Our controller achieves (i) better position performance (standard deviation of approximately 2.3 meters in the Y-Z plane, compared to about 4.6 meters and (ii) lower overall collective control values (which roughly represents the amount of energy being used to fly the maneuver).
 Similarly, Figure 5(b) shows the X-Z position and the collective control inputs for the in-place flips for both controllers. Like for the rolls, we see that our con-troller significantly outperforms that of Abbeel et al. (2007), both in position accuracy and in control energy expended. Besides flips and rolls, we also performed autonomous  X  X ic tocs X  X  X idely considered to be an even more chal-lenging aerobatic maneuver. During the (tail-down) tic-toc maneuver the helicopter pitches quickly back-ward and forward in-place with the tail pointed toward the ground (resembling an inverted clock pendulum). The complex relationship between pitch angle, hori-zontal motion, vertical motion, and thrust makes it ex-tremely difficult to create a feasible tic-toc trajectory by hand. Our attempts to use such a hand-coded tra-jectory with the DDP algorithm from (Abbeel et al., 2007) failed repeatedly. By contrast, our algorithm readily yields an excellent feasible trajectory that was successfully flown on the first attempt. Figure 5(c) shows the expert trajectories (in color), and the au-tonomously flown tic-toc (black dotted). Our con-troller significantly outperforms the expert X  X  demon-strations.
 We also applied our algorithm to successfully fly a complete aerobatic airshow, which consists of the fol-lowing maneuvers in rapid sequence: split-S, snap roll, stall-turn, loop, loop with pirouette, stall-turn with pirouette,  X  X urricane X  (fast backward funnel), knife-edge, flips and rolls, tic-toc and inverted hover. The trajectory-specific local model learning typically captures the dynamics well enough to fly all the afore-mentioned maneuvers reliably. Since our computer controller flies the trajectory very consistently, how-ever, this allows us to repeatedly acquire data from the same vicinity of the target trajectory on the real helicopter. Similar to Abbeel et al. (2007), we incorpo-rate this flight data into our model learning, allowing us to improve flight accuracy even further. For exam-ple, during the first autonomous airshow our controller achieves an RMS position error of 3.29 meters, and this procedure improved performance to 1.75 meters RMS position error.
 Videos of all our flights are available at: Although no prior works span our entire setting of learning for control from multiple demonstrations, there are separate pieces of work that relate to var-ious components of our approach.
 Atkeson and Schaal (1997) use multiple demonstra-tions to learn a model for a robot arm, and then find an optimal controller in their simulator, initializing their optimal control algorithm with one of the demonstra-tions.
 The work of Calinon et al. (2007) considered learning trajectories and constraints from demonstrations for robotic tasks. There, they do not consider the system X  X  dynamics or provide a clear mechanism for the inclu-sion of prior knowledge. Our formulation presents a principled, joint optimization which takes into account the multiple demonstrations, as well as the (complex) system dynamics and prior knowledge. While Calinon et al. (2007) also use some form of dynamic time warp-ing, they do not try to optimize a joint objective cap-turing both the system dynamics and time-warping. Among others, An et al. (1988) and, more recently, Abbeel et al. (2006b) have exploited the idea of trajectory-indexed model learning for control. How-ever, contrary to our setting, their algorithms do not time align nor coherently integrate data from multiple trajectories.
 While the work by Listgarten et al. (Listgarten, 2006; Listgarten et al., 2005) does not consider robotic con-trol and model learning, they also consider the prob-lem of multiple continuous time series alignment with a hidden time series.
 Our work also has strong similarities with recent work on inverse reinforcement learning, which extracts a re-ward function (rather than a trajectory) from the ex-pert demonstrations. See, e.g., Ng and Russell (2000); Abbeel and Ng (2004); Ratliff et al. (2006); Neu and Szepesvari (2007); Ramachandran and Amir (2007); Syed and Schapire (2008).
 Most prior work on autonomous helicopter flight only considers the flight-regime close to hover. There are three notable exceptions. The aerobatic work of Gavrilets et al. (2002) comprises three maneuvers: split-S, snap-roll, and stall-turn, which we also include during the first 10 seconds of our airshow for com-parison. They record pilot demonstrations, and then hand-engineer a sequence of desired angular rates and velocities, as well as transition points. Ng et al. (2004) have their autonomous helicopter perform sustained inverted hover. We compared the performance of our system with the work of Abbeel et al. (2007), by far the most advanced autonomous aerobatics results to date, in Section 5. We presented an algorithm that takes advantage of multiple suboptimal trajectory demonstrations to (i) extract (an estimate of) the ideal demonstration, (ii) learn a local model along this trajectory. Our algo-rithm is generally applicable for learning trajectories and dynamics models along trajectories from multi-ple demonstrations. We showed the effectiveness of our algorithm for control by applying it to the chal-lenging problem of autonomous helicopter aerobatics. The ideal target trajectory and the local models out-put by our trajectory learning algorithm enable our controllers to significantly outperform the prior state of the art.
 We thank Garett Oku for piloting and building our helicopter. Adam Coates is supported by a Stanford Graduate Fellowship. This work was also supported in part by the DARPA Learning Locomotion program under contract number FA8650-05-C-7261.

