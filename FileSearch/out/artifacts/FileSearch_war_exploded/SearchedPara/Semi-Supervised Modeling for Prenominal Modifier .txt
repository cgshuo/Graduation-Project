 In any given noun phrase (NP), an arbitrary num-ber of nominal modifiers may be used. The order of these modifiers affects how natural or fluent a phrase sounds. Determining a natural ordering is a key task in the surface realization stage of a natural language generation (NLG) system, where the adjectives and other modifiers chosen to identify a referent must be ordered before a final string is produced. For ex-ample, consider the alternation between the phrases  X  X ig red ball X  and  X  X ed big ball X . The phrase  X  X ig red ball X  provides a basic ordering of the words big and red . The reverse ordering, in  X  X ed big ball X , sounds strange, a phrase that would only occur in marked situations. There is no consensus on the ex-act qualities that affect a modifier X  X  position, but it is clear that some modifier orderings sound more natu-ral than others, even if all are strictly speaking gram-matical.

Determining methods for ordering modifiers prenominally and investigating the factors underly-ing modifier ordering have been areas of consider-able research, including work in natural language processing (Shaw and Hatzivassiloglou, 1999; Mal-ouf, 2000; Mitchell, 2009; Dunlop et al., 2010), lin-guistics (Whorf, 1945; Vendler, 1968), and psychol-ogy (Martin, 1969; Danks and Glucksberg, 1971). A central issue in work on modifier ordering is how to order modifiers that are unobserved during sys-tem development. English has upwards of 200,000 words, with over 50,000 words in the vocabulary of an educated adult (Aitchison, 2003). Up to a quar-ter of these words may be adjectives, which poses a significant problem for any system that attempts to categorize English adjectives in ways that are useful for an ordering task. Extensive in-context observa-tion of adjectives and other modifiers is required to adequately characterize their behavior.

Developers of automatic modifier ordering sys-tems have thus spent considerable effort attempting to make reliable predictions despite sparse data, and have largely limited their systems to order modifier pairs instead of full modifier strings. Conventional wisdom has been that direct evidence methods such as simple n-gram modeling are insufficient for cap-turing such a complex and productive process.
Recent approaches have therefore utilized in-creasingly sophisticated data-driven approaches. Most recently, Dunlop et al. (2010) used both dis-criminative and generative methods for estimat-ing class-based language models with multiple-sequence alignments (MSA). Training on manually curated syntactic corpora, they showed excellent in-domain performance relative to prior systems, and decent cross-domain generalization.

However, following a purely supervised training approach for this task is unduly limiting and leads to conventional assumptions that are not borne out in practice, such as the inapplicability of simple n-236 gram models. NP segmentation is one of the most reliable annotations that automatic parsers can now produce, and may be applied to essentially arbitrary amounts of unlabeled data. This yields orders-of-magnitude larger training sets, so that methods that are sensitive to sparse data and/or are domain spe-cific can be trained on sufficient data.

In this paper, we compare an n-gram language model and a hidden Markov model (HMM) con-structed using expectation maximization (EM) with several recent ordering approaches, and demonstrate superior performance of the n-gram model across different domains, particularly as the training data size is scaled up. This paper presents two important results: 1) N-gram modeling performs better than previously believed for this task, and in fact sur-passes current class-based systems. 1 2) Automatic parsers can effectively provide essentially unlimited training data for learning modifier ordering prefer-ences. Our results point the way to larger scale data-driven approaches to this and related tasks. In one of the earliest automatic prenominal mod-ifier ordering systems, Shaw and Hatzivassiloglou (1999) ordered pairs of modifiers, including adjec-tives, nouns ( X  X aseball field X ); gerunds, ( X  X unning man X ); and participles ( X  X eated debate X ). They described a direct evidence method, a transitivity method, and a clustering method for ordering these different kinds of modifiers, with the transitivity technique returning the highest accuracy of 90.67% on a medical text. However, when testing across domains, their accuracy dropped to 56%, not much higher than random guessing.

Malouf (2000) continued this work, ordering prenominal adjective pairs in the BNC. He aban-doned a bigram model, finding it achieved only 75.57% prediction accuracy, and instead pursued statistical and machine learning techniques that are more robust to data sparsity. Malouf achieved an accuracy of 91.85% by combining three systems. However, it is not clear whether the proposed or-dering approaches extend to other kinds of modi-fiers, such as gerund verbs and nouns, and he did not present analysis of cross-domain generalization.
Later, Mitchell (2009) focused on creating a class-based model for modifier ordering. Her system mapped each modifier to a class based on the fre-quency with which it occurs in different prenominal positions, and ordered unseen sequences based on these classes. Dunlop et al. (2010) used a Multiple Sequence Alignment (MSA) approach to order mod-ifiers, achieving the highest accuracy to date across different domains. In contrast to earlier work, both systems order full modifier strings.

Below, we evaluate these most recent systems, scaling up the training data by several orders of mag-nitude. Our results indicate that an n-gram model outperforms previous systems, and generalizes quite well across different domains. Following Dunlop et al. (2010), we use the Wall St. Journal (WSJ), Switchboard (SWBD) and Brown corpus sections of the Penn Treebank (Marcus et al., 1993) as our supervised training and testing base-lines. For semi-supervised training, we automati-cally parse sections 02-21 of the WSJ treebank using cross-validation methods, and scale up the amount of data used by parsing the New York Times (NYT) section of the Gigaword (Graff and Cieri, 2003) cor-pus using the Berkeley Parser (Petrov and Klein, 2007; Petrov, 2010).

Table 1 lists the NP length distributions for each training corpus. The WSJ training corpus yields just under 5,100 distinct modifier types (without normal-izing for capitalization), while the NYT data yields 105,364. Note that the number of NPs extracted from the manual and automatic parses of the WSJ are quite close. We find that the overlap between the two groups is well over 90%, suggesting that extract-237 ing NPs from a large, automatically parsed corpus will provide phrases comparable to manually anno-tated NPs.

We evaluate across a variety of domains, includ-ing (1) the WSJ sections 22-24, and sections com-mensurate in size of (2) the SWBD corpus and (3) the Brown corpus. Table 2 lists the NP length distri-butions for each test corpus. In this section, we present two novel prenominal modifier ordering approaches: a 5-gram model and an EM-trained HMM. In both systems, modifiers that occur only once in the training data are given the Berkeley parser OOV class labels (Petrov, 2010).
In Section 5, we compare these approaches to the one-class system described in Mitchell (2010) and the discriminative MSA described in Dunlop et al. (2010). We refer the interested reader to those pa-pers for the details of their learning algorithms. 4.1 N-Gram Modeling We used the SRILM toolkit (Stolcke, 2002) to build unpruned 5-gram models using interpolated mod-ified Kneser-Ney smoothing (Chen and Goodman, 1998). In the testing phase, each possible permuta-tion is assigned a probability by the model, and the highest probability sequence is chosen.

We explored building n-gram models based on entire observed sequences (sentences) and on ex-tracted multiple modifier NPs. As shown in Table 3, we found a very large (12% absolute) accuracy improvement in a model trained with just NP se-quences. This is likely due to several factors, in-cluding the role of the begin string symbol &lt; s &gt; , which helps to capture word preferences for occur-ring first in a modifier sequence; also the behav-ior of modifiers when they occur in NPs may dif-fer from how they behave in other contexts. Note that the full-sentence n-gram model performs sim-ilarly to Malouf X  X  bigram model; although the re-sults are not directly comparable, this may explain the common impression that n-gram modeling is not effective for modifier ordering. We find that syntac-tic annotations are critical for this task; all n-gram results presented in the rest of the paper are trained on extracted NPs.
 4.2 Hidden Markov Model Mitchell X  X  single-class system and Dunlop et. al X  X  MSA approach both group tokens into position clus-ters. The success of these systems suggests that a position-specific class-based HMM might perform well on this task. We use EM (Dempster et al., 1977) to learn the parameterizations of such an HMM.
The model is defined in terms of state transition probabilities P( c 0 | c ) , i.e., the probability of transi-tioning from a state labeled c to a state labeled c 0 ; and state observation probabilities P( w | c ) , i.e., the probability of emitting word w from a particu-lar class c . Since the classes are predicting an or-dering, we include hard constraints on class tran-sitions. Specifically, we forbid a transition from a class closer to the head noun to one farther away. More formally, if the subscript of a class indicates its distance from the head, then for any i,j , P( c i | c ) = 0 if i  X  j ; i.e., c i is stipulated to never occur closer to the head than c j .

We established 8 classes and an HMM Markov order of 1 (along with start and end states) based on performance on a held-out set (section 00 of the WSJ treebank). We initialize the model with a uni-form distribution over allowed transition and emis-sion probabilities, and use add- X  regularization in the M-step of EM at each iteration. We empirically determined  X  smoothing values of 0.1 for emissions and 500 for transitions. Rather than training to full convergence of the corpus likelihood, we stop train-ing when there is no improvement in ordering accu-racy on the held-out dataset for five iterations, and output the best scoring model.

Because of the constraints on transition probabil-ities, straightforward application of EM leads to the transition probabilities strongly skewing the learn-ing of emission probabilities. We thus followed a generalized EM procedure (Neal and Hinton, 1998), updating only emission probabilities until no more improvement is achieved, and then training both emission and transition probabilities. Often, we 238 find no improvement with the inclusion of transition probabilities, and they are left uniform. In this case, test ordering is determined by the class label alone. Several measures have been used to evaluate the accuracy of a system X  X  modifier ordering, includ-ing both type/token accuracy, pairwise accuracy, and full string accuracy. We evaluate full string ordering accuracy over all tokens in the evaluation set. For every NP, if the model X  X  highest-scoring ordering is identical to the actual observed order, it is correct; otherwise, it is incorrect. We report the percentage of orders correctly predicted.

We evaluate under a variety of training conditions, on WSJ sections 22-24, as well as the testing sec-tions from the Switchboard and Brown corpus por-tions of the Penn Treebank. We perform no domain-specific tuning, so the results on the Switchboard and Brown corpora demonstrate cross-domain appli-cability of the approaches. 5.1 Manual parses versus automatic parses We begin by comparing the NPs extracted from manual parses to those extracted from automatic parses. We parsed Wall Street Journal sections 02 through 21 using cross-validation to ensure that the parses are as errorful as when sentences have never been observed by training.

Table 4 compares models trained on these two training corpora, as evaluated on the manually-annotated test set. No system X  X  accuracy degrades greatly when using automatic parses, indicating that we can likely derive useful training data by automat-ically parsing a large, unlabeled training corpus. 5.2 Semi-supervised models We now evaluate performance of the models on the scaled up training data. Using the Berkeley parser, we parsed 169 million words of NYT text from the English Gigaword corpus (Graff and Cieri, 2003), extracted the multiple modifier NPs, and trained our various models on this data. Rows 3-6 of Table 4 show the accuracy on WSJ sections 22-24 after training on 10%, 20%, 50% and 100% of this data. Note that this represents approximately 150 times the amount of training data as the original treebank training data. Even with just 10% of this data (a 15-fold increase in the training data), we see across the board improvements. Using all of the NYT data results in approximately 5% absolute performance increase for the n-gram and MSA models, yielding roughly commensurate performance, over 92% ac-curacy. Although we do not have space to present the results in this paper, we found further improve-ments (over 1% absolute, statistically significant) by combining the four models, indicating a continued benefit of the other models, even if none of them best the n-gram individually.

Based on these results, this task is clearly amenable to semi-supervised learning approaches. All systems show large accuracy improvements. Further, contrary to conventional wisdom, n-gram models are very competitive with recent high-accuracy frameworks. Additionally, n-gram models appear to be domain sensitive, as evidenced by the last row of Table 4, which presents results when the 1.8 million NPs in the NYT corpus are augmented with just 11 thousand NPs from the WSJ (auto) col-lection. The n-gram model still outperforms the other systems, but improves by well over a percent, while the class-based HMM and MSA approaches 239 are relatively static. (The single-class system shows some domain sensitivity, improving nearly a point.) 5.3 Cross-domain evaluation With respect to cross-domain applicability, we see that, as with the WSJ evaluation, the MSA and n-gram approaches are roughly commensurate on the Brown corpus; but the n-gram model shows a greater advantage on the Switchboard test set when trained on the NYT data. Perhaps this is due to higher re-liance on conventionalized collocations in the spo-ken language of Switchboard. Finally, it is clear that the addition of the WSJ data to the NYT data yields improvements only for the specific newswire domain  X  none of the results change much for these two new domains when the WSJ data is included (last row of the table).

We note that the improvements observed when scaling the training corpus with in-domain data per-sist when applied to very diverse domains. Interest-ingly, n-gram models, which may have been consid-ered unlikely to generalize well to other domains, maintain their superior performance in each trial. In this paper, we demonstrated the efficacy of scal-ing up training data for prenominal modifier or-dering using automatic parses. We presented two novel systems for ordering prenominal modifiers, and demonstrated that with sufficient data, a simple n-gram model outperforms position-specific models, such as an EM-trained HMM and the MSA approach of Dunlop et al. (2010). The accuracy achieved by the n-gram model is particularly interesting, since such models have previously been considered inef-fective for this task. This does not obviate the need for a class based model  X  modifier classes may in-form linguistic research, and system combination still yields large improvements  X  but points to new data-rich methods for learning such models.
 This research was supported in part by NSF Grant #IIS-0811745 and DARPA grant #HR0011-09-1-0041. Any opinions, findings, conclusions or recom-mendations expressed in this publication are those of the authors and do not necessarily reflect the views of the NSF or DARPA.
 nologies 2007: The Conference of the North American 240
