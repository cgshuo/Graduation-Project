 Question Answering (QA) and Information Retrieval (IR) tasks. Query-focused sum-DUC conference. task there are 10 document-sets with 25 documents and a single query in each. Every generate summaries for the subsets in each document-set conforming to the query and the summary generated for one subset cannot contain information from previous sub-set(s). There are three levels of task: 1) a ssuming reader has read nothing, generate a in the first document-set, generate a summary for the second document-set; 3) assum-ing reader has already read the former two document-sets, generate a summary for the third document-set [4]. This pilot task shows the update-style summarization. 
The precondition of update-style summarization is that readers are reading a series of documents about the same topic or topics re lated closely and they need a piece of without information they have already known from documents they read before, which we call previous documents. The update-style summarization can filter redun-dancies while compressing information and thus make information retrieving more efficient. 
Our summarization system was initially designed for DUC2005 and was further developed for DUC 2006. For DUC2007 we carried out a number of necessary im-provements aimed at enhancing its efficiency and performance. After DUC2007 we strategy and update-style summary generation. 
We use the existing lexical chain algorithm optimized by Barzilay and Elhadad in cations have been made, for ameliorating its efficiency and adapting it to general and update-style query-focused multi-document summarization. 
The remaining sections of this paper are organized as follows: in section 2, the re-tion of whole summarization process in section 3. We describe the focus of our study 4 and section 5 and discuss the evaluation in Section 6. Lastly we conclude this paper in Section 7. highly complex by the researchers and their systems in the last decade. X  [14] 
The process of summarization has been generally divided into two steps in current intermediate representation to generate a coherent summary of the source text. [8] 
In the first step most summarization approach es can be generally classified into the following three categories according to the semantic analysis level: [19] 1. Based on extraction. These methods analyze global statistical feathers (word 2. Based on simple semantic analysis. Take Lexical Chain for example, it first con-3. Based on deep semantic analysis. For example, Marcu [11] proposed an 
Obviously methods based on deep semantic analysis can offer the best opportunity to create an appropriate and fluent summary. The problem with such methods is that they rely on detailed semantic representations and domain specific knowledge bases. 
And the major problem with methods based on extraction is that they do not take context into account. Specifically, finding the about-ness of a document relies largely component in a coherent text [5]. Morris and Hirst first gave a logical description of the implementation of lexical chain using Roget dictionary [13]. It has been used in a variety of IR and NLP applications Afterwards Hirst and Onge used WordNet for lexical chain construction and adopted [7]. An optimized strategy was put forward by Barzilay and Elhadad in [1] to insure that all senses of a candidate word be prope rly considered. It was applied to generate coherent summaries for single document summarization. Barzilay and Elhadad also on the number of  X  X trong X  chain words they contain. X  [3] lecting candidate words for chain building, 2) constructing and scoring chains to rep-summary. And before main steps we need preprocessing to transform raw documents ure 1 shows our system architecture. candidate words and build lexical chains (single chains and multi chains). Thus, in this paper we mainly focus on chain scoring step and update-style summary generation step. multi-document summarization, but it doesn X  X  suit query-focused summarization well. Chain with highest score calculated by this strategy means that the theme represented Therefore we propose a new chain scoring strategy described below. 1. Select all noun words from query and retrieve their senses using WordNet. the word and choosing the highest one). Record this similarity score as chain score on this query word. 3. Repeat 2) until all noun words in query have been calculated. 4. Accumulate chain scores on all query words as the final score of the chain. 5. Repeat 2), 3) and 4) until all chains have been calculated. 
Step 1) is just like candidate words selectio n procedure. Step 2), 3) and 4) calculate sures all chains are calculated. We evaluate both strategies in our experiments to give a comparison. between new information and information already known. We have experimented strategy. 
For convenience we call chain set from ca ndidate document-set the candidate chain set and chain set from previous document-set the previous chain set. 5.1 Simple Strategy Assuming in candidate document-set info rmation already known, namely candidate ment-set and previous document-set(s), but only extract sentences from candidate document-set. In other words we process candidate documents and previous docu-ments together while constructing chains but separately while generating summaries. ment-set, which ought to be summarized, to generate the summary. 
By this way we treat update-style summary generation as general summarization having external chains. We use previous document-set (previous chain set), to help our intermediate representation construction without filtering information they contain. DUC2007. 5.2 Chain Filtering Strategy candidate chain set into two parts by comparing chains from the candidate chain set to information new for reader. In the other part chains are related to the previous chain set and considered containing information r eader has already read. We give a bounty (5.0 by default) on score to chains in the first parts while extracting sentences to gen-erate summaries. It is carried out by the following procedure. 1. Select the first chain not been processed from the candidate chain set. 2. For the first chain in the previous chain set calculate similarity between it and the chain selected from candidate chain set in 1). 3. For the next chain in the previous chain set calculate similarity between it and the chain selected in 1) these similarities and choose the highest one as the similarity between the chain se-lected in 1) and the previous chain set. 5. Select the next chain not been processed from the candidate chain set and repeat 2), 3) and 4). 6. Repeat 5) until all chains in the candidate chain set have been processed. into two parts. Chain has a similarity to the previous chain set less than the criteria will take the bounty. 
Step 2), 3) and 4) traverse all chains in the previous chain set to calculate the high-lated. Step 7) divides the candidate chain set into two parts for further process. 
The method calculating similarity between chains mentioned in step 2) imple-mented as follows. 1. Prepare two chains to be calculated, chain A and chain B. 2. For the first element in chain A calculate similarities between it and each element in chain B by WordNet. Take the highest one. 3. For the next element in chain A calculate similarities between it and each element in chain B. Take the highest one. 4. Repeat 3) until all elements in chain A have been calculated. way. 6. Calculate average of highest values of all elements in chain A and chain B taken in chain B. Step 2), 3) and 4) calculate similarities between chain B and each element in chain tween two chains utilizing results from previous steps. 5.3 Summary Generation entities extracted in preprocessing. candidate sentence, S(query) is the sum of the co-occurrences of key words in a query and the sentence, and S(named entity) is the number of name entities existing in both the query and the sentence. 
The upper formula is used for calculating sentence score with chains built by gen-eral chain scoring strategy and the lower one with chains built by new chain scoring with query terms, there is no need to calculate query score again in sentence scoring procedure. until reaching the word number limit for a summary. 
In our experiments, S(query) and S(named entity) are found to affect the system X  X  performance remarkably. Empirically, for the upper formula the three coefficients  X  ,  X  are set to 0.5 and 0.5, respectively. 6.1 Data Set We use the DUC2007 update task dataset for evaluation in the experiments. The up-summaries with a length of approximately 100 words or less. The dataset has already the dataset. Documents in each document-set in this dataset are topic-related and each news reports and are in XML format. 6.2 Evaluation Toolkit In our experiments we use the ROUGE toolkit for evaluation, which is widely adopted by DUC for automatic summarization evaluation. ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It measures summary quality by count-ing overlapping units such as the n-gram, word sequences and word pairs between the candidate summary and the reference summary (generated manually). 
ROUGE toolkit reports separate scores for 1, 2, 3 and 4-gram, and also for longest common subsequence co-occurrences. Among the scores, bi-gram (ROUGE-2), 4-gram (ROUGE-4) and skip-4-gram co-occurrence (ROUGE-S4 &amp; ROUGE-SU4) perform best for multi-document summarizat ion according to Lin X  X  conclusion in [10]. And DUC takes ROUGE-2 and ROUGE-SU4 for evaluation criteria. For con-venience and impartiality we use the same criteria as DUC does. 6.3 General Summary Evaluations In the first level in DUC2007 update task participants need to generate summaries for the first document-set and there isn X  X  any pervious document. Thus this level of task can be considered as general query-focused multi-document summarization and we can evaluate our chain scoring strategy without the influence of update-style summary generation. 
Table 3 shows resulting scores of our summarizer with new scoring strategy gener-chain scoring strategy), the best system X  X  performance and baselines in DUC2007 extracted from DUC 2007 update task evaluation results table. And they are visually compared in Figure 2. 
From the table and figure above we can see clearly that new chain scoring strategy improves system X  X  performance in evidence. 6.4 Update-Style Summary Evaluation There are two groups of parameters in update-style experiments, the critical value in should take bounty. Lower CV means less chains take bounty and higher means more should be inserted into chain. Lower CSS means longer chains and higher mean shorter (details described in Section 2.2.1). 
We have experimented with CV 0.1, 0.3, 0.5, 0.67 and 0.8 respectively, and CSS 0.3, 0.5, 0.67 and 0.8. We abstract the highest one and the mean from these twenty groups of ROUGE resulting scores and compare them to DUC baselines, our evaluation re-performances according to their workshop papers in DUC2007 [6][16][17][18]. Table 4 and Figure 3 show the comparison. 
The table and figure above state that the chain filtering strategy does work with up-date-style summarization. But the performance improvement enhanced by this new divide document into themes accurately enough, which causes extra noise during implementing the new strategy. Experiment results on different parameters are shown in Figure 3. The left figure in Figure 3 shows system X  X  ROUGE-2 and ROUGE-SU4 scores with respect to different with respect to different CSS (taking CV of 0.1). Seen from Figure 4, while critical value going higher, the ROUGE scores increase. too low leads longer chains being built and more themes being contained in one chain, and one theme being broken up and distributed into several chains, which brings more noise, too. In this paper we proposed a novel chain scoring method and an update-style summari-tween chains, and then treat them separately during summary generating. Experimen-tal results on the DUC 2007 update task dataset demonstrate the performance im-while scoring chains can adapt Lexical Chain algorithm to query-based summariza-tion better. And by introducing chain filtering summarizer based on Lexical Chain is adapted for update-style summarization. 
There are more implementations than chain filtering strategy in this study. In future ment some kind of complex method for better performance. Acknowledgments. This work is partially supported by National Natural Science 863 project of China with the contract No. 2006AA010108. 
