
We present a new framework for classifier fusion that uses a shared sampling distribution for obtaining a weighted classifier ensemble. The weight update process is self regularizing as subsequent classifiers trained on the disjoint views rectify the bias introduced by any classifier in preceding iterations. We provide theoretical guarantees that our approach indeed provides results which are bet-ter than the case when boosting is performed separately on different views. The results are shown to outperform other classifier fusion strategies on a well known texture image database. Given a set of training points X = { x 1 ,x 2 .....x N } and M disjoint features available for each point Each member x j i in the set x i is known as a view of point x .A view may be thought of as a representation of point x using disjoint feature sets. For instance, for a color im-age, each training point x i may be thought of as a set of three views each consisting of three disjoint features ob-tained from the intensities of Red, Green and Blue color components. In this case, the number of views will be three and we can represent the three views of the point x as { x R i ,x G i ,x B i , } . Similarly, for a moving target cap-tured using visible range and infra red sensors the num-ber of views available for each training point in the train-ing set will be two. The idea may be extended to any conceivable situation such as face recognition (many views of the same face), biometric authentication (different bio-metrics such as a mugshot, iris data and finger print for the same person) and automatic target recognition (visi-ble range and infra red images of the targets). The goal of classifier fusion is to obtain a classifier C such that C learns from all the views available for each training point and has classification accuracy that is better than the case when only one view is available. One can ask how help-ful could introducing additional views be? A toy exam-ple can be used to illustrate this simple concept. In Fig-ure 1(a,b and c), two classes (circles and squares) are dis-played on the XY, YZ and XZ planes. It is not always pos-sible to separate the classes using information from a sin-gle view. On the other hand, if information from all the views is combined, a better classification performance may be acheived. It is generally known that a good fusion algo-rithm outperforms or at least performs as well as the indi-vidual classifiers [15]. Considerable research in the pattern recognition field is focused on fusion rules that aggregate the outputs of the first level experts and make a final deci-sion. Various techniques for fusion of expert observations such as linear weighted voting, the naive Bayes classifiers, the kernel function approach, potential functions, decision trees or multilayer perceptrons have been proposed in re-cent years. [7], [12],[13],[6]. Other approaches are based on bagging, boosting, and arching classifiers [1], [2], [3]. Comprehensive surveys of various classifier fusion studies and approached can be found in [9], [10], [12] and [11]. Kim et al [8]present prototype reduction schemes and clas-sifier fusion strategies to optimize kernel-based nonlinear subspace methodsby subdividing the data into smaller sub-sets, and utilizing a Prototype Reduction Scheme (PRS) as a preprocessing module, to yield more refined representative prototypes. A Classifier Fusion Strategy (CFS) is invoked as a postprocessing module, to combine the individual KNS classification results to derive a consensus decision. In [11] various classifier fusion strategies such as minimum, max-imum, average, majority vote and oracle are discussed and the results have been compared. Lanckriet et al. introduce in [5] a kernel-based data fusion approach for protein func-tion prediction in yeast. The method presented in that pa-per combines multiple kernel representations in an optimal fashion by formulating the problem as a convex optimiza-tion problem that can be solved using semidefinite program-ming techniques. Kuncheva et al. [14] discuss the effect of dependence between individual classifiers in classifier fu-sion. They study the limits on the majority vote accuracy when combining dependent classifiers. A Q statistics based measure has been proposed to quantify the dependence be-tween the classifiers. It is shown that dependent classifiers could offer a dramatic improvement over the individual ac-curacy. A synthetic experiment demonstrates the intuitive result that, in general, negative dependence is preferable. In [19] Wu and Chang address two relevant issues related to sequence-data mining, sequence-data representation, and representation-to-semantics mapping. They propose repre-senting sequence data in multiple views. For each represen-tation, they introduce methods to construct a valid kernel as the distance function to measure similarity between se-quences. For mapping, they find the best combination of the individual distance functions, which measure similar-ity of different views, to depict the target semantics. They propose a super-kernel function-fusion scheme to achieve the optimal mapping. In this paper, we present a classi-fier fusion strategy that performs classification using weak learners trained on different views of the training data. The final ensemble contains learners that are trained to focus on different views of the test data. The combination weights for the final weighting rule are obtained using a shared sampling distribution. In each iteration, a weak learner is greedily selected from the pool of weak learners trained on disjoint views. This results in a minimization of the train-ing error for the final hypothesis. We show that a lower training and generalization error bound can be achieved if a shared sampling distribution is used and weak learner from the lowest error view is selected. We support this argument with empirical studies performed on a benchmark texture image dataset from [18]. The rest of the paper is organized as follows: Section 2 explains the proposed technique in detail. A tighter bound on training error and the generaliza-tion error for our algorithm is derived in section 3. In the remaining sections, the experimental results are presented and discussed.
AdaBoost has been shown to improve the prediction ac-curacy of weak classifiers using an iterative weight update process [3]. The technique combines weak classifiers (clas-sifiers having classification accuracy slightly greater than that of chance) in a weighted vote fashion giving an overall strong classifier. Detailed explanation of the AdaBoost al-gorithm is skipped here for brevity, interested readers may refer to [16], [17] and [4] for more on AdaBoost. One of the ways boosting may be used for classifier fu-
Algorithm 1 : Boosting With Shared Sampling Distri-bution
Input : 1 . N training examples in a training set S . 2 . M views available for each training point and hence M training sets such that where j =1 ....M and y i  X  X  +1 ,  X  1 } and each ( x j the i th training example.

Initialization The weights of the training examples are initialized to W 1 = 1 N , therefore respective views for all the training examples are assigned equal weights resulting in a uniform distribution W 1 ( i ) . for k =1 to k max do
Figure 1. Analogy between using additional information from three different views and us-ing information from three different dimen-sions sion would be to run boosting separately on each view, ob-tain separate ensembles for each view and take a majority vote among the ensembles when presented with test data. In this case, separate training of classifiers is needed for each view and the sampling distributions of the data points are also disjoint. Unlike this approach, we perform separate training for each view but the training error computation and sampling of training examples is done using a shared distribution of example weights in a given iteration. The training algorithm is shown in Algorithm 1.

In the initialization step of Algorithm 1, all the views for a given training point are initialized with the same weight. To understand this we go back to the RGB component ex-ample. Suppose we have N training examples each having three disjoint views such that a given training example x can be represented as .

Weak learners h R , h G and h B will be trained on the training sets , and such that
Since the sampling distribution for all views of a given example is shared, the sampling weight of the the R, G and B views of example x i in iteration k are given by
After a classifier h  X  k with lowest error rate  X  k is selected in step 4 of Algorithm 1 and combination weight  X   X  k is obtained, the weights of the views are updated. As a conse-quence of the shared sampling distribution, weights for all views of a given training example are updated in manner that will be explained using the toy example.

Suppose that the classifier h G k is the lowest error rate classifier among all the views in iteration k . Then G k is the minimum among the error rates for all the views and  X  G k the corresponding combination weight (In Algorithm 1, the best weak learner and the corresponding error rate and com-bination weight are represented by a more general notation i.e. h  X  k ,  X  k and  X   X  k ). The sampling weights for the R,G and B views will be updated as w It should be noted here that weights of R, G and B views of a training example x i are updated based whether h G k classifies view x G i correctly or not. It may be possible that classifiers k and h tively but the opinion of the winning weak learner h G k will override the opinion of the losing weak learners h R k and h As a result, the sampling distribution of the weights remains the same for all views.
As discussed in Algorithm 1 in the previous section, the weights of all views of the training example are updated using the same exponential cost function value. Since the same distribution is used for sampling training examples and updating the weights for all the views, the weights for each view in subsequent iterations will be given by
Multiplying the equations and performing some simpli-fications gives us
Since the starting weights for all training examples are all equal to 1 N , Equation( 1), can be rewritten as
If the final hypothesis H assign a label to one of the training examples that is different from the ground truth i.e. I [
H ( x i ) = y i ] , this would mean that y i F ( x i )  X  0
Combining ( 2) and ( 3) we get 2.2 Justification for the choice of  X  k
We prove along the same lines as Schapire and Fre-und [17]. From ( 5), it can be seen that to minimize the training error,  X  k max k =1 Z  X  k should be minimized. Suppose that boosting is performed independently on each view, let us assume in this case for simplicity of notation k fixed, u Finding the value of combination weight  X  which mini-mizes Z can be done as follows. Since we know that
If we minimize the right hand side of Equation ( 6) the value of  X  obtained is Therefore ( 6) becomes
If the value of  X  k for each iteration k is given by Equa-tion ( 7), then the training error bound on hypothesis H is at most
From ( 9), we conclude that we need to find a hy-pothesis h k during each iteration that maximizes p k = ing p k results in maximum value of  X  k . Since maximum  X  k corresponds to minimum k , the value of  X  Step 5 of Algorithm 1 is the optimal value of combination coefficient in each iteration. Hence for an ensemble of clas-sifiers that fuses M distinct views, the bound on the training error is given by where  X  k = 1  X  p  X  k 2 . In Equation ( 10), h  X  k is the hypoth-esis from one of the M views at iteration k , such that = min { 1
Let us consider the final hypothesis H , misclassifies a training example ( x i ,y i ) . In this case, H ( x i ) = y
F ( x i )  X  0 , as a result
When the final hypothesis H correctly classifies the training example ( x i ,y i ) ,wehave H ( x i )= y i and hence y
F ( x i )  X  0 , as a result we have
Computing the training error bound along the lines sug-gested by Schapire and Singer [17], we take the sum over all training points N for ( 11)
The right hand side of ( 11) is positive quantity. If N mis is the number of training examples that are misclassified then we know that N mis  X  N . The summation represented by the right hand side of ( 12) includes the exponential terms corresponded to the correctly classified training examples as well as the wrongly classified training examples. The bound can be made tighter by subtracting the sum of ex-ponential terms corresponding to the correctly classified in-stances. Therefore ( 12) can be rewritten as
As a result of this, we can rewrite ( 10) as
Freund and Schapire [4] define the margin of the training example ( x i ,y i ) as
Hence ( 14) becomes
For an ensemble of classifiers that fuses M distinct views, the bound on the training error is given by
The generalization error is defined as the probability of misclassifying a new example, [4]. Since the final hypothe-sis of our algorithm is then the final hypothesis output will not be affected by the division of F(x) by a positive quantity, namely k max k =1 Let us define The generalization error bound of our algorithm is a gener-alization for multiple views of the AdaBoost error bound. The proof of our bound follows the lines of that introduced by Schapire at al.in [16]. Given H the space from where the base hypothesis are chosen, the function f ( x ) , as defined above, clearly belongs to the convex hull C of H . C is the set of mappings that can be generated by taking a weighted average of hypothesis from H : C = { f : x  X  Throughout the rest of the paper the notation P ( x,y )  X  W will mean the probability of the event A when the exam-ple (x,y) is sampled according to W , and P ( x,y )  X  S [ mean the probability with respect to sampling uniformly at random an example from the training set. Their abrevia-tion will be P W [ A ] and P S [ A ] .The expected value will be denoted E W [ A ] and E S [ A ] .

Theorem 1 Let S be a sample of N examples chosen inde-pendently at random according to W. Assume that the base hypothesis space H has the VC-dimension d and let  X &gt; 0 Then with probability at least 1  X   X  over the random choice of the training set S, every weighted average function f  X  X  satisfies the following bound for all  X &gt; 0 : The proof of Theorem 1 can be found in [16].
 The empirical error bound for the shared sampling distri-bution based algorithm for fusion of weak learners for M views is provided by Theorem 2.

Theorem 2 Given the weighted training errors at it-eration k for hypothesis corresponding to the M views Then for any  X  , we have that
Proof: It is obvious that if yf ( x )  X   X  then
Hence and P
In order to evaluate the proposed fusion technique, five binary class data sets have been generated from the MIT VisTex texture images database benchmark [18]. This data-base is a collection of color pictures of 18 different nat-ural objects. The images that we use for evaluating our algorithm are from the following categories: Fabric01, Fab-ric02, Fabric03, Fabric04, Fabric05, Flowers04, Flowers05, Flowers06, Food01 and Food02. Some sample images from
Figure 2. Sample images from each binary data set (each row represents samples from each dataset). each binary data set are illustrated in Figure 2. The R, G and B components from each image are extracted and Principal Component Analysis is used in order to reduce the dimen-sionality of the individual components and throw away the features that are irrelevant. The size of each data set and the corresponding reduced number of dimensions (after ap-plying PCA) are shown in Table I for each view. In Table I, Data Set 1 includes 144 vectors with 112 dimensions for the R component, 144 vectors with 113 dimensions for the G component and 144 vectors with 112 dimensions for the B component.

Experimental results and comparison with other fusion techniques are presented in Tables II X  X I. The results are the average accuracy of 10 tests, each time the data sets being randomly partitioned such that half of the data is in the train-ing set and the remaining half is in the test set. The average accuracy of individual classifier from each view before fu-sion is shown in the columns A R , A G and A B . We used a modified C4.5 as a weak learner. In order to make C4.5 a weak learner, we allow each node to tolerate a certain de-gree of impurity, which is 20% of the training set size. The number of base learners for boosting was 30. We empiri-cally compare our technique with fusion methods proposed by Wu and Chang in [19] and majority vote. The SVM al-gorithm has two procedural parameters:  X  and C, the soft margin parameter. Ten-fold cross-validation was used for model selection, taking  X  values in [10  X  2 , 10 2 ] and C in [10  X  2 , 10 2 ] . The majority vote is also used for fusion of expert observations for the fusion techniques SVM X  X V in which SVM has been used as classifier for each view.
We have shown both theoretically and empirically that a strong classifier can be obtained by fusing classifiers from multiple views. We have taken a multiview approach, that performs classifier selection based on the lowest training er-ror among the views. In each iteration, sampling and weight update is performed using a shared sampling distribution. As a result, the weights for all views of a given training ex-ample are updated according to the opinion of the classifier from the lowest error view. In the first few iterations, weak learners from only those views give a good classification ac-curacy that provide a good separability among the training points. So in the first iteration, weak learner from that view is likely to be lowest error weak learner that inherently is a more separable view.

As the number of iterations progress, the training points which are difficult to classify from the winning views in previous iterations accumulate high values of sampling weights. In subsequent iterations, examples will be sampled according to a distribution which is more likely to sample examples that were deemed difficult by previously winning views. A winning classifier is less likely to emerge from the views that were winning views in previous iterations. This results in a cyclic behavior and the classifiers that lose for some iterations eventually have a low weighted error rate and become a winning classifier for a given iteration. This cyclic behavior continues until there is a consensus between the classifiers over which training examples are difficult to classify. In the end , all weak learners from all classifiers agree on what examples are difficult and try to achieve a low error rate.

The proposed algorithm has been evaluated on a well known benchmark texture image dataset. The performance of the algorithm has been compared with other classifier fu-sion algorithms and its superiority over other fusion tech-niques has been established on most data sets with &gt; 95% confidence using a two X  X ided paired T-test with 18 degrees of freedom.
 = 112 ,D . 82 . 81 . 797 comparable
