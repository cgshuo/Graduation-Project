 In this paper, we focus on the problem of extending a given knowledge base by accurately predicting additional true facts based on the facts included in it. This is an essential prob-lem of knowledge representation systems, since knowledge bases typically suffer from incompleteness and lack of abil-ity to reason over their discrete entities and relationships. To achieve our goals, in our work we introduce an inducing space nonparametric Bayesian large-margin inference model, capable of reasoning over relationships between pairs of enti-ties. Previous works addressing the entity relationship infer-ence problem model each entity based on atomic entity vec-tor representations. In contrast, our method exploits word feature vectors to directly obtain high-dimensional nonlin-ear inducing space representations for entity pairs. This way, we allow for extracting salient latent characteristics and in-teraction dynamics within entity pairs that can be useful for inferring their relationships. On this basis, our model performs the relations inference task by postulating a set of binary Dirichlet process mixture large-margin classifiers, presented with the derived inducing space representations of the considered entity pairs. Bayesian inference for this inducing space model is performed under the mean-field in-ference paradigm. This is made possible by leveraging a recently proposed latent variable formulation of regularized large-margin classifiers that facilitates mean-field parameter estimation. We exhibit the superiority of our approach over the state-of-the-art by considering the problem of predict-ing additional true relations between entities given subsets of the WordNet and FreeBase knowledge bases.
 G.3 [ Probability and Statistics ]: Nonparametric statis-tics; I.2.6 [ Learning ]: Induction c  X  Knowledge bases; large-margin classifiers; mean-field infer-ence; Dirichlet process.
Knowledge bases are relational databases designed for knowl-edge management, collection, and retrieval. Recently, signif-icant research effort has been devoted to building large web-based knowledge bases, which encompass a huge amount of data regarding general or specific-domain knowledge; Word-Net [1], Freebase [2] and Yago [3] are just a few characteris-tic such examples. Each of these knowledge bases has been developed with the goal to address a specific problem, for instance producing an intuitively usable dictionary and the-saurus, and proposing a global on-line information resource for semantic web applications. Nevertheless, it is now widely accepted that such knowledge bases could be also leveraged to perform a large number of diverse data mining applica-tions, e.g. question answering (Siri), query expansion [4], coreference resolution [5], and information retrieval [6].
Despite the potential of such systems, this kind of applica-tions is greatly hindered by inherent inadequacies of knowl-edge bases, namely their incompleteness and the fact that they do not explicitly provide appropriate reasoning capa-bilities. To resolve these issues, several researchers have pro-posed approaches for knowledge base extension based on big text data inference and analysis methods. Nevertheless, the lack of adequate textual information to base the knowledge base extension procedure upon has rendered such attempts rather unsuccessful [7, 8, 9]. Inspired from these drawbacks, a complementary approach widely adopted in the related lit-erature consists in predicting the likely truth of additional facts based on existing facts in the given knowledge bases.
To effect this task, existing approaches resort to represent-ing the entities (objects or individuals) in the given knowl-edge bases as feature vectors; [10, 11, 12] are characteris-tic such works. In these methods, (optimal) vector repre-sentations are learned for the given entities and are subse-quently used to train classification models for performing the relation inference task. In a recent modification of this paradigm, [13] represented the given entities as combina-tions of their constituent word vectors, with the latter being learned from the available data; the so-extracted representa-tions are then presented to a neural network classifier suit-able for classifying pairs of feature vectors to learned relation classes. Finally, several researchers have considered tensor factorization models to perform the entity relation inference task. Two characteristic such cases are: (i) the method presented in [14], where tensor factorization and nonpara-metric Bayesian clustering is utilized for learning relational structures, and inference is performed by means of Markov chain monte-carlo (MCMC) sampling; and (ii) the approach proposed in [15], which instead of MCMC inference yields point-estimates of the model parameters by resorting to the DEDICOM tensor decomposition [16].

In this work, we propose a completely different feature ex-traction mechanism for performing the entity pair relations inference procedure. Specifically, we introduce a hierarchical method for entity pair representation extraction comprising two subsequent stages: On the first stage, we perform fea-ture representation for individual entities in the given knowl-edge bases. For this purpose, for each entity we compute the average of its word vectors, as discussed in [13]; these word vectors are trained on the available data. Further, on the second stage, we postulate an inducing space projection model, that maps the individual feature vectors of an en-tity pair to a joint high-dimensional inducing space feature vector that is eventually used to perform the relation infer-ence task. The resulting pairwise inducing space feature vectors generated from the so-learned mapping encode both the salient characteristics of each entity in the pair, as well as the dynamics/correlations between them; this way, they are expected to yield much more useful entity pair represen-tations for performing the relations inference task compared to existing techniques.

Turning to prediction generation, existing entity relations inference approaches typically resort to similarity-based score models [12, 10], simple bilinear classifiers [11], or more ad-vanced single-layer neural network classifiers [13, 17], ap-propriately designed to allow for relating pairs of feature vectors to class labels. On the contrary, in this work we pro-pose utilization of large-margin classification techniques to perform the relationship inference task . Large-margin meth-ods, inspired from the literature of support vector machines (SVMs) [18, 19, 20], have attracted significant interest from the machine learning community due to their unparalleled performance in challenging classification tasks. Thus, we expect that leveraging the strengths of large-margin clas-sifiers in the context of our method will yield significant performance advantages for the entity relationship inference algorithm.

Despite these advantages, a significant challenge that per-plexes the utilization of large-margin classifiers in the con-text of our approach concerns the fact that the modeled entity pairs are represented as inducing space vectors, ex-pressed as deterministic functions of unknown parameters that must be trained as part of the model learning proce-dure. To resolve these issues, and facilitate efficient training of our model under an elegant algorithmic framework, we re-sort to a recently proposed latent variable formulation of the objective function of regularized large-margin classifiers [21]. This latent variable formulation expresses the large-margin classification objective function as a variance-mean mixture of linear models with normal errors. This way, it allows for seamlessly integrating the inducing space feature represen-tation component of our model into the overall parameter estimation procedure.

Finally, a significant problem the affects large-margin clas-sification models concerns the fact that conventional large-margin classifiers are not capable of taking into account that the modeled data may comprise a set of multiple underly-ing structures (e.g., clusters), each of which might be bet-ter modeled by using a separate dedicated model compo-nent. To overcome such limitations, recent progress has been made on developing mixture-of-experts formulations of large-margin classifiers, which split the input space into a number of subregions and learn a separate classifier within each region (e.g., [22, 23]). Under such a model formulation, a problem that comes to the fore concerns data-driven deter-mination of the most appropriate number of model compo-nents (underlying clusters). As a solution toward the amelio-ration of this issue, [20] recently proposed the exploitation of nonparametric Bayesian techniques, which offer a promising direction to bypass the model selection problem and auto-matically resolve the unknown number of experts.
 Nonparametric Bayesian modeling techniques, especially Dirichlet process mixture (DPM) models, have become very popular in statistics over the last few years, for performing nonparametric density estimation [24, 25, 26]. Briefly, a re-alization of a DPM can be seen as an infinite mixture of distributions with given parametric shape (e.g., Gaussian). This theory is based on the observation that an infinite num-ber of component distributions in an ordinary finite mix-ture model tends on the limit to a Dirichlet process (DP) prior [25, 27]. Eventually, as a part of the model fitting procedure, the nonparametric Bayesian inference scheme in-duced by a DPM model yields a posterior distribution on the proper number of model component densities (inferred clus-ters) [28], rather than selecting a fixed number of mixture components. Hence, the obtained nonparametric Bayesian formulation eliminates the need of doing inference (or mak-ing arbitrary choices) on the number of mixture components (clusters) necessary to represent the modeled data.
Inspired from these advances, our method postulates a nonparametric Bayesian model, based on the introduction of a DP prior over the space of possible large-margin classi-fiers defined in the whole space of inducing variables. This way, we allow for better dealing with tasks entailing multiple modes, which simple large-margin models often fail to suc-cessfully handle. We derive efficient model training and in-ference algorithms for our model, resorting to the mean-field framework [29, 28, 30, 31], and a truncated stick-breaking formulation of the DP [32, 33]. We dub our approach in-ducing space Dirichlet process mixture large-margin entity relationship inference (iDLER).
 The remainder of this paper is organized as follows: In Section 2, we provide a brief introduction to the Dirichlet process and its function as a prior in nonparametric Bayesian models. In Section 3, we introduce our model and derive its inference and training algorithms. In Section 4, we per-form the experimental evaluation of our approach by train-ing on relationships in WordNet and Freebase, and evaluat-ing model predictive performance on a held-out set of un-seen relational triplets. As we show, our model outperforms previously introduced related models, such as the state-of-the-art approaches presented in [10, 11, 12, 13, 17]. Finally, in the concluding section of this paper, we summarize our results and discuss directions for future research.
Dirichlet process models were first introduced by Fergu-son [34]. A DP is characterized by a base distribution G and a positive scalar  X  , usually referred to as the innovation parameter, and is denoted as DP(  X ,G 0 ). Essentially, a DP is a distribution placed over a distribution. Let us suppose we randomly draw a sample distribution G from a DP, and, subsequently, we independently draw M random variables {  X  Integrating out G , the joint distribution of the variables {  X  m } M m =1 can be shown to exhibit a clustering effect. Specif-ically, given the first M  X  1 samples of G , {  X   X  m } M  X  1 be shown that a new sample  X   X  M is either (a) drawn from the base distribution G 0 with probability  X   X  + M  X  1 , or (b) is selected from the existing draws, according to a multino-mial allocation, with probabilities proportional to the num-ber of the previous draws with the same allocation [35]. Let {  X  c } C c =1 be the set of distinct values taken by the vari-ables {  X   X  m } M  X  1 m =1 . Denoting as  X  M  X  1 c the number of values in {  X   X  m } M  X  1 m =1 that equal to  X  c , the distribution of  X  {  X  m } M  X  1 m =1 can be shown to be of the form [35] where  X   X  c denotes the distribution concentrated at a single point  X  c . These results illustrate two key properties of the DP scheme. First, the innovation parameter  X  plays a key-role in determining the number of distinct parameter values. A larger  X  induces a higher tendency of drawing new param-eters from the base distribution G 0 ; indeed, as  X   X   X  we get G  X  G 0 . Conversely, as  X   X  0 all {  X   X  m } M m =1 cluster to a single random variable. Second, the more often a parameter is shared, the more likely it will be shared in the future.

A characterization of the (unconditional) distribution of the random variable G drawn from a DP, DP(  X ,G 0 ), is pro-vided by the stick-breaking construction of Sethuraman [32]. Consider two infinite collections of independent random vari-ables v = [ v c ]  X  c =1 , {  X  c }  X  c =1 , where the v c Beta distribution, and the  X  c are independently drawn from the base distribution G 0 . The stick-breaking representation of G is then given by where and Under the stick-breaking representation of the DP, the atoms  X  , drawn independently from the base distribution G 0 , can be seen as the parameters of the component distributions of a mixture model comprising an unbounded number of com-ponent densities, with mixing proportions $ c ( v ).
As already discussed, the problem addressed in this paper concerns extending existing knowledge bases that contain incomplete information. Specifically, our goal is to create a model capable of inferring relations between the entities in an existing knowledge base that are not already included in it due to its partial and incomplete nature.

Let us denote as ( e 1 ,e 2 ) a pair of entities included in a given knowledge base. Let us also denote as R = { 1 ,...R } the set of relation types encompassed in this knowledge base. Under this formulation, the addressed problem consists in creating a model capable of inferring for any pair of entities ( e 1 ,e 2 ) from this knowledge base whether they satisfy each one of the encoded relationships; for instance, whether the pair ( e 1 ,e 2 ) = (snow leopard, head) satisfies the relationship r =  X  X as part. X 
As briefly reviewed in the previous section, existing ap-proaches address this problem by obtaining feature vector representations for each entity, and presenting them to some appropriate classifier trained to correlate pairs of feature vectors with learned classes (relations). Specifically, most related works learn to optimally extract entity-level feature representations, which are subsequently used to perform a classification task, assigning pairs of entities to relationships (e.g., [10, 11, 12]). More recently, [13] proposed that entity-level feature representation be performed by learning fea-ture vectors on the word level , and combining the feature vectors of the words that comprise each entity. This way, the extracted feature representations allow for sharing sta-tistical strength between the words consisting the modeled entities, thus yielding better predictive accuracy compared to the previous approaches. The so-obtained feature vector representations of the modeled entities are eventually pre-sented to suitable classifiers to effect the relationship infer-ence (classification) task: neural network architectures [13, 17], distance-based models [12, 10], and bilinear classifiers [11, 14] are characteristic examples of classification models used for this task.

As discussed in Section 1, in this work we propose to ad-dress this problem by exploiting the merits of large-margin classifiers. Specifically, to effect the relationship inference task, we consider postulating DPM models comprising reg-ularized large-margin binary classifiers, suitable for classifi-cation of pairs of feature vectors. Let us consider a modeled relationship type r  X  R . To create a model capable of in-ferring whether a pair of entities ( e 1 ,e 2 ) in a given knowl-edge base satisfy this relationship, we postulate a DPM of L -norm regularized large-margin classifiers; the objective function of the c th model component classifier reads [18]: d  X  (  X  c , X  c ) = where  X  k is the standard deviation of the k th element of the feature vectors x ,  X  c is a tuning hyperparameter of the c th postulated large-margin classifier,  X  c = [  X  ck ] K parameters vector of the c th model component, N is the number of training examples, the classification labels y n { X  1 , 1 } indicate whether the n th entity pair satisfies the relationship or not, and  X   X  (0 , 2].

In our model, the feature vectors x n correspond to en-tity pairs, and constitute joint inducing space projections of the feature vectors of each entity in the pair, that cap-ture the salient characteristics of both each entity as well as of the dynamics of their interactions/correlation patterns. Specifically, we postulate the following nonlinear determin-istic inducing space projection function: where e 1 n , e 2 n  X  R F are the individual feature vectors of each entity in the n th training pair; the trainable parameters W k  X  R F  X  F are matrices designed for extracting the salient dynamics/correlations between entities in a pair, while the V 1 , V 2  X  R K  X  F are trainable parameter matrices designed for capturing salient individual features of each entity in the pair,  X  is a bias parameter, and h (  X  ) is a sigmoid nonlin-earity. Note that, in the above postulated model, the size of the derived inducing space projections K is a model hy-perparameter that can be heuristically selected to optimize model performance. The same holds for the dimensionality F of the feature vector representations e of each individual entity.

Turning to the individual feature vector representations of each entity, as we have already discussed, we elect to perform modeling and training on the word level instead of the entity level, similar to [13]. That is, we model each word as an F -dimensional vector, and compute entity feature vectors by composing their constituent word vectors. This way, we allow for sharing of statistical strength between the words describing each entity.

Note also that training word vectors instead of entity vec-tors as part of the model learning procedure has the addi-tional advantage of allowing to leverage previously obtained unsupervised word vectors, capturing some salient syntactic and semantic distribution properties. This way, we obtain a proper model initialization scheme that enables faster con-vergence to good parameter estimates (much better than, e.g., having to resort to randomly initialized feature vectors, which is the solution usually adopted in the literature [10, 11, 12]).
To allow for derivation of an elegant and computationally efficient training algorithm for our model, we resort to a recently proposed latent variable representation of regular-ized large-margin classifiers [21]. This formulation renders training of our model amenable to the mean-field inference paradigm, giving rise to expectation-maximization (EM)-style [36] algorithms with proved convergence [29, 31, 33].
Specifically, let us consider the model pertaining to the relationship type r  X  R . We introduce the set of variables { z cn }  X  ,N c,n =1 , with z cn = 1 if the n th training vector x longs to the c th cluster formulated in the considered induc-ing space (modeled by the c th postulated model component), z cn = 0 otherwise. Then, following [21], minimization of the objective function (8) of the DPM component classifiers of our model w.r.t. the parameters  X  c and  X  c can be shown to be equivalent to finding the mode of the pseudo-posterior distribution p (  X  c ) and p (  X  c |  X  c , X  ) are appropriate prior distributions on the model parameters, L ( Y |  X  c ; X ,Z ) is a pseudo-likelihood function of the c th model component of the form: L ( Y |  X  c ; X ,Z ) = and C (  X  c ) is an appropriate normalization constant. In these derivations, both the pseudo-likelihood contribution of each training example, L ( y n |  X  c ; x n ,z cn = 1), as well as the prior p (  X  c |  X  c , X  ) can be expressed as location-scale mixtures of normals. Regarding the component-wise pseudo-likelihood contribution of each training example { x n ,y n } , the equiva-lent location-scale mixture of normals expression reads [21]:
L ( y n |  X  c ; x n ,z cn = 1) = Z where the  X  cn are introduced latent variables (one for each training example) that characterize the derived location-scale mixture formulation. Similarly, the prior p (  X  can be shown to yield [21] where p (  X  ck |  X  c , X  ) = with the introduced latent variables  X  ck being imposed the prior where St +  X  tive stable random variable of index  X  2 , and  X  (  X |  X ,s cumulative distribution function of a normal with mean  X  and variance s 2 .

On this basis, we essentially yield a data augmentation scheme for our model [21], which can be straightforwardly exploited under the mean-field inference principle to obtain an elegant and computationally efficient iterative training algorithm. For this purpose, we proceed by utilizing the stick-breaking formulation of the DP prior, imposed over the postulated component classifiers of our model as described in Section 2. We have with This concludes the prior configuration of our model.
As usual with nonparametric Bayesian modeling techniques, a problem training algorithm derivation for our model is con-fronted with concerns the need to deal with a theoretically infinite number of parameters, which is clearly computa-tionally intractable. In this work, to resolve this issue we resort to a common strategy in the literature of Bayesian nonparametrics, formulated on the basis of a truncated stick-breaking representation of the DP [33]. That is, we fix a value C and we set $ c ( v ) equal to zero for c &gt; C . Note that, under this setting, the treated iDLER model involves a full DP prior; truncation is not imposed on the model it-self, but only on the posterior distribution to allow for a tractable inference procedure.

Exploiting these results, we proceed to derive an alter-nating expectation-conditional maximization (AECM)-type algorithm for training of our iDLER model. AECM [37] is an extension of the EM algorithm [36] where the specifica-tion of the complete data is allowed to be different on each algorithm step. By utilizing the AECM framework, we are capable of seamlessly handling parameter estimation for the large-margin component classifiers, the nonlinear inducing space entity pair representation component of our model, and the word feature vector representations, under compu-tationally simpler updating equations compared to conven-tional EM-type formulations.

On each iteration , our proposed model training algorithm comprises simple and computationally efficient conditional maximization steps, organized in four distinct cycles. On the first cycle , we update the estimates of the parameters of the large-margin component classifiers. For this purpose, we specify the complete data of the algorithm to comprise the latent variables Z ,  X  c = [  X  cn ] N n =1 , and  X  c c  X  { 1 ,...,C } , as well as the training labels set Y and the corresponding inducing space projections X . Under this for-mulation, the maximized quantity on the first cycle of the AECM algorithm is the posterior expectation of the com-plete data log-posterior Q ( {  X  c , X  c } C c =1 |{  X   X  ated at the current parameter estimates, which reads Q (  X  c , X  c |  X   X  c ,  X   X  c ) = where the  X  Z comprises the posterior expectations of the la-parameters {  X  c , X  c } , p (  X  c ,  X  c |  X   X  c ,  X   X  c ; X ,Y, rior of the latent variables  X  c and  X  c , and we have (ignoring constant terms) [21]: log p (  X  c , X  c |  X  c ,  X  c ; X ,Y,  X  Z ) =  X  1 Following [37], to effect the first cycle of the proposed AECM-type algorithm, we first perform an E-step that cal-culates the value of the objective function Q (  X  c , X  c | and then proceed with an M-step, where new updates of the model parameter estimates  X   X  c and  X   X  c are obtained by inverse Gamma prior distribution over  X   X  c , i.e. similar to [21], it can be shown that the E-step of the first cycle of the derived AECM-type algorithm reduces to com-putation of the sufficient statistics [21] where diag(  X  ) denotes a square diagonal matrix with the elements of vector  X  on the main diagonal, as well as of the latent cluster assignment posteriors: where E [  X  ] denotes the posterior expectation of a quantity, and On the other hand, the M-step reduces to the updates  X  and where  X  = diag([  X  2 k ] K k =1 ), and  X  X c = [  X  x cn  X  z
On the second cycle of the derived AECM-type algorithm, we update the estimates of the parameters of the nonlin-ear projection component of the postulated models that yields the inducing space feature vectors X . For this pur-pose, we specify the complete data of the algorithm to com-prise the latent variables Z and {  X  c } C c =1 , as well as the training labels set Y and the corresponding entity pairs set E = { e 1 n , e 2 n } N n =1 . Under this formulation, the maximized quantity on the second cycle of the AECM algorithm be-comes = where log p ( { W k } K k =1 , V 1 , V 2 ,  X  |{  X  c ;  X   X  (ignoring constant terms) [21]: with x ( e 1 n , e 2 n ) being now considered a function of the un-known parameters { W k } K k =1 , V 1 , V 2 , and  X  , given by (9). On this basis, and similar to the first cycle of the algorithm, the E-step on the second cycle of the AECM algorithm re-duces to computation of the sufficient statistics  X   X   X   X  c , given by Eqs. (25)-(26), and the posterior expectations  X  z cn , given by (29), while the M-step reduces to local execu-tion of a gradient-based optimization algorithm to optimize (34). In our experiments, to effect these M-step updates, we resort to stochastic gradient ascent.

Further, the third cycle of the proposed AECM algorithm is focused on updating the learned feature representations of the words in the modeled knowledge bases. This procedure yields a complete data configuration similar to the second cycle of the AECM algorithm, with similar resulting E-step. The only difference consists in the fact that, on the M-step, instead of optimization of (34) w.r.t. { W k } K k =1 , { V and  X  , we optimize it over the word vectors that constitute the entity feature representations { e jn } 2 ,N j,n =1 of the modeled entities. For this latter purpose, we resort to optimization by means of stochastic gradient ascent, similar to the second cycle of the algorithm.

Finally, on the fourth cycle of the AECM-type training algorithm of our model, we consider the latent variables to comprise the latent cluster assignment variables, Z , and we effect the updating of the posteriors of the stick-variables { v c } C c =1 . It can be shown that these updates are similar to the updates obtained in [33] for the case of a simple DPM comprising Gaussian distributions as its components. Specifically, on the E-step of the fourth cycle of the algo-rithm, we update the posterior expectations set  X  Z , given by (29), while the M-step comprises the updates of the stick-breaking posteriors; we have [33]: where whence In the above equations,  X  (  X  ) denotes the Digamma function.
An outline of the AECM-type training algorithm of our model is provided in Table 1.
Once we have trained our model, inference for a pair of entities ( e 1 ,e 2 ) w.r.t. a relationship type r  X  R can be performed by computing the decision function value of the corresponding large-margin mixture-of-experts classifier. Specifically, under the proposed model configuration scheme, the decision function of the postulated models yields: where x ( e 1 , e 2 ) is calculated based on (9), using the ob-tained model parameter estimates, and we have and
In this section, we perform an extensive experimental eval-uation of our method, and compare its performance to re-lated state-of-the-art approaches. Specifically, we consider two popular knowledge bases, namely WordNet [1] and Free-Base [2], and use our model to predict whether some rela-tions hold based on existing relations described therein. This way, we essentially attempt to mimic common sense reason-ing in the context of an inferential procedure driven by our method. For instance, if some entity in a given knowledge base represents an individual born in London, our model should be capable of inferring that their nationality should be British. In a similar fashion, when it comes to a Bengal tiger, our model should be capable of both inferring that its natural habitat should be somewhere in India, and that it should be classified as a carnivorous mammal.

For comparative purposes, apart from evaluation of our method, we also cite the performance of two similarity-based score models, namely the distance-based model presented in [10] and the Hadamard model presented in [12], a bilinear classifier-based method, presented in [11], the method pre-sented in [17], which is based on single-layer neural network classifiers, and the recently proposed neural tensor network (NTN) model [13], which is based on a single-layer neural network classifier that learns feature representations on the word level instead of the individual entity level, similar to our approach.

For reference purposes, we replicate the experimental setup of [13], setting the dimensionality of the feature vectors used to represent the modeled entities equal to F = 100. Model training is performed using a subset of the actual relations included in the used knowledge bases as positive examples; an equal number of negative examples are created by tak-ing the true relationships in the used positive examples and replacing one of the two entities correlated therein with ran-domly selected irrelevant ones. The word feature vectors of Table 1: Outline of the AECM-type training algo-rithm of iDLER. 1. Set the iteration counter to it := 0 . 2. Run the first cycle of the algorithm: 3. Run the second cycle of the algorithm: 4. Run the third cycle of the algorithm: 5. Run the fourth cycle of the algorithm: 6. If not converged, increase the iteration counter, it := our approach are initialized by adopting the method pre-sented in [38]; this method essentially consists in exploiting unsupervised semantic word vectors, capturing some salient syntactic and semantic distribution properties, as discussed in Section 3.1. We set the innovation parameter of our model equal to a modest  X  = 1 in all our experiments.

We developed our source codes in MATLAB R2014b. We executed our experiments as single-threaded processes on an Intel Xeon 2.5GHz Quad-Core CPU with 64GB RAM.
WordNet is a large lexical database of English, where nouns, verbs, adjectives, and adverbs are grouped into sets of cognitive synonyms ( synsets ), each expressing a distinct concept. Synsets are interlinked by means of conceptual-semantic and lexical relations. WordNet superficially resem-bles a thesaurus, in that it groups words together based on their meanings. However, there are some important distinc-tions. First, WordNet interlinks not just word forms X  X trings of letters X  X ut specific senses of words. As a result, words that are found in close proximity to one another in the net-work are semantically disambiguated. Second, WordNet la-bels the semantic relations among words, whereas the group-Table 2: WordNet: Prediction accuracy (%) of the evaluated methods (optimal model configuration).
 ings of words in a thesaurus does not follow any explicit pattern other than meaning similarity.

WordNet contains a total of 38,696 unique entities that take part in 11 different types of relations. Based on these data, we construct 112,581 relational triplets for training , half of which contain actual relationships between entity pairs included in the knowledge base (positive examples), and half comprise negative examples created by random re-placement of the entities in the positive examples, as de-scribed previously. Further, in order to allow for performing model selection, we create a development set. For this pur-pose, we use a small subset of the remainder of the actual entity pair relationships (positive examples), and we create an equal number of negative examples by randomly switch-ing entities from actual relations. This way, we obtain a development set with 2,609 triplets in total.

Finally, to obtain a testing set for classification, we use the remainder of the actual relationship triplets in Word-Net, and augment it by randomly switching entities from actual testing relations, resulting in a testing set with equal numbers of positive and negative examples. Note that, simi-lar to [13], we filter out trivial triplets from our testing data, namely triplets both the entities of which also appear in the training set in a different relation or order. Finally, in the case of synsets containing multiple words, we pick only the first, most frequent one. This experimental setup eventu-ally yields a total of 10,544 triplets used for testing. The final accuracy is based on how many triplets are classified correctly as relevant or irrelevant.
In Table 2, we provide the prediction accuracy obtained by both our iDLER method and its considered competi-tors. These results have been obtained for optimal model configuration, as deemed by exploiting the available devel-opment dataset. In the case of iDLER, this entails pos-tulating models with different values of the regularization constant  X   X  (0 , 2], and different dimensionalities of the in-ducing space (pairwise entity) projections, x ( e 1 , e 2 that our method inherently derives. As we observe from these results, iDLER offers an improved predictive perfor-mance compared to the state-of-the-art.
Freebase is a large collaborative knowledge base consisting of metadata composed mainly by its community members. It is an online collection of structured data harvested from many sources, including individual, user-submitted wiki con-tributions. In our experiments, we use the relational triplets included in the People domain, out of which we extract 13 relation types that contain a total of 75,043 entities. Similar to [13], we use data from all of these relations for training, but retain only data from 7 of them for testing, discard-ing the remaining 6, namely place of death, place of birth, location, parents, children, spouse . We elect for this exper-imental setup since the omitted 6 relation types are very hard to predict, e.g., the name of somebody X  X  spouse is hard to infer from other knowledge in the database.

On this basis, we obtain a training dataset that comprises 316,232 relational triplets , half of which contain actual re-lationships between entity pairs included in the knowledge base (positive examples), and half consist of negative exam-ples created by random replacement of the entities in the positive examples, as in the previous experiments.

Model selection is performed by creating an appropriate development set from a fraction of the remaining data, sim-ilar to Section 4.1. Here, the used development set contains a total of 5,908 triplets of both positive and negative exam-ples. Finally, to obtain a testing set for classification, we use the remainder of the actual relationship triplets in FreeBase, and augment it by randomly switching entities from actual testing relations, resulting in a testing set with equal num-bers of positive and negative examples. We filter out trivial triplets from our testing data, similar to the experiments of Section 4.1.

Note that, similar to [13], in the context of the procedure of development set and test set formation, we constrain the entities from the possible answer set by allowing entities in a given position in the formed pairs only if they appeared in the same position in the training dataset. For exam-ple, given a correct pair (Pablo Picasso, Spain) w.r.t. the relationship  X  X ationality, X  a potential negative example to include in our development and test sets is (Pablo Picasso, United States): Such a pair includes a country that is ir-relevant to the nationality of Pablo Picasso; thus, correctly labeling this pair as negative requires that the evaluated model can infer subtle correlations from the used training datasets. On the other hand, a pair, e.g., of the form (Pablo Picasso, Van Gogh) would be much easier for a model to la-bel as negative: Since neither of the entities included in this pair contain words pertaining to countries , correctly labeling it as irrelevant w.r.t. the  X  X ationality X  relationship can be performed merely on a word content basis. Hence, we elect to exclude such pairs, since they do not allow for assessing the performance of an evaluated model in the much more difficult task of inferring subtle correlations from the used training datasets, which is the ultimate goal of our approach.
In Table 3, we provide the prediction accuracy obtained by both our iDLER method and its considered competi-tors. These results have been obtained for optimal model configuration, as deemed by exploiting the available devel-opment dataset. In the case of iDLER, this entails pos-tulating models with different values of the regularization constant  X   X  (0 , 2], and different dimensionalities of the in-ducing space (pairwise entity) projections, x ( e 1 , e 2 that our method inherently derives. As we observe from these results, iDLER offers an improved predictive perfor-mance compared to the state-of-the-art.
 Table 3: FreeBase: Prediction accuracy (%) of the evaluated methods (optimal model configuration).
 Table 4: FreeBase: Computational complexity com-parisons between iDLER and the second best per-forming method (time in seconds).

To conclude, let us now turn our investigations to the com-putational complexity of our method, and how it compares to the competition. As we observe from the discussions of Sections 3.2 and 3.3, iDLER model training and inference do not entail computationally demanding calculations. As a result, training and inference using our model are not ex-pected to impose significantly higher computational require-ments compared to the second best performing method, i.e. NTN.

Indeed, let us focus on the FreeBase dataset; similar re-sults can be also obtained for the WordNet dataset (since both datasets are of similar scale). As we illustrate in Ta-ble 4, one iteration of the training algorithm of iDLER in the case of the FreeBase dataset imposes only a slight com-putational overhead compared to the second best perform-ing method, i.e. NTN (these results pertain to the model configuration that yields optimal accuracy). Similar is the case when it comes to prediction generation. Indeed, as also shown in Table 4, the time required by both iDLER and NTN for prediction generation on the FreeBase dataset is of the same order of magnitude, and very low in both cases. Thus, from these discussions we deduce that our ap-proach offers a favorable accuracy-complexity tradeoff over the state-of-the-art.
In this paper, we presented a large-margin methodology for reasoning over relationships between pairs of entities in knowledge bases. Contrary to existing works, our proposed method relies on large-margin classification techniques to ef-fect the entity relations inference task, based on an inducing space representation of entity pair feature vectors. These inducing space representations capture salient information of both the individual features of each entity and of the en-tity correlations in the context of the modeled pairs; thus, they allow for better performing the large-margin inference task for the modeled data. As we discussed, feature rep-resentation learning for the observed data is performed on the individual word level instead of the entity level, to allow for sharing statistical strength between the feature vector representations of entities with common words.

To allow for better dealing with data comprising multi-ple modes, we assumed that the postulated inducing space comprises a set of subspaces (clusters), each of which can be modeled by a separate large-margin classifier. This way, we essentially obtained a mixture-of-experts model formulation, over which we imposed a DP prior to allow for data-driven determination of the most appropriate number of component experts. To perform model training, we utilized a recently proposed data augmentation scheme that yields an elegant and computationally efficient model training algorithm un-der the AECM framework.

One issue our current work does not sufficiently address concerns appropriate data-driven selection of the number of inducing space dimensions. Indeed, as we have shown through our experimental evaluations, inducing space di-mensionality is a factor that affects model performance in both the considered knowledge bases. Therefore, coming up with an appropriate model formulation allowing for au-tomatic data-driven inducing space dimensionality selection would be a significant improvement of our model. Toward this end, one could consider adoption of techniques from the field of Bayesian nonparametrics, similar to the proce-dure we adopted in this work for automatic selection of the number of component classifiers. For instance, one could consider imposition of Indian buffet process priors [39, 40] to achieve this goal.

Another issue our approach does not explicitly address is the case of having to deal with polysemous words. In such cases, sharing statistical strength between entities with sim-ilar word content may yield misleading results. One possible way to remedy this limitation could be obtained by allowing for multiple feature vectors per word, similar to [38]. Exam-ination of solutions to both the above problems, along the lines discussed here or other alternative approaches, is part of our ongoing and future research work. [1] G. Miller,  X  X ordNet: A lexical database for English, X  [2] K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and [3] F. M. Suchanek, G. Kasneci, and G. Weikum,  X  X ago: [4] J. Graupmann, R. Schenkel, and G. Weikum,  X  X he [5] V. Ng and C. Cardie,  X  X mproving machine learning [6] A. Yates, M. Banko, M. Broadhead, M. J. Cafarella, [7] R. Snow, D. Jurafsky, and A. Y. Ng,  X  X earning [8] A. Fader, S. Soderland, and O. Etzioni,  X  X dentifying [9] G. Angeli and C. D. Manning,  X  X hilosophers are [10] A. Bordes, J. Weston, R. Collobert, and Y. Bengio, [11] R. Jenatton, N. L. Roux, A. Bordes, and [12] A. Bordes, X. Glorot, J. Weston, and Y. Bengio,  X  X oint [13] R. Socher, D. Chen, C. D. Manning, and A. Y. Ng, [14] I. Sutskever, R. Salakhutdinov, and J. B. Tenenbaum, [15] M. Nickel, V. Tresp, and H. Kriegel,  X  X  three-way [16] R. A. Harshman,  X  X odels for analysis of asymmetrical [17] R. Collobert and J. Weston,  X  X  unified architecture [18] V. N. Vapnik, Statistical Learning Theory . New [19] Y. Altun, I. Tsochantaridis, and T. Hofmann,  X  X idden [20] J. Zhu, N. Chen, and E. P. Xing,  X  X nfinite SVM: a [21] N. G. Polson and S. L. Scott,  X  X ata augmentation for [22] R. Collobert, S. Bengio, and Y. Bengio,  X  X  parallel [23] Z. Fu, A. Robles-Kelly, and J. Zhou,  X  X ixing linear [24] S. Walker, P. Damien, P. Laud, and A. Smith, [25] R. Neal,  X  X arkov chain sampling methods for [26] P. Muller and F. Quintana,  X  X onparametric Bayesian [27] C. Antoniak,  X  X ixtures of Dirichlet processes with [28] D. Blei and M. Jordan,  X  X ariational methods for the [29] T. Jaakkola and M. Jordan,  X  X ayesian parameter [30] H. Attias,  X  X  variational Bayesian framework for [31] J. Winn and C. Bishop,  X  X ariational message passing, X  [32] J. Sethuraman,  X  X  constructive definition of the [33] D. M. Blei and M. I. Jordan,  X  X ariational inference for [34] T. Ferguson,  X  X  Bayesian analysis of some [35] D. Blackwell and J. MacQueen,  X  X erguson [36] A. Dempster, N. Laird, and D. Rubin,  X  X aximum [37] X. L. Meng and D. van Dyk,  X  X he EM algorithm -an [38] E. H. Huang, R. Socher, C. D. Manning, and A. Y. Ng, [39] T. Griffiths and Z. Ghahramani,  X  X nfinite latent [40] Y. W. Teh, D. Gorur, and Z. Ghahramani,
