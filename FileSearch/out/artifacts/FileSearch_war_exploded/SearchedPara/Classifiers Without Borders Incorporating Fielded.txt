 Accurate web page classification often depends crucially on in-formation gained from neighboring pages in the local web graph. Prior work has exploited the class labels of nearby pages to im-prove performance. In contrast, in this work we utilize a weighted combination of the contents of neighbors to generate a better vir-tual document for classification. In addition, we break pages into fields, finding that a weighted combination of text from the target and fields of neighboring pages is able to reduce classification er-ror by more than a third. We demonstrate performance on a large dataset of pages from the Open Directory Project and validate the approach using pages from a crawl from the Stanford WebBase. Interestingly, we find no value in anchor text and unexpected value in page titles (and especially titles of parent pages) in the virtual document.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Information filtering ; I.5.2 [ Pattern Recognition ]: Design Methodology X  Classifier design and evaluation ; H.3.1 [ Information Storage and Retrieval ]: Content Analysis and In-dexing Algorithms, Performance Web page classification, SVM, naive Bayes, Neighboring
Web page classification is the process of assigning a web page to one or more predefined category labels. Classification is often posed as a supervised learning problem in which a set of labeled data is used to train a classifier which can be applied to label future examples.

Web page classification helps retrieval and management of web information in various respects. Many tasks may benefit from ac-curate categorization of web pages, such as the development of web directories like those provided by Yahoo! 1 and the dmoz Open Di-rectory Project (ODP) 2 , topic-sensitive web link analysis [17, 20, 24], contextual advertising, analysis of the topical structure of the Web [7], and focused crawling [8].

One baseline approach to web page classification is to treat web pages as text documents without considering the additional fea-tures that the web can provide (e.g., hyperlinks, markups). Such an approach usually yields suboptimal performance. In a prelimi-nary experiment in which we applied support vector machines to a 3,600-page ODP dataset, merely 77% of the pages were correctly classified into one of the twelve broad categories when only on-page textual features are used.

Research has been widely conducted exploring the usefulness of non-textual features (sometimes in addition to textual features) in web page classification. A number of studies have shown that web page classification can be performed more accurately using infor-mation gathered from neighboring pages in the web graph [6, 15, 4, 23]. However, when using content of neighbors, existing work typically uses information from neighboring pages as a whole. In-spired by the success of the fielded extension of BM25 in informa-tion retrieval [25], we conjecture that permitting text from different fields of neighboring pages to contribute differently may improve classification performance. Our intuition is that information from different parts of neighboring pages should have different impor-tance. For example, anchor text, considered a concise description of the target page, should be more important than generic text.
In this paper, we propose the F-Neighbor algorithm, as a fielded extension to our Neighboring algorithm [23] which used class in-formation and full content of neighboring pages. In particular, we break up neighboring web pages, as well as the page to be classified into several text fields (e.g., title, anchor text), and combine them according to the individual importance they have. Our experiments show that this method is able to reduce the error rate of textual clas-sifiers by more than half. We als o found that pa ge titles, especially parent titles, are more useful than generic text.

The rest of this paper is organized as follows. In Section 2, we review related work exploiting hyperlink information to enhance web page classification. In Section 3, we introduce and describe F-Neighbor, the proposed algorithm. We tune and test this algorithm in Section 4, and conclude with a discussion and a summary of our results. http://www.yahoo.com/ http://www.dmoz.org/
In 1994, Robertson and Walker [26] proposed a function to rank documents based on the appearance of query terms in those doc-uments, which was later referred to as Okapi BM25. In time, BM25 became a common component of information retrieval sys-tems. Later, Robertson et al. [25] extended this model to combine multiple text fields including anchor text, and showed improvement over original BM25. Currently the fielded BM25 model (BM25F) is widely used, taking the place of its non-fielded predecessor. Be-sides retrieval, the fielded model has also shown to be useful in expert finding from email corpora [3, 22]. In this paper, we bor-row the idea that extended BM25 to BM25F, hoping it can boost classification performance as it did for retrieval. Categorization of web pages has been an active research area. Recently, many studies have focused on using information from neighboring web pages. Exploitin g the topical locality of the web [11, 7], this type of approach gathers information from neighboring pages on a local link graph to help classify the page in question.
Many kinds of information from neighbors can be used in classi-fication. Some approaches benefit from taking advantage of neigh-boring pages that have been labeled by human experts [6, 28, 4, 23]. However, if an approach exclusively relies on such labels, its applicability is restricted due the limited number of labeled pages on the web.

The content of neighboring pages is also useful. In general, di-rectly incorporating text from neighboring pages introduces more noise than signal [6, 14, 33]. Such noise can be greatly reduced by separating local text and text from neighbors [23], by also consid-ering the human-assigned labels of neighbors [21, 23], or by using only important parts of content from neighbors, su ch as titles, an-chor text, and the surrounding text of anchor text [2, 12, 13, 33, 30, 10, 15, 31, 34]. In particular, Attardi et al. [2] showed promising results using title, anchor text, a nd a portion of text surrounding the anchor text on parent pages to help determine the target page X  X  topic. Similarly, F X rnkranz [12] proposed an approach to classifi-cation using features on the parent pages, such like anchor text, sur-rounding text of anchor text and the headings that precede the hy-perlink. A graph-based approach to classification was proposed by Angelova and Weikum [1] based on relaxation labeling, in which labels of pages in a weighted local link graph are iteratively ad-justed. The use of information from neighbors is not restricted to topical classification; it can also be applied in spam classification as proposed by Cas tillo et al. [5], as well as document representation refinement for the purpose of retrieval [29].

The following points separate our current work from the previ-ous:
Before introducing our proposed algorithm, we briefly summa-rize our earlier Neighboring algorithm [23] as it is the basis of the work described in this paper.

The basic idea of Neighboring algorithm is the use of pages nearby in the link graph (called  X  X eighboring pages X ) to help clas-sify the page in question (called  X  X arget page X ). As illustrated in Figure 1 from [23], four types of neighboring pages are used: par-ent, child, sibling and spouse.

The Neighboring algorithm, as shown in Figure 2, consists of four steps: page level weighting, grouping, group level weight-ing, and the weighting between the target page and neighbors. First, each neighboring page, represented by its topic vector, is weighted according to its individual properties. Such properties include whether its human-generated label is available, whether it is from the same host as the target page, etc. Then, these pages are assembled into four groups according to their link relationship with the target page, that is, parent, child, sibling, or spouse. At this step, the topic vectors of the pages within the same group are ag-gregated. The topic vector of a group is computed as the centroid of the topic vectors of all its members. After grouping, there are four topic vectors, one for each group. Then, group level weight-ing combines the four vectors and generates the topic vector of all the neighbors. Finally, the topic vector of neighbors and the topic vector of the target page are combined to generate the final topic vector, based on which a prediction of the page X  X  topic is made. The parameters used in these weighting schemes are tuned through experiments.

The intuition of the Neighboring a lgorithm is to utilize the infor-mation from neighbors to adjust the classifier X  X  view of the target page. For example, consider the scenario illustrated in Figure 2. The target page was originally classified as  X  X ellow X  by a classifier that only considers the page itself. However, the neighbors strongly suggest otherwise. Therefore, by considering the information of neighbors, the Neighboring algorithm is able to adjust its decision and classify the target page as  X  X ed X .

Through studying the effect of multip le factors in web classifica-tion, our earlier work found that human-generated labels of neigh-bors should be used whenever they are available; neighbors within the same site as the target page are useful; a sibling (or a spouse) should be weighted by the number of parents (or children) it has in common with the target page; and finally, while all the four neighbor types can contribute, sibling pages are the most important. Those experiments showed an approximately two thirds reduction in error rate compared with support vector machines operating on target text only. In Section 4 we reproduce these experiments on revised datasets.
The default Neighboring algorithm considers text on each neigh-boring page as a whole, disregarding where the text appears. Here, we argue that text appearing in different fields may carry differ-ent values. For example, anchor text (the text to be clicked on to activate and follow a hyperlink to another web page, placed be-tween HTML &lt; A &gt; and &lt; /A &gt; tags) is usually considered a good description of the page to which it points; therefore, anchor text could be more useful in classification than other text on the parent page. As an extension to the Neighboring algorithm, we examine the importance of text in different fields on neighboring pages.
The idea of differentiating text in different fields of hypertext is not new. It has been successfully applied to web information retrieval [25], to web classification [16], as well as other research. However, little research has examined the importance of text fields on neighboring pages in classification problems. In this paper, we propose to utilize t he textual information appearing in the following fieldstohelpclassifyawebpage: For each page that needs be classified, these fields are extracted. Unlike fields residing on the target page, each type of the text fields from neighboring pages usually has m ultiple instances. For exam-ple, a target page with ten sibling pages has ten instances of  X  X ib-ling:title X  field. These instances are aggregated by computing the centroid of each type of field, as described below.
We formalize the computations described above using the fol-lowing equations. For the fields on the target page, a tfidf represen-tation is computed for each virtual document using the equations used by the Cornell SMART system [27]. The term frequency and inverse document frequency are defined by Equation 1 and 2, where n ( d, t ) is the number of times term t appears in document d , the the total number of documents in the collection D ,and the number of documents that contain term t .

TF ( d, t )= Each document d is represented by a vector d in which each com-ponent d t is its projection on axis t ,givenby Finally, vector d is normalized using Equation 4 so that the length of the vector is 1. The vector d computed by Equation 4 is used to represent a field from the target page.

Equations 1 through 4 are also applied to fields of neighboring pages. However, in contrast to the fields on the target page, each type of neighboring field usually has multiple instances coming from different neighbors (as in the previous example, a target page with ten siblings has ten instances of sibling title and ten instances of sibling text). In order to generate a single representation for each type of neighboring fi eld, an additional step is needed. This is performed by simply computing the centroid of the instances of the type. Empty fields (such as an empty title) are not considered in this computation. Note that the tfidf vectors are normalized be-fore combination to prevent a long textual field of one page from dominating the fields from other pages. Now for each target document we have computed twelve vectors ( d f 1 through d f 12 ) representing the various text fields. We will combine them by weighted sum as in Equation 6 to form a single vector on which the classifier is performed. The vector d comb is used as the document representation in F-Neighbor classification. The weights w f i in Equation 6 will be determined experimentally.
The determination of the weights w f i in Equation 6 can be seen as an optimization problem in which the classification accuracy based on the aggregated representation is the target to be optimized. Therefore, any generic optimization technique can be applied. In this work, we used a two-layer optimization approach, in which the lower layer optimization optimizes the combination among fields on the same neighbor type (e.g., child title and ch ild text), while the upper layer optimizes among all the neighbors based on the result of the lower layer optimization. Figure 3 illustrates this opti-mization process.
So far, we have described a parameterized model for web classi-fication. In this section, we tune and test it on real-world datasets.
Two datasets are used in our experiments to measure perfor-mance: a sample of 12,000 web pages from ODP and a sample of 2,000 web pages from the Stanford WebBase collection [9]. The ODP metadata being used was downloaded from dmoz.org in September 2004, and c ontains 0.6 millio n categories and 4.4 mil-lion leaf nodes. A crawler was used to fetch the web pages pointed to by the ODP, out of which 95% were successfully retrieved.
For the ODP dataset, as in the work by Chakrabarti et al. [6], 12 out of the 17 top-level categories of the dmoz Directory were selected, and we list them in Table 1. The 12,000 pages are sam-Table 1: Set of twelve top-level categories used from the dmoz Directory. pled only from these 12 categories, 1,000 per category. From each category, 700 target pages are randomly sampled to tune the pa-rameters and another 300 for testing the algorithms. The URLs of the neighboring pages are obtained and then the union of those pages is crawled from the Web. The outgoing links are directly ex-tracted from the web pages, while the incoming links are obtained by querying Yahoo! search with  X  X nlink: X  queries through the Ya-hoo API 3 . Due to API usage limits, at most the first 50 incoming links for each page were obtained.

For the WebBase dataset, 2000 target pages are selected from a 2005 crawl. The link graph provided with the data collection is used to find the neighboring pages. The purpose of using WebBase dataset is to offset the bias of the ODP dataset. The ODP pages are mostly high quality pages, while WebBase is a generic crawl from the Web. Therefore, experime nts on the WebBase dataset are potentially able to demonstrate performance on more typical web pages rather than just high-quality pages.
All of the web pages in our experiments have gone through the same text preprocessor as in [23]. The functionality of the prepro-cessor is as follows: http://develope r.yahoo.com/ After preprocessing, each HTML file is transformed into a stream of terms. The preprocessing increases classification accuracy by a marginal improvement and reduces time and space required by the classifier.

Our prior work pointed out that  X  X moz copies X  in the experi-ment dataset may produce over-optimistic estimation of classifica-tion performance [23]. A  X  X moz copy X  is defined as a mirror of a portion of the ODP. The existence of  X  X moz copies X  creates extra connections between the target page and its sibling pages. There-fore, it may benefit algorithms wh ich utilize such c onnections. In that paper, we also proposed a simple heuristic approach to prune  X  X moz copies X , which matches page URLs against ODP category names. A careful check through the dataset finds that there are still  X  X moz copies X  after applying such an approach. In this work, we extended this pruning approach one step further: to match page ti-tles as well as URLs against ODP category names. This extended approach found 160,868  X  X moz copy X  pages from the ODP dataset, 24,024 (18%) more than the previous approach.

After the preprocessing described above, feature selection is per-formed based on mutual information (MI). Note that the same term may have different importance (different discriminating power) in different text fields. Therefore, rather than computing a global mu-tual information for each term, we calculated MI on a per field basis. For each field, 5000 terms with highest MI are selected. The final set of features is the union of the top 5000 features selected for each field. On our ODP dataset, 11,395 features are selected out of a vocabulary of 14 million terms.
A linear kernel support vector machine with default settings based on SVM light [18] is used as the base classifier in our experi-ments. Since SVM is a binary classifier, we generalize it using one-against-others approach, i.e., to train twelve classifiers each using one class as the positive class and the rest as the negative class. For each parameter combination we examined in the tuning process, a two-fold cross validation is performed. The reported classification accuracy is the average across the two folds.
At lower layer optimization, fields of each neighbor type are combined independently to achieve the best accuracy within that neighbor type. The questions are: what is the best combination that can be gained within each neighbor type; and for parents, how much is the benefit of emphasizing anchor text over other text.
We start by showing the benefit of emphasizing titles by investi-gating the combina tions of title and text w ithin each neighbor type (and target page itself), where the weight of title and the weight of text add up to one. Figure 4 shows the result. The x-axis is the weight of the title. A weight of 0 means classification over the whole content, without emphasizing the title . A weight of 1 means classification using only the title . The results show that although there is marginal benefit in emphasizing titles of the target and sib-lings, other neighbor types benefit noticeably (with a top relative improvement of more than 14%).

We continue by examining the usefulness of anchor text and ex-tended anchor text. However, unlike previous work [12, 15], the result is not encouraging. We combined target text with one of anchor text, extended anchor text, and parent title. The result is shown in Figure 5. Although compared with only using target text alone (74.8% accuracy), there is some benefit when target text is combined with anchor text (75.9%) or with extended anchor text Figure 5: Combining target text with anchor, extended anchor, or parent title. (76.3%). However, combining parent title with target text provides much better improvement (raising accuracy to 84.3%).

We also combined the four fields on parent page together, i.e., parent title, parent text, anchor text, extended anchor text, with their weights summing to one. As shown in Table 2, the top combina-tions with highest accuracy assign little value to anchor text and extended anchor text.

In addition, we examined the combination of the four fields on parent with the text on the targ et page (without emphasizing title of target). The result is similar: anchor text and extended anchor text is somewhat useful when combined with target text, but not as useful as the others.

F X rnkranz [12] and Glover et al. [15] showed that off-page an-chor text is quite useful. Although we also find anchor text useful, it is not as significant. The reasons could be:
Table 2: Combinations of parent fields with highest accuracy. Table 3: Summary of lower layer optimization for each neigh-bor type.
Throughout our experiments, we f ound that the title of a parent page is more important than anchor text. Possible explanations in-clude the following.
In summary, the best combination achieved in lower layer opti-mization is listed in Table 3. To show the benefit from emphasizing titles, we compared them with a baseline which classifies based on the full content of that neighbor type.
Based on the optimal combinations achieved in the lower layer optimization of each neighbor type, upper layer optimization tunes the weighting between neighbor types with the weight-ing within each neighbor type fixed. For example, if the best weight of sibling title and sib ling text found in lower layer listed in Table 3, with values 0.3 and 0.7), respectively, then their final weight in the full combination is weight sibling spectively, where weight sibling (as well as weighting of other neighbor types) is to be determined by experiments.

The top combinations with highest accuracy are listed in Table 4, which shows little va lue for child and spouse pages, low but consistent value of sibling pages, and high value of parent pages. Compared with the usefulness showed by the default Neighboring algorithm in Section 2.3, the usefulness of child and spouse is sim-ilar. Parent pages gain more importance because of emphasizing title; while siblings become less useful. Table 4: Combination of higher layer optimization with highest accuracy. Table 5: Combination of non-fielded (full) content of neighbors with highest accuracy.

In order to compare fielded classification with a non-fielded ver-sion, we also performed another tuning in which only full text of the target and its neighbors are used. The top combinations are listed in Table 5. The best combination of full text is not as accurate as the fielded combination.

After lower and upper layer optimization, the best parameter combination of all fields is listed in Table 6.
After tuning the weighting of each field, we apply the best pa-rameter setting (as listed in Table 6) to the set-aside test documents. We compare the result with SVM classifiers (based on SVM light using only target text, with the default Neighboring algorithm using the best parameter setting reported in [23], and with our implemen-tation of IO-bridge , one of the algorithms suggested by Chakrabarti et al. [6].

The main idea of IO-bridge is to build an engineered document corresponding to each document in the dataset, in which the con-structed document consists only of prefixes of the category names of the sibling pages of the target page. In IO-bridge , only the sib-ling pages within a human labeled dataset are considered. After that, the training and testing is performed on the engineered dataset rather than the original one. In the following, we compare our al-gorithm with both IO-bridge and the baseline, textual classifiers. We trained IO-bridge on the same 8,400 documents as was used to tune our F-Neighbor algorithm, and tested it on the same test set.
Note that the feature selection based on mutual information was not present in the original implementation of the default Neighbor-ing algorithm as described in [23]. However, the results reported here is based on the selected feature subset for a fair comparison. Figure 6: Comparison of accuracy of different algorithms on ODP data.

The result is shown in Figure 6. Textual SVM classifiers classi-fied 77% of the pages correctly. The default Neighboring algorithm (based on SVM) outperformed textual SVM by 5%. Previously in the optimization process, we tested combinations using only the full content of the target page and its neighbors. We also applied the best combination obtained from that experiment to the test set (labeled as  X  X ull content of neighbors X ), and got an accuracy of 86.6%. F-Neighbor algorithm further raised that to 90%, reduc-ing the error rate of the default Neighboring algorithm by almost half. Two-tailed student t-tests show that the improvements of F-Neighbor algorithm over other algorithms are statistically signifi-cant ( p&lt;&lt; .01).
In order to demonstrate our algorithm X  X  performance on more generic web pages, we also apply the trained classifier to the Web-Base dataset as mentioned in Section 4.1. Since there are no human-assigned labels for WebBase pages, we manually labeled 122 pages randomly sampled from the WebBase dataset using one of the twelve categories listed in Table 1. On our labeled subset of pages, the default Neighboring algorithm increased the accuracy of textual SVM classifiers from 31.7% to 37.7%. The F-Neighbor algorithm further improved that to 44.7%.
This paper has demonstrated the value of utilizing text fields from neighboring pages in web page classification. Here, we dis-cuss some limitations of our current approach.
This paper has exploited field information from neighboring pages to help judge the topic of a target web page. In contrast to previous approaches, our approach utilizes text from different fields from neighbors with consideration of their different impor-tance. As a result, our approach is able to generate more accurate representations of documents, facilitating more accurate classifica-tion. In summary, our contributions include the following: In the future, we plan to incorporate human-generated labels into this classification approach for further improvement.
 We thank Na Dai for helpful discussions. This material is based upon work supported by the National Science Foundation under Grant Numbers IIS-0328825 and IIS-0545875. [1] R. Angelova and G. Weikum. Graph-based text [2] G. Attardi, A. Gulli, and F. Sebastiani. Automatic web page [3] K. Balog and M. de Rijke. Finding experts and their details [4] P. Calado, M. Cristo, E. Moura, N. Ziviani, B. Ribeiro-Neto, [5] C. Castillo, D. Donato, A. Gionis, V. Murdock, and [6] S. Chakrabarti, B. E. Dom, and P. Indyk. Enhanced hypertext [7] S. Chakrabarti, M. M. Joshi, K. Punera, and D. M. Pennock. [8] S. Chakrabarti, M. van den Berg, and B. E. Dom. Focused [9] J. Cho, H. Garcia-Molina, T. Haveliwala, W. Lam, [10] W. W. Cohen. Improving a page classifier with anchor [11] B. D. Davison. Topical locality in the Web. In Proc. of the [12] J. F X rnkranz. Exploiting structural information for text [13] J. F X rnkranz. Hyperlink ensembles: A case study in [14] R. Ghani, S. Slattery, and Y. Yang. Hypertext categorization [15] E. J. Glover, K. Tsioutsiouliklis, S. Lawrence, D. M. [16] K. Golub and A. Ardo. Importance of HTML structural [17] T. H. Haveliwala. Topic-sensitive PageRank. In Proc. of the [18] T. Joachims. Making large-scale support vector machine [19] Q. Lu and L. Getoor. Link-based classification. In Proc. of [20] L. Nie, B. D. Davison, and X. Qi. Topical link analysis for [21] H.-J. Oh, S. H. Myaeng, and M.-H. Lee. A practical [22] D. Petkova and W. B. Croft. Hierarchical language models [23] X. Qi and B. D. Davison. Knowing a web page by the [24] M. Richardson and P. Domingos. The Intelligent Surfer: [25] S. Robertson, H. Zaragoza, and M. Taylor. Simple BM25 [26] S. E. Robertson and S. Walker. Some simple effective [27] G. Salton, editor. The SMART Retrieval System: Experiments [28] S. Slattery and T. Mitchell. Discovering test set regularities [29] K. Sugiyama, K. Hatano, M. Yoshikawa, and S. Uemura. [30] A. Sun, E.-P. Lim, and W.-K. Ng. Web classification using [31] H. Utard and J. F X rnkranz. Link-local features for hypertext [32] X. Xiao, Q. Luo, X. Xie, and W.-Y. Ma. A comparative study [33] Y. Yang, S. Slattery, and R. Ghani. A study of approaches to [34] S. Zhu, K. Yu, Y. Chi, and Y. Gong. Combining content and
