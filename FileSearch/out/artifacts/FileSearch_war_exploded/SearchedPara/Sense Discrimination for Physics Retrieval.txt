 Information Retrieval in technical domains like physics is characterised by long and precise queries, whose meaning is strongly influenced by term context and domain. We treat this as a disambiguation problem, and present initial find-ings of a retrieval model that posits a higher probability of relevance for documents matching disambiguated query terms. Preliminary evaluation on a real-life physics test col-lection shows promising performance improvement.
 H.3.3 [ Information Systems ]: Information Search and Re-trieval Algorithms, Experimentation, Performance Information Retrieval, Sense Discrimination
User queries in technical domains like physics tend to be long and precise, e.g. I am looking for a general ex-pression for the heat transfer coefficient in a li-quid solid interface . For such queries, standard bag of word approaches may fail to capture the dependence be-tween the query constituents, or the modification relation-ships of their components. For a shorter query this may not harm retrieval, and often the collocation of the query terms in the collection suffices. However, for longer queries, a sim-iliar approach would result in notably more collocations in the collection, causing accidental matches with nonrelevant documents. An additional problem of technical domains like physics is that words may acquire special domain-specific meanings, e.g. in the query above, expression denotes a formula. This sense is not only different to other senses that expression may convey, but also central to the user need.
We present initial findings of a retrieval model that posits a higher prior probability of relevance for documents match-ing disambiguated query terms. Disambiguation is realised with Sch  X  utze &amp; Pedersen X  X  [3] unsupervised approach that disambiguates query terms by considering their context rep-resentations in the retrieval collection. This approach out-puts a disambiguated sense for a query term, and a cluster of documents that contain the query term in its disambiguated sense. We treat this output as evidence of relevance of those documents to the query, and we embed it into the retrieval model as a prior probability. Initial findings on a real-life physics test collection are presented. Let t  X  be a query term that we wish to disambiguate. We use Sch  X  utze &amp; Pedersen X  X  [3] algorithm to build a con-text vector for each occurrence of t  X  in the collection and in the query. This context vector consists of features oc-curring within n = 10 words of t  X  (following [3]), and uses as features sequences of alphanumeric characters. Context vectors are weighted using inverse context frequency and component j of context vector ~v i of term t  X  ;  X  ( i, j ) is 1 if feature j occurs in context i , 0 otherwise; N ( j ) is the num-ber of contexts of t  X  in which feature j occurs; and N is the total number of contexts of t  X  . The weighted context vectors in the collection are then clustered into k = 20 sense clus-ters using k-means clustering. Disambiguating t  X  consists of assigning the query context vector t  X  to the closest centroid of the sense clusters computed for t  X  in the collection.
The single sense cluster assigned to t  X  consists of contexts that occur in documents in the collection. These documents are deemed by our method to contain t  X  with the same sense as used in the query. Hence, boosting the ranking of these documents may benefit retrieval performance. We imple-ment this boosting as a prior probability that these do-cuments are relevant, which we incorporate into the basic query likelihood model used for retrieval.

Given a query Q and a document D , the query likelihood model [1] ranks D by P ( D | Q ). By Bayes rule, P ( D | Q )  X  P ( Q | D ) P ( D ), where P ( Q | D ) is the probability that D gen-erates Q , and P ( D ) is the document X  X  prior probability. There are several ways of estimating P ( Q | D ), for instance using Jelinek-Mercer or Dirichlet smoothing. P ( D ) is usu-ally assumed to be uniform, in which case documents are ranked solely by P ( Q | D ). Alternatively, P ( D ) can be viewed as prior knowledge about the relevance of D [4]. Various types of prior evidence about document relevance have been implemented as P ( D ), e.g. document quality [4]. In this work, we measure the degree of match between the sense of a term in the document with its sense used in the query, and we implement it as P ( D ).

The value of P ( D ) is typically derived from measurements about the evidence being modelled as the document prior. In this work we choose a fixed value ( P ( D ) = 0 . 5) to boost those documents that disambiguate query terms. The per-formance of our approach reported in this work may further improve if prior values are tuned, however, at this stage, our aim is to test whether our approach is beneficial to retrieval, and not to optimise its performance.
We use the iSearch real-life physics test collection [2], which contains approx. half a million physics documents and 65 queries with graded relevance assessments, created by physicists. Queries contain 5 fields: 1. information need, 2. task, 3. background, 4. ideal answer, 5. keywords . We use Indri for indexing and retrieval without removing stop-words or stemming, because these settings give the highest baseline performance. Our baseline runs use the Kullback-Leibler language model [1] with Dirichlet and Jelinek-Mercer smoothing (Dir, JM resp.). Our sense disambiguation (SD) runs use Dir &amp; JM, enhanced with the SD priors presented above. We also include runs with pseudo-relevance feed-back (FB) using Indri X  X  default implementation, in order to compare our method to a more competitive approach. We use short queries (fields 1&amp;5) 1 and long queries (all fields), to check the effect of query length on SD. We measure performance with: mean average precision (MAP), binary preference (BPREF), and normalised discounted cumula-tive gain (NDCG). For each measure, we tune: (for Dir)  X  0 . 99 } ; (for FB) the number of feedback documents f bD  X  { 1 , 2 , 5 , 10 , 20 } and the number of feedback terms f bT  X  { 3 , 5 , 10 , 20 , 40 } .

Table 1 shows that for MAP &amp; NDCG, SD performs the best, with large relative improvements over the baseline and FB. Note that MAP uses binary relevance, whereas NDCG uses graded relevance; SD is shown superior on both of these measures at all times, which is a good indication of its use-fulness to IR. For BPREF, the improvement of SD over the baseline and FB is not large, but the best run for both types of queries is again a SD run. This result may be affected by a bias in iSearch: its relevance assessments contain a much larger proportion of documents judged as nonrelevant than of documents judged as relevant [2]. BPREF is affected by
We u se fields 1&amp;5 because they give higher baseline perfor-mance than field 5 alone.
 Table 1: Retrieval performance. * = stat. signif-icanc e at p &lt; 0 . 05 (2-tailed t-test). Bold = better than the baseline. this because it depends heavily on the number of judged non-relevant documents that are retrieved at higher ranks than relevant documents (whereas MAP &amp; NDCG do not dis-tinguish between non-relevant and non-judged documents). Regarding query length, the longer the query, the higher the improvement brought by SD, most likely because longer queries have lower baseline scores, so there is more room for improvement. The improvement of SD is not heavily reliant on smoothing optimisation, as Figure 1 shows (the plots of long queries and BPREF are similar to these): the improve-ment of SD is more stable across different parameter values than the baseline and FB, especially for short queries.
This poster presented initial work on a retrieval model that includes a word sense disambiguation component, the output of which is embedded into the ranking function as a prior probability. The motivation was to improve retrieval performance for technical domains like physics, where words often have domain-specific senses. Preliminary experiments on a real-life physics test collection gave positive findings; more testing on other technical collections are needed to further explore our so-far promising approach. [1] W. B. Croft and J. Lafferty. Language Modeling for [2] M. Lykke, B. Larsen, H. Lund, and P. Ingwersen. [3] H. Sch  X  utze and J. O. Pedersen. Information retrieval [4] Y. Zhou and W. B. Croft. Document quality models for
