 As an important alternative to learning a single classifier (i.e., a single deci-sion surface), ensemble-based classification approaches [ 31 ], such as bagging [ 6 ], boosting [ 12 ] and random forests [ 7 ], build several base classifiers (often using sampling) and combine their predictions to determine the final class label for a given query point. In this paper, we propose a new method for building a classifier ensemble, based on the well-known subgroup discovery problem in data mining. Many real-life datasets can be viewed as containing information about a set of entities. Further, one or more continuous or discrete valued columns in such datasets can be interpreted as a performance or evaluation measure for each entity. Given such a dataset, it is then of interest to automatically dis-cover interesting subsets of entities (also called subgroups ), such that each subset (a) is characterised by a common (shared) pattern or description; and (b) has unusual characteristics in terms of the performance attribute, as a set, when compared to the remaining set of entities. The problem of automatically dis-covering interesting subsets is well-known in data mining as subgroup discovery . on the case when the domain of possible values for the performance measure column is finite and discrete. In this paper, we use descriptors for subsets which have the form Cond  X  Class , where Cond is a conjunction of attribute-value pairs of the form A i = v j .
 class label as the performance measure for each entity, which means that each discovered interesting subset (i.e., the associated logical descriptor or rule )can be viewed as a relatively high-precision (but possibly low-recall) base classifier. These base classifiers are typically neither disjoint nor exhaustive i.e., an entity may satisfy 0, 1 or more of these rules. We empirically demonstrate that an ensemble of such base classifiers can give us an improved accuracy. Most ensemble techniques create base classifiers based on random selections whereas as we create base classifiers in a non-random manner.
 management: identifying high potential (HIPO) employees. HIPO employees are critical for  X  X uture-proofing X  the organization in the face of continual attrition of highly skilled and experienced employees, ever-present economic uncertain-ties and new business challenges [ 4 , 5 , 10 , 25 , 29 ]. Current HIPO identification processes are mostly manual (supervisor recommendations, shortlisting, inter-views and evaluation) and consequently may suffer from subjectivity, bias, dis-agreements and incompleteness. We present analytics algorithms for identifying HIPO employees using organizational databases, to supplement the manually identified ones. We present a real-life case-study involving a large multinational IT services company.
 building a classifier ensemble, based on the subgroup discovery problem in data mining. We illustrate this new approach on a new application in human resource management domain: identifying high potential employees, where we demon-strate that the proposed method outperforms other classifier ensemble methods. The paper is organized as follows. Section 2 outlines the related work. Section 3 describes the real-life case-study dataset. Section 4 applies anomaly detection and standard classification approaches. Section 5 formalizes the new approach and demonstrates its results on the case-study dataset. Section 6 discusses con-clusions and further work. There is rich work on classifier ensemble methods. Bagging creates datasets (called bootstrap samples ) by sampling with replacement the original training dataset D ,fit m base classifier models to these take majority voting among these m models to predict the class label for a given query point. Bagging is useful to improve the accuracy, when the base classifiers are  X  X nstable X . Boosting is an iterative procedure that adaptively changes dis-tribution of training samples in each round to focus base classifiers on  X  X ard X  examples. Initially each example has equal weight. A sample is drawn (with replacement) according to the sampling distribution. A classifier is induced from the dataset of selected samples. Weights are reduced (increased) for examples that are correctly (wrongly) classified. A voting-based ensemble is created using the base classifiers. The Random Forest (RF) algorithm uses decision trees as base classifiers. RF combines the predictions made by multiple decision trees using majority voting. There are several ways to construct each decision tree; e.g., one way is to use only a subset of randomly selected features when building the base decision tree and ignore the rest of the features (i.e., use a vertical partition of the data).
 Initial approaches to subgroup discovery were based on a heuristic search framework [ 13 ]. More recently, several subgroup discovery algorithms adapt well-known classification rule learning algorithms to the task of subgroup discovery. For example, CN2-SD [ 21 ] adapts the CN2 classification rule induction algorithm to the task of subgroup discovery, by inducing rules of the form They use a weighted relative accuracy (WRA) measure to prune the search space of possible rules. Roughly, WRA combines the size of the subgroup and its accuracy (difference between true positives and expected true positives under the assumption of independence between Cond and Class). They also propose several interestingness measures for evaluating induced rules. Some recent work has adopted well-known unsupervised learning algorithms to the task of subgroup discovery. [ 17 ] adapts the apriori association rule mining algorithm to the task of subgroup discovery. The SD-Map algorithm [ 2 ] adopts the FP-tree method for association rule mining to the task of minimum-support based subgroup discovery. Some sampling based approaches to subgroup discovery have also been proposed [ 26 , 27 ].
 There has been insufficient research within the HR community into the com-parison and effectiveness of HR processes used within various organizations for identifying and managing HIPO (e.g., maturity of the assessment procedures, objectivity in the identification process) [ 8 ]. Gaps are likely between the best practices reported in the literature for HIPO identification and the actual HR practices. In particular, the data and reasoning used for identifying HIPO have not been subject of much research. Even the various possible definitions of high potential have not been rigorously compared and analyzed in enough details [ 23 ]. Research in HR has focused on identifying and using appropriate attributes for HIPO identification [ 1 , 10 , 11 , 23 , 28 ]. Commonly used (subjective) variables for identifying high potentials include communication skills, results orientation, flex-ibility/adaptability, strategic thinking, decision-making skills, learning agility, teamwork, vision etc. Performance appraisals and past results are among the two of the more quantitative data sources used for HIPO identification [ 9 , 22 , 23 ]. However, these data elements are largely evaluated manually and subjectively, without recourse to more quantitative indicators from within organizational databases. Employees X  (often unfavourable) psychological reactions to the HIPO programs have been explored [ 14 , 16 ].
 We studied the HIPO identification process within a particular business unit of a large multi-national IT organization. Most employees work in software develop-ment, maintenance and support related tasks, while some are involved in tasks like consulting, business development etc. Each employee works on a project and handles specific tasks within the project; e.g., requirements analysis, software design, coding or testing. The employee is also assigned a role that is closely related to the tasks that the employee carries out in the project. About 146 roles were used for these employees. Some example roles are: Solution Architect, Test Engineer, Developer, Design Lead etc. The projects are typically done for a particular customer, who is in a particular business domain such as Insur-ance, Banking or Telecom. An employee has to log the task-wise efforts (i.e., time) spent by her for each project every day. An employee may undergo certain internal training programs and courses, which are also tracked. The projects vary in their complexity, which is often indicated by the total efforts spent on the project, the team size (larger team size usually implies a more complex project) and duration (longer lasting projects tend to be more complex). We have assumed that an employee works on only one project at a time. It is also possible that an employee may not be assigned to any customer project (e.g., when the person is doing internal work, training, on long leave etc.). We used 51 variables for each employee; e.g., experience, efforts (rolewise, technology-wise etc.), trainings etc. These variables are  X  X umulative X  over time; e.g., the variable leadership course count gives the total number of leadership related courses that the employee has completed so far (i.e., since joining the organization). tion about the employees as on 01-April-2011 and 01-April-2012 respectively. For uniformity, we have selected a subset of employees in this unit, consisting of those that belong to 3 linearly ordered grades G3, G4 and G5 (G5 being the higher grade). The grades include mostly software engineers and increase with experience and denote higher responsibilities; employees typically spend 3 to 5 years in a grade. The grade G5 usually correspond to leadership responsibili-ties, often based on technical knowledge, domain knowledge, project and team management skills or business development skills. Table 1 shows the basic demo-graphic summary of both these datasets. The org. experience denotes the time spent by the employee in the organization and the total experience equals org. experience + the experience the employee had prior to joining the organization. ees for this unit, created on 01-Apr-2011 and 01-Apr-2012 respectively. Both these lists were created using the manual HIPO identification process as prac-ticed within the organization. Table 2 shows summaries of both these HIPO lists. Note that the % of employees in HIPO lists grows along the grades i.e., the higher grades tend to have a higher percentage of employees designated as HIPO. This is because of two reasons. First, the grade sizes give a roughly pyramid shape to the organization, with the senior grades being smaller than the junior grades. Secondly, the progression to higher grades is based on performance and through a controlled promotion process. Only employees with proven track record and demonstrated high performance (and not just experience) tend to get promoted into the higher grades. 4.1 Anomaly Detection An obvious way to detect HIPO employees in an unsupervised manner is to use anomaly detection techniques. The experimental results (Table 3 ) show that the anomaly detection techniques are not that effective in identifying HIPO employ-ees. The values in brackets are the parameters used. The Mahalanobis algorithm computes the Mahalanobis distance of each point from the mean vector and lists the top K as potentially anomalies. The Knorr algorithm [ 19 ] classifies a point P as an anomaly if the number of points within a given distance than the given number M . For each point P , the RRS algorithm [ 24 ] computes the distance of the k 0 -th nearest neighbour (for a given points having maximum value for this distance as anomalies. A possible reason why HIPO employees need not correspond to anomalies is that HIPO employ-ees may not have unusually large values on all attributes; more often they have unusual combinations of values for different attributes -a reason that motivated us to explore subgroup discovery techniques.
 4.2 Classification Since we have labeled data, the next obvious step is to try classification tech-niques. The training data consists of employees, each described by the 51 vari-ables. There are two classes of employees, HIPO or NON-HIPO i.e., there is a binary class label HIPO Flag for each employee (+1 if the employee is in HIPO list and  X  1 otherwise). We used several built-in classification algorithms in the well-known WEKA tool to learn the HIPO classification model on dataset D1 (a separate model for each grade) and applied it to predict the HIPO employees in D2. The prediction results are shows in Table 4 . Naive Bayes models are overall the best; Decision Tree models have consistently the best precision. The use of boosting with different classifiers sometimes showed a slight improvement, but more often reduced the accuracy; and the best performance was always without boosting. Interestingly, when we applied various feature selection methods to work with a reduced subset of features, generally, the accuracy did not improve. Subgroup discovery is well-known in data mining, where the problem is to discover logically characterized subsets of the given dataset, such that each subset is  X  X nteresting X  in some way. We use the class label of each point in the training dataset as a measure and adapt a subgroup discovery algorithm to discover interesting subsets of employees in dataset D1. Each green flag or positive rule has the form Cond  X  Class = +1 and corresponds to a subset of employees which has an  X  X nusually high X  % of HIPO employees (i.e., employees with label +1). Similarly, each red flag or negative rule has the form and corresponds to a subset of employees which has an  X  X nusually high X  % of non-HIPO employees (i.e., employees with label  X  1). Each discovered rule has high precision and often low recall. The discovered rules need not be disjoint nor exhaustive i.e., an employee may satisfy 0, 1 or more rules. An employee may even satisfy multiple green flag rules as well as multiple red flag rules. Before applying the subgroup discovery algorithm, we discretized each vari-able, by dividing its range into 5 intervals of equal width, denoted For example, variable V44 has min and max values of 0 and 28 respectively, for grade G5 in dataset D1. Then the corresponding 5 equal length intervals are I 1=[0 , 5 . 6] ,I 2=[5 . 6 , 11 . 2] and so forth. To control the quality of the green flag rules, we specify values for parameters minimum precision size n 0 . We retain only those green flag rules whose precision is at least which apply to a minimum of n 0 employees. In addition, we define a parameter k , which we use to further prune the rules in G as follows: order the rules in G (retained using given values of n 0 ,p 0 ) in descending order of their precision, retain only the top k 0 rules and discard the rest. If k 0 step. We define corresponding parameters p 1 , n 1 and k 1 shows some green flag rules for grade G4, with n 0 =15and second rule in Table 5 is true for 20 employees in grade G5 in dataset D1, out of which 12 are in the HIPO list L1, yielding the precision of 0 the number of green flag rules generated on dataset D1 for different grades for different values of p 0 ( n 0 = 15).
 G3:: 0.20:32, 0.25:22, 0.30:3 G4:: 0.30:665, 0.35:203, 0.40:57, 0.45:9, 0.50:4 G5:: 0.30:304, 0.35:170, 0.40:60, 0.45:20, 0.50:14 dataset D1 using particular values for the parameters n 0 given grade. There are several ways in which we can devise ensemble methods to combine these green and red flag rules in G and R to predict the class label (HIPO or non-HIPO) for any given employee e in D2. Let n g respectively denote the number of green and red flag rules satisfied by the given employee e  X  D 2. 1. Voting-threshold : This method uses only the green flag rules. The intuition 2. Employee-ranking : This method uses only the green flag rules. We rank 3. Combined-voting-threshold : This method uses both green and red flag 4. Combined-employee-ranking : This method uses both green and red flag We can define more ensemble methods. For example, we could weigh the vote of each rule in Voting-threshold or Combined-employee-ranking by the precision of that rule, and then define a cut-off based on the aggregation of weighted votes. For brevity, we omit these details; their results are very similar to the results of the above methods. To find the best values for the parameters, we search over the space of all possible parameter (discretized) value combinations. For employee ranking methods, we use the same value of K as used in Table 3 . Table 6 shows the best results obtained for each of the above methods, along with the corresponding values for the parameters. Strictly speaking, we should search over dataset D1 to find the best values of the parameters (since we cannot really assume that class labels are available for D2) and use them to predict the results on dataset D2. However, we perform the search for best parameter values on D2 itself, in order to show the best possible results that above methods can potentially achieve on D2. The parameter settings identified on D1 and applied to D2 yield very slightly lower results.
 Figure 1 shows some charts (for Combined-voting-threshold applied to grade G5 in D2) that depict the effect of varying one parameter and keeping all others constant on the oeverall accuracy. Increasing  X  m initially increases the accuracy but after some point leads to reduced accuracy, because requiring a larger difference between green and red flags before an employee is classified as +1 is a stricter decision rule. Increasing k 0 leads to using lesser precision rules, which leads to increased accuracy but eventually leads to a slow reduction in the accuracy. Increasing p 0 eventually leads to reduced accuracy, because higher values of p 0 lead to using lesser number of rules. The effect of is similar, though much less pronounced. In a similar manner, Fig. 2 shows the precision-recall curves (closed related to ROC curve) (for Combined-voting-threshold applied to grade G5 in D2) obtained by varying one parameter and keeping all others constant.
 Increasing the discretization level of an attribute (no. of discrete values) generally reduces the accuracy. For example, with 6, 7 and 9 discretization levels for all numeric atributes, the F -measure values for grade G4 for the Combined-voting-threshold ensemble methods were 0.523, 0.501 and 0.461 (compared to 0.543 for discretization level 5). We observed similar trends for other grades and other ensemble methods.
 We also tried different subgroup discovery methods using the CORTENA tool ( http://datamining.liacs.nl/cortana.html ). The results are generally similar or better. For example, the beam-search based subgroup discovery algorithm in CORTANA resulted in the following F -measure values for grade G4 for the 4 ensemble methods: 0.575, 0.556, 0.605 and 0.588 (compared to 0.540, 0.532, 0.543 and 0.548). In this paper, we proposed novel ensemble methods that combine the interest-ing subgroups (discovered using standard subgroup discovery techniques) into a single clsssifier. We applied these new ensemble methods, along with standard anomaly detection and classification, to automatically identify high potential (HIPO) employees -an important problem in management. We showed through experiments that the new ensemble methods perform better than other meth-ods, including other ensemble methods on a real-life case-study dataset of a large multinational IT services company. The algorithms have been implemented and are being used by HR managers to augment the manually prepared HIPO employee lists and for other tasks like succession planning, role changes etc. Effectiveness of our approach can be improved by using different types of logical expressions such as DNF. One could use clustering, than subgroup dis-covery, to find dense regions in the training datasets which have a predominance of one class, characterize each region by an appropriate classifier and create an ensemble of them. Effectiveness of the proposed analytics algorithms is depen-dent on the completeness and quality of the employee work history data. HIPO employee identification is often a speculative and subjective judgmental process that makes use of organizational citizenship (engagement) data as well as behav-ioural and personality related human factors, which are not well-captured in business data. We are looking at using such datasets, if available. What costi-tutes true outstanding achievements is context dependent; we need models that include domain knowledge to  X  X nderstand X  the actual work of employees. Poten-tial is related to the future performance, of which past data is only a limited indicator. Much depends on external factors, opportunities available and teams involved. We are looking at building human performance prediction models.
