 Spectral clustering refers to a flexible class of clustering proce-dures that can produce high-quality cl usterings on small data sets but which has limited applicability to large-scale problems due to its computational complexity of O ( n 3 ) in general, with n the num-ber of data points. We extend the range of spectral clustering by developing a general framework for fast approximate spectral clus-tering in which a distortion-minimizing local transformation is first applied to the data. This framework is based on a theoretical anal-ysis that provides a statistical characterization of the effect of local distortion on the mis-clustering rate. We develop two concrete in-stances of our general framework, one based on local k -means clus-tering (KASP) and one based on random projection trees (RASP). Extensive experiments show that these algorithms can achieve sig-nificant speedups with little degradation in clu stering accuracy. Spe-cifically, our algorithms outperform k -means by a large margin in terms of accuracy, and run several times faster than approximate spectral clustering based on the Nystr X m method, with comparable accuracy and significantly smaller memory footprint. Remarkably, our algorithms make it possible for a single machine to spectral cluster data sets with a million observations within several min-utes.
 H.3.3 [ Information Search and Retrieval ]: Clustering; I.2.6 [ Art-ificial Intelligence ]: Learning Algorithms, Experimentation, Performance Unsupervised Learning, Spectral Clustering, Data Quantization
Clustering is a problem of primary importance in data mining, statistical machine learning and scientific discovery. An enormous variety of methods have been developed over the past several decades to solve clustering problems [15, 19]. A relatively recent area of focus has been spectral clustering , a class of methods based on eigendecompositions of affinity, dissimilarity or kernel matri-ces [20, 28, 31]. Whereas many clustering methods are strongly tied to Euclidean geometry, making explicit or implicit assump-tions that clusters form convex regions in Euclidean space, spectral methods are more flexible, capturing a wider range of geometries. They often yield superior empirical performance when compared to competing algorithms such as k -means, and they have been suc-cessfully deployed in numerous applications in areas such as com-puter vision, bioinformatics, and robotics. Moreover, there is a sub-stantial theoretical literature suppor ting spectral clustering [20, 34].
Despite these virtues, spectral clustering is not widely viewed as a competitor to classical algorithms such as hierarchical clustering and k -means for large-scale data mining problems. The reason is easy to state X  X iven a data set consisting of n data points, spectral clustering algorithms form an n  X  n affinity matrix and compute eigenvectors of this matrix, an operation that has a computational complexity of O ( n 3 ) in general. For applications with n on the order of thousands, spectral clustering methods begin to become infeasible, and problems with n in the millions are entirely out of reach.

In this paper we focus on developing fast approximate algorithms for spectral clustering. Our approach is not fundamentally new. As in many other situations in data mining in which a computa-tional bottleneck is involved, we aim to find an effective preproces-sor that reduces the size of the data structure that is input to that bottleneck (see, e.g., [25, 27]). There are many options that can be considered for this preprocessing step. One option is to perform various forms of subsampling of the data, selecting data points at random or according to some form of stratification procedure. An-other option is to replace the original data set with a small number of points (i.e.,  X  X epresentatives X ) that aim to capture relevant struc-ture. Another approach that is specifically available in the spectral clustering setting is to exploit the literature on low-rank matrix ap-proximations. Indeed, this last approach has been the approach most commonly pursued in the literature; in particular, several re-searchers have proposed using the Nystr X m method for this pur-pose [10, 35, 12]. While it is useful to define such preprocessors, simply possessing a knob that can adjust computational complexity does not constitute a solution to the problem of fast spectral cluster-ing. What is needed is an explicit connection between the amount of data reduction that is achieved by a preprocessor and the sub-sequent effect on the clustering. Indeed, the motivation for using spectral methods is that they can pr ovide a high-quality clustering, and if that high-quality clustering is destroyed by a preprocessor then we should consider other preprocessors (or abandon spectral clustering entirely). In particular, it is not satisfactory to simply re-duce the rank of an affinity matrix so that an eigendecomposition can be performed in a desired time frame, unless we have certain understanding of the effect of this rank reduction on the clustering.
In this paper we propose a general framework for fast spectral clustering and conduct an end-to-end theoretical analysis for our method. In the spirit of rate-distortion theory, our analysis yields a relationship between an appropriately defined notion of distor-tion at the input and some notion of clustering accuracy at the out-put. This analysis allows us to argue that the goal of a preprocessor should be to minimize distortion; by minimizing distortion we min-imize the effect of data reduction on spectral clustering.
To obtain a practical spectral clustering methodology, we thus make use of preprocessors that minimize distortion. In the current paper we provide two examples of such preprocessors. The first is classical k -means, used in this context as a local data reduction step. The second is the Random Projection tree (RP tree) of [8]. In either case, the overall approximate spectral clustering algorithm takes the following form: 1) coarsen the affinity graph by using the preprocessor to collapse neighboring data points into a set of local  X  X epresentative points, X  2) run a spectral clustering algorithm on the set of representative points, and 3) assign cluster memberships to the original data points based on those of the representative points.
Our theoretical analysis is a perturbation analysis, similar in spirit to those of [20] and [28] but different in detail given our focus on practical error bounds. It is also worth noting that this analysis has applications beyond the design of fast approximations to spec-tral clustering. In particular, as discussed by [18], our perturbation analysis can be used for developing distributed versions of spectral clustering and for analyzing robustness to noise.

The remainder of the paper is organized as follows. We begin with a brief overview of spectral clustering in Section 2, and sum-marize the related work in Section 3. In Section 4 we describe our framework for fast approximate spectral clustering and discuss two implementations of this framework X  X  X ASP, X  which is based on k -means, and  X  X ASP, X  which is based on RP trees. We eval-uate our algorithms in Section 5, by comparing both KASP and RASP with Nystr X m approximation and k -means. We present our theoretical analysis in Section 6. In particular, in that section, we provide a bound for the mis-clustering rate that depends linearly on the amount of perturbation to the original data. We then turn to an analysis of the performance of our approximate algorithms in Section 7. Finally, we conclude in Section 8.
Given a set of n data points x 1 ,..., x n , with each x i we define an affinity graph G =( V, E ) as an undirected graph in which the i th vertex corresponds to the data point x i . For each edge ( i, j )  X  E , we associate a weight a ij that encodes the affinity (or similarity) of the data points x i and x j . We refer to the matrix A =( a ij ) n i,j =1 of affinities as the affinity matrix .
The goal of spectral clustering is to partition the data into m dis-joint classes such that each x i belongs to one and only one class. Different spectral clustering algorithms formalize this partitioning problem in different ways [31, 26, 28, 37]. In the current paper we adopt the normalized cuts (Ncut) formulation [31]. 1 Define W ( V 1 ,V 2 )= sets V 1 and V 2 of V .Let V =( V 1 ,...,V m ) denote a partition of Algorithm 1 SpectralClustering ( x 1 ,..., x n ) Input : n data points { x i } n i =1 , x i  X  R d Output : Bipartition S and  X  S of the input data 1. Compute the affinity matrix A with elements: 2. Compute the diagonal degree matrix D with elements: 3. Compute the normalized Laplacian matrix: 4. Find the second eigenvector v 2 of L 5. Obtain the two partitions using v 2 : 6. S = { i : v 2 i &gt; 0 } ,  X  S = { i : v 2 i  X  0 } V , and consider the following optimization criterion: In this equation, the numerator in the j th term is equal to the sum of the affinities on edges leaving the subset V j and the denominator is equal to the total degree of the subset V j . Minimizing the sum of such terms thus aims at finding a partition in which edges with large affinities tend to stay within the individual subsets V which the sizes of the V j are balanced.

The optimization problem in (1) is intractable and spectral clus-tering is based on a standard relaxation procedure that transforms the problem into a tractable eigenvector problem. In particular, the relaxation for Ncut is based on rewriting (1) as a normalized quadratic form involving indicator vectors. These indicator vectors are then replaced with real-valued vectors, resulting in a general-ized eigenvector problem that can be summarized conveniently in terms of the (normalized) graph Laplacian L of A defined as fol-lows: where D = diag ( d 1 , ..., d n ) with d i =
Ncut is based on the eigenvectors of this normalized graph Lapla-cian. The classical Ncut algorithm focuses on the simplest case of a binary partition [31], and defines multiway partitions via a recur-sive invocation of the procedure for binary partitions. In the case of a binary partition, it suffices to compute the second eigenvector of the Laplacian (i.e., the eigenvector with the second smallest eigen-value). The components of this vector are thresholded to define the class memberships of the data points. Although spectral clustering algorithms that work directly with multiway partitions exist [4, 37], in the current paper we will focus on the classical recursive Ncut algorithm. We assume that the number of clusters is given a pri-ori and we run the recursion until the desired number of clusters is reached. See Algorithm 1 for a specific example of a spectral bipartitioning algorithm where a Gaussian kernel is used to define the pairwise affinities.
An influential line of work in graph partitioning approaches the partitioning problem by reducing the size of the graph by collaps-ing vertices and edges, partitioning the smaller graph, and then un-coarsening to construct a partition for the original graph [17, 22]. Our work is similar in spirit to this multiscale approach, provid-ing theoretical analysis for a particular kind of coarsening and un-coarsening methodology. More generally, our work is related to a tradition in the data mining community of using data preprocessing techniques to overcome computational bottlenecks in mining large-scale data. Examples include [27], who proposed a nonparametric data reduction scheme based on multiscale density estimation, and [5], who proposed a fast algorithm to extract small  X  X ore-sets X  from the input data, based on which (1 + ) -approximation algorithms for the k -center clustering have been developed.

Our work is also related to the literature on kernel-based learn-ing, which has focused principally on rank reduction methods as a way to attempt to scale to large data sets. Rank reduction refers to a large class of methods in numerical linear algebra in which a matrix is replaced with a low-rank approximation. These methods have been widely adopted, particularly in the context of approxi-mations for the support vector machine (SVM) [10, 35, 11, 32]. The affinity matrix of spectral clustering is a natural target for rank reduction. In particular, [12] have used the Nystr X m approxima-tion, which samples columns of the affinity matrix and approxi-mates the full matrix by using correlations between the sampled columns and the remaining columns. A variety of sampling pro-cedures can be considered. [35] use uniform sampling without re-placement, and [12] use a similar strategy in applying the Nystr X m method image segmentation. A drawback of these procedures is that they do not incorporate any information about the affinity ma-trix in choosing columns to sample; moreover, they do not come with performance guarantees. [10] replace the uniform sampling step with a judiciously-chosen sampling scheme in which columns of the Gram matrix are sample d with probability proportional to their norms. While this yields a rigorous bound on the approxima-tion error of Gram matrix, this method may need to select a large number of columns to achieve a small approximation error (e.g., for data generated from Gaussian mixture model, when the Gaus-sian kernel is used, the number of columns sampled is expected to be O ( n ) ).

Although the Nystr X m method reduces the rank of the kernel matrix, its working memory requirement can be very high (see Sec-tion 5 for more details). Another issue with the Nystr X m method is that in data sets that are unbalanced the number of observations selected by the sampling procedure from the small clusters may be small (if not zero), which can cause small clusters to be missed and may potentially lead to problems with numerical stability.
In this section we present our algorithmic framework for fast spectral clustering. Our approach reposes on the theoretical analy-sis of spectral clustering that we present in Section 6. In that section we establish a quantitative relationship between the mis-clustering rate at the output of a spectral clustering algorithm and the dis-tortion in the input. This motivates our interest in algorithms that invoke a distortion-minimizing transformation on the original data before performing spectral clustering.

Our algorithm consists of a data preprocessing step and the spec-tral clustering step. In the current section we present two different ways of achieving the first step: one is based on k -means and the other is based on random projection trees. We have chosen these two approaches because of their favor able computational properties and the simplicity of their implementation; other methods (e.g., k -nearest neighbor, weighted k -means [9], etc.) can be used as well. Table 1 summarizes our notation.
Vector quantization is the problem of choosing a set of represen-tative points that best represent a data set in the sense of minimizing a distortion measure [13]. When the distortion measure is squared error, the most commonly used algorithm for vector quantization is k -means, which has both theoretical support and the virtue of simplicity. The k -means algorithm employs an iterative procedure. At each iteration, the algorithm assign each data point to the near-est centroid, and recalculates the cluster centroids. The procedure stops when the total sum of squared error stabilizes.

The use that we make of k -means is as a preprocessor for spec-tral clustering. In particular, we propose a  X  k -means-based approx-imate spectral clustering X  (KASP) algorithm that has the form in Algorithm 2.
 Algorithm 2 KASP ( x 1 ,..., x n ,k ) Input : n data points { x i } n i =1 , # representative points k Output : m -way partition o f the input data 1. Perform k -means with k clusters on x 1 ,..., x n to: a) Compute the cluster centroids y 1 ,..., y k as the b) Build a correspondence table to associate each x i 2. Run a spectral clustering algorithm on y 1 ,..., y k to obtain a m -way cluster membership for each of y i . 3. Recover the cluster membership for each x i by looking up the cluster membership of the corresponding centroid y j in the correspondence table.

The computational complexity of step 1, k -means, is O ( knt ) , where t is the number of iterations 2 . Given that the complexity of step 2 is O ( k 3 ) and the complexity of step 3 is O ( n ) , the overall computational complexity of KASP is O ( k 3 )+ O ( knt ) .Inthe evaluation section we compare KASP to the alternative of simply running k -means on the entire data set.
RP trees are an alternative to k -means in which a distortion-reducing transformation is obtained via random projections [8]. An RP tree gives a partition of the data space, with the center of the mass in each cell of the partition used as the representative for the data points in that cell. RP trees are based on k -d trees, which are spatial data structures that partition a data space by re-cursively splitting along one coordinate at a time [2]. Rather than splitting along coordinate directions, RP tree splits are made ac-cording to randomly chosen directions. All points in the current cell are projected along the random direction and the cell is then split. While classical k -d trees scale poorly with dimensionality of the data space due to the restriction to axis-parallel splits, RP trees more readily adapt to the intrinsic dimensionality of the data.
Using the RP tree as a local distortion-minimizing transforma-tion, we obtain the  X  X P-tree-based approximate spectral clustering X  (RASP) algorithm by replacing step 1 in Algorithm 2 with: The total computational cost of this method is O ( k 3 )+ O ( hn ) , where the O ( hn ) term arises from the cost of building the h -level random projection tree.
Before turning to our theoretical analysis of KASP and RASP, we first present a comparative empirical evaluation of these al-gorithms. We have conducted experiments with data sets of var-ious sizes taken from the UCI machine learning repository [3]; an overview is given by Table 2. For the USCI (US Census Income) data set, we excluded instances and features that contain missing items, and were left with 285,799 instances with 37 features, with all categorical variables converted to integers. The Poker Hand data set is extremely unbalanced, so we merged small classes to-gether while leaving the large classes untouched. We obtained 3 fi-nal classes which correspond to about 50 . 12% , 42 . 25% and 7 . 63% of the total number of instances, respectively. We normalized the Connect-4 and USCI data sets so that all features have mean 0 and standard deviation 1. For spectral clustering, we set kernel band-widths via a cross-validatory search in the range [0 , 200] (with step size 0 . 1 ) for each data set.

Spectral algorithms have not previously been studied on data sets as large as one million data points; the largest experiment that we are aware of for spectral algorithms involves the MNIST data set, which consists of 60,000 handwritten digits. In particular, [14] re-ported experiments using this data set, where a total running time of about 30 hours was required when using a fast iterative algorithm.
We used two quantities to assess the clustering performance: the running time and the clustering accuracy as measured by using the true class labels associated with each of the data sets. Our experi-ments were performed on a Linux machine with 2.2 GHz CPU with 32 GB main memory. The running time was taken as the elapsed time (wall clock time) for clustering. Clustering accuracy was com-puted by counting the fraction of labels given by a clustering algo-rithm that agree with the true labels. This requires a search over permutations of the classes. Let z = { 1 ,...,k } denote the set of class labels, and  X  (  X  ) and f (  X  ) denote the true label and the la-bel given by the clustering algorithm of a data point, respectively. Formally, the clustering accuracy  X  is defined as where I is the indicator function and  X  z is the set of all permuta-tions on z .
We compare the performance of KASP and RASP with two com-peting algorithms: k -means clustering and Nystr X m approximation based spectral clustering (referred to simply as Nystr X m hence-forth) as implemented in [12]. Unless other specified, all algo-rithms were implemented in R code, which is available at http: //www.cs.berkeley.edu/~jordan/fasp.html .

The existing work on spectral clustering has focused principally on rank reduction methods as a way to scale to large-size data. We thus compare KASP and RASP algorithms to the rank reduction ap-proach, focusing on the Nystr X m approximation. The idea of Nys-tr X m is to sparsify the Laplacian matrix by random sampling and then take advantage of the fact that eigendecomposition is usually much faster on a sparse matrix. There are several variants available for Nystr X m extension based spectral clustering, and we choose the Matlab implementation due to Fowlkes et al. [12].

The performance of k -means can vary significantly depending on the initialization method. Recen tly a variety of approaches have been proposed for the initialization of k -means [21, 1, 30, 24, 6]. We chose to study three initializa tion methods, based on their doc-umented favorable performance [6, 29, 30, 24], as well as their rel-atively straightforward implementation: the Hartigan-Wong algo-rithm (KM-1) [16], the sampling-based two-stage algorithm (KM-2) (i.e., the Matlab implementation of k -means with the  X  X luster X  option), and the Bradley and Fayyad algorithm (BF) [6]. For all k -means results we report the highest level of accuracy attained across these three algorithms for each data set.
 KM-1 is simply the R function kmeans () with option  X  X artigan-Wong. X  This function has two parameters, n rst and n it ,which denote the number of restarts and the maximal number of iterations during each run, respectively. We ran KM-1 with ( n rst ,n (20 , 200) , (50 , 200) , (20 , 1000) , respectively.

KM-2 consists of two stages of k -means. Theideaistorun k -means in the first stage on a subset of the data to obtain good initial centroids so that substantially fewer iterations are required for k -means in the second stage. In the first stage, we sample 10% of the data uniformly at random, and run k -means with k clusters. In the second stage, we run k -means with the k cluster centers obtained in the first stage as initial centroids. The parameters for k -means were chosen to be ( n rst ,n it )=(20 , 200) for the first stage and ( n rst ,n it )=(1 , 200) for the second stage.

BF consists of three stages of k -means. In the first stage, BF runs k -means several times (we used 10 runs) on randomly selected subsets, using, say, a fraction  X  of the entire data set. The output centroids from all individual runs constitutes a new data set, on which the second stage k -means runs. The centroids so obtained are used as the initial cluster centers for the third stage of k -means. In our experiment, we fixed the parameters ( n rst ,n it )=(20 , 200) for the first and second stages and ( n rst ,n it )=(1 , 100) for the third stage, while varying  X   X  X  0 . 01 , 0 . 05 , 0 . 1 }
The above are the standard settings for our first set of experi-ments. See below for discussion of an additional set of experiments in which the running time of k -means was matched to that of KASP.
ImageSeg 51.15 51.10 54.76 58.95 53.66 penDigits 52.85 54.40 51.63 53.36 53.02 mGamma 64.91 70.97 68.60 70.61 70.36 Table 3: Evaluation of k -means, Nystr X m and KASP on medium-size data Medium-size data. We first evaluated k -means using different ini-tialization methods and parameter configurations on the medium-size data sets. The complete results are documented in [36]; here we report the best result across all k -means experiments. This result is reported in the second column of Table 3, which also presents the performance of Nystr X m and our KASP methods on the medium-size data sets.

We run KASP with KM-2 for data preprocessing using different data reduction ratios  X  ,where  X  is the ratio of the size of original data set to the reduced data set. As expected, we see that k -means runs the fastest among the three methods; KASP runs faster and re-quires less working memory than Nystr X m when both of them use the same data reduction ratio (  X  =8 ). In terms of accuracy, KASP and Nystr X m are comparable with each other, and both are better than k -means, particularly on the data sets Musk and mGamma. From Table 3 we also see that the running time and working mem-ory required by KASP decrease substantially as the data reduction ratio  X  increases, while incurring little loss in clustering accuracy. In fact, we see that sometimes clustering accuracy increases when we use the reduced data. (This is presumably due to the regulariz-ing effect of the pre-grouping, where neighboring observations are forced into the same final clusters.) Large-size data. We now turn to the three large-size data sets. We present the results of the KASP algorithm in in Table 4, where we note that we have used relatively large data reduction ratios  X  due to the infeasibility of running spectral clust ering on the origi-nal data. For each data set, we observe that when we increase the data reduction ratio  X  , there is little degradation in clustering accu-racy while both computation time and working memory decrease substantially.

In our experiments on RASP, we used the C++ implementation of Nakul Verma to build the RP tree [8] and used this as input to our spectral clustering algorithm (implemented in R). We varied the tree depth and required that each leaf node in the tree contains at least 50 data points. The running time of RASP consists of three parts X  X he construction of the tree, spectral clustering on the re-duced set, and the cluster membership recovery. The results for RASP are shown in Table 5. Here we again see that accuracy does not decrease over this range of data reduction values. Comparing Table 5 and Table 4, we see that RASP is roughly comparable to
Connect-4 65.70 65.69 65.70 65.69
Poker Hand 50.03 50.01 50.01 49.84 Table 4: Evaluation of KASP on the three large-size data sets with differ-Table 5: Evaluation of RASP on the three large-size data sets with differ-KASP in terms of both speed and accuracy. Due to the random nature of RASP during the tree construction stage, we are not able to match the data reduction ratio in RASP to that of KASP. Hence only a rough comparison is possible between KASP and RASP.
In Table 6 we compare our methods (using the largest values of the reduction ratio) to k -means, again using different initialization methods and parameter configurations for k -means and reporting the best result as the third column of the table (the complete results are documented in [36]). We again see the significant improve-ment in terms of accuracy over k -means for two of the data sets. We also compared to Nystr X m, where the memory requirements of Nystr X m forced us to restrict our experiments to only the largest values of the data reduction ratios studied for KASP and RASP. We see that KASP and Nystr X m have comparable clustering accu-racy. As for the running time, we see from Table 6 that KASP (and RASP) are 3-5 times faster than Nystr X m. (Note also that KASP and RASP were implemented in R and Nystr X m runs in Matlab; the slowness of R relative to Matlab suggests that we are underestimat-ing the difference.) Another difficulty with Nystr X m is the memory requirement, which is of order O ( n 2 ) . The actual memory usages were approximately 4 GB, 12 GB and 17 GB, respectively, for the three large data sets, while the working memory required by KASP waslessthan 1 GB.

Given the large size of these data sets we are not able to assess the loss in clustering accuracy due to data reduction in KASP and RASP relative to the original data set (because we are unable to
Connect-4 75.00 65.33 65.82 65.69 63.95
Poker Hand 60.63 35.56 50.24 49.84 49.70 Table 6: Comparison of Random Forests (RF) classification, k -means, run spectral clustering on the original data). Instead, to provide a rough upper bound, we treat the clustering problem as a classifi-cation problem and present results from a state-of-the-art classifi-cation algorithm, the Random Forests (RF) algorithm [7]. These results suggest that the data reduction in KASP and RASP have not seriously degraded the clustering accuracy.

We also performed a further comparison of k -means and our methods in which we increased the number of restarts and itera-tions for k -means so that the running time matches that of KASP on the large data sets. For these experiments we used the BF imple-mentation of k -means. Our results, which are documented in [36], showed that the longer runs of k -means did not yield significant improvements in accuracy to the results we have reported here; k -means continued to fall significantly short of KASP and Nystr X m on USCI and Poker Hand.
In this section we present a theoretical analysis of the effect on spectral clustering of a perturbation to the original data. Section 7 shows how this analysis applies to the specific KASP and RASP algorithms. Fig. 1 summarizes the data processing procedure of our algorithms, and Fig. 2 summarizes the underlying analysis. It is worth noting that our analysis is a general one, applicable to a variety of applications of spectral clustering. In particular, per-turbations arise when the original data are truncated, compressed, filtered, quantized or distorted in some way. These degradations may be unavoidable consequences of a noisy channel, or they may arise from design decisions reflecting resource constraints, compu-tational efficiency or privacy considerations.

We work with an additive noise model for the data perturbation analysis, due to its simplicity and its proven value in a number of problem areas such as data filtering, quantization and compression. We assume that the original data x 1 ,..., x n are independently and identically distributed (i.i.d.) according to a probability distribution G , and we treat data perturbation as adding a noise component to for each i =1 ,...,n , and we denote the distribution of  X  To make the analysis tractable, we further assume that: (1) independent of x i , which is a good approximation for many real Figure 1: Data processing (top-applications [13]; (2) all components of all i s are independent with a symmetric distribution with mean zero and bounded support; (3) the variance of i is small relative to that of the original data, a natural assumption in our setting in which we control the nature of the data transformation.

We aim to investigate the impact on the clustering performance of the perturbation. Specifically, we wish to assess the difference between the clustering obtained on the original x 1 ,..., obtained on the perturbed data  X  x 1 ,...,  X  x n . We quantify this differ-ence by the mis-clustering rate , which is defined as where I is the indicator function, I =( I 1 ,...,I n ) being a vec-tor indicating the cluster membership for x 1 ,..., x n ,and (  X 
I ,...,  X  I n ) for  X  x 1 ,...,  X  x n .

Our approach to quantify (i.e., to upper bound) the mis-clustering rate  X  consists of two components: (1) a bound that relates  X  to the perturbation of the eigenvector used in spectral clustering (see Sec-tion 6.1); (2) a perturbation bound on the matrix norm of the Lapla-cian in terms of the amount of data perturbation (see Section 6.2). 6.1 Mis-clustering rate via the 2 nd eigenvector
Let  X  A and  X  L denote the affinity matrix and the Laplacian ma-trix, respectively, on the perturbed data. We wish to bound mis-clustering rate  X  in terms of the magnitude of the perturbation =  X  x  X  x . In our early work we derived such a bound for two-class clustering problems [18]. The bound is expressed in terms of the perturbation of the second eigenvector of the Laplacian matrix. We begin by summarizing this result. Let v 2 and  X  v 2 denote the unit-length second eigenvectors of L and  X  L , respectively, then we can bound the mis-clustering rate of a spectral bipartitioning algorithm (a spectral clustering algorithm that forms two classes) as follows.
T HEOREM 1 ([18]). Under the assumptions discussed in [18], the mis-clustering rate  X  of a spectral bipartitioning algorithm on the perturbed data satisfies
There are two limitations to this result that need to be overcome to be able to use the result in our design of a fast spectral clustering algorithm. First, the bound needs to be extended to the multiway clustering problem. We achieve that by considering recursive bipar-titionings. Second, we need to estim ate the amount of perturbation to the second eigenvector of the Laplacian matrix. In [18] this was done by assuming availa bility of the perturb ed data, an assump-tion which is reasonable for applications that involve resource con-straints in a distributed computing environment, but which is not appropriate here. We instead approach this problem via a model-based statistical analysis, to be discussed in Section 6.2. That anal-ysis allows us to bound the perturbation of the Laplacian matrix expressed in terms of a Frobenius norm. To connect that analysis to Theorem 1, we make use of the following standard lemma.
L EMMA 2 ([33]). Let g denote the eigengap between the sec-ond and the third eigenvalues of L . Then the following holds: With these links in the chain of the argument in place, we turn to a discussion of these two remaining problems, that of dealing with multiway clusterings and t hat of bounding the norm of the perturbation of the Laplacian matrix.

Our approach to obtaining theo retical bounds for multiway spec-tral clustering is a relatively simple one that is based on recursive bipartitioning. A lthough it may be possible t o obtain a direct per-turbation bound of the form of Theorem 1 for the multiway case, the problem is challenging, and in our current work we have opted for a simple approach.

T HEOREM 3. Assume that: 1) the assumptions of Theorem 1 hold throughout the recursive invocation of the Ncut algorithm, 2) the smallest eigengap g 0 along the recursion is bounded away from zero, and 3) the Frobenius norm of the perturbation on Laplacian matrices along the recursion is bounded by c ||  X  L  X  L || 2 constant c  X  1 . Then the mis-clustering rate for an m -way spectral clustering solution can be bounded by (ignoring the higher order term on the right hand side): This theorem provides an upper bound on  X  via the perturbation of the Laplacian matrix. The proof is based on repeatedly applying Theorem 1 and Lemma 2; see [36].
In this section, we develop a bound for the Frobenius norm of the perturbation on the Laplacian matrix. Let  X  A = A + X  and  X  D = D + X  . Based on the Taylor expansion and using standard properties of matrix Frobenius norm [33], we have the following approximation for  X  L  X  L F (see [18]): ||  X 
L  X  L || F  X || D  X  1 2  X  D  X  1 2 || F +(1+ o (1)) ||  X  D  X  As reported in detail in [36], we can work out the perturbation series expansions, so that we can bound ||  X  L  X  L || F . We summarize the results in the remainder of this section.

At this stage of our argument we need to introduce a statistical model for the original data. Specifically, we need to introduce a model for data that fall into two clusters. To obtain a tractable anal-ysis, we have modeled distribution G as a two-component mixture model: where  X   X  X  0 , 1 } with P (  X  =1)=  X  . The effect of data perturba-tion is to transform this model into a new mixture model specified by  X 
G =(1  X   X  )  X   X  G 1 +  X   X   X  G 2 ,where  X  G 1 and  X  G 2 through Eq. (4).

The perturbation to the affinity between x i and x j is given by  X  We can simplify  X  ij by a Taylor expansion of function f ( exp we are then able to prove the following theorem for the perturbation bound on the Laplacian matrix (see [36] for details):
T HEOREM 4. Assume that: 1) x 1 , ..., x n  X  R d are generated i.i.d. from (7) such that inf 1  X  i  X  n d i /n &gt; c 0 holds in probab ility for some constant c 0 &gt; 0 , 2) the distribution of components in is symmetric about 0 with bounded support, and 3) ||  X  D  X  1 || o (1) ,then for some universal constants c 1 and c 2 as k  X  X  X  ,where  X   X  (4) denote the second and fourth moments of , respectively, and  X   X  p  X  indicates that inequa lity holds in probability. The result of Theorem 4 holds when there are more than two clus-ters. By combining Theorems 3 and 4, we have obtained a pertur-bation bound for the mis-clusteri ng rate under suitable conditions. Remark. i) The fourth moment is often negligible compared to the second moment. In such cases the main source of perturbation in the matrix norm comes from the second moment of . ii) The assumption that d i /n  X  X  are bounded away from 0 is a technical assumption that substantially simplifies our proof. We believe that the theorem holds more generally.
In this section we show how the analysis described in the previ-ous section can be applied to KASP and RASP. In this analysis the noise component models the difference between the original data and their corresponding representative points. With either k -means or RP tree preprocessing, the variance of perturbation on original data can be made small according to Theorem 7 and Theorem 9, which satisfies the requirement in the model.

In the rest of this section, we first present a set of embedding lemmas , which establish the connection between the cluster mem-bership of the representative points and those of the original data. We then present a performance analysis for KASP and RASP.
Let the set of representative data points (with repetitions) that correspond to each original data point be denoted by with repetition counts (i.e., the num ber of points sharing the same representative point) denoted by r 1 ,r 2 ,...,r k such that n .Let S 1 = { y 1 , y 2 ,..., y k } denote the set of unique representa-tive points. We show that the second eigenvector of the Laplacian matrix corresponding to data set S can be computed from that cor-responding to S 1 . Since the Laplacian matrix of set S 1 much smaller than that of set S , a significant reduction in compu-tational cost can be achieved.

L EMMA 5. Let v 2 denote the second eigenvector of the Lapla-cian matrix corresponding to data set S .Then v 2 can be written in the following form: where the number of repetitions of each v i is exactly r i L EMMA 6. Let the k  X  k matrix B have the form where matrix [ a 1 , a 2 ,..., a k ] is the affinity matrix computed from S , the set of unique representative points. Let  X  B [ u Laplacian matrix of B , respectively. Then the following equality holds (up to scaling): v 1 = u 1 ,v 2 = u 2 ,...,v k = u k v are the components of v 2 in Eq. (9) .
 The proofs of Lemma 5 and Lemma 6 are provided in [36]. Remark. Empirically we find that r 1 ,...,r k are not very different in data sets preprocessed by KASP and RASP, so in practice we do not perform the scaling; i.e., we set all r 1 ,...,r k equal to 1 .

Based on Lemma 5 and Lemma 6, the second eigenvector of the n  X  n Laplacian matrix L A corresponding to the large data set S =[ y 1 ,,..., y 1 , y 2 ,..., y 2 ,..., y k ,..., y computed from a reduced k  X  k Laplacian matrix L B , after proper scaling. In the case that k n (which usually occurs in practice), a substantial computation speedup can be achieved (as demonstrated in Section 5). The remaining issue is how to approximate the orig-inal data set { x 1 , x 2 ,..., x n } (with small distortion) using the re-duced representative data set { y 1 , y 2 ,..., y k } . With this achieved, Theorem 4 then ensures that the resulting mis-clustering rate will be small, and will tend to zero in probability. In the remainder of this section, we show that the distortion can be made small if the representative points are computed by k -means or by the RP tree quantization method.
Existing work from vector quantization [38, 13] allows us to characterize precisely the amount of distortion when the represen-tative points are computed by k -means clustering if the probability distribution of the original data is given.

Let a quantizer Q be defined as Q : R d  X  X  y 1 ,..., y k y i  X  R d .For x generated from a random source in R d ,letthe distortion of Q be defined as: D ( Q )= E ( x  X  Q ( x )) s ,whichis the mean square error for s =2 .Let R ( Q )=log 2 k denote the rate of the quantizer. Define the distortion-rate function  X  ( R ) as Then  X  ( R ) can be characterized in terms of the source density of G and constants d, s by the following theorem.

T HEOREM 7 ([38]). Let f be the density function for G (de-fined in Eq. (7) )in R d . Then, for large rates R , the distortion-rate function of fixed-rate quantization has the following form: where  X  = means the ratio of the t wo quantities t ends to 1, b constant depending on s and d , and Thus, by Theorems 3 and 4, we arrive at the following characteri-zation for the mis-clustering rate of KASP.

T HEOREM 8. Let the data be generated from a distribution with density f . Let assumptions for Theorem 3 and Theorem 4 hold. Then the mis-clustering rate  X  can be computed as: where c is a constant determined by the number of clusters, the variance of the original data, the bandwidth of the Gaussian kernel and the eigengap of Laplacian matrix (or minimal eigengap of the Laplacian of all affinity matrices used in Ncut).
We now briefly discuss the case of RASP, where the distortion-minimizing transformation is given by an RP tree instead of by k -means. By combining our perturbation analysis for spectral clus-tering with quantization results from [8], we obtain an analysis for RASP. Define the average diameter of input data X = { x 1 as [8] It is clear that if u ( X ) is the center of mass for the data set X , said to have local covariance dimension ( d ,,r ) if its restriction to any ball of radius r has a covariance matrix whose largest d eigenvalues satisfy The quantization error of the RP tree is characterized in terms of the local covariance dimension as follows.
 T HEOREM 9 ([8]). Suppose an RP tree is built using data set X  X  R d , then there exist constants 0 &lt;c 1 ,c 2 &lt; 1 with the fol-lowing property. Consider any cell C of radius r such that X has local covariance dimension ( d ,,r ) with &lt;c 1 . Pick a point x  X  X  X  C at random, and let C be the cell that contains x next level down. Then where the expectation is taken over the randomiza tion in splitting C and the choice of x  X  X  X  C .
 Theorem 9 shows that the vector quantization error of RP tree be-haves as e  X  O ( h/d ) with h the depth of the tree and d the intrinsic dimension of the data. Thus the quantization error can be made small as the tree depth grows, and a result similar to Theorem 8 holds for RASP.
We have proposed a general framework and presented two fast algorithms for approximate spectral clustering. Our algorithms lever-age k -means and RP tree methods to pre-group neighboring points and produce a set of reduced representative points for spectral clus-tering. These algorithms significantly reduce the expense of the matrix computation in spectral clustering, while retaining good con-trol on the clustering accuracy. Evaluation on a set of real data sets shows that a significant speedup for spectral clustering can be achieved with little degradation in clustering accuracy. Remark-ably, our approximate algorithms enable a single machine to per-form spectral clustering for a large dataset X  X he Poker Hand dataset X  which consists of one million instances.

We also presented a theoretical analysis of our approximate spec-tral clustering algorithms using statistical perturbation theory. Our perturbation bound reveals that the mis-clustering rate is closely related to the amount of data perturbation X  X ne can make the mis-clustering rate small by reducing the amount of perturbation. We show that the mis-clustering rate converges to zero as the number of representative points grows. These results provide a theoretical foundation for our algorithms and also have potentially wider appli-cability. In particular, a natural direction to pursue in future work is the use of other local data reduction methods (e.g., data squashing and condensation methods) for preprocessing; we believe that our bounds can be extended to these methods. We also plan to explore other methods for assigning clustering membership to the original data according to the membership of the representative data based on local optimization and edge-swapping methods. [1] D. Arthur and S. Vassilvitskii. k -means++: The advantages [2] S. Arya, D. Mount, N. Netanyahu, R. Silverman, and A. Wu. [3] A. Asuncion and D. Newman. UCI Machine Learning [4] F. R. Bach and M. I. Jordan. Learning spectral clustering, [5] M. B  X  adoiu, S. Har-Peled, and P. Indyk. Approximate [6] P. S. Bradley and U. M. Fayyad. Refining initial points for [7] L. Breiman. Random forests. Machine Learning , 45(1):5 X 32, [8] S. Dasgupta and Y. Freund. Random projection trees and low [9] I. Dhillon, Y. Guan, and B. Kulis. Weighted graph cuts [10] P. Drineas and M. W. Mahoney. On the Nystr  X  om method for [11] S. Fine and K. Scheinberg. Efficient SVM training using [12] C. Fowlkes, S. Belongie, F. Chung, and J. Malik. Spectral [13] R. M. Gray and D. L. Neuhoff. Quantization. IEEE [14] S. Gunter, N. N. Schraudolph, and A. V. N. Vishwanathan. [15] J. A. Hartigan. Clustering Algorithms . Wiley, New York, [16] J. A. Hartigan and M. A. Wong. A k-means clustering [17] B. Hendrickson and R. Leland. A multilevel algorithm for [18] L. Huang, D. Yan, M. I. Jordan, and N. Taft. Spectral [19] A. Jain, M. Murty, and P. Flynn. Data clustering: a review. [20] R. Kannan, S. Vempala, and A. Vetta. On clusterings: Good, [21] T. Kanungo, D. M. Mount, N. S. Netanyahu, C. D. Piatko, [22] G. Karypis and V. Kumar. A fast and high quality multilevel [23] A. Kumar, Y. Sabbarwal, and S. Sen. A simple linear time [24] J. F. Lu, J. B. Tang, Z. M. Tang, and J. Y. Yang. Hierarchical [25] D. Madigan, I. Raghavan, W. Dumouchel, M. Nason, [26] M. Meila and J. Shi. Learning segmentation with random [27] P. Mitra, C. A. Murthy, and S. K. Pal. Density-based [28] A. Y. Ng, M. Jordan, and Y. Weiss. On spectral clustering: [29] J. M. Pena, J. A. Lozano, and P. Larranaga. An empirical [30] S. J. Redmond and C. Heneghen. A method for initialising [31] J. Shi and J. Malik. Normalized cuts and image [32] A. Smola and B. Scholkopf. Sparse greedy matrix [33] G. Stewart. Introduction to Matrix Computation . Academic [34] U. von Luxburg, M. Belkin, and O. Bousquet. Consistency of [35] C. Williams and M. Seeger. Using the Nystr  X  om method to [36] D. Yan, L. Huang, and M. I. Jordan. Fast approximate [37] S. Yu and J. B. Shi. Multiclass spectral clustering. In [38] P. L. Zador. Asymptotic quantization error of continuous
