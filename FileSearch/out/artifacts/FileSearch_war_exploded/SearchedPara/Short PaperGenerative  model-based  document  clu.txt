
Document clustering has become a fundamental operation in unsupervised document
Until the mid-1990s, hierarchical agglomerative clustering using a suitable similarity measure such as cosine, Di ce, or Jaccard, formed the dominant paradigm for clus-tering documents. The increasing interest in processing larger collections of docu-ments has led to a new emphasis on desi gning more efficient and effective tech-niques, leading to an explosion of diverse approaches to the document clustering problem, including the (multilevel) self-organising map (Kohonen et al. 2000), spher-ical k -means (Dhillon and Modha 2001), bisecting k -means (Steinbach et al. 2000), mixture of multinomials (Vaithyanathan and Dom 2000; Meila and Heckerman 2001), multilevel graph partitioning (Karypis 2002) , mixture of vMFs (Banerjee et al. 2003), information bottleneck (IB ) clustering (Slonim and Tishby 2000), and coclustering using bipartite spectral graph partitioning (Dhillon 2001). This richness of approaches prompts a need for detailed comparative studies to establish the relative strengths or weaknesses of these methods.
 cently introduced a unified framework for such approaches (Zhong and Ghosh 2003), which allows one to understand and compare a vast range of model-based parti-tional clustering methods using a common viewpoint that centers around two steps X  a model reestimation step and a data reassignm ent step. This two-step view enables one to easily combine different models with different assignment strategies. We now apply this unified framework to design a set of comparative experiments, involving three probabilistic models suitable for clus tering documents: multivariate Bernoulli, multinomial and von Mises-Fisher (vMF), in conjunction with four types of data assignments, thus leading to a total of 12 algorithms. Note that all the three models directly handle high-dime nsional vectors without dime nsionality reduction and have been recommended for dealing with the peculiar characteristics of document cluster-ing. In contrast, Gaussian -based algorithms, such as k -means, perform very poorly for such datasets (Strehl et al. 2000). All 12 instantiated algorithms are compared on a number of document datasets derived from the Text REtrieval Conference (TREC) collections and internet newsgroups, bot h with and without feature selection. Our tering and identify which model works better in what situations. We also compare all the model-based algorithms with two state-of-the-art graph-based approaches, the vcluster algorithm in the CLUTO toolkit (Kar ypis 2002) and a bipartite spectral co-clustering method (Dhillon 2001). A comp arison to recent Kullback-Leibler (KL) clustering and multinomial model-based clustering (Banerjee et al. 2004). multinomial models for text classification but not for clustering. Comparisons of dif-ferent document clustering methods have been done by Steinbach et al. (2000) and by Zhao and Karypis (2004). They both focu ssed on comparing partitional with hier-archical approaches either for one model or f or similarity-based clustering algorithms (in the CLUTO toolkit). Meila and Heckerman (2001) compared hard vs. soft as-signment strategies for text clustering using multinomial models. This paper provides a substantially expanded empirical study in terms of both model coverage and dataset variety, yielding additional insights into the problem of large-scale text clustering. at the core of four related clustering algorithms X  X odel-based k -means ( mk-means ),
EM clustering, stochastic mk -means and deterministic annealing, respectively. We also summarise the three base generative models used to represent single clusters.
Ghosh (2003). The traditional vector space representation is used for text documents, the document. The word here is used in a broad sense because it may represent individual words, stemmed words, tokenised words or short phrases.

Data assignment: Let  X  ={  X  1 , ...,  X  K } be the set of K models, one per cluster (typically they come from the same family, e.g. multinomials, and just differ in the parameter values). The mk-means algorithm assigns each data object x to only one cluster X  X he cluster y that gives the maximum likelihood, P
P ( y | mk -means. It stochastically assigns each data object entirely to one cluster (and not fractionally, as in soft clustering), with the probability of object x going to cluster y set to be the posterior probability P ( y | x , X ) . The stochastic mk -means can be viewed as a sampled version of EM clustering, wh ere one uses a sampled E-step based on the posterior probability.

Model-based deterministic annealing (Zhong and Ghosh 2003) extends EM clustering by parameterising the E-step with a temperature parameter T ,thatis,
P ( y | ing process. Model-based k -means and EM clustering can be viewed as two special stages of a model-based deterministic annealing process, with T respectively (Zhong and Ghosh 2003).

Let y ( x ) = arg max y P ( x |  X  y ) . For text data, the condition P  X  y = y ( x ) is often encountered for the models discussed next, which means that
P ( y | x , X ) will be dominated by the likelihood values and be very close to 1 for y y ( x ), and 0 otherwise, independent of most choices of T  X  X  and P that the difference between hard and soft versions is small, i.e. their clustering results in this paper. The complexities of the above model-based clustering algorithms are linear in K , number of clusters, N , number of data objects, and M , number of iterations.

Base models: The multivariate Bernoulli and multinomial models (with na X ve Bayes assumption) have been commonly used when the document-term matrix has binary-and integer-valued entries, respectively (McCallum and Nigam 1998; Zhong and
Ghosh 2004). So we just describe the third model, which is based on the vMF distri-data (Mardia 1975), and is given by where x is a normalised (unit-length in L 2 norm) document vector and the Bessel function Z ( X  y ) is a normalisation term. The parameter bution. Hard assignment on a vMF mixture with the same k -means (Dhillon and Modha 2001; Banerjee and Ghosh 2004), which has performed well for several text collections. For vMF-based algorithms, we use TF-IDF (Term
Frequency-Inverse Document Frequency)-weighted document vectors that are nor-malised to unit length.

In Banerjee et al. (2003), the EM-based maximum likelihood solution has been derived, including updates for  X  . While it provides markedly better results than those obtained with a fixed  X  , it is computationally much more expensive even if an ap-proximation for estimating  X   X  X  is used. In this paper, for convenience, we use a sim-pler soft assignment scheme that is simila r to deterministic annealing. We use a that is constant across all models at each iteration, start with a low value of gradually increase the  X  (i.e. make the distributions more peaked) in unison with each iteration. Note that  X  has the effect of an inverse temperature parameter. can be found in McCallum and Nigam (1998), Zhong and Ghosh (2003) and Baner-jee and Ghosh (2004).
For document clustering, external meas ures are commonly used because typically the benchmark documents X  category labels are actually known (but not used in the clustering process). Examples of external measures include the confusion matrix, formation (Ghosh 2003). Based on the arguments in Strehl et al. (2000) and in Strehl and Ghosh (2002), in our experiments, we use normalised mutual information (NMI) between cluster and class random variable s as the evaluation criterion. Because the three probabilistic models use slightly different representations of documents, we cannot directly compare their objective functions (data likelihoods) under different probabilistic models.
We used the 20-newsgroups data 1 and a number of datasets from the CLUTO toolkit (Karypis 2002). These datasets provide a good representation of different characteris-tics: number of documents ranges from 204 to 19,949, number of words from 5,832 to 43,586, and number of classes from 3 to 20. A summary of all the datasets used in this paper is shown in Table 1.
 ent usenet newsgroups, 1,000 messages from each. We preprocessed the raw dataset using the Bow toolkit (McCallum 1996), including chopping off headers and re-moving stop words as well as words that occur in less than three documents. The
NG17-19 dataset is a subset of NG20, containing  X  1 pected to be difficult to separate. All the datasets associated with the CLUTO toolkit have already been preprocessed (Zhao and Karypis 2004) and we further removed those words that appear in two or fewer documents.
The four algorithms based on the Bernoulli model are k -Bernoullis, stochastic k -Bernoullis, mixture-of-Bernoullis and Bernou lli-based deterministic annealing (DA), abbreviated as kberns , skberns , mixberns and daberns , respectively. Similarly, the abbreviated names are kmnls , skmnls , mixmnls and damnls for multinomial-based al-gorithms and are kvmfs , skvmfs , softvmfs and davmfs for vMF-based algorithms. We use softvmfs instead of mixvmfs for the soft vMF-based algorithm for the following reason: As mentioned in Sect. 2, the estimation of parameter difficult but is needed for the mixture-of-vMFs algorithm. As a simple heuristic, we use  X  ( m ) = 20 m ,where m is the iteration number. So  X  is a constant for all clusters at each iteration and gradually increasing over iterations.

For the davmfs algorithm, the temperature parameter T can be assimilated into the inverse temperature. We set  X  to follow an exponential schedule starting from 1 and up to 500. We call this algorithm davmfs .Forthe daberns and damnls algorithms, an inverse te mperature parameter  X  = terise the E-step in the mixberns and mixmnls algorithms. The annealing schedule for daberns is set to  X  ( m + 1 ) = 1 . 2  X  ( m ) ,and  X  damnls ,itissetto  X  ( m + 1 ) = 1 . 3  X  ( m ) ,and  X  grows from 0.5 up to 200.
For all the model-based algorithms (except for the DA algorithms), we use a max-imum number of iterations of 20 (to make a fair comparison). Our results show that most runs converge within 20 iterations if a relative convergence criterion of 0.001 is used. Each experiment is run 10 times, each time starting from a different random initialisation. The averages and standa rd deviations of the NMI and running time results are reported.

After surveying a range of spectral or graph-based partitioning techniques, we picked two state-of-the-art graph-based clustering algorithms as leading representa-tives of this class of similarity-based approaches in our experiments. The first one is
CLUTO (Karypis 2002), a cluste ring toolkit based on the M etis graph partitioning al-gorithm. We use vcluster in the toolkit with the default setting, which is a bisecting graph partitioning-based algorithm. The other one is the bipartite spectral coclus-tering algorithm (Dhillon 2001), modified according to Ng et al. (2002) 10 times, each run using a different order of documents.
Table 2 shows the NMI results on the NG20 , NG17-19 , classic , ohscal and hitech datasets. All numbers in the table are shown in the format average space, we show the NMI re sults for one specific K only for each dataset (results for other datasets are shown in Tables 3 and 4).
 following seven hypotheses: bb &gt; wb  X  X he best of kberns, skberns and mixberns is better than the worst of them (in terms of NMI performance); bm best of kmnls, skmnls and mixmnls is better than the worst of them; bv best of kvmfs, skvmfs and mixvmfs is better than the worst of them; dam damnls is better than the best of kmnls, skmnls ,and mixmnls ; dav is better than the best of kvmfs, skvmfs and mixvmfs ; dav than damnls ; dav &gt; cluto  X  davmfs is better than CLUTO. The p -values shown in the table range from 0 to 1. A value of 0.05 or lower indicates significant evidence highlighted in boldface in the table.

Of the three types of models, vMF leads to the best performance and multivari-ate Bernoulli the worst. The Bernoulli-ba sed algorithms significantly underperform only whether or not a word occurs in a document but not the number of occur-rences, is a limited representation. The vMF-based algorithms perform better than the multinomial-based ones, especially for most of the smaller datasets, i.e. NG17-19 , outperforms damnls on 12 out of 15 datasets while it significantly underperforms on only one dataset ( classic ).
 means, produce very comparable clustering results across all datasets. The soft EM assignment is only slightly better than the other two. The t -test results also show that, for most datasets, there is n o significant difference in NMI performance between soft and hard assignment strategies. For the vMF models, however, one should note that the exact EM clustering can achieve signi ficant improvement over hard assignment due to an annealing effect observed in Banerjee et al. (2003).
 the performance of corresponding soft clustering algorithms, sometimes significantly.
For example, the t -test results show that damnls significantly outperforms the best of kmnls, skmnls and mixmnls on 12 out of 15 datasets; davmfs does so on 7 out of 15 datasets. A trend seen is that the DA clustering algorithms gain more on medium to small ( n d  X  3,000) datasets.
 berns , however, as shown by the NMI results. By further looking into the log-likelihood objective values and actual resu lting clusters, we observed that determin-istic annealing improves the objective value but puts most documents in one cluster, indicating that maximising data likeli hood with Bernoulli models does not align with generating well-separated clusters.
 the vMF-based methods and sometimes gives very poor results (with most docu-ments grouped into one cluster and NMI values close to 0). The other graph-based algorithm, CLUTO (actually the vcluster algorithm with default setting), performs much better and is, overall, one of the best among all the algorithms we have com-pared. The t -test results show that CLUTO significantly outperforms davmfs on 7 out of the 15 datasets but also significantly underperforms on five of them. experiments. All the numbers are recorded on a 2.4 GHz PC running Windows 2000 with 768 MB memory and reflect only the clustering time, not including the data I/O cost. Note that CLUTO is written in C, whereas all the other algorithms are in
Matlab. Clearly, algorithms using soft assignment take longer time than those using hard assignments. Overall, the kvmfs algorithm is the fastest one.
In text-information-retrieval applications, feature-selection techniques are often used to select a subset of words, to achieve more compact representation of text documents and reduced computational complexity for man ipulating text data. Feature selection is not the focus of this paper; rather, we intend to see how dimensionality reduction for text documents will affect the model-based clustering results. Therefore, we employ two simple feature selection methods X  X ord-frequency-based selection and word-variance-based selection (Salton and McGill 1983).
 than 0 . 1% and less than 15% of all documents. For the second selection method, we sort all the words based on their variances and keep only the N words with the highest variances. That is, we reduce the number of dimensions to be the same as the number of documents. The variance of the l th word is defined as
The clustering results as well as paired t -test results with feature-selected text datasets are omitted due to space limit, but can be found in Zhong and Ghosh (2004).
The main notable changes in clustering res ults on feature-selected datasets are 1. In terms of NMI, the multinomial model-based methods generally produce better results for feature-selected datasets as compared with the basic vMF-based ap-proaches. For example, the average NMI values of damnls algorithm significantly improves on 12 out of the 15 frequency-selected datasets and on 14 out of the 15 variance-selected datasets. An explanation of this finding is obtained by reconsid-ering the objective functions for kmnls and kvmfs . Note that the former maximises word index, and P y ( x ) is the word distribution for cluster y  X  pirically estimated based on the training data. Note that kmnls involves a log function, which magnifies the magnitude of P y ( x ) small. That is, when the dimensionality of document vectors is high, the discrete word distribution will be diluted, most P y ( x ) ( l will be large negative numbers that may dominate the objective function. If this is the case, the cluster assignment of x based on l accurate. But if dimensionality decreases (e .g. after feature selection), the dis-criminative power of x will likely increase in the objective function (relative to log P y ( x ) ), thus improve the partitioning of documents into clusters. Though fea-ture selection may remove words that contain useful discriminating information, our results suggest that the benefits fro m dimensionality reduction outweigh the possible information loss from reduced features for the multinomial model. On the other hand, there is no corresponding benefit for the vMF model and thus 2. The relative performance between damnls and davmfs changes to the opposite X  3. After feature selection, CLUTO seems t o deliver lower NMI performance whereas
The comparative study of generative models for document clustering provided sev-eral insights and some surprises. We also noted that, while the model-based algo-rithms (except DA) have a computational advantage over graph-partitioning based approaches, they need better initialisation strategies to generate more stable cluster-ing results. Updating multiple solutions simultaneously based on different initiali-sations, using online updates of the means or performing local search around the found solutions are some viable approaches for further improving performance and robustness of model-based text clustering approaches.

