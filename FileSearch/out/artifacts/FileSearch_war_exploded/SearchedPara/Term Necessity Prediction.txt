 The probability that a term appears in relevant documents ( P  X   X  |  X  X  ) is a fundamental quantity in several probabilistic re-trieval models, however it is diffi cult to estimate without relev-ance judgments or a relevance model. We call this value term necessity because it measures the percentage of relevant docu-ments retrieved by the term  X  ho w necessary a term X  X  occurrence is to document relevance. Prior research typically either set this probability to a constant, or est imated it based on the term's in-verse document frequency, neither of which was very effective. 
This paper identifies several factors that affect term necessity, for example, a term X  X  topic centrality, synonymy and abstractness. It develops term-and query-dependent features for each factor that enable supervised learning of a predictive model of term ne-cessity from training data. Experi ments with two popular retrieval models and 6 standard datasets demonstrate that using predicted term necessity estimates as user te rm weights of the original query terms leads to significant improvements in retrieval accuracy. H.3.3 [ Information Search and Retrieval ]: Retrieval Models Theory, Experimentation, Measurement Term weighting, Necessity, Mism atch, Ad-hoc retrieval models Term necessity is defined as the probability of a term  X  occurring the set of relevant documents for the query. It is the proportion of relevant documents that match  X  , thus, equivalently, it measures term recall , or 1  X  term mismatch rate . 
Term necessity is not a new idea. It plays an important role in the theory of probabilistic information retrieval, similar to inverse document frequency (idf) [1]. Prior research did not make much progress in predicting necessity, for example setting it to a con-stant [7], or used simple methods to predict it [2, 14]. And these efforts were not especially successf ul in understanding necessity. For example, idf  X  a query independe nt statistic  X  was used as the only feature to predict necessity, which is a query dependent probability. 
It has been known since the 1970s that more accurate necessity probabilities offer the possibility of significant improvements in ad-hoc retrieval [1]. 
This paper revisits the problem of predicting term necessity. It identifies several factors that might cause a term to have low ne-cessity (or equivalently, to mismatch relevant documents). Such factors include a query term not being central to the information need, a query term having synony ms, and a query term being too abstract. This work further develops a small set of easily-computed term-dependent and query-dependent features related to each factor, enabling each query term to be represented as a fea-ture vector. Training data from past queries enables a predictive model to be learned, which can predict the necessity of new terms in previously unseen queries. Th is approach to predicting term necessity produces significant impr ovements in ad-hoc retrieval accuracy. It is also easy to extend as new hypotheses are devel-oped about factors that contribute to term necessity. 
Term necessity prediction has b een a difficult problem for more than 30 years. Our contribution to understanding this problem is a reconsideration of the factors that affect necessity, the expression of these factors as easily-computed features, and the framing the problem as one of learning a predictive model from data. 
The following section provides background for understanding term necessity and how it is in stantiated in two well-known prob-abilistic retrieval models. Section 3 considers term necessity to be a prediction problem. It discusses the factors that might affect term necessity, and how they might be expressed as features. Section 4 presents experiments that investigate the effectiveness of this approach to estimating P  X   X  |  X  X  in well-known retrieval models. Section 5 discusses rela ted work that is not covered elsewhere. Section 6 concludes. The Probability Ranking Principle states that the best retrieval effectiveness is achieved when documents are ranked in decreas-ing probability of relevance to the user that submitted the request, where probabilities are estimated using all available evidence [1]. The Probability Ranking Principle is usually instantiated as rank-ing by the Odds ratio,  X P X   X   X  X  | where  X  is the set of relevant documents,  X   X  is the set of non-relevant documents,  X  is a document to be ranked,  X  is the query, and  X   X  is a query term in  X  . This is known as the Binary Indepen-dence Model (BIM) [1]. And broken down to each query term, the BIM yields the well known RSJ term weight for that term. Two probabilities determine the final term relevance score: ument frequency of the term divided by the collection size, as demonstrated in the predictive version of BIM [1] and also con-firmed by the data analysis by Greiff [2]; it leads to the well-known inverse document frequency (idf) metric. P  X   X  |  X  X  is  X   X  X  necessity to relevance: the prob ability that a relevant document contains  X  . Historically P  X   X  |  X  X  has been more difficult to predict. The BIM itself is not particularly effective, but the later Okapi BM25 [3] uses this term weight as part of the retrieval model. Thus, if there is a way to predict term necessity, the predictions can be directly plugged into BIM, and in turn BM25, without any modification to the retrieval models. The original generative language model [4] directly scores a doc-ument by how likely the document model generates the query, and does not model relevance. The relevance model [17] provides a way to model relevance in the la nguage model framework. Given a search topic, the relevance model is an ideal term distribution P  X   X  |  X  X  for all  X  , estimated from true relevant documents if complete relevance judgements are given. When there are no relevance judgements for the curre nt topic, the relevance model can be estimated in an unsuperv ised manner from top documents of an initial retrieval, thus, instantiated as a query expansion and pseudo-relevance feedback method. The final retrieval status value of a result document is gi ven by the KL-divergence between the relevance model and the docu ment model, or equivalently, weighting each term by its relevance model probability: 
RSV  X  D, X   X   X   X  P  X   X   X  |  X   X   X  X ogP  X   X  | D  X  V X  X  , where P the relevance model, a multinomial distribution over all terms in the vocabulary, i.e.  X  P  X   X   X  |  X   X  V X  X   X 1 . 
In theory, term necessity is a Bernoulli distribution of whether a term appears or does not appear in a relevant document, i.e. P  X   X  |  X  X  X P X   X   X   X  |  X  X  X 1 X  . Nevertheless, it can be normalized into a multinomial term distribution and fitted into the relevance model: sider the necessity of the query terms, but in theory there is no such restriction.) 
It is possible to fit the necessity probabilities into the relevance model because conceptually the relevance model provides more of a representation that allows one to plug in relevance based term weights into the language model re trieval framework, rather than a particular way of es timating these weights. 
In fact, the necessity probabil ity based term weights can be shown as a particular way of es timating the relevance model for a given query. Suppose term occurrences are binary, (tf is either 0 or 1), suppose each term occurrence in the relevant documents is an independent observation from the relevance model, and let L denote the total length of all the | X | relevant documents for the query (sum of numbers of unique terms), then, the relevance mod-el probability P  X   X   X  |  X  X  is only a constant factor off its necessity: 
P  X   X   X  |  X  Count  X  X   X   X  |  X   X   X   X  Count X  X  X  X  X | X   X   X  | | X | X  X  X  X  
Despite the connection, this work focuses on the supervised prediction of the necessity probabilities, which provides a novel and effective way of estimating th e relevance model, and can be used to improve the traditional unsupervised estimation. And at the same time, term necessity, being independent of the average length of the relevant documents, is a more direct and easier ob-jective for prediction than the near 0 multinomial term probabili-ties of the relevance model. 
Empirically, the Bernoulli necessity estimation based on binary term occurrences also leads to better retrieval performance, as shown in Section 4.5.1. This treatment of using a multinomial language model for query generation, but using instead a Bernoul-li model to estimate term necessity creates an inconsistency in the overall model, and de mands a theoretically cleaner solution. Necessity values for 5 example terms from 5 TREC queries are shown below, idf values  X  df X / X log X   X  are listed for reference: Table 1 Necessity values of 5 example terms on sample queries 
Table 1 shows that necessity is not uniform for all query terms, nor is it constant for a specific term. For example,  X  term  X  has two different necessity values in the two different queries. There is also no simple correlation between necessity and idf. No prior work named this probability. The RSJ term weight [1]  X  the product term in Equation (1)  X  has been called  X  X erm relevance X  by Salton and  X  X elevance weight X  by Sp X rck Jones [21], but not the necessity probability itself (which is effectively the only part of the weight about relevance). We argue that necessity is the right name: it shows what this probability is, where it comes from and suggests how it can be used. 
First , generally P  X   X  |  X  X  measures in probability how likely event  X  occurring is a necessary condition for  X  [23], (whether event  X  must occur in order for  X  to be true.) P  X   X  |  X  X  X 1 X  means term  X  must appear in the document in order for it to be relevant, while, low P  X   X  |  X  X  means  X  is unlikely necessary for relevance. 
Second , it urges one to reason wh y a term becomes low neces-sary. For example, low necessity does not imply that a term is irrelevant to the information need . Synonyms, appearing in place of the query term in relevant documents, cause mismatch and lower necessity. Thinking about f actors that affect necessity helps in designing features to predict it for unseen terms in new queries. 
Third , not only for weighting terms, necessity can also help query reduction and expansion. Intuitively, terms unnecessary for relevance can be safely removed or replaced, while low necessary terms should be expanded, as found in [26]. Prior research had difficulty modeling P  X   X  |  X  X  , so it focused on P  X   X  |  X   X   X  (idf) and probabilistic indexing (tf). Robertson and Sp X rck Jones [1] recognized that term necessity could be accurate-ly estimated from relevance judgments, which is the retrospective case of BIM. For a term  X  , if it appears in  X  number of totally | X | relevant documents, then an unbiased estimate of necessity is, 
Without relevance judgments, th e predictive case of BIM as-sumes necessity to be always 0.5 [1]. Croft and Harper treated necessity as a tuned constant (the Croft/Harper Combination Match [7]). Greiff [2] used a 3-piece linear function of idf to predict the overall term weighting which includes necessity and idf. More recently Metzler [14] predicted necessity as a function of df. But because idf was the only feature used in all of the above work, necessity prediction was inaccurate, and improve-ments were only shown over simplistic baselines. Our goal is to predict the necessity of each term in a query, with-out using relevance judgments of the current query. This section presents a framework for using te rm-and query-dependent statis-tics to predict necessity. It includes a discussion of factors that affect necessity, rela ted features and the prediction model. We cast the necessity prediction pr oblem into a standard regres-sion prediction problem, where each query term together with its true necessity value is treated as a sample, for training or testing. The query term is represented as a set of features so that necessity values of query terms in the training set can be generalized to predict necessity for test terms previously unseen, and the objec-tive is to minimize the prediction error of term necessity values. More formally, for a set of topics  X  together with the correspond-ing document collections, a traini ng sample consists of a query judged relevant documents  X   X  (as described in Section 3.2). Each term is represented as a set of features  X   X  X  X  X .. X   X   X   X   X ,  X  depending on the corpus, the term  X   X  and the query  X  . A regression model  X  predicts necessity as a function of the features: 
P
Prediction error  X   X   X   X   X  can be measured in average L1 loss as  X   X   X   X   X   X   X  X  X   X  X   X   X   X   X  |  X   X   X  X  X P  X   X   X  |  X   X   X  X  X   X   X   X  X   X  X  X 
Here, using the necessity probability to measure prediction loss is intuitive, but also arbitrary. One may train over odds probabili-ty (or log odds), and then translate the predicted odds back into probability. Odds maps [0, 1) into [0, +inf), stressing the higher probability region. In our retrie val experiments, training in odds probability is slightly more stab le than probability or log odds. To both train and evaluate necessity predictors, ground truth ne-cessity values need to be es timated from relevance judgments. 
In practice, the unbiased estimate of term necessity, | X |/ X  , can be unstable for topics with a small number of judged relevant documents | X | , thus, some sort of smoothing is usually applied. We use the standard Lapla ce smoothing in the form of  X  X  X  X   X 2 X  X  X | X /1 X  to get more reliable estimates of P  X   X  |  X  X  . 
Pooling is a technique used wide ly in IR evaluation to save judgment efforts, where only top ranked documents from a set of retrieval runs are manually assesse d for relevance. Using pooled judgments might bias the estimati on of term necessity. Our re-sults on multiple TREC datasets s how that sparser judgments still give similar amount of improveme nt, which suggests that necessi-ty can be effectively estim ated on pooled judgments. The main contribution of this work is pointing out some of the factors that might affect term ne cessity and designing features for them. Listed below are the factors that we discovered. sary. In practice, we use the occurrences of a term in top returned documents from an initial retrieval to estimate centrality, assum-ing that terms occurring consistently in top documents are central. For the query  X  Vitamin  X  the cure of or cause for human ailments  X ,  X  ailments  X  appears less consistently in top documents than vita-min , and is in fact much less necessary. 2. Synonyms may replace the original query term in relevant documents, lowering the necessity of the query term. In  X  US educational system  X , if relevant documents say America or USA instead of US , it lowers the necessity of US . For terms with mul-tiple senses, only the synonyms of the queried sense should matter. 3. If a term does not often co-occur with its synonyms, then it is often replaceable by the synonyms, and the term is less necessary. Lu et al [13] used a similar meas ure, synonym novelty, to measure how many new documents a synonym can match. 4. Many abstract but rare terms are unnecessary, because they are at a different abstraction level than the relevant documents. Examples are maulings in  X  dog maulings  X , fundamentalism in  X  Christian fundamentalism  X  and ailments in the vitamin query. Often, hyponyms of abstract terms appear in relevant documents. 
It is important to note that term necessity is different from con-cept necessity.  X  maulings  X  is a necessary concept for the  X  dog maulings  X  query, but the term  X  maulings  X  is not necessary, be-cause of searchonyms such as attack or bite . 
Our research explores a set of features designed to capture fac-tors 1, 2 and 3. There is no direct way to capture abstractness, so we resort to using features that correlate with factor 4. 
In generating the features, we used minimal external informa-tion by restricting ourselves on only the corpus and the queries, avoiding sources such as WordNet or query log data, which have the out of vocabulary problem and may bring in their own biases. 
Features related to topic centrality, synonymy, and replaceabili-ty can be computed based on term similarity in a lower-dimensional space formed by Singular Value Decomposition. 
Synonyms may not co-occur in the same document, but they typically occur in similar cont exts (i.e. a higher order co-occurrence). As [20] pointed out, Singular Value Decomposition (SVD) identifies these higher order co-occurrences. 1 SVD, entries of the term-documen t matrix are tf*idf weighted. 
Here SVD is used only to provide features for predicting term weights, no expansion terms are used in the final queries. 
SVD can be applied globally to a corpus, or locally to top-ranked documents returned for a given query. Local SVD is more efficient and may improve the quality of synonyms. This is be-cause a local SVD improves the quality of the identified syn-onyms by focusing on the sense being queried. For words with multiple senses, mutual disambiguation among query terms causes the top-ranked documents to be about the senses intended by the user, thus the synonyms identified are not about an arbitrary sense of a query term, but the sense bei ng queried. Sch X tze et al [19] used a similar approach, which they called local LSI. 
Denote  X , X S X   X   X  to be the inner product (similarity) of terms  X  and  X   X  in the SVD concept space. Also assume terms  X  descending order of similarity to the query term  X  , i.e.  X , X S X   X , X S X   X   X  X  X  ..., so that, higher in ranking, more likely synonym. 
A topic centrality feature is defined as 
Term  X   X  with maximal similarity to  X  is usually  X  itself, thus this feature indicates how much weight of the term is preserved after SVD. (When  X  does not equal  X   X  , we still use  X , X S X  the feature value.) This residual weight is a measure of how close Other methods of finding higher order co-occurrences should also work, for example topic mode ls like pLSI or Latent Dirich-let Allocation which may also be used to compute term similari-ty in a concept space. the term is to the space spanned by the top documents. For exam-ple, if a term appear frequently in many top documents, then most of its weight will be kept. Tf*i df weighting is used in SVD to prevent stopwords from having the highest centrality. 
A synonymy feature is defined as 
Similarity in the concept space indicates the likelihood that two terms are synonyms, so this feature simply takes an average of the next c highest similarities. We fixed c to be 5 from a pilot study. As evident in Table 2, this feat ure can not only capture synonyms, but also antonyms, hyponyms or even misspellings that  X  X ust be considered equivalent (to the query term) for search purposes X , which is called searchonyms by Richard P.C. Hayden [22]. 
A replaceability feature is defined as where  X  &amp;  X C X   X   X  is the number of documents in the collection matching both  X  and  X   X  , and  X  X   X  is the document frequency of  X  This is a modified version of the synonymy feature and measures how likely the original query term is replaced by its searchonyms in the documents.  X   X  X   X   X  X   X   X , X   X   X   X   X  X /  X   X  X  X  X   X   X  | the likelihood that searchonym  X   X  matches additional documents that  X  does not match. Normalizing by  X  X , X S X  removes the effect of the topic centrality of term  X  . Overall, this feature has a nega-tive correlation (about -0.2) with necessity. 
Table 2 shows top similar terms and their similarities to the query term, for one term in each of the 4 queries. Table 2. Query terms and their top 5 similar terms using SVD It X  X  easy to see from Table 2 that searchonyms extracted with SVD are query dependent, the same term  X  term  X  results in two different sets of synonyms. But, mutual disambiguation in local SVD is not perfect. For  X  term  X  in the  X  term limitation  X  query, the word  X  care  X  which co-occurs with the  X  long term care  X  sense of  X  term  X  shows up as similar (0.0997 similarity) to the query term  X  term  X . The extracted searchonyms and similarities are not perfect. For example, one query term may become another query term X  X  searchonym, ( X  spill  X  is identified as a searchonym for  X  oil  X ), simp-ly because they co-occur. External resources, such as thesauri or Wikipedia, or better searchon ym extraction techniques may be used to improve these features. Nevertheless, our experiments show that the current features based on SVD term similarities can still be used to effectively predict necessity and improve retrieval. 
Overall, the SVD features depend on 2 meta-parameters, 1) the number of top retrieved documents to perform SVD and 2) the number of latent dimensions to k eep. These parameters are tuned on a development set in experiment s using 5-fold cross validation. 
A term rareness feature is simply defined as the inverse docu-ment frequency (idf) of a term. It was often used in prior research e.g. [2, 14] to estimate term weights. Idf is a real-valued term-specific statistic. We use the specific form of  X   X  X  X df X  df X / X log X  , where df is the document frequency of  X  in a corpus of totally  X  documents. Together with the abstractness feature below, we use it to capture fa ctor 4 which affects necessity. 
The modified terms of TREC queries are usually abstract. For example, in the query  X  US educational system  X , system is the head noun being modified by the other tw o terms, and it is abstract. Since the head is an internal node in a dependency parse tree, while the modifiers are leaves, a binary abstractness feature can be defined as whether a term is a leaf node in the dependency tree of the query. We used version 1.6.1 of the Stanford parser (http://nlp.stanford.edu/software/le x-parser.shtml), with the output format "typedDependenc iesCollapsed"  X  so that the output can be conveniently transformed into a dependency tree. Our pilot study shows a 0.2 correlation with necessity. 
Several other features have been tested, but do not perform as well as the combination of the 5 above listed features. 
A first order co-occurrence statistic between two terms was used to measure term similarity. But when combined with the SVD features, this feature does not improve performance. 
Term clarity [10] is a characteristic of the term itself, and initial-ly we thought that it would correlate well with necessity. Intui-low necessity. It does perform reasonably well without the SVD features, but when combined with the SVD features, clarity hurts performance. This is expectable because terms with low clarity would not appear consistently in top-ranked documents, thus topic centrality would be low, making clarity more or less redundant. Regression is typically used to pr edict continuous variables such as a probability, and in our case, the term necessity probability. Features like idf do not have a lin ear relation with necessity, thus a non-linear model being able to fit more complex prediction curves is a better choice. Ho wever, a non-linear model requires more training data, or a smaller num ber of features to avoid data sparsity. In this work, the model trains on over 400 samples and only 5 features, justifying the us e of a non-linear model. More complex or more features may force the model to be linear to achieve high accuracy, such as in Regression Rank [6]. 
A pilot study confirms that for the current set of features, non-linear kernel regression model outperforms linear regression. Kernel regression is an instance based regression method similar to nearest neighbor regression. It weights the necessity probabili-ties of the nearby training samples according to their similarities to the test term to produce a prediction. A training term closer to the test term shares a higher weight. The rationale for using ker-nel regression is that when the features are discriminative enough, we can expect terms close to each other in the feature space to have similar necessity. In our pilo t study we tested kernel regres-sion with linear, polynomial and RBF kernels using SVM-light version 6.02 (http://svmlight.joach ims.org). Results show that RBF kernel regression performs the best. 
Except simple scaling to make each feature roughly range from 0 to 1, the final regression model treats all features the same. A  X  parameter controls the RBF kernel width. A larger  X  gives more weights to nearby training instances.  X  is the third and last meta-parameter in this work. We tune the meta-parameters on devel-opment sets, and report results of 5-fold cross validation. This section examines necessity prediction and its application in retrieval, showing the effectiven ess of the current necessity pre-diction model, its positive impact on retrieval and the potential of using more accurate necessity pr edictions to improve retrieval. We test necessity prediction on 12 standard TREC ad-hoc retriev-al datasets, 6 of which are used as training sets. We looked at only training set topics and releva nt documents for data analyses. Smaller datasets with more comp lete judgments include ad-hoc retrieval tracks of TREC 3 to 8. Larger datasets are Web tracks of TREC 9 and 10, Topic Distillation tasks of TREC 11/2002, 12/2003 and Terabyte tracks of TREC 13/2004 and 14/2005. The larger datasets also have sparser judgments. Stopwords in the standard INQUERY stopword list [16] (except the word  X  X s X  which was used extensively in the queries to mean United States) were removed from the term necessity statistics. Stopwords are easy targets for necessity prediction. They are generally highly necessary give n their frequent occurrence in relevant documents. Furthermore, their low idf and topic centrali-ty (again, due to low idf weight) make them very distinct from the other terms. To make the term necessity statistics more revealing for the content terms, stopwords we re removed. (For retrieval, stopwords were left in, which di d not affect performance much). 
Meta-language terms in the descriptions, e.g. instances , discuss , relevant etc., are generally unnecessary. We used simple rules to remove those phrases that the de scriptions begin with, e.g. "( find | identify | provide ) ( documents | reports | information ) ( on | about )". Removing these improves ba seline ad-hoc retrieval per-formance, but has little effect on necessity weighting based runs. 
Web documents include anchor texts from in-links as part of the content, which improves term matching and increases necessity. Krovetz stemmer was used for pa rsing queries and documents. Without stemming, the un-stemme d query term matches fewer relevant documents, and leads to lower necessity. Smucker et al. [15] recommende d the two tailed randomization test for measuring the statistical significance of a difference be-tween mean performance measures, say MAP. The sign test exhi-bits very different characteristics from the randomization test, and may make false alarms when the randomization test does not, thus they recommended to abandon sign test in comparing MAP. 
We draw different conclusions from [15]. The different charac-teristics of the two tests allow us to view test results from two different perspectives. The ran domization test takes the magni-tudes of the differences into account, but because of that, may favor runs with outliers far from the baseline. The sign test does not take into account the magnitude of the difference, thus is ro-bust to outliers, but at the same time may favor runs which im-prove slightly on many topics, but fail wildly on a small set of the topics. Thus, in order to avoid false alarms and draw safer con-clusions, in this work, both tests were used. Bold faced results mean significant by both tests. Our research is based on the hypothesis that P  X   X  |  X  X  varies by term and query. We begin by examining this assumption. Figures 1 and 2 show that true necessity varies by term for TREC 3 titles and TREC 9 description queries (excluding stopwords). There are a total of 245 terms for TREC 3. Necessity values dis-tribute quite evenly from 0 to 1, averaged at 0.55, and are definite-ly not constant as prior work assu med. This also suggests that it might be worthwhile to lear n and predict term necessity. 
In Figure 2, a very similar distribution is seen on the WT10G corpus for descriptions of TREC 9, totally 314 query terms. Ex-cept, there is a slight increase in the average necessity to 0.59, and there are much fewer terms with extremely low necessity (lower than 0.05). This is likely beca use, given a larger collection, judgment pools are shallower, and judged relevant documents are more biased toward those co ntaining the query terms. This section explores the questio n  X  X ow much does necessity of the same term vary across topics? X  In the 50 TREC 3 title queries, 22 unique terms appear in more than one topic, accumulating 47 individual occurrences whose true necessity values are plotted in Figure 3. The values for a single term are grouped within vertical dashed lines. The variation is less than 0.1 for 2/3 of the term occurrences, when comparing an occurrence to the previous oc-currence of the same term. The ot her 1/3 occurrences have larger variation, typically because of term sense difference ( X  X ear X  the animal in one query has higher necessity than  X  X ear X  the verb in another query), or term context difference ( X  X isorder X  appearing in a medical phrase or proper noun has a much higher necessity than as a general noun). The same 1/3 vs. 2/3 divide is observed on a larger query set consisting of 250 queries from TREC 3 to 7 de-0.2 0.4 0.6 0.8 Figure 1. TREC 3 query term necessity in descending order same term recurring in different queries. The term relate has the largest difference across its two occurrences, with 0.1935 in topic 172  X  X he Effectiv eness of Medical Products and Related Programs Utilized in th e Cessation of Smoking X  scription queries. This suggest s that query-specific necessity prediction is important, while at the same time, for many term occurrences, the computationally more expensive feature based prediction may be avoided, and a historic necessity value of the same term and the same sense (or use) being queried may be suf-ficient to provide an accurate prediction. TREC 3 titles have a similar verbosity level as TREC 9 descrip-tions, with an average of 5-6 terms per query. TREC 4-8 descrip-tions are more verbose, averaging 8 to 9 terms per query. Reflect-ed in necessity values, TREC 4-8 descriptions have an average term necessity of 0.38 to 0.43, while TREC 3 titles and TREC 9 descriptions have an average necessity of 0.54 to 0.59. 
Despite the change of characteristics of TREC queries from year to year, we show below that training data from one TREC dataset can be used to predict necessity for other datasets. This section measures the accuracy of several term necessity pre-diction methods in per term aver age absolute prediction error (L1 loss) as defined in Section 3.1. TREC 3 dataset with relevance judgments is used as training da ta and TREC 4 as test data. 
Table 3 shows the performance of several prediction methods, including a baseline that always predicts 0.55 (the average train-ing set necessity), kernel regression with different sets of features, and predicting from the true necessity of a previous occurrence of the same term. With idf as the only feature, prediction is slightly worse than the baseline, indicating that idf alone as used in [2, 14] does not predict necessity effectiv ely. After adding dependency parse leaf and clarity features, error drops by 13%. The SVD features clearly outperform clarity, where meta-parameters are defaulted to 1000 feedback documents, 100 latent dimensions and  X  equals 1.5. Cross validation finds the meta-parameters to be 180, 150 and 1.5 which further decreases prediction error to half of the baseline. Removing any of the 5 features hurts performance. 
The last row of Table 3 uses the true necessity of a previous oc-currence of a term to predict th e necessity of another occurrence of the term in a different query. Different from previous methods, this was tested on recurring terms of the TREC 3 training set. Given that the necessity distributi on of recurring terms (Figure 3) is quite similar to that of all query terms, the prediction of necessi-ty on just recurring terms should not be much easier than on all terms. And this low prediction error of 0.1341 shows promise in this history based necessity predic tion method. It also indicates that for many occurrences of the same term, because a majority sense or majority use dominates, necessity varies little across those topics, and a term depe ndent prediction method may per-form well enough for most terms. But we should bear in mind that 0.1341 is an optimistic estim ate, being obtained from a single TREC 3 dataset and tested on a small subset of possibly biased terms  X  recurring terms. Since the average L1 losses of the other methods were all evaluated on TREC 4 description terms, the 0.1341 loss is not comparable to the other numbers in the table. To really make a comparable ev aluation would require coverage of all query terms in TREC 4, wh ich requires relevance judgments for so many topics that it becomes unpractical for TREC scale judgments. The requirement of ma ny training topics is a limita-tion of the history based method, on academic scale topic sets. The necessity enhanced retrieval models used here are those de-scribed in Section 2.1 and 2.2. Baseline models include Language Modeling (LM) with Dirichlet smoothing and Okapi BM25. From a pilot study, we fixed the baseline Dirichlet prior  X  at 900 for TREC 3-8, 11-12, and 1500 on other sets. BM25 k1=1.2, b=0.75, k3=7. After adding nece ssity term weighting, less smoothing is needed, because w ithout necessity weighting, a higher level of smoothing is needed to explain/generate the low necessity query terms from releva nt documents. But we did not fine tune the smoothing parameter, and leave the relationship between necessity weighting a nd smoothing as future work. 
Retrieval accuracies are reported in Mean Average Precision (MAP), which is used as th e objective for tuning the meta-parameters on the development se ts. In Top 10 and 20 Precision, 10%-20% improvements were obser ved for the experiments in Table 5, with less significance. All experiments and significance tests were run using the Lemur/I ndri toolkit version 4.10. Small modifications were made to allow for setting term necessity weights into the RSJ weight for the BM25 baseline, and for run-ning local SVD to obtain term similarity values. This subsection evaluates the cas e where relevance judgments are used to estimate term necessity (as per Section 3.2). True necessi-ty values are used to weight query terms, and retrieval is evaluated on the same set of relevance judgments that provided the necessity term weighting. This is only intended to show the potential of using necessity predictions as term weights, similar to the retros-pective case of [1]. To our knowledge, this is the first work to report performance of applying true necessity weights on retrieval models other than BIM. Improve ments over state-of-the-art mod-els underscore the potential of applying necessity prediction. 
Table 4 shows that on different datasets, using true necessity term weights gives a consistent 30%-80% improvement over the baseline model on description queries. Bold faced results are significantly better than the base line by both significance tests. Baselines include language mode ling with Dirichlet smoothing, and Okapi BM25. Title queries (denoted as title in the tables) perform better than descriptions ( desc in tables). Applying true necessity weights on title quer ies give steady but smaller im-provements over title base lines. True necessi ty weighted descrip-tion queries outperform true necessity weighted titles, simply because there are more terms to tune weights on. For the same reason, predicted necessity applied to title queries does not lead to a significant change in retrieval performance, (these results are omitted from Table 5). 
As Section 2.2 mentions, the re levance model if assuming mul-tinomial term occurrences sugges ts to estimate relevance term weights from the relevant documents as a multinomial term distri-bution. The main difference with multiple Bernoulli necessity estimation lies in the fact that multiple term occurrences in the same document will be rewarded. Applying the multinomial es-timates as term weights gives the  X  X ultinomial RM X  row in Table 4. It is less stable and leads to a retrieval performance much worse than the Bernoulli necessity estimates in all test collections. 
Table 3. Term necessity prediction performance, training (TREC4 queries as provided by TREC do not include titles.) Prediction method Avg L1 loss Change Baseline: Average (constant) 0.2936 0% IDF+Leaf+Clarity 0.2539 -13.52% IDF+Leaf+SVD features 1-3 0.1945 -33.75% Tuning meta-parameters 0.1400 -52.32% 
TREC 3 previous occurrences 0.1341 N/A Contrary to this result, a prior finding showed that the document language model is best estimated as a multinomial term distribu-tion instead of multiple Bernoulli [27]. We leave to future work a more unified model for document and relevance model estimation. 
We note that the performance ga in using true necessity term weights is similar to the gain reported on the WT10g (TREC 9, 10) collection by the best sampled term weights [6]. The Oracle case of [6] directly tuned term weig hts using MAP as objective, thus, this close performance means true necessity weights are close to the best sampled term weights of [6] in retrieval effectiveness. This subsection tests whether term necessity can be effectively predicted to improve retrieval. In Table 5, we present results using predicted necessity values (Section 3) as user term weights. Models were trained on TREC t opics from previous year(s), and were tested using 5-fold cross validation on the 50 TREC topics of the next year. Here, the RBF kernel regression model was always trained on the 50 training topics (if training set includes only one TREC dataset). 50 test topics were split into 5 folds, 4 of which were used as development set to tune meta-parameters, 1 fold was used for testing. Howe ver, the learning model does not require that much development data . 2-fold cross validation uses fewer (only 25) development topics , and still yields the same per-formance and optimal parameter values as 5-fold cross validation does. Meta-parameters m  X  the number of latent dimensions to keep after SVD and  X  which controls the kernel width were fixed at 150 and 1.5, because on all datasets, cross validation found the two parameters to be within 130~170, and 1.2~1.6. 
Consistent improvements of 10% to 25% are observed in MAP across all 6 ad-hoc retrieval datasets, statistically significant by both sign and randomization tests. Most runs have strong signi-ficance levels of p &lt; 0.005. This shows that models trained on one dataset can reliably predict term necessity on a similar dataset. It also shows that the prediction model and the features do adapt to collections of different scales, with different levels of judgment depths, and do predict necessity reliably. 
The number of feedback documents  X  n plays an important role in adapting the SVD features to different collections. During cross validation, when tuning n on development set, the optimal n is found to be 180 on smaller TREC 3-8 datasets. For the larger WT10g of TREC 9-10, n is 200, the even larger .GOV TREC 11-12 increases n to 500. And for .GOV2 of TREC 13-14, n is 600. The larger collections probably contain a larger number of helpful or even relevant documents, thus expectedly, th e optimal number of feedback documents increases as collection size does. n might also depend on how many relevant documents there are for the query, and whether the majority sense of a query term is used. 
Comparing training on one collection vs. training on multiple model to optimize MAP. More training topics from dissimilar collections neither im prove retrieval, nor hurt performance much. 
Necessity weighting also yield c onsistent improvements in top precision, as shown in Table 5. The improvements are even sta-tistically significant on multiple collections. At first, it may seem counterintuitive that a better estimate of term recall actually im-proves top precision. However, better estimates of term recall in fact corrects the idf-based retrieval models X  bias to the matching of rare but possibly unnecessary te rms, thus reduces top ranked false positives and boosts up partially matched relevant results . 
Since idf has been used by prior research (as the only feature) to predict necessity, Table 5 also includes a baseline of using idf as the only feature to predict necessity. Results show that even though idf does not predict necessity well (Table 3), using idf predicted necessity does lead to a perceivable gain in retrieval, though not stable across collections. This is encouraging because without computing more complex term features based on an initial retrieval, idf based prediction can still provide some gain. Clarity has been shown to correlate well with query performance, howev-er, using term clarity as the only feature to predict necessity does not lead to a stable retrieval performance. The relevance model theory [17] suggests to use term relevance probabilities as user term weight s, which we follow, as shown in Section 2.2. Relevance model also provides a way to estimate the relevance based term weights from top documents of an initial retrieval. This estimation proce dure, though different in goal, is similar to how we created the SVD features in terms of the sources of information being used. Given the similarities, we also include the relevance model as another baseline. 
The relevance model (RM3) inte rpolates the weighted expan-sion query with the original quer y. The same de velopment sets were used to tune the free para meters (number of feedback docu-ments, feedback terms, and mi xing weight with the original query). Same cross validated results were reported. 
As shown in Table 5, despite a more expressive feedback query (weighted expansion terms + inter polation with original query), trained on the same datasets, the Relevance Model method is unst-able compared to predicted necessity weighting. It is significantly better than the baseline on 3 collections, but insignificant on 3 others. It hurts performance for the low initial retrieval run of TREC 12. This performance is expectable because even though relevance model estimates relevance based term weights, it esti-mates term weights in an unsupervised fashion, while our super-vised framework uses relevance judgements from training topics to guide the prediction. Another difference is that the relevance model tends to use only a few top documents (around 5-20), while our local SVD performs optimally with hundreds of documents. The SVD based features can leverage term appearance informa-tion from more documents, achievi ng a more stable performance. 
The RM reweight-Only method uses the term weights estimated by the relevance model from top-ranked documents to only re-weight the original query terms. It is a variation of the original relevance model approach, and is mo re similar to our approach of using top-ranked documents to com pute user term weights for the original query terms. Results in Table 5 show this baseline to be unstable. On some collections, it is significantly better than the Language Model baseline, but on as many other collections, the improvement is insignificant by bot h tests. This is quite expecta-ble, as relevance models can be seen as directly using topic cen-trality as term weights, thus, it shall not outperform the prediction based on the whole set of features. In some cases, RM reweight-Only outperforms necessity prediction. This shows promise in favoring higher ranked documents according to their relevance scores as the relevance model do es, while our current local SVD treats all top documents the same. 
RM reweight-Only performs just slightly lower than the RM baseline which includes expansion terms. Thus, it is either the case that expansion terms contribute little to retrieval, for descrip-tion queries of about 5-10 words long, or expanding terms as a weighted combination of feedback terms, as in the RM, is not effective, other forms of expansion may do better. 
Overall, these results suggest that for description queries, most of the performance gain from ps eudo relevance feedback (PRF) methods is due to the fact that term weights computed from PRF methods tend to correlate well with true necessity . The reason is that terms that occur consistently in relevant documents also tend to appear consistently in top ranked documents . In fact, a 0.63 correlation is observed on TREC 4 test set, while our best pre-dicted necessity have a higher 0. 80 correlation with necessity, explaining the better performance. Details are listed in Table 6. On title queries, however, because of the more accurate user query, better initial retrieval performance and the introduction of weighted expansion terms, Rele vance Model (Table 5) outper-forms true necessity weighted title queries on 4 out of the 6 col-lections, and in most cases outperforms predicted necessity weighted description queries (bol d faced means significance over title query baseline). True necessity weighted description queries still outperform Relevance Model consistently, suggesting to ap-ply necessity prediction on the Re levance Model expa nsion terms. Since the focus of this work is on reweighting query terms, we leave necessity prediction for expansion terms to the future. 
Relevance model uses unsupervised estimation of term weights, while this work adds supervised learning of necessity into the picture. We can combine the two by using RM term weights as a feature to predict necessity, reported in Table 5 as the RM re-weight-Trained runs. The RM reweight-Trained runs used relev-ance model term weights as the only feature and trained a necessi-ty prediction model to predict term necessity. A simple per query scaling is used to map the near 0 RM weights to [0, 1], so that the maximum query term weight is always 1 for a given query. 
RM reweight-Trained performed consistently better than the base reweight-Only method, except on TREC 12. This consistent improvement empirically differen tiates our framework from the relevance model, and shows that supervised learning of term ne-cessity improves the unsupervised relevance model estimates. 
Table 6. Correlation values between features and true ne-cessity, tested on TREC 4. Here, predicted necessity based on all 5 features yields a high correlation of 0.7989. (RMw uses the term weights estimated by the Relevance Model.) -0.1339 0.1278 0.3719 0.3758 -0.1872 0.6296 
Table 7. Effects of features on TREC 4. Bold face means significance over LM baseline by both tests ( p &lt; 0.005). Features used MAP Features used MAP IDF only 0.1776 All 5 features 0.2261 IDF + Centrality 0.2076 All but Centrality 0.2235 IDF + Synonymy 0.2129 All but Synonymy 0.2066 IDF + Replaceable 0.1699 All but Replaceable 0.2211 
IDF + DepLeaf 0.1900 All but DepLeaf 0.2226 The reweight-Trained method performs slightly better than with the SVD features on average, but also slightly less stable, in terms of significance levels over the Language Model baseline. 
The resulting prediction model is a function that maps the RM term weights into necessity, and the final form of the function, trained on the TREC 3 dataset, is pl otted in Figure 4. It basically boosts terms with lower middle range RM weights (0-0.6) to larg-er values. This simple mapping works because the Relevance Model tends to underestimate the term necessity values . 
Including Relevance Model weight (RMw) as the 6th feature further improves MAP by about 5%, but the meta-parameters become less stable during cross va lidation. Given that the focus of the paper is to establish the framework of supervised necessity learning and prediction, we omit these further results. To see the effects of individual features, in Table 7, we present retrieval performance for different feature combinations using TREC 3 as training and TREC 4 as test set. Results show that all the features contribute to effect ive retrieval. Among them, the Synonymy feature is most effective and results in more stable improvements, but the other features can still significantly im-prove retrieval without Synonymy. Similar trends are observed on the other test sets. The only exception is the TREC 10 test set, where topic centrality outperforms synonymy, and the improve-ment from synonymy alone is not significant by the sign test. 
Overall, the evaluation using L1-loss of necessity prediction (Table 3) and feature correlation (T able 6) show consistent trends as retrieval effectiveness (Table 7). IDF alone is not effective in predicting necessity. All the 5 designed features contribute to necessity prediction accuracy, and further, retrieval effectiveness. For the current set of features, better necessity prediction leads to better retrieval performance, whic h is also consistent with the Oracle performance of Table 4. SVD and dependency parsing take most of the processing time. Per query, SVD on 300 documents with 20,000 terms takes ~1 second. Further speedup is possi ble. Since the goal of SVD is just to find possible searchonym s of query terms, SVD does not need to converge; a smaller numb er of iterations may suffice. Dependency parsing takes ~3 seconds per query. Speedup is also possible, as the feature only indicates whether a term is a leaf node in the parse tree, and does not require knowledge of the whole parse tree. Simpler but more direct dependency leaf identi-fication models would suffice. Alternative term abstractness measures can also help avoid parsing the queries. Research on predicting P  X   X  |  X  X  can be viewed as one specific line of research within a broad body of research intended to predict effective term weights. Earlier work includes the Binary Inde-pendence Indexing [24] and the Berkeley regression [25]. More recently, Lease et al applied regression rank [6] to optimize re-trieval performance (MAP) by direct ly tuning term weights. All of these approaches used regres sion to predict term weights and most of this prior work used only features based on simple tf-or idf-based statistics. Regression rank used more features, includ-ing part-of-speech and a term X  X  location in a query. It also tuned user term weights to directly op timize MAP. Wit hout a relevance probability prediction in the middle , its formulation is more direct than ours, but also harder, e.g. optimal term weights are difficult to find, and not unique. Regre ssion rank normalized term weights within each query to make the problem tractable, which produces weights that cannot be compared easily across different queries. This is not generally viewed as a serious flaw, however our data analysis suggests that term nece ssity probabilities estimated for one query are often effective for the same term in other queries. Moreover, our necessity predicti on framework has a simple and well defined objective  X  term necessity, it uses fewer well justi-fied and interpretable features, and achieves comparable perfor-mance. Understanding necessity leads to a better understanding of why basic retrieval models and PRF methods work, which can guide query formulation, and shows the generality of our work. 
Query reduction for long queries [11, 12] is another relevant problem. Essentially, unnecessary terms in long queries are to be reduced. In fact,  X  X  X   X   X  X  X  in equation (2) of [11] can be interpreted as a necessity measure of a query concept  X   X  . [11] even hinted at concept necessity in the introduction,  X  X oncepts ... must be ... in a retrieved document in order for it to be relevant X , though concept keyness was actually pursued in the experiments. Term necessity, the necessity of a term X  X  occurrence to the relev-ance of a document P  X   X  |  X  X  , is of central importance for probabil-istic retrieval models, sharing a similar role as idf. The necessity to non-relevance is a precision meas ure easily predicted by idf [2], while the necessity to relevance is a term recall measure and is harder to predict. Most previous work predicted term weights as a whole, consisting of the idf and the necessity components [2], or as a way to improve idf weighting [14]. We attempt to predict the necessity probability itself, through understanding the probability and identifying factors that may affect it. These factors include a term X  X  centrality to the topic, term synonymy, replaceability by synonyms, rareness and abstractness. Using 5 features designed to capture the factors, term necessity is effectively predicted. 
Predicted necessity used as user query term weig hts significant-ly improves ad-hoc retrieval of verbose queries. On 6 standard TREC ad-hoc retrieval and web search collections of different sizes and judgment depths, predicte d necessity brings a consistent and significant 10% to 25% impr ovement on verbose queries. Results also show that weighting terms by their ground truth ne-cessity estimated from relevance j udgments gives a significant 30% to 80% improvement over state-of-the -art ad-hoc retrieval models. 
Necessity improves our understa nding of why pseudo relevance feedback works, and can be used to improve PRF methods by supervised necessity learning. Experiments show that most of the performance gain from pseudo relevance feedback comes from the ability to reweight query terms and the weights correlate well with necessity, even though estimated in an unsupervised fashion. 
Figure 4. The learnt function using Relevance Model term weights (x-axis) alone to predict term necessity (y-axis).
To follow up on this work, a more complete understanding of the causes of low necessity or mismatch is needed. Creating bet-ter features for these causes and factors will also motivate the design of better NLP tools to at tack problems such as measuring term abstractness and identify ing synonyms or searchonyms. 
For ad-hoc retrieval , a better understanding and theoretical jus-tification of applying necessity is needed, especially what form of weighting should be used in cases of prediction inaccuracies. 
Necessity prediction can also be applied to (structured) query formulation in general. For exam ple, in dealing with long queries [6, 11, 12], necessity can help decide which terms to include. 
Among the factors that affect necessity, the relation between necessity and synonymy is especially important. It derives effec-tive features for predicting necessity, and also suggests that neces-sity can be applied to guide query expansion toward the terms that actually need expansion. Expansion can be applied to a specific query term, instead of the whol e query, for example using the synonym (#syn) operator in the Indri query language. This search strategy gives more control over where and how much to expand, and is favored by expert searchers in library, biomedical or legal domains, but is less explored by ge neral retrieval systems. Query formulation and modification here can be totally automatic or through suggestions and user interactions. We thank Andrea Bastoni and Lo renzo Clemente for maintaining and making the SVD code availabl e for the Lemur toolkit. This work is supported by National Science Foundation grant IIS-0707801 and IIS-0534345. The views and conclusions are the authors X , and do not reflect those of the sponsor. [1] S. E. Robertson and K. Sp X rck Jones. Relevance weighting of [2] W. Greiff. A theory of term weighting based on exploratory data [3] S. E. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and [4] J. M. Ponte and W. B. Croft. A language modeling approach to [5] INDRI -Language modeling meets inference networks. [6] M. Lease, J. Allan and W. B. Cr oft. Regression rank: Learning to [7] W. B. Croft and D. J. Harper. Using probabilistic models of docu-[8] C. T. Yu, K. Lam, and G. Salton. Term weighting in information [9] S.E. Robertson. On relevance weight estimation and query expan-[10] S. Cronen-Townsend, Y. Zhou and W. B. Croft. Predicting query [11] M. Bendersky, W. B. Croft. Disc overing key concepts in verbose [12] G. Kumaran and V. Carvalho. Reducing long queries using query [13] Y. Lu, H. Fang and C. Zhai. An empirical study of gene synonym [14] D. Metzler. Generalized inverse document frequency. In Pro-[15] M. D. Smucker, J. Allan and B. Carterette. A comparison of sta-[16] J. Allan, M. Connell, W. B. Croft, F. Feng, D. Fisher and X. Li. [17] V. Lavrenko and W. B. Croft. Relevance-based language models. [18] L. Zhao and J. Callan. Effective and efficient structured retrieval [19] H. Sch X tze, D.A. Hull and J.O. Pe dersen. A comparison of clas-[20] A. Kontostathis and W. M. Pottenger. Detecting patterns in the [21] C.J. van Rijsbergen. Information Retrieval (2nd Edition) , chapter 6. [22] R. Lawlor. Information technology and the law. Advances in [23] G. Goertz and H. Starr (eds.) Necessary conditions: theory, me-[24] N. Fuhr and C. Buckley. A probabilistic learning approach for [25] W. Cooper, A. Chen and F. Gey. Full text retrieval based on [26] V. Dang and W. B. Croft. Query reformulation using anchor text. [27] D. Metzler, V. Lavrenko and W. B. Croft. Formal Multiple-
