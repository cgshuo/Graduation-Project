 Categories and Subject Descriptors: H.3.3 [Informa-tion Storage and Retrieval]:Information Search and Retrieval General Terms: Algorithms, Design, Human Factors Keywords: Semi-supervised Learning, Metric Learning
Maintaining a hierarchical taxonomy of text documents is of great interest for many applications. Hierarchical taxon-omy provides a well-structured organization of documents by different levels of abstraction. The connection of the nodes in the taxonomy typically conveys the generalization and specification relationships. Very often it is infeasible for a user to construct the full taxonomy. However, it is much easier for a user to specify a partial taxonomy with some sample documents. For example, as shown in Figure 1, the user provides a partial taxonomy organized in a two-level hierarchy in the finance news domain. The first level consists of general clusters, namely,  X  X orporate/industrial X ,  X  X conomics X , and  X  X arkets X . More specific sample clusters under each node in first level are specified. 1 In each of the leaf node, the user also provides some sample documents.
This paper investigates a new method for expanding the hierarchical taxonomy based on labeled and unlabeled doc-uments. Given a user provided partial hierarchical taxon-omy, some sample documents in the partial taxonomy, and a set of unlabeled documents, the objective is to expand the taxonomy and cluster all labeled and unlabeled docu-ments. Figure 2 depicts an example of a refined taxonomy discovered from the partial taxonomy given in Figure 1. The unlabeled news stories can be assigned to the existing sam-ple clusters. New clusters with similar granularity are also generated appropriately. In the first level, a new node is discovered from the labeled and unlabeled documents. Fur-thermore, in the second level, new nodes are also formed with similar granularity as the sibling nodes.
 There are some existing work on hierarchical clustering. Ying and George [4] evaluated nine agglomerative algorithms and six partitional algorithms. Shivakumer and Byron [3] investigated a model-based hierarchical clustering by formu-lating an objective function based on a Bayesian analysis. None of these approaches considers using the user provided sample documents to guide the clustering process. There are also some semi-supervised learning research work. Sug-ato et al. [1] investigated a theoretically motivated frame-work for semi-supervised clustering that employs Hidden Random Markov Fields. Mikhail et al. [2] described an ap-proach that unifies the constraint-based and metric-based semi-supervised clustering methods. A shortcoming of these approaches is that they cannot handle hierarchical taxon-omy structures.
This problem can be formulated as a semi-supervised clus-tering with taxonomy expansion. The whole document set D consists of a set of labeled documents L and unlabeled documents U . The labeled documents are organized in a hierarchy. The goal is to conduct clustering on the whole document set D and the taxonomy provided by user is al-lowed to expand if needed. The user provided labels of the documents in L are not violated in the result.

This problem can be decomposed into a series of sub-problems by considering the local structure of the taxon-omy for each node. Consider a certain node. The problem can be regarded as a semi-supervised flat clustering. The labeled documents are regarded as constraints to guide the clustering algorithm towards a more appropriate data par-titioning. In high-dimensional situations, clusters are better characterized by a subset of the whole feature dimension. Each cluster emphasizes on different features. Therefore, the performance of discovering the clusters can be improved if the appropriate different set of features for each cluster can be chosen. We investigate the idea of learning a sepa-rate weight metric vector locally for each cluster, denoted by a l for cluster l . We assume that the number of final clus-ters K is predefined for the particular node. Our algorithm tries to learn the clusters by minimizing an objective func-tion using an EM-based iterative algorithm. The objective function  X  is formulated as follows: where D l is the document set that belongs to the cluster m l is the number of features in a l ; x and  X  l are the vector representation of a document and the centroid of the cluster l respectively; x i , a l i ,and  X  l i are the weights of feature the vector representation of a document, local weight metric, and the centroid of the cluster l respectively; || X || denotes the L 2 norm. The outline of the algorithm is presented in as follows:
The user provided samples are used as seeds to initialize the cluster centroids. Let  X  be the number of user provided sample clusters. We assume that  X   X  K . In addition to the  X  sample document centroids, we select K  X   X  centroids using the farthest-first algorithm.

In the E-step, the assignments of the documents to the clusters are updated. Since we intend not to violate the user provided sample document labels in the result, each document in L is kept unchanged in the progress of the algorithm. Each document in U is assigned to the cluster that minimizes its contribution to the objective function.
The M-ste pconsists of two parts. Every cluster centroid  X  is re-estimated using the current document to cluster as-signment as follows:  X  l = of the M-ste p performs weight metric learning on each clus-ter. The metric is re-estimated to decrease the objective function  X . Fletcher-Reeves conjugate gradient algorithm is employed to learn the vector a l until a local optimal value of the objective function is reached.
In our experiments, we used 2 data sets. The first dataset was derived from the TDT3 corpus 2 . We selected the na-tive English newswire news documents with human assigned clusters. The dataset contains 2450 news stories organized as a two-level hierarchical taxonomy. In the first level, there are 12 topic categories. There are 94 event clusters in the second level. The second dataset was derived from Reuters RCV1 corpus. We randomly selected stories belonging to at most one of the first two-level clusters. 2400 news stories were chosen. In the first level, there are 4 general clusters. There are 24 specific clusters in the second level. 100 news stories were selected for each cluster.

We used F-measure as our evaluation measure. Two sets of experiments were conducted. In each experiment, we var-ied the percentage of the clusters that have labeled samples. In the first experiment, we compared the performance of the algorithm with and without local metric learning. The qual-ity of the clustering solution is evaluated separately in each level of the taxonomy. The results are illustrated in Figure 3 and Figure 4. The results demonstrate that the local metric learning obviously improves the clustering performance.
In the second experiment, the number of sample labeled documents for each cluster was also varied. As shown in Figure 5 and Figure 6, the percentage of the cluster with labeled samples affects the clustering performance to a larger extent than the number of the sample labeled documents for each cluster.
