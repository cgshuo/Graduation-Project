 Automated evaluation of topic quality remains an impor-tant unsolved problem in topic modeling and represents a major obstacle for development and evaluation of new topic models. Previous attempts at the problem have been formu-lated as variations on the coherence and/or mutual informa-tion of top words in a topic. In this work, we propose several new metrics for evaluating topic quality with the help of dis-tributed word representations; our experiments suggest that the new metrics are a better match for human judgement, which is the gold standard in this case, than previously de-veloped approaches.
 topic quality; topic modeling; text mining
Evaluating topic quality has been an important problem in topic modeling since its very inception. The problem here is that while it is usually immediately evident for a human whether a topic is  X  X ood X  or not, i.e., whether it is easily in-terpretable and can serve to draw conclusions regarding the dataset, it is hard to evaluate it automatically. This problem is especially prominent in real-life applications of topic mod-eling to social sciences, where the goal is usually to get an overview of what the dataset is about and which documents to actually read, which is impossible without interpretable topics [5, 15]. However, it it still an open and interesting problem to devise metrics that would give good approxi-mations to human interpretability; actual human evaluation remains the gold standard here.

In this work, we propose several candidate metrics based on distributed word representations. Word representations have been extensively used in natural language processing; the idea is to map words into some kind of semantic space where geometric relations between vectors will supposedly correspond to semantic relations between the original words. In this work, we posit that such embeddings and the se-mantic information that they capture can be leveraged to evaluate topic models, with results significantly improving upon previously known techniques. We perform quantitative evaluation by comparing the ranking produced by automatic quality metrics with rankings produced by human experts asked specifically to evaluate topic interpretability. In other words, we measure interpretability directly by a consensus of human experts and try to approximate it by automated metrics. We show that even very simple metrics that mea-sure how close top words in a topic are in the semantic space still significantly outperform previously used metrics.
The paper is organized as follows. In Section 2, we de-scribe the problem setting in detail and survey related work, including previously known approaches to topic evaluation that we compare with. In Section 3, we describe the new metrics based on distributed representations. Section 4 de-scribes our experimental setup and presents the results, and Section 5 concludes the paper.
We begin by surveying (very briefly due to space con-straints) the topic models whose results we try to evaluate. Let D be a finite set (collection) of texts and W a finite set (vocabulary) of all terms from these texts. Probabilis-tic topic models represent the text collection as a sequence of triples ( d i ,w i ,z i ), where d i is a document, w i and z i is the topic from which w i has been drawn in this instance; d i and w i are observed from the data, while z are latent variables. Introducing the word-topic distribu-tions  X ,  X  wt = p ( w | t ), and document-topic distributions  X ,  X  P t  X  T  X  wt  X  td , and the generative process is similar in all topic models: for every word instance, we first sample the topic t i from distribution p ( t | d ) and then sample the word w from distribution p ( w | t i ).

In the basic probabilistic latent semantic analysis (pLSA) model [10],  X  and  X  matrices are learned by directly opti-mizing the log-likelihood of the training dataset L ( X  ,  X ) = P approach known as additive regularization of topic models (ARTM) [19], the basic pLSA model is augmented with ad-ditive regularizers, and  X  and  X  matrices are learned by maximizing a linear combination of L ( X  ,  X ) and r regulariz-ers R i ( X  ,  X ), i = 1 ,...,r with regularization coefficients  X 
R ( X  ,  X ) = This makes it easy to devise new regularizers, adding desired properties to the topic model [19, 20].

The latent Dirichlet allocation (LDA) model [3, 4, 8] in-troduces prior Dirichlet distributions for the vectors of term probabilities in topics  X  t  X  Dir(  X  ) as well as for the vectors of topic probabilities in documents  X  d  X  Dir(  X  ) with param-eters  X  and  X  respectively. Inference in LDA is usually done via either variational approximations or Gibbs sam-pling. Over the last decade, LDA has been subject to many extensions (too many to survey them here), each of them presenting either a variational of a Gibbs sampling algorithm for a model that builds upon LDA to incorporate some ad-ditional information or additional presumed dependencies.
In each case, the result of learning a topic model can be represented as the  X  and  X  matrices. In this work, we con-centrate on evaluating the quality of the  X  word-topic dis-tributions, usually presented to a user as an ordered list of top words for every topic, i.e., words with the largest  X  wt = p ( w | t ).
The modern neural network approaches to natural lan-guage processing can be roughly divided into two subprob-lems: constructing and training new models for individual words (this field is known as word embeddings or distributed word representations) and developing subsequent layers of deep architectures to find syntactic and semantic features while taking into account the context of a word in a sentence and specific problems that a system attempts to solve.
To train distributed word representations, one first con-structs a vocabulary with one-hot representations of individ-ual words (where each word is represented with a vector of size equal to vocabulary size with a single 1) and then trains representations for individual words starting from there, ba-sically as a dimensionality reduction problem. For this pur-pose, researchers have usually employed a model with one hidden layer that attempts to predict the next word based on a window of several preceding words. Then representa-tions learned at the hidden layer are taken to be the word X  X  features; this approach has been applied, for instance in the Polyglot system developed in 2013 [1] and in other methods of learning distributed word representations [17]. A recent study on the performance of various vector space models for word semantic similarity evaluation [16] demostrates that compositions of models such as GloVe and Word2Vec as well as unsupervised one-model approaches show reasonable results for the Russian language (which we use in evalua-tions since we have expert evaluations available for Russian-language topics).
Next, we survey the topic quality metrics that we build upon in this work. We begin with coherence , proposed as a topic quality metric in [7, 13]. For a topic t characterized by its set of top words W t , coherence is defined as where d ( w i ) is the number of documents that contain w d ( w i ,w j ) is the number of documents where w i and w cur, and is a smoothing count usually set to either 1 or 0 . 01. Coherence and word cooccurrence statistics in gen-eral have also been used to initialize LDA parameters [18]. However, in a recent work [15] coherence was criticized for a number of shortcomings, primarily because it was found to be too heavily reliant on common words that cooccur often but do not define interpretable topics. To alleviate this, the work [15] proposed a modification of the coherence metric called tf-idf coherence defined as where the tfidf metric is computed with augmented fre-quency, tfidf( w,d ) = tf( w,d )  X  idf( w ) = where f ( w,d ) is the number of occurrences of term w in document d . This skews the metric towards topics with high tfidf scores in top words, since the numerator of the coherence fraction has quadratic dependence on the tfidf scores and the denominator only linear.

Another class of topic quality metrics is based on the notion of pairwise pointwise mutual information (PMI) be-tween the top words in a topic. Following a recent work [11], we compute three variations on this idea. For a given or-dered set of top words W t = ( w 1 ,...,w N ) in a topic, (1) the basic pairwise PMI metric [14] is computed as (2) the normalized PMI variation [6] is computed as (3) the pairwise log conditional probability (LCP) metric
In this section, we propose a number of metrics for eval-uating topic quality based on distributed word representa-tions. Suppose that a vector space model has been trained, and every vocabulary word w  X  W has been assigned with a vector v w  X  R d . The basic assumption in our metrics is that a good metric should be well localized in the semantic space: specifically, top words in a topic should be close to each other in the semantic space.

Hence, we use the following general scheme for topic qual-ity metrics: given a set of top words W t for a topic t with weights (word probabilities)  X  wt , w  X  W t , their distributed representations c w  X  R d , and a distance function d : R R d  X  R , we define topic quality as the average distance between the top words in the topic: If d ( w 1 ,w 2 ) is a distance function in the space results correspond to, supposedly, worse topics (with words not as localized as in topics with smaller average distances). We have compared four different distance-like metrics: (1) cosine distance d cos ( x,y ) = 1  X  x &gt; y (no normalization (2) L 1 -distance d L 1 ( x,y ) = P d i =1 | x i  X  y i | ; (3) L 2 -distance d L 2 ( x,y ) = P d i =1 ( x i  X  y i ) 2 (4) coordinate distance d coord ( x,y ) = P d i =1 [ x
The core of all proposed metrics are word vector represen-tations. We used the skip-gram word2vec model of dimen-sion 500 trained on a large Russian language corpus [2, 16]; the corpus consisted of: A large collection of general-purpose texts ensured good re-sulting distributed representations; for an in-depth descrip-tion of the model we refer to [2, 16].

To evaluate the proposed approach, we have used a dataset with approximately 1 . 58 million lemmatized posts from the top LiveJournal bloggers, all in Russian; the Russian lan-guage was chosen since we had experts estimates available for topic interpretability only in Russian. The complete vo-cabulary amounted to 860K words, but after preprocessing it was reduced to 90K words; after dictionary reduction and filtering, there remained about 1 . 38 million nonempty doc-uments. We have trained all models with T = 400 topics, a number chosen by training pLSA models with 100, 300, and 400 topics and evaluating the results.

We have evaluated the quality of six different topic mod-els; since the human coding results were obtained as part of a case study for mining ethnic-related content, two mod-els work specifically with ethnonyms, but in each case the assessors simply evaluated top words in every topic: (1) probabilistic latent semantic analysis model (pLSA); (2) latent Dirichlet allocation model (LDA); (3) ARTM model with smoothing and sparsity regularizers; (4) ARTM model with a decorrelation regularizer; (5) ARTM model with a separate modality for ethnonyms, (6) ARTM model with a separate modality for ethnonyms,
Human assessors were asked to interpret the topics based on 20 most probable words in every topic of each model. For each topic, assessors answered the following question:  X  X o you understand why these words are collected together in this topic? X . They were also told that the idea was to see whether the topic was generally understandable and given three options: (a) absolutely not; (b) partially; (c) yes.
The  X  X orrect X  (human-generated) ranking of topics was produced according to the total number of positive answers, counting (b) as 1 and (c) as 2. For comparison, we have computed five previously known metrics: coherence, tf-idf coherence, PMI, NPMI, and LPC 1 , and four word2vec-based metrics with four different distances shown in Section 3.
To evaluate how well a metric matches this ranking, we have computed, for each subject and each metric, the area-under-curve (AUC) measure [9, 12]. AUC is a popular qual-ity metric for classifiers that produce ranking results; by definition it represents the probability that for a uniformly selected pair consisting of a positive and a negative example the classifier ranks the positive one higher. Thus, the opti-mal AUC is 1 (all positive examples come before negative ones), the worst possible AUC is 0, and a random classifier would get an AUC of 0 . 5. In our case AUC is the perfect fit since the actual values of a metric are mostly irrelevant, and the users are interested in the ranking (to view the best topics from a dataset).

Results of our experiments are shown in Table 1. We see that the new word2vec-based metrics outperform, often significantly, previously known approaches. As expected, tf-idf coherence is better than regular coherence and NPMI is better than PMI, but all of them lose to vector space metrics in most cases. There is little difference between the new metrics themselves, but we can recommend to use L 1 and L 2 metrics.
In this work, we have proposed a number of simple metrics based on distributed word representations for the purpose of evaluating topic quality in topic modeling results. We have shown that the new metrics outperform previously used topic quality metrics in terms of agreeing with human inter-pretation. The metrics introduced in this work are rather straightforward; in further work we expect to significantly improve upon our results shown here by learning distribu-tions in the semantic space that are better aligned with ac-tual topics. For instance, it is perfectly plausible for a topic to have several clusters of closely matching words that de-scribe the same issue from different sides; in this case, it would probably be beneficial to consider a clustering model. We expect new exciting developments along these lines, al-though even the basic metrics proposed here already outper-form existing state of the art topic evaluation approaches.
To compute PMI, NPMI, and LPC we have used the com-panion software for [11] available at https://github.com/ jhlau/topic interpretability. Model Quality metrics human-generated interpretability evaluation and automatic quality metrics. This work was supported by the Russian Science Founda-tion grant no. 15-18-00091. I thank Olessia Koltsova and Sergei Koltcov for the experimental human coding data, Konstantin Vorontsov, Anna Potapenko, Murat Apishev, and Oleksander Frei for the topics themselves, and Alexan-der Panchenko and Nikolay Arefyev for the Russian-language training corpora used for the word2vec models. [1] R. Al-Rfou, B. Perozzi, and S. Skiena. Polyglot: [2] N. Arefyev, A. Panchenko, A. Lukanin, O. Lesota, and [3] D. M. Blei. Introduction to probabilistic topic models. [4] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [5] S. Bodrunova, S. Koltsov, O. Koltsova, S. I.
 [6] G. Bouma. Normalized (pointwise) mutual [7] J. Chang, J. Boyd-Graber, S. Gerrish, C. Wang, and [8] T. Griffiths and M. Steyvers. Finding scientific topics. [9] D. J. Hand and R. J. Till. A simple generalisation of [10] T. Hoffmann. Unsupervised learning by probabilistic [11] J. H. Lau, D. Newman, and T. Baldwin. Machine [12] C. X. Ling, J. Huang, and H. Zhang. AUC: a [13] D. Mimno, H. M. Wallach, E. Talley, M. Leenders, [14] D. Newman, J. H. Lau, K. Grieser, and T. Baldwin. [15] S. I. Nikolenko, O. Koltsova, and S. Koltsov. Topic [16] A. Panchenko, N. Loukachevitch, D. Ustalov, [17] J. Pennington, R. Socher, and C. Manning. GloVe: [18] A. S. Rathore and D. Roy. Performance of LDA and [19] K. Vorontsov. Additive regularization for topic models [20] K. V. Vorontsov and A. A. Potapenko. Additive
