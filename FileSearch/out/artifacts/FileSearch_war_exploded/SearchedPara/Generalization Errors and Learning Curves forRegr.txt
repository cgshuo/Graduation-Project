 Gaussian processes (GPs) (see e.g., [1]) have been applied to many practical problems. In recent years, a number of models for multi-task learning with GPs have been proposed to allow different tasks to leverage on one another [2 X 5]. While it is generally assumed that learning multiple tasks together is beneficial, we are not aware of any work that quantifies such benefits, other than PAC-works on GPs in machine learning, our goal is to quantify the benefits using average-case analysis. We concentrate on the asymmetric two-tasks case, where the secondary task is to help the learning of the primary task. Within this setting, the main parameters are (1) the degree of  X  X elatedness X   X  curve and optimal error, and investigate the influence of  X  and  X  S on these quantities. We will give non-trivial lower and upper bounds on the generalization error and the learning curve. Both types of bounds are important in providing assurance on the quality of predictions: an upper bound provides an estimate of the amount of training data needed to attain a minimum performance approach relates multi-task GPs to single-task GPs and admits intuitive understandings of multi-task GPs. For one-dimensional input-space under optimal sampling with data only for the secondary task, we show the limit to which error for the primary task can be reduced. This dispels any misconception that abundant data for the secondary task can remedy no data for the primary task. 2.1 Multi-task GP regression model and setup The multi-task Gaussian process regression model in [5] learns M related functions { f m } M m =1 by placing a zero mean GP prior which directly induces correlations between tasks. Let y m be an observation of the m th function at x . Then the model is given by similarities, and  X  2 m is the noise variance for the m th task.
 performance of the primary task T ; this is the asymmetric multi-task learning as coined in [10]. We n The covariance matrix of the noisy training data is K (  X  ) +  X  2 n I , where K
TS is the matrix of cross-covariances from locations in X T to locations in X S ; and K transposed. The posterior variance at x  X  for task T is  X 
T ( x  X  , X , X  between locations in X T (resp. X S ) and x  X  . Where appropriate and clear from context, we will  X  x actual y -values observed at X do not effect the posterior variance at any test location. Problem statement Given the above setting, the aim is to investigate how training observations learning curve and optimal error, and investigate how these quantities vary with  X  and  X  S . 2.2 Generalization errors, learning curves and optimal errors We outline the general approach to obtain the generalization error and the learning curve [1,  X  7.3] under our setting, where we have two tasks and are concerned with the primary task T . Let p ( x ) be the probability density, common to both tasks, from which test and training locations are drawn, by averaging the generalization error over training sets X : design methodology and minimize T over X to find the optimal generalization error [11, chap. II]: training observations at X S , while the latter includes them. Similar analogues to single-task GP  X  Figure 1: The two tasks S and T have task correlation  X  . The data set X T (resp.
 X
S ) for task T (resp. S ) consists of the  X  s (resp. s). The test location x  X  for task T is denoted by ~ . 2.3 Eigen-analysis  X   X  [11, Proposition IV.10, Remark IV.2]. Broadly speaking, an order r process is exactly r times mean square differentiable. For example, the stationary Ornstein-Uhlenbeck process is of order r = 0 . In this section, we derive expressions for the generalization error (and the bounds thereon) for the and three observations for task S . We roughly follow [13, Fig. 2], and use squared exponential co-veloping each solid curve are the lower and upper bounds derived in this section; the dashed curves are hardly visible because the bounds are rather tight. The dotted line is the prior noise variance. definition, it is clear that the following trivial bounds on  X  2 T ( x  X  , X  ) hold: Proposition 1. For all x  X  ,  X  2 T ( x  X  , 1) 6  X  2 T ( x  X  , X  ) 6  X  2 T ( x  X  , 0) . Integrating wrt to x  X  then gives the following corollary: Corollary 2. T (1 , X  2 n ,X T ,X S ) 6 T (  X , X  2 n ,X T ,X S ) 6 T (0 , X  2 n ,X T ,X S ) . Sections 3.2 and 3.3 derive lower and upper bounds that are tighter than the above trivial bounds. 3.1 The degenerate case of no training data for primary task  X  primary task, that is, X T =  X  , we instead have the following proposition: location for task S from having correlation higher than  X  with a test location for task T . Suppose translate correlations into distances between data locations, then any training location from task S T may lay arbitrarily close to a test location for task T , subject to the constraints of noise. and using the fact that the mean prior variance is given by the sum of the process eigenvalues. Corollary 4. T (  X , X  2 n ,  X  ,X S ) =  X  2 T (1 , X  2 n ,  X  ,X S ) + (1  X   X  2 ) P  X  i =1  X  i . 3.2 A lower bound  X  depends on x  X  obstructs further analysis. The next proposition gives a lower bound same form satisfying  X  2 T (1) 6 Proposition 5. Let The proofs are in supplementary material  X  S.2. The lower bound  X  gives the trivial lower bound  X  2 T (1) on  X  2 T (  X  ) . The tighter bound is possible because  X  is independent of x  X  .
 Corollary 6. Let  X  (  X , X  2 n ,X T ,X S ) 6 T (  X , X  2 n ,X T ,X S ) . 3.3 An upper bound via equivalent isotropic noise at X S The following question motivates our upper bound: if the training locations in X S had been observed of  X  2 T (  X  ) to include a different noise variance parameter s 2 for the X S observations: the equivalent isotropic noise is a function of x  X  defined by the equation For any x  X  there is always a  X   X  2 n that satisfies the equation because the difference is a continuous and monotonically decreasing function of s 2 . To make progress, we seek an upper  X   X  Proof sketch. Matrix K (  X  ) may be factorized as motivates the question that began this section. Note that  X   X  2 &gt; 1 , and hence  X  &gt; 0 . The increase in posterior variance due to having X S at task S with noise variance  X  2 n rather than  X  S.3. The tightness  X   X   X  2 n is evident from the construction.
 by is obtained by substituting in zero for an additional  X  (  X   X  +  X  2 n ) noise variance, and at least as  X  X oisy X  as an additional  X  ( i.e., when the task S is more correlated with task T .
 in section 2.3. For large enough n S , we may write  X   X   X  n S  X   X  and  X   X  =  X  (  X n S )  X  2 r  X  1 . Since  X   X   X  2 n and and contrast the lower bound even for moderate sizes n S . Therefore, the lower bound is not as useful as the upper bound. Finally, if we refine T as we have done for  X  2 T in (8), we obtain the following corollary: 3.4 Exact computation of generalization error integral expression. An example is the case of the squared exponential covariance function with normally distributed x , when the integrand is a product of three Gaussians. Proposition 9. Under optimal sampling on a 1-d space, if the covariance function satisfies Sacks- X  the average. As this is a lower bound, the same can be said for incorrectly specified GP priors. Using the results from section 3, lower and upper bounds on the learning curve may be computed by averaging over the choice of X using Monte Carlo approximation. 1 For example, using Corollary 2 and integrating wrt p ( X )d X gives the following trivial bounds on the learning curve: Corollary 10. avg T (1 , X  2 n , X  S ,n ) 6 avg T (  X , X  2 n , X  S ,n ) 6 avg T (0 , X  2 n , X  S ,n ) .
T (0 , X  the trivial bounds becomes wider with  X  S .
 providing simulation results. Theoretical bounds are particularly attractive for high-dimensional input-spaces, on which Monte Carlo approximation is harder. 5.1 Lower bound shall call this the single-task OV bound. This lower bound can be combined with Corollary 6. Proposition 11. avg T (  X , X  2 n , X  S ,n ) &gt;  X  2  X  2 n or equivalently, avg T (  X , X  2 n , X  S ,n ) &gt;  X  2 n or equivalently, avg T (  X , X  2 n , X  S ,n ) &gt;  X  2 n single-task OV bound twice. For the second inequality, its i th summand is obtained by combining the second by swapping the denominator of b 1 i with that of  X  i / (  X  2 n + n X  i ) for every i . OV  X  over OV 1 is more noticeable for lower  X  2 , higher  X  S and rougher processes. The second expression in the Proposition 11 is useful for comparing with the OV 1 . Each summand b Similarly, the third expression of the proposition is useful for comparing with OV 0 : each summand which multi-task learning can outperform the single-task learning that ignores the secondary task. 5.2 Upper bound using equivalent noise An upper bound on the learning curve of a single-task GP is given in [16]. We shall refer to this as the single-task FWO bound and combine it with the approach in section 3.3 to obtain an upper on the learning curve of task T . Although the single-task FWO bound was derived for observations bound that has yet to assume isotropic noise: Theorem 12. ([16], modified second part of Theorem 6) Consider a zero-mean GP with covari- X  ij def =  X  j ( x i ) . Then the learning curve at n is upper-bounded by P c = ( X  T H  X ) ii /n , and the expectation in c i is taken over the set of n input locations drawn independently from p ( x ) .
 variance given by (12). Thus we set the observation noise (co)variance  X  2 ( x i , x j ) to so that, through the definition of c i in Theorem 12, we obtain details are in the supplementary material  X  S.5. This leads to the following proposition: Proposition 13. Let  X  def =  X   X  2  X  1 . Then, using the c i s defined in (15) , we have Denote the above upper bound by FWO  X  . When  X  =  X  1 or  X  S = 0 , the single-task FWO upper bound is recovered. However, FWO  X  with  X  = 0 gives the prior variance P  X  i instead. A trivial upper bound can be obtained using Corollary 10, by replacing n with (1  X   X  S ) n in the single-task 5.3 Comparing bounds by simulations of learning curve We compare our bounds with simulated learning curves. We follow the third scenario in [13]: the in-and consists of the  X  X rue X  multi-task learning curve (middle ), the theoretical lower/upper bounds of Propositions 11/13 (lower/upper ), the empirical trivial lower/upper bounds using Corollary thickness of the  X  X rue X  multi-task learning curve reflects 95% confidence interval. The empirical average over X def = X T  X  X S , denoted by  X  X  X  X  X  , is computed over 100 randomly sam-pled training sets. The process eigenvalues  X  i s needed to compute the theoretical bounds are given in [17]. Supplementary material  X  S.6 gives further details.
 and 13; (c) the trivial upper and lower bounds that are single-task learning curves  X  X  T (0)  X  X  and  X  X  We summarize with the following observations: (a) The gap between the trivial bounds  X  X  T (0)  X  X   X  X   X  the much overlap between the  X  lines and the middle lines in Figure 3. (c) The curve for the asymptotically exact single-task OV bound and the the multi-task learning curve rather closely (point (b)).
 Conclusions We have measured the influence of the secondary task on the primary task using the generalization error and the learning curve, parameterizing these with the correlation  X  between the two tasks, and the proportion  X  S of observations for the secondary task. We have provided bounds GPs with more than two tasks. Analysis on the degenerate case of no training data for the primary of multi-task learning that is orthogonal to the existing PAC-based results in the literature. Acknowledgments I thank E Bonilla for motivating this problem, CKI Williams for helpful discussions and for propos-ing the equivalent isotropic noise approach, and DSO National Laboratories, Singapore, for financial support. This work is supported in part by the EU through the PASCAL2 Network of Excellence. References [1] Carl E. Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning . [2] Yee Whye Teh, Matthias Seeger, and Michael I. Jordan. Semiparametric latent factor models. [3] Edwin V. Bonilla, Felix V. Agakov, and Christopher K. I. Williams. Kernel Multi-task Learning [4] Kai Yu, Wei Chu, Shipeng Yu, Volker Tresp, and Zhao Xu. Stochastic Relational Models for [5] Edwin V. Bonilla, Kian Ming A. Chai, and Christopher K.I. Williams. Multi-task Gaussian [6] Jonathan Baxter. A Model of Inductive Bias Learning. Journal of Artificial Intelligence Re-[7] Andreas Maurer. Bounds for linear multi-task learning. Journal of Machine Learning Re-[8] Shai Ben-David and Reba Schuller Borbely. A notion of task relatedness yielding provable [9] Christopher K. I. Williams and Francesco Vivarelli. Upper and lower bounds on the learning [11] Klaus Ritter. Average-Case Analysis of Numerical Problems , volume 1733 of Lecture Notes in [12] Christopher T. H. Baker. The Numerical Treatment of Integral Equations . Clarendon Press, [13] Peter Sollich and Anason Halees. Learning curves for Gaussian process regression: Approxi-[14] Noel A. Cressie. Statistics for Spatial Data . Wiley, New York, 1993. [15] Manfred Opper and Francesco Vivarelli. General bounds on Bayes errors for regression with [16] Giancarlo Ferrari Trecate, Christopher K. I. Williams, and Manfred Opper. Finite-dimensional [17] Huaiyu Zhu, Christopher K. I. Williams, Richard Rohwer, and Michal Morciniec. Gaussian [18] Michael J. Kearns, Sara A. Solla, and David A. Cohn, editors. Advances in Neural Information
