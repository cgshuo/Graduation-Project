 Advanced Technologies Application Center, Siboney, Playa, Havana, Cuba 1. Introduction
Data mining is the nontrivial extraction of implicit, previously unknown, and potentially useful infor-mation from databases. Many data mining approaches have been proposed to extract information, such as itemsets discovery, association rule mining and sequential pattern mining [16].

Sequential pattern mining is more complex than the problem of frequent itemsets discovery and association rule mining [2]. In sequential pattern mining it is very important the order of items and itemsets, as opposed to what happens in itemset mining where this order is not that important. With respect to this, we can say that the set of all frequent sequences is a superset of the set of frequent itemsets. Also, sequential pattern mining discovers inter-transaction patterns (sequences) and association rule discovery covers only intra-transaction patterns (itemsets).
 A sequence is a succession of things (items or itemsets) that bear some relationship with each other. Sequential pattern mining is the discovery of the set of all sequences that occurs in a database with a minimum support also known as threshold.

Sequential pattern mining from large sequence databases is an important problem in the data mining fi eld and it was introduced in 1995 by Agrawal and Srikant with the algorithms, AprioriAll, AprioriSome and DynamicSome [1]. From this moment on it has also been used in a wide variety of applications including the medical fi eld (disease treatments; biological sequences such as DNA, RNA and proteins), analysis of Web access, customers purchasing patterns (credit card usage record), telecommunications, music, text mining and so on.

In sequential pattern mining there is an important kind of sequence: sequences motifs. This kind of sequence is essentially a short distinctive sequence pattern shared by a number of related sequences. Motifs have been studied in music [12] and mostly in biological sequences [7,8,25] for identifying the characteristic of families of DNA, RNA or protein sequences (i.e. comparing families associated with different species/diseases). Moreover, sequential pattern mining brings potential information for diagnoses and treatment of patients; it describes t he amino acid com position of protein s and encodes the function and structure of proteins and so on.

In the case of web mining however, this usage refers to the extraction of useful information and knowledge from Web content and Web usage. Sequential pattern mining from Web logs [20,26] may allow improving Web sites design and services, following user  X  s behavior on particular Web sites and building adaptable Web sites according to different scenarios.

Sequential pattern mining can help to understand the behavior of human activity such as customers purchasing patterns, which means that it can help to identify products to be promoted according to these purchasing patterns and the target customers. Besides, it can also be used to construct the shopping history sequences of customers.

Telecommunications is another area of application, frequent sequences of alarms output by network switches capture important relationships between alarm signals that can be employed for online predic-tion, analysis, and correction of network faults.

For text mining which is another area of applications, sequential pattern mining [10,15] allows the identi fi cation of appropriate text tokens (an important tasks of text preprocessing) and may have great in fl uence on the results of text analysis, categorization and classi fi cation.

The sequential pattern mining approaches can be split into two major groups: apriori [9,36] and sequence growth [3,17,32,33]. The apriori methods are based on a generate-and-test approach, and the sequence growth methods are based on the divide-and-conquer technique. Aside from these two groups, a new technique was introduced in the MEMISP algorithm [21] with the novelty of having a memory-indexing approach which earlier algorithms did not have. Thanks to this approach, this algorithm does not need either candidate generation or database projection. In this paper we analyze those approaches and the performance of some algorithms that use these strategies.

Srikant and Agrawal generalized sequential pattern mining with the algorithm GSP [27], in order to include time constraints, sliding time window and taxonomy. Following this idea of generalizing sequential pattern mining, Garofalakis proposed in [23] the use of regular expressions as a fl exible constraint speci fi cation for mining frequent sequences.

One of the most important characteristics of the sequential pattern algorithms is the kind of sequence that they obtain. The algorithms can mine the full set of frequent sequences, closed sequences [28,29] or maximal sequences [22]. The best option is to obtain the set of closed sequences because it leads to, not only more compact yet complete result set, but also better ef fi ciency.

Due to the importance of sequential pattern mining in the data mining fi led and the complexity that it conveys, several algorithm types has been created in this area such as parallel algorithms [35] and algorithms forincrementaldatabases [5,6,34]. In the realworld, sequence databases change continuously; if we run the sequential pattern mining algorithms from scratch each time records are modi fi ed, deleted or added to a database, it will be very inef fi cient. For this case we may use the incremental algorithms. The main goal of the parallel algorithms is to decrease the processing time for sequential pattern mining.
This paper is organized as follows. The sequential patterns mining de fi nitions are exposed in Sec-tion 2. Section 3 presents the main characteristics of some sequential pattern mining algorithms. The experimental results appear in Section 4 and the conclusions of the study in Section 5. 2. Problem de fi nitions
Let I = { i 1 ,i 2 ,...,i k } be the set of all items. An itemset is a non-empty subset of items in I and can be denoted as ( i 1 i 2 ...i n ) where i j is an item. An itemset with k items is called k -itemset .For example, ( adc ) is a 3 -itemset .
 called an element or transaction of the sequence. An item cannot occur more than once in an element of a sequence, but it can occur multiple times in different elements of a sequence.

In this document we will use the following notation. The set of items of a transaction will be enclosed within parenthesis if the transaction is composed of more than one item. For example, ( cb a ) represents a transaction with three items, ( cb ) a ( da ) represents three transactions with 2, 1 and 2 items respectively.

The number of itemsets in the sequence is called the size of the sequence, and the number of the items in the sequence is called length and is de fi ned as: For example, ( abd )( ca ) is a 5 -sequence of size 2 .

A sequence  X  =  X  1  X  2 ... X  n is a subsequence of another sequence  X  =  X  1  X  2 ... X  m , denoted as  X   X   X  , if there exists integers 1 j 1 &lt;j 2 &lt; ... &lt; j n m such that  X  1  X  j  X 
Given two sequences  X  =  X  1  X  2 ... X  n and  X  =  X  1  X  2 ... X  m with ( m n ), s =  X   X   X  means apre fi xof ( k, h ) jkh and kh is its suf fi x.

Given one sequence  X  = e 1 e 2 ...e m and an element  X  , we can say that  X  is an itemset  X  extension if  X   X  i  X  = e 1 e 2 ... ( e m , X  ) .Also,  X  can be a sequence  X  extension if  X   X  s  X  = e 1 e 2 ...e m  X  ) .
A sequence database ( D = s 1 , s 2 ,...,s n ) is a set of sequences where each sequence in the database has a unique identi fi er. The support of a sequence  X  with relation to a sequence database D is the number of sequence in D that are super-sequences of  X  ( support (  X  ) = s | s  X  D and  X   X  s ). Given a number that represents a threshold called minimum support (denoted minsup ), we say that a sequence  X  is frequent if it occurs more than a minsup speci fi ed. For example, in the database that appears in Table 1, the sequence hm is frequent if minsup = 3.

In the sequential pattern analysis, there are three different sets of sequences that can be obtained: the set of frequent sequential patterns, which includes all the sequenceswhose support is bigger than or equal to the minimum support; the set of closed frequent sequential patterns, which includes all the sequences that have a super-sequence with a different support and is de fi ned as CS = {  X  |  X   X  FS and  X  FS such that  X   X   X  and support(  X  ) = support(  X  ) } ; and the set of maximal frequent sequential patterns, which includes all the sequences that do not have a super-sequence. 3. Algorithms characteristics and techniques
Generally, sequentialpattern mining algorithms are basedeither on the apriori technique (also knownas generate-and-test) or on the pattern growth technique (also known as divide-and-conquer). In this section we expose some differences between these techniques and also some differences among algorithms that use them.

In this section we will present the main features of a group of algorithms based on apriori and pattern growth techniques. This group was selected by their characteristics, historical impact and publication date. 3.1. The apriori technique The fi rst step of this technique is the generation of a candidate sequence set of a given length l (see Section 2) from another set of frequent sequences with length l  X  1 , and the next step is to check if each one of these candidate sequences is frequent in the database, see Algorithm 1. This method begins with the set of all frequent sequences of length one or with the set of frequent items, and each new set of frequent sequences found will be a new set of candidate sequences to obtain a new set of frequent sequences.

In most algorithms that use the apriori technique, the new generated candidate sequences have an extra item with respect to the sequences in the seed set. This does not happen in CAMLS [30] algorithm, because CAMLS grows by events (or transactions).

One of the main problems of these algorithms resides in the potential set of candidate sequences. This set may be a large set of candidate sequences even for a moderated seed set. For example, if there are 1000 frequent items, an algorithm that uses this technique will generate 1,499,500 candidate sequences. The computation of the amount of candidate sequences to be discovered is as follows: a 1000 . The second term is derived from ( a 1 a 2 ) , ( a 1 a 3 ) ,..., ( a 999 a 1000 ) .
Another problem is the multiple scans that must be performed in the database. This problem depends on the length of the sequential pattern. Since each candidate sequence grows by one at each database scan, if there exists a sequential pattern as ( mkh )( mh )( nkh )( mkn )( khn ) , this method (apriori) must scan the database at least 14 times. This gives the idea that if the amount of sequences in the database is very big, the cost of this operation will be excessive.

In order to avoid these problems, the algorithms that use this technique try to reduce the search space by executing pruning methods. In this section we will show some algorithms that use this technique, their characteristics and their improvements.

The algorithms selected to be described in this section are:  X  GSP, is the fi rst algorithm that introduces the use of generalization in sequential pattern mining.  X  SPADE, is selected because it presents some interes ting features such as ver tical database, lattices  X  CAMLS, is one of the latest developed algorithms which present growth by events in the candidate GSP (Generalized Sequential Patterns)
GSP is a typical algorithm of the sequential pattern mining fi eld and is based on generation-and-test technique. The major contribution of this algorithm to data mining is the generalization of the sequential patterns mining.

GSP includes the use of time constraints , sliding window and user-de fi ned taxonomies . The time constraints specify a time period between adjacent elements in the sequence. The sliding window generalization relaxes the de fi nition of sequence support. This is performed by allowing a set of transactions to contain an element of a sequence, as long as the difference in transaction-times between the transactions in the set is less than the user-speci fi ed window-size [27]. The taxonomy allows the user to include items from any levels. The relation between two nodes in the taxonomy is of type is-a .
The use of the generalizations has great importance in the sequential pattern mining. For example, let X  X  suppose we have a store and two regular customers. Customer1 buys product A one day and a few days later, he buys product B ; Costumer2 buys the same product A and a few months later, he buys product B . We can say that the sequence AB is frequent if the elapsed time between each action is not relevant. If the elapsed time is important, this sequence might not be frequent, depending on the de fi ned time constraints.

The steps of the GSP algorithm to obtain frequent sequences are the following. First, we obtain the set of all frequent items. In this step the algorithm scans the database to determine the support of each item (i.e. the number of sequences that include the item), and selects the frequent items. For example, let X  X  take the sequence database in Table 2. In this case, the set of frequent items for minsup = 2are { a =4 ,b =4 ,e =2 ,f =3 } .

GSP generates candidate sequences by joining sequences from the set of frequent sequences (seed set). A sequence  X  = ( ab ) c is joined with another sequence  X  = b ( cd ) if the sequences obtained by eliminating the fi rst item of  X  (item a ) and the last item of  X  (item d ) are equal (sequence bc ). The resulting sequence of this joining operation is ( ab )( cd ) .

Another important step in the apriori algorithms is the pruning. GSP prunes (deletes) the candidate sequences that have a subsequence that is not frequent. SPADE (Sequential PAttern Discovery using Equivalence classes) SPADE is an algorithm for discovering all frequent sequences and is based on the apriori technique. As opposed to other algorithms based on the same technique, SPADE discovers all sequences in three database scans.

One of the key features of the SPADE algorithm is the use of lattices to decompose the original problem (original lattice) into sub-problems, which can be independently processed in main memory. This small sub-problems or sub-lattices are called equivalence classes. The discovery of the frequent sequences in the use of a vertical id-list format.

In Table 4 we show the id-list associated to the frequent items of the database shown in Table 3. The way to obtain all frequent sequences and the support of the new sequence is the cardinal of the new generated id-list .

The search space in the SPADE algorithm is given to a lattice that has all possible sequences on the set of frequent items. The complete lattice for this example (Table 3) is shown in Fig. 1. Each element in the lattice (Fig. 1) has id-list associated.

The lattice can be divided into equivalence classes whe re each one is a sub-lattice a nd the equivalence relation collapses all sequences having a common length pre fi x (see Fig. 1). In this case the pre fi xto equivalence relation are frequent items ( a , b , c and d ).

A good feature of SPADE is that the algorithm only performs three scans on the database, thus partially solving one of the main problems of the apriori-based algorithms. It also uses equivalence classes which are useful for parallelization purposes. The use of vertical databases adds extra processing to the algorithm which is one problem to consider.
 CAMLS (Constraint-based Apriori algorithm for Mining Long Sequences)
CAMLS [30] is an algorithm based on apriori technique that allows mining long sequential patterns under constraints. The constraints used for this algorithm are intra-event and inter-event .
The key feature is the candidate sequence generation for events. Like all sequential pattern mining algorithms, CAMLS has two phases, the candidate generation and the pruning. The growth by events in each iteration is a new characteristic of the candidate generation phase that executes this algorithm. Let X  X  take two frequent sequences  X  =  X  1  X  2 ... X  n and  X  =  X  1  X  2 ... X  m with the same pre fi xof size  X  1 (see Section 2) from the previous iteration. Then the two sequences generated will be  X  with the last event of  X  (  X  1  X  2 ... X  n  X  m )and  X  with the last event of  X  (  X  1  X  2 ... X  m  X  n ). For example, the sequence  X  = ( ab ) c ( bd ) and  X  = ( ab ) cf produce two candidate sequence, ( ab ) c ( bd ) f and ( ab ) cf ( bd ) .

The prune phase is similar to other algorithms that use the same technique. A candidate sequence is deleted if there is a subsequence of size  X  1 that is not frequent. Also, CAMLS perform another checking: the candidate sequence can not have a subsequence of the same size that is not frequent.
One of the main advantages of CAMLS algorithm is the introduction of a novel candidate pruning strategy, which increases the ef fi ciency of the mining process reducing the size of the search space considerably. The growth of sequences by events in the candidate sequences generation process is another advantage of this algorithm.

The main disadvantage is the multi-scan of the database in order to count candidate sequences. This is a problem that bear most algorithms of this type. 3.2. Pattern growth technique
This technique emerges as a solution to the problem of candidate generation (apriori technique). The main idea in this technique is to eliminate the different problems that appear in the apriori algorithms with the candidate generation step, and to focus the search on portions of the initial database. The search space partition is an important characteristic in the pattern growth technique.

Each pattern growth algorithm proposes a way to partition the search space with the objective of generating less candidate sequences. This operation starts after representing the database that builds each algorithm for its execution. The main problems in the pattern growth algorithms appear in the way that patterns grow and in the projection of the database. In this section, we show some algorithms that use the pattern growth technique, their characteristics and their improvements.

The algorithms selected to be described in this section are:  X  Pre fi xSpan, is one of the most relevant algorithms within sequential pattern mining.  X  MEMISP, is an algorithm that implements a new idea: memory indexing.  X  LAPIN, is a recent algorith m that includes int eresting ideas such as main taining the la st positio n of  X  PRISM, is one of the latest algorithms that includes the use of the primal encoded idea. Pre fi xSpan (Pre fi x-projected Sequential pattern mining)
The Pre fi xSpan algorithm is based on the divide-and-conquer technique and it is an improvement of the algorithm FreeSpan [13]. FreeSpan uses frequent items to recursively project sequence databases into a set of smaller projected databases and grows subsequence fragments in each projected database. Pre fi xSpan only projects the suf fi xes of the sequences.

The general idea is that it examines the pre fi x subsequences and projects their corresponding suf fi x subsequences into projected databases. In each projected database, sequential patterns are grown by exploring the local frequent sequences.

The mining process of the Pre fi xSpan algorithm is as follows [14]: 1. Find frequent 1 -sequences (items): Scan the sequence database once to fi nd all frequent items 2. Divide the search space: The com plete set of sequentia l patterns is partiti oned according to the 3. Find subsets of sequential patterns: The subsets of sequential patterns can be mined by constructing
The pre fi x-projection of Pre fi xSpan reduces the size of the projected databases. Moreover, two kinds of database projection are explored. The fi rst one is the level-by-level projection, which can be used to reduce the projection costs when a projected database can fi t in the main memory. The second one is the bi-level projection, which has better performance when the database is large.

Pre fi xSpan is one of most representative algorithms for sequential pattern mining and the fi rst one to apply the pattern growth technique. The database projection method is the main characteristic of this algorithm and at the same time the biggest drawback, since the size of the projected database can grow very large depending on the characteristics of the sequences in the database.
 MEMISP (MEMory Indexing for Sequential Pattern mining)
MEMISP is an algorithm based on fi nd-then-index technique. The strategy of this algorithm is that each sequential pattern has a list of indexes to the sequences that contains it. Then, the frequent items in the sequences pointed by the pattern are obtained. Each new frequent item is joined with the previous sequential pattern, thus creating a new sequential pattern and a list of indexes associated to it. This algorithm is recursive.
 For example, in Fig. 2 is presented the a pattern with pointers to the sequences where it appears.
In Fig. 3 we can see that the frequent item e was detected, and was joined with a pattern, thus forming the new sequential pattern ae which has its own list of indexes.

In the case that the database cannot be loaded in memory, the algorithm applies the partition-and-validation technique. This technique splits the database into smaller databases that fi t in the memory. Then, the algorithm fi nds the sequential patterns in each database and join them in a single set.
The new method proposed in MEMISP for memory indexing and the fact that it reads only once the data sequences for loading them into memory, are the main highlights of this algorithm. Also, MEMISP outperforms both GSP and Pre fi xSpan in their original versions.
 LAPIN (LAst Position INduction)
LAPIN is an algorithm for mining sequential patterns which is based on the simple idea that the last position of an item,  X  , is the key to judge whether or not a frequent k -length sequential pattern can be extended to be frequent ( k +1) -length pattern by appending the item  X  to it [33].
 There is a series of algorithms that follow the previous strategy. Among them we have LAPIN SPAM [31], which uses a bitmap representation for the database. Other related algorithms are LAPIN LCI ( LCI-oriented ) [19] and LAPIN Suf fi x( Suf fi xoriented ) [19].

LAPIN obtains, in the fi rst step, a list with the last positions of the items in the sequences. Let X  X  take the database of the Table 5, the result of the fi rst scan over database, can be seen in Table 6.
Let X  X  suppose we have a frequent sequence k and its positions in all the sequences (s1:2, s3:1, s4:1 and s5:3). In order to grow the pattern we fi nd the frequent items ( 1 -sequence ) whose last position is bigger than the sequence k position.

Then, we select the frequent items (e.g. h ) with respect to the positions of the sequential pattern (e.g. k ). The pattern can be grown in these items and the method continues from this new pattern (e.g. k h ). The positions of the new sequential pattern kh are the positions of item h where h coincides with the sequence k . In sequence 1, the sequential pattern kh is located in position 4, and in sequence 5, it is located in position 5.

The simple idea of using the last position of an item for sequential pattern mining is the most important characteristic of LAPIN; this can largely reduce the search space during the mining process. However, if the sequences in the database have too many different items or the sequence size is too long, this algorithm is inef fi cient in memory usage.
 PRISM (PRIme-Encoding Based Sequence Mining)
PRISM is an algorithm that uses a vertical approach for enumeration and support counting, based on the novel notion of prime block encoding, which in turn is based on prime factorization theory [18].
The main feature of PRISM is the prime block encoding that is based on the fundamental theorem of arithmetic which states that any integer greater than 1 can be written as a unique product of prime numbers [11] (e.g. 3780 =2 2  X  3 3  X  5  X  7 ). sequence. For example, in the sequence database of Table 7, item a appears in sequence 1 in position 1, in sequence 2 in positions 3 and 6, in sequence 3 in position 1 and, in sequence 4 in positions 1 and 3. The bit-encoded position for this item is shown in the second column of Table 8.

Once we have the bit-encoded position o f each item, we can fi nd its prime-encoded position. We start with an ordered set of prime numbers which induces a lattice [17]. Each prime number will be encoded the other elements will be zero (0). When a new bit-encoded item is represented with this system, we take the prime numbers corresponding to the positions having value 1, and the prime-encoded position will be the multiplication of this corresponding prime num bers. For example, let X  X  take the fourth sequence represented in Table 8 (SID = 4), and the set of prime numbers { 2, 3, 5, 7 } . Since item a appears in the 1st and 3rd positions (see Table 8) of this sequence, the corresponding prime numbers are 2 and 5 (1st and 3rd elements in the prime numbers set). Then, the prime-encoded position for this item will be the multiplication of these two numbers, i.e, 2 * 5 = 10. This process is the same for the bit-encoded SID. The bit-encoded SID of the sequences for item a is shown in column one of Table 9.

There are two ways for growing the pattern, the itemset extension and the sequence extension. For both of them, the pattern growth is performed through the primal block join. This operation is based on the greatest common divisor between two numbers. The numbers represented to the prime-encoded of the sequence in join operation.

The advantages of this algorithm are the use of a vertical approach for enumeration and support counting, using a novel notion of primal block encoding, which is based on prime factorization theory; and the use of a bit-vector for sequence representation. As disadvantage we found that it can take a lot of processing time to convert sequences databases to bit-vectors which depends on the size of the longest sequence. 3.3. Sequential pattern mining algorithms comparison
This comparison is based in the analysis and descriptions presented in the previous sections. Table 10 summarizesthe main features of eachalgorithm studied for better understandingof their main differences. The algorithms are described in terms of the technique used (candidate generation (apriori) or pattern growth), constrains usage, whether they use bit vectors or not, whether they generate closed sequences or not, type of database used and their main contribution to the fi eld. The algorithms in the table appear in alphabetic order.
 4. Experimental results
The experiments were performed on synthetic databases using an Intel Celeron PC at 2 GHz and 2 GB of RAM. We used three databases for this process, which we named A, B and C. The fi rst database generated (A) consisted of 10 items and 10 000 sequences of size 20. The second database (B) had 20 items and 20 000 sequences of size 40, while the third database (C) had 50 items and 500 sequences of size 40.
 Next, several charts are presented with the results obtained from the experimental process for database A. For instance, in Fig. 4 we show a comparison among three algorithms regarding their execution time and the variations they present when the support is changed. Here we can observe that the GSP algorithm, which is based in the apriori technique, needs much more time than the other algorithms, due to the candidate sequences generation process and the counting of these sequences that is performed by all the algorithms using this technique. Figure 4(b) is a zoomed version of Fig. 4(a), showing in more detail the area where most of the values are concentrated.

We also took into account the virtual and physical memory usage. From this analysis we can see that GSP algorithm does not make signi fi cant changes in the amount of memory it uses (see Figs 5 and 6). For the case of LAPIN algorithm, it does not change much the virtual memory used (see Fig. 6), but the physical memory does suffer some variations (see Fig. 5). In Pre fi xSpan algorithm, the memory usage, in general shows larger changes (see Figs 5 and 6), a fact that could be due to the process of projecting the databases that correspond to the pre fi xes.

The same experiments were carried out for databases B and C, and the results can be seen in Figs 7 and 8. In the case of memory usage, the algorithms exhibited very similar behavior for the three databases. The LAPIN algorithm was not included in the experiments on database C, since the implementation used was not able to work in this database.
 In Tables 11, 12 and 13 we present a summary of the experimental process.

In order to achieve a bigger completion of this review, we present here the results obtained from [17, 18] in the comparison of SPADE and PRISM, which are both algorithms that use vertical databases. The chart presented in Fig. 9 shows a comparison in terms of minimum support vs. execution time. These experiments were run on a laptop with 2.4 GHz Intel Celeron processor, with 512 MB memory and using Linux as operating system.

According to the experimental results, we were able to notice that the pattern-growth-based algorithms perform better than the apriori-based ones.

In the charts presented in this section, we can observe the behavior of the algorithms according to their characteristics. For example, the LAPIN and Pre fi xSpan algorithms, which are based on the pattern growth technique, execute faster than GSP, which is based on the apriori approach. We can also notice that the LAPIN algorithm, albeit using the same technique employed by Pre fi xSpan, has a faster execution since it works with the last position of the sequence elements.

Contrary to what occurs with the execution speed of the algorithms, regarding the memory usage, GSP is more ef fi cient than Pre fi xSpan and LAPIN. This can be explained by the fact that, being of apriori type, it keeps on memory the candidate sequences set and the frequent sequences set obtained in each previous iteration, which makes the memory usage quite stable, just like LAPIN. On the other hand, since Pre fi xSpan performs database projection it uses more memory than the previous and it is dependent on the support to be used for the sequential pattern mining.

Concerning LAPIN, it is worth noticing that it does not work when the database has too many different elements and the sequential patterns are large with few element repetitions.

For the case of the PRISM and SPADE algorithms, we can say that they both work with vertical databases. PRISM has to convert the sequences to binary vectors and SPADE is of the apriori type, having to generate and check candidate sequences, only scanning the database three times. This last feature makes SPADE faster with respect to others of the same type. As we can observe they both compensate time, and this may be the reason why they exhibit similar execution times under speci fi c conditions. 5. Conclusions
In this paper we have presented a review of the state-of-the-art algorithms used for mining sequential patterns. For better understanding, the algorithms were split up into two categories taking into account the technique used (apriori and pattern growth). We carried out an experimental study which showed that, under the same prede fi ned conditions, the pattern-growth-based algorithms exhibit better performance than the apriori-based ones.

The main problems of the apriori-based algorithms are the candidate sequences generation and the multi-scan of the database in order to count candida te sequences. Therefore, we recommend that these algorithms should be use mainly in small databases or in databases containing short sequential patterns, for example, for Web logs analysis in small enterprises or to fi nd customers purchase patterns if the databases generated by this process ful fi ll the previous speci fi cations. Within this group of algorithms, CAMLS is the best choice in case of having databases with large sequential patterns,since in the candidate sequences generation step, the growth of sequences is performed by events, therefore, each generated candidate sequence grows faster (longer) in each iteration. In terms of reducing database scans, SPADE algorithm is a good option, since it discovers all sequences in three database scans. For this reason it would be suitable for large databases such as customer purchases and text mining. The main problem of this algorithm is the conversion of the original database into a vertical database.
 For the case of the algorithms based on pattern-growth that have been analyzed in this document, Pre fi xSpan is one of the most complete algorithms for sequential pattern mining although it has a drawback when the database has a large number of sequential patterns and the items are repeated frequently. The projected database constructed by the algorithm can duplicate the memory size with respect to original sequence database. In the latter case, we can use the LAPIN algorithm, since it executes very fast, but if the sequences in the databases have too many different items or the sequence size is too long, this algorithm is inef fi cient in memory usage. Therefore, it should not be used in applications like text mining. For this case we should use the PRISM algorithm instead, which works with binary codi fi cation, thus improving the use of memory.
 References
