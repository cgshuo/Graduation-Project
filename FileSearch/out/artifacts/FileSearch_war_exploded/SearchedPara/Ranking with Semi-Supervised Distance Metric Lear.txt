 This paper proposes a semi-supervised distance metric learn-ing algorithm for the ranking problem. Instead of giving the computer what are the important factors that affect the fi-nal rank value, we only give several most certainly ranked points which implicitly contain the knowledge of the rank-ing factors. Then the computer can automatically use the most certain points and plenty of unlabeded data to learn an informative metric for ranking. This metric not only can help to regress an order in the observed data, but also can be used to retrieve the data by querying new test points. Moreover, the lower-rank distance metric can be used to visualize high-dimensional data. We also present an appli-cation to the housing potential estimation problem. It is shown that the algorithm is efficient to help consultants to refine their consulting work.
 I.2.6 [ Artificial Intelligence ]: Learning; I.5.4 [ Pattern Recognition ]: Applications; H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing Algorithms, Experimentation Metric Learning, Dimensionality Reduction, Ranking, Infor-mation Retrieval, Semi-Supervised Learning.  X  This work was done when the first author visited IBM China Research Lab (CRL).
 Copyright 2007 ACM 978-1-59593-803-9/07/0011 ... $ 5.00.
Ranking is a common way to find an order of real world data. It is not only used in computer science society, but also used in other domains such as business and finance. Scoring the multi-variate examples by adding a weight to each feature is usually done in business consulting and anal-ysis. However, it is simple and intuitive, and the results may be not good enough. Another method to get the weights of features is to learn them from data which contain implicit domain knowledge.

Learningweightsoffeaturesisregardedasakindofdis-tance metric learning problem [9]. For ranking, learning an informative distance metric is helpful to reduce the compu-tational complexity and improve the ranking accuracy. For high-dimensional data, it is also helpful to visualize them in a low-dimensional coordinates system. Many problems, such as text and content based multimedia retrieval, need to learn a good distance metric.

In general, we should utilize some prior assumption or do-main knowledge to learn a distance metric. For example, for classification the domain knowledge is the class label. For ranking, we can not make the cluster assumption because there is no clear class boundary. Thus, other domain knowl-edge should be proposed to learn a more reasonable ranking metric. It is impossible to sort a large amount of data as an ordered sequence manually. Conversely, it is effortlessly for human to provide some examples as the most certainly  X  X ood X  and  X  X ad X  points, which can be seen as a kind of do-main knowledge. Based on the domain knowledge of most certain points, there are two jobs: (1) to obtain the order of the observed data points. (2) for new test point, to find what is its position in the whole sequence, or to find what are the most similar points in the already observed data set.
In this paper, we design an algorithm that can deal with this type of domain knowledge and can do the two types of ranking tasks. Both the human hints and the geometric information provided by observed data should be used. The essence of the proposed algorithm is a semi-supervised learn-ing method. However, it is different from the start point of traditional semi-supervised classification and clustering [11]. Generally, we do not make the assumption of existence of classes and do not want to find the classification boundaries. Instead, we only assume that the data point cloud can con-struct a graph which describes the manifold structure, and there are multiple concepts on different parts of the man-ifold. By maximizing the distance between different con-cepts and simultaneously preserving the local structure on the manifold, the learned metric can indeed give good rank-ing results. Besides giving the test results on the benchmark data, we also show an application to business consulting.
The intuitive start is to both preserve intrinsic geometry of the data point cloud and use the information provided by user. We consider a toy problem of  X  X  X  shape which is shown in Fig. 1 (a) and (b). (a) Ranking in Linear Space. (b) Ranking in Kernel Space. Figure 1: Toy example of metric learning for rank-ing. Red points (left solid points): labeled as  X  X est X ; blue points (right solid points): labeled as  X  X orst X . Blank points are unlabeled data, which reveal the manifold geometry.

In general, if no human knowledge is available, the com-puter does not know which point is  X  X ood X  and which point is  X  X ad X . The two coordinates in Fig. 1 may have the same weight. Contrarily, if we allow the user to provide the most certain points, it will lead to a more meaningful ranking re-sult. In Fig. 1 (a), we see that, labeling the points in the left as  X  X est X  and the points in the right as  X  X orst X , we may probably select x axis as the important feature. Note that the projection direction of the linear transformation may be similar to the one of Fisher. However, there are many un-labeled data which show that the  X  X  X  shape is a non-linear manifold. Ideally, the good ranking result should be along the manifold which is shown in Fig. 1 (b). Using only lin-ear transformation can not discover the intrinsic geometry. Thus, we need to embedded the data into a non-linear space. This can be solved by using the kernel trick [6, 8].
We denote the input points as D = { X , Y } .Thereare l points having been appointed by human as the most cer-tain points and u unlabeled points. Then, the observed in-put points can be written as X =( X L , X U ), where X L = ( x 1 , x 2 , ..., x l )and X U =( x l +1 , x l +2 , ..., x l x  X  R d is a d -dimensional vector. The goal of our lin-ear transformation algorithm is to find a projection matrix W  X  R d  X  m that transforms the original Euclidean space to a more informative space for ranking.

To maximize the distances between the projected values of the most certainly  X  X ood X  and  X  X ad X  points, we have the following objective function: where w j is the jth projection direction. The graph Lapla-cian L b is defined based on the graph which is constructed by the labeled data. Specifically it is L b = D b  X  A b ,where ( D ( A l is the number of the points labeled as the kth concept. Actually, D b is a zero matrix since the sum of rows of A are zeros.

To formulate the information provided by unlabeled data, we define G =( V, E ) as a weighted neighborhood graph to describe point cloud X . V is the vertex set of graph. E is the edge set which contains the pairs of neighboring vertices ( x i , x j ). A typical adjacency matrix A of neighborhood graph is: Then the normalized graph Laplacian of a neighborhood graph [2] is: where the diagonal matrix D satisfies D ii = d i ,and d i j =1 A ij is the degree of vertex
Based on the information provided by user and the ob-served data points, the learning work is to find a projection that can balance two terms. The first term is the force that makes the most certain points be mostly separated. The sec-ondtermisconsideredasaspringthatpreservestheintrinsic structure of the data point cloud. The two forces are also shown in Fig. 1. Generalizing to the multi-dimensionality case, we have the following objective function: where W =( w 1 , w 2 , ..., w c )  X  R d  X  m is the projection ma-trix. Then the objective function (5) has the solution: where w  X  j s ( j =1 , ..., m ) are the eigenvectors corresponding to the m largest eigenvalues  X  j s of ( XLX T )  X  1 X L L
We test the algorithms with the USPS digits image data set. 1 User submit a query example and the computer re-trieves and ranks the points in the observed data base. The retrieval accuracy is defined as: The original database contains 7291 training data and 2007 test data, and each data point is an image with 16  X  16 resolution. All data are  X 0 X - X 9 X  digit representations.
We test our algorithms with PCA [3], LDA [4], LPP [5] and the existent corresponding kernel versions. We ran-domly select 3000 data points as seen and 3000 data points unseen. Then seen data is randomly splitted into labeled and unlabeled data. For unsupervised methods PCA and LPP, we use the seen data to find the projection vectors. For supervised method LDA, we use the labeled data in the first d  X  1 dimensions in PCA sub-space as input features. http://www.kernel-machines.org For our semi-supervised distance metric learning algorithm (SSDML), we make use of the partially labeled seen data in the d  X  1 dimensions in PCA sub-space as input features. For kernelized version, we directly input the original vector of digit images. New distance metric is adopted to test the retrieval accuracy on the unseen set to find the first K near-est neighbors in the training set. The retrieved number K is set to 20. Results are shown in Fig. 2 (a) and (b). Each test accuracy is an average of 50 random trials. We see that SSDML is competitive with PCA and LPP, and significantly better than LDA. SSKDML (kernelized SSDML) even out-performs KPCA and KLDA. It is shown that the accuracy rate is near 90% when we only label 10 points in each class. We also vary the retrieved numbers to show the accuracy differences. We select retrieved numbers as 5, 20, 50 and 100 and the average accuracy rates of 50 random trials are shown in Fig. 2 (c) and (d). As expectation, the accuracy decreases when the retrieved number increases. (a) Different labeled numbers, linear. (c) Different retrieved num-bers, linear. Figure 2: Query Results: USPS data set, digits  X 0 X - X 9 X . Each test accuracy is an average of 50 random trials.
In this section, we present another application, which is the computer aided housing potential estimation and loca-tion recommendation system. As we know, the factors that can affect the housing value are numerous. Therefore, to rank the value of different housing location is difficult for human. The most usually adopted foregoing method for business consultants is simple: they multiply a weight to each feature and then combine them together. However, the results may be not sufficiently good. In this experiment, we show that our algorithm can efficiently solve this problem and it has been embedded in a real system for consultants X  work.

To estimate whether there is big value at a location for housing, consultants should investigate several factors around Figure 3: An example of map with housing locations and their impact factors. Factors around the hous-ing location within a million square meters should be considered as features for ranking. the housing location within a million square meters 2 .First, they count the commercial services sites such as shopping centers, banks, supermarkets, carnies and amusement parks and so on. Second, they count the social service sites such as hospitals, hotels, schools and colleges. Third, they evaluate traffics such as bus and subway stations, even the railway and air stations. Other sites such as restaurants and bars are also considered. After counting the units, they normalize them as probabilities. Moreover, environmental and social conditions around the housing location should also be eval-uated. Finally, we obtain a vector that contains 32 features to represent housing potential factors. The main features are shown in Fig. 3. Only 47 housing locations are plotted. Figure 4: An example of a small set (47 points). The areas under the ROC curve and convex hull of ROC curve are: AUROC=0.8303, AUROCCH=0.8744.

We first present a result that uses the 47 locations plot-ted in Fig. 3, since only these samples has manually ranking values. We appoints the first four most certainly  X  X ood X  lo-cations and last four most certainly  X  X ad X  locations as the labeled points. After running our algorithm SSKDML, a 2D visualization of these 47 points is plotted in Fig. 4 (a). Each location has an ID for distinguishing. We can see that the data is reduced to one dimension, since the second dimension is scaled to 10  X  11 . The ROC (receiver operating character-istic) curve and its convex hull are plotted in Fig. 4 (b). The
While different countries may have different factors, we only present a case study of a city in China. areas under the ROC curve and convex hull of ROC curve are: AUROC=0.8303, AUROCCH=0.8744. It is acceptable since even manually labeled rank value has mistakes. We also compare the proposed algorithm with some linear and non-linear metric learning algorithms: PCA, LDA, kernel LDA (KLDA), Laplace Eigenmaps (LapEigs) [1], LLE [7] an LTSA [10]. Table 4 shows that SSKDML gives the best result.

To show the efficiency of our algorithm, we also plot a vi-sualization results of 497 points with the same labeled data as the 47 points case. In Fig. 5 (f), we can see that the points is also reduced to one dimension for our algorithm SSKDML, since there are two coordinates scaled to 10  X  7 . The other linear and non-linear methods also give visual-ization results in Fig. 5 (a)-(e). However, they show less ranking information.
In this paper, we present a novel type of domain knowl-edge for ranking, where only most certain points are pro-vided by user. Having this domain knowledge, we can (1) give an order of the observed data points, (2) retrieve from an observed data base by querying new points. We pro-pose a distance metric learning algorithm to deal with the new domain knowledge and can be used both for regression and for retrieval by querying. Experiments show that the semi-supervised method can improve the retrieval accuracy.
Besides giving the test results on the benchmark set, we also show an application to computer aided housing poten-tial estimation and location recommendation problem. The implemented algorithm has been used in practice. The com-puter ranking results are satisfactory compared to the man-ually ranking value. It saved much time for consultants.
We would like to thank Feiping Nie and Shiming Xiang for their helpful comments and discussions. [1] M. Belkin and P. Niyogi. Laplacian eigenmaps for [2] F. Chung. Spectral Graph Theory . Number 92 in [3] R.O.Duda.,P.E.Hart.,andD.G.Stork. Pattern Figure 5: 3D visualization of 497 points. The gray level and size of the points indicate the final rank values. The rad circle and blue cross indicate the labeled points. [4] K. Fukunaga. Introduction to Statistical Pattern [5] X. He, S. Yan, Y. Hu, P. Niyogi, and H. Zhang. Face [6] K.-R. muller, S. Mika, G. R  X  atsch, K. Tsuda, and [7] S. T. Roweis and K. S. Lawrance. Nonlinear [8] B. Sch  X  olkopf, A. Smola, and K. Muller. Nonlinear [9] L. Yang and R. J. and. Distance metric learning: A [10] Z. Zhang and H. Zha. Principal manifolds and [11] X. Zhu. Semi-supervised learning literature survey.
