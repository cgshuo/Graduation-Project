 Takeshi Yamada yamada@cslab.kecl.ntt.co.jp Kazumi Saito saito@cslab.kecl.ntt.co.jp Naonori Ueda ueda@cslab.kecl.ntt.co.jp In many scientific and engineering domains, compli-cated relational data structures are frequently repre-sented by networks or, equivalently, graphs. For exam-ple, WWW (World Wide Web) sites are often repre-sented by hyperlink networks , with pages as nodes and hyperlinks between pages as edges, the interactions between genes, proteins, metabolites and other small molecules in an organism are represented by gene reg-ulatory networks , and the relationships between peo-ple and other social entities are characterized by social networks . This is because network representations of-ten provide important insights for researchers to un-derstand the intrinsic data structure with the help of some mathematical tools such as graph theory, as well as by examining an embedded layout in a low-dimensional Euclidean space.
 However, when the size of the network grows large and complicated, it becomes extremely difficult to obtain relevant embedding of networks. Therefore, develop-ing a network embedding algorithm that encourages researchers to make scientific discoveries about under-lying knowledge or principles from network data is a quite challenging and important task in the field of machine learning.
 One of the most fundamental methods to study a net-work and intuitively understand its inherent structures is browsing over a network layout embedded in a low-dimensional Euclidean space; to examine nodes man-ually one by one by following their connections and by comparing their connectivities with other nodes. Our goal is to develop an algorithm that embeds a network into a low-dimensional Euclidean space in a manner that is suitable for browsing.
 It is difficult to evaluate whether a given network lay-out is suitable for browsing or not. Aesthetically pleas-ing measures have been used in the literature, but they depend on subjective concepts. In this paper, we start from the following simple and basic principle: Principle A : connectivity preserving principle We propose an algorithm that fulfills this principle by only using connectivity information between nodes as a direct criterion, based on the cross-entropy directed energy function for minimizing. On the other hand, a large body of exiting work assumes pairwise distances between nodes. For example, the spring method pro-posed by Kamada and Kawai (1989) (hereafter re-ferred to as the KK spring method) first calculates graph-theoretic distances for each pair of nodes. The graph-theoretic distance can be calculated using the shortest path algorithm on a graph, such as the Floyd X  X  algorithm (Floyd, 1962). It then embeds nodes into the low-dimensional Euclidean space such that these graph-theoretic distances are most preserved. In other words, the KK spring method attempts to fulfill the following principle: Principle B : distance preserving principle It is clear that the complete fulfillment of principle B implies principle A , however the converse is not true. We will show that an attempt to fulfill the stronger principle B may often fail and end up with a biased embedding that also fails to fulfill principle A , and is not suitable for browsing, especially when the network size is large.
 When browsing a large and complex network, we often want to focus our attention on some restricted por-tion of the network by removing nodes and connec-tions that are out of our focus, and by re-optimizing that portion at full scale for more detailed browsing. When this re-optimization does not change the basic structure of the layout drastically, we describe that the ebmedding algorithm has good clipping stability . Removing a certain amount of nodes may change the graph-theoretic distances between the remaining nodes drastically, and therefore, completely change the em-bedding results produced by the KK spring method that is based on principle B . We will see that the pro-posed approach performs quite well in this sense. 2.1. Objective Function Consider a network (graph) with N nodes (vertices), where its adjacency matrix is denoted by A = ( a i,j ). In this paper, we focus on undirected graphs, i.e., a i,j  X  X  0 , 1 } , a i,i = 1, and a i,j = a j,i , but our approach can be easily extended to directed ones. For a given network, our objective is to compute a K -dimensional embedding of the N nodes such that principle A stated in Section 1 is fulfilled in terms of the K -dimensional Euclidean norm.
 Let x 1 , x 2 ,  X  X  X  , x N be the positions of the N nodes in a K dimensional space. As usual, the Euclidean distance between x i and x j is defined as follows: We now introduce a monotonic decreasing function  X  ( u )  X  [0 , 1] with respect to u  X  0, where  X  (0) = 1 and  X  (  X  ) = 0. Here  X  ( d i,j ) can be regarded as a con-tinuous similarity function between x i and x j . The basic idea is to consider a modified version of principle A using the concept of similarity; each node attempts to place other nodes such that the similarities between the node and its adjacent nodes are higher than those between the node and its non-adjacent nodes. This again can be reformulated in a slightly relaxed form in terms of a i,j and  X  ( d i,j ); each node attempts to place other nodes such that the continuous similarity func-tion  X  ( d i,j ) becomes the closest approximation of the discrete similarity measure a i,j as possible. To approximate a i,j by  X  ( d i,j ), a cross-entropy func-tion between a i,j and  X  ( d i,j ) is introduced as follows:
E i,j =  X  a i,j ln  X  ( d i,j )  X  (1  X  a i,j ) ln(1  X   X  ( d Equation ( 2 ) attains its minimum when  X  ( d i,j ) = a i,j with respect to x i and x j ; i.e., when the discrete sim-ilarity measure and the continuous measure become identical. Keeping the symmetric nature of E i,j in mind, we consider the following total energy function to be minimized with respect to x 1 ,  X  X  X  , x N : When node i is fixed, a i,j can be viewed as a binary class label of node j ; i.e., if a i,j = 1, then node j has label 1, and, because a i,i = 1, this means that node j belongs to the same class with node i . Otherwise, if a i,j = 0, then node j has label 0 and belongs to a dif-ferent class from node i . The problem can be viewed as a set of N binary classification tasks, and Equation ( 3 ) as a standard cost function of classification problems with parameters x 1 ,  X  X  X  , x N . Namely, the main char-acteristic of our approach is to solve the embedding problem the same way we would train a classification function.
 In this paper, we use  X  ( u ) = exp(  X  u/ 2) as the simi-larity function. Note that our approach is not limited to this particular type of  X  ( u ). The energy function corresponding to Equation ( 2 ) can be reformulated as follows: Finally, we define the following objective function with a regularization (weight-decay) term that controls the size of the resultant embeddings:
N  X  N adjacency matrix A = ( a i,j ), embedding di-mensionality K , and the convergence precision are given as inputs. 1. Set t = 1, and initialize points x 1 ,  X  X  X  , x N ran-2. Calculate gradient vectors J (1) x 3. Select x i such that i = arg max j {k J ( t ) x 5. Calculate modification vector  X  X  i . 6. Update gradient vectors J ( t +1) x 7. Update x i by x i = x i +  X  X  i . 8. Set t = t + 1 and go to Step 3 .
 where  X  is a predetermined constant. Hereafter, our method is referred to as the CE (Cross-Entropy) method. 2.2. Learning Algorithm The basic structure of our CE learning algorithm in-herits the fundamental idea of the KK spring algo-rithm. More specifically, instead of all the N points be-ing moved simultaneously, a point x i having the max-imum gradient vector norm is selected and is moved using the Newton-Raphson method, while the other points are fixed. This learning scheme, together with the objective function, can be interpreted as on-line learning of a classification problem using cross-entropy with a weight-decay term, which is well-known in the field of artificial neural networks.
 The CE algorithm can be summarized as shown in Fig-ure 1 . Here, J x function J with respect to x i calculated as follows: where the derivative of E i,j is Step 2 initially calculates the gradient vectors for all points by using Equation ( 6 ). Assuming that dimen-sionality K is much smaller than N , the computational complexity for this calculation is O ( N 2 ). However, when updating the gradient vectors in Step 6 , only the gradient vector for the selected point x i must be calculated from scratch, and those for the other points can be efficiently updated by only calculating the dif-ferences as follows: This is because of the following equalities: where h denotes an index such that h 6 = i,j . Thus, Step 6 can be performed in O ( N ).
 In Step 5 , according to the Newton-Raphson method,  X  X  i is calculated using the Hessian matrix H as fol-lows: Here, x 0 denotes a transposed vector of x . The KK spring method also uses Equation 10 . However, be-cause the Hessian matrix H is not always positive def-inite, either in the KK spring method whose energy function is described later in Equation 16 or in the CE all t . Therefore, in the CE method, if J increases after Equation 10 is applied, we undo it and instead resort to Equation 11 as follows: Note that since J ( t ) x decreases as long as a sufficiently small  X  is chosen and thus, the algorithm is guaranteed to converge. J can be updated in O ( N ) by only calculating the differences as follows: In our experiments, the regularization term added in Equation 10 is observed to encourage H being positive definite as much as possible, and resorting to Equa-tion 11 happens only occasionally. In such cases, each iteration of the above algorithm can be performed in approximately O ( N ). 3.1. Embedding Methods In our experiments, we compared our method with three representative conventional methods: the clas-sical Multidimensional Scaling (MDS) developed by Torgerson (1958), the spectral clustering method (Ng et al., 2002), and the KK spring method. In the fol-lowing, we briefly review each method.
 Let G = ( g i,j ) be a pairwise graph-theoretic distance matrix, and O = ( o i,j ) be a matrix defined by o i,j = i,j . Let X = [ x 1 ,  X  X  X  x N ] 0 be the N  X  K matrix of coordinates in a K dimensional Euclidean space. The classical MDS has the following objective function to be minimized: Here, Y denotes the N -dimensional Young-Householder transformation matrix.
 Let B = ( b i,j ) be an affinity matrix defined by b i,j = exp(  X  g i,j / 2) if i 6 = j and b i,i = 0. The spectral clus-tering method has the following objective function to be minimized: Here, D denotes a diagonal matrix whose ( i,i )-element is the sum of the i -th row of B . In this method, the final embedded points are obtained by re-normalizing each row of X to have unit length, i.e., The KK spring method has the following objective function to be minimized: 3.2. Experimental Data Three different types of networks assembled from real data are used to evaluate the proposed method. The first network data, denoted as E.Coli , is biology orig-inated. E.Coli is the gene regulatory network of the bacterium Escherichia Coli as described by Shen-Orr et al. (2002). The second data, denoted as NIPS , is human relation data generated by assembling a co-authorship relations in the conference papers appeared in NIPS (Neural Information Processing Systems) vol-umes 1 to 12, obtained from the web site of Roweis (2002), in which two persons who have at least one joint paper are directly linked. The third data, de-noted as NTT , is a WWW hyperlink network. NTT is generated by collecting all WWW pages that are lo-cated in NTT (Nippon Telegraph and Telephone Cor-poration) domain and have  X  www.ntt.co.jp  X  in com-mon in their URL (Universal Resource Locator). In our experiments, we first transform these networks into undirected ones, extract the maximally connected components, and then apply embedding methods to them. If we need to embed the whole network, then we can treat each connected component separately. Table 1 shows the statistics of the extracted connected networks. In the table, N denotes the number of nodes, and L ,  X  L and L  X  denote total, average and maximum number of links respectively. Let L i be the number of links connected with i . They are then cal-culated as follows:
L =  X  G and G  X  denote the average and maximum value of pairwise graph-theoretic distances in each graph re-spectively. Let g i,j be the graph-theoretic distances between nodes i and j . They are then calculated as follows:  X 
G = Here, we emphasize that these networks are all sparse in terms of the adjacency matrices, but have different statistics to some degree. C denotes total number of connected components in the original networks and N 2 denotes the number of nodes in the second largest component, which is much smaller than N . 3.3. Evaluation Measure In Section 1 , we claimed that network embedding should fulfill principle A and proposed an embedding algorithm based on cross-entropy and local connectiv-ities in Section 2 . In this section, we attempt to eval-uate the embedded results in a strictly quantitative fashion.
 Assume we have an embedding of a network with N nodes, with x 1 , x 2 ,  X  X  X  , x N as their corresponding em-bedded positions in a K -dimensional space. Consider a K -dimensional ball B i ( r i ) with its center x i and radius r . From principle A , it is expected that in an ideal embedding, each node i can maintain B i ( r i ) with an appropriately chosen radius r i such that it includes all the points corresponding to the adjacent nodes (i.e., nodes directly connected with i ) and excludes all the non-adjacent ones. In reality, however, especially when K is small, there may not exist such r i for each i . How-ever, it may be still possible to find an optimal r i in terms of some relevant criterion.
 In a sparse network, where there are many more non-adjacent nodes than adjacent ones, the standard ac-curacy measure that counts the number of correctly included points and correctly excluded points is not appropriate to determine the optimal radius. This is because high accuracy can be achieved even if it ex-cludes all the points regardless of their connectivities. Instead, we adopt the idea of the F -measure that is widely used in the field of information retrieval. The F -measure is defined as the weighted harmonic average of precision and recall . Let # X be the number of elements in set X . The precision P i ( r i ) for the i -th ball B i ( r i ) corresponding to x i and r i is defined as follows: while the recall R i ( r i ) is defined as: Roughly speaking, high precision favors a small r i and high recall favors a large r i ; the optimal r i should be found in between. We choose the optimal radius b r i for each i that maximizes the following F -measure with weight  X  . (  X  = 1 / 2 is used throughout our experi-ments.) The proposed measure, denoted as the connectivity F -measure, to evaluate an embedded network layout is defined as the average over all N points as follows: It may be arguable to straightforwardly apply our criterion to the embeddings produced by the other methods based on principle B , because their objective functions do not directly incorporate this F -measure. However, the complete fulfillment of principle B im-plies principle A . In fact, when principle B is fulfilled, F = 1 is achieved by setting the optimal radius b r i = 1 for each i . Therefore, our criterion can be thought of as a reasonable measure for evaluating any embedding method. 3.4. Comparisons Using the Connectivity We applied our method together with the three con-ventional methods, the classical MDS, the KK spring method, and the spectral clustering method, to the three different kinds of networks mentioned above, and obtained embedding results in K -dimensional spaces. We then quantitatively evaluated these results accord-ing to the proposed connectivity F -measure. In the experiments, we changed K from 2 to 25 for E.Coli and from 2 to 9 for NIPS and NTT respectively. Figure 2 summerizes the experimental results. The axis of ordinates shows the values of the connectivity F -measure and the axis of abscissas shows the dimen-sionalities of the embedded space. For the CE and the KK spring methods, the results of five trials with different random initializations are plotted to see if these methods suffer from the local optimality prob-lem. One can see that variances are very small for all networks, and therefore the local optimality problem is almost negligible in our experiments. The perfor-mances of all methods are monotonically improved as the dimensionality K , and thus the degree of freedom, increases.
 As expected, the performance of the CE method is bet-ter than those of the other methods, especially in the lower dimensions, where embeddings are more diffi-cult. In particular, the experiments using NTT , which is the largest in size and the most complicated, most impressively demonstrate that the CE method signif-icantly outperforms the others. Figure 2 only shows the F -measure, but more detailed analysis reveals that, in low dimensions, all the methods show high recall values, more or less 90%, but poor precision values, except for the CE method. 3.5. Two Dimensional Visualizations Each of the pictures on the left of Figure 3 shows a two-dimensional embedding result obtained by the classical MDS, the KK spring method, and the CE method for NTT , the WWW network data. The results of the spectral clustering method are not included here be-cause it embeds on the surface of a sphere, as described in Equation 15 .
 As discussed earlier, it would be difficult to evaluate these results in the sense of the aesthetically pleasing measure. Nevertheless, some characteristic features of each method can be observed below. The result of the classical MDS shown in Figure 3 (a) is problematic for visualization: many nodes are collapsed to a single point, which is not desirable for a browsing purpose. This seems to be a limitation of the linear projection methods.
 In contrast, the result of KK spring method shown in Figure 3 (b) and the result of the proposed CE method shown in Figure 3 (c) do not suffer from the node collapse problem. Looking at Figures 3 (b) and 3 (c) carefully, one can observe that in the former picture, many nodes tend to be arranged densely in semi-circles, which is characterized as a dandelion ef-fect (Buja et al., in press), while in the latter picture, nodes are laid more uniformly in a radial manner. The CE method appears to exploit the space more effi-ciently than the KK spring method. This agrees with the quantitative evaluation results using the connec-tivity F -measure, and also agrees with the fact that the KK spring method tries to preserve the pairwise graph-theoretic distances that take only discrete val-ues. Besides, it appears that it is harder for the former to follow links, especially in a relatively congested area, than for the latter.
 As mentioned earlier, we may want to look into some portions of the network in detail. For this purpose, the sub network shown in the dotted circle in the left of each picture in Figure 3 is clipped out by removing the nodes and connections outside this circle, and is re-embedded. Note that each clipped region corresponds to the same sub network. The re-embedded results are shown on the right of Figure 3 . It can be seen that the embedded network layouts before and after the clipping are quite different from each other in the case of the classical MDS and KK spring methods, although the layouts in the dotted circles shown on the left and right of each picture correspond to the same sub network. In contrast, the CE method is much more stable with the clipping than the classical MDS and the KK spring method, and thus has better clipping stability. We believe that the clipping stability is an important property for browsing nodes and discovering knowledge from given networks. Several other embedding methods based on the eigenvector analysis other than the classical MDS and the spectral clustering methods exist, including Isomap (Tenenbaum et al., 2000) and LLE (Roweis &amp; Saul, 2000). These methods assume high dimen-sional coordinates or at least pairwise distances be-tween nodes. In contrast, our method only assumes the adjacency matrix. Stochastic embedding, recently proposed by Hinton and Roweis (in press), also as-sumes pairwise distances.
 Eades (1984) first proposed a spring-directed graph embedding algorithm. In his algorithm, each pair of adjacent nodes is linked by a spring of length one, and each non-adjacent pair by a spring of infinite length. He claims that the linear spring that obeys Hooke X  X  law is too strong and that a logarithmic version should be used. Eades X  algorithm was the basis for the sub-sequent spring methods. The KK spring method has extended Eades X  idea and proposed an algorithm in which each pair of nodes is linked by a spring of length equal to the graph-theoretic distance between those nodes. This method is much more popular than its ancestor and is widely refered to as the spring method. The well-known state-of-the-art graph drawing pro-gram called  X  X EATO X  in the graph-viz software pack-age (North, 1992) includes an almost literal implemen-tation of the KK spring method.
 Many of the existing algorithms, including the classi-cal MDS and KK spring methods utilize the pairwise distances between nodes. Because the distance matrix is not sparse (even if the network is sparse), they need to deal with a huge non-sparse matrix, especially for a large network. In such cases, it is possible to modify the matrix sparse by thresholding large distances to infinity (or, equivalently, small affinities to zero), but still necessary to find an appropriate threshold. Our CE approach only deals with the sparse adjacency ma-trix when the network is sparse, and needs no such thresholding. Although we have been encouraged by our results to date, there remain a number of directions in which we must extend our approach before it can become a useful tool for scientific discovery. One promising di-rection might be to combine our method with existing discovery systems such as the IPM (Inductive Process Modeler) proposed by Langley et al. (2002). For in-stance, we can regard a process model as relational data, with processes as nodes and shared variables be-tween them as links. Our method can be directly in-corporated as an interface to support the browsing of large scale process models obtained by the IPM using empirically observed data.
 The WWW is a complex system that changes over time. It would be more difficult than any other dy-namic system, but still highly expected, to understand its inherent structures in scientific terms and to re-veal regularities in its complicated behavior. Thus, another direction would be to develop a discovery sys-tem for the WWW. To this end, we need to extend our method by incorporating a number of important characteristics of the Web networks, such as network motifs (Shen-Orr et al., 2002). As the first step, we are planing to evaluate our method by using a wider variety of the Web networks.
 In this paper, we have newly proposed the connec-tivity F -measure based on the connectivity preserving principle. However, it seems quite difficult to develop an algorithm for obtaining results by direct optimiza-tion on this measure due to its non-smoothness. On the other hand, our method based on the cross-entropy energy function works reasonably well on this measure, as shown in our experiments. From these results, we believe that the cross-entropy approach is suitable for preserving the principle. However, we need to perform further experiments to confirm this claim.
 Buja, A., Swayne, D. F., Littman, M., Dean, N., &amp;
Hofmann, H. (in press). Xgvis: Interactive data vi-sualization with multidimensional scaling. Journal of Computational and Graphical Statistics .
 Eades, P. (1984). A heuristic for graph drawing. Con-gressus Numerantium , 42 , 149 X 160.
 Floyd, R. W. (1962). Algorithm 97: shortest path. Communications of the ACM , 5 , 345.
 Hinton, G., &amp; Roweis, S. T. (in press). Stochastic neighbor embedding. Advances in Neural Informa-tion Processing Systems 15 . Cambridge, MA.: MIT Press.
 Kamada, T., &amp; Kawai, S. (1989). An algorithm for drawing general undirected graph. Information Pro-cessing Letters , 32 , 7 X 15.
 Langley, P., Sanchez, J., Todorovski, L., &amp; Dzeroski,
S. (2002). Inducing process models from continuous data. Proceedings of the Nineteenth International Conference on Machine Learning (pp. 347 X 354). San Francisco: Morgan Kaufmann.
 Ng, A., Jordan, M., &amp; Weiss, Y. (2002). On spectral clustering: Analysis and an algorithm. Advances in
Neural Information Processing Systems 14 . Cam-bridge, MA.: MIT Press.
 North, S. C. (1992). NEATO user X  X  guide . AT&amp;A Bell Laboratories.
 Roweis, S. T. (2002). Data for MATLAB hackers: NIPS conference papers Vols 0 X 12. http://www.cs.toronto.edu/  X roweis/data.html.
 Roweis, S. T., &amp; Saul, L. K. (2000). Nonlinear di-mensionality reduction by locally linear embedding. Science , 290 , 2323 X 2326.
 Shen-Orr, S. S., Milo, R., Mangan, S., &amp; Alon, U. (2002). Network motifs in the transcriptional regu-lation network of escherichia coli. Nature Genetics , 31(1) , 64 X 68.
 Tenenbaum, J. B., de Silva, V., &amp; Langford, J. C. (2000). global geometric framework for nonlinear dimensionality reduction. Science , 290 , 2319 X 2323. Torgerson, W. S. (1958). Theory and methods of scal-ing . New York: Wiley.
