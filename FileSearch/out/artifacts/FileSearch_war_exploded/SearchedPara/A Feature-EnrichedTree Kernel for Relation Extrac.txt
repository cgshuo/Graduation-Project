 Relation Extraction (RE) aims to identify a set of predefined relations between pairs of entities in text. In recent years, r elation extraction has re-ceived considerable research attention. An effec-tive technique is the tree kernel (Zelenko et al., 2003; Zhou et al., 2007; Zhang et al., 2006; Qian et al., 2008), which can exploit syntactic parse tree information for relation ext raction. Given a pair of entities in a sentence, the tree kernel -based RE method first represents the relation information between them using a proper sub -tree (e.g., SPT  X  the sub -tree enclosed by the shortest path linking the two involved entities). For example, the three syntactic tree representations in Figure 1 . Then the similarity between two trees are computed using a tree kernel, e.g., the convolution tree kernel pro-posed by Collins and Duffy (2001). Finally, new relation instances are extracted usi ng kernel based classifiers, e.g., the SVM classifier.

Unfortunately , one main shortcoming of the traditional tree kernel is that the syntactic tree rep-resentation usually cannot accurately capture the
Figure 1. The ambiguity of possess ive structure rela tion information between two entities. This is mainly due to the following two reasons: 1) The syntactic tree focuses on representing syntactic relation/structure , which is often too coarse or ambiguous to capture the semantic re-lation information . In a syntactic tree, each node indicates a clause/phrase/word and is only labeled with a Treebank tag ( Marcus et al., 1993 ). The Treebank tag, unfortunately, is usually too coarse or too general to capture semantic information. For example, all the three t rees in Figure 1 share the same possessive syntactic structure, but ex-press quite different semantic relations: where  X  Mary  X  s brothers  X  expresses PER -SOC Family relation,  X  Mary  X  s toy s  X  expresses Possession rela-tion, and  X  New York X  s airport s  X  expresses PHYS -Located relation. 2) Some critical information may lost during sub -tree representation extraction . For example, in Figure 2, when extracting SPT representation, all nodes outside the shortest -path will be pruned, such as the nodes [NN plants] and [POS  X  X ] in tree T1 . In this pruning process, the critical infor-mation  X  X ord town is the possessor of the posses-sive phrase the town X  X  plants  X  will be los t , which in turn will lead to the misclassification of the DISC relation between one and town .

This paper propose s a new tree kernel, referred as feature -enriched tree kernel (FTK) , which can effectively resolve the above problem s by enhanc-ing the traditional tree kernel in following ways: 1) We refine the syntactic tree representa-tion by annotating each tree no de with a set of dis-criminant features. These features are utilized to better capture the semantic relation information between two entities. For example, in order to dif-ferentiate the syntactic tree representation s in Fig-ure 1, FTK will annotate them with several fea-tures indicating  X  brother is a male sibling  X  ,  X  toy is an artifact  X  ,  X  N ew Y ork is a city  X  ,  X  airport is facility  X , etc. 2) Based on the refined syntactic tree repre-sentation, we propose a new tree kernel  X  fea ture -enriched tree kernel , which can bet ter measure the similarity between two trees by also taking all fea-tures into consideration.

We have experimented our method on the ACE 2004 RDC corpus. Experimental results show that our method can achieve a 5.4% F -measure im-provement over the traditional convolution tree kernel based method.

This paper is organized as follows. Section 2 describes the feature -enriched tree kernel . Section 3 presents the features we used . Section 4 dis-cusses t he experiment s . Section 5 briefly reviews t he related work. Finally Section 6 conclude s this paper . In this section, we describe the proposed feature -enriched tree kernel (FTK) for relation extraction . 2.1 Refining Syntactic Tree Representati on As described in above, syntactic tree is often too coarse or too ambiguous to represent the semantic relation information between two entities . To re-solve th is problem, we refine the syntactic tree representation by annotating each tree node with a set of discriminant features.
 Figure 3. Syntactic tree enriched with features
Specifically, for each node in a syntactic tree , we represent it as a tuple: where is its phrase label (i.e., its Treebank tag), and is a feature vector which indicates the characteristics of node , which is represented as: where f i is a feature and is associated with a weight teristics of relation instance, phrase properties and context information (See Section 3 for details).
For demonstration, Figure 3 shows t he feature -enriched version of tree T2 and tree T4 in Figure 2. We can see that, although T2 and T 4 share the same syntactic structure, the annotated features can still differentiate them. For example, the NP 5 node in tree T2 and the NP 5 node in tree T 4 are differentiated using their features Possessive-Phrase and PPPhrase , which indicate that NP 5 in T2 is a possessive phrase, meanwhile NP 5 in T 4 is a preposition phrase. 2.2 Feature -Enriched Tree Kernel This section describes how to take into account the anno tated features for a better tree similarity.
In Collins and Duffy X  X  convolution tree kernel (CTK), the similarity between two trees T 1 and T 2 is the number of their common sub -trees: Using this formula, CTK only considers whether two enumerated sub -trees have the identical syn-tactic structure (the indicator is 1 if the two sub -trees and have the identical syntac-tic structure and 0 otherwise). Such an assumption makes CTK can only capture the syntactic struc-ture similarity between two trees, while ignoring other useful information.
 riched t ree ke rnel (FTK) compute the similarity between two trees as the sum of the similarities between their common sub -trees: where is the similarity between enumer-ated sub -trees and , which is computed as: where is the same indicator function as in CTK; is a pair of aligned nodes between and , where and are correspondingly in the same position of tree and ; is the set of all aligned node pairs; is the feature vector similarity between node and , computed as the dot product between their feature vectors and .
 features, will be equal to . In this perspective, we can view as a similarity adjusted version of , i.e., only considers whether two nodes are equal, in contrast CTK, FTK can be efficiently computed as: where is the set of nodes in tree , and common sub -trees rooted at node and node , which is recursively computed as follows: 1) If the production rules of and are differ-2) If both and is pre -terminal nodes, 3) Calculate recursively as: This section presents the features we used to en-rich the syntactic tree representation. 3.1 Instance Feature Relation instances of the same type often share some common characteristics. In this paper, we add the following instance features to the root node of a sub -tree representation: ture indicates whether a relation instance has the following four syntactico -semantic structures in (C han &amp; Roth, 2011)  X  Premodifiers, Possessive, Preposition, Formulaic and Verbal . ments . Features about the entity information of arguments, including: a) #TP1 -#TP2 : the concat of the major entity types of arguments; b) #ST 1 -#ST2 : the concat of the sub entity types of argu-ments; c) #MT1 -#MT2 : the concat of the mention types of arguments. tures about the phrase path between two argu-ments and the phrases X  head before and after the arguments, w hich are the same as the phrase chunking features in (Zhou, et al., 2005). 3.2 Phrase Feature As discussed in above, the Treebank tag is too coarse to capture the property of a phrase node. Therefore, we enrich each phrase node with fea-tures about its lexical pattern, its content infor-mation, and its lexical semantics: pat tern of a phrase node using the following fea-tures: a) LP_Poss: A feature indicates the node is a possessive phrase; b) LP_PP : A feature indi-cates the n ode is a preposition phrase; c) LP _CC : A feature indicates the node is a conjunction phrase; d) LP_EndWithPUNC : A feature indicates the node ends with a punctuation; e) LP_EndWith-POSS : A feature indicates the node ends with a possessive word. property of a node  X  X  content using the following features: a) MB_#Num : The number of mentions contained in the phrase ; b) MB_C_#Type : A fea-ture indicates that the phrase contains a mention with major entity type #Type ; c) MW _#Num : The number of words within the phrase. terminal node, we capture its lexical semantic by add ing features indicating its WordNet sense in-formation. Specifically, the first WordNet sense of the terminal word , and all t his sense  X  X  hyponym sense s will be added a s features. For example, WordNet senses { New York#1, city#1, district#1, region#1, ... } will be added as features to the [NN New York] node in Figure 1. 3.3 Context Information Feature The context information of a phrase node is criti-cal for identifying the role and the importance of a sub -tree in the whole relation instance. This pa-per captures the following context information: the phrase node . As shown in Zhou et al. (2007), the context path from root to the phrase node is an effective context information feature. In this paper, we use the same settings in (Zhou et al., 2007), i.e., each phrase node is enriched with its context paths of length 1, 2, 3. observe d that a phrase X  X  relative position with the relation X  X  arguments is useful for identifying the role of the phrase node in the whole relation in-stance. To capture the relative position infor-mation, we define five possible relative posit ions between a phrase node and an argument, corre-sponding match , cover , within , overlap and other . Using these five relative positions, we capture the context information using the following features: dicates the rela tive position of a phrase node with argument 1 X  X  head phrase, where #RP is the rela-tive position (one of match , cover , within, overlap , other ), and #Arg1Type is the major entity type of argument 1. One example feature may be Match_Arg1Head_LOC . position with argument 2 X  X  head phrase; tive position with argument 1 X  X  extended phrase; tive position with argument 2 X  X  extended phrase. tures with an uniform weight , which is used to control the relative importance of the fea-ture in the final tree similarity: the larger the fea-ture weight, the more important the feature in the final tree similarity. 4.1 Experimental Setting To assess the feature -enriched tree kernel, we evaluate our method on the ACE RDC 2004 cor-pus using the same expe rimental settings as ( Qian et al., 2008 ). That is, we parse all sentences using the Charniak X  X  parser (Charniak, 2001), relation instances are generated by iterating over all pairs of entity mentions occurring in the same sentence. In our experiments, we i mplement the feature -en-chims, 1998) with the proposed tree kernel func-tion (Moschitti, 2004). We apply the one vs. oth-ers strategy for multiple classification using SVM. For SVM training, the parameter C is set to 2.4 for all experiments, and the tree kernel parameter  X  is tuned to 0.2 for FTK and 0.4 (the optimal param-eter setting used in Qian et al.(2008) ) for CTK. 4.2 Experimental Results 4.2.1 Overall p erformance We compare our method with the standard convo-lution tree kernel (CTK) on the state -of -the -art context sensitive shortest path -enclosed tree rep-resentation (CSPT, Zhou et al., 2007 ). We exper-iment our method with four different feature set-tings, correspondi ngly: 1) FTK with only instance features  X  FTK(instance) ; 2) FTK with only phrase features  X  FTK(phrase) ; 3) FTK with only context information features  X  FTK(context) ; and 4) FTK with all features  X  FTK . The overall per-formance of CTK and FTK is shown in T able 1, the F -measure improvements over CTK are also shown inside the parentheses . The detailed perfor-mance of FTK on the 7 major relation types of ACE 2004 is shown in Table 2.

Table 2. FTK on the 7 major relation types and From Table 1 and 2, we can see that: minant features and incorporating these features into the final tree similarity, FTK can significantly improve the relation extraction performance: compared with the convolution tree kernel base-line CTK , our method can achieve a 5.4% F -meas-ure improvement. formance of relation extraction: FTK can corre-spondingly get 2.6%, 2.2% and 4.9% F -measure improvements using instance features, phrase fea-tures and context information fea tures. information feature can achieve the highest F -measure improvement. We believe this may be-cause:  X  The context information is useful in providing clues for identifying the role and the im-portance of a sub -tree; and  X  The context -free as-sumption of CTK is too strong, some critical in-formation will lost in the CTK computation. varies significantly on different relation types: in Table 2 , most performance improvement gains from the EMP -ORG , PHYS, GPE -AFF and DISC relation types. We believe this may because th e discriminant features will better complement th e syntactic tree for capturing EMP -ORG, PHYS, GPE -AFF and DISC relation . On contrast the fea-tures may be redundant to the syntactic infor-mation for other relation types.
 Table 3. Comparison of different systems on the 4.2.2 Comparison with other systems Finally, Table 3 compares the performance of our method with several other systems. From Table 3, we can see that FTK can achieve competitive per-formance:  X  It achieves a 0.8% F -measure im-provement over the feature -based system of Jiang &amp; Zhai (2007);  X  It achieves a 0.5 % F -measure improvement over a state -of -the -art tree kernel : context sensitive CTK with CSPT of Zhou et al., (2007) ;  X  The F -measure of our system is slightly lower than the current best performance on ACE 2004 (Qian et al., 2008)  X  73.7 v s . 77.1 , we believe this is because the system of (Qian et al., 2008) adopts two extra techniques: composing tree ker-nel with a state -of -the -art feature -based kernel and using a more proper sub -tree representation. We believe these two techniques can also be used to further improve the performance of our system. This section briefly reviews the related work. A classic al technique for relation extraction is to model the task as a feature -based classification problem (Kambhatla, 2004; Zhou et al., 2005; Jiang &amp; Zhai, 2007; Chan &amp; Roth, 2010; Chan &amp; Roth, 2011), and feature engineering is obviously the key for performance improvement . As an al-ternative, tree kernel -based method implicitly de-fines features by directly measuring the similarity between two structures (Bunescu and Mooney, 2005; Bunescu and Mooney, 2006; Zelenko et al, 2003; Culotta and Sorensen, 2004; Zhang et al., 2006). Composite kernels were also be used (Zhao and Grishman, 2005; Zhang et al., 2006).

The main drawback of the current tree kernel is that the syntactic tree representation often cannot accurately capture the relation information. To re-solve this problem, Zhou et al. (2007) took the an-cestral information of sub -trees into consideration; Reichartz and Korte (2010) incorporated depend-ency type information into a tree kernel; Plank and Moschitti (2013) and Liu et al. (2013) embedded semantic information into tree kernel. Bloehdorn and Moschitti (2007a, 2007b) proposed Syntactic Semantic Tree Kernels (SSTK), which can cap-ture the semantic similarity between leaf nodes. Moschitti (2009) proposed a tree kernel which specify a kernel function over any pair of nodes between two trees, and it was further extended and applied in other tasks in ( Croce et al., 2011; Croce et al., 2012; Mehdad et al., 2010 ). This paper proposes a feature -enriched tree kernel , which can: 1) refine the syntactic tree representa-tion ; and 2) better measure the similarity between two trees. For future work, we want to develop a feature weighting algorithm which can accurately measu re the relevance of a feature to a relation in-stance for better RE performance .
 This work is supported by the National Natural Science Foundation of China under Grants no. 61100152 and 61272324 , and the Open Project of Beijing K ey Laboratory of Internet Culture and Digital Dissemination Research under Grants no. ICDD201204 .
