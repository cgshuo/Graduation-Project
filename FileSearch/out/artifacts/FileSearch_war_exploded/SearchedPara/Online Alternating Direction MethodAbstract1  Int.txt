 Arindam Banerjee BANERJEE @ CS . UMN . EDU In recent years, online learning (Zinkevich, 2003; Hazan et al., 2007) and its batch counterpart stochastic gradient descent (Juditsky et al., 2009) has contributed substantially to advances in large scale optimization techniques for ma-chine learning. Online convex optimization has been gen-eralized to handle time-varying and non-smooth convex functions (Duchi et al., 2010; Duchi &amp; Singer, 2009; Xiao, 2010). Distributed optimization, where the problem is di-vided into parts on which progress can be made in parallel, has also contributed to advances in large scale optimiza-tion (Boyd et al., 2010; Bertsekas &amp; Tsitsiklis, 1989; Cen-sor &amp; Zenios, 1998).
 Important advances have been made based on the above ideas in the recent literature. Composite objective mirror descent (COMID) (Duchi et al., 2010) generalizes mirror descent (Beck &amp; Teboulle, 2003) to the online setting. CO-MID also includes certain other proximal splitting methods such as FOBOS (Duchi &amp; Singer, 2009) as special cases. Regularized dual averaging (RDA) (Xiao, 2010) general-izes dual averaging (Nesterov, 2009) to online and com-posite optimization, and can be used for distributed opti-mization (Duchi et al., 2011).
 First introduced in (Gabay &amp; Mercier, 1976), the alternat-ing direction method (ADM) has become popular in recent years due to its ease of applicability and empirical perfor-mance in a wide variety of problems, including composite objectives (Boyd et al., 2010; Eckstein &amp; Bertsekas, 1992; Lin et al., 2009). The proof of convergence of ADM can be found in (Eckstein &amp; Bertsekas, 1992; Boyd et al., 2010), although the rate of convergence rate has not been estab-lished. For further understanding of ADM, we refer the readers to the comprehensive review by (Boyd et al., 2010). An advantage of ADM is that it can handle linear equality constraints of the form { x , z | Ax + Bz = c } , which makes distributed optimization by variable splitting in a batch set-ting straightforward (Boyd et al., 2010). However, in an online or stochastic gradient descent setting, one obtains a double-loop algorithm where the inner loop ADM itera-tions have to be run till convergence after every new data point or function is revealed. As a result, ADM has not yet been generalized to the online setting.
 In this paper, we consider optimization problems of the fol-lowing form: where the functions f t ,g are (non-smooth) convex func-R n 1  X  1 , z  X  Z  X  R n 2  X  1 , where X and Z are convex sets. In the sequel, we drop the convex sets X and Z for ease of exposition, noting that one can consider g and other addi-tive functions to be the indicators of suitable convex feasi-ble sets. The problem is studied both in the batch setting, where f t = f , and in the online setting for time-varying f We introduce a new proof technique for ADM in the batch setting, which establishes a O (1 /T ) convergence rate of ADM based on variational inequalities (Facchinei &amp; Pang, 2003). Further, the convergence analysis for the batch set-ting forms the basis of regret analysis in the online setting. We consider two scenarios in the online setting, based on whether or not the solution needs to lie in the feasible set in every iteration.
 We propose efficient online ADM (OADM) algorithms for both scenarios which make a single pass through the up-date equations and avoid a double loop algorithm. In the online setting, while a single pass through the ADM up-date equations is not guaranteed to satisfy the linear con-straints Ax t + Bz t = c , we consider two types of regret: regret in the objective as well as regret in constraint viola-tion . We establish both types of regret bounds for general and strongly convex functions. We also present preliminary experimental results illustrating the performance of the pro-posed OADM algorithms in comparison with FOBOS and RDA (Duchi &amp; Singer, 2009; Xiao, 2010).
 The key advantage of the OADM algorithms can be sum-marized as follows: Like COMID and RDA, OADM can solve online composite optimization problems, matching the regret bounds for existing methods. The ability to ad-ditionally handle linear equality constraints of the form Ax + Bz = c makes non-trivial variable splitting pos-sible yielding efficient distributed online optimization al-gorithms based on OADM. Further, the notion of regret in both the objective as well as constraints may contribute towards development of suitable analysis tools for online constrained optimization problems (Mannor &amp; Tsitsiklis, 2006; Mahdavi et al., 2011).
 The rest of the paper is organized as follows. In Section 2, we analyze batch ADM and establish its convergence rate. In Section 3, we introduce the online optimization problem with linear constraints. The OADM algorithm is also given in Section 3. In Sections 4 and 5, we present the regret analysis in two different scenarios based on the constraints. We discuss connections to related work in Sec-tion 6, present preliminary experimental results in Section 7, and conclude in Section 8. We consider the batch ADM problem (1) where f t is fixed. The augmented Lagrangian for (1) is
L ( x , y , z ) = f ( x )+ g ( z )+  X  y , Ax + Bz  X  c  X  + where z is the primal variable and y is the dual variable,  X  &gt; 0 is the penalty parameter. Batch ADM executes the following three steps iteratively till convergence (Boyd et al., 2010): x =argmin z =argmin y At step ( t + 1) , the equality constraint is not necessar-ily satisfied in ADM. However, one can show that the lim t  X  X  X  Ax t + Bz t  X  c  X  0 . We first analyze the con-vergence of objective and constraint separately using a new proof technique, which plays an important role for the re-gret analysis in the online setting. Then, a joint analysis of the objective and constraint using a variational inequal-ity (Facchinei &amp; Pang, 2003) establishes the O (1 /T ) con-vergence rate for ADM.
 Without loss of generality, we assume that z 0 = 0 , y 0 = 0 . eigenvalue of B T B . 2.1. Bounds for Objective and Constraints The following theorem shows that both the cumulative objective difference w.r.t. the optimal and the cumulative norms of the constraints, known as the primal and dual residuals (Boyd et al., 2010), are bounded by constants in-dependent of the number of iterations T .
 Theorem 1 Let the sequences { x t , z t , y t } be generated by ADM. For any x  X  , z  X  satisfying Ax  X  + Bz  X  = c , for any T , we have
X
X mented lagrangian (2) hold if (7) holds. The convergence of equality constraint and primal residual implies the con-vergence of ADM. A result similar to (7) has been shown in (Boyd et al., 2010), but our proof is different and self-contained along with (6). Although (6) shows that the ob-jective value converges to the optimal value, x t +1 need not be feasible and the equality constraint is not nec-essarily satisfied. 2.2. Rate of Convergence of ADM We now prove the O (1 /T ) convergence rate for ADM us-ing a variational inequality (VI) based on the Lagrangian given in (2). Let  X  = X  X  Z  X  R m . Any w  X  = ( x  X  , z  X  , y  X  )  X   X  solves the original problem in (1) op-timally if it satisfies the following variational inequal-ity (Facchinei &amp; Pang, 2003; Nemirovski, 2004):  X  w  X   X  , h ( w )  X  h ( w  X  ) + ( w  X  w  X  ) T F ( w  X  )  X  0 , (8) where F ( w ) T = [ y T A y T B  X  ( Ax + Bz  X  c ) T ] is the gradient of the last term of the Lagrangian, and h ( w ) = problem with accuracy if it satisfies We show that after T iterations, the average  X  w T isfies the above inequality with = O (1 /T ) .
 Theorem 2 Let  X  w T = 1 T P T t =1 w t , where w t = ( x t , z t , y t ) from (3)-(5). Then,  X  w  X   X  , h (  X  w T )  X  h ( w )+(  X  w T  X  w ) T F (  X  w In this section, we extend the ADM to the online learn-ing setting. Specifically, we focus on using online ADM (OADM) to solve the problem in (1). For our analysis, A and B are assumed to be fixed. At round t , we consider solving the following regularized optimization problem: where  X   X  0 is a learning rate and Bregman divergence B in every step, standard analysis techniques (Hazan et al., 2007) can be suitably adopted to obtain sublinear regret bounds. While (10) can be solved by batch ADM, we es-sentially obtain a double loop algorithm where the function f changes in the outer loop and the inner loop runs ADM iteratively till convergence so that the constraints are satis-fied. Note that existing online methods, such as projected gradient descent and variants (Hazan et al., 2007; Duchi et al., 2010) do assume a black-box approach for projecting onto the feasible set, which for linear constraints may re-quire iterative cyclic projections (Censor &amp; Zenios, 1998). For our analysis, instead of requiring the equality con-straints to be satisfied at each time t , we only require the equality constraints to be satisfied in the long run, with a notion of regret associated with constraints. In particular, we consider the following online learning problem: min s.t.
 so that the cumulative constraint violation is sublinear in T . The augmented lagrangian function of (10) at time t is L ( x , y , z ) = f t ( x )+ g ( z )+  X  y , Ax + Bz  X  c  X  +  X B At time t , our algorithm consists of just one pass through the following three update steps: The x -update (13) has two penalty terms: a quadratic term and a Bregman divergence. If the Bregman divergence is not a quadratic function, it may be difficult to solve x ef-ficiently. A common way is to linearize the objective such that x (16) is known as inexact ADM (Boyd et al., 2010) if  X  is a quadratic function. In the sequel, we focus on the algorithm using (13).
 Operationally, in round t , the algorithm presents a solution { x t , z t } as well as y t . Then, nature reveals function f we encounter two types of losses. The first type is the tradi-the residual of constraint violation, i.e., k Ax t + Bz t The goal is to establish sublinear regret bounds for both the objective and the constraint violation, which we do in Sec-tion 4. We consider another scenario, where in round t , we use a solution {  X  x t , z t } based on z t such that A  X  x + Bz to establish sublinear regret of the objective f t (  X  x t as well as the constraint violation for the true ( x t , z the second scenario, we use  X  = 0 in (13) and present the results in Section 5. As the updates include the primal and dual variables, in line with batch ADM, we use a stronger both primal and dual residuals, where Before getting into the regret analysis, we discuss some ex-ample problems which can be solved using OADM. Like FOBOS and RDA, OADM can deal with machine learn-ing methods where f t is a loss function and g is a regu-larizer, e.g. ` 1 or mixed norm, or an indicator function of a convex set. Examples include generalized lasso and group lasso (Boyd et al., 2010; Tibshirani, 1996; Xiao, 2010). OADM can also solve linear programs, e.g. MAP LP relaxation (Meshi &amp; Globerson, 2011) and LP decod-ing (Barman et al., 2012), and non-smooth optimization, e.g. robust PCA (Lin et al., 2009) where f t is nuclear norm and g is ` 1 norm. Another promising scenario for OADM is consensus optimization (Boyd et al., 2010) where dis-tributed local variables are updated separately and reach a global consensus in the long run. More examples can be found in (Boyd et al., 2010).
 In the sequel, we need the following assumptions: (1) The norm of subgradient of f t ( x ) is bounded by G f (2) We assume g ( z 0 ) = 0 and g ( z )  X  0 . (3) x 0 = 0 , y 0 = 0 , z 0 = 0 . For any x  X  , z  X  satisfying Ax  X  + Bz  X  = c , B  X  ( x  X  , 0 ) = D 2 x , k z  X  k 2 = D (4) For any t , f t ( x t +1 )+ g ( z t +1 )  X  ( f t ( z  X  which is true if the functions are lower bounded or Lips-chitz continuous in the convex set (Mahdavi et al., 2011). As discussed in Section 3, we consider two types of regret in OADM. The first type is the regret of the objective based on variable splitting, i.e.,
R ( T )= Aside from using splitting variables, R 1 is the standard re-gret in the online learning setting. The second is the regret of the constraint violation R c defined in (17). 4.1. General Convex Functions The following establishes the regret bounds for OADM. Theorem 3 Let the sequences { x t , z t , y t } be generated by OADM and assumptions (1)-(4) hold. For any x  X  , z  X  satis-fying Ax  X  + Bz  X  = c , setting  X  = G f we have Note the bounds are achieved without any explicit assump-tions on A , B , c . 1 The subgradient of f t is required to be bounded, but the subgradient of g is not necessarily bounded. Thus, the bounds hold for the case that g is an in-dicator function of a convex set. In addition to the O ( regret bound, OADM achieves the O ( the-art online learning algorithms (Duchi et al., 2010; Duchi &amp; Singer, 2009; Xiao, 2010), since they do not ex-plicitly handle linear constraints of the form Ax t + Bz = c . The bound for R c could be reduced to a constant if addi-tional assumptions on B and the subgradient of g are satis-fied. 4.2. Strongly Convex Functions We assume both f t ( x ) and g are strongly convex. Specifi-cally, we assume f t ( x ) is  X  1 -strongly convex with respect to a differentiable function  X  , i.e., f ( x  X  )  X  f t ( x )+  X  f 0 t ( x ) , x  X   X  x  X  +  X  1 B  X  ( x where  X  1 &gt; 0 , and g is a  X  2 -strongly convex function, i.e., g ( z  X  )  X  g ( z )+  X  g 0 ( z ) , z  X   X  z  X  + where  X  2 &gt; 0 . Then, logarithmic regret bounds can be established.
 Theorem 4 Let assumptions (1)-(4) hold. Assume f t ( x ) and g are strongly convex given in (19) and (20). For any x , z  X  satisfying Ax  X  + Bz  X  = c , setting  X  t =  X  1 t, X  t  X  t/ X  B max , we have R 1 ( T )  X  G 2 f log ( T + 1) / (2  X  X  1 ) +  X  2 D 2 z / 2 +  X  R ( T )  X  2 F X  B max log( T + 1) / X  2 +  X  B max D 2 z +2  X  1 To guarantee logarithmic regret bounds for both objec-tive and constraints, OADM requires both f t and g to be strongly convex. FOBOS, COMID, and RDA only require g to be strongly convex although they do not consider linear constraints explicitly. We analyze the regret bound when  X  = 0 . In this case, OADM has the same updates as ADM. For the analysis, we consider z t to be the key primal variable, and compute  X  x using z t so that A  X  x t + Bz t = c . Since (  X  x t , z t the constraints by design, we consider the following regret:
R ( T ) = where A  X  x t + Bz t = c . A common case we often encounter is when A = I , B =  X  I , c = 0 , thus  X  x t = z t . While {  X  x satisfy Ax t + Bz t  X  c = 0 . Thus, in addition to R 2 ( T ) , we also consider bounds for R c as defined in (17). To guarantee that A  X  x t + Bz t = c , A  X  R m  X  n 1 is feasible, it implicitly requires the assumption m  X  n 1 . On the other hand, to establish a bound for R 2 , A should be full-column rank, i.e., rank ( A ) = n 1 . Therefore, we assume that A is a square and full rank matrix, i.e., A is invertible. Let  X  be the smallest eigenvalue of AA T , then  X  A min &gt; 0 . 5.1. General Convex Functions The following theorem shows the regret bounds.
 Theorem 5 Let  X  = 0 in OADM and assumptions (1)-(4) and A is invertible hold. For any x  X  , z  X  satisfying Ax Bz  X  = c , setting  X  = G f Without requiring an additional Bregman divergence, R 2 achieves the which may not stay in the feasible set, R 2 is defined on  X  x t which always satisfies the equality constraint. The cor-responding algorithm requires finding  X  x t in each iteration such that A  X  x t = c  X  Bz t , which involves solving a linear system. The algorithm will be efficient in some settings, e.g., consensus optimization where A = I . 5.2. Strongly Convex Functions The following theorem establishes the logarithmic regret bound under the assumption g is  X  -strongly convex given in (20).
 Theorem 6 Let  X  = 0 in OADM. Assume that g ( z ) is  X  2 -strongly convex, A is invertible, and assumptions (1)-(4) hold. Setting  X  t =  X  2 t/ X  B max , we have Unlike Theorem 4, Theorem 6 shows that OADM can achieve the logarithmic regret bound without requiring f t to be strongly convex, which is in line with other online learning algorithms for composite objectives. In this section, we assume  X  = 0 , A = I , B =  X  I , c = 0 , thus x = z . The three steps of OADM reduce to x z y optimality conditions for (24) and (25) give Adding them together yields OADM can be considered as taking the implicit subgradi-ent of f t and g at the yet to be determined x t +1 and z FOBOS has the following update (Duchi &amp; Singer, 2009): FOBOS takes the explicit subgradient of f t at current z t As a matter of fact, FOBOS can be considered as an inexact OADM, which linearizes the objective of (24) at z t : It has the following closed-form solution: (25) is equivalent to the following scaled form : Let  X  =  X  and z t + 1 &amp; Singer, 2009). Furthermore, if g ( z ) is an indicator func-tion of a convex set  X  , substituting (28) into (29), we have We recover the projected gradient descent (Hazan et al., 2007). In this section, we use OADM to solve the generalized lasso problems (Boyd et al., 2010), including lasso (Tib-shirani, 1996) and total variation (TV)(Rudin et al., 1992). We present simulation results to show the convergence of objective as well as constraints in OADM. We also com-pare it with batch ADM and other two online learning al-gorithms: FOBOS and regularized dual averaging (RDA) in selecting sparse dimension in lasso and recovering data in total variation. 7.1. Generalized Lasso The generalized lasso problem is formulated as follows: scalar. If D = I , (30) yields the lasso. If D is an up-per bidiagonal matrix with diagonal 1 and off-diagonal  X  1 , (30) becomes the total variation. The ADM form of (30) is: where z  X  R m  X  1 . The three updates of OADM are: where u = y / X  , v = a T t b t +  X b t D T ( z  X  u ) +  X  x , and S  X / X  denotes the shrinkage operation.
 For lasso, the x -update is For total variation, we set  X  = 0 so that where Q = ( D T D )  X  1 .
 In both cases, the three updates (32)-(34) can be done in O ( n ) flops (Golub &amp; Loan, 1996). In contrast, in batch ADM, the complexity of x -update could be as high as 2010). Here, we do not run them in parallel.
 FOBOS and RDA cannot directly solve the TV term. We first reformulate the total variation in the lasso form such that where y = Dx . FOBOS and RDA can solve the above lasso problem and get y . x can be recovered by using x = D 7.2. Simulation Our experiments follow the lasso and total variation exam-ples in Boyd X  X  website, 2 although we modified the codes to accommodate our setup. We first randomly generated A with N examples of dimensionality n . A is then nor-malized along the column. Then, a true x 0 is randomly generated with certain sparsity pattern for lasso and TV. b is calculated by adding gaussian noise to Ax 0 /N . In all experiments, N = 100 , which facilitates the matrix inverse in ADM and will be gone through cyclically in the three on-line learning algorithms. For lasso, we keep the number of nonzeros (NNZs) k = 100 in x and try different combina-tion of parameters from n = [1000 , 5000] ,  X  = [0 . 1 , 1 , 10] and q = [0 . 1 , 0 . 5] for  X  = q  X | A T b/N |  X  . All experiments are implemented in Matlab.
 Convergence : We go through the examples 100 times us-ing OADM. Figure 1(a) shows that NNZs converge to some value close to the actual k = 100 before t = 2000 . Fig-ure 1(b) shows the convergence of objective value. In Fig-ure 1(c), the dashed lines are the stopping criteria used in ADM (Boyd et al., 2010). It shows that the equality con-the online setting. While the objective converges fast, the equality constraints relatively take more time to be satis-fied.
 Sparsity: We compare NNZs found by batch ADM and three online learning algorithms, including OADM, FO-BOS, and RDA. We set  X  = 1000 for OADM and  X  = 1 for RDA. For FOBOS, we use a time varying parameter  X  t =  X /  X  examples 100 times. We run the experiment 20 times and the average results are plotted. Due to the limited space, we only show the results for N = 100 ,n = 1000 ,q = 0 . 5 in Fig. 2. While ADM and RDA tend to give the sparsest results, OADM seems more conservative and converges to reasonably sparse solutions. Fig.2 shows OADM is closest to the actual NNZs 100. The NNZs in FOBOS is large and oscillates in a big range, which has also been observed in (Xiao, 2010). Total Variation: We compare the patterns found by the four algorithms. For all algorithms, N = 100 ,n = 1000 , X  = 0 . 001 and  X  is chosen through cross valida-tion. In RDA,  X  = 100 . Recall that  X  = 0 in OADM. While we use a fixed  X  for OADM and RDA, FOBOS uses  X  =  X / results found by the algorithms. ADM seems to follow the pattern with obvious oscillation. OADM is smoother and generally follows the trend of the patterns. For the first two examples, FOBOS works well and the patterns found by RDA tend to be flat. In the last example, both FOBOS and RDA oscillate. In this paper, we propose an efficient online learning algo-rithm named online ADM (OADM). New proof techniques have been developed to analyze the convergence of ADM, which shows that ADM has a O (1 /T ) convergence rate. Using the proof technique, we establish the regret bounds for the objective and constraint violation for general and strongly convex functions in OADM. Finally, we illustrate the efficacy of OADM in solving lasso and total variation. The research was supported by NSF CAREER award IIS-0953274, and NSF grants IIS-0916750, IIS-0812183, and IIS-1029711.
 Barman, S., Liu, X., Draper, S., and Recht, B. Decom-position methods for large scale LP decoding. In Arxiv , 2012.
 Beck, A. and Teboulle, M. Mirror descent and nonlinear projected subgradient methods for convex optimization. Operations Research Letters , 31:167 X 175, 2003.
 Bertsekas, D. P. and Tsitsiklis, J. N. Parallel and Dis-tributed Computation: Numerical Methods . Prentice Hall, 1989.
 Boyd, S., N. Parikh, E. Chu, Peleato, B., and Eckstein, J.
Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundation Trends Machine Learning , 3(1):1 X 122, 2010.
 Censor, Y. and Zenios, S. Parallel Optimization: Theory,
Algorithms, and Applications . Oxford University Press, 1998.
 Duchi, J. and Singer, Y. Efficient online and batch learning using forward backward splitting. JMLR , 10:2873 X 2898, 2009.
 Duchi, J., Shalev-Shwartz, S., Singer, Y., and Tewari, A. Composite objective mirror descent. In COLT , 2010. Duchi, J., Agarwal, A., and Wainwright, M. Dual averaging for distributed optimization: Convergence analysis and network scaling. arXiv , 2011.
 Eckstein, J. and Bertsekas, D.P. On the douglas-rachford splitting method and the proximal point algorithm for maximal monotone operators. Mathematical Program-ming , 55:293 X 318, 1992.
 Facchinei, F. and Pang, J.-S. Finite-Dimensional Varia-tional Inequalities and Complementarity Problems , vol-ume I. Springer, 2003.
 Gabay, D. and Mercier, B. A dual algorithm for the solu-tion of nonlinear variational problems via finite-element approximations. Computers and Mathematics with Ap-plications , 2:17 X 40, 1976.
 Golub, G. H. and Loan, C. V. Matrix Computations,3rd ed. Johns Hopkins University Press, 1996.
 Hazan, E., Agarwal, A., and Kale, S. Logarithmic re-gret algorithms for online convex optimization. Machine Learning , 69(2-3):169 X 192, 2007.
 Juditsky, A., Lan, G., Nemirovski, A., and Shapiro, A.
Stochastic approximation approach to stochastic pro-gramming. SIAM J. Optim. , 19:1574 X 1609, 2009.
 Lin, Z., Chen, M., Wu, L., and Ma, Y. The augmented la-grange multiplier method for exact recovery of corrupted low-rank matrices. UIUC Technical Report UILU-ENG-09-2215 , 2009.
 Mahdavi, M., Jin, R., and Yang, T. Trading regret for effi-ciency: Online convex optimization with long term con-straints. Arxiv , 2011.
 Mannor, S. and Tsitsiklis, J. N. Online learning with con-straints. In COLT , pp. 529 X 543, 2006.
 Meshi, O. and Globerson, A. An alternating direction method for dual MAP LP relaxation. In ECML11 , 2011. Nemirovski, A. Prox-method with rate of convergence
O (1 /t ) for variational inequalities with lipschitz contin-uous monotone operators and smooth convex-concave saddle point problems. SIAM J. Optim. , 15:229 X 251, 2004.
 Nesterov, Y. Primal-dual subgradient methods for convex problems. Mathematical Programming , 120:221 X 259, 2009.
 Rudin, L., Osher, S. J., and Fatemi, E. Nonlinear total vari-ation based noise removal algorithms. Physica D , 60: 259 X 268, 1992.
 Tibshirani, R. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society, Series B , 58:267 X 288, 1996.
 Xiao, L. Dual averaging methods for regularized stochastic learning and online optimization. JMLR , 11:2543 X 2596, 2010.
 Zinkevich, M. Online convex programming and general-ized infinitesimal gradient ascent. In ICML , pp. 928 X  936, 2003.
 Proof: We start by noting that the VI corresponding to the update of x t +1 in (3) is given by:  X  x  X  X  f ( x )  X  f ( x t +1 )+  X  x  X  x t +1 , A T { y t +  X  ( Ax Using (5),  X  x  X  X  The VI corresponding to the update of z t +1 in (4) is given by:  X  z  X  X  , g ( z )  X  g ( z t +1 )+  X  z  X  z t +1 , B T { y t +  X  ( Ax Using (5),  X  x  X  X  Adding (36) and (37) and denoting h ( w ) = f ( x ) + g ( z ) , we have  X  w  X   X  h ( w t +1 )  X  h ( w )+  X  w t +1  X  w ,F ( w t +1 )  X  (38)  X   X   X  Ax  X  Ax t +1 , Bz t  X  Bz t +1  X  + The first term can be rewritten as 2  X  Ax  X  Ax t +1 , Bz t  X  Bz t +1  X  (39) The second term in (38) is equivalent to Substituting (39) and (40) into (38) and summing over t , tion of  X  w . Dividing both sides of (41) by T , recalling that  X  w h (  X  w T )  X  h ( w ) +  X   X  w T  X  w ,F (  X  w T )  X 
