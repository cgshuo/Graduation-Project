 Pratik Jawanpuria PRATIK . J @ CSE . IITB . AC . IN Microsoft Research India, Bangalore, India 560 001 Feature selection is an important problem in machine learn-ing motivated by considerations of elimination of noisy, expensive and redundant features, model compression for learning and predicting on a budget, model interpretability, etc . In many real world applications, one needs to deter-mine the entire feature selection path, i.e. the variation in prediction accuracy as the fraction of selected features is varied from null to unity, so as to determine the most fea-sible operating point on the path in the context of the given application.
 There has been much recent progress in non-linear fea-ture selection (Li et al., 2006; Bach, 2008; Hwang et al., 2011; Song et al., 2012) where the predictor is a non-linear function of the input features. Most algorithms have a pa-rameter influencing the number of selected features and one would need to try thousands of parameter settings to generate the entire feature selection path. This is a chal-lenge since state-of-the-art non-linear feature selection al-gorithms remain computationally expensive and training them thousands of times can be prohibitive (even with warm restarts).
 In particular, Multiple Kernel Learning (MKL) techniques have been shown to be amongst the most effective for non-linear feature selection (Ji et al., 2008; Chen et al., 2008; Varma &amp; Babu, 2009; Vedaldi et al., 2009; Levinboim &amp; Sha, 2012; Hwang et al., 2012). They have been demon-strated to be superior to a number of linear and non-linear filter and wrapper methods (Varma &amp; Babu, 2009). MKL feature selection techniques were also used to reduce pre-diction time in the joint winning entry of the competi-tive PASCAL VOC 2009 object detection in images chal-lenge (Vedaldi et al., 2009). However, even though many specialized MKL optimization techniques have been de-veloped (Kloft et al., 2011; Vishwanathan et al., 2010; Orabona &amp; Jie, 2011; Orabona et al., 2012; Jain et al., 2012), training them thousands of times with different pa-rameter settings is often infeasible.
 Our objective, in this paper, is to develop MKL formula-tions and algorithms for efficiently determining the non-linear feature selection path. Our starting point is a novel conjecture that, for certain MKL formulations subject to l p  X  1 regularization, the number of features (kernels) se-lected in the optimal solution monotonically decreases as p is decreased from an initial value to unity. We first prove that the conjecture is true for a generic family of Kernel Target Alignment (KTA) based l p -MKL formulations. This implies that regulating p , in the formulations of this family, provides a principled way of generating the feature selec-tion path (see Fig 1(a), 1(b)). In fact, for this family, we further strengthen the conjecture and prove that the fea-ture weights themselves decay (grow) monotonically once they are below (above) a certain threshold at optimality (see Fig 1(c)-1(e)). This implies that there exist juncture points along the path and an algorithm can exploit these by elimi-nating certain features from the optimization for potentially large intervals of p . It should be noted that these conjec-tures are non-trivial and we show that they do not hold for the popular square loss based l p -MKL formulation. Based on these results, we propose a Generalized l p -KTA formulation which extends the KTA formulations of (Cris-tianini et al., 2001; Lanckriet et al., 2004; Cortes et al., 2012). The proposed formulation is strongly convex and leads to robust feature selection (Bousquet &amp; Elisseeff, 2002; Zou &amp; Hastie, 2005; Kivinen et al., 2006). Fur-thermore, the feature monotonicity results allow us to de-velop a predictor-corrector based path following algorithm for Generalized l p -KTA that exploits the presence of junc-ture points for increased efficiency.
 We perform extensive experiments to demonstrate that the proposed Generalized l p -KTA formulation can lead to clas-sification accuracies which are as much as 10% higher on benchmark data sets not only as compared to other l p -MKL formulations and uniform kernel baselines but also lead-ing feature selection methods. We further demonstrate that our algorithm reduces training time significantly over other path following algorithms and state-of-the-art l p -MKL op-timizers such as SMO-MKL. In particular, we generate the entire feature selection path for data sets with a hundred thousand features in approximately half an hour on stan-dard hardware. Entire path generation for such data set is well beyond the scaling capabilities of other methods. Our contributions are as follows: (a) we propose, and prove, a novel conjecture regarding the monotonicity of se-lected features, and feature weights themselves, for certain l regularized MKL formulations; (b) we propose a Gen-eralized l p -KTA formulation for robust non-linear feature selection that is capable of achieving significantly higher classification accuracies as compared to state-of-the-art; and (c) our algorithm is many times faster than other lead-ing techniques. Considerable progress has been made in the area of non-linear feature selection. A popular approach is to map the input features to a kernel-induced feature space while simultaneously performing feature selection in the origi-nal input space. For example, (Li et al., 2006) propose to perform LASSO regression in the induced space and thereby perform non-linear feature selection while (Weston et al., 2000; Grandvalet &amp; Canu, 2002) pose this problem as that of tuning the hyper-parameters of the kernel. Another promising direction is to find un-correlated or independent features in the kernel induced feature space. While (Wu et al., 2005; Cao et al., 2007) aim at selecting orthogo-nal features in the feature space, (Chen et al., 2008; Song et al., 2012) employ Hilbert-Schmidt Independence Crite-rion based measures. Most of these approaches are either computationally expensive or resort to approximately min-imizing their non-convex objectives.
 Multiple Kernel Learning based techniques for non-linear feature selection have been explored in settings such as multi-label classification (Ji et al., 2008), bio-informatics (Chen et al., 2008; Levinboim &amp; Sha, 2012) and object categorization (Vedaldi et al., 2009; Hwang et al., 2012). In (Varma &amp; Babu, 2009), MKL tech-niques for non-linear feature selection were shown to be better than boosting (Baluja &amp; Rowley, 2007), lasso (An-drew &amp; Gao, 2007), sparse SVM (Chan et al., 2007), LP-SVM (Fung &amp; Mangasarian, 2002) and BAHSIC (Song et al., 2012). Various MKL formulations have been de-veloped including l p  X  1 -norm regularization over the ker-nel weights (Lanckriet et al., 2004; Rakotomamonjy et al., 2008; Cortes et al., 2009a; Kloft et al., 2011; Orabona &amp; Jie, 2011; Orabona et al., 2012), mixed-norm regulariz-ers (Aflalo et al., 2011), non-linear combinations of base kernels (Cortes et al., 2009b; Varma &amp; Babu, 2009), Breg-man divergence based regularizers (Vishwanathan et al., 2010) and for regularized kernel discriminant analysis (Ye Figure 1: (a) The feature selection path for the proposed Generalized l et al., 2008). State-of-the-art l p  X  1 -MKL optimization tech-niques such as SMO-MKL (Vishwanathan et al., 2010) and SPG-GMKL (Jain et al., 2012) have been shown to scale to a million kernels. Nevertheless, these techniques take more than a day to train on a standard desktop and so computing them thousands of times for determining the entire feature selection path is infeasible.
 Path following over the regularization parameter has been studied in the context of both non-sparse regulariza-tion (Hastie et al., 2004) as well as sparse linear clas-sifiers (Zhu et al., 2003). Some of the other settings where path following has been studied are: l 1 -MKL (Bach et al., 2004), general non-linear regularization paths (Ros-set, 2004), boosting (Zhao &amp; Yu, 2007) and l 1 regularized feature selection (Li &amp; Sminchisescu, 2010). Tracing the solution path for other hyper-parameters has also been ex-plored. While (Gunter &amp; Zhu, 2005; Wang et al., 2006) perform path following over the tube width parameter in support vector regression, (Wang et al., 2007) follow the path for the kernel hyper-parameter in SVMs.
 To the best of our knowledge, p -norm path following has not been studied in the literature so far. The closest com-peting technique to ours is the path following l 1 -MKL ap-proach of (Bach et al., 2004) and we present comparative results in Section 6. To perform non-linear feature selection, we associate a non-linear base kernel with each individual feature, such as RBF kernels defined per feature, and then learn a sparse combination of base kernels. We introduce our General-ized l p -KTA formulation for sparse kernel learning using the following notation.
 Let k 1 ,...,k r denote the given base kernel functions (one per feature) and let K 1 ,...,K r denote the correspond-ing centered gram matrices obtained from the training data so that the features induced in the RKHS have zero mean (Cortes et al., 2012). Let y denote the vector with entries as the labels of the training data. We are interested in learning a kernel, k , that is a conic combination of the given base kernels, i.e. k = P r i =1  X  i k i ,  X  i  X  0  X  i . We fo-cus on the following family of Bregman divergence based l -KTA formulations for learning the kernel weights  X  . Let F be a strictly convex and differentiable function and let  X  F denote its gradient. Then, the Bregman divergence generated by the function F is given by B F ( x ) = F ( x )  X  F ( x 0 )  X  ( x  X  x 0 ) &gt;  X  F ( x 0 ) , where x 0 is some fixed and given point in the domain of F . As an example, F ( x ) =  X  x,x  X  leads to B F ( x ) = k x  X  x 0 k 2 , the squared Euclidean distance. The proposed Generalized l p -KTA formulations have the following form: where the first term representing the Bregman diver-gence based regularizer is decomposable as  X  B F (  X  ) = P i B F (  X  i ) , the second term is the sparsity inducing l regularizer and the third term captures the alignment of the learnt kernel to the ideal kernel yy &gt; . Note that p  X  1 ,  X  1  X  0 and  X  2  X  0 are regularization parameters. A large number of popular Bregman divergences such as the squared Euclidean distance, generalized KL-divergence etc. are decomposable (Banerjee et al., 2005) and hence ad-missible under the Generalized l p -KTA formulation. Such Bregman divergences are known to promote robust feature and kernel selection (Bousquet &amp; Elisseeff, 2002; Zou &amp; Hastie, 2005; Kivinen et al., 2006) while the l p -norm regu-larizer achieves variable sparsity (Kloft et al., 2011). Note that the classical KTA formulations studied in (Lanckriet et al., 2004; Cristianini et al., 2001) can be obtained as spe-cial cases of our formulation by taking F to be the squared Euclidean distance and having  X  2 = 0 . On the other hand, substituting  X  1 = 0 results in the KTA formulation stud-ied in Proposition (7) in (Cortes et al., 2012). The normal-ized KTA formulation in Proposition (9) in (Cortes et al., 2012) employs a non-decomposable Bregman divergence and hence is not a special case of (1). However we em-pirically demonstrate in Section 6 that the proposed Gen-eralized l p -KTA formulation outperforms this normalized KTA formulation both in terms of generalization accuracy as well as computational cost. In particular, the normalized KTA formulation needs to operate on an r  X  r dense ma-trix which becomes infeasible for large r ( r = 10 5 for the standard Dorothea feature selection data set).
 The final classification results are obtained by training an SVM using the learnt kernel k = P r i =1  X  i k i . This cor-responds to non-linear feature selection since features for which  X  i is zero do not contribute to the kernel and can be dropped from the data set.
 We conclude this section with the following observation. Though (1) optimizes the kernel weights independently for a given  X  1 and  X  2 , the proposed algorithm employs cross-validation for optimizing  X  2 . Hence, the final optimal ker-nel weights need not be independent. In this section, we prove the monotonicity conjecture for the proposed Generalized l p -KTA formulation. Let  X   X  i denote the optimal weight of the kernel k i obtained with (1) at p = p 0 and let K ( p 0 ) denote the set of kernels active at p = p 0 . A kernel k i is said to be active at p = p 0 k i  X  K ( p 0 ) . if and only if  X   X  i ( p 0 ) &gt; , where &gt; 0 is a user-defined tolerance. The proposed conjecture can now be formally stated as We begin our analysis by noting the following theorem: Theorem 1. The path of the optimal solutions of (1) with respect to p , i.e.  X   X  ( p ) , is unique, smooth (provided F is twice-differentiable) and, in general, non-linear. Proof The optimal solution path of (1), i.e.  X   X  ( p ) , is unique since the objective of (1) is strictly convex. In order to prove the smoothness of the optimal solution path, we be-gin by deriving the necessary and sufficient conditions of optimality for (1). To this end, we first define Next, we consider two cases: Case 1 :  X   X  i = 0 for a given p : The necessary and suffi-cient conditions of optimality for a convex function is that the gradient of the function at optimality, g 0 (  X  lie in the normal cone of the feasibility set (Ben-Tal &amp; Ne-mirovski, 2001). This results in the following inequality: Since F is a convex function, we have F (  X  i )  X  F (  X  0 F (  X  0 i )(  X  i  X   X  0 i ) as well as F (  X  0 i )  X  F (  X  i  X  ) . Summing these two inequalities, we get 0  X  ( F 0 (  X  i F (  X  0 i ))(  X  0 i  X   X  i ) . Now substituting  X  i = 0 , we get 0  X   X  ( F 0 (0)  X  F 0 (  X  0 i )) . Since  X  0 i  X  0 (feasible set of  X   X  0 ), it follows 0  X  F 0 (0)  X  F 0 (  X  0 i ) . Hence, in the LHS of (3), ( F 0 (0)  X  F 0 (  X  0 i ))  X  0 and  X  y &gt; K i y  X  0 . It follows that the optimality conditions for  X   X  i = 0 are Note that both the above equalities are independent of p . It follows that if  X  p 0 &gt; 1 s.t.  X   X  i ( p 0 ) = 0 then  X  0  X  p &gt; 1 . Thus, the optimal solution path,  X   X  smooth, in fact linear, when  X  p 0 &gt; 1 s.t.  X   X  i ( p 0 Case 2 :  X   X  i &gt; 0 for a given p : In this case, the neces-sary and sufficient conditions of optimality (Ben-Tal &amp; Ne-mirovski, 2001) simplifies to g 0 (  X   X  i ) = 0 . Thus we get the following optimality condition In the following, we prove that the path of the optimal solution,  X   X  i ( p ) , of (5) is smooth for the non trivial case:  X  ( p ) &gt; 0  X  p &gt; 1 .
 Since the pair (  X   X  i ( p ) ,p ) always satisfy the equality in (5), we must have that d G i (  X   X  i ( p ) ,p )  X  P j  X  X  d p = 0  X  p &gt; 1 . This leads to The terms ln  X   X  i ( p ) and  X   X  i ( p ) p  X  2 are always finite as  X  ( p ) &gt; 0 and the denominator of (6) is always non-zero, in fact positive, because for any convex, twice-differentiable F we have: F 00 (  X   X  i ( p ))  X  0 . Hence, the derivative along the optimal solution path (6) is well de-fined and itself a continuous function; proving that the op-timal solution path is smooth (but generally non-linear) in this case too.
 The next theorem states the key result that proves the monotonicity conjecture for the proposed Generalized l KTA formulation.
 Theorem 2. Given  X   X  i ( p 0 ) , the following holds as p de-creases from p 0 to unity: (1)  X   X  i ( p ) decreases monotonically Proof. The monotonic behavior of  X   X  i ( p ) follows from ob-serving the sign of (6) and from the fact that e  X  1 p is a mono-tonically increasing function of p . Note that the denomina-tor of (6) is positive.
 Theorem 2 implies that the monotonicity conjecture for the Generalized l p -KTA formulation in (1) holds whenever the user defined tolerance parameter (above which a kernel is said to be active) is set to be less than e  X  1 . Points wherever an optimal kernel weight becomes less than the threshold are referred to as juncture points. Note that Theorems 1,2 and the monotonicity conjecture are non-trivial as they do not hold for all loss functions. In particular, we analyze l -MKL formulations with the popular square loss in Ap-pendix A, and present settings where the conjecture does not hold. This implies that path following algorithms for such formulations can not be speeded up by eliminating features whose weights touch zero from the optimization since they can increase to become non-zero at a later stage (see Fig 2). In this section, we present an efficient path following al-gorithm which closely approximates the true feature selec-tion path. The algorithm exploits the presence of juncture points to improve upon the standard Predictor-Corrector (PC) technique and scales effortlessly to optimization prob-lems involving a hundred thousand features. Finally, we give a bound on the deviation from the optimal objective function value caused due to the approximation.
 Theorem 1 states that the solution path of (1) is smooth but non-linear in general. Path following is typically implemented using the standard Predictor-Corrector algo-rithm (Allgower &amp; Georg, 1993) in such cases (Bach et al., 2004; Rosset, 2004). The PC algorithm is initialized with the optimal solution at p = p 0 . At every iteration, the cur-rent value of p is decreased by a small step size,  X  p and the following key iterative steps are performed: Predictor : The predictor is a close approximation of  X  ( p  X   X  p ) given  X   X  i ( p ) . This can either be the warm start approximation, i.e.  X   X  i ( p  X   X  p ) =  X   X  i ( p ) , the first order ap-proximation,  X   X  i ( p  X   X  p ) =  X   X  i ( p )  X   X  p d  X   X  i order approximation  X   X  i ( p  X   X  p ) =  X   X  i ( p )  X   X  p provided in the supplementary material).
 Corrector : The Newton method is used to correct the ap-proximations of the predictor step leading to a quadratic convergence rate (Allgower &amp; Georg, 1993).
 We modify the standard PC algorithm so as to closely ap-proximate the solution path at a lower computational cost. The key idea is to exploit the monotonicity in the active Algorithm 1 Generic Algorithm for Computing Solution Path set size and directly set certain kernel weights to zero for all the subsequent p values. Algorithm 1 summarizes the proposed algorithm. The algorithm maintains an active-set that is initialized to those kernels whose optimal weight at p 0 is above . At every iteration, the first order PC step is used to determine the kernel weights for the next p value. Kernels whose weight falls below are eliminated from the active set for the remainder of the optimization due to the monotonicity property. The error incurred by the pro-posed method can be bounded in terms of . The following Lemma holds when the squared Euclidean distance is used as the Bregman divergence: Lemma 1. For any p , the deviation in the objective value of (1) obtained using the approximate path following algo-rithm from the true optimal objective is upper bounded by r (  X  1 2 +  X  2 ( p  X  1) p ) .
 This result follows from the optimality conditions (5) and Theorem 2. We also state an analogous lemma for the gen-eralized KL-divergence in the supplementary material. our proposed method with linear features is the best in general. We carry out experiments to determine both the classifica-tion accuracy as well as the training time of the proposed Generalized l p -KTA over the feature selection path. Data sets &amp; kernels : We present results on a variety of fea-ture selection data sets taken from the NIPS 2003 Feature Selection Challenge (Guyon et al., 2006) and the ASU Fea-ture Selection Repository (Zhao et al., 2010). Table 2 lists the number of instances and features in each data set. Un-less otherwise stated, results are averaged via 5-fold cross-validation (standard deviations are reported in the supple-mentary material for lack of space). We define an RBF kernel per feature as our base kernels and center and trace normalize them as recommended in (Cortes et al., 2012). Baseline techniques : We compare the proposed approach to a number of baseline techniques including state-of-the-art Centered KTA formulation (Cortes et al., 2012), highly optimized l p -MKL techniques such as SMO-MKL (Vish-wanathan et al., 2010), leading feature selection methods such as BAHSIC (Song et al., 2012) as well as path fol-lowing approaches for l 1 -MKL (PF-l 1 -MKL) (Bach et al., 2004). While evaluating classification accuracy we also compare to the uniform kernel combination baseline (  X  i = 1 /r  X  i ) referred to as Uniform as well as path follow-ing linear feature selection approaches (PF-l 1 -SVM) (Zhu et al., 2003). On the ASU data sets we also compare our classification accuracy to 8 other state-of-the-art feature se-lection techniques whose details are given in Zhao et al. (2010). The final classification results for all the feature selection algorithms are obtained using an SVM with the the feature weights provided by the algorithm. While eval-uating computational cost, we also compare the cost of our proposed approximate first order predictor-corrector algo-rithm to the exact path following algorithm.
 Parameter settings : For the Generalized l p -KTA formula-tion we set  X  1 ,  X  2 and the SVM misclassification penalty C by 5-fold cross-validation while varying p from 2 to 1 in decrements of 0 . 01 . We use the squared Euclidean distance as the Bregman divergence with x 0 = 0 . BAHSIC, PF-l -MKL and PF-l 1 -SVM are parameter free when comput-ing the feature selection path. The parameters of the other techniques were also set via extensive cross-validation ex-cept for the computationally intensive SMO-MKL where this was feasible only for the smaller data sets. On the to generate results.
 larger data sets, we follow the SMO-MKL X  X  authors X  ex-perimental protocol and fix  X  = 1 and validate over C with p  X  { 1 . 01 , 1 . 33 , 1 . 66 , 2 } . As in the case of previous l MKL algorithms (Vishwanathan et al., 2010; Orabona &amp; Jie, 2011; Orabona et al., 2012; Jain et al., 2012), we em-ploy the common strategy of thresholding to obtain sparse solutions for the proposed Generalized l p -KTA.
 Results : Table 2 lists the maximum classification accuracy achieved along the entire feature selection path and the cor-responding number of selected features (i.e. corresponding to the best oracle results on the test set). Our proposed Generalized l p -KTA formulation gets significantly higher classification accuracies than all competing methods. For instance, on Arcene data set, Generalized l p -KTA achieves a classification accuracy which is 11% higher than the clos-est competing method. Table 3 presents results on the ASU data sets following the ASU experimental protocol where only 50% of the data is used for training and the number of selected features is restricted to be less than 200. This tests the capabilities of feature selection algorithms under the demanding conditions of both limited training data and lim-ited prediction budget. As can be seen, Gen l p -KTA (RBF) clearly outperforms all the linear methods thereby demon-strating the power of non-linear feature selection which is the focus of our paper. Amongst all the linear feature selec-tion methods, our proposed Gen l p -KTA (Linear) is the best in general. It is the best method on 3 data sets  X  on Relathe it is better than the second best method by 4%, on Arcene by 1.4% and on Dorothea by 0.27%. On 2 data sets it is the second best method  X  lagging behind the best method on Madelon by 0.02% and on Pcmac by 0.11%. These results demonstrate that the Generalized l p -KTA formulation can lead to better results not only as compared to other KTA and l p -MKL formulations but also as compared to leading feature selection techniques.
 Table 4 assesses the cost of computing the feature selec-tion path. Note that algorithms such as BAHSIC compute the path a feature at a time while other algorithms, such PF-l 1 -MKL, have a much sparser sampling of the path (in the limit Centered KTA produces only a single point on the path). Therefore, for a fair comparison, we report the to-tal time taken by each algorithm divided by the number of points generated by it along the path. This measures the average time taken by each algorithm to generate a point along the path. Table 4 shows that the proposed approx-imate first order predictor-corrector algorithm can be 16 to 878 times faster than competing techniques. Further-more, on the largest data set, Dorothea with 10 5 features, Centered-KTA, SMO-MKL and PF-l 1 -MKL were not able to generate even a single point on the path whereas our al-gorithm was able to generate the entire path in 21 minutes using pre-computed kernels and 36 minutes using kernels computed on the fly. BAHSIC generates the path by adding a single feature at each iteration and was able to generate the initial path segment up to 500 features but at a high computational cost of more than 3 hours. All experiments were carried out on a standard 2.40 GHz Intel Xeon desk-top with 32 GB of RAM. Finally, we note that our approx-imate path following algorithm was also found to be faster than the exact path following algorithm implemented with first order predictor or with warm restarts without exploit-ing juncture points and eliminating features from the opti-mization. The speedups against both the baselines varied with more than 3 times on Madelon and almost 4 times on Dorothea while the maximum relative deviation from the exact path was only 7 . 3  X  10  X  6 and 3 . 6  X  10  X  4 respectively. Figure 1(a) shows the feature selection path as computed by our approximate algorithm on all the data sets. We developed an efficient p -norm path following algorithm for non-linear feature selection in this paper. Our starting point was a novel conjecture that the number of selected features, and the feature weights themselves, vary mono-tonically with p in l p -MKL formulations. We proposed a Generalized l p -KTA formulation and proved that the con-jecture holds for this formulation but not for other popular formulations such as the square loss based l p -MKL. The monotonicity property of the Generalized l p -KTA formu-lation allows us to eliminate features whose weight goes below a certain threshold from the optimization for large intervals of p leading to efficient path following. It was the-oretically and empirically demonstrated that this resulted in only a minor deviation from the exact path. Experiments also revealed that our proposed formulation could yield sig-nificantly higher classification accuracies as compared to state-of-the-art MKL and feature selection methods (by as much as 11% in some cases) and be many times faster than them. All in all, we were able to effortlessly compute the entire path for problems involving a hundred thousand fea-tures which are well beyond the scaling capabilities of ex-isting path following techniques.
 We thank the anonymous reviewers for the valuable com-ments.
 In this section, we analyze the proposed conjecture in the context of the l p -MKL formulation for the ridge regres-sion (Cortes et al., 2009a): and  X  1 , X  2 &gt; 0 are the regularization parameters. Firstly, we consider a setting employed in (Lanckriet et al., 2004; Cristianini et al., 2001; 2000), which leads to the selec-tion of a low-dimensional subspace: unit rank base kernels K i = u i u &gt; i s.t. trace ( K i ) = 1 and  X  K i ,K j  X  = 0  X  i 6 = j . The following theorem holds in this setting: Theorem 3. Let  X   X  i ( p ) denote the optimal weight corre-sponding to the i -th kernel in (7) at p . Given  X   X  i ( p lowing holds as p decreases from p 0 to unity: (1)  X   X  i ( p ) de-creases monotonically whenever  X   X  i ( p 0 ) &lt; e  X  1 ; (2)  X  increases monotonically whenever  X   X  i ( p 0 ) &gt; e  X  1 The proof involves deriving and analyzing the d  X   X  ( p ) / d p term and is provided in the supplementary material. Needless to say, the above theorem implies that the conjec-ture is true for the case mentioned above. We now present an interesting example where the conjecture does not hold with = 0 : Theorem 4. Consider the regression formulation in (7) with two given base kernels of unit rank and unit trace: k 1 and k 2 . Additionally, let y &gt; K 2 K 1 y &gt; y &gt; Then for some  X  1 , X  2 ,  X  p 0 &gt; 1 such that  X  1 = 0 if and only if p = p 0 .
 The proof involves deriving and analyzing the necessary conditions for the optimal value of a kernel weight in (7) to be zero and is detailed in the supplementary material. The above theorem shows a case in which the kernel weight, after attaining the lowest feasible value (zero), at p = p 0 , grows as p further decreases from p 0 . Figure 2 shows an instance from a real world data set (Parkinson disease data set from the UCI Repository) where the opti-mal kernel weight in (7) grows after attaining zero. We can observe that at around p = 1 . 5 , the optimal kernel weight corresponding to Feature 1 is zero, and it again starts grow-ing as p is further decreased. This observation show that the conjecture is not universally true for low values of . How-ever, in the same setting as in Theorem 4, the conjecture may be true for some other &gt; 0 . The proposed algorithm is usable whenever such an is small 1 . In summary, the proposed conjecture is itself non-trivial and requires careful analysis for different l p -MKL formu-lations. Interestingly, in case of the proposed Generalized l -KTA, it holds for small enough tolerance, &lt; e  X  1 .
