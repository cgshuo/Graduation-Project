 CHUNG-HSIEN WU and HUNG-YU SU National Cheng Kung University, Taiwan YU-HSIEN CHIU
Industrial Technology Research Institute, Taiwan and CHIA-HUNG LIN National Cheng Kung University, Taiwan 1. INTRODUCTION
Sign language is a visual/gestural language that serves as the primary means of communication for deaf individuals, just as spoken languages are used among the hearing. Recently, sign languages have been considered as natural lan-guages instead of simply hand movements. A big difference between sign lan-guage and traditional spoken languages is that signers always encounter mis-trust when communicating with the hearing. Multimedia such as books, news, and TV are displayed in written and spoken forms of natural languages. To help hearing or speech-impaired people, machine translation (MT) from writ-ten or spoken languages is needed to provide assistance for augmentative and alternative communication. However, most statistical MT models need a huge amount of bilingual parallel text for training. Data sparseness is an important issue for statistical machine translation.

Current research into sign-language processing has focused on machine translation and sign video recognition [Nina et al. 2002]. For machine trans-lation, a number of recent papers deal with the problem of automatically ob-taining pairs of aligned sentences from parallel corpora [Manning and Sch  X  utze 1999; Chou and Juang 2003]. Of all alignment models, IBM models [Brown et al. 1993] and Verbmobil [Wahlster 2000] are two typical approaches to sta-tistical MT. Recently, a phrase-based approach [Koehn et al. 2003 and Zens and Ney 2003] has been proposed to improve the performance of the statistical MT.
Furthermore, a syntax-based alignment model was proposed for dealing with the problem of large corpus size. But they are word-to-word alignment-based translation without using syntactic information such as sentence structures.
Another approach to machine translation is transfer-based. Wu [1997] showed that restricted word alignment between sentence pairs could reduce the com-plexity of the alignment problem and allow a polynomial-time solution. Alshawi et al. [2000] proposed an approach that uses tree structure and finite-state au-tomata to derive the dependency structure of the source and target languages.
Yamada and Knight [2001] presented a method for translating the syntactic tree structure of the source language to that of the target language. The prob-abilities that a node acts are estimated from the parallel tree structures. The use of definite syntactic information for both sides of the parallel corpus in this model achieved better performance in translation. Gildea [2004] estimated the duplicate probabilities of the subtrees for coping a sub-tree to the position it should be in the translation. But these approaches only considered a node in translation instead of the whole parse tree, and they are tree-to-string models.
Current research on sign language translation adopted the machine trans-lation approaches just described. Marshall and S  X  af  X  ar [2002] proposed an ap-proach for translating English to British sign language by using a rule-based approach, and the grammar was modeled utilizing HPSG. Huenerfauth [2004] introduced a rule-based concept for translating English to American sign lan-guage. Morrissey and Way [2005] investigated a corpus-based method for sign language translation. With the small size of the corpus, the system has a prob-lem with unseen chunks. Chiu et al. [2006] proposed a two-pass alignment model for translating Chinese to TSL. Bungeroth and Ney [2004] proposed a statistical scheme modified from IBM models for sign language translation. The size of the bilingual corpus remains an important issue for corpus-based and statistical sign-language translation. For transfer-based sign-language transla-tion, Speers [2001] proposed a method using lexical function grammar to trans-fer from English to American sign language (ASL), and the transfer grammars between ASL and English were annotated manually.

In this article, a transfer-based three-stage statistical translation model us-ing probabilistic context-free grammars (PCFG) is proposed. In this model, the input Chinese sentence is first segmented into a word sequence. Each Chinese word sequence is then parsed into all possible phrase structure trees (PSTs) using the PCFGs trained from a Chinese Treebank. Second, the context-free grammar (CFG) rules used in the Chinese PSTs are transferred to the CFG rules of the corresponding TSL PSTs using the CFG rule transfer probabilities trained from the annotated Chinese-TSL bilingual corpus. Third, the trans-ferred TSL PSTs are used to generate the TSL sequences using the TSL PCFGs trained from the bilingual corpus. Finally, the Viterbi algorithm is employed to obtain the best translation result.

The rest of this article is organized as follows. The probability estimation of the CFG rules and the translation model are first introduced in Section 2. In
Section 3, the proposed model is compared with IBM Model 3 in both objective and subjective methods and some analysis of the translation system is given.
Finally, Section 4 draws conclusions and suggests future work. 2. TRANSFER-BASED STATISTICAL TRANSLATION MODEL There are several structural differences between natural sign language and
Chinese, such as word order, negative and adverb postposition, syntax conju-gation, etc. From our observation, the structural differences generally occur at both the syntax level and phrase level. In the proposed transfer-based machine translation, each sentence is parsed and represented as several possible PSTs [Fromkin and Rodman 1998] as shown in Figure 1. The hierarchical syntactic structure of a sentence can be observed through its PST. The PCFGs are then adopted to statistically model the PST. 2.1 Probability Estimation of the CFG Rule
For probability estimation of each CFG rule, Chomsky normal form (CNF) is applied to simplify the representation of rules by transforming rules into binary and unary rules of the form N i  X  N j N k and N i  X  w l , and the probabilities for these rules must satisfy the following constraint: where G is the PCFG model. For a subsequence W m , n = w m from the nonterminal node N i is given by P ( N i  X   X  X  X  W m , n denotes all possible derivations. A proper method to calculate the probability of a string is to use the inside-outside algorithm, which is a dynamic programming algorithm based on the inside probabilities [Baker 1979]. P ( N represents the probability of the sequence W m , n derived from a node N the PCFG model G as shown in Figure 2. P ( N i  X   X  X  X  W m , n recursively as the probability of the word sequence inside the constituent. Another probability where  X  i ( m , n | G ) denotes the outside probability for the nonterminal node N covering w m to w n (Baker 1979).

From the previous estimation, P ( N 0  X   X  X  X  w 1, T , N i  X  the summation of the probabilities of the derivation from root N the derivation from node N i to w m , n .
 The probability for node N i used in the derivation is estimated as
And the probability of N i which derives N j N k and then w
The estimate for how many times this particular rule is used in the derivation can be summed over all the words in the training corpus:
E ( N i  X  N j N k )
In the maximization step, for the sentence W 1, T = w 1 , w probability is estimated as
P ( N i  X  N j N k | G )
The probability of N i which derives N j N k over the whole corpus is defined as follows.

P ( N i  X  N j N k | G ) 2.2 Translation Model
In this study, a transfer-based statistical translation model is proposed. In this model, each Chinese sentence is parsed into possible Chinese PSTs, and then transferred to TSL PSTs using the transfer probabilities between Chinese and
TSL CFG rules are estimated from the bilingual parallel corpus. Finally, an op-timal translation TSL word sequence is determined using the Viterbi algorithm as the translation result. Figure 3 shows an example of the translation process.
To translate a Chinese sentence C to a TSL sequence S , the conditional proba-bility P ( S | C ) is considered. According to Bayes X  theory:
The optimal translated TSL sequence  X  S , for which P ( C is determined as follows: where P ( C | S ) represents the translation model probability, and P ( S )isthe language model probability of TSL. P ( C | S ) can be described as follows: where  X  T C is the Chinese phrase structure tree, and  X  ture tree. The translation model can be expressed by three translation stages.
Hence, to obtain a translated TSL sentence  X  S from the Chinese sentence C ,we have
In this equation, the optimal translation TSL sentence in three stages: (1) P ( C |  X  T C ) is the Chinese generation probability given the TSL, and (3) P ( S |  X  T S ) is the TSL sequence-generation probability for a given
TSL PST. Through these three stages, the optimal translation output is deter-mined using the Viterbi algorithm. The three-stage translation mechanism is shown in Figure 4. 2.2.1 Phrase Structure Tree of a Chinese Sentence. For an input Chinese sentence C with N words, CPS u = cps u 1 , cps u 2 , ... cps ble parsed POS sequence of the Chinese sentence C . Figure 5 shows the possible
POS sequences for the word sequence: {  X  (I) X   X  (want to) X   X  (go to) X   X  (Taipei) X   X  (for a meeting) X  } ; one of the possible POS sequences of the Chinese sentence C is { Nhaa, Ndabd, VE2, VC1, Ndabd, VA4 } .

For the estimation of the probability P ( C |  X  T C ), each input sentence is first parsed into several possible POS sequences. The probability P ( C mated as follows:
P ( C |  X  T C )  X  P ( CPS u | CRS ) where CPS u represents the U th possible POS sequence of the Chinese word sequence C , CRS represents the Chinese CFG rules used to parse CPS  X  T C , and is the start symbol of the phrase structure tree  X 
For the construction of PST, the out-of-rule problem generally occurs. To solve the problem, this study employs a constrained POS back-off model [Wu et al. 2002] by replacing an unseen POS with its syntactically similar POS. Figure 6 shows that there is no such rule to reduce Nhaa and VH11 (in Figure 6(a)),
VH11, and Td (Figure 6(b)) or Nhaa, VH11, and Td (Figure 6(c)) into their higher-level nonterminals for the POS sequence of the sentence  X  (how are you) X . However, backing off the POS of the word  X  (Good) X  (VH11) to its syntac-tically similar POS VH12 could solve this out-of-rule problem as shown in Fig-ure 6(d). In this example, POSs VH11 and VH12 have similar syntactic property. The constrained POS back-off model is constructed based on POS clustering.
A bidirectional bi-gram Vector ( BBV ) is defined for each POS. BBV contains the frequencies of its preceding and succeeding POSs for each POS within its siblings. Each component in the BBV is the frequency of the forward/backward bigram of the POS. The bidirectional bi-gram vector for the POS ps is given by where f ( bf i , ps ) is the frequency that POS ps comes after POS bf
POSs defined in this study. Cosine measure is used to calculate the similarity between POSs for clustering. Each POS cluster can be regarded as a syntactic constituent with similar syntactic characteristics. Therefore, it can be used to solve the out-of-rule problem in PST construction. 2.2.2 CFG Rule Transfer Probability. For the transfer from Chinese PST to TSL PST, the Chinese CFG rule-set is collected from the Chinese Treebank and the TSL CFG rule-set is obtained from the parallel bilingual corpus. Each
Chinese sentence in the bilingual corpus is parsed, and the CFG rules used for parsing the sentence are obtained. For each CFG rule, the terminal nodes represent the words defined in the lexicon and the nonterminal nodes represent the chunks containing several words. The Chinese CFG rules are rearranged according to the word order in the corresponding TSL sequence and assigned as the TSL CFG rules.

The correspondence between the Chinese CFG rules and TSL CFG rules is established and the transfer probabilities from the Chinese CFG rules to TSL CFG rules are estimated based on the bilingual corpus. For example, the Chinese rule  X  X P  X  VC31(buy) + NP X  in Figure 3 contains a nonterminal node
NP. In the bilingual sentence pair, VC31 maps to a word  X  X uy X  and NP is derived as  X  X M + Nab X  which map to words  X  X  X  and  X  X ook, X  respectively. In this rule, NP maps to the word  X  X ook X . The rule  X  X P  X  VC31(buy) + NP(book) X  is transferred to  X  X P  X  NP(book) + VC31(buy) X  as the word order in the bilingual sentence pair.
On the other hand, the Chinese CFG rules obtained from the Chinese Treebank, which do not appear in the bilingual corpus, are directly adopted as the TSL CFG rules without considering word order. As Figure 7 shows, the Chinese CFG rules in the upper block with dotted line are the rules which appear in the bilingual corpus, while the Chinese CFG rules in the lower block, which do not appear in the bilingual corpus, are directly used as the TSL CFG rules.  X  T C into TSL PST  X  T S is estimated as
P (  X  T C ,  X  T S ) = DeriveRule (1, N ) where DeriveRule (1, N ) represents the rule transfer probability from Chinese CFG rules to TSL CFG rules for the Chinese sequence with N words. CRS and
SRS represent the Chinese and TSL CFG rule-sets used to parse the Chinese and TSL sentences, respectively. cr i and sr i stand for the i th rule in Chinese and
TSL rules for the Chinese and TSL sentences, respectively. sr the corresponding transferred TSL rule of the Chinese rule cr 2.2.3 TSL PCFG Training. Because of the small-sized TSL corpus, the probabilities for the TSL CFG rules cannot be obtained directly from the occur-rence frequencies of the rules in the TSL corpus. However, the probabilities of the TSL CFG rules can be derived from the corresponding Chinese CFG rule probabilities weighted by the transfer probabilities. In addition, the remaining
Chinese CFG rules without any corresponding TSL CFG rules in the parallel bilingual corpus are directly adopted as the TSL CFG rules, and the proba-bilities of the adopted TSL CFG rules are weighted by a predefined weighting factor  X  .
 where Z is a normalization factor for P ( sr j ) defined in the following to meet the probability definition.
 where LHS ( SR ) = sr j represents that the left-hand side term of the TSL CFG is the probability of the i th Chinese CFG rule, transfer ( cr probability from the i th Chinese CFG rule to the j th TSL CFG rule. R represents the set of the Chinese CFG rules, which appear in the bilingual cor-pus, while R Treebank-Bilingual represents the set of the Chinese CFG rules, which appear in the Treebank but not in the bilingual corpus. Since the Chinese CFG rules obtained from the Chinese Treebank, which do not appear in the bilingual corpus, are directly adopted as the TSL CFG rules, a weighting parameter used to lower the probabilities of the adopted TSL CFG rules.

Since each sign-word sequence can be derived from several TSL PSTs, in-side probability is adopted to compute the probability of each TSL PST. The TSL sequence-generation probability from  X  T S is estimated from the probability P ( S |  X  T S ) as follows: where SRS is the TSL CFG rule set. T (1, N ) represents the number of the translated TSL sequence. Finally, through the previous three stages, the opti-mal translated TSL sequence  X  S is determined by 3. EXPERIMENTAL RESULTS 3.1 Corpus Analysis In the Chinese Treebank, there are 36,925 manually annotated sentences. the Treebank. This corpus consists of 234,201 words with 34,261 lexicons and 398,448 CFG rules with 27,323 distinct rules.

A bilingual corpus containing frequently-used sentence patterns was col-lected in Chinese as Table I shows. Each Chinese sentence was annotated with a sequence of corresponding Taiwanese sign words by one native TSL signer and checked by three TSL linguists.

There are 2,036 pairs of sentences in this bilingual corpus with an average sentence length of 5.6 words in Chinese. In total, 2,967 rule transfers between
Chinese and TSL CFG rules were extracted. In translation, a constrained POS back-off model is proposed for dealing with the out-of-rule problem, while con-structing a phrase structure tree from the Chinese sentences. Table II shows that verbs and nouns are the most frequently used parts of speech in a sentence.
An experiment about the POS back-off mechanism is proposed to process verbs and/or nouns, while constructing a phrase structure tree. 400 sentences are chosen from those sentences that are unable to be converted into PSTs directly by the PCFG for this experiment. Table III shows the result of the experiment.

The experiment was implemented in Visual C++ version 6.0 under the Win-dows 2000 platform with a P4 2.4GHz CPU and 512MB RAM. For the nouns using the back-off model, the translation time was 0.3 second and the accuracy was 93.25%. For verbs, the translation time was 0.7 second and the accuracy was 97.5%. Backing off both nouns and verbs, took the longest translation time of 1.9 second, but the accuracy achieved was 100%.

In the proposed model, there is a generated forest of the PST during trans-in translation, 20% of the bilingual sentences (407 sentences with an average length of 9.2 words) was chosen. The number of candidate PSTs was set to 50, 100, 200, and 500. Table IV shows the result of this experiment with 50 can-didate PSTs. It took 0.8 second in translation, but the translation accuracy is only 91.75%. With the increase in the number of candidates, the accuracy and translation time increases accordingly. 3.2 Evaluation of Translation Quality
Evaluations were conducted using objective and subjective metrics. Objective evaluation contains the alignment error rate (AER) [Och and Ney 2000], Top-N precision and BLUE [Papineni et al. 2002]. AER evaluates the word precision, while Top-N and BLUE evaluate the translation results at the sentence level. In subjective evaluation, the metric of mean opinion score (MOS) was used. In objective evaluation, the proposed model was compared with IBM Model 3.
In this experiment, 80% of the corpus was used for training, and 20% was used for evaluation. 3.2.1 Objective Evaluation. Alignment Error Rate (AER) presented by Och et al. [2000] is a mechanism for evaluating the performance of word alignment.
It can be used for machine translation, speech recognition, etc. and is defined as: where A indicates the set of word sequences after translation. G is the set of sign-word sequences tagged for these Chinese sentences. If AER is low, the translation result is precise. In the experiment, the proposed model is compared with IBM Model 3. About 80% of the corpus was used for training and 20% for testing. The average sentence length of the test set is 4.8 words for Chinese and 3.5 words for TSL. The result of AER is shown in Figure 9. The proposed approach has a lower AER of 0.09 compared to 0.225 for IBM Model 3. The proposed approach produces a more precise translation than IBM Model 3.
The statistical translation model generates several candidate translation results and uses probabilities to rank each candidate. The Top-1 candidate presents only the most probable result, but not always the most suitable result.
Top-N method provides the evaluation on the likely results of the translated candidates which exist in the first N-ranked candidates. The Top-N trans-lated sequences were compared with the sequences in the bilingual corpus. Figure 10 shows the results of Top-N precision for the proposed approach and
IBM Model 3. The vertical axis indicates the correct translation rate and the horizontal axis indicates the number of candidates represented by Top-N preci-sion. The result shows that the proposed approach outperformed IBM Model 3. The correct translation rates of Top-1 and Top-5 achieved 81.6% and 91.5%, respectively. In error analysis, we discovered that some sign sequences which were ranked beyond Top-5 were also understandable from the viewpoint of
TSL. Considering this factor, the Top-5 correct translation rate can be further increased to be higher than 91.5%.

Finally, BLEU metric was also adopted for evaluating the translation perfor-mance. BLEU metric uses modified N-Gram precision and the brevity penalty to calculate a score for the translated sentence: where p n is the modified N-gram precision, w n = 1 / N , and BP is the penalty for sentence length computed as follows: where c is the length of the translated sentence, and r is length of the reference sentence. Figure 11 shows the BLEU scores of the proposed model and IBM
Model 3, considering one reference translation on bi-gram precision. 3.2.2 Subjective Evaluation. For subjective evaluation, two groups of sub-jects were chosen. Group 1 (10 people) is composed of the hearing people who have learned or used TSL for years, including sign translator and sign lan-guage association members. Group 2 (10 people) are native TSL signers who are hearing/speech impaired. Three traditional opinions X  X ood, fair, and poor X  X re used for evaluating the performance of translation. The translated sentences for evaluation involve twenty short sentences (with an average word length of 4) and twenty long sentences (with an average word length of 8). Figure 12 and Figure 13 show the evaluation results for Group 1 and Group 2. The verti-cal axis indicates the histogram of the three opinions, and the horizontal axis indicates the subjective opinions. Figure 12 shows that the reading compre-hension for Group 1 is 83.25%, with 52% good and 31.25% fair opinions (the average of long sentences and short sentences). Figure 13 shows that the read-ing comprehension for Group 2 is 78% with 46% good and 32% fair opinions (the average of long sentences and short sentences).
 4. CONCLUSION AND DISCUSSION This study has presented an approach to the translation of Chinese to
Taiwanese sign language. The proposed transfer-based statistical translation model is based on syntactic information of Chinese and TSL. In translation, this approach parses the Chinese sentence into possible POS sequences and determines the possible PSTs. Each of the Chinese PSTs is then transferred into a TSL PST through the transfer between Chinese and TSL CFG rules.
The translated TSL PSTs are used to generate the TSL sequences. Finally, the translation result is obtained from the maximum probability throughout these three stages. In the experiments, objective evaluation using AER, Top-N preci-sion, and BLUE, and subjective evaluation using MOS was adopted and shows that the proposed approach outperforms IBM Model 3.

This study has shown the ability to transfer the CFG rules from a major-ity language to a minority language for transfer-based machine translation.
Nevertheless, some disadvantages still exist in the proposed transfer-based approach. First, the translation performance for transfer-based machine trans-lation is highly dependent on the performance of the parser. Incorrect parsing results will degrade the correctness for rule transfer. A robust parser is needed to obtain accurate parsing results for transfer-based translation. Second, using statistical approaches with a small-sized corpus suffers from the problem of unobserved TSL CFG rules and therefore cannot obtain the statistical char-acteristics of the TSL CFG rules. This is still an open problem in statistical processing of minority languages with small-sized corpora.

