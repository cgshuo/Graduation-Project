 The Web holds a massive number of reviews de-scribing the sentiments of customers about prod-ucts and services. These reviews can help the user reach purchasing decisions and guide companies X  business activities such as product improvements. It is, however, almost impossible to read all re-views given their sheer number.

These reviews are best utilized by the devel-opment of automatic text summarization, partic-ularly sentiment summarization. It enables us to efficiently grasp the key bits of information. Senti-ment summarizers are divided into two categories in terms of output style. One outputs lists of sentences (Hu and Liu, 2004; Blair-Goldensohn et al., 2008; Titov and McDonald, 2008), the other outputs texts consisting of ordered sentences (Carenini et al., 2006; Carenini and Cheung, 2008; Lerman et al., 2009; Lerman and McDonald, 2009). Our work lies in the latter category, and a typical summary is shown in Figure 1. Although visual representations such as bar or rader charts are helpful, such representations necessitate some simplifications of information to presentation. In contrast, text can present complex information that can X  X  readily be visualized, so in this paper we fo-cus on producing textual summaries.

One crucial weakness of existing text-oriented summarizers is the poor readability of their results. Good readability is essential because readability strongly affects text comprehension (Barzilay et al., 2002).

To achieve readable summaries, the extracted sentences must be appropriately ordered (Barzilay et al., 2002; Lapata, 2003; Barzilay and Lee, 2004; Barzilay and Lapata, 2005). Barzilay et al. (2002) proposed an algorithm for ordering sentences ac-cording to the dates of the publications from which the sentences were extracted. Lapata (2003) pro-posed an algorithm that computes the probability of two sentences being adjacent for ordering sen-tences. Both methods delink sentence extraction from sentence ordering, so a sentence can be ex-tracted that cannot be ordered naturally with the other extracted sentences.

To solve this problem, we propose an algorithm that chooses sentences and orders them simulta-neously in such a way that the ordered sentences maximize the scores of informativeness and read-ability. Our algorithm efficiently searches for the best sequence of sentences by using dynamic pro-gramming and beam search. We verify that our method generates summaries that are significantly better than the baseline results in terms of ROUGE score (Lin, 2004) and subjective readability mea-sures. As far as we know, this is the first work to simultaneously achieve both informativeness and readability in the area of multi-document summa-rization.

This paper is organized as follows: Section 2 describes our summarization method. Section 3 reports our evaluation experiments. We conclude this paper in Section 4. Formally, we define a summary S  X  =  X  s 0 , s 1 , . . . , s n , s n +1 ing of n sentences where s indicating the beginning and ending of the se-quence, respectively. Summary S  X  is also defined as follows: where Info( S ) indicates the informativeness score of S , Read( S ) indicates the readability score of S , T indicates possible sequences com-posed of sentences in the target documents,  X  is a weight parameter balancing informativeness against readability, length( S ) is the length of S , and K is the maximum size of the summary.

We introduce the informativeness score and the readability score, then describe how to optimize a sequence. 2.1 Informativeness Score Since we attempt to summarize reviews, we as-sume that a good summary must involve as many sentiments as possible. Therefore, we define the informativeness score as follows: where e indicates sentiment e =  X  a, p  X  as the tu-ple of aspect a and polarity p = { X  1 , 0 , 1 } , E ( S ) is the set of sentiments contained S , and f ( e ) is the score of sentiment e . Aspect a represents a stand-point for evaluating products and services. With regard to restaurants, aspects include food , atmo-sphere and staff . Polarity represents whether the sentiment is positive or negative. In this paper, we define p =  X  1 as negative, p = 0 as neutral and p = 1 as positive sentiment.

Notice that Equation 2 defines the informative-ness score of a summary as the sum of the score of the sentiments contained in S . To avoid du-plicative sentences, each sentiment is counted only once for scoring. In addition, the aspects are clus-tered and similar aspects (e.g. air , ambience ) are treated as the same aspect (e.g. atmosphere ). In this paper we define f ( e ) as the frequency of e in the target documents.

Sentiments are extracted using a sentiment lex-icon and pattern matched from dependency trees pairs of sentiment expressions and their polarities, for example, delicious , friendly and good are pos-itive sentiment expressions, bad and expensive are negative sentiment expressions.

To extract sentiments from given sentences, first, we identify sentiment expressions among words consisting of parsed sentences. For ex-ample, in the case of the sentence  X  X his restau-rant offers customers delicious foods and a relax-ing atmosphere. X  in Figure 1, delicious and re-laxing are identified as sentiment expressions. If the sentiment expressions are identified, the ex-pressions and its aspects are extracted as aspect-sentiment expression pairs from dependency tree using some rules. In the case of the example sen-tence, foods and delicious , atmosphere and relax-ing are extracted as aspect-sentiment expression pairs. Finally extracted sentiment expressions are converted to polarities, we acquire the set of sen-timents from sentences, for example,  X  foods , 1  X  and  X  atmosphere , 1  X  .

Note that since our method relies on only senti-ment lexicon, extractable aspects are unlimited. 2.2 Readability Score Readability consists of various elements such as conciseness, coherence, and grammar. Since it is difficult to model all of them, we approximate readability as the natural order of sentences.
To order sentences, Barzilay et al. (2002) used the publication dates of documents to catch temporally-ordered events, but this approach is not really suitable for our goal because reviews focus on entities rather than events. Lapata (2003) em-ployed the probability of two sentences being ad-jacent as determined from a corpus. If the cor-pus consists of reviews, it is expected that this ap-proach would be effective for sentiment summa-rization. Therefore, we adopt and improve Lap-ata X  X  approach to order sentences. We define the readability score as follows: where, given two adjacent sentences s s tivity of the two sentences, is the inner product of w and  X  ( s i , s i +1 ) , w is a parameter vector and  X  ( s i , s i +1 ) is a feature vector of the two sentences. That is, the readability score of sentence sequence S is the sum of the connectivity of all adjacent sen-tences in the sequence.
 As the features, Lapata (2003) proposed the Cartesian product of content words in adjacent sentences. To this, we add named entity tags (e.g. LOC , ORG ) and connectives. We observe that the first sentence of a review of a restaurant frequently contains named entities indicating location. We aim to reproduce this characteristic in the order-ing.

We also define feature vector  X ( S ) of the entire sequence S =  X  s Therefore, the score of sequence S is w &gt;  X ( S ) . Given a training set, if a trained parameter w as-signs a score w &gt;  X ( S + ) to an correct order S + that is higher than a score w &gt;  X ( S  X  ) to an incor-rameter will give higher score to naturally ordered sentences than to unnaturally ordered sentences.
We use Averaged Perceptron (Collins, 2002) to find w . Averaged Perceptron requires an argmax operation for parameter estimation. Since we at-tempt to order a set of sentences, the operation is regarded as solving the Traveling Salesman Prob-lem; that is, we locate the path that offers maxi-mum score through all n sentences as s are starting and ending points, respectively. Thus the operation is NP-hard and it is difficult to find the global optimal solution. To alleviate this, we find an approximate solution by adopting the dy-namic programming technique of the Held and Karp Algorithm (Held and Karp, 1962) and beam search.

We show the search procedure in Figure 2. S indicates intended sentences and M is a distance matrix of the readability scores of adjacent sen-tence pairs. H i ( C , j ) indicates the score of the hypothesis that has covered the set of i sentences C and has the sentence j at the end of the path, i.e. the last sentence of the summary being gener-ated. For example, H 2 ( { s a hypothesis that covers s tence is s score of 0, and new sentences are then added one by one. In the search procedure, our dynamic pro-gramming based algorithm retains just the hypoth-esis with maximum score among the hypotheses that have the same sentences and the same last sen-tence. Since this procedure is still computationally hard, only the top b hypotheses are expanded.
Note that our method learns w from texts auto-matically annotated by a POS tagger and a named entity tagger. Thus manual annotation isn X  X  re-quired. 2.3 Optimization The argmax operation in Equation 1 also involves search, which is NP-hard as described in Section 2.2. Therefore, we adopt the Held and Karp Algo-rithm and beam search to find approximate solu-tions. The search algorithm is basically the same as parameter estimation, except for its calculation of the informativeness score and size limitation. Therefore, when a new sentence is added to a hy-pothesis, both the informativeness and the read-ability scores are calculated. The size of the hy-pothesis is also calculated and if the size exceeds the limit, the sentence can X  X  be added. A hypoth-esis that can X  X  accept any more sentences is re-moved from the search procedure and preserved in memory. After all hypotheses are removed, the best hypothesis is chosen from among the pre-served hypotheses as the solution. This section evaluates our method in terms of ROUGE score and readability. We collected 2,940 reviews of 100 restaurants from a website. The average size of each document set (corresponds to one restaurant) was 5,343 bytes. We attempted to generate 300 byte summaries, so the summa-rization rate was about 6%. We used CRFs-based Japanese dependency parser (Imamura et al., 2007) and named entity recognizer (Suzuki et al., 2006) for sentiment extraction and construct-ing feature vectors for readability score, respec-tively. 3.1 ROUGE We used ROUGE (Lin, 2004) for evaluating the content of summaries. We chose ROUGE-2, ROUGE-SU4 and ROUGE-SU9. We prepared four reference summaries for each document set.
To evaluate the effects of the informativeness score, the readability score and the optimization, we compared the following five methods.

Baseline : employs MMR (Carbonell and Gold-stein, 1998). We designed the score of a sentence as term frequencies of the content words in a doc-ument set.

Method1 : uses optimization without the infor-mativeness score or readability score. It also used term frequencies to score sentences.

Method2 : uses the informativeness score and optimization without the readability score. Method3 : the proposed method. Following Equation 1, the summarizer searches for a se-quence with high informativeness and readability score. The parameter vector w was trained on the same 2,940 reviews in 5-fold cross validation fash-ion.  X  was set to 6,000 using a development set.
Human is the reference summaries. To com-pare our summarizer to human summarization, we calculated ROUGE scores between each reference and the other references, and averaged them. The results of these experiments are shown in Table 1. ROUGE scores increase in the order of Method1, Method2 and Method3 but no method could match the performance of Human. The methods significantly outperformed Baseline ac-cording to the Wilcoxon signed-rank test.
 We discuss the contribution of readability to ROUGE scores. Comparing Method2 to Method3, ROUGE scores of the latter were higher for all cri-teria. It is interesting that the readability criterion also improved ROUGE scores.

We also evaluated our method in terms of sen-timents. We extracted sentiments from the sum-maries using the above sentiment extractor, and averaged the unique sentiment numbers. Table 2 shows the results.

The references (Human) have fewer sentiments than the summaries generated by our method. In other words, the references included almost as many other sentences (e.g. reasons for the senti-ments) as those expressing sentiments. Carenini et al. (2006) pointed out that readers wanted  X  X e-tailed information X  in summaries, and the reasons are one of such piece of information. Including them in summaries would greatly improve sum-marizer appeal. 3.2 Readability Readability was evaluated by human judges. Three different summarizers generated summaries for each document set. Ten judges evaluated the thirty summaries for each. Before the evalua-tion the judges read evaluation criteria and gave points to summaries using a five-point scale. The judges weren X  X  informed of which method gener-ated which summary.

We compared three methods; Ordering sen-tences according to publication dates and posi-tions in which sentences appear after sentence extraction ( Method2 ), Ordering sentences us-ing the readability score after sentence extrac-tion ( Method2+ ) and searching a document set to discover the sequence with the highest score ( Method3 ).
 Table 3 shows the results of the experiment. Readability increased in the order of Method2, Method2+ and Method3. According to the Wilcoxon signed-rank test, there was no signifi-cance difference between Method2 and Method2+ but the difference between Method2 and Method3 was significant, p &lt; 0 . 10 .

One important factor behind the higher read-ability of Method3 is that it yields longer sen-tences on average (6.52). Method2 and Method2+ yielded averages of 7.23 sentences. The difference is significant as indicated by p &lt; 0 . 01 . That is, Method2 and Method2+ tended to select short sen-tences, which made their summaries less readable. This paper proposed a novel algorithm for senti-ment summarization that takes account of infor-mativeness and readability, simultaneously. To summarize reviews, the informativeness score is based on sentiments and the readability score is learned from a corpus of reviews. The preferred sequence is determined by using dynamic pro-gramming and beam search. Experiments showed that our method generated better summaries than the baseline in terms of ROUGE score and read-ability.

One future work is to include important infor-mation other than sentiments in the summaries. We also plan to model the order of sentences glob-ally. Although the ordering model in this paper is local since it looks at only adjacent sentences, a model that can evaluate global order is important for better summaries.
 We would like to sincerely thank Tsutomu Hirao for his comments and discussions. We would also like to thank the reviewers for their comments.
