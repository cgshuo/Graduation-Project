 Bryan Catanzaro catanzar@eecs.berkeley.edu Narayanan Sundaram narayans@eecs.berkeley.edu Kurt Keutzer keutzer@eecs.berkeley.edu Driven by the capabilities and limitations of modern semiconductor manufacturing, the computing indus-try is currently undergoing a massive shift towards parallel computing (Asanovi  X c et al., 2006). This shift brings dramatically enhanced performance to those al-gorithms which can be adapted to parallel computers. One set of such algorithms are those used to implement Support Vector Machines (Cortes &amp; Vapnik, 1995). Thanks to their robust generalization performance, SVMs have found use in diverse classification tasks, such as image recognition, bioinformatics, and text processing. Yet, training Support Vector Machines and using them for classification remains very com-putationally intensive. Much research has been done to accelerate training time, such as Osuna X  X  decompo-sition approach (Osuna et al., 1997), Platt X  X  Sequential Minimal Optimization (SMO) algorithm (Platt, 1999), Joachims X  SV M light (Joachims, 1999), which intro-duced shrinking and kernel caching, , and the working set selection heuristics used by LIBSVM (Fan et al., 2005). Despite this research, SVM training time is still significant for large training sets.
 In this paper, we show how Support Vector Machine training and classification can be adapted to a highly parallel, yet widely available and affordable computing platform: the graphics processor, or more specifically, the Nvidia GeForce 8800 GTX, and detail the perfor-mance gains achieved.
 The organization of the paper is as follows. Section 2 describes the SVM training and classification problems briefly. Section 3 gives an overview of the architec-tural and programming features of the GPU. Section 4 presents the details of implementation of the paral-lel SMO approach on the GPU. Section 5 explains the implementation details of the SVM classification prob-lem. We present our results in Section 6 and conclude in Section 7. We consider the standard two-class soft-margin SVM classification problem (C-SVM), which classifies a given data point x  X  R n by assigning a label y  X  { X  1 , 1 } . 2.1. SVM Training Given a labeled training set consisting of a set of data points x i ,i  X  { 1 ,...,l } with their accompanying la-bels y i ,i  X  { 1 ,...,l } , the SVM training problem can be written as the following Quadratic Program, where  X  i is a set of weights, one for each training point, which are being optimized to determine the SVM classifier, C is a parameter which trades classifier generality for accuracy on the training set, and Q ij = y i y j  X ( x i ,x where  X ( x i ,x j ) is a kernel function. We consider the standard kernel functions shown in table 1.
 2.1.1. SMO Algorithm The SVM Training problem can be solved by many methods, each with different parallelism implications. We have implemented the Sequential Minimal Opti-mization algorithm (Platt, 1999), with a hybrid work-ing set selection heuristic making use of the first order heuristic proposed by (Keerthi et al., 2001) as well as the second order heuristic proposed by (Fan et al., 2005).
 The SMO algorithm is a specialized optimization ap-proach for the SVM quadratic program. It takes ad-vantage of the sparse nature of the support vector problem and the simple nature of the constraints in the SVM QP to reduce each optimization step to its minimum form: updating two  X  i weights. The bulk of the computation is then to update the Karush-Kuhn-Tucker optimality conditions for the remaining set of weights and then find the next two weights to update in the next iteration. This is repeated until conver-gence. We state this algorithm briefly, for reference purposes.
 Algorithm 1 Sequential Minimal Optimization Input: training data x i , labels y i ,  X  i  X  X  1 ..l } Initialize:  X  i = 0, f i =  X  y i ,  X  i  X  X  1 ..l } ,
Update  X  i repeat For the first iteration, we initialize b high =  X  1, i high min { i : y i = 1 } , b low = 1, and i low = min { i : y i During each iteration, once we have chosen i high and i low , we take the optimization step: where  X  =  X ( x i 2 X ( x i sible,  X  0 i range 0  X   X  i  X  C .
 The optimality conditions can be tracked through the structed iteratively as the algorithm progresses. After each  X  update, f is updated for all points. This is one of the major computational steps of the algorithm, and is done as follows: In order to evaluate the optimality conditions, we de-fine index sets: Because of the approximate nature of the solution pro-cess, these index sets are computed to within a toler-ance , e.g. { i : &lt;  X  i &lt; ( C  X  ) } .
 We can then measure the optimality of our current solution by checking the optimality gap, which is the difference between b high = min { f i : i  X  I high } , and b low = max { f i : i  X  I low } . When b low  X  b high + 2  X  , we terminate the algorithm. 2.1.2. Working set selection During each iteration, we need to choose i high and i low which index the  X  weights which will be changed in the following optimization step. The first order heuristic from (Keerthi et al., 2001) chooses them as follows: The second order heuristic from (Fan et al., 2005) chooses i high and i low to optimize the unconstrained SVM functional. An optimal approach to this problem would require examining l 2 candidate pairs, which would be computationally intractable. To simplify the problem, i high is instead chosen as in the first order heuristic, and then i low is chosen to maximally im-prove the objective function while still guaranteeing progress towards the constrained optimum from prob-lem (1). More explicitly: i high = arg min { f i : i  X  I high } (9) i low = arg max {  X  F i (  X  ) : i  X  I low ,f i After choosing i high , we compute for all i  X  X  1 ..l } We then find the maximum  X  F i over all valid points ( i  X  I low ) for which we are guaranteed to progress towards the constrained optimum ( f i 2.1.3. Adaptive heuristic The second order heuristic utilizes more information from the SVM training problem, and so it generally re-duces the number of iterations necessary during the so-lution process. However, it is more costly to compute. In our GPU implementation, the geometric mean of iteration time over our benchmark set using the sec-ond order heuristic increased by 1.9  X  compared to the first order heuristic. On some benchmarks, the total number of iterations decreased sufficiently to provide a significant speedup overall, but on others, the sec-ond order heuristic is counterproductive for our GPU implementation.
 To overcome this problem, we implemented an adap-tive heuristic that chooses between the two selection heuristics dynamically, with no input or tuning from the user. The adaptive heuristic periodically samples progress towards convergence as a function of wall-clock time using both heuristics, then chooses the more productive heuristic.
 This sampling occurs every l/ 10 iterations, and dur-ing each sample, the heuristic under test is executed for two phases of 64 iterations each. The average op-timality gap in each of these phases is computed, and then the rate of progress is estimated by dividing the change in the optimality gap over the two phases by the time it has taken to execute them. The same sam-pling process is then performed with the other heuris-tic, and the best heuristic is then used until the next sampling period. 2.2. SVM Classification The SVM classification problem is as follows: for each data point z which should be classified, compute where z  X  R n is a point which needs to be classified, and all other variables remain as previously defined. From the classification problem definition, it follows immediately that the decision surface is defined by ref-erencing a subset of the training data, or more specif-ically, those training data points for which the cor-responding  X  i &gt; 0. Such points are called support vectors.
 Generally, we classify not just one point, but a set of points. We exploit this for better performance, as explained in Section 5. Graphics processors are currently transitioning from their initial role as specialized accelerators for trian-gle rasterization to general purpose engines for high throughput floating-point computation. Because they still service the large gaming industry, they are ubiq-uitous and relatively inexpensive.
 GPU architectures are specialized for compute-intensive, memory-intensive, highly parallel computa-tion, and therefore are designed such that more re-sources are devoted to data processing than caching or control flow. State of the art GPUs provide up to an order of magnitude more peak IEEE single-precision floating-point than their CPU counterparts. Addition-ally, GPUs have much more aggressive memory sub-systems, typically endowed with more than 10x higher memory bandwidth than a CPU. Peak performance is usually impossible to achieve on general purpose ap-plications, yet capturing even a fraction of peak per-formance yields significant speedup.
 GPU performance is dependent on finding high degrees of parallelism: a typical computation running on the GPU must express thousands of threads in order to effectively use the hardware capabilities. As such, we consider it an example of future  X  X any-core X  process-ing (Asanovi  X c et al., 2006). Algorithms for machine learning applications will need to consider such par-allelism in order to utilize many-core processors. Ap-plications which do not express parallelism will not continue improving their performance when run on newer computing platforms at the rates we have en-joyed in the past. Therefore, finding large scale par-allelism is important for compute performance in the future. Programming for GPUs is then indicative of the future many-core programming experience. 3.1. Nvidia GeForce 8800 GTX In this project, we employ the NVIDIA GeForce 8800 GTX GPU, which is an instance of the G80 GPU ar-chitecture, and is a standard GPU widely available on the market. Pertinent facts about the GPU plat-form can be found in table 2. We refer the reader to the Nvidia CUDA reference manual for more details (Nvidia, 2007).
 3.2. CUDA Nvidia provides a programming environment for its GPUs called the Compute Unified Device Architecture (CUDA). The user codes in annotated C++, acceler-ating compute intensive portions of the application by executing them on the GPU. Figure 1 illustrates how the GPU appears to the pro-grammer. The programmer organizes the computa-tion into grids, which are organized as a set of thread blocks. The grids run sequentially on the GPU, mean-ing that all computation in the grid must finish before another grid is invoked. As mentioned, grids contain thread blocks, which are batches of threads that exe-cute together, sharing local memories and synchroniz-ing at programmer specified barriers. A maximum of 512 threads can comprise a thread block, which puts a limit on the scope of synchronization and communica-tion in the computation. However, enormous numbers of blocks can be launched in parallel in the grid, so that the total number of threads that can be launched in parallel is very high. In practice, we need a large number of thread blocks to ensure that the compute power of the GPU is efficiently utilized. Since GPUs need a large number of threads to effi-ciently exploit parallelism, we create one thread for every data point in the training set. For the first phase of the computation, each thread computes f 0 i from equation (4). We then apply a working set selec-tion heuristic to select the next points which will be optimized. The details are explained in the following section. 4.1. Map Reduce At least since the LISP programming language, pro-grammers have been mapping independent computa-tions onto partitioned data sets, using reduce oper-ations to summarize the results. Recently, Google proposed a Map Reduce variant for processing large datasets on compute clusters (Dean &amp; Ghemawat, 2004). This algorithmic pattern is very useful for ex-tracting parallelism, since it is simple to understand, and maps well to parallel hardware, given the inherent parallelism in the map stage of the computation. The Map Reduce pattern has been shown to be useful for many machine learning applications (Chu et al., 2007), and is a natural fit for our SVM training algo-rithm. For the first order heuristic, the computation of f for all points is the map function, and the search for b low , b high , i low and i high is the reduction operation. For the second order heuristic, there are two Map Re-duce stages: one to compute f 0 i , b high and i high , and another where the map stage computes  X  F i for all points, while the reduce stage computes b low and i low . Because the CUDA programming model has strict lim-itations on synchronization and communication be-tween thread blocks, we organize the reductions in two phases, as shown in figure 2. The first phase does the map computation, as well as a local reduce within a thread block. The second phase finishes the global re-duction. Each phase of this process is implemented as a separate call to the GPU. 4.2. Implementation Details 4.2.1. Caching Since evaluating the kernel function  X (  X  ) is the dom-inant part of the computation, it is useful to cache as much as possible from the matrix of kernel func-tion evaluations K ij =  X ( x i ,x j ) (Joachims, 1999). We compute rows of this matrix on the fly, as needed by the algorithm, and cache them in the available memory on the GPU.
 When updating the vector f , we need access to two rows of K , since we have changed exactly two entries in  X  . In our system, the CPU checks to see which of these two rows, if any, are present in the cache. If a row is not present, the CPU voids the least recently used row of the cache, and assigns it to the new row which is needed. For the rows which hit in the cache, the GPU avoids doing the kernel evaluations. Otherwise, the GPU writes out the appropriate row or rows after computing the kernel values. When using the second order heuristic, the computation of  X  F references the row of K corresponding to i high , which guarantees that the next update of f will have a cache hit for its access to the same row. 4.2.2. Data Movement Programming the GPU requires manually copying data from the host computer to the GPU and vice versa, and it also requires manually copying data from the GPU X  X  global memory to the fast local stores. As mentioned previously, if the cache does not contain a particular row of K corresponding to the point x j , that row will need to be generated, which means that we need to compute  X ( x i ,x j )  X  i  X  1 ..l . Since the vector x j is shared between all computations, we load it into the GPU X  X  local store. This is key to performance, since accessing the local store is orders of magnitude faster than accessing the global memory. 4.3. Related Work There have been previous attempts to parallelize the SVM training problem. The most similar to ours is (Cao et al., 2006), which parallelizes the SMO algo-rithm on a cluster of computers using MPI. Both our approach and their approach use the concurrency in-herent in the KKT condition updates as the major source of parallelism. However, in terms of imple-mentation, GPUs present a completely different model than clusters, and hence the amount of parallelism ex-ploited, such as the number of threads, granularity of computation per thread, memory access patterns, and data partitioning are very different. We also imple-ment more sophisticated working set selection heuris-tics.
 Many other approaches for parallelizing SVM train-ing have been presented. The cascade SVM (Graf et al., 2005) is another proposed method for paralleliz-ing SVM training on clusters. It uses a method of di-vide and conquer to solve large SVM problems. (Zanni et al., 2006) parallelize the underlying QP solver us-ing Parallel Gradient Projection Technique. Work has been done on using a parallel Interior Point Method for solving the SVM training problem (Wu et al., 2006). (Collobert et al., 2002) proposes a method where the several smaller SVMs are trained in a parallel fashion and their outputs weighted using a Artificial Neural Network. (Ferreira et al., 2006) implement a gradi-ent based solution for SVM training, which relies on data parallelism in computing the gradient of the ob-jective function for an unconstrained QP optimization at its core. Some of these techniques, for example, the training set decomposition approaches like the Cas-cade SVM are orthogonal to the work we describe, and could be applied to our solver. (Bottou et al., 2007) give an extensive overview of parallel SVM implemen-tations. We implemented the parallel SMO training algorithm because of its relative simplicity, yet high performance and robust convergence characteristics. We approached the SVM classification problem by making use of Map Reduce computations as well as vendor supplied Basic Linear Algebra Subroutines -specifically, the Matrix Matrix Multiplication routine (SGEMM), which calculates C 0 =  X AB +  X C , for matrices A , B , and C and scalars  X  and  X  . For the Linear, Polynomial, and Sigmoid kernels, calcu-lating the classification value involves finding the dot product between all test points and the support vec-tors, which is done through SGEMM. For the Gaus-sian kernel, we use the simple identity || x  X  y || x  X  x + y  X  y  X  2 x  X  y to recast the computation into a Matrix Matrix multiplication, where the SGEMM computes D ij =  X   X  || z i  X  x j || 2 = 2  X  ( z i  X  x j )  X   X  ( z i  X  z for a set of unknown points z and a set of support vec-tors x . We then apply a map reduce computation to combine the computed D values to get the final result. Continuing the Gaussian example, the map function exponentiates D ij element wise, multiplies each col-umn of the resulting matrix by the appropriate y j  X  j . The reduce function sums the rows of the matrix and adds b to obtain the final classification for each data point as given by equation (14). Other kernels require similar Map Reduce calculations to finish the classifi-cation. The SMO implementation on the GPU is compared with LIBSVM , as LIBSVM uses Sequential Minimal Op-timization for SVM training. We used the Gaussian kernel in all of our experiments, since it is widely em-ployed. 6.1. Training We tested the performance of our GPU implementa-tion versus LIBSVM on the datasets detailed in tables 3 and 4.
 The sizes of the datasets are given in table 4. Refer-ences for the datasets used and the (C,  X  ) values used for SVM training are provided in table 3.
 We ran LIBSVM on an Intel Core 2 Duo 2.66 GHz pro-cessor, and gave LIBSVM a cache size of 650 MB, which is larger than our GPU implementation was allowed. CPU-GPU communication overhead was included in the solver runtime, but file I/O time was excluded for both our solver and LIBSVM . Table 5 shows results from our solver. File I/O varies from 1.2 seconds for USPS to about 12 seconds for Forest dataset. The CPU -GPU data transfer overhead was also very low. The time taken to transfer the training data to the GPU and copy the results back was less than 0.6 sec-onds, even for our largest dataset (Forest).
 Since any two solvers give slightly different answers on the same optimization problem, due to the inex-act nature of the optimization process, we show the number of support vectors returned by the two solvers as well as how close the final values of b were for the GPU solver and LIBSVM , which were both run with the same tolerance value  X  = 0 . 001. As shown in the table, the deviation in number of support vectors be-tween the two solvers is less than 2%, and the deviation in the offset b is always less than 0.1%. Our solver pro-vides equivalent accuracy to the LIBSVM solver, which will be shown again in the classification results section. Table 6 contains performance results for the two solvers. We see speedups in all cases from 9  X  to 35  X  . For reference, we have shown results for the solvers using both heuristics statically. Examining the data shows that the adaptive heuristic performs robustly, surpassing or coming close to the performance of the best static heuristic on all benchmarks. 6.2. Classification Results for our classifier are presented in table 8. We achieve 81  X  138  X  speedup over LibSVM on the datasets shown. As with the solver, file I/O times were excluded from overall runtime. File I/O times vary from 0.4 seconds for Adult dataset to about 6 seconds for MNIST dataset. 6.2.1. Optimizations to CPU based classifier LIBSVM classifies data points serially. This effectively precludes data locality optimizations and produces sig-nificant slowdown. It also represents data in a sparse format, which can cause overhead as well.
 To optimize the CPU classifier, we performed the fol-lowing: 1. We changed the data structure used for storing 2. To maximize performance, we used BLAS rou-3. Wherever possible, loops were parallelized (2-way These optimizations improved the classification speed on the CPU by a factor of 3 . 4  X  28 . 3  X  . The speedup numbers for the different datasets are shown in table 8. It should be noted that the GPU version is better than the optimized CPU versions by a factor of 4 . 9  X  23 . 9  X  . For some insight into these results, we note that the op-timized CPU classifier performs best on problems with a large number of input space dimensions, which helps make the SVM classification process compute bound. For problems with a small number of input space di-mensions, the SVM classification process is memory bound, meaning it is limited by memory bandwidth. Since the GPU has much higher memory bandwidth, as noted in section 3, it is even more attractive for such problems.
 We tested the combined SVM training and classifica-tion process for accuracy by using the SVM classifier produced by the GPU solver with the GPU classifi-cation routine, and used the SVM classifier provided by LIBSVM  X  X  solver to perform classification with LIB-SVM . Thus, the accuracy of the classification results presented in table 7 reflect the overall accuracy of the GPU solver and GPU classifier system. The results are identical, which shows that our GPU based SVM system is as accurate as traditional CPU based meth-ods.
 This work has demonstrated the utility of graphics processors for SVM classification and training. Train-ing time is reduced by 9  X  35  X  , and classification time is reduced by 81  X  138  X  compared to LIBSVM , or 5  X  24  X  over our own CPU based SVM classifier. These kinds of performance improvements can change the scope of SVM problems which are routinely solved, increasing the applicability of SVMs to difficult clas-sification problems. For example, training a classifier for an input data set with almost 600000 data points and 50 dimensions takes only 34 minutes on the GPU, compared with over 18 hours on the CPU.
 The GPU is a very low cost way to achieve such high performance: the GeForce 8800 GTX fits into any modern desktop machine, and currently costs $300. Problems which used to require a compute cluster can now be solved on one X  X  own desktop. New machine learning algorithms that can take advantage of this kind of performance, by expressing parallelism widely, will provide compelling benefits on future many-core platforms.
 The authors acknowledge the support of the Gigascale Systems Research Center, one of five research centers funded under the Focus Center Research Program, a Semiconductor Research Corporation program. Bryan Catanzaro is also supported by a National Science Foundation Graduate Research Fellowship. The au-thors thank the anonymous reviewers for their com-ments and suggestions.

