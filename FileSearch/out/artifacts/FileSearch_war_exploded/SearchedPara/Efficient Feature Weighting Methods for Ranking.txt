 Feature weighting or selection is a crucial process to identify an important subset of features from a data set. Removing irrelevant or redundant features can improve the generalization performance of ranking functions in information retrieval. Due to fundamental differences between classification and ranking, feature weighting methods developed for classification cannot be readily applied to feature weighting for ranking. A state of the art feature selection method for ranking, called GAS, has been recently proposed, which exploits importance of each featur e and similarity between every pair of features. However, GAS must compute the similarity scores of all pairs of features, thus it is not scalable for high-dimensional data and its performance degrades on nonlinear ranking functions. This paper proposes novel algorithms, RankWrapper and RankFil-ter , which is scalable for high-dimensional data and also performs reasonably well on nonlinear ranking functions. RankWrapper and RankFilter are designed based on the key idea of Relief algorithm. Relief is a feature selection algorithm for classification ,whichex-ploits the notions of hits (data points within the same class) and misses (data points from different classes) for classification. How-ever, there is no such notion of hits or misses in ranking. The proposed algorithms inst ead utilize the ra nking distances of near-est data points in order to identify the key features for ranking. Our extensive experiments show that RankWrapper and RankFil-ter generate higher accuracy overall than the GAS and traditional Relief algorithms adapted for ranking, and run substantially faster than the GAS on high dimensional data.
 H.m [ Information Systems ]: Miscellaneous Algorithms, Performance corresponding author Feature Weighting, Rank Learning
Feature weighting, or selection, is a crucial process for identi-fying an important subset of features from a data set [5, 10, 12]. The feature weighting methods for classification identify a subset of discriminant features by removing irrelevant features. Remov-ing such features usually improves generalization performance of classification. Identifying key features also helps to interpret the classification model and results.

Feature selection methods for classification typically fall into three categories [10]. (1) Filte r methods: Feature selection is done as a preprocessing step before learning. A filter method computes a score for each feature before lear ning, and thus runs independently of the learning machine [18, 24, 7]. (2) Wrapper methods: Inputs and outputs of a learning machine are used to select features. Thus, the feature selection quality is influenced by the accuracy of the learning machine. However, wrapper methods often generate bet-ter learning performance than filter methods, as they interact with the learning machine. Sensitivity analysis belongs to this category. (3) Embedded methods: Feature selection is performed within the process of learning. RFE (Recursive Feature Elimination), which is embedded in SVM [11], belongs to this category.

Learning ranking functions has recently gained much attention in data mining and machine learni ng communities a nd produced var-ious applications for information retrieval [2, 4, 8, 13, 15, 19, 23, 25, 26]. Unlike classification functions, a ranking function F returns a ranking score of a data point x , such that F ( when x i is ranked higher than x j . Such a global ranking function is learned from either relative orderings or multi-level relevance judg-ments . An example of relative ordering is { ( x 2 x 3 x 1 x 5 x 4 ) } ,where X  X  B X  means  X  X  is preferred to B. X  An example of multi-level relevance judgments is { ( highly relevant x ,x 3 ) , ( partially relevant : x 2 ) , ( not relevant : x rank learning methods have been developed to learn an accurate ranking function from such training data, and RankSVM is one of the earliest rank learning methods.

Feature weighting for ranking is an important task. However, it has not been researched extensively yet. Removing irrelevant fea-tures improves the generalization performance of ranking functions and helps to interpret the ranking function. For example, when learning preference functions from the user X  X  implicit feedback on Web pages [15, 21], feature selection can play crucial roles in iden-tifying key features that expl ain the user X  X  preference. GAS: A filter-based feature selection method for ranking, called GAS, has been recently proposed [9]. The method, however, is not very scalable to the number of dimensions (or features). The complexity of the method is at least quadratic to the number of features, i.e., O ( n 2 ) where n is the number of features, because the method needs to compute the Kendall X  X   X  for every pair of features. GAS also does not fully consider the nonlinear relationships among features.
 RFE: Recursive Feature Elimination (RFE) can also be used for ranking if the ranking function is linear or represented by the sum of weighted features [11]. RFE si mply eliminates those features whose weights are insignificant at each iteration. As long as the function is linear, the RFE strategy can be used for both classifica-tion and ranking functions. However, the RFE cannot be directly applied to nonlinear functions, as the weight of each individual fea-ture is often hidden in nonlinear functions.
 Relief: Relief algorithms have been extensively researched and are considered to be the most successful feature weighting methods for classification [3, 16, 20, 22]. Relief algorithms compute the im-portance of features based on the feature distance of nearest hits (data points within the same class) and nearest misses (data points from different classes). These algorithms are efficient for high-dimensional data (of linear complexity w.r.t. the number of fea-tures) and can correctly estimate the quality of features when there exist nonlinear relationships among the features. However, tradi-tional Relief algorithms cannot be directly applied to ranking data sets, because in ranking, there are no notions of hit or miss .
Based on the key idea of Relief, this paper proposes two efficient and effective feature weighting methods for ranking  X  RankWrap-per and RankFilter .
By designing our algorithms based on the key idea of the Re-lief algorithm, we are able to exploit the inherent advantages of it. Thus, RankWrapper and R ankFilter are scalable to high di-mensional data and can correctly estimate the quality of features when there exist nonlinear relationships among the features. Our extensive experiments show that RankWrapper and RankFilter gen-erate higher accuracy overall than the GAS and Relief algorithms adapted for ranking, and run substantially faster than the GAS on high dimensional data.
This paper is organized as follows. We first overview Relief al-gorithm for classification (Section 2) and present the RankWrapper (Section 3) and RankFilter methods (Section 4). We then present our empirical analysis of the methods on synthetic and real data sets (Section 5). We conclude our study in Section 6.
The Relief family of algorithms identify discriminating features for classification [20]. A training set for classification is denoted as D = { ( x 1 ,y 1 ) , ... , ( x m ,y m ) } ,where x i is a data point, and y the class label of x i .

The key idea of Relief is to iteratively estimate the feature weights according to their ability to discriminate betw een neighboring data points. In each iteration, for a random data point x , two nearest neighbors of x are found, one from the same class (termed the near-est hit or NH ) and the other from the different class (termed the nearest miss or NM ). When two neighboring data points from the same class have different values of some attributes, the attributes separate the two data points, which is not desirable, thus the at-tributes are not discriminant. On the contrary, when two neighbor-ing data points from different classes have different values of some attributes, those attributes separate the two data points, which is desirable, thus the attributes are discriminant.

Based on this intuition, Relief updates the weight of the i ture by the following formula. x ( i ) is the i th feature of a data point x ,and NM ( i ) ( NH ( i ) ( x ) are the i th features of the nearest miss and nearest hit of x respectively. All the features are updated based on the feature distance between x and its nearest miss and hit, and this process is repeated on some training points x . After some iterations, the weight vector w will have high values on discriminant attributes only.
 Algorithm 1 Relief Input: m training vectors of n attributes and the class label for Output: the weight vector w (= &lt;w 1 ...w n &gt; )
Algorithm 1 shows the Relief algorithm [20, 17]. Relief finds the nearest hit and nearest miss of a point x . It decreases the weights that are discriminant between the point x and the nearest hit, and in-creases the weights that are discriminant between x and the nearest miss.

Note that, even when there are nonlinear relationships among the features, Relief correctly identifies high quality features. For ex-ample, suppose features x (3) , x (4) ,and x (5) play crucial roles only when they appear together. Then, although their individual impor-tance is low, Relief will eventually increase the weights for them (i.e., w 3 , w 4 ,and w 5 ), because, if the three features are discrimi-nant only together, there will be some NM having different values in the three features together, whi ch will increase the weights of the features according to the weight update rule of Relief.
This section presents RankWrapper algorithm which computes weights of features when the training data is given as a set of rela-tive orderings. The Relief algorithm cannot be used for such data because the information about classes or absolute relevance levels is missing, and thus there are no notions of hit or miss of a data point in the training set. To compute feature weights from a set of relative orderings, RankWrapper utilizes the information about ranking distances among data points instead of the absolute rele-vance levels of data points.

RankWrapper, as a wrapper method, first uses a rank learning method (e.g., RankSVM) to generate a set of data points and their ranking scores R = { ( x 1 ,y 1 ) , ..., ( x m ,y m ) } where y ing score of x such that y i &gt;y j when x i x j .

P ROPERTY 1. Let NP be the nearest points or the points close to x in the feature space . Then, among the NP , the points rela-tively far from x in the ranking scores contain more disparate and crucial features for ranking than the points relatively close to x in the ranking scores .

R ATIONALE 1. Since the NP are close to x in the feature space, their feature values are likely to be similar to each other. If some of the NP are relatively far from x in the ranking scores, they must have some features containing disparate values from those of x , and those features must play crucial roles in making their ranking distant from x .

P ROPERTY 2. Among the nearest points NP in the feature space, if the points relatively close to x in the ranking scores contain dis-parate feature values from x , those features are unimportant for ranking.
 The rationale of Property 2 is straightforward.

Based on Properties 1 and 2, we introduce new notions of me-dian ranking distance and rank significance . The median ranking distance of x is the ranking score distance between x and the me-dian point in a sequence of the NP ordered by the ranking distance from x . For example, consider the data points in a two-dimensional space in Figure 1 and a linear ranking function f where the rank-ing score of each data point is determined based on the projection onto f . Among the five nearest points x 1 , ..., x 5 that are close to p in the feature space, the points closest and farthest to p in ranking are x 3 and x 4 respectively. The median point is x 2 and the median ranking distance in this example is  X  1 that is the ranking distance of x 2 from p .

The rank significance , denoted as rsignificance , is a function re-turning a positive or negative magnitude so as to increase or de-crease the weights of corresponding features, depending on the ranking distance from x compared to the median ranking distance. Specifically, the feature weight is updated according to the feature difference between x and a nearest point np  X  NP multiplied by the rank significance as follows. w i = w i + rsignificance ( medianRankDist , rankDist ( x, np
Note that, the NH of the Relief corresponds to the data points relatively close to x in ranking, thus the rank significance returns a negative magnitude for the data points whose ranking distance from x is smaller than the median ranking distance in order to decrease the corresponding feature weights. Likewise, the rank significance returns a positive magnitude for the data points whose ranking dis-tance from x is larger than the median ranking distance. Thus, the rank significance is defined as follows. rsignificance ( medianRankDist , rankDist )= rankDist
To illustrate the effect of the rank significance function, consider the example of Figure 1. Points x 1 and x 4 are located farther from p than from the median point according to f , and their attribute values are disparate from p mainly in dimension d 1 . Thus, the rank significance returns a positive magnitude to increase the weight of d . On the other hand, points x 3 and x 5 are closer to p than to the median point according to f , and their attribute values are differ-ent from p mainly in dimension d 2 . Thus, the weight of d be decreased by the rank significance. In this example, dimension d 1 will be determined to be more important than d 2 for ranking function f .

Next, the distance between x and np in the feature space must also be considered in the weight update because closer np have greater influence on the weight update than farther np .Itisbe-cause closer data points have smaller feature differences overall, and thus, the same feature difference in the closer points becomes more meaningful than in the farther points. Accordingly we design the following function  X  X im X , which returns a magnitude of the sim-ilarity (opposite to distance) between x and np in the feature space. Note that similar functions were also proposed in [20].  X  X im_un( x , np ) X  returns an unnormalized similarity in which the influence of np is exponentially decreased with the distance from x . X  X iff( x , np ) X  is the difference between np and x mea-sured by the rank of np in a sequence of data points ordered by the distance from x .  X  is a user defined parameter controlling the in-fluence of the distance. The reason for using the difference instead of actual distance is to assure that the nearest data point always has the same impact on the weights.

Therefore, the final weight update formula becomes w i = w i + rsignificance ( medianRankDist , rankDist ( x, np
RankWrapper algorithm is described in Algorithm 2. RankWrap-per first finds k nearest points of a data point x (line 4), sorts the nearest points according to their ranking distances from x , and finds the ranking distance between x and the median point ( X  X edian-RankDist X  in line 5). k is not a sensitive parameter but desirable to set to an odd value since the rank significance is computed based on the median point. ( k is safely set to 5 in our experiments.) The median ranking distances are used inside the function rsignificance in line 8. For each nearest point np , weights are updated according to the equation of line 8.
 The complexity of the RankWrapper is the same as that of the Relief algorithm, which is at most O ( nm 2 ) where n and m are the numbers of features and data points respectively. It is possible to reduce it further using the same techniques used for the Relief algorithm such as k-d trees [20]. Moreover, the outer loop does not have to run for every data point in the training set. Instead, it is still Algorithm 2 RankWrapper Input: a set of relative ordering Output: the weight vector w (= &lt;w 1 ...w n &gt; ) 1: Compute a set R = { ( x 1 ,y 1 ) , ..., ( x m ,y m ) } 2: Set all weights w =0; 3: for each data point x do 4: Select k nearest points NP in the feature space; 5: Set medianRankDist = median ranking distance between x 6: for each np  X  NP do 7: for each attribute j do 8: w j = w j + rsignificance(medianRankDist, rankDist( x , 9: end for 10: end for 11: end for effective with a constant number of iterations especially when the data size is large. The complexity then becomes O ( nm ) .
Note that our feature weighting task becomes similar to the fea-ture weighting for regression since regression task also learns from asetof ( x i ,y i ) pairs where y i is a real value. We will thus compare our RankWrapper with RRelief (Relief for regression [20]) as well as the GAS in our experiments in Section 5.
RankFilter computes weights of features when the training data is given as multi-level relevance judgments. When ranking involves multi-level relevance judgments, the training data is similar to that of multiclass classification in the sense that each data point belongs to one of the given categories or relevance levels.

In ranking, however, there is a notion of distance among the categories while the multiclass classification does not have such notion. For example, the distance between  X  X ighly relevant X  and  X  X artially relevant X  categories is closer than that between  X  X ighly relevant X  and  X  X ot relevant X  categories. Thus, when weights are updated based on the nearest misses in the Relief algorithm, the contribution of each miss must be weighted according to its rank-ing distance from the data point x . Specifically, the misses from the relevance levels that are farther from the relevance level of x must have a greater influence than the misses from closer levels.
Thus, the RankFilter X  X  weight update rule becomes: w diff ( C, x, N M ) returns the ranking difference between x and NM . We logarithmically increases the impact of the ranking dif-ference such that:
The contribution of each miss is also weighted according to the prior probability P ( C NM ) estimated from the training set. Since the P ( C NM ) excludes the relevance level of hits, we divide it by the sum of probabilities for the misses X  relevance levels, i.e., P ( Algorithm 3 RankFilter Input: m training vectors of n attributes and the relevance judg-Output: the weight vector w (= &lt;w 1 ...w n &gt; ) 1: set all weights w := 0; 2: for each data point x do 3: find k nearest hit NH ; 4: // let C x be the relevance level of x ; 5: for each relevance level C = C x do 6: from relevance level C ,find k nearest misses NM ( x ) 7: end for 8: for j =1 ...n do 9: w j = w j + 10: w j = w j  X  11: end for 12: end for Algorithm 3 shows the RankFilter algorithm. RankFilter finds k NH , i.e., nearest neighbors from the same relevance level, and k NM , i.e., nearest neighbors from each of the different relevance level. It increases the weights that are disparate between x and the NM (line 9) and decreases the weights that are disparate between the point x and the NH (line 10). The contributions of the hits and misses are averaged. Selection of k hits and misses ensures greater robustness of the algorithm concerning noise. The time complexity of the RankFilter is the same as that of the RankWrapper as well as the Relief algorithms, i.e., O ( nm cussed at the end of Section 3).
We experimentally evaluate our methods on synthetic datasets (Section 5.2) and real-world datasets (Secton 5.3), comparing with the GAS [9], ReliefF (a Relief-based method for multiclass classi-fication), and RRelief (a Relief-based method for regression) [20].
To evaluate the qualities of feat ure weighting methods, we select highly weighted features after pe rforming each feature weighting method and compute the ranking accuracy using only the selected features.

We use NDCG and Kendall X  X   X  as evaluation measures. NDCG is used when the ranking labels are given as multi-level relevance judgments, and Kendall X  X   X  is used when there is a desirable global ordering of data R  X  . NDCG is popularly used for IR applications where ranking on top results is more important than that on bottom results [4, 2, 19, 23], and Kendall X  X   X  isfavorablyusedtomea-sure the overall accuracy based on the number of correctly ranked pairs [15, 21, 25]. There are other measures for evaluating the rank-ing accuracy such as AUC (Area Under the Curve) and MAP (Mean Average Precision). They are used when there are two levels of ranking. Descriptions of NDCG and Kendall X  X   X  follow.
 Kendall X  X   X  : Let R  X  be the optimal ranking of data in which the data is ordered perfectly accordin g to the user X  X  preference. A rank-ing function F is evaluated by how closely its ordering R imates R  X  . Kendall X  X   X  has been a widely used measure for sim-ilarity between two orderings R  X  and R F [15, 9]. For two strict orderings R a and R b , Kendall X  X   X  is defined based on the number Q of discordant pairs. If R  X  and R F agree in how they order a pair, x and x j , the pair is concordant, otherwise, it is discordant. For or-dering R  X  and R F on a dataset D , we define the similarity function  X  as the following, which is equivalent to that defined in [9]:
To illustrate, suppose R  X  and R F order five vectors x 1 as follow:
In this example,  X  ( R  X  ,R F ) is computed as 0 . 7 , as the number of discordant pairs is 3, i.e., { x 1 ,x 2 } , { x 1 ,x 3 remaining 7 pairs are concordant.
 Normalized Discount Cumulative Gain (NDCG): NDCG focuses on the accuracy on the top results rather than on the bottom results. For example, when users rated movies from 1 (meaning  X  X errible X ) to 5 (meaning  X  X xcellent X ), the learning machine learned a ranking function from the training data and returned top n results. Then, NDCG score is computed as follows.
 where j is the position from the top, R ( j ) is the rating of the j  X  X  movie. Z n is a normalization factor to guarantee that the NDCG score of a perfect ranking is equals to 1. As j increases or as the returned movie becomes farther from the top, its impact on the NDCG score decreases logarithmically. Data Generation: The following is the procedure of our synthetic data generation. 1. We randomly generated a data set D ,where D contains 100 2. We randomly generated a global ranking function F  X  ,by 3. We ranked the data points in D according to F  X  .Wethen
Let R  X  be the D ranked according to F  X  . R  X  was used for eval-uating RankWrapper, RRelief, and GAS. We then equally divided R  X  into five levels and used the five levels of data for evaluating RankFilter, ReliefF, and GAS.
We evaluate the accuracy of each feature selection method by investigating how well each feature selection method maximizes the ranking performance. We used RankSVM with RBF kernels and also Committee Perceptron as the learning method. Committee Perceptron is a recently develope d Ensemble-based rank learning method [6]. At each iteration, we run the following procedure. 1. We run each feature selection method on the synthetic data 2. We train a ranking function from the rest of the features for Ranking from Relative Ordering: Figure 2(a) and (b) reports the Kendall X  X   X  of RankWrapper, RRelief, and GAS at each iteration for Committee Perceptron and RankSVM respectively. The meth-ods run as wrapper methods learning from partial orderings of R (thus Kendall X  X   X  is used here). That is, R  X  is randomly divided into three groups (for 3-fold cross validation), two groups of or-derings are used to select the highly weighted features using each method, and the other group is used to evaluate the quality of se-lected features by computing the Kendall X  X   X  of the rank learning methods on the selected features. The results are averaged over 30 runs. The highest accuracies are achieved by RankWrapper. The GAS X  X  performance is lower than the others overall. It is because the data is generated using the nonlinear function with the RBF kernel, but the GAS only considers the similarity of every pair of features to capture the nonlinearity of features. (a) Committee Perceptron Figure 2: Accuracy. X-axis: # of features; Y-axis: NDCG@10 Ranking from Multi-Level Relevance Judgments: Figure 3(a) and (b) reports the NDCG@10 of the RankFilter, ReliefF, and GAS at each iteration for Committee Perceptron and RankSVM respec-tively. The methods run as filter methods learning from five levels of absolute relevance judgments (thus NDCG is used here). 3-fold cross validations are used and the results are averaged over 30 runs. The highest accuracies are also achieved by RankFilter. The GAS X  X  performance is lower than the others for the same reason. (a) Committee Perceptron
Figure 3: Accuracy. X-axis: # of features; Y-axis: NDCG@10
We compare the execution times of the filter methods on varying size of features and data points. These experiments are done on a dual quadcore machine with 2660MHz and 48G memory.

We first fixed the number of data points into 100, varied the num-ber of features from 10 to 100, and examined the time for each method to compute the weights of the features from the divided R . Figure 4 shows the execution times of the three filter methods. As the number of features increases, the execution time of the GAS increases quadratically while the other filter methods increase lin-early. As discussed in Section 1.2, the complexity of GAS is at least quadratic to the number of features because it needs to compute the Kendall X  X   X  for every pair of features.

Next, we fixed the number of features to 100 and varied the num-ber of data points from 10 to 100. Figures 5(a) and (b) show the execution times of the filter methods in different scales. The exe-cution time of the GAS increases much more rapidly than the other methods.

Note that the scalab ilities of wrappe r methods, i.e., RankWrap-per and RRelief, will be highly dependent on the scalability of the learning method, because wrapper methods need to execute a rank learning method first before computing the weights. If RankSVM is used as the learning method, the scalabilities o f RankWrapper and RRelief will be better than that of GAS w.r.t. the number of Figure 4: Execution Time. X-axis: # of features; Y-axis: run-time (sec.) Figure 5: Execution Time. X-axis: # of data points; Y-axis: runtime (sec.) features but worse w.r.t. the number of data points, because the complexity of RankSVM is linear w.r.t. the number of features but O ( m 4 ) where m is the number of data points. If scalable rank learning methods such as AdaR ank [23] or Committee Perceptron [6] are used, the scal abilities of the wr apper methods will improve w.r.t. the number of data points. In this section, we evaluate the accuracy of each method on the OHSUMED data set. The OHSUMED data set consists of 348,566 documents and 106 queries [14]. In total, there are 16,140 query-document pairs on which relevance judgments are made. The rel-evance judgments are either  X  X  X  (definitely relevant),  X  X  X  (partially relevant), or  X  X  X  (not relevant). The data has been used in many experiments in IR [23].
We first use the OHSUMED data set from the LETOR site [1] (Section 5.3.1). Each data point (query-document pair) in the data set is represented by 45 features. Detailed information about the data set can be found from the site. We used the first 45 queries of documents to run a 3-fold cross validation, using 30 queries for learning and the other 15 queries for testing and repeating this three times.
 We used two rank learning methods for this experiment  X  (1) RankSVM with RBF kernel and (2) Committee Perceptron [6] that is a recently developed Ensemble-based rank learning method. We evaluate the accuracy of the feature selection methods (1) from rel-ative ordering and also (2) from m ulti-level relevance judgments. Ranking from Relative Ordering: To evaluate the Kendall X  X   X  on the data set, we learned a ranking function F  X  using the RankSVM from the training set and constructed a global ranking R  X  query of documents. Given R  X  , we evaluated RankWrapper, RRe-lief, and GAS using the 3-fold cross validation: at each iteration, (1) from the training set, we removed three features of the lowest weights using each feature selection method, (2) learned ranking functions F using RankSVM and Committee Perceptron from the training set, and (3) tested the accuracy of F on the testing set. This process was repeated until we consume all the features. The results were averaged over three runs by the 3-fold cross validation. Figure 6: Accuracy. X-axis: # of features; Y-axis: Kendall X  X   X  ; CP: Committee Perceptron
Figure 6 shows the Kendall X  X   X  of the six methods at each num-ber of features. The accuracies of the methods fluct uate a little, but the RankWrapper with Committee Perceptron shows the highest accuracy overall.
 Ranking from Multi-Level Relevance Judgments: Here we eval-uate RankFilter, ReliefF, and GAS using NDCG@10. We directly used the three levels of relevance given in the LETOR as the rank-ing labels. We used the same 3-fold cross validation method  X  30 queries for training and the other 15 queries for testing  X  and also ran RankSVM and Committee Perceptron: Using each feature se-lection method, at each iteration we eliminated the lowest weighted three features and estimated NDCG@10 of the RankSVM and Com-mittee Perceptron functions using the 3-fold cross validation.
While RankSVM and Committee Perceptron show comparable accuracies for the relative ordering, RankSVM showed much higher accuracy than Committee Perceptron in this experiment. Thus, we show the accuracies of the two learning methods in different graphs. Figures 7(a) and (b) show the results of the three methods with Committee Perceptron and RankSVM respectively.
 The highest accuracies were achieved by RankFilter in both graphs. The accuracy differences between RankFilter and ReliefF were, however, not as significant as those in the experiments on synthetic data set. It is because the synthetic data contains five levels of rel-evances while the LETOR data set only involves three levels of relevances. Note that, as the number of relevance levels decreases, the algorithms RankFilter and ReliefF become equivalent. This section compares the performance of RankWrapper and RRelief on high dimensional data. The GAS is excluded from this experiment and it is not scalable enough to run on high dimen-sional data. RankWrapper and RRelief are only plausible solutions for nonlinear feature selection from relative ordering on high di-mensional data.

We preprocessed the OHSUMED documents and extracted fea-tures by running stopwords, word stemming, and computing TFIDF. A feature is a TFIDF value for each word. We preprocessed the documents for each query independently. The number of features
Figure 7: Accuracy. X-axis: # of features; Y-axis: NDCG@10 extracted for each query varied from one to three thousands.
We used the first twelve queries (i.e., query id = 1 to 12) in this experiment excluding a few queries containing too few documents (i.e., query id = 8 and 10). Table 1 shows the number of documents and extracted features for each query. Table 1: The number of documents and extracted features for each query.

For each query of documents, we learned a ranking function F using the RankSVM and constructed a global ranking R  X  .The wrapper methods are run and evaluated based on the R  X  . Assume the R  X  is a m -by-n matrix, i.e., m data points of n -dimensions, and the m data points are ordered by the hidden function F ing each feature selection method, we removed a half of lowly weighted dimensions (or features) at each iteration and repeated this until we have one dimension left. Let R  X  ( i ) be a m -by-n matrix after removing lowly weighted dimensions at i th iteration, each feature selection method is evaluated by learning a RankSVM function on R  X  ( i ) and evaluating the accuracy of the function.
For evaluation, we run 3-fold cross validations. At each itera-tion, we tuned the soft margin parameter C and the kernel param-eters, i.e., degree p for polynomial and  X  for RBF, by trying C and reported the highest accuracy. We experimented with polyno-mial and RBF kernels, but since the linear kernel is equivalent to the polynomial kernel with degree p =1 ,weendeduptryinglin-ear, polynomial, and RBF kernels in this experiment.

Figure 8 show the feature selection results. The highest perfor-mances are achieved by RankWrapper for most queries except for qid = 3 and 9 which show the highest performances by RRelief and RBF kernels. Note that the X-axis is logarithmically scaled, thus in most cases the highest performances are achieved when a sub-stantial amount of features are removed. For example, the highest performances for qid = 1 and 2 are achieved when a half of the features are removed (by RankWrapper and RBF kernel for qid = 1, and by RankWrapper and polynomial kernel for qid = 2). For qid = 5, the highest performance is achieved by RankWrapper and polynomial kernel when 87.5% of features are removed.
This paper proposes efficient and effective feature weighting meth-ods for ranking, i.e., RankWrapper and RankFilter .Themeth-ods compute the feature weights that maximize the ranking perfor-mance. RankWrapper, as a wrapper method, computes the weights from a set of relative orderings; it uses a rank learning method to precompute the ranking scores of data points and uses the scores for computing the featur e weights. RankFilter, as a filter method, com-putes the weights from multi-leve l relevance judgments indepen-dently of rank learning methods. Based on the similar idea of Relief for classification, RankWrapper and RankFilter update the feature weights based on the ranking distance of points that are close to each other in the feature space. When two close points have a large ranking distance, the disparate features of the two points are impor-tant for discriminating the ranking of the points. On the other hand, when two close points have a small ranking distance, the disparate features of the points are unimportant for ranking. Based on this intuition, RankWrapper and RankFilter efficiently and effectively identify key features for ranking even when there exist nonlinear relationships among the features. We experimentally confirm that our proposing methods generate higher accuracy overall than the state-of-the-art feature weighting methods and are more scalable to high dimensional data. [1] Letor: Learning to rank for information retrieval. [2] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, [3] B. Cao, D. Shen, J.-T. Sun, Q. Yang, and Z. Chen. Feature [4] Y.Cao,J.Xu,T.-Y.Liu,H.Li,Y.Huang,andH.-W.Hon.
 [5] M. Dash and H. Liu. Feature selection for classification. [6] J. Elsas, V. Carvalho, and J. Carbonell. Fast learning of [7] G. Forman. An extensive empirical study of feature selection [8] Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. An efficient [9] X. Geng, T. Liu, T. Qin, and H. Li. Feature selection for [10] I. Guyon and A. Elisseeff. An introduction to variable and [11] I. Guyon, J. Weston, S. Ba rnhill, and V. Vapnik. Gene [12] M. A. Hall and G. Holmes. Benchmarking attribute selection [13] R. Herbrich, T. Graepel, and K. Obermayer, editors. Large [14] W. Hersh, C. Buckley, T. Leone, and D. Hickam.
 [15] T. Joachims. Optimizing search engines using clickthrough [16] K. Kira and L. A. Rendell. A practical approach to feature [17] I. Kononenko. Estimating attributes: analysis and extensions [18] D. Mladenic and M. Grobelnik. Feature selection for [19] T. Qin, T.-Y. Liu, W. Lai, X.-D. Zhang, D.-S. Wang, and [20] M. R.-Sikonja and I. Kononenko. Theoretical and empirical [21] F. Radlinski and T. Joachims. Query chains: Learning to rank [22] Y. Sun and J. Li. Iterative RELIEF for feature weighting. In [23] J. Xu and H. Li. Adarank: A boosting algorithm for [24] M. Yang and J. O. Pedersen. A comparative study on feature [25] H. Yu. SVM selective sampling for ranking with application [26] H. Yu, S.-W. Hwang, and K. C.-C. Chang. Enabling soft
