 We propose a weakly-supervised approach for extracting class attributes from structured text available within Web documents. The overall precision of the extracted attribut es is around 30% higher than with previous methods operating on Web documents. In addition to attribute extraction, this approach also automatically identifies values for a subset o f the extracted class attributes.
 H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing; I.2.7 [ Artificial Intelligence ]: Nat-ural Language Processing; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Experimentation Weakly-supervised information extraction, class attribu te extraction, knowledge acquisition, structured text colle c-tions
Information extraction systems typically exploit unstruc -tured text within Web documents to acquire data for var-ious applications. Despite the unstructured nature of doc-uments available on the Web, some inherent structure of-ten exists within the text. For example, a website listing telephone directory information contains recognizable fie lds such as a person X  X  name and phone number . Web documents from encyclopedic resources such as Wikipedia [13] pertain -ing to specific entities (e.g., Italy ) contain emphasized fields within the text (e.g., capital, official languages, population,  X  Contributions made during an internship at Google. culture, etc.) that signal important information regarding the entity. If the same set of fields appear within many documents related to multiple instances (e.g., Italy, France, Japan, etc.) representing the same class (e.g., Country ), then these fields may be important properties of the particu-lar class, and they may be potential candidates for attribut e extraction. Sometimes, the text may also contain field tu-ples (e.g., h capital,Rome i for Italy ), in which case they may provide information for extracting the value (e.g., Rome ) of an attribute (e.g., capital ) of a particular instance (e.g., Italy ) [16].

Existing extraction systems use specific hand-crafted pat-terns [8] or schemas [6] to discover relational instances or structured tuples like h class, attribute i pairs from unstruc-tured text. Most of these approaches are constrained to specific domains or particular target relations [3] specifie d in advance, and therefore do not gracefully scale to more generic classes and applications for other domains. A previ -ous approach for attribute extraction from Web document collections looks at the immediate textual vicinity of in-stances (e.g., Austria ) from a target class (e.g., Country ) to identify potential attributes of the class [10]. However , the attributes that can be extracted using this technique are limited to specific types, following certain patterns (e .g., X-of-Y patterns [15]).

Sources such as Wikipedia or the CIA FactBook consti-tute appealing resources for obtaining documents related t o specific instances of a class, and these often contain more than just attribute information. Some of these documents may provide additional information, such as relations be-tween different entities associated with the particular in-stance. For example, the Wikipedia document pertaining to Austria also specifies that its capital is Vienna . Together, the pair h capital, Vienna i constitutes a piece of factual infor-mation for the instance Austria of the class Country . If we can find many such matching tuples for various instances of the class, then this would help us in mining facts related to the class on a large scale. But open-domain value extraction is a difficult problem, especially from noisy data like that available on the Web. There have been approaches in the past [3, 2] for extracting factual relations between object s, but many of them are geared towards extracting facts that correspond to answers to factual questions (e.g.,  X  X hat is the capital of Austria? X  ), often following hand-crafted pat-terns. For any attribute extracted from a Web document relevant to a specific class instance, it is difficult to predic t whether the source document also contains a matching value for the particular attribute. Even if the document contains Figure 1: Weakly-Supervised Attribute Extraction Algo-rithm using Structured Text this information, finding the actual value within the text is non-trivial.

In this paper, we present a new approach for extracting attributes from Web documents, making use of the existing structure (HTML tag hierarchy) within text. This method scales to a large number of classes, and the attribute ex-traction is also not restricted to any pre-determined pat-tern types (e.g., X-of-Y patterns). Instead, we automatica lly learn a wide variety of patterns for certain seed attributes of the target class provided as input, and use these to ex-tract other attributes of the same class. The use of struc-tured text for attribute extraction has another advantage: the weakly supervised algorithm used for attribute extrac-tion can be extended to also perform value extraction, thus yielding h attribute,value i tuples for various class instances.
The weakly supervised attribute extraction algorithm used in our approach is shown in Figure 1. Given a target class C , the algorithm retrieves documents from the Web that are relevant to instances representative of the class in Ste ps 1-2, then selects candidate attributes (in Steps 3-7) and co r-responding patterns (in Steps 8-9) from the retrieved doc-uments. Steps 10-14 are aimed at extracting patterns cor-Figure 2: Fetching Web documents relevant to attribute ex-traction for a given class responding to a small set of seed attributes (5 known at-tributes per class) which guide the extraction of other at-tributes from the same class. The algorithm finally ranks the selected candidate attributes for the particular class (St eps 15-18).
Each target class C is available as a set of representa-tive entities or instances { I } . A large collection of instances (e.g., Honda Accord, Audi A4, Mini Cooper, Ford Mustang etc.) sharing the characteristic properties of a particula r class (e.g., CarModel ) can be mined from text collections available on the Web using various techniques that have been explored before [1, 14]. Given a particular class, a general -purpose search engine retrieves Web documents that are rel-evant to instances from the given class. In Steps 1-2 of the algorithm from Figure 1, each instance phrase I from the set representing the target class C is provided as a search query, and the top N documents returned for the query are fetched locally. Since we wish to exploit the structure within the document text, we limit our extraction to only documents containing HTML tag information; documents with exten-sions such as .pdf, .doc, .txt , etc. are ignored. Figure 2 illustrates via an example how relevant documents are col-lected from the Web to perform attribute extraction for a particular target class ( Actor ).
Steps 3 and 4 in Figure 1 scan the retrieved documents and collect a pool of candidate phrases that could be po-tential attributes of the given class. For this purpose, we identify fields that are recognizable within a particular do c-ument, utilizing the structure information available in th e form of HTML tags. For example, a document related to a particular entity (e.g., Oracle ) from the given class (e.g., Company ) might contain certain phrases (such as ceo, stock price, headquarters, etc. ) which convey important infor-mation regarding the particular entity. These phrases are usually emphasized within the structured text using certai n extract such emphasized phrases from each document and add them to our pool of candidate attributes { P } . Fig-ure 3 shows how the candidates headquarters and net in-come (phrases enclosed within HTML tags &lt;th&gt; ... &lt;/th&gt; ), are extracted from a Wikipedia document related to Intel , which is an instance of the class Company .
 Filtering Out Spurious Candidates: Due to the am-biguous nature of some of the instance phrases, some of the retrieved documents may be irrelevant. For example, the instance Chicago is associated with multiple classes ( City and Movie ), and therefore attributes relevant for the class Movie might be added to the candidate pool for the class City , and vice versa. To avoid extracting a lot of such un-desirable attributes from random documents, Step 6 filters out documents that do not contain fields matching at least one of the seed attributes in { K } . We also restrict our can-didate selection to relatively short phrases, i.e., contai ning five words or less.
 Extracting Hierarchical Patterns: For every candidate phrase generated, Step 8 of the algorithm from Figure 1 extracts the corresponding hierarchical tag pattern assoc i-ated with it from the document. There have been many approaches in the past which utilize the HTML structure within documents for information extraction, but most of these approaches tend to focus only on specific elements like HTML tables within Web documents [4]. For a particular candidate P selected from document D , its hierarchical tag structure is composed of the HTML elements correspond-ing to its tag, parent tag, grandparent tag, and so on (all the way to the top of the document, within 10 previous tag levels). This tag structure along with other informa-tion (domain name of the website from which D was re-trieved) constitutes a pattern. Figure 3 illustrates how th e pattern corresponding to the candidate headquarters is ex-tracted from a Wikipedia document for the instance Intel . All such patterns extracted for candidate P are then col-lected into a pattern vector V P (Step 9 in Figure 1). The pattern vector represents the fingerprint or signature for a particular candidate phrase, and overlap between different pattern vectors indicates that candidates corresponding t o these vectors share common patterns.
Every candidate extracted for an instance I represents a potential attribute of the particular class C to which I belongs. In Figure 3, the candidates headquarters and net income extracted for the instance Intel are attributes of the class Company . The selection process builds a large pool of such candidate attributes for each target class. Ranking th e attributes within the candidate pool can be done in multiple ways:  X  Frequency-based ranking: One method is to rank the selected candidates for a particular class, by frequency [1 0] as shown in Equation 1. Each candidate P extracted for a class C is assigned a score based on the number of instances I from C that produced P . A candidate that is produced by more instances from the same class, receives a higher score.
SCORE freq ( P ) = | I : (( I  X  C )  X  ( I produces P )) |  X  Hierarchical pattern-based ranking: Pattern vectors V P are populated for every phrase P in the candidate pool { P } , for a particular class C . The individual patterns within each vector are weighted elements (the frequency of occurrence within documents in the collection). Patterns from vectors corresponding to the candidate phrases that match any of the seed attributes (e.g., headquarters ) are then aggregated in Steps 10-14 (Figure 1) to form a reference vector V ref the class (e.g., Company ). Similarly to [9], the relevance of each candidate attribute ( P ) for the class is then computed as a similarity score of its pattern vector V P with respect to the reference vector ( V ref ) for the class (Steps 15-17 in Figure 1). The similarity score is obtained by computing the Jaccard coefficient between the two vectors as shown in Equation 2. A higher similarity score indicates that it is more likely for patterns within V P to match those of the seed attribute patterns, which in turn suggests that the candida te phrase P could potentially be a good attribute for the given class.

The framework described in Figure 1 (and illustrated via an example in Figure 3) is used to build two separate systems for attribute extraction: (1) using frequency-based ranki ng (which is the baseline for our comparison), and (2) using hierarchical pattern-based ranking. All the previous step s in attribute extraction (including candidate selection) a re the same for both these systems. In addition to all the steps mentioned, the candidates may also pass through some post-processing steps to obtain the final ranked list of class at-tributes. Attributes that are added simultaneously to the candidate pools for multiple classes, are less useful in ide nti-fying characteristic properties that are unique to a partic ular class. These may include generic attributes like history, def-inition, etc. or phrases that commonly occur in boilerplate text within Web documents (e.g., contents, contact informa-tion, help, etc.). Consequently, if a candidate is extracted simultaneously for more than half the classes from the targe t class set, the candidate is removed from consideration.
The approach presented above is generic and can be used to extract attributes for a large number of classes, with min -imal supervision. We propose to leverage the attribute ex-traction framework (described in Section 2.1) to perform value extraction, using a simple extension. For a particu-lar document D that was retrieved for class instance I , we find the most frequent pattern (HTML hierarchical tag pat-tern within the pattern vector V P ) corresponding to every attribute P that was extracted from D . This information is subsequently used for extracting matching values for P . For many class instances, structured text retrieved from en -cyclopedic sources contains many recognizable field tuples such as h attribute,value i pairs associated with the particu-lar instance or entity described in the document. Usually, the fields constituting each such tuple occur in the vicinity of one other within the document. Therefore, documents containing attributes as fields might also contain potentia l matching values in nearby context.

Given an attribute P , the document D from which it was extracted, and the corresponding instance I  X  C , the value extraction process collects the text V AL P (from the tag ele-ment) immediately following the hierarchical tag pattern f or P inside D , and returns the triplet h I | P  X  V AL P i as fac-tual information for the class instance ( C : I ). In Figure 3, for the instance Intel of the class Company , the text  X  X anta Clara, California X  is matched as the value corresponding to the candidate attribute headquarters , by first locating the position of the attribute using the hierarchical pattern, a nd then selecting the immediately following element (in this case, h td i ... h /td i ). Using this integrated approach, we can inexpensively identify matching values for some of the cor-rectly identified attributes, if this information is presen t in the document. Some example values extracted in this man-ner are shown below.

Country: h Austria | population  X  2007 estimate Target Classes: A total of 40 target classes [9] are pro-vided as input in the form of sets of instances { I } in the experiments. Table 1 illustrates target classes and exampl e instances used in the attribute extraction experiments. Th e number of instances per class varies from 25 (for SearchEngine ) to 1500 (for Actor ). The classes also vary widely with re-spect to the domain or genre (e.g., Entertainment for Movie , Championship Leagues for SportEvent ) and types of instances (e.g., Boeing or Airbus for AircraftModel ) comprising the particular class.
 Seed Attributes: As part of the weak supervision required for this approach, we provide a small set of 5 seed (known) attributes per class. These attributes are chosen indepen-dently, and are provided as input at the beginning of at-tribute extraction. An example of a seed attribute set is { headquarters, stock price, ceo, location, chairman } for the class Company .
 Evaluation Methodology: Regardless of the differences in methodologies used for attribute extraction, all experi -ments produce a ranked list of candidate attributes for each of the 40 target classes. Multiple lists of attributes (one l ist per class) from all experiments are evaluated in the same manner using precision as the metric. To avoid any unde-sirable bias towards higher ranked attributes during evalu -ation, each list is sorted alphabetically into a merged list . Each attribute in this merged list is assigned a correctness label by a human judge, as shown in Table 2.

The strategy for assigning correctness labels is similar to the assessment method used in previous work [10]. An attribute is  X  X ital X  if it must be present in an ideal list of attributes for the target class;  X  X kay X  if it provides useful but non-essential information; and  X  X rong X  if it is incorrect. Table 2: Correctness labels for attribute quality assessme nt All attributes extracted for each of the 40 target classes, are assigned correctness labels. These correctness labels are then mapped to quantitative values as shown in Table 2. The overall precision score at some rank N for a particular class C is then computed as the sum of the correctness val-ues of the first N candidates in the ranked list of attributes extracted for C , divided by N.
 Parameter Settings: There are a few parameters in the attribute extraction pipeline that can be used to tweak dif-ferent aspects of the system. Varying one or more of these parameters may have a bearing on the final results (i.e., clas s attributes) generated by the system:
Top Documents [N = 50 or 200 (default=200)]: During data collection, we use a search engine to extract relevant documents (corresponding to top N search results) for mem-ber instances of the target class. We can vary the number of top results extracted per class instance by adjusting thi s parameter, and observe its effect on the attribute extractio n results.

WordNet Pruning [WN = on / off (default=on)]: Since the candidate attributes are extracted from noisy Web data, there could be spurious entries that make it into the final ranked list of class attributes (e.g.,  X 1989 X ,  X 3.2-mp X , etc.) During the post-processing stage in the pipeline, we can fil-ter out candidates that do not contain meaningful English words or terms using a general-purpose lexical resource suc h as WordNet [7].
 Comparative Attribute Extraction Runs : We compare three different systems for attribute extraction in our expe r-iments: ( B ) -Baseline system with structured text: The baseline system is implemented using the extraction frame-work shown in Figure 1, and uses the structured text within relevant Web documents (N = top 200 documents, per class instance) to select candidate attributes. The selected can di-dates are then ranked using frequency-based ranking (Equa-tion 1 from Section 2.1.3). Generic attributes are removed and WordNet pruning is applied during the post-processing stage. ( S hier ) -System using hierarchical patterns with structured text: The second system is identical to the baseline system, except it uses the hierarchical tag patter ns within the structured text to rank selected candidates (Equ a-tion 2 from Section 2.1.3). WordNet pruning and generic at-tribute removal strategies are similarly applied. This sys tem is also capable of performing value extraction. ( D patt ) -Previous approach with unstructured text: We also implemented a third system using the attribute extraction strategy described in previous work [10]. This approach uses hand-crafted patterns (such as X-of-Y pat-terns) to extract attributes from unstructured text within Web documents. To ensure a faithful comparison, we used the same normalized ranking function introduced in [10] to rank the candidate attributes extracted for each class, sin ce it was shown to perform better than a frequency-based rank-ing function. various ranks, are also shown in the last two rows. Table 3: Top attributes extracted (using system S hier ) from structured text in Web documents for a few target classes
The attributes extracted by all these systems are eval-uated in the same manner, following the methodology de-scribed earlier in this section.
The value extraction procedure runs simultaneously with attribute extraction, as described earlier in Figure 3. For every candidate attribute P extracted from text for a par-ticular instance I  X  class C , we obtain some matching text V AL P that is extracted as the value for P .
 Target Classes: Since the evaluation of value extraction results is time consuming, we consider a smaller set of class es and instances as compared to earlier experiments -i.e., onl y 20 out of the 40 target classes that were used for attribute extraction are used for value evaluation. For each of these 2 0 classes, 5 instances are selected randomly from its member set to represent the class.
 Attributes: The text within the extracted element V AL P would constitute a meaningful value for the candidate P , only if P can be considered a valid attribute of the class C for which it was extracted. This is a minimum requirement for our value extraction approach to work. Consequently, from the ranked list of candidates extracted for a target class, we pick a set of 5 high quality attributes for that clas s, and use only these in our value evaluation experiments. For example, candidates such as climate, currency, population, president, and religion are considered high quality attributes for the class Country .
 Evaluation Methodology: Each instance ( I  X  C ) is com-bined with (1) an attribute P (one among the 5 attributes selected for class C ), and (2) the value V AL P extracted for the attribute P , from a document relevant to instance I ; generating a triplet h I | P  X  V AL P i for the class instance ( C : I ). A total of 500 such triplets are generated for the 20 classes (5 attributes per class, 5 instances per class, 1 val ue per attribute-instance pair). Each of these triplets is man u-ally evaluated by a human judge and assigned a correctness label. A triplet is assigned a label  X  X orrect X  if it forms a valid tuple -i.e., if the real value matching attribute P for the class instance I exists within V AL P ; otherwise the triplet is labeled as  X  X ncorrect X  . For example, the triplet h Italy | cap-ital  X  Rome i is a valid tuple for the class Country . The precision scores for value extraction are then obtained by computing the number of triplets marked correct , divided by the total number of triplets.
Every system used in the attribute extraction experiments produces a ranked list of class attributes corresponding to each target class. Table 3 shows some of the top attributes extracted for a few classes, using the seed-based approach with hierarchical tag patterns ( S hier ). Many candidates ranked among the top list such as awards, height, age, birth-place, etc. for class Actor , and atomic number, symbol, atomic weight, etc. for class ChemicalElement represent vi-tal characteristic properties of the particular class. But the list also contains a few wrong attributes, e.g., stage, hurri-cane for the class Hurricane , and prints for the class Painter . The list may also contain a few rather generic attributes, such as career or personal life , mixed with other more spe-cific attributes of the same class ( Actor ).

Table 4 displays some of the results from our experiments with respect to precision scores for the top N attributes ex-tracted by different systems. Individual precision scores f or a couple of the target classes ( Actor, BasicFood ), as well as overall average precision (combining scores from all the target classes) for each system are shown for comparison.
It can be seen from Table 4 that the precision for different systems at a particular rank N varies with respect to the class. For some classes, like Actor , precision at lower ranks (N=5) is higher for the baseline system ( B ) when compared to the previous approach ( D patt ), whereas for other classes like BasicFood , the phenomenon is reversed. However, the system S hier introduced in this paper, which uses hierarchi-cal tag patterns from structured text, consistently outper -forms the other two systems ( B and D patt ), both in terms of individual precision scores for most of the classes, as well as average precision for all the target classes, over a wide ran ge of ranks. For instance, the system S hier exhibits a 24% im-Figure 4: Relative system performance for attribute extrac -tion in terms of average precision over all the classes provement in precision (which maps to around 17% reduc-tion in the error rate) at rank 50, when compared directly to results from the previous approach. Secondly, the result s also indicate that ranking using hierarchical tag patterns ( S hier ) produces much better attributes than a frequency-based ranking approach (baseline system B ). The graph in Figure 4 further illustrates these results, by showing a head-to-head comparison of the different systems in terms of overall attribute precision at ranks 1 through 50. Figure 5 shows the individual precision results for some of the targe t classes. The quality of the extracted attributes varies fro m class to class. In general, the algorithm is able to extract good quality attributes for many of the classes (e.g. Ac-tor, AircraftModel, Country, etc. ) especially at lower ranks
Parameter Precision Top Documents (N = 50) 0.76 0.65 0.56 0.53 0.49 Top Documents (N = 200) 0.76 0.65 0.58 0.52 0.49 WordNet pruning (WN = off) 0.76 0.65 0.58 0.52 0.49 WordNet pruning (WN = on) 0.76 0.65 0.59 0.55 0.52 Table 5: Effect of individually varying different parameters of the system S hier on average precision of attributes (N &lt; 30). For the class Country , the system yields high qual-ity attributes, and the precision only starts to drop (from 1.0) at rank 40, whereas for the class BasicFood , the preci-sion starts dropping at earlier ranks.
 Varying parameter settings : Results for attribute ex-traction from a single system might also vary depending on the parameter settings used for the particular system. Ta-ble 5 shows that for the system S hier , varying the parame-ter N (top search documents) does not affect the precision of extracted attributes by much, whereas applying WordNet pruning (WN =on) slightly improves the average precision at higher ranks.
 Seed selection : The semi-supervised approach presented here uses seeds to mine patterns and extract attributes with similar patterns from Web documents, and hence varying the input seed attributes provided to the system might have an effect on the precision of extracted attributes. The at-tributes provided as seeds for attribute extraction were ch o-sen randomly from a list of potential attributes compiled for each class, and may also include some generic ones, that do not describe any representative property of that partic-ular class. Figure 6 shows how the average class precision of attributes is affected by providing the system with more Figure 6: Effect of varying the number of seeds per class provided as input during attribute extraction, with respec t to average precision over all the classes input seeds. Using 15 seeds (instead of the original 5 seeds) per class helps in improving the average precision as shown in the graph, even at higher ranks (N=50). Adding more seeds allows the algorithm to extract more patterns for a particular class, and candidates simultaneously associat ed with many of these patterns are potentially good attributes for the class, thus improving the overall precision. Howeve r, even though more seeds might yield better performance, it is not always possible to find a large number of seeds, especiall y for uncommon classes. Since we wish to perform attribute extraction for a variety of classes, we limit the amount of supervision provided to the system, by limiting the number of seeds specified as input to five per class.

The quality of the attributes chosen as seeds might also influence the patterns (coverage, type) that are mined from Web documents for the target class, and as a result have an effect on the attributes that are extracted for the particula r class. However, this paper does not focus on comparing the quality or type (generic/specific, etc.) of seeds provided a s input, with the precision scores of extracted attributes.
Using the method proposed in this paper, values are ex-tracted for a small set of high quality attributes correspon d-ing to 20 different classes (5 instances per class, 5 attribut es per class). Results from human evaluation (shown in Ta-ble 6) indicate that the system S hier finds correct values for the provided instance-attribute pairs around 74% of the time. We acquire useful values for attributes related to classes such as Country and Drug ; some correct examples are shown in Table 6, along with the class precision scores.
The precision results confirm that structured text from the Web may be a good source for extracting information regarding relationships between objects and entities (as i n the case of attribute extraction). A previous approach [10] using unstructured text extracts attributes from the immed i-ate vicinity of class instances. In comparison, the algorit hm presented in this paper is not only generic in nature (re-quiring minimal supervision); but also yields attributes t hat have higher quality (with a 30% improvement in precision). Despite the potential use of Web documents for attribute extraction, this data source also poses some disadvantages . Table 6: Examples of high precision attributes correspond-ing to some class instances (i.e. h I, P i pairs), for which value extraction generated the correct answers The large quantity of data available within Web documents may adversely cause a lot of spurious candidates to be ex-tracted. Additionally, due to the limited availability of d oc-uments pertaining to certain classes, we might not be able to extract many attributes for the particular class. For exam-ple, many of the otherwise relevant documents retrieved for instances from the class SearchEngine do not contain much information describing the roles or characteristic featur es representative of the particular class. For classes such as SearchEngine , where fewer retrieved documents whose con-tents is rich in attributes may be available, attribute extr ac-tion using structured text within Web documents retrieved via search engines might not be feasible. Nevertheless, the use of structure does provide us with other advantages such as being able to find potential  X  X alues X  for the candidate at-tributes that we extract. Using the hierarchical pattern an d URL information obtained during attribute extraction, we can first locate the document from which the attribute was extracted, and then find potential matching values from its immediate vicinity or nearby context. This is a useful ap-proach, leveraging the semi-supervised attribute extract ion framework to guide value extraction, as a step towards min-ing factual information from Web data on a large scale.
Our extracted attributes are relations among objects in the given class, and objects or values from other classes. Th e lists of extracted attributes have direct benefits in gaugin g existing methods for harvesting pre-specified semantic rel a-tions [3, 11], towards the acquisition of relations that are of real-world interest to a wide set of Web users, e.g., towards finding mechanisms of action for a Drug .

In [5], the acquisition of attributes and other knowledge relies on users who explicitly specify it by hand. In contras t, we collect attributes automatically from structured text.
Several studies [15, 10, 17, 12] acquire attributes, possib ly along with corresponding values, from Web documents. The method proposed in [15] applies handcrafted lexico-syntac tic patterns to unstructured text within a small collection of Web documents. The evaluation consists in users manually assessing how natural the resulting candidate attributes a re, when placed in a wh-question. In contrast to [15] and simi-larly to [10], our evaluation is stricter, since attributes that would otherwise pass the question answerability test used in [15] are marked as wrong in our evaluation.
In [10], the target classes are specified as sets of represen-tative instances, which is similar to our method. However, the output attributes are collected by applying hand-writt en patterns (rather than using seed attributes) to unstructur ed text (rather than structured text).

The method introduced in [17] acquires attributes as well as associated values from structured text within Web docu-ments, and does so by submitting queries to general-purpose search engines, and analyzing the contents of the top search results. There are, however, a few key differences. First, th e search queries issued in [17] to identify relevant Web doc-uments for a given class are based on hand-written query templates using the label of the target class, e.g.,  X  X ists of movies X  for the class Movie . In comparison, our search queries correspond to a more flexible and scalable approach, in that they are issued automatically based on individual instances provided as input for each class. Besides using hand-written query templates, further manual interventio n is employed in [17] in the form of strict hand-written pat-terns to filter out some of the spurious candidate collected from Web documents. In contrast, the method presented in our paper uniformly applies the same scoring function to all candidate attributes, and refrains from post-filtering can di-date attributes with any hand-written patterns, with poten -tially higher coverage over a wider range of target classes.
Structured text within Web documents constitutes a use-ful source of information for the task of attribute extracti on. A one-pass approach using structural patterns can be used to perform attribute and value extraction simultaneously. Using minimal supervision, this method extracts relevant attributes for a large number of classes spanning a wide range of domains. For some of the high precision attributes that are extracted, matching values can be found within the same document to yield pieces of factual information relate d to various class instances, which can then be used to build a fact repository or a similar type of knowledge resource. [1] E. Agichtein and L. Gravano. Snowball: Extracting [2] M. Banko, M. J. Cafarella, S. Soderland, [3] M. Cafarella, D. Downey, S. Soderland, and [4] H. Chen, S. Tsai, and J. Tsai. Mining tables from [5] T. Chklovski and Y. Gil. An analysis of knowledge [6] A. Doan, R. Ramakrishnan, F. Chen, P. DeRose, [7] C. Fellbaum, editor. WordNet: An Electronic Lexical [8] T. Jayram, R. Krishnamurthy, S. Raghavan, [9] M. Pa  X sca. Organizing and searching the World Wide [10] M. Pa  X sca, B. Van Durme, and N. Garera. The role of [11] P. Pantel and M. Pennacchiotti. Espresso: Leveraging [12] K. Probst, R. Ghani, M. Krema, A. Fano, and Y. Liu. [13] M. Remy. Wikipedia: The free encyclopedia. Online [14] K. Shinzato and K. Torisawa. Acquiring hyponymy [15] K. Tokunaga, J. Kazama, and K. Torisawa. Automatic [16] F. Wu and D. Weld. Automatically refining the [17] N. Yoshinaga and K. Torisawa. Open-domain
