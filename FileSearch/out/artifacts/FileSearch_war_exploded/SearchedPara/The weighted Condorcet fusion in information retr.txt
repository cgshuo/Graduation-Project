 1. Introduction results when fusing those results from different systems.
 algorithms.
 to find suitable weights for weighted Condorcet has not been investigated.
Montague and Aslam (2002) investigated Condorcet fusion and weight Condorcet fusion. Their experiments show that more sophisticated data analysis techniques are used for that.
 this paper, we present a LDA based approach to training such weights. 2. Related work on data fusion ( Wu, 2012a ), automatic ranking a group of retrieval systems ( Nuray &amp; Can, 2006 ) and others. fusion and others, different systems are treated differently in one way or another. mance-level weighting was investigated in Thompson (1990a, 1990b) , a series of power function of performance was weighted Condorcet fusion as well.
 to calculate scores for every document d k . Here s ( d k
A little different from CombSum, CombMNZ uses the following equation to calculate scores. Here u is the number of results in which s ( d use the following equation to calculate scores. Here w i is the weight predefined for component system IR CombSum for the fusion process.

Fuse, a document at rank t is assigned a score of v map / t , where
In a sense, MAPFuse is a special type of the linear combination method, in which
The Condorcet fusion will be discussed in the next section. 3. Condorcet and weighted Condorcet fusion Condorcet methods are named for the 18th century French mathematician and philosopher Marie Jean Antoine Nicolas column as well. If there are m candidates (documents), then we need m all the elements. If d i is preferred to d j , then we add 1 to the element at row i and column j ( a all the ballots are processed. For every element a ij ,if a ple to see how the Condorcet voting can work as a data fusion method in information retrieval.
Example 1. Let us assume that L 1 =&lt; d 2 , d 3 , d 1 , d voting to fuse it. In L 1 , d 2 has higher preference than d preference than d 4 . We add 1 to the corresponding units and the matrix looks like this:
Note that there are three voters (information retrieval systems) in total. For each element ( a above, then d i defeats d j ;if a ij is 1 or less, then d &lt;( d , 3), ( d 1 , 2), ( d 2 , 1), ( d 4 , 0)&gt;.
 all weights are equal to 1.

In the above example, if retrieval systems IR 1 , IR 2 , and IR scores for d 1 , d 2 , d 3 , and d 4 will be score ( d 1 ) = 1(4 :2:3), score ( d
Therefore, the final ranking is changed to &lt; d 2 , d 3 this work, we do not address it and just take a simple solution by ranking those documents at random. not affect all data fusion methods in the same way. Let us discuss two examples to illustrate this. (denoted as L ), then the fused result by Condorcet fusion is the same as L .
This can be proven by considering any pair of documents { d us assume that d i is ranked ahead of d j ,or&lt; d i , d majority X  X . If we use CombSum or the Borda count to fuse them, then a fusion result like this is very unlikely. n 1 ranked lists, d 1 is ranked one place higher than d 2 ranked in the last place. If we use the Borda count to fuse them, then d
The conclusion can be proven by considering the score difference between the two documents d n 1 results, d 1 only obtains one more score than d 2 ; while in the last result, d d is ranked ahead of d 1 because d 2 obtains more scores than d extreme situations: one is that d j is just next to d i ; the other is d count and the Condorcet fusion can be found in Saari (1995) .

We hypothesize that: compared with some other methods such as CombSum and the Borda count, unfavourable condi-some positive evidence in support of this hypothesis in Section 7 . 4. Linear discrimination based approach for weights training In this section, we discuss how to apply linear discrimination to train weights for weighted Condorcet fusion. and both space and time complexities are O ( t ), where t is the number of instances ( Alpaydin, 2010 ). an optimum straight line g ( x , y )= w 0 + w 1 x + w 2 y that can separate them. calculate g ( x 0 , y 0 )= w 0 + w 1 x 0 + w 2 y 0 .If g ( x an instance of class b . fusion.

Suppose there are n information retrieval systems IR 1 , IR documents L j (1 6 j 6 n ) and D is the set of all the documents involved. For simplicity, we assume that all L uments D r and irrelevant documents D i . There are j D r collection, then we have a total number of 2 j D r jj D i 2 j D r jj D i j pairs, we divide them into two classes: Class C document is ranked ahead of an irrelevant document, represented by  X +1 X ; and Class C ponent result IR i to see if it is supported or not. If the ranked pair &lt; d ranked ahead of d b in L i , then we use  X +1 X  in the corresponding column f supported by IR i , which means that d b is ranked ahead of d obtained from a component information retrieval system IR
Example 4. D r ={ d 1 , d 3 , d 5 }, D i ={ d 2 , d 4 }, R table can be used to represent all the instances with their features.
Now we want to distinguish the instances of the two classes by a linear combination of n features. Let
If g ( f 1 , f 2 , ... , f n ) &gt; 0, then the instance in question belongs to Class C belongs to Class C b . For Example 4 , by using LDA 3 we obtain the weights w and 1.897, respectively. Note that in the above table, each feature f tem IR i , thus the weight obtained for f i is for IR i as well. 5. Experimental settings and methodologies 2008) are used. Their information is summarized in Table 1 .
 nology can still further improve effectiveness. See Appendix A for the detailed lists of them. Apart from weighted Condorcet fusion, CombSum, CombMNZ, the linear combination with performance-level weighting, cision, recall-level precision, and precision at 10 document level, are used for retrieval performance evaluation.
For CombSum, CombMNZ, and the linear combination method with performance level weighting, score normalization is needed. In this experiment, the zero-one linear score normalization method is used. used odd-numbered queries to train system weights and even-numbered queries to test data fusion methods; then we validation.
 comprise the same group of documents. This is not a realistic assumption. Here we withdraw this assumption and look evant D r and irrelevant D i . For any possible document pairs &lt; d supported by those component results or not. More specifically, for any document pair &lt; d is ranked ahead of d b in L i ,or d a appears in L i but d represent it; if d b is ranked ahead of d a in L i ,or d b and we use  X  1 X  to represent it; otherwise, neither d a nor d obtain the weights for weighted Condorcet fusion.

Example 5. Let us reconsider Example 4 . D r ={ d 1 , d 3 each information retrieval system only retrieves 3 rather than all 5 documents. R d &gt;. Let us see how the table for LDA can be populated.

We look at some of the pairs for IR 1 ( f 1 ). One pair is &lt; d that L 1 supports this pair (+1). Another pair is &lt; d 1 pair (+1) since we are sure that if retrieved, d 4 must be ranked behind d not. For the same reason, we can decide that L 1 does not support this pair ( 1). Two other pairs are &lt; d d and d 5 do not appear in L 1 , we cannot decide they are supported by L pairs. L 2 and L 3 can be processed similarly. The table we obtain is as follows:
Number Pair f 1 ( IR 1 ) f 2 ( IR 2 ) f 3 ( IR 3 ) Category 10 &lt; d 2 , d 5 &gt;+1 +1 1 1 11 &lt; d 5 , d 4 &gt;00+1+1 12 &lt; d 4 , d 5 &gt;0 0 1 1
Now the data in the table are ready for LDA. This time the weights we obtain are 1.273, 0.299, and 1.197 for IR
IR , respectively. 6. Experimental results of fusion performance
In this section we present experimental results. Each of the three subsections is focused on a different aspect. 6.1. Fusing top systems them is not large. See also next subsection for the investigation of using less than 100 documents. decreases quickly and not as good as that of the best system. In Section 7 this issue will be further addressed. 2008 is the worst, while TREC 2003 is in the middle. better than the Borda count when 3 or 4 systems are fused.

If we use both metrics MAP and RP, then on average all data fusion methods excluding the Borda count and Condorcet combination method with performance-level weighting are better than the best component system, while the others are Apart from the case that three component systems are fused, The t -test shows that the difference between weighted Condorcet fusion and all others are significant at the level of 99% ( p -value is 0.01).
In this experiment, we can notice one interesting phenomenon: when more and more information retrieval systems are the order of decreasing performance, therefore, when more and more component systems are added, the average perfor-other fusion methods are worse off due to diverse performance of component systems. 6.2. Stability of the weight assignment component result. All other aspects are the same as in Section 6.1 . 7.61%, and 7.05%, respectively. They are significant at the level of 99% (two-tailed t -test). such weights, the weighted Condorcet is still able to produce better results than Condorcet. 6.3. Fusing results from randomly chosen results methodology remain the same as in Section 6.1 .
 much better results for all the data fusion methods involved. 7. Positive evidence in support of the hypothesis hypothesis. We reuse the same experimental data and results reported in Section 6.1 .
First we compare the performance of data fusion methods when different number of component systems are fused. To all component systems.
 This interesting phenomenon is a piece of positive evidence in support of the hypothesis proposed in Section 3 . CombSum, respectively.
 &lt;0005. In overall, the model for CombSum is the most accurate, with a R for the Borda count, with a R 2 value of .911; while the model for Condorcet fusion is the least accurate, with a R Condorcet fusion is more complex and more difficult to describe by a linear model than the Borda count and CombSum. to the Condorcet fusion, but neither the Borda count nor CombSum. Therefore, in order to achieve good performance, weighted Condorcet fusion needs to be used in more situations than some other fusion methods such as weighted Borda count and the linear combination method. 8. Conclusions
In this paper we have discussed a linear discriminant analysis based approach to assigning appropriate weights for some other data fusion methods such as the linear combination method with performance level weighting and MAPFuse. Condorcet fusion becomes a desirable option for performance improvement. Appendix A. Systems used in each year group
TREC 8 (The figures in parentheses are values of MAP). 1. input.orcl99man (0.4130) 2. input.CL99XTopt (0.3766) 3. input.8manexT3D1N0 (0.3346) 4. input.pir9Aatd (0.3303) 5. input.ok8alx (0.3240) 6. input.Flab8atdn (0.3240) 7. input.MITSLStdn (0.3227) 8. input.att99atde (0.3165) 9. input.apl8p (0.3154) 10. input.UniNET8Lg (0.3139)
Note: input.READWARE2 (0.4632) and input.iit99ma1 (0.4104) are not selected because both of them include far fewer documents (5785 and 32,061, respectively) than the others (50,000).
TREC 2003 (The figures in parentheses are values of MAP). 1. input.pircRBa2 (0.3111) 2. input.aplrob03a (0.2998) 3. input.uwmtCR0 (0.2763) 4. input.VTgpdhgp2 (0.2731) 5. input.THUIRr0302 (0.2666) 6. input.UIUC03Rtd1 (0.2660) 7. input.humR03de (0.2627) 8. input.fub03InOLe3 (0.2518) 9. input.UAmsT03RFb (0.2452) 10. input.oce03Xbm (0.2446)
TREC 2008 (The figures in parentheses are values of MAP). 1. NOpMMs43 (0.5395) 2. KGPBASE4 (0.4800) 3. top3dt1mRd (0.4085) 4. KLEDocOpinT (0.4062) 5. B4PsgOpinAZN (0.3924) 6. DCUCDVPgoo (0.3874) 7. FIUBL4DFR (0.3760) 8. b4dt1mRd (0.3742) 9. DUTIR08Run4 (0.3393) 10. uams08n1o1sp (0.3351) References
