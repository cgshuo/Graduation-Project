 Heinrich-Heine-Universit  X  at D  X  usseldorf Heinrich-Heine-Universit  X  at D  X  usseldorf
Probabilistic Linear Context-Free Rewriting Systems (PLCFRSs). LCFRS, an extension of CFG, of parse items, some of them allowing for A  X  parsing. We evaluate our parser with grammars extracted from the German NeGra treebank. Our experiments show that data-driven LCFRS parsing is feasible and yields output of competitive quality. 1. Introduction received growing interest. A direct effect of morphological richness is, for instance, data sparseness on a lexical level (Candito and Seddah 2010). A rather indirect effect is that morphological richness often relaxes word order constraints. The principal intuition is that a rich morphology encodes information that otherwise has to be conveyed by a particular word order. If, for instance, the case of a nominal complement is not provided by morphology, it has to be provided by the position of the complement relative to other complements in the sentence. Example (1) provides an example of case marking and free word order in German. In turn, in free word order languages, word order can encode information structure (Hoffman 1995). (1) a. der not hold in both directions. Although it is generally the case that languages with a rich morphology exhibit a high degree of freedom in word order, languages with a free word order do not necessarily have a rich morphology. Two examples for languages with a very free word order are Turkish and Bulgarian. The former has a very rich and the this discussion.
 allows for a free word order, as we have already seen in Example (1): Arguments can be scrambled, and topicalizations and extrapositions underlie few restrictions. Conse-quently, discontinuous constituents occur frequently. This is challenging for syntactic M  X  uller 2004), and for treebank annotation in particular (Skut et al. 1997). uents on the basis of German. In this section, we inspect the type of data we have to deal with, and we describe the way such data are annotated in treebanks. We briefly discuss different parsing strategies for the data in question and motivate our own approach. 1.1 Discontinuous Constituents
Consider the sentences in Example (2) as examples for discontinuous constituents (taken from the German NeGra [Skut et al. 1997] and TIGER [Brants et al. 2002] tree-banks). Example (2a) shows several instances of discontinuous VPs and Example (2b) shows a discontinuous NP. The relevant constituent is printed in italics. (2) a. Fronting: 88 uous constituents as well. Example (3a) is a Bulgarian example of a PP extracted out of an NP, taken from the BulTreebank (Osenova and Simov 2004), and Example (3b) is an example of fronting in Korean, taken from the Penn Korean Treebank (Han, Han, and
Ko 2001). (3) a. Na kyshtata as English, resulting from, for instance, long-distance movements. Examples (4a) and (4b) are examples from the Penn Treebank for long extractions resulting in discontin-uous S categories and for discontinuous NPs arising from extraposed relative clauses, respectively (Marcus et al. 1994). (4) a. Long Extraction in English: 1.2 Treebank Annotation and Data-Driven Parsing Most constituency treebanks rely on an annotation backbone based on Context-Free
Grammar (CFG). Discontinuities cannot be modeled with CFG, because they require a larger domain of locality than the one offered by CFG. Therefore, the annotation back-bone based on CFG is generally augmented with a separate mechanism that accounts for the non-local dependencies. In the Penn Treebank (PTB), for example, trace nodes and co-indexation markers are used in order to establish additional implicit edges in the tree beyond the overt phrase structure. In T  X  uBa-D/Z (Telljohann et al. 2012), a German
Treebank, non-local dependencies are expressed via an annotation of topological fields (H  X  ohle 1986) and special edge labels. In contrast, some other treebanks, among them
NeGra and TIGER, give up the annotation backbone based on CFG and allow annota-tion with crossing branches (Skut et al. 1997). In such an annotation, non-local depen-dencies can be expressed directly by grouping all dependent elements under a single node. Note that both crossing branches and traces annotate long-distance dependencies are less theory-dependent because they do not make any assumptions about the base positions of  X  X oved X  elements.

Figures 1 and 2. Figure 1 shows the NeGra annotation of Example (2a-i) (left), and an annotation of the same sentence in the style of the T  X  uBa-D/Z treebank (right). Figure 2 shows the PTB annotation of Example (4a-ii) (on the left, note that the directed edge NeGra-style annotation of the same sentence (right).

Context-Free Grammar (PCFG). In order to extract a PCFG from a treebank, the trees need to be interpretable as CFG derivations. Consequently, most work has excluded non-local dependencies; either (in PTB-like treebanks) by discarding labeling conven-tions such as the co-indexation of the trace nodes in the PTB, or (in NeGra/TIGER-like treebanks) by applying tree transformations, which resolve the crossing branches (e.g.,
K  X  ubler 2005; Boyd 2007). Especially for the latter treebanks, such a transformation is problematic, because it generally is non-reversible and implies information loss. in NeGra and TIGER have crossing branches (Maier and Lichte 2011). In the Penn
Treebank, this holds for approximately 20% of all sentences (Evang and Kallmeyer 2011). This shows that it is important to properly treat such structures. 1.3 Extending the Domain of Locality
In the literature, different methods have been explored that allow for the use of non-for instance, through complex labels (Hockenmaier 2003) or through the derivation 90  X  consists of approaches that aim at producing trees which contain non-local information. processing step to PCFG parsing (Johnson 2002; Dienes 2003; Levy and Manning 2004;
Cai, Chiang, and Goldberg 2011). Other work uses formalisms that accommodate the direct encoding of non-local information (Plaehn 2004; Levy 2005). We pursue the latter approach.

Rewriting Systems (LCFRSs) (Vijay-Shanker, Weir, and Joshi 1987) have been estab-lished as a candidate for modeling both discontinuous constituents and non-projective dependency trees as they occur in treebanks (Maier and S X gaard 2008; Kuhlmann and the non-terminals can span tuples of possibly non-adjacent strings (see Figure 3). Be-cause LCFRSs allow for binarization and CYK chart parsing in a way similar to CFGs,
PCFG techniques, such as best-first parsing (Caraballo and Charniak 1998), weighted deductive parsing (Nederhof 2003), and A  X  parsing (Klein and Manning 2003a) can be transferred to LCFRS. Finally, as mentioned before, languages such as German have recently attracted the interest of the parsing community (K  X  ubler and Penn 2008; Seddah, K  X  ubler, and Tsarfaty 2010).

LCFRS (PLCFRS), continuing the promising work of Levy (2005). Our parser pro-dependencies while not making any additional assumptions concerning the position of hypothetical traces. We have implemented a CYK parser and we present several figures-of-merit in a best-first parsing context or as estimates for A a real-world-sized data set shows that our parser achieves competitive results. To our knowledge, our parser is the first for the entire class of PLCFRS that has successfully been used for data-driven parsing. 1 tions 3 and 4 present the binarization algorithm, the parser, and the outside estimates which we use to speed up parsing. In Section 5 we explain how to extract an LCFRS from a treebank and we present grammar refinement methods for these specific treebank work to other approaches. 2. Probabilistic Linear Context-Free Rewriting Systems 2.1 Definition of PLCFRS
LCFRS (Vijay-Shanker, Weir, and Joshi 1987) is an extension of CFG in which a non-terminal can span not only a single string but a tuple of strings of size k called its fan-out . We will notate LCFRS with the syntax of Simple Range Concate-nation Grammars (SRCG) (Boullier 1998b), a formalism that is equivalent to LCFRS.
A third formalism that is equivalent to LCFRS is Multiple Context-Free Grammar (MCFG) (Seki et al. 1991).
 Definition 1 (LCFRS) a) N is a finite set of non-terminals with a function dim : N b) T and V are disjoint finite sets of terminals and variables; c) S  X  N is the start symbol with dim ( S ) = 1; d) P is a finite set of rules computed from the yields of the right-hand side non-terminals. The rules A ( ab , cd ) yield of A and (2) one can compute a new tuple in the yield of A from an already existing one by wrapping a and b around the first component and c and d around the second. ACFGrule A  X  BC would be written A ( XY )  X  B ( X ) C ( Y ) as an LCFRS rule. Definition 2 (Yield, language)
Let G = N , T , V , P , S be an LCFRS. 1. For every A  X  N , we define the yield of A , yield ( A ) as follows: 92 2. The language of G is then L ( G ) = { w | w  X  yield ( S ) given a pair in the yield of A , we can obtain an element in the yield of S by concate-nating the two components. Consequently, the language generated by this grammar is { a  X  -freeness will be referred to later and are therefore introduced in the following defini-tion. They are taken from the LCFRS/MCFG terminology; the SRCG term for fan-out is arity and the property of being monotone is called ordered in the context of SRCG. Definition 3
Let G = N , T , V , P , S be an LCFRS. 1. The fan-out of G is the maximal fan-out of all non-terminals in G . 2. Furthermore, the right-hand side length of a rewriting rule r 3. G is monotone if for every r  X  P and every right-hand side non-terminal A 4. A rule r  X  P is called an  X  -rule if one of the left-hand side components of r Boullier 1998a) and monotone (Michaelis 2001; Kracht 2003; Kallmeyer 2010). tion of PCFG and thus it follows (Levy 2005; Kato, Seki, and Kasami 2006) that: Definition 4 (PLCFRS)
LCFRS and p : P  X  [0 .. 1] a function such that for all A
PLCFRS with non-terminals { S , A , B } , terminals { a } 0 . 2: S ( X )  X  A ( X )0 . 8: S ( XY )  X  B ( X , Y ) 0 . 7: A ( aX )  X  A ( X )0 . 3: A ( a )  X   X  0 . 8: B ( aX , aY )  X  B ( X , Y )0 . 2: B ( a , a )  X   X  a . Words with an even number of a s and nested dependencies are more probable than words with a right-linear dependency structure. For instance, the word aa receives probability 0 . 16 and (b) (right-linear dependencies) has probability 0 . 042. 3. Parsing PLCFRS 3.1 Binarization
Similarly to the transformation of a CFG into Chomsky normal form, an LCFRS can be binarized, resulting in an LCFRS of rank 2. As in the CFG case, in the transformation, we introduce a non-terminal for each right-hand side longer than 2 and split the rule right-hand sides are of length 2. The transformation algorithm is inspired by G  X  omez-
Rodr  X   X guez et al. (2009) and it is also specified in Kallmeyer (2010). need the notion of a reduction of a vector  X   X  [( T  X  V ) variables in x occur in  X  . A reduction is, roughly, obtained by keeping all variables in  X  that are not in x . This is defined as follows: Definition 5 (Reduction)
Let N , T , V , P , S be an LCFRS,  X   X  [( T  X  V )  X  ] i and x separated by a new symbol $ /  X  ( V  X  T ).
 all a  X  T , h ( X ) = $ for all X  X  X  x 1 , ... x j } and h ( y ) = y in all other cases. y , ... y m is the reduction of  X  by x .
 duced with X 2 yields X 1 , X 3 as well.
 94 like the CFG binarization algorithm in the sense that for right-hand sides longer than 2, we introduce a new non-terminal that covers the right-hand side without the first element. Figure 8 shows an example. In this example, there is only one rule with a right-hand side longer than 2. In a first step, we introduce the new non-terminals and rules the rules from R to the grammar, whenever a right-hand side argument contains several variables, these are collapsed into a single new variable.
 forward. Note, however, that the fan-out of the LCFRS can increase.
 that needs to be binarized, we choose unique new non-terminals. Later, in Section 5.3.1, we will introduce additional factorization into the grammar rules that reduces the set of new non-terminals. 3.1.2 Minimizing Fan-Out and Number of Variables. In LCFRS, in contrast to CFG, the order of the right-hand side elements of a rule does not matter for the result of a derivation. following, we present a binarization order that yields a minimal fan-out and a minimal (2010). We assume that we are only considering partitions of right-hand sides where one of the sets contains only a single non-terminal.
 s ( c , A i )ofthe A i -reduction of c as follows: Concatenate the elements of x new additional symbols $ while replacing every component from x define the arity of the characteristic string, dim ( s ( c , A strings x  X  V + in s ( A i ). Take, for example, a rule c = VP ( X , YZU ) Then s ( c , VP ) = $$ Y $ U , s ( c , V ) = X $$ ZU .
 we determine the optimal candidate for binarization based on the characteristic string predicate. This check provides the optimal candidate. In a second step we then perform the same binarization as before, except that we use the optimal candidate now instead of the first element of the right-hand side. 3.2 The Parser
We can assume without loss of generality that our grammars are  X  -free and monotone (the treebank grammars with which we are concerned all have these properties) and that they contain only binary and unary rules. Furthermore, we assume POS tagging to be done before parsing. POS tags are non-terminals of fan-out 1. Finally, according to our grammar extraction algorithm (see Section 5.1), a separation between two components always means that there is actually a non-empty gap in between them. Consequently, component of the left-hand side. The rules are then either of the form A ( a )
POS tag and a  X  T or of the form A ( x )  X  B ( x )or A (  X  ) x hand sides. 96 to portions of the input string. For this purpose we need the notions of ranges, range vectors, and rule instantiations. A range is a pair of indices that characterizes the span of a component within the input. A range vector characterizes a tuple in the yield of a non-terminal. A rule instantiation specifies the computation of an element from the left-hand side yield from elements in the yields of the right-hand side non-terminals based on the corresponding range vectors.
 Definition 6 (Range) 1. Pos ( w ): = { 0, ... , n } . 2. We call a pair l , r  X  Pos ( w )  X  Pos ( w )with l  X  r a range in w .Its yield 3. For two ranges  X  1 = l 1 , r 1 ,  X  2 = l 2 , r 2 ,if r 4. A  X   X  ( Pos ( w )  X  Pos ( w )) k is a k -dimensional range vector for w iff definition follows the definition of clause instantiations from Boullier (2000). An in-stantiated rule is a rule in which variables are consistently replaced by ranges. Because we need this definition only for parsing our specific grammars, we restrict ourselves to  X  -free rules containing only variables.
 Definition 7 (Rule instantiation)
A ( x 1 )  X  X  X  A m ( x m )  X  P (0 &lt; m ) that does not contain any terminals, 1. an instantiation with respect to a string w = t 1 ... t 2. if f is an instantiation of r , then A ( f (  X  ))  X  A 1 rithm is formulated using the framework of parsing as deduction (Pereira and Warren 1983; Shieber, Schabes, and Pereira 1995; Sikkel 1997), extended with weights (Nederhof 2003). In this framework, a set of weighted items representing partial parsing results is characterized via a set of deduction rules, and certain items (the goal items) represent successful parses.
 the input string. For a given input w , our items have the form [ A ,  X  ] where A is a range vector that characterizes the span of A . Each item has a weight in that encodes the Viterbi inside score of its best parse tree. More precisely, we use the log probability log( p ) where p is the probability.

Consequently, they are axioms; their probability is 1 and their weight therefore 0. The instantiation of a unary rule. In our grammar, terminals only occur in rules with POS tags and the grammar is ordered and  X  -free. Therefore, the components of the yield of the right-hand side non-terminal and of the left-hand side terminals are the same. The rule binary applies an instantiated rule of rank 2. If we already have the two elements and binary , the probability p of the new rule is multiplied with the probabilities of the antecedent items (which amounts to summing up the antecedent weights and log ( p )). as in Figure 11. Because for all our deduction rules, the weight functions f that compute the weight of a consequent item from the weights of the antecedent items are monotone non-increasing in each variable, the algorithm will always find the best parse without the need of exhaustive parsing. All new items that we deduce involve at least one of the agenda items as an antecedent item. Therefore, whenever an item is the best in the agenda, we can be sure that we will never find an item with a better (i.e., higher) weight.
Consequently, we can safely store this item in the chart and, if it is a goal item, we have found the best parse.
 when parsing aa with the PLCFRS from Figure 5, transformed into a PLCFRS with pre-terminals and binarization (i.e., with a POS tag T terminal B ). The new PLCFRS is given in Figure 13.
 an A with span 0, 2 into an S .This S has however a rather low probability and is therefore not on top of the agenda. Later, when finding the better analysis, the weight 98 of the S item in the agenda is updated and then the goal item is the top agenda item and therefore parsing has been successful.
 parser, we do the following: Whenever we generate a new item, we store it not only with its weight but also with backpointers to its antecedent items. Furthermore, whenever we update the weight of an item in the agenda, we also update the backpointers. In order to read off the best parse tree, we have to start from the goal item and follow the backpointers. 4. Outside Estimates to speed up parsing, we add the estimate of the costs for completing the item into a goal item to its weight X  X hat is, to the weight of each item in the agenda, we add an summary estimates . A context summary is an equivalence class of items for which we specific enough to be helpful for discriminating items in the agenda. our outside estimates are admissible (Klein and Manning 2003a), which means that are too optimistic about the costs of completing the item into an S item spanning the entire input. For the full SX estimate described in Section 4.1 and the SX estimate with span and sentence length in Section 4.4, the monotonicity is guaranteed and we can do true A  X  parsing as described by Klein and Manning. Monotonicity means that for each antecedent item of a rule it holds that its weight is greater than or equal to the weight of the consequent item. The estimates from Sections 4.2 and 4.3 are not monotonic. This means that it can happen that we deduce an item I 2 from an item I
I is greater than the weight of I 1 . The parser can therefore end up in a local maximum that is not the global maximum we are searching for. In other words, those estimates are only figures of merit (FOM).
 len 4.1 Full SX Estimate
The full SX estimate is a PLCFRS adaption of the SX estimate of Klein and Manning (2003a) (hence the name). For a given sentence length n , the estimate gives the maximal probability of completing a category X with a span  X  into an S with span 0, n . span  X  , regardless of the actual terminals in our input. This inside estimate is computed as shown in Figure 14. Here, we do not need to consider the number of terminals outside the span of C (to the left or right or in the gaps), because they are not relevant for the inside score. Therefore the items have the form [ A , l 1 terminal and l i gives the length of its i th component. It holds that because our grammar extraction algorithm ensures that the different components in the yield of a non-terminal are never adjacent. There is always at least one terminal in between two different components that does not belong to the yield of the non-terminal. of length 1; therefore this case has probability 1 (weight 0). The rules unary and binary are roughly like the ones in the CYK parser, except that they combine items with length 100 or equal to in + log ( p ). For each item, we record its maximal weight (i.e., its maximal probability). The rule binary is slightly more complicated because we have to compute the first component, the string in between the first and second component, and so on. component length len and no terminals to the left or to the right of the S component estimate of the right-hand side item is greater or equal to the outside estimate of the we have to further add the inside estimate of the other daughter. For this, we need a different length vector (without the lengths of the parts in between the components).
Therefore, for a given range vector  X  = l 1 , r 1 , ... , l we distinguish between the inside length vector l in outside length vector l out (  X  ) = l 1 , r 1  X  l 1 , l 2
A and B , which is very expensive. Furthermore, for a category A with a certain number of terminals in the components and the gaps, we compute the lower part of the outside estimate several times, namely, for every combination of number of terminals to the left and to the right (first and last element in the outside length vector). In order to avoid these problems, we now abstract away from the lengths of the part to the left and the right, modifying our items such as to allow a bottom X  X p strategy.
 certain lower C up to some A ( C is a kind of  X  X ap X  in the yield of A ) while summing up the inside costs of off-spine nodes and the log of the probabilities of the corresponding rules. We use items [ A , C ,  X  A ,  X  C , shift ] where A , C with a first component starting at position 0. The integer shift positions to the right the C span is shifted, compared to the starting position of the A .  X 
A and  X  C represent the spans of C and A while disregarding the number of terminals to the left and the right (i.e., only the lengths of the components and of the gaps are encoded). This means in particular that the length n of the sentence does not play a role here. The right boundary of the last range in the vectors is limited to len obtains from adding i to all range boundaries in  X  and shift (  X  , one obtains from subtracting i from all boundaries in  X  .

C tree with yield  X  C into an A tree with yield  X  A such that, if the span of A starts at position j , the span of C starts at position i + j . Figure 16 gives the computation. The value of in ( A , l ) is the inside estimate of [ A , l ].
 first component of  X  and with sentence length n is then given by the maximal weight of [ S , C , 0, n , shift (  X  ,  X  i ), i ]. 4.2 SX with Left, Gaps, Right, Length
A problem of the previous estimate is that with a large number of non-terminals (for treebank parsing, approximately 12,000 after binarization and markovization), the com-putation of the estimate requires too much space. We therefore turn to simpler estimates with only a single non-terminal per item. We now estimate the outside score of a non-terminal A with a span of a length length (the sum of the lengths of all the components right of the last component, and gaps terminals in between the components of the A len + left + right + gaps  X  len max , len  X  dim ( X ), gaps we have left A variables for A -components preceding the first variable of a B component, right A variables for A -components following the last variable of a B component, and right B variables for B -components following the last variable of an A component. (In our grammars, the first left-hand side argument always starts with the first variable from A .)
Furthermore, we set gaps A = dim ( A )  X  left A  X  right A computation of the full SX estimate in Figure 15, except that now the items are simpler. 102 of the probability of an X category with a span of length l . Its computation is specified in Figure 18.
 n  X  r 4.3 SX with LR, Gaps, Length estimate, we can simplify the previous estimate by subsuming the two lengths left and len + lr + gaps  X  len max , len  X  dim ( X ), gaps  X  dim ( X ) and Binary-right , we have limited lr in the consequent item to the lr of the antecedent number of items while having only little effect on the parsing results.
 maximal weight of [ C , len , l 1 + r , n  X  len  X  l 1  X  r ].
 4.4 SX with Span and Sentence Length
We will now present a further simplification of the last estimate that records only the mate is actually monotonic and allows for true A  X  parsing.
 of [ C , len , n ].
 ing and therefore guarantees that the best parse will be found, let us have a look at the
CYK deduction rules when being augmented with the estimate. Only Unary and Binary are relevant because Scan does not have antecedent items. The two rules, augmented with the outside estimate, are shown in Figure 21.
 w and a consequent item with weight w , then w  X  w .
 Unary rule for computing the outside estimate and because of the unary production, 104 we obtain that, given the outside estimate out A of [ A ,  X  ] the outside estimate out item [ B ,  X  ]isatleast out A + log ( p ), namely, out B  X  of the C antecedent item and the consequent. The treatment of the antecedent B is symmetric. To show: in C + out C  X  in B + in C + log ( p ) + out stantiated rule p : A (  X  A )  X  B (  X  B ) C (  X  C ), we have that the outside estimate out C -item is at least out A + in ( B , l B ) + log ( p ). Furthermore, in ( B , l out C  X  in B + log ( p ) + out A . 4.5 Integration into the Parser
Before parsing, the outside estimates of all items up to a certain maximal sentence length len max are precomputed. Then, when performing the weighted deductive parsing as explained in Section 3.2, whenever a new item is stored in the agenda, we add its outside estimate to its weight.
 score, given the input, the weight of an item in the agenda is always greater than or equal to the log of the actual product of the inside and outside score of the item. In this sense, the outside estimates given earlier are admissible.
 mate with span and sentence length are monotonic and allow for A two estimates, which are both not monotonic, act as FOMs in a best-first parsing context.
Consequently, they contribute to speeding up parsing but they decrease the quality of the parsing output. For further evaluation details see Section 6. 5. Grammars for Discontinuous Constituents 5.1 Grammar Extraction
The algorithm we use for extracting an LCFRS from a constituency treebank with cross-ing branches has originally been presented in Maier and S X gaard (2008). It interprets the treebank trees as LCFRS derivation trees. Consider for instance the tree in Figure 22.
S  X  VP VMFIN . The VP is discontinuous with two components that wrap around the yield of the VMFIN. Consequently, the LCFRS rule is S ( XYZ ) mediate, except for the fan-out of the non-terminal categories: In the treebank, we can have the same non-terminal with different fan-outs, for instance a VP without a gap (fan-out 1), a VP with a single gap (fan-out 2), and so on. In the corresponding LCFRS, predicates.

P dominating some terminal a . Then for all other non-terminals A
A  X  X  X  A is the number of discontinuous parts in their yields. The components of A nations of variables that describe how the discontinuous parts of the yield of A obtained from the yields of its daughters.
 where V is the set of nodes, E  X  V  X  V the set of immediate dominance edges, r in V , E , r with a linear precedence relation w i  X  w j for 1 variable X i for every w i ,1  X  i  X  n .
 106
Estimation. 5.2 Head-Outward Binarization
As previously mentioned, in contrast to CFG the order of the right-hand side elements of a rule does not matter for the result of an LCFRS derivation. Therefore, we can reorder the right-hand side of a rule before binarizing it.
 where the head is the lowest subtree and it is extended by adding first all sisters to its then the head itself. Figure 24 shows the effect this reordering and binarization has on Rule extracted for the S node: S ( XYZU )  X  VP ( X , U ) VMFIN ( Y ) NN ( Z ) Reordering for head-outward binarization: S ( XYZU )  X  NN ( Z ) VP ( X , U ) VMFIN ( Y ) New rules resulting from binarizing this rule:
S ( XYZ )  X  S bin 1 ( X , Z ) NN ( Y ) S bin 1 ( XY , Z ) Rule extracted for the VP node: VP ( X , YZ )  X  NN ( X ) AV ( Y ) VA I N F ( Z ) New rules resulting from binarizing this rule: VP ( X , Y )  X  NN ( X ) VP bin 1 ( Y ) VP bin 1 ( XY )  X  AV ( X ) VA I N F ( Y ) Tree after binarization: and Manning (2003b) do. To mark the heads of phrases, we use the head rules that the Stanford parser (Klein and Manning 2003c) uses for NeGra.
 binarization non-terminal, because this was neither beneficial for parsing times nor for the parsing results. 5.3 Incorporating Additional Context 5.3.1 Markovization. As already mentioned in Section 3.1, a binarization that introduces a large amount of non-terminals and fails to capture certain generalizations. For this reason, we introduce markovization (Collins 1999; Klein and Manning 2003b). new rules introduced during binarization and adding vertical and horizontal context from the original trees to each occurrence of this new non-terminal. As vertical context, we add the first v labels on the path from the root node of the tree that we want to binarize to the root of the entire treebank tree. The vertical context is collected during grammar extraction and then taken into account during binarization of the rules. As horizontal context, during binarization of a rule A (  X  ) non-terminal that comprises the right-hand side elements A m ), we add the first h elements of A i , A i  X  1 , ... , A and h = 2. Here, the superscript is the vertical context and the subscript the horizontal context of the new non-terminal X . Note that in this example we have disregarded the fan-out of the context categories. The VP, for instance, is actually a VP fan-out 2. For the context symbols, one can either use the categories from the original treebank (without fan-out) or the ones from the LCFRS rules (with fan-out). We chose the latter approach because it delivered better parsing results. 5.3.2 Further Category Splitting. Grammar annotation (i.e., manual enhancement of an-notation information through category splitting) has previously been successfully used beneficial effect in PLCFRS parsing as well, we perform different category splits on the (unbinarized) NeGra constituency data.
 categories S). Relative clauses mostly occur in a very specific context, namely, as the 108 right part of an NP or a PP. This splitting should therefore speed up parsing and increase precision. Furthermore, we distinguish NPs by their case. More precisely, to all nodes with categories N, we append the grammatical function label to the category label. We finally experiment with the combination of both splits. 6. Experiments 6.1 Data
Our data source is the NeGra treebank (Skut et al. 1997). We create two different data sets for constituency parsing. For the first one, we start out with the unmodified NeGra treebank and remove all sentences with a length of more than 30 words. We pre-process the treebank following common practice (K  X  ubler and Penn 2008), attaching all nodes which are attached to the virtual root node to nodes within the tree such that, ideally, comes in pairs (parentheses, quotation marks) to the same nodes. For the second data set we create a copy of the pre-processed first data set, in which we apply the usual tree transformations for NeGra PCFG parsing (i.e., moving nodes to higher positions until all crossing branches are resolved). The first 90% of both data sets are used as the training set and the remaining 10% as test set. The first data set is called NeGra and the second is called NeGra CFG .
 (excluding terminals). Furthermore, gap degrees (i.e., the number of gaps in the spans of non-terminal nodes) are listed (Maier and Lichte 2011).
 due to the fact that, unlike us, they removed the punctuation from the trees. 6.2 Parser Implementation
We have implemented the CYK parser described in the previous section in a system called rparse . The implementation is realized in Java. 3 6.3 Evaluation
For the evaluation of the constituency parses, we use an EVALB-style metric. For a tree over a string w , a single constituency is represented by a tuple A ,  X  with A being a node label and  X   X  ( Pos ( w )  X  Pos ( w )) dim ( A ) . We compute precision, recall, and F on these tuples from gold and de-binarized parsed test data from which all category splits have been removed. This metric is equivalent to the corresponding PCFG metric for dim ( A ) = 1. Despite the shortcomings of such a measure (Rehbein and van Genabith 2007), it still allows to some extent a comparison to previous work in PCFG parsing (see also Section 7). Note that we provide the parser with gold POS tags in all experiments. 6.4 Markovization and Binarization
We use the markovization settings v = 1and h = 2 for all further experiments. The setting which has been reported to yield the best results for PCFG parsing of NeGra, v = 2and h = 1 (Rafferty and Manning 2008), required a parsing time which was too high. 4 tions: Head-driven and KM are the two head-outward binarizations that use a head which we always choose the rightmost daughter of a node as its head. the left-hand side such that the fan-out of the binarized rules is optimized (described in we binarize strictly from left to right (i.e., we do not reorder the right-hand sides of productions, and choose unique binarization labels).
 together; the results for the deterministic binarization are worse. This indicates that the presence or absence of markovization has more impact on parsing results than the actual binarization order. Furthermore, the non-optimal binarizations did not yield a binarized grammar of a higher fan-out than the optimal binarization: For all five binarizations, the fan-out was 7 (caused by a VP interrupted by punctuation). 110 for different parsing speeds. The respective leftmost graph in Figures 26 and 27 show a visual representation of the number of items produced by all binarizations, and the corresponding parsing times. Note that when choosing the head with head rules the the left of the head and then to the right of the head or vice versa. The optimal bina-rization produces the best results. Therefore we will use it in all further experiments, in spite of its higher parsing time. 6.5 Baseline Evaluation and Category Splits Table 3 presents the constituency parsing results for NeGra with and without the different category splits. Recall that NeGra branches and consequently leads to a PLCFRS of fan-out &gt; 1 whereas NeGra not contain crossing branches and consequently leads to a 1-PLCFRS X  X n other words, before we evaluate the experiments with category splits, we replace all split labels in the parser output with the corresponding original labels.

NeGra LCFRS . Twenty-nine sentences had no parse, therefore, the parser output has 1,804 sentences. The average tree height is 4.72, and the average number of children per node (excluding terminals) is 2.91. These values are almost identical to the values for the gold data. As for the gap degree, we get 1,401 sentences with no gaps (1,361 in the gold set), 334 with gap degree 1 (387 in the gold set), and 69 with 2 or 3 gaps (85 in the gold set).
Even though the difference is only small, one can see that fewer gaps are preferred. This is not surprising, since constituents with many gaps are rare events and therefore end up with a probability which is too low.
 more information than the output of a PCFG parser) does not lag far behind the quality of the PCFG parsing results on NeGra CFG . With respect to the category splits, the results
PLCFRS parser output. The gains in speed are particularly visible for sentences with a length greater than 20 words (cf. the number of produced items and parsing times in
Figures 26 and 27 [middle]). 6.6 Evaluating Outside Estimates
We compare the parser performance without estimates (OFF) with its performance with the estimates described in Sections 4.3 (LR) and 4.4 (LN).
 were too expensive to compute both in terms of time and space, given the restrictions imposed by our hardware. We could, however, compute the LN and the LR estimate.
Unlike the LN estimate, which allows for true A  X  quality of the parsing results deteriorate: Compared with the baseline, labeled F from 74.90 to 73.76 and unlabeled F 1 drops from 77.91 to 76.89. The respective rightmost graphs in Figures 26 and 27 show the average number of items produced by the parser and the parsing times for different sentence lengths. The results indicate that the estimates have the desired effect of preventing unnecessary items from being produced. This is reflected in a significantly lower parsing time.
 trade-off between maintaining optimality and obtaining a higher parsing speed. In 112 other words, it raises the question of whether techniques such as pruning or coarse-to-fine parsing (Charniak et al. 2006) would probably be superior to A implementation of a coarse-to-fine approach has been presented by van Cranenburgh (2012). He generates a CFG from the treebank PLCFRS, based on the idea of Barth  X  elemy et al. (2001). This grammar, which can be seen as a coarser version of the actual PLCFRS, is then used for pruning of the search space. The problem that van Cranenburgh tackles is specific to PLCFRS: His PCFG stage generalizes over the distinction of labels by their fan-out. The merit of his work is an enormous increase in efficiency: Sentences with a length of up to 40 words can now be parsed in a reasonable time. For a comparison of the results of van Cranenburgh (2012) with our work, the same version of evaluation parameters would have to be used. The applicability and effectiveness of other coarse-to-fine approaches (Charniak et al. 2006; Petrov and Klein 2007) on PLCFRS remain to be seen. 7. Comparison to Other Approaches Comparing our results with results from the literature is a difficult endeavor, because provide in this section is a comparison of the performance of our parser on NeGra an overview on previous work on parsing which aims at reconstructing crossing branches.
 formed experiments with Helmut Schmid X  X  LoPar (Schmid 2000) and with the Stanford were provided with gold POS tags. Recall that our parser produced labeled precision, recall, and F 1 of 76.32, 76.46, and 76.34, respectively. The plain PCFG provided by LoPar delivers lower results (LP 72.86, LR 74.43, and L F 1 73.63). The Stanford Parser results (markovization setting v = 2, h = 1 [Rafferty and Manning 2008], otherwise default parameters) lie in the vicinity of the results of our parser (LP 74.27, LR 76.19, L F implemented by our parser and the Stanford parser, it remains to be investigated why the lexicalization component of the Stanford parser does not lead to better results. In any case the comparison shows that on a data set without crossing branches, our parser comparison is the PaGe workshop experimental data (K  X  ubler and Penn 2008). lists the results of some of the papers in K  X  ubler and Penn (2008) on TIGER, namely, 2007); Rafferty and Manning (2008) (R&amp;M), who use the Stanford parser (see above); and Hall and Nivre (2008) (H&amp;N), who use a dependency-based approach (see next paragraph). The comparison again shows that our system produces good results. Again the performance gap between the Stanford parser and our parser warrants further investigation.
Labeled F 1 using Probabilistic Discontinuous Phrase Structure Grammar (DPSG), albeit only on sentences with a length of up to 15 words. On those sentences, we obtain 83.97.
The crucial difference between DPSG rules and LCFRS rules is that the former explicitly specify the material that can occur in gaps whereas LCFRS does not. Levy (2005), like us, proposes to use LCFRS but does not provide any evaluation results of his work. Very
Penn Treebank such that the trace nodes and co-indexations are converted into crossing branches and parse them with the parser presented in this article, obtaining promising results. Furthermore, van Cranenburgh, Scha, and Sangati (2011) and van Cranenburgh (2012) have also followed up on our work, introducing an integration of our approach with Data-Oriented Parsing (DOP). The former article introduces an LCFRS adaption of Goodman X  X  PCFG-DOP (Goodman 2003). For their evaluation, the authors use the same data as we do in Maier (2010), and obtain an improvement of roughly 1.5 points encounter a bottleneck in terms of parsing time. In van Cranenburgh (2012), a coarse-to-fine approach is presented (see Section 6.6). With this approach much faster parsing is made possible and sentences with a length of up to 40 words can be parsed. The cost of the speed, however, is that the results lie well below the baseline results for standard PLCFRS parsing.
 dependency-counterpart to discontinuity in constituency parsing. A meaningful com-parison is difficult to do for the following reasons, however. Firstly, dependency parsing deals with relations between words, whereas in our case words are not considered in the parsing task. Our grammars take POS tags for a given and construct syntactic trees.
Also, dependency conversion algorithms generally depend on the correct identification of linguistic head words (Lin 1995). We cannot rely on grammatical function labels, such as, for example, Boyd and Meurers (2008). Therefore we would have to use heuristics for the dependency conversion of the parser output. This would introduce additional noise.
Secondly, the resources one obtains from our PLCFRS parser and from dependency parsers (the probabilistic LCFRS and the trained dependency parser) are quite different because the former contains non-lexicalized internal phrase structure identifying mean-ingful syntactic categories such as VP or NP while the latter is only concerned with rela-tions between lexical items. A comparison would concentrate only on relations between lexical items and the rich phrase structure provided by a constituency parser would not be taken into account. To achieve some comparison, one could of course transform the discontinuous constituency trees into dependency trees with dependencies between heads and with edge labels that encode enough of the syntactic structure to retrieve the original constituency tree (Hall and Nivre 2008). The result could then be used for 114 the head-to-head dependencies one would obtain are not necessarily the predicate-argument dependencies one would aim at when doing direct dependency parsing (Rambow 2010). 8 8. Conclusion
We have presented the first efficient implementation of a weighted deductive CYK parser for Probabilistic Linear Context-Free Rewriting Systems (PLCFRS), showing that LCFRS indeed allows for data-driven parsing while modeling discontinuities in summary estimates of parse items, some acting as figures-of-merit, others allowing for
A  X  parsing. We have implemented the parser and we have evaluated it with grammars extracted from the German NeGra treebank. Our experiments show that data-driven LCFRS parsing is feasible and yields output of competitive quality.
 Acknowledgments References 116 118
