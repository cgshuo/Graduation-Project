 XIPENG QIU, LING CAO, ZHAO LIU, and XUANJING HUANG, Fudan University As a generic paradigm of textual inference recognition, recognizing textual entailment (RTE) [Dagan et al. 2006] gained growing attention in recent years. The RTE task is to automatically recognize, given two text fragments, whether the meaning of one text can be inferred from the other. The RTE task is generic relevant to various natural language processing (NLP) tasks, such as question answering (QA) [Harabagiu and Hickl 2006], information retrieval (IR) [Clinchant et al. 2006], information extraction (IE) [Cowie and Lehnert 1996], and machine translation (MT) [Pad  X  o et al. 2009], owing to the variability of semantic expression, which is a common phenomenon of natural language.

Recognizing Inference in Text (RITE) [Shima et al. 2011], organized by NTCIR, is a task for large-scale open evaluation for Simplified/Traditional Chinese and Japanese. There are two basic subtasks in NTCIR-9 RITE: the BC (Binary Class), MC (Multi Class). Given a text pair, the BC subtask aims to detect whether the entailment holds, while MC subtask demands more detailed inferential relations: F (Forward Entail-ment), R (Reverse Entailment), B (Bidirectional Entailment), I (Independence), or C (Contradiction).
 Table I provides some examples for both BC and MC subtasks.

In the NTCIR-9 RITE task [Shima et al. 2011], variant methods have been used by different participants. Specified to Simplified Chinese (CS), most of the systems with better performances use machine learning-based methods with their respective elabo-rate features or rules [Cao et al. 2011; Harris 2011; Mai et al. 2011; Ren et al. 2011; Zhang et al. 2011]. These machine learning-based systems provide shallow seman-tic analysis to recognize inference in text. They first extract surface string features, syntactic features or inference rules, and then they train classifiers on development datasets. Most of these systems use decision tree (DT) [Duda et al. 2001] or support vector machine (SVM) [Cristianini and Shawe-Taylor 2000] as their classifiers. Some additional resources are also used for lexical semantic analysis, such as HowNet [Dong and Dong 2006]. These resources can provide common sense knowledge information, such as lexical relations on synonym, antonym, hyponym, and so on.

Inspired by the best system [Harris 2011] in NTCIR-9 RITE task, a good RITE sys-tem should be able to utilize large amounts of semantic rules. However, if rules are just used as features in a classifier (SVM is used by Harris [2011]), they are underuti-lized in the respects of expression and logic entailment. People often recognizes the inference relation of the text pair through some logic rules, which can be formulated as first-order logic [Genesereth and Nilsson 1987]. These logic rules are quite hard to be factorized by the state-of-the-art systems.

Motivated by such situations, we present a novel approach for RITE with Markov logic networks (MLNs) framework [Domingos and Lowd 2009; Richardson and Domingos 2006] to incorporate the flexible semantic representations (first-order rules). MLNs explores an efficient way to combine first-order logic and probability in a sin-gle representation, which uses probabilistic graphical models [Lauritzen 1996] to effi-ciently handle uncertainty and first-order logic to compactly represent a wide variety of knowledge. We first define predicates and formulas for RITE and then transform shallow and deep semantic features to logic forms.
 Garrette et al. [2011] also use MLNs based method for RTE task. Different from Garrette et al. [2011], which uses sophisticated analysis tool Boxer [Bos et al. 2004] to obtain first-order logic forms directly, our work is based on dependency analysis and addresses the use of all kinds of existing knowledge resources in the Chinese RTE task, such as HowNet, WordNet [Stark and Riesenfeld 1998] and Extended WordNet [Harabagiu et al. 1999; Moldovan and Rus 2001]. Due to lack of deep analysis tools for Chinese language processing, the knowledge resources are vital for understanding Chinese texts.

We translate Chinese words into English and assign weights to the knowledge-related formulas based on lexical chain in WordNet. With MLNs, we can integrate different kinds of information into one whole reasoning framework and utilize the common sense knowledge and artificial rules in a more natural way. The weights of formulas can be learned with evidences or be given based on human experience.
The rest of the article is organized as follows. We first briefly describe Markov logic networks in Section 2. Then we analyze the task of recognizing inference in text in Section 3. Section 4 describes our approach of MLNs-based RITE. Section 5 presents our experimental analysis. In the end, Section 6 will summarize the article and our future efforts for improvement. Markov logic networks (MLNs) [Richardson and Domingos 2006] is a powerful frame-work combining first-order logic and probabilistic reasoning. A knowledge base (KB) is a set of first-order logic with weights. Different from traditional artificial intelligence (AI) framework, formulas of Markov logic are soft constraints: a world that violates a formula is less probable than one that satisfies it, but not impossible [Domingos and Lowd 2009]. A first-order knowledge base (KB) is a set of sentences or formulas in first-order logic [Genesereth and Nilsson 1987]. Formulas consist of following symbols.  X  Constants, which represent objects in the domain of interest.  X  Variables, which range over the objects in the domain.  X  Functions, which represent mappings from tuples of objects to objects.  X  Predicates, which symbols represent relations among objects in the domain or at-tributes of objects.

World knowledge expressed by natural language is able to transform to first-order logic form. For instance, the first-order logic form of  X  X IDS transmit from the mother tothechild X  X sasfollows.
 F ORMULA 2.1. MotherChild(p1, p2), AIDS(p1)  X  AIDS(p2)
In contrast to probability models which require complex distributions and details to be specified, first-order provides the ability to compactly and flexibly represent wide variety of knowledge and features. But a first-order knowledge base should be seen as a set of hard constraints. If a world violates even one formula, it has zero probability. It is worth noticing that only a few knowledge and features are always to be true. Actually, it is quite hard to come up with rules that are exactly correct in most domains, for such rules and features capture only a fraction of the whole world. Consequently, pure first-order logic has limited applicability to practical problems. Even the first-order rule above cannot be guaranteed. But what is true is the following.
So it seems that such problems could be well solved by probabilistic graphic mod-els, which enable us to efficiently handle uncertainty. But pure probabilistic graphic models are unable to compactly and flexibly represent knowledge and features. A Markov network [Pearl 1988] is a model for the joint distribution of a set of vari-ables, with mapping to each node of an undirected graph. Each clique in the graph corresponds to a potential function  X  k , which is a non-negative real-valued function of the state of the clique. The joint distribution represented by a Markov network is given by the following equation.
 where x { k } denotes the k th clique and Z is a normalizing term Z = x  X  X k  X  k ( x { k } ). The Markov logic networks (MLNs) framework explorers a way to incorporate first order knowledge base with probability. Compared with first-order knowledge base, such framework adds the ability to deal with uncertainty and tolerate contradictory. Compared with probability graphic models [Jordan 1998], such framework provides novel methods to compactly and flexibly represent knowledge and features. The syntax of logical formulas in an MLN is the standard syntax of first-order logic. MLNs soften the constraints of first-order logic by attaching weight to each formula (or rule). In the basic idea of MLNs, when a world violates a formula, it is less probable instead of impossible. The weight indicates the constraints of a formula to a world: the fewer formulas a world violates, the more probable it is. Consequently, the higher the weight is, the stronger the formula constrains MLNs inference system. It is obvious that the MLNs framework is able to transform into pure first-order logic by associating each rule with a weight of +  X  .

According to Domingos and Lowd [2009], a Markov logic network is formally defined Markov network M L , C as follows.  X  M L , C contains one binary node for each possible grounding of each predicate appear-ing in L . The value of the node is 1 if the ground predicate is true, and 0 otherwise.  X  M L , C contains one feature for each possible grounding of each formula F i in L. The value of this feature is 1 if the ground formula is true, and 0 otherwise. The weight of the feature is the w i associated with F i in L .

Actually, a MLN is a pattern of Markov networks. Different sets of constants (weights) may ground different networks. But all groundings have certain structures and parameters, and the same formula will have the same weight in all groundings. From the definition of MLNs and Equation (2.2), the probability distribution over possible worlds x specified by the ground Markov network M L , C is given by the fol-lowing equation.
 of the predicates appearing in F i ,and  X  i ( x { i } )= e w i .

Conceptually, the MLNs framework consists of the MLNs program, the evidence, and the query. The MLNs program includes predicate definitions and rule definitions. Each rule of an MLNs program is associated with a weight, and each predicate ap-pearing in a single rule should be defined in the predicate definition. The evidence is a set of predicates that were previously certified to be true, and the query is what MLNs framework eventually predict. Table II briefly illustrates an MLNs framework for AIDS predicting. Recognizing textual entailment (RTE) is a task of identifying directional inferential relations between texts. The task contains two basic subtasks: BC (Binary Class) and MC (Multi Class).

Given a text pair (t1, t2), BC subtask aims to detect whether the entailment holds (t1 entails t2), while MC subtask demands more detailed inferential relations includ-ing the following categories.  X  Forward Entailment (F) . If t1 entails t2 while t2 does not entail t1.  X  Reverse Entailment (R) . If t2 entails t1 while t1 does not entail t2.  X  Bidirectional Entailment (B) . If t1 entails t2 and t2 entails t1. Bidirectional entail-ment can be seen as paraphrase [Androutsopoulos and Malakasiotis 2010].  X  Independence (I) . If neither t1 nor t2 entails each other while t1 and t2 are not in contradictory.  X  Contradiction (C) . Semantic expression of t1 and t2 are in contradictory.
Most RITE systems use machine learning-based methods which provide shallow se-mantic analysis. Dependency analysis is of great importance for such systems because while the feature representation of dependency relations has great difficulty, the de-pendency representation is so flexible that it can hardly be factorized soundly. A com-mon way to handle dependency representation is to calculate the structural similarity score between dependency trees. Haghighi et al. [2005] provides a method for calculat-ing matching score between the dependency tree of texts by utilizing graph-matching techniques. But it is obvious that the matching score is insufficient for dependency structures without complex feature representation.

Such situations are well explained in the examples given by Table III, where the syntactic structures and dependency structures of the text pairs are quite hard to dis-tinguish without complex feature representation. The attributes of subjective, objec-tive, and predicate interact quite a lot with each other in a single dependency relation.
For example, both the text pairs in lines 1 and line 2 have the same subject  X   X  X  X  X  X  (population) X  and different predicates  X   X  X  (first)/  X  X  X  (very large) VS  X  X  (first)/  X  X  (second) X  1 . However, the entailments are totally different in line 1 and line 2.

For another example, both the text pairs in line 3 and line 4 have different at-tributes of subject  X   X  X  (China)/  X  X  (first) X . However, the entailments are also totally different in line 3 and line 4.
Furthermore, observing lines 5  X  7, the dependency relation of the three text pairs has the same pattern, that is,  X  X ame predicates, different subjects, and different ob-jects X , but the semantic relations of these pairs are also quite different. Different from line 4, line 5 exchanges the position of subject and object in the dependency relation. And the predicate  X  X qual to X  in line 7 has symmetric attribution while  X  X ore than X  in line 5 does not have.

The reason in the above cases is that dependency relation does not contain rich semantic information. Therefore, such problems need to utilize the semantic informa-tion with semantic knowledge bases such as HowNet, WordNet and Wikipedia 2 ,which provide common sense knowledge information for words and expressions.

However, there is another problem to be settled is how to translate the semantic information into feature representation. Feature representation of knowledge bases has long been a difficulty for textual entailment approaches, as the knowledge repre-sentation is various. Some knowledge resources use logic form and others use net-work form. For example, WordNet present more than 10 different kinds of lexical semantic relations between words, and WordNet gloss provides further information for each synonym sets (synsets). Wikipedia provides more complex relations between concepts as content, category, info-box link, and so on.

When we wish to fully exploit all kinds of knowledge resources, we need to use a unified form to represent them. In this section, we present a MLNs-based system for RITE. First, we will describe the architecture and pipeline of the system. Afterward, we will provide the meth-ods for MLNs rules generation from both the natural language parser and the knowl-edge bases. Finally we will introduce MLNs inference steps in detail, including MLNs grounding and MLNs searching. Figure 1 illustrates the system architecture of our MLN-based RITE. As mentioned in Section 2, the MLNs framework consists of predicates and formulas definition, evi-dence, and query.

We define MLNs formulas (or rules) mainly in the following three ways.  X  Transform shallow semantic features into logic form.  X  Generate deep semantic rules from HowNet and Extended WordNet.  X  Manual selection.
 Evidence is generated by semantic analysis of text pairs. As illustrated in Figure 1, text pairs are initially delivered into the semantic information extraction module, which do following work.  X  Word segmentation, which is particularly necessary for Chinese.  X  Part-of-speech tagging.  X  Dependency parsing.  X  Date and time normalizing.  X  Deep semantic parsing with HowNet [Dong and Dong 2006] and Extended WordNet [Harabagiu et al. 1999].

To extract semantic information from texts, we use our implemented open source toolkit, FudanNLP 3 , for natural language processing. Specially optimized for Chi-nese language, FudanNLP provides various language processing functions including word segmentation, part-of-speech tagging, named entity recognition, syntactic pars-ing, time phrase recognition, anaphora resolution, and so on. After preprocessing, the system automatically generates evidence by the semantic information of text pairs, according to predicate definition. Finally, with generated evidence, predefined MLNs rules and predicates, the MLNs logic prover provides a conclusion for the inferential relation of text pairs. For the shallow semantic features (such as named entity, time, number, negations, and antonyms), we first transform them into the corresponding first-order logic forms. The weights of shallow semantic logical formulas are given by training. 4.2.1. Named Entity. Named entities in a text pair ( t 1, t 2) are very important while recognizing their inferential relations. Generally, if t 2 refers to a named entity which is not mentioned by t 1, then t 1 can hardly entails t 2, and vise versa. See the following example.
The named entity  X   X  X  X  (Britain) X  mentioned by t 2 does not exist in t 1. t 1 could not entail t 2since t 1 cannot provide any information about Britain. While all the named entities mentioned in t 1, including  X   X  X  X  (Hong Kong) X  and  X   X  X  (China) X , appear in t 2. It is more likely that t 1 can be entailed by t 2. In fact, the semantic relation of the text pairs is  X  X  X  (reverse entailment).

For the MLNs framework, we transformed such feature into the following logic forms.
  X  Contains(t 1 , NE), Contains(t 2 ,NE)  X  X  Entail(t 1 ,t 2 ),  X  Contains(t 1 , NE1), Contains(t 2 , NE1), Contains(t 1 ,NE2),  X  Contains(t 2 ,NE2)  X  Independent(t 1 ,t 2 ), where NE is a named entity.

Handling coreference problem is also essential to RTE systems. The coreference problem can be divided into two different kinds. One is that a same named entity rep-resented by different semantic expressions, such as  X   X  X  X  (America) X  and  X   X  X  X  X  X  X  X  X  (The United States of America) X . This problem can be soundly settled by HowNet and the info-box of Wikipedia. Another is the anaphora resolution. We use FudanNLP toolkit to recognize third person pronouns and replace them with their corresponding antecedents. As the text and hypothesis in a text pair do not have context dependency, this kind of problem seems not quite necessary for the RITE task. We will seriously consider the coreference problem in our future work, as the problem is necessary to many other RTE applications. 4.2.2. Time. Information about time or number in a text is also worth noticing. Given a text pair ( t 1, t 2), if t 1and t 2 happened in different periods of time, they can hardly be entailed by each other. And if we know when t 1 happened while the time t 2 happened is not mentioned, then t 2 cannot entail t 1. The following example may better explain the facts.
To analyze the time of event, dependency analysis is necessary. If the same events happened in different periods of time, the text pair is possibly contradictory. As for those different events, which happened in different times, they are possibly indepen-dence without additional indication. Look at the following example.

Considering such circumstance, we represent the feature of time with logic form as the following.
 TimeStamp(t 1 , event, time1), TimeStamp(t 2 , event, time2), time1 = time2  X  Contradictory(t 1 ,t 2 )
A difficulty is that time can be represented by various kinds of expressions. For instance, the following expressions have the same meaning.
Sometimes text pairs are even lexically equal if times are normalized.
We use a rule-based time normalizer implemented in FudanNLP to handle such problems.
 4.2.3. Number. Number information in text pairs is also worth paying attention to, as in the following example. The scores Jingjing Guo got in t 1and t 2 are not identical, so they are contradictory. Number information is handled similarly as time.
 Number(t 1 , event, num1), Number(t 2 , event, num2), num1 = num2  X  Contradictory(t 1 ,t 2 ) 4.2.4. Negations and Antonyms. The existence of negation words and antonyms may indicate contradictions between text pairs. So recognizing negations and antonyms in texts is indispensable for textual entailment. HowNet X  X  [Dong and Dong 2006] antonym dictionary provides a good method for finding antonyms in Chinese language, as in the following example.
HowNet explains  X   X  X  X   X  X s X  { enlarge |  X  X  X  }  X  X nd X   X  X  X   X  X s X  { shrink |  X  X  X  }  X . We can recognize  X   X  X  X   X  X nd X   X  X  X   X  as antonyms on semantic level by looking up HowNet X  X  antonym dictionary.
 Negation of a text is always expressed by negation indicators, which is limited in Chinese language as single Chinese characters such as  X   X  ,  X  ,  X  ,  X  ,  X  ,  X   X . But such indicators do not confirm the negation of texts. In some cases, indicators may appear in Chinese words that are not negative, such as  X   X  X  X  (very),  X  X  X  (Africa) ,  X  X  X  (future) X . On the other hand, different dependency relations may substantially influence the semantic meaning of negation indicators and antonyms, as in the following example. The text pairs above share a common negation word  X   X  X  X  (do not) X , while the meaning completely diverged by different dependency relations between words. In the first pair, the negation  X   X   X  (do not) X , depends on  X   X  X  (publish) X , expressed the meaning of  X  X o not let other know X . While the second  X   X  X  X  (do not) X , depends on  X   X   X  X  X  (purchase) X , expressed the meaning of  X  X o not have a plan X . Without dependency analysis, system cannot distinguish the different semantic expressions of negation and antonym. So we represent the logic form of negations and antonyms as the following.
 Antonym(w1, w2), Dependency(t 1 , w1, object), Dependency(t 2 , w2, object)  X  Contradictory(t 1 ,t 2 ) where w1 and w2 are two different words.

Contains(t 1 , object), Dependency(t 2 , negation, object)  X  Contradictory(t 1 ,t 2 ) Though transformed into logic forms, shallow semantic features are insufficient for textual entailment task, as in the following example. We cannot extract any effective evidence by shallow semantic analysis, but rules can be generated by deep semantic analysis as shown in the following example. F ORMULA 4.7. PURCHASE(Charlie, banana)  X  BUY(Charlie, banana) F ORMULA 4.8. BUY(Charlie, banana), IS-A(banana, fruit)  X  BUY(Charlie, fruit) In such cases, deep semantic analysis is especially important. 4.3.1. WordNet Lexical Chain. Among manually prepared lexical resources, Word-Net [Miller 1995] is most widely used in variant applications. WordNet is a lexical knowledge base which organized English nouns, verbs, and adjectives into synonym sets (synsets). WordNet specifies relations including hypernym, hyponym, synonym, meronym, holonym, attribute, cause and entailment, and these relations can be used for semantic inference. And for each synset, WordNet provides a piece of gloss to ex-plain the meaning of such synset in natural language.

WordNet relations provide the ability to recognize semantic relations between words. While the relations are quite sparse between words of different part-of-speech tags, in many cases, it is essential to get the connection between words that are topic related. For instance, see the following example. where the topic relation between  X  X hirsty X  and  X  X ater X  is particularly important, as  X  X hirsty X  indicates a desire to drink water.

Moldovan and Novischi [2002] proposed a novel approach to recognize topic rela-tions between words with a lexical chain. Lexical chain is a sequence of semanti-cally related words that link two synsets. Besides original WordNet relations, lexical chain develops information in the gloss of WordNet synsets, which makes topic rela-tion recognition of synsets surprisingly easier. Considering the example above, we can find the following information.  X  Gloss of  X  X hirsty X : feeling a need or desire to drink  X  Gloss of  X  X rink X : take in liquids  X  Hyponym of  X  X iquid X : water
Therefore, the topic relation between  X  X hirsty X  and  X  X ater X  can be recognized by the following lexical chain.

Obviously, there are several possible lexical chains between two words. Their topic relation is calculated by summing up the scores of all paths of a lexical chain, which keeps following principles.
 According to the principles, topic relation between words w 1and w 2 is given by where p i represents a lexical chain path between words w 1and w 2, and score ( p i )is defined as the following. where I is the initial score. W R where N R  X  Gloss is the total amount of R  X  Gloss relation of a word ( R  X  Gloss relation is reverse to gloss relation). According to Moldovan and Novischi [2002], we choose CONST = 500.
 The MLNs logic form of lexical chain is represented as the following.

F ORMULA 4.9. predicate(w1, w2), TopicRelated(w2, w3)  X  predicate(w1, w3), where  X  X redicate X  will be grounded according to the predicate definitions, and the weight of each grounded formulas will be given according to the topic related score calculated by Equation (1).
 Noticing that the concept of synset is not limited to English, since the design of WordNet is inspired by current psycholinguistic theories of human lexical memory. We also use the synset to calculate the relation between Chinese words. We use HowNet to translate Chinese words into English and find the corresponding synsets. Extended WordNet [Harabagiu et al. 1999] is a lexical knowledge base that enhanced WordNet [Stark and Riesenfeld 1998] morphologically and semantically. Extended WordNet disambiguated all gloss of WordNet and mapped them to corresponding synsets. Furthermore, Extended WordNet transformed each gloss of WordNet to logic form which first introduced by Hobbs [1985] and enhanced with semantic and syntactic information by Harabagiu et al. [1999].

A predicate for such logic form is generated for every noun, verb, adjective, or ad-verb encountered in any gloss. However, in Formula 4.10 and others, predicates are represented as the concatenation of the base form and POS, and WordNet sense is not incorporated. It seems inconsistent. For example, the logic form of synset  X  X reathe X  is the following.
 breathe:VB(e1, x1, x2)  X  draw:VB(e2, x1, x3) air:NN(x3) into:IN(e2, x4) and:CC(e1, e2, e3) expel:VB(e3, x1, x3) out of:IN(e3, x4) lung:NN(x4) The transformation of WordNet gloss into logic form is to follow the syntactic parser. For each grammar rule of the parser, one or more transformation rules are provided for logic form transformation.

To make use of the logic form of WordNet gloss in Extended WordNet, we should transform them into Markov logic representation. The Markov logic form of WordNet gloss represents verbs as predicates and nouns as parameters of the corresponding predicates. We manually prepare rules according to the part-of-speech tag and the parameter of the predicates. More than one Markov logic rule may be generated by a single gloss. Considering the example above, two pieces of Markov logic form rules are extracted as the following.
 F ORMULA 4.11. breathe(subject, object)  X  draw into(subject, air, lung)
F ORMULA 4.12. breathe(subject, object)  X  expel out of(subject, air, lung) 4.4.1. Verb Predicates. All verb predicates of Extended WordNet logic form have three arguments:  X  e i represents the eventuality of the action, state or event i stated by the verb to take place.  X  x m represents the syntactic subject of action, state or event.  X  x n represents the syntactic object of action, state or event.

For MLNs logic form, we can directly transform Verb ( e i , x m , x n )to Verb ( x m , x n ). 4.4.2. Noun. A noun predicates of Extended WordNet logic form has only one argu-ment x i which denotes the tag of the noun in formula. Nouns are not represented as predicates in MLNs logic form. So we substitute the noun predicates for their tags in the formula. After such substitution, we transform the gloss of  X  X reathe X  as the following.
 breathe:VB(e1, x1, x2)  X  draw:VB(e2, x1, air) into:IN(e2, lung) and:CC(e1, e2, e3) expel:VB(e3, x1, air) out of:IN(e3, lung) 4.4.3. Conjunctions. The predicates of conjunctions have a variable number of argu-ments, which enable the aggregation of several predicates under the same syntactic role. We deal with conjunctions by dividing the logic formula into pieces according to the arguments of conjunctions predicates. With conjunction handling, we divide the logic formula of  X  X reathe X  as the following.
 breathe:VB(e1, x1, x2)  X  draw:VB(e1, x1, air) into:IN(e1, lung) breathe:VB(e1, x1, x2)  X  expel:VB(e1, x1, air) out of:IN(e1, lung) 4.4.4. Prepositions. The preposition predicates of Extended WordNet logic form al-ways have two arguments: The first argument corresponding to the predicate of the head of the phrase to which prepositional phrase is attached, whereas the second ar-gument corresponds to the prepositional object. As prepositions are not represented as predicates in MLNs logic form, we attached object to the head of the preposition according to the arguments of the predicate. We transformed the previous formulas one step further as the following.
 breathe:VB(e1, x1, x2)  X  draw into(e1, x1, air, lung) breathe:VB(e1, x1, x2)  X  expel out of(e1, x1, air, lung) 4.4.5. Complements. The predicates of complement, including adjectives and adverbs, share the same arguments as the nouns or verbs they modify. The problem of comple-ments transformation is quite hard, as MLNs predicates do not receive modifiers as arguments, and complements should not be represented as predicates. To deal with such difficulty, we finally represent modifiers corresponding to the dependency rela-tions of shallow semantic MLNs rules. Considering the gloss of  X  X hout X : shout:VB(e1, x1, x2)  X  utter:VB(e1, x1, x4) in:IN(e1, x3) loud:JJ(x3) voice:NN(x3) We represent the modifier  X  X oud X  as
Contains(t 1 , shout)  X  Dependency(t 1 , loud, voice) The MLNs framework consists of predicates definition, rules definition and evidence. After the previous step, we acquire first-order logic rules for MLNs framework. With MLNs rules, we generate predicates of MLNs and extract evidence from text pairs according to the predicates. We utilize Tuffy [Niu et al. 2011] for MLNs inference. We will next introduce grounding and searching methods of MLNs inference. 4.5.1. Grounding. According to Domingos and Lowd [2009], a possible world is a set of objects, a set of functions (mappings from tuples of objects to objects), and a set of rela-tions that hold between those objects; together with an interpretation, they determine the truth value of each ground predicate. The most direct way to obtain ground clauses of an MLNs formula is to enumerate all possible assignments to the free variables in the formula, as Algorithm 1 shows. However, this method is inefficient and unneces-sary in practice. Tuffy substantially optimize the algorithm by pruning groundings that have no effect on inference results [Niu et al. 2011]. 4.5.2. Searching. A basic type of inference in Markov logic is to find the most likely state of the world consistent with some evidence. This problem is NP-hard in general, but effective solvers exist, both exact and approximate. Algorithm 2 shows the Pseudocode for MaxWalkSAT, which is the most commonly used approximate solver. MaxWalkSAT performs this stochastic search by repeatedly picking an unsatisfied clause at random and flipping the truth value of one of the atoms in it. With a certain probability, the atom is chosen randomly; otherwise, the atom is chosen to maximize the sum of satisfied clause weights when flipped. This combination of random and greedy steps allows MaxWalkSAT to avoid getting stuck in local optima while search-ing. MaxWalkSAT can solve hard problems with hundreds of thousands of variables in minutes [Domingos and Lowd 2009; Kautz et al. 1997]. In this section, we empirically evaluate the performance of our MLNs-based system in RITE task on Simplified Chinese texts.

We trained and tested the performance of our system on the NTCIR-9 RITE develop and test set, separately [Shima et al. 2011]. The statistic information of dataset is summarized in Tables V and Table VI.
 We first observe the performance of both two-way and five-way entailment task. Then we provide a label-by-label analysis for multi-classification of entailment task. Afterward, we present a probabilistic analysis of MLNs framework compared with SVN-based framework. Finally, we analyze shallow semantic and deep semantic MLNs rules in the system.

To evaluate the performance of our approach, we compare it with the following methods.  X  The best learning-based approach ICRC [Zhang et al. 2011] among all participants on both BC and MC Simplified Chinese subtasks in NTCIR-9 RITE.  X  Baseline method of NTCIR-9 RITE: A character overlap method provided by
NTCIR9-RITE [Shima et al. 2011].  X  FudanNLP: Our previous SVM-based approach in NTCIR-9 RITE. The system extracts features by recognizing lexical relations between text pairs, such as the equation of name entities, dates, times, and numbers, or the existence of synonyms hyponyms, antonyms, and negations.

Tables VII and VIII show that the MLNs-based system substantially improves the performances of both the BC and MC subtasks. As mentioned before, the MLNs framework is a combination of first-order logic and probabilistic reasoning. In this experiment we would like to settle whether the model of weighted formulas performs well as other likelihood methods in confidence scaling. The Confidence-Weight Score (CWS) [Dagan et al. 2006] evaluates the systems X  ability to assign a higher confidence score to the correct judgments than to the wrong ones. The calculation of CWS is given by Equation (4). where n is the number of the pairs in the test set, and i ranges over the pairs sorted by their confidence (in decreasing order).

We sort the classification results of our system by the confidences given by MLNs marginal inference [Niu et al. 2011]. We take our previous system as baseline, where results are sorted by the confidences of SVM outputs.

According to Table IX, the MLNs-based approach performs better than our previous method. The Confidence-Weighted evaluation substantially awards the MLNs-based approach by 4% and 6% in BC and MC subtasks separately. Furthermore, by observing test results, the accuracy of the top 50 text pairs is shown in Table X, which is much higher than global accuracy. In this part we provide further discussion on system performance of different entail-ment relations. We present label-by-label analysis and case study on each semantic class in RITE MC subtask, where precision, recall and F1-Measure are given. We use our previous work as the baseline. 5.2.1. F-Label and R-Label Analysis. Though directionally diverged, F-label and R-Label share the same entailment relation between texts. As Figures 2 and 3 illustrate, the MLNs-based system substantially improve the performance of F-label and R-label (see Tables XI and XII). Observing the statistics of our previous work, a large amount of F and R labeled text pairs are mistakenly classified as I, as shallow-semantic features can hardly get the semantic connections between words. With deep semantic MLNs rules generated by Lexical Chain and Extended WordNet, such problems are soundly handled.

In the 40th text pair of NTCIR-9 RITE test set the following occurs. The word  X   X  X   X  X n t 1and X   X  X  X  X  X   X  X n t 2 are topic related, but not synonyms in Chinese. HowNet translate  X   X  X   X  as  X  X lace X  and  X   X   X   X   X   X  as  X  X nfrastructure X , and we can get the topic relation between  X  X lace X  and  X  X nfrastructure X  following the lexical chain as the following.
 where
Consequently, the MLNs-based system successfully classifies this text pair as R, while classification result of our previous system is I. 5.2.2. B-Label and I-Label Analysis. As illustrated in Figures 4 and 5, the performance of both B-Labels and I-Labels are improved by MLNs-based system (see Tables XIII and XIV), especially for the precision of I-Label, as our system enables deep semantic relation recognition between words, which has just been mentioned in the previous part.

What worth noticing is that the recall of B-Label is much higher than precision while the precision of I-Label is much higher than recall. In fact, many I-Label text pairs are mistakenly labeled as B in our current system, as deep semantic MLNs rules extend the connection of words by a large margin, which may sometimes pro-duce noises, as the 244th text pair of NTCIR-9 RITE test set shows. Lexical chain detects the topic relation between  X   X  X  X   X  (man and woman) in t 1and  X   X  X   X  (lover) in t 2, which misleads the system to tag such pair as B.

Furthermore, I-Label text pairs may sometimes be classified as C because of the dependency analysis, as the 180th text pair of NTCIR-9 RITE test set shows.
With dependency analysis, system finds out that same people infect different kind of virus, which is contradictory. But  X  X he residents of Hong Kong X  does not mean all residents of Hong Kong. The range and scope of semantic meanings are not considered in our system.

But still, the performance of both I-Labels and B-Labels are improved, as the shal-low semantic features are optimized while converting to MLNs rules, and the rules are soft constraints in MLNs framework. The probabilistic experiment and feature analysis will be presented later. 5.2.3. C-Label Analysis. As Figure 6 shows, the MLNs-based system surprisingly im-proved the precision of C-Labels (see Table XV) by 19% without sacrificing the recall (even increased by 7%). The dependency relation detection of negations and antonyms effectively works in our system.

For the sixth text pair of NTCIR-9 RITE test set, the following occurred.
The  X   X   X  (always) in t 1and X   X   X  (rarely) in t 2 are antonyms, but the dependency relation is quite different. Our previous system classified this text pair as C while our current system corrects it.

Above all, these experiments show that our method improves the RITE performance by integrating several knowledge resources instead of using them separately. It is also to boost the performance to introduce semantic information knowledge based on de-pendency analysis. Recognizing Textual Entailment task is attracting widespread attention in recent years. PASCAL/TAC RTE has been conducting a series of shared task evaluations [Bar-Haim et al. 2006; Bentivogli et al. 2009; Dagan et al. 2006; Giampiccolo et al. 2007] for entailment relation classification tasks, as well as EVALITA/IRTE [Bos et al. 2009] for Italian language.

The earliest approaches of textual entailment task are always in a bag-of-word fash-ion which assuming the independence between words [Corley and Mihalcea 2005]. A pair is then in entailment when sim (t1 , t2) &gt; threshold . Such system is used as a baseline for evaluation in NTCIR-9 RITE. Different methods may operate at different levels of representation of the input expressions; for example, they may treat the in-put expressions simply as surface strings, they may operate on syntactic or semantic representations of the input expressions, or on representations combining information from different levels [Androutsopoulos and Malakasiotis 2010]. For example, Haghighi et al. [2005] provide a method to handle dependency relation of texts, which utilizes graph-matching techniques to calculate match score between the dependency tree of texts.
 Several logic based approaches have been proposed in the RTE task.

Bos and Markert [2005] check whether an entailment holds with theorem prover and model builder. These automated reasoning tools rely on discourse representation structures for text pairs as well as lexical and world knowledge. Tatu and Moldovan [2007] transform the text pairs into three-layered semantically rich logic form repre-sentations, and search for a proof for the entailment between the text and a possibly relaxed version of the hypothesis, with world knowledge bases such as XWN Lexical Chain. But without probabilistic reasoning, first-order logic-based RTE systems have no way to handle uncertain knowledge.

Garrette et al. [2011] present an MLNs based framework to combine logic-based meaning representations with probabilities. Since they can obtain directly approxi-mate first-order logical representations from text with Boxer [Bos et al. 2004], their work mainly focuses on three natural language phenomena and their interactions: im-plicativity and factivity, word meaning, and coreference. However, there are no sophis-ticated deep analysis tools for Chinese, so we need to design the more skillful formulas and use all kinds of world knowledge bases to help the understanding on Chinese texts. We present a more natural way to make use of common sense knowledge bases by transforming them into Markov logic forms.

In NTCIR-9 RITE Simplified Chinese subtask, Zhang et al. [2011] extract three types of different linguistic level features (EDIT-based features, directional entailment features, contradiction feature) and use cascade voting to combine several classifiers. The system achieves best performances among learning-based systems.

Our previous system [Cao et al. 2011] utilizes FudanNLP as a Chinese language analyzer, which extracts various shallow semantic features including named entities, date and time expression, word overlap and n -gram score, negation, and antonym. These shallow semantic features are efficient, but not sufficient, for the textual entail-ment task. In this article, we propose a novel approach for recognizing inference in texts with Markov Logic Networks (MLNs), which combines semantic analysis with all kinds of knowledge resources. Our approach can use more flexible semantic representations for the reference rule, which not only covers the surface, syntactic and semantic features used in state-of-the-art RITE systems, but provides the ability of complex inference rules as similar as possible to that people thinks. We translate all kinds of features used in traditional machine learning-based methods into MLNs predicates and formu-las. Experiment results show that our system can achieve better performance than state-of-art RITE systems.

In the future, we will design more respective formulas for specific RITE tasks, and will utilize more deep sentence-level semantic information (such as PropBank [Palmer et al. 2005; Xue and Palmer 2003]) to improve the performance. We also wish to inves-tigate our approach in the relevant applications, such as question answering.
