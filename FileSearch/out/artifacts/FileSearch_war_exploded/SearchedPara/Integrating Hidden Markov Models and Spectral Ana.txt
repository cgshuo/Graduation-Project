
We present a novel approach for clustering sequences of multi-dimensional trajectory data obtained from a sensor network. The sensory time-series data present new chal-lenges to data mining, including uneven sequence lengths, multi-dimensionality and high levels of noise. We adopt a principled approach, by first transforming all the data into an equal-length vector form while keeping as much tempo-ral information as we can, and then applying dimensionality and noise reduction techniques such as spectral clustering to the transformed data. Experimental evaluation on syn-thetic and real data shows that our proposed approach out-performs standard model-based clustering algorithms for time series data.
Clustering is a fundamental and widely used technique in machine learning and data mining areas. Traditional clus-tering algorithms typically assume that data are represented by feature vectors of equal lengths and low dimensionality, and contain little noise. Using traditional approaches, such data are normally handled either by model-based methods or similarity-based methods [6]. However, as pointed out by [7], in many real-world applications, the dynamic char-acteristics of an environment often evolve over time, and as such, produce data that are of multi-dimensionality, contain noise and are of uneven lengths. A typical example is a sen-sor network, which has gained a great amount of interest in recent years. In a sensor network, a moving object may receive sequences of signals, whose values are stochastic in nature; that is, they may change significantly depending on the relative distance between the moving object and a set of sensors in the wireless environment. In such a situation, it is highly desirable to have a method that can produce accurate clusters even when the data are non-traditional.
In this paper, we address the problem of clustering temporal sequences into different groups. Consider a data set C consisting of N temporal sequences, C = {
Y 1 ,Y 2 ,...,Y N } . Each sequence Y is a multivariate time series containing T measurements from a set of sensors such that Y = { O t | 1  X  t  X  T } . The observations O t are potentially multi-dimensional signal vectors containing stochastic values measured from each sensor at a given time point t . Our ultimate goal is to automatically partition a set of N temporal sequences C = { Y 1 ,Y 2 ,...,Y N } into K clusters such that the inter-cluster similarity is small and intra-cluster similarity is large. However, a key question is: when | O t | is very large and O t contains lots of noise, and when | Y i | are all of different lengths, how can we obtain the clusters of high quality?
Clustering multi-dimensional sensory time series is in-herently more complex than clustering traditional data that are of fixed dimensions and contain little noise. First, tem-poral sequences may be of different lengths, making it dif-ficult to embed them into a metric space and use a distance measure such as the Euclidean distance to determine the similarity matrix. For sensory data obtained from a trajec-tory in a sensor network, the sequences obtained at different times may be of different lengths. Second, the data obtained by sensors at different time points are stochastic in nature, making it very difficult to apply straightforward similarity measures. Moreover, each sequence may consist of varying amounts of temporal information. Therefore, it is highly desirable to design an effective transformation method to extract important temporal information from the sequences while reducing the noise and dimensionality of the data.
In this paper, we propose a novel clustering algorithm to deal with the above-mentioned problems. Inspired by the work of [15], we first devise an affinity similarity matrix between the observation sequences. Our similarity matrix is obtained by modeling each sequence as a hidden Markov model (HMM), so that the log-likelihood of each sequence given each model can be computed. These probability mea-sures are then used to construct a uniform-length feature-vector to capture the temporal information from each se-quence, regardless of its length. We contend and experi-mentally verify that this transformation step indeed captures the needed global temporal information, even when the data are sequences of vectors.

Our second innovation is reducing the noise and dimen-sionality at the same time. After the first step, each time series is modeled as an HMM. This representation allows us to construct an affinity similarity matrix for all the se-quences. We then apply spectral methods [10, 11] on the similarity matrix for noise and dimensionality reduction. Specifically, a spectral clustering algorithm uses significant eigenvectors of the affinity matrix to map the original sam-ples into a lower dimensional subspace. These vectors are then clustered by standard clustering algorithms in the Eu-clidean space. Through performing eigendecomposition, a spectral analysis algorithm helps reduce the uncertainty of the similarity matrix. This enhances the quality of subse-quent clustering. We demonstrate the effectiveness of the two-phase approach through tests on synthetic and real data. The real data are obtained from a wireless local-area net-work (WLAN) environment, which consist of a number of access points (APs) that are used to track a client X  X  move-ment trajectories. We show that our proposed algorithm can produce very good clustering results in this uncertain envi-ronment.

The remainder of this paper is organized as follows. Sec-tion 2 discusses previous work related to clustering algo-rithms. Section 3 briefly reviews HMMs and the algorithms used to train HMMs from the observation data. Section 4 describes our proposed algorithm for clustering temporal sequences. Section 5 presents the results of experiments using both synthetic and real data. Section 6 concludes the paper and discusses future work.
Over the years, temporal-sequence clustering has at-tracted much attention in research and practice, because many problems that range from stock-market analysis, se-curity monitoring, to gene analysis involve time-series clus-tering. In general, clustering can be classified into two broad categories [2]: model-based approaches [1,5,15]and similarity-based approaches [4, 13].

For similarity-based approaches, the main task is to de-fine pairwise distances between all the sequences and then apply distance-based clustering algorithms. For example, Eisen et al. [4] adopt correlation coefficients to define the similarity among gene expression data from different time-course experiments. Agglomerative hierarchical clustering is then applied to find clusters of genes with similar pat-terns of expression. Oates et al. [13] use Dynamic Time Warping (DTW) to measure the similarities between multi-variate experiences of mobile robots. For complex problem domains, similarity-based approaches encounter great diffi-culty in how to define effective similarity measures. This definition, which is difficult to obtain, can affect the cluster-ing quality to a large extent.

Model-based approaches rely on an analytical model for each cluster where the objective is to find the best mod-els to fit the observation sequences. Examples of mod-els include regression models [5], ARMA models[16] and HMMs [1, 3, 9, 15]. Among these models, HMMs have attracted increasing attention over the last decade. Smyth [15] presents a probabilistic model-based approach to clus-tering sequences using HMMs. This approach first devises a pairwise distance matrix between observation sequences by computing a symmetrized similarity. This similarity is obtained by training an HMM for each sequence, so that the log-likelihood of each sequence, given each model, can be computed. This information is then used to cluster the sequences into K groups using a hierarchical clustering al-gorithm. After that, one HMM is trained for each cluster; the resulting K models are then merged into a  X  X ompos-ite X  global HMM. This initial estimate is further refined using the EM algorithm. As a result, a global HMM for modeling all the sequences is obtained. In [9], the model-based HMM clustering problem is addressed by focusing on the model selection issue, i.e., searching for the best HMM topology and finding the most likely number of clusters. In [12], the clustering result obtained using DTW as a sim-ilarity metric is used to provide an estimate of K and to yield an initial partitioning of the data. While model-based approaches provide a general probabilistic framework for clustering temporal sequences, the quality of clustering de-pends critically on the initial conditions. In addition, since an HMM assumes that the observations are independent, the EM algorithm used to train the mixture model cannot achieve a good resolution when a large amount of noise ap-pears in consecutive observations.

Our work is also closely related to spectral clustering , a new clustering algorithm that has emerged over the past few years and has been successfully applied to the problem of image segmentation [10, 11]. These methods use sig-nificant eigenvectors constructed from a pairwise similarity matrix between pixels and then group the pixels into im-ages in a spectral domain. However, most spectral methods assume the number of clusters K to be known in advance, whereas the estimation of the optimal number of clusters has not been well studied. In this paper, we apply spec-tral clustering techniques to help remove the noise in the temporal data and to automatically determine the number of clusters for the problem domain.

Our objective is to cluster multi-dimensional time series data that originate from tracking a moving object in a sensor network. These sequential data are different in sequence lengths and vary greatly in values due to a high level of uncertainty. Therefore, how to model these data for better similarity measures is a challenging issue.

Our observation is that, since such data are generated by a hidden mechanism associated with an underlying mov-ing object, it is desirable to model such data using a gen-erative model-based method. Among others, HMMs have been demonstrated empirically to be capable of modeling such generative processes in a wide variety of real-world applications that include speech recognition [14] and ges-ture recognition [1]. Therefore, we adopt HMMs to model the temporal sequences for our solution.

An HMM is a non-deterministic stochastic finite state automata . The basic structure of an HMM consists of a connected set of states, each of which emits an observable output. A first-order continuous HMM with Gaussian ob-servation density is formally defined by: 1. A set of Q states, S = { S 1 ,S 2 ,...,S Q } . 2. The initial state probability distribution  X  = {  X  i } 3. The state transition probability distribution A = { a ij 4. The observation probability density is b j ( O t )=
Above, O t and q t indicate the observation and state at time t , respectively. The parameters of a continuous HMM can be represented in the following compact form
Let Y be an observation sequence, and let  X  be the pa-rameters of an HMM. The following are the main tasks of an HMM learning algorithm [14]: 1. Compute the probability of the observation sequence 2. Find an optimal sequence of states that maximizes the 3. Learn the parameters  X  that maximize the probability
Using the HMMs, each sequence can now be modeled as a set of model parameters. However, it is unreliable to measure the sequence similarities by directly using these parameters. We tackle this problem using spectral cluster-ing in the next section.
In this section, we present our approach to the problem of clustering temporal sequences, which is referred to as the HMM-Spectral algorithm in this paper. Figure 1 shows the flow diagram of our proposed HMM-Spectral algorithm. Given a set of temporal sequences { Y 1 ,Y 2 ,...,Y N } ,we first construct an affinity similarity matrix S from all the sequences. A spectral clustering algorithm is then applied to the affinity matrix to group these sequences into K clusters, where K can be automatically determined from the data. In the following, we discuss these two major steps in detail.
Y Y
Y . . .

Figure 1. Flow diagram of our HMM-Spectral algorithm To construct the similarity matrix, we fit NQ -state HMMs, one for each individual sequence Y i , 1  X  i  X  N , by using the Baum-Welch algorithm [14]. These HMMs can be initialized in a default manner: we first set the transi-tion matrices to be uniform; we then set the means and vari-ances from clusters learned through a k -means algorithm. For each fitted model with parameters  X  i , we calculate the log-likelihood of each of the N sequences given the model parameters  X  i . The log-likelihood value for each pair of sequence and HMM is computed as follows: This is done by applying the standard forward-backward al-gorithm [14]. We can thus obtain a log-likelihood distance matrix L . This distance matrix is clearly not symmetric. In-stead, we define the distance between two sequences Y i and Y j using the mutual fitness measure as follows: which represents the cross-fitness of two sequences to the models. In this equation, the terms L ( Y i ;  X  i ) and L ( Y indicate the likelihood of the sequences given their own fit-ted models. The cross terms L ( Y j ;  X  i ) and L ( Y i ;  X  dicate the likelihood of a sequence generated by the fitted model of another sequence. On the one hand, if two se-quences are identical, the cross terms would have maximum values. Thus Equation 3 would be equal to zero. On the other hand, if two sequences are different, their likelihood of being generated from other models would be small. Thus the distance between them would be large. In this way, we can transform the temporal sequences into a set of new fea-ture vectors in the  X  X og-likelihood space X .

The motivation behind the above transformation is as fol-lows. The hypothesis is that all the sequences are generated by K models. Thus, when we fit models to an individual se-quence, we might get noisy estimates of model parameters but the parameters should be clustered in some manner into K groups based on their true values. Clustering directly in the parameter space would be inappropriate because the distance between parameters is hard to define; however, the log-likelihoods provide a natural way to define pairwise dis-tances between sequences. The distance matrix of Equation (3) is taken as input to the subsequent clustering process.
We construct an affinity matrix S from the distance mea-sure. Each element s ij of the matrix reflects the similarity of the corresponding sequences i and j . The similarity ma-trix is defined as follows: where d ( i, j ) is the distance between the sequences i and j . Clearly, the similarity matrix S is a symmetric and affinity matrix because s ij = s ji  X  0 for any pair of sequences i and j where i = j . Here, the scaling parameter  X  con-trols how fast s ij falls off with the distance between i and j . While this parameter is usually pre-specified, Ng et al. [11] proposed a method of choosing  X  automatically, which we adopt in this paper.
After the similarity matrix is constructed, we apply spec-tral clustering methods to partition the sequences into K clusters; we discuss how to obtain the appropriate value for K in Section 4.3. Given an N  X  N affinity matrix S , each element s ij can be viewed as the similarity between the vec-tors v i and v j . For an undirected graph G with vertices v and edges s ij ,where 1  X  i, j  X  N , the matrix S is consid-ered as an adjacency matrix for G .Let d i = j  X  V s ij be the degree of vertex v i ,andlet D be a diagonal matrix with d being its diagonal element. We can obtain a normalized stochastic matrix: where the sum of each row is one. Based on the definition of a Markov chain, m ij represents the transition probability of moving from v i to v j . In practice, we consider a matrix where L is symmetric and stable in eigendecomposition [11]. Then, the symmetric matrix L can be decomposed into the following form: where X =[ x 1 ,x 2 ,...,x N ] isamatrixbystackingthe eigenvectors of L in columns;  X = diag (  X  1 ,..., X  N ) is a diagonal matrix with the nonnegative singular eigenvalues in descending order along the diagonal, that is,  X  1  X   X  2 ...  X   X  N  X  0 . These eigenvalues represent the impor-tance of the corresponding eigenvectors. Since the top K eigenvectors, K  X  N , can capture a significant amount of information on the original samples, we can map the origi-nal samples into the K dimensional vectors in the spectral domain and then apply the standard clustering algorithms based on Euclidean distance.

The spectral clustering algorithm we apply is similar to the one proposed in [11]. Given the number of clusters K , the algorithm works as follows: Spectral Clustering (See Figure 1) (1) Find K principal eigenvectors x 1 ,x 2 ,...,x K , corre-(2) Normalize the rows of the matrix P so that they have (3) Treating each row of P as a point in R K , cluster them (4) Assign the original sample v i to cluster j if and only if
The main assumption of the above clustering algorithm is that the number of clusters K needs to be pre-specified. To estimate the number of clusters, an appropriate criterion is required to measure the quality of the resultant clusters. For a specific number of clusters J , we compute a correla-tion matrix Q J = P J P J when the normalized matrix P J is obtained after step (2) of the Spectral Clustering algorithm. Each element q ij of the matrix Q J represents the similarity between the vectors v i and v j . The closer to one q ij is, the more similar two vectors v i and v j are in the spectral do-main. Therefore, based on the matrix Q J , we can compute a quality score  X  J using the clustering result as follows: where Z c is the set of sequences included in the cluster c , and N c is the number of sequences in Z c . This quality score would have a higher value if the sequences in each cluster are more similar. Therefore, the number of clusters can be automatically determined by evaluating the local maximum value of this quality score.

In summary, our proposed algorithm works as follows: given a maximum number of clusters K max ,for J = 1 , 2 ,...,K max , iterate the steps of Spectral Clustering from (1) to (4). Find the optimal number of clusters K  X  such that the corresponding quality score  X  K  X  is maxi-mized. With this iterative algorithm, we can automatically group all the sequences into K  X  clusters.
In order to evaluate the performance of our proposed algorithm, experiments were carried out on both synthetic data and real data from a wireless LAN environment. For comparison, four different clustering approaches were used as the baselines. The first one is referred to as K-Means (Loglik). This algorithm differs from our HMM-spectral al-gorithm in that, after the similarity matrix S is built, a stan-dard k -means algorithm rather than a spectral algorithm is applied for clustering. This baseline is used to test the abil-ity of our algorithm in reducing noise and dimensionality. The other three approaches are used to test the sensitivity of our algorithm against different initialization procedures used to train a mixture of HMMs for clustering when the EM algorithm is applied. The first one uses a random ini-tialization, which is called MHMMs (Random). The second one trains a mixture model using a clustering-based initial-ization in  X  X og-likelihood space X  as given in [15]. In this approach, after computing the log-likelihood distance ma-trix L , the authors computed a different similarity matrix Then a k -means algorithm is used to cluster the sequences into K groups which are subsequently used to initialize the mixture of HMMs. We call this method EHMMs (Loglik). The third algorithm is called EHMMs (DTW), which ini-tializes the mixture of HMMs based on the clustering results using a DTW-based similarity measure [13].

In summary, we compare the performance of five cluster-ing algorithms: (1) K-Means (Loglik), (2) EHMMs (Ran-dom), (3) EHMMs (Loglik), (4) EHMMs (DTW), and (5) HMM-Spectral (our proposed algorithm). In the following, we first introduce the criterion used to evaluate the clus-tering results in Section 5.1. Based on this criterion, we compare the performance of the five algorithms on synthetic data in Section 5.2. We then demonstrate the effectiveness of our HMM-Spectral clustering algorithm on real data col-lected from a wireless LAN environment in Section 5.3.
The criterion used for testing the validity of the cluster-ing algorithms is the F-measure, which combines the con-cept of recall and precision measures in information re-trieval area [8]. Specifically, for each actual cluster i ,we first compute the recall and precision measures of all the detected clusters j . The definitions of recall and precision are given as Recall ( i, j )= n ij /n i and P recision ( i, j )= n ij /n j ,where n i is the number of sequences belonging to cluster i , n j is the number of sequences belonging to cluster j ,and n ij is the number of sequences in cluster i that are correctly identified in cluster j . Based on the recall and the precision measures, the F-measure of two clusters i and j is defined as
The overall F-measure of the final clustering result is computed as a weighted sum over all the values of F ( i, j ) , which is defined as follows: where n is the total number of sequences. For a particular cluster i ,the max operation is taken over all the detected clusters j . In the following, we use F-measure to evaluate the performance of clustering.
In this experiment, 80 sequences of an average length of 180 ranging from 150 to 200 are generated from a 2-component HMM mixture (40 sequences from each compo-nent). Each observation consists of a one-dimensional value (in the next section, we extend to multi-dimensional vec-tor sequences). Both HMMs are modeled with two states, which use a one-dimensional Gaussian observation density for generating observations in each state. Similar to [1], we model the amount of noise by varying the amount of over-lap between the generative models and by varying the mean separation between the Gaussian densities of the two states. The variations are similar for both HMMs. Specifically, the transition matrices for two HMMs are as follows: We kept the variances of the Gaussian densities for two states as  X  2 1 =  X  2 2 =1 , and the mean of the first state as  X  1 =0 . Then we varied the mean of the second state  X  2 in a range from 1 to 3. This corresponds to a change in the mean separation  X   X   X  2 between the two Gaussian den-sities. This clustering task is non-trivial both because the data have exactly the same marginal statistics if the tempo-ral information is removed from the sequences, and because the Markov dynamics governed by A 1 and A 2 are relatively similar for each HMM.
Figure 2. Comparison of clustering algo-rithms vs. Mean separation between the ob-servation Gaussian densities
For each value of  X   X   X  2 , the five clustering algorithms were applied to the generated sequences with 20 trials. Figure 2 shows the clustering results with respect to different val-ues of the mean separation between two Gaussian densities. Each value plotted in this figure corresponds to the overall F-measure averaged over 20 trials. We can see from the fig-ure that, as the mean separation between the two Gaussian densities in the two states decreases, that is, when the noise level increases, the overall performance of all the clustering algorithms decreases. This occurs because, when the mean separation becomes smaller, the two Gaussian densities be-come more indistinguishable from each other and thus more noise is involved in the observations. Consequently, the clustering task becomes more difficult.

Let us look into Figure 2 in detail. First, when the mean separation is small, our HMM-Spectral algorithm signifi-cantly outperforms the K-Means (Loglik) algorithm. This indicates that, by performing eigendecomposition, HMM-Spectral can reduce the uncertainty of the similarity ma-trix, which enhances the quality of the similarity-based clustering. Second, our HMM-Spectral algorithm consis-tently outperforms the other three EM-based clustering al-gorithms. EHMMs (Random) gives the poorest clustering result because the convergence of the EM algorithm is af-fected by the random initialization to a large extent. By applying better initialization methods, EHMMs (Loglik) and EHMMs (DTW) improve the performance of EHMMs (Random), whereas their performance is roughly compara-ble. In summary, our HMM-Spectral algorithm performs the best among the five clustering algorithms.

We performed another set of experiments to evaluate the sensitivity of the five clustering algorithms to the observa-tion noise. In this experiment, we kept the means of two Gaussian densities as  X  1 =0 and  X  2 =1 . 5 , and kept the variances as  X  2 1 =1 and  X  2 2 =1 . 5 . We set the transition matrices A 1 and A 2 for each HMM to be uniform. Since the observations in temporal sequences are not usually in-dependent, we added Gaussian noise to the observations, whereby we set the mean  X  =0 and  X  2 to vary from 0.1 to 0.9. For each setting, we also generated 80 sequences of average length 200 from the 2-component HMM mixture.
Figure 3. Comparison of clustering algo-rithms vs. Variance of Gaussian noise
Figure 3 shows the clustering results with respect to dif-ferent values of the variance  X  2 of the Gaussian noise. As we can see from the figure, the three EHMM algorithms performed poorly when the Gaussian noise is added into the consecutive observations. Since HMMs assume that the observations are independent of each other over time, the three model-based algorithms cannot accurately estimate the parameters of the mixture model. In contrast, K-Means (Loglik) and HMM-Spectral performed better in the case of Gaussian noise. However, when more noise is involved in the observations, our HMM-Spectral algorithm outper-forms K-Means (Loglik) because it can effectively reduce the noise through spectral clustering.

In this experiment, we clustered the signal sequences obtained from a moving object X  X  trajectory data in a wire-less LAN environment as shown in Figure 4. This environ-ment is the office area of the Computer Science Department at the Hong Kong University of Science and Technology. This area is equipped with an IEEE 802.11b wireless net-work in the 2.4 GHz frequency bandwidth. A user X  X  activ-ities are carried out in the three main areas (Office, Room1 and Room2), three entrances and seven hallways. The two rooms provide facilities for printing services and holding seminars. In the figure, four access points (APs) are marked with double solid circles, each of which is identified by its unique Media Access Control (MAC) address.

While a user with a mobile device performs different ac-tivities in this environment, the mobile device can period-ically record signal-strength measurements from the APs. For illustration, an observation o = &lt; ( AP 1 :  X  81)( AP  X  77)( AP 3 :  X  64)( AP 4 :  X  41) &gt; is a signal vector where each element consists of the MAC address of an AP and the corresponding signal-strength value. Accordingly, the observed sequence on users X  behavior is represented as a sequence of signal-strength measurements recording their movements in the environment. These sequences contain lots of noise due to the multi-path fading effect in signal propagation, and are of different lengths.

Using the device driver and API that we have devel-oped, we collected 180 sequences of a professor X  X  6 dif-ferent activities in this office area. These activities in-clude  X  X ntrance1-to-Office X ,  X  X ffice-to-Room2 X ,  X  X ffice-to-Entrance2 X ,  X  X ffice-to-Entrance3 X , and  X  X ntrace2-to-Room1 X . The number of sequences for each activity is 30. In total, 25 APs can be detected: four are distributed within this area; the others are distributed in adjacent areas on the same or different floors. In our experiment, we chose six APs because their signals occurred frequently and their av-erage signal-strength values are the strongest. Therefore, each sequence is a 6-dimensional time series. The average length of sequences is 122, ranging from 80 to 140 (within sequence lengths between 80-100, 101-120, 121-140, the mean lengths are 92.34, 116.08, 128.79, respectively, and the variances are 12.3, 4.7, 26.3, respectively). For evalua-tion purpose, we manually labeled each sequence by its cor-responding intended activity. These labels serve as ground truth for evaluating the clustering results in terms of the F-measure, as well as for evaluating the automatically deter-mined number of clusters against the number of different user activities. The task of performing clustering on this data set is difficult because signal-strength measurements are extremely noisy and uncertain in an indoor environment.
Figure 5. Comparison of clustering algo-rithms on real sensory trajectory data
Figure 6. Clustering quality score  X  vs. Num-ber of clusters We applied the five clustering algorithms to this data set. The clustering results are summarized in Figure 5. Each value of F-measure plotted in the figure is also the average of 20 trials. We can see from the figure that our HMM-Spectral algorithm performs the best on this real data set. Compared with the K-Means (Loglik) algorithm, the per-formance of HMM-Spectral is much better. This shows that HMM-Spectral is effective in reducing the uncertainty in-volved in the similarity matrix used for further clustering. HMM-Spectral also outperforms the other three EHMMs algorithms because the EM-based algorithms can be easily trapped in a local minimum when lots of noise exists.
Experiments were also carried out to test the ability of our HMM-Spectral algorithm to determine the optimal number of clusters. Figure 6 shows different values of the quality score  X  by varying the number of clusters. As we can see from the figure, the maximum value of  X  is achieved when the number of clusters is equal to six, which is the same as our ground truth from the human labeled data set.
In this paper, we investigated the problem of cluster-ing variable length, noisy and multi-dimensional time series data. These data are abound in tracking and monitoring ap-plications in a sensor network. We argued that traditional clustering algorithms have difficulty in dealing with these data due to the simplicity of their assumptions. We intro-duced the HMM-based mixture models for transforming the time series data into equal length vectors, which are in turn used to produce an affinity similarity matrix. We then ex-plored how to apply spectral clustering on this matrix to fur-ther remove noise and obtain the final clusters. Our experi-mental results on both synthetic and real data demonstrated that our proposed HMM-Spectral algorithm is both robust and accurate for noisy data clustering.

In the future, we plan to apply other types of generative models to replace the HMM model. For example, we might apply a linear dynamic model for this purpose. In addition, we plan to test the utility of other clustering algorithms to replace the k -means algorithm in the HMM-Spectral algo-rithm on the projected eigenspace to find the best combina-tion of algorithms for different types of time series data.
We would like to thank Hong Kong RGC for supporting this work under grant CA 03/04.EG01 (HKBU 2/03C).
