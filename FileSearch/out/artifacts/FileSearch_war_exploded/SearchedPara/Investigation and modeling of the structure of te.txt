 ORIGINAL PAPER Monojit Choudhury  X  Rahul Saraf  X  Vijit Jain  X  Animesh Mukherjee  X  Sudeshna Sarkar  X  Anupam Basu Abstract Language usage over computer mediated dis-courses, such as chats, emails and SMS texts, significantly differs from the standard form of the language and is refer-red to as texting language (TL). The presence of intentional misspellings significantly decrease the accuracy of existing spell checking techniques for TL words. In this work, we for-mally investigate the nature and type of compressions used in SMS texts, and develop a Hidden Markov Model based word-model for TL. The model parameters have been esti-mated through standard machine learning techniques from a word-aligned SMS and standard English parallel corpus. The accuracy of the model in correcting TL words is 57.7%, which is almost a threefold improvement over the perfor-mance of Aspell. The use of simple bigram language model results in a 35% reduction of the relative word level error rates.
 Keywords Texting language  X  SMS  X  Hidden Markov Model  X  Text correction  X  Spell checking 1 Introduction While communicating over texting media, such as chats, emails and short messages over mobile phones (SMS), users have a tendency to use a non-standard form of the language that disregards grammar, punctuation and spelling rules. In order to type faster, especially while using a small keyboard like that of the mobile phones, users employ a large number of compression techniques to reduce the message length. Com-monly used abbreviations, shorter phonetic substitutions, deletion of words and characters, are some of the popular methods for shortening the message length. Nevertheless, the characters and words cannot be deleted arbitrarily, as it may seriously hamper the understandability of the mes-sage. Thus, two opposing forces X  X horter message length and semantic unambiguity X  X hape the structure of this com-pressed non-standard form, called the NetSpeak [ 16 ]orthe Texting Language (TL) [ 46 ].

The objective of the current work is to study and formally model the characteristics of TL, and based on the compres-sion model so obtained, construct a decoder from TL to the standard language. The decoder is expected to transform a sentence in TL, such as  X  btw wenz ur flt 2moro  X  to its stan-dard English counterpart, i.e.,  X  By the way, when is your flight tomorrow?  X  However, here we focus on the word level ana-lysis and modeling of TL, which forms the first step towards a sentence level decoder. Compressions at the level of syntax or those grounded in semantic or pragmatic knowledge are beyond the scope of the current work.

We model the problem as a noisy channel process; we assume that the standard word is compressed or distorted to the TL form, while being transmitted over the noisy chan-nel. Hidden Markov Models (HMM) [ 39 ] have been used to characterize the stochastic properties of the channel. The conception and construction of the HMM is a novel as well as a significant contribution of the present work. In order to learn the characteristics of the noisy channel (or equivalently the TL), we have gathered 1,000 English SMS texts from http://www.treasuremytext.com . The website hosts a large number of SMS texts in several languages uploaded by ano-nymous donors. The SMS texts were manually translated to their standard English form and automatically aligned at the word level using a heuristic algorithm. The resulting 20,000 word parallel-corpus has been inspected to formulate the structure of the word level HMM. The HMM parameters are estimated from the training corpus using standard machine learning techniques. Even though only English SMS texts have been used here as the primary TL data, it is representa-tive of the TL used over other computer mediated discourses as well [ 16 , 22 ].

A decoder from TL to the standard language has several practical applications including search engines and automa-tic correction tools for noisy text documents such as blogs, chatlogs and emails. Non-standard spellings and grammati-cal usage is very common over the web. Therefore, a search engine for noisy texts (say blogs, chatlogs or emails) must be immune to the several TL variants of a word. For example, given a query  X  X ranslator X , a search engine is expected to also return the webpages with words like  X  X ransl8or X ,  X  X rnsl8r X ,  X  X rnsltr X , etc. This can be readily achieved through a word level decoder from the TL to standard language. Similarly, owing to a large number of spelling variations, search for proper nouns also calls for appropriate normalization. For instance, we find several variations for the name  X  X aurav Ganguly X  in the web ( X  X aurabh Ganguly X ,  X  X ourav Gan-guli X ,  X  X ourabh Ganguly X , etc.). The proposed model for TL can also efficiently capture the variations in proper nouns. Yet another application of this technique could be in text-to-speech systems that read out webpages, emails and blogs to the visually challenged.

Apart from the aforementioned practical applications, the present study is also interesting from the perspective of diachronic linguistics. It has been claimed by several resear-chers in the past that languages change over time to opti-mize its function, i.e., communication (see, for example, [ 10 , 11 , 15 , 28 ] and references therein). Such accounts of lan-guage change are called functional explanations (see [ 11 ] for review), where optimal communication is often equated to the two opposing objectives of shorter message length (or ease of message generation) and robustness to noise (maximally distinct messages). In the field of diachronic linguistics, where evidence is rare and experimentation is impossible [ 1 , 30 ], the structure of TL provides an ideal case for studying language change in the functionalist framework [ 6 , 8 , 41 ]. As we shall see here, in TL shorter message lengths are achieved through a variety of ingenuous compression techniques.

The rest of the paper is organized as follows: Sect. 2 pre-sents a brief overview of the linguistic and computational studies regarding the language usage over computer media-ted discourses and automatic correction of the same. Sec-tions 3 and 4 describe a noisy channel formulation of the problem and the creation of training data respectively. Sec-tion 5 discusses the compression characteristics of the TL, as has been observed from the corpus. On the basis of the obser-vations, in Sect. 6 we propose an HMM-based representa-tion of the words. Section 7 discusses automatic construction of the HMMs as well as the learning algorithm for estima-tion and generalization of the HMM parameters. The evalua-tion results of the model is presented in Sect. 8 . Section 9 concludes the paper by summarizing the contributions and listing out possible improvement schemes. 2 Related work The structure of the TL and its socio-cultural effects have been a subject matter of diachronic as well as socio-linguistic studies for the past two decades (see [ 16 , 22 , 41 ] and refe-rences therein). Some of the researchers, e.g. [ 19 , 32 ], have suggested that computer-mediated communication, particu-larly on-line, synchronous communication, challenges the generally assumed dichotomy between written and oral lan-guage. The orthograhic and syntactic structures used in TL have been extensively studied, not only for English [ 6  X  8 , 22 ], but also for several other languages, such as Arabic [ 37 ], German [ 2 , 18 ], Japanese [ 34 ] and Swedish [ 41 ]. 2.1 Studies on SMS Short messages over mobile phones or SMS are arguably the most distorted form of TL [ 18 , 41 ]. The misspelling in SMS texts are, in most of the cases, intentional, because, the typi-cal mobile-phone keypad, that has only nine keys, requires pressing of a key multiple times for entering a character. For example, the characters  X  X  X  and  X  X  X  require four key presses. The small display and the bounded message length (usually 160 characters) create further pressure towards reduction of message length.

The common trends of compression and/or distortion observed in SMS texts, as discussed in [ 41 ] for Swedish, are summarized below.  X  Spelling and punctuation :  X  Grammatical features :  X  Abbreviations and other features : 2.2 Computational models Despite the applications and aforementioned linguistic stu-dies, there are very few computational models built exclusi-vely for decoding or analyzing TL (see for example [ 4 , 5 , 40 , 42 , 47 ]) . Bangalore et al. [ 5 ] trained a hierarchical statisti-cal translation model for general domain instant messaging language collected from Internet chat rooms. However, as discussed above, the structure of the chat language is very different from that of the SMS. More recently, Aw et al. [ 4 ] developed an English SMS text normalization technique based on standard statistical machine translation models. They formulate the problem of normalization as machine translation from TL to SL, train a phrase-based MT system on a parallel data of 5,000 SMS messages, and report a signifi-cant improvement over the baseline model in terms of BLEU score. Some basic analysis of the TL is also reported. Never-theless, the method performs poorly for out-of-vocabulary (OOV) TL words, because no technique has been used to estimate the probability of an unseen token based on cha-racter level or pronunciation level similarities. The aim of the present work is to develop a very powerful word-model which can efficiently deal with unseen TL tokens and thereby, to address some of the limitations of the technique described in [ 4 ]. In other words, the current work is complementary in nature to [ 4 ], because while the latter tries to deal with the issues at the sentence level, the former focuses on word level compressions.

The problem of normalizing the TL words to their stan-dard counterpart is in many ways similar to spelling correc-tion. Spell checking is a well researched area in NLP and there are several approaches to spelling error detection and correction (see [ 26 ] for a dated, but relevant and extensive survey). Spelling errors are usually classified as typographic and cognitive errors. Typographic errors occur due to slip of finger (pressing a wrong key unknowingly), whereas cog-nitive errors arise when the user does not know the correct spelling and usually substitute the letters by their phonetic equivalents. For example, the word  X  X atex X  might be miss-pelt as  X  X ayex X  X  X  typograhic error, or  X  X atecks X  X  X  cognitive error.

It has been documented that around 80% of the unin-tentional misspellings deviate from their correct spellings by at most one insertion, deletion, substitution or transpo-sition [ 17 ]. Therefore, majority of the spell checking litera-ture is devoted to small edit-distance [ 27 ] based error correc-tion (see, e.g. [ 25 , 29 , 31 ]). Nevertheless, as is evident from the  X  X atex X  X  X  X atecks X  example, cognitive errors can signifi-cantly increase the edit-distance between the misspelling and the correct word.

Several techniques have been proposed to incorporate pho-netic information in the spell checkers, of which the tech-niques based on metaphone encoding [ 35 , 36 , 38 ] and noisy channel models [ 12 , 20 , 44 ] are the most popular ones. In the metaphone encoding or similarity key based techniques, the letters or letter sequences in a word are substituted by metaphones, and the the correct spelling is searched for in the space of strings over metaphones, rather than the let-ters. Although these techniques perform better than nearest edit-distance based techniques, noisy channel models that take into account the pronunciation information are known to substantially boost up the performance of the spell che-ckers [ 44 ]. 2.3 Pronunciation modeling for spell checking Statistical spelling correction techniques can be viewed as instances of noisy channel model, where a word w while being transmitted over the channel gets transformed to w .A spelling error refers to a situation when w = w . The pro-blem of finding the intended spelling w , given the observed spelling w is equivalent to searching for w in the lexicon, for which Pr (w | w ) = is maximized. Since Pr (w ) is independent of w , the pro-blem boils down to maximizing Pr (w | w) Pr (w) , where the distribution Pr (w | w) models the characteristics of the noisy channel (also known as the error model), whereas Pr (w) is known as the language model.

Brill and Moore [ 12 ] proposed an improved error model for spelling correction, that takes into account substitution of a string of characters of arbitrary length (upto 5) by another string of arbitrary length. The model automatically learns the probability of substitutions Pr ( X   X   X  | PSN ) from a corpus of words and their misspellings. In this expression,  X  is a sequence in w ,  X  the corresponding sequence in w and PSN refers to the position of the string  X  in w . Given a partition of w ,  X  1  X  2 ,..., X  k , and a corresponding partition for  X   X  w is defined as Finally, Pr (w | w) is estimated as the maximum over all par-titions of w of the probability that w is generated from w given that partition.

Toutanova and Moore [ 44 ] extended the Brill and Moore model by combining the pronunciation information. Let Pr by substitution of letter sequences as proposed in [ 12 ], and Pr by substituting the pronunciation (sequence of phones) of by letter sequences. Then, according to [ 44 ], the scores of a word w are estimated using the following equation (higher the Pr (w | w) , higher is the score S (w | w) ) S (w | w) = log Pr The quantity Pr PHL (w | w) is estimated as follows: Pr N is the number of possible pronunciations of w , w w represent pronunciations of w and w , respectively, and Pr PH ( x | y ) is the probability of the phone sequence x being generated from the phone sequence y .

In the Toutanova and Moore model the probability dis-tributions have been estimated from word/misspelling and word/pronunciation pairs. The pronunciation of the unseen words (misspellings) are generated stochastically using the pronunciation model, which is learnt from the word/ pronunciation pairs. The pronunciation model was trained on around 100,000 pairs and the letter model was trained on approximately 7,500 pairs. The resulting system has been reported to significantly reduce the error rate over the Brill and Moore model.

Since words in TL feature a large number of phonetic substitutions, pronunciation modeling becomes extremely important in the perspective of the current work. However, for the following reasons, it seems inappropriate to directly employ the aforementioned models for the correction of TL.  X  The Toutanova and Moore model assumes that w is gene- X  There is insufficient data for TL that can be used to train
In the subsequent sections, we propose a novel approach to combine the orthographic and phonetic information of a word using HMMs. 3 Noisy channel formulation Let S = s 1 s 2 ,..., s l be a sentence in the standard form, where s i represents the tokens including words, punctuations and abbreviations. When S is transmitted through the noisy channel, it is converted to T = t 1 t 2 ,..., t l , where T is the corresponding sentence in TL and t i is the transmitted form for the token s i . The noisy channel is characterized by the conditional probability Pr ( T | S ) . In order to decode T to the corresponding standard form  X ( T ) , where  X  is the decoder model, we need to determine Pr ( S | T ) , which can be stated as follows:  X ( T ) = argmax Under the assumption that the compressed form t i of s depends only on s i , and not the context, we obtain the follo-wing relation,  X ( T ) = argmax Thus, the decoding model  X (  X  ) for a TL sentence can be abs-tracted out into two distinct levels: (1) the word error model , Pr ( t | s ) and (2) the language model , Pr ( S ) . Although, we report some initial experimentations with n -gram language models, the focus of the present work is the word error model. Note that the basic formulation of the word error model Pr ( t | s ) is identical to the noisy channel model of spell che-cking discussed earlier, where t and s has been denoted by w and w respectively.

At this point, it might be worthwhile to inspect the validity of the aforementioned assumption made regarding word-to-word transmission. Although it is quite reasonable to assume that for TL the compression applied on a word is independent of its context, a significant drawback of the assumption lies in the fact that often words are deleted or combined during typing (deletion of space or splitting of compounds as repor-tedin[ 41 ]). For instance, in the source-TL sentence pair cited in Sect. 1 , if we assume that a word for word translation has taken place, then the corresponding alignment is ( stands for the null symbol, i.e. the deletion of a token) However, in this case T will not reflect the tokenization shown in the above alignment, since the delimiter ( white space ) is missing between the tokens 1 and 2, 2 and 3, and 5 and 6. To solve this issue, a more general formalism such as the IBM Models of translation [ 13 ] could have been used, where phrases can be aligned to phrases. However, that would be an overkill, because manual inspection of the TL data reveals that intentional deletion of space takes place only under two specific situations: (1) for commonly used abbre-viations and (2) for certain grammatical categories like the auxiliaries ( X  X hen is X  to  X  X enz X  or  X  X ow are X  to  X  X owz X ) and the negative marker ( X  X re not X  to  X  X int X  or  X  X ould not X  to  X  X udnt X ).

Therefore, we believe that a word for word decoding scheme along with a dictionary of commonly used abbrevia-tions and rules for handling the space omission for specific grammatical classes can achieve a reasonably good accuracy for sentence level translations. As described in [ 4 ], one might also resort to standard machine translation techniques for sol-ving these issues. The scope of this work, however, is res-tricted to the design of a word level decoder that is motivated by the cognitive process behind the text compressions. We note that the use of acronyms, ellipses, and other syntac-tic and pragmatic compressions present interesting research challenges to SMS text normalization, which we intend to address in the future as an extension to this work. 4 Creation of training data The estimation of the error model Pr ( t | s ) demands the infor-mation about the variations of a word s over the texting medium and their probabilities. In order to gather realistic TL data we created a 20,000 word aligned corpus of SMS texts and standard English. The process of data collection, translation, cleaning and alignment are described below. 4.1 Data collection There are a few websites that host a good amount of medium specific TL data. We chose to work with SMS data, because apparently it features the maximum amount of compression and thus, is the most challenging one to model.

The website http://www.treasuremytext.com hosts a huge number of SMS texts in several languages, which have been donated by anonymous users. The messages are not indexed by languages and many of them feature code switching. A large number of messages were downloaded from the site, from which around 1,000 English SMS texts were collected such that (a) The texts are written only in English, and contain no (b) There is at least one misspelling in the message. This
The SMS texts were manually translated to their stan-dard English forms. The proper nouns (names of people, places, etc.) have not been translated, but replaced by a tag &lt;
NAME &gt; in both the TL and standard data. There are around 20,000 tokens (words) in the translated standard text out of which around 2,000 are distinct. On an average there are 83 characters in the SMS text for every 100 characters in the corresponding standard English text. 4.2 Automatic alignment The translated corpus has been automatically aligned at the word level using a heuristic algorithm. The algorithm sear-ches for sites (tokens) in the original and translated texts, which can be aligned to each other with high confidence .We define these sites as pivots . The unaligned words between two consecutive pivots are then recursively aligned by searching for more pivots between them. During each recursive call the conditions for alignment are further relaxed. A sketch of the algorithm is provided below. 1. Input: SMS text T = t 1 t 2 ,..., t k , translated standard 2. We introduce two hypothetical start symbols t 0 and s 3. Each t i is compared with s i and other tokens around s 4. Suppose there are p + 2 pivots including the two boun-5. If a region has 0 or 1 token in either of T or S ,theyare 6. If both the regions have more than one token, the cha-7. The best matches are aligned and declared as new pivots, The accuracy of the alignment algorithm is around 80%.
There are two main assumptions behind the alignment algorithm; first, the ordering of words in TL and SL are iden-tical, and second, there are tokens that can be aligned as pivots with high confidence during the initial few calls of the procedure. In fact, inspection of the TL data reveals that both of these assumptions hold good. The first one regarding the ordering of words is clearly true, because while typing over the texting medium, the sentence of SL is compres-sed rather than garbled. The second assumption regarding existence of undistorted or predictably distorted words holds good because, usually the frequently used words show very little variations and therefore, they can be readily aligned as pivots with high confidence. By the virtue of their high fre-quency, one or more of these words are present in almost all the SMSes. Table 1 shows the five most frequent tokens of the SL and their corresponding variations. It is evident from the table that a pattern such as  X  X  X  in TL can be ali-gned with  X  X ou X  in SL with a high confidence. Some of the other frequent words, for which we have not observed any distortion are (frequencies are provided in parentheses): in (169), so (157), on (159), it (143), if (138).

Note that there are several statistical alignment algorithms that are used in machine translation for word or phrase level alignment of parallel data (see [ 21 ] and references therein for examples of state-of-the-art alignment algorithms). These algorithms assume that the order of the words/phrases bet-ween the source and target languages can be different, which is seldom the case for TL. Moreover, the rich information in the form of orthographic similarity between the corres-ponding SL and TL tokens are not exploited by most of the standard alignment algorithms. In [ 4 ], the authors report a statistical alignment algorithm for SMS texts based on expec-tation maximization (EM), where they try to overcome both of the aformentioned limitations. The search space is res-tricted to monotone alignments (i.e., the phrases are assu-med to be in the same order in SL and TL), whereas the initial alignments are generated based on othographic simi-larity between the words. It would be interesting to compare the performance of these standard algorithms on the TL data to that of the heuristic approach suggested here that makes maximal use of the orthographic and phonetic information to circumvent problems arising from data scarcity. 4.3 Extraction of word-variant pairs From the aligned corpus, a list of the unique English words and their corresponding variations in the SMS texts were extracted along with the occurrence frequencies. The list was manually cleaned to remove noise introduced due to error in alignment. Out of the 2,000 distinct English words, only 234 occur at least ten times in the corpus, and thus, can provide useful information about the nature of the TL; the other words being quite infrequent cannot be used for statistical learning purpose.

The total number of variants corresponding to these 234 frequent words is 702. This extracted list of 234 words { w racteristics { v i 1 , f i 1 , v i 2 , f i 2 ... v i m f j denote the j th variation and its frequency for the word w i respectively, constitutes our word level training corpus. Table 2 shows the variation characteristics of the words  X  X ack X ,  X  X oday X  and  X  X hanks X . The largest number of varia-tions has been observed for the word  X  X omorrow X , which are listed in Table 3 . The word aligned parallel corpus as well as the manually cleaned training data are available at [ 14 ]. 5 Variation characteristics The distortions observed in the SMS text can be broadly clas-sified into two groups X  intentional and unintentional errors. The unintentional errors are caused by (1) pressing of the wrong key, (2) pressing of a key more than the desired num-ber of times, (3) deletion of a character or (4) inadequate knowledge of spelling. Table 4 shows some of the uninten-tional errors in the SMS corpus. Given an SMS text, it is quite difficult to predict which of the errors are unintentio-nal and which are intentional. This is because, the text is usually highly distorted. Nevertheless, a few thumb rules for detection of unintentional errors are as follows.  X  The misspelling is incomprehensible (assuming that  X  The misspelling uses a variant that does not minimize the  X  The misspelling is an extremely rare variant. This is a
Unlike unintentional errors, intentional errors are quite frequently observed in the SMS text. They can be classified into four categories: character deletion , phonetic substitu-tion , abbreviations and non-standard usage. We describe underneath each of these categories in details. 5.1 Character deletion There are three major types of intentional deletions  X  Vowel deletion: Deletion of vowels from the words  X  Repeated consonant deletion: Examples are  X  X omoro X   X  Truncation of words: Examples are  X  X omm X  (1),  X  X om X  5.2 Phonetic substitution Phonetic substitutions are the next common compression style, after vowel deletion. The various types of phonetic substitutions are enumerated below.  X  Substitution of phonetically equivalent vowels/  X  Deletion of silent characters: Examples are  X  X o X  (27)  X  Substitution of single letters for sequences of phones:  X  Substitution of numerals for phone sequences: Exam-5.3 Abbreviations Abbreviations used in TL can be classified as standard and non-standard abbreviations. The standard abbreviations are those, which are understood by all the TL users. Examples include  X  X b X  for  X  X ext back X ,  X  X ol X  for  X  X aughs out loud X ,  X  X day X  (7) for  X  X irthday X  (15), and  X  X tw X  for  X  X y the way X . The non-standard abbreviations, on the other hand, are known and used within a community. For example,  X  X gp X  for  X  X haragpur X  or  X  X nt X  for  X  X omplex network X . It is extremely difficult, even for human translators, to identify and decode the non-standard abbreviations. 5.4 Other non-standard usage Several other non-standard forms are used in TL, some of which are discussed below.  X  Dialect variations and spoken forms: Examples are  X  Alternative pronunciations: Examples are  X  X etta X  (4)  X  Character substitution:  X  X  X  or  X  X  X  is used instead of  X  Miscellaneous:  X  X /c X  (1) for  X  X ecause X  (72)
Often more than one of these compression techniques are used over a single word to achieve maximal reduction in length. The token  X 2mro X , for example, is obtained from  X  X omorrow X  through phonetic substitution as well as cha-racter deletion. Similarly,  X  X z X  (3) has been obtained from  X  X ecause X  through the following steps:  X   X  X ecause X   X   X  X ause X  (non standard usage, transforma- X   X  X ause X   X   X  X oz X  or  X  X uz X  (phonetic substitution, trans- X   X  X oz X ,  X  X uz X   X   X  X z X  (vowel deletion, transformation
The rest of this article focuses on modeling of the unin-tentional errors as well as the first two kinds of intentional errors, and their combinations. Abbreviations and non-standard usages can be dealt with using dictionaries and rules, which is beyond the scope of the current work. Neverthe-less, distortions due to alternative pronunciations and cha-racter substitution gets automatically captured in the model, as these can be represented in terms of the aforementioned compression and error patterns. 6 Conception of the word model The purpose of the word model is to capture the intentio-nal compressions employed by the user to shorten the length of the word as well as the unintentional errors made during typing. There are several works related to automatic speech recognition in general (see [ 23 ] and references therein) and grapheme to phoneme conversion [ 43 ] in particular that uti-lize HMM to capture the variations in pronunciation and spel-lings. Inspired by such works, here we model each word w in the standard language as an HMM (see [ 39 ]forareview).
Recall that the error model in [ 12 ] is based on the assump-tion that the user, while typing a word, mentally splits the word into fragments of letter sequences (the length of each sequence being restricted to five). Each of these fragments are then replaced by an equivalent letter sequence (possi-bly the same) independent of the structure and the fate of the other sequences. Irrespective of whether this assumption reflects the underlying cognitive process of typing or not, the above analogy provides an intuitive understanding and visualization of the error model involved. In order to explain and justify the word error model proposed here, we appeal to a similar thought experiment.

Let w = g 1 g 2 ,..., g l , where g i represents a character or grapheme of the standard language. Let w = p 1 p 2 ,..., p be the pronunciation of w , where p i represents a phone of the standard language. 2 For example, if w =  X  X oday X , then w =  X /T/ /AH/ /D/ /AY/ X .

Suppose, a user wants to reduce the length of the word w while typing it on a mobile keypad. She mentally splits the word into fragments, as in [ 12 ], and inspects each of the fragments independently. For each of the fragments, she might decide to type a phonetic or a graphemic equivalent of the fragment. If she chooses to type a graphemic equivalent, she inspects the fragment character by character and decides whether to delete or type the particular character under consi-deration. On the other hand, if she chooses to proceed with a phonetic equivalent, then she might decide to replace the whole fragment with a single character if possible (e.g.,  X 2 X  for  X /T/ /AH/ X ), or she might inspect the pronunciation of the fragment phone by phone and replace each of the phones by letters following some sound-to-letter rules. Note that for every key press she makes, there is also a possibility of unin-tentional error, that is pressing of an unintended key.
We encode the aforementioned typing process through the structure of an HMM. Each state in the HMM represents a possible key press. Transition between the states represent the possible choices after a key press (for example, whether to type the next fragment of the word graphemically or pho-netically). Thus, the transition probabilities reflect how often the users choose to type a particular fragment in a specific manner, whereas the emission probabilities associated with a state reflect the uncertainty in the output (say due to uninten-tional errors), given a particular choice. We describe below the structure of the word HMM and its construction. 6.1 The Basic HMM Suppose the word w = g 1 g 2 ,..., g l is typed without any compression or error. This situation can be modeled using a left-to-right HMM having l + 2 states as follows. There is a start state and an end state denoted by S 0 and S respectively. The only observation associated with these two states is the special symbol $, which marks the beginning and the end of a word. There are l intermediate states G 1 G , where the only observation associated with state G i is the grapheme g i . There is a transition from S 0 to G 1 .From each G i there is a transition to G i + 1 and from G l there is a transition to S l + 1 . The HMM so defined has a probability of 1 for the observation sequence $ g 1 g 2 ,..., g l $, i.e. the word, and 0 for all other sequences.

As illustrated in Fig. 1 for the word  X  X oday X , a series of modifications applied on this basic HMM finally leads to the word model of TL. This process has been detailed out in the subsequent sections. 6.2 Graphemic path We define each state G i as a graphemic state and the path from S 0 to S l + 1 through the sequence of graphemic states as the graphemic path . At each graphemic state, the user might choose to type or delete the grapheme associated with that state. Suppose we want to model the deletion of the character g . There are two possibilities: (1) a new transition is added from G i  X  1 to G i + 1 , and (2) we associate the emission of the null character with G i .

The disadvantage of choice (1) is that in order to model the deletion of any arbitrary number of characters in the HMM, we need to add transitions from a state G i to all states G whenever j &gt; i . Therefore, the number of transitions in the HMM becomes O ( l 2 ) , and consequently, we have to estimate O ( l 2 ) transition probabilities, which calls for more training examples. For choice (2), in order to model deletion of an arbitrary number of characters, it suffices to add a null emis-sion to every state. Thus, we need to estimate only l new emission probabilities. Therefore, we have chosen option (2), whereby every graphemic state is associated with a null emission.

It may be noted that existence of null emissions makes it difficult to estimate the observation probability of a string for a given HMM because, Viterbi algorithm cannot be used for HMMs with null emission. Nevertheless, as described in the next section, we modify the Viterbi algorithm suitably to deal with null emissions efficiently.

In order to take care of the unintentional errors, we also allow each of the graphemic states to emit all other characters including numbers and punctuation marks, which we collec-tively represent by the wildcard symbol  X  X  X . We shall denote the different observation probabilities associated with the graphemic state G i by Pr ( g i | G i ) , Pr ( | G i ) and Pr where for all i , Pr ( g i | G i ) + Pr ( | G i ) + Pr ( @ | G i ) = 1(5)
Note that the above representation precludes the possibi-lity of assigning different emission probabilities for the dif-ferent instantiations of the wildcard symbol, which is clearly an oversimplification. For instance, consider a state G i responding to the grapheme g i =  X  X  X . The probability of observation of  X  b  X  X r X  c  X  X n G i is expected to be greater than that of  X  x  X  X r X  y  X , because unintentional errors due to pressing of a single key either more or less than the desired number of times are more frequent than those arising from pressing of an altogether different key. Ideally, the emission probabi-lities of all the symbols for a given graphemic state should be estimated from the corpus. However, owing to the pau-city of appropriate TL data, we have decided to merge all the emissions due to unintentional errors into a single wildcard emission @.

Figure 1 a shows the structure of the HMM for the word  X  X oday X , after the incorporation of the aforementioned modi-fications. The graphemic path, so modeled, captures the unin-tentional as well as the deletion errors (described in Sect. 5.1 ). It is interesting to note that the graphemic path for a word w returns a non-zero observation probability for any string that has a length smaller than or equal to | w | . However, for all strings of length greater than w it returns an observation probability of 0. Stated differently, the graphemic path, in a sense, computes the edit distance between an arbitrary string w and w , where the cost of insertion is infinity, and that of substitution, deletion and perfect match are  X  log Pr ( @  X  log Pr ( | G i ) and  X  log Pr ( g i | G i ) , respectively. 6.3 Phonetic path We define k phonetic states P 1 to P k , corresponding to the phones p 1 to p k in the pronunciation w of w .Asshownin Fig. 1 b, there is transition from P i to P i + 1 for all i from 1 to k  X  1. There are also two transitions from S to S 6 . Each phonetic state P i emits a set of characters that might be used for representing the phone p i . For example, the phone /AH/ may be represented by  X  X  X  (as in  X  X ud X  for  X  X ood X ) or  X  X  X  (as in  X  X oday X ). We shall refer to this part of the HMM as the phonetic path . The purpose of the phonetic path is to model the intentional phonetic substitutions (as described in Sect. 5.2 ).

Although the path through P i s can capture the substitution of a single phone by a single character, it cannot capture the substitution of a syllable or string of phones by a single letter (as in  X 2day X  for  X  X oday X , where  X 2 X  stands for the phonemic string  X /T/ /AH/ X ). Therefore, we introduce the concept of syllabic states represented by S 1 , S 2 , etc. Each syllabic state S , is a bypass between two phonetic states P i  X  1 and P j ( i &gt; j ), such that the observation s shorter substitution for the phonetic string p i p i + 1 ,..., We introduce syllabic states for all possible substrings of that have length greater than one and can be substituted by a single character.

Note that a syllabic state emits only one character and therefore, the observation probability associated with that character is always one. However, in a phonetic state, more than one observations are possible. We shall denote the pro-bability of observation of a character g in a phonetic state P as Pr ( g | P i ) . Thus, in order to model the unintentional errors committed while typing a letter corresponding to a phonetic or syllabic state, ideally, we should allow null and wildcard emissions with these states. However, it turns out to be unnecessary, because any deletion or typo in the phonetic path can be equivalently modeled by a deletion or typo in the graphemic path. In fact, by inspecting a TL word, it is impossible to predict whether an unintentional error (such as those in Table 4 ) has been caused in the context of phonetic substitution or not. 6.4 Crosslinkages and state minimization While typing a word, people often switch between the gra-phemic and phonemic domains. For example, in the case of  X  X ransl8in X  for  X  X ranslating X ,  X  X ransl X  and  X  X n X  are in the  X  X raphemic X  domain, whereas the  X 8 X  represents a phone-tic substitution. To capture this pheonomenon, we introduce crosslinkages between the graphemic and the phonetic paths. A transition is added from every graphemic state G i to a state P , if and only if the grapheme g i + 1 in w is the surface reali-zation of the phone p j . Similarly, a transition is added from a phonetic state P i to a graphemic state G j , if and only if p is the phonetic correspondent of g j . Similar transitions are also introduced from the graphemic states to syllabic states, and vice versa. Figure 1 c shows the HMM for  X  X oday X  after addition of the crosslinkages.

The heuristics used for identification of crosslinking sites are as follows.  X  The graphemic and phonetic states are scanned simul- X  States corresponding to vowels are also aligned similarly.  X  If G  X  After alignment of the states, finding the crosslinking sites  X  Let there be a transition from the state G
The HMM so constructed might have some redundant phonetic states, which emit only a single character that is identical to the primary character emitted by the correspon-ding graphemic state. For example, P 1 in the HMM of  X  X oday X  emits only the letter  X  X  X , which is also the primary observa-tion for the graphemic state G 1 . Therefore, P 1 can be merged with G 1 without affecting the word model. We minimize the HMM by identifying and merging such redundant phonetic states and appropriately redefining the transition edges. The minimized HMM for  X  X oday X  is shown in Fig. 1 d. The algo-rithm for minimization is straightforward; after the alignment of the phonetic and graphemic states using the procedure described above, the corresponding graphemic and phone-tic states, say G i and P j , are inspected. If the only possible observation for P j is g i , then the state P j is removed. A tran-sition is added from P j  X  1 to G i and from G i to P j + is a transition to/from P j from/to a syllabic state S k , then a transition is introduced to/from G i from/to S k .

At this point, it is worthwhile to compare the word model of TL proposed here with the Toutanova and Moore error model. Though not exactly, but in essence, the graphemic and phonetic paths in the TL word model capture the distributions Pr correspondence is not exact, because unlike the graphemic path, which models only letter-by-letter substitution or dele-tion, the probability Pr LT R (w | w) models the substitution of letter-strings by letter-strings. The phonetic path, however, also models the case of substitution of a string of letter by a single character. But we would like to emphasize the fact that while in Toutanova and Moore model the final score of a word is computed through a log X  X inear combination of the phone-tic and graphemic error models, here the two models (i.e., the graphemic and phonetic paths) are combined through appro-priate crosslinking of the states.

Thus, conceptually, the Toutanova and Moore model assumes that a word w could have been generated from w , either by graphemic or by phonetic substitutions applied independently on the whole word, and a weighted sum of these two distinct hypotheses is used to score a word. On the other hand, we assume that w has been generated through the transformation of certain fragments of w at the grapheme level, while certain other fragments of w at the phonetic level. Therefore, we estimate a single probability Pr (w | w ) of the transformation, during which we also identify the fragments that have been transformed at the grapheme level, and those that have been transformed at the phonetic level. 6.5 Extended state Initial experimentation with the HMMs so constructed revealed that the model failed to suggest the correct word for several cases, where an  X  X  X ,  X  X  X  or  X  X  X  was appended at the end of TL word (e.g.,  X  X ite X  for  X  X ight X  and  X  X nyways X  for  X  X nyway X ). In order to capture both this, we introduce an extra state E X T between G l and S l + 1 , which emits  X  X  X ,  X  X  X  and  X  X  X . However, the transition between G l and S l + 1 is also preserved, because the extra character need not be appended always. Figure 1 e shows the HMM after addition of the extra state.
 This completes the description of the structure of the HMM. In the next section, we describe automatic construc-tion and parameter estimation of the HMMs. 7 Construction of the word model There are two steps involved in the construction of the word model: (1) construction of the structure of the HMM, and (2) assignment of the transition and emission probabilities. The second step can be further subdivided into two steps: (2a) estimating the parameters from a training corpus for the frequently observed words, and (2b) generalization of the parameters for any given word. These three processes are described below. 7.1 Construction of the structure Given a word w = g 1 g 2 ,..., g l , the structure of the HMM for w is automatically constructed as follows: 1. The initial state S 0 and the final state S l + 1 are defined, 2. The l graphemic states, G 1 to G l , are constructed. Tran-3. Every state G i is allowed to emit the three symbols: g 4. The pronunciation, w = p 1 p 2 ,..., p k ,for w is searched 5. The k phonetic states, P 1 to P k are constructed. Tran-6. Every state P i is allowed to emit a set of characters cor-7. Similarly, a list of mapping from characters to possible 8. The crosslinkages are introduced based on the heuristics 9. The redundant phonetic states are removed using the 10. A state EXT is constructed that emits  X  X  X ,  X  X  X  and  X  X  X .
The structure of the HMM for a word described so far defines the possibilities, in the sense that the transitions and observations that are present have a probability greater than 0 and the ones not present in the model have probability equal to 0. The process of estimation of the parameters are described underneath. 7.2 Supervised estimation In order to specify the HMM completely, we need to define the associated model parameters  X   X  ( A , B , X ) , where A is the state transition probabilities, B is the observation proba-bilities and  X  is the initial state distribution. We shall denote the probability of transition from a state X to a state X as Pr ( X  X  X ) . The probabilities of all the outgoing transi-tions from a state must sum up to 1.

By the definition of the model, the initial state distribution is given by  X (
X ) =
The parameters A and B are estimated from the training corpus using the EM algorithm. For this purpose, we constr-uct the structure of the HMMs for the frequent words w 1 to w 234 present in the training set. For each word an initial model  X  0 j , where the initial state distribution defined according to Eq. 7 . The emission probabilities for each graphemic state G i are defined as follows: Pr ( g i , G i ) = 0 . 7 , Pr (, G i ) = 0 . 2 , Pr ( @ , The probabilities have been so chosen to reflect the fact that unintentional errors are rarer than deletion, whereas faithful-ness to the word form gets even a higher priority.

The emission probabilities of phonetic, syllabic and exten-ded states are assigned a uniform distribution. Therefore, if there are n characters emitted by a phonetic state P i , then for each character g , the emission probability Pr ( g , P i ) Similarly, Pr (  X  s  X  , EXT ) = Pr (  X  z  X  , EXT ) = Pr (  X  e  X  , EXT
The transition probabilities for the edges from the start state are defined uniformly. This implies that to start with, we assume that phonetic and graphemic substitutions have the same probability. For the graphemic states, the transition to the next graphemic state is assigned twice the probability of the transition to a phonetic or syllabic state. Thus, if from the state G i , there are transition to the states P j and S Pr ( G i  X  P j ) = Pr ( G i  X  S k ) = 0 . 25 (11) On the other hand, if there is no transition to a syllabic state, then Pr ( G i  X  G i + 1 ) = 0 . 67 (12) Pr ( G i  X  P j ) = 0 . 33 (13)
Similarly, for a phonetic state P i , the transition probabi-lity to another state in the phonetic path (say S k or P i is twice the probability of transition to a graphemic state G j . The same principle is also followed for assigning the probabilities to the transitions from a syllabic state. These initial assignments of the transition probabilities are moti-vated by the fact that there is an inherent inertia in the user against switching between graphemic and phonetic substitu-tions. Note that by construction there can be at most three transitions from a state.

The EM algorithm proceeds as follows. Given  X  0 j , we find out the most likely sequences of states through the HMM for each observation (i.e. variation in the training set) v j using the Viterbi algorithm [ 39 ]. This is called the Viterbi path of v j i in the HMM for w j with parameter  X  0 j . We keep a count with every transition and emission in the HMM, which are initialized to 0. Then for every variant v j i , the count of the transitions and emissions in the Viterbi path of v j i is incre-mented by f j i (the frequency of the variant v j i ). Let the final counts associated with emissions and transitions be C ( g and C ( X  X  Y ) , respectively. The model parameters of the HMM are re-estimated using the following equations.: Pr ( X  X  Y ) =
We add the term  X  = 0 . 1 to deal with 0 counts; this process is known as Add - X  smoothing (see [ 24 ] for details). We shall denote this re-estimated model for w j as  X  1 j .Using  X  1 the initial model, we re-estimate the parameters again follo-wing the aforementioned step, and arrive at  X  2 j . The process is continued till a stage, after which re-estimation does not change the model parameters any more, i.e.,  X  m j  X   X  m + This is the final HMM model, which we shall denote as  X  j Thus, at the end of the supervised estimation, we have 234 HMM models  X  1 to  X  234 .

There are two points worth mentioning. First, the paucity of training instances led to very fast convergence of the models. In fact, in many cases just one iteration was suffi-cient, and there were hardly any case that required more than two iterations. The second issue is regarding the modifica-tion of the Viterbi algorithm to deal with null emissions. In the standard form of the algorithm that uses dynamic progra-ming, a matrix V m  X  n is filled up columnwise, where the entry v j denotes the probability of being in the i th state having seen till the j th input symbol. The implicit assumption is that the Viterbi path constructed so far has j states, one state each for the j input symbols seen so far. However, if null emissions are allowed, the number of states in the Viterbi path can be much larger than the number of input symbols seen. To resolve this problem, we define a three-dimensional matrix V m  X  n  X  r , where the entry v i , j , k denotes the probabi-lity of being in the i th state, having seen till the j th input symbol, and having traversed k states (i.e., the partial Viterbi path has exactly k states). The definition of v i , j , k extension of the standard recursive definition followed while filling up the two-dimensional Viterbi matrix, and therefore, omitted. Nonetheless, it must be emphasized that the acyclic structure of the HMM essentially bounds the value of r by l + 2( l being the length of the word w ), making the extension possible. 7.3 Generalization Due to the paucity of training data it is not possible to learn the HMM parameters for every word of the standard lan-guage. Therefore, in order to generate the model parameters of unseen and infrequent words, we extrapolate the HMM models  X  1 to  X  234 learnt from the training set. In Sect. 5 we have attempted to enlist and categorize the different inten-tional and unintentional distortions observed in TL. We have identified the correlates of each of these error (or compres-sion) types with the HMM parameters. The values of the parameters or the compression operations are learnt as func-tions of the features of the word and the characters.
The list of generalization parameters used for extrapola-tion and their physical significance are described below.  X  Pr (, G  X  The emission probability Pr ( g , P  X  The ratios of path switching probabilities (i.e., Pr ( G  X  The ratios of transition probabilities from the start state
Since, we have only 234 instances (HMMs) to learn from, we cannot afford to learn any of the aforementioned para-meters as continuous functions of l . Therefore, we split the words into three categories: short (1  X  l  X  3 ) , medium (4  X  l  X  6 ) and long ( l  X  7 ) , and learn the functions for each of these word categories separately. Similarly, the posi-tional information i is further discretized as word-beginning, word-middle and word-end, where the value of i distingui-shing for word-middle and word-end is chosen based on l .
These design choices help us restrict the domain of each of the functions, so that we can compute the values of the functions for each point in the domain by averaging over the corresponding values obtained from the 234 HMMs. For example, if out of the 234 HMMs, in 60 the value of Pr ( S G HMMs, there is no transition from S 0 to P 1 (may be because P 1 has been merged with G 1 during the state-minimization phase), then the estimated value of the ratio is ( 60  X  2 1 )/ 100 = 1 . 6
Given a word w ,if w belongs to the set of 234 words that has been used for training the initial models, then we adopt the learnt model  X  k for w (where k is the index of w in the training set) without any change. However, if is an infrequent or unseen word, 4 we construct the HMM for w as described in Sect. 7.1 and assign the emission and transition probabilities based on the functions learnt for the generalization parameters. Note that the ratios of transition probabilities along with the constraint that the sum of all the outgoing transition probabilities from a state is 1, allows us to compute the exact probabilities for each transition. 8 Evaluation In order to evaluate the goodness of the word model of TL, we build a decoder from TL to SL as described in Eq. 4 .Given a TL token t and a word w i of SL, the term Pr ( t | w i ) be estimated as follows: first, we construct the HMM for w then we compute the Viterbi path in the HMM for t . If there exists such a path, then Pr ( t | w i ) is assigned the probability of the Viterbi path, else it is assigned a value of 0. However, as shown in Eq. 4 , to build the decoder, we also need to estimate the value of Pr ( S ) , that is the model of SL. We run experiments for the following three assumptions of SL:
Uniform model: Pr ( S ) is same for all S . The correspon-ding decoder  X  0 is defined as follows.  X  ( T ) = argmax
Unigram model: Under the unigram assumption, the probability of a sentence S is the product of the probabilities of the words, where the probability of a word, Pr (w) is its normalized occurrence frequency. Thus, the corresponding decoder equation is  X  ( T ) = argmax
Bigram model: Under the bigram assumption, the deco-der equation is  X  ( T ) = argmax
Note that for the uniform and unigram models, sentence level decoding is equivalent to a word by word decoding. Therefore, while testing these models, we shall use a list of words as test cases rather than sentences. However, for the bigram model, sentence level testing is necessary. The implementation of  X  0 and  X  1 is straightforward. We carry out an exhaustive search over all the words in the lexicon of SL to find out the best match. Nevertheless, exhaustive search is not possible in the case of the bigram model, and there we do a beam search [ 33 ], with a beam width of 20 words. Although it is possible to optimize the search process in several ways, that would only increase the search speed (which is not our concern here), and not the accuracy. 8.1 Data for SL model In order to implement the decoders, we need a list of words in SL, over which the argmax search has to be carried out. For this purpose, we use a lexicon of 12,000 most frequent English words. For building  X  1 , the unigram frequencies of the words are also required. The unigram frequencies were obtained from [ 45 ]. The bigram frequencies required for have been obtained from [ 9 ]. It has been observed that the results of the decoder is poorer for the bigram model than the other two. This apparently absurd result is due to the fact that the bigram estimates obtained from a standard English corpus hardly reflects the word distribution of the SMS corpus. This led us to carry out experiments with a bigram model, where the bigram frequencies are estimated from the SL part of the SMS corpus that has been collected during this work. We shall denote this decoder model as  X  2 TL ,wherethe2inthe subscript stands for bigram and the TL stands for Texting language corpus, from which the model has been estimated.
We also carried out some experiments on language model adaptation, where the bigram frequencies were estimated through a weighted linear interpolation of the bigram fre-quencies obtained from the TL corpus and a standard English corpus [ 9 ]. Best results are obtained when the weights assi-gned to the TL and Standard English models are 1 and 0 respectively. Therefore, we do not report the detailed results of language model adaptation. 8.2 Test data We test the different decoder models on the following four data sets:  X  Test Set 1 (TS1) contains 702 distinct TL tokens, which  X  Test Set 2 (TS2) consists of 1,228 distinct TL tokens ran- X  Test Set 3 (TS3) is a subset of TS2, consisting of all the  X  Test Set 4 (TS4) has been constructed to test the bigram
Note that for all the test data sets, we also know the gold standards, as they are available from the manual tranlsations. All the test sets are available at [ 14 ]. 8.3 Testing methodology The word level decoder models (  X  0 and  X  1 ) are tested as follows: given a TL token t , the probability Pr (w | t ) puted for each word w of SL following Eqs. 17 and 19 .The words are sorted in decreasing order of Pr (w | t ) . For ease of implementation, we use negative logarithms of the probabi-lities, which monotonically decrease with the probabilities. Consequently, we arrange the results (i.e. w ) in increasing order of  X  log Pr (w | t ) .The rank of a translation (decoder X  X  output) is its index in this sorted list. Thus, the rank 1 transla-tion, which has the maximum value for Pr (w | t ) , is the output of the decoder, in case only the single best translation is allo-wed. However, we evaluate the decoder based on the first 20 suggestions (i.e., form rank 1 to 100) because, incorporation of a more sophisticated language model is expected to further discriminate and elicit the correct suggestion among the top 100.

Note that the unseen TL tokens (i.e. OOV) are handled appropriately by the word-level decoder as long as the cor-responding SL word is present in the dictionary. However, if the TL token is generated from a word w which is not present in the dictionary, then w will not be included in the translations suggested by the decoder. This is more often the case with names and other proper nouns, and is an extremely difficult problem to deal with.

Let the gold standard translations of t be gold 1 , gold 2 For example, for the token  X  X in X , the gold standard transla-tions are  X  X een X  and  X  X eing X . If the output of the decoder matches any of these translations, we assume it to be correct. Thus, given a test data set, we can compute the ranked-recall A ( r ) of a decoder as the ratio of  X  X he number of tokens, for which at least one of the gold standard translations are within the top r suggested translations X  to  X  X he total num-ber of tokens X . Note that A ( r ) is a monotonically increasing function of r .

The sentence level decoders,  X  2 and  X  2 TL , are evaluated as follows: for every input sentence in TL, the output is a sentence in SL. Although, in order to analyze the incorrect results, we have tapped the list of suggestions for every token in the suggestion list separately, here we report the accuracy for only the final single best sentence obtained from the deco-der. The accuracies are computed at the word level, i.e., the ratio of the number of words correctly translated to the total number of words. 8.4 Baseline experiments In order to compare the proposed method with existing spell checkers, we use the suggestions of Aspell [ 3 ], when run in the default mode, as the baseline. The baseline tests are conducted on TS1. The baseline results are: A ( 1 ) = 21 . and A ( 20 ) = 62 . 12%. Aspell is an open source spell checker that uses the double metaphone encoding strategy [ 38 ]for correcting spellings. The technique used is very similar to that of Soundex [ 35 ]. Note that the baseline results are only indicative and not very rigorous, since the standard language dictionary used in the baseline experiment is not same as that used for testing our model.

It would be nice to compare the results with that of Brill and Moore model [ 12 ] or Toutanova and Moore model [ 44 ]. However, we could not do so for several reasons. Firstly, the decoder models proposed here are customized for TL and do not capture other common types of errors such as insertion and transposition. Therefore, it would be crude to compare the models on the data used in [ 44 ]. Secondly, it is difficult to train the other two models solely on the TL data (due to the size and the type of the data), which is important if we want to compare the models on the TL data. Nevertheless, with some modifications it may be possible to train the Brill and Moore, and Toutanova and Moore models on the TL data, which we deem as a part of our future work. 8.5 Results for word level decoders Table 5 summarizes the results of the experiments for the word level decoders. Comparing experiment E1 (refer to the table) with E2, E4 or E5, we observe that the word level decoders proposed here reduces the error rate of Aspell by around 50% for rank 1 suggestions and 70% for rank 20. We also note that the surprisingly high accuracy of E3 on unseen data as compared to the results of model testing, i.e. E2, is due to the presence of a large number of undistorted tokens in TS2. The experiment E4, conducted on TS3 that contains only distorted tokens, is comparable to that of the model tes-ting results. The fact that the accuracy of  X  0 for unseen data is marginally less than that for seen data, shows that the genera-lization process has successfully captured the structure of TL and the models are not overfitted. The increment in the accu-racy due to the incorporation of the unigram language model has been nominal. This is presumably due to the fact that the unigram frequencies estimated from a standard English corpus do not reflect the characteristics of TL.

Table 6 shows some of the suggestions generated by  X  0 during experiment E2 and E4. Table 7 compares the first few suggestions generated by  X  0 with that of Aspell for some representative TL tokens (taken from E1 and E2). Since Aspell cannot handle tokens with numerals (e.g.,  X 2day X ) it does not generate any suggestions. It is worthwhile to note that whereas the suggestions generated by Aspell is strongly correlated to the token in terms of edit distance, the same is not the case for our model. 8.6 Results for sentence level decoders The fact that in around 90% of the cases the correct sugges-tion has a rank less than or equal to 20, we choose the beam width to be 20 while implementing the argmax search for the bigram models. The bigram models  X  2 and  X  2 TL are tested on TS4. TS2 and TS3 have all unseen tokens while TS4, which has been used for testing  X  2 and  X  2 TL , has both seen and unseen tokens. Therefore, in order to measure the improve-ment obtained through incorporation of the language model, we re-estimate the accuracy of  X  0 on TS4, which varies from A ( 1 ) = 86% to A ( 20 ) = 96% (Table 5 ).

The accuracy of  X  2 (where bigram probabilities are esti-mated from standard corpus) measured in terms of number of words correctly translated is 65%. This drastic fall in the accuracy is due to the fact that the bigram characteristics of the SMS text is largely different from that of standard English text. For  X  2 TL , where the bigram characteristics are obtained from the TL data, the accuracy is 91%. Thus, the incorpora-tion of the bigram model reduces the error rate of  X  0 (for rank 1 suggestions) by 35%. Some example input X  X utput pairs for  X  2 TL are shown in Table 8 . 9 Conclusion In this article, we have described an HMM-based word error model for TL. The model has been used to construct a decoder from English SMS texts to their standard English forms with a considerable accuracy. We summarize below the salient contributions of this work.  X  A word-aligned parallel corpus for SMS and standard  X  A taxonomy has been proposed for the errors or distortion  X  An HMM-based error model for the TL words, which  X  We have devised algorithms for construction of the word  X  The decoders constructed on the basis of the word model
Although the proposed word error model is tailor-made for the SMS error patterns, we believe that it can be appropriately modified to capture distortions over other types of TL. For instance, a commonly observed pattern in the chat texts is the repetition of characters to show emphasis (e.g.,  X  X ooooo X  for  X  X o X ). This can be captured by incorporating self-loops in the graphemic states. Transposition errors (e.g.,  X  X ks X  for  X  X sk X ) are also common with texts typed over a computer keyboard. This can be dealt with appropriately by introducing backward edges in the graphemic states. The extension of the error model forms a part of our future work.

The learning technique proposed here can also be enhan-ced in several ways. For instance, syllable and word level analogical learning technique, where the HMM parameters of an unseen word, say  X  X reatest X , can be learnt from the HMMs of the known words having similar phonetic or gra-phemic structure, such as  X  X ate X  and  X  X est X , can be useful during the generalization phase. Another important research direction, as pointed out earlier, can be the study of the syn-tactic and semantic properties of TL, which in turn can faci-litate better language modeling and provide useful linguistic insights.

We have already discussed the possible applications of this work in search engines, text correction over web and other texting medium, and text-to-speech systems. We would like to further emphasize the fact that TL is not only a co-existing, infrequent and informal variant of the standard language; rather it is a process of language change, propelled by the technological and social changes, and has the potential to change the structure of the standard language altogether [ 6 , 16 , 41 ]. In this light, study of the structure of TL becomes an extremely important research program, where diachronic linguistics can provide valuable insights.

In this work, we have modeled the principle of economy through the structure of HMM. Nevertheless, functionalists also suggest the existence of an opposing force, i.e. distinc-tiveness, which shape the structure of language. The concept of distinctiveness has not been modeled explicitly in this work. Consequently, we find several counterintuitive sug-gestions obtained from the decoder. For instance, in Table 7 , we observe that for the token  X  X 8 X ,  X  X y X ,  X  X s X  and  X  X r X  is ranked lower than  X  X ate X . Similarly,  X  X oin X ,  X  X hin X , etc. gets lower ranks than  X  X eeing X , which, in fact, does not feature in the top 100 suggestions for the token  X  X in X .

Note that if  X 8 X  was an unintentional substitution error in  X  X 8 X  or  X  X in X  was obtained by deletion of some charac-ter between  X  X  X  and  X  X  X , then there are numerous possibili-ties, from which the tokens could have been generated. The force of distinctiveness (or unambiguity) would act against cases of such unintentional substitution or intentional dele-tion. Thus, appropriate modeling of the distinctiveness prin-ciple (i.e., the concept that user would provide just enough information to alleviate any confusion) can improve the per-formance of the decoder. Unlike the case of the principle of economy, which is a word level phenomenon, distinctive-ness has to be measured globally, with respect to the whole lexicon. This is clearly a useful and stimulating direction of research. References
