 Singular Value Decomposition (SVD) is computationally costly and therefore a naive implementation does not scale to the needs of sce-narios where data evolves continuously. While there are various on-line analysis and incremental decomposition techniques, these may not accurately represent the data or may be slow for the needs of many applications. To address these challenges, in this paper, we propose a Low-rank, Windowed, Incremental SVD (LWI-SVD) algorithm, which (a) leverages efficient and accurate low-rank ap-proximations to speed up incremental SVD updates and (b) uses a window-based approach to aggregate multiple incoming updates (insertions or deletions of rows and columns) and, thus, reduces on-line processing costs. We also present an LWI-SVD with restarts (LWI2-SVD) algorithm which leverages a novel highly efficient partial reconstruction based change detection scheme to support timely refreshing of the decomposition with significant changes in the data and prevent accumulation of errors over time. Experiment results, including comparisons to other state of the art techniques on different data sets and under different parameter settings, con-firm that LWI-SVD and LWI2-SVD are both efficient and accurate in maintaining decompositions.
Feature selection and dimensionality reduction techniques [20] usually involve some (often linear) transformation of the vector space containing the data to help focus on a few features (or combi-nations of features) that best discriminate the data in a given corpus. For example, the singular value decomposition (SVD [7]) of a data feature matrix A is of the form A = USV T , where the r orthog-onal column vectors of U form an r dimensional basis in which the n data objects can be described. Also, the r orthogonal column vectors of V (or the rows vector of V T ) form an r dimensional basis in which the m features can be placed. These r dimensions are referred to as the latent variables [16] or the latent semantics #1318788. This work is also supported in part by the NSF I/UCRC Center for Embedded Systems established through the NSF grant #0856090.
 of the database [7]: Intuitively, the columns of U can be thought of as the eigen-objects of the data, each corresponding to one inde-pendent concept/cluster, and the columns of V can be thought of as the eigen-features of the collection, each, once again, correspond-ing to a concept/cluster in the database. In other words, SVD can be used for co-clustering both data-objects and features simultane-ously. The r  X  r diagonal matrix S , can be considered to represent the strength of the corresponding latent concepts in the database: the amount of error caused by the removal of a concept from the database is proportional to the corresponding singular value.
SVD is computationally costly and therefore a naive implemen-tation does not match the real-time needs of scenarios where data evolve continuously: decomposition of an n  X  m matrix requires O ( n  X  m  X  min ( n, m )) time. While there are various on-line tech-niques, these are often slow or inaccurate. For example, one of the fastest techniques, SPIRIT [13] focuses on row insertions and can-not directly handle row deletions or column insertions/deletions. While a forgetting factor can be introduced to discount old objects, it cannot immediately reflect the properties of the removed entries on the decomposition. Moreover, since SPIRIT primarily considers data insertions and deletions, it is not applicable in situations where features of interest themselves evolve with the data (examples in-clude weights of tags extracted from data and proximity to the hubs within an evolving network). As we see in Section 5, it has a higher inaccuracy compared to other incremental techniques, such as [5].
Other incremental SVD algorithms, such as [5, 6, 8, 9, 11, 12, 14, 15, 17], operate on an existing SV decomposition by folding-in new data and features into an existing (often low-rank) SVD; algebraic matrix manipulation techniques are used to rewrite the new SV de-composition matrices in terms of the old SV decomposition and update (including downdating) matrices. [5] showed that a number of database updates (including removal of columns) can all be cast as additive modifications to the original n  X  m database matrix, A . These updates then can be reflected on the SVD in O ( nmr ) time as long as the rank, r , of the matrix A is such that r  X  p min ( p , q ) , where p is the number of new rows and q is the number of new columns. In other words, as long as the latent dimensionality of the database is low, the singular value decomposition can be updated in linear time. [5] further showed that the update to SVD can be computed in a single pass over the data matrix making the process highly efficient for large data. This and other existing algorithms can nevertheless be slow for many real-time applications.
In Section 2 we formalize these challenges and in Section 3, we propose a Low-rank, Windowed, Incremental SVD (LWI-SVD) al-gorithm, which leverages efficient and accurate low-rank approxi-mations t o speed up incremental SVD updates and uses a window-based approach to aggregate multiple incoming updates (insertion or deletions) and, thus reduces on-line costs. We also present, in Section 4, an LWI2-SVD algorithm which leverages a novel partial reconstruction based change detection technique to support timely refreshing of the decompositions to prevent accumulation of errors.
Experiment results reported in Section 5 confirm that LWI-SVD and LWI2-SVD are both efficient and accurate in maintaining de-compositions. We conclude the paper in Section 6.
At time stamp i , we are given a set of n data tuples D i { t given the set, L i , containing the r latent semantics of D weights). As time moves, new tuples arrive and some of the ex-isting tuples expire: at the next time stamp, t i +1 , the tuple set is D that expired and  X  D + i +1 are the new tuples that arrived. More-over, at time ( i + 1) , we have a new set, F i +1 , of features, where F not of interest anymore and  X  F + i +1 are the new features of interest.
Our goal is to quickly obtain L i +1 containing the r latent seman-tics corresponding to time instance, i + 1 , and efficiently maintain these r latent semantics as time further moves.
Let us be given an n  X  m data matrix X = U x S x V T x and an n  X   X  m  X  updated data matrix X  X  = X +  X  , where  X  is a max ( n, n  X  )  X  max ( m, m  X  ) change matrix. Note that if X  X  has larger dimension than X , X is padded with n  X   X  n rows of zero and m  X   X  m columns of zero to match the dimension of  X  , the removal of rows and columns are modeled by additions that result in zeroing of the corresponding rows and columns (which are then dropped from the matrix). Let us further assume that the change matrix  X  can be decomposed into  X  = AB T . Note that we can rewrite the matrix X  X  as
Given these, [5] incrementally maintains SVD as follows:
Let us also define Q A as the orthogonal basis of ( I  X  U and Q B as the orthogonal basis of ( I  X  V x V T x ) B . Both Q can be obtained through QR decomposition [4] of ( I  X  U x and ( I  X  V x V T x ) B :
Here Q A and Q B are orthogonal matrices and R A and R B are upper-triangular. It is easy to see, through basic matrix algebra, that the following holds:
Moreover, by substituting Equations 3 and 4 into Equation 1, we can get where K is equal to
Let us remember that X is an n  X  m matrix and X  X  is an n  X   X  m  X  matrix. Given this Let us consider the SV decomposition of K ; i.e., K = U K Equation 1 can be rewritten [5] as giving us the SVD of the new tuple matrix, X  X  .
 The challenge, of course, is to obtain the matrices, Q A and Q and the SV decomposition of K efficiently. In order to keep the complexity down, [5] suggests that A and B should be taken as combination of simple column vectors so that AB T can be the sum of multiple rank-1 matrices. This, however, may be a significant constraint in real-applications where the change matrix  X  itself can have a large size, indicating great amount of rank-1 matrices it pro-duces and updating a sequence of rank-1 matrix is not effective as treating them as a whole. In the next section, we discuss how to relax this assumption of [5] without impacting efficiency and accu-racy.
We now present our key ideas for efficient incremental SVD op-erations. As described above, this involves efficiently searching for matrices, Q A and Q B , and the SVD of K .
As described above, Q A is the orthogonal basis of ( I  X  U and Q B is the orthogonal basis of ( I  X  V x V T x ) B . These can be ob-tained using two expensive QR decomposition operations for both Q
A and Q B . One way to reduce the number of QR decompo-sition operations would be to seek a decomposition of  X  where X  X  = X +  X  = AA T ; i.e., A = B . However, not all  X  will have such a convenient decomposition. When  X  is negative definite, it cannot be written as the format of A  X  B where A = B .
 Instead, in this paper, we propose to reduce the cost of the overall QR decomposition step by setting A to the identity matrix I and setting B T to  X  . This does not lose any generality on the algorithn since  X  ( B T ) can be any matrix. When we do this, since A = I , it would also be the case that Q A = 0 I . Therefore, we need only one QR decomposition. What is more, if the  X  only reflect a small amount of data insertions and deletions, then it will be a sparse matrix with last few rows and columns of nonzero values. This lead to efficient computation of ( I  X  V x V T x ) B a nd V matrix multiplication. Let X  X  first find the zero block of B when it is data insertion. Then  X  ( B ) is a n  X   X  m  X  matrix with a block of zero values on the first n  X  m position. We can rewrite B as
Then, we can divide ( I  X  V x V T x ) and V T x into the same block size as B . For example, we can rewrite V T x as Then, the multiplication of V T x B becomes
Note that, the multiplication of V x T 0 and the corresponding block of B is avoided since the corresponding block of B is all zeros. Also, the other part of V T x and B are small size thin matrices. Thus, the multiplication of V T x  X  B can be done very efficiently. The same applies to ( I  X  V x V T x )  X  B and when the data are deleted.
As we experimentally show in Section 5.4.1, this optimization provides significant gains in time, without any noticeable loss in the final accuracy.
The next challenge is to obtain the singular value decomposition of the matrix, K . Performing SVD on K directly would be costly as the SVD operation is expensive. However, as we prove next, in Section 3.2.1, in the presence of row and column insertions, K takes a special structure:
More specifically, in the presence of insertions, (a) since S diagonal, K is mostly sparse, and (b) it is shaped like an arrow: (aside from the diagonal) there are non-zeros only on its last rows and columns. We verify these next. Let X be an n  X  m matrix and X  X  be an n  X   X  m  X  matrix. In Section 2.2.2, we have seen that K is either of size n  X   X  m  X  , ( n + 1)  X  ( m +1) , ( n +1)  X  m , or n  X   X  ( m +1) , depending on whether the numbers of rows and columns increase or decrease when the data matrix transforms from X to X  X  . Let us further assume that n  X  m and m  X  &gt; m and n  X  &gt; n , which is rows and columns insertion. As we already discussed before, let us set A = I B that AB T is equal to the update matrix  X  . Finally, let SVD of X be X = U x S x V T x , or simply X = USV T .
 Given the fact that X  X  = X +  X  , we can also deduce that X  X  = U  X  SV  X  T +  X  , where
Intuitively, U and V are augmented by padding n  X   X  n rows of zeros to U and m  X   X  m rows of zeros to V to make it compatible with  X  . This padding gives us
Secondly, using a similar zero-padding, we can get the following equalities:
The right hand side of Equation 9 has n  X   X  n independent columns and, thus, it has a simple QR decomposition:
Since the right hand side of the Equation 10 consists of 0s except for the last m  X   X  m rows, the QR decomposition of the left hand Let us further partition R B into two, Given the above, we can rewrite the matrix, K , as where Here Y is a m  X  ( n  X   X  n ) matrix. Note that we can further rewrite as Thus, K simplifies to
This confirms that when m &lt; m  X  and n &lt; n  X  , K is shaped like an arrow: it is diagonal, except for the last n  X   X  n rows and last m  X   X  m columns. This, however, is not true when m  X  m  X  or n  X  n  X  ; in this case K can be a dense matrix, with its last row and columns equal to 0 . In the rest of this section, we argue that, especially when m &lt; m  X  and n &lt; n  X  , we can leverage K  X  X  specific structure (sparse, arrow-like) to quickly obtain a highly-accurate approximate decomposition, K  X   X  U  X  S  X  V T and use it instead of the exact decomposition K = U  X  S  X  V  X  T . In particular we propose to build on the SVD through QR decomposition with column pivoting technique proposed in [4]. Experiment results reported in Section 5 show that this leads to efficient and accurate decompositions even in cases where m  X  m  X  or n  X  n  X  . Pivoted QR Factorization. Let E be a matrix. A pivoted QR factorization of E has the form EP = Q e R e where P is a per-mutation matrix, Q e is orthonormal and R e is upper triangular. [4] has shown that a rank-k approximation can be obtained efficiently through a pivoting process where columns of E are considered one at a time and used to compute an additional column of Q e row of R e . The k th round of the process leads to a rank-k ap-proximation of the pivoted QR factorization of E . In particular, let us assume that we are given a QR decomposition of the form F = Q f R f and need to compute QR decomposition of [ F a ] for some column vector a : The rank-k approximation can be obtained efficiently by the quasi-Gram-Schmidt method, which further eliminates the need to store dense Q f matrices [4]: the quasi-Gram-Schmidt process can be ap-plied successively to columns of a given input matrix E to produce a pivoted QR factorization for E .
 Low-Rank Decomposition of K . Let us assume that we are tar-geting a rank-k decomposition of K . We first sample k columns to obtain column-sample matrix C ; we then sample k columns from K
T to obtain a row-sample matrix R T . We then apply the QR decomposition with column pivoting to C and R T to obtain upper triangular matrices, R c and R r .

The sampling is done by selecting the longest row and column vectors. We note that when m &lt; m  X  and n &lt; n  X  , K is not only sparse, but also has an arrow-like shape: where the n  X  m matrix S x is diagonal, whereas n  X  ( m  X   X  m ) matrix  X  , ( n  X   X  n )  X  m matrix  X  , and ( n  X   X  n )  X  ( m  X   X  m ) matrix  X  are potentially dense as we discussed in Section 3.2.1. As a result, the sampling is arrow-sensitive in the sense that it focuses on the last few rows and columns: The sampled columns usually come from the first few columns (which contain the largest singular values at the top-left corner of the matrix) and the last few columns, which contain entries from the dense,  X   X  . Similarly, the sampled rows come from the first few rows (which contain large singular values in S x ) and the last few rows from  X   X  .

Given these, to obtain a decomposition of K , we need to find a matrix H such that k K  X  X  H R T k is minimized. According to [15], the value of H which minimizes this can be computed as Thus, we can rewrite C H R T as If we further set W = R  X  T c C T K R R  X  1 r and decompose W into W = U w S w V T w , then we can obtain the SV decomposition of K as K = U K S K , V T K , where where U K and V K are orthonormal and S K is diagonal. While this process also involves an SV decomposition step involving W , since W is a much smaller, k  X  k , matrix, its decomposition is much faster than the direct decomposition of K .
Algorithm 1 provides the pseudo-code of the proposed Low-rank, Windowed, Incremental Singular Value Decomposition (LWI-SVD) algorithm for incrementally maintaining the SVD of an evolv-ing matrix X . As we later see in Section 5, the LWI-SVD algorithm has a smaller approximation error than other algorithms, such as SPIRIT [13], yet is also much faster than optimal as well as the basic incremental SVD [5] algorithms. Yet, as in any incremen-tal approximate algorithm, in which each step takes the output of Algorithm 1 L WI-SVD.
 Input: Output: 1: Calculate factors R A a nd R B in Equation 2 which, as dis-2: Calculate the matrix K in Equation 7; 3: Obtain the low-rank (rank-r ) decomposition of K into K = 4: Combine the factors as shown in Equation 8 to obtain rank-r the previous step as its input, there is a likelihood that erro rs will accumulate over time and the reconstruction error relative to the actual matrix will reach an unacceptable rate. To prevent errors to accumulate, in the next section we propose a novel LWI-SVD with Restart (LWI2-SVD) algorithm which restarts the SVD by performing a fresh SVD on the current data matrix. In this section, we build on LWI-SVD and propose a novel LWI-SVD with Restart (LWI2-SVD) algorithm which punctuates the in-cremental SVD sequence by occasionally performing a full SVD on the current data matrix. Obviously, there is a direct, positive correlation between the frequency of restarts and the overall accu-racy of the LWI2-SVD algorithm. Unfortunately, however, there is also a strong positive correlation between the cost of LWI2-SVD and the frequency of restarts. Therefore, restart rate should be such that the process is restarted only when the costly SVD is in fact needed to help reduce the overall error.
We see that there are two distinct types of errors:
Figure 1 shows an example run with and without restarts. Note that without the restarts errors continuously accumulate due to struc-tural changes in the data. Restarts (both periodic and on-demand) c an limit the accumulation of errors. Error accumulations due to approximations generally show a regular behavior and the frequency with which periodic restarts are scheduled can be set empirically. The structural changes in the data, however, do not necessarily have a regular behavior; therefore, the challenge is to quickly and effi-ciently detect the structural changes in the data. We will discuss this next.
In order to detect major structural changes in the data we need to measure or estimate the reconstruction errors. The naive way to achieve this would be to reconstruct the entire matrix from the incrementally maintained decomposition and compare the recon-structed matrix to the ground truth (which is the actual, revised data matrix). If the difference is high, it means that due to some struc-tural changes, the incrementally maintained decomposition devi-ated from the true decomposition of the matrix. Obviously, per-forming a full reconstruction of the matrix at each time step would be extremely costly. Instead, in this section, we propose a change detection scheme which relies on a partial reconstruction as de-picted in Figure 2: (a) a fair data matrix sampler, which identifies a small subset of the matrix cells as ground truth and (b) a par-tial reconstructor, which reconstructs a given subset of matrix cells, without reconstructing the full data matrix.
We propose a fair sampler, where all matrix cells have a uni-form probability of being selected independently of when they are updated.
 Basic Reservoir Sampling. Reservoir sampling [18] is a random sampling method that works well in characterizing data streams. It is especially efficient because (a) it needs to see the data only once
Figure 3: Overview of the reservoir based matrix sampling and (b) it uses a fixed (and small) buffer, referred to as the  X  reser-voir  X . Furthermore, while (c) it does not require a priori knowledge of the data size, it (d) ensures that each data element has an equal chance of being represented in the sample. Let S be a data stream consisting of a sequence of elements s i . The reservoir sample keeps a fixed reservoir of, say w elements. Once the reservoir is full, each new element, s i , replaces a (randomly) selected element in the reservoir with a decreasing probability, inversely proportional to the index, i , of the new element s i . More specifically, a random element in the reservoir is replaced by s i with probability itively, in a fair sampling, each element up to i should have a w/i chance of being in the random sample of size w . Therefore, s selected to be included in the reservoir with probability sample it replaces, on the other hand, is chosen randomly among the existing w samples in the reservoir to ensure that the reservoir forms a random sample of the first i elements in the stream. Matrix-Reservoir Model. As we described earlier, we consider the general case where the data matrix can grow or shrink with insertions or deletions of rows and columns. More specifically, we model the evolving data matrix as a stream, S , of s i =  X h row where row i and col i are the row and columns affected in the update with index i : + h row i , col i i indicates that the update inserts a new cell in the matrix at location h row i , col i i , whereas  X  X  row indicates that the cell at location h row i , col i i is being removed.
The reservoir, R i = { r i, 1 , . . . , r i,w } , at time i consists of w matrix cell positions, which serve as the representatives for the cur-rent matrix. In other words, each r i,j  X  R i is a triple of the form r the update that deposited the cell, located at row i,j and col the reservoir.
 Matrix-Reservoir Maintenance for s i = + h row i , col i i . As dis-cussed earlier, reservoir sampling randomly selects some of the in-coming stream elements for the updating the contents of the reser-voir When the (probabilistically) selected incoming stream entry s is of the form + h row i , col i i , the basic reservoir sampling pro-cess is applied: a random element, r i  X  1 ,j from the current reservoir R i  X  1 i s selected and this is replaced with h i, row i , col cess is visualized in Figure 3(a).
 Matrix-Reservoir Maintenance for s i =  X  X  row i , col i i . When the (probabilistically) selected incoming entry s i is of the form  X  X  row i , col i i , on the other hand, the basic reservoir sampling pro-cess cannot be applied as this denotes removal of a cell, not inser-tion. We handle deletions as follows: Intuitively, the matrix reservoir (and its history) is revised as if the future insertion s h had in fact arrived in the past , instead of s index i  X  1 ,j , which had originally deposited the cell, h row i (which is being deleted) into the reservoir. This process is visual-ized in Figure 3(b).
At time t = i , let us have the reservoir R i = { r i, 1 , . . . , r where for all 1  X  h  X  w , r i,h = h index i,h , row i,h , col itively, the reservoir consists of a set of matrix cell positions (that were fairly sampled from the overall matrix). During the partial reconstruction step, we use the (incrementally maintained) SV de-composition, U i , S i , and V i , of the data matrix X i only the row and column positions that appear in the reservoir, r
More formally, the partially reconstructed matrix value set {  X  v i, 1 , . . . ,  X  v i,w } , is such that for all 1  X  h  X  w ,  X  Note that the cost of the partial reconstruction of the matrix de-pends on the size of the reservoir and when | R i |  X  | X reconstruction is much faster than full reconstruction.
At time t = i , given the reservoir R i = { r i, 1 , . . . , r construct a ground truth value set V i = { v i, 1 , . . . , v for all 1  X  h  X  w , v i,h = X i [ row i,h , col i,h ] . Similarly, we also have the partially reconstructed value set  X  V i = {  X  v i, 1 where for all 1  X  h  X  w ,  X  v i,h =  X  X i [ row i,h , col  X  X [ row i,h , col i,h ] is the partially reconstructed value for the cell location h row i,h , col i,h i . Given these, we detect a major structural change in the data matrix if where  X  is the inaccuracy threshold. We provide the pseudocode of the LWI-SVD with Restart (LWI2-SVD), which was detailed in this section, in Algorithm 2. In the next section, we evaluate the efficiency and effectiveness gains of LWI2-SVD algorithm on top of the gains provided by LWI-SVD. Algorithm 2 L WI2-SVD Input: Output: 1: X  X  = X +  X  2: if f = true then 3: h U  X  x , S  X  x , V  X  x i = topK _ SVD( X  X  , r ) ; 4: R  X  = updateReservoir( R ,  X ) ; 5: else 6: h U  X  x , S  X  x , V  X  x i = LWI -SVD( X, U x , S x , V 7: R  X  = updateReservoir( R ,  X ) ; 9: E = measurePartialError(  X  V , R  X  , X  X  ) ; 10: if E &gt;  X  then 11: h U  X  x , S  X  x , V  X  x i = topK _ SVD( X  X  , r ) ; 12: end if 13: end if
Symbol Desc. Default Alternative dim ( n  X  n ) Initial(for inser-num u pd Numbers of  X  u pd Strength of the updates I n this section, we evaluate the efficiency and effectiveness of LWI-SVD and LWI2-SVD on both synthetic and real datasets and for different scenarios and parameter settings.

Each experiment, consisting of len consecutive update iterations, was run 10 times and averages are reported. Note that to simplify the interpretation of the results we have considered insertion se-quences and deletion sequences ; but not hybrid insertion/deletion sequences. Also, to make sure that the results for experiments in-volving sequences of insertions and deletions are comparable, we have set the initial dimensions for an insertion sequence and the final dimensions of a deletion sequence to the same value, dim .
The various parameters varied in the experiments, default values, and value ranges are presented in Table 1. Below we describe the experimental setting, including the data sets, in greater detail.
We use Digg.com data set [2] from Infochimps to evaluate the effectiveness and efficiency for real data. The complete data set was recorded from August to November 2008 and has 3 main com-p onents: stories, comments and replies. "Stories" contain 1490 ar-ticles that users have posted within the time period. For our exper-iments, we created data streams by considering the first n + len  X  num upd articles in the data set(the first n articles make up the ini-tial data matrix; for each of the len iterations in the update stream, we considered num upd new articles).

Given this data set, we removed the stop words and applied stem-ming. We then selected the first n stories and identified the most frequent n keywords 1 . X ij denotes occurrence of keyword j in story i . Intuitively, the low-rank decomposition of the data ma-trix X simultaneously cluster stories and keywords, resulting a co-clustering of the data matrix X . We moved the window at each iter-ation by inserting or deleting num upd records of the story trace and recomputing the n most frequent keywords (meaning that num many rows and columns are inserted and deleted). These corre-spond to row and column insertions/deletions on X .
We have also experimented with synthetic data sets where we could freely vary the characteristics of the data and updates to ob-serve the accuracy and efficiency of our algorithms under different scenarios. For these experiments, we have created synthetic activity traces which we then converted into data matrices as before. Since the matrices for real data is sparse, we focus on dense matrices.
In particular, we have generated an initial n -length random se-quence of 5 dimensional data, where each dimension has a value from 0 to 10. Given these n consecutive records in the trace, we have created a n  X  n initial matrix measuring pairwise Euclidean distances of the records in the sequence. Insertions in the random trace were generated by randomly picking numbers with exponen-tial distribution, with the rate parameter,  X  upd (i.e., prob ( x ) = rameter  X  upd is large, there is a higher likelihood of having more large amplitude changes. If the rate parameter  X  upd is low, there is a lower frequency of large amplitude changes in the trace.
As before, we enlarged or shrank X at each iteration by adding or deleting num upd units of the random activity trace (meaning that num upd many rows and columns are inserted to or deleted from into the matrix, X ). We evaluate the LWI-SVD and LWI-SVD with Restart (LWI2-SVD) algorithms by comparing them to alternative approaches:
I n these experiments, without loss of generality, we kept the ma-trix in square shape, i.e., n = m
LWI-SVD family of the algorithms extend our implementation of the Brand X  X  algorithm described in [5] along with the Algorithm 844 [4] obtained from [1].

As evaluation criteria, we use three metrics: reconstruction error overhead, execution time, and execution time gain: All experiments were conducted using a 4-core Intel Core i5-2400, 3.10GHz, machine with 8GB memory, running 64-bit Windows 7 Enterprise. The codes were executed using Matlab 7.11.0(2010b).
Figure 4 presents the accuracy and efficiency results for the real trace data for the default parameters reported in Table 1. Accuracy. The first thing to note in Figure 4(a), which reports average relative reconstruction errors for the various versions of the LWI-SVD algorithm proposed in this paper, is that restarts dis-cussed in Section 4 are highly effective in reducing the overall error. While both partial reconstruction-based and periodic restarts used in LWI2-SVD are effective in improving accuracy over the LWI-SVD (which does not use restarts), the best results are obtained when these are used together, bringing down the average relative reconstruction error to 0.3-0.7 % of the low-rank decomposition ob-tained through full SVD.

The second thing to note in Figure 4(a) is that row/column in-sertions, which bring in new data into the matrix, results in larger relative reconstruction errors than row/column deletions. Note that, when both reservoir-based and periodic restarts are employed, the accuracy penalty relative to the low-rank decomposition of full SVD is negligibly low for both insertions and deletions.
 Efficiency. Figure 4(b) shows the efficiency results for this data set under the default parameter configuration.

The first thing to note is that there is minimal time difference between the LWI-SVD and LWI2-SVD algorithm. This indicates !""#"$%&amp;'"(')*$ F igure 4: Accuracy and efficiency for the real trace data set -default settings Figure 5: Accuracy and efficiency results for the s ynthetic trace data set for SVD, Spirit, and LWI2-SVD with periodic and on-demand refreshes that the time overhead of reservoir maintenance and occasion al on-demand full decompositions are negligible in the long run. Sec-ondly, performing full SVD takes  X  75 -100% more than the pro-posed LWI-SVD family of algorithms. Under this configuration, the naive incremental SVD takes a little more time than full SVD, as the basic algorithm reported in [5] involves a full SVD with same dimension as the original matrix and several matrix multiplications. Further-more, under this configuration, SVDS takes even longer than the full SVD.

Finally, a close look at the LWI-SVD family of algorithms in-dicates that insertions require slightly longer time to maintain than deletions. This is expected because, as discussed in Section 2.2.1, there is no need for computing R A and R B since they are all zero. F igure 6: Accuracy and efficiency for synthetic trace data set -default settings Impact of the QR-Elimination Optimization. In Section 3.1, we had discussed an optimization strategy whereby we eliminate one of the two expensive QR operations by forcing A to be equal to the identity matrix, I . As shown in Table 2, setting A = I causes less than half percentage point impact on the accuracy; on the other hand, this optimization helps save close to 12% in execution time.
Figure 6 presents results for the synthetic trace data set under the default parameter settings. The key observation from this figure is that the accuracy and efficiency results for the synthetic trace data set are very similar to the results for real trace data set, reported in Figure 4. The similarity is especially pronounced in the execu-tion time results in Figure 6(b). This indicates that the execution time gains of the LWI-SVD family of algorithms (and to a certain degree, the accuracies they provide  X  especially with the help of pe-riodic and reservoir-based restarts) are inherent properties of these algorithms rather than being highly data specific.
 SPIRIT. Since the SPIRIT [13] algorithm approaches the problem differently (e.g. cannot directly handle deletions, cannot handle deletions/insertion of columns), we present it separately from the rest in Figure 5. For these experiments, we use a synthetic data trace that does not include any column insertions or deletions on the data matrix X . As the figure shows, SPIRIT algorithm works much faster than SVD or LWI2-SVD for the default configuration. However, this speed comes with a significant increase in the re-construction error, relative to the optimal low-rank decomposition using SVD. In contrast, LWI2-SVD achieves an accuracy almost identical to the optimal, yet costs only half as much.
In this subsection, we evaluate the impacts of the various data and systems parameters on the efficiency and effectiveness of the LWI-SVD family of algorithms. As representative, we select the Figure 7: Accuracy and efficiency for r eal trace data set for dif-ferent rank, r Figure 8: Accuracy and efficiency results for the r eal trace data set varying the size, dim , of the initial (for insertions) / final (for deletions) matrix LWI2-SVD with the default parameters. We then vary, one-by-one, the various data and system parameters, and compare the results against the optimal SVD based rank-r decomposition. Since, as we have seen, the results are similar for real and synthetic data, for the most part we report the results with the real trace data. We use the synthetic trace only for experiments where we vary the strengths of the updates.
Figure 7 presents efficiency and accuracy results for the real trace data set where the target rank, r is varied. The results show that, as expected (due to the low-rank nature of the LWI-SVD family of algorithms), as the target rank increases, the time gain drops and the relative error rate slightly increases. The drop in time gains is because the incremental process involves a lot of matrix multipli-cations where the sizes of matrices are directly related to the target rank. This confirms the observation that LWI-SVD and LWI2-SVD are most effective when the target rank is low.
Figure 8 presents accuracy and efficiency results when we change the dimensions, dim , of the initial data matrix (for insertions) and the final data matrix (for deletions). Here, we see that increasing the size of matrix does not have a big impact on accuracy and effi-ciency.
Figure 9 presents efficiency and accuracy results for the real trace data set where the number, num upd , of row and column updates per each iteration is varied. The results indicate that, as expected, an increase in the number of updates per iteration impacts accuracy as well as efficiency. The slight impact on the accuracy is due to Figure 9: Accuracy and efficiency results for the r eal trace data set varying the amount updates per iteration, num upd Figure 10: Accuracy and efficiency for r eal trace data varying reservoir size, w Figure 11: Accuracy and efficiency results for the r eal trace data set varying the change threshold,  X  , for on-demand restarts the approximation nature of the algorithm. The impact on the time gain is due to more on-demand restarts.
Figure 10 presents efficiency and accuracy results for the real trace data set where the reservoir size, w , is varied. The results confirm that a larger reservoir (even only  X  1 . 5% of the matrix) can help to trigger on-demand restarts more fairly, since larger reservoir has more accurate amortized error measuring.
Figure 11 confirms that a slightly tighter threshold,  X  = 0 . 1 instead of the default  X  = 0 . 2 will trigger more on-demand restarts and thus can further reduce the error rates (which are already very low), with little impact on execution time gains.
Figure 12 confirms that increasing the number of restarts by re-ducing the restart period, per , may improve the final accuracy. However, unlike the on-demand restarts based on change detec-Figure 12: Accuracy and efficiency results for the r eal trace data set varying the restart period, per , for periodic restarts Figure 13: Accuracy and efficiency results for the s ynthetic trace data set varying the update strength,  X  upd tion (shown in Figure 11), blindly increasing the frequency of the periodic restarts may negatively impact the time gain.
Finally, in Figure 13, we see the impact of the strength (in am-plitude) of the incoming insertions. The figure shows that, when  X  upd increases, the LWI2-SVD algorithm adjusts its operation by scheduling more on-demand restarts at a cost of decreasing the time gain.
The results shown above are conducted with small window size, however, in some cases, we need large windows to monitor and analyze a large portion of the data. In this subsection, we analyze the scalability of LWI Algorithm by choosing large base number. Since we have shown that under the small base number condition, SVD out performs SVDS in execution time, however, when the base number is large, seeking a low rank deposition using SVDS is more efficient. Also, as we know that SVDS is very efficient when the data is sparse, but performs less efficient on dense data. We showed that LWI algorithm can concur this short coming when the data is dense. Recall in section 3.2.1, we showed that K is an arrow-like matrix which is very sparse, this leads to the efficiency by using pivoted QR compared to a direct SVDS on the dense data. Therefore, in the incremental maintenance of SVD on a dense ma-trix, we are actually seeking a second layer reduced rank approx-imation of a sparse matrix K . It is the main advantage of LWI algorithm compared to SVDS when the data is dense and the base dimension is large. Table 3 shows the execution time and error overhead results under a synthetic dense data, the results confirm that with big base number especially when the base is a thin and tall matrix, LWI algorithm can have advantages in execution time with negligible error overhead .

I n this paper, we presented a Low-rank, Windowed, Incremen-tal SVD (LWI-SVD) algorithm, which relies on low-rank approx-imations to speed up incremental SVD updates. LWI-SVD algo-rithm also aggregates multiple row/column insertions and deletions to further reduce on-line processing cost. We also presented a LWI-SVD with restarts (LWI2-SVD) algorithm which performs periodic and change detection based on-demand refreshing of the decom-position to prevent accumulation of errors. Experiment results on real and synthetic data sets have shown that the LWI-SVD family of incremental SVD algorithms are highly efficient and accurate compared to alternative schemes under different settings.
