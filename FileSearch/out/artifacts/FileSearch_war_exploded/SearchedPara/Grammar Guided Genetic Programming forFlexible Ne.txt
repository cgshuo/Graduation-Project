 There has been growing interest in evolv ing architecture and parameters of a higher order Sigma-Pi neural network based on a sparse neural tree encoding [1]. Recently some approaches for evolvi ng the neural tree model based on tree-structure-based evolutionary algorithm and random search algorithm have been proposed in [11][12][14].

Antonisse [15] used grammars firstly to constrain the generation of chromo-some in his proposed system, which is called grammar-based GA. After then, some grammars-based GP systems was proposed. Stefanski [16] proposed the use of abstract syntax trees to set a declarative bias for GP. Robston [6] demon-strated how a formal grammar might be used to specify constraints for GP in the context of engineering design. Mizoguchi and Hemmi [7] suggested the use of production rules to generate hardware language descriptions during the evo-lutionary process.
 Three typical grammar guided GP systems can be classified as: Whigham X  X  CFG-GP system [8], Schultz X  X  grammar-based expert systems and Wong X  X  LO-GENPRO system[10][5]. Grammar Guided Genetic Programming (GGGP) [3][4] is a typical tree-structure-based genetic programming system. GGGP using a grammar to constrain search space. The individual GP tree in GGGP must re-spect the grammar. This overcomes the closure problem in GP and provides a more formalized mechanism for typing (strongly-typed genetic programming). Actually, the grammar model can do mor e than just constrain the search space. In Whigham X  X  work [9], in addition to the normal GGGP search, the gram-mar is slightly modified during the search. The updated grammar represents the accumulated knowledge found in the process of search.

In this paper, GGGP is firstly employed to optimize the Flexible Neural Tree (FNT). Based on a pre-defined instruction/operator sets, a flexible neural tree model can be created and evolved. FNT allows input variables selection, over-layer connections and different activation functions for different nodes. In our previous work, the hierarchical structure of FNT was evolved using PIPE with using the GGGP. The fine tuning of the parameters encoded in the structure is accomplished using Particle Swarm Optimization (PSO) [20]. The novelty of this paper is in the usage of GGGP for flexible neural tree optimization and for selecting the important inputs in the modeling of stock index.
 The rest of paper is organized as follows. A simple introduction of Grammar Guided Genetic Programming is given in Section 2, and a hybrid-learning algo-rithm for evolving the FNT is also presented in this Section. Some simulation results for stock index prediction are given in Section 3. Finally, some conclusions are given in Section 4. The function set F and terminal instruction set T used for generating a FNT model are described as S = F T = { + 2 , + 3 ,..., + N } { x 1 ,...,x n } ,where + ( i =2 , 3 ,...,N ) denote non-leaf nodes X  instructions and taking i arguments. x , x 2 , ... , x n are leaf nodes X  instructions and taking no other arguments. The output of a non-leaf node is calculated as a flexible neuron model (see Fig.1). From this point of view, the instruction + i is also called a flexible neuron op-erator with i inputs. In the creation process of neural tree, if a nonterminal instruction, i.e., + i ( i =2 , 3 , 4 ,...,N ) is selected, i real values are randomly generated and used for representing the connection strength between the node + i and its children. In addition, two adjustable parameters a i and b i are ran-domly created as flexible activation function parameters. For developing the fore-casting model, the flexible activation function f ( a i ,b i ,x )= e  X  ( x The total excitation of + n is net n = n j =1 w j  X  x j ,where x j ( j =1 , 2 ,...,n ) are the inputs to node + n . The output of the node + n is then calculated by can be computed from left to right by d epth-first method, recursively. 2.1 Tree Structure Optimization by GGGP Grammar Guided Genetic Programming (GGGP) is one of the important ex-tensions for GP [2]. The purpose of presented GGGP is mainly to overcome the closure problem [2], the generation and preservation of valid programs in GP system. For an object, some grammars are used to guide the generation of programs in GP, and a chosen declaration of bias can be set on the space of programs.

In this research, Context-free Grammar (CFG) [9] was chosen for FNT op-timization. A CFG consists of 4 sets, G = { N, T, P,  X  } ,Where N is a set of non-terminal symbols, T is a set of terminal symbols, P is set of production rules and  X  is set of start symbols, and N T =  X  , X   X  N . The production rules have the format x  X  y ,where x  X  N , y  X  N T . The production rules specify how the non-terminal symbols should be written into one of their deriva-tions until the expression contains terminal symbols only. For an example (Fig. 2), a CFG for generation one variable simply arithmetic expression can be de-scribed as follows,
Although the components of GGGP are the same as GP, there are still some distinct difference between GGGP and GP. In GGGP a tree-based program is generated according to the context-fr ee grammar. In crossover, two internal nodes labeled with the same non-terminal symbol of the grammar are chosen at random, and the two sub-derivation trees underneath them are exchanged. In mutation, a new randomly generated sub-derivation tree rooted at the same non-terminal symbol replaces the sub-derivation tree of the selected node. The general evolutionary process in GGGP can be described as the same as GP. For detailed description of GGGP algorithm, please refer to [3] and [4]. 2.2 Parameter Optimization with PSO The Particle Swarm Optimization (PSO) conducts searches using a population of particles which correspond to individuals in evolutionary algorithm (EA). A population of particles is ra ndomly generated initially. E ach particle represents a potential solution and has a position represented by a position vector x i .A swarm of particles moves through the pro blem space, with the moving velocity of each particle represented by a velocity vector v i . At each time step, a function f i representing a quality measu re is calculated by using x i as input. Each particle keeps track of its own best position, whi ch is associated with the best fitness it has achieved so far in a vector p i . Furthermore, the best position among all the particles obtained so far in the population is kept track of as p g . In addition to this global version, another version of PSO keeps track of the best position among all the topological neighbors of a particle. At each time step t ,byusingthe for particle i is updated by where c 1 and c 2 are positive constant and  X  1 and  X  2 are uniformly distributed random number in [0,1]. The term v i is limited to the range of  X  v max .Ifthe velocity violates this limit, it is set to its proper limit. Changing velocity this way enables the particle i to search around its individual best position, p i ,and global best position, p g . Based on the updated velocities, each particle changes its position according to the following equation: For detailed description of PSO algorithm, please refer to [20]. 2.3 The General Learning Algorithm The general learning algorithm for GGGP-FNT model can be described as follow: 1) Initialization. Set the initial value of parameters used in GGGP and PSO al-2) Structure optimization with GGGP algorithm, in which the fitness function 3) If a better structure is found then go to step 4), otherwise go to step 2). 4) Parameters optimization with PSO algorithm. In this stage, the structure of 5) If the maximum number of iterations of GGGP algorithm is reached, or no 6) If satisfactory solution is found, then stop; otherwise go to step 2). 3.1 Stock Index Modeling Prediction of stocks is generally believed to be a very difficult task -it behaves like a random walk process and time varying. The obvious complexity of the problem paves the way for the importance of intelligent prediction paradigms [17]. In this experiment, we analyze the seemingly chaotic behaviour of two well-known stock indices namely the Nasdaq-100 index of NasdaqSM [18] and the S&amp;P CNX NIFTY stock index [19]. The Nasdaq-100 index reflects Nasdaq X  X  largest companies across major industry groups, including computer hardware and software, telecommunications, retail/wholesale trade and biotechnology. The Nasdaq-100 index is a modified capitalization weighted index, designed to limit domination of the Index by a few large stocks while generally retaining the cap-italization ranking of companies. Through an invest-ment in Nasdaq-100 index tracking stock, investors can participate in the collective performance of many of the Nasdaq stocks that are often in the news or have become household names. Similarly, S&amp;P CNX NIFTY is a well-diversified 50 stock index accounting for 25 sectors of the economy. It is used for a variety of purposes such as benchmarking fund portfolios, index-based derivatives and index funds. The CNX Indices are computed using the market capitalizatio n weighted method, wherein the level of the Index reflects the total market value of all the stocks in the index relative to a particular base period. The method also takes into account constituent changes in the index and importantly corporate actions such as stock splits, rights, and so on, without affecting the index value. 3.2 Experimental Setup and Results In this experiment, we considered 7-year stock data for the Nasdaq-100 Index and 4-year for the NIFTY index. Our res earch investigates the performance of GGGP-FNT, GP and ANN for modeling the Nasdaq-100 and NIFTY stock mar-ket indices [13]. We used the same training and test data sets to evaluate the different models. The assessment of the prediction performance of the different paradigms were done by quantifying the prediction obtained on an independent data set. The Root Mean Squared Error ( RMSE) is used to performance evalu-ation index.

The settings for GGGP-FNT are populatio n size 100, cross rate 0.9, mute rate 0.1 and maximum depth 5. A FNT model was constructed using the training data and then the model was used on the test data set. The instruction sets used +
The grammars used for modeling the Nasdaq-100 index (left) and for model-ing the NIFTY index (right) are shown as follow,
For comparison purpose, a GGGP was also implemented to forecast the stock index. The settings for GGGP are populat ion size 100, cross rate 0.9, mute rate 0.1, and maximum depth 15. The instruction sets S = { + ,  X  ,  X  ,sin,cos,exp, x ,x 2 ,x 3 } and S = { + ,  X  ,  X  ,sin,cos,exp,x 1 ,x 2 ,x 3 ,x 4 ,x 5 } are used for modeling the Nasdaq-100 index and the NIFTY index, respectively. Training was terminated after 3000 epochs on each dataset.
 Two ANN models with architecture { 3  X  10  X  1 } and { 5  X  10  X  1 } trained by PSO are also implemented for modeling the Nasdaq-100 index and the NIFTY index, respectively. Training was terminated after 3000 epochs on each dataset.
Table 1 summarizes the training and test results achieved for the two stock indices using the three different approaches. Figures 3 and 4 depict the test results for the one-day ahead prediction of the Nasdaq-100 index and the NIFTY index, respectively.

Comparing GGGP-FNT with GGGP and ANN, we found that GGGP-FNT has better generalization ability and high accuracy than GGGP and ANN fore-casting models. In this paper, a GGGP and PSO based learning algorithms are employed to optimal design of the FNT models. Simulation results on stock index forecasting problems show the feasibility and effectiveness of the proposed method. For the GGGP algorithm itself, the vital topic is a Context-free Grammar model (CFG). The gammer and its self-turning should be further discussed in our future work. It should be noted that other grammar model can also be used to guide the GP and used to design of FNT, and therefore it is valuable to give a further investigation.
 This research was partially supported by the Natural Science Foundation of China under contract num ber 60573065 and the Key Subject Research Founda-tion of Shandong Province.

