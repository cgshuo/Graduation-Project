
We present a collection of over 15,000 queries, issued to commercial web search engines, whose answer is a single fact. The collection was produced based on queries landing on questions within a large community question answering web-site, each with a best answer no longer than 3 words and an explicit reference to a Wikipedia page. We describe the col-lection generation process and provide a variety of descrip-tive characteristics, demonstrating the collection X  X  unique-ness compared to existing datasets and its potential use for research of factoid question answering and retrieval.
In recent years, commercial search engines have started to provide inline results to factoid questions, which present a short answer (e.g., a number, a unit, or a few words) di-rectly on the search results page (SERP) [5]. This is part of a broader trend of presenting search results, such as weather, sport scores, or dictionary definitions, directly on the SERP, sparing the user the need to click. Commercial search en-gines put considerable effort to satisfy this kind of need, usually under the  X  X ard X  paradigm, where a dedicated mod-ule is built to respond to a particular type of query [7].
Despite the increasing popularity of such fact retrieval, lit-tle has been published about the technology behind it, and, in particular, little is known about how web search users formulate factoid queries and express  X  X actual X  information needs. Previous research collections synthesized queries by making assumptions on the actual query distribution; most noticeably, these queries were generally thought of as start-ing with one of the WH-question words 1 [3, 4, 6]. Yet, as we will later show, this common perception is inaccurate, as many factual queries  X  X n the wild X  do not match this pattern. In addition, synthesis of the queries, however accurate, gives no information about their distribution. Without click logs, there is no way to determine which facts are more popu-lar, and what phrasing is more commonly used to query for them. Who, where, why, when, how, what, and which.

In this paper, we present the FactQueries collection 2 : a collection of 15,557 queries that landed on factoid question pages within the Yahoo Answers Community Questions An-swering (CQA) website. Factoid questions were identified by considering questions with best answers that include no more than 3 words and an explicit link within the  X  X ource X  field to English Wikipedia. The factuality of the questions and the correctness of the corresponding answers were vali-dated by manual judgment. We then fetched, for each fac-toid question, up to 5 queries from commercial web search engines that landed on the question X  X  page. Each entry in the collection includes a query, the corresponding question X  X  title and best answer, an integer indicating the query X  X  land-ing frequency on the question X  X  page, the answer X  X  source field, and a link to the question X  X  page on Yahoo Answers.
The FactQueries collection reflects queries by web search users that are formulated to retrieve factual answers. To the best of our knowledge, it is the first to include web search queries that express factual intent and are not restricted to predefined patterns. The collection offers the following contributions:  X  It includes queries from commercial web search engines, exactly as they were generated by users. Over 70% of the questions have multiple landing queries, demonstrating the diverse ways users may query for the same fact.  X  It includes an indication of the frequency of each query.  X  Nearly 60% of the queries are not phrased as WH-questions, reflecting the difference between query language and ques-tion language.  X  As opposed to other datasets, the generation process of
FactQueries did not apply any restrictions on the ques-tion or query, but rather on the answer. It therefore reflects a broader set of topics and questions for facts web users are interested in, which span the diverse cate-gories of Yahoo Answers, from Science and Mathematics to Music and Entertainment.  X  While the collection includes a link to the knowledge source (Wikipedia), it is not restricted to facts that map to a knowledge graph structure [3, 4]. On the other hand, the collection X  X  facts are not as complex as trivia ques-tions that deliberately combine multiple facts [8], as these are not typically pursued by web search users.  X  The correctness of the answer is validated both by the Ya-hoo Answers  X  X est answer X  mechanism and in-house an-notators, reducing the likelihood of wrong answers, such as recently reported for existing factoid datasets [2]. http://webscope.sandbox.yahoo.com/catalog.php? datatype=l&amp;did=76 The collection was generated based on questions from the Yahoo Answers (YA) CQA website, which satisfy the fol-lowing two preconditions: 1. The text of the question X  X   X  X est answer X  contains be-2. The  X  X ource X  field of the best answer includes a link to
We conjectured that these two stipulations would yield a subset of answers likely to be factual. Applying these two preconditions on the entire set of Yahoo Answers non-deleted English questions, posted between 2006 and 2014, produced a set of over 31,000 different questions. We ap-plied additional rule-based filtering to detect answer text that does not contain the actual answer, including reference pointing (e.g., answers that contain  X  X ry X ,  X  X heck X ,  X  X o to X ,  X  X ikipedia X ,  X  X ttp:// X ,  X  X isit X ,  X  X ope this helps X , or  X  X here you go X ), multiple choice selection (e.g.,  X  X es X ,  X  X ope X , or starting with  X 2. X ,  X  X   X , or  X 3) X ), and expression of doubt (e.g.,  X  X hink X ,  X  X elieve X ,  X  X aybe X ,  X  X robably X ,  X  X  X ). This fil-tering left us with nearly 19,000 questions. For 7,500 of these questions, we had a non-empty set of  X  X anding queries X , i.e., queries from commercial web search engines that resulted in a click on their YA page. The landing queries we inspected were issued between 2012 and 2014.

We sent this set of 7,500 questions to manual judgment by 25 professional in-house annotators ( X  X ditors X ). Each ques-tion was evaluated by a single editor. The editors were given the question X  X  title, its (up-to-3-words) best answer, the link to the question X  X  page, and the link(s) provided in the source field of the best answer. They were asked the following ques-tions:  X  Q1: Does the entry describe a question that has ex-actly one factual answer, which can be expressed in a few words?  X  Q2: If Q1 is affirmative, is the provided answer correct?
In addition, the editors were given the following notes with regards to Q1:  X  We seek facts, not opinions. So for a question like  X  X hat is the best book series by J.K. Rowling? X , Q1 should be negative.  X  If the answer requires a longer explanation than a few words, Q1 should be negative.  X  For classic Yes/No questions, Q1 should be negative (e.g.,  X  X s Sidney the capital of Australia? X ).  X  We are not looking for lists. If there is more than one answer, Q1 should be negative (e.g.,  X  X hat is the cast of Star Wars? X  or  X  X ho are Barack Obama X  X  Children? X ). For determining the answer X  X  correctness (Q2), editors were instructed to use Wikipedia and, if needed, a web search. Overall, for 79 . 7% of the 7,500 questions given to editors, Q1 was affirmative, i.e., they were indicated to contain a factual question (19 . 8% negative, 0 . 5%  X  X on X  X  know X ). This high portion indicates that our methodology for extracting factual questions is effective, to the degree of achieving al-most 80% precision without human interference. Out of the questions indicated to be factual, for 86 . 4% the answer was
Table 2: Example questions marked factual but incorrect. indicated to be correct (Q2 affirmative). Table 1 lists a few examples for question titles marked non-factual (Q1 nega-tive), while table 2 lists a few examples for question titles marked as factual, but with incorrect answers (Q1 affirma-tive and Q2 negative). For the collection, we only considered questions for which both Q1 and Q2 were affirmative (68 . 9% of the original set of questions).

Figure 1 shows the portion of questions marked as factual (Q1 affirmative) and answers marked as correct (Q1 and Q2 affirmative) according to the answer length in words. The portion of factual answers decreases from 82 . 4% for 1-word and 81 . 1% for 2-word answers, to 74% for 3-word answers, indicating that as answers become longer, they are less likely to be factual.

After selecting our subset of questions based on the feed-back from annotators, we sampled a list of up to 5 landing queries per question. We filtered out a small subset of the queries, such as queries that included  X  X ahoo answers X  or very long queries. This left us with a set of 15,557 queries, referring to 4,691 distinct YA questions. Each query is rep-resented as a tab-separated line in the collection, with the following fields: 1. Query . The query X  X  text as logged by the search engine. 2. Occurrence Score ( OcSc ) . An integer between 1 and 3. Question Title . The title of the question in YA. 4. Answer . The best answer for the question (1-3 words). 5. Source . The source field for the answer, which includes 6. URL . The URL of the Yahoo Answers page. In addition
Generated by manual bucketing, to obscure sensitive data. Figure 1: Percentage of questions marked factual and an-swers marked correct by answer length.
It should be noted that the matching of the landing queries to their corresponding questions was not manually verified. We generally observed a good match across the collection, but the OcSc can be used for filtering to queries with more  X  X anding evidence X .

Table 3 includes a few examples from the collection. The examples demonstrate differently-phrased queries that landed on the same YA page for three different questions.
In this section, we describe the characteristics of the col-lection. We first focus on queries and then discuss questions.
The average length of a query in the collection is 6 . 6 words (stdev: 3 . 5, median: 6, max: 53). Figure 2 shows the de-tailed distribution. The portions of 1-word queries (0 . 02%) and 2-word queries (3 . 33%) are very low, whereas 11 . 4% of the queries contain more than 10 words. This distribution of query length is in accordance with previous findings re-ported for queries that landed on CQA websites [9].
Table 4 shows the distribution of queries per question in the collection. As can be seen, 28 . 4% of the questions have only one landing query, while almost half have 5 queries, which is the maximum number allowed for this collection.
The next analysis focuses on the portion of question queries [11]. The  X  X ueries X  row of Table 5 shows the distribution of queries starting with a WH-word. The most popular WH-word, by a large margin, is  X  X hat X , opening over 20% of all queries in the collection, followed by  X  X ho X  and  X  X ow X  (half of the latter start with  X  X ow many X ). On the other hand, not even one query starts with  X  X hy X , which typi-cally represents broader, open-ended types of questions [10]. Table 5: Distribution of queries and questions starting with WH-words. As a sanity check to the collection generation process, we also inspected yes/no queries. Overall, very few queries follow a yes/no pattern [11]: no queries start with  X  X as X ,  X  X id X ,  X  X oes X ,  X  X re X ,  X  X o X ,  X  X ill X ,  X  X ould X ,  X  X hould X ,  X  X an X , or  X  X ould X . Very small portions of the queries start with  X  X s X  (4 in total) and  X  X ere X  (10 queries), with many of the latter resulting from misspelling of the more common  X  X here X . In total, it can be seen that only slightly over 40% of the queries are question queries, leaving nearly 60% of the queries not phrased as classic questions. Considering queries that con-tain a WH-word anywhere in the text, although these are often not WH-questions (e.g.,  X  X erson who studies music X ), the portion grows to 47 . 6%. Other common words to open a query in the collection are  X  X he X  (4.4% of the queries),  X  X  X  (1.6%),  X  X ame X  (0.8%),  X  X n X  (0.7%),  X  X ovie X  (0.7%), and  X  X irst X  (0.4%).
 As mentioned in Section 2, the occurrence score, marked OcSc , represents the frequency of the query in our inspected log, and ranges from 1 to 10. Table 6 shows the distribu-tion of the OcSc across all queries ( X  X ll X  row). The majority of the queries (62 . 9%) have an OcSc of 1, while lower por-tions have higher OcSc , down to 1 . 6% for OcSc of 10. For questions with 1 query, as can be seen in the respective row of Table 6, the OcSc is typically lower, while for questions with 5 queries, the OcSc is higher across all queries. The two lowest rows of Table 6 show the OcSc distribution of the 1st query (the one with highest OcSc ) and the 5th query (lowest OcSc ) for questions with 5 landing queries. It can be seen that the OcSc for the 1st query is distributed roughly evenly across the 10 buckets, while the OcSc score of the 5th query is all but minimal.
The average question title length is 10 words (stdev: 4 . 2, median: 9, max: 24). The full distribution is shown in Fig-ure 2, as compared to the query length distribution. The cor-relation between the question length and the average length of its landing queries is positive, but not extremely high (r=0 . 36, p &lt; 0 . 001). As can be seen in Table 5 (bottom row), a substantially higher portion of the questions (over 70%) start with a WH-word, with  X  X hat X  alone opening 42 . 9% of the questions, and  X  X hy X  opening as few as 4 questions.
As previously mentioned, the source field of the best an-swer is provided as part of the collection. While this field usually contains a single URL, the user interface does not strictly impose it and in some cases the field may include multiple URLs and even some free-form text. In rare cases (2 . 9% of the questions), the field is null due to various pro-cessing issues. For the rest of the questions (4,555 in total), 95 . 5% have exactly one URL in their source field (linking to English Wikipedia), while 4 . 5% have multiple URLs (up to 4, except one case with 7), to a total of 455 URLs. Of these, 67% link to English Wikipedia, while the rest link to a variety of websites (114 in total), with imdb.com (14 URLs) and youtube.com (12 URLs) the most popular. Inspect-ing all the URLs to English Wikipedia within the source field, including the cases where the field contains multiple URLs (a total of 4,651 URLs), we see that they cover 4,391 different Wikipedia values. The vast majority of these val-ues (95%) appear exactly once in FactQueries , 4 . 2% appear twice, 0 . 7% appear 3 times, and 0 . 1% (5 values) appear 4 times in the collection: Adenosine triphosphate, Cellular res-piration, DNA, List of Bleach episodes, and Generation Y.
Table 7 shows several statistics of question metadata not directly included in the collection, including the number of answers aside from the best answer, the length of the ques-tion X  X  description (body), and the number of upvotes and downvotes the best answer received.

Questions on Yahoo Answers are assigned to categories, in a 3-level taxonomy with over 1500 nodes [1]. The col-lection X  X  questions span all 26 top-level categories of Yahoo Answers. Table 8 shows the 10 most common top-level cat-egories, alongside the corresponding portion of questions in the collection. The two most common categories, jointly covering over 50% of the questions, are Entertainment &amp; Music and Science &amp; Mathematics. The top-level categories with the smallest portions in the collection are Pregnancy &amp; Parenting (8 questions), Dining Out (4), and Local Busi-nesses (3).
 Since some categories are more frequent than others on YA, it is also interesting to inspect the frequency of the cat-egory in the collection relative to its general YA frequency. The  X  X req-Ratio X  column of Table 8 indicates the ratio be-tween the portion of category questions in FactQueries and on YA in general (considering over 150M non-deleted En-glish questions with a best answer). A ratio greater than 1 indicates higher frequency of the category across Fac-tQueries  X  questions than across all YA X  X  questions. The categories with highest ratio are Science &amp; Mathematics (3 . 98), Arts &amp; Humanities (2 . 44), and Entertainment &amp; Mu-sic (2 . 23). The three categories with the lowest ratio (not shown in the table) are Beauty &amp; Style (0 . 12), Pregnancy &amp; Parenting (0 . 05), and Family &amp; Relationships (0 . 02).
Thus far, we inspected the top-level categories in the YA taxonomy. To gain a more fine-grained sense of the col-lection X  X  topics, we also examine the specific categories as-signed to questions (any category in the taxonomy, at any level, can be assigned to a question). Table 9 shows the 15 most common categories within FactQueries . The most popular categories are Movies, Biology, Celebrities, History, Chemistry, and Comics, reflecting the mix of science and en-tertainment. The categories with the highest freq-ratio are Geography, Trivia, Comedy, and Biology. The categories with the lowest freq-ratio among the 30 most common YA categories are Polls &amp; Surveys, Hair, Diet &amp; Fitness, Preg-nancy, Singles &amp; Dating, Friends, and Marriage &amp; Divorce (the last two without any associated questions in the collec-tion).
The FactQueries collection provides researchers and prac-titioners with over 15,000 queries used by web search users to find answers to a variety of factual questions. The fac-tual nature of the questions and the correctness of the an-swers were validated in a large editorial effort. The collection strives to help the community better understand the domain of factoid queries, which plays an interesting role in today X  X  web search.
