 In order to assess the trustworthiness of information on social media, a consumer needs to understand where this information comes from, and which processes were involved in its creation. The entities, agents and activities involved in the creation of a piece of information are referred to as its provenance, which was standardized by W3C PROV. However, current social media APIs cannot always capture the full lineage of every message, leaving the consumer with incomplete or missing provenance, which is crucial for judging the trust it carries. Therefore in this paper, we propose an approach to reconstruct the provenance of messages on social media on multiple levels. To obtain a fine-grained level of provenance, we use an approach from prior work to reconstruct information cascades with high certainty, and map them to PROV using the PROV-SAID extension for social media. To obtain a coarse-grained level of provenance, we adapt our similarity-based, fuzzy provenance reconstruction approach  X  previously applied on news. We illustrate the power of the combination by providing the reconstructed provenance of a limited social media dataset gathered during the 2012 Olympics, for which we were able to reconstruct a significant amount of previously unidentified connections.
Nowadays, information from social media is frequently analysed and processed for professional use. Examples include online jour-nalism, rumor detection, and viral marketing [10]. In all these cases, it is important for the consumer to know the level of trust and relevance that the information carries. An important step in the process of determining trust of information is to expose its prove-nance [4]. To model provenance for information diffusion on social media, we specified PROV-SAID [14], an extension to the W3C PROV model. Using this model, the social and influence graphs can be represented in an interoperable way. However, automatically reconstructing the aforementioned graphs based on the APIs that most social media provide poses a challenge. Most current methods are designed to only model direct, high certainty influence edges, caused by explicit re-emission of messages (e.g., retweets) and combined with connections between users (social graph), in order to unveil who was influenced by whom. This approach does not consider the potentially large amount of inexplicit influences that are less certain, and thus more difficult to detect automatically (e.g., a user adapting another user X  X  message, without explicitly referring to it). In this case, the provenance must be reconstructed somehow, unravelling the unobserved references that users are using but not giving credit to, and revealing their influencers.

In this paper, we combine a fine-grained, high-certainty ap-proach, with a coarse-grained, less certain approach for provenance reconstruction. By doing this, we propose a multi-level approach for provenance reconstruction of information diffusion on social media. Our contributions in this paper are: 1) an approach for creation and integration of multi-level provenance; 2) a real-world application and evaluation of the PROV-SAID model; 3) a mapping in order to convert input data from social media into RDF; 4) a novel application of our previous work on similarity-based prove-nance reconstruction in the context of social media.
While information diffusion in social media has received a lot of attention, in particular its modeling [7], there is limited work on the reverse procedure, i.e. information provenance , which is the focus of this paper. We divide the state-of-the-art in this area in the following categories: (i) provenance through content similarity; (ii) provenance through social graph connections; (iii) provenance through user profile metadata. (i) The work of [11] focuses on tracing news and quotes (referred to as memes ) on the Web over time. The focus is on temporal patterns, mutations (alterations) that online phrases undergo and properties of the news X  life cycle. A subsequent work using the same datasets and methods [13] shifts the focus on fine-grained content alterations. In [3], we reconstructed provenance of news articles automatically using semantic similarity. In this paper, we adapt this approach for social media and the PROV-SAID model. (ii) Traditional information diffusion research includes tracing a piece of information back to its sources through social con-nections, revealing the concepts of influence and trust among the users involved. The work of [6] recovers information recipients sub-graphs given a small fraction of known recipients. In [8] unknown recipients are identified under the assumptions of degree and closeness propensity: nodes with a higher degree and closer to the sources are more likely to propagate information. [2] provides a provenance reconstruction method through social connections based on well established information diffusion models. Finally, in [15], we automatically reconstruct information cascades that show which paths information took, given a piece of information that propagates over a social graph. Information cascades are graphs that model how information is being diffused from user to user; in other words, our approach in [15] reconstructs the paths of users who propagate information back to the sources by finding intermediate influencers. (iii) Lastly, provenance can be derived through user profile metadata, attributing relevance and trust to the information emit-ted according to the characteristics of the contributor. The work of [9] implements a tool for collecting such user information from different media sites, while not providing any information on the provenance paths and sources.

The work in this paper combines concepts from (i) and (ii) in order to reveal provenance paths, by extending and adapting the solutions proposed in [15] and [3]. Finally, the results are modeled and combined in an interoperable way using the PROV-SAID model [14], which extends the W3C PROV model [12]. PROV-SAID provides a rich description of provenance with regard to information diffusion concepts such as: direct and indirect derivations , copied and modified messages , and influence types such as follow relationships and interaction influences .
As highlighted in the related work and illustrated in Figure 1, we reveal provenance paths on two levels: (1) low-level (fine-grained) , based on structure as in [15] and (2) high-level (coarse-grained) , based on content similarity as in [3]. These methods are then combined using PROV-SAID [14]. In order to convert the XML-based influence graph of [15] into PROV-SAID, we use the RML mapping language [5]. RML is used in combination with a processor to convert proprietary data  X  such as XML  X  to RDF. In our case the data is converted to PROV-O, which is the ontology that expresses the PROV Data Model. Note that by using RML, we ensure that any input can be converted to PROV-O, rendering our method interoperable and reusable in many applications.
To obtain low-level provenance, we build upon on our previous work [15], that reconstructs the so-called information cascades found in social media. Diffusion paths are reconstructed according to who is influenced by whom given messages that propagate over a social graph, with the assumption that users propagate identical messages (e.g., by retweeting) and identify possible influencers. When applied to the Twitter dataset described in Section 4, the reconstructed information cascades comprise of retweets, where users give credit only to the initial source of a message, not the intermediate source that exposes the message to them. In other words, it remains unclear which paths information took from the sources to the recipients. Therefore, the algorithm as described in [15] leverages the social graph in order to reconstruct the in-termediate diffusion paths and find influencers, given the assump-tions that information flows over the social graph and users are influenced by their connections in order to propagate a piece of information.
 The algorithm outputs edges , directed from a tweet A to a tweet B . For each tweet, we have access to the tweet-id , timestamp and userid . When we map this to PROV-SAID using RML, we obtain the following PROV-O sub-graph for each edge: Note that the prefixes status: and user: refer to https:// twitter.com/statuses/ and https://twitter.com/ intent/user?user_id= , respectively, and that the prefixes prov: and prov-said: refer to their respective namespaces. This representation of the information cascades as provenance is now suitable to be merged with other interoperable provenance, such as the high-level provenance described in Section 3.2.
To obtain high-level provenance, we consider what is missing from the dataset generated in Section 3.1. Since the approach in [15] only relies on relationships exposed through a social media API, it does not consider all messages that were copied or revised without this being tracked by the social media software (e.g., when a user copy-pastes a message instead of retweeting it). To recon-struct this kind of information diffusion, we adapt our approach introduced in [3] to be usable with social media content. The core assumption of this approach is:  X  X f two messages are highly similar, there is a high probability that they share some provenance X  . The adapted approach consists of the following steps: 1. remove all tracked copied messages from every information 2. index this reduced dataset using a feature model and seman-3. apply a similarity-based clustering algorithm such as Sim-4. for each cluster:
The expected result of this approach is that the vast majority of messages will be clustered as a singleton, meaning that no new relationships are introduced. Nonetheless, for those messages that do get clustered together, we know that they exhibit a high similarity. We use their temporal information to estimate their provenance relationship, thereby enriching the dataset and expos-ing previously hidden knowledge about the information diffusion. When we integrate this result in the next step, we are effectively re-connecting entire information cascades, whose connection was lost to the social media API. Note that due to the calculation of the full similarity matrix, this approach will have an quadratic complexity w.r.t. the number of messages considered, so it should always be applied on a pre-filtered dataset (e.g., a search result).
Because both algorithms output interoperable PROV, the inte-gration of the two aforementioned levels of provenance consists of simply merging the two sets of RDF statements. However, it is important to understand the new structure this will give to the data. We clarify how the data is enriched by the combination of the two reconstructed provenance sets using Figure 1.
 Each level of provenance differs in precision and granularity. The fine-grained, low-level provenance is very detailed, and was constructed with high certainty, since it consists solely of copied messages exposed by a social media API (in our case: the Twitter API). The coarse-grained, high-level provenance, however, was constructed in a much less certain way, relying on semantic sim-ilarity to reconstruct connections that were lost to the social me-dia API. The two levels enrich each other, providing previously unidentifiable connections between messages for data consumers (e.g., social media analysts) to explore.
As a preliminary evaluation, we tested our approach on a dataset gathered using the Twitter Streaming API during the 2012 Olympics. We chose Twitter because it provides trace information for copied messages (retweets). The dataset was collected by following the keywords  X  X lympics2012 X  and  X  X ondon2012 X . We limited the dataset by only considering tweets with a certain keyword, in our case:  X  X rrest X  . This simulates a realistic scenario where a social media analyst first searches for a broad keyword (e.g., a trend-ing topic), and then investigates the information diffusion paths among the results. Complementary, we desired to avoid messages carrying not important information, for example: "I am watching the Olympics". This way, we include relevant events that attract attention both by individual users and mass media, while yielding information cascades by being retweeted. The final dataset consists of 9047 tweets, of which 5174 are copied messages (retweets), and 3873 are original messages according to the Twitter API.
We identified 31 cascades using the low-level reconstruction approach from Section 3.1, resulting in a skewed distribution from 5 to 1771 recorded retweets with the root tweet contained in the dataset (out of the total of 5174 retweets). This approach has already been thoroughly evaluated in [15], so we can safely assume that the identified cascades are correct.
Using the approach described in Section 3.2, we clustered the 3873 original messages from the dataset based on their semantic similarity. More specifically, we used the TF-IDF approach from traditional information retrieval to model all messages as vectors, and computed their similarity using the cosine similarity. We then executed the SimClus algorithm described in [1]. Essentially, SimClus divides the set of messages into clusters of messages that all exhibit a similarity higher than a predefined threshold to their respective cluster centre. To use the clusters to reconstruct provenance as described in Section 3.2, the major challenge lies in identifying the optimal similarity threshold. The threshold must be high enough to ensure that only messages that actually share provenance get clustered together, while it must also be low enough to avoid that too many messages are clustered as singletons, which would result in missed connections. Ideally, the optimal threshold would be found empirically by analysing the precision and recall of the provenance reconstruction approach, as it was done for news in [3]. For this paper we do not have access to a ground truth as the authors of [3] did. However, we can investigate the influence of the similarity threshold on the number of clusters and their size, which at least gives us an idea of its behaviour.

As illustrated by Figure 2, the total number of clusters is approx-imately proportionate to the similarity threshold. This means that if we use a low threshold, we will have a small number of relatively large clusters. On the other hand, if we use a high threshold, we can expect a high number of smaller clusters. When the threshold is set to 1, only identical messages will be clustered together, and there-fore only retweets  X  no modified tweets  X  missed by the Twitter API will be identified. This is further confirmed by our observations of the number of clusters per cluster size, as illustrated by Figure 3. Here, we see that for the lower thresholds (0.3 and 0.5), the cluster size varies highly, whereas there are less different cluster sizes for the threshold 0.7. These observations are an indication that for the Figure 2: Total number of clusters for each similarity thresh-old.
 Figure 3: Distribution of the number of clusters per cluster size. lower thresholds, many clusters are incorrectly merged, which will affect the precision of the reconstruction. On the other hand, we see that if the threshold is set too high (e.g., 0.9), that the larger clusters are split, resulting in missed provenance relationships  X  and thus affecting the recall. In all cases above 0.3, we see that the number of singletons does not vary significantly, which means that messages that do not belong together will most likely not be clustered together, regardless of the similarity threshold. While it is too early to make a definite decision regarding the optimal threshold without a content-based evaluation, these results lead us to expect that the optimum will be somewhere in the vicinity of 0.7. Using this threshold (0.7), we generated a set of 3094 clusters, and used the 206 non-singletons to reconstruct 879 provenance relationships (32 quotations and 847 revisions).

In other words, when we integrate this high-level provenance with the 31 cascades discovered by the low-level provenance re-construction, we effectively introduce 879 new connections that were previously unidentified. This creates much larger graphs for the consumers of the provenance data to analyse, and provides an enriched view on the information diffusion process. The entire reconstructed provenance graph can be downloaded at http: //semweb.mmlab.be/ns/prov-said/cikm2015.ttl
We proposed a method to reconstruct and integrate provenance on two levels of granularity: low-level through information cas-cades, and high-level through similarity-based clustering. This method augments the provenance of messages on social media, especially when there is external influence not deriving from one single source (in our case: Twitter) or for copied messages that do not give credit to their initial contributors. In these cases, an obvious influencer is not exposed by the social media API. Such messages do not produce large cascades resulting in low-level provenance, but are clustered together in the high-level provenance reconstruction of our approach.

For future work, we will extensively evaluate our approach on diverse datasets and combined data from different social media. Additionally, we will improve our method by applying more suit-able metrics of message similarity for micropost text. [1] M. Al Hasan, S. Salem, and M. J. Zaki. Simclus: an effective [2] G. Barbier, Z. Feng, P. Gundecha, and H. Liu. Provenance [3] T. De Nies, S. Coppens, D. Van Deursen, E. Mannens, and [4] T. De Nies, S. Coppens, R. Verborgh, M. Vander Sande, [5] A. Dimou, M. Vander Sande, P. Colpaert, R. Verborgh, [6] Z. Feng, P. Gundecha, and H. Liu. Recovering information [7] A. Guille, H. Hacid, C. Favre, and D. A. Zighed. Information [8] P. Gundecha, Z. Feng, and H. Liu. Seeking provenance of [9] P. Gundecha, S. Ranganath, Z. Feng, and H. Liu. A tool for [10] J. Leskovec, L. A. Adamic, and B. A. Huberman. The [11] J. Leskovec, L. Backstrom, and J. Kleinberg. Meme-tracking [12] L. Moreau, P. Missier (Eds.), and W3C Provenance Working [13] M. P. Simmons, L. A. Adamic, and E. Adar. Memes Online: [14] I. Taxidou, T. De Nies, R. Verborgh, P. M. Fischer, [15] I. Taxidou and P. M. Fischer. Online analysis of information
