 Humans inevitably develop a sense of the relationships be-tween objects, some of which are based on their appearance. Some pairs of objects might be seen as being alternatives to each other (such as two pairs of jeans), while others may be seen as being complementary (such as a pair of jeans and a matching shirt). This information guides many of the choices that people make, from buying clothes to their inter-actions with each other. We seek here to model this human sense of the relationships between objects based on their ap-pearance. Our approach is not based on fine-grained model-ing of user annotations but rather on capturing the largest dataset possible and developing a scalable method for uncov-ering human notions of the visual relationships within. We cast this as a network inference problem defined on graphs of related images, and provide a large-scale dataset for the training and evaluation of the same. The system we develop is capable of recommending which clothes and accessories will go well together (and which will not), amongst a host of other applications.
We are interested here in uncovering relationships between the appearances of pairs of objects, and particularly in mod-eling the human notion of which objects complement each other and which might be seen as acceptable alternatives. We thus seek to model what is a fundamentally human no-tion of the visual relationship between a pair of objects, rather than merely modeling the visual similarity between them. There has been some interest of late in modeling the visual style of places [ 6, 27 ], and objects [39 ]. We, in con-trast, are not seeking to model the individual appearances of objects, but rather how the appearance of one object might influence the desirable visual attributes of another.
There are a range of situations in which the appearance of an object might have an impact on the desired appearance of another. Questions such as  X  X hich frame goes with this Figure 1: A query image and a matching accessory, pants, and a shirt. picture X ,  X  X here is the lid to this X , and  X  X hich shirt matches these shoes X  (see Figure 1) inherently involve a calculation of more than just visual similarity, but rather a model of the higher-level relationships between objects. The primary commercial application for such technology is in recommend-ing items to a user based on other items they have already showed interest in. Such systems are of considerable eco-nomic value, and are typically built by analysing meta-data, reviews, and previous purchasing patterns. By introducing into these systems the ability to examine the appearance of the objects in question we aim to overcome some of their limitations, including the  X  X old start X  problem [ 28 , 41 ].
The problem we pose inherently requires modeling human visual preferences. In most cases there is no intrinsic con-nection between a pair of objects, only a human notion that they are more suited to each other than are other potential partners. The most common approach to modeling such hu-man notions exploits a set of hand-labeled images created for the task. The labeling effort required means that most such datasets are typically relatively small, although there are a few notable exceptions. A small dataset means that com-plex procedures are required to extract as much information as possible without over-fitting (see [2 , 5, 22 ] for example). It also means that the results are unlikely to be transferable to related problems. Creating a labeled dataset is particu-larly onerous when modeling pairwise distances because the number of annotations required scales with the square of the number of elements.

We propose here instead that one might operate over a much larger dataset, even if it is only tangentially related to the ultimate goal. Thus, rather than devising a process (or budget) for manually annotating images, we instead seek a freely available source of a large amount of data which may be more loosely related to the information we seek. Large-scale databases have been collected from the web (without other annotation) previously [7 , 34 ]. What distinguishes the approach we propose here, however, is the fact that it suc-ceeds despite the indirectness of the connection between the dataset and the quantity we hope to model.
We have developed a dataset suitable for the purposes de-scribed above based on the Amazon web store. The dataset contains over 180 million relationships between a pool of almost 6 million objects. These relationships are a result of visiting Amazon and recording the product recommen-dations that it provides given our (apparent) interest in the subject of a particular web page. The statistics of the dataset are shown in Table 1. An image and a category la-bel are available for each object, as is the set of users who reviewed it. We have made this dataset available for aca-demic use, along with all code used in this paper to ensure that our results are reproducible and extensible. 1 We label this the Styles and Substitutes dataset.

The recorded relationships describe two specific notions of  X  X ompatibility X  that are of interest, namely those of substi-tute and complement goods. Substitute goods are those that can be interchanged (such as one pair of pants for another), while complements are those that might be purchased to-gether (such as a pair of pants and a matching shirt) [ 23]. Specifically, there are 4 categories of relationship represented in the dataset: 1)  X  X sers who viewed X also viewed Y X  (65M edges); 2)  X  X sers who viewed X eventually bought Y X  (7.3M edges); 3)  X  X sers who bought X also bought Y X  (104M edges); and 4)  X  X sers bought X and Y simultaneously X  (3.4M edges). Critically, categories 1 and 2 indicate (up to some noise) that two products may be substitutable, while 3 and 4 indi-cate that two products may be complementary. According to Amazon X  X  own tech report [ 19] the above relationships are collected simply by ranking products according to the cosine similarity of the sets of users who purchased/viewed them.

Note that the dataset does not document users X  prefer-ences for pairs of images, but rather Amazon X  X  estimate of the set of relationships between pairs objects. The human notion of the visual compatibility of these images is only one factor amongst many which give rise to these estimated rela-tionships, and it is not a factor used by Amazon in creating them. We thus do not wish to summarize the Amazon data, but rather to use what it tells us about the images of related products to develop a sense of which objects a human might feel are visually compatible. This is significant because many of the relationships between objects present in the data are not based on their appearance. People co-purchase ham-mers and nails due to their functions, for example, not their appearances. Our hope is that the non-visual decision fac-tors will appear as uniformly distributed noise to a method which considers only appearance, and that the visual deci-sion factors might reinforce each other to overcome the effect of this noise.
The closest systems to what we propose above are content-based recommender systems [ 18 ] which attempt to model each user X  X  preference toward particular types of goods. This is typically achieved by analyzing metadata from the user X  X  previous activities. This is as compared to collaborative rec-ommendation approaches which match the user to profiles generated based on the purchases/behavior of other users have been shown to help address the sparsity of the review http://cseweb.ucsd.edu/~jmcauley/ data available, and the cold-start problem (where new prod-ucts don X  X  have reviews and are thus invisible to the recom-mender system) [28 , 41 ]. The approach we propose here could also help address these problems.

There are a range of services such as Jinni 2 which promise content-based recommendations for TV shows and similar media, but the features they expoit are based on reviews and meta-data (such as cast, director etc.), and their ontology is hand-crafted. The Netflix prize was a well publicized com-petition to build a better personalized video recommender system, but there again no actual image analysis is taking place [17 ]. Hu et al. [ 9] describe a system for identifying a user X  X  style, and then making clothing recommendations, but this is achieved through analysis of  X  X ikes X  rather than visual features.

Content-based image retrieval gives rise to the problem of bridging the  X  X emantic-gap X  [32 ], which requires returning results which have similar semantic content to a search im-age, even when the pixels bear no relationship to each other. It thus bears some similarity to the visual recommenda-tion problem, as both require modeling a human preference which is not satisfied by mere visual similarity. There are a variety of approaches to this problem, many of which seek a set of results which are visually similar to the query and then separately find images depicting objects of the same class as those in the query image; see [ 2, 15 , 22 , 38 ], for ex-ample. Within the Information Retrieval community there has been considerable interest of late in incorporating user data into image retrieval systems [ 37 ], for example through browsing [36 ] and click-through behavior [26 ], or by making use of social tags [ 29]. Also worth mentioning with respect to image retrieval is [ 12], which also considered using images crawled from Amazon, albeit for a different task (similar-image search) than the one considered here.

There have been a variety of approaches to modeling hu-man notions of similarity between different types of images [30], forms of music [31 ], or even tweets [ 33], amongst other data types. Beyond measuring similarity, there has also been work on measuring more general notions of compatibility. Murillo et al. [25 ], for instance, analyze photos of groups of people collected from social media to identify which groups might be more likely to socialize with each other, thus im-plying a distance measure between images. This is achieved by estimating which of a manually-specified set of  X  X rban tribes X  each group belongs to, possibly because only 340 im-ages were available.

Yamaguchi et al. [ 40] capture a notion of visual style when parsing clothing, but do so by retrieving visually similar items from a database. This idea was extended by Kiapour et al. [14 ] to identify discriminating characteristics between different styles (hipster vs. goth for example). Di et al. [ 5] also identify aspects of style using a bag-of-words approach and manual annotations.

A few other works that consider visual features specifically for the task of clothing recommendation include [10 , 13 , 20 ]. In [10 ] and [13 ] the authors build methods to parse complete outfits from single images, in [10 ] by building a carefully la-beled dataset of street images annotated by  X  X ashionistas X , and in [13 ] by building algorithms to automatically detect and segment items from clothing images. In [ 13] the au-thors propose an approach to learn relationships between http://jinni.com them. clothing items and events (e.g. birthday parties, funerals) in order to recommend event-appropriate items. Although related to our approach, these methods are designed for the specific task of clothing recommendation, requiring hand-crafted methods and carefully annotated data; in contrast our goal is to build a general-purpose method to understand relationships between objects from large volumes of unla-beled data. Although our setting is perhaps most natural for categories like clothing images, we obtain surprisingly accurate performance when predicting relationships in a va-riety of categories, from recommending outfits to predicting which books will be co-purchased based on their cover art.
In summary, our approach is distinct from the above in that we aim to generalize the idea of a visual distance mea-sure beyond measuring only similarity. Doing so demands a very large amount of training data, and our reluctance for manual annotation necessitates a more opportunistic data collection strategy. The scale of the data, and the fact that we don X  X  have control over its acquisition, demands a suit-ably scalable and robust modeling approach. The novelty in what we propose is thus in the quantity we choose to model, the data we gather to do so, and the method for extracting one from the other.
We label the process we develop for exploiting this data a visual and relational recommender system as we aim to model human visual preferences, and the system might be used to recommend one object on the basis of a user X  X  ap-parent interest in another. The system shares these charac-teristics with more common forms of recommender system, but does so on the basis of the appearance of the object, rather than metadata, reviews, or similar. Our notation is defined in Table 2.

We seek a method for representing the preferences of users for the visual appearance of one object given that of another. A number of suitable models might be devised for this pur-pose, but very few of them will scale to the volume of data available.

For every object in the dataset we calculate an F -dimensio-nal feature vector x  X  R F using a convolutional neural network as described in Section 2.3 . The dataset contains Figure 2: Shifted (and inverted) sigmoid with pa-rameter c = 2 . a set R of relationships where r ij  X  R relates objects i and j . Each relationship is of one of the four classes listed above. Our goal is to learn a parameterized distance trans-form d ( x i , x j ) such that feature vectors { x i , x j that are related ( r ij  X  X  ) are assigned a lower distance than those that are not ( r ij /  X  X  ). Specifically, we seek d (  X  ,  X  ) such that P ( r ij  X  X  ) grows monotonically with  X  d ( x i , x Distances and probabilities: We use a shifted sigmoid function to relate distance to probability thus This is depicted in Figure 2. This decision allows us to cast the problem as logistic regression, which we do for reasons of scalability. Intuitively, if two items i and j have distance d ( x i , x j ) = c , then they have probability 0 . 5 of being related; the probability increases above 0 . 5 for d ( x i , x in advance, but rather c is chosen to maximize prediction accuracy.
 We now describe a set of potential distance functions. Weighted nearest neighbor: Given that different feature dimensions are likely to be more important to different rela-tionships, the simplest method we consider is to learn which feature dimensions are relevant for a particular relationship. We thus fit a distance function of the form where  X  is the Hadamard product.
 Mahalanobis transform: (eq. 2 ) is limited to modeling the visual similarity between objects, albeit with varying emphasis per feature dimension. It is not expressive enough to model subtler notions, such as which pairs of pants and shoes belong to the same  X  X tyle X , despite having different ap-pearances. For this we need to learn how different feature dimensions relate to each other, i.e., how the features of a pair of pants might be transformed to help identify a com-patible pair of shoes.

To identify such a transformation, we relate image fea-tures via a Mahalanobis distance , which essentially general-izes (eq. 2) so that weights are defined at the level of pairs of features. Specifically we fit
A full rank p.s.d. matrix M has too many parameters to fit tractably given the size of the dataset. For example, using features with dimension F = 2 12 , learning a transform as in (eq. 3) requires us to fit approximately 8 million parameters; not only would this be prone to overfitting, it is simply not practical for existing solvers.

To address these issues, and given the fact that M param-eterises a Mahanalobis distance, we approximate M such that M ' YY T where Y is a matrix of dimension F  X  K . We therefore define Note that all distances (as well as their derivatives) can be computed in O ( FK ), which is significant for the scalability of the method. Similar ideas appear in [ 4, 35 ], which also consider the problem of metric learning via low-rank em-beddings, albeit using a different objective than the one we consider here.
In addition to being computationally useful, the low-rank transform in (eq. 4) has a convenient interpretation. Specif-ically, if we consider the K -dimensional vector s i then (eq. 4) can be rewritten as In other words, (eq. 4) yields a low-dimensional embedding of the features x i and x j . We refer to this low-dimensional representation as the product X  X  embedding into  X  X tyle-space X , in the hope that we might identify Y such that related ob-jects fall close to each other despite being visually dissimilar. The notion of  X  X tyle X  is learned automatically by training the model on pairs of objects which Amazon considers to be re-lated.
So far we have developed a model to learn a global notion of which products go together, by learning a notion of  X  X tyle X  such that related products should have similar styles. As an addition to this model we can personalize this notion by learning for each individual user which dimensions of style they consider to be important.

To do so, we shall learn personalized distance functions d
Y ,u ( x i , x j ) that measure the distance between the items i and j according to the user u . We choose the distance function where D ( u ) is a K  X  K diagonal (positive semidefinite) ma-trix. In this way the entry D ( u ) kk indicates the extent to which the user u  X  X ares about X  the k th style dimension.

In practice we fit a U  X  K matrix X such that D ( u ) X uk . Much like the simplification in (eq. 5), the distance d
Y ,u ( x i , x j ) can be conveniently written as In other words, X u is a personalized weighting of the pro-jected style-space dimensions.

The construction in (eq. 6 and 7) only makes sense if there are users associated with each edge in our dataset, which is not true of the four graph types we have presented so far. Thus to study the issue of user personalization we make use of our rating and review data (see Table 1). From this we sample a dataset of triples ( i,j,u ) of products i and j that were both purchased by user u (i.e., u reviewed them both). We describe this further when we outline our experimental protocol in Section 4.1 . Features are calculated from the original images using the Caffe deep learning framework [ 11]. In particular, we used a Caffe reference model 3 with 5 convolutional layers followed by 3 fully-connected layers, which has been pre-trained on 1 . 2 million ImageNet (ILSVRC2010) images. We use the output of FC7, the second fully-connected layer, which re-sults in a feature vector of length F = 4096.
Since we have defined a probability associated with the presence (or absence) of each relationship, we can proceed by maximizing the likelihood of an observed relationship set R . In order to do so we randomly select a negative set Q = { r ij | r ij /  X  X } such that |Q| = |R| and optimize the log likelihood l ( Y ,c |R , Q ) = X Learning then proceeds by optimizing l ( Y ,c |R , Q ) over both Y and c which we achieve by gradient ascent. We use (hy-brid) L-BFGS, a quasi-Newton method for non-linear opti-mization of problems with many variables [21 ]. Likelihood bvlc reference caffenet from caffe.berkeleyvision.org (eq. 8) and derivative computations can be na  X   X vely paral-lelized over all pairs r ij  X  R X  X  . Training on our largest dataset (Amazon books) with a rank K = 100 transform required around one day on a 12 core machine. We compare our model against the following baselines:
We compare against Weighted Nearest Neighbor (WNN) classification, as is described in Section 1.3 . We also compare against a method we label Category Tree (CT) ; CT is based on using Amazon X  X  detailed category tree directly (which we have collected for Clothing data, and use for later exper-iments), which allows us to assess how effective an image-based classification approach could be, if it were perfect. We then compute a matrix of coocurrences between categories from the training data, and label two products ( a,b ) as  X  X e-lated X  if the category of b belongs to one of the top 50% of most commonly linked categories for products of category a . 4 Nearest neighbor results (calculated by optimizing a threshold on the ` 2 distance using the training data) were not significantly better than random, and have been sup-pressed for brevity.
 Comparison against non-visual baselines As a non-visual comparison, we trained topic models on the reviews of each product (i.e., each document d i is the set of reviews of the product i ) and fit weighted nearest neighbor classifiers of the form where  X  i and  X  j are topic vectors derived from the reviews of the products i and j . In other words, we simply adapted our WNN baseline to make use of topic vectors rather than image features. 5 We used a 100-dimensional topic model trained using Vowpal Wabbit [ 8].

However, this baseline proved not to be competitive against the alternatives described above (e.g. only 60% accuracy on our largest dataset,  X  X ooks X ). One explanation may sim-ply be that is is difficult to effectively train topic models at the 1M+ document scale; another explanation is simply that the vast majority of products have few reviews. Not surprisingly, the number of reviews per product follows a power-law, e.g. for Men X  X  Clothing: This issue is in fact exacerbated in our setting, as to predict a relationship between products we require both to have reliable feature representations, which will be true only if both products have several reviews.

Although we believe that predicting such relationships us-ing text is a promising direction of future research (and one
We experimented with several variations on this theme, and this approach yielded the best performance.
We tried the same approach at the word (rather than the topic) level, though this led to slightly worse results. Table 3: Accuracy of link prediction on top-level categories for each edge type with increasing model rank K . Random classification is 50% accurate across all experiments. we are exploring), we simply wish to highlight the fact that there appears to be no  X  X ilver bullet X  to predict such relation-ships using text, primarily due to the  X  X old start X  issue that arises due to the long tail of obscure products with little text associated with them. Indeed, this is a strong argument in favor of building predictors based on visual features, since images are available even for brand new products which are yet to receive even a single review. We split the dataset into its top-level categories (Books, Movies, Music, etc.) and further split the Clothing category into second-level categories (Men X  X , Women X  X , Boys, Girls, etc.). We focus on results from a few representative subcat-egories. Complete code for all experiments and all baselines is available online. 6
For each category, we consider the subset of relationships http://cseweb.ucsd.edu/~jmcauley/ Table 4: Accuracy of link prediction on subcate-gories of  X  X lothing, Shoes, and Jewelry X  with in-creasing rank K . Note that  X  X uy after viewing X  links are not surfaced for clothing data on Amazon. from R that connect products within that category. After generating random samples of non-relationships, we separate R and Q into training, validation, and test sets (80/10/10%, up to a maximum of two million training relationships). Al-though we do not fit hyperparameters (and therefore do not make use of the validation set), we maintain this split in case it proves useful to those wishing to benchmark their algo-rithms on this data. While we did experiment with simple ` regularizers, we found ourselves blessed with a sufficient overabundance of data that overfitting never presented an issue (i.e., the validation error was rarely significantly higher than the training error).

To be completely clear, our protocol consists of the fol-lowing: 1. Each category and graph type forms a single experi-2. Our goal is to distinguish relationships from non-relati-F igure 3: Examples of closely-clustered items in style space (Men X  X  and Women X  X  clothing  X  X lso viewed X  data). 3. We consider all positive relationships and a random 4. All results are reported on the test set.
 Results on a selection of top-level categories are shown in Table 3, with further results for clothing data shown in Table 4. Recall when interpreting these results that the learned model has reference to the object images only. It is thus estimating the existence of a specified form of relationship purely on the basis of appearance.

In every case the proposed method outperforms both the category-based method and weighted nearest neighbor, and F igure 4: A selection of widely separated members of a single K-means cluster, demonstrating an ap-parent stylistic coherence.
 F igure 5: Examples of K-means clusters in style space (Books  X  X lso viewed X  and  X  X lso bought X  data). Although  X  X tyles X  for categories like books are not so readily interpretable as they are for clothes, visual features are nevertheless able to uncover meaning-ful distinctions between different product categories, e.g. the first four rows above above appear to be chil-dren X  X  books, self-help books, romance novels, and graphic novels. the increase from K = 10 to K = 100 uniformly improves performance. Interestingly, the performance on compliments vs. substitutes is approximately the same. The extent to which the K = 100 results improve upon the WNN results may be seen as an indication of the degree to which visual similarity between images fails to capture a more complex human visual notion of which objects might be seen as being substitutes or compliments for each other. This distinction is smallest for  X  X ooks X  and greatest for  X  X lothing Shoes and Jewelery X  as might be expected.

We have no ground truth relating the true human visual preference for pairs of objects, of course, and thus evalu-ate above against our dataset. This has the disadvantage that the dataset contains all of the Amazon recommenda-tions, rather than just those based on decisions made by humans on the basis of object appearance. This means that in addition to documenting the performance of the proposed method, the results may also be taken to indicate the extent to which visual factors impact upon the decisions of Amazon customers. The comparison across categories is particularly interesting. It is to be expected that appearance would be a significant factor in Clothing decisions, but it was not ex-pected that they would be a factor in the purchase of Books. One possible interpretation of this effect might be that cus-tomers have preferences for particular genres of books and that individual genres have characteristic styles of covers.
Finally we evaluate the ability of our model to personalize source t arget F igure 6: Navigating to distant products: each col-umn shows a low-cost path between two objects such that adjacent products in the path are visually con-sistent, even when the end points are not.
 F igure 7: A 2-dimensional embedding of a small sample of Boys clothing images ( X  X lso viewed X  data). co-purchasing recommendations to individual users, that is we examine the effect of the user personalization term in (eqs. 6 and 7). Here we do not use the graphs from Tables 3 and 4, since those are  X  X opulation level X  graphs which are not annotated in terms of the individual users who co-purchased and co-browsed each pair of products. Instead for this task we build a dataset of co-purchases from products that users have reviewed . That is, we build a dataset of tuples of the form ( i,j,u ) for pairs of products i and j that were purchased by user u . We train on users with at least 20 purchases, and randomly sample 50 co-purchases and 50 non-co-purchases from each user in order to build a balanced dataset. Results are shown in Table 5; here we see that the addition of a user personalization term yields a small but significant improve-ment when predicting co-purchases (similar results on other categories withheld for brevity).
Recall that each image is projected into  X  X tyle-space X  by the transformation s i = x i Y , and note that the fact that it is based on pairwise distances alone means that the em-bedding is invariant under isomorphism. That is, applying rotations, translations, or reflections to s i and s j will pre-serve their distance in (eq. 5). In light of these factors we perform k-means clustering on the K dimensional embedded coordinates of the data in order to visualize the effect of the embedding.

Figure 3 shows images whose projections are close to the centers of a set of selected representative clusters for Men X  X  Category method accuracy Men X  X  clothing
Women X  X  clothing Table 5: Performance of our model at predicting copurchases with a user personalization term (eqs. 6 and 7 ). and Women X  X  clothing (using a model trained on the  X  X lso viewed X  graph with K = 100). Naturally items cluster around colors and shapes (e.g. shoes, t-shirts, tank tops, watches, jewelery), but more subtle characterizations exist as well. For instance, leather boots are separated from ugg (that is sheep skin) boots, despite the fact that the visual dif-ferences are subtle. This is presumably because these items are preferred by different sets of Amazon users. Watches cluster into different color profiles, face shapes, and digital versus analogue. Other clusters cross multiple categories, for instance we find clusters of highly-colorful items, items containing love hearts, and items containing animals. Fig-ure 4 shows a set of images which project to locations that span a cluster.

Although performance is admittedly not outstanding for a category such as books, it is somewhat surprising that an accuracy of even 70% can be achieved when predicting book co-purchases. Figure 5 visualizes a few examples of style-space clusters derived from Books data. Here it seems that there is at least some meaningful information in the cover of a book to predict which products might be purchased together X  X hildren X  X  books, self-help books, romance nov-els, and comics (for example) all seem to have characteristic visual features which are identified by our model.

In Figure 6 we show how our model can be used to nav-igate between related items X  X ere we randomly select two items that are unlikely to be co-browsed, and find a low cost path between them as measured by our learned distance measure. Subjectively, the model identifies visually smooth transitions between the source and the target items.
Figure 7 provides a visualization of the embedding of Boys clothing achieved by setting K = 2 (on co-browsing data). Sporting shoes drift smoothly toward slippers and sandals, and underwear drifts gradually toward shirts and coats.
We here demonstrate that the proposed model can be used to generate recommendations that might be useful to a user of a web store. Given a query item (e.g. a product a user is currently browsing, or has just purchased), our goal is to recommend a selection of other items that might comple-ment it. For example, if a user is browsing pants, we might want to recommend a shirt, shoes, or accessories that belong to the same style.

Here, Amazon X  X  rich and detailed category hierarchy can help us. For categories such as women X  X  or men X  X  cloth-F igure 8: Outfits generated by our algorithm (Women X  X  outfits at left; Men X  X  outfits at right). The first column shows a  X  X uery X  item that is randomly selected from the product catalogue. The right three columns match the query item with a top, pants, shoes, and an accessory, (minus whichever category contains the query item). ing, we might define an  X  X utfit X  as a combination of pants, a top, shoes, and an accessory (we do this for the sake of demonstration, though far more complex combinations are possible X  X ur category tree for clothing alone has hundreds of nodes). Then, given a query item our goal is simply to se-lect items from each of these categories that are most likely to be connected based on their visual style.

Specifically, given a query item x q , for each category C (represented as a set of item indices), we generate recom-mendations according to i.e., the minimum distance according to our measure (eq. 4) amongst objects belonging to the desired category. Ex-amples of such recommendations are shown in Figures 1 and 8, with randomly chosen queries from women X  X  and men X  X  clothing. Generally speaking the model produces ap-parently reasonable recommendations, with clothes in each category usually being of a consistent style.
An alternate application of the model is to make assess-ments about outfits (or otherwise combinations of items) that we observe  X  X n the wild X . That is, to the extent that the tastes and preferences of Amazon customers reflect the zeit-geist of society at large, this can be seen as a measurement of whether a candidate outfit is well coordinated visually.
To assess this possibility, we have built two small datasets of real outfits, one consisting of twenty-five outfits worn by the hosts of Top Gear (Jeremy Clarkson, Richard Ham-mond, and James May), and another consisting of seventeen  X  X efore X  and  X  X fter X  pairs of outfits from participants on the television show What Not to Wear (US seasons 9 and 10). For each outfit, we cropped each clothing item from the im-age, and then used Google  X  X  reverse image search to identify images of similar items (examples are shown in Figure 9).
Next we rank outfits according to the average log-likeli-hood of their pairs of components being related using a model trained on Men X  X /Women X  X  co-purchases (we take the average so that there is no bias toward outfits with more or fewer components). All outfits have at least two items. Figure 9 shows the most and least coordinated outfits on Top Gear ; here we find considerable separation between the level of coordination for each presenter; Richard Hammond is typically the least coordinated, James May the most, while Jeremy Clarkson wears a combination of highly coordinated and highly uncoordinated outfits.

A slightly more quantitative evaluation comes from the television show What Not to Wear : here participants receive an  X  X utfit makeover X , hopefully meaning that their made-over outfit is more coordinated than the original. Examples of participants before and after their makeover, along with the change in log likelihood are shown in Figure 10 . Indeed we find that made-over outfits have a higher log likelihood in 12 of the 17 cases we observed ( p ' 7%; log-likelihoods are normalized to correct any potential bias due to the number of components in the outfit). This is an important result, as it provides external (albeit small) validation of the learned model which is independent of our dataset.
We have shown that it is possible to model the human notion of what is visually related by investigation of a suit-ably large dataset, even where that information is somewhat tangentially contained therein. We have also demonstrated that the proposed method is capable of modeling a variety
Our measure of coordination is thus undefined for a subject wearing only a single item, though in general such an outfit would be a poor fashion choice in the opinion of the authors. of visual relationships beyond simple visual similarity. Per-haps what distinguishes our method most is thus its ability to model what makes items complementary . To our knowl-edge this is the first attempt to model human preference for the appearance of one object given that of another in terms of more than just the visual similarity between the two. It is almost certainly the first time that it has been attempted directly and at this scale.

We also proposed visual and relational recommender sys-tems as a potential problem of interest to the information retrieval community, and provided a large dataset for their training and evaluation. In the process we managed to figure out what not to wear, how to judge a book by its cover, and to show that James May is more fashionable than Richard Hammond.
 increase in coordination).
