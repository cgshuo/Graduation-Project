 Query segmentation is an important task toward under-standing queries accurately, which is essential for improv-ing search results. Existing segmentation models either use labeled data to predict the segmentation boundaries, for which the training data is expensive to collect, or employ unsupervised strategy based on a large text corpus, which might be inaccurate because of the lack of relevant infor-mation. In this paper, we propose a probabilistic model to exploit clickthrough data for query segmentation, where the model parameters are estimated via an e ffi cient EM al-gorithm. We further study how to properly interpret the segmentation results and utilize them to improve retrieval accuracy. Specifically, we propose an integrated language model based on the standard bigram language model to ex-ploit the probabilistic structure obtained through query seg-mentation. Experiment results on two datasets show that our segmentation model outperforms existing segmentation models. Furthermore, extensive experiments on a large re-trieval dataset reveals that the results of query segmentation can be leveraged to improve retrieval relevance by using the proposed integrated language model.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Query Segmentation and Language Modeling Algorithms, Performance, Experimentation  X  Work done as a summer intern at Microsoft Research Query Segmentation, Expectation Maximization Algorithm, Language Modeling, QSLM
Since an accurate understanding of the user X  X  query is crit-ical to improving retrieval results, query segmentation is one of the most important tasks in modern information retrieval. For example, accurate query segmentation is the prerequi-site for semantic retrieval models, phrase-based query re-formulation and automatic relevance feedback. Supervised techniques have been used to solve the query segmentation problem in the past [3, 23]. However, they require lots of seg-mentation labels which are expensive to collect. An unsuper-vised approach based on a large text corpus and Wikipedia has been reported to achieve competitive performance [19]; but its accuracy without Wikipedia is still low, partly due to the lack of relevant information about the query structure.
In a modern search engine, there is a large amount of rel-evant data in the form of clickthroughs. Such data reflects users X  implicit preference of documents, and can be lever-aged to infer the underlying segmentation of the queries. In this paper, we propose a unsupervised probabilistic model to exploit user clickthroughs for query segmentation. Model parameters are estimated by an e ffi cient EM algorithm. Seg-mentation results on a standard dataset demonstrate that our model significantly outperforms the EM model in [19] without the use of Wikipedia. Additionally, by combining more data from external resources, such as the Microsoft Web N-gram [22], our model can outperform state-of-the-art baselines.

One of the most important applications for query seg-mentation is to improve the retrieval models by incorporat-ing query segmentation. Most information retrieval tech-niques, such as vector space models and language modeling approaches, rely on the bag-of-words assumption that every query term is independent in the relevance computation. But this assumption is over simplified; users have an order in mind when formulating queries to search for information. One of the reasons why bag-of-words based methods remain popular is because data sparsity makes it harder to estimate models imposing term dependencies [4, 5, 18]. Successful query segmentation has a great potential to lead to better retrieval models that can utilize higher-order term depen-dencies.

However, query segmentation is ambiguous in nature  X  the same query can be segmented in di ff erence ways by di ff erent people. Although several methods for query segmentation have been proposed, surprisingly little research has been per-formed to address the segmentation ambiguity and incorpo-rate this information into retrieval models. In this paper, we propose a query segmentation model that quantifies the un-certainty in segmentation by probabilistically modeling the query and clicked document pairs. We further incorporate the probabilistic query segmentation into a unified language model for information retrieval. Experiments on a large web search dataset from a major co mmercial search engine show that the integrated language model with query segmenta-tion (QSLM) outperforms both the BM25 model and other language models.
Query segmentation models have been well studied in re-cent literature [10, 11, 3, 23, 19, 6]. Initially, the mutual information (MI) between adjacent words in a query is em-ployed to segment queries with a cuto ff [10, 11].The ma-jor drawback of MI based methods is that they are unable to detect multi-word or phrase based dependencies. Com-pared with MI based models, supervised query segmenta-tion approaches can achieve higher accuracies [3, 23]. For example, by considering every boundary between two con-secutive query words as a binary decision variable, Bersgma and Wang [3] trains the weights of a linear decision func-tion with a set of syntactic and shallow semantic features extracted from the labeled data. However, its focus on noun phrase features may not be appropriate for the segmenta-tion of web queries. Furthermore, acquiring training labels demands a great deal of manual e ff ort that may not scale to the web. As another supervised learning approach, Yu and Shi [23] applies conditional random fields to obtain good query segmentation performance. However, it relies on field information features specific to databases, not available for general unstructured web queries. Moreover, the evaluation was conducted only on synthetic data, which is less desirable than real query data.

Tan and Peng [19] introduce a generative model in the unsupervised setting by adopting n-gram frequency counts from a large text corpus and computing the segment scores via expectation maximization (EM). It also utilizes Wikipedia as another term in the minimum description length objec-tive function. Similarly our model in this paper also in-cludes n-gram statistics and applies the EM algorithm to estimate the model parameters. However, we employ click-through query-document pairs to improve segmentation ac-curacy and further refine the retrieval model by utilizing probabilistic query segmentation. Similar probabilistic model is also proposed in [24], but this model focuses in parsing noun phrases thus not generally applicable to web queries.
This work is also closely related to the retrieval mod-els that capture higher order dependencies of query terms. There are several research attempts to incorporate term de-pendency in query or document to retrieval models [12]. For example, some attempts have been made to add proximity heuristics to the vector space model or generative query LM model [13, 20]. However these methods rely on heuristics, which is not a principled way of incorporating term depen-dency. More unified higher-order language models have been studied by Srikanth et al. (Biterm LM) [18]. However, their assumption that every position is dependent is too strong. In fact, the word dependency is stronger within a semantic unit than across the unit, which is what we assume in our work. LM with query syntactics [5] assumes a structure on the query, but they are too complex to estimate accurately. More important, the query syntactic models usually take only the top (most likely) query structure in the modeling process. However, it is more appropriate to assess the prob-ability for all possible segmentation if multiple structures have comparable probabilities to represent the query.
In addition, search log and clickthrough data have been re-ported to be able to improve the performance of personalized search [9, 1, 17]. For example, Joachims [9] first proposes to improve the retrieval quality of search engines by learn-ing from the user clickthrough. Shen et al. [17] propose a decision-theoretic approach to improve search performance via user feedback. And Agichtein et al. [1] demonstrate that web search ranking can be improved by considering user be-havior. We add to this line of work a novel way of exploiting the clickthrough data for query segmentation.
The task of query segmentation is to separate the query words into disjoint segments so that each segment maps to asemanticunit.Givenaquery Q = w 1 ,w 2 ,...,w n of length n ,asegmentation S = S 1 S 2 ...S M of length M is consistent with the query Q if S m = w b m w b m +1 ...w b m +1  X  1 for 1 = b b &lt;...&lt;b M +1 = n +1. Wedefine B = b 1 ,b 2 ,...,b M +1 the segmentation partition, independent of the actual query words, and B n as the set of all possible segmentation par-titions consistent with any query of length n .Therearea total of 2 n  X  1 segmentation partitions in B n .Notethatgiven aquery Q ,thesegmentationpartition B and the query seg-ments S can be uniquely derived from each other.
Because query segmentation is potentially ambiguous, we are interested in assessing the probability of a query segmen-tation under some probability distribution: P ( S |  X  ). With such a probabilistic model, we can then select those seg-mentations with high probabilities and use them to con-struct models for information retrieval. For example, for the query  X  bank of america online banking  X , { [ bank of america ] [ online banking ], 0.502 } , { [ bank of america online banking ], 0.428 } and { [ bank of ][ america online ][ banking ], 0.001 } are all valid segmentations, where brackets [] are used to in-dicate segment boundaries and the number at the end is the probability of that particular segmentation. In this ex-ample, the first two segmentations are likely segmentations with high probabilities, whereas the last one is a rare seg-mentation, as reflected by the low probability. In the next section, we discuss how to compute the probability P ( S | Q ) of a segmentation S given a query Q .
The search log in a modern search engine usually contains alotofuserclickthroughdata,whereuser-issuedqueriesand corresponding clicked documents are recorded. This kind of data contains rich information about users X  preferences for each query. By carefully modeling the clickthroughs, we can assess the likelihood of a segmentation structure according to the collective user behavior. Table 1 shows examples of the clicked documents for two real-world queries from the search log. In these examples, although there are variations in the query words and documents, the sub-sequence  X  X ank of america X  remains intact in all clicked documents. The evidence strongly suggests that  X  bank of america  X  X houldbe asegment. Thisobservationmotivatesustomodelthequery segmentation using the query and clicked document pairs, a previously unexplored idea.
 Table 1: Examples of Query and Clicked Documents
We now propose an unsupervised query segmentation model using user clickthroughs. We first describe the model for generating queries and will later extend it to query-click document pairs. The process of generating a query can be described as follows: 1. Pick a query length n under a length distribution. 2. Select a segmentation partition B  X  B n ,accordingto 3. Generate query segments S m consistent with B ,ac-
Recall that given a query Q of length n ,thequeryseg-ments S and the segmentation partition B can be derived from each other. Thus, we can compute the probability of a segmentation as: where P ( Q | B,  X  )istheprobabilityofgeneratingaquery Q given segmentation partition B .The P ( B | n )canbeesti-mated by an expectation maximization algorithm described in the following section. However, in this paper we set P ( B | n )toaparticularformbyimposinganinfinitestrong prior that penalizes longer segments: where | S m ( B ) | is the length of the m t h segment specified by B ,and f is a factor controlling the segment length penalty. Note that the denominator is constant for a fixed length n . Since the probability of a segmentation is the product of all segment probabilities P ( S m |  X  )and P ( B | n,  X  ), such a seg-ment length penalty is crucial to counter the bias for longer segments as they result in fewer segments and hence fewer terms in the final product. This need for segment length penalty is also discussed by Peng et. al in [14].
To extend the model to observed pairs of the query Q and the clicked document D ,weconsider Q to be generated from an interpolated model, consisting of the global component P ( S m |  X  )andadocument-specificcomponent P ( S m |  X  D ically, we redefine the query probability given a segmenta-tion partition in Equation (2) as: Mathematically, this is equivalent to generating each query segment using a log-linear interpolation of the global and document-specific models. Figure 1 illustrates the segmen-tation partition and the process of generating a query given the model. Figure 1: The Generative Model of Segmentation.
 Left: the query segmentation partition; middle: the process of generating a query Q ;right:theprocess of generating query Q with clicked document D
For P ( S m |  X  D ), we employ a smoothed bigram language model trained from the document D and interpolated with the global document collection statistics  X  C to model the where  X  is the mixture weight,  X  C and  X  D are the bigram backo ff weights, and f w l , f w l  X  1 w l are the n-gram counts in document D or corpus C .

Overall, we want to estimate  X   X  to maximize the log like-lihood of observing all the query-clicked document pairs in the dataset: With  X   X  ,wecancomputethemostprobablesegmentations for any query according to Equation (1).
Since the joint probability in Equation (7) involves the logarithm of a summation over hidden variables B ,thereis no exact analytical solution for  X   X  .However,wecanapply the expectation maximization (EM) algorithm to maximize the joint probability of all observed data. In the E-step, we evaluate the posterior probability of a valid segmentation of Q given the previous model parameter estimate  X  ( k  X  1) : where In the M-step, we update the estimate of  X  according to: P ( w 1 ...w r |  X  ( k ) )= 1 where Z is the normalization factor and  X  () is an indicator function checking if segment S m is equal to n-gram w 1 ...w For a query of n keywords, a naive computation for the M-step requires summing of all 2 n  X  1 possible segmentations, which is computationally impractical for longer queries. For-tunately, it can be computed e ffi ciently using the Baum-Welch algorithm [2].

Here we introduce a graph representation for query seg-mentation. Given a query Q of length n ,allsegmentations consistent with Q can be represented by a graph G with n +1nodes. Figure2illustratesagraphrepresentationfor two valid segmentations of the query  X  bank of america on-line banking  X . For a graph with n +1nodes, thereare a total of 2 n  X  1 ways to connect node 1 to node n +1, each corresponding to a valid segmentation. For example, in the upper panel of Figure 2, there is a connection from node 1to4,correspondingtoasegmentationboundarybetween america and online .Inthiscase,thearcfromnode1to4 corresponds to the segment  X  X ank of america X . Figure 2: Graph Representation of Segmentations
Using this graph representation, Equation (9) in the M-step can be rewritten as: where  X  ( i, j )= P ( S i  X  j | Q,  X  ( k  X  1) )istheprobabilityofthe segment S m represented by the arc from node i to node j for the query Q ,  X  ( S i  X  j = w 1 ...w r )isanindicatorfunction with a value of 1 when S i  X  j = w 1 ...w r and 0 otherwise.
In order to compute  X  l ( i, j ), we introduce Q from the beginning of the graph to node i ,and  X  ( j )= P ( Q | j,  X  ( t  X  1) ), the probability of observing Q from node j to the end of the graph:  X  ( j )=  X  ( i, j )=  X  l ( i )  X   X  l ( j )  X  P ( S i  X  j |  X  ( t  X  1) with the initial condition  X  l (1) = 1 ,  X  l ( n )=1. Algorithm1 summarizes the steps for estimating  X  .Forasetofqueries with equal length, the computation complexity for each it-eration is O ( Ln 2 ), where L is the number of input query-document pairs and n is the number of words in each query. Once the optimal  X  is obtained, the probability of a segmen-tation P ( S | Q,  X  ,  X  )canbecomputedbyEquation(1).
N-gram statistics from a very large scale of text resources can also be utilized to improve query segmentation. In fact in [19], the biggest improvement in segmentation accuracy is achieved by utilizing information from Wikipedia. In addi-tion, [6] also reports a well-performing naive query segmen-tation method using Google Web N-gram. Here we propose asimpleapproachutilizingtheMicrosoftWebN-gramser-vice. MS Web N-gram is essentially a distribution of n-gram probability  X  # over the web. The probability of a segmenta-
Algorithm 1 :N-gramconceptprobabilityestimation input :Asetofquery-clickeddocumentpairs output :Optimalestimateof  X  = { P ( S m ) } for t  X  1 to T do return  X  = { P ( S m ) } ; tion given Q is defined as:
Furthermore, we can combine our query segmentation model with clickthrough and the simple model with Web N-gram into an interpolated model: we find the setting of  X  =0 . 5 ,f =2 . 0 ,f # =2 . 0resultsina model with good segmentation accuracy.
In this section we report the query segmentation results obtained by our model and other baselines on two datasets. One is from a standard dataset established by previous re-search, and the other is constructed by ourselves. We also conduct extensive analysis on several aspects of the results.
We use two sets of queries for evaluating the query seg-mentation models. The first set (Set 1) is a standard query segmentation dataset established by Bergsma and Wang [3], which is also applied in [19]. In this dataset, annotator A, B, and C independently segmented 500 queries which are sampled from the AOL 2006 query log. Among these 500 human queries, the 220 where the 3 judges agree are called the  X  X ntersection X  set.
 The above segmentation dataset is focused on noun queries. But in this work we are also interested in web queries. There-fore we prepare another set of 1,000 queries sampled from the search log of a major commercial search engine, which we name the 1000-query dataset. We invite three domain ex-perts to segment the queries independently, employing the same evaluation metrics as Set 1. Besides expert annota-tions, this dataset also has clickthrough information and relevance judgments for the top documents, which is used by subsequent experiments when comparing retrieval mod-els.

To measure the segmentation e ff ectiveness, we report re-sults on three evaluation metrics. (1) Query accuracy: the percentage of queries for which the predicted segmentation matches the gold standard completely; (2) Classification ac-curacy: the ratio of correctly predicted boundaries in be-tween every two consecutive words; (3) Segment accuracy: how well the predicted segments match the gold standard under the information retrieval measures of precision, re-call, and F-score.

As baseline, we include the three models in [19]: Mutual information, EM + corpus (query log), and EM + corpus +Wikipedia.WealsoincludeamethodusingGoogleWeb N-gram [6] and a simple model with MS Web N-gram, as defined in Section 4.2. Our model + clickthrough and our model + clickthrough + MS Web N-gram are included in the comparison. The parameters of our segmentation model is trained on a large set of search log containing about 20 millions query-clicked document pairs.
Table 2 shows the results of our model as well as the baseline models on the standard dataset. Columns 3 to 5 represent models without using external data source (ba-sic models), while columns 6 to 9 are models utilizing large external sources, such as Wikipedia and web-scale n-gram (extended models). Among the basic models, our model performs the best according to annotator A, C and the in-tersection of these annotators. These results are significantly better than the corresponding results by the EM + corpus model in [19]. For the result based on annotator B, our model is comparable to that of [19] (0.571 vs 0.573 on seg-ment F score). For the extended models, simple model + MS Web N-gram performs well, similar to the results for sim-ple model with Google Web N-gram as reported in [6]. It indicates the positive impact of n-gram statistics on query segmentation. However, our model, as well as EM model +Wikipediain[19]outperformsthesimplemodelsconsis-tently in all annotators X  judgments; and our extended model performs better than that of [19]. For example, in the inter-section judgments, the F score of our model is 0.779, while model in [19] is 0.774. Compared to the simple model + MS Web N-gram, whose intersection F score is 0.728, our model achieves a 7.0% gain on the same measure. It suggests the e ff ectiveness of our model and the benefit from combining additional large scale N-gram statistics.
We compare our query segmentation model with the sim-ple model + MS Web N-gram on the 1000-query dataset. Table 3 shows the segmentation results on the this set. Al-though the simple segmentation model with web n-gram works very well in the standard dataset, it performs infe-rior to our model in the 1000-query dataset. In 2 out of 3annotatorjudgments,ourmodeloutperformsthesimple model. And in the intersection judgments our model also works better than the simple model by 4.6%. Since this dataset is sampled from a set of web search queries, re-sults in this experiment indicate that our model fits web search queries, whose characteristics are di ff erent from noun queries, better.
The factor f in Equation (4), which controls how much penalty is given to a segment of length | S m | ,isimportantto the our proposed model. We now investigate how the seg-mentation result changes according to di ff erent values of f . For this purpose, we re-run our model (without web n-gram) on the standard dataset with f ranging from 1.5 to 3.0 in steps of 0.25. Figure 3 summarizes the results. There are common trends across annotator A, B, C and their intersec-tion. The F score increases when f increases from 1.5 to 2.0, and decreases afterwards. It suggests that too little penalty (small f )favorslongsegmentsandhurtssegmentationaccu-racy, while too much penalty (big f )negativelyimpactson the results since it favors segments with very short length. It also indicates that a moderate penalty at f =2 . 0isa reasonable choice for the proposed model.
In this section we will introduce the proposed integrated language model with query segmentation (QSLM). We first Figure 3: Query Segmentation Performance with Respect to Penalty Factor motivate QSLM with an oracle experiment, then describe the derivation of QSLM, and finally conduct extensive ex-periments on a large scale web search dataset.
To motivate the formulation of QSLM, we have carried out an intuitive and interesting experiment. Given an oracle ranker, we let the ranker choose the bigram or unigram lan-guage model for each query, whichever gives a better NDCG score. Table 4 lists the result of the oracle ranker compared to other models. As such a simple oracle performs signif-icantly better than either the bigram or unigram language models, it suggests that it may be possible to improve the search ranking if one can successfully emulate the behavior of the Oracle  X  to accurately predict when to use a unigram model and when to use a bigram model. We will show that query segmentation can help achieve a similar e ff ect. Method NDCG@1 NDCG@3 NDCG@10 BM25 0.3108 0.3358 0.3986 Unigram LM 0.3117 0.3366 0.3999 Bigram LM 0.3141 0.3380 0.3999
Oracle Ranker 0.3471 0.3628 0.4186
Given a model for computing the probability of a segmen-tation S for a query Q ,wecanexploitthisinformationand develop a new retrieval model incorporating the query seg-mentation structure. Note that the retrieval model proposed here is independent of the query segmentation technique. We start by formulating the integrated language model with query segmentation based on the probabilistic ranking prin-ciple [15]. Specifically, we can rewrite the probability that a document is relevant to a query as follows: where:
As the query segmentation is performed independently of the document, P ( B | Q, D )= P ( B | Q ). Furthermore, when adocumentisirrelevant,wecanapproximatethequeryas being generated from the background corpus statistics, in-dependent of the document: Finally, as the relevance of a document is independent of the segmentation partition without knowing the query, we will assume that all document has an equal probability of being relevant. Thus, we can approximate b as the average ratio of irrelevant to relevant documents over a set of queries.
In this work, we apply a language model approach to es-timate the the probability ratio a : For irrelevant documents, the query segments are gener-ated from the an n-gram language model trained from the background corpus. For relevant documents, the query seg-ments are modeled using a smoothed bigram model trained from the document, interpolated with the background cor-pus. Specifically: In this section we conduct a set of experiments for the QSLM model on the web search task. The main reason why we did not carry out experiments on the TREC datasets is due to the lack of clickthrough data for TREC queries, which is important to our study. In the following sections, we invest several variants of the model and discuss the choice in model parameters.
We evaluate the retrieval models on a large-scale real world dataset, containing 12,064 English queries sampled from the query log of a major c ommercial search engine. On average, each query is associated with 74 web docu-ments, with each query-document pair manually assigned arelevancelabelona5-levelscale:0meansthatthedocu-ment D is detrimental to query Q ,4meansthatthedocu-ment D is most relevant to Q .Forcomparison,weinclude 3 baseline models in the results: BM25 [16], unigram LM with Dirichlet smoothing [25], and bigram LM as specified in Equation (15). In order to obtain the optimal parame-ters in our model as well as in the baselines, we divide the whole dataset evenly into a training set and a test set, each containing 6,032 queries, and estimate the parameters from the training set using grid search, as proposed in [21]. The optimal parameters of the models are reported in Table 5. Finally we also list the simple oracle results as reference. The performance of all the retrieval models is measured by mean normalized discounted cumulative gain (NDCG) [7] at truncation levels 1, 3, and 10. We list the dataset statistics in Figure 4 and report detailed r esults of the retrieval mod-els in Table 6. In Table 6, we report the results by query Model NDCG Unigram LM  X  =2 . 702 Bigram LM  X  C =425026 , X  D =0 . 51 ,  X  =0 . 681 QSLM  X  C =500213 , X  D =0 . 50 ,  X  =0 . 90 ,b =720 length. For short queries, there are few variations in the segmentation. Thus, there is little room for improvement by exploiting segmentation information. However, the e ff ect of query segmentation is more pronounced when the query con-tains 4 or more words, which we consider as a long query. In this case, the NDCGs of BM25 and unigram LM are similar, both outperformed by the bigram LM. However, QSLM X  X  performance (0.3419, 0.3539 and 0.4040) is significantly bet-ter than all other models at all levels of NDCG truncation. In fact, we have conducted a paired t-test between QSLM and the other models. At confidence level  X  =0 . 01, the dif-ference between QSLM and BM25/unigram LM at all three levels of NDCG truncation is statistically significant. The di ff erence between QSLM and bigram LM at both NDCG@1 and NDCG@3 are significant.

As the choice of smoothing methods is critical to language modeling for information retrieval [25], we also test QSLM with an alternative form of bigram model (2-stage smoothed bigram) proposed in [8]: Results in Table 7 show that QSLM with 2-stage smoothed bigram performs slightly better than the original form of smoothed bigram (see Equation (15)) on NDCG3 and NDCG10. However, this smoothed bigram LM has a larger set of pa-rameters to tune than the original bigram LM, from 3 in the original to 7. Therefore, the original form of smoothed bi-gram model might be more generalizable to other scenarios. Table 7: QLSM with Di ff erence Bigram Models
The e ff ectiveness of a retrieval model combined with query segmentation relies on the ability to properly quantify the uncertainty of a segmentation. Ambiguous queries tend to have several segmentations with equal likelihood. So it X  X  necessary to explore how the retrieval model performance changes with respect to the number of segmentations. In this experiment, we run QSLM on long queries from the web search test set (1855 queries) and vary the maximum number of query segmentations from 1 to 10. All segmentations are sorted by their probability. Figure 5 shows the performance trends at NDCG@1, NDCG@3 and NDCG@10. At all three levels of NDCG, it follows a common trend: the results do improve as the number of segmentations increases. In gen-eral, the NDCG scores reach a max when the number of seg-mentations reaches 3 or 4. This demonstrates that we should consider the segmentation probability and incorporate more than one candidate segmentations into the retrieval model. It also illustrates that we can achieve a reasonable result by considering only the top few segmentations.
As discussed in Section 5.4, the penalty factor f is im-portant to query segmentation. Here we investigate how Figure 5: Retrieval Performance with Respect to #Segmentations this factor impacts the retrieval performance. Towards this goal, we run QSLM on the long queries from the web search test set (1855 queries) with f ranging from 1.5 to 3.5 in steps of 0.25. Figure 6 indicates a very di ff erent result from query segmentation: for our retrieval model with query segmen-tation, the result is not sensitive to the change in f .For example, for NDCG@1, the performance of QSLM increases only slightly from f =1 . 5to f =2 . 0, and decreases slowly afterwards. For NDCG@3 and NDCG@10, the di ff erences in NDCG scores at di ff erence values of f are negligible. The results suggest that our integrated retrieval model is robust to the choice of the penalty factor f . Figure 6: Retrieval Performance with Respect to Penalty Factor
The formulation of the QSLM model is not constrained by our query segmentation model. Theoretically it can work with query segmentations computed by any other model. It is thus interesting to know whether di ff erent query seg-mentation models will lead to di ff erent retrieval results. As mentioned in Section 5.3, we have constructed a 1000-query dataset with expert-labeled segmentations. More impor-tantly, this dataset is randomly sampled from our web search dataset of 12,064 queries. Therefore, there are relevance judgments on each query. In addition to the baselines BM25, unigram LM and bigram LM, we report the results of 3 other models in this experiments: QSLM with query segmenta-tions labeled by three human experts (QSLM a ), QSLM with segmentations computed by simple model + MS Web N-gram (QSLM b ), and QSLM with segmentations computed by our segmentation model with clickthrough only (QSLM c ). Table 8 summarizes the results of these models on 382 long queries. In general, the QSLM models work better than the 3baselines,nomatterwhatquerysegmentationmodelis used. However, the results demonstrate that there are no-ticeable di ff erences with respect to di ff erent segmentation models. Although the simple segmentation model with web n-gram works very well for query segmentation, it is infe-rior to using human-labeled query segmentations. Mean-while, QSLM, when coupled with our query segmentation model, works better than the other variants of QSLMs at all NDCG truncation levels. We believe the superior perfor-mance of our model is attributed to the appropriate mod-eling of relevance information in clickthroughs. Such rele-vance information is embedded in the query segmentations in the users X  preferences and subsequently exploited by the integrated language model.

In this paper we propose a novel unsupervised query seg-mentation model by jointly modeling the query-clicked doc-uments from the search log. Experimental results on two datasets confirm the e ff ectiveness of our model. Further-more, we develop a unified language model with query seg-mentation to improve the search ranking. The implicit rele-vance information in the clickthrough data is the bridge be-tween our query segmentation model and QSLM. Thorough experiments on a large-scale web search dataset show that search relevance can be improved by leveraging the query segmentations. As there is still a large gap in retrieval per-formance between the oracle ranker and the QSLM model, we plan to further refine the model to reduce gap in the fu-ture. Specifically, we would like to explore the use of QSLM as features to other advanced retrieval models [12]. This paper is based upon work supported in part by the National Science Founda tion under gra nt CNS-0834709. [1] E. Agichtein, E. Brill, and S. Dumais. Improving web [2] L. E. Baum, T. Petrie, G. Soules, and N. Weiss. A [3] S. Bergsma and Q. I. Wang. Learning noun phrase [4] G. Cao, J.-Y. Nie, and J. Bai. Integrating word [5] J. Gao, J.-Y. Nie, G. Wu, and G. Cao. Dependence [6] M. Hagen, M. Potthast, B. Stein, and C. Braeutigam. [7] K. J  X  arvelin and J. Kek  X  al  X  ainen. Cumulated gain-based [8] S. Javanmardi, J. Gao, and K. Wang. Optimizing two [9] T. Joachims. Optimizing search engines using [10] R. Jones, B. Rey, O. Madani, and W. Greiner. [11] T. M. K. M. Risvik and P. Boros. Query segmentation [12] D. Metzler and W. B. Croft. A markov random field [13] G. Mishne and M. de Rijke. Boosting Web Retrieval [14] F. Peng and D. Schuurmans. Self-supervised chinese [15] S. E. Robertson. The probability ranking principle in [16] S. E. Robertson and S. Walker. Some simple e ff ective [17] X. Shen, B. Tan, and C. Zhai. Implicit user modeling [18] M. Srikanth and R. Srihari. Biterm language models [19] B. Tan and F. Peng. Unsupervised query segmentation [20] T. Tao and C. Zhai. An exploration of proximity [21] M. Taylor, H. Zaragoza, N. Craswell, S. Robertson, [22] K. Wang, C. Thrasher, E. Viegas, X. Li, and B.-j. P. [23] X. Yu and H. Shi. Query segmentation using [24] C. Zhai. Fast statistical parsing of noun phrases for [25] C. Zhai and J. La ff erty. A study of smoothing methods
