 In this work we investigate three important aspects of pa-rameterized retrieval models: estimation, sensitivity, and generalization. Since all parameterized models, even those based on heuristics, have inherent uncertainty, we study these issues using statistical tools.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Storage and Retrieval]:Information Search and Retrieval General Terms: Algorithms, Experimentation, Theory Keywords: Estimation, sensitivity, generalization
In information retrieval, most of the commonly used re-trieval models are parameterized models . A parameterized model is one whose scoring function has one or more tun-able parameters. In these kinds of models, there are several important issues that have not received a great deal of atten-tion, but are critical for developing a deeper understanding of such models.

In this paper 1 we analyze parameter estimation, sensi-tivity, and generalization in parameterized retrieval models from a statistical point of view. Our work formalizes and ex-tends the current understanding of the relationship between parameters and models. In addition to providing deeper in-sight into existing models, the analytical tools proposed here can also be used to develop better models in the future.
There has been relatively little work in the information retrieval literature that has looked at these issues in any substantial detail. Previous work [1, 3] provides some sensi-tivity analysis via the use of plots, but little has been done to address the issue formally.
In information retrieval, the goal of parameter estimation is to choose a parameter setting that yields the most effective model possible. Since uncertainty is inherent in all parame-terized retrieval models, even those based on heuristics, the problem is best motivated and studied in a statistical frame-work. See [2] for an extended version of this paper.

Given a model, parameterized by  X  ,let M  X  be the pa-rameter space and m (  X  , T ) be the value of the effectiveness metric evaluated at  X  with regard to training data T .Fur-thermore, let P (  X  |T ) be the likelihood that parameter set-ting  X  is the optimal parameter setting given the training data. This is the posterior distribution over optimal param-eter settings after observing the training data.

The most common approach to parameter selection in-volves choosing a single parameter setting for use on all queries. Such estimates are point estimates .Twopointes-timation techniques based on the metric surface and the posterior distribution may be used. The first technique is based on the metric surface. In this approach, the param-eter that maximizes the metric over the parameter space is selected. That is,  X   X  =argmax  X  m (  X  , T ), where  X   X  is our estimate. The other technique is based on the posterior dis-tribution. Here, the parameter that maximizes the posterior is selected. That is,  X   X  =argmax  X  P (  X  |T ). This is the max-imum a posteriori estimate. Both approaches assume that the maximum/mode of the training set metric/posterior and the maximum/mode of the test set metric/posterior will be similar.

Alternatively, we can take a Bayesian approach. Rather than choosing a single estimate to use across all queries, a new parameter is selected for each query. This is done by repeatedly sampling parameters from some underlying dis-tribution. The most straightforward choice is to sample pa-rameters from the posterior. Such sampling can overcome the problems involved when the posterior may be multi-modal. In such cases, sampling can be  X  X afer X  and ensure that parameters around each mode are used.
Given a retrieval model and effectiveness metric, how sen-sitive is the effectiveness to perturbations of the parameter? This is the question that parameter sensitivity analysis tries to answer. If a slight perturbation causes the effectiveness to drastically change then the model is sensitive, but if the ef-fectiveness is unchanged even with large shifts in the param-eter then the model is insensitive. The reason for studying parameter sensitivity is because models that are sensitive are less prone to drastic changes in effectiveness if a poor parameter setting is chosen.

We propose two statistically motivated measures of sen-sitivity. Our first sensitivity measure is the entropy of the posterior distribution. The entropy of a distribution can be thought of as the uncertainty inherent in it. The entropy alone is not a valid indicator of sensitivity. A posterior dis-tribution with a large entropy is not necessarily sensitive, because the metric surface may still be flat. Similarly, if the posterior has low entropy, but the metric surface varies widely over the high confidence parameter values then there exists parameter sensitivity. Therefore, we must also include some notion of the flatness of the metric surface.
In order to measure how flat a distribution is over a set of parameter values, we compute the spread of the effectiveness metric, which is computed as: where {  X  : P (  X  |T ) &gt; 0 } is the support of  X  .
The spread, when combined with the entropy, provides a novel, robust way of looking at parameter sensitivity. For example, a model with high entropy, but low spread is more stable than a model with low entropy, but large spread. An ideal model will have low entropy and low spread, which indicates very high confidence over a small, flat range of the parameter space.
We are particularly interested in intracollection and in-tercollection generalization, which are two different ways of measuring the generalization properties of a model.
Intracollection generalization deals with how well a model trained on a set of topics from some collection generalizes to another set of topics on that same collection. This is a common setting in TREC evaluations, where collections are often reused from year to year, and systems are typically trained on the topics from the previous year(s).

Intercollection generalization measures how well a model trained on a topic set from one collection generalizes to a different topic set on a different collection. This is a practical scenario for  X  X ff the shelf X  retrieval systems that may be used across a wide range of different collections.

We measure generalization properties of a model by com-puting the effectiveness ratio , which is the ratio of the ob-served effectiveness of a (trained) model to the optimal ef-fectiveness. Thus, an effectiveness ratio of 100% represents a model that generalizes optimally.
We use the tools developed here to analyze the properties of BM25, F2EXP, language modeling (Dirichlet smoothing), and Metzler X  X  dependence models using two newswire col-lections (AP, Robust 2004) and two web collections (wt10g, GOV2). Due to space constraints, we present an overview of our analysis and results. See [2] for more details.
In terms of parameter estimation, we investigated whether one of the point estimates or the Bayesian sampling tech-nique resulted in a clearly optimal parameter selection strat-egy. Our results showed that no single method clearly domi-nates. Each method, in fact, works well for all model/collection pairs, with negligible differences between the methods in most cases. There are several interesting trends in the data, however. For the wt10g collection, it is almost always better to use the sampling method. The posterior distribution es-timated using the Dirichlet model has several modes, which indicates that there may actually be multiple optimal pa-rameter values that are appropriate, rather than a single, fixed value. Figure 1: Sensitivity plots for robust04 and gov2.
When looking at parameter sensitivity, both the entropy and spread of the posterior distribution are considered to-gether. Figure 1 plots the models with respect to these measures for two collections.

Our results indicate that, in terms of sensitivity, the F2EXP and Dirichlet models are the least sensitive models, and that slight variations in their parameter settings are less likely to produce drastic changes in effectiveness. In addition, the re-sults indicate that both of these models have rather focused (low entropy) posterior distributions.

The intracollection generalization results indicate that all of the models do a relatively good job of generalizing across topic sets within the same collection, with average effective-ness ratios well above 98%. We note that the F2EXP model tends to generalize better within newswire collections, while the dependence model generalizes better for web collections. The BM25 model, however, has the best average effective-ness ratio, which indicates its parameters do a particularly good job of capturing collection-dependent characteristics, rather than topic set-specific ones.

For intercollection generalization, the dependence model, on average, comes within 1% of the optimal setting regard-less of which collection is what trained on, whereas the F2EXP model only comes within 4% of the optimal on av-erage. The Dirichlet and BM25 models have average effec-tiveness ratios of 98.9% and 96.9%, respectively. Therefore, the dependence model and Dirichlet models are more robust when it comes to cross-collection generalization which make them good  X  X ut of the box X  algorithms.

Finally, we note that there is a certain disconnect between sensitivity and generalization. Models that are less sensitive are not necessarily those that generalize the best. This is mainly caused by the characteristics of the posterior distri-bution. If the distribution changes across collections, then the model is unlikely to generalize.

