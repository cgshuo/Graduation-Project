 UniversityofEdinburgh JohnsHopkinsUniversity UniversityofEdinburgh and also in developing linguistically rich paraphrase models based on syntactic structure. 1. Introduction The ability to paraphrase text automatically carries much practical import for many
NLP applications ranging from summarization (Barzilay 2003; Zhou et al. 2006) to question answering (Lin and Pantel 2001; Duboue and Chu-Carroll 2006) and machine translation (Callison-Burch, Koehn, and Osborne 2006). It is therefore not surprising that recent years have witnessed increasing interest in the acquisition of paraphrases fro mreal world corpora. These are most often monolingual corpora containing parallel translations of the same source text (Barzilay and McKeown 2001; Pang, Knight, and
Marcu2003).Truly bilingual corporaconsistingofdocumentsandtheirtranslationshave also been used to acquire paraphrases (Bannard and Callison-Burch 2005; Callison-
Burch 2007) as well as comparable corpora such as collections of articles produced by two different newswire agencies about the same events (Barzilay and Elhadad 2003).
 theacquiredparaphrasesoftenvaryingranularityastheycanbelexical( fighting, battle ) syntax trees X  X hey all rely on some form of alignment for extracting paraphrase pairs.
In its simplest form, the alignment can range over individual words, as is often done inmachinetranslation(Quirk,Brockett,andDolan2004).Inothercases,thealignments range over entire trees (Pang, Knight, and Marcu 2003) or sentence clusters (Barzilay andLee2003).
 phrase pairs are presented to judges who are asked to decide whether they are seman-tically equivalent, that is, whether they can be generally substituted for one another in the same context without great information loss (Barzilay and Lee 2003; Barzilay and
McKeown 2001; Pang, Knight, and Marcu 2003; Bannard and Callison-Burch 2005). In somecasestheautomaticallyacquiredparaphrasesarecomparedagainstmanuallygen-eratedones(LinandPantel2001)orevaluatedindirectlybydemonstratingperformance increase for a specific application, such as machine translation (Callison-Burch, Koehn, andOsborne2006).
 backs.First,itisinfeasibletoperformfrequentevaluationswhenassessingincremental syste mchanges or tuning syste mpara meters. Second, it is difficult to replicate results presentedinpreviousworkbecausethereisnostandardcorpus,andnostandardevalu-ationmethodology.Consequentlycomparisonsacrosssystemsarefewandfarbetween.
Thethirddrawbackconcernstheevaluationstudiesthemselves,whichprimarilyfocus on precision. Recall is almost never evaluated directly in the literature. And this is for a good reason: There is no guarantee that participants will identify the same set of paraphrases as each other or with a computational model. The problem relates to the nature of the paraphrasing task, which has so far eluded formal definition (see the discussion in Barzilay [2003]). Such a definition is not so crucial when assessing precision,becausesubjectsareaskedtoratetheparaphraseswithoutactuallyhavingto identify them. However, recall might be measured with respect to some set of  X  X old-standard X  paraphrases which will have to be collected according to some concrete definition.
 these problems. Specifically, we create a monolingual parallel corpus with human paraphrase annotations. Our working definition of paraphrase is based on word and phrase 1 alignments between semantically equivalent sentences. Other definitions are possible, for instance we could have asked our annotators to identify all constituents that are more or less meaning preserving in our parallel corpus. We chose to work with alignments for two reasons. First, the notion of alignment appears to be central in paraphrasing X  X ost existing paraphrase induction algorithms rely on alignments either implicitly or explicitly for identifying paraphrase units. Secondly, research in machinetranslation,whereseveralgold-standardalignmentcorporahavebeencreated, shows that word alignments can be identified reliably by annotators (Melamed 1998; OchandNey2000b;MihalceaandPedersen2003;Martin,Mihalcea,andPedersen2005).
We therefore create word alignments similar to those observed in machine transla-tion,namely,featuringone-to-one,one-to-many,many-to-one,andmany-to-manylinks between words. Alignment blocks larger than one-to-one are used to specify phrase correspondences. 598 annotationguidelines.Section3givesthedetailsofanagreementstudy,demonstrating thatourannotatorscanidentifyandalignparaphrasesreliably.Wemeasureagreement using alignment overlap measures from the SMT literature, and also introduce a novel agreement statistic for non-enumerable labeling spaces. Section 4 illustrates how the corpus can be used in paraphrase research, for example, as a test set for evaluating the output of automatic systems or as a training set for the development of paraphrase systems.Discussionofourresultsconcludesthearticle. 2. Corpus Creation and Annotation
Our corpus was compiled from three data sources that have been previously used for paraphrase induction (Barzilay and McKeown 2001; Pang, Knight, and Marcu 2003; Dolan, Quirk, and Brockett 2004): the Multiple-Translation Chinese (MTC) corpus, Jules Verne X  X  Twenty Thousand Leagues Under the Sea novel (Leagues), and the Microsoft
Research (MSR)paraphrase corpus. Thesearemonolingual parallel corpora, aligned at thesentencelevel.BothsourceandtargetsentencesareinEnglish,andexpressthesame contentusingdifferentsurfaceforms.
 Chinese text. 2 These stories were translated into English by 11 translation agencies.
Because the majority of the translators were non-native English speakers, occasionally translations contain syntactic or grammatical errors and are not entirely fluent. After inspection, we identified four translators with consistently fluent English, and used their sentences for our corpus. The Leagues corpus contains two English translations of the French novel Twenty Thousand Leagues Under the Sea . The corpus was created by Tagyoung Chung and manually aligned at the paragraph level. sentence level paraphrase pairs, we sampled from the subset of one-to-one sentence alignments. The MSR corpus was harvested automatically from online news sources.
Theobtainedsentencepairswerefurthersubmittedtojudgeswhoratedthemasbeing semanticallyequivalentornot(Dolan,Quirk,andBrockett2004).Weonlyusedseman-tically equivalent pairs. The sentence pairs were filtered for length ( ratio(  X  1 : 9betweentheshorterandlongersentence).Thiswasnecessarytopruneout incorrectlyalignedsentences.
 300 pairs (100 per corpus) were first annotated by two coders to assess inter-annotator agreement. The remaining 600 sentence pairs were split into two distinct sets, each consisting of 300 sentences (100 per corpus), and were annotated by a single coder.
Each coder annotated the same amount of data. In addition, we obtained a trial set of 50 sentences fro mthe MTC corpus which was used for fa miliarizing our annotators with the paraphrase alignment task (this set does not form part of the corpus). In sum, we obtained paraphrase annotations for 900 sentence pairs, 300 of which are doubly annotated.
 callyandthenhand-corrected.WeusedGiza++(OchandNey2003),apubliclyavailable implementation of the IBM word alignment models (Brown et al. 1993). Giza++ was trainedonthefull993-sentenceMTCpart1corpus 5 usingall11translatorsandallpair-ingsofEnglishtranslationsastraininginstances.Thisresultedin55 = pairs per sentence and a total of 54,615 training pairs. In addition, we augmented the training data with a word-identity lexicon, as proposed by Quirk, Brockett, and Dolan (2004).ThisfollowsstandardpracticeinSMTwhereentriesfromabilingualdictionary are added to the training set (Och and Ney 2000a), except in our case the  X  X ictionary X  is monolingual and specifies that each word type can be paraphrased as itself. This is necessaryinordertoinformGiza++aboutwordidentity.
 one source word can only be aligned to one target word, whereas one target word can be aligned to multiple source words. In SMT, word alignments are typically predicted inbothdirections:source-to-targetandtarget-to-source.Thesetwoalignmentsarethen merged (symmetrized) to produce the final alignment (Koehn, Och, and Marcu 2003).
Symmetrizationimprovesthealignmentqualitycomparedtothatofasingledirectional model, while also allowing a greater range of alignment types (i.e., some many-to-one, one-to-many, and many-to-many alignments can be produced). Analogously, we obtainedwordalignmentsinbothdirections 6 whichwesubsequentlymergedbytaking theirintersection.Thisresultedinahighprecisionandlowrecallalignment.
 to show which parts of these were in correspondence by aligning the mon a word-by-wordbasis. 7 Ourdefinitionofalignmentwasfairlygeneral(OchandNey2003):Givena sourcestring X = x 1 , ... , x N andatargetstring Y = y 1 twowordstringsisthesubsetoftheCartesianproductofthewordpositions:
We did not provide a formal definition of what constitutes a correspondence. As a rule of thumb, annotators were told to align words or phrases x ( X , Y )wheneverthewords x couldbesubstitutedfor y in Y ,orviceversa.Thisrelation-ship should hold within the context of the sentence pair in question: the relation x need not hold in general contexts. Trivially this definition allowed for identical word pairs.

Daum  X  e III and Marcu 2004), we distinguished between sure (S) and possible (P) align-ments, where S  X  P. The intuition here is that sure alignments are clear-cut decisions and typical of genuinely substitutable words or phrases, whereas possible alignments flagacorrespondencethathasslightlydivergentsyntaxorsemantics.Annotatorswere encouraged to produce sure alignments. They were also instructed to prefer smaller alignments whenever possible, but were allowed to create larger block alignments.
Smaller alignments were generally used to indicate lexical correspondences, whereas block alignments were reserved for non-compositional phrase pairs (e.g., idiomatic expressions) or simply expressions with radically different syntax or vocabulary. In 600 cases where information in one sentence was not present in the other, the annotators wereaskedtoleavethisinformationunaligned.
 make alignments in cases of ambiguity. These heuristics handled the alignment of (e.g., had been and shall be ), noun phrases with mismatching determiners (e.g., aman and the man ), verb complexes (e.g., was developed and had been developed ), phrasal quent speeches by Bush ), pronouns, repetitions, typographic errors, and approximate correspondences. For more details, we refer the interested reader to our annotation guidelines. 8 first pair ( Australia is concerned with the issue of carbon dioxide emissions. of greenhouse gases has attracted Australia X  X  attention. ) contains examples of word-to-word ( the  X  The ; issue  X  problem ; of  X  of ; Australia  X  alignments ( carbon dioxide emissions  X  greenhouse gases ). Importantly, we do not use a large many-to-many block for Australia is concerned with and has attracted Australia X  X  attention because it is possible to decompose the two phrases into smaller alignments.
The second sentence pair illustrates a possible alignment ( could have very long term effects  X  was of profound significance )indicatedbythegraysquares.Possiblealignments are used here because the two phrases only loosely correspond to each other. Possible alignmentsarealsousedtomarksignificantchangesinsyntaxwherethewordsdenote a similar concept: for example, in cases where two words have the same stem but are expressed with different parts of speech, (e.g., co-operative verbsareusedthatarenotsynonyms(e.g., this is also  X  this also marks ). 3. Human Agreement
As mentioned in the previous section, 300 sentence pairs (100 pairs from each sub-corpus) were doubly annotated, in order to measure inter-annotator agreement. Here, wetreatoneannotatorasgold-standard(reference)andmeasuretheextenttowhichthe otherannotatordeviatesfromthisreference.

Word-Based Measures. The standard technique for evaluating word alignments is to represent the mas a set of links (i.e., pairs of words) and co mpare the magainst gold-standardalignments.Thequalityofanalignment A (definedinEquation(1))compared to reference alignment B can be then computed using standard recall, precision, and F1measures(OchandNey2003):
Precision = where the subscripts S and P denote sure and possible word alignments, respectively.
Notethatbothprecisionandrecallareasymmetricinthattheycomparesetsofpossible and sure alignments. This is designed to be maximally generous: sure predictions which are present in the reference as possibles are not penalized in precision, and the converse applies for recall. We adopt Fraser and Marcu (2007) X  X  definition of F1, an
F-measure between precision and recall over the sure and possibles. They argue that it is a better alternative to the commonly used Alignment Error Rate (AER), which doesnotsufficientlypenalizeunbalancedprecisionandrecall. 9 lingual, in order to avoid artificial score inflation, we limit the precision and recall calculations to consider only pairs of non-identical words (and phrases, as discussed subsequently).
 beenproducedbythetwoannotatorsA(left)andB(right).Table1showstheindividual word alignments for each annotator and their type (sure or possible). In order to mea-sure F1, we must first estimate Precision and Recall (see Equation (2)). Treating annota-tor B as the gold standard, | A S | = 4, | B S | = 5, | A resultsinaprecisionof 4 4 = 1,arecallof 4 5 ,andF1of 2  X  1 alignments over identical words (i.e., discussed  X  discussed , the .  X  . ).

Phrase-Based Measures. The given definitions are all word-based; however, our annota-tors,andseveralparaphrasingmodels,createcorrespondencesnotonlybetweenwords but also between phrases. To take this into account, we also evaluate these measures over larger blocks (similar to Ayan and Dorr [2006]). Specifically, we extract phrase pairs from the alignments produced by our annotators using a modified version of the standardSMTphraseextractionheuristic(Och,Tillmann,andNey1999).Theheuristic 602 extractsallphrasepairs consistent withthewordalignment.Theseincludephrasepairs whose words are aligned toeach otheror nothing, but not towords outside thephrase boundaries. 10 The phrase extraction heuristic creates masses of phrase pairs, many of which are of dubious quality. This is often due to the inclusion of unaligned words or simplytotheextractionofoverly-largephrasepairswhichmightbebetterdecomposed into smaller units. For our purposes we wish to be maximally conservative in how we processthedata,andthereforewedonotextractphrasepairswithunalignedwordson theirboundaries.
 thepair and reached  X  and arrived at isconsistentwiththewordalignment.Incontrast, the pair and reached  X  and arrived isn X  X ; there is an alignment outside the hypothetical phrase boundary which is not accounted for ( reached is also aligned to at ). The phrase pair and reached an  X  and arrived at is consistent with the word alignment; however it hasanunalignedword(i.e., an )onthephraseboundary,whichwedisallow.
 atomic ,thatis,thesmallestpossiblephrasepairs,and composite ,whichcanbecreated by combining smaller phrase pairs. For example, the phrase pair and reached arrived at in Figure 3 is composite, as it can be decomposed into and reached  X  arrived at . Table 2 shows the atomic and composite phrase pairs extracted fro mthe possible align ments produced by annotators A and B for the sentence pair in Figure2.
 alignmentsasfollows: where A p and B p are the predicted and reference phrase pairs, respectively, and the atom subscript denotes the subset of atomic phrase pairs, in Equation (3) we measure precision and recall between atomic phrase pairs and the full space of atomic and composite phrase pairs. This ensures that we do not multiply reward composite phrase pair combinations, 11 while also not unduly pe-nalizing non-matching phrase pairs which are composed of atomic phrase pairs in 604 the reference. Returning to the example in Table 2, with annotator B as the gold precision= 5 7 = 0.71, recall = 4 8 = 0 . 50, and F1 = 2  X  identicalphrasepairs.
 automatically inducedandmaynotcorrespond tolinguisticintuition.Toevaluatethis, wehadtwoannotatorsreviewarandomsampleof166atomicphrasepairsdrawnfrom the MTC corpus (sure), classifying each phrase pair as correct, incorrect, or uncertain given the sentence pair as context. Fro mthis set, 73% were dee med correct, 22% un-certain,and5%incorrect. 12 Annotatorsagreedintheirdecisions75%ofthetime(using the Kappa 13 statistic, their agreement is 0.61). This confirms that the phrase-extraction processproducesreliablephrasepairsfromourword-aligneddata(althoughwecannot claimthatitisexhaustive).
Chance-Corrected Agreement. Besides precision and recall, inter-annotator agreement is commonly measured using the Kappa statistic (Cohen 1960). Thus is a desirable mea-surebecauseitisadjustedforagreementduepurelytochance: where Pr( A ) is the proportion of times two coders 14 agree, corrected by Pr( E ), the proportionoftimeswewouldexpectthemtoagreebychance.
 classification task, where two coders must assign n linguistic instances (e.g., sentences or words) into one of m categories. Given this situation, it would be possible for each codertoassigneachinstancetothesamecategory.Kappaallowsustoquantifywhether the coders agree with each other about the category membership of each instance. It is relativelystraightforwardtoestimatePr( A ) X  X tistheproportionofinstancesonwhich the two coders agree. Pr( E ) requires a model of what would happen if the coders were toassigncategoriesrandomly.Undertheassumptionthatcoders r dent, the chance of the magreeing on the j th category is the product of each of them assigninganinstancetothatcategory: Pr( C j | r 1 )Pr( C j su mof this product across all categories: Pr( E )= m describes two different methods for estimating Pr( C j | r is estimated for each coder (Cohen 1960) or the same distribution for all coders (Scott 1955;Fleiss1971;SiegelandCastellan1988).WerefertheinterestedreadertoDiEugenio andGlass(2004)andArtsteinandPoesio(2008)foramoredetaileddiscussion.
 exampleisstructuredlabelingproblemsthatallowawidevarietyofoutputcategories.
Importantly, the number and type of categories is not fixed in advance and can vary fro minstance to instance. In parsing, annotators are given a sentence for which they mustspecifyatree,ofwhichthereisanexponentialnumberinthesentencelength.Sim-ilarly,inourcasethespaceofpossiblealignmentsforasentencepairisalsoexponential in the input sentence lengths. Considering these annotations as nominal variables is inappropriate.
 facilitate the annotation of paraphrases. Ideally, we would like to measure agreement over the set of phrase pairs which are specified by our annotators (via the word align-ments),notthealignmentmatricesthemselves.
 especiallydesignedforsetsofvariables: 606
Here, A i and B i are the coders X  predictions on sentence pair i fro mour corpus of I sentence pairs. Each prediction is a subset of the full space of k items. Expression (5) measures the agreement (or concordance) between coders A and B and follows the generalformofKappafromEquation(4),whichisdefinedanalogouslywithPr( A )and Pr( E )takingtherolesof  X   X  and  X  0 ,butwithdifferentdefinitions.
 nostictasksinmind.Forexample,twophysiciansclassifysubjectsinto k = 3diagnostic categoriesandwishtofindoutwhethertheyagreeintheirdiagnoses.Here,eachcoder mustdecidewhich(possiblyempty)subsetfrom k categoriesbestdescribeseachsubject.
The size of k is thus invariant with the instance under consideration. This is not true in our case, where k will vary across sentence pairs as sentences of different lengths license different numbers of phrase pairs. More critically, the formulation in Equa-tion (5) assumes that items in the set are independent: All subsets of the same car-dinality as k are equally likely, and no combination is impossible. This independence assumption is inappropriate for the paraphrase annotation task. The phrase extraction heuristic allows each contiguous span in a sentence to be aligned to either zero or one span in the other sentence; that is, nominating a phrase pair precludes the choice of many other possible phrase pairs. Consequently relatively few of the subsets of the full set of possible phrase pairs are valid. Formally, an alignment can specify only
O ( N 2 )phrasepairsfromatotalsetof k = O ( N 4 )possiblephrasepairs.Thisdisparityin magnitudesleadstoincreasinglyunderestimated  X   X  forlarger N ,namely,lim lim N  X  X  X  O ( N 2 ) / O ( N 4 ) = 0.Theendresultisanoverestimateof our highly interdependent ite msets. We use  X  C fro mEquation (5) as our agree ment sta-tisticdefinedoversetsofatomicphrasepairs,thatis, A = A p atom  X  0 asfollows: pair i ,andPr( A p atom )andPr( B p atom )arepriorsoverthesesetsforeachannotator.Aconse-quenceofdroppingtheindependenceassumptionsisthatcalculating  X  moredifficult.
 cated for larger phrase pairs or with an expressive prior. For the sake of flexibility we estimate  X  0 using Monte Carlo sampling. Specifically, we approximate the full sum by drawing samples from a prior distribution over sets of phrase pairs for each of our using the intersection metric. This is repeated many times and the results are then averaged.Moreformally: where for each sentence pair, i ,wedraw J samples of pairs of sets of phrase pairs, ( not defined how we sample valid sets of phrase pairs. This is done via the word align-ments. Recall that the annotators start out with alignments from an automatic word-aligner. Firstly, we develop a distribution to predict how often an annotator changes a cell from the initial alignment matrix. We model the number of changes made with a binomialdistribution,thatis,eachlocalchangeisassumedindependentandhasafixed probability,Pr( edit | r , N i , M i )where r isthecoderand N
This distribution is fit to each annotator X  X  predictions using a linear function over the combined length of two sentences. Next we sample word alignments. Each sample starts with the automatic alignment, and each cell is changed with probability Pr( edit ). These changes are binary, swapping alignments for non-alignments and vice versa.
Finally, the phrase-extraction heuristic is run over the alignment matrix to produce a set of phrase pairs. This is done for each annotator, A and B , after which we have a sample, ( A p atom , B p atom ).EachsampleisthenfedintoEquation(7).Admittedly,thisisnot themostaccurateprior,asannotatorsarenotjustrandomlychangingthealignment,but instead are influenced by the content expressed by the sentence pair and other factors such as syntactic complexity. However, this prior produces estimates for  X   X  several orders of magnitude larger than those using Kupper and Hafner X  X  model of  X  inEquation(5).
 respecttotheexampleinFigure2.Here, | A p atom | = 7, | B p ment cells, respectively, of the initial alignment matrix. This translates into Pr( edit
Carlo sampling process from Equation (7), which results in  X   X  agreementestimate,  X   X  ,andchancecorrectionestimate,  X   X  0  X  same as before), annotator B edits nine alignment cells, but annotator A chooses not to make any edits. This leads to an increased estimate of  X   X   X 
C = 0 . 442.Ifbothannotatorswerenottomakeanyedits,  X   X  0 = 1and ingly, at the other extreme when Pr( edit | r = A ) = Pr( edit perfect,  X   X  0 = 1and  X  C =  X  X  X  . This is because only one phrase pair can be extracted whichconsistsofthetwofullsentences.

Results. Tables3and4displayagreementstatisticsonourthreecorporausingprecision, recall, F1, and  X  C . Specifically, we estimate  X  C by aggregating  X   X  and  X   X  level estimates. Table 3 shows agreement scores for individual words, whereas Table 4 shows agreement for phrase pairs. In both cases the agreement is computed over non-identical word and phrase pairs which are more likely to correspond to paraphrases.
Theagreementfiguresarebrokendownintopossible(Poss)andsurealignments(Sure) forprecisionandrecall.
 three corpora (MTC, Leagues, and News). Recall on Possibles seems worse on the
News corpus when compared to MTC or Leagues. This is to be expected because this corpus was automatically harvested from the Web, and some of its instances may not berepresentativeexamplesofparaphrases.Forexample,itiscommonforonesentence to provide considerably more details than the other, despite the fact that both describe thesameevent.Theannotatorsinturnhavedifficultydecidingwhethersuchinstances are valid paraphrases. The  X  C scores for the three corpora are in the same ballpark. 608
Interestingly,  X  C ishighestontheNewscorpus,whereasF1islowest.Whereasprecision and recall are normalized by the number of predictions from annotators A and B , respectively,  X  C isnormalized bytheminimum numberofpredictions betweenthetwo. Therefore, when the predictions are highly divergent,  X  C will paint a rosier picture than
F1 (which is the combination of precision and recall). This indeed seems to be the case fortheNewscorpus,whereprecisionandrecallhaveahigherspreadincomparisonto theothertwocorpora(seethePosscolumninTable3).

This is expected because annotators are faced with a more complex task; they must generally make more decisions: for example, determining the phrase boundaries and how to align their constituent words. An exception to this trend is the News corpus wheretheF1ishigherforphrasepairsthanforindividualwordpairs.Thisisduetothe fact that there are many similar sentence pairs in this data. These have many identical words and a few different words. The differences are often in a clump (e.g., person names, verb phrases), rather than distributed throughout the sentence. The annotators tendtoblockaligntheseandthereisalargescopefordisagreement.Whereasestimating agreement over words heavily penalizes block differences, when phrases are taken into account in the F1 measure, these are treated more leniently. Note that so lenient, as it measures agreement over the sets of atomic phrase pairs rather than between atomic and composite phrase pairs in the F1 measure. This means that under  X 
C , choosing different granularities of phrases will be penalized, but would not have beenundertheF1measure.
 ically,weplotobservedagreement  X   X  ,chanceagreement  X  0 ,and binned by (the shorter) sentence length. In all cases we observe that chance agreement issubstantiallylowerthanobservedagreementforallsentencelengths.Wealsoseethat  X 
C tends to be higher for shorter sentences. Differences in mostly of small magnitude across all three corpora. This indicates that disagreements maybeduetootherfactors,besidessentencelength.
 to gauge the quality of the obtained agreements. The use of precision, recall, and F1 is widespread in SMT, but these measures evaluate automatic alignments against a gold standard, rather than the agreement between two or more annotators (but see
Melamed[1998]foranexception).Nevertheless,wewouldexpectthehumanstoagree morewitheachotherthanwithGiza++,giventhatthelatterproducesmanyerroneous word alignments and is not specifically tuned to the paraphrasing task. Table 5 shows agreement between one annotator and Giza++ for atomic phrase pairs. similar results for the other annotator and with the word-based measures. As can be seen, human X  X iza++ agreement is much lower than human X  X uman agreement on all three corpora (compare Tables 5 and 4). Taken together the results in Tables 3 X 5 show a substantial level of agreement, thus indicating that our definition of paraphrases via wordalignmentscanyieldreliableannotations.Inthefollowingsectionwediscusshow ourcorpuscanbeusefullyemployedinthestudyofparaphrasing. 610 4. Experiments
Our annotated corpus can be used in a number of ways to help paraphrase research: for example, to inform the linguistic analysis of paraphrases, as a training set for the development of discriminative paraphrase systems, and as a test set for the automatic evaluationofcomputationalmodels.Here,webrieflydemonstratesomeoftheseuses. Paraphrase Modeling. Muchpreviousresearchhasfocusedon lexical paraphrases(butsee
Lin and Pantel [2001] and Pang, Knight, and Marcu [2003] for exceptions). We argue that our corpus should support a richer range of structural (syntactic) paraphrases.
To demonstrate this we have extracted paraphrase rules from our annotations using the grammar induction algorithm from Cohn and Lapata (2007). Briefly, the algorithm extracts tree pairs fro mword-aligned text by choosing aligned constituents in a pair of equivalentsentences.Thesepairsarethengeneralizedbyfactoringoutalignedsubtrees, thereby resulting in synchronous grammar rules (Aho and Ullman 1969) with variable nodes.
 rulesfromthegold-standardalignments.AsampleoftheserulesareshowninFigure5.
Here we see three lexical paraphrases, followed by five structural paraphrases. In example4, also isreplacedwith moreover andismovedtothestartofthesentencefrom the pre-verbal position. Examples 5 X 8 show various reordering operations, where the boxednumbersindicatecorrespondencesbetweennon-terminalsinthetwosidesofthe rules.
 at the syntactic level, and also a practical means for developing algorithms for para-phrase generation  X  X  task which has received little attention to date. For instance, we could envisage a paraphrase model that transforms parse trees of an input sentence into parse trees that represent a sentential paraphrase of that sentence. Our corpus can be used to learn this mapping using discriminative methods (Cowan, Ku  X  cerov  X  a, and Collins2006;CohnandLapata2007).

Evaluation Set. AsmentionedinSection1,itiscurrentlydifficulttocomparecompeting approaches due to the effort involved in eliciting manual judgments of paraphrase output.Ourcorpuscouldfilltheroleofagold-standardtestset,allowingforautomatic evaluationtechniques.
 this article. Nevertheless, we illustrate how the corpus can be used for this purpose. For example we could easily measure the precision and recall of an automatic system againstourannotations.Computingprecisionandrecallforanindividualsystemisnot perhaps the most meaningful test, considering the large potential for paraphrasing in a given sentence pair. A better evaluation strategy would include a comparison across manysystemsonthesamecorpus.Wecouldthenrankthesesystemswithout,however, paying so much attention to the absolute precision and recall values. We expect these comparisons to yield relatively low numbers for many reasons. First and foremost the task is hard, as shown by our inter-annotator agreement figures in Tables 3 and 4.
Secondly, there may be valid paraphrases that the systems identify but are not listed inourgoldstandard. Thirdly,systemsmayhavedifferentbiases,forexample,towards producing more lexical or syntactic paraphrases, but our comparison would not take this into account. Despite all these considerations, we believe that comparison against our corpus would treat these systems on an equal footing against the same materials whilefactoringoutnonessentialdegreesoffreedominherentinhumanelicitationstud-ies(e.g.,attentionspan,taskfamiliarity,background).
 is simply Giza++ trained on the 55,615 sentence pairs described in Section 4. The second syste muses a co-training-based paraphrase extraction algorith m(Barzilay and
McKeown 2001). It was also trained on the MTC part 1 corpus, on the same data set usedforGiza++,withitsdefaultparameters.Foreachsystem,wefilteredthepredicted paraphrases to just those which match part of a sentence pair in the test set. These paraphraseswerethencomparedtothesurephrasepairsextractedfromourmanually aligned corpus. Giza++ X  X  precision is 55% and recall 49% (see Table 5). The co-training syste mobtained a precision of 30% and recall of 16%. To confir mthe accuracy of the precision estimate, we performed a human evaluation on a sample of 48 of the predicted paraphrases which were treated as errors. Of these, 63% were confirmed as beingincorrectandonly20%wereacceptable(theremainingwereuncertain).Theinter-annotator agreement in Table 4 can be used as an upper bound for precision and recall (precision for Sure phrase pairs is 67% and recall 66%). These results see mto suggest that a hypothetical paraphrase extractor based on automatic word alignments would obtain performance superior to the co-training approach. However, we must bear in mindthattheco-trainingsystemishighlyparametrizedandwasnotspecificallytuned toourdataset. 5. Conclusions
In this article we have presented a human-annotated paraphrase corpus and argued that it can be usefully employed for the evaluation and modeling of paraphrases. We havedefinedparaphrasesaswordalignmentsinacorpuscontainingpairsofequivalent sentences and shown that these can be reliably identified by annotators. In measur-ing agreement, we used the standard measures of precision, recall, and F1, but also proposed a novel formulation of chance-corrected agreement for word (and phrase) alignments. Beyond alignment, our formulation could be applied to other structured tasksincludingparsingandsequencelabeling.
 uating the precision and recall of paraphrase induction systems trained on parallel monolingual corpora. The corpus could be further used to develop new evaluation metrics for paraphrase acquisition or novel paraphrasing models. An exciting avenue forfutureresearchconcernsparaphrase prediction ,thatis,determiningwhenandhowto paraphrase single sentence input. Because our corpus contains paraphrase annotations atthesentencelevel,itcouldprovideanaturaltest-bedforpredictionalgorithms. 612 Acknowledgments References
