
Data clustering plays an indispensable role in various fields such as computer science, medical science, social sci-ence and economics [1]. In the real-world applications, it is important to identify the nonlinear clusters and kernel-based clustering is a popular method for this purpose [2]. The basic idea of kernel-based clustering is to seek an assignment of each point to one of some clusters in the kernel feature space such that, the within-cluster similarity is high and the between-cluster one is low. However, exhaustive search for the optimal assignments of the data points in the projected space is computationally infeasible [2]. Since the number of all possible partitions of a dataset grows exponentially with efficient approach to find satisfactory sub-optimal solutions.  X  -means is such an iterative method [3], [4]. Despite great success, one serious drawback is that, its performance would instance, in the randomly ill-initialized assignment, some cluster is assigned with a small number of remote and quently, two contributions have been made in this paper. 1) The proposed COLL method is insensitive to the ill-2) Compared with other techniques aiming at tackling ill-Experimental results on synthetic and large-scale real-world datasets, as well as that in the application of video clustering, have demonstrated the significant improvement over existing kernel clustering methods.
 The remainder of the paper is organized as follows. Section II formulates the optimization problem of kernel clustering and reviews some related work. In section III, we describe in detail the proposed conscience on-line learning method. Experimental results and applications are reported in section IV. We conclude our paper in section V.
 A. Problem Formulation
Given an unlabelled dataset  X  = { x 1 ,..., x  X  } of  X  data points in  X   X  which is projected into a kernel space  X  by a mapping  X  , and the number of clusters  X  , we wish to find an assignment of each data point to one of  X  clusters, such that and the between-cluster one is low. That is, we seek a map to optimize [2] where  X &gt; 0 is some parameter and we use the short notation  X  =  X  ( x  X  ) .
 Theorem 1. The optimization criterion (2) is equivalent to the criterion where  X   X  is the mean of data points assigned to cluster  X  and  X   X  satisfies Then the updated prototypes  X  are implicitly expressed by the assignment  X  , which is further used in the next iteration. Let  X   X  denote the old assignment before the  X  -th iteration, the convergence criterion (9) is computed as The above procedures lead to the well-known kernel  X  -the sub-optimal solution (local minima).

Despite great success in practice, it is quite sensitive to the initial positions of cluster prototypes, leading to degenerate local minima.
 Example 1. In Figure 1(b),  X  2 is ill-initialized relatively far away from any points such that it will get no chance to be assigned with points and updated in the iterations of  X  -means. As a result, the clustering result by  X  -means is trapped in degenerate local minima as shown in Figure 1(c), where  X  2 is assigned with no point.

To overcome this problem, current  X  -means algorithm and its variants usually run many times with different initial pro-totypes and select the result with the smallest distortion error (6) [11], which are however computationally too expensive. In this paper, we propose an efficient and effective approach to the optimization problem (3), which can output smaller distortion error that is insensitive to the initialization. A. The Conscience On-line Learning Model
Let  X   X  denote the cumulative winning number of the  X  -th prototype, and  X   X  =  X   X  / Example 2. The same ill-initialization as kernel  X  -means in Example 1 is used by COLL. However, since the winning frequency of the ill-initialized  X  2 will become smaller in the later iterations, it gets the chance to win according to (13). Finally, an almost perfect clustering with small distortion error is obtained as shown in Figure 1(d).
 B. The Computation of COLL
As discussed before, any effective algorithm in the kernel space must compute with only the kernel matrix  X  .To this end, we devise an efficient framework for computation of the proposed COLL based on a novel representation of prototypes termed prototype descriptor .

Let  X   X   X   X  (  X  +1) denote the  X   X  (  X  +1) matrix space satisfying  X  is nonnegative. We define the prototype descriptor based on kernel trick as follows.
 Definition 2 ( Prototype descriptor ) . A prototype descriptor is a matrix  X   X   X   X   X   X   X  (  X  +1) , such that the  X  -th row represents prototype  X   X  by i.e.,
With this definition, the computation of the distortion error (6) now becomes:
Let X  X  consider the computation of 4 ingredients of the proposed COLL model.
 Theorem 2 ( Initialization ) . The random initialization can be realized in the way of where diag (  X  ) denotes the main diagonal of a matrix  X  and the positive matrix  X  =[  X   X , X  ]  X   X   X   X   X   X   X   X  + has the form That is, the matrix  X  reflects the initial assignment  X  . as  X  . Substitute the computation of the prototypes (4) to the  X   X  Then we get the on-line winner updating rule as (26).
It is a bit complicated to compute the convergence cri-one and only one winning prototype. Let array  X   X  = 1 , X   X  2 ,..., X   X   X   X  ] stores the indices of  X   X  ordered points assigned to the  X  -th prototype in one iteration. For in-ordered points assigned to the 2 -nd prototype in the  X  -th iteration, then the index array of the 2 -nd prototype is following lemma formulates the cumulative update of the  X  Lemma 1. In the  X  -th iteration, the relationship between the updated prototype  X   X  and the old  X   X   X  is:  X   X  ordered points assigned to the  X  -th prototype in this iteration.
 of Mathematical Induction. One can easily verify that (29) is true for  X   X  =1 directly from (14), ordered points, Then for  X   X  =  X  +1 , i.e., the (  X  +1) -th point, from (14) Algorithm 1: Conscience on-line learning (COLL) Input: kernel matrix  X   X   X   X   X   X  ,  X  , {  X   X  } ,  X  ,  X   X  X  X  X  . Output: cluster assignment  X  subject to (3). 1: Randomly initialize assignment  X  and set  X  =0 ; 2: repeat 3: Get  X  empty index arrays {  X   X  =  X  :  X  =1 ,..., X  } 4: for  X  =1 ,..., X  do 5: Select the winning prototype  X   X   X   X  , : of the  X  -th point 6: Update the winning prototype  X   X   X   X  , : with learning 7: end for 8: Compute  X   X  via (33). 9: until  X   X   X   X  or  X   X   X   X  X  X  X  10: Obtain the cluster assignment  X   X  by (24),  X   X  =1 , ...,  X  . C. Computational complexity
The computation of the proposed COLL method con-to update  X   X  . From (20), the initialization of  X   X  takes  X  tational complexity is  X  Since  X  (  X  (  X  +(  X  +1))) operations are needed to per-ner and  X  (  X  +1) to update the winner, there being  X  points) and  X  compute the convergence criterion  X   X  (the first term of (33) taking  X  (  X  ) operations, the second term at most  X  (  X  2 ) operations and the third term  X  (  X  ) operations). Assume the iteration number is  X   X  X  X  X  , since in general 1 &lt; X &lt; X  , the computational complexity for iteration procedure is  X  experiments are implemented in Matlab7.8.0.347 (R2009a) 64bit edition on a workstation (Windows 64bit, 8 Intel 2.00GHz processors, 16GB of RAM).
 A. Synthetic Dataset
We first performed synthetic dataset clustering to demon-strate the effectiveness of the proposed COLL in handling the ill-initialization problem in nonlinear clustering condi-tion. The classic two moons dataset consisting  X  =2000 points was generated as 2 half-circles in  X  2 .The  X  for constructing Gaussian kernel matrix was set at  X  =0 . 15 and fixed for all the compared kernel methods. Figure 2 shows the original two moons dataset and clustering results obtained by kernel  X  -means and COLL respectively. Due to trapped into degenerate local optima as shown in Figure 2(b). However, since the conscience mechanism is capable of handling ill-initialization problem by reducing the winning rate of the frequent winner, the clustering result by the proposed COLL with the same initialization is promising as shown in Figure 2(c). It demonstrates the effectiveness of COLL in tackling ill-initialization problem.
 B. Digit Clustering
For digit clustering, we selected four widely tested digit datasets, including Pen-based recognition of handwritten digit data set (Pendigits), Multi-feature digit data set (Mfeat), USPS [16] and MNIST [17]. The first 2 datasets are from UCI repository [18]. Table I summarizes the properties of the four datasets, as well as the  X  used in constructing the Gaussian kernel matrix. 1) Convergence Analysis: We first analyzed the con-vergence property of the proposed COLL. Figure 3 plots the logarithm of the convergence criterion  X   X  value as a function of iteration step obtained by COLL and kernel  X  -means when the actual number of underlying clusters  X  (i.e.,  X  =10 ) is provided. Since the main procedure of the global kernel  X  -means is to run the kernel  X  -means many times, it does not output the single convergence criterion .One can observe from Figure 3 that, the log(  X   X  ) on all datasets except USPS Figure 3(c), monotonically decrease from some relatively large values to some small values as the iteration step increases, which implies that the solution trends to we also tested the compared methods in terms of the external clustering validation with the number of underlying clusters  X  prespecified. Although there exist many external clustering evaluation measurements, such as clustering errors, average purity, entropy-based measures [19] and pair counting based indices [20], as pointed out by Strehl and Ghosh [21], the mutual information provides a sound indication of the shared information between a pair of clusterings. The mu-tual information based measurements may include variation of Information (VI) [22], normalized mutual information (NMI) [21], adjusted mutual information (AMI) [23], etc. And normalized mutual information (NMI) [21] is widely used in measuring how closely the clustering and underlying class labels match. Given a dataset  X  of size  X  , the clustering labels  X  of  X  clusters and actual class labels  X  of  X   X  classes, tation). Video clustering plays an important role in auto-matic video summarization/abstraction as a preprocessing step [24]. Consider a video sequence in which the camera is fading/switching/cutting among a number of scenes, the goal of automatic video clustering is to cluster the video frames according to the different scenes. The gray-scale values of the raw pixels were used as the feature vector for each is taken as the dataset, where  X  =  X  X  X  X  X  X  X   X   X  X  X  X  X  X  X  X  X  and  X  is the length of the video sequence. We selected the 11 video sequences from the open-video website [25], which are 11 segments of the whole  X  X ASA 25th Anniversary Show X  with  X  =320  X  240=76800 and  X  (i.e., the duration of the sequence) varying from one sequence to another.

Figure 5 illustrates the clustering result of one video sequence  X  X ASA 25th Anniversary Show, Segment 2 X  (ANNI002) by the proposed COLL. 2492 frames have been clustered into 16 scenes. Except for the frames from 400 to 405 and 694 to 701 as well as the last two clusters, where the separation boundaries are not so clear, satisfactory segmentation has been obtained. For comparison,  X  X round truth X  segmentation of each video sequence has been man-ually obtained, through which NMI values are computed to compare COLL with kernel  X  -means and global kernel  X  -means. Table IV lists the average values (running 10 times) of NMI and computational time in seconds on the 11 video methods, thus can be applied to the real-world applications such as large-scale digit clustering and video clustering.
This project was supported by the NSFC-GuangDong (U0835005), NSFC (60633030, 60803083), 973 Program (2006CB303104) in China, and GuangDong Program (2010B031000004).

