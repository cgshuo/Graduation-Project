 This work focuses on solving the context-aware implicit feed-back based recommendation task with factorization and is heavily influenced by the practical considerations. I pro-pose context-aware factorization algorithms that can effi-ciently work on implicit data. I generalize these algorithms and propose the General Factorization Framework (GFF) in which experimentation with novel preference models is pos-sible. This practically useful, yet neglected feature results in models that are more appropriate for context-aware rec-ommendations than the ones used by the state-of-the-art. I also propose a way to speed up and enhance scalability of the training process, that makes it viable to use the more accurate high factor models with reasonable training times. I.2.6 [ [Artificial Intelligence] ]: Learning -Parameter Learn-ing Algorithms, Experimentation recommender systems; context-awareness; factorization; pref-erence modeling; implicit feedback; scalability
Recommender systems are more and more widely used in e-commerce and on multimedia sites. My work focuses on advanced algorithms that can be used in practical rec-ommender system. One of the biggest distinction between practice and academic research is that the latter focuses on explicit feedback and the prediction of ratings, while the former uses implicit feedback and requires top N recommen-dations. Although there has been a shift towards this more practical setting in the last few years, the majority of re-search still focuses on ratings.

Implicit feedback is collected via monitoring the behaviour of users while they use a service (e.g. a web shop). User in-teraction is not required in order to get the feedback, there-fore it is available in large quantity. This is of key impor-tance in practical scenarios. Explicit feedback on the other hand is usually either not available or its amount is negligi-ble compared to implicit feedback. The primary challenge of implicit feedback is that it does not explicitly encode user preferences. These preferences must be inferred from the interactions. The presence of an user action on an item (e.g. purchase) is considered to be a noisy sign of positive prefer-ence. However it is much harder to infer negative feedback as the absence of an event can be traced back to multiple causes, the most common being that the user does not know about the item. Although there are some ways to infer neg-ative feedback in special cases, it is generally assumed to be missing and the absence of positive feedback is considered as a very week sign of negative preference.

Context-aware recommender systems (CARS) consider ad-ditional information (termed context) besides user X  X tem in-teractions. Any information can be considered as context, however I argue that it is useful to distinguish event con-text from other types of data, such as item metadata, socio-demographic information or social network of the user. The common property of latter categories is that they are bound either to the item or to the user. Also, they are thoroughly examined in specific research topics, such as content based or hybrid recommenders. On the other hand, event context is associated with the interaction of the users and items and can not be bounded to either one. Typical examples are the time or the location of the event. The hypothesis of context-aware recommendations is that they can significantly im-prove recommendation accuracy, because (1) context related effects can be handled during training; (2) recommendation lists can be tailored according to the actual value of the context, which may influence the users X  needs.

One of the most extensive data models for representing context-enhanced data is the Multidimensional Dataspace Model (MDM) [1] in which the dataspace is the Cartesian product of several dimensions, and each dimension is the Cartesian product of one or more attributes. Attributes are atomic and nominal and their value comes from a finite set of values. Almost all practically used context-enhanced data can be expressed in a more simple dataspace model, where each dimension consist of exactly one attribute. I refer to this dataspace model as single attribute MDM or SA-MDM. Note that if data is representable in SA-MDM it is also representable in a tensor. The SA-MDM representation is powerful enough for commonly used context dimensions, such as time or location 1 .

Latent feature based collaborative filtering methods X  X uch as matrix factorization X  X ave gained popularity in the last decade due to their high accuracy and good scalability. My work focuses on factorization methods for implicit feedback that also incorporate context dimensions.
In my early work I proposed a context-aware tensor fac-torization algorithm X  X TALS (implicit Tensor Alternating Least Squares) [6] X  X or the implicit feedback problem. The algorithm works with SA-MDM. The presence of an event (i.e. a combination of attributes is in the training data) is considered to be strong positive preference, while the ab-sence of an event is considered to be weak negative prefer-ence. The method uses pointwise ranking by optimizing for weighted root mean squared error (wRMSE). The value of the target is 1 for positive and 0 for negative feedback; the weight for positive feedback is much higher than for neg-ative. Interaction data is inherently sparse and the usage of additional context dimension makes it even sparser; thus majority of the values in the tensor are zeros. However, contrary to explicit problems, there are no missing values in the tensor that makes the direct application of explicit algorithms inefficient. In iTALS, each dimension is assigned with a feature matrix that contains feature vectors of K length for all of the possible attribute values in that dimen-sion. For the prediction of preferences iTALS uses the N-way model, i.e. the  X  X ot product X  of one feature vector from every dimension, corresponding to the user X  X tem X  X ontext(s) configuration on which the prediction is requested.
The iTALSx algorithm [4][5] is a variation of iTALS. The difference is in the prediction model for which iTALSx uses a variant of the pairwise model, i.e. the sum of dot products between pairs of feature vectors. iTALSx considers only the user X  X tem, user X  X ontext(s) and item X  X ontext(s) pairs 2 . The change in the model affects the optimization procedure.
Generalizing the idea of factorization, preference models X  i.e. the expression with which the preference (or rating) is approximated X  X an be considered as the sum of various in-teractions. An interaction type is the dot product of feature vectors from selected dimensions, one vector per dimension (e.g. the user and the item feature vector for the given user and item). With only the user and item dimensions, there is only one possible interaction type: the user X  X tem interac-tion ( U I ). This results in one possible factorization model. Adding one context dimension ( S ) gives the following new interaction types. U SI is a reweighted user X  X tem interaction where the weight is dependent on the value of the context. U S and IS are context-dependent user and biases. The number of different models with 3 dimensions is 15. Adding another context (Q) further increases the number of interac-tion types. Besides U QI , U Q and IQ , there is U SQI (the
Even context dimensions that contain more than one at-tribute can be represented in SA-MDM, but less effectively.
Other methods, such as Factorization Machines [11] also use context X  X ontext pairs in their variation of the this model. user X  X tem interaction whose reweighting depends on both context dimensions); U SQ , ISQ and SQ in which there is some kind of interaction between context dimensions. The number of possible models with 4 dimensions is 2047.
However, not all of the interaction types are of equal im-portance. Generally items are recommended to users, there-fore these two dimensions are more important. Also, users are the only entities which act and the target of these actions are the items. Context on the other hand is not a direct par-ticipant in the transactions, but may influence behaviour.
Despite of the large number of models, only two of them are used widely in the literature: the N-way and the full pairwise interaction model. Both of these models are sym-metric, thus all dimensions are considered to be equal.
The choice of the model affects the optimization, but it is ineffective to implement a new version of an algorithm for every one of them. Therefore I created the General Fac-torization Framework (GFF) [7], which is a single flexible algorithm that takes the preference model as an input and does the computations accordingly. GFF allows its user to easily experiment with various linear models on any context-aware recommendation task. The following properties were important at the design of GFF. 1. No restriction on context 3 : GFF works on any context-2. Large preference model class: the only restriction on 3. Data type independence: the implicit case is in the 4. Flexibility: the weighting scheme of GFF is very flex-5. Scalability: GFF scales well both in terms of the num-
Along with the users ( U ) and items ( I ), I used two context dimensions X  X easonality ( S ) and sequentiality ( Q ) X  X hat are available with every implicit datasets as long as the times-tamp of the events is recorded. Their availability and use-fulness makes these dimensions suitable for the experiment.
Seasonality: Many application areas of recommender systems exhibit the seasonality effect, because periodicity can be observed in many human activities. Thus seasonal data is an obvious choice for context [9]. First, the length of the season has to be defined. No repetitions are expected in the aggregated behavior of users within a season, and simi-lar aggregated behaviour is expected at the same time offset in different seasons. Next, time bands need to be created within the seasons that are the context-states. Time bands specify the time resolution of a season. The length of time
The basic GFF builds on SA-MDM, but the algorithm also has an extension that is compatible with the full MDM.
Meaning that a dimension can not directly interact with itself in the model. bands can be equal or different. In the final step, events are assigned to time bands according to their time stamp.
Sequentiality: In some domains, like movies or music, users consume similar items. In other domains, like elec-tronic gadgets or e-commerce in general, they avoid items similar to what they already consumed and look for comple-mentary products. Sequential patterns can be observed on both domain types. Sequentiality was introduced in [6] and uses the previously consumed item of the user as a context for the actual item. This information helps in the charac-terizations of repetitiveness related usage patterns and se-quential consumption behavior.

Besides the traditional N-way and pairwise models, I se-lected the following preference models for experimentation.
I used five genuine implicit feedback data sets to evaluate the models in GFF. Three of them are public (LastFM 1K, [2]; TV1, TV2, [3]), the other two are proprietary (Grocery, VoD). The properties of the data sets are summarized in Table 1. The train X  X est splits are time-based: the first event in the test set falls chronologically after the last event of the training set. Artists were used as items in LastFM.
The primary evaluation metric is recall@20. The reason for using recall@N is threefold: (1) in live recommender sys-tems recall correlates well with click-through rate (CTR), an important metric for recommendation success. (2) Re-call@20 is a good proxy of estimating recommendation ac-curacy offline for real-world applications[6][10]. (3) Recall is event based, while ranking based metrics like MAP and NDCG are query based. The inclusion of context changes the query set of the test data, therefore the comparison by query based metrics is unfair.

The hyperparameters of the algorithms, such as regular-ization coefficients were optimized on a part of the training data (validation set). Then the algorithm was trained on the whole training data (including the validation set) and recall was measured on the test set. The number of epochs was set to 10, because all methods converge in at most 10 epochs. The number of features was set to K = 80 that is a good trade-off between accuracy and training time in practice. Table 2: Recall@20 values for models within GFF.
 Grey: traditional models. Bold: best results.

Table 2 shows the accuracy of two traditional models and six novel models. There exists a novel model with all five datasets that performs better than both traditional models. 4 out of 5 cases the interaction model ( U I + U SI + U QI ) is the best and it is the second best in the remaining one case. Thus this model is not only intuitively sound but also performs well that underpins its assumptions.
GFF, iTALS and iTALSx use Alternating Least Squares (ALS) during training. In ALS, feature matrices are com-puted in an alternating fashion and all but the currently computed matrix are fixed. The efficient usage of ALS with implicit problems is not straightforward, although it is pos-sible with the smart separation of computations. The com-putation of one feature matrix can be divided into three phases[6][5]: (1) computing common statistics 5 that are the same for all feature vectors; (2) feature vector specific up-date of the statistics using the training events; (3) solving a K  X  K sized system of linear equations per feature vector.
This way, the complexity of one epoch (computing each feature matrix once) is O ( N D | O | N + K 2 + P N D i =1 N + , K , N D , S i are the number of events, features, dimen-sions, different values of the attribute in the i th dimension and | O | is the complexity of the preference model. The method scales linearly with the number of events, which is very important in practice. It scales cubically with the num-ber or features, but since N D | O | N + P N D i =1 S
A K  X  K sized matrix and a vector of K length. Figure 1: Time of one epoch of ALS, CD &amp; CG with iTALS w.r.t. different number of features, using one CPU core term is dominant in the range of practically used feature numbers an thus it scales quadratically with K in practice. This scaling property is fine for smaller K values but the training takes a lot of time for high factor models. For prac-tical applicability, the training time of the algorithms is a key aspect. Faster training allows to (1) capture a more recent state of the system modeled; (2) retrain the models more frequently; (3) apply trade-off between running times and accuracy by using more features or running more epochs.
I created two approximate solutions that scale better in the number of features than ALS, but achieve similar rec-ommendation accuracy[8]. The first uses Coordinate De-scent (CD), an ALS variant in which each feature is com-puted separately while the others are fixed. The appli-cation of CD to this problem requires the efficient han-dling of the large amounts of  X  X issing X  feedback. CD does not approximate the ALS solution and has a complexity of practice for the lower range of practical K values), where N
I is the number of inner iterations. The second method uses Conjugate Gradient (CG). CG is a fast way to ap-proximate the solution of a system of linear equations with a symmetric coefficient matrix. The efficiency of CG de-pends on the efficiency of the matrix X  X ector multiplication between the coefficient matrix and a vector. In my meth-ods the feature vector specific update is done by adding a dyadic sum to the common statistics. This allows the mul-tiplication to be done in K 2 + N + j K time for the j th ture vector. The complexity of one epoch can be reduced number of features for the range of practical K values. CG approximates the ALS solution and yields the exact solution if the number of inner iterations equals to K .

Experimentation showed that ALS-CD and ALS-CG per-forms similarly to ALS in terms of recommendation accu-racy. ALS-CD was found to be unstable with N-way com-ponents in the model if the number of features is high and one of the interacting dimensions is small. The speed up achieved by ALS-CG and ALS-CD is significant: for the commonly used K = 80 the speed up to ALS is  X  3 . 5 and  X  1 . 3 respectively; for K = 200 it increases to  X  10 . 6 and  X  2 . 9. Figure 1 shows the scaling of ALS, ALS-CG and ALS-CD with the number of features.

Overall, ALS-CG seems to be the better of the two ap-proximation methods, due to its stability, speed, better ap-proximation of ALS and other properties.
My research is heavily influenced by practical considera-tions. It focuses on the implicit feedback problem and tack-les it using context-awareness and factorization. The iTALS and iTALSx algorithms solve this task efficiently. These algorithms use different preference models and are better for different datasets. The preference modeling has been unjustly neglected in recommender related research. There-fore I created GFF, a single flexible algorithm that takes the preference model as an input and computes the latent fea-ture matrices accordingly. Novel models were examined for a 4 dimensional context-aware problem using GFF. Novel models outperformed traditional ones and the interaction model performs well generally. Scalability is another impor-tant aspect in practice, therefore I proposed two solutions that enable the usage of high factor models for the implicit context-aware task. The ALS-CG method scales linearly in the number of features in practice that results in significant speed up compared to the basic ALS.

In future research I aim to add an automatic model learn-ing feature to GFF. While the flexibility of GFF is a great asset for finding new models, the users of the framework may be overwhelmed by the possible preference models, es-pecially with novel context dimensions. Although the intu-itively sound interaction model is a good starting point, it may not be the best model for all context-aware problems. Therefore it would be useful if GFF could propose a good model for any context-aware recommendation problems.
