 The Shannon/Nyquist sampling theorem tells us that in order to preserve information when uni-formly sampling a signal we must sample at least two times fas ter than its bandwidth. In many important and emerging applications, the resulting Nyquis t rate can be so high that we end up with too many samples and must compress in order to store or transm it them. In other applications, in-cluding imaging systems and high-speed analog-to-digital converters, increasing the sampling rate or density beyond the current state-of-the-art is very expe nsive. A transform compression system sparse expansion in some basis (for example, the discrete co sine transform for JPEG). By sparse we mean that only K  X  N of the basis coefficients are nonzero.
 The new theory of compressive sensing (CS) combines sampling and compression into a single sub-samples but rather inner products with M &lt; N known measurement vectors; random measurement agrees with the measurements. Research in CS to date has focu sed on reducing both the number of measurements M (as a function of N and K ) and on reducing the computational complexity of the recovery algorithm. Today X  X  state-of-the-art CS syste ms can recover K -sparse and more general compressible signals using M = O ( K log( N/K )) measurements using polynomial-time linear programming or greedy algorithms.
 While such sub-Nyquist measurement rates are impressive, o ur contention in this paper is that for of natural images. Coding this structure using an appropria te model enables JPEG and other similar algorithms to compress images close to the maximum amount po ssible, and significantly better than a naive coder that just assigns bits to each large coefficient independently. of a graphical model [3]. We use Markov Random Fields (MRFs) to represent sparse s ignals whose nonzero coefficients also cluster together. Our new model-b ased recovery algorithm, dubbed Lattice Matching Pursuit (LaMP) , performs rapid and numerically stable recovery of MRF-mod eled signals using far fewer measurements than standard algorithms.
 The organization of the paper is as follows. In Sections 2 and 3, we briefly review the CS and MRF theories. We develop LaMP in Section 4 and present experimen tal results in Section 5 using both model-based CS research in Section 6. Sparse signal recovery. Any signal x  X  R N can be represented in terms of N coefficients {  X  in a basis {  X  that x =  X   X  . We say that x has a sparse representation if only K  X  N entries of  X  are nonzero, and we denote by  X  compressible if the sorted magnitudes of the entries of  X  decay rapidly enough that it can be well approximated as K -sparse.
 In Compressive Sensing (CS), the signal is not acquired by me asuring x or  X  directly. Rather, we measure the M &lt; N linear projections y =  X  x =  X  X   X  using the M  X  N matrix  X  . In the sequel, without loss of generality, we focus on two-dimensi onal image data and assume that  X  = I (the N  X  N identity matrix) so that x =  X  . The most commonly used criterion for evaluating the quality of a CS measurement matrix is the restricted isometr y property (RIP). A matrix  X  satisfies the K -RIP if there exists a constant  X  The recovery of the set of significant coefficients  X  matrix that has the 2 K -RIP with  X  complete) and numerically unstable. If we instead use a matr ix that has the 3 K -RIP with  X  or a greedy algorithm [4]. Intriguingly, a random Gaussian o r Bernoulli matrix works with high probability, leading to a randomized acquisition protocol instead of uniform sampling. Structured sparsity. While many natural and manmade signals and images can be desc ribed to the and faster.
 tion. Figure 1(b) is a background subtracted image computed from a video sequence of a parking lot with two moving people (one image frame is shown in Figure 1(a)). The moving people form background subtraction was computed from CS measurements o f the video sequence. Background subtracted images play a fundamental role in making inferen ces about objects and activities in a scene and, by nature, they have structured spatial sparsity corresponding to the foreground innova-Probabilistic RIP. The RIP treats all possible K -sparse supports equally. However, if we incor-porate a probabilistic model on our signal supports and cons ider only the signal supports with the measurements required for stable recovery.
 We say that  X  satisfies the ( K,  X  ) -probabilistic RIP (PRIP) if there exists a constant  X  number of random measurements needed under this new criteri on; this is a direct consequence of Theorem 5.2 of [8]. (See also [9] for related results.) Lemma 1. Suppose that M , N , and  X   X  [0 , 1] are given and that the signal x is generated by a known probabilistic model. Let  X  probability that a K -sparse signal x has supp ( x ) /  X   X  If
 X  is a matrix with normalized i.i.d. Gaussian or Bernoulli/Ra demacher (  X  1 ) random entries, then  X  has the ( K,  X  ) -PRIP with probability at least 1  X  e  X  c 2 M if M  X  c c , c 2 &gt; 0 depend only on the PRIP constant  X  K .
 To illustrate the significance of the above lemma, consider t he following probabilistic model for an N -dimensional, K -sparse signal. We assume that the locations of the non-zero s follow a ho-mogeneous Poisson process with rate  X  =  X  log(  X /K ) N  X   X  , where  X   X  1 . Thus, a particular determine the size of the likely K -sparse support set  X  the second coefficient is among the next N  X  indices immediately following the first location with fixed, we have that the j th non-zero coefficient is among N  X  candidate locations with probability M = cK (1 +  X  log N ) rows, as compared to the cK log( N/K ) rows required for the standard K complexity of the solution method grow essentially linearly in K , which is a considerable improve-ment over the best possible M = O ( K log( N/K )) measurements required without such a priori information. by a probabilistic graphical model such as a Markov random field (MRF); in this paper we will focus for concreteness on the classical Ising model [10].
 Support model. We begin with an Ising model for the signal support. Suppose w e have a K -sparse signal x  X  R N whose support is represented by s  X  X  X  1 , 1 } N such that s s = 1 when x i 6 = 0 . The probability density function (PDF) of the signal suppo rt can be modeled using a graph G of the support indices  X  and E neighbors (see Figure 2(a)). The contribution of the intera ction between two elements { s the support of x is controlled by the coefficient  X  controlled by a coefficient  X  so that it integrates to one. The parameter vector  X  quantifies our prior knowledge regarding the resulting coefficients, (c) Markov random field with CS measu rements. signal support s and consists of the edge interaction parameters  X   X  . These parameters can be learned from data using  X  real background subtracted image in Figure 1(b) with the dis persed  X  X ndependent X  sparsity of the model (  X  image (c).
 Signal model. Without loss of generality, we focus on 2D images that are spa rse in the space domain, as in Figure 1(b). Leveraging the Ising support mode l from above, we apply the MRF graphical model in Figure 2(b) for the pixel coefficient valu es. Under this model, the support is controlled by an Ising model, and the signal values are indep endent given the support. We now We begin with the support PDF p ( s ) from (2) and assume that we are equipped with a sparsity-is related to the  X  are related to the  X  y noise variance.
 measurements y are independent using the D -separation property of graphs [13]. Hence, the joint distribution of the vertices in the graph in Figure 2(b) can b e written as where z = [ s T , x T , y T ] T . Then, (3) can be explicitly written as Using the coefficient graphical model from Section 3, we are n ow equipped to develop a new model-based CS signal recovery algorithm. Lattice Matching Pursu it (LaMP) is a greedy algorithm for required number of CS measurements and increasing the numer ical stability.
 Algorithm. The LaMP pseudocode is given in Algorithm 1. Similar to other greedy recovery al-gorithms such as matching pursuit and CoSaMP [4], each itera tion of LaMP starts by estimating a residual, LaMP calculates a temporary signal estimate (Ste p 2) denoted by x { k } Using this temporary signal estimate as a starting point, La MP then maximizes the likelihood (4) over the support via optimization (Step 3). This can be effici ently solved using graph cuts with Algorithm 1: LaMP  X  Lattice Matching Pursuit Input : y ,  X  , x { 0 } = 0 , s { 0 } =  X  1 , and e K (desired sparsity).
 Output : A e K -sparse approximation x of the acquired signal.
 Algorithm: repeat { Matching Pursuit Iterations } until Maximum iterations or r { k } &lt; threshold ; Return x = x { k } . e K controls the sparsity of the approximation. In Step 4, a conj ugate gradient method is used for between the signal values x algorithm to efficiently solve for the signal values x { k } within Step 4.
 problem-dependent. Here, we provide one approximation tha t mimics the  X  tion 5. The state s have equal probability, and the value x s =  X  1 represents a zero-valued coefficient; thus, the mass of its p robability function is concen-trated at zero. Hence, we use the approximations for x against noise and numerical issues; and ( ii ) to extend the usage of the algorithm from sparse to compressible signals.
 We approximate log p ( x constant  X  is a slack parameter to separate large and small signal coeffi cients, and  X  chosen according to  X  and L to normalize each PDF. We also denote a =  X  the normalization constraints, it is possible to show that a s the dynamic range increases, Hence, we approximate the likelihoods using the utility fun ctions U The optimization problem used by Step 3 of LaMP to determine t he support is then approximately equivalent to the following problem where e  X  =  X  be changed to enforce the positivity during estimation. The choice of e  X  sparseness on the lattice structure.
 the 2D Ising model and choose e  X  magnetization. In our recovery problem, the average magnet ization and the desired signal sparsity has a simple relationship: m = threshold; this gives preference to the largest 5 e K coefficients that attain states s then enforces the desired sparsity e K . which have high likelihood under our MRF model, LaMP require s far fewer measurements and fewer computations for robust signal recovery than state-o f-the-art greedy and optimization tech-Experiment 1: Shepp-Logan phantom. Figure 4 (top left) shows the classical N = 100  X  100 Shepp-Logan phantom image. Its sparsity in the space domain is K = 1740 . We obtained compressive measurements of this image, which were then imm ersed in additive white Gaussian noise to an SNR of 10dB. The top row of Figure 4 illustrates the iterative image estimates obtained using LaMP from just M = 2 K = 3480 random Gaussian measurements of the noisy target. iteration.
 Figure 4 (bottom) compares LaMP to CoSaMP [4], a state-of-th e-art greedy recovery algorithm, and fixed-point continuation (FPC) [17], a state-of-the-art  X  ing the same set of measurements. Despite the presence of hig h noise (10dB SNR), LaMP perfectly recovers the signal support from only a small number of measu rements. It also outperforms both CoSaMP and FPC in terms of speed.
 Experiment 2: Numerical stability. We demonstrate LaMP X  X  stability in the face of substantial measurement noise. We tested both LaMP and FPC with a number o f measurements that gave close to perfect recovery of the Shepp-Logan phantom in the presen ce of a small amount of noise; for LaMP, setting M = 1 . 7 K suffices, while FPC requires M = 4 K . We then studied the degradation demonstrate that LaMP is stable for a wide range of measureme nt noise levels. Indeed, the rate of to noise) is comparable to that of FPC, while using far fewer m easurements.
 Experiment 3: Performance on real background subtracted im ages. We test the recovery algorithms over a set of background subtraction images. The images were obtained from a test video sequence, one image frame of which is shown in Figure 1, by choosing at random two frames images are spatially clustered and thus are well-modeled by the MRF enforced by LaMP. We created 100 different test images; for each image, we define the spars ity K as the number of coefficients Noise-free target LaMP Iter. #1 LaMP Iter. #2 LaMP Iter. #3 La MP Iter. #4 Figure 4: Top: LaMP recovery of the Shepp-Logan phantom ( N = 100  X  100 , K = 1740 , SNR = 10 dB) from M = 2 K = 3480 noisy measurements. Bottom: Recoveries from LaMP, CoSaMP, and FPC, including running times on the same computer. FPC and CoSaMP require M &gt; 5 K to achieve the same performance. that contain 97% of the image energy. We then performed recov ery of the image using the LaMP, CoSaMP, and FPC algorithms under varying number of measurem ents M , from 0 . 5 K to 5 K . An example recovery is shown in Figure 6.
 For each test and algorithm, we measured the magnitude of the estimation error normalized by the magnitude of the original image. Figure 5(b) shows the mean a nd standard deviations for the nor-malized error magnitudes of the three algorithms. LaMP X  X  gr aphical model reduces the number of measurements necessary for acceptable recovery quality to M  X  2 . 5 K , while the standard algo-rithms require M  X  5 K measurements to achieve the same quality. We have presented an initial study of model-based CS signal r ecovery using an MRF model to cap-for signals conforming to our model, the resulting LaMP algo rithm requires significantly fewer CS measurements, has lower computational complexity, and has equivalent numerical stability to the modern compression and data modeling methods for CS reconst ruction.
 Much work needs to be done, however. We are working to precise ly quantify the reduction in the required number of measurements (our numerical experiment s suggest that M = O ( K ) is sufficient to formulating inference problems in the compressive measu rement domain since in many signal a detection or classification decision. Acknowledgements. We thank Wotao Yin for helpful discussions, and Aswin Sankar anarayanan for data used in Experiment 3. This work was supported by gran ts NSF CCF-0431150 and CCF-0728867, DARPA/ONR N66001-08-1-2065, ONR N00014-07-1-09 36 and N00014-08-1-1112, AFOSR FA9550-07-1-0301, ARO MURI W311NF-07-1-0185, and th e TI Leadership Program.
