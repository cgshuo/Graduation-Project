 Axel Blumenstock  X  Franz Schweiggert  X  Markus M X ller  X  Carsten Lanquillon Abstract With the complexity of modern vehicles tremendously increasing, quality engineers play a key role within today X  X  automotive industry. Field data analysis supports corrective actions in development, production and after sales support. We decompose the requirements and show that association rules, being a popular approach to generating explan-ative models, still exhibit shortcomings. Interactive rule cubes, which have been proposed recently, are a promising alternative. We extend this work by introducing a way of intuitively visualizing and meaningfully ranking them. Moreover, we present methods to interactively factorize a problem and validate hypotheses by ranking patterns based on expectations, and by browsing a cube-based network of related influences. All this is currently in use as an interactive tool for warranty data analysis in the automotive industry. A real-world case study shows how engineers successfully use it in identifying root causes of quality issues. Keywords Rule cubes  X  Subgroup discovery  X  Descriptive induction  X  Causal analysis  X  Interactivity 1 Introduction Taking action requires knowledge about cause X  X ffect relationships. Such knowledge predicts what results actions will probably have, and helps to decide among several action options.
This very general statement holds in our industrial domain, too. Vehicle engineers keep on improving product quality and reliability through careful design and elaborate tests. However, this is not always enough. In real vehicle life, quality issues may occur that could not be foreseen. Analysis of field data X  X redominantly, this is warranty data X  X romises valu-able knowledge about how to make vehicles even better. Causality plays the key role. Only if engineers know what really generated a quality issue, they can devise actions that precisely address it. This contrasts to more common data mining applications, which realize some kind of detection or prediction without referring to the notion of causality.

While causality is so important, data analysis alone cannot provide it. Causal induction is not complete and based on strong assumptions that do not hold in applications where available dataonlyscratchesthesurfaceofreal-worldaffairs.Theentireprocessofcausalinvestigations comprises much more: delineating the case, finding hypotheses, and knowledgeable deci-sions about ways of gaining further information to support these hypotheses X  X e it designing experiments to understand the causal chain behind the case. It is actually the experts who build causal models in their minds. Some of their tools may be application specific (e.g., sim-ulators), and one of them is our data analysis system, providing the experts access to valuable information. The possibly most important meta-goal to draw from these thoughts is to make data analysis for causal investigations most open, simple, easy-to-use, and understandable. Interaction is a simple but unparalleled means of incorporating background knowledge on the one hand, and help users understand and take their findings to practice on the other.
The experts X  ultimate goal with respect to data analysis could be formulated as to have a fast and reliable way of finding what ( and how ) to change within a set of reachable influence variables so that the probability of quality deficiencies is reduced at the best . Keeping in mind that it would not be reachable as such, we make our best efforts to come close (Fig. 1 ).
Considering vehicles as instances, we encode occurence of a certain quality issue as a class variable, and take vehicle configuration and other information as independent variables. Then the task decomposes to several subgoals: 1. Unrestricted search. If the answer is not available by direct calculation, some kind of 2. Focusable search. If you point your flashlight to the ceiling, so that the entire room 3. Problem-adequate pattern validation. In our key search metaphor, one would want to 4. Ease of use and understanding. To transport the findings into the users X  minds, and With the basic scheme of interactive and iterative search settled, many degrees of freedom still remain. A primary decision is about model type, which determines much about how good the goals above can be met.

Model types evolved historically (Fig. 1 ). Our presumptions for any of these are shown at the top: a class variable, several potential influence variables, and a frequency distribution over these. 1 The other end of the figure shows the ultimate but unreachable goal of getting user-readable statements about the causal impact of the given variables. Many approaches of feature selection, regression or decision trees for modelling roughly boil down to taking indi-vidual variables as the hypotheses and ranking them. The observation that influence variables interact and hence the need for multivariate modelling has been addressed by rules-based approaches. Making use of association rules became popular not only due to the existence of efficient mining algorithms. To some extent, rules also fulfill the goals above. In the same time, association rules raised the need for extensive post processing to reduce redundancy and igate the shortcomings that originate in the model type itself (Sect. 3 ). Adding these up, users hardly work productively with rules in the application of interactive causal investigations. Rule cubes have been proposed as an alternative model type [ 22 ].

Our contributions in this paper are the following:  X  We review the shortcomings of association rules in serving the goals at hand (Sect. 3 )and  X  We extend the work of [ 22 ] by introducing a way to rank all rule cubes, especially con- X  We discuss how to process numeric attributes and categorical values with large domains  X  After outlining some use cases of interaction in Sect. 8 we re-state two methods to imple- X  As a further application of relative interestingness, we devise a simple way of using cubes The methods described in this paper have been cast into a productive tool, which is success-fully applied by our domain experts to solve real-world problems.

Undoubtedly, there is still much room for further development. The process of generat-ing actionable knowledge from raw data has evolved from a long tradition within the KDD community. In a way, we just adapt this process in several aspects: model choice, pattern validation, and presentation. We claim that such tuning makes our users X  work of improving product quality more effective. While this claim is hard to evaluate, we wish to give an idea of it by describing an analysis session in Sect. 11 . 2 Related work For us, the primary source of inspiration was the work of [ 22 ] on rule cubes. Their appli-cation task is to find causes for phone call failures at Motorola. They observed that experts never looked at rules with three influence factors or more. With this restriction to at most two influence variables per rule, they mine the entire set of rules without support constraints, and present the underlying distributions using bar charts. Additionally, they generate more abstract descriptions ( X  X eneral impressions X ). Cubes can be ranked, but only those with a singular influence variable.

They were not the first to display rules in context. Mosaic plots and double decker plots are other visualizations of discrete distributions [ 8 ]. Here, it is more difficult to spot which tiles are semantic neighbors, and they do not help with selecting an interesting attribute set.
Referring to prior knowledge is a very widespread means for a more focused pattern discovery, even in applications such as clustering [ 6 ]. The more specific context of rank-ing explanative patterns has been studied by still many groups. Padmanabhan et al. [ 13 ] propose putting expectations as rules and inducing further rules that confirm or contradict these expectations textually. Jaroszewicz et al. [ 9 , 10 ] make use of the maximum entropy principle to derive expected itemset supports from subrules or Bayesian networks. Several studies, originally [ 19 ], exploit taxonomic or partonomic hierarchies in attribute domains to automatically create expected support and confidence values and use them for pruning rules and spotting exceptional rules.

Sequential covering algorithms refer to some kind of prior knowledge as well: patterns discovered earlier. They incorporate this knowledge by removing or downweighting covered examples. However, this does not suit induction of exhaustive descriptions, because dissimi-lar patterns can be indistinguishable on the instance level. Furthermore, removal of instances can erode interesting structures beneath. Scholz devised a resampling approach to address this [ 16 ]. 3 Using rules for causal investigations Association rule mining for causal investigations fulfills our application requirements to a grates [ 9 , 17 ] background knowledge ( 2b ). In some respects, however, there are shortcomings that deserve improvement:  X  Goal 1a states that neither the hypothesis language nor search algorithms should impede  X  In our application, patterns are interpreted by humans. Hence, we should assess them  X  Rules are widely considered understandable. There is no doubt about that for a rule itself.  X  Understandability is strongly determined by pattern visualization [Goal 4b ]. How can 4 Cubes A rule cube is a discrete frequency distribution (contingency table) over a small set of vari-ables, among which is the class attribute. Rule cubes are not to be confused with OLAP cubes. (Zhao et al. (2006) still propose OLAP -style operations for exploring each individual rule cube.) An OLAP cube is not a model but a mere data representation to allow for fast and easy exploration. Rule cubes are patterns that can take on the role of rules. Each cube states that its variables caused the problem at hand. Our ultimate (with respect to causality yet unreachable) goal is to assess patterns according to how valid their statement is.
The pattern set is as follows. Let I be the set of all potential influence variables, C the class variable and B  X  I a fixed set of variables we want to use as priors later (see Sect. 9.1 ). We generate the respective instance distributions over the attribute sets V = A  X  X  C } X  B for all
A  X  I \ B with | A | X  X  1 , 2 } .Theseare | I \ B |+ 1
There are various visualization techniques for multivariate multinomial frequency distri-butions X  X s cubes are. The majority of them is bar charts: nested [ 22 ], 3D or double decker plots [ 8 ]. Another way are area partitions (tree maps). Facing the necessity of comparing distributions vector-wise, as well as extremely skewed distributions (particularly of the class variable), these visualizations expose weaknesses. Exploiting that our class variable is binary (one class of interest) and part of every pattern, we encode its distribution by a color scale, freeing both area dimensions for two influence variables.

Figure 2 shows an example. Each cell corresponds to a contingency table cell (when marginalized over the class variable). The area of the block therein is proportional to the coverage of the respective rule, while its lift (class distribution) determines the color, taken from a continuous scale from green (for lifts near zero) via yellow to red (for lifts far greater than one). Comparing block sizes vector-wise (i.e., between rows or columns), one can easily see that the influence variables interact strongly. Furthermore, it is immediately clear that the positive concentrate around the slot for B = 2, while A is irrelevant.
Compare this to what an association rule miner would generate from the same data (with a minimum lift of 1 and the consequent fixed to C =+ ). In this most simple case already, the roles of A and B are not obvious: 5 Generating cubes Zhao et al. [ 22 ] generate cubes by reorganizing the output of a class association rule miner. Section 11.2 shows that there is no runtime benefit in this intermediate step. Direct count-ing on the data base is just as good. Counting time can be reduced further by adaptively marginalizing cubes from higher dimensions.

Figure 3 shows how for eight influence variables A ,..., H the 8 2 cubes with two variables each can be marginalized from a single 8-dimensional cube. (The class variable adds to each and is omitted here for clarity.) Moving up to even higher dimensions reduces the number of database scans but raises marginalization costs exponentially so that a trade-off must be made. To this end, our counting algorithm enumerates all pairs { A , B } X  I \ B , and if the cube for a given pair does not yet exist, it calls createCube ( { A , B } ). function createCube ( A ) sizeLimit is the data base size times a relative cost factor.
 6 Absolute cube interestingness 6.1 Cell interestingness We first consider individual cube cells. A cell corresponds to a rule, or a subgroup. Under Goal [ 3a ], a reasonable subgroup quality measure should grow if recall increases or coverage decreases, other parameters respectively fixed. These properties are known as Piatetsky-Shap-iro X  X  claims on good rule measures [ 14 ].

Many rule quality measures can be cast as a comparison 2 between an observed fre-contingency table cell count. By default, E V just reflects statistical independence of the vari-ables in V . Each p i is a predicate (a conjunction of variable-value pairs) that selects the examples falling into the respective cell. For example, the well-known rule measure lift for an association rule  X   X   X  can be written as n p e
The rule measure of our choice is When squared, i ( p ) becomes a summand in the  X  2 statistic. It thus complies with Piatetsky X  Shapiro X  X  criteria.
 Example 1 Consider the case with a single influence variable A with dom ( A ) ={ 0 , 1 } and a class variable with dom ( C ) ={+ ,  X  X  . N AC (short for N { AC } ) thus is: where a predicate like ( A = 1 )  X  ( C =+ ) is just denoted 1 + for readability. Our  X  X nter-estingness X  measure of a rule ( A = 1 )  X  ( C =+ ) can then be calculated as i ( 1 + ) where e + = n 1 n + n . The lift of this rule is n p e 6.2 Cube interestingness To assess and rank cubes [ 22 ] only consider cubes with | A |= 1, i.e., univariate influences. These cubes are assessed according to either discriminative power (weighted sum of cell assessments) of an attribute, or strength of a trend within its domain. Since, we include bivariate influences into our ranking as well, we take a different approach (Sect. 6.3 ).
As the first idea to assess a cube over V , we simply take the maximum of the cell assess-ments: For later reference, we abbreviate this interestingness-targeted measure of divergence of frequency distributions as d i . Using the maximum therein ensures that the best subgroup determines the best cube. Note that this approach is crucial to focus on the task of subgroup discovery and description rather than classification. 6.3 Assessing according to pattern intension In classic subgroup assessment, measures are calculated no matter how many influence vari-ables constitute the antecedent predicate p .Forarule ( A = a )  X  ( B = b )  X  ( C = c ) , e abc is calculated as n ab n c n . This is fine in applications focusing on instance subsets them-selves (e.g., classification). For causal analyses however, we must look into the descriptions (see Goal 3b ). The interestingness of a pattern about two influence variables should reflect whether the class shift originates from one variable only, from the two independently, or from the interaction of both. A pattern with two variables actually states a joint influence. In the first two cases, however, this statement is wrong, because there is no such joint influence but only a superposition. Hence this pattern is uninteresting. The idea of rating a joint influence only according to its additional value over its components is quite common in other fields, such as analysis of variance. What we do, is to transfer this practice to cubes.
Recall that interestingness measures share the idea of comparing some observed value to the respective expectation as taken from a model-based distribution. The model behind e abc = n ab n c n is that not only the joint influence A  X  B is independent of C , but also its individual elements A and B .

This is not the only model we can choose. A more appropriate one would reflect that A and B have been assessed individually elsewhere and thus take the dependences in A  X  C and B  X  C into account, hence, the marginals N AC and N BC in addition to N AB . Constructing the three-dimensional E ABC from three two-dimensionals produces a still under-determined system of equations. The maximum entropy principle states to choose from the solution space the (unique) distribution with greatest entropy H , or semantically equivalent, the least com-mitment. In general, this solution can only be approximated numerically. We use generalized iterative scaling [ 3 ] for that. In other words, we replace the expected distribution E V in Eq. 2 by where the D S are all proper marginal sums of the frequency distribution D , which are claimed to fit the respective observed distributions N S . For two-dimensional contingency tables, the resulting distribution exactly reflects statistical independence. 7 Processing attribute domains Above, we described our approach to assess cube interestingness assuming that the data is split into cell constituting subgroups according to the values the instances take on for any attribute under consideration. As we use the maximum cell interestingness to assess cube interestingness, it is essential to have meaningful subgroups of reasonable size. In this respect, we want to address issues that arise when dealing with large categorical domains and continuous attributes. 7.1 Problem description In contrast to small categorical domains both large and continuous domains have a higher risk of yielding undesired cell artifacts due to singletons, outliers or, in general, very small subgroups for some of the values. Even though our cube assessment combines aspects of both lift and coverage into a single measure, extremely skewed cube distributions may still result in unacceptable cube assessment. Moreover, in many situations the mere multitude of resulting cells may render a model unreadable. With more than five to eight subgroups, model readability already deteriorates. As a consequence, when dealing with large categori-cal or continuous attribute domains, appropriate domain partitioning by means of clustering (or: grouping) of values is necessary. We consider any resulting domain partitioning for both categorical and continuous attributes as a system generated default which may be edited inter-actively by the users based on their background knowledge so as to achieve more meaningful subgroups.

The following addresses only univariate domain partitioning. Currently, we use partitions attained that way for joint distributions as well. A more elaborate approach would be to find an optimal partitioning based on the multivariate influence of each attribute combination. Practical examples show that multivariate partitioning is highly valuable because meaning-ful subgrouping depends on context. In fact, it is crucial to consider multivariate domain partitioning in order to fully exploit the potential of multivariate data analyses. Yet, the downside is that the users may easily be confused when confronted with different groups of values for the same attribute in different combinations. This is an issue that requires further attention and research. 7.2 Categorical attributes with large domains Let us focus on the number of cells resulting from categorical attributes first. By default, each attribute value will result in one cell. How do we find adequate groups of values? We pur-sue two approaches: grouping based on information about some semantic structure within the attribute domain, and grouping of unordered domain values without any background knowledge. Still, for either approach, overall cube quality will be determined by the cell representing the best partition.

To be used automatically, any domain structure must be explicitly modeled and provided prior to the analyses. For some attributes, structural information is available from the under-lying data warehouse. For example, the values of a sales area attribute can be subjected to several partitionings according to, e.g., geographic entities, weather conditions, or legal stan-dards. We take advantage of this structural information to derive value clusters. In case there are even hierarchical structures, each level of the hierarchy will be considered and evaluated separately.

If neither structural information nor even some natural domain order is available, we auto-matically partition an attribute X  X  domain above a specified size as follows. After assigning a cell to each possible attribute value, simply sort the attribute values by lift. Preserving this order, cluster the cells using several heuristics: first, merge smallest cells with their nearest neighbors. This increases robustness of the approach with respect to small cells. Then, pro-ceed in agglomerative hierarchical clustering manner: merge adjacent cells with lowest cell interestingness difference until the desired number of cells is reached. This approach allows efficient and reasonable grouping of even unordered domains without the burden to search the entire power set space and is a significant step towards Goal 1a . It also mitigates the common knowledge acquisition bottleneck with regard to modeling domain structure for all relevant attributes prior to analyzing the data.

In case there are partitionings both with and without background knowledge, the latter one will not be presented to the users unless it significantly improves cube quality. This is because automatic partitioning is less likely to produce structurally sound clusters. 7.3 Ordered and continuous attributes For association rules, handling continuous attributes is not easy. Discretizing them prior to induction risks choosing bins either too large (thus missing interesting subgroups) or too small (thus failing the minimum support threshold). Various mitigations have been proposed, such as context-aware discretizations [ 11 ], or avoiding discretization by numeric association rules.

Rule cubes require discretization but offer another way out: with the minimum support limit dropped, we can apply a fine-grained, minimally committing discretization prior to the cube generation and use the procedures from above to localize good subgroups within large categorical domains. Of course, however, continuous attributes have natural domain orders, which must be preserved. Similar to above, we need to find a small number of reasonable split points. In contrast to above, the lift may vary arbitrarily along the ordered domain values.
What we adapt is well-known from decision tree analysis as the optimal multisplitting problem [ 4 ]. The optimal partition in the context of rule cubes is the one that maximizes the cube interestingness i ( V ) .As i ( V ) is defined as the maximum of all cell assessments (see Sect. 6.2 ) we can reduce the problem to a search for the optimal binary or ternary split that maximizes the interestingness for one of the intervals. In this case, time complexity is qua-dratic with the number of possible split borders ( O ( n 2 ) ). Preprocessing to reduce the number of possible cut points by combining values can be done in linear time. Focusing on boundary points and segment borders rather then testing any cut point can improve processing time drastically [ 5 ].

As performance is very important for an interactive application, we choose a different approach. We get a very good approximation of the optimal partition if we apply Auer X  X  linear-time optimized algorithm for multi-splitting of numeric variables [ 5 ]. The idea is both simpleandefficient:Thealgorithmminimizesthe X  X rainingSetError X ( TSE ),whichisdefined as the sum of misclassified elements. As the class distribution is very skewed, we reweight the instances to make it uniform. TSE is zero if every element is put into a bucket of its own and labeled according to the element X  X  class. TSE is large (not necessarily the maximum though) 50% of all weight is misclassified. By setting a maximum number of buckets allowed X  X n practice we use the arity 3 X  X he algorithm optimizes the size of regions with a specific class variable value. As the underlying ranking metric (i.e., the weight of lift and coverage in the calculation of the rank) is slightly different from i ( V ) ,the TSE algorithm might output a the deviation is very small, though. The complexity of the algorithm for the case of a binary class variable and an arity of three is O ( 6 n ) [ 5 ]. This efficiency simply outweighs the fact that its underlying metric is not exactly equal to i ( V ) . 8 Interaction model So far, we have described how we generate, rank and display a type of pattern named cube . But all this theory does not come to life until we think about how to involve the user. Suppose the user is provided a list of ranked patterns X  X ust like a list of web search results. He will start to read it top-down. This is reasonable because the top-ranked patterns name those variables that most likely have something to do with the class, and this order was created to the best of our current knowledge , which is nothing but the initial frequency distribution. What may happen now?  X  The user finds an influence he knows and expects. This may happen because some action  X  The user finds an influence that he accepts as causal and actionable X  X ossibly after some  X  The user finds an influence he suspects to be noncausal and only  X  X ushed X  by interaction All these reactions require that the user can communicate something back to the system, and his message is that one or more variables should now be considered as  X  X nown X . Effecively, this is Goal 2b .

Several approaches to this have been proposed. The maybe most simple one is to apply a syntactic filter and remove all patterns that contain the known variables (e.g., [ 15 ]). Obvi-if they are similar. Additionally, in the case of cubes we note that A and A  X  B are completely different and independent statements, and the second must not be suppressed if the first is.
Another approach is to condition on certain values of the given variables. This is what happens within common multivariate pattern types, namely rules and decision trees. The drawback is that partition sizes often become too small for reasonable statistics. Moreover, the effect of other influences should be assessed considering all values of the priors, not just one.

In our view, the better way of incorporating user feedback about  X  X nown X  influences is to devise a relative interestingness measure. We can use it to re-assess all patterns such that all those related to the given knowledge are rated low. While the idea of such a relative (or:  X  X ubjective X ) assessment is old [ 17 ], and the interactive use cases above show that there is a strong practical need for it, to the best of our knowledge this need has still not been responded to in a satisfactory manner. 9 Quantifying relative interestingness Here,  X  X nowledge about influences X  simply refers to the their frequency distributions. It can be drawn from the data itself and does not place heavy modeling burdens on the user. This is an important requirement.

In a similar approach, Jaroszewicz et al. [ 9 ] devised a framework that lets the user model a Bayesian network structure, draws the relevant distributions from the data and assesses the patterns based on this network. While this is a universal and clever approach, it still has some drawbacks to us:  X  Deciding about causal direction X  X s necessary for Bayesian nets X  X s not always possible  X  A network comprising all real variable interactions is intractable.  X  We double-use relative interestingness as a means to find a  X  X ariable neighborhood X  We denote the relative interestingness of variable set (or: pattern) A with respect to class var-iable C and under incorporation of the  X  X nown X  variables (or: priors) B as i ( A , C | B ) .To begin with, we consider only local network structures and put up the following requirements on i : R : i ( A , C | X  ) = i ( A , C )
This means that relative interestingness should be a continuous extension to absolute interestingness.
 R : i ( A , C | B ) = 0iff A  X   X  C | B .

This means that relative interestingness should serve as a generalization (or:  X  X oft replace-ment X ) for the conditional independence test.
 R : i ( A , C | B  X  B ) = i ( A , C | B ) if A  X   X  B | C .

This means that relative interestingness should not change if we add a prior that is inde-pendent of A given C . (Conditioning on C is essential. Simple independence of A and
B is not affected if we change class distribution within their intersection. This however means a strong change of how these influences interact towards the class.) We will now discuss two different methods to quantify relative interestingness. 9.1 Method 1: conditioning the data This is the classic way as used in, e.g., Bayesian networks: to get rid of an influence, condition on it.

We denote the relative interestingness as calculated by this method with i cond ( V | B ) (where V = A  X  X  C } ). It can be calculated in a similar way as in Eq. 2 by comparing observed and expected distributions, both extended by one dimension for each variable in B . where the expected distribution is built from marginals in which the variables used as priors (
B ) are always present: Discussion It is obvious that this kind of relative interestingess meets requirements R 1 (abso-lute interestingness is just a special case) and R 2 by construction ( i is zero iff N and E are equal, which holds iff for every value vector of BA and C are independent).

Unfortunately,itfailsrequirement R 3 .Thiscanbeshowneitherbysimplenumericcounter-examples, or on the following grounds: comparing some absolute and relative interestingness would mean here to compare magnitudes drawn from two distributions of different dimen-sionalities. This will hardly work, because we can add an unlimited number of independent priors to B , thereby scattering instances over more and more cells. Sooner or later, this will break a reasonable interestingness calculation.

Apart from failing R 3 , method is disadvantageous in another respect: distribution sizes grow exponentially in the number of priors, so that elimination of few influences already renders calculation infeasible. These difficulties made us ask for a better method. 9.2 Method 2: reweighting instances In this approach, we only change instance weights, but neither the interestingness calcula-already met.

To eliminate one influence B , weights should be modified such that B and C become independent. Claiming that marginals N B and N C must not change, [ 16 ] showed that there is a unique solution. Put in our words: where for all b  X  dom ( B ) and c  X  dom ( C ) : N AC is then just marginalized from N ABC ,and E is calculated from N like above (Eq. 3 ). The lift ( B = b  X  C = c ) = n bc n n is independent of C . Therefore, with the weights modified, B and C are independent by construction.

Scholz proposed this technique as a better alternative to sequential covering, i.e., to induce a rule, remove the positive examples covered, and re-iterate. By analogy, we could term this method lift covering . In our practical application, two extensions were necessary.
First, we note that this method does not only work for a binary B (considering a rule antecedent as a binary predicate), but for arbitrary categorical variables. Moreover, it is not limited to relations with the class variable (like B  X  C ), but it can incorporate any partial distribution.

Second [ 16 ], does not consider what happens when eliminating more than one depen-dence, say A  X  C and B  X  C . Of course we could consider all priors as a single big variable, which is the cross product of all, and eliminate them at once. This would entail the same size problems as above.

If we eliminate them sequentially by Eq. 7 ,and A is not independent of B (given C ), some dependence between A and C will reappear after handling B  X  C . Luckily, we can mitigate this by iterating cyclically through all influences to be eliminated, and the weights will converge to a distribution in which all these have indeed gone. Sketch of proof: lift covering is equivalent to fitting the distribution over all variables to the unique marginal over B  X  C which states the independence of these two. This reduces lift covering to iterative pro-portional fitting, proven to converge [ 3 ]. Note that iterative proportional fitting (also known as generalized iterative scaling) does not presume the initial weights to be unity, but only non-zero.
 Discussion The most interesting property of lift covering however is that it meets require-ments R 2 and R 3 .

For R 2 , we must show the equivalence of statistical indepencence of A and C given B (
A  X   X  C | B ) and the independence of A and C after elimination of B  X  C by lift covering (i.e., i weight ( A C | B ) = 0). By writing a  X  dom ( A ) below, we mean that a is a value vector from the cross product over all domains of the variables in set A . We presume that the class variable C is binary with dom ( C ) ={ X  , } .Be n p the original number of instances that match some arbitrary predicate p ,and n p the respective number after removal of B  X  C by lift covering. Note that n = n and n c = n c for all c  X  dom ( C ) .

The following is claimed true for all a  X  dom ( A ) and c  X  dom ( C ) .
Without loss of generality we can now set c = X  . Otherwise, the subtraction would just apply to the other summand. Note further that n = n  X  + n . Since, the inital equation must be true independent of any n b , we can conclude by coefficient comparison, that each summand is zero. It one were not, we could twist the respective n b to render the inital equation false. Hence,
For R 3 , we presume that A  X   X  B | C , i.e. n abc = n ac n bc n above. We observe: After marginalization, n ac = n ac , that is, elimination of B did not modify anything, and interestingness stays the same.

Unfortunately, method 2 has a serious drawback as well: if at least one n bc is zero, elim-ination is impossible. Even if some lifts are only near zero, the reweighted data set will contain instances with extraordinarily high weights, which jeopardizes induction of anything reasonable. Sometimes, empty cells are domain artifacts that can be remedied, e.g. by leaving out the respective instance set partitions, or by rescaling attributes. Still, we must note that method 2 cannot be applied universally either.

We conclude this section by illustrating both methods 1 and 2 based on a simple experi-ment. Given three binary variables A , B and C (the class variable), the distribution over them has eight degrees of freedom. By fixing the marginals over A  X  C and B  X  C (and thus the strength of the singleton influences), only two degrees of freedom remain. They specify how much A and B interact. We plotted the relative interestingness of AC given B over these interaction parameters (Fig. 4 ).

Not that these  X  X unnels X  reach zero at the same point, which is exactly where A  X   X  C | B , hence fulfilling requirement R 2 . Apart from this, they agree only qualitatively, yielding the higher a value the further the conditional independence point is. 10 Variable neighborhoods Actionability has been associated with unexpectedness [ 17 ]. Yet even more essential for deciding about actions is causality. Inference of causal relations requires strong assumptions [ 18 ] that do not hold in our real-world application. In particular, we cannot assume causal sufficiency: There may be some hidden influence on two or more of the visible variables.
What works for sure however is ruling out a direct causal link between variables A and C in case A  X   X  C | B for any B [Fig. 5 ]. This decision about conditional independence is usually based on a statistical test with some arbitrary error probability. To better reflect the vagueness of attribute interaction, it is desirable to incorporate at least this kind of causal validation into a continuous interestingness measure. Besides, such a causality-aware interestingness would fit better into Goals 1c and 2a of assessing and ranking all patterns. However, there are problems:  X  With our data failing the causal sufficiency condition, we are not aware of any method  X  If both A  X   X  C | B and B  X   X  C | A hold, A and B can be considered (roughly)
We therefore take a pragmatic approach. Recall that the relative interestingness measures from above are soft replacements for the conditional independence test. So we define a directed suppression strength as This reflects how much A vanishes when knowing B . Note that at this point requirement R 3 plays the important role to make absolute and relative interestingness comparable. In particular, they are equal in case B is unrelated to A (given C ).
 For each variable B we define its neighborhood as all other variables A ,rankedby s C ( B  X  A ) + s C ( A  X  B ) . Following the argument above, if the first of these summands is much greater than the second, we deem B a possible cause of A , and vice versa. If the summands are roughly equal (e.g., within a range of 2:3), we deem the influences congeneric. That way, for each variable, there are three categories of neighbors. Though theoretically not fully sound, they turn out quite meaningful in practice, as can be seen in our case study below (Fig. 8 ). 11 Evaluation 11.1 Search depth In our domain, influence variables interact strongly. Our users thus need multivariate anal-yses. When they create models based on decision trees or rule sets, they even dig to depths of four conditions and more. This raises questions what we lose by constraining search to combinations of only two influences.
 We observed that in many cases some of the conditions in trees or rules had been  X  X bused X : When users recognized that an issue divided into several subproblems, they added restric-tions to better focus the problem. However, these conditions were not part of the final causal explanation.

Our cubes implementation separates these two usages of conditions. It allows for an unlim-ited number of such focusing restrictions, taking this burden from the cube-based search for causal influences. Recall that a cube over two influences states that these two really inter-acted to cause the issue. In only three of eight reviewed cases, the final explanation referred to such interaction of two variables, and in all cases things were already apparent among the one-dimensional projections. Therefore, we are confident that in practice, a search depth of two suffices, and constellations like Simpson X  X  paradox are rather rare. 11.2 Computational expenses of counting Association rule algorithms [ 7 ] heavily rely on support-based pruning. As we drop the min-supp constraint, questions about computational costs arise, the more so as the system is interactive. Since with cubes, runtime does not depend on data density any more [Sect. 4 ], we report on runs with only a few data sets. These are taken from real investigations. Associ-ation rule algorithms were set up with a support limit of zero and a search depth of up to two antecedent items. Experiments were carried out on a modest 1.7GHz/1GB machine, with all algorithms implemented in Java, except for Apriori, for which we used the C implementation of [ 2 ]. For Apriori, I/O time is not included in the listed runtimes.

Table 1 shows no runtime penalty while generating cubes. Association rule mining even took more time, which can be attributed to several factors: First, rule miners that realize support pruning entail considerable organizational overhead. Second, in cube generation we exploited the fact that cubes can be marginalized from higher dimensions. Third, for cubes, a rule is a bare counter within an array, whereas in rule mining, a rule sooner or later must be materialized as an object. 11.3 Case study The primary claim of our approach is to be better understandable and to increase effectiveness of the engineers X  work. User surveys could be a way to measure this, but apart from that, exact evaluation of added value is difficult. We therefore present a real-world case study to give an idea of the system and its overall value.

Our users want to identify root causes of quality issues. Our approach provides them with a ranked list of cubes that have the strongest influence on an issue. Variables describe vehicle configuration, manufacturing, and usage. One important feature of the system is its ability to recognize false influences, according to Goal 3c . Although the ultimate cause might not even be among the variables, the interactive approach supports the engineers to better understand the problem and discover hidden influences. The following real-world example illustrates how this works in practice.
 Several vehicles are brought to dealerships because a lamp indicates an engine issue. Diagnostic tools point to the exhaust system. Finding no trouble, dealers replace oxygen sensors on suspicion. Early warning systems for warranty cost control show a significant increase in warranty costs for these sensors while engineers cannot find an explanation for the issue. The replaced sensors are ok, and no other part seems to have failed.

The data analyst knows that only one engine type can set the fault code. Therefore he restricts the data set to all instances with Opt_Engine=E (Fig. 6 ). The system shows a ranked list of many possible influences (e.g. State , Opt_Emission , ClimateZone ), confirming the engi-neer X  X  assumption that the majority of service claims are related to the CARB states emissions expects that State does no longer show up. However, it remains a high-ranked influence, and the north-eastern CARB states still have high positive rates. To relate this to other influences, he first derives a new attribute CARBStates with values NortheasternCARBStates , California and OtherStates . Figure 7 shows its associated cube. It is interesting that north-eastern CARB states still have a much higher failure rate than California. A possible explanation for this could be the stop-and-go traffic in New York.

The engineer examines the cube neighborhood of CARBStates (Fig. 8 )toverifythis hypothesis. However, as described in Sect. 10 , the tool suggests that the high rank of CARB-States might have been caused by weather conditions, e.g., the minimum temperature over the last seven days (variable Min7Temp ). As temperatures in California are higher than in the north-eastern states (to be seen from the tile sizes in Fig. 8 ) the failure just does not occur that often there.

To verify this, the engineer eliminates the influence CARBStates (Sect. 9.2 ). CARBStates obviously gets rank 0, but Min7Temp is still ranked quite high. If however the user eliminates the influence Min7Temp the influence CARBStates almost vanishes (Fig. 9 ).
 We can observe another interesting fact: despite this interaction the cube CARBStates  X  Min7Temp is ranked low (1.95). This is fine that way, because this cube states some interaction of these variables as to cause the issue X  X ut since the main influence is the temperature that just pushes the other, this statement is not valid.

Based on these results the engineer actually finds out that there was a calibration issue that sometimes caused the engine control module to set a diagnostic code when the vehicle was driven at cold temperatures in wide open throttle mode. 12 Conclusion and open issues In this paper, we decomposed the ultimate goal of finding actionable causes to quality issues in vehicles into several (possibly still not exhaustive) subgoals. We noted that association rule based approaches fulfill many of them, but are less adequate to some. These critical requirements include the scale problem (an important aspect of Goal 1a ), the evaluation of rules as black box predicates, and limitations to understand what is behind a rule. We there-fore adopted the work of Zhao et al. [ 22 ], added an intuitive visualization [Goal 4b ], ranking of multivariate influences [Goal 3b ], and introduced two different ways of incorporating background knowledge, supporting the user to interactively factorize a problem and shift focus as he needs [Goal 2b ]. Finally, we discussed how cubes can be used in particular to see how factors relate causally. That way, the user can walk through a web of related influences and at least recognize those that are pushed by others [Goal 3c ].

We evaluated the design decision of a search depth limit of two as reasonable, and the runtime as acceptable for interactive use. Our expert users appreciate data analysis as an essential source of inspiration and as a support for their decisions. It is however hard to quantify the exact value our approach adds. We are going to conduct a systematic survey to get a better idea of this.

Several open issues remain. We are going to extend subgroup merging within singular variables to an optimization over two-influence cubes. Although the suppression strength (Eq. 8 ) works fine in practice, we are working on a better founded definition. Finally, we are interested in how the web of causally related influences can be visualized more intuitively. References Author Biographies
