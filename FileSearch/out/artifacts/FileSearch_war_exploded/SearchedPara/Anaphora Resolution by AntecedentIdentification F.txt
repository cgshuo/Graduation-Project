 RYU IIDA, KENTARO INUI, and YUJI MATSUMOTO Nara Institute of Science and Technology 1. INTRODUCTION
Anaphora resolution is an important process for various NLP applications. In contrast with rule-based approaches, such as Brennan et al. [1987], Lappin and Leass [1994], Baldwin [1995], Nakaiwa and Shirai [1996], Okumura and
Tamura [1996], and Mitkov [1997], empirical, or corpus-based approaches to this problem have shown to be a cost-efficient solution achieving performance that is comparable to the best performing rule-based systems [McCarthy and
Lehnert 1995; Ge et al. 1998; Soon et al. 2001; Ng and Cardie 2002a; Strube and M  X  uller 2003; Iida et al. 2003; Yang et al. 2003].

Anaphora resolution can be divided into two subtasks: anaphoricity deter-mination and antecedent identification . Anaphoricity determination is the task of classifying whether a given noun phrase (NP) is anaphoric or nonanaphoric .
Here we say an NP is anaphoric if it has any antecedent [i.e., NP(s) that are coreferent with it] in the context preceding it in the discourse, and nonanaphoric otherwise. The second task, antecedent identification, is identification of the antecedent(s) of a given anaphoric NP.

Early corpus-based work on anaphora resolution does not address anaphoric-ity determination; it assumes that the anaphora resolution system knows a priori all the anaphoric noun phrases. However, this problem has recently been the subject of increased attention [Bean and Riloff 1999; Ng and Cardie 2002b;
Uryupina 2003; Ng 2004; Poesio et al. 2004], because determining anaphoric-ity is not a trivial problem, even in languages, such as English and French, where definite articles can be used as clues [Ng and Cardie 2002b], and the overall performance of anaphora resolution crucially depends on the accuracy of anaphoricity determination.

Obviously, the problem of anaphoricity determination is even more critical in the case of languages, such as Japanese, which do not have such clues as definite articles.
 Previous efforts to tackle this problem have provided the following findings:
One of the useful clues for determining the anaphoricity of a given NP can be obtained by searching for an antecedent. If an appropriate candidate for the antecedent is found in the preceding context of the discourse, the NP is likely to be anaphoric [Soon et al. 2001; Ng and Cardie 2002a].

Anaphoricity determination can be effectively carried out by a binary classi-fier that learns instances of nonanaphoric NPs, as well as those of anaphoric NPs [Ng and Cardie 2002b; Ng 2004].

As we discuss in the next section, previous approaches to anaphora resolution [Ng and Cardie 2002a, 2002b; Iida et al. 2003] make use of a range of cues, but none of the previous models effectively combines the three previous approaches shown in Section 2. This leaves significant room for improvement in anaphora resolution.

In this paper, we propose a machine learning-based model that effectively combines the sources of evidence used in existing models, while overcoming their drawbacks. We show the effectiveness of our approach through experi-ments on Japanese anaphora resolution, comparing previous machine learning-based approaches including Ng and Cardie [2002a] X  X  search-based approach and Ng [2004] X  X  classification-then-search approach.

The rest of the paper is organized as follows. In Section 2, we review previous machine learning-based approaches to anaphora resolution. Section 3 describes how the proposed model effectively combines advantages of each previous ap-proach. We then report the results of our experiments on Japanese noun phrases anaphora resolution in Section 4. We conclude in Section 5. 2. PREVIOUS APPROACHES
Previous learning-based methods for anaphora resolution can be classified into two approaches: the search-based approach and the classification-based approach . We discuss their advantages and disadvantages below (see Table I for summary). 2.1 Search-Based Model
The search-based approach determines the anaphoricity of a given NP indi-rectly as a by-product of searching the preceding context for its antecedent. If an appropriate candidate for the antecedent is found, the NP is classified as anaphoric; otherwise, nonanaphoric. Models proposed by Soon et al. [2001] and
Ng and Cardie [2002a] fall into this class. In Soon et al. X  X  method (see Figure 1), for example, given a target NP ( Ana ) for resolution, the model processes each of its preceding NPs (i.e., candidate antecedents) in a right-to-left order, deter-mining whether or not it is coreferent with the NP i , until a positive answer (i.e., antecedent) comes up. If all the preceding NPs are classified negative, Ana is judged to be nonanaphoric. We call this approach the search-based approach .
It has the advantage of using broader context information in the sense that the model determines the anaphoricity of an NP by examining whether the con-text preceding the NP in the discourse has a plausible candidate antecedent or not. Soon et al., in fact, defined the feature set including broad contextual information, such as that shown in Table II.

A flaw of this approach, on the other hand, is that models are not designed to learn nonanaphoric cases directly in the training phase. As an example, let us take a closer look at Soon et al. X  X  model (see Figure 2). For training, their model creates a positive instance from an anaphoric NP paired with its closest an-tecedent ( NP 5 -ANP ) and a negative instance from each of the intervening NPs paired with the anaphor ( NP 6 -ANP , NP 7 -ANP , and NP training instance is derived from nonanaphoric NPs. This drawback is shared also by other search-based models, including Ng and Cardie [2002a] and Yang et al. [2003]. As we show in Section 4, this may well significantly degrade per-formance.

Another drawback of the approach is that it may suffer also from highly im-balanced distributions of positive and negative instances. The aforementioned method of generating training instances tends to generate much more nega-tive instances than positive ones. For example, in the experiments described in
Section 4, the ratio of the positive instances to the negative instances is 1 to 22. The model requires proper selection of training instances [Ng and Cardie 2002c]. However, it is not a trivial problem.
 2.2 Classification-Then-Search Model
The second approach is to introduce the process of anaphoricity determina-tion separate from antecedent identification [Ng and Cardie 2002b; Ng 2004].
We call this approach the classification-based approach . Unlike the search-based approach, it has an advantage in that it uses labeled instances derived from nonanaphoric NPs as well as those from anaphoric NPs to induce an anaphoricity classifier. For example, Ng [2004] proposed the following model (see Figure 3): 1. First it carries out anaphoricity determination using a classification-based model to filter out a target NP ( Ana ) whose anaphoricity score Ana Score is below threshold  X  ana ; 2. It then searches for the antecedent for the remaining Ana ; 3. Finally, it outputs the best-scored candidate antecedent Max Ant if its score
Ant Score is above threshold  X  ant , or classifies the Ana as nonanaphoric otherwise.

Here we term this model the classification-then-search model because the model first determines the anaphoricity of a given candidate anaphor and then searches for the antecedent for the candidate anaphor.

The classification-then-search model cautiously filters out nonanaphoric NPs according to the threshold parameter  X  ana at the first step. Second, the model also determines the anaphoricity of the remaining candidate anaphor according to the threshold parameter  X  ant as well as identifies an antecedent. This two-step anaphoricity determination model is designed because the anaphoricity determination component is not powerful enough to entirely free the antecedent identification component from the charge of anaphoricity determination. As
Ng [2004] reports, optimizing the two threshold parameters could improve the performance for the overall task of anaphora.

As reported in Ng and Cardie [2002b] and also in Section 4 of this paper, this model significantly outperforms the search-based model. However, it still has several drawbacks, and there is room for improvement.

First, Ng and Cardie [2002b] reports that the performance of the anaphoricity determination component is so low that applying it would not improve the performance of the overall task unless it incorporated features that effectively capture contextual information (see Table III). This indicates that it is crucially important in anaphoricity determination to know whether or not the preceding context of the discourse contains NPs that are likely to be the antecedent of a current target NP. While such features as in Table III appear to be useful clues for this reason, they appear to be rather ad hoc and only provide an extremely rough summary of the context.

Second, in the classification-then-search model, not only the anaphoricity classifier but also the antecedent identification component takes charge of anaphoricity determination. This rather unclear way of division of labor con-strains the range of algorithms that can be used for antecedent identification.
The model cannot employ as, for example, the tournament model, which we review below.

Third, as long as it employs such an algorithm as Ng and Cardie [2002a] for the antecedent identification subtask, the model inherits the drawbacks of the algorithm; in particular, it is important to note the problem of imbalanced distribution of positive and negative training instances. 2.3 Tournament Model
For the task of antecedent identification alone, it is worth referring to a model called the tournament model proposed by Iida et al. [2003] (Figure 4). The model conducts a tournament consisting of a series of matches in which candidate antecedents compete with each other for a given anaphor. In the tournament, it processes the candidate antecedents in the right-to-left order. In the first round, the model consults a trained classifier to judge which of the right-most two candidates is more likely to be the antecedent for the anaphor. The winner then plays a match with the third right-most candidate. Likewise, each of the following matches is arranged, in turn, between the current winner and a right-most new challenger until the left-most candidate antecedent. The model selects the winner of tournament.

This model has several advantages over such previous antecedent identifica-tion models as reviewed in Section 2.1. First, it can incorporate the learning of some of centering factors, such as the expected center order, proposed in Center-ing Theory Grosz et al. [1995]. Second, unlike the previous models, the task of the classifier is to determine which of a pair of candidates is more likely to be the antecedent. This way of task decomposition inherently avoids the problem of imbalanced distributions of positive and negative instances which such a model as Soon et al. (2001) and Ng and Cardie (2002a, 2002b) would suffer from. Due to these advantages, Iida et al. [2003] report that the tournament model outper-forms the Ng and Cardie [2002a] X  X  model in Japanese zero-anaphora resolution.
Despite these advantages, however, the tournament model has a strict limi-tation; namely, it is not capable of anaphoricity determination because it always selects a candidate antecedent for a given NP whether the NP is anaphoric or not. 3. SELECTION-THEN-CLASSIFICATION APPROACH
This section discusses how to design an anaphora resolution model that inherits all the advantages of the previous models reviewed in the last section.
We explore an alternative way of incorporating contextual clues into anaphoricity determination. One way that has not yet been examined before is to implement an anaphora resolution process that reverses the steps of the classification-then-search model. Assuming that we have an antecedent iden-tification model and an anaphoricity classification model, the new model pro-cesses each target X  X oun phrase ( TNP ) in a given text in two steps (see Figure 5): 1. Select the most likely candidate antecedent CA ( NP 2 using an antecedent identification model. 2. Classify TNP paired with CA as either anaphoric or non-anaphoric us-ing an anaphoricity classification model. If pair CA -TNP is classified as anaphoric , CA is identified as the antecedent of TNP ; otherwise, TNP is judged nonanaphoric .

To contrast the classification-then-search model, we call this model the selection-then-classification model .

To implement this new model, we extend a anaphoricity determination com-ponent designed in the classification-based approach so that the model deter-mines whether a given NP paired with its most likely candidate antecedent is anaphoric or not. For training the classifier, we create positive (anaphoric) and negative (nonanaphoric) training sets in the following way: 1. For each NP appearing in the training corpus, we add the pair of the NP and 2. If the NP is nonanaphoric, we first use the antecedent identification model
By providing anaphoric and nonanaphoric training sets, we can use a wide range of classifier induction algorithms.

The new model might not look considerably different from such previous models as the classification-then-search model. However, the model, in fact, effectively combines the advantages of all the previous models we reviewed in Section 2.

First, the new model inherits the advantage of the search-based model. It determines the anaphoricity of a given NP, taking into account the information of its most likely candidate antecedent. The candidate, antecedent selected in the first step can be expected to provide contextual information useful for anaphoricity determination; if the best candidate does not appear to be the real antecedent of the target NP, it is unlikely that the target NP has any an-tecedent in the discourse. In this respect, the proposed model makes better use of contextual clues than the classification-then-search model, which accesses to contextual information only through ad hoc string-based features.

Second, the proposed model uses nonanaphoric instances together with anaphoric instances to induce an anaphoricity classifier, which is an important advantage inherited from the classification-then-search model.

Third, in the proposed model, the division of labor between the two compo-nents is clearer than that in the selection-then-classification model. The an-tecedent identification component always selects a candidate antecedent for a given NP (i.e. candidate anaphor) whether the NP is anaphoric or not. This way of task decomposition allows us to employ the tournament model in antecedent identification (see Figure 7). Recall that in the classification-then-search model, the anaphoricity determination component is not reliable enough to entirely free the antecedent identification component from the charge of anaphoricity determination. This deficiency prohibits the model from incorporating the tour-nament model. As we report in Section 4.4, this gives a significant advantage to the new model. 4. EXPERIMENTS ON NP-ANAPHORA RESOLUTION
We conducted an empirical evaluation of our method by applying it to Japanese newspaper articles. In the experiments, we compared three models: the search-based model, the classification-then-search model, and the selection-then-classification model. 4.1 Models
For the search-based model, we created a model designed to simulate the model described in Ng and Cardie [2002a]. Pseudocode describing the model is given in Figure 1. We employed Support Vector Machines [Vapnik 1998] for learning and used the distance between an input feature vector and the hyperplane as the score for classification.

For the classification-then-search model, we created a model based on the pseudocode given in Figure 3. In these experiments, instead of preparing the development data for the estimation of two thresholds, we evaluated the per-formance by fine-tuning these thresholds by hand. In addition to the origi-nal classification-then-search model, we also implemented the model using the tournament model for the antecedent identification model instead of the search-based model. Thus, we can investigate whether or not the tournament model improves the classification-then-search model.

Regarding the selection-then-classification model, we implemented the model based on the process in Figure 7.

In addition to the original selection-then-classification model, we also im-plemented a model using the search-based model for the antecedent identi-fication model instead of the tournament model. Thus, we can evaluate the effectiveness of the tournament model itself by comparing the two selection-then-classification models.

Like the search-based model, the classification-then-search model and the selection-then-classification model also used SVMs for both antecedent identi-fication and anaphoricity classification. 4.2 Training and Test Instances
We created a coreference-tagged corpus consisting of 90 newspaper articles (1104 sentences). The corpus contained 884 anaphoric NPs and 6591 non-anaphoric NPs (7475 NPs in total), each anaphoric NP being annotated with information indicating its antecedent. For each experiment, we conducted ten-fold cross-validation over 7475 noun phrases so that the set of the noun phrases from a single text was not divided into the training and test sets. 4.3 Feature Sets We used the following five types of features:
ANA : Features designed to capture the lexical, syntactic, semantic, and posi-tional information of a target noun phrase (i.e. a candidate anaphor);
ANT : Features designed to capture the lexical, syntactic, semantic, and posi-tional information of a candidate antecedent;
ANA-ANT : Features designed to capture the relation between the candidate antecedent and the target NP (e.g. the distance, semantic compatibility be-tween the two);
ANT-ANT : Features designed to capture the relation between two candidate antecedents (e.g., the distance between the two);
ANT SET : Features designed to capture the relation between the set of the candidate antecedents in the preceding context and the target NP (e.g., the binary feature that a target NP and an candidate antecedent in the preceding context contain the same string).
 The features of the types ANA , ANT , and ANA-ANT cover the feature set that Ng and Cardie [2002a] used in their search-based model. On the other hand, the
ANT-ANT type of features were those that cannot be used in the search-based model, but only in the tournament model, because the search-based model refers only to a single candidate antecedent at the time of classification. The ANT SET type of features is based on the feature set in Ng and Cardie X  X  work [2002b].
Table IV summarizes which types of features were used for each model. Table V and Table VI present the details of the feature set.

In the experiment, all the features were automatically computed with the help of publicly available NLP tools, the Japanese morphological analyzer
ChaSen [Matsumoto et al. 2000] and the Japanese dependency structure an-alyzer CaboCha [Kudo and Matsumoto 2002], which also performed named-entity chunking. 4.4 Results
To compare the performance of the three models on the task of anaphora res-olution, we plot a recall-precision curve for each model, as shown in Figure 8, by altering threshold parameter  X  ana [and  X  ant in the case of the classification-then-search model using the search-based model (CSM SM)], where recall R and precision P are calculated by:
Note that the curves of the classification-then-search model using the search-based model (CSM SM) are plotted by altering two threshold parameters and  X  ant . The curves indicate the upperbound of the performance of CSM SM because in practical settings, these two parameters would have to be trained beforehand.
 For the SCM algorithm, we implemented two models. One model employed
SM for antecedent identification (SCM SM) and the other employed the tour-nament model (SCM TM).

The comparison between the search-based model and the classification-then-search model supports Ng and Cardie [2002b] X  X  claim that incorpo-rating the anaphoricity classification process into the search-based model can improve the performance if the threshold parameters are appropriately selected.

By comparing the selection-then-classification model using the search-based model (SCM SM) with the classification-then-search model using the search-based model (CSM SM), one can measure the effects of using the most likely antecedent while preserving the advantage of referring to the nonanaphoric information. The performance of the SCM SM approached the upper bound of the performance of the CSM SM. Recall that the CSM SM algorithm requires the two interdependent threshold parameters to be trained beforehand while the proposed model need to tune only one parameter. We consider it as an important advantage of the proposed model. This advantage comes from the design of the proposed model, where the model makes use of anaphoric/nonanaphoric training instances, as well as contextual clues, given by most likely candidate antecedents simultaneously in the anaphoricity determination phase.

The results also indicate that even if the parameters for CSM SM are opti-mally tuned, the proposed model significantly outperforms it when it employs the tournament model for antecedent identification (i.e., SCM TM). The per-formance of the search-based model (SM) and the tournament model (TM) for antecedent identification alone is compared in Table VII. The table shows that
TM outperforms SM by 2.5 points in accuracy. This difference is clearly reflected in the difference between SCM TM and the SCM SM. This is also an important advantage of the proposed model because previous model such as Ng [2004] cannot employ the tournament model, as we noted in Section 3.

By comparing the selection-then-classification model using the tourna-ment model (SCM TM) with the classification-then-search model using the tournament model (CSM TM), we can see whether or not the tournament model improves the CSM TM. The results show that even if the tournament model is incorporated into the classification-then-search model, the SCM TM still out-performs it. 4.5 Discussion
According to our error analysis, a majority of errors are caused by the diffi-culty of judging the semantic compatibility between a candidate anaphor and candidate antecedent. For example, the lexical resources we employed in the experiments did not contain gender information; the model did not know that  X  ani (elder brother) X  was semantically incompatible with  X  kanojo (she) X  and thus could not be an antecedent of it. This raises an interesting issue, namely, how to develop a lexical resource that includes a broad range of semantically compatible relations between nouns; for example, the model needs to know that Russia can be an antecedent of Russian government , but president is not compatible with yesterday . One of our future directions should aim at this issue.
There is also still room for improvement in the architecture of the proposed model. The model could make better use of the semantic information of can-didate antecedents if it also referred to ancestors of coreference chains. For example, if a named-entity expression is referred to by such a word as  X  dousha (the/this company) X  in the preceding context, we can enrich the coreference-chain information about by combining the relevant information from each noun phrase. This line of refinement will also lead us to explore methods to search for a globally optimal solution to a set of anaphora resolution problems for a given text, as discussed by McCallum and Wellner [2003]. 5. CONCLUSION
In this paper, we reported that our selection-then-classification approach to anaphora resolution improves the performance of the previous learning-based models by combining their advantages, while overcoming their drawbacks. It does so in the following two respects: (1) our model uses nonanaphoric instances together with anaphoric instances to induce an anaphoricity classifier, retain-ing the advantage inherited from the classification-based approach and (2) our model determines the anaphoricity of a given NP taking the information of its most likely candidate antecedent into account. Our argument has been sup-ported by empirical evidence obtained from our experiment on Japanese NP-anaphora resolution.

Analogous to NP-anaphora resolution, zero-anaphora resolution also deals with the issue of anaphoricity determination. Motivated by this parallelism be-tween NPs and zero-anaphora, in future work, we want to attempt anaphoric-ity determination for zero pronouns using the selection-then-classification ap-proach proposed here.

