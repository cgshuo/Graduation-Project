 particular system model. It is to be noted that the optimal control problem, as stated above, may have multiple solutions (i.e., a solution may not be unique). Thus, it is most often the case that any solution to the optimal control problem is only locally mini-mizing the objective representing system X  X  performance. problems. These are often labeled as direct and indirect methods ( von Stryk and Bulirsch, 1992 ). An indirect method transforms the problem into another form before solving it. Typically Pontryagin X  X 
Maximum Principle ( Pontryagin, 1962 ) is used to find the neces-sary conditions for the existence of an optimum. This allows the original optimal control problem to be transformed into a Bound-ary Value Problem (BVP), which can then be solved analytically or numerically using well-known techniques for differential equa-tions. This boundary-value problem actually has a special struc-ture because it arises from taking the derivative of a Hamiltonian.
The indirect method is sometimes described as first optimize then discretize because optimality conditions are found before numer-ical techniques are applied. These techniques were used in early years of optimal control. The disadvantage of indirect methods is that the boundary-value problem is often extremely difficult to solve (particularly for problems that span large time intervals or problems with interior point constraints). An excellent introduc-tion to this method can be found in a recent text by Lenhart and Workman (2007 ).
 modification. Section 4 discusses the application of this method considering various instantiations of the optimal control problem. The focus here is to confirm that th is new direct method is effective and efficient for a wide range of problems. In each case, the examples used can be solved analytically by an indirect method. This permits comparison of the two solutions and validates the proposed method. Section 5 demonstrates the effectiveness of the proposed approach by comparing the result obtained wi th this approach with the popular ecologically inspired optimization methods. The main focus here is to show that the modified version of IWO performs better as compared to a few significant nature-inspired stochastic algorithms used as evolutionary direct methods. Finally Section 6 concludes the paper uncovering a few future research directions. 2. Be  X  zier parameterization and Invasive Weed Optimization 2.1. Be  X  zier Control Parameterization
Be  X  zier curves were widely publicized in 1962 by the French engineer Be  X  zier (1972 ), who used them to design automobile bodies. His UNISURF system ( Be  X  zier, 1974 ) has been applied to define the outer panels of several cars marketed by Renault ( http:// en.wikipedia.org/wiki/Be  X  zier_curve ).Thecurveswerefirstdevel-oped in 1959 by Paul de Casteljau using de Casteljau X  X  algorithm, a numerically stable method to evaluate Be  X  zier curves ( http:// en.wikipedia.org/wiki/Be  X  zier_curve ). In 1926, Bernstein presented a constructive proof of the Weierstrass approximation theorem ( Brinkhuis and Tikhomirov, 2005 ) using functions that have become known as Bernstein polynomials. Be  X  zier curves have a very similar form, and are sometimes referred to as Be  X  zier X  X ernstein polyno-P  X  z  X  X  only if all the control points are collinear. A curve can be split at any point into 2 sub curves, or into arbitrarily many sub curves, each of which is also a Be  X  zier curve. The polygon formed by connecting the Be  X  zier points with line, starting with P 0 and parameterize smooth, non-oscillatory function with minimum epistasis with minimum number of parameters.

Here, Be  X  zier Control Parameterization (BCP) is used for single control function. A fixed regular mesh is used on the t -axis to make the curve single valued and to reduce the dimension of the optimization vectors to n  X  1. The BCP u  X  X  u i n i  X  0 completely encodes the control function u ( t ) as the n th order parametric Be  X  zier curve u ( t )  X  / t ( z ), u ( z ) S as follows: t  X  z  X  X  u  X  z  X  X  8 &gt; &gt; &gt; &gt; &gt; &lt; &gt; &gt; &gt; &gt; &gt; : time. The objective function F ( u ) is formulated in the following curve parameterization. The values are stored as parameters z  X  0, h ,2 h , y ,1. A step-size h  X  0.001 is used and can be refined when more accuracy is required, but this will make IWO to take more time to converge to the optimal solutions. The Initial Value Problem (IVP) is then solved numerically for x ( t ), interpolating 3. Experiments and results solutions to the standard range of optimal control problems, we consider a wide range of problems from Lenhart and Workman (2007 ). Thus in addition to one problem in standard form also considered are examples with payoff terms, with fixed state end-points and with bounded controls. Both minimization and maximiza-tion problems are considered. Single state functions and multiple state functions are considered here for completeness. The last one has has a single control function, but the method can be extended to solve problems with multiple control functions. All problems considered have continuous optimal controls a nd can be solved analytically. This allows validation of the method by comparing with the results of exact solutions ( Lenhart and Workman, 2007 ). 3.1. Standard form following example there is only one control function u ( t ) and only level and may deteriorate the convergence rate of the optimiza-tion. It was shown that the value of nonlinear modulation index n has a considerable effect on the performance of IWO ( Mehrabian and Lucas, 2006 ). It was suggested that the best choice for n is 3. Simulation shows that the best choice is with n  X  3. Maximum and minimum numbers of seeds are the two other important parameters needed to be selected. Based on different examples, it can be concluded that selecting the maximum number of seed between 3 and 5 leads to a good performance of the optimizer. Moreover, the minimum number of seeds is set to zero for all examples. Number of weeds is another important parameter. Increasing the parameter does not necessarily increase the per-formance of the algorithm. The IWO parameters chosen are final  X  0.0001, s initial  X  2.0, maximum number of seeds  X  5 and minimum number of seeds  X  0, number of weeds  X  4, nonlinear modulation index n  X  3 with maximum population size NP  X  15. Except the population size, all the other parameters are kept same for all the other examples as well. Except the population size all the parameters are listed in Table 1 .

The initial population is formed by random selection of control parameters, within the bounds [ 5, 15]. Optimization is termi-nated after 50 generations. The control points for BCP solution is shown in Table 2 . The solution obtained and the analytical solution is plotted in Fig. 3 . 3.2. Payoff term
In some control problems there can be one objective over entire time interval and a second objective at a specific time as a function of t f . These two are combined into one objective function through weighted sum min F  X  u  X  X  where a is the weight of the second objective relative to the first. The term outside the integral is called the payoff term. This might be necessary when the payoff term is used to minimize the final population. As the second objective is function of t f , it is called terminal payoff term. In this test case, the integral objective depends upon the control function and payoff term x (1) 2 min F  X  u  X  X  1 2 optimal state x ( t )  X  ( t 2 3 t )/4. The IWO X  X CP method proposed here uses Runge X  X utta initial value problem solver which cannot handle the fixed end point problem. Thus the problem is to be reformulated. One way to do this is to formulate the state equations as constrained initial value problem, with the fixed endpoint as the constraint. Evolutionary algorithms typically deal with constraints by using a penalty function, in which a numerical penalty is added to the fitness function when the solution does not meet the constraint. Penalties imposed are proportional to the extent to which the constraint is violated. In unconstrained problems the fitness function is equal to the objective function.
But in constrained problems, the fitness function is equal to the sum of objective function and penalty function. The fitness function now becomes after adding the penalty term min F  X  u  X  X  subject to : where m is a scaling constant, representing the weight of the penalty relative to the objective functional. The BCP solution to (10) has n  X  3. The parameters used for algorithm are shown in Table 1 . The strategy used for optimization is NP  X  25 and actual solution is defined piecewise on three intervals I with u  X  t  X  X  x  X  t  X  X 
Three Be  X  zier control points are used for this problem. The solution is shown in Fig. 6 and the control points are shown in Table 2 . The parameters used for algorithm are summarized in Table 1 . A population size of NP  X  30 is taken, maximum number of generations taken as 100. Due to constrained problem here the maximum number of generations has to be increased. IWO optimizes the fitness function such that the constraint is not violated. 3.5. Constrained control with payoff
In this example a BCP solution with mixed constraints is depicted. Here both the upper bound on the control and terminal payoff term for the state is present max F  X  u  X  X  x  X  4  X  subject to
The actual solution for the control and state is u  X  t  X  X  x  X  t  X  X 
For the BCP solution three Be  X  zier points are used. The para-meters used for algorithm are summarized in Table 1 . The population size is taken as NP  X  40 and maximum number of generations is kept at 100. The result is a BCP solution ( Table 2 ) that is closely approximating the actual solution ( Fig. 7 ).
The IWO/BCP method uses Runge X  X utta initial value problem solvers which cannot handle the fixed state end point. So it is necessary to reformulate the problem to eliminate this problem.
Here state equations are reformulated as constrained initial value problems with fixed state end point as a constraint.
 blems by using a penalty function in addition to objective function to make fitness function different from the objective function. Whenever the solution does not meet the constraint a numerical penalty is imposed, proportional to the extent to which the constraint is violated. The penalty function formulation is as follows: min subject to
NP  X  30 and maximum number of generations 50. Penalty scaling factor is m  X  10. The parameters used for algorithm are same as shown in Table 1 . The BCP solution ( Table 2 ) is again in excellent agreement with the actual solution ( Fig. 8 ).
 parent solutions and creates two offspring solutions to simulate the working principle of the single-point crossover operator on binary strings in real paradigm. This real coded genetic algorithm is the most widely used. SBX is a real parameter recombination operator, in which the probability distribution used to create offspring depends on a spread factor that is defined as ratio of the absolute difference in children values to that of the parent values. In this scheme, two offspring solutions are generated from two parent vectors. We have used real coded genetic algorithm. 4.2. Differential Evolution (DE) Differential Evolution (DE) ( Storn and Price, 1997 ; Das and Suganthan, 2011 ) is arguably one of the most powerful stochastic real-parameter optimization algorithms in current use. Like other population-based search techniques, DE generates new points (trial solutions) that are perturbations of existing points, but these deviations are not samples from a predefined probability density function, like those in Evolutionary Strategies (ES) ( De Jong, 2006 ). Instead, DE perturbs current generation vectors with the scaled difference of two randomly selected population vec-tors. In its simplest form, DE adds the scaled, random vector difference to a third randomly selected population vector to create a donor vector corresponding to each population vector (also known as target vector). Next the components of the target and donor vectors are mixed through a crossover operation to vector competes against the population vector of the same index, i.e. the parent vector. Once the last trial vector has been tested the survivors of all the pair wise competitions become parents for the next generation in the evolutionary cycle. 4.3. Particle Swarm Optimization (PSO) The concept of particle swarms ( Kennedy and Eberhart, 1995 ; Kennedy et al., 2001 ), although initially introduced for simulating the social behavior commonly observed in animal kingdom, has become very popular these days as an efficient algorithm for intelligent search and optimization. In PSO dynamics, a swarm of particles (or agents), each representing a potential solution to the optimization problem at hand, navigates through the search space. The particles are initially distributed randomly over the feasible search space with a random velocity, and the goal is to converge to the global optimum of an objective function. Each particle keeps track of the best solution that it has so far achieved. This is the personal best value (the so-called pbest Kennedy et al., 2001 ). In addition, the PSO process also keeps track of the global best solution so far detected in a neighborhood of the current particle or in the entire swarm (the so-called gbest )) Kennedy et al., 2001 ). Thus, the velocity of each agent in the next iteration is computed by using the information of the gbest (as the social component), the best personal position of the particle ( pbest) as the cognitive component), and its current velocity (the memory term). Both social and cognitive components contribute randomly to the position of the agent in the next iteration
The parameters of the above mentioned algorithms were chosen very carefully through a series of empirical experiments. The optimal set of parameters for each algorithm is given in Table 3 . The parameters setup for modified IWO algorithm has already been mentioned earlier and the parameters are given in Table 1 .We have used two most popular Differential Evolution strategies DE/rand/1/bin and DE/best/1/bin. Moreover, De/rand/1/ bin is known to be robust but less efficient in terms of convergence rate, whereas DE/best/1/bin is known to have fast convergence rate, but may lead into premature convergence. Thus, we have incorporated two strategies having contrasting characteristics. for problems that are difficult or impossible to solve indirectly.
Having validated the method generally, it is to these that attention can now be turned, particularly problems that are multiobjective and have complicated constraints.
 References
