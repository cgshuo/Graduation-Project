 The paper presents a novel approach to story link detection, where the goal is to determine whether a pair of news stories are linked, i.e., talk about the same event. The present work marks a departure from the prior work in that we measure similarity at two distinct levels of textual organization, the document and its collection, and combine scores at both levels to determine how well stories are linked. Experiments on the TDT-5 corpus show that the present approach, which we call a  X  X wo-tier similarity model, X  comfortably beats conventional approaches such as Clarity enhanced KL diver-gence, while performing robustly across diverse languages. Categories and Subject Descriptors : H.3 [ Information Storage and Retrieval ]: Content Analysis and Indexing; H.3 [ Information Search and Retrieval ]: Relevance feedback General Terms : Algorithms Keywords : story link detection, TDT-5, topic tracking, similarity measures, pseudo relevance feedback
Story link detection concerns the problem of telling whether a pair of stories talk about the same topic, and had been under inten-sive study led by the DARPA sponsored TDT project. The signif-icance of story link detection lies in the potential it has to create such socially relevant applications as monitoring media coverage and tracking the ebb and flow of a particular news event across the world, such as swine flu, food poisoning, conflicts, flooding, oil spill, etc., which could provide useful insights into how the media coverage of a given event differs across and within countries [9, 10].

Pew Research Center X  X  Project for Excellence in Journalism (PEJ) provides a weekly update on news events most covered by the American news media, hand-created from over 1,000 news outlets by professional content-analysts. Figure 1 shows PEJ X  X  media cov-erage index (NCI) for April 27 to May 3, 2009. 2 (NCI is a measure http://www.journalism.org http://www.journalism.org/index_report/news_coverage_index_ april_ 27_may_3 developed by PEJ to quantify how much media attention was given to news topics.) In the figure, swine flu tops the list, followed by the economic crisis, with news on Obama X  X  100 days in office com-ing at the third. This is followed by Republican Senator Specter X  X  defection to the Democrats and news on ailing US auto industry. NCI is currently produced by human labor at an enormous finan-cial cost. Adapting the NCI for media operating in languages other than English is also inhibitive, as it requires a battalion of human translators. These are the sort of challenges the technology for story link detection can be brought to bear on. 3
On the technical side, much of prior work in story link detec-tion [16, 3, 7, 5, 21, 6, 15, 17, 27] focused on exploiting textual properties largely internal to the document, such as term frequency, named entities [21], topical structure [7], collocations [17], and burstiness [17]. A break came when [16] and [15] introduced an idea of adopting relevance feedback in link detection, where sto-ries are measured for similarity based not on the document but on a collection of relevant documents. The effectiveness of the approach was noted in [7], [6] and elsewhere.

In this work, we are interested in taking this idea further by run-ning a similarity measure both at the document and collection level in tandem , independently of one another. An obvious benefit of the approach is that it enables us to compare stories from two distinct perspectives unlike the past work which confined themselves to a single perspective on similarity, whether at the document level or at the collection level. Experiments later show that the approach we take, which we call the  X  X wo-tier model of similarity, X  yields a notable improvement over conventional approaches. Indeed, the approach was found to be robust against languages, and similarity measures we use to compare stories.
Our similarity model has a two-tiered structure: given a pair of stories, the first tier determines the similarity at the document level, i.e., by looking at words or phrases contained in individual docu-ments; on the second tier, we move past the immediate stories to a collection of stories via relevance feedback and determine how much related original stories are based on a collection of the asso-ciated documents. Figure 2 gives a schematic view of how things work with our model: stories A and B are compared for similarity not only at the document level, but at the collection level.
The second tier similarity is intended to detect linked stories whose relations are somewhat remote such as those which are on the same topic but talk about different aspects of it. Reading sum-maries of two news transcripts from CNN in Table 1, for instance,
Media Tenor (www. mediatenor.com), a media research company, also relies heavily on trained professionals in its reporting and mon-itoring of news generated by various media outlets. Figure 1: PEJ X  X  News Coverage Index for April 27 -May 3, 2009 while both concern a cease-fire negotiated by Jimmy Carter, does not give us a clear sense that they are connected in any significant way.

Formally, we take the two-tier model as a linear combination of the first and second tier similarities [26], with the following form, set of stories, respectively, with  X  1 and  X  2 controlling contribution of the similarity functions involved.

As we do not impose any restriction on what similarity measure we may use at either tier, various combinations of similarity func-tions would be possible. In this work, we will look at a wide range of similarity measures, including the symmetric KL divergence, the skew divergence [18], the cosine similarity, the Hellinger distance [5, 8, 6], the L 1 distance, clarity-enhanced KL divergence [16], and the string kernel [4], exploring whether or how they impact perfor-mance in story link detection.

Note that we could turn Eq. 1 into a Logit model by setting which would gives us
Fighting has been triggered in Sarajevo following a mortar shell attack in the city center. Bosnians and United Nations forces are hopeful [that] the attacks don X  X  mean the failure of an upcoming cease-fire. ( CNN )
Former President Jimmy Carter announced that a com-plete cease-fire will be imposed on the whole of Bosnia-
Hercegovina on December 23rd. The cessation of fighting will include the turbulent area of Bihac. ( CNN )  X  ,  X  1 , and  X  2 are parameters whose values are determined through maximum likelihood estimation. Below we consider models repre-sented by Eqs. 3 and 1, corresponding to supervised and unsuper-vised versions of the two-tier model. In the unsupervised case (i.e., Eq. 1), we simply set  X  1 and  X  2 to 1.
In this work, we adopt the bag-of-words (BOW) assumption in representing a document where a document is viewed as consisting of a set of free-standing word tokens. With that, we explore three schemes as a way of representing tokens that comprise a document, namely, Latent Dirichlet Allocation or LDA [2], tfidf, and term fre-quency. LDA is a widely used generative model of the document which could be thought of as a Bayesian version of probabilistic Latent Semantic Indexing or pLSI. As with pLSI, LDA takes the probability of a term t to be a mixture of probabilities that it occurs with a latent topic z i ( 1  X  i  X  k ), except that topics themselves are randomly generated in accordance to some Dirichet prior.
In LDA, the probability of a term t is defined as:  X  is a randomly generated k -parameter vector representing weights for latent topics.  X  represents a per-document prior distribution of topics and  X  a prior on the per topic word distribution, both of which, together with hidden variables  X  and z , are estimated using the training data by a variational EM. We produce the probability of a term t by marginalizing over  X  and z . The probability of a term t in an unseen document d is given as w ( t | d,  X   X  ,  X   X  probability of t given d and trained parameters  X  and  X  .
In the tfidf scheme, we make use of the following formula. Eq. 5 largely follows [15]. tf ( t ) represents the term frequency of t in a document. N indicates the length of a document collection, df ( t ) the document frequency of term t .

Finally, the frequency approach simply posits that So the weight of a term corresponds to the number of times it ap-pears in a document. Moreover, we assume that the term X  X  weight in Eqs. 5 and 6 is normalized with ) =
Listed in Table 2 are similarity-and distance-based functions we will use to measure how closely stories are related. Most of the functions are commonly found in the natural language processing and information retrieval literature. 4 Of note here are  X  X kew di-vergence, X  an approximation to the Kullback-Leibler divergence, which [18] found to be effective in the estimation of word co-occurrence probabilities, and the Hellinger distance, whose utility for story link detection was extensively explored in [5, 8, 6]. Clarity is a similarity based measure introduced by [16] and later explored by [15]; it ranks among the best in story link detection. Later in the paper, we spend some time discussing the approach, as it is going to serve as a baseline for the present work.

String kernels represent an approach to measuring similarity be-tween a pair of possibly discontiguous symbol sequences [4]. In this work, we will work with a particular string kernel which takes the following form: for given documents d 1 and d 2 , where  X  n stands for a set of all possible n word long sequences over the vocabulary  X  . For a given word sequence p = w 1  X  X  X  w d [ i ] = p means that d 1 [ i ] is a running string that appear at a particular position specified by i in d 1 , which spans the sequence p . (Thus p may not appear contiguously in d 1 [ i ] .)  X  decay factor representing a weight of a word n-gram p . We let  X  p =  X  w 1  X  X  X   X  w n . In particular we define  X  w by
Throughout the paper, largely for the simplicity X  X  sake, we inten-tionally abuse the term  X  X imilarity X  to refer to measures listed in Table 2 collectively as a similarity measure, though some are obvi-ously distance based. reusing the idf part of Eq. 5. It is worth noting here that Eq. 9 enables the string kernel (Eq. 8) to represent term weights in a way that roughly corresponds to tfidf, i.e., tf ( p )  X  2 p .
Notice that K n is a regular sequence kernel of the n -th order with a gap penalty set to 1. In other words, K n ( d 1 , d the degree to which d 1 and d 2 overlap in terms of unique n -grams found in them, adjusted for the weights of matched n -grams, with gaps ignored. We run a kernel with d 1 flipped around as we like to as a match. Hence the reason for having d r in Eq. 7, which is the reverse form of d 1 .

As finding every n -long subsequence in text is generally intractable, we resort here to DP (dynamic programming), aka kernel trick, to compute K n . We follow here [4].

Moreover we set K  X  X  j ( s, t ) = K  X  j ( s, t ) = 0 if the length of either s or t is less than j . Also  X  g indicates a gap weight and  X  weight for a match.

Let us work through a simple example to get a feel of how the formulation works, which is somewhat involved. Suppose that we have two strings S = AY and T = AQY W ERY , and like to find out what K 2 ( S, T ) is. We begin by searching T for any oc-currence of Y , which appears at the very end of S (Eq. 14). (Note that we work backward from tail to head.) Since we have two oc-currences of Y in T , one in the third from left, and another in the last, we recurse on each of them, while stripping matching symbols off the strings, i.e., S = A , T 1 = AQ , T 2 = AQY W ER (step 1 in Table 3). We leave Eq. 11 idling over non-matching symbols. At step 2 in Table 3, we hit a match in T 1 , i.e., A . As we have nothing left in T 1 with A taken off, we are done with T 1 T , we keep Eq. 11 running until it reaches step 6, where Eq. 12 and 10 will be called upon to finish the process. As we mentioned earlier, we set a gap weight  X  g to 1, which allows us to ignore the gap weights altogether when calculating the kernel score. Hence we have K n ( S, T ) = 2  X  2 A  X  2 Y = 2  X  2 AY . Finally, we normalize K n ( x, y ) by K n ( x, y ) / Despite a substantial improvement DP provides for computing K n , it rapidly becomes impractical as the text grows larger. The challenge here is, how we might run the sequence kernel K collection of news stories with a reasonable efficiency. A usual way out is to reduce features, namely, words, that the kernel deals with, in a manner specific to the problem at hand. [24], for instance, ex-plored the use of  X  2 statistic in selecting features for string kernels in their approach to named entity recognition, where they would choose features based on how closely they are associated with ei-ther of the binary classes in terms of  X  2 statistic.

While it is obviously possible to make use of  X  2 statistic for our domain by selecting words strongly associated with a particular period, along the lines of [25] and [17], a preliminary experiment with the temporal  X  2 did not produce favorable results. In light of this, we turned to the conventional tfidf scheme, which somewhat to our surprise, led to a visible improvement in performance. The scheme consisted of choosing 100 unique words from a document collection that ranked highest by tfidf. We will work with this idea in the experiment described below.
Pseudo relevance feedback works by constructing a short query from a document and collecting a set of documents relevant to the query using a search engine. For our experiment later described, we created a query by selecting 20 highest ranking terms in terms of tfidf from a story at the first tier, motivated by [16], which while pursuing a closely related approach, found no improvement in per-formance with a query that exceeds more than 30 words in length. Also [15] reports an improved performance with a 10-word query.
As for the search engine, we used KinoSearch, 5 a highly efficient perl adaptation of Apache X  X  Lucene, which makes use of a tfidf http://www.rectangular.com/kinosearch based cosine similarity in ranking documents. 6 KinoSearch makes available various  X  X oost options X  which allow the user to give an extra weight to a particular term in query, or segments of document such as title, author, body, etc. We turned them off by leaving them with the default value, i.e., 1.

With KinoSearch, we selected 10 highest ranking documents from a relevant portion of the TDT-5 corpus (described later) by running a query consisting of 20 words derived from an input story at the first tier. The decision on how many documents to retrieve was also motivated by [16], which reported that the use of 5 to 70 documents led to a good performance.
It was suggested in [16] that we use the notion of  X  X larity X  in measuring similarity between stories. Clarity is intended to mea-sure how apart a story is from a vast majority of texts in the way words it contains are distributed. Formally, for a given story D , it is expressed as the Kullback-Leibler divergence, or KL divergence between D and what [16] calls  X  X eneral English, X  and takes the following form: where w denotes a word. Thus a story gets a high mark on clarity if it exhibits word usage significantly different from general English  X  which is thought of as a large body of English documents. [16] discusses an approach to link detection which makes use of similarity, not at the document level but the collection level, where given two stories, it expands each into a collection of documents using relevance feedback, and determines the similarity based on the collection each story spawned, which they call a topic model.
The similarity between collections or topic models is then mea-sured by calling on two-way, i.e., symmetric KL divergence en-hanced with clarity.
 M 1 stands for a collection spawned by a document d 1 and M collection by a document d 2 . Note that M 1 and M 2 will get a high mark on (16) if they have small mutual divergence and large divergence from GE. A simple algebraic transformation of (16) will produce
Clarity enhanced KL is generally found to work better than KL in link detection. To see why this should be the case, note first that KL lacks the ability to represent a term weight, in particular, the inverse document frequency (IDF), as it is basically built out of frequencies of terms. As pointed out in [16], Clarity works to compensate KL X  X  inability to represent IDF through supplying the measure that reflects the uniqueness of M 1 and M 2 . http://lucene.apache.org/java/2_9_0/
We conducted evaluation using the TDT-5 corpus, which con-tains some 400,000 news stories, collected from Chinese, Arabic, English news sources including Agence France Presse (AFP), An-Nahar, Xinhua, CNN, the New York Times, from April to Septem-ber 2003 [12, 11]. Table 4 gives statistics on how many documents were collected from different language sources. TDT-5 comes with human judgments on relevance of news stories to 250 topics, i.e., whether or not a given document is primarily concerned with a par-ticular topic. In TDT-5, a topic is defined as an event or an activity occurring at a specific point in time and space; Table 5 shows what is called a  X  X opic profile, X  an event description the annotator refers to when he or she looks for a relevant event [11]. 7 In the TDT-5 corpus, topics are roughly spread evenly among English, Mandarin, and Arabic. Approximately 25% of the topics are monolingual English, 25% are monolingual Mandarin Chinese, 25% are monolingual Arabic, and 25% are multilingual, which consist of events covered in multiple languages. Machine gener-ated translations are also provided for non-English documents.
Answer keys, or gold standard data for the link detection task, created by NIST, contain a set of paired identifiers of stories which are annotated for whether they are linked, i.e., on the same topic. Our experiment made use of an answer key set provided in a file  X  X nk_SR=nwt_TE=mul,eng.key, X  whose detail is shown in Table 7. The file contains gold standard monolingual links involving news articles in a same language, and also cross-lingual links, which in-volve articles from different languages. A specifier  X  X ng X  in the file name means that non English articles involved in links are trans-lations of original articles created by machine translation. Table 8 gives per topic statistics on answer keys broken down by language: on-topic links range from 3 to 100, off-topic links roughly from 200 to 500, except for the cross-lingual key set whose off topic links go up to as many as 1,500.
In order to compare systems, NIST developed a particular met-ric they call a  X  X etection cost, X  or DET. The idea is to quantify the cost, rather than accuracy, that results from using a particular cut-off threshold to separate linked from non-linked stories. The detec-tion cost is given by: Cf. TDT-4: Annotation Manual. Version 1.2., 2004.
 Table 7: Answer keys for link detection in lnk_SR= nwt_TE= mul,eng.key Language # topics # on-topic links # off-topic links Arabic 66 3,858 32,967 English 100 6,645 47,973 Mandarin 59 2,491 28,450 Cross-lingual 47 7,085 44,698 C miss represents a predefined penalty for missing a true link, set to 1.0 here following the prior work. P miss is the ratio of true links a system missed. P target denotes a prior chance probability that a pair of documents are about the same topic, which we set to 0.02, following the convention in the literature. C fa a predefined penalty for reporting a false positive, and is set here to 0.1, again following the prior literature. P fa is the ratio of false positives a system reported. P nontarget represents a prior chance probability that a pair of documents are about different subjects, i.e., 1  X  P miss = 0 . 98 .

Note that with this setting, one can get 0.98 by reporting that every pair of documents is linked, or 0.02 by reporting that none is linked. So normalizing C det against either of the approaches is a good way to see how much improvement was made beyond the simple baseline. This would give us
Moreover, in TDT link detection, systems are customarily com-pared in terms of the minimum C norm (aka MinDET) which rep-resents the best possible performance systems achieved on a given task. MinDET is obtained by letting a threshold sweep across link scores and picking one that gives the minimum C norm . An obvious benefit of using MinDET is that it prevents systems from becom-ing incomparable due to an arbitrary choice of threshold. MinDET ranges from 0 to over 1. A perfect system lies at 0, and one at or above 1 is viewed as performing no better or worse than the simple baseline. Less is better, so if system A has a MinDET score lower than system B, A is doing a better job at link detection than B does.
The experiment focused on examining how each similarity func-tion listed in Table 2 performs in link detection when run either at the first or second level alone, as well as when it is run in the two-tier mode where similarities on both tiers are combined. For KER, we set m to 5, that is, we combined string kernels with n , the length of search patterns p , ranging from 2 to 5. In addition, we made use of a stopword list containing a total of 599 words, derived from the one Salton and Buckley developed as a part of the SMART information retreval system [23].
Table 8: Per topic statistics in lnk_SR=nwt_TE=mul,eng.key Language min max avg min max avg Arabic 3 100 58.45 469 500 499.50 English 3 100 66.45 311 500 479.73 Mandarin 3 100 42.22 232 500 482.20 Cross-lingual 9 300 150.74 272 1,500 951.02
A baseline or a reference system was served by Clarity enhanced two-way KL divergence, a current state of the art. Although Clar-ity was originally designed to operate at the collection level or the second tier, we decided to run it in the two tier mode, along with other similarity measures, to see if it has any effect on performance. To distinguish between Clarity used as a baseline and Clarity used as a similarity measure, we denote the former use of Clarity by  X  X CLA X  and the latter by  X  X LA. X  As for GE or  X  X eneral English X  which Clarity requires, we turned to the British National Corpus (BNC), which consists of 100,106,029 tokens and 938,971 types. We built GE from BNC and smoothed the model with the  X  X dd-one X  rule [19]. The document frequency, another model parameter we need for Eq. 5, was estimated from the TREC corpus [13, 14], together with the TDT Pilot Study Corpus. 9
We ran the models on the gold standard data provided by  X  X nk_SR =nwt_TE =mul,eng.key, X  which consists of paired stories marked for whether they are linked with respect to a given topic. For each model, we determined its MinDET by having a threshold sweep across similarity scores it generated for the paired stories.
Shown in Tables 9 through 12 are results of their performance by language, with tfidf used as an indexing scheme. Boxed figures in the tables represent performance by bCLA. Numbers that appear under the heading  X 1 ST TIER  X  indicate performance of similarity functions run at the first tier, those under  X 2 ND TIER  X  indicate per-formance when each similarity function was run at the second tier alone, while figures under  X 1 ST +2 ND  X  show what happens when we equally combine similarity functions at both tiers.

The results generally find the superiority of the two-tier models over bCLA across the languages considered. 10 Of a particular note
We used a version of the BNC called  X  X ll.al X  available at http://www.kilgarriff.co.uk/bnc-readme.html.
The corpus contains 15,863 English newswire and broadcast sto-ries that appeared from July 1, 1994 to June 30, 1995 [1].
Pseudo relevance feedback was run on a portion of the corpus relevant to a particular language a model is working on. Thus the here is that CLA, when run in the two tier mode, outperforms bCLA across the board. With the exception of EUC or the Euclidean dis-tance, it seems fair to say that: Thus what is relevant to improving performance in link detection is not a particular similarity measure we use but to take into account both original documents and their expansions. It is clear that the first tier is just as relevant in improving performance as the sec-ond tier. Combining both leads to an improvement in performance neither is able to achieve single-handedly. Note that the two-tiered CLA X  X  superiority over bCLA was made possible by its inclusion of the first tier CLA. As regards to the behavior of EUC, we have no explanation to offer at this point, except to say that it is likely that the poor performance of the two tiered EUC was caused by its devastatingly poor performance at the first tier.

Figure 4 shows how the size of data from which we retrieve doc-uments by relevance feedback impacts performance. The x -axis represents the proportion of data we randomly collected for use with relevance feedback from the source corpus, the y -axis scores in MinDET. Again with the exception of EUC, there is a clear pat-tern in the results indicating that regardless of what similarity mea-sure we use, performance improves with the size of a sampled cor-pus. The dashed line indicates performance of a relevant similarity model which makes use of the full-length corpus for relevance feed-back. Most of the similarity models reach the level of performance Arabic models retrieved documents only from the Arabic portion of the corpus. indicates a MinDET score achieved by using the entire source corpus. comparable to that of their full-length counterparts at around 0.5, indicating that it is enough to have about a half the size of full-length corpus in order to make them as good as their full-length counterparts.

Table 13 shows results in MinDET for the supervised version of the two-tier model where parameters  X  0 ,  X  1 , and  X  2 estimated from the training data. Numbers under P in the table rep-resent performance of the supervised two tier model for each of the language set in Table 7. (We placed MinDET scores for the unsu-pervised version alongside for comparison.) Evaluation was done by 5-fold cross validation, where we start by splitting the corpus into 5 blocks, then train the model with 4 blocks and test it on the remaining one block, which you repeat for each of the blocks. Fig-ures listed in the table are those averaged over 5 evaluation folds. The results find that for most of the times, supervised models out-perform unsupervised counterparts, though differences between the two approaches tend to be modest.

Figure 5 gives some idea of how the way we represent terms may affect performance. Each panel has the x -axis representing the ratio of false alarms and the y -axis the ratio of misses. Graphs are known as a  X  X ET curve, X  widely used in the TDT community as a summary of system performance. A system with an inward bending DET curve is preferred as it indicates it is less prone to misses and false alarms. Figure 5 shows what DET curves for each similarity function look like on the English data under three differ-ent representation schemes, FREQ, TFIDF, and LDA, where terms are weighted based on their frequencies, tfidf scores, and posterior probabilities dictated by LDA, which was implemented with a pub-lic domain LDA package [20] and trained on 20% of the English portion of the TDT corpus. What Figure 5 shows is that FREQ and TFIDF, despite their simplicity, fare far better than LDA. Although the reason is not immediately clear, we could interpret the results as implying that the probability of a term does not necessarily re-flect its salience in a document, and the salience is what counts in determining whether two stories are linked.

One interesting question that remains is whether we may im-prove the two tier model by combining similarity functions that vary in kind, say, COS and L1. Table 14 shows what happens if we combine the string kernel (KER) with each of the similarity func-tions we have discussed. As KER works with (possibly discon-tiguous) n-grams larger than or equal to 2 words, it makes sense to combine it with other similarity functions, which work only with uni-grams, as it allows us to incorporate information from different perspectives on a document. The results in Table 14 generally bear out our expectation: similarity functions coupled with KER show COS 0.1277 0.1295 L1 0.1039 0.1071 EUC 0.2975 0.4283 KL 0.1068 0.1134 JS 0.0961 0.1054 SK 0.0948 0.1051 HL 0.0954 0.1051 CLA 0.1023 0.1018 KER 0.1407 0.1389 COS 0.1630 0.1653 L1 0.1523 0.1540 EUC 0.5238 0.7511 KL 0.1599 0.1623 JS 0.1445 0.1464 SK 0.1452 0.1480 HL 0.1470 0.1505 CLA 0.1508 0.1521
KER 0.1439 0.1522 2 2 2 2 improved performance in comparison to the  X  X are X  two tier mod-els, where models are built out of the same (non-compound) sim-ilarity function applied at two levels. Figures in parentheses indi-cate MinDET scores obtained by applying non-compound similar-ity functions at relevant levels, i.e., those given in Table 9 through 12, repeated here for comparison. Boxed figures in the rows headed by CLA represent baseline scores, namely, those by bCLA, while boxed figures that appear in the columns headed by  X 1 ST +2 represent best scores achieved for each language. We see two tier models coupled with KER outperform bare counterparts in English, Mandarin and Cross-lingual, scoring three wins out of four, which underpins the effectiveness of combining KER with other uni-gram based similarity functions.

In English, with bCLA, we get 0.0602 in MinDET, which goes down to 0.045 with the two-tiered HL, which is further reduced to 0.0421 by the KER coupled two tier HL, achieving more than 30% improvement over the baseline.
We have presented an approach to story link detection, which marks a departure from the past work with its use of textual simi-larity both at the document and collection level. We demonstrated its superiority through experiments with the TDT-5 corpus, show-ing that the two-tier approach is more effective in link detection than conventional approaches such as Clarity-enhanced two-way KL divergence, and even helped Clarity improve itself.

The experiments found that a key factor that brought about im-provement is the recognition of an equal importance of the similar-ity at the document and collection level: by combining the similar-ities at both tiers, we were able to achieve a level of performance a single tiered approach, whether based on the first or second tier, did not. Future lines of research would include expanding stories by running relevance feedback in ways that are sensitive to the time-line, as in [27], and making the feature selection more sophisti-cated through the use of sequential pattern mining techniques such as PrefixSpan [22]. [1] J. Allan, Y. Yang, J. Carbonell, J. Yamron, G. Doddington, [2] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet [3] R. Brown. Dynamic stopwording for story link detection. In [4] N. Cancedda, E. Gaussier, C. Goutte, and J.-M. Renders. [5] F. Chen, A. Farahat, and T. Brants. Story link detection and [6] F. Chen, A. Farahat, and T. Brants. Multiple similarity [7] Y.-J. Chen and H.-H. Chen. NLP and IR approaches to [8] A. Farahat, F. Chen, and T. Brants. Optimizing story link [9] E. Feldherr and A. Mitchel. Swine flu coverage around the [10] J. Galtung and M. H. Ruge. The structure of foreign news. [11] M. Glenn, S. Strassel, J. Kong, and K. Maeda. TDT5 Topics [12] D. Graff, J. Kong, K. Maeda, and S. Strassel. TDT5 [13] D. Harman and M. Liberman. Text Research Collection Vol. [14] D. Harman and M. Liberman. Text Research Collection Vol. [15] L. S. Larkey, F. Feng, M. Connell, and V. Lavrenko. [16] V. Lavrenko, J. A. E. DeGuzman, D. LaFallme, V. Pollard, [17] K.-S. Lee and K. Kageura. Korean-Japanese story link [18] L. Lee. On the effectiveness of the skew divergence for [19] C. D. Manning and H. Sch X tze. Foundations of Statistical [20] D. Mochihashi. lda, a latent dirichlet allocation package. [21] R. Nallapati. Semantic language models for topic detection [22] J. Pei, J. Han, B. Mortazavi-asl, H. Pinto, Q. Chen, U. Dayal, [23] G. Salton and M. E. Lesk. The SMART automatic document [24] J. Suzuki, H. Isozaki, and E. Maeda. Convolution kernels [25] R. Swan and J. Allen. Automatic generation of overview [26] C. C. Vogt and G. W. Cottrell. Predicting the performance of [27] X. Zhang, T. Wang, and H. Chen. Story link detection based
