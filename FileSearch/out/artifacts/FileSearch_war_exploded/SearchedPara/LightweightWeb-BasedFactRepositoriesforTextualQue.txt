 Since answ ers to fact-seeking questions usually reside within small factual text nuggets, often \hidden" within full-length documen ts, their relev ance to a question is not necessarily correlated to the relev ance of the full-length documen t to the question. Yet previous approac hes to open-domain textual question answ ering from large documen t collections quasi-unanimously emplo y a documen t retriev al stage, in order to apply widely di eren t, often exp ensiv e answ er mining tech-niques to only a small subset of documen ts. Dep ending on the collection size, 95% or more of the documen ts in the col-lection (much more in the case of the Web) are left out of the selected subset for any given query , and thus become invisi-ble to subsequen t pro cessing stages for actual answ er mining. This pap er introduces a new mo del for answ er retriev al for question answ ering. The collection is distilled oine into large rep ositories of facts. Eac h fact constitutes a poten tial direct answ er to questions seeking a particular kind of en-tity or relation, suc h as questions asking about the date of particular events. Question answ ering becomes equiv alen t to online fact retriev al, whic h greatly simpli es the de-facto system architecture for fact-seeking question answ ering. In addition to simplicit y, exp erimen ts on a fact rep ository ac-quired from appro ximately a billion Web documen ts illus-trate the impact of fact rep ositories in extracting accurate answ ers to a standard evaluation set of open-domain test questions and additional sets of domain-sp eci c questions. H.3.3 [ Information Storage and Retriev al ]: Information Searc h and Retriev al; H.3.1 [ Information Storage and Retriev al ]: Con ten t Analysis and Indexing; I.2.7 [ Arti cial Intelligence ]: Natural Language Pro cessing Algorithms, Exp erimen tation Web information retriev al, ligh tweigh t text analysis, ques-tion answ ering, fact extraction
In the eld of automated question answ ering (QA), a va-riet y of information sources and multiple extraction tech-niques can all con tribute to pro ducing relev ant answ ers in resp onse to natural-language questions submitted by users. Yet the nature of the information source whic h is mined for answ ers, together with the scop e of the questions, have the most signi can t impact on the overall architecture of a QA system. Most recen t e orts con verged towards an-swering open-domain questions by iden tifying and extract-ing answ ers from large text collections. Within this class, a large num ber of the prop osed system architectures are sim-ilar to one another, at least at a higher level. Through an implicit agreemen t on whic h common sub-tasks must be ac-complished in order to return correct answ ers, open-domain QA systems have mo dules for question pro cessing, documen t retriev al, and answ er detection and ranking [15, 12]. In par-ticular, documen t retriev al iden ti es a relativ ely small subset of documen ts from the underlying text collection, whic h are deemed to be the most relev ant to the given question [22].
In order to build open-domain QA systems for large text collections, researc hers judiciously reuse existing documen t retriev al techniques for implemen ting the retriev al mo dules of QA systems. Usually , the mo dules rely on Boolean [13] or vector-space [1] retriev al mo dels. Besides the availabilit y of a large body of previous researc h in documen t retriev al, another reason for using documen t retriev al in open-domain QA is eciency . For eac h input question the poten tial searc h space, i.e., the entire text collection, needs to be drastically reduced in order to apply widely di eren t, often exp ensiv e answ er mining techniques to only a small subset of docu-men ts. Consequen tly, previous approac hes to open-domain textual QA from large text collections quasi-unanimously emplo y a documen t retriev al stage. The documen ts may be available locally [1] or as part of the index of an external Web searc h engine [14, 6, 19].
The de-facto paradigm in QA can be describ ed as \re-trieval of potential ly relevant documents" , then \extr action of potential answers" , then \return of top answers" , all of whic h are performed online for eac h question. Dep ending on the collection size, 95% or more of the documen ts in the collection (much more in the case of the Web) are left out of the selected subset for any given query , and thus become invisible to subsequen t pro cessing stages for answ er mining.
Without neglecting the unquestionable practical adv an-tages of emplo ying documen t retriev al in QA, this pap er explores an alternativ e approac h for reducing the amoun t of text being searc hed for answ ers for eac h submitted question. The approac h can be describ ed as a sequence of \extr action of potential ly relevant facts from all documents" (performed oine in one pass, similarly to documen t indexing), follo wed by \retrieval of relevant facts" and \return of top answers asso ciate d with prominent facts" (performed online for eac h question). For robustness and scalabilit y, the extraction re-lies only on ligh tweigh t tools and minimal lexical resources. The assumption is that the type of entities about whic h users may ask, as well as the relations of interest that apply to those entities, are kno wn in adv ance. For example, some questions ask about the dates when various events must have occurred (e.g., \When was the transistor invente d?" ). Sim-ilarly , other questions inquire about locations (e.g., \Wher e was the Declar ation of Indep endenc e signe d?" ). Note that, even in the presence of the assumptions, the range of target questions remains open-domain rather than being restricted to a specialized domain. Indeed, questions about dates still span widely , from the year when a tool was invented, or the date when someb ody was born, to when a book was writ-ten etc. More imp ortan tly, making assumptions about the types of information of interest to users is common practice in QA, rather than speci c to this approac h. One example is the reliance of man y QA systems [1, 14] on answ er type detectors and some form of named-en tity recognition. It is through explicit assumptions about the questions ask ed by users that answ er type recognizers attempt to categorize the questions into a closed set of entity types exp ected as an-swers (e.g., Persons , Locations , Organizations , Dates ), and then named-en tity recognizers scan the retriev ed documen ts for candidate answ ers that corresp ond to the same closed set of entity types, but not necessarily to other types ( Buildings , Eur opean cities , High-sp eed networks etc.).

Without loss of generalit y, the exp erimen ts rep orted in the pap er involve the oine extraction from Web documen ts of facts capturing the dates when various events occurred. In order to test the limits of the approac h, the facts are the only source of answ ers to a variet y of questions asking about dates. The portion of the documen ts outside of the extracted facts is discarded and not used in answ ering the questions, even though a poten tially more successful alternativ e would bac k o to using entire documen ts, whenev er no facts fully matc h a given question.
Section 2 illustrates the prop osed mo del for distilling the unstructured text of Web documen ts into rep ositories of facts, and how the facts pro vide answ ers to questions asking about dates. The follo wing two sections describ e the ap-proac h in more detail, by introducing a ligh tweigh t metho d for deriving fact rep ositories related to dates from unstruc-tured text (Section 3), and sho wing how the task of answ er-ing questions becomes equiv alen t to that of retrieving facts from the rep ository . For a comparativ e evaluation with pre-vious approac hes to open-domain QA, the evaluation set in Figure 1: Web-deriv ed fact rep ositories for QA Section 5 con tains questions from the QA trac k of the Text REtriev al Conference (TREC) [23]. Additional test sets of domain-sp eci c questions and their desired answ ers are as-sem bled in Section 6 from sev eral external resources. Sub-sequen t sections explain why our approac h is more suitable in answ ering categories of questions that traditionally cause diculties, suc h as questions asking about recurring events ( \When wer e the Olympic Games?" ). After discussing them in Section 7, we conclude in Section 8.
One of the main con tributions of this researc h are changes in the organization of information and type of the indexed objects in large-scale QA, from traditional full-length doc-umen ts to factual text nuggets. Eac h nugget is a sen tence fragmen t that enco des open-domain factual information as-sociated with some entity. In the exp erimen ts describ ed in this pap er, the entities are dates (e.g., 1947 ) when the events captured in the nuggets (e.g., \Bel l Labs invente d the tran-sistor" ) have occurred.

As illustrated in Figure 1, the nuggets are extracted from unstructured text, and organized oine into fact rep osito-ries rather than documen t indices. Eac h fact is equiv alen t to a pseudo-do cumen t that is stored, indexed and retriev ed with standard information retriev al techniques. Whenev er a question (e.g., \When was the transistor invente d?" ) tar-gets the same type of information as that enco ded in the fact rep ositories, the entities (e.g., 1947 ) asso ciated with the matc hing text nuggets are returned as candidate answ ers.
In addition to o ering direct answ ers in the form of entities of the desired type (dates), the prop osed fact rep ositories have sev eral adv antages over the traditional QA data o w. First, they greatly simplify the system architecture. An en-tire sequence of previously-prop osed pro cessing stages for QA (namely , documen t retriev al, passage retriev al, answ er detection and answ er ranking) is replaced with a single stage for fact retriev al. Second, the matc hing text nuggets pro vide at-a-glance justi cations as to why the asso ciated entities have been returned. Third, the results merge textual infor-mation and build upon evidence originating from scattered documen ts. As suc h, the sets of supp orting text nuggets o er a practical, simpli ed solution to the more complex problem of nding information scattered across unrelated, or even obscure documen ts.

There are two poten tially negativ e factors that require at-ten tion when using fact rep ositories in QA. The rst is the additional cost of extracting and storing the facts, if they are added as complemen tary information to the main doc-umen t index. However, the extraction is an oine one-pass operation over the entire rep ository , whic h can be part of the main indexing pro cedures and does not a ect the time re-quired to answ er eac h question online. The second factor is the non-graceful degradation of the results, if the fact rep os-itories replace, rather than complemen t, the main documen t index. As the complexit y of the questions increases, up to the point where it may be necessary to decomp ose a question into simpler questions in order to answ er it, the probabilit y that any of the extracted facts con tains a direct answ er to the complex question becomes smaller. The inabilit y to an-swer complex questions is not speci c to this approac h, but it applies equally to all QA systems [21]. However, in practice, systems that access the full text of a documen t sometimes return \luc ky" answ ers, as they incorrectly matc h the ques-tion against unrelated portions of the documen t, but one of the top returned answ ers happ ens to be correct. Despite this inheren t disadv antage over previous approac hes, the exp er-imen ts rep orted in this pap er use only the fact rep ositories as sources of answ ers. As suc h, they pro vide lower bounds on the poten tial impact of fact rep ositories in QA, and as-sess their usefulness in isolation from other evidence that the source documen ts may con tain.
For robustness and scalabilit y, the extraction relies only on ligh tweigh t tools and minimal resources. As a pre-requisite, the Web documen ts are pro cessed to lter out html tags. The resulting text-only data is tok enized, split into sen-tences and part-of-sp eech tagged using the TnT tagger [3]. No other tools or lexical resources are necessary . Com-parativ ely, the use of syn tactic parsers, lexico-seman tic re-sources of variable complexit y, or even text-surface infer-ence is widespread in complex QA systems. Ligh tweigh t fact rep ositories have the adv antage of simplicit y as a com bined result of relying on few er language-dep enden t resources and tools, and o er a much simpler architecture for QA. All exp erimen ts involve about one billion documen ts in English from a 2003 Web rep ository snapshot of the Google searc h engine. After html tag remo val, simpli ed sen tence boundary detection, tok enization and part-of-sp eech tag-ging, the unstructured text of the documen ts represen ts the input to the mec hanism for extracting factual text nuggets. The part-of-sp eech tags are used in the detection of dates asso ciated with nuggets, as well as for better appro ximation of the nugget boundaries, as sho wn below.
A sequence of sen tence tok ens represen ts a poten tial date if it has one of the follo wing formats: single year (four-digit num bers, e.g., 1929 ); or simple decade (e.g., 1930s ); or mon th name and year (e.g., January 1929 ); or mon th name, day num ber and year (e.g., January 15, 1929 ). Dates occurring in text in any other format are ignored. To avoid spurious matc hes, suc h as 1929 people , poten tial dates are discarded if they are immediately follo wed by a noun or noun mo di er, or immediately preceded by a noun.
To con vert documen t sen tences into a few text nuggets asso ciated with dates, the overall structure of sen tences is roughly appro ximated. Deep text analysis may be desirable but simply not feasible on the Web. As a ligh tweigh t alter-nativ e, the prop osed extraction metho d appro ximates the occurrence and boundaries of factual text nuggets through the follo wing set of lexico-syn tactic patterns:
The rst extraction pattern, P 1 , targets sen tences with adv erbial relativ e clauses introduced by wh-adv erbs and pre-ceded by a date, e.g.: \By [ Date 1910], when [ Nugget Korea was annexed to Japan], the Korean population in America had gro wn to 5,008" .
Comparativ ely, P 2 and P 3 matc h sen tences that start or end in a simple adv erbial phrase con taining a date. In the case of P 4 , the occurrence of relev ant dates within sen tences is appro ximated by verbs follo wed by a simple adv erbial phrase con taining a date. P 4 marks the entire sen tence as a poten tial nugget because it lacks the punctuation clues in the other three patterns.

The patterns must satisfy additional constrain ts in order to matc h a sen tence. These constrain ts constitute heuristics to avoid, rather than solv e, complex linguistic phenomena. Thus, a nugget is alw ays discarded if it does not con tain a verb, or con tains any pronoun. Furthermore, the nuggets in P 2 and P 3 must start with, and the nugget in P 4 must con tain a noun phrase, whic h in turn is appro ximated by the occurrence of a noun, adjectiv e or determiner. The com bi-nation of patterns and constrain ts is by no means de nitiv e or error-free. It is a practical solution to achiev e graceful degradation on large amoun ts of data, reduce the extraction errors, and impro ve the usefulness of the extracted nuggets.
One of the earliest prop osals for mining unstructured text with ligh tweigh t extraction patterns aimed at the disco v-ery of concept-sub concept relations [11]. Others successfully applied ligh tweigh t patterns to text collections for summa-rization, information extraction and question answ ering [20]. In the same spirit, the extraction metho d prop osed in this pap er scans the textual con ten t of the Web in searc h of a certain type of information, namely meaningful textual frag-men ts asso ciated with dates. Clearly , more adv anced tech-niques exist and have been used to iden tify dates occurring in a variet y of forms. Named entity recognizers can detect explicit dates in text with good performance. Additional work considers the pro cessing and disam biguation of rela-tive or otherwise undersp eci ed dates (e.g., two days ago or last week ), esp ecially in the con text of news articles [9]. The complexit y of the full range of temp oral expressions used in natural language can be addressed by dev eloping a ro-bust speci cation language for events and temp oral expres-sions [18]. After time stamps are iden ti ed in text, it was sho wn that it is possible to construct timelines of events, again in the con text of much cleaner and more reliable news article collections [2, 5], rather than noisy Web documen ts. In comparison, the extraction mec hanism introduced earlier in this section is geared towards simplicit y and robustness. It collects pairs of text nuggets and asso ciated dates from scattered Web documen ts, and organizes them oine into a temp oral fact rep ository . The rep ository replaces traditional documen t indices, o ering a new persp ectiv e to the QA task as describ ed in the follo wing.
The basic pre-pro cessing of input questions consists in the remo val of stop words. The set of English stop words of the Google searc h engine is augmen ted with in ections of auxiliary verbs ( do , have etc.), and with two question stems speci c to temp oral questions, namely When and What year .
The dates pro vide direct answ ers for temp oral queries suc h as those in Table 1. The fact retriev al stage is a four-step inverted nugget searc h: 1. Match the query against the actual nugget texts of the facts; 2. Score the matc hing nuggets individually relativ ely to the query; 3. Aggregate the best-matc hing nuggets asso ciated with the same date. Within eac h group, com bine (i.e., sum) the scores of the matc hing nuggets into an aggregated score as-signed to the common date; 4. Sele ct the dates with the highest scores.

The initial step includes all query terms by default, thus implemen ting a conjunctiv e Boolean searc h. The second step computes relev ance scores through heuristics that fa-vor any nugget in two cases, namely a) if the num ber of nugget non-stop terms, based on words as well as parts of speech, is lower; and b) if the average distance in the nugget between pairs of non-stop query matc hes, as measured by the normalized coun t of in-b etween tok ens, is lower. The actual form ula for scoring a nugget is: where Dist ( i; j ) is the minim um distance among matc hes of query terms in the nugget, computed over all pairs ( i , j ) of non-stop query terms, or 1 if there is only a single suc h term in the query; V is the num ber of nugget non-stop terms; and C is a scaling constan t, whic h is set to 100 in the exp er-imen ts. In conjunction with the third step, whic h transfers scores from nuggets to common asso ciated dates, the for-mula determines the ranking of the dates returned as results. For example, in Table 1, the third-column score (1294.38) of the rst result, 1990 , accum ulates the scores of the 37 matc h-ing nuggets for the question \When was Germany uni e d?" .
The scores sho wn for eac h result in Table 1 are not normal-ized, but used strictly internally for computing the ranking of the results. Comparativ ely, it may be useful to exp ose to the users a few of the matc hing nuggets, under eac h returned date. Indeed, the nuggets pro vide implicit, text-based justi-cations of either a) why the result (date) is relev ant (e.g., 1990 is relev ant for the query Germany uni e d because \East and West Germany wer e uni e d" ), or b) why the result is re-lated to the query even though it may not matc h the original user's inten t, e.g., in the case of the nugget under 1903 for the question \When was the Taj Mahal built?" (cf. Table 1).
The Boolean matc hing returns a relev ant text nugget only if the nugget con tains all non-stop question terms. This sometimes results in empt y sets of answ ers, due to lexical mismatc hes between the question words, on one side, and words in individual text nuggets, on the other. A frequen t cause of mismatc h is the construct \When did [..] [VerbBase-Form] [..]?" in temp oral questions. The text nuggets tend to use the Past Tense form of the verb (e.g., invente d ), rather than the base form ( invent ). Therefore, the rst extension to the conserv ativ e fact retriev al mec hanism describ ed in the previous subsection adds morphological pro cessing of the questions. For this purp ose, the input questions are tok enized and part-of-sp eech tagged. The morphological Table 2: Samples from the test set of open-domain questions collected from the QA trac ks of TREC pro cessing is curren tly limited to replacing the rst base-form verb in the query with the Past Tense form, whenev er the above-men tioned question construct applies to the input question. The replacemen t relies on standard morphological in ection rules, re ned with 176 irregular verbs.

For a more general, yet practical solution to the word mismatc h problem, the second extension mo di es the fact retriev al stage to issue a series of progressiv ely more general Boolean queries, rather than a single Boolean query . More general queries are generated through iterativ e remo val of question keyw ords, inspired by a previously prop osed query generation loop for QA [13]. The part-of-sp eech tags deter-mine the order in whic h the question words are discarded: verbs are remo ved rst, follo wed by adjectiv es, nouns and adv erbs, then follo wed by prop er nouns. In case of ties, tok ens situated closer to the end of the question are dis-carded rst. The iterativ e remo val of keyw ords applies only as needed, as long as the returned answ er set is empt y, and terminates once there is at least a matc hing factual nugget. The follo wing metrics are used in the evaluation: all answ ers are incorrect) [23] at rank i swer at some rank from i through j whether correct or incorrect. To assess the impact of the two extensions prop osed in Section 4.3, they are switc hed on and o in four system con-guration settings, namely KnMn (no extensions enabled), KyMn (only iterativ e remo val of keyw ords enabled), KnMy (only morphological pro cessing of base form verbs enabled), and KyMy (both extensions enabled).
The fact rep ository deriv ed from the Web documen ts pro-duces answ ers to temp oral questions by matc hing the ques-tions against the text nuggets and retrieving, merging and ranking the asso ciated dates, as describ ed earlier. The rst test question set, TrecQa, collects all When and What year of the 1893 main-task queries, from the Question Answ ering trac k [23] of past editions of TREC from 1999 through 2002. Table 3: Examples of answ ers to TREC questions conserv ativ ely mark ed as incorrect (R 1 =answ er re-turned at rank 1; K=answ er key in gold standard) Table 4: Accuracy of answ ers retriev ed from the temp oral fact rep ository for the TrecQa test set of open-domain questions Since 8 of the queries in the original set were discarded by the TREC organizers, the set consists of 199 temp oral, but otherwise open-domain questions, some of whic h are sho wn in Table 2. The organizers also pro vide a gold standard of answ er keys, as well as an automated evaluation script, in addition to the query set.

Due to di erences between the sources of the answ ers (a local documen t collection in TREC, versus Web documen ts in these exp erimen ts), and also to occasionally verb ose an-swer keys, some of the answ er keys do not necessarily re ect an ideal \truth" . As sho wn in Table 3, some of the disagree-men ts are due to errors in the answ er keys (e.g., for Q674, Q685 and Q1682), others to answ er keys that are more spe-ci c than the returned results (e.g., for Q156) or vice-v ersa (for Q397), and yet others to con tradictory evidence (e.g., for Q1314). At least 13 of the 199 questions have ques-tionable answ er keys, resulting in good answ ers returned at rank 1 that are mark ed as incorrect in the best-scoring run. Nev ertheless, for a clean evaluation, answ er keys remain un-mo di ed and no questions are remo ved from the test set.
Table 4 sho ws the scores for the 199 open-domain temp o-ral questions from TREC. The impact of the system con g-uration is progressiv e. In the KnMy con guration, the rst returned answ er is correct for 22 additional questions when compared to KnMn. Similarly , KyMn correctly answ ers 31 more questions at rank 1 than KnMn does. The KyMy con-guration gives the best results: 112 (56%) questions have a correct answ er at rank 1, and 131 (65%) questions have a correct answ er at ranks 1 through 5. The resulting MRR score on TrecQa is 0.608.

Due to the simplifying assumptions made during the ex-traction of facts, the answ ers retriev ed from the fact rep os-itory alw ays consist of a single year (e.g., 1929 ), possibly Table 5: Distribution of errors over open-domain test questions without a correct answ er among the top ve returned accompanied by a mon th name (e.g. January ) and a day (e.g. 15 ). This format cannot matc h any of the answ er keys for 19 of the 199 TrecQa questions, whic h do not exp ect any date con taining a year. For instance, Q1331: \When is the ocial rst day of summer?" has an answ er key \June 21" . Similarly , Q1630: \When was the dog domestic ated?" exp ects an answ er \100,000 years ago" . In a separate exp erimen t, a smaller subset of 180 questions was temp orarily created by discarding the 19 non-y ear questions from the TrecQa set. The corresp onding MRR score on the smaller subset is 0.663; the rst returned answ er is correct for 71% of the questions. The scores illustrate both the e ectiv eness of fact rep ositories in open-domain QA, and the poten tial bene ts of using a more complex recognizer of temp oral expressions to populate the rep ositories.
The 19 questions whose answ ers cannot matc h any of the answ er keys corresp ond to 28% of the 68 questions for whic h none of the top ve answ ers returned is correct (that is, ques-tions that are not among Q R 1 5 ). Table 5 sho ws other types of errors that a ect the retriev ed answ ers. The second most imp ortan t category of errors corresp onds to the conserv ativ e evaluation of answ ers (see Table 3), including errors due to disagreemen t of returned answ ers with the answ er keys. An equally imp ortan t factor is the mismatc h between terms in questions vs. terms in relev ant facts. More precisely , the fact rep ository con tains facts that are relev ant to the ques-tion and would pro vide the correct answ er, but those facts fail to be retriev ed for the given question due to lexical mis-matc h between the question (and generated queries) vs. the extracted facts. As sho wn in Table 6, examples of seman ti-cally equiv alen t yet di eren t terms in questions and facts are: broadcasting vs. \on the air" (Q740); colonize vs. \become a colony" (Q1047); U.S. vs. \Unite d States" ; travel vs. skirt or visit or \stop over" (Q1518); Asia vs. Vietnam or Aceh or Sumatr a (also Q1518); and make vs. invent (Q1803). Similar types of errors were rep orted for other QA systems [20, 17], and in general retriev al errors due to the mismatc h between terms in queries and documen ts are well-kno wn in documen t retriev al.

The retriev al extensions describ ed in Section 4.3 cause problems in about a fth of the incorrectly answ ered ques-tions. Wrong morphological pro cessing of verbs (namely , the incorrect con version of have into have d ) causes 4% of the er-rors. Less than optimal order in remo ving the question key-words pro duces an additional 17% of the errors in Table 5. For instance, consider Q692: \What year was Desmond Mpilo Table 6: Examples of open-domain test questions without a correct answ er among the top ve re-turned, due to mismatc h between questions and rel-evant facts Tutu awar ded the Nob el Peace Prize?" . The rep ository con-tains a variet y of facts asso ciated with the correct answ er 1984 , including \Archbishop Desmond Tutu won the Nob el Peace Prize" and \Bishop Desmond Tutu receive d the No-bel Peace Prize" . However, the rst query after keyw ord re-moval that retriev es a non-empt y set of answ ers is ( Desmond AND Mpilo AND Tutu ). Since none of the relev ant facts men tion the middle name of the person, they do not matc h the query . The answ ers to these questions would impro ve if the middle name Tutu were discarded before other keyw ords.
Answ er ranking errors occur in 5% of the incorrectly an-swered questions, whenev er a correct answ er is returned but not within the rst ve answ ers. Finally , 8% of the errors are due to limited coverage of the fact rep ository , as no relev ant facts pro viding the answ er could be found by searc hing the fact rep ository with a variet y of hand-pic ked queries. Thus, for Q23: \When did Spain and Kor ea start amb assadorial re-lations?" , the query ( Spain AND Kor ea ) does not retriev e any relev ant facts. For Q45: \When did Luc elly Gar cia, a former amb assador of Columbia to Hondur as, die?" , none of the facts in the rep ository con tain Luc elly . Similarly , the rep ository does not have any fact that answ ers Q699: \What year was Janet Jackson 's rst album released?" .
Most Web-based open-domain QA systems rely on searc h engines as external retriev al mo dules, in order to extract answ ers to open-domain questions directly from Web doc-umen ts. Bey ond implemen tation di erences, suc h QA sys-tems send queries to external Web searc h engines, fetc h the top searc h results (do cumen ts or snipp ets) locally , and mine the documen ts for candidate answ ers [14, 6, 19, 24]. An eval-uation on 200 open-domain questions from the QA trac k of TREC-8 leads to a Q R 1 score of 34% in [14], although the original set of answ er keys was man ually extended by the authors to accommo date additional answ ers that they judged as correct, but were not part of the original keys. Another study in extracting answ ers from the Web rep orts a mean recipro cal documen t rank of 0.151 on the 200 ques-tions from TREC-8, and similar scores on questions from subsequen t editions of TREC [19]. The scores are upp er bounds to what the MRR scores migh t be if the answ ers were perfectly iden ti ed and extracted from eac h Web doc-umen t con taining them. In [6], the accuracy in answ ering the rst 500 questions from TREC-9 corresp onds to an MRR score of 0.507, and a Q R 1 5 score of 61%. It is imp ortan t to emphasize that de-facto Web-based QA systems are not nec-essarily at a qualitativ e disadv antage just because they do not have complete con trol over the access to the entire docu-men t collection at all times. Even though they dynamically fetc h an extremely small fraction of the total num ber of doc-umen ts indexed by the searc h engine, that fraction is likely to con tain the most relev ant Web documen ts for that query . Furthermore, the decision over whic h queries to send to the Web searc h engine is entirely up to the QA systems. Note that non-W eb-based QA systems also defer the resp onsibil-ity of retrieving the best documen ts to existing documen t retriev al systems with little or no mo di cation [1].
The metho d presen ted in this pap er collects large fact rep ositories from the Web, thus sharing a few researc h goals with [8]. Our facts represen t direct sources of answ ers via fact retriev al. A di eren t kind of source of factual infor-mation for QA is considered in [16], in the form of semi-structured documen ts from authoritativ e Web sites. Kno wl-edge about the structure of the Web sites allo ws the authors to compile hand-written rules that transform questions into specialized queries, whenev er the questions matc h into a par-ticular domain. For instance, questions asking for the birth year of a person are con verted into queries to an external Web site specialized in biographies. For higher coverage, and to avoid empt y answ er sets for questions that do not matc h the supp orted domains, the domain-sp eci c retriev al comp onen t is coupled with the de-facto retriev al techniques for Web-based open-domain QA. Thus, the QA system also mines Web documen ts dynamically via external Web searc h engines. The com bination of domain-sp eci c factual infor-mation and open-domain Web con ten t gives a Q R 1 score of 37% on all 500 questions from TREC-2001. Appro ximately 16% of the correct answ ers are pro vided by the factual in-formation from authoritativ e Web sites rather than dynamic con ten t from searc h engines [16], whic h corresp onds to a Q
R 1 score of 6%. This num ber corresp onds to the score of 56% (112 out of 199) from Table 4, but the scores are not directly comparable because the test question sets only partially overlap.

In [7], man ually-written rules extract around 100,000 pairs of a question and a factual text nugget from Web documen ts available within sev eral large Web sites. Based on kno wl-edge about the structure of the documen ts, eac h factual text nugget is assumed to con tain an answ er to the ques-tion. When evaluated on the same set of questions from TREC-2001 as [16], poten tially relev ant facts are found for 24 of the 500 test questions, whic h corresp onds to an upp er-bound Q R 1 score of 5%. Again, the test question set in [7] has only a small overlap with the set used in this pap er.
The work rep orted in [10] creates a data rep ository oine from a 15-gigab yte collection of newspap er text. The rep os-itory pro vides a Q R 1 score of 35% over a set of 100 \Who is.." test questions. The Q R 1 scores in the curren t pap er are not directly comparable due to the di eren t query sets. The authors specify that answ ers are returned up to three orders of magnitude faster than with a standard QA system. Simi-larly , our fact rep ository pro duces answ ers at a signi can tly lower cost than if the searc h were applied to the complete Web rep ository from whic h the facts were extracted. How-ever, it would be unfair and we cannot quan tify the speedup when answ ering questions from the fact rep ository , as all our exp erimen ts use a proprietary distributed architecture rather than a serial system.

Another data-driv en approac h to QA is evaluated in [17] on the same set (TrecQa) of temp oral questions from TREC. The answ ers are extracted freely from the Web through an external searc h engine, rather than con ned to a more lim-ited, local text collection. The authors rep ort an MRR score of 0.447, and indicate that the score is consisten tly above the sixth highest score of the systems participating in the TREC QA trac k. Therefore, our MRR score of 0.608 on TrecQa is comparable to the best performing systems.
Even though TREC questions represen t a standard benc h-mark in open-domain QA, additional test sets of domain-speci c questions are emplo yed in this pap er for a more thorough evaluation. Since there are man y possible domains of interest, the questions are selected suc h that they ask about the same type of information as questions from the TrecQa set of open-domain questions. A quic k analysis of the TrecQa set indicates that sev eral questions ask about someb ody's date of birth (Q1658: \What year was Robert Frost born?" , or Q1698: \When was Julius Caesar born?" ); about the date when something was invented (Q330: \When was the slinky invente d?" , or Q538: \When was the bar-c ode invente d?" ); and about the date when a movie was released (Q593: \When was the movie, Caligula, made?" , or Q1546: \What year was the movie 'Ole Yeller' made?" ).

The second test set, Actors, con tains questions in the for-mat \When was X born?" , based on the compiled lists of movie actors and actresses with their years of birth from Wikip edia, after discarding the list entries with unsp eci ed or appro ximated dates (e.g., \c. 1956" for Antonio Sanchez ). Since man y of the items in Actors are not well kno wn, four human sub jects were ask ed to select only the names they were familiar with, resulting in a third, smaller evaluation subset, ActorsKno wn. The fourth query set, Inventions, builds upon the complete list of inventions from an on-line educational site. 1 Eac h invention whose description clearly speci es the year is con verted into a \When was X invente d?" question and an asso ciated answ er key. Finally , the fth query set, Mo vies, pro duces questions \When was X released?" by com bining the lists of all-time top US box oce movies and all-time movies from the Internet Mo vie Database. 2 Mo vies released in 2003 or later are discarded. In con trast to the open-domain TREC questions in TrecQa, all the other query sets (Actors, ActorsKno wn, Inventions
Located at www.enc han tedlearning.com
Located at www.imdb.com Table 7: Samples from test sets of domain-sp eci c questions assem bled based on Wikip edia, Enc han ted Learning and the Internet Mo vie Database Table 8: Accuracy of answ ers retriev ed from the temp oral fact rep ository for domain-sp eci c ques-tions and Mo vies) represen t resources for domain-sp eci c evalua-tions. Table 7 illustrates questions from eac h test set.
Table 8 summarizes the answ er accuracy on eac h of the four sets of domain-sp eci c questions (see Section 5.1 for a description of the notations). The answ ers at rank 1, re-triev ed from the fact rep ository , are correct for as man y as 35% (988 of 2769) of the Actors questions. The percen t-age increases to 65% (401 of 611) on the smaller subset Ac-torsKno wn questions, con rming that it is easier to nd the year of birth of a better-kno wn person.

A common observ ation, across the domain-sp eci c test question sets, is that the KyMy system con guration again pro vides the best results. Enabling the morphological pro-cessing of query verbs (e.g., KyMn vs. KyMy) does not mo d-ify the scores, since it has no e ect on the domain-sp eci c questions of the test sets. Moreo ver, the iterativ e remo val of keyw ords (e.g., KnMn vs. KyMn) has a negligible e ect on the Actors and ActorsKno wn questions, with MRR score changes of only 0.003 and 0.004. In other words, text nuggets that are relev ant to these questions tend to con tain the per-son's name and the verb born . The situation is di eren t for Inventions and esp ecially for Mo vies questions, due to a more common occurrence of equiv alen t but lexically di eren t verbs in the relev ant facts. For example, the facts relev ant to Inventions sometimes men tion that a particular invention was designe d or even proposed rather than invente d . Simi-larly , a movie is often \set for release" or \due out" or \sche d-uled to hit theaters" , rather than simply released . As itera-tive keyw ord remo val goes into e ect, the question matc hes on more of the relev ant facts despite the initial verb-to-v erb mismatc h. The num ber of correctly answ ered questions and the MRR scores increase accordingly .

Although they cover only a few of the man y domains of interest, the four sets of domain-sp eci c questions are rep-resen tativ e as they target the same kind of temp oral rela-tions as questions from the TrecQa set. The MRR scores on the four question sets vary within an interv al from 0.379 on Actors to 0.683 on ActorsKno wn, and a sligh tly lower 0.677 on Mo vies. These scores can be considered very good, given that a) authoritativ e Web sites often arrange relev ant information suc h as movie release years in semi-structured format (e.g., tables), rather than sen tences in unstructured text; and b) the fact extraction mec hanism does not have kno wledge of authoritativ e sites, nor does it attempt to per-form more complex extraction from suc h sites.
After the formal evaluations on open-domain and domain-speci c questions, this section switc hes to a more exploratory analysis of the impact of the temp oral fact rep ository . For questions that inquire about recurren t events suc h as soccer comp etitions , tsunamis , presidential elections , or awar ds of wel l-known prizes , the underlying documen t collection often con tains man y correct answ ers. Some of the exp ected an-swers are more imp ortan t, or at least more popular than others. For instance, while the occurrence of earthquak es in the San Francisco area is not a rare event, the impact of most of them on the city is dwarfed by the earthquak e of April 1906. In order to answ er questions suc h as \When was the San Francisc o earthquake?" or \When was ther e an earth-quake in San Francisc o?" , a QA system must iden tify man y candidate answ ers (ideally including 1906), as well as collect signi can t amoun ts of evidence supp orting eac h of the can-didate answ ers to e ectiv ely rank them. This comes at odds with the fact that QA systems operating on large text collec-tions are designed as precision-orien ted rather than recall-orien ted systems, as they try to nd a correct-answ er needle in a haystac k of textual documen ts. In fact, as the num ber of exp ected answ ers increases, the performance of QA systems degrades [25]. Part of the reason is the use of a documen t retriev al stage, whic h introduces a hard limit on the num ber of documen ts being searc hed for poten tial answ ers, whic h in turn limits the num ber of poten tial answ ers.

By asso ciating text nuggets with dates when the events captured in the nuggets must have occurred, the temp oral fact rep ository constitutes a simple yet powerful resource for questions about recurring events. The interpretation of the system output is that an answ er is not just a returned date, but rather consists of the group of text nuggets asso ciated with a common, returned date. The text nuggets indirectly Figure 2: Top ten answ ers reordered by date, re-triev ed from the fact rep ository for questions about recurring events rev eal multiple answ ers, as they occur in their textual con-text. Concurren tly, the asso ciated dates pro vide a temp oral anc hor to the answ ers, and allo w for their organization into timelines. As a pro of of concept, Figure 2 illustrates the top ten dates retriev ed for three questions asking about recur-ring events. When arranged in temp oral order, the top ten dates retriev ed for \When did Brazil win the World Cup?" range from 1958 to 2002 . Ab ove eac h returned date, the g-ure displa ys the rst supp orting text nugget retriev ed from the fact rep ository , without any mo di cations. The num bers displa yed in paren theses under the dates represen t their po-sition (rank) in the list of answ ers for that question. Thus, the rst answ er to \When did Brazil win the World Cup?" is the date 1994 with its text nugget \To all soccer fans some inter esting stu ... Brazil last won the world cup" .
The presence of a tick next to a rank in Figure 2 indi-cates that the corresp onding answ er is correct, based on a comparison against information assem bled man ually from other sources. An answ er is considered to be correct if ei-ther the date is a correct, direct answ er, or the supp orting text nugget con tains the correct answ er. Out of the top ten answ ers, the rst, second and third question have six, nine, and eigh t correct answ ers resp ectiv ely, although the top four answ ers are correct for the three questions.
Implicitly enco ded kno wledge on the Web is often con-tained within localized text nuggets, whic h are not only submerged inside long documen ts whose main topic may be completely di eren t, but also scattered across unconnected documen ts from di eren t sources. This pap er describ es a ligh tweigh t approac h for collecting, accessing and exploiting one of the man y useful types of text nuggets, namely those asso ciated with dates in unstructured text. Through a sim-pli ed architecture that consists of oine fact extraction and online fact retriev al, the nuggets pro vide answ ers directly to a variet y of open-domain temp oral questions. The corre-sponding MRR score on a set of TREC questions asking about dates is 0.608, whic h compares favorably to the per-formance on the same test question set of the top-p erforming systems in TREC. It is also higher that the scores of other studies that exp erimen t with oine extraction from docu-men ts as an alternativ e source of answ ers in QA, and ad-ditional work that extracts answ ers from the Web. Even more encouraging, we feel that the Web-deriv ed fact rep os-itories can answ er open-domain temp oral questions at even higher levels of accuracy that those obtained in this pap er, since man y pro cessing details were simpli ed on purp ose to test the limits of the approac h. For example, a more com-plex date recognition should result in a higher num ber of extracted facts. Other poten tial sources for increasing the coverage of the fact rep ository are online collections of news articles, by exploiting the time stamp asso ciated to eac h ar-ticle in order to disam biguate relativ e dates men tioned in the article into absolute dates [9]. Similarly , individual facts extracted from Web documen ts are not curren tly asso ciated with any estimated con dence or imp ortance value, e.g. as a function of the PageRank [4] asso ciated to the documen t from whic h a fact is extracted. Suc h values could be ex-ploited during retriev al, to promote dates asso ciated to more reliable text nuggets.

The pap er tests the abilit y of the fact rep ository to fur-nish direct answ ers to additional test sets of domain-sp eci c questions. The answ er accuracy corresp onds to MRR scores between 0.379 and 0.683. By covering four of the man y possible target domains, the sets of domain-sp eci c ques-tions complemen t well the evaluation on TREC questions, and pro vide insigh ts into the poten tial coverage of the fact rep ository . The matc hing nuggets also naturally form time-lines that capture multiple answ ers to fact-seeking queries asking about recurring events.

Temp oral questions are only one of the categories of open-domain questions that can be answ ered through fact rep os-itories extracted from the Web. The exact same approac h applies towards answ ering questions about locations, for in-stance \Wher e was Tesla born?" , or \What country are Go-diva chocolates from?" . Similarly , it can pro duce answ ers to questions about prop erties of objects and entities, suc h as \Who was the 23rd president of the Unite d States?" , or \What is the second highest mountain peak in the world?" , or \What is the capital of Mongolia?" . To answ er non-temp oral questions, the required mo di cations are limited to the ex-traction patterns, to target other types of localized facts (adv erbial and other types of relativ e clauses, app ositiv es etc.). Earlier work suggests that the creation of regular ex-pressions to deal with a much wider range of types of facts is not only possible, but in fact it can be quite successful at answ ering open-domain questions [20]. [1] S. Abney , M. Collins, and A. Singhal. Answ er [2] J. Allan, V. Khandelw al, and R. Gupta. Temp oral [3] T. Bran ts. TnT -a statistical part of speech tagger. In [4] S. Brin and L. Page. The anatom y of a large scale [5] H. Chieu and Y. Lee. Query based event extraction [6] S. Dumais, M. Bank o, E. Brill, J. Lin, and A. Ng. [7] A. Echihabi and D. Marcu. A noisy-c hannel approac h [8] O. Etzioni, M. Cafarella, D. Downey , S. Kok, A.-M. [9] E. Filato va and E. Hovy. Assigning time-stamps to [10] M. Fleisc hman, E. Hovy, and A. Echihabi. Oine [11] M. Hearst. Automatic acquisition of hyponyms from [12] J. Ko, T. Mitam ura, and E. Nyb erg. Language [13] J. Kupiec. MURAX: A robust linguistic approac h for [14] C. Kw ok, O. Etzioni, and D. Weld. Scaling question [15] J. Lin. An exploration of the principles underlying [16] J. Lin and B. Katz. Question answ ering from the Web [17] L. Lita and J. Carb onell. Instance-based question [18] J. Pustejo vsky , J. Castano, R. Ingria, R. Sauri, [19] D. Radev, W. Fan, H. Qi, H. Wu, and A. Grew al. [20] D. Ravichandran and E. Hovy. Learning surface text [21] E. Saquete, P. Martinez-Barco, R. Munoz, and [22] S. Tellex, B. Katz, J. Lin, A. Fernandez, and [23] E. Voorhees and D. Tice. Building a [24] Y. Wu, R. Zhang, X. Hu, and H. Kashiok a. Learning [25] H. Yang and T. Chua. Web-based list question
