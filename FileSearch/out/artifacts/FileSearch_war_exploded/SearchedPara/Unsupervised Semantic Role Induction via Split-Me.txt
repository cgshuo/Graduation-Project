 Recent years have seen increased interest in the shal-low semantic analysis of natural language text. The term is most commonly used to describe the au-tomatic identification and labeling of the seman-tic roles conveyed by sentential constituents (Gildea and Jurafsky, 2002). Semantic roles describe the re-lations that hold between a predicate and its argu-ments, abstracting over surface syntactic configura-tions. In the example sentences below. window oc-cupies different syntactic positions  X  it is the object of broke in sentences (1a,b), and the subject in (1c)  X  while bearing the same semantic role, i.e., the physical object affected by the breaking event. Anal-ogously, rock is the instrument of break both when realized as a prepositional phrase in (1a) and as a subject in (1b). (1) a. [Joe] A0 broke the [window] A1 with a
The semantic roles in the examples are labeled in the style of PropBank (Palmer et al., 2005), a broad-coverage human-annotated corpus of seman-tic roles and their syntactic realizations. Under the PropBank annotation framework (which we will as-sume throughout this paper) each predicate is as-sociated with a set of core roles (named A 0, A 1, A 2, and so on) whose interpretations are specific to that predicate 1 and a set of adjunct roles (e.g., loca-tion or time ) whose interpretation is common across predicates. This type of semantic analysis is admit-tedly shallow but relatively straightforward to auto-mate and useful for the development of broad cov-erage, domain-independent language understanding systems. Indeed, the analysis produced by existing semantic role labelers has been shown to benefit a wide spectrum of applications ranging from infor-mation extraction (Surdeanu et al., 2003) and ques-tion answering (Shen and Lapata, 2007), to machine translation (Wu and Fung, 2009) and summarization (Melli et al., 2005).

Since both argument identification and labeling can be readily modeled as classification tasks, most state-of-the-art systems to date conceptualize se-mantic role labeling as a supervised learning prob-lem. Current approaches have high performance  X  a system will recall around 81% of the arguments correctly and 95% of those will be assigned a cor-rect semantic role (see M ` arquez et al. (2008) for details), however only on languages and domains for which large amounts of role-annotated training data are available. For instance, systems trained on PropBank demonstrate a marked decrease in per-formance (approximately by 10%) when tested on out-of-domain data (Pradhan et al., 2008).

Unfortunately, the reliance on role-annotated data which is expensive and time-consuming to produce for every language and domain, presents a major bottleneck to the widespread application of semantic role labeling. Given the data requirements for super-vised systems and the current paucity of such data, unsupervised methods offer a promising alternative. They require no human effort for training thus lead-ing to significant savings in time and resources re-quired for annotating text. And their output can be used in different ways, e.g., as a semantic prepro-cessing step for applications that require broad cov-erage understanding or as training material for su-pervised algorithms.

In this paper we present a simple approach to un-supervised semantic role labeling. Following com-mon practice, our system proceeds in two stages. It first identifies the semantic arguments of a pred-icate and then assigns semantic roles to them. Both stages operate over syntactically analyzed sentences without access to any data annotated with semantic roles. Argument identification is carried out through a small set of linguistically-motivated rules, whereas role induction is treated as a clustering problem. In this setting, the goal is to assign argument instances to clusters such that each cluster contains arguments corresponding to a specific semantic role and each role corresponds to exactly one cluster. We formu-late a clustering algorithm that executes a series of split and merge operations in order to transduce an initial clustering into a final clustering of better qual-ity. Split operations leverage syntactic cues so as to create  X  X ure X  clusters that contain arguments of the same role whereas merge operations bring together argument instances of a particular role located in different clusters. We test the effectiveness of our induction method on the CoNLL 2008 benchmark dataset and demonstrate improvements over compet-itive unsupervised methods by a wide margin. As mentioned earlier, much previous work has focused on building supervised SRL systems (M ` arquez et al., 2008). A few semi-supervised ap-proaches have been developed within a framework known as annotation projection . The idea is to com-bine labeled and unlabeled data by projecting an-notations from a labeled source sentence onto an unlabeled target sentence within the same language (F  X  urstenau and Lapata, 2009) or across different lan-guages (Pad  X  o and Lapata, 2009). Outwith annota-tion projection, Gordon and Swanson (2007) attempt to increase the coverage of PropBank by leveraging existing labeled data. Rather than annotating new sentences that contain previously unseen verbs, they find syntactically similar verbs and use their annota-tions as surrogate training data.

Swier and Stevenson (2004) induce role labels with a bootstrapping scheme where the set of la-beled instances is iteratively expanded using a clas-sifier trained on previously labeled instances. Their method is unsupervised in that it starts with a dataset containing no role annotations at all. However, it re-quires significant human effort as it makes use of VerbNet (Kipper et al., 2000) in order to identify the arguments of predicates and make initial role assign-ments. VerbNet is a broad coverage lexicon orga-nized into verb classes each of which is explicitly associated with argument realization and semantic role specifications.

Abend et al. (2009) propose an algorithm that identifies the arguments of predicates by relying only on part of speech annotations, without, how-ever, assigning semantic roles. In contrast, Lang and Lapata (2010) focus solely on the role induction problem which they formulate as the process of de-tecting alternations and finding a canonical syntactic form for them. Verbal arguments are then assigned roles, according to their position in this canonical form, since each position references a specific role. Their model extends the logistic classifier with hid-den variables and is trained in a manner that makes use of the close relationship between syntactic func-tions and semantic roles. Grenager and Manning (2006) propose a directed graphical model which re-lates a verb, its semantic roles, and their possible syntactic realizations. Latent variables represent the semantic roles of arguments and role induction cor-responds to inferring the state of these latent vari-ables.

Our own work also follows the unsupervised learning paradigm. We formulate the induction of semantic roles as a clustering problem and propose a split-merge algorithm which iteratively manipulates clusters representing semantic roles. The motiva-tion behind our approach was to design a concep-tually simple system, that allows for the incorpo-ration of linguistic knowledge in a straightforward and transparent manner. For example, arguments occurring in similar syntactic positions are likely to bear the same semantic role and should therefore be grouped together. Analogously, arguments that are lexically similar are likely to represent the same semantic role. We operationalize these notions us-ing a scoring function that quantifies the compatibil-ity between arbitrary cluster pairs. Like Lang and Lapata (2010) and Grenager and Manning (2006) our method operates over syntactically parsed sen-tences, without, however, making use of any infor-mation pertaining to semantic roles (e.g., in form of a lexical resource or manually annotated data). Per-forming role-semantic analysis without a treebank-trained parser is an interesting research direction, however, we leave this to future work. We follow the general architecture of supervised se-mantic role labeling systems. Given a sentence and a designated verb, the SRL task consists of identify-ing the arguments of the verbal predicate (argument identification) and labeling them with semantic roles (role induction).

In our case neither argument identification nor role induction relies on role-annotated data or other semantic resources although we assume that the in-put sentences are syntactically analyzed. Our ap-proach is not tied to a specific syntactic representa-tion  X  both constituent-and dependency-based rep-resentations could be used. However, we opted for a dependency-based representation, as it simplifies ar-gument identification considerably and is consistent with the CoNLL 2008 benchmark dataset used for evaluation in our experiments.

Given a dependency parse of a sentence, our sys-tem identifies argument instances and assigns them to clusters. Thereafter, argument instances can be labeled with an identifier corresponding to the clus-ter they have been assigned to, similar to PropBank core labels (e.g., A 0, A 1). In the supervised setting, a classifier is employed in order to decide for each node in the parse tree whether it represents a semantic argument or not. Nodes classified as arguments are then assigned a se-mantic role. In the unsupervised setting, we slightly reformulate argument identification as the task of discarding as many non-semantic arguments as pos-sible. This means that the argument identification component does not make a final positive decision for any of the argument candidates; instead, this de-cision is deferred to role induction. The rules given in Table 1 are used to discard or select argument can-didates. They primarily take into account the parts of speech and the syntactic relations encountered when traversing the dependency tree from predicate to ar-gument. For each candidate, the first matching rule is applied.

We will exemplify how the argument identifica-tion component works for the predicate expect in the sentence  X  The company said it expects its sales to remain steady  X  whose parse tree is shown in Fig-ure 1. Initially, all words save the predicate itself are treated as argument candidates. Then, the rules from Table 1 are applied as follows. Firstly, words the and to are discarded based on their part of speech (rule (1)); then, remain is discarded because the path ends with the relation IM and said is discarded as the path ends with an upward-leading OBJ relation (rule (2)). Rule (3) does not match and is therefore not applied. Next, steady is discarded because there is a downward-leading OPRD relation along the path and the words company and its are discarded be-cause of the OBJ relations along the path (rule (4)). Rule (5) does not apply but words it and sales are kept as likely arguments (rule (6)). Finally, rule (7) does not apply, because there are no candidates left. 1. Discard a candidate if it is a determiner, in-2. Discard a candidate if the path of relations 3. Keep a candidate if it is the closest subject 4. Discard a candidate if the path between the 5. Discard a candidate if it is an auxiliary verb. 6. Keep a candidate if the predicate is its parent. 7. Keep a candidate if the path from predicate 8. Discard all remaining candidates.
 We treat role induction as a clustering problem with the goal of assigning argument instances (i.e., spe-cific arguments occurring in an input sentence) to clusters such that these represent semantic roles. In accordance with PropBank, we induce a separate set of clusters for each verb and each cluster thus repre-sents a verb-specific role.

Our algorithm works by iteratively splitting and merging clusters of argument instances in order to arrive at increasingly accurate representations of se-mantic roles. Although splits and merges could be arbitrarily interleaved, our algorithm executes a sin-gle split operation (split phase), followed by a se-ries of merges (merge phase). The split phase par-titions the seed cluster containing all argument in-stances of a particular verb into more fine-grained (sub-)clusters. This initial split results in a clustering with high purity but low collocation, i.e., argument instances in each cluster tend to belong to the same role but argument instances of a particular role are Figure 1: A sample dependency parse with depen-dency labels SBJ (subject), OBJ (object), NMOD (nominal modifier), OPRD (object predicative com-plement), PRD (predicative complement), and IM (infinitive marker). See Surdeanu et al. (2008) for more details on this variant of dependency syntax. located in many clusters. The degree of dislocation is reduced in the consecutive merge phase, in which clusters that are likely to represent the same role are merged. 5.1 Split Phase Initially, all arguments of a particular verb are placed in a single cluster. The goal then is to partition this cluster in such a way that the split-off clusters have high purity, i.e., contain argument instances of the same role. Towards this end, we characterize each argument instance by a key, formed by concatenat-ing the following syntactic cues:  X  verb voice (active/passive);  X  argument linear position relative to predicate  X  syntactic relation of argument to its governor;  X  preposition used for argument realization. A cluster is allocated for each key and all argument instances with a matching key are assigned to that cluster. Since each cluster encodes fine-grained syn-tactic distinctions, we assume that arguments occur-ring in the same position are likely to bear the same semantic role. The assumption is largely supported by our empirical results (see Section 7); the clusters emerging from the initial split phase have a purity of approximately 90%. While the incorporation of additional cues (e.g., indicating the part of speech of the subject or transitivity) would result in even greater purity, it would also create problematically small clusters, thereby negatively affecting the suc-cessive merge phase. 5.2 Merge Phase The split phase creates clusters with high purity, however, argument instances of a particular role are often scattered amongst many clusters resulting in a cluster assignment with low collocation. The goal of the merge phase is to improve collocation by ex-ecuting a series of merge steps. At each step, pairs of clusters are considered for merging. Each pair is scored by a function that reflects how likely the two clusters are to contain arguments of the same role and the best scoring pair is chosen for merging. In the following, we will specify which pairs of clus-ters are considered (candidate search), how they are scored, and when the merge phase terminates. 5.2.1 Candidate Search In principle, we could simply enumerate and score all possible cluster pairs at each iteration. In practice however, such a procedure has a number of draw-backs. Besides being inefficient, it requires a scoring function with comparable scores for arbitrary pairs of clusters. For example, let a , b , c , and d denote clusters. Then, score ( a , b ) and score ( c , d ) must be comparable. This is a stronger requirement than de-manding that only scores involving some common cluster (e.g., score ( a , b ) and score ( a , c ) ) be com-parable. Moreover, it would be desirable to ex-clude pairings involving small clusters (i.e., with few instances) as scores for these tend to be unre-liable. Rather than considering all cluster pairings, we therefore select a specific cluster at each step and score merges between this cluster and certain other clusters. If a sufficiently good merge is found, it is executed, otherwise the clustering does not change. In addition, we prioritize merges between large clus-ters and avoid merges between small clusters. Algorithm 1 implements our merging procedure. Each pass through the inner loop (lines 4 X 12) selects a different cluster to consider at that step. Then, merges between the selected cluster and all larger clusters are considered. The highest-scoring merge is executed, unless all merges are ruled out, i.e., have a score below the threshold  X  . After each comple-tion of the inner loop, the thresholds contained in the scoring function (discussed below) are adjusted and this is repeated until some termination criterion is met (discussed in Section 5.2.3).

Algorithm 1: Cluster merging procedure. Oper-ation merge ( L i , L j ) merges cluster L i into cluster
L j and removes L i from the list L . while not done do 2 L  X  a list of all clusters sorted by number 3 i  X  1 4 while i &lt; length ( L ) do 5 j  X  arg max 6 if score ( L i , L j )  X   X  then 7 merge ( L i , L j ) 8 end 9 else 10 i  X  i + 1 13 adjust thresholds end 5.2.2 Scoring Function Our scoring function quantifies whether two clusters are likely to contain arguments of the same role and was designed to reflect the following criteria: 1. whether the arguments found in the two clus-2. whether clause-level constraints are satisfied, 3. whether the arguments present in the two clus-
Qualitatively speaking, criteria (2) and (3) provide negative evidence in the sense that they can be used to rule out incorrect merges but not to identify cor-rect ones. For example, two clusters with drastically different parts of speech are unlikely to represent the same role. However, the converse is not neces-sarily true as part of speech similarity does not im-ply role-semantic similarity. Analogously, the fact that clause-level constraints are not met provides ev-idence against a merge, but the fact that these are satisfied is not reliable evidence in favor of a merge. In contrast, lexical similarity implies that the clus-ters are likely to represent the same semantic role. It is reasonable to assume that due to selectional re-strictions, verbs will be associated with lexical units that are semantically related and assume similar syn-tactic positions (e.g., eat prefers as an object edible things such as apple, biscuit, meat ), thus bearing the same semantic role. Unavoidably, lexical similarity will be more reliable for arguments with overt lex-ical content as opposed to pronouns, however this should not impact the scoring of sufficiently large clusters.

Each of the criteria mentioned above is quantified through a separate score and combined into an over-all similarity function, which scores two clusters c and c 0 as follows: score ( c , c 0 ) = The particular form of this function is motivated by the distinction between positive and negative evi-dence. When the part-of-speech similarity ( pos ) is below a certain threshold  X  or when clause-level constraints ( cons ) are satisfied to a lesser extent than threshold  X  , the score takes value zero and the merge is ruled out. If this is not the case, the lexical similar-ity score ( lex ) determines the magnitude of the over-all score. In the remainder of this section we will explain how the individual scores ( pos , cons , and lex ) are defined and then move on to discuss how the thresholds  X  and  X  are adjusted.
 Lexical Similarity We measure lexical similar-ity between two clusters through cosine similarity. Specifically, each cluster is represented as a vec-tor whose components correspond to the occurrence frequencies of the argument head words in the clus-ter. The similarity on such vectors x and y is then quantified as: Clause-Level Constraints Arguments occurring in the same clause cannot bear the same role. There-fore, clusters should not merge if the resulting clus-ter contains (many) arguments of the same clause. For two clusters c and c 0 we assess how well they satisfy this clause-level constraint by computing: where viol ( c , c 0 ) refers to the number of pairs of in-stances ( d , d 0 )  X  c  X  c 0 for which d and d 0 occur in the same clause (each instance can participate in at most one pair) and NC and NC 0 are the number of instances in clusters c and c 0 , respectively. Part-of-speech Similarity Part-of-speech similar-ity is also measured through cosine-similarity (equa-tion (3)). Clusters are again represented as vectors x and y whose components correspond to argument part-of-speech tags and values to their occurrence frequency. 5.2.3 Threshold Adaptation and Termination As mentioned earlier the thresholds  X  and  X  which parametrize the scoring function are adjusted at each iteration. The idea is to start with a very restrictive setting (high values) in which the negative evidence rules out merges more strictly, and then to gradually relax the requirement for a merge by lowering the threshold values. This procedure prioritizes reliable merges over less reliable ones.

More concretely, our threshold adaptation pro-cedure starts with  X  and  X  both set to value 0 . 95. Then  X  is lowered by 0 . 05 at each step, leaving  X  unchanged. When  X  becomes zero,  X  is lowered by 0 . 05 and  X  is reset to 0 . 95. Then  X  is iteratively decreased again until it becomes zero, after which  X  is decreased by another 0 . 05. This is repeated until  X  becomes zero, at which point the algorithm termi-nates. Note that the termination criterion is not tied explicitly to the number of clusters, which is there-fore determined automatically. In this section we describe how we assessed the per-formance of our system. We discuss the dataset on which our experiments were carried out, explain how our system X  X  output was evaluated and present the methods used for comparison with our approach. Data For evaluation purposes, the system X  X  out-put was compared against the CoNLL 2008 shared task dataset (Surdeanu et al., 2008) which provides PropBank-style gold standard annotations. The dataset was taken from the Wall Street Journal por-tion of the Penn Treebank corpus and converted into a dependency format (Surdeanu et al., 2008). In addition to gold standard dependency parses, the dataset also contains automatic parses obtained from the MaltParser (Nivre et al., 2007). Although the dataset provides annotations for verbal and nominal predicate-argument constructions, we only consid-ered the former, following previous work on seman-tic role labeling (M ` arquez et al., 2008). Evaluation Metrics For each verb, we determine the extent to which argument instances in a cluster share the same gold standard role (purity) and the extent to which a particular gold standard role is as-signed to a single cluster (collocation).

More formally, for each group of verb-specific clusters we measure the purity of the clusters as the percentage of instances belonging to the majority gold class in their respective cluster. Let N denote the total number of instances, G j the set of instances belonging to the j -th gold class and C i the set of in-stances belonging to the i -th cluster. Purity can then be written as: Collocation is defined as follows. For each gold role, we determine the cluster with the largest number of instances for that role (the role X  X  primary cluster) and then compute the percentage of instances that belong to the primary cluster for each gold role as: The per-verb scores are aggregated into an overall score by averaging over all verbs. We use the micro-average obtained by weighting the scores for indi-vidual verbs proportionately to the number of in-stances for that verb.

Finally, we use the harmonic mean of purity and collocation as a single measure of clustering quality: Comparison Models We compared our split-merge algorithm against two competitive ap-proaches. The first one assigns argument instances to clusters according to their syntactic function (e.g., subject, object) as determined by a parser. This baseline has been previously used as point of com-parison by other unsupervised semantic role label-ing systems (Grenager and Manning, 2006; Lang and Lapata, 2010) and shown difficult to outperform. Our implementation allocates up to N = 21 clus-ters 2 for each verb, one for each of the 20 most fre-quent functions in the CoNLL dataset and a default cluster for all other functions. The second compar-ison model is the one proposed in Lang and Lapata (2010) (see Section 2). We used the same model set-tings (with 10 latent variables) and feature set pro-posed in that paper. Our method X  X  only parameter is the threshold  X  which we heuristically set to 0 . 1. On average our method induces 10 clusters per verb. Our results are summarized in Table 2. We re-port cluster purity (PU), collocation (CO) and their harmonic mean (F1) for the baseline (Syntactic Function), Lang and Lapata X  X  (2010) model and our split-merge algorithm (Split-Merge) on four Table 3: Clustering results for individual verbs with our split-merge algorithm and the syntactic function baseline. datasets. These result from the combination of au-tomatic parses with automatically identified argu-ments (auto/auto), gold parses with automatic argu-ments (gold/auto), automatic parses with gold argu-ments (auto/gold) and gold parses with gold argu-ments (gold/gold). Bold-face is used to highlight the best performing system under each measure on each dataset (e.g., auto/auto, gold/auto and so on).
On all datasets, our method achieves the highest purity and outperforms both comparison models by a wide margin which in turn leads to a considerable increase in F1. On the auto/auto dataset the split-merge algorithm results in 9% higher purity than the baseline and increases F1 by 2 . 8%. Lang and Lap-ata X  X  (2010) logistic classifier achieves higher collo-cation but lags behind our method on the other two measures.

Not unexpectedly, we observe an increase in per-formance for all models when using gold standard parses. On the gold/auto dataset, F1 increases by 2 . 7% for the split-merge algorithm, 2 . 7% for the logistic classifier, and 5 . 5% for the syntactic func-tion baseline. Split-Merge maintains the highest pu-rity and levels the baseline in terms of F1. Perfor-mance also increases if gold standard arguments are used instead of automatically identified arguments. Consequently, each model attains its best scores on the gold/gold dataset.
 We also assessed the argument identification com-Table 4: Clustering results for individual semantic roles with our split-merge algorithm and the syntac-tic function baseline. ponent on its own (settings auto/auto and gold/auto). It obtained a precision of 88 . 1% (percentage of se-mantic arguments out of those identified) and recall of 87 . 9% (percentage of identified arguments out of all gold arguments). However, note that these fig-ures are not strictly comparable to those reported for supervised systems, due to the fact that our ar-gument identification component only discards non-argument candidates.

Tables 3 and 4 shows how performance varies across verbs and roles, respectively. We compare the syntactic function baseline and the split-merge sys-tem on the auto/auto dataset. Table 3 presents results for 12 verbs which we selected so as to exhibit var-ied occurrence frequencies and alternation patterns. As can be seen, the macroscopic result  X  increase in F1 (shown in bold face) and purity  X  also holds across verbs. Some caution is needed in interpret-ing the results in Table 4 3 since core roles A0 X  X 3 are defined on a per-verb basis and do not necessar-ily have a uniform corpus-wide interpretation. Thus, conflating scores across verbs is only meaningful to the extent that these labels actually signify the same role (which is mostly true for A0 and A1). Further-more, the purity scores given here represent the av-erage purity of those clusters for which the specified role is the majority role. We observe that for most roles shown in Table 4 the split-merge algorithm im-proves upon the baseline with regard to F1, whereas this is uniformly the case for purity.

What are the practical implications of these re-sults, especially when considering the collocation-purity tradeoff? If we were to annotate the clus-ters induced by our system, low collocation would result in higher annotation effort while low purity would result in poorer data quality. Our system im-proves purity substantially over the baselines, with-out affecting collocation in a way that would mas-sively increase the annotation effort. As an exam-ple, consider how our system could support humans in labeling an unannotated corpus. (The following numbers are derived from the CoNLL dataset 4 in the auto/auto setting.) We might decide to annotate all induced clusters with more than 10 instances. This means we would assign labels to 74% of instances in the dataset (excluding those discarded during argu-ment identification) and attain a role classification with 79 . 4% precision (purity). 5 However, instead of labeling all 165 , 662 instances contained in these clusters individually we would only have to assign labels to 2 , 869 clusters. Since annotating a cluster takes roughly the same time as annotating a single instance, the annotation effort is reduced by a factor of about 50. In this paper we presented a novel approach to un-supervised role induction which we formulated as a clustering problem. We proposed a split-merge al-gorithm that iteratively manipulates clusters repre-senting semantic roles whilst trading off cluster pu-rity with collocation. The split phase creates  X  X ure X  clusters that contain arguments of the same role whereas the merge phase attempts to increase col-location by merging clusters which are likely to rep-resent the same role. The approach is simple, intu-itive and requires no manual effort for training. Cou-pled with a rule-based component for automatically identifying argument candidates our split-merge al-gorithm forms an end-to-end system that is capable of inducing role labels without any supervision.
Our approach holds promise for reducing the data acquisition bottleneck for supervised systems. It could be usefully employed in two ways: (a) to cre-ate preliminary annotations, thus supporting the  X  X n-notate automatically, correct manually X  methodol-ogy used for example to provide high volume anno-tation in the Penn Treebank project; and (b) in com-bination with supervised methods, e.g., by providing useful out-of-domain data for training. An important direction for future work lies in investigating how the approach generalizes across languages as well as reducing our system X  X  reliance on a treebank-trained parser.
 Acknowledgments We are grateful to Charles Sutton for his valuable feedback on this work. The authors acknowledge the support of EPSRC (grant GR/T04540/01).
 The relations in Rule (2) from Table 1 are IM  X  X  X  , SUB  X  X  X  , ROOT  X  , TMP  X  , SBJ  X  , OPRD  X  . The sym-bols  X  and  X  denote the direction of the dependency arc (upward and downward, respectively).
 The relations in Rule (3) are ADV  X  X  X  , AMOD  X  X  X  , PRT  X  X  X  , PUT  X  X  X  , SBJ  X  X  X  , SUB  X  X  X  , SUFFIX  X  X  X  . De-pendency labels are abbreviated here. A detailed description is given in Surdeanu et al. (2008), in their Table 4.

