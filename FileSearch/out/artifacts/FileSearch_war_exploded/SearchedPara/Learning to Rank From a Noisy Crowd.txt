 We study how to best use crowdsourced relevance judgments learning to rank [1, 7]. We integrate two lines of prior work: unreliable crowd-based binary annotation for binary classi-fication [5, 3] and aggregating graded relevance judgments from reliable experts for ranking [7]. To model varying per-formance of the crowd, we simulate annotation noise with varying magnitude and distributional properties. Evalua-tion on three LETOR test collections reveals a striking trend contrary to prior studies: single labeling outperforms con-sensus methods in maximizing learner accuracy relative to annotator effort. We also see surprising consistency of the learning curve across noise distributions, as well as greater H.3.3 [ Information Search and Retrieval ] Algorithms, Design, Experimentation, Performance Crowdsourcing, learning to rank, active learning are changing the dynamics of how we train our learners. While labeled data is no longer as difficult to obtain, indi-vidual labels tend to be noisier and require greater quality assurance, e.g. by requesting redundant labels from mul-tiple annotators and resolving disagreements automatically via consensus [5, 3]. When annotation is noisy, how do we best utilize labeling effort to maximize learning? Do we la-bel additional examples (improve coverage), or request more labels for already labeled examples to reduce label noise [5]? How should we compute consensus with such multi-labeling? For learning to rank [1, 7], how sensitive is the learner to dif-ferent quanitities and distributions of label noise? https://www.mturk.com
Prior work compares single labeling ( SL ) each example to multi-labeling for binary classification [5]. Given a fixed seed set of N singly-labeled examples and an infinite pool of unlabeled examples, SL grows this set of labeled examples with each new label (to increase example coverage), whereas multi-labeling requests additional labels for examples in the seed set (to increase label accuracy). With 2 N labels, SL covers 2 N examples with a single label while multi-labeling covers the N examples with two labels each in round-robin fashion. Simple majority vote ( MV ) is used for consensus.
Our prior work [3] on binary classification compared SL to multi-labeling with Naive Bayes ( NB ) [6] as well as MV. We studied effects of modeling vs. ignoring worker accuracy and saw across methods that modeling worker accuracy signif-icantly improved classifier accuracy, indicating a limitation of the oft-used simple majority vote with noisy annotation. While a variety of crowd behaviors and noise may arise in practice, both prior studies [5, 3] assumed assumed uniform noise, as well as each label coming from a unique annota-tor. Other worker behaviors and noise characteristics may be observed in practice and could be usefully modeled.
Prior work by Yang et al. [7] studied learning to rank (with graded judgments) rather than binary classification, evalu-ating SL, MV, and other consensus algorithms for ranking with LambdaRank. They assumed labels come from reliable experts and provided limited analysis of the relationship be-tween consensus method and the resulting learning curve.
This paper extends our earlier study from binary classifi-cation to learning to rank, and we consider learning under different noise quantities and distributions. We compare SL, MV, and NB for consensus, and we measure resulting List-Net [1] ranking accuracy on three LETOR [4] collections: OHSUMED , MQ2007 and MQ2008 . We observed similar results across all three and so present results on OHSUMED only due to space constraints. We respect LETOR X  X  standard 5-fold partition with 3 training folds and the others for validation and testing. While training labels come entirely from the crowd, we make a significant assumption of having expert labels for the entire validation fold (  X  3500 examples). Note this validation data is used only by ListNet, not by consen-sus methods. We also use expert labels as ground truth for evaluation. This reflects a scenario in which more costly ex-pert annotation suffices for validation and testing, but larger volumes of more affordable data is desired for training.
We use a seed set size of N = 800 (potentially noisy) singly-labeled examples, reflecting a minimal training size to obtain stable results. The learning curve is then measured as a function of adding L additional labels. For each setting of L , we compute consenus labels (no-op for single labeling) and then train ListNet using them. We report label accuracy achieved as well as the resultant ranking accuracy achived by ListNet. We measure this across different noise settings.
We simulate noisy annotation via a fixed-size pool of 100 annotators who select between C = 3 possible labels (ternary graded relevance classes: non-relevant, relevant, or highly relevant). Each annotator i has a unique parameter p i de-noting the probability he will produce the correct label for a given example. Otherwise he produces one of the other two possible labels (uniformly) at random. New labels are gen-erated by selecting an annotator i from the pool at random and then generating a label according to p i as just described.
Results without annotation noise and for five possible noise settings are shown in Table 1. Noiseless ranking accu-racy with N = 800 L = 0 provides an approximate upper-bound for MV and NB consensus results across settings of L since perfect consensus would restore us to the noiseless condition. While level of noise clearly impacts the learning curve (Figure 1), we see relatively little impact of different noise distributions on ranking accuracy. Overall, it seems when average accuracy exceeds 50%, sufficient  X  X ood X  anno-tators exist to overcome the noise of their less reliable peers.
Between N = 800 and N = 1600, SL begins to con-sistently outperform NB and MV across noise distribu-tions, with greater example coverage apparently more im-portant than label accuracy. Effects here may be task-specific or learner-specific, and having expert validation la-bels may benefit SL more than MV and NB since SL label-ing accuracy on training examples is lowest. We aso see NB typically outperform MV across noise distributions.
We define an adversarial annotator for multi-class anno-tation with C classes as one whose p i &lt; 1 C . In such cases, a simple way fix is to randomly pick one of the other C  X  1 classes. We saw little benefit from doing so. Suppose an an-notator has accuracy 0.2. Assuming a uniform prior over remaining classes, each has probability 0.4, so not much higher than the class originally labeled. We expect more benefit from handling adversarial labeling when accuracy is extremely low (i.e. strongly anti-correlated), or when we have a better prior for selecting between remaining classes.
Future work includes: a similar study with real crowd workers and data, developing more representative models for simulation, studying additional consensus methods and noise settings, and dynamic example selection for labeling.
Acknowledgments . We thank the anonymous review-ers for their valuable feedback. Eunho Yang provided the ListNet implementation. This work was partially supported by a John P. Commons Fellowship for the second author.
