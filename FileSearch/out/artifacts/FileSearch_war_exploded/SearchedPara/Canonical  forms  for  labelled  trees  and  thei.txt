
Graphs are widely used to represent data and relationships. Among all graphs, a par-ticularly useful family is the family of trees . Trees in some applications are rooted: in the database area, XML documents are oft en rooted trees where vertices represent elements or attributes and edges represent element X  X ubelement and attribute X  X alue relationships; in web page traffic mining, access trees are used to represent the ac-cess patterns of different users (Zaki 2002). Trees in other applications are unrooted, phylogeny), which can be either a rooted tree or a free tree, is used to describe the evolution history of certain species (Hein et al. 1996); in pattern recognition, a free tree called shape axis tree is used to represent shapes (Liu and Geiger 1999); in computer networking, unrooted multicast trees are used for packet routing (Cui et al. 2002). From the above examples, we can also see that trees in real appli-cations are often labelled , with labels attached to vertices and edges where these labels are not necessarily unique. In this paper, we study some issues in mining databases of labelled trees, where the trees can be either rooted unordered trees or free trees. Recently, there has been growing interest i n mining databases of graphs and trees.
Inokuchi et al. (2000) presented an algorithm, AGM, and Kuramochi and Karypis (2001) presented another algorithm, FSG, both for mining subgraphs in a graph database. The two algorithms used a levelwise Apriori (Agrawal and Srikant 1994) approach: the AGM algorithm extends subgraphs by adding a vertex per level and the FSG algorithm by adding an edge. Yan and Han (2002, 2003) and Huan et al. (2003) presented subgraph mining algorithms based on traversing different enumer-ation trees. However, to check if a transaction supports a graph is an instance of the subgraph isomorphism problem, which is NP-complete (Garey and Johnson 1979); to check if two graphs are isomorphic (in order to avoid creating a candidate multi-ple times) is an instance of the graph isomorphism problem, which is not known to be in either P or NP-complete (Garey an d Johnson 1979). Therefore, without taking advantage of the tree structure, these gra ph algorithms are not likely to be efficient for the frequent tree mining problem.

Many recent studies have focused on dat abases of trees partially because of the increasing popularity of XML in databases. Chen et al. (2001) provided data struc-tures and algorithms to accurately estimate the number of occurrences of a small tree (twig) in a large tree. Termier e t al. (2002) presented an algorithm, TreeFinder , which finds a subset of frequent trees in a set of tree-structured XML data. Shasha et al. (2002) gave a detailed survey on keytree and keygraph searching in databases of trees and graphs and the survey focused mainly on approximate containment queries.
The work in Termier et al. (2002) and Shasha et al. (2002), however, does not guar-antee completeness, i.e. some frequent s ubtrees may not be in the search results.
In a recent paper, Zaki (2002) presented an algorithm called TreeMiner to discover all frequent embedded subtrees, i.e. those subtrees that preserve ancestor X  X escendent presented algorithm FREQT to discover frequent rooted ordered subtrees. They used a string encoding similar to that defined by Zaki (2002) and built an enumeration tree for all (frequent) rooted ordered trees. The rightmost expansion is used to grow the enumeration tree.

Asai et al. (2003), Nijssen and Kok (2003) and R X ckert and Kramer (2004) in-dependently proposed canonical forms for labelled trees that are similar to the ones that we will be describing in this paper. We introduce their results and compare them with our work in the following sections.
The main contributions of this paper are (1) We introduce two new canonical forms, which are based on breadth-first traversal and depth-first traversal, respectively, to uniquely represent a labelled rooted unordered tree. We give the procedures to con-struct the canonical forms and study some properties of the canonical forms. (2) In order to mine frequent labelled rooted unordered subtrees, based on our breadth-first canonical form, we define an enumeration tree to systematically enumerate all (frequent) labelled rooted unordered t rees. (3) Then we extend our definition of canonical form to labelled free trees and introduce an Apriori-like algorithm to mine all (frequent) labelled free trees efficiently. (4) Finally, we have imple-mented all of our algorithms and have carried out extensive experimental analysis.
We use both synthetic data and real application data to evaluate the performance of our algorithms.
 give the background that includes the concepts we will be using in the paper and the definition of the frequent subtree min ing problem. Section 2 introduces the two canonical forms. Section 3 presents our algorithm, RootedTreeMiner , for mining fre-quent subtrees in a database of labelled rooted unordered trees. The algorithm is based on an enumeration tree that enumerates all (frequent) subtrees in their breadth-first canonical form. Section 4 presents our algorithm, FreeTreeMiner , for mining frequent subtrees in a database of labelled free trees. This algorithm is based on the extended definition of the depth-first canoni cal form for free trees. Section 5 includes experiments and performance analysis. Finally, Sect. 6 concludes our work and gives future directions. remainder of the paper.
 alphabet  X  for vertex and edge labels and a labelling function L assigns labels to vertices and edges. In this paper, we assume that the elements in
 X  areatomicbut  X  is not necessarily finite. A graph is directed ( undirected )when each edge is an ordered (unordered) pair of vertices. A path is a list of vertices the same. A graph is acyclic if the graph contains no cycle. A graph is connected if there exists at least one path between any pair of vertices, and is disconnected otherwise. A free tree is an undirected graph that is connected and acyclic. A rooted tree is a free tree with a distinguished vertex that is called the root . In a rooted tree, if vertex v is on the path from the root to vertex w ,then w is a descendent of v . If, in addition, v and w are adjacent, then w and w is a child of v . A rooted ordered tree is a rooted tree that has a predefined left-to-right order among children o f each vertex. A labelled free tree t is a subtree of another labelled free tree s if t can be obtained from s by repeatedly removing vertices with degree 1. Subtrees of rooted trees are defined similarly. Two labelled free trees t and s are isomorphic to each other if there is a one-to-one mapping from the vertices of t to the vertices of s that preserves vertex labels, edge labels, and adjacency. Isomorphisms for rooted tr ees are defined similarly except that the mapping should preserve the roots as well. An automorphism is an isomorphism that maps from a tree to itself. A subtree isomorphism from t to s is an isomorphism from t to some subtree of s . For convenience, in this paper, we call a tree with k vertices a k -tree.
From a labelled rooted unordered tree, we can derive many labelled rooted ordered trees, as shown in Fig. 1. From these labelled rooted ordered trees, we want to uniquely select one as the canonical form to represent the corresponding labelled rooted unordered tree. It turns out that there are at least two ways we can define the unique representation X  X ne based on the breadth-first traversal and the other based on the depth-first traversal of the tree. We define these two representations and study their properties in this section.

Notice that, if a labelled tree is rooted, then without loss of generality, we can assume that all edge labels are identical: because each edge connects a vertex with its parent, so we can consider an edge, together with its label, as a part of the child vertex. (For the root, we can assume that there is a null edge connecting to it from above.) So for all the running examples in the following discussion, we assume that all edges in all trees have the same label or, equivalently, are unlabelled, and we therefore ignore all edge labels.
We first define the breadth-first string encoding for a rooted ordered tree. Assume there are two special symbols,  X $ X  and  X # X , which are not in the alphabet of edge labels and vertex labels. We also assume that (1) there exists a total ordering among edge and vertex labels and (2)  X # X  sorts g reater than  X $ X  and both sort greater than any other symbol in the alphabet of vertex and edge labels. The breadth-first string encoding of a rooted ordered tree is obtained by traversing the tree in a breadth-first order (also called level-order traversal (Valiente 2002)), level by level. Following the the string, we use  X $ X  to partition the families of siblings and use  X # X  to indicate the end of the string encoding. Now we prove the one-to-one correspondence between a labelled rooted ordered tree and its breadth-first string encoding.

Lemma 2.1. Each labelled rooted ordered tree corresponds to a unique breadth-first string encoding; each valid breadth-firs t string encoding corresponds to a unique labelled rooted ordered tree.

Proof. The first statement is implied by the definition of the breadth-first string en-coding. We prove the second statement by induction on the number of vertices k in a labelled rooted ordered tree. For the base case, when k this case, the corresponding labelled rooted ordered tree is a single vertex, which is unique. For the induction step, we assume that, for each breadth-first string en-coding S n with k = n vertices, there is a unique labelled rooted ordered tree K corresponding to it. We indicate the index for the parent vertex of the n th vertex in
K n as m . A valid breadth-first string encoding S n + the form S n $ ... $ l #. Obviously S n determines a unique labelled rooted ordered tree with n vertices. In addition, the last vertex (with label l ) either becomes the only right sibling of vertex n , if there is no  X $ X  between S child of a certain vertex with index i ,where m &lt; i  X  n and i is uniquely determined by the number of  X $ X  between S n and l in S n + 1 . In both cases, the labelled rooted ordered tree K n + 1 corresponding to S n + 1 is determined uniquely.
 corresponding breadth-first string encodings, by assigning different orders among the children of internal vertices. The breadth-first canonical string (BFCS) of the rooted unordered tree is defined as the minimal one among all these breadth-first string encodings, according to the lexicogra phic order. The corresponding orders among children of internal vertices define the breadth-first canonical form (BFCF), which is a rooted ordered tree, of the rooted unordered tree. The breadth-first string encodings for each of the four trees in Fig. 1 are for (a) A $ BB $ C $ DC #, for (b) A $ BB $ C $ CD #, for (c) A $ BB $ DC $ C # and for (d) A $ BB $ CD $ C #. The breadth-first string encoding for tree (d) is the BFCS, and tree (d) is the BFCF for the corresponding labelled rooted unordered tree. Notice that the total ordering on strings and the unique-ness of the breadth-first string encoding for a labelled rooted ordered tree guar-antee the uniqueness of the BFCS and the BFCF for a labelled rooted unordered tree.
 unordered tree. Starting from the bottom, level by level, for each vertex level, because by recursion all the s ubtrees induced by the children of in the correct forms, we can co mpare the string encodings of these subtrees and order them from left to right from small to large. We repeat the procedure until finally all the children of the root vertex are reordered. Figure 2 gives a running example on how to obtain the BFCF for a labelled rooted unordered tree. In the figure, the levels surrounded by the dashed boxes are the corresponding levels of vertices we are working on in each stage.
Theorem 2.1. The above construction procedure gives the BFCF for a labelled rooted unordered tree.

Proof. For a rooted unordered tree t , we denote the root of t by r , the children of r by r 1 ,..., r m , and the subtrees induced by r 1 the recursive construction procedure and b ecause of the fact that a tree consisting of a single vertex is in its BFCF, we only have to prove the following statement: If all t ,..., t left to right in nondecreasing order (accordin g to the lexicographic order among their
BFCSs) to get the rooted ordered tree t ,then t is the BFCF for t . We prove this statement by contradiction. If, for the sake of contradiction, the BFCF of t is t , such that there are a pair of BFCF trees among t
BFCS ( t r i )&lt; BFCS ( t r j ) ,but t r i is to the right of t into segments s i 1 ,..., s ih , where each segment s of t i at level p of the tree t .Wedothesameto t r j to get s
BFCS ( t r i )&lt; BFCS ( t r j ) , there is an integer q ,where1 for 0  X  p &lt; q and s iq = s jq . With s ip = s jp for 0 number of  X $ X  symbols in s iq and s jq . (Actually, for both t of  X $ X  symbols at level q is the same as the number of vertices at level q
In addition, both s iq and s jq end with a  X $ X . Therefore, s which implies s iq &lt; s jq because BFCS ( t r i )&lt; BFCS the order of t r i and t r j in t , we will get a breadth-first encoding that is smaller than the string encoding of t ; hence, a contradiction.
 Theorem 2.2. The above BFCF construction procedure has time complexity
O ( k 2 c log c ) ,where k is the number of vertices the tree has and c is the maximal degree of the vertices in the tree.

Proof. For each vertex v , to order all its children takes O because the comparisons are among subtrees induced by the children of parison takes O ( k ) time. The tree has k vertices; therefore, the total time complexity for normalization is O ( k 2 c log c ) .

Theorem 2.3. The length of the BFCS for a labelled rooted unordered tree is at most 3 k ,where k is the number of vertices of the tree.

Proof. The symbols in the BFCS for a labelled rooted unordered tree include vertex labels, edge labels,  X $ X  symbols and a  X # X  symbol. The number of vertices is k and the number of edges is k  X  1, so the number of symbols in the BFCS for edge/vertex labels is 2 k  X  1. There is one  X $ X  at the root level. For all levels below, the number
In addition, the last  X $ X  will be replaced by a  X # X . So the BFCS contains one  X # X  and at most k  X $ X  symbols. Therefore, the length of the BFCS is at most 3 k .
Now we introduce a second way to define a canonical form. We first define the depth-first string encoding for a rooted ordered tree through a depth-first traversal (also called the preorder traversal (Valiente 2002)) and use  X $ X  to represent a backtrack and  X # X  to represent the end of the string encoding. Similar to the case of the breadth-first string encoding, there is a one-to-one correspondence between a labelled rooted ordered tree and its depth-first string encoding.

Lemma 2.2. Each labelled rooted ordered tree corresponds to a unique depth-first string encoding; each valid depth-first st ring encoding corresponds to a unique la-belled rooted ordered tree.

Proof. The first statement is implied by the definition of the depth-first string en-coding. We prove the second statement by induction on the number of vertices k in a labelled rooted ordered tree. For the base case, when k this case, the corresponding labelled rooted ordered tree is a single vertex, which is unique. For the induction step, we assume th at, for each depth-first string encoding
S n with k ponding to it. A valid depth-first string encoding S of the form S n $ ... $ l #. Obviously S n determines a unique labelled rooted ordered tree with n vertices. In addition, the last vertex (with label l ) becomes the rightmost child of vertex n or one of the ancestors of vertex n , depending on the number of  X $ X  between S n and l in S n + 1 . As a result, the labelled rooted ordered tree K corresponding to S n + 1 is determined uniquely.
 (a) ABC $$ BD $ C #, for tree (b) ABC $$ BC $ D #, for tree (c) ABD $ C $$ BC #andfor tree (d) ABC $ D $$ BC #. If we define the depth-first canonical string (DFCS) of the rooted unordered tree as the minimal one among all possible depth-first string encod-ings, then we can define the depth-first canonical form (DFCF) of a rooted unordered tree as the corresponding rooted ordered tree that gives the minimal DFCS. In Fig. 1, the depth-first string encoding for tree (d) is the DFCS, and tree (d) is the DFCF for the corresponding labelled rooted unordered tree. Again, the total ordering on strings and the uniqueness of the depth-first string encoding for a labelled rooted ordered tree guarantee the uniqueness of the DFCS and DFCF for a labelled rooted unordered tree.
 a tree isomorphism algorithm given by Aho et al. (1974). The algorithm sorts the vertices of the rooted unordered tree level by level bottom up. When sorting vertices ranks (in order) of each of the children (in th eir own level) of these vertices. Figure 3 is a running example for the algorithm. In the figure, for each vertex, the symbols ( X # X  denotes the end of the encoding); the symbol in front of the parentheses is the rank of the vertex in its level. After sorting all levels, the tree is scanned top down level by level, starting from the root, and children of each vertex in the current level are rearranged to be in the determined order.

Theorem 2.4. The above construction procedure gives the DFCF for a labelled rooted unordered tree.

Proof. For a rooted unordered tree t , we denote the root of t by r , the children of r by r 1 ,..., r m , and the subtrees induced by r 1 the recursive construction procedure and b ecause of the fact that a tree consisting of a single vertex is in its DFCF, we only have to prove the following two statements: (1) If all t r 1 ,..., t r m are in their DFCFs and we rearrange the order among t from left to right in nondecreasing order (acco rding to the lexicographic order among their DFCSs) to get the rooted ordered tree t ,then t is the DFCF for t ;(2)The rank among r 1 ,..., r m , which is given by the above construction procedure, is the same as the order of t r 1 ,..., t r m according to the lexicographic order of their DFCSs.
For statement (1), we note that, in order to construct the string encoding of t ,we combine, in order, the vertex label of the root r , the string encoding of the subtree induced by r  X  X  first child, ... , the string encoding of the subtree induced by r  X  X  last child. (Note that there are some  X $ X  X  betw een to represent backtracks.) Obviously, nondecreasing order according to the lexic ographic order among their DFCSs, we will get the minimal string encoding (hence the DFCS) for t . We can prove statement (2) by recursion: it is trivially true for the bottom level; for each level above, the rank among the vertices in the level is obtained by first comparing vertex labels, then the rank (in order) of their children. From the argument in the proof of statement (1), we can see the resulting rank among the verti ces at each level is the same as the order of the corresponding subtrees induced by thes e vertices according to the lexicographic order of their DFCSs.
 Theorem 2.5. The above DFCF construction procedure has time complexity
O ( ck log k ) ,where k is the number of vertices the tree has and c is the maximal degree of the vertices in the tree.

Proof. Assuming there are k h vertices in level h of the tree for h sort vertices at level h takes O ( k h log k h ) comparisons; the total number of compar-isons for normalizing the whole tree is h O ( k h log k tice that h ( k h log k h )  X  h ( k h log k ) = k log k ); the time for each comparison is bounded by the maximal fan-out c of the tree because we can consider c as the length of the keys to be compared.

Theorem 2.6. The length of the DFCS for a labelled rooted unordered tree is at most 3 k  X  1, where k is the number of vertices of the tree.

Proof. In a DFCS, in addition to the 2 k  X  1 symbols representing edge/vertex labels, there are  X $ X  symbols to represent backtracks and a  X # X  symbol to represent the end of the DFCS. If we look at the procedure of depth-first traversal, we can see that each edge is visited at most twice: one forw ard visit and one b acktrack. A backtrack visited edge, but a  X # X  will be introduced in to the DFCS instead. Because there are k  X  1 edges in the tree, there are one  X # X  and at most k
DFCS. Therefore, the length of the DFCS is at most 3 k bound on the DFCS to 3 k  X  1.
 be the same. The example in Fig. 1 is a case in which they are the same and the example in Fig. 2 is a case in which they are not.
With canonical forms, we introduce a unique representation for labelled rooted un-ordered trees. With such a unique representation, a traditional indexing method such as hash tables can be used on databases of labelled rooted unordered trees. In add-ition, we also introduce a total order among labelled rooted unordered trees. Hence, we can apply traditional database indexing methods that depend on such a total order, such as B-trees, on databases of labelled rooted unordered trees. It will be shown shortly that the canonical forms, with the ab ove desirable properties, can be extended to labelled free trees as well. In the implementations of our algorithms that will be introduced in the following sections, we have used memory-resident data structures and used the container maps in the C++ standard library (which are implemented as balanced binary trees) to store frequent subtrees and candidates for frequent subtrees. We have used the canonical strings of the frequent subtrees as the key for the maps.
In future work, we plan to extend the algorithms to disk-resident data structures, such as B-trees.
In Aho et al. (1974), Aho et al. discussed the tree isomorphism problem and gave a bottom-up algorithm, with linear time complexity, to check whether or not two unlabelled rooted unordered trees are isomorphic. Their algorithm also works for la-belled rooted unordered trees, with the same time complexity if we assume the vertex labels to be integers in the range between 1 and n (so that a linear sorting, e.g. radix sorting, can be used). We have adopted Aho X  X  algorithm in our DFCF normaliza-tion procedure (as shown in Fig. 3). However, Aho X  X  algorithm compared two rooted unordered trees but did not normalize a given rooted unordered tree as we did. In
Valiente (2002), Valiente presented a very detailed discussion on the tree isomorph-ism problem for both rooted ordered trees and rooted unordered trees. For a rooted unordered tree T , Valiente defined the isomorphism code of T as a sequence of inte-gers in the range between 1 and | T | : Code [ root [ T ]] = [
Code[ w k ] , where vertices w 1 ,...,w k are the children of the root of T arranged in nondecreasing lexicographic order of isomorphism code. This isomorphism code de-fined a unique representation for a rooted unordered tree and introduced a linear ordering among all rooted unordered trees, as we did in the canonical strings and the canonical forms. Although the isomorphism code is defined for unlabelled trees, between Valiente X  X  isomorphism code and our canonical strings is that isomorphism code explicitly records the sizes of all (sub)trees while our canonical strings do not.
Because of this distinction, we can add a new vertex to the end of a canonical string or remove a vertex from the end of a canonical string and the result is a canoni-cal string. (In later sections, we will see that these operations are very important in frequent subtree mining.) In contrast, adding a new vertex to or removing a vertex from a tree in Valiente X  X  isomorphism code requires a global update (at least for trees rooted at ancestors of the newly added or removed vertex will change. In Buss (1997), Buss studied the tree isomorphism, the tree comparison, and the tree canon-ization problems. The canonical string representation in Buss (1997) is equivalent to the isomorphism code in Valiente (2002). Buss showed that tree isomorphism, tree comparison and tree canonization can all be solved in alternating logarithmic time.
However, all the algorithms and complexity analysis in Buss (1997) are oriented to-ward unlabelled trees. In other words, only the topological structures are used to distinguish the trees. In our canonical strings and canonical forms, the lexicographic order of vertex and edge labels play an equally important role in tree isomorphism, tree comparison and tree canonization.
 Independent of our work (Chi et al. 2003), Asai et al. (2003) and Nijssen and
Kok (2003) defined equivalent string encodings for labelled rooted ordered trees. In explicitly recorded in the string encoding. According to their definitions, the string encoding for tree (a) in Fig. 1 is (( 0 , A ), ( 1 , B ), ( each pair represents a vertex: the first num ber in the pair is the depth of the vertex and the second symbol is the vertex label. Both Asai et al. (2003) and Nijssen and
Kok (2003) defined the canonical form for a labelled rooted unordered tree as the labelled rooted ordered tree that gives the l exicographically minimal string encoding.
Their definitions can be shown to be equivalent to our DFCF.
We now apply the canonical forms for the labelled trees to the problem of mining frequent subtrees. In this section, we will show a method to use the BFCFs for labelled rooted unordered trees to discover all frequently occurring subtrees from a database of labelled rooted unordered trees. In the next section, we will extend the DFCF from rooted trees to free (or unroot ed) trees and develop an Apriori-like algorithm for mining all frequent free subtrees from a database of free trees. We first define the frequent subtree mining problem.
Let D denote a database where each transaction s  X  D is a labelled rooted unordered tree (or D is a database of free trees). For a given pattern t , which is a rooted unordered tree (or a free tree correspondingly), we say t occurs in a transaction s (or s supports t ) if there exists at least one subtree of s that is isomorphic to t .The support of a pattern t is the fraction of transactions in database D that support t .
A pattern t is called frequent if its support is greater than or equal to a minimum support ( minsup ) specified by a user. The frequent subtree mining problem is to find all frequent subtrees in a given database.

Closely related to the frequent subtree min ing problem is the market-basket asso-ciation rule mining, whose first step is to mine all frequent itemsets. Two categories of algorithms are used in frequent itemset mining. The first category of algorithms level. Apriori (Agrawal and Srikant 1994) is a representative algorithm of this cate-gory. The second category of algorithms put all frequent itemsets in an enumeration a depth-first traversal order, as in Agarwal et al. (2001). Algorithms of the latter type are usually called vertical mining algorithms. The main advantage of the Apriori-like algorithms is efficient pruning: an itemset becomes a potentially frequent candidate only if it passes the  X  X ll subsets are frequent X  check. The main advantage of ver-tical mining algorithms is their relatively small memory footprint: for Apriori-like algorithms, in order to generate candidate ( k + 1 ) are involved and hence must be in memory; in contrast, in vertical mining, only the parent of a candidate ( k + 1 ) -itemset in the enumeration tree needs to be in memory. uses an enumeration tree, which is based on the BFCF, to enumerate all (frequent) labelled rooted unordered subtrees. In the next section, we will give an Apriori-like algorithm, FreeTreeMiner , that mines all frequent labelled free subtrees, using an index based on the DFCF.
Enumeration trees are used extensively in frequent itemsets mining (Agarwal et al. 2001; Bayardo 1998). The enumeration tre e technique was first used for frequent subtree mining by Asai et al. (2002), where an enumeration tree is used to enumerate (frequent) labelled rooted ordered trees. Lat er, Asai et al. extended the enumeration tree to mining labelled rooted unordered trees (Asai et al. 2003). Independently, Nijs-sen et al. proposed a similar technique in Nijssen and Kok (2003). Both Asai et al. and Nijssen et al. built their enumeration trees based on canonical forms defined by the depth-first traversal. Here, we defin e an enumeration tree that enumerates all rooted unordered trees based on their BFCFs, which are defined by the breadth-first traversal.
 pattern. For convenience, we call a leaf (together with the edge connecting it to rightmost leaf at the bottom level the last leg . The following lemma provides the basis for our enumeration tree:
Lemma 3.1. Removing the last leg, i.e. the rightmost leg at the bottom level, from a rooted unordered ( k + 1 ) -tree in its BFCF will result in the BFCF for another rooted unordered k -tree.

Proof. We prove that, by removing the last leg l from a BFCF, there is no change to the order between any two subtrees i nduced by a pair of sibling vertices. Assume t and t 2 are two subtrees induced by a pair of sibling vertices and l belongs to t (then l must be the last leg of t 2 ). In the first case, if BFCS by removing the last leg, we change BFCS ( t 2 ) from  X  where l 1 is the vertex label of the last leg, l 2 is the vertex label of the second-to-last vertex in the BFCF, and there are 0 or more  X $ X  X  in between. Therefore BFCS is increased so the original order between t 1 and t if BFCS ( t 1 )&gt; BFCS ( t 2 ) , then because t 1 is to the right of t no leg of the BFCF belonging to t 1 . So the order between t at some level above the bottom level. In this case, removing a leg from the bottom level of t 2 does not change the original order between t second case, in some extreme situations, removing the last leg from t BFCS ( t 1 ) = BFCS ( t 2 ) .) of the enumeration tree consist of all rooted unordered trees in their BFCFs and the parent for each rooted unordered tree i s determined uniquely by removing the last leg from its BFCF. Figure 4 shows a fraction of the enumeration tree (for all rooted unordered subtrees with A as the root) for the BFCF tree at the bottom of the figure.
We want to grow the enumeration tree efficiently. Starting from a node enumeration tree, we need to find all valid children of v by adding a new vertex to v so that the new vertex becomes the new last leg of the new BFCF (recall that the last leg of a BFCF is the rightmost leaf at the bottom level ). Therefore, the possible position for adding the new last leg to a BFCF is the lower border oftheBFCF,asshowninFig.5.
 in their canonical forms, we never need to convert an arbitrary rooted unordered tree into its canonical form X  X fter adding a new vertex to a rooted unordered tree in its canonical form, we only need to check if th e resulting new tree is in the canonical form or not. As a result, the time complexity O ( k 2 unordered tree into the BFCF does not contribute to the complexity of our mining algorithm. Moreover, instead of adding a ver tex and then checking whether the result is in the canonical form, we can compute the range of vertices that are allowable at a given position before starting to add vertices.
 we want to add a new rightmost leg to the given position of the BFCF tree with 19 vertices. By adding a new vertex at the given position at the bottom, we may violate the BFCF by changing the order between some ancestor of the new vertex (including the vertex itself) and its immediate left sibling. In order to determine the range of allowable vertex labels for the new vertex (so that adding the new vertex will guarantee to result in a new BFCF), we check each vertex along the path from the new vertex to the root. For convenience of discussion, we attach an index number to each vertex and use T ( k ) to represent the maximal subtree rooted at vertex k (i.e. the vertex k plus all its descendants) and BFCF ( T (
T ( k ) .
  X  A # X  while BFCF ( T ( 20 )) =  X ?# X , and we need BFCF So the label of the new vertex must satisfy ?  X  A . Similarly, for comparison (b),
BFCF ( T ( 8 )) =  X  D $ AB # X , BFCF ( T ( 9 )) =  X  D $ A ?# X , and we require BFCF
BFCF ( T ( 9 )) . So the label of the new vertex must satisfy ? son (c), BFCF ( T ( 2 )) =  X  D $ DDD $ AB $ AC # X , BFCF ( and we require BFCF ( T ( 2 ))  X  BFCF ( T ( 3 )) . So the label of the new vertex must satisfy ?  X  C .
 adding a new vertex v (vertex 20 in Fig. 6) to a BFCF will change the string encod-ings of the subtrees rooted at v  X  X  ancestors ( T (
The second observation is that, because a dding a new vertex decreases the order of a string encoding, we only need to compare the new subtrees rooted at tors other than the root ( T ( 3 ) , T ( 9 ) and T ( 20 ) their corresponding left siblings ( T ( 2 ) , T ( 8 ) ,and T the range of the valid labels for the new vertex. Our RootedTreeMiner algorithm incorporates this technique in the Compute-Range function given in Fig. 8.
The occurrence list L t v for a rooted unordered k -tree t records information on each occurrence of t v in the database. Each element l of the form l = ( tid , i 1 ,..., i k ) ,where tid is the id of the transaction in the database that contains t v and i 1 ,..., i k represent the mapping between the vertex indices in t and those in the transaction. From the occurrence list L t v is frequent because the support of t v is the number of elements in L tid  X  X .
 t in BFCF. l can potentially be extended to an element l in the occurrence list L for a ( k + 1 ) -tree t v in BFCF, where t v is a child of t l = ( tid , i where i 1 ,..., i k , i k + 1 is a valid mapping between the vertex indices in t in the transaction identified by tid and (2) i k + 1
Figure 7 gives an example for support counting. On the top of Fig. 7 is a database consisting of two transactions. Below th e database is a portion of the enumeration tree with candidate 2-trees and candidate 3-trees (the roots of which have label A ) together with their occurrence list. The frequent subtrees are encircled by solid lines and the infrequent candidate trees are encircled by dashed lines (with minsup
Figure 8 summarizes our RootedTreeMiner algorithm. The main step in the algorithm is the function Enum-Grow , which grows the whole enumeration tree. Enum-Grow calls the function Compute-Range to obtain the range of valid labels for the newly added vertex at a given position.
We now derive an upper-bound for our frequent subtree mining algorithm. First, for the time to extend one node v in the enumeration tree (we assume that the tree t resented by v has k vertices and has height h ): to compute the allowable vertex label range at each position at the lower border of the BFCF takes O there are at most h comparisons and each comparison takes O total number of positions at the lower border of the BFCF is bounded by k , the total time for computing the allowable vertex labels at all possible positions is O
After computing the allowable range, we beg in scanning the database; for each trans-lower border of the BFCF all possible new vertices the transaction can introduce; therefore, the time f or each transaction is O ( kc ) ,where c is the maximal fan-out in the transaction. The total time for this step for the whole database is O finally, the time complexity of our algorithm is O ( | the set of all frequent subtrees, D is the database, h is the maximal height and k is the maximal size of all frequent subtrees, and c is the maximal degree among all vertices in all transac tions of the database.
If the transactions in the database are fre e trees, then the problem becomes mining to represent free trees compared with that of rooted trees. Therefore, the problem of mining frequent free trees seems to be a much more difficult problem compared with mining frequent rooted unordered trees. However, it turns out that we can easily we introduce an Apriori-like algorithm that uses the canonical forms to mine all frequent free subtrees from a database of labelled free trees. We choose to use the depth-first canonical form (DFCF), because of its efficient O algorithm.
Free trees do not have roots, but we can uniquely create roots for them for the purpose of constructing a unique canonical form. Starting from a free tree in each step, we remove all leaf vertices (together with their incident edges), and we repeat this step until a single vertex or two adjacen t vertices are left. For the first case, the free tree is called a centered tree and the remaining vertex is called the center ;for the second case, the free tree is called a bicentered tree and the pair of remaining vertices are called the bicenters (Aldous and Wilson 2000). A free tree is either centered or bicentered. The above procedure takes O ( k ) of vertices in the free tree. Figure 9 shows a centered free tree and a bicentered free tree as well as the procedure to obtain the corresponding center and bicenters.
If a free tree is centered, we can uniquely identify its center and make it the root to obtain a rooted unordered tree. Then we can normalize the rooted unordered roots (together with an edge connecting the m), then from a bicentered free tree we can again obtain a rooted tree (with a pai r of roots) and obtain the corresponding of roots is fixed in its canonical form.
 rion: first, by removing the edge connecting the bicenters, two rooted subtrees are obtained; then by comparing the canonical representations of the two rooted subtrees, the root of the subtree with lower lexicographical order is chosen as the unique root for the free tree.
Figure 10 gives FreeTreeMiner , our algorithm for solving the frequent subtree mining problem. This algorithm, like most previous studies on the frequent itemsets mining problem, is based on the bottom up Apriori method (Agrawal and Srikant 1994).
However, the number of patterns with 2 or fewer vertices is not very large; so for these patterns, to avoid the step of support checking, which is time-consuming, we have used a brute-force method: we scan all transactions in the database to find and count all vertices as well as subtrees with 2 vertices, then remove those that do not meet the minimum support requirement.
 (2) frequency counting. We now describe each in detail.
By the downward closure property, for a ( k + 1 ) -tree to be frequent, all its k -subtrees must be frequent. On the other hand, if we have discovered all the frequent k -trees, we can combine a pair of frequent k -trees to get a candidate for frequent as long as this pair of k -trees share all structure but one leaf vertex. This method is usually called a priori in the data-mining literature.
 assume the vertices of the ( k + 1 ) -tree are distinct. As we can see from the example given in Fig. 11, the answer is equal to the number of leaves the obtain a k -subtree, we need to remove one vertex (together with all edges incident with it) from the ( k + 1 ) -tree; the vertex to be removed can be any of the leaves but not any of the internal nodes, the removal of which will make the remaining graph disconnected.
In order to create candidate ( k + 1 ) -trees, a self-join on the list of all the frequent k -trees is needed. During the self-join, it is time consuming to determine if two the two frequent k -trees are joinable). Here we use a technique that is based on a container X  X he maps  X  X n the C++ standard library to expedite the self-join step.
For a frequent k -tree, we remove one of its leaves. The remaining graph is a tree with k  X  1 vertices. We call this ( k  X  1 ) -tree a core of the k -tree and the removed vertex (together with the removed edge) the corresponding limb . The number of cores a limb. A pair of frequent k -trees can be joined to obtain a candidate and only if they share a core with k  X  1 vertices. For each frequent k -tree, we can to all its cores, where the cores are stored in maps . We know that maps are stored internally as balanced binary trees in C++, so searching for a core in maps is a very efficient operation. For the maps container, we provide the DFCS of a core as its key for searching. Two frequent k -trees registered at the same core can be joined together to create candidate ( k + 1 ) -trees.
A frequent k -tree can have as many as k  X  1 cores. If each k -tree is registered to all its cores, considerable redundancy can result because there are multiple ways to create a candidate ( k + 1 ) -tree from frequent k -trees. For example, in Fig. 11, any to reduce the redundancy as much as possibl e. In other words, we want a candidate to be generated in a unique way.

We use the idea from the traditional market-basket data-mining problem. In the traditional market-basket problem, for example, although a 4-itemset abcd can be obtained in multiple ways by joining 3-itemsets, if we join a pair of 3-itemsets only if they share the prefix (after sorting by items), then the candidate abcd is generated in a unique way by joining abc and abd .

Following the same idea, we take advantage of the labels of the leaves of a tree, as shown in Lemma 4.1. Again, we first a ssume the labels among the leaves of our frequent trees are distinct.

Lemma 4.1. In generating candidate ( k + 1 ) -trees, we have to combine two frequent k -subtrees only if they share the same core and the corresponding limbs are the top 2 leaves in the resulting ( k + 1 ) -tree, where the top 2 leaves are defined by the order of labels.
 two maximal leaves (i.e. the top 2 leaves ) leaf A and leaf B .One k -subtree can be obtained from the ( k + 1 ) -tree by removing leaf A (call it tree1 ) and another by removing leaf B (call it tree2 ). Tree1 and tree2 share the same core. The core can be obtained from tree1 by removing leaf B , and from tree2 by removing leaf A .
A ( k + 1 ) -tree always has these two special k -subtrees. As a result, if we combine two frequent k -subtrees only when they share the same core and the corresponding limbs are the top 2 leaves in the resulting ( k + 1 candidate ( k + 1 ) -tree.
 not necessary, because not all leaves can become one of the top 2 leaves in the generated candidate ( k + 1 ) -tree. This is shown in Lemma 4.2.

Lemma 4.2. In generating candidate ( k + 1 ) -trees, for a frequent k -tree the vertex labels of which are distinct, we only have to consider two cores created by removing each of the top 2 leaves.

Proof. Continuing the proof of Lemma 4.1, leaf B is the top leaf of tree1 ,orthe second top leaf if removing leaf A from the candidate leaf with higher order than leaf B ; leaf A is the top or the second top leaf of tree2 .
Consequently, for tree1 , we only need to remove the top 2 leaves to guarantee leaf B register to the two cores created by removing each of the top 2 leaves . this is not the case, Lemma 4.3 extends the above results. In the lemma, we define the second among all leaf labels. For example, if the leaf labels of a frequent k -tree are { C , C , B , B , B , A } , then the leaves with top 2 labels are limbs labels are { C , B , B , B , A } , then the leaves with top 2 labels are limbs
Lemma 4.3. For trees with leaf labels that are not necessarily distinct, we need to change  X  X op 2 leaves X  in Lemma 4.1 to  X  X eaves with top 2 labels X  and to change  X  X wo cores created by removing each of the top 2 leaves  X  in Lemma 4.2 to  X  X ll cores created by removing each of the leaves with top 2 leaves X .
 { l ,..., l us a set of k -subtrees { t l 1 ,..., t l n } . Combining any pair, t and i = j ,ofthese k -subtrees will give us t . To extend Lemma 4.1, we notice that there is at least one pair of such k -subtrees, t i = j ), combining which will give us t , while the corresponding leaves l are among the leaves with top 2 labels in t . To extend Lemma 4.2, we notice that, when we combine two k -subtrees, say t 1 and t 2 , that share the same core in order in t 1 ,then l cannot be among the leaves with the top 2 labels in t either.
Automorphisms of a tree are the isomorphisms of the tree to itself. If the core of a tree has automorphisms, then the join procedure becomes more complicated. For example, the two trees in Fig. 12 create 9 candidate trees because of the automor-creating candidate ( k + 1 ) -trees, joining a frequent k -tree with itself is necessary.
Therefore, we need an efficient scheme to record all possible automorphisms of a DFCF and consider them while growing the enumeration tree. In order to record the information on tree automorphisms, we introduce the equivalence relation in the sense of automorphisms among vertices of a tree in its DFCF:
Definition 4.1 (Equivalence classe s in the sense of automorphisms). Vertices of a given tree in its DFCF belong to the same equivalence class if and only if (1) They are at the same level of the tree. the same DFCF.

The information on automorphisms can be obtained through the DFCF normaliza-tion procedure: after ordered vertices at al l levels, we apply the following procedure top-down recursively: the root is the only member in its equivalence class; all chil-dren with the same order at a given level belong to the same equivalence class if their parents belong to the same equivalence class. Figure 13 gives a running ex-ample for obtaining automorphisms. It X  X  obvious that this procedure of obtaining automorphisms has time complexity O ( ck log k ) .
 phisms of a core in the joining procedure: a limb attaches to its core through a ver-tex; if the vertex belongs to an equivalence class with multiple elements, then in the joining procedure, we consider all the combinations that result from attaching the limb to each element in the equivalence class.
 the RootedTreeMiner algorithm that is introduced in the previous section, automor-phisms are not an explicit issue because th ey do not affect the BFCF and the enumer-ation tree X  X  tree with automorphisms still has a unique BFCF and its parent in the numeration tree is still uniquely determined by removing the last leg. However, the occurrence list for a tree with automorphi sms is more compli cated because it must record all the possible permutations (due to automorphisms) of the mapping between the tree indices and those of the transactions. Fortunately, this is handled automat-ically by the incremental fashion of occurrence list building. For example, the first candidate 3-tree in Fig. 7 has an occurrence list L which implicitly stores the automorphism s for the corresponding candidate 3-tree.
In the last step of candidate generation, we use the downward closure checking to filter out those candidates that cannot be frequent. The downward closure property says that, in order for a candidate ( k + 1 ) -tree to be frequent, each of its k -subtrees must be frequent. As a result, after all candidate we check the downward closure property for each candidate by removing a leaf at a time from the candidate and checking if the remaining k -subtrees are all frequent.
If any of its k -subtrees fails to be frequent, a candidate downward closure checking and therefore can be eliminated.
To summarize, Fig. 14 gives the candidate generation procedure in our FreeTreeMiner algorithm. In the figure, F k represents the list of frequent k -trees, CL is the core list, only once, in step 5 and step 11 of the algorithm, we use our indexing technique, i.e. the depth-first canonical form (DFCF) for labelled free trees, to index all cores in CL and all candidate ( k + 1 ) -trees in C k + 1 . checking its support in the database. Because of the large memory footprint of the
Apriori-like algorithms, it is impractical to use the same occurrence list method that we have used in mining frequent rooted unordered subtrees. Here, we introduce a disk-resident method. The key work for frequency counting is, for each transaction t in the database and each candidate c , we want to check if t supports c .Thatis, we want to detect if c is embedded in t . This is a subtree isomorphism problem.
We have implemented, with some variations, the O ( k
Chung (1987) (where n is the number of vertices in t and k is the number of ver-resulting rooted tree t r ) then test for each vertex v as the root is isomorphic to some subtree of t r . The test is done on each subtree of t r in a postorder and is reduced to maximum bipartite matching problems. For the maximum bipartite matching problem, w e have adopted the algorithms described in Setubal (1996). We performed extensive experiments to evaluate the performance of the RootedTree-
Miner algorithm and the FreeTreeMiner algorithm, using both synthetic datasets and datasets from real applications. All experiments were done on a 2-GHz Intel Pentium IV PC with 1-GB main memory, running the RedHat Linux 7.3 operating system.
All algorithms are implemented in C++ and compiled using the g++ 2.96 compiler.
To generate synthetic data that reflect properties of real applications instead of gen-erating datasets of trees arbitrarily, we start from a graph that we call the base graph . To create the base graph, we use the universal Internet topology generator
BRITE (Medina et al. 2001), developed by Medina et al. at Boston University, that generates random graphs simulating Int ernet topologies with some specific network characteristics, such as the link bandwidth. We use the bandwidths of the links as the edge labels of our base graph and assign the vertex labels to the base graph uniformly. The base graph created by BRITE has the following characteristics: the number of vertex labels is 10; the number of edge labels is 10; the number of ver-from the base graph, we create datasets of trees with controlled parameters. Table 1 provides these parameters a nd their meanings. Note that the size of transactions and trees are defined in terms of the number of vertices.
 follows: starting from the base graph, we first sample a set of sizes of which are determined by | I | . We call this set of (For data of rooted unordered trees, for each seed tree, we randomly select a vertex as the root.) Each seed tree is the starting point for these | D | X | S | transactions is obtained by first randomly permuting the seed tree, then adding more random vertices to increase the size of the transaction to step, more random transactions with size | T | are added to the database to increase the cardinality of the database to | D | . The number of distinct edge and vertex labels is controlled by the parameter | L | . In particular, edge labels as well as the number of distinct vertex labels.
In our first experiment, we want to study the effect of the size of maximal frequent subtrees on our algorithm. With all other parameters fixed ( |
N |= 100 , | S |= 1%), we increase the maximal frequent tree size
Figure 15(a) gives the number of frequent subtrees versus size can see that the number of frequent subtrees grows exponentially with the size of the maximal frequent subtrees (notice the logarithm scale of the y -axis in the figure). As we know, in all these databases, the numbe r of maximum frequent subtrees (a frequent subtree is maximum if it is not a subtree of any other frequent subtree) is fixed to be |
N |= 100. As a result, the experiment result suggests that, in some circumstances, the total number of frequent subtrees can be dramatically larger than that of maximum fre-quent subtrees. F igure 15(b) shows the average tim e to mine each frequent subtrees. As can be seen, the average time for our algorithm to mine each pattern is not affected very much by | I | . (The curves are not smooth because of the randomness in datasets generat-ing.) However, this average time decreases a little as the size tion for this decline is that, for a node v in the enumeration tree, as the size of t represented by v , grows larger, v will have many children and, for these children, we only need to scan the database once to check if they are frequent. Therefore, the amor-tized time for each child is decreased.

Next, we want to check how sensitive the running time is to the size of the database. We created a set of databases with the same number ( trees embedded. With | T |= 50 and | I |= 20, we increased the number of transac-the occurrence of each seed tree to be 50. As a result, the support from 1% to 0.1% as | D | increases. The experiment result is given in Fig. 16(a). As revealed by the figure, for a fixed number of maximal frequent subtrees, the running time is not sensitive to the size of the database. This result seems to contradict the because, in computing the upper bound, we have assumed the worst-case scenario, in which each transaction has at least one frequent subtree embedded in it; however, if frequent subtrees occur only in some transactions, then by the occurrence list, we only need to check those transactions with frequent subtrees embedded. To verify our belief, in the second test, as | D | increases, we fix the support a result, the occurrence of each frequent subtree increases proportionally to the size |
D | of the databases). Figure 16(b) gives the performance for this test. As we can see from the figure, the average time for mining each pattern increases with the size |
D | , which verifies our argument.
 algorithm. First, in our time-complexity analysis, we assume that the number of dis-tinct labels is fixed so that it only contributes a constant factor to the time complexity.
Now we check this assumption. We fixed other parameters ( |
I |= 20 , | N |= 100 , | S |= 1%) while changing | L | from 10 to 100. Notice that, for example, when | L | is 100, there are 100 distinct vertex labels and 100 distinct edge labels, so totally there are 10,000 distinct pairs of combinations. As can be seen from Fig. 17(a), the running time inc reases linearly with the number of these pairs of combinations. Second, we study whether the  X  X hape X  of trees affects the performance of our algorithms. When generating the synthetic trees, by fixing all other parameters but increasing the hei ghts (from 2 to 10), we get different families of trees where trees in each family change from flat (when the maximal height is small) to tall (when the maximal height is large). Figure 17(b) shows the running time for different families of trees. It is ver y interesting that, when the trees are ex-tremely flat (when the maximal height is 2 or 3), the performance of the algorithm deteriorates a little.
In this section, we present an applicatio n on mining frequently accessed web pages from web logs. We ran experiments on the log files at UCLA Data Mining Labora-tory (http://dml.cs.ucla.edu). First, we used the WWWPal system (Punin and Krish-namoorthy 1998) to obtain the topology of the web site and wrote a program to gen-erate a database from the log files. Our pr ogram generated 2,793 user-access trees from the log files collected over year 2003 at our laboratory that touched a total of 310 web pages. In the user access trees, the vertices correspond to the web pages and the edges correspond to the links between the web pages. We take URLs as the vertex labels and each vertex has a distinct label. We do not assign labels to edges.
For support equals 1%, our RootedTreeMiner algorithm mined 16,507 frequent sub-trees in less than 2 sec. Among all the frequent subtrees, the maximum subtree has 18 vertices. Figure 18 shows this maximum subtree. It turns out that this subtree is a part of a web site for the ESP 2 Net (Earth Science Partners X  Private Network) project. From this mining result, we can infer that many visitors to our web site are interested in details about the ESP 2 Net project.
In this subsection, we compare our RootedTreeMiner algorithm with the TreeMiner algorithm given in Zaki (2002). However, there are a few differences between the two algorithms. First, TreeMiner is an algorithm that mines frequent rooted ordered trees, while RootedTreeMiner is an algorithm that mines frequent rooted unordered trees.
Because TreeMiner mines frequent rooted ordered trees, the order of vertices in the transactions provides information that can be used in the candidate generation step, as the scope-list join algorithm did in Zaki (2002). In contrast, RootedTreeMiner , which mines frequent rooted unordered trees, does not have this advantage. Notice although the transaction on the left is in the BFCF, an algorithm that mines rooted the BFCF.

The second difference between the two algorithms is that TreeMiner discovers all frequent embedded subtrees. while RootedTreeMiner discovers all frequent induced sub-trees. An induced subtree t i of a tree t must preserve the parent X  X hild relationship in t , while an embedded subtree t e only has to preserve the ancestor X  X escendent relationship.
Figure 20 illustrates the difference between the two definitions: while all the four 3-trees on the right are embedded subtrees (with A as the root and with the root having two chil-dren) of the tree on the left, only tree (a) is an induced subtree.
 the two algorithms in order to compare their performance and the frequent subtrees they discover. We have created the T1M dataset in Zaki (2002) using the generator provided by the author of Zaki (2002), and run the two algorithms on the dataset. Figure 21 shows the number of frequent subtrees and the r unning time for each frequent subtree versus the minimum support (with the minimum support, less than 0.02%, TreeMiner exhausted all available memory). From Fig. 21(a), we can see that the number of frequent induced sub-trees grows exponentially as the minimum support decreases. In comparison, the num-ber of frequent embedded subtrees is even 1 to 2 orders of magnitude higher. We believe that the large number of frequent subtrees is not necessarily a benefit because it will be difficult for end users to get useful information from such large number of frequent sub-trees. Figure 21(b) shows the average running time for the two algorithms to discover each frequent subtree. From the figure, we can see that TreeMiner is more efficient than
RootedTreeMiner when the minimum support is large. However, as the minimum sup-port decreases further, the average running time for RootedTreeMiner decreased dramat-ically. This is because the total running time of RootedTreeMiner is very small X  X rom 6 seconds when the minimum support is 1% to 78 seconds when the minimum support is 0.001% X  X herefore, the overheads (such as scanning the whole database to get the fre-quent vertices and frequent 2-trees) dominates in the support range in Fig. 21. As shown in Fig. 21, the incremental cost pe r frequent subtree is smaller for RootedTreeMiner than for TreeMiner and, therefore, as the support decreases and more frequent trees are found,
RootedTreeMiner performs better than TreeMiner in terms of time per frequent subtree.
In this section, we report our experiment s on datasets of labelled free trees.
In the first experiment, we study the performance of FreeTreeMiner on the number of transactions. We use datasets of | D |= 10 , 000, 20,000, 40,000, 60,000, 80,000, and 100,000. The other parameters are fixed: | S |= 1%, | and | T | ranges within { 10 , 15 , 20 , 30 } .(When | D |= 100
TreeMiner exhausted all available memory.) These results are shown in Fig. 22. As we can see from the figure, the total running time of FreeTreeMiner scales linearly with the number of transactions.

In the second experiment, we fix all parameters other than |
T |= 30 , | N |= 10 , | S |= 1%), while changing the maximal frequent tree size |
I | from 4 to 20. Figure 23 gives the results. Figure 23(a) shows that, similar to that of rooted unordered trees, the number of frequent subtrees grows exponentially with the size of maximal frequent subtrees. Figure 23(b) gives the average time for
FreeTreeMiner to mine each frequent subtree. The average time for FreeTreeMiner to mine each frequent subtre e decreases a little as the maximal frequent tree size |
I | increases. Our explanatio n of this decrease is that, as of automorphisms decreases and the number of false positives (those join attempts that do not result in top 2 new limbs) decreases. Both c ontribute to the decrease of average time to mine a frequent subtree.
We apply our FreeTreeMiner on a real chemical compound dataset. The dataset con-tains 17,663 tree-structured chemical compounds sampled from a graph dataset of the Developmental Therapeutics Program (DTP) at National Cancer Institute (NCI) ((NCI) 2003). In the tree transactions, the vertices correspond to the various atoms in the chemical compounds and the edges correspond to the bonds between the atoms.
We take atom types as the vertex labels and bond types as the edge labels. There are a total of 80 distinct vertex labels and 3 distinct edge labels. We explored a wide range of the minimum support from 0 . 1% to 50%. Figure 24(a) gives the numbers of all frequent subtrees and maximum frequent subtrees under different supports. We can see that, compared with all frequent subtrees, there are much fewer (about 10 times) maximum frequent subtrees. The numbers for both frequent subtrees and max-imum frequent subtrees decrease exponentially with the support. Figure 24(b) gives the running time for the FreeTreeMiner algorithm to mine all frequent subtrees under different supports.
In this paper, we presented two novel canonical forms, the breadth-first canonical form (BFCF) and the depth-first canonical form (DFCF), for both labelled rooted unordered trees and labelled free trees. I n addition, we built an enumeration tree to enumerate all (frequent) rooted unordered trees in their BFCFs and developed the
RootedTreeMiner algorithm that uses this enumeration tree to discover all frequent subtrees in a database of labelled rooted unordered trees. Moreover, we extended one of our canonical forms, the DFCF, to represent labelled free trees and presented an efficient algorithm, FreeTreeMiner , to discover all frequent subtrees in a database of labelled free trees. We used both synthe tic and real application datasets to study the performance of our algorithms.
 experiment results, we can see that the number of subtrees grows exponentially with respect to the size of the tree. As a result, efficient algorithms to mine maximum fre-quent trees, instead of mining all frequent subtrees, are called for. In this direction, we have had some preliminary results (Chi et al. 2004b). Second, enumeration tree-based methods usually have much better running time performance than Apriori-like methods. We are working on applying the enumeration tree techniques to mining fre-quent free trees and have some preliminary results, as reported in Chi et al. (2004a).
Third, we are working on algorithms that mine frequent subgraphs with mining fre-the trees are not necessarily atomic. F or example, for a web access tree, we have chosen the URL as the vertex labels. Ho wever, each web page file can have multiple attributes, such as the size of the file, the time of creation, the type (html, jpg, etc.) possible for the vertex labels of a frequent subtree to be defined over any subset of of html type, has two children both built before January 1, 2000, and is visited by a visitor from .edu domain X . To mine such frequent subtrees with general vertex labels efficiently is a challenge that we plan to work on in the future.
