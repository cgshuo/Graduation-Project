 Current movie title retrieval models, such as IMDB, mainly focus on utilizing structured or semi-s tructured data. However, user queries for searching a movie ti tle are often based on the movie plot, rather than its metadata. As a solution to this problem, our movie title retrieval model pro poses a new way of elaborately utilizing associative relations between multiple key terms that exist in the movie plot, in orde r to improve search performance when users enter more than one keyword. More specifically, the proposed model exploits associative networks of key terms, called knowledge structures, derived from movie plots. Using the search query terms entered by Amazon Mechanical Turk users as the golden standard, experiments we re conducted to compare the proposed retrieval model with the extant state-of-the-art retrieval models. The experiment results s how that the proposed retrieval model consistently outperforms the baseline models. The findings have practical implications for sema ntic search of movie titles in particular, and of online entert ainment contents in general. H.3.3 [ Information Search and Retrieval ]: Retrieval models Algorithms Movie search; knowledge structure; proximity Considering and incorporating query term proximity has been shown to be an effective probabilistic retrieval model in multiple studies [2, 11, 13, 14]. A key underlying assumption for proximity is that the more compact the query terms, the more likely that they are closely related; thereby, the more potentially relevant the documents will be to the topic repr esented in that particular set of user queries. For movie contents in particular, users often use scenic queries. For example, consider the following actual question that was observed on a commercial Q&amp;A website (Naver Knowledge-iN 1 ) in Korea:  X  X lease tell me the title of the movie, in which a car is transformed into a robot. I want to watch it, but don X  X  remember it X  X  title X  (transla ted). This example supports the idea that users tend to recall movies by describing the scenes or impressive moments from the movi es, indicating that the query terms are related to each other, rather than being independent. In general, the term  X  X ar X  is not associated with  X  X obot X ; however, those terms become closely linke d in the context of the movie  X  X ransformers X , in which a  X  X ar X  is transformed into a  X  X obot. X  To verify our assumption that the query terms entered to find a movie title are closely related, we analyzed the query sets of approximately 1,000 movies coll ected via Amazon Mechanical Turk. 2 We asked users to type queries for movies that they had seen once, but did not remember the titles clearly. The analysis results showed that a significant number of user queries were formulated from the movie plot, meaning that those terms have considerable associative relations. Although our analysis demands the full utilization of the query term proximity information for movie retrieval, probabilistic proximity measures suggested in previous studies [2, 11, 13, 14] do not fully reflect the genuine re lationships between the terms. For instance, the current best proximity measure is MinDist, reported in [13], which is the smallest positional distance of all pairs of unique matched query terms. Consider the following two * Mun Y. Yi is the corresponding author http://kin.naver.com https://www.mturk.com terms as an example: one that occurs at the end of a paragraph, and the other that occurs at the beginning of the next paragraph. The MinDist of the two terms is 1 and they are considered to have semantically separated segment, th ere is a high probability that the two terms are not semantically associated. More specifically, continuing from a previous example, given the actual query set Q = {giant, robot, car}, MinDist m odel located the target movie Transformers at the third position while located a non-target movie (i.e., Monsters vs. Aliens, designated MvA ) at the first. This unsatisfying retrieval result is due to the minimum distance scores, which were 1 for both Transformers and MvA , thereby failing to provoke the re-ranking process. To counteract the aforementioned limitation, in this paper we suggest a new proximity measure for exploiting knowledge structure, which was originally conceptualized in the field of educational psychology [5]. Unlike the probabilistic proximity measures, knowledge structures de pict the various concepts and their associative relationships that exist in people X  X  minds with regard to a specific domain. The knowledge structures of domain experts regarding a specific domain are known to be similar [5] and can be reliably extracted from a document [7]. By representing each movie as a knowledge structure that preserves the proximity semantics among the te rms, the movies can be more reachable using descriptive sets of queries. Thus, we present a new movie title retrieval model that effectively searches for movie titles by leveraging the knowledge structures extracted from movie plots. Furthermore, the expe riment results reveal that the proposed model outperforms other state-of-the-art retrieval models. In this section, we review some of previous studies that are related to our movie retrieval model. Proximity-aware retrieval model. Numerous studies have applied proximity measures to re trieval models. The early works discussed in [11] calculates a proximity score by considering the co-occurrence of a pair of queries in a document. In [2], the proximity computation process was then tuned to be faster for large text collection. In [13], a systematic approach was provided to heuristically combine proximi t y measures with the existing models of BM25 [12] and Language Model [9]. A probabilistic model [8] and enumerating sub-tree model [4] were proposed for multi-field documents such as XML. Furthermore, in [14], proximity factor was integrated into the unigram language model to weight the parameters of the multinomial document language models. In movie sear ch, collaborative filtering method was used to generate personalized item authorities which were combined with item proximities for better search ranking [10]. Knowledge structure. A person is said to be knowledgeable if he or she knows the concepts present in a domain, and the relations between those concepts, all of which are captured in a knowledge structure [5]. Knowledge structures have mainly been used to understand cognitive behaviors duri ng the learning process in the field of education [3]. Based on the co-occurrence of terms and the Pathfinder algorithm [6], a knowledge structure can be automatically created from a document. It was proven that the knowledge structure produced from a series of automated processes was similar to that produced by domain experts [7], meaning that the relations between terms were adequately represented in the generated knowledge structure. To the best of our knowledge, knowledge structure has never been applied to information retrieva l, though it can potentially be effective in developing probabilistic retrieval models. In this paper, we propose an automati c method of generating knowledge structures for movies, and exploit the use of knowledge structures on proximity-aware movi e retrieval models. In this section, we first in troduce an automated method for generating knowledge structures from movie plots, then moving on to explain how to utilize it in a movie retrieval model. The proposed method that automa tically generates a knowledge structure from a movie plot requir es two specific information of the source: A set of keywords and distance scores of the keywords. These pieces of information are then processed with a number of refining steps to remove weak re lations for noise deduction. As a first step, the keywords of the movie m need to be extracted to form the basis of a knowledge stru cture. To capture concepts, we only extracted nouns from a synopsis document  X   X  that contains a movie plot about movie m and added those nouns to concept list  X  . The distance between each pair of the keywords in the list  X  then can be measured by sentence co-occurrences similarity (SS). For SS, co-occurrence between two terms is defined only if those two terms appear in the same sentence. The distance score for SS between two terms  X   X  and  X   X  (  X   X   X ,  X   X  X   X  ) are defined as follows: where N s is the number of sentences,  X  X  X   X   X  X  occurrences of two terms  X   X  and  X   X  ,  X  X  X  X  X   X  maximum  X   X  between any terms in  X   X  for normalization. Similar to SS, we define paragraph co-occurrences similarity (PS) as the co-occurrence of two terms in the same paragraph. On the other hand, we also can measure the distance score between two terms  X   X  and  X   X  in a different way by adapting cosine similarity of the co-occurrence matrices as shown in Table 1 as follows: where SV i and SV j are vectors based on the frequencies of  X   X  occurring in each sentence. Paragraph co-occurrence cosine similarity (PCS), can be defined si milar to equation (3) but only to consider co-occurrence per paragraph. As the manual knowledge structure creation in [3] measured the distance between two terms by involving human judges, for automatic knowledge structure creation, we also convert our initial distance score into a 7-point Likert Scale (1: strongly related, 7: not related at all) as follows: Finally, the knowledge structure goes through the pathfinder algorithm [6] to eliminate data noise by removing redundant nodes. To combine the word associative relations into a new retrieval model, we obtained the original ranking scores using the existing retrieval model, Okapi BM25 [12] at first, and then re-ranked the result based on the proximity dist ance in a knowledge structure. Once the original ranking was retrieved, proximity scores between the terms in a query were calculated in each movie plot. Because the query can have more than two words, average distance scores are calculated to integrate all associative relations. Given a query set  X  and synopsis document  X   X  , the formula to determine the proximity score (PS) between  X  and  X   X  is as follows: where t i and t j are terms in a query,  X  X  X  X  X  X  X  X   X   X  X   X  score between the two terms  X   X  and  X   X  in the knowledge structure of document  X   X  ,  X  is the number of queries in the query set Q, and  X  X  X  X  X  X  X  X  X  X  X   X  X   X   X  is the longest distance between any two terms in the knowledge structure. For normalization, the formula is divided by  X  X  X  X  X  X  X  X  X  X  X   X  X   X   X  because the average distance between two terms and the plot length have a positive correlation (  X  = 0.5638). Furthermore  X  is multiplied to differentiate the score based on the length of queries. In the case that either of two terms does not occur in a movie plot, the distance between the terms is defined as  X  X  X  X  X  X  X  X  X  X  X   X  X   X   X  . To reflect proximity characteristic that a distance score drops fast when the distance between two terms is small while it does not change much as the distance becomes larger [13], we used a convex curve of which the first derivative is negative, and the second one is positive as follows: We used an exponential function to put the range of the proximity score in the [0, 1] range, and to introduce  X  as a parameter for variation. As  X  becomes smaller, the proximity function becomes linear. Finally, we combined this function with the existing retrieval model, BM25, as follows: where  X   X  and  X  are two parameters often set to the standard values of 2 and 0.75,  X   X   X   X   X ,  X   X  is the term frequency of  X  , |  X   X  | is the length of the document vector, and average length of all synops is document vectors. In this section, we describe our evaluation methodology and the evaluations we performed. To evaluate our re-ranking model, we have crawled top 1,000 movies (based on box office sales) from IMDB, 3 one of the most popular movie portals. Among severa l sources for movie profiles, we chose to exploit synopsis offered by IMDB as it contains http://www.imdb.com abundant amount of movie plot in formation. The average number of words, sentences, and paragraphs in a synopsis is 904.613, 94.316, and 18.158 respectively, indicating that synopsis is substantial enough to create a re presentative knowledge structure for individual movies. We also collected 10 queries 4 for each movie via Amazon Mechanical Turk, which has been used in information retrieval research for relevance assessment [1]. To control the quality of the input, we restricted users wh ose HIT approval rate was greater than or equal to 90%. The participants were asked to formulate a search query consisting of multi ple keywords for a given movie and received $0.02 per query. In the end, 355 users participated, with an average time to formul ate each query of 40.051 seconds, and an average number of words in each query of 3.749. As our algorithm considers semant ics of the words, we compared our algorithm with the proximity retrieval model (PRM) [13], which calculates proximity between words by measuring the minimum pair distance between the query terms. It was also reported to perform best among other state-of-the art models. Given a query Q  X  X  X  X   X   X ,...,  X   X , the PRM score is tuned to show a consistent performance in our dataset as follows: measure defined as the smallest positional distance of all pairs of uniquely matched-query terms. In the experiment, we set  X 0.3 X  , which is known to work best in a prior study [13]. This parameter value is also shown to perform best in our dataset. For evaluation metrics, we use the Mean Reciprocal Rank (MRR) metric that assigns a value of performance for a target resource of 1/r, where r is the position of the relevant document 'd' in the result list. We also provide th e P@N (Precision at position N) metric, which has a value of 1 iff r X  X  . In this section, we analyze the performance of the proximity approaches while our approach adopts the different distance measures: SS, PS, SCS, and PCS. Table 2 shows MRR and P@N values of the different proximity approaches. An asterisk indicates that the value is statistically si gnificantly higher than the BM 25 is statistically signi ficantly higher than the PRM counterpart relation to the BM25 approach. P@10 0.8515 0.8691* 0.8691* 0.8698* 0.8797  X  0.8813  X  Our approach shows higher performance (statistically significant) than the existing state-of-the-art algorithms in all metrics, regardless of the distance me tric used, indicating that consideration of semantics of words through knowledge structure The data are available at http://courseshare.kaist.ac.kr/movie/ positively affects search performance consistently. In particular, our algorithm with SS presents the best performance. This combination outperformed BM25 and PRM by 8.47% and 5.30% on MRR, 13.02% and 8.56% on P@1, 9.27% and 6.10% on P@2, respectively. Especially, P@1 result for SS implies that users are more likely to find their target movie in the top of the search result compared to PRM, indicating that our model can be more effective in the case that users would like to find a specific movie. To understand why the performance of our approach increases compared to the other state-of-the -art methods, we re-visited our motivating example and analyzed the results. Given the query set Q = {giant, robot, car}, Table 3 s hows that the distance for each pair of Q produced different results depending on the distance metric used. Note again that PRM adopts the mi nimum distance (MinDist), and thus does not pr ovoke the re-ranking process. On the other hand, we can see that three queries are closer to each other in the knowledge structure 5 for Transformers , rather than in the knowledge structure for MvA . This implies that our model can discover that semantics among th e three terms are stronger for Transformers , relative to MvA . Furthermore, we investigated the effect of varying  X  for overall search performance. Enlarging the constant  X  in Equation 6 forms convex relations between two terms in a document. Figure 1 shows the MRR and P@N values when the constant  X  varies from 0 to 2. We can see that the best performance is achieved when  X  is between 0.6 and 0.8 in PS, PCS, and SCS, while the overall performance gradually reduces as  X  increases. However, SS shows the most stable performance regardless of the value of  X  , suggesting that our algorithm, in combination with the SS distance metric, has promise as an effe ctive, parameter-free method. In this paper, we have observed that user queries are more descriptive and associative in searching movies because users tend to recall the scenes, or impressive moments of the movies, mainly in relation to the movie plot. We then presented a new movie-retrieval model that effectively searches movies by exploiting knowledge structures extracted fr om movie plots and measuring the proximity of terms in a query. Our algorithm outperformed Sample knowledge structures for those two movies are shown at http://courseshare.kaist.ac.kr/movie/ other state-of-the-art proximity algorithms as it effectively utilizes the semantics of terms from the movie plots. Our study needs further work. Fi rst, we should expand knowledge structure incorporating other info rmation about movies, not only movie plots. Second, we expect that other multimedia content is also likely to have similar associ ative queries, thus we should test our algorithm on other types of multimedia content, such as music and books. Despite the need for further work, the proposed algorithm already shows promise fo r utilizing the potential of knowledge structure to enhance proximity-probabilistic retrieval of multimedia content. This research was supported by the BK21 Plus Program of Research and Talent Manageme nt on Intelligent Knowledge Service for Innovating Human-Machine Communication and Cooperation hosted at the Depa rtment of Knowledge Service Engineering, KAIST. [1] R. Blanco, H. Halpin, D. M. He rzig, P. Mika, J. Pound, H. S. [2] S. B X ttcher, C. L. Clarke, a nd B. Lushman. Term proximity [3] F. D. Davis, M. Y. Yi. Impr oving computer skill training: [4] K. Goldenberg, B. Kimelfeld, and Y. Sagiv, Keyword [5] T. E. Goldsmith, P. J. Johnson, and W. H. Acton. Assessing [6] S. Hauguel, C. Zhai, and J. Han. Parallel PathFinder [7] H. W. Kim, and M. Y. Yi. Empirical validation of an [8] J. Y. Kim, X. Xue, W. Bruce Croft, A probabilistic model for [9] J. Lafferty, and C. Zhai. Document language models, query [10] S. T. Park, D. M. Pennock, Applying collaborative filtering [11] Y. Rasolofo and J. Savoy. Term Proximity Scoring for [12] S. E. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, [13] T. Tao, and C. Zhai. An exploration of proximity measures in [14] J. Zhao, and Y. Yun. A proximity language model for 
