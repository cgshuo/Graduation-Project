 In artificial intelligence, changes in representation sometimes suggest new algorithms. For example, increased attention to distributed meaning repre-sentations suggests that existing combinatorial al-gorithms for NLP might be supplanted by alterna-tives designed specifically for embeddings. In this work, we consider summarization.

Classical approaches to extractive summariza-tion represent each sentence as a bag of terms (typically bigrams) and seek a subset of sentences from the input document(s) that either (a) trade off between high relevance and low redundancy (Car-bonell and Goldstein, 1998; McDonald, 2007), or (b) maximize bigram coverage (Yih et al., 2007; Gillick et al., 2008). The sentence representa-tion is fundamentally discrete , and a range of greedy (Carbonell and Goldstein, 1998), approx-imate (Almeida and Martins, 2013), and exact op-timization algorithms (McDonald, 2007; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011) have been proposed.

Recent studies have explored continuous sen-tence representations, including the paragraph vector (Le and Mikolov, 2014), a convolutional neural network architecture (Kalchbrenner et al., 2014), and a dictionary learning approach (Jenat-ton et al., 2011). If sentences are represented as low-dimensional embeddings in a distributed se-mantic space, then we begin to imagine a geomet-ric relationship between a summary and a doc-ument. We propose that the volume of a sum-mary (i.e., the semantic subspace spanned by the selected sentences) should ideally be large. We therefore formalize a new objective function for summarization based on semantic volume (  X  2), and we provide a fast greedy algorithm that can be used to maximize it (  X  3). We show that our method outperforms competing extractive base-lines under similar experimental conditions on benchmark summarization datasets (  X  4). Assume we are given a set of N sentences: D = { s 1 , s 2 , . . . , s N } from one or many documents, and the goal is to produce a summary by choos-ing a subset S of M sentences, where S  X  D and M  X  N , and the length of the summary is less than or equal to L words. In this work, we as-sume no summaries are available as training data. Denote a binary indicator vector y  X  R N , where sentence i is included if and only if y i = 1 and 0 otherwise. Extractive summarization can be writ-ten as an optimization problem: with a scoring function score ( D , y ) . A good scor-ing function should assign higher scores to bet-ter summaries. In the following, we describe two commonly used scoring functions and our pro-posed scoring function. 2.1 Maximal Marginal Relevance The Maximal Marginal Relevance (MMR) method (Carbonell and Goldstein, 1998) considers the fol-lowing scoring function: score ( D , y ) = where Rel ( s i ) measures the relevancy of sentence i and Sim ( s i , s j ) measures the (e.g., cosine) simi-larity between sentence i and sentence j . The in-tuition is to choose sentences that are highly rel-evant to the document(s) and avoid redundancy. The above maximization problem has been shown to be NP-hard, solvable exactly using ILP (Mc-Donald, 2007). A greedy algorithm that approxi-mates the global solution by adding one sentence at a time to maximize the overall score (Lin and Bilmes, 2010) is often used in practice. 2.2 Coverage-Based Summarization Another popular scoring function aims to give higher scores for covering more diverse concepts in the summary. Gillick et al. (2008) use bigrams as a surrogate for concepts. Following convention, we extract bigrams from each sentence s i  X  D . Denote the number of unique bigrams extracted from all sentences by B . We introduce another binary vector z  X  R B to indicate the presence or absence of a bigram in the summary, and a binary and only if bigram j is present in sentence i and 0 otherwise. The scoring function is: and the two additional constraints are: where we use [ B ] as a shorthand for { 1 , 2 , . . . , B } . The first constraint makes sure that selecting a sen-tence implies selecting all its bigrams, whereas the Figure 1: A toy example of seven sentences projected into a two-dimensional semantic space. Consider the case when the maximum summary length is four sentences. Our scoring function is optimized by chooseing the four sentences in red as the summary, since they maximize the volume (area in two dimensions). second constraint makes sure that selecting a bi-gram implies selecting at least one of the sentences that contains it. In this formulation, there is no ex-plicit penalty on redundancy. However, insofar as redundant sentences cover fewer bigrams, they are implicitly discouraged. Although the above scor-ing function also results in an NP-hard problem, an off-the-shelf ILP solver (Gillick et al., 2008) or a dual decomposition algorithm (Almeida and Martins, 2013) can be used to solve it in practice. 2.3 Semantic Volume We introduce a new scoring function for summa-rization. The main idea is based on the notion of coverage, but in a distributed semantic space: a good summary should have broad semantic cover-age with respect to document contents. For every sentence s i , i  X  [ N ] , we denote its continuous se-mantic representation in a K -dimensional seman-tic space by  X ( s i ) = u i  X  R K , where  X  is a func-tion that takes a sentence and returns its semantic vector representation. We denote embeddings of all sentences in D with the function  X  by  X ( D ) . We will return to the choice of  X  later. We propose to use a scoring function that maximizes the vol-ume of selected sentences in this semantic space: score ( D , y ) = Volume ( X ( D ) , y ) = Volume ( X ( S )) In the case when K = 2 , this scoring function maximizes the area of a polytope, as illustrated in Figure 1. In the example, there exists a maximum number of sentences that can be selected such that adding more sentences does not increase the score, i.e., the set of selected sentences forms a convex hull of the set of all sentences. The sentences forming a convex hull may together be longer than L words, so we seek to maximize the volume of the summary under this constraint.

There are many choices of  X  that we can use to produce sentence embeddings. As an exploratory study, we construct a vector of bigrams for each sentence, that is, s i  X  R B ,  X  i  X  [ N ] . If bigram b is present in s i , we let s i,b be the number of doc-uments in the corpus that contain bigram b , and zero otherwise. We stack these vectors in columns to produce a matrix S  X  R N  X  B , where N is the number of sentences in the corpus and B is the number of bigrams. We then perform singular value decomposition (SVD) on S = U X V &gt; . We where K is a parameter that specifies the number of latent dimensions. Instead of performing SVD, we can also take s i  X  R B as our sentence repre-sentation, which makes our method resemble the bigram coverage-based summarization approach. However, this makes s i a very sparse vector. Pro-jecting to a lower dimensional space makes sense to allow the representation to incorporate informa-tion from (bigram) cooccurrences and share infor-mation across bigrams. Given the semantic coverage scoring function in  X  2.3, our optimization problem is: with respect to S subject to length ( S )  X  L For computational considerations, we propose to use a greedy algorithm that approximates the so-lution by iteratively adding a sentence that max-imizes the current semantic coverage, given that the length constraint is still satisfied. The main steps in our algorithm are as follows. We first find the sentence that is farthest from the cluster centroid and add it to S . Next, we find the sen-tence that is farthest from the first sentence and add it to S . Given a set of already selected sen-tences, we choose the next one by finding the sen-tence farthest from the subspace spanned by sen-tences already in the set. We repeat this process until we have gone through all sentences, break-ing ties arbitrarily and checking whether adding a sentence to S will result in a violation of the length constraint. This method is summarized in Algorithm 1. We note that related variants of our method for maximizing volume have appeared in Algorithm 1 Greedy algorithm for approximately maximizing the semantic volume given a budget constraint.
 Input: Budget constraint L , sentence representa-tions R = { u 1 , u 2 , . . . , u N } S = {} , B = {} Compute the cluster centroid c : 1 p  X  index of sentence that is farthest from c .
S = S  X  X  s p } . I add first sentence q  X  index of sentence that is farthest from s p .
S = S  X  X  s q } . I add second sentence total length = length ( s p ) + length ( s q ) for i = 1 , . . . , N  X  2 do end for other applications, such as remote sensing (Nasci-mento and Dias, 2005; Gomez et al., 2007) and topic modeling (Arora et al., 2012; Arora et al., 2013).
 Computing Distance to a Subspace Our algo-rithm involves finding a point farthest from a sub-space (except for the first and second sentences, which can be selected by computing pointwise dis-tances). In order for this algorithm to be efficient, we need this operation to be fast, since it is ex-ecuted frequently. There are several established methods to compute the distance between a point to a subspace spanned by sentences in S . For com-pleteness, we describe one method based on the Gram-Schmidt process (Laplace, 1812) here.
 We maintain a set of basis vectors, denoted by B . Our first basis vector consists of one element: b above. Next, we project each candidate sentence i to this basis vector: and find the distance by computing Distance ( u i , B ) = k u i  X  Proj b find the farthest sentence r , we add a new basis vector B = B  X  { b r } , where b r = u r repeat this process. When there are more than one basis vectors, we find the distance by computing: Distance ( u i , B ) = u i  X  4.1 Setup We evaluate our proposed method on the non-update portion of TAC-2008 and TAC-2009. The datasets contain 48 and 44 multi-document sum-marization problems, respectively. Each problem has 10 news articles as input; each is to be sum-marized in a maximum of L = 100 words. There are 4 human reference summaries for each prob-lem, against which an automatically generated summary is compared. We compare our method with two baselines: Maximal Marginal Relevance (MMR,  X  2.1) and the coverage-based summariza-tion method (CBS,  X  2.2). ROUGE (Lin, 2004) is used to evaluate the summarization results.
 For preprocessing, we tokenize, stem with the Porter (1980) stemmer, and split documents into sentences. We remove bigrams consisting of only stopwords and bigrams which appear in less than 3 sentences. As a result, we have 2,746 and 3,273 bigrams for the TAC-2008 and TAC-2009 datasets respectively. Unlabeled data can help generate better sentence representations. For each sum-marization problem in each dataset, we use other problems in the same dataset as unlabeled data. We concatenate every problem in each dataset and perform SVD on this matrix (  X  2.3). Note that this also means we only need to do one SVD for each dataset. 4.2 Results Table 1 shows results on the TAC-2008 and TAC-2009 datasets. We report results for our method with K = 500 (Volume 500), and K = 600 (Vol-ume 600). We also include results for an oracle model that has access to the human reference sum-maries and extracts sentences that maximize bi-gram recall as an upper bound. Similar to previous findings, CBS is generally better than MMR. Our method outperforms other competing methods, al-though the optimal value of K is different in each dataset. The improvements with our proposed ap-proach are small in terms of R-2. This is likely because the R-2 score computes bigram overlaps, and the CBS method that directly maximizes bi-gram coverage is already a resonable approach to optimizing this metric (although still worse than the best of our methods).
 Table 1: Results on the TAC-2008 and TAC-2009 datasets.  X  X olume X  refers to our method, shown with two embedding sizes. Figure 2: R-SU4 scores as we vary the number of dimensions ( K ) on the TAC-2008 datasets. Runtime comparisons In terms of inference running time, all methods perform reasonably fast. MMR is the slowest, on average it takes 0.38 sec-onds per problem, followed by our method at 0.17 seconds per problem, and CBS at 0.15 seconds per problem. However, our implementations of MMR and Algorithm 1 are in Python, whereas we use an optimzed solver from Gurobi for our CBS baseline. For preprocessing, our method is the slowest, since we need to compute sentence em-beddings using SVD. There are about 10,000 sen-tences and 3,000 bigrams for each dataset. SVD takes approximately 2.5 minutes (150 seconds) us-ing Matlab on our 12-core machine with 24GB RAM. Our method introduces another hyperpa-rameter, the number of latent dimensions K for sentence embeddings. We observe that the optimal value depends on the dataset, although a value in the range of 400 to 800 seems best. Figure 2 shows R-SU4 scores on the TAC-2008 dataset as we vary K .
 Other sentence projection methods We use SVD in this study for computing sentence embed-dings. As mentioned previously, our summariza-tion approach can benefit from advances in neural-network-based sentence representations (Jenatton et al., 2011; Le and Mikolov, 2014; Kalchbrenner et al., 2014). These models can also produce vec-tor representations of sentences, so Algorithm 1 can be readily applied to the learned representa-tions. Our work opens up a possibility to make summarization a future benchmark task for evalu-ating the quality of sentence representations.
Our method is related to determinantal point processes (DPPs; Gillenwater et al., 2012; Kulesza and Taskar, 2012) in that they both seek to maxi-mize the volume spanned by sentence vectors to produce a summary. In DPP-based approaches, quality and selectional diversity correspond to vector magnitude and angle respectively. In this work, the length of a sentence vector is not tai-lored to encode quality in terms of representative-ness directly. In contrast, we rely on sentence em-bedding methods to produce a semantic space and assume that a good summary should have a large volume in the semantic space. We show that a sim-ple singular value decomposition embedding of sentences X  X ne that is not especially tuned for this task X  X roduces reasonably good results. We leave exploration of other sentence embedding methods to future work.
 Future work Our method could be extended for compressive summarization, by simply including compressed sentences in the embedded space and running Algorithm 1 without any change. This re-sembles the summarization methods that jointly extracts and compresses (Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013). Another alternative is a pipeline approach, where extractive summarization is fol-lowed or preceded by a sentence compression module, which can be built and tuned indepen-dent of our proposed extractive method (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013; Li et al., 2013).

We are also interested in exploring volume as a relevance function within MMR. MMR avoids redundancy by penalizing redundant sentences, whereas in our method semantic redundancy is inherently discouraged since the method chooses sentences to maximize volume. Depending on the method used to embed sentences, this might not translate directly into avoiding n -gram redun-dancy. Plugging our scoring function to an MMR objective is a simple way to enforce diversity.
Finally, an interesting future direction is find-ing an exact tractable solution to the volume max-imization problem (or demonstrating that one does not exist). We introduced a summarization approach based on maximizing volume in a semantic vector space. We showed an algorithm to efficiently perform volume maximization in this semantic space. We demonstrated that our method outperforms exist-ing state-of-the-art extractive methods on bench-mark summarization datasets.
 Acknowledgments We thank anonymous reviewers for helpful sug-gestions. This work was supported by the Defense Advanced Research Projects Agency through grant FA87501420244 and by NSF grant SaTC-1330596. This work was completed while the au-thors were at CMU.

