 University of Pennsylvania University of Pennsylvania
The Proposition Bank project takes a practical approach to semantic representation, adding a represent coreference, quantification, and many other higher-order phenomena, but also broad, be calculated.
 and to analyze the frequency of syntactic/semantic alternations in the corpus. We describe an performance of various types of information, including a comparison of full syntactic parsing 1. Introduction
Robust syntactic parsers, made possible by new statistical techniques (Ratnaparkhi 1997; Collins 1999, 2000; Bangalore and Joshi 1999; Charniak 2000) and by the language processing in recent years. However, the syntactic analyses produced by these parsers are a long way from representing the full meaning of the sentences that are parsed. As a simple example, in the sentences (1) John broke the window. (2) The window broke. underlying semantic role in both cases. Note that both sentences are in the active voice and that this alternation in subject between transitive and intransitive uses of the verb does not always occur; for example, in the sentences (3) The sergeant played taps. (4) The sergeant played. the subject has the same semantic role in both uses. The same verb can also undergo syntactic alternation, as in (5) Taps played quietly in the background. and even in transitive uses, the role of the verb X  X  direct object can differ: (6) The sergeant played taps. (7) The sergeant played a beat-up old bugle.
 affecting most English verbs in some way, and the patterns exhibited by specific verbs vary widely (Levin 1993). The syntactic annotation of the Penn Treebank makes it examples. While the treebank provides semantic function tags such as temporal and examples. Because the same verb used with the same syntactic subcategorization can assign different semantic roles, roles cannot be deterministically added to the treebank by an automatic conversion process with 100% accuracy. Our semantic-role annotation process begins with a rule-based automatic tagger, the output of which is then hand-corrected (see section 4 for details).
 such phenomena, enabling the development of better domain-independent language understanding systems and the quantitative study of how and why these syntactic alternations take place. We define a set of underlying semantic roles for each verb and annotate each occurrence in the text of the original Penn Treebank. Each verb X  X  roles are numbered, as in the following occurrences of the verb offer from our data: (8) ... [ (9) ... [ (10) ... [ (11) ... [ applications including information extraction, question answering, and machine 72 translation. Over the past decade, most work in the field of information extraction has shifted from complex rule-based systems designed to handle a wide variety of semantic phenomena, including quantification, anaphora, aspect, and modality (e.g.,
Miller et al. 1998). These newer systems rely on a shallower level of semantic representation, similar to the level we adopt for the Proposition Bank, but have also tended to be very domain specific. The systems are trained and evaluated on corpora annotated for semantic relations pertaining to, for example, corporate acquisitions or terrorist events. The Proposition Bank (PropBank) takes a similar approach in that we quantification and discourse-level structure. By annotating semantic roles for every verb in our corpus, we provide a more domain-independent resource, which we hope will lead to more robust and broad-coverage natural language understanding systems. complete corpus annotated with semantic roles, including roles traditionally viewed as arguments and as adjuncts. It allows us for the first time to determine the frequency of understanding, and the strategies to which they may be susceptible.
 of semantic arguments and drawing connections to previous research into verb alter-compares our PropBank methodology and choice of semantic-role labels to those of another semantic annotation project, FrameNet. We conclude the article with a dis-cussion of several preliminary experiments we have performed using the PropBank annotations, and discuss the implications for natural language research. 2. Semantic Roles and Syntactic Alternation
Our work in examining verb alternation behavior is inspired by previous research into comprehensive study of Levin (1993). Levin argues that syntactic frames are a direct reflection of the underlying semantics; the sets of syntactic frames associated with a particular Levin class reflect underlying semantic components that constrain allowable arguments. On this principle, Levin defines verb classes based on the ability of sense meaning-preserving (diathesis alternations). The classes also tend to share some semantic component. For example, the break examples above are related by a transitive/intransitive alternation called the causative/inchoative alternation. Break bread, This loaf cuts easily . However, it cannot also occur in the simple intransitive: The break verbs cannot: *John broke at the window. The explanation given is that cut describes a series of actions directed at achieving the goal of separating some object into pieces.
These actions consist of grasping an instrument with a sharp edge such as a knife and performed without the end result being achieved, but such that the cutting manner can still be recognized, for example, John cut at the loaf . Where break is concerned, the only thing specified is the resulting change of state, in which the object becomes separated into pieces.
 extends Levin X  X  classes by adding an abstract representation of the syntactic frames for each class with explicit correspondences between syntactic positions and the semantic roles they express, as in Agent REL Patient or Patient REL into pieces for break . extensions of Levin, see also Dorr and Jones [2000] and Korhonen, Krymolowsky, and
Marx [2003].) The original Levin classes constitute the first few levels in the hierarchy, with each class subsequently refined to account for further semantic and syntactic differences within a class. The argument list consists of thematic labels from a set of 20 represent a mapping of the list of schematic labels to deep-syntactic arguments.
Additional semantic information for the verbs is expressed as a set (i.e., conjunction) of classes are completely described, including their semantic predicates. In many cases, the additional information that VerbNet provides for each class has caused it to subdivide, or use intersections of, Levin X  X  original classes, adding an additional level to the hierarchy (Dang et al. 1998). We are also extending the coverage by adding new classes (Korhonen and Briscoe 2004).
 sentation and a corpus of annotated data to enable empirical study of these issues. We classes are given consistent role labels. However, there is only a 50% overlap between defines.
 about alternation patterns and their semantics, the frequency of these alternations and their effect on language understanding systems has never been carefully quantified.
While learning syntactic subcategorization frames from corpora has been shown to be possible with reasonable accuracy (Manning 1993; Brent 1993; Briscoe and Carroll arguments. More recent work has attempted to group verbs into classes based on alternations, usually taking Levin X  X  classes as a gold standard (McCarthy 2000; Merlo and Stevenson 2001; Schulte im Walde 2000; Schulte im Walde and Brew 2002). But without an annotated corpus of semantic roles, this line of research has not been able to measure the frequency of alternations directly, or more generally, to ascertain how well the classes defined by Levin correspond to real-world data.
 annotation which, coupled with minimal coreference links, could provide the text. This will in turn improve the performance of basic parsing and generation 74 components, as well as facilitate advances in text understanding, machine translation, and fact retrieval. 3. Annotation Scheme: Choosing the Set of Semantic Roles basis. An individual verb X  X  semantic arguments are numbered, beginning with zero. totypical Agent (Dowty 1991), while Arg1 is a Prototypical Patient or Theme. No consistent generalizations can be made across verbs for the higher-numbered arguments, though an effort has been made to consistently define roles across mem-bers of VerbNet classes. In addition to verb-specific numbered roles, PropBank defines several more general roles that can apply to any verb. The remainder of this section describes in detail the criteria used in assigning both types of roles.
 and kick below. These examples are taken from the guidelines presented to the annotators and are also available on the Web at http://www.cis.upenn.edu/  X  cotton/ cgi-bin/pblex_fmt.cgi. (12) Frameset accept.01  X  X  X ake willingly X  X  (13) Frameset kick.01  X  X  X rive or impel with the foot X  X  be associated with a set of syntactic frames indicating allowable syntactic variations in frameset . A polysemous verb may have more than one frameset when the differences documentation but does not have any theoretical standing. In addition, each frameset is complemented by a set of examples, which attempt to cover the range of syntactic referred to as the verb X  X  frames file .
 number of reasons. Foremost, the numbered arguments plot a middle course among many different theoretical viewpoints. 3 The numbered arguments can then be mapped easily and consistently onto any theory of argument structure, such as traditional theta role (Kipper, Palmer, and Rambow 2002), lexical-conceptual structure (Rambow et al. 2003), or Prague tectogrammatics (Hajic  X ova and Kuc  X erova  X  2002).
 in particular for certain verbs of motion: 4 (14) Frameset edge.01  X  X  X ove slightly X  X  external force could cause the Agent to execute the action in question. For example, in walk , modern uses of volunteer (e.g., Mary volunteered John to clean the garage, or more (This usage does not occur as such in the Penn Treebank corpus, although it is evoked without having mastered the 3 R X  X  at the level that prevailed 20 years ago. (wsj_1286)) set of general, adjunct-like arguments (ArgMs), distinguished by one of the function tags shown in Table 1. Although they are not considered adjuncts, NEG for verb-level negation (e.g., John did n X  X  eat his peas ) and MOD for modal verbs (e.g., John would eat 76 everything else ) are also included in this list to allow every constituent surrounding the verb to be annotated. DIS is also not an adjunct but is included to ease future discourse connective annotation. 3.1 Distinguishing Framesets
The criteria for distinguishing framesets are based on both semantics and syntax. Two verb meanings are distinguished as different framesets if they take different numbers of arguments. For example, the verb decline has two framesets: (15) Frameset decline.01  X  X  X o down incrementally X  X  (16) Frameset decline.02  X  X  X emure, reject X  X 
However, alternations which preserve verb meanings, such as causative/inchoative or object deletion, are considered to be one frameset only, as shown in the example (17).
Both the transitive and intransitive uses of the verb open correspond to the same frameset, with some of the arguments left unspecified: (17) Frameset open.01  X  X  X ause to open X  X  criteria for distinguishing among framesets. For example, see.01 allows for either an NP object or a clause object: (18) Frameset see.01  X  X  X iew X  X  corresponding simplex verb, whether the meanings are approximately the same or not. Example (19-21) presents three of the framesets for cut : (19) Frameset cut.01  X  X  X lice X  X  (20) Frameset cut.04  X  X  X ut off = slice X  X  (21) Frameset cut.05  X  X  X ut back = reduce X  X  78
Note that the verb and particle do not need to be contiguous; (20) above could just as well be phrased The seed companies cut the tassels of each plant off .
 4,500 framesets described, implying an average polysemy of 1.36. Of these verb frames, only 21.6% (721/3342) have more than one frameset, while less than 100 verbs have four or more. Each instance of a polysemous verb is marked as to which frameset it belongs to, with interannotator (ITA) agreement of 94%. The framesets can be viewed as extremely coarse-grained sense distinctions, with each frameset corresponding to one or more of the Senseval 2 WordNet 1.7 verb groupings. Each grouping in turn corresponds to several WordNet 1.7 senses (Palmer, Babko-Malaya, and Dang 2004). 3.2 Secondary Predications associated with numbered arguments in the frames files. The first one, EXT (extent), or walked 3 miles. The second, PRD (secondary predication), marks a more subtle relationship. If one thinks of the arguments of a verb as existing in a dependency tree, all arguments depend directly on the verb. Each argument is basically independent of the others. There are those verbs, however, which predict that there is a predicative relationship between their arguments. A canonical example of this is call in the sense of between John and an idiot (at least in Mary X  X  mind). The PRD tag is associated with the predicates on the Arg1 John . This helps to disambiguate the crucial difference between the following two sentences: predicative reading ditransitive reading
Mary called John a doctor. Mary called John a doctor. 5 ( Arg0: Mary Arg0: Mary Rel: called Rel: called Arg1: John (item being labeled) Arg2: John (benefactive) Arg2-PRD: a doctor (attribute) Arg1: a doctor (thing summoned) annotator, as in example (28). 3.3 Subsumed Arguments
Because verbs which share a VerbNet class are rarely synonyms, their shared argument structure occasionally takes on odd characteristics. Of primary interest among these are the cases in which an argument predicted by one member of a class cannot be attested by another member of the same class. For a relatively simple example, consider the verb hit , in VerbNet classes 18.1 and 18.4. This takes three very obvious arguments: (22) Frameset hit  X  X  X trike X  X  necessarily included in the semantics of the verb itself. For example, kick is essentially  X  X  X it with the foot X  X  and hammer is exactly  X  X  X it with a hammer. X  X  For these verbs, then, the Arg2 might not be available, depending on how strongly the instrument is incorporated into the verb. Kick , for example, shows 28 instances in the treebank but only one instance of a (somewhat marginal) instrument: (23) [
Hammer shows several examples of Arg2s, but these are all metaphorical hammers: (24) Despite the relatively strong economy, [ merged into one in certain syntactic situations. Consider the case of meet , which canonically takes two arguments: (25) Frameset meet  X  X  X ome together X  X  80
It is perfectly possible, of course, to mention both meeting parties in the same constituent: (26) [
In these cases there is an assumed or default Arg1 along the lines of  X  X  X ach other X  X : (27) [ attached as either one constituent or two: (28) Frameset connect.01  X  X  X ttach X  X  3.4 Role Labels and Syntactic Trees The Proposition Bank assigns semantic roles to nodes in the syntactic trees of the Penn
Treebank. Annotators are presented with the roleset descriptions and the syntactic tree constituents are not explicitly marked either in the treebank trees or in the semantic one node may be assigned the same role. The annotation software does not require that the ways in which we handle the specifics of the treebank syntactic annotation style in this section.
 3.4.1 Prepositional Phrases. The treatment of prepositional phrases is complicated by several factors. On one hand, if a given argument is defined as a  X  X  X estination, X  X  then in a sentence such as John poured the water into the bottle, the destination of the water is information that the water will end up inside the bottle. Thus arguments should properly be associated with the NP heads of prepositional phrases. On the other hand, however, ArgMs which are prepositional phrases are annotated at the PP level, not the NP level. For the sake of consistency, then, numbered arguments are also tagged at the
PP level. This also facilitates the treatment of multiword prepositions such as out of , according to, and up to but not including . 7 (29) [ 3.4.2 Traces and Control Verbs. The Penn Treebank contains empty categories known as traces, which are often coindexed with other constituents in the tree. When a trace is assigned a role label by an annotator, the coindexed constituent is automatically added to the annotation, as in (30) [ problem for the analysis and annotation of semantic structure. Consider a sentence $1.55 a share. (wsj_0015). The Penn Treebank X  X  analysis assigns a single sentential (S) argument to the verb force . In the PropBank annotation, we split the sentential phrase and verb phrase but not to the S node which subsumes them: (31) Frameset cause, force, persuade , etc.  X  X  X mpelled action X  X 
In such a sentence, the object of the control verb will also be assigned a semantic role by the subordinate clause X  X  verb: (32) Commonwealth Edison said the ruling could force [ 82 promise result in the subject of the main clause being assigned two roles, one for each verb: (33) [ (34) [
We did not find a single case of a subject control verb used with a direct object and an infinitival clause (e.g., John promised Mary to come ) in the Penn Treebank. exceptional case marking (ECM) verbs, where an infinitival subordinate clause is a single semantic argument: (35) Frameset expect  X  X  X ook forward to, anticipate X  X 
While Ford is given a semantic role for the verb meet , it is not given a role for expect . property that the verb and its subject can be inserted almost anywhere within another this situation, there is no constituent holding the whole of the utterance while not also holding the verb of saying. We annotate these cases by allowing a single semantic role to point to the component pieces of the split constituent in order to cover the correct, discontinuous substring of the sentence. (36) Frameset say
In the flat structure we have been using for example sentences, this looks like a case of repeated role labels. Internally, however, there is one role label pointing to multiple constituents of the tree, shown in Figure 1.
 4. The Propbank Development Process
Since the Proposition Bank consists of two portions, the lexicon of frames files and the annotated corpus, the process is similarly divided into framing and annotation. 4.1 Framing lexeme, begins with the examination of a sample of the sentences from the corpus containing the verb under consideration. These instances are grouped into one or more major senses, and each major sense is turned into a single frameset. To show all the included in the frames file, in the same format as the examples above. In many cases a particular realization will not be attested within the Penn Treebank corpus; in these cases, a constructed sentence is used, usually identified by the presence of the characters of John and Mary. Care was taken during the framing process to make synonymous verbs (mostly in the sense of  X  X  X haring a VerbNet Class X  X ) have the same framing, with the same number of roles and the same descriptors on those roles.
Generally speaking, a given lexeme/sense pair required 10 X 15 minutes to frame, although highly polysemous verbs could require longer. With the 4,500+ framesets currently in place for PropBank, this is clearly a substantial time investment, and the frames files represent an important resource in their own right. We were able to use membership in a VerbNet class which already had consistent framing to project accurate frames files for up to 300 verbs. If the overlap between VerbNet and
PropBank had been more than 50%, this number might have been higher. 4.2 Annotation We begin the annotation process by running a rule-based argument tagger (Palmer,
Rosenzweig, and Cotton 2001) on the corpus. This tagger incorporates an extensive lexicon, entirely separate from that used by PropBank, which encodes class-based 84 mappings between grammatical and semantic roles. The rule-based tagger achieved 83% accuracy on pilot data, with many of the errors due to differing assumptions corrected by hand. Annotators are presented with an interface which gives them access to both the frameset descriptions and the full syntactic parse of any sentence from the treebank and allows them to select nodes in the parse tree for labeling as arguments of the predicate selected. For any verb they are able to examine both the descriptions of the arguments and the example tagged sentences, much as they have been presented here. The tagging is done on a verb-by-verb basis, known as lexical sampling, rather than all-words annotation of running text.
 subsequent sections). For this reason a domain-specific subcorpus was automatically extracted from the entirety of the treebank, consisting of texts roughly primarily anywhere in the text. This  X  X  X inancial X  X  subcorpus comprised approximately one-third of the treebank and served as the initial focus of annotation.
 with a few verbs occurring very often (say , for example, is the most common verb, with over 10,000 instances in its various inflectional forms) and most verbs occurring two or fewer times. As with the distribution of the lexical items themselves, the framesets also display a Zipfian distribution: A small number of verbs have many framesets ( go has 20 when including phrasal variants, and come, get, make, pass, take, and turn each have more than a dozen) while the majority of verbs (2581/3342) have only one frameset.
For polysemous verbs annotators had to determine which frameset was appropriate information was explicitly marked only during a separate pass.

Treebank without actually replicating any of the lexical material or structure of that corpus. The process of annotation was a two-pass, blind procedure followed by an labeling decisions and the choice of frameset were adjudicated.
 undergraduates to holders of doctorates, including linguists, computer scientists, and others. Undergraduates have the advantage of being inexpensive but tend to work for overall judgments although several of our nonlinguist annotators also had excellent skills. The learning curve for the annotation task tended to be very steep, with most annotators becoming comfortable with the process within three days of work. This contrasts favorably with syntactic annotation, which has a much longer learning curve (Marcus, personal communication), and indicates one of the advantages of using a corpus already syntactically parsed as the basis of semantic annotation. Over three years. The framesets were created and annotation disagreements were adju-dicated by a small team of highly trained linguists: Paul Kingsbury created the frames files and managed the annotators, and Olga Babko-Malaya checked the frames files for consistency and did the bulk of the adjudication.
 using the kappa statistic (Siegel and Castellan 1988), which is defined with respect to the probability of interannotator agreement, P  X  A  X  , and the agreement expected by chance, P  X  E  X  :
Measuring interannotator agreement for PropBank is complicated by the large num-ment between annotators to be much higher than chance, because while any node in the parse tree can be annotated, the vast majority of arguments are chosen from the small number of nodes near the verb. In order to isolate the role classification decisions the interannotator agreement probability P  X  A  X  is the number of node observation agreements divided by the total number of nodes considered, which is the number of nodes in each parse tree multiplied by the number of predicates annotated in the sentence. All the PropBank data were annotated by two people, and in calculating kappa we compare these two annotations, ignoring the specific identities of the annotators for the predicate (in practice, agreement varied with the training and skill that were marked as arguments by both annotators and compute kappa over the choices of possible argument labels. For both role identification and role classification, we compute kappa for two ways of treating ArgM labels. The first is to treat ArgM labels as arguments like any other, in which case ArgM-TMP, ArgM-LOC, and so on are considered separate labels for the role classification kappa. In the second scenario, we ignore ArgM labels, treating them as unlabeled nodes, and calculate agreement for identification and classification of numbered arguments only.
 on role identification is very high (.99 under both treatments of ArgM), given the large number of obviously irrelevant nodes. Reassuringly, kappas for the more difficult sidering only numbered arguments. Kappas on the combined identification and types of ArgM and .93 over numbered arguments only. Interannotator agreement among nodes that either annotator identified as an argument was .84, including ArgMs and .87, excluding ArgMs.
 on the selection of function tags, as shown in the confusion matrices of Tables 3 and 4. 86 Certain types of functions, particularly those represented by the tags ADV, MNR, and DIS, can be difficult to distinguish. For example, in the sentence Also, substantially lower growth (wsj_0132), the phrase relative to earnings growth could be interpreted as a manner adverbial (MNR), describing how the tax outlays were kept flat, or as a general-purpose adverbial (ADV), merely providing more information on the keeping temporal adverb marking time or a sequence of events ( . . . the Senate then broadened the list further . . . (wsj_0101)) but can also mark a consequence of another action ( ...iffor (TMP, ADV, and DIS, respectively) and can easily trip up an annotator.
 release in June 2002. The fully annotated and adjudicated corpus was completed in
March 2004. Both of these are available through the Linguistic Data Consortium, although because of the use of the stand-off notation, prior possession of the treebank is also necessary. The frames files are distributed separately and are available through the project Web site at http://www.cis.upenn.edu/  X  ace/.
 5. FrameNet and PropBank The PropBank project and the FrameNet project at the International Computer Science
Institute (Baker, Fillmore, and Lowe 1998) share the goal of documenting the syntactic realization of arguments of the predicates of the general English lexicon by annotating ogies are quite different. FrameNet is focused on semantic frames , as a schematic representation of situations involving various participants, props, and other conceptual roles (Fillmore 1976). The project methodology has proceeded on a frame-by-frame basis , that is, by first choosing a semantic frame (e.g., Commerce), defining the frame and its participants or frame elements (BUYER, GOODS, SELLER,
MONEY), listing the various lexical predicates which invoke the fram e( buy, sell, etc.), and then finding example sentences of each predicate in a corpus (the British National
Corpus was used) and annotating each frame element in each sentence. The example sentences were chosen primarily to ensure coverage of all the syntactic realizations of the frame element s, and simple examples of these realizations were preferred over those involving complex syntactic structure not immediately relevant to the lexical were annotated. A word with multiple distinct senses would generally be analyzed as belonging to different frames in each sense but may only be found in the FrameNet corpus in the sense for which a frame has been defined. It is interesting to note that the semantic frames are a helpful way of generalizing between predicates; words in the same frame have been found frequently to share the same syntactic argument structure (Gildea and Jurafsky 2002). A more complete description of the FrameNet project can be found in Baker, Fillmore, and Lowe (1998) and Johnson et al. (2002), and the ramifications for automatic classification are discussed more thoroughly in Gildea and Jurafsky (2002).

Treebank, no matter how complex or unexpected. Similarly to FrameNet, PropBank
VerbNet classes for determining semantic relatedness. However, there is much less emphasis on the definition of the semantics of the class that the verbs are associated with, although for the relevant verbs additional semantic information is provided through the mapping to VerbNet. The PropBank semantic roles for a given VerbNet class may not correspond to the semantic elements highlighted by a particular FrameNet frame, as shown by the examples of Table 5. In this case, FrameNet X  X 
COMMERCE frame includes roles for Buyer (the receiver of the goods) and Seller (the receiver of the money) and assigns these roles consistently to two sentences describing the same event:
FrameNet annotation: (37) [ (38) [ 88
PropBank annotation: (39) [ (40) [
PropBank requires an additional level of inference to determine who has possession of sentences is an Agent, represented in PropBank by labeling both subjects as Arg0. construction:
FrameNet annotation: (41) [ (42) [ (43) [
PropBank annotation: (44) [ (45) [ (46) [ and adjectives. 10 PropBank annotation also differs in that it takes place with reference to the Penn Treebank trees; not only are annotators shown the trees when analyzing annotators mark the beginning and end points of frame elements in the text and add predicate. 6. A Quantitative Analysis of the Semantic-Role Labels
The stated aim of PropBank is the training of statistical systems. It also provides a rich resource for a distributional analysis of semantic features of language that have hitherto been somewhat inaccessible. We begin this section with an overview of general characteristics of the syntactic realization of the different semantic-role labels verb class membership. We base this analysis on previous work by Merlo and
Stevenson (2001). In the following section we discuss the performance of a system trained to automatically assign the semantic-role labels. 6.1 Associating Role Labels with Specific Syntactic Constructions
We begin by simply counting the frequency of occurrence of roles in specific syntactic positions. In all the statistics given in this section, we do not consider past-or present-participle uses of the predicates, thus excluding any passive-voice sentences. The syntactic positions used are based on a few heuristic rules: Any NP under an S node in the treebank is considered a syntactic subject, and any NP under a VP is considered an object. In all other cases, we use the syntactic category of the argument X  X  node in the treebank tree: for example, SBAR for sentential complements and PP for prepositional phrases. For prepositional phrases, as well as for noun phrases that are the object of
PP-in, PP-with. Table 6 shows the most frequent semantic roles associated with var-ious syntactic positions, while Table 7 shows the most frequent syntactic positions for various roles.
 interpreting the results, as the semantic-role labels are defined on a per-frameset basis and do not necessarily have corpus-wide definitions. Nonetheless, a number of trends are apparent. Arg0, when present, is almost always a syntactic subject, while the thematic hierarchy in which the highest-ranking role present in a sentence is given the 90 honor of subjecthood. Going from syntactic position to semantic role, the numbered arguments are more predictable than the non-predicate-specific adjunct roles. The two exceptions are the roles of  X  X  X odal X  X  (MOD) and  X  X  X egative X  X  (NEG), which as previously discussed are not syntactic adjuncts at all but were simply marked as ArgMs as the best means of tracking their important semantic contributions. They are almost always respectively. 6.2 Associating Verb Classes with Specific Syntactic Constructions
Turning to the behavior of individual verbs in the PropBank data, it is interesting to see how much correspondence there is between verb classes proposed in the literature and the annotations in the corpus. Table 8 shows the PropBank semantic role labels for thesubjectsofeachverbineachclass.MerloandStevenson(2001)aimto automatically classify verbs into one of three categories: unergative, unaccusative, and object-drop . These three categories, more coarse-grained than the classes of Levin examples: Unergative: [ 92 Unaccusative: [ Object-Drop: [ 6.2.1 Predictions. In our data, the closest analogs to Merlo and Stevenson X  X  three roles of Causal Agent, Agent, and Theme are ArgA, Arg0, and Arg1, respectively. We hypothesize that PropBank data will confirm 1. that the subject can take one of two roles (Arg0 or Arg1) for 2. that Arg1s appear more frequently as subjects for intransitive and Stevenson verbs which appear in PropBank (80%), regardless of transitivity, in order to measure whether the data in fact reflect the alternations between syntactic and semantic roles that the verb classes predict. For each verb, we show counts only for unmarked sense. 6.2.2 Results of Prediction 1. The object-drop verbs of Merlo and Stevenson do in fact show little variability in our corpus, with the subject almost always being Arg0. The unergative and unaccusative verbs show much more variability in the roles that can have Arg0 as subject, presumably as a result of the small number of occurrences. 6.2.3 Results of Prediction 2. As predicted, there is in general a greater preponderance of Arg1 subjects for unaccusatives than for unergatives, with the striking exception of a few unergative verbs, such as jump and rush , whose subjects are almost always Arg1.
Jump is being affected by the predominance of a financial-subcorpus sense used for stock reportage (79 out of 82 sentences), which takes jump as rise dramatically: Jaguar shares jumped 23 before easing to close at 654, up 6. (wsj_1957) Rush is being affected by a framing decision, currently being reconsidered, wherein rush was taken to mean cause to move quickly . Thus the entity in motion is tagged Arg1, as in Congress in Congress would and unaccusatives is not apparent from the PropBank data in this table, since we are not distinguishing between transitives and intransitives, which is left for future experiments.
 most common, but in a few cases this is not the case because of the domain of the text.
For example, the second frameset for kick, corresponding to the phrasal usage kick in, meaning begin, accounted for seven instances versus the five instances for frameset 1. ponding to Arg1, as in (47) [ corresponds to the entry for kick in the  X  X  X bject-Drop X  X  section of Table 8. role played by even the relatively coarse-grained sense tagging exemplified by the framesets. 7. Automatic Determination of Semantic-Role Labels
The stated goal of the PropBank is to provide training data for supervised automatic role labelers, and the project description cannot be considered complete without a discussion of PropBank X  X  suitability for this purpose. One of PropBank X  X  important features as a practical resource is that the sentences chosen for annotation are from the same Wall Street Journal corpus used for the original Penn Treebank project, and thus hand-checked syntactic parse trees are available for the entire data set. In this section, we examine the importance of syntactic information for semantic-role labeling by comparing the performance of a system based on gold-standard parses with one using automatically generated parser output. We then examine whether it is possible that the additional information contained in a full parse tree is negated by the errors present in automatic parser output, by testing a role-labeling system based on a flat or  X  X  X hunked X  X  representation of the input.
 the FrameNet project to automatically assign semantic roles. The system first passed sentences through an automatic parser (Collins 1999), extracted syntactic features from the parses, and estimated probabilities for semantic roles from the syntactic and lexical features. Both training and test sentences were automatically parsed, as no hand-annotated parse trees were available for the corpus. While the errors introduced by the parser no doubt negatively affected the results obtained, there was no direct way of quantifying this effect. One of the systems evaluated for the Message Understanding
Conference task (Miller et al. 1998) made use of an integrated syntactic and semantic model producing a full parse tree and achieved results comparable to other systems that did not make use of a complete parse. As in the FrameNet case, the parser was not 94 trained on the corpus for which semantic annotations were available, and the effect of better, or even perfect, parses could not be measured.

Jurafsky (2002) system were applied to the PropBank corpus. The existence of the hand-annotated treebank parses for the corpus allowed us to measure the improvement in performance offered by gold-standard parses. 7.1 System Description
Probabilities of a parse constituent belonging to a given semantic role are calculated from the following features: semantic roles: Examples include noun phrase (NP), verb phrase (VP), and clause (S). constituent to the predicate. 11 It is defined as the path from the predicate through the parse tree to the constituent in question, represented as a string of parse tree nonterminals linked by symbols indicating upward or downward movement through the tree, as shown in Figure 2. Although the path is composed as a string of symbols, our systems treat the string as an atomic value. The path includes, as the first element of the string, the part of speech of the predicate, and as the last element, the phrase type or syntactic category of the sentence constituent marked as an argument. before or after the predicate. This feature is highly correlated with grammatical feature may overcome the shortcomings of reading grammatical function from the parse tree, as well as errors in the parser output.
 in predicting semantic roles, because direct objects of active verbs correspond to subjects of passive verbs. An instance of a verb is considered passive if it is tagged as a past participle (e.g., taken ), unless it occurs as a descendent verb phrase headed by any form of have (e.g., has taken ) without an intervening verb phrase headed by any form of be (e.g., has been taken ). type of the role filler. Headwords of nodes in the parse tree are determined using the same deterministic set of headword rules used by Collins (1999).
 highest-probability assignment of roles r i to all constituents i in the sentence, given the predicate p :
P  X  r topmost distributions available from a back-off lattice, shown in Figure 3. for a set of roles appearing in a sentence given a predicate, using the following formula: interaction among the role assignments for individual constituents while making certain independence assumptions necessary for efficient probability estimation. In particular, we assume that sets of roles appear independent of their linear order and given the constituent X  X  role. 7.1.1 Results. We applied the same system, using the same features, to a preliminary release of the PropBank data. The data set used contained annotations for 72,109 predicate-argument structures containing 190,815 individual arguments and examples 96 from 2,462 lexical predicates (types). In order to provide results comparable with the statistical parsing literature, annotations from section 23 of the treebank were used as version of the data used in these experiments was not tagged for WordNet word sense or PropBank frameset. Thus, the system neither predicts the frameset nor uses it as a feature.
 constituents which are arguments to the predicate and merely has to predict the correct role, and one in which it has to both find the arguments in the sentence and label them correctly. Results are shown in Tables 10 and 11. Results for FrameNet are based on a test set of 8,167 individual labels from 4,000 predicate-argument structures.
As a guideline for interpreting these results, with 8,167 observations, the threshold for statistical significance with p &lt; .05 is a 1.0% absolute difference in performance (Gildea and Jurafsky 2002). For the PropBank data, with a test set of 8,625 individual labels, the threshold for significance is similar. There are 7,574 labels for which the predicate has been seen 10 or more times in training (third column of the tables).
 number of training examples for many of the predicates. The FrameNet data contained at least 10 examples from each predicate, while 12% of the PropBank data had fewer than 10 training examples. Removing these examples from the test set gives 82.8% accuracy with gold-standard parses and 80.9% accuracy with automatic parses. 7.1.2 Adding Traces. The gold-standard parses of the Penn Treebank include several types of information not typically produced by statistical parsers or included in their evaluation. Of particular importance are traces , empty syntactic categories which interpreted and include a link to the relevant constituent. Traces are used to indicate cases of wh -extraction, antecedents of relative clauses, and control verbs exhibiting the syntactic phenomena of raising and  X  X  X qui. X  X  Traces are intended to provide hints as to the semantics of individual clauses, and the results in Table 11 show that they do so effectively. When annotating syntactic trees, the PropBank annotators marked the ignored, and the semantic-role label was assigned to the antecedent in both training information, and in cases of trace chains, the semantic-role label is assigned to the trace in training and test conditions. Trace information boosts the performance of the system by roughly 5%. This indicates that systems capable of recovering traces (Johnson 2002; Dienes and Dubey 2003) could improve semantic-role labeling.
 behavior in the system warrants a closer look. The path feature is most useful as a way of finding arguments in the unknown boundary condition. Removing the path feature from the known-boundary system results in only a small degradation in performance, from 82.0% to 80.1%. One reason for the relatively small impact may be sparseness of the feature: 7% of paths in the test set are unseen in training data. The most common values of the feature are shown in Table 12, in which the first two rows correspond to standard subject and object positions. One reason for sparsity is seen in the third row: additional VP node to appear in our path feature. We tried two variations of the path feature to address this problem. The first collapses sequences of nodes with the same label, for example, combining rows 2 and 3 of Table 12. The second variation uses only position). Neither variation improved performance in the known-boundary condition.
As a gauge of how closely the PropBank semantic-role labels correspond to the path feature overall, we note that by always assigning the most common role for each path (for example, always assigning Arg0 to the subject position), and using no other features, we obtain the correct role 64.0% of the time, versus 82.0% for the complete system. Conditioning on the path and predicate, which allows the subject of different verbs to receive different labels but does not allow for alternation behavior within a verb X  X  argument structure, yields an accuracy rate of 76.6%.
 in the gold standard. Results are shown for the unknown-boundaries condition, using gold-standard parses and traces (last row, middle two columns of Table 11). The 98 given role is correctly identified as being a semantic role, even if it is labeled with the wrong role. The more central, numbered roles are consistently easier to identify than the adjunct-like ArgM roles, even when the ArgM roles have preexisting Treebank function tags. 7.2 The Relation of Syntactic Parsing and Semantic-Role Labeling
Many recent information extraction systems for limited domains have relied on finite-
Among such systems, Hobbs et al. (1997) built finite-state recognizers for various entities, which were then cascaded to form recognizers for higher-level relations, while
Ray and Craven (2001) used low-level  X  X  X hunks X  X  from a general-purpose syntactic analyzer as observations in a trained hidden Markov model. Such an approach has a avoided. It is also possible that this approach may be more robust to error than parsers.
Our experiments working with a flat,  X  X  X hunked X  X  representation of the input sentence, described in more detail in Gildea and Palmer (2002), test this finite-state hypothesis.
In the chunked representation, base-level constituent boundaries and labels are pres-ent, but there are no dependencies between constituents, as shown by the following sample sentence: (48) [ by Tjong Kim Sang and Buchholz (2000). Thus, the experiments were carried out using gold-standard rather than automatically derived chunk boundaries, which we believe will provide an upper bound on the performance of a chunk-based system. Distance in chunks from the predicate was used in place of the parser-based path feature. chunked representation for labeling semantic roles. This is the case even if we relax the scoring criteria to count as correct all cases in which the system correctly identifies the first chunk belonging to an argument (last row of Table 14).
 systems, consider the following sentence, with human annotations showing the arguments of the predicate support : (49) [
Our system based on automatic parser output assigned the following analysis: (50) Big investment banks refused to step up to the plate to support syntactically distant from the verb support . The original treebank syntactic tree contains a trace which would allow one to recover this relation, coindexing the empty subject position of support with the noun phrase Big investment banks . However, our automatic parser output does not include such traces. The system based on gold-standard trees and incorporating trace information produced exactly the correct labels: (51) [
The system based on (gold-standard) chunks assigned the following semantic-role labels: (52) Big investment banks refused to step up to [
Here, as before, the true Arg0 relation is not found, and it would be difficult to imagine 100 unlike in the tree-based output, the Arg0 label is mistakenly attached to a noun phrase immediately before the predicate. The Arg1 relation in direct-object position is fairly easily identifiable in the chunked representation as a noun phrase directly following the verb. The prepositional phrase expressing the Manner relation, however, is not constituent is VB j VP , PP, which identifies the prepositional phrase as attaching to the based system sees this as a prepositional phrase appearing as the second chunk after the predicate. Although this may be a typical position for the Manner relation, the fact represented.

Ma ` rquez 2004) have reported higher results for chunk-based systems, but to date chunk-based systems have not closed the gap with the state-of-the-art results based on parser output. (1999) return much richer representations than a chunker, they do not include a great deal of the information present in the original Penn Treebank. Specifically, long-distance dependencies indicated by traces in the treebank are crucial for seman-parsers.
 (Steedman 2000). The parser, described in detail in Hockenmaier and Steedman (2002), is trained on a version of the Penn Treebank automatically converted to CCG representations. The conversion process uses the treebank X  X  trace information to make underlying syntactic relations explicit. For example, the same CCG-level relation transitive clause, a relative clause, or a question with wh -extraction. Using the CCG-based parser, Gildea and Hockenmaier (2003) find a 2% absolute improvement over the Collins parser in identifying core or numbered PropBank arguments. This points to the shortcomings of evaluating parsers purely on constituent precision and recall; we feel that a dependency-based evaluation (e.g., Carroll, Briscoe, and Sanfilippo 1998) is more relevant to real-world applications. 8. Conclusion
Treebank one step closer to a detailed semantic representation by adding semantic-role structures are more complex than one might at first expect. Alternations in the realization of semantic arguments of the type described by Levin (1993) turn out to be common in practice as well as in theory, even in the limited genre of Wall Street Journal verb, rapid consistent annotation has been achieved, and the corpus is available through the Linguistic Data Consortium. For information on obtaining the frames file, please consult http:/ /www.cis.upenn.edu/  X  ace/.
 taggers, and in addition to ourselves there is a growing body of researchers engaged in this task. Chen and Rambow (2003) make use of extracted tree-adjoining grammars.
Most recently, the Gildea and Palmer (2002) scores presented here have been improved markedly through the use of support-vector machines as well as additional features for named entity tags, headword POS tags, and verb clusters for back-off (Pradhan et al. 2003) and using maximum-entropy classifiers (He and Gildea 2004, Xue and Palmer 2004). This group also used Charniak X  X  parser instead of Collins X  X  and tested the system on TDT data. The performance on a new genre is lower, as would be expected. providing information relevant for this level of semantic interpretation. In addition to the constituent structure, the headword information, produced as a side product, is an important feature. Automatic parsers, however, still have a long way to go. Our results using hand-annotated parse trees including traces show that improvements in parsing should translate directly into more accurate semantic representations.
 can be used to simplify the effort involved in developing information extraction (IE) systems. Researchers were able to construct a reasonable IE system by simply mapping necessity of building explicit regular expression pattern matchers (Surdeanu et al. 2003). There is equal hope for advantages for machine translation, and proposition banks in Chinese (Xue and Palmer 2003) and Korean are already being built, focusing where possible on parallel data. The general approach ports well to new languages, with the major effort continuing to go into the creation of frames files for verbs. sentation. Annotation of nominalizations and other noun predicates is currently being added by New York University, and a Phase II (Babko-Malaya et al.) that will include eventuality variables, nominal references, additional sense tagging, and discourse connectives is underway.
 role labeling. As a first step we are producing a version of PropBank that uses more in-formative thematic labels based on VerbNet thematic labels (Kipper, Palmer, and
Rambow 2002). We are also working with FrameNet to produce a mapping between our annotation and theirs which will allow us to merge the two annotated data sets.
Finally, we will explore alternative machine-learning approaches and closer integra-tion of semantic-role labeling and sense tagging with the parsing process. 102 Acknowledgments References 104
