 Document generation from low level data and its utilization is one of the most challenging tasks in document engineering. Word occurrence detection is a fundamental problem in the recognized document utilization obtained by a recognizer, such as OCR and speech recognition. Given a set of words, such as a dictionary, this paper proposes an efficient dy-namic programming (DP) algorithm to find the occurrences of each word in a text. In this paper, the string similarity is measured by a statistical similarity model that enables a definition of the similarities in the character level as well as edit operation level. The proposed algorithm uses tree structures to measure similarities in order to avoid measur-ing similarities of the same substrings appearing in different parts of the text and words. The time complexity of the proposed algorithm is O ( | W | X | S | X | Q | ), where | W | | S | ) denote the number of nodes in the trees representing the word set (resp. the text), and | Q | donotes the number of the states of the model used for string similarity. This pa-per shows the proposed algorithm is experimentally about six times faster than a naive DP algorithm.
 H.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Experimentation Dynamic Programming, Suffix Tree, Statistical Model
Document recognition from low level data and its utiliza-tion is one of the challenging tasks in document engineer-ing. Recently a lot of documents are created in digital form, Copyright 2006 ACM 1-59593-433-2/06/0011 ... $ 5.00. however there still remains a lot of information that should be recognized from low level data. For example, in digi-tal libraries, old books and journals printed on papers are scanned and OCR is applied to digitize them. Speech data is another type of low level data that contains valuable infor-mation. For example, transcripts of radio and video news are useful for providing effective access to the data using information retrieval and information filtering technologies. In cooperative working environments, conversations between people contain important information, and transcripts of it become a valuable information source. Call centers are an-other important application area for speech data utilization [10] where conversations between operators and customers are accumulated and utilized for later calls.

In order to utilize information represented by low level data, we need both document recognition and utilization technologies. As for document recognition, researchers have developed various techniques and systems for each type of data. OCR is one of the typical recognizers for scanned doc-ument images. Recent systems have achieved higher recog-nition accuracy. Speech recognition is another typical rec-ognizer.

As for the recognized document utilization, approximate string matching [12] plays a central role. Several kinds of approximate string matching algorithms were examined [11, 22, 14] for OCR processed documents. Taghva et al. [17, 19, 18] experimentally studied the affect of OCR errors on IR techniques, such as the vector space model and the prob-abilistic model [2]. Spoken document retrieval [26, 16] is an-other recognized document utilization technique. In these studies spoken documents are usually handled as phonemic sequences instead of characters, and an approximate match-ing of the phonemic sequences is applied for document re-trieval.
 The recognition accuracy is affected by various factors. For example, the recognition accuracy of OCR is affected by the quality of the scanned documents. For multi-lingual text, the accuracy of OCR often seriously decreases. As a result, we obtain documents that consists of erroneous texts with a wide range of recognition accuracy. On the other hand, the accuracy of speech recognition is much lower than for OCR and it is a very sensitive subject in the record-ing environment and for speakers. It is very hard to accu-rately recognize speech data recorded in noisy environments. Therefore, recognized document utilization technology must be able to handle the recognition errors with a wide range of accuracy.

We have been studying a recognized document utilization system. For the recognized document utilization, we pro-posed a statistical text recognition error model and devel-oped an algorithm for a parameter estimation of the model [21]. We applied the model to a syntactical analysis and in-formation extraction from OCR generated strings [20]. Sta-tistical error models usually require high computational cost and our model has the same disadvantage. So, in this paper we discuss an algorithm for detecting word occurrences from a recognized text using the statistical error model. With re-cent IR techniques, documents are usually regarded as a set of words, i.e., a bag of words model. The algorithm for word occurrence detection is important as a basic tool to convert recognized documents into the bag of words, and then, state-of-the-art IR techniques can be applied to them. The main contribution of this paper is to develop an efficient algorithm for multi-word detection to apply the text error model to IR from OCR generated text.

Now let us define the problem. Here are some notations used throughout this paper. Problem Definition. The problem discussed in this paper is to enumerate the substrings that are similar to one of the given set of words. Assume that for any pair of strings  X  and  X  the similarity sim (  X ,  X  ) is defined. Then, the problem is:
The rest of this paper is organized as follows. We first overview the string similarities used in approximate match-ing and describe our statistical model in Section 2. Then, we propose an efficient word occurrence detection algorithm us-ing dynamic programming technique in Section 3. Section 4 presents the experimental results from the viewpoint of processing efficiency. Finally Section 5 concludes the paper and addresses some future directions of our studies.
In order to solve the word occurrence detection problem, we need to define the similarity of the strings. The edit dis-tance is one of the most popular similarities used for measur-ing the string similarity. More elaborate string similarities have been proposed in the study of OCR text utilization. The most typical OCR error model is the confusion matrix, which maps a character c to a set of characters that an OCR may incorrectly recognize as c . For each entry in the confu-sion matrix, we can assign the probability that an OCR will recognize the character incorrectly [6]. The confusion matrix is also used in spoken document retrieval [16]. OCR errors usually contain n-to-m errors such as ri  X  n or m  X  iii [7]. However, the confusion matrix cannot handle n-to-m errors. Li and Lopresti proposed an OCR error model in which error patterns were categorized from the viewpoint of their string lengths [9]. In this model, a pair ( i, j )oforig-inal and recognized string lengths is used as an OCR error pattern, and a weight or a penalty for incorrect recognition is assigned to each pattern to calculate the similarity of two strings. This model can represent n-to-m errors as well as insertion, deletion, and substitution errors. However, to ap-ply the model to a matching problem, we must determine the cost for every pattern. Furthermore, the weight is the same if an error pattern is the same.

When using edit distance we need to define the costs of in-sertion, deletion and substitution operations. However, cost tuning is labor intensive work. Ristad et al. [15] proposed a learning edit distance that has the ability to learn about costs from the training data. The similarity proposed by these studies is defined based on the edit operations.
We proposed a similarity model called the Dual and Vari-able Length Hidden Markov Model (DVHMM) [21] that de-fines the similarity of strings at the character level like the confusion matrix, but at the same time, it can learn the similarity of characters from training data like the learning edit distance. In the DVHMM we assume that a string  X  is converted to a string  X  through a noisy communication channel. Then the similarity of a pair (  X ,  X  ) is defined as the joint probability that the original string  X  is observed as  X  .

The DVHMM is a form of an HMM. Instead of produc-ing a string, it produces a pair of strings, one representing the original string and the other representing the observed string.

The DVHMM produces a pair of strings by walking around the finite states like HMM and produces a portion of the pair of strings at each state. A state of the DVHMM is charac-terized by the pair of lengths of the original and observed output strings and the state characterized by ( i, j ) produces a pair of original and observed strings with lengths i and respectively.

Figure 1 shows an example of a DVHMM that consists of four states. A state q is characterized by (1 , 0). From the viewpoint of the edit operation, this state corresponds to the delete operation. A state r is characterized by (0 and it corresponds to the insert operation. A state s characterized by (1 , 1) and it corresponds to the substitution or no edit operation. A state f is the final state where every state transition ends.

Each state except for the final state produces a pair of strings according to an output probability distribution. Sup-pose the alphabet is { a, b } . Then, the state q in Figure 1 produces a pair of strings ( a,  X ) or ( b,  X ) where  X  stands for a null string. Output probability is assigned to each pair of strings, and the DVHMM produces a pair according to the probabilities. The tables in Figure 1 (b) show the output probabilities at each state.

The DVHMM has the initial probability distribution, which is omitted in Figure 1 and the transition probabilities, which are denoted as arcs in Figure 1. In this way, costs of edit operations are represented in the DVHMM as the combina-tion of transition and output probabilities. Note that we can easily add a state characterized with ( n, m ) to the transition graph in Figure 1. Since the node corresponds to the n-to-m OCR errors, a DVHMM naturally represents an OCR error model.
The DVHMM is denoted by a quintuple M  X  ( X  ,Q,I,T,O ).
The DVHMM defines the joint probability of a pair of original and observed strings. Let us consider a pair ( ab, aab of strings and the state transitions sqsf in the DVHMM of Figure 1. Since s is substitution and q is an insertion, the pair ( ab, aab ) is produced by producing a portion of the pair of strings at each state in the following way: where the second and third columns stand for the original and observed substrings produced by the state in the first line, respectively.

Generally there are multiple state transitions that produce a given pair of strings. For example, a path ssqf represents the following recognition of the pair ( ab, aab ):
For an original string  X  andanobservedstring  X  , suppose that the DVHMM produces (  X ,  X  )byapath q  X  q 1 q 2  X  X  X  q and that it produces substrings  X  i and  X  i at each state q . Then the DVHMM M gives the joint probability of  X  and  X  with the state transition q as
Pr (  X ,  X , q ; M )=  X  ( q 1 )  X  This probability represents a string similarity. However, it tends to be lower for a word  X  that appears less frequently. It is often the case that the words whose frequency is medium are more important than frequently appeared words in IR. In order to handle this feature of IR, let us introduce the following conditional output probability for each state q string pair (  X ,  X  ) where j stands for the length of recognized string produced at the state q . Then, we consider the following conditional probability instead of the joint probability defined by Eq. (1)
Pr (  X  |  X , q )  X   X  ( q 1 )  X  For an original string  X  andanobservedstring  X  , suppose Q (  X ,  X  ) denotes the set of state transitions of the DVHMM that produces  X  and  X  . Then let us consider the highest joint probability that the DVHMM produces (  X ,  X  ) where the last state of q is the final state f .Wecallthe state transition sequence satisfying condition (4) the most likely transition sequence and denote the sequence as q  X 
As described in Section 2.2, each state of a DVHMM corre-sponds to an edit operation, and a state transition sequence corresponds to edit operations. From this perspective, the state transition that satisfies condition (4) corresponds to the edit operation sequence with the lowest cost. In this way, the probability defined by Eq. (4) gives similarity to apair  X  and  X  of strings. We use Eq. (4) as the similarity function sim (  X ,  X  ) of the problem defined in Section 1. Initial, transition and output probabilities used in the DVHMM can be estimated from t raining data as the param-eters of a statistical model. We developed a maximum likeli-hood estimation algorithm using the expectation maximiza-tion technique. See reference [21] for details. This learning ability of the DVHMM is very important when adapting the similarity to the error patterns of various recognizers and recognition environment. It is sufficient to prepare rec-ognized strings with their original strings to obtain an error model for the objective recognizer and environment. Al-though the learning edit distance has the same ability, the DVHMM can describe more comp lex error patterns by incor-porating states corresponding to the edit operations other than insertion, deletion and substitution.

The DVHMM is a finite state machine and it has the ability to describe the syntax of regular grammar. We ap-plied this model to a syntactical analysis of OCR processed reference strings in academic articles [20]. In this applica-tion, we utilized the abilities of both syntactical description and recognition error modeling of the DVHMM and achieved high performance of syntactical analysis of garbled reference string.
This section derives the algorithms for calculating the sim-ilarity (Eq. (4)). It is defined as the optimization problem with respect to the state transition sequence. Since the enu-meration of all state transition sequences has combinatorial problem, we need efficient algorithms. This section first de-rives recurrent formulas for c alculating the similarity for a pair (  X ,  X  ) of strings, and then, expands it to a tree struc-ture to solve the multi-word detection problem. From the result, we derive an algorithm for the similarity for a set of words and substrings simultaneously using the dynamic programming (DP) technique.
Let us consider a word  X   X  a 1 a 2  X  X  X  a l and a substring b  X  and  X  and a state q of the DVHMM, let R  X  (  X  [: i ] , X  [: denote the highest probability that the DVHMM produces (  X  [: i ] , X  [: j ]) and reaches the state q , i.e., For a state r characterized by ( r i ,r j ), let us consider the highest probability that the DVHMM produces the pair vis-iting r before q . Then from Eq. (1), this probability is given by where i = i  X  r i and j = j  X  r j , respectively. Therefore the highest probability R  X  ((  X  [: i ] , X  [: j ] ,q ) defined by Eq. (5) is derived from the probability, Eq. (6), by the following formula inductively Figure 2 shows the inductive step for the DVHMM in Figure 1. Let us consider R  X  ( a, aa, r ). In order to reach the state r after producing the pair ( a, aa ), the DVHMM must Then, R  X  ( a, aa, r ) is the highest probability in these three cases.

When  X  [: i ]=  X  [: j ] =  X  holds, the highest probability is expressed by With Eqs. (7) and (8) the highest probability R  X  (  X ,  X , q obtained by applying dynamic programming without enu-merating all the state transitions producing the pair (  X ,  X 
Note that for a pair (  X ,  X  ) of strings, the similarity (Eq. (4)) is obtained by This section expands the recurrent formulas derived in Section 3.1 for a set of words and substrings in a text. First we define a word tree for handling the given set of words in which Figure 3 shows a word tree for a word set { aa, ab, ba, bb } We denote a node as W  X  where  X  stands for the substring from the root to the node. This string is the prefix of all the descendant leaves. For example, W a corresponds to a prefix a of the words aa and ab . In this way, any prefix in the given word set is represented with an inner node of the word tree.

On the other hand, we represent a text string with a suffix tree. Figure 4 shows a suffix tree for a text string  X  X baab X . From the characteristics of the suffix tree, any prefix of a substring in the text is represented by a path from the root to the corresponding inner node of the suffix tree. We denote anodeofthesuffixtreeas S  X  where  X  is the string from the root to the node.

For a node n in a tree, we denote the i-th generation ancestor as anc ( n, i ). For example anc ( S aba , 2) is the node S
For a node W  X  and S  X  let N  X  ( W  X  ,S  X  ,q ) denote the high-est joint probability that the DVHMM reaches a state q after producing the corresponding strings, i.e., For a pair ( W  X  ,S  X  ), the highest joint probability with which the DVHMM produces (  X ,  X  ) and reaches a state q after visiting r is given by
N  X  ( anc ( W  X  ,i ) ,anc ( S  X  ,j ) ,r )  X  o (  X  |  X  ,r ) where the state r is characterized by ( i, j )and  X  (resp. is the suffix of  X  (resp.  X  ) whose length is i (resp. j ). For example, let us consider N  X  ( W a ,S aa ,r ). Then the DVHMM must Then, N  X  ( a, aa, r ) is the highest probability among these three cases.

From Eq. (10), we can derive the following recurrent for-mula On the other hand the probability for the pair of roots is given by
For a word  X  and a substring  X  in the objective text, sup-pose that W  X  and S  X  are corresponding nodes in the word tree and the suffix tree, respectively. Then, the similarity is obtained as follows: tree is the answer to the word occurrence detection problem: Note that a node in the suffix tree corresponds to the end of a substring in the text.
The word occurrence detection problem is solved by ob-taining the set (Eq. (13)) for each leaf of the word tree. For this purpose, for each node W  X  let us consider a two-dimensional array N  X  [ S  X  ][ q ]thatstores N  X  ( W  X  ,S each node S  X  in the suffix tree and a state q in the DVHMM. From Eq. (11), for calculating the probability for the node W  X  and S  X  we need to calculate the probabilities for the nodes of their ancestors. Therefore, we start from the root to traverse both trees in a preorder, where parents are vis-ited before children, and siblings are visited in left-to-right order [25]. At each pair fo nodes, we calculate N  X  ( W  X  for each a state in the DVHMM.

Figure 5 shows the algorithm. The main routine is calcu-lateProbability that takes an DVHMM M , the roots of the word tree W  X  , and the suffix tree S  X  as the input. It sets the probabilities of the pair of roots to A according to Eq. (12), then calls the function traverseWordTree that traverses the word tree in a preorder. At each node of the word tree, this function calls traverseSuffixTree that, in turn, traverses the suffix tree in a preorder. At each node of the suffix tree, this function calculates N  X  ( W  X  ,S  X  ,q ) by using Eq. (11) and sets the value to the array N  X  .Ateachleafnode W  X  in the word tree, the function traverseWordTree outputs the substrings corresponding to the nodes of the suffix tree that meet the condition (Eq. (13)).

From Eq. (11) N  X  ( W  X  ,S  X  ,q ) can be calculated in con-stant time. It is clear that it is calculated O ( | W | X | S | X | Q | times where | W | and | S | denote the numbers of nodes of the word tree and the suffix tree, respectively, and | Q | is the
Figure 5: Algorithm for Probability Calculation number of nodes of HMM. Therefore, this algorithm cal-culates the joint probabilities in O ( | W | X | S | X | Q | ). In this algorithm, we need spaces for the DVHMM | M | ,theword tree | W | ,thesuffixtree | S | and the array used for storing the joint probabilities. As shown in the traverseWordTree in Figure 5, the space of the array for a node W  X  is allocated when the algorithm first visits the node in the traverse. On the other hand the space is released when the algorithm leaves the node. From the nature of the depth first traverse, at most the algorithm allocates the arrays for nodes on the path from the root to a leaf at one time. Therefore, the size of the arrays is O ( | w  X  || S || Q | )where | w  X  | stands for the longestwordlengthinthegivenset. Asawhole,thespace required by this algorithm is O ( | M | + | W | + | w  X  | X | S | X | Q |
When implementing the algorithm, we calculate the fol-lowing logarithmic formulas and instead of Eqs. (12) and (11).
The proposed algorithm is categorized into the approx-imate string matching problem. The approximate string matching has a long research history and various kinds of techniques have been proposed [12].

Dynamic programming (DP) is a basic technique to de-velop approximate string matching algorithms. The simple DP algorithm has been improved both from theoretical and practical aspects (e.g., [23, 8, 3]). Some of the improve-ment techniques such as the diagonal transition algorithms [23] assume the unit cost edit distance where the cost of the edit distance is fixed to one. The proposed algorithm is categorized into the DP-based algorithm. However, many of the algorithms developed so far are for the unit cost edit distance, whereas the proposed algorithm can handle more complex similarity functions defined by the DVHMM. Fur-thermore, the proposed algorithm can detect the occurrence of a set of words by scanning the text once. On the other hand, the other algorithms usually handle one word at a time except for multi-pattern approximate search algorithms [12, 1]. The proposed algorithm design is based on the DVHMM, but the idea can be used for the similarities based on finite state machines.

Filtering is another approach for improving the process-ing efficiency. In this approach, the candidate words are efficiently selected from the text, then an elaborate match-ing procedure is applied to the selected words. Usually the filtering algorithms first decompose a word into fragments and apply an exact string matching algorithm that is much more efficient than the approximate string matchings. Q-gram is one of the typical filtering techniques [24, 13]. The metric space search is another technique. In this approach, the word is mapped into a high-dimensional feature space and a range query search is applied to find the candidate words [4].

Recently, compression techniques have been introduced into the approximate string matching[27, 5]. It seems to be effective for our problem based on the statistical model. We plan to study the compression approach in the future.
We evaluate the processing efficiency of the proposed algo-rithm through experiments. In this experiment we intended to apply the proposed algorithm to IR from articles, such as news and abstracts from academic journals in digital li-braries. Usually digital libraries contain old articles that are digitized through document image analysis.

In the experiments, we use a text corpus Reuters-21578 1 which is frequently used for text categorization. This cor-pus contains 21,578 news articles. The total types of words contained in the corpus is 46,213. In order to evaluate the performance of the proposed algorithm for a wide range of recognition error rates, we add noise to the text in the cor-pus artificially, where we apply the operations of insertion, deletion, and substitution with given probabilities randomly. In the experiment, we set the probability of the insertion, deletion and substitution errors to an equal value and varied the probabilities ranging from 1 to 30%.

In order to build the DVHMM, we first determined the state transition graph. In this experiment we examine two kinds of state transitions. One is the graph shown in Figure 1 (a), which is referred to as DVHMM4 in this section. The http://www.daviddlewis.com/resources/testcollections
Figure 6: Processing time wrt number of words second DVHMM consists of six states, where two new states characterized by (1 , 2) and (2 , 1) are added to the states of the DVHMM4 . As a result, the set of states is the state produces the pair of the original and recognized strings whose lengths are n and m , respectively. The state f is the final state. Each state, except for the final state, has transitions to all other states. This model is referred to as DVHMM6 .

For these two models, the parameters are estimated from the training data. We randomly chose 1,000 articles contain-ing 121,613 words from the corpus as the training data. For each sentence  X  in these articles, a corresponding sentence  X  is extracted from the noise-added articles. Then, the pair (  X ,  X  ) is used as the training data.

For the query, we randomly selected fixed numbers of words from the extracted word set. The number varies from 500 to 5000 in this experiment. For a query set of words, we make the following procedures and measure the processing time and required memory space: 1. construct a word tree of the query, 2. for each article of the test data, construct a suffix tree, 3. apply the algorithm described in Figure 5, and 4. enumerate all similar substrings satisfying the condi-
In this experiments we used 0,0001, 0.001, 0.01, 0.1 and 0.5 for the parameter in the condition (Eq. (13)) when enumerating the similar substrings.

The experiments are executed on an Intel Xeon Processor (3.6GHz). First we discuss the processing time of the proposed method. We measured the processing time for various combinations of parameters described in Section 4.1. Figures 6 and 7 show the processing time of the proposed algorithm with respect to the number of words and text string lengths, respectively. In Figure 6 the text length is fixed to 856, which corresponds to a short news story in the corpus and the number of query words varied from 500 to 5000. On the other hand, Figure 7 shows the processing time for a text length ranging from about 800 to 3500 with 500 query words. We compared the proposed method with a naive approximate substring matching algorithm in which for each pair of words in a dictionary and a substring of a fixed length in the text, we applied a dynamic programming algorithm using Eqs. (7) and (8). The DVHMM used in the dynamic programming algorithm is same as DVHMM4 . In both figures, the process-ing time of the proposed algorithm is denoted as DVHMM4 and DVHMM6 , whereas the processing time of the naive algorithm is denoted as Naive DP .

First, let us compare DVHMM4 and Naive DP .Asshown in the figures, the proposed method is about six times as fast as the naive one except for a longer text. As described in the previous section, the proposed algorithm is proportional to the number of nodes of the word tree and the suffix tree, whereas the naive algorithm is proportional to total sum of word and text lengths. This effect comes from the compres-sion of the word tree and the suffix trees. The experimen-tal results show that the compression ratio by both trees is about 1/6. Next, let us compare the DVHMM4 and the DVHMM6 .Sincethe DVHMM6 has more states than the DVHMM4 , the processing time of the DVHMM6 is longer than the DVHMM4 .

Tables 1 (a) and (b) show the required memory spaces in these experiments. In both tables, the first row labeled by  X # cells X  shows the total number of cells of the arrays N probabilities described in the algorithm, whereas the second row shows the real memory sizes used in the calculation. As discussed in Section 3.3, the space for keeping the joint probabilities is almost the same, even if the number of words increases (See Table 1 (a)). However the memory used for the calculation increases because the size of the word tree grows according to the number of words. On the other hand, both the space for the joint probabilities and the suffix tree increases as the text length does (see table 1 (b)). As a result, the growth rate of the memory is much higher than the case that the number of word increases.

The number of words detected by the algorithm varies depending on the value of the parameter in the prob-lem definition given in Section 1. It is clear that the lower threshold realizes the word detection with higher precision space (KB) 8,944 13,440 14,852 17,660 23,656 and lower recall, whereas the lower threshold does the word detection of the opposite characteristics. The number of de-tected words varies depending on the threshold, and con-sequently, the processing time increases as the threshold becomes higher. Figure 8 sho ws the processing time with respect to the value of the threshold. In the figure, the X axis is scaled logarithmically. As shown in the figure, the processing time exponentially decreases as the threshold in-creases. However, the processing time for the parameter 0.0001 is twice as much as for the parameter 0.5. Therefore, the efficiency of the proposed algorithm is not affected by the threshold significantly.

Finally, Figure 9 shows the processing time with respect to the error rate of the text. As shown in the figure, the processing time is not affected by the error rate.
In this paper we proposed an algorithm to enumerate the occurrences of strings similar to the one in the given set of words. The algorithm differs from other algorithms as it uses a flexible string similarity using the DVHMM. The proposed algorithm uses trees for avoiding repetitive simi-larity calculations of the same substrings appearing in the different parts of the text and words. The experimental re-sults showed that the proposed algorithm is about six times faster than a naive DP algorithm.

The proposed algorithm requires the traverse of the suffix tree at each node of words. This step is time consuming for a very long text. We plan to study the application of the technique for diagonal transition algorithms [12] to improve the performance of longer texts. From the application point of view, we plan to apply the proposed method both to OCR processed articles in digital libraries and spoken news articles. In these applications, the proposed algorithm is used to apply the state of the art IR technique to erroneous texts. [1] R. Baeza-Yates and G. Navarro. New and faster filters [2] R. Baeza-Yates and B. Ribeiro-Neto.  X  X odern [3] W. Chang and T. Marr.  X  X heoretical and Empirical [4] E. Ch  X  avez and G. Navarro.  X  X  Metric Index for [5] S. Inenaga, A. Shinohara, and M. Takeda. A fully [6] S. Kahan, T. Pavlidis, and H. S. Baird. On the [7] K. Kukich.  X  X echniques for Automatically Correcting [8] G. Landau and U. Vishkin.  X  X ast Parallel and Serial [9] Y. Li, D. Lopresti, and A. Tomkins.  X  X alidation of [10] J. Mamou and R. H. D. Carmel. Spoken document [11] A. Myka and U. Guntzer.  X  X uzzy Full-Text Searches [12] G. Navarro. A guided tour to approximate string [13] G. Navarro and R. Baeza-Yates. A practical q-gram [14] M. Ohta, A. Takasu, and J. Adachi.  X  X robabilistic [15] E. S. Ristad and P. N. Yianilos. Learning string-edit [16] S. Srinivasan and G. Petkovic. Phonetic confusion [17] K. Taghva, J. Borsack, and A. Condit. Results of [18] K. Taghva, J. Borsack, and A. Condit. Effects of ocr [19] K. Taghva, J. Borsack, and A. Condit. Evaluation of [20] A. Takasu.  X  X ibliographic Attribute Extraction from [21] A. Takasu and K. Aihara.  X  X VHMM: Variable Length [22] Y. Tanaka and H. Torii.  X  X ransmedia Machine and its [23] E. Ukkonen.  X  X lgorithms for Approximate String [24] E. Ukkonen.  X  X pproximate String Matching with [25] G. Valiente.  X  X lgorithms on Trees and Graphs X  . [26] M. Wechsler, E. Munteanu, and P. Schauble. New [27] N. Ziviani, E. S. Moura, G. Navarro, and
