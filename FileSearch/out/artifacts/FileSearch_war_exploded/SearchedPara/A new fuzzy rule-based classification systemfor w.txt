 Department of Computer Science and Engineering, School of Electrical and Computer Engineering, Shiraz University, Shiraz, Iran 1. Introduction
Machine translation (MT) is the process of automatically analyzing a text in a source language and producing a text in a target language. To date, machine translation has met with limited success. Conventional machine translation systems used to adopt rule-based methods, in which grammatical and linguistic restrictions are applied for translation. However, rule-based machine translation systems have many shortcomings. One of the major issues is ambiguity resolution and meaning interpretation. Rule-based systems suffer from inability to select the most suitable equivalent translation in many cases [3].
 Word sense ambiguity can be thought of as the most serious problem in machine translation systems. The human mind is able to select the proper target equivalent of any source language word by compre-hension of the context. A human being may also automatically consider a group of words, rather than just one word, in order to understand the meaning of a sentence, even if the words of the group are not relevant. In order to simulate this behavior in a machine, a huge amount of data will be required as input and the output may still not be free from errors.

There are two other categories of translation methods namely, example-based and statistics-based approaches proposed to overcome the shortcomings of rule-based methods.
In example-based translation methods, a large set of translation samples (i.e., pairs of source text and its translation) are stored and used for similar translations. Example-based methods are mostly used in order to detect and translate expressions.

Statistics-based machine translation was fi rstly proposed by Warren Weaver in 1949. It was then re-introduced in more details by researchers of IBM X  X  Thomas J. Watson Research Center in 1991. Today, this category of machine translation methods is widely-studied and has attracted the attention of many other researchers in the fi eld of machine translation.

In statistics-based Translation methods translations are generated on the basis of statistical or proba-bilistic models whose parameters are extracted from the analysis of a bilingual corpus.

Statistical translation is based on the study of frequencies of various linguistic units, including words, lexemes, morphemes, letters, etc., in a sample corpus in order to calculate a set of probabilities, so that various linguistic problems such as ambiguity can be solved.

Although example-based and statistics-based techniques outperform rule-based methods [3], they still have their own problems. For example, both m ethods require a huge bilingual corpus which is dif fi cult to be collected, stored and processed [2,33]. The other problem is that there is really no ef fi cient algorithm to extract knowledge from this large-scale amount of data, which is required to be used for ambiguity resolution and other related purposes.

WSD can be thought of as the most challenging task in the process of machine translation. Most of the methods already proposed follow a supervised approach for ambiguity resolution. In supervised approach, WSD for a typical multi-meaning word that contains n different senses can be viewed as an n-class classi fi cation problem which wills to determine the correct sens e of the word according to a set of features extracted from the context. Such a classi fi cation system needs a corpus containing the ambiguous word in different contexts to be available. In order to train the classi fi er, the correct meaning of each occurrence of the ambiguous word has to be speci fi ed in the training corpus.

The rest of this paper is organized as follows. Section 2 is devoted to the introduction of the related work in the literature. In Section 3, Fuzzy Rule-based Classi fi cation Systems (FRBCSs) are brie fl y introduced. In Section 3, two different methods of rule-base construction is given. In Section 4, the proposed method of rule-weight learning is discussed. Experimental results are presented in Section 5. Finally, Section 6 concludes the paper. 2. Related work There are a lot of proposed methods for WSD which follow supervised learning techniques, e.g., Na  X   X  ve Bayesian [1], Decision List [8], Nearest Neighbor [40], Transformation Based Learning [26], Winnow [32], Boosting [14], and Na  X   X  ve Bayesian Ensemble [41]. Among the mentioned methods, the method that uses Na  X   X  ve Bayesian Ensemble has been reported to have the best performance for ambiguity resolution tasks with respect to data set used [41]. In order to determine the correct meaning of each ambiguous word, all of the above methods build a classi fi er, using features that represent the context of the ambiguous word.

Brown et al. proposed a corpora-based disambiguation method which can be applied in machine translation systems [30]. They use data from syntactically related words in the local context of the ambiguous word. In order to obtai n statistical data, a word-aligned bilingual corpus is required.
Each occurrence of an ambiguous word should be labeled with a sense by asking a question about the context in which the word appears. The system was tested by translating 100 randomly selected Hansard sentences, each containing 10 words or less in length and obtained the accuracy of 45%.

There are many other WSD methods which require a bilingual corpora to be a vailable. This problem is more challenging for some languages such as Persian, since a suf fi cient bilingual corpus can hardly be found. In [4], Sarrafzadeh et al. try to overcome the lack of knowledge resources and NLP tools for the Persian language. They propose a cross lingual approach to tagging the word senses in Persian texts. The method makes use of English sense disambiguators, the Wikipedia articles in both English and Persian, and a newly developed lexical ontology named FarsNet which has a similar structure to WordNet.
In [10], Yarowsky et al. assumes that each word is located in a major category. In order to disambiguate word senses they have used the Roget X  X  Thesaurus data set. By searching the hundred surrounding words as indicators of each category, the most probable category of a word can be determined. During the training phase, fi rstly, a stemming process is performed over all words in order to achieve more useful statistics. Then, by examining the hundred surrounding words for indicators of each category, the indicator words are obtained and weighted.
 The system proposed in [10] is not limited to particular word categories and works in a wide domain. This system achieves accuracy of between 72% and 99%. The fi rst challenge of the system is that it cannot disambiguate topic-independent distinction words that occur in many topics. Another problem is that it does not consider the distance of words in the contexts it handles.

Another method for word sense disambiguation was proposed in [22] by Dagan et al. The method chooses the most probable sense of a word using frequencies of the related word combinations in a target a source language parser and maps those relations to several possibilities in t he target corpus using a bilingual lexicon. Two evaluations were carried out for this method, one using Hebrew sentences and the other using German sentences. The accuracy of the system was 91% and 78% for Hebrew and German sentences, respectively.

The other method of word sense disambiguation proposed in [23] by Justeson et al., uses syntactically or semantically relevant clues. This method disambiguates adjectives using only nouns that are combined by the adjectives. The system was evaluated on fi ve of the most frequent ambiguous adjectives in English:  X  X ight X ,  X  X ard X ,  X  X ight X ,  X  X ld X , and  X  X hort X  on large sets of randomly selected sentences from the corpus that contained the adjectives and the accuracy of the system reached 97%. However, for adjectives which can be differently accompanied by the same noun, this method cannot be helpful in disambiguation.
The system presented by Ng and Lee [21] is based on the Nearest Neighbor method. The prototypes are the instances of the ambiguous word in the training corpus, each containing the following features: singular/plural; POS tags of the current word; three words on either side; support for verbs, which have a different verbal morphological feature; a verb-object syntactic feature for nouns; and nine local collection features. These features are calculated for each instance of w in the sense-tagged training data. The results are stored as exemplars of their senses. By calculating the same feature vector for the current word and comparing by all the examples of that word, the given word is disambiguated choosing the closest matching instance. The accuracy of the system on test sets from Brown corpus and WSJ corpus was reported to be 58% and 75.2%, respectively. The results were calculated on a task including 121 nouns and 70 verbs, using fi ne-grained sense distinctions from WordNet.

The method presented by Brown et al. [30] requires a bilingual word-aligned corpus, which is costly to build. This is one of the challenges of this method, which makes dif fi cult the applicability of the method to other pairs of languages.

The other method proposed by Mosavi et al. [39] is somewhat the same as the method presented in [22] which uses a target language model. They use Persian as the target language and consider the co-occurrences of the mu ltiple-meaning words in a monolingual corpus of the Persian language. By calculating the frequencies of these words in the corpus, the most probable sense for the multiple-meaning words is chosen. However, instead of considering syntactic tuples in the target language corpus, they consider just co-occurrences of certain words in that corpus without having a syntactic analysis for the corpus. In this method, no analysis is performed either for the source or the target language corpus from the syntactic. viewpoint. The only task of the proposed algorithm, for gaining the required statistical information, is determining the nearest noun, pronoun, adjective, or verb to the ambiguous word, whether it is a noun, a verb, an adjective, or an adverb. When applying this method for the comparison of English and Persian, only a small portion of ambiguous words in English can be correctly translated into Persian.
In addition to supervised approaches, unsupervised approaches and combinations of them have also been proposed for word sense disambiguation. For example [18], proposed an ambiguity resolution technique which divides the occurrences of a word into a number of classes by determining for any two occurrences whether they belong to the same sense or not, which is then used for the full ambiguity resolution task. The approaches proposed by [22,24] are other examples of unsupervised learning methods [25]. Had proposed an unsupervised learning method using the Expectation-Maximization (EM) algorithm for text classi fi cation problems, which then was improved by [20] in order to apply it to the ambiguity resolution problem [11]. Combined both supervised and unsupervised lexical knowledge methods for word sense disambiguation [9,15] used rule-learning and neural networks respectively.
In recent years, some researchers have applied fuzzy logic to provide uncertainty and imprecise reasoning in WSD. In [5], the problem of WSD is formulated as a problem of imprecise associations between words and word senses in a textual contex t. Initially, it cons iders that for each sense, a fuzzy set is given that provides the degrees of association between a number of words and the sense. In the second part, a method based on WordNet constructs the fuzzy sets for the senses.

In [19], the proposed method represents senses using conceptual fuzzy sets. Firstly, atomic conceptual fuzzy sets are generated automatically using word sequences just before the target word and the modi fi ed confabulation model (a prediction method similar to the n-gram model). Then a word is assigned to the appropriate fuzzy set using a method based on co-occurrences. 3. Fuzzy rule-based classi fi cation systems
Fuzzy rule-based systems have recently been used to solve various classi fi cation problems. There are a few methods in the literature in which fuzzy logic h as a role in the process of WSD [5,19]. However, in none of them, a fuzzy rule-based classi fi cation system is constructed. The main reason is that the fuzzy methods need training data sets to contain continuous numeric data.

A Fuzzy Rule-Based Classi fi cation System (FRBCS) is a special case of fuzzy modeling where the output of the system is crisp and discrete. Different approaches used to design these classi fi ers can be grouped into two main categories: descriptive and accurate .Inthe descriptive approach, the emphasis represented by a compact set of short (i.e., with a few number of antecedent c onditions) fuzzy rules. Using linguistic values to specify the antecedent conditions of fu zzy rules makes it a suitable tool for knowledge representation. However, the main objective in designing accurate FRBCSs is to maximize the generalization accuracy of the classi fi er. No attempt is made to impr ove the understandability of the classi fi er in this approach.

Rule-weight has often been used as a simple mechanism to tune a FRBCS. Fuzzy rules of the following form are commonly used in these systems.
 where, X =[ x consequent class, A jk is the fuzzy set associated to x k , CF j is the certainty grade (i.e. rule-weight) of rule R j .

Many heuristic [12,27] and learning methods [28] have already been proposed to specify the weights of fuzzy rules in a FRBCS. The purpose of rule weighting is to use the information from the training data to improve the generalization ability of the classi fi er.

In this paper, we propose a simple and ef fi cient method of constructing accurate fuzzy classi fi cation systems. Using a speci fi ed number of triangular fuzzy sets to partition the domain inter val of each feature, we propose a rule generation scheme that limits the number of generated fuzzy rules to the number of training examples. Each rule uses all features of the problem in the antecedent to specify a fuzzy subspace in feature space. We use rule weight as a simple mechanism to tune the constructed rule-base and propose an ef fi cient rule-weight speci fi cation method.

A FRBCS is composed of three main conceptual components: database, rule-base and reasoning method. The database describes the semantics of fuzzy sets associated to linguistic labels. Each rule in the rule-base speci fi es a subspace of pattern space using the fuzzy sets in the antecedent part of the rule. The reasoning method provides the mechanism to classify a pattern using the information from the rule-base and database. Different rule types have been used for pattern classi fi cation problems [29].
For an n-dimensional problem, suppose that a rule-base consisting of N fuzzy classi fi cation rules of Eq. (1) is available. In order to classify an input pattern X t =[ x t of compatibility of th e pattern with each rule is calculated (i.e., u sing a T-norm t o model the  X  X nd X  connectives in the rule antecedent). In case of using the product as T-norm, the compatibility grade of rule R j with the input pattern X t can be calculated as:
In the case of using single winner reasoning method,the pattern is classi fi ed according to the consequent class of the winner rule R w . With the rules of Eq. (1), the winner rule is speci fi ed using:
Note that the classi fi cation of a pattern not covered by any rule in the rule-base is rejected. The value of  X  ( X t ) . CF in Eq. (2). 4. The proposed scheme
In this section, we illustrate the s tructure of the proposed system developed for WSD. The main parts of the system include feature extraction, rule-base construction and rule-weight learning mechanism, all of which will be described in detail. 4.1. Feature extraction
Feature extraction is a very important step in developing WSD system, which will then have a high effect on the system performance. In this problem, features are the set of words which exist in the context of the ambiguous word that is under investigation. We extract two sets of features from the corpus; the set of words that have occurred frequently in the text and the set of words surrounding the ambiguous word. For this purpose, fi rst of all, we omit all stop-words (i.e., the words which are not quite valuable, such as articles, all types of pronouns, etc.) as well as the punctuations from the context. The processing we perform over the text is not case sensitive, i.e., does not make difference between upper case and lower case letters.

Since we are going to design a fuzzy classi fi er, the extracted features should get just continuous numeric values. This fact has been considered in de fi ning metrics for assignment of features, as will be shown in the following subsections. 4.1.1. Set of frequent words
The fi rst subset of the extracted features includes frequent words. Every word that frequently occurs in the training corpus is represented as a feature in this set. The value of a typical feature is dependent on the number of times it has co-occurred with the ambiguous word (in the same paragraph), ignoring the position of its occurrence in the paragraph.

A word can be selected as a feature of this category, if it meets the following pair of conditions: 1 Every word w i that occurs at least n times in the paragraph that the ambiguous word is in its k th 2 prod ( k | w i ) p
Where p is a prede fi ned value determined through experiments,and the value of prob (k | w i ) is calculated as follows:
Where N ( k, w i ) refers to the number of paragraphs in which the word w i occurs with the ambiguous word that is in its k th sense, and N ( w i ) refers to the total number of paragraphs in which the word w i occurs.

The second condition is checked for the words that satisfy the fi rst condition. The fi rst condition tries to prevent selecting the words based on spurious and rare occurrences. The second condition is used to reduce the probab ility of selecting words that are frequent, b ut co-occur with all senses of the ambiguous word.

After detecting a number of words that satisfy the above pair of conditions, m words that co-occur more frequently with the k th sense of the ambiguous word are selected as features to construct the dataset (If the number of these words for a given sense k exceeds m). Notice that the value of m is determined via experiments. 4.1.2. Set of the surrounding words
Every feature in thi s set is assigned a weight value accordin g to its positional distance to the ambiguous word. For this purpose, we select s words on either sides of the ambiguous word (where s is determined experimentally) and for all of them, check the pair of conditions discussed in the previous section. Among all the words satisfying both conditions, m words that co-occur more frequently with the k th sense of the ambiguous word are selected for the dataset (If the number of these words for a given sense k exceed m).

This set of features do not get binary values (showing whether the word exists or not), instead their positional distance to the ambiguous word will be considered in assignment of their weight. As a heuristic, we assign the value of 1 | i | to a word which has the distance of i to the ambiguous word. Our meaning of distance here is the number of words between two words in a text.
 4.2. Rule-base construction
For an M -class problem in an n -dimensional feature space, assume that m labeled patterns X p = [ x p 1 ,x p 2 ,...,x pn ] , p =1 , 2 ,...,m from M classes are given. A simple approach for generating fuzzy of each input attribute. Some exampl es of this partitioning (using triangular memb ership functions) are shown in Fig. 1.

Given a partitioning of pattern sp ace, one approach is to consider al l possible combi nations of an-tecedents for generating the fuzzy rules. The selection of the consequent class for an antecedent combi-nation (i.e. a fuzzy rule) can be easily expressed in terms of con fi dence of an association rule from the fi eld of data mining [31]. A fuzzy classi fi cation rule can be viewed as an association rule of the form A j  X  class C j ,where, A j is a multi-dimensional fu zzy set represen ting the antecedent conditions and C j is a class label. Con fi dence (denoted by C ) of this fuzzy association rule is de fi ned as [17]: Where,  X  j ( X p ) is the compatibility grade of pattern X p with the antecedent of the rule and m is the number of training patterns. The consequent class C q of an antecedent combination A j is speci fi ed by fi nding the class with maximum con fi dence. This can be expressed as: Note that, when the consequentclass C q can not be uniquely determined, the fuzzy rule is not generated.
The problem with grid partitioning is that for an n -dimensional problem, k n antecedent combinations should be considered. It is impractical to consider such a huge number of antecedent combinations when dealing with high dimensional problems.

In this paper, we use two different solutions to tackle the above problem. We had introduced them in [35,36], as two different methods for ef fi cient rule generation in high dimensional problems. Our goal is to assess the effect of the new rule-weight learning method, proposed in this paper, on both of the systems.
The goal of this method is to propose a solution that enables us to generate fuzzy rules with any number of antecedents, i.e., there would be no restriction on the number of antecedents especially for high dimensional data sets (the problem which originates from the exponential growth of rule-base by increasing the number of features). For this purpose, we consider the well-known evaluation measure, Support measure is presented. of training patterns and h is a class label. After determining a minimum support threshold (denoted by MinSupp ), a set of 1-dimensional rules (containing one antecedent), is generated. This set is then fi ltered by selecting only rules having a support value above the MinSupp . Combining the rules within this set in the next step, results in the set of 2-dimensional candidate rules. The 1-dimensional rules which are pruned through the fi rst step because of their bad supports, can not absolutely lead to 2-dimensional rules with good supports and thus there is no need to consider them. Another key point in combination of a pair of 1-dimensional rules is the conditions under which the rules can be combined: In order to generate rules containing more than two antecedents, a similar procedure is followed. Generating 3-dimensional rules is accomplished using the 1 and 2-dimensional candidate rules. Any possible combination of the rules from these two sets, having the same consequent and not containing common antecedentswould be a 3-dimensional candidate rule. Of course, some principles are considered in order to avoid the time-consuming evaluation of some useless rules (which can not have high support values).

Following the above process, it will also be possible to generate rules having 4 and more antecedents, for any data set having arbitrary number of features.

Although the set of rules is pruned to some extent, in some cases the number of rules is still large. This problem gets more sensible as we increase the number of antecedents. In order to obtain a more concise data set, we divide the set of candidate rules into M distinct groups, according to their consequents ( M is the number of classes). The rules in each group are sorted by an appropriate evaluation factor and the fi nal rule-base is constructed by selecting p rules from each class, i.e., in total, M.p rules are selected. Many evaluation measures have already been proposed [16]. In this work, we use the measure proposed in [2] as the rule selection metric, which evaluates the rule A j  X  class C j through the following equation:
This method of rule generation is illustrated and discussed in more details in [35].
Unlike the fi rst approach, in this method, all of the generated rules are equi-length. Every rule X  X  premise includes all the features of the dataset under investigation. The main idea in this method is to generate a fuzzy rule only if it has at least one training pattern in its decision area. Using fuzzy sets of Fig. 1, to partition each attribute of a pr oblem, a pattern is in the decisi on area of a rul e only if each of that rule.

As an example, for a 2-feature problem with four fuzzy sets on each feature, the decision areas of 16 fuzzy rules are shown in Fig. 2. Note that, although the covering areas of different rules are overlapped, area, the number of rules generated will be at most equal to the number of training patterns. Making use of this key issue, the time complexity of the rule generation process in high dimensional problems is 4.3. The proposed method of rule weighting
For a speci fi c problem, assume that the method of previous section is used to generate a set of N fuzzy rules. Initially, all rules are assumed to have a weight of one (i.e. CF k =1 ,k =1 , 2 ,...,N ) .
In this section we propose an algorithm that assigns a weight in the interval [0,  X  ] to each rule. Our objective is to minimize the following index that represents the leave-one-out misclassi fi cation rate of the classi fi er: Where, the step function is de fi ned as: grades associated to the rules of the same-class and the different-class that are most compatible with pattern x , respectively.
In order to minimize J using gradient descent, we need the derivative of J with respect to weights (i.e., w Replacing the step function in Eq. (8), we have: To simplify Eq. (10), we de fi ne r ( x ) as: Using this, Eq. (10) can be re-written as: To derive the gradient descent update equations, we need to use  X  (  X  ) , which is given as:
 X  (  X  ) is a function whose maximum value occurs in  X  =1 and vanishes for |  X   X  1 | 0 . This function approaches the Dirac delta function for large values of  X  . For small values of  X  , it is approximately constant in a wide range of its domain.

Regarding the mentioned equations, the following gradient descent equation is obtained to be used for updating the rule weights. Similarly, To adjust the weights of the rule s, the algorithm start s by visiting each traini ng example (i.e., x ) and updates the weights of two rules (i.e. the same-class and different-class rules that are most compatible with pattern x ) . The optimization process can terminate after a speci fi ed number of passes over the training data or when no signi fi cant improvement of the performance index J was observed over previous pass. The weight update law can be expressed as: where  X  is a small positive real number that represents the learning factor of the algorithm. The rule-weight learning algorithm is given in Fig. 3.

One of the parameters needed by the algorithm is  X  . It can take just a fi xed value for i or may vary among different rules using some heuristics; for example, it may be proportional to the no. of right-class patterns existing in the decision area of a rule. Moreover, for smoother (but slower) convergence, we had better decrease this factor value along the successive iterations of the algorithm X  X  loop. In general, the it should not have an important in fl uence on the learning results. A generally suitable value for the learn step factor to be used for any data can not be found since it depends on the nature and characteristics of the training data.

The effect of the update equation in the learning algorithm is obvious. The rule-weight learning mechanism is much related to the Reward-And-Punishment approach used in some well-known methods in the literatu re, such as LVQ1, LVQ2 [37,38] and DSM [34] . In an iterative manner, each training pattern, x, rewards its nearest rule that classi fi es it correctly (by increasing the associated weight). Meanwhile, it punishes another rule, having the maximum compatibility to x, which misclassi fi es it (by decreasing the associated weight). During this learning process, the decision areas of the rules covering many compatible patterns and few incompatible ones grow more signi fi cantly. 5. Experimental results
We arranged the following experiments on arti fi cial and real-life data sets to assess the behavior of the proposed scheme and to show the effect of the rule-weight learning method on the generalization accuracy of WSD. 5.1. Arti fi cial data
Before applying the designed classi fi er to word sense disambiguation, in order to visualize the effect of the proposed rule weighting scheme, we constructed an arti fi cial data set for a 2-feature, 2-class problem. We generated 600 data points for each class using normal distribution. The data was generated using variance of 0.3 for both classes, one with mean of (0,0) and the other with mean of (1,1). The initial rule-base was constructed by both methods of rule generation (discussed in Sections 4.2.1 and 4.2.2). Using triangular fuzzy se ts to partition the domain interval of each attribute, we used the weighting algorithm introduced in Section 4 to specify the weights of the generated rules.

The main parameters of the weighting algorithm, which have to be initially adjusted, are the slope of the sigmoid function,  X  , and the learning factor,  X  . For all parts of the experiments, we used  X  =0 . 001 (found experimentally), as the learning factor. Since the selected value for  X  has a signi fi cant effect on For this purpose, the algorithm was repeatedly run by exponentially varying the value of  X  from 0.001 2 to 5 and the average error rate obtained with different numbers of fuzzy sets was computed, for each value of  X  . Figure 4 shows Error rate of the classi fi er for different values of  X  , using both methods of rule generation. As seen in Fig. 4, the classi fi er shows the best behavior when the value of  X  is selected around 100. For the remaining sets of our experiments, we use  X  = 100 for the slope of the sigmoid function.
 The next goal of this experiment is to evaluate the smoothness of the decision boundary of the classi fi er. The optimal decision boundary, which leads to the best generalization ability, is known to be the diagonal line separating the two classes. Through this experiment we want to visualize the effect of the weighting algorithm on smoothening and moving the decision boundary towards the optimal line.

Figure 5 shows the decision boundary of the classi fi er for the case of weighted and non-weighted fuzzy rules. As seen, the decision boundary for the case of weighted rules is much closer to the optimal case and expected to perform better than non-weighted case on test data. 5.2. Real-life data ( benchmark corpora )
In order to evaluate our system for WSD, we focused on 6 nouns, namely palm , bass , crane , plant , motion and tank all of which involve sense ambiguity. There is a benchmark corpus named TWA [42] (developed at University of North Texas by Mihalcea and Yang in 2003) which includes a set of texts including the mentioned words in their different senses. These ambiguous words have higher frequencies compared to other words in the texts and appear in different situations in the training data. Further information about TWA can be found in Table 1.

In order to construct an initial rule-base for a speci fi c data set, each feature was fi rst normalized to interval [0,1]. For rule-base construction, we followed both of the methods given in Section 3, in turn. When applying the second method, we used fi ve triangular fuzzy sets to partition the domain interval of each featur e. To assess the generalizatio n ability of various schemes, we used ten-fold cros s validation scheme [6].

For a given training subset, a rule-base was generated using one the methods of Section 4.2 and the method of Section 4.3 was then used to specify the weight of each fuzzy rule. Five trials of ten-fold cross order to evaluate the effect of th e weighting method on the genera lization ability, t he accuracy of each of the two classi fi ers presented in Section 4.2 was monitored before and after rule-weigh speci fi cation. In Fig. 6, we report on generalization error-rate of the rule-base before and after rule weighting for disambiguation of different words.

In Table 2, we report on ten-fold generalization ability of the system using both methods of rule generation, denoted by Eqs (1) and (2). The proposed scheme is compared with different classi fi ers already proposed for WSD. The competitive results for these classi fi ers are given in Table 2. In all schemes, the rule selection is performed using the single winner reasoning method. 6. Conclusion
In this paper, we introduced an accurate fuzzy rule-based classi fi cation system to be applied for word sense disambiguation (WSD). The proposed scheme aims at optimization of the generalization accuracy and uses rule-weight as a simple mechanism to tune the constructed rule-base. Rule-weight learning in this system is accomplished by solving the minimization problem (minimization of the classi fi cation error) through gradient descent.

The effect of the weighting method was visualized in two ways; fi rst, by showing the relatively smoothness of the classi fi er decision boundary, using an arti fi cial dataset and then by measuring the generalization error rate of the classi fi er on TWA data sets.

The comparison results obtained from computer simulations on TWA datasets were promising. The proposed scheme was compared with some other classi fi ers already proposed for WSD. It outperforms its counterparts in most cases. Moreover, it should be noticed that the proposed method is the only fuzzy classi fi er already proposed for WSD and has the advanta ge of interpre tability be side its accuracy, too. References
