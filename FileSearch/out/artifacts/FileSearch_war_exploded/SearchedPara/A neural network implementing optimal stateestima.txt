 is able to optimally estimate the probability distribution of the hidden world state. implementation.
 quantified within the formalism.
 [3], and serves as a convenient starting point for many possi ble extensions. Consider a dynamic environment characterized at time t by a state X states, namely X tor matrix Q . The matrix Q , [ Q ] The normalization requirement is that P progress according to  X   X  ( t ) =  X  ( t ) Q , where  X  The state X observation point processes { N ( k ) given the current state X differential equation for the posterior probabilities reader to Section 4 for precise mathematical definitions.
 We interpret the rate  X  the k -th sensory cell responds with strength  X  differential equations for p sidering a set of non-normalized  X  X robability functions X   X  Based on the theory presented in Section 4 we obtain where { t k form by defining and  X  = (  X  that the tuning functions  X  at time t is strongest on cell i for which  X  j for which s Several observations are in place regarding (4). (i) The so-lution of (4) provides the optimal posterior state estimato r given the spike train observations, i.e., no approximation is in-volved. (ii) The equations are linear even though the equati ons obeyed by the posterior probabilities p The temporal evolution breaks up neatly into an observation -independent term, which can be conceived of as implementing a Bayesian dynamic prior, and an observation-dependent ter m, which contributes each time a spike occurs. Note that a simi-lar structure was observed recently in [1]. (iv) The observa tion process affects the posterior estimate through two terms. F irst, input processes with strong spiking activity, affect the ac tivity more strongly. Second, the k -th input affects most strongly the components of  X  ( t ) corresponding to states with large values of the tuning curve  X  matrix Q is known. In a more general setting, one can expect Q to be learned on a slower time scale, through interaction with the environment. We leave this as a topic for future work . straightforward to extend the derivation of (4), leading to lating the future posterior distribution p h with the initial condition  X  h (0) = e hQ &gt;  X  (0) , and where  X   X  equations obtained are identical to (4), except that the sys tem parameters are modified. sians (e.g., spatial receptive fields), namely  X  [13] that for small enough  X  x , and a large number of sensory cells, P M plying that  X  = P the inter-arrival dynamics is simply  X   X  ( t ) = ( Q  X   X ) &gt;  X  ( t ) . Defining t spike from any one of the sensors, the solution in the interva l ( t When a new spike arrives from the k -th sensory neuron at time t infinitesimal window of time as we can derive an explicit solution to (4), given by where k ( t conditions  X  (0) at t 3.1 Demonstrations served by a retina with M sensory cells. Each world state s and each sensory cell k generates a Poisson spike train with rate  X  rather well with only 10 sensory cells.
 ( s the general trend is well predicted, even though the estimat ed uncertainty is increased. simply by creating a new state space, s k -th object. network activity in (i) represents Pr X 1 3.2 Behavior Characterization (e.g., the tuning curves  X  q probability vector q of the elements is 1 . The entropy of q assume uniformly placed Gaussian tuning curves,  X  that our model does not require any special constraints on th e tuning curves. mance using the L highest firing activity  X  choice of the L about the world, the less weight need be assigned to the a-pri ori knowledge. and (b) M = 17 , N = 17 ,  X  = 3 ,  X  max = 50 , and for (c) N = 25 ,  X  = 3 ,  X  max = 50 . expenditure. [3]. Consider a finite-state continuous-time Markov proces s X dependent rate functions  X  Consider first a single point process observation N t law for the state and observation process by P (e.g. [6]). More generally, the problem can be phrased as com puting E case of (1), f is a vector function, with components f of E probability spaces ( X  , F , P is said to be absolutely continuous with respect to P P L (  X  ) ,  X   X   X  , such that for all A  X  X  , where E Nykodim derivative of P Consider two continuous-time random processes -X the different probability measures -P conditional expectation E where L sigma-algebra generated by { N t Using (1) and (12) we have defining  X  Based on the above derivation, one can show ([3], chap. 6.4) t hat {  X  ential equation (SDE) A SDE of the form d X  ( t ) = a ( t ) dt + b ( t ) dN jump occurred in the counting process N the jump locations are random,  X  ( t ) is a stochastic process, hence the term SDE. N t , N differential equations for the non-normalized posterior p robabilities is Recalling that N ( k ) is the arrival time of the n -th event in the k -th observation process. (namely, the variables  X  Q in (4) is assumed to be known. Combining approaches to learni ng Q and adapting the tuning curves  X  of experimental verification of the framework is a crucial st ep in future work. [6] A.H. Jazwinsky. Stochastic Processes and Filtering Theory . Academic Press, 1970.
