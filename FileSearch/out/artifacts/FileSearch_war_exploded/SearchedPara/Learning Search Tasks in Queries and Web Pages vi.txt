 As the Internet grows explosively, search engines play a more and more important role for users in effectively accessing online infor-mation. Recently, it has been recognized that a query is often trig-gered by a search task that the user wants to accomplish. Similarly, many web pages are specifically designed to help accomplish a cer-tain task. Therefore, learning hidden tasks behind queries and web pages can help search engines return the most useful web pages to users by task matching. For instance, the search task that triggers query  X  X hinkpad T410 broken X  is to maintain a computer, and it is desirable for a search engine to return the Lenovo troubleshoot-ing page on the top of the list. However, existing search engine technologies mainly focus on topic detection or relevance ranking, which are not able to predict the task that triggers a query and the task a web page can accomplish.

In this paper, we propose to simultaneously classify queries and web pages into the popular search tasks by exploiting their content together with click-through logs. Specifically, we construct a task-oriented heterogeneous graph among queries and web pages. Each pair of objects in the graph are linked together as long as they poten-tially share similar search tasks. A novel graph-based regulariza-tion algorithm is designed for search task prediction by leveraging the graph. Extensive experiments in real search log data demon-strate the effectiveness of our method over state-of-the-art classi-fiers, and the search performance can be significantly improved by using the task prediction results as additional information. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  search process ; I.2.6 [ Artificial Intelligence ]: Learn-ing  X  T his work was done when the first author was visiting Microsoft Research Asia.
 Figure 1: The user is searching for a web page which can effec-t ively accomplish the search task that triggers the query. Algorithms Web search task, classification, graph regularization
Until a few years ago, most search engine technologies focus on how to ideally rank web pages according to the relevance to a given query. Although this has been very successful, merely measuring topic relevance is not precise enough to capture the actual search task that the user wants to accomplish by issuing a query. Taking the situation in Figure 1 as an example, it can be inferred that the user X  X  search task that triggers query  X  X hinkpad T410 broken X  is to  X  X aintain a computer X . The Lenovo troubleshooting page 1 might be the most useful to help accomplish this task, and the user would like to click it to obtain information. However, without search task detection, it is difficult for current search engines to return this web page or similar pages in the top few results of relevance ranking. As shown in Figure 2, Bing search and Yahoo! return some reviews about thinkpad T410, while Google finds some forum discussions and other general information. Although these web pages returned by search engines are very relevant to thinkpad T410, they cannot accomplish the right search task in the users X  mind. In addition, the search task of  X  X aintain a computer X  is actually very popular. According to our analysis of the search log of a commercial search engine, about 14% of the queries related to computers are triggered h ttp://www-307.ibm.com/pc/support/site.wss/document.do? sitestyle=lenovo&amp;lndocid=MIGR-4YRRG6 (a) Search results of Bing (b) Search results of Google
F igure 2: The result pages of query  X  X hinkpad T410 broken X  by this task. Therefore, it is crucial to learn the popular search tasks behind queries and web pages in order to return the most useful web pages to users by task matching.

Given some queries and web pages labeled by several pre-defined popular search tasks, together with large amounts of unlabeled queries and web pages, the purpose of our study is to learn two content-based search task predictors for queries and web pages, respec-tively. A natural idea for learning popular search tasks behind queries and web pages is to cast it as a semi-supervised classifi-cation problem, so as to make use of both labeled and unlabeled data. Current query or web page classification methods usually exploit the content together with click-through information [12] [16]. However, most of the algorithms work on one side of the query-page click graph [12], collecting discriminative information either centering around queries, or around web pages. And gen-eral classifiers including Support Vector Machines, Maximum En-tropy [14], logistic regression, etc., only work with one type of data. Therefore, directly applying existing approaches can either classify queries and web pages separately, or treat queries and web pages as the same type of data and train one unified classifier to handle both of them. These two simple solutions are likely to be suboptimal due to the following challenges: 1. Close interrelationship between data. Queries and web pages 2. Different types of data. Queries and web pages are essen-
In this paper, we propose to predict the search tasks behind queries and web pages simultaneously by designing a novel semi-supervised classification framework which addresses both challenges. Specif-ically, we organize the content and click-through information of both sides of the query-page click relationship into a task-oriented heterogeneous graph among queries and web pages as shown in Figure 3, where each pair of objects are linked together according to task similarity. Through constructing two content-based affin-ity subgraphs among queries and web pages, respectively, we fully exploit the content information of both labeled and unlabeled data. Meanwhile, the semantic gap between queries and web pages is bridged by the click-through subgraph. Then we perform graph-based regularization over the heterogeneous graph to let the inter-mediate task predictions of queries and web pages mutually en-hance each other throughout the learning process. We further incor-porate a linear regression model to directly train two content-based search task predictors for queries and web pages, respectively. By employing our method, we can return web pages to users that are not only relevant, but are also effective in accomplishing the right tasks.

The rest of our paper is structured as follows. We go over the related work in Section 2. Then we formally define the problem of search task classification in Section 3. Section 4 introduces our construction of the graph among queries and web pages. In Section 5, we propose our novel semi-supervised classification framework to train two search task predictors for queries and web pages, re-spectively. Section 6 provides the experimental results on the real data set of click-through logs. Finally, we conclude this work in Section 7. Query classification has received substantial interest in literature. Many existing approaches try to combine the click-through infor-mation centering around queries together with the query content to boost the classification performance. Shen et al. [18] enrich the feature representation of queries by using search snippets and sim-ilar queries discovered from click-through data. Li et al. [12] use content-based classification to regularize the learning process on click graphs. They essentially transform the bipartite click graph into a homogeneous graph among queries for label propagation. Different from these methods, we aim to directly exploit the con-tent and click-through information on both sides of the bipartite click graph, so that the prediction results of queries and web pages can mutually enhance each other.

On the other hand, extensive research has been dedicated to clas-sifying web pages into given topics [16]. Existing methods exploit the textual content [13], hyperlinks [3] and other information of web pages to improve the classification results. Although hyper-links are often useful when grouping web pages according to dif-ferent topics, in our problem of search task classification, however, hyperlinks may be not that helpful, or even be misleading. The rea-son is that web pages linked by hyperlinks are very likely to aim at accomplishing different tasks. For example, web pages for search tasks like  X  X urchase computers X ,  X  X aintain hardware X  and  X  X own-load software X  are all linked with the Lenovo homepage 2 , and hy-perlinks are also built among these web pages for users to jump from one task to another conveniently. Shen et al. [19] leverage the click-through information by drawing implicit links between web pages that are clicked after the same query. Moreover, recent years have witnessed a surge of interest in mining multi-typed web objects concurrently by exploiting their interrelationships, with the h ttp://www.lenovo.com effectiveness widely recognized [22]. Xue et al. [20] design an iterative reinforcement categorization algorithm which initializes the label prediction via a content-based classifier and then prop-agates the category information between web pages and queries through the click graph. Ji et al. [10] perform transductive clas-sification over heterogeneous networked data without content fea-tures. One key distinction of our work is that we unify the content and click-through information into a data-dependent regularization framework. In addition, we aim to directly build two content-based classifiers which not only give prediction for the data already seen in the training phase, but are also defined everywhere in the ambi-ent feature space.

Meanwhile, it is worth noticing that many research efforts are devoted to learning the user goals in web search. Broder et al. [2] divide the web queries into three main categories, namely navi-gational , informational and transactional . Rose et al. [17] build a search goal taxonomy with a similar top level except that the transactional category is replaced by resource . This three-class based web search taxonomy has been widely employed and trig-gered many follow-up studies [11, 9]. Recent works start discov-ering some other goals, or in other words, query intents, which are useful for commercial search engines. Proposed intents in-clude product intent, job intent [12], vertical search intent [8], etc. These approaches make binary predictions that a query has a gen-eral intent or not. Our goal is significantly different from the pre-vious work since we try to directly understand the search task that the user wants to accomplish. Queries and web pages with dif-ferent topics (such as computers or cars) naturally have different search tasks, thus making our task definition at a finer scale than the existing binary or three-class web search taxonomy. Yin et al. [21] build a hierarchical taxonomy of the generic search intents for a class of named entities by analyzing the relationships between queries and grouping them into a tree structure, which is essen-tially data-driven. On the contrary, we focus on classifying queries and web pages into several pre-defined tasks that are of special in-terest among search engine users. To the best of our knowledge, our work is one of the first to study the search task that a web page can accomplish.

Another group of related work is graph-based semi-supervised learning. Most of these methods construct an affinity graph over both labeled and unlabeled examples based on data features to en-code the similarity between instances. They then design a learner which preserves the smoothness and consistency over the intrinsic geometry of the data, which is modeled by the affinity graph [10] [6]. Zhu et al. [24] formulate the problem using a Gaussian ran-dom field model defined with respect to the graph. Zhou et al. [23] propose to let each point iteratively spread its label information to neighbors so as to ensure both local and global consistency. Our algorithm is closely related to manifold regularization [1], which is a framework for data-dependent regularization that exploits the ge-ometry of the probability distribution on the labeled and unlabeled data. However, traditional graph-based learning mainly works on one type of data, and thus cannot distinguish the multi-typed data. In this paper, we extend the manifold regularization framework to study queries and web pages simultaneously.
Our study is about the search task that triggers a query, and the task a web page can accomplish. As discussed in [21], the content in a query can be divided into two parts: named entities 3 terms. In this paper, we define the search task to be the action
H ere we mainly work with named entity queries and related web that the user wants to perform towards the entities. For instance, the entity in query  X  X hinkpad T410 broken X  is  X  X hinkpad T410 X , while the search task can be described as  X  X aintain a computer X  which is inferred from the word  X  X roken X . The search task behind a web page is defined similarly. The Lenovo troubleshooting web page can help accomplish the search task of  X  X aintain a computer X , regardless of the named entities involved such as  X  X hinkpad X  and  X  X enovo X . Queries only containing named entities can be filtered out from the data that we study, since their search tasks cannot be inferred even manually. We use the word  X  X ntity X  and  X  X amed entity X  interchangeably in this paper to refer to the same concept.
Moreover, we focus on classifying queries with a certain cate-gory of named entities and the related web pages. A named entity category is a set of entities that are usually considered to be of the same kind, such as computers, cars, cities, etc. Instead of study-ing all the queries as a whole, it is more appropriate to work on queries with the same category of entities because they are likely to have the same possible tasks, while the search tasks of queries with different categories of entities can vary greatly. For exam-ple, the entity category of  X  X omputers X  can have search tasks like  X  X urchase computers X ,  X  X ownload software X ,  X  X ind reviews X , etc. And the popular tasks of the entity category of  X  X ars X  include  X  X ent a car X ,  X  X urchase a used car X , etc. It is often easy to group enti-ties into categories by using some state-of-the-art methods [15] or through parsing Wikipedia categories/lists.

Now the problem we are going to address can be formally de-fined as follows. The input data contain a set of queries Q = { q 1 , . . . , q |Q| } with entities of the same category, a set of web pages P = { p 1 , . . . , p |P| } clicked by different users after issuing these queries, and a set of search tasks T = { t 1 , . . . , t want to predict. A subset of queries { q 1 , . . . , q n } and a subset of web pages { p 1 , . . . , p m } are labeled by the search tasks, n &lt; |Q| , m &lt; |P| . Given any query q and any web page p , we aim at com-puting a task indicator vector f ( q ) = [ f (1) ( q ) , . . . , f R |T | , and a task indicator vector g ( p ) = [ g (1) ( p ) , . . . , g R |T | . Each f ( t ) ( q ) measures the confidence that query q is trig-gered by task t , and each g ( t ) ( p ) measures the confidence that web page p can accomplish task t , 1  X  t  X  |T | . Then we can predict the most probable task behind each query and each web page by finding the maximum value in f ( q ) and g ( p ) : task ( q ) = arg max
Non-goals. In this paper, we do not study the following prob-lems: 1. How to recognize the named entities in queries or web pages 2. How to define the popular search tasks to be learned. In this
In this section, we try to unify the content and the click-through information of both sides of the query-page click relationship into pages, since named entity queries are the most popular which a c-count for about 71% of all the search queries as reported in [7]. a task-oriented heterogeneous graph among queries and web pa ges, as illustrated in Figure 3. The principle is to link two objects if and only if they are likely to share similar search tasks. The whole graph over all the objects can be divided into three sub-graphs, which are explained in detail in the following subsections.
As discussed in Section 3.1, the search tasks can be inferred from the content of queries and web pages excluding the named entities. Similar to [21], we extract the task phrases from queries as the substring left after removing the terms corresponding to named en-tities. For instance, for query  X  X hinkpad T410 broken X , the named entity is  X  X hinkpad T410 X , and  X  X roken X  is the task phrase. We consider a task phrase representing the same task for entities of the same category, where a category is composed of a set of entities that people usually consider them to be of the same kind. Exam-ples include computers, cars, cities, actors, movies, etc. It is usu-ally easy to obtain entity categories with the help of Wikipedia or by employing algorithms such as [15].

After extracting entities and task phrases from queries, we merge queries with the same task phrases to clusters because they share the same search tasks. For example, queries  X  X hinkpad T410 bro-ken X ,  X  X acBook Pro broken X  and  X  HP Pavilion dv6z broken X  are grouped into a single node represented by task phrase  X   X  broken X  in the graph, where we use  X   X   X  to denote a named entity. As long as the search task of a query is predicted, the task of all the other queries sharing the same task phrase is known. In this way, task classification in queries is equivalent to classifying query task phrases. For convenience, we still use Q = { q 1 , . . . , q resent the set of query task phrases, and let n denote the number of labeled task phrases.

We use the words of the task phrases as the task-oriented con-tent features of the query side, which can be extracted as a term-frequency vector. Task phrases containing similar words are likely to share similar tasks. Then the content-based task similarity be-tween two query clusters represented by task phrases can be com-puted by any distance measurement in the task-oriented feature space. We use cosine similarity here for simplicity. A k-nearest neighbor subgraph G q over queries can be built, with W q the adjacency matrix as follows: where W q,ij denotes the element at the i -th row and j -th column of matrix W q . N k ( q ) denotes the set of k nearest neighbors of query q , and sim ( q i , q j ) denotes the similarity between q sured by the given distance measurement (here cosine similarity).
Similarly, we can extract the task-oriented terms for each web page after removing the terms representing named entities in the content. But the web pages are not clustered as queries are, since the terms of web pages after removing named entities still vary drastically. Then a k-nearest neighbor subgraph G p can be built over web pages according to the similarity between the task-oriented content features. We let W p denote the corresponding adjacency matrix.

Recently, the local consistency idea has received substantial in-terest [1]. It assumes that two nearby data points in the feature space tend to have the same label. In our problem, it is also natu-ral to assume that two nearby web pages or queries in the content feature space have the same task. Following this idea, we construct the two nearest neighbor subgraphs for queries and web pages in order to ensure the local consistency in their feature spaces. In each subgraph, two objects are connected according to task simi-Figure 3: The heterogeneous graph among queries and web p ages. larity measured by the lexical content. However, the disadvantage of using all the words is that the search tasks are likely to be over-whelmed by the entity names. For instance, the named entities like  X  X hinkpad X ,  X  X acBook X  and  X  X P Pavilion X  are not helpful in re-vealing the hidden search tasks. Our method avoids the influence of named entities by simply filtering them out in the content fea-tures.
According to the click-through logs, we assume that a web page p clicked by the user after issuing query q is likely to be useful in accomplishing the search task behind q . Therefore, we build a bipartite subgraph G qp between queries and web pages, where the task similarity between a query and a web page is measured by the click-through relationship. In this subgraph, each edge con-nects a query q and a web page p if and only if p is clicked by a user after issuing q , with the edge weight being the total number of clicks. Similar as before, queries containing the same task phrases are clustered with the click counts added up accordingly. Since G is bipartite, there are no edges between query task phrases or be-tween web pages. G qp is often called the click graph in literature [12]. We let R qp be a |Q|  X  |P| adjacency matrix corresponding to G qp , and R qp,ij denote the element at the i -th row and j -th column of matrix R qp .

It is worth noticing that because queries are often very short, some query task phrases only consist of a single word, such as  X   X  broken X , which will not link to any other task phrase in G ever, by constructing G qp , these task phrases are linked to some web pages so that we can still make inference on them.
As mentioned in the introduction, our ultimate goal is to learn two content-based search task predictors for queries and web pages, respectively. In other words, our final task predictor should be able to correctly classify a query or a web page into one of the |T | search tasks given its content-based feature representation. In this work, we consider a simple linear regression model for predicting the confidence measure of each query and web page having task t , 1  X  t  X  |T | . Let q i denote the d q -dimensional content-based feature vector of a query task phrase q i , as discussed in section 4.1. Similarly, we let p j denote the d p -dimensional task-oriented content feature of a web page p j . Then we have: where w ( t ) q and w ( t ) p are the two weight vectors to be estimated for queries and web pages, respectively. Let u i = [ u (1) i , . . . , u R |T | denote the task indicator vector for a labeled query task phrase q vector for a labeled web page p j . Then it is natural to define:
Now our problem becomes: given { u i } n i =1 , { v i } m m &lt; |P| , and the constructed heterogeneous graph G composed of G
According to the construction of the task-oriented heterogeneous graph in Section 4, we have the assumption that the confidence estimations of each query task phrase q i and web page p j as consistent as possible with the graph structure. And the task prediction on labeled queries and web pages should be similar to their labels. We formulate the consistency assumption as follows: 1. Within each subgraph, the confidence estimations of two ob-2. The confidence estimations of the labeled query task phrases
For the adjacency matrix R qp corresponding to G qp , we further define two diagonal matrices D qp and D pq , whose entries are the row sums and column sums of R qp : Since W q and W p are symmetric matrices whose row sums and column sums are the same, we only need to define one diagonal matrix for each of them:
Then the consistency assumption discussed before leads to min-imizing the following objective function: f or t  X  { 1 , . . . , |T |} . || . || denotes the L2 norm. The first three terms encode the consistency assumption in the three subgraphs G qp , G q and G p among query task phrases and web pages. These terms are normalized by respectively, in order to reduce the impact of popularity of objects. For example, the subgraph in Figure 4 contains five objects de-noted as { o 1 , . . . , o 5 } , each of which is either a query task phrase or a web page. Suppose o 1 , o 2 and o 3 are labeled to have tasks t , t 2 and t 3 , respectively. The edge between o 5 and o 2 while the edge between o 4 and o 2 has the weight of 10 w . How-ever, this does not mean that the confidence of o 4 having task t should be 10 times higher than that of o 5 . In fact, the confidence of o 4 triggered by task t 2 should be the lowest among all the three tasks, since the edges between o 4 and objects having tasks t t weigh more than the edge between o 4 and o 2 does. Similarly, the confidence of o 5 having t 2 should be the highest among the three tasks. Therefore, we normalize the weight of each edge by the sum of the weight on all the edges connected to the two objects at the end of the edge in order to prevent the confidence of popular objects having each task from increasing incorrectly. This normal-ization technique is adopted in traditional graph-based learning and its effectiveness is well proved [23]. The fourth and fifth terms en-sure the consistency between the estimated results and the given labels. Finally, the last two terms are two Tikhonov regularizers imposed on w ( t ) q and w ( t ) p in order to ensure the stableness of the obtained solution [1].

The trade-off among these terms is controlled by the parameters  X  qp ,  X  q ,  X  p ,  X  q ,  X  p ,  X  q and  X  p in the range of (0 , 1] . Note that  X  qp ,  X  q ,  X  p ,  X  q and  X  p encode the relative importance of five dif-ferent types of information, namely the click-through information, the content of queries and web pages, the labels of queries and web pages, respectively. The larger the corresponding parameter, the more value is placed on certain type of information. For instance, if the user believes that the click-through information is more trust-worthy and influential than the content-based features, then  X  be set larger than  X  q and  X  p . However, we will show in Section 6 that the parameter setting will not influence the performance of our algorithm dramatically.

We then generate the normalized form of R qp , W q and W p follows: We further make the following notations: u where I n is the identity matrix of size n  X  n . Note that L L qp are the normalized graph Laplacians [5] of the three subgraphs G , G p and G qp , respectively.

Then with simple algebraic formulations, the first term of objec-tive function (1) can be rewritten as: Following similar derivations, the second and third terms can be rewritten as:  X   X  Then we can rewrite objective function (1) in the following matrix-vector form: We further define Finally, objective function (3) is equivalent to the following:
It is easy to check that L q , L p and L qp , which are the three nor-malized graph Laplacians [5] over the three subgraphs, G q G qp , are positive semi-definite. b L is the weighted summation of L , L p and L qp , which is also positive semi-definite.  X   X   X  and  X   X   X  are diagonal matrices whose entries are all positive, therefore are both positive definite. We then check the Hessian matrix of the objective function (1), which is easy to derive from equation (4): Since b X b L b X T and b X L  X   X   X  b X T L are positive semi-definite and  X  0 , we conclude that H J ( w ( t ) ) is positive definite. Therefore, the objective function (4) is strictly convex. The unique global min-imum is obtained by differentiating (4) with respect to w Finally, we give the closed form solution of w ( t ) = [ w as follows: for t  X  { 1 , . . . , |T |} .
Equations (4) and (6) show that our proposed algorithm has a consistent form with Laplacian Regularized Least Squares (LapRLS) [1], which is a semi-supervised manifold regularization framework on homogeneous data. If we set  X  qp =  X  p =  X  p =  X  p = 0 , our algorithm reduces to LapRLS on queries only. Similarly, letting  X  qp =  X  q =  X  q =  X  q = 0 reduces to LapRLS on web pages. Under the assumption that data reside on or close to an underlying submanifold in the ambient feature space, LapRLS makes use of both labeled and unlabeled examples to learn a regression model whose prediction result is locally consistent along the geodesics of the data manifold. However, as discussed before, queries and web pages essentially have different feature spaces, therefore reside on two different pieces of submanifold in the ambient word space. We thus construct two homogeneous graphs G q and G p to ensure the lo-cal consistency of queries and web pages, respectively. And the bi-partite graph G qp is built to let the task prediction results on queries and web pages mutually enhance each other in the whole learning process.
In this section, we present an empirical study of the effective-ness of our Graph-based Regularization framework for Search Task Classification in queries and web pages simultaneously (denoted by GRSTC) on the click-through data over a continuous period of time from the search log of a commonly used commercial search engine. As discussed before, we try to classify each query and each web page to one of the pre-defined search tasks.
W e use two real click-through data sets of queries containing the entity categories of computers and cars, respectively, from a commonly used commercial search engine. Some statistics of the two data sets are listed in Table 1. We ignore queries involving entities belonging to more than one category.

For the computer category, we collect 780k distinct queries from which 2,268 task phrases are extracted. Among all the web pages clicked by these queries, we select 36,890 web pages which re-ceived totally no less than 5 clicks over all of these queries, since these web pages are the most popular and the most important to study. Then we extract the lexical features from the task phrases, which have totally 3,210 dimensions. And the content-based lexi-cal features for web pages extracted in the same way have totally 8,532 dimensions. From the different numbers of dimensions of the content-based features, we can see that queries and web pages do have very different feature spaces. Many terms contained in web pages never appear in the query task phrases.

For the car category, we obtain 3,308 task phrases covering to-tally 7,600k distinct queries. Although the number of task phrases is similar to the computer category, the number of queries covered is significantly larger. This is because the car category involves many more named entities than the computer category, as observed in the first row of Table 1. Moreover, the large number of queries leads to many more web pages clicked. Then we select totally 33,039 web pages which received no less than 20 clicks from all the queries as our experimental data, while our learned classifier can actually work on all the web pages. Similar as before, we extract the 2,997-dimensional lexical features of the query task phrases, and the 11,926-dimensional lexical features of the web pages.
Since we are trying to predict search tasks of queries and web pages based on the textual content and the click-through informa-tion, the problem can also be cast as a traditional classification task, as discussed above. We compare our proposed method with several state-of-the-art classification approaches as follows: Maximum Entropy (ME) [14] is a supervised content-based classi-fier widely used in web mining and information retrieval. We use the same content-based features extracted from query task phrases and web pages for all the algorithms. Since our algorithm belongs to the category of semi-supervised learning, which has been re-ported to perform generally better than purely supervised methods [4], we tried several supervised classifiers including Support Vec-tor Machines, Regularized Least Squares, ME, etc., and present the best results generated by ME. LapRLS [1] is a semi-supervised manifold regularization framework preserving local consistency in the feature space of the data, which is the homogeneous reduc-tion of our algorithm as discussed in Section 5.2. Here we try two versions of LapRLS: (1) LapRLS-content, the original version running on homogeneous data, where a nearest-neighbor graph is constructed based on local features 4 ; and (2) LapRLS-click, which considers queries and web pages as the the same type of data. How-ever, since queries and web pages have different feature spaces, we can no longer build a feature-based nearest-neighbor graph over all the data. Then we use the click graph to play the role of the affinity graph. The idea of LapRLS-click is similar to [20], with an out-of-sample extension performed by incorporating linear regression.
The original LapRLS algorithm [1] works on homogeneous data, therefore has only one  X  , one  X  and one  X  . And  X  is fixed to be 1 since only the ratio between the three parameters matters in the model selection. In our experiments, we follow this configuration and search  X  and  X  in the grid { 10  X  5 , 2  X  10  X  5 , 5  X  10 10 1 , 2 , 5 , 10 } , where the best results for LapRLS are obtained by  X  = 0 . 5 and  X  = 10  X  4 . It has been reported that the perfor-mance of LapRLS is generally not very sensitive to the parameter setting. In order to have a fair comparison, we treat the three sub-graphs G qp , G q and G p in GRSTC as equally important and use the same set of parameters as LapRLS, i.e.,  X  qp =  X  q =  X  p = 0 . 5 ,  X  =  X  p = 10  X  4 . This may not be the best choice for GRSTC, but it is good enough to show the effectiveness of this method. We also fix  X  q = 1 in GRSTC. But for  X  p , since many web pages can help accomplish more than one search task, therefore strictly labeling a web page to one most relevant search task is not very accurate. In this way, we slightly reduce  X  p and empirically set it to 0.2. The number of nearest neighbors k is empirically set to 15 for LapRLS-content and GRSTC.

Finally, we use the F 1 measure to evaluate the classification per-formance of different algorithms, which is computed as follows:
We try to classify the query task phrases and web pages of the computer category into 7 popular search tasks as listed in Table 2, which are discovered according to manual study of the search log summary.

For performance evaluation, we manually labeled all the 2,268 task phrases and 1,634 web pages with the largest number of clicks. In the following sections, we randomly choose l %(= 5% , 10% , . . . , 70%) of the labeled queries and web pages and use their label infor-mation as prior knowledge. The search task prediction performance is evaluated by comparing with manually labeled results on the rest of the labeled queries and web pages. For each given l % , we aver-age the results over 10 random splits. We show the F 1 measure of
W e also tried transforming the query-page bipartite graph into ho-mogeneous graphs among queries and among web pages to play the role of the nearest-neighbor graphs, which is similar to the idea of [12]. But this implementation did not outperform LapRLS-content in our experiments and therefore we do not present the results.
No. Search task Description Example task phrases 1 Purchase computer Buy a computer  X  a mazon, coupon  X  ,  X  deal 3 Compare Compare two computers on various aspects  X  v ersus  X  ,  X  or  X  queries and web pages with different percentage of labeled da ta of different algorithms in Table 3 and Table 4, respectively.
When classifying queries, LapRLS-content and LapRLS-click perform comparably to each other and better than ME, verifying the effectiveness of learning from both labeled and unlabeled data. When classifying web pages, LapRLS-content still outperforms ME. And it is interesting to note that LapRLS-click performs much bet-ter than LapRLS-content. This is because that the content-based features of web pages are noisier than those of queries, therefore merely relying on content is not very accurate, as reported in many past studies in web page classification [16]. LapRLS-click takes the advantage of learning from labeled queries by exploiting the click-through information, while LapRLS-content can only use the la-bels on web pages even if some labels on queries are also available. However, LapRLS-click fails to consider the local consistency in the content feature space of the data.

Overall, our proposed GRSTC algorithm performs the best in both queries and web pages. Even though GRSTC uses the same set of parameters as its homogeneous reduction, LapRLS, GRSTC still outperforms the two versions of LapRLS by unifying the content and click-through information of both queries and web pages in an organized way. We also performed the two-tailed t-test over the F measure of the experimental results. All the p -values between the results of GRSTC and other algorithms with varying percentage of labeled data are less than 0 . 05 . Therefore, the improvements of our GRSTC algorithm are statistically significant.
The pre-defined search tasks to be classified in the car category are listed in Table 5. Similar as the computer category, these popu-lar search tasks discovered by manual study of the search log sum-mary are of special interest to the search engine users. Notice that several tasks of the car category are very different from those of the computer category, such as search tasks  X  X urchase a used car X ,  X  X ent a car X , etc. That is the reason why we work on queries con-taining named entities of the same category and related web pages.
Similar as before, we manually labeled all the 3,308 task phrases and 1,434 web pages with the largest number of clicks. We then randomly choose l %(= 5% , 10% , . . . , 70%) of the labeled queries and web pages as prior knowledge, and evaluate the performance of search task prediction by comparing with manually labeled results on the rest of the labeled data. The results for each l % are averaged over 10 random splits. The F 1 measure of queries and web pages with different portions of labeled data are shown in Table 6 and Table 7, respectively.
 As can be observed, the semi-supervised LapRLS-content and LapRLS-click still generally outperform the supervised ME method. It is interesting to note that when classifying web pages in this category, the F 1 measure of LapRLS-click drops below that of LapRLS-content, although LapRLS-click uses additional labels of the queries. This indicates that we need to handle the label informa-tion on queries carefully when it is used to study web pages. After all, queries and web pages are two different types of data with dif-ferent semantic meanings, therefore it is inappropriate to treat them equally as LapRLS-click does. Our proposed GRSTC consistently outperforms all the other algorithms by well respecting the seman-tic differences between queries and web pages through making full use of their content information, while letting their classification results mutually enhance each other at the same time.

Finally, we performed the two-tailed t-test over the F 1 measure of the experimental results. All the p -values between the results of GRSTC and other algorithms with different portions of labeled data are less than 0 . 05 , indicating that the improvements of our GRSTC method are statistically significant.
In order to verify the usefulness of search task prediction in search ranking, here we test a simple task-oriented re-ranking scheme by directly incorporating our search task classification results as additional information. This may not be the optimal solution to consider search tasks in ranking, but it is good enough to show the benefit of employing our algorithm.

The basic idea is to use search task prediction as an additional feature to re-evaluate the relevance between the query and the web pages to rank. Given a query q , the search engine can retrieve the top-k relevant web pages { p 1 , . . . , p k } using its original rank-ing function. Then we can use the query classifier trained by our GRSTC algorithm to predict the search task t q behind the query. On the other hand, the web page classifier trained by GRSTC can estimate the confidence that each web page p j could accomplish t
No. Search task Description Example task phrases which is viewed as the task-oriented relevance score (scaled into the range [0 , 1] ). Then the web pages that have high confidence in accomplishing t q are promoted in the ranking list, with a parameter  X  controlling the weight of the task-oriented relevance score. We summarize our task-oriented re-ranking scheme in Algorithm 1.
To test the effectiveness of the task-oriented re-ranking scheme, we design the following experiment. For each entity category, we sample 40% of the labeled query task phrases and web pages and use GRSTC to train two classifiers for queries and web pages, re-spectively. Then we sample 500 of the rest of the task phrases and randomly choose one query containing each task phrase as testing data. For each of the 500 queries, we submit it to the search engine and crawl the top-50 returned web pages. Then we run Algorithm 1 to re-rank these web pages, where  X  is empirically set to 0.1 in our experiments. The web pages clicked by the user are regarded as relevant (ground truth). We measure the ranking performance be-fore and after re-ranking using the Mean Average Precision (MAP) metric. For the totally 1000 queries in the computer category and the car category, the MAP increases by 4 . 87% after employing the task-oriented re-ranking scheme, indicating that taking the search tasks into consideration can improve the search quality. Algorithm 1 . Task-oriented re-ranking for search.
 Input : query q , weight parameter  X  .
 Output : re-ranked web pages { p  X  1 , . . . , p  X  k } .
Procedure : 1. Retrieve k relevant web pages { p 1 , . . . , p k } of q using the 2. Run the classifiers trained by GRSTC to predict the search 3. For each j = 1 , . . . , k , do: 4. Re-rank p 1 , . . . , p k in the descending order of r
F ollowing the manifold regularization framework [1], we fix pa-rameter  X  q which controls the confidence of labels of queries, and let the other parameters vary to perform model selection. There-fore, the rest of the parameters  X  q ,  X  p ,  X  qp ,  X  p ,  X  essential in our GRSTC algorithm which control the relative impor-tance of different terms. We empirically set  X  q =  X  p =  X   X  p = 0 . 2 , and  X  q =  X  p = 10  X  4 in the previous experiments. In this subsection, we try to study the impact of parameters on the performance of GRSTC. Empirically, parameters  X  q and  X  p two Tikhonov regularizers [1] imposed on w q and w p are less im-portant than other parameters,  X  q ,  X  p ,  X  qp ,  X  p , which control the importance of the three subgraphs and the confidence of the labels of web pages. So we mainly evaluate the sensitivity of our model with parameters  X  q ,  X  p ,  X  qp , and  X  p by fixing all the other param-eters and letting one of {  X  q ,  X  p ,  X  qp ,  X  p } vary. We also change  X  and  X  in LapRLS-content and LapRLS-click accordingly. Figure 5 shows the average F 1 measure of queries and web pages in the two categories as a function of the parameters, with 20% of the data la-beled. Parameter sensitivity curves of other percentages of labeled data are similar to Figure 5 and therefore omitted due to lack of space.

It can be observed that over a large range of parameters, GRSTC achieves significantly better performance than all the other algo-rithms, including two versions of its homogeneous reduction, LapRLS-content and LapRLS-click, with the parameters varying the same way. So the parameter selection will not critically affect the perfor-mance of GRSTC.
In this work, we propose to classify queries and web pages into the popular search tasks. One key distinction of our study is that the search tasks are defined as the specific action that the user wants to perform towards the entity in the query, which is at a finer scale than existing binary or three-class taxonomy of user goals or in-tents. We then organize the content and click-through information of both sides of the query-page click relationship into a heteroge-neous graph, where each pair of objects are connected according to task similarity. By designing a novel semi-supervised classification framework based on the task-oriented graph, we not only preserve the local consistency in the content feature spaces of queries and web pages, but also make full use of the close interactions between the two sets of data to let their task predictions mutually enhance each other. Through employing the content-based task classifiers trained by our algorithm, we can predict the search tasks of future queries and web pages so as to return the most useful information to users by task matching.

In the future, we plan to study how to automatically discover what are the popular search tasks among queries instead of man-ual study. One possible solution is to develop some task-oriented clustering methods. Another promising direction is how to better consider search tasks in ranking. Besides directly using the search task prediction results as an additional feature, it is also interesting to design some task-oriented ranking models. The work was supported in part by the U.S. National Science Foundation under grant IIS-09-05215, and by the U.S. Army Re-search Laboratory under Cooperative Agreement No. W911NF-09-2-0053 (NS-CTA). The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Laboratory or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on.
