 Max-margin Markov networks (M 3 N) have shown great pro-mise in structured prediction and relational learning. Due to the KKT conditions, the M 3 N enjoys dual sparsity. How-ever, the existing M 3 N formulation does not enjoy primal sparsity, which is a desirable property for selecting signifi-cant features and reducing the risk of over-fitting. In this pa-per, we present an 1 -norm regularized max-margin Markov network ( 1 -M 3 N), which enjoys dual and primal sparsity si-multaneously. To learn an 1 -M 3 N, we present three meth-ods including projected sub-gradient, cutting-plane, and a novel EM-style algorithm, which is based on an equivalence between 1 -M 3 N and an adaptive M 3 N. We perform exten-sive empirical studies on both synthetic and real data sets. Our experimental results show that: (1) 1 -M 3 N can effec-tively select significant features; (2) 1 -M 3 N can perform as well as the pseudo-primal sparse Laplace M 3 Ninprediction accuracy, while consistently outperforms other competing methods that enjoy either primal or dual sparsity; and (3) the EM-algorithm is more robust than the other two in pre-diction accuracy and time efficiency.
 I.5.1 [ Pattern Recognition ]: Models -Statistical Algorithms, Experimentation -norm Max-margin Markov networks, Primal sparsity, Dual sparsity
In recent years, discriminative learning has expanded its scope to model structured data based on composite features that explicitly exploit the structural dependencies among el-ements in high-dimensional inputs (e.g., text sequences, im-age lattices) and structured interpretational outputs (e.g., part-of-speech tagging, image segmentation). Three major approaches have been successfully explored to perform dis-criminative learning by using graphical models to capture sequential, spatial or relational structure: (1) maximum con-ditional likelihood [16], (2) max-margin [20], and (3) maxi-mum entropy discrimination [27].

Discriminative models, such as conditional random fields (CRFs) [16] and max-margin Markov networks (M 3 N) [20], usually have a complex and high-dimensional feature space, because in principle they can use arbitrary and overlapping features of inputs. Thus it is desirable to pursue a sparse representation of such models that leaves out irrelevant fea-tures. We say a model enjoys the primal sparsity if only a few input features in the original model have non-zero weights (see Section 3.1 for precise definitions). Primal sparsity is an important property for selecting significant features and reducing the risk of over-fitting [12]. Feature selection is also useful to interpret complex data and to reduce memory cost and running time. To achieve primal sparsity, in likelihood-based estimation, a commonly used strategy is to add an 1 penalty to the likelihood function, which can be viewed as a maximum a posteriori (MAP) estimation under a Laplace prior. The sparsity of the 1 -norm regularized maximum (conditional) likelihood estimation is due to a hard thresh-old introduced by the Laplace prior, and weights less than the threshold will be set to exact zeros [14].

Another type of sparsity is the dual sparsity as enjoyed by large margin models, like the unstructured (i.e., the output is univariate) SVM and the structured M 3 N [20] or struc-tural SVM [23, 13]. Dual sparsity refers to a phenomenon that only a few lagrange multipliers in the dual form of the original model are non-zero. A dual sparse model has a robust decision boundary which depends only on a few sup-port vectors. The dual sparsit y also provides a theoretical motivation of the cutting-plane algorithms [23, 13] and the bundle methods [21], which generally explore the fact that in max-margin Markov models only a few (e.g., polynomial) number of constraints are necessary to achieve a sufficiently accurate solution. Although both primal and dual spar-sity can benefit structured prediction models, unfortunately, they do not co-exist in a single existing structured predic-tion model. For example, the M 3 N is only dual sparse, while the 1 -norm regularized CRF [2] is only primal sparse.
In this paper, we introduce the 1 -norm regularized max-margin Markov networks ( 1 -M 3 N), which enjoy the primal and dual sparsity simultaneously. The 1 -M 3 Nisagener-alization of the unstructured 1 -norm SVM [4, 26] to the much broader structured prediction. The primal sparsity of the 1 -M 3 N makes it able to select significant features. To learn an 1 -M 3 N, we present three methods: (1) projected sub-gradient; (2) cutting-plane; and (3) a rather novel EM-stylealgorithmbasedonanequivalencebetween 1 -M 3 N and a novel adaptive M 3 N. The EM-algorithm is similar to the variational learning method of the Laplace max-margin Markov network (LapM 3 N) [27] but essentially differs in up-dating scaling coefficients. Because of a smooth shrinkage effect, LapM 3 Nis pseudo-primal sparse (i.e., only a few in-put features have large weights) and does not select signifi-cant features. Finally, we perform extensive studies on both synthetic and real data sets. Results show that 1 -M 3 Ncan effectively select significant features and can perform as well as the closely related pseudo-primal sparse LapM 3 N, while consistently outperforms competing models that enjoy ei-ther dual or primal sparsity; and the EM-algorithm is more robust than the other two methods.

This paper is structured as follows. The next section presents the preliminaries of max-margin Markov networks. Section 3 presents the primal sparse 1 -M 3 N. Section 4 presents the three learning algorithms for the 1 -M 3 N. Section 5 presents our empirical studies, and Section 6 concludes this paper.
In structured prediction, our goal is to learn a predictive function h : X X  X  from a structured input x  X  X  to a structured output y  X  X  ,where Y = Y 1  X  X  X  X  X Y l represents a space of multivariate and structured outputs. For example, in part-of-speech (PO S) taggi ng, each input x is a word sequence, Y i consists of all the POS tags and each output (label) y =( y 1 ,  X  X  X  ,y l ) is a sequence of POS tags. We assume a finite feasible set of labels Y ( x ) for any x .
Let F ( x , y ; w ) be a parametric discriminant function. In this paper, we concern ourselves with the special case of a linear model, where F is defined by a set of K feature functions f k : X X Y X  R and their weights w k : F ( x , y ; w )= w f ( x , y ). The generalization to the non-linear case can be done similarly as the feature selection in an SVM [11].
By using different loss functions, the parameters w can be estimated by maximizing the conditional likelihood [16] or by maximizing the margin [1, 20, 23]. We focus on the max-margin models and will provide empirical comparison with likelihood-based methods.
The max-margin Markov networks (M 3 N) [20] approach the structured prediction problem by defining a predictive rule as an optimization problem: where the model parameter w is estimated by solving a con-strained optimization problem, with a set of fully labeled training data D = { ( x i , y i ) } N i =1 : where  X  i represents a slack variable absorbing errors in train-ing data, C is a positive constant, and  X  f i ( y )= f ( x f ( x i , y ). w  X  f i ( y ) is the  X  X argin X  favored by the true la-bel y i over a prediction y ,and X  i ( y ) is a loss penalized on y compared with y i , e.g., hamming loss [20]:  X  i ( y )= equals to one if the argument is true and zero otherwise.
The problem P0 can be efficiently solved or approximately solved with a cutting-plane [23], message-passing [20], or gradient-decent [3, 19] method, which generally explores the sparse dependencies among individual labels in y ,asre-flected in the specific design of the feature functions (e.g., based on pair-wise labeling potentials). As described shortly, these algorithms can be directly employed as subroutines in solving our proposed model. The prediction problem (1) can be efficiently solved too by exploring these sparse de-pendencies in y , e.g., using the Viterbi algorithm when the graphical model is a linear chain [16]. In this section, we introduce a primal sparse max-margin Markov network. We begin with a brief overview of three types of sparsity.
Primal Sparsity : We say a model enjoys the primal sparsity, if only a few features in the original model have non-zero weights. The term  X  X rimal X  stems from a conven-tion in the optimization literature, which generally refers to (constrained) problems pertaining to the original model. For example, P0 is the primal form of the max-margin Markov networks. By primal sparsity, we mean that only a few ele-ments of w are non-zero.

As we have stated, primal sparsity is important for se-lecting significant features and reducing the risk of over-fitting. To achieve the primal sparsity, 1 -regularization has been extensively studied in different learning paradigms. For likelihood-based estimation, recent work includes the struc-ture learning of graphical models [17, 24]. For max-margin learning, the unstructured 1-norm SVM [26, 29] has been proposed. Our work represents a generalization of the 1-norm SVM to structured learning, as we shall see.
Dual Sparsity : Dual sparsity refers to a phenomenon that only a few lagrange multipliers in the dual form of the original model turn out to be non-zero . Dual sparsity is an intrinsic property as enjoyed by max-margin models. For instance, for the max-margin Markov networks, the La-grangian of P0 is as follows: where  X  i ( y ) ,  X  i,  X  y  X  X  ( x i ) are nonnegative lagrange mul-tipliers, one for each of the margin constraints in P0, and v are the lagrange multipliers for the constraints  X  i  X  0 ,
Since P0 is a convex program and satisfies the Slater X  X  condition [6], the saddle point of the Lagrangian L is the KKT point of P0. From the stationary condition, we get the optimum solution of P0: From the Complementary Slackness condition, we get: Thus, for those constraints that are inactive, i.e., the equal-ity does not hold, the corresponding lagrange multipliers  X  ( y ) will be zero, and the optimum solution w does not depend on these inactive constraints.

When a model is dual sparse, its decision boundary de-pends on a few number of support vectors, which in principle leads to a robust decision boundary. Moreover, as we have stated, the dual sparsity provides a theoretical motivation of the cutting-plane [23, 13] and bundle [21] methods.
Pseudo-primal Sparsity : Besides the primal and dual sparsity, there is another type of regularization which yields pseudo -primal sparse estimates. We say a model is pseudo-primal sparse, if only a few elements of w have large val-ues. In other words, w can have many non-zero small el-ements. Thus, a pseudo-primal sparse model does not ex-plicitly discard features by setting their weights to exactly zeros. The recently proposed Laplace max-margin Markov network (LapM 3 N) [27] is pseudo-primal sparse because of a smooth shrinkage effect.

Unfortunately, although both the primal and dual sparsity can benefit structured prediction models, they usually do not co-exist. For example, the powerful M 3 N is not primal sparse, because it employs an 2 -norm penalty that cannot automatically select significant features. Below, we present a primal sparse max-margin Markov network to achieve both primal and dual sparsity in one single model.
To introduce the primal sparsity in max-margin Markov networks, we propose to use the 1 -norm instead of the 2 -norm of model parameters. Therefore, we formulate the -M 3 N as follows, which is a generalization of the unstruc-tured 1-norm SVM [26, 4] to the structured learning setting: s . t .  X  i,  X  y  X  X  ( x i ): w  X  f i ( y )  X   X  i ( y )  X   X  where . 1 is the 1 -norm.

Like P0, the problem P1 is a convex program and satis-fies the Slater X  X  condition. Therefore, due to KKT condi-tions, the 1 -M 3 N enjoys the dual sparsity. The difference between the 1 -M 3 N and the standard M 3 N lies in the reg-ularizer they use. Compared to the 2 -norm in M 3 N, which is differentiable everywhere, the 1 -norm in the 1 -M 3 Nis not differentiable at the origin. This singularity property will ensure that the 1 -M 3 N is able to remove many noise features by estimating their weights to exactly zeros. How-ever, the differentiability of 2 -norm makes the M 3 Nhaveall the input features for prediction. When the feature space is high dimensional and has many noise features, the M 3 N will suffer a poor generalization ability caused by these noise fea-tures. Thus, the 1 -M 3 N would be a better choice when the underlying true model is sparse. More insights about the primal sparsity of the 1 -M 3 N will be presented in the next section, along with the algorithm development.
In this section, we introduce three algorithms for learning an 1 -M 3 N and will empirically compare them.
The first method we propose is a cutting-plane method [15], which has been used to learn an M 3 N [23, 13].
Basically, the cutting-plane method aims to find a small sub-set of the constraints in the problem P1 to get a suffi-Algorithm 1 Cutting-plane Method Input: data D = { ( x i , y i ) } N i =1 , constants C and Output: w
Initialize S i  X  X  X  ,  X  1  X  i  X  N. repeat until no change in S . ciently accurate solution. To construct such a sub-set, the algorithm takes an iterative procedure to select informa-tive (e.g., the mostly violated) constraints under the current model. This procedure will terminate when no constraints are selected or the solution is accurate enough. As we have stated, due to the dual sparsity of the max-margin models, only a few constraints are active for the optimum solution. Therefore, the cutting-plane method is a valid strategy. For example, for the M 3 N, as shown in [23], a polynomial num-ber of constraints are sufficient to get a good solution.
Let H i ( y ; w )= X  i ( y )  X  w  X  f i ( y ). The cutting-plane algorithm is outlined in Algorithm 1, where is a precision parameter. Specifically, the algorithm maintains working sets S i foreachtrainingdatatorecordtheselectedcon-straints. The constraints in the sub-set S =  X  N i =1 S i a relaxation of the problem P1. The algorithm proceeds to find the most violated constraint, i.e., the constraint associ-ated with  X  y ,foreachdata x i . If the margin violation of this constraint exceeds the current  X  i by more than , the con-straint is added to the working set S i . Once a new constraint has been added, the algorithm re-computes the solution by solving an LP (linear programming) problem over the work-ing set S . The LP formulation of the problem P1 over the working set S =  X  N i =1 S i is as follows:
Let w =  X   X  v ,where  X  k ,v k  X  0 ,  X  k . For each compo-nent k , we can assume that at least one of  X  k and v k is zero; otherwise, we can minus each of them by the smaller one, without changing w k . Therefore, w 1 =  X  + v ,andtheLP formulation of the problem P1 is as follows: s . t .  X  i,  X  y  X  S i :(  X   X  v )  X  f i ( y )  X   X  i ( y )
This LP problem can be solved with a standard solver, such as MOSEK. In Alg. 1, finding  X  y is a loss-augmented pre-diction problem:  X  y =argmax y  X  X  ( x i ) w f ( x i , y )+ X  which can be efficiently done as the prediction problem (1).
Since the working set S is increased during the iteration, the relaxation of the problem P1 gets tighter and the solu-tion gets more accurate as the algorithm proceeds.
The second learning algorithm we present is a projected sub-gradient method based on an equivalent reformulation of the problem P1.
In the problem P1, each  X  i is associated with a set of con-straints:  X  y  X  X  ( x i ): w  X  f i ( y )  X   X  i ( y )  X   X  i constraints can be equivalently written as one constraint:
Since the above constraint ensures that  X  i  X  0, the prob-lem P1 can be equivalently written as: where the equality of the constraints holds when the convex program gets the optimum; otherwise, we can find a new  X  that yields a smaller objective value while satisfying all the constraints. Putting all the above arguments together, we get the following equivalent formulation of the problem P1: is the structured hinge loss. It is piecewise linear and convex.
The last step is to show that the formulation (2) is equiv-alent to the following constrained optimization problem: This is because for the optimum solution w of the problem (2) with a specific C ,wecanset  X  = w 1 in P1 . Then, w is the optimum solution of P1 ,otherwise,wecanfindanew w that achieves a smaller objective value in the problem (2). Conversely, we can inverse the mapping to find those values of C in (2) that give rise to the same solution as P1 .The problem P0 has a similar formulation as the P1 , but with the constraint replaced by w 2 2  X   X  .
To solve the constrained convex optimization problem P1 , probably the most widely used algorithm is the projected sub-gradient method for convex optimization [5].

Let W denote the convex set of w defined by the con-straint w 1  X   X  ,andlet denote a sub-gradient of the structured hinge loss R hinge ( w ), then projected sub-gradient methods minimize the hinge loss R hinge ( w ) by generating the sequence { w ( t ) } via: where ( t ) is any sub-gradient evaluated at w ( t ) ,  X  t learning rate, and  X  W ( w )=argmin  X   X  X   X   X  w 1 . Here, by sub-gradient, we mean a vector defined as follows:
Definition 1 (Sub-gradient). Let h : W X  R be a convex function. A sub-gradient at w  X  X  is a vector g  X  R K such that  X  w  X  X  ,h ( w )  X  h ( w )+ g ( w  X  w ) .
Finding one sub-gradient of the hinge loss R hinge ( w )is easy. Specifically, for each component: max y  X  X  ( x i ) [ X  w  X  f i ( y )], one sub-gradient 1 is: g i ( w )= f ( x i where y =argmax y  X  X  ( x i ) w f ( x i , y )+ X  i ( y ), which is the loss-augmented prediction under the current model and
This is because max y  X  X  ( x i ) [ X  i ( y )  X  w  X  f last term is a constant. This is a piecewise linear maxi-mization problem, whose sub-gradient is determined by the component that achieves the maximum value [7].
 Algorithm 2 Projected Sub-gradient Method
Input: data D = { ( x i , y i ) } N i =1 , constants  X  , iteration number T
Output: w T for t =1 to T  X  1 do end for Figure 1: Projection of a point to the 2 -ball (solid); and 1 -ball (dashed) in the two-dimensional space. can be efficiently done as we stated in Section 4.1. Therefore, one sub-gradient of R hinge ( w )is: g ( w )= 1 N N i =1 g and can be efficiently computed.

With the sub-gradients computed, we can develop a batch or an online algorithm to learn the 1 -M 3 N, like the sub-gradient methods [19] for M 3 N. Algorithm 2 outlines the batch version, where the projection to an 1 -ball can be per-formed with the efficient projection algorithms in [8].
From the perspective of projection, we can also see the difference between the M 3 Nand 1 -M 3 N. As illustrated in Figure 1, the first orthant (the other orthants are similar) in a two-dimensional space is partitioned into three regions by the dashed lines. For the points inside the balls (either -ball or 2 -ball), no projection is needed. Thus, we only consider the points outside of the balls. For the point p which is far from both axes, the projection to the 2 -ball is the point p 1 , and the projection to the 1 -ball is p 1 .Both p 1 and p 1 do not have a zero coordinate. For the point p whose second coordinate is small (i.e., close to the axis u ), the projection to the 2 -ball is p 2 , while the projection to the 1 -ball is p 2 whose second coordinate is zero. Similarly, the projection of the point p 3 to the 1 -ball is p 3 , whose first coordinate is zero, while the projection to the 2 -ball (i.e., the point p 3 ) does not have a zero coordinate. In general, for the 1 -M 3 N, the projection of a small weight (i.e., close to one axis) to an 1 -ball tends to be zero. In contrast, the projection of a non-zero weight to an 2 -ball is always non-zero. Thus, the existing 2 -norm M 3 N does not explicitly set small weights to zeros and cannot select significant features, while the 1 -M 3 N can select significant features. The third rather novel method we present to learn the 1 -M 3 NisanEM-stylealgorithm,whichisbasedonanequiv-alence between the 1 -M 3 Nandan adaptive M 3 N.
In [10], an equivalence between the adaptive regression and LASSO is presented. Here, we extend the result to the max-margin Markov networks. Basically, we show that the -M 3 Nisequivalenttoan adaptive Max-Margin Markov Network (AdapM 3 N), defined as follows: where  X  = diag (  X  ). Here, by adaptivity, we mean that the values of the scaling factors  X  are automatically adapted during the estimation process.

The rationale behind the adaptive M 3 N is that: by adap-tively penalizing different components, the coefficients of ir-relevant features can be shrunk to zero, i.e., the correspond-ing  X  k go to zero. Because of the constraint 1 K K k =1  X  each estimate is a balance among  X  k . The idea of using adaptive regularization to achieve sparse estimates has been extensively studied in Automatic Relevance Determination (ARD) [18] and sparse Bayesian learning [22].

Now, we show that the adaptive M 3 N produces the same estimate as the 1 -M 3 N and thus gives sparse estimates.
Theorem 2. The AdapM 3 N yields the same estimate as the 1 -M 3 N.
 Proof: See Appendix A.
Based on Theorem 2, we can develop an EM-style algo-rithm to approximately learn an 1 -M 3 N by solving the prob-lem P2. Specifically, the algorithm iteratively solves the fol-lowing two steps until a fixed point (i.e., a local optimum) is obtained:
Step 1 : keep  X  fixed, optimize P2 over ( w , X  ):
This is an M 3 N problem and can be efficiently solved with the sub-gradient [19] or exponentiated gradient [3] methods.
Step 2 : keep ( w , X  )fixed,optimizeP2over  X  . The prob-lem reduces to:
By forming a Lagrangian and doing some algebra, it is easy to show that the solution is:
In practice, to avoid divergent results (i.e., zero diagonal entries in  X ), re-parametrization of the problem P2 can be performed. Specifically, we define new variables:
Then, the EM-style algorithm alternatively updates (  X ,  X  ) and  X  , as outlined in the Algorithm 3 and detailed below:
Update (  X ,  X  ): solve the problem: Algorithm 3 EM-Style Algorithm
Input: data D = { ( x i , y i ) } N i =1 , constants  X  and C ,itera-tion number T Output: w T
Initialize  X  k  X  for t =1 to T  X  1 do end for
Again, this is an M 3 N problem and can be efficiently solved with the methods [19, 3, 13].

Update  X  :
More details about the derivation are in the proof of The-orem 2. Note that the EM-algorithm is similar to the varia-tional learning method of the LapM 3 N [27], which also iter-ates between solving an M 3 N problem and updating adap-tive coefficients. The essential difference lies in the second step. When  X  k =0inEq. (4),or  X  k =0inEq. (6), the feature k will be discarded in the final sparse estimate. But, the update rule in the LapM 3 N ensures that adaptive coefficients are always positive and finite. Therefore, the LapM 3 N does not explicitly discard features. This obser-vation (approximately) explains why the 1 -M 3 Nisprimal sparse, while the LapM 3 N is pseudo-primal sparse. See [28] for more theoretical analysis. In this algorithm, we usually keep C fixed, while tuning the parameter  X  . This section presents some empirical evaluation of the 1 -M 3 N on both synthetic and real data sets. We compare it with the un-regularized conditional random fields (CRFs) [16], the 2 -norm regularized CRFs ( 2 -CRF), the primal sparse 1 -norm regularized CRFs ( 1 -CRF), the dual sparse M 3 N, and the pseudo-primal sparse LapM 3 N. We use the method [2] to learn an 1 -CRF and [19] to learn an M 3 N.
We follow the method as described in [27] to do the ex-periments. We generate sequence data sets, i.e., each input x is a sequence ( x 1 ,  X  X  X  ,x L ), and each component x l d -dimensional vector of input features. The synthetic data are generated from pre-spec ified CRF models with either i.i.d. instantiations of the i nput features or correlated in-stantiations of the input features, from which samples of the structured output y , i.e., a sequence ( y 1 ,  X  X  X  ,y be drawn from the conditional distribution p ( y | x ) defined by the CRF based on a Gibbs sampler.

Due to space limitation, we only report the results on the data sets with correlated input features. We get the same conclusions in the i.i.d case. Specifically, we set d = 100 and 30 input features are relevant to the output. The 30 relevant features are partitioned into 10 groups. For the features in each group, we first draw a real-value from a standard nor-mal distribution and then corrupt the feature with a random Gaussian noise (zero mean and standard variance 0.05) to get 3 correlated features. Then, we generate 10 linear-chain CRFs with 8 binary states (i.e., L =8and Y l = { 0 , 1 } ). The feature functions include: 200 real valued state-feature func-tions, of which each is over a one-dimensional input feature and a class label; and 4 (2  X  2) transition feature functions capturing pairwise label dependencies. Each CRF generates a data set that contains 1000 instances.

We do K -fold cross-validation on each data set and take the average (both accuracy and the variance) over the 10 data sets as the final results. In each run we choose one part to do training and test on the rest K  X  1parts. K is changed from 20, 10, 7, 5, to 4. In other words, we use 50, 100, about 150, 200, and 250 samples during the training. Figure 2 shows the average performance. We can see that the pri-mal sparse models (i.e., 1 -M 3 Nand 1 -CRFs) outperform the M 3 N, which is only dual sparse. Because of a smooth shrinkage effect [27], the LapM 3 N can shrink the weights of irrelevant features to be extremely small by choosing an appropriate regularization constant. In fact, the 1 -M 3 an extreme case of the LapM 3 N when the LapM 3 N X  X  regu-larization constant goes to infinity [28]. Thus, the LapM performs similarly to the primal-sparse models. Obviously, the un-regularized CRFs perform much worse than the other models which use regularization on these sparse data sets. The 2 -CRFs perform comparably with the M 3 N, which also uses the 2 -norm regularizer. This is reasonable because as notedin[9],the 2 -norm regularized maximum likelihood estimation of CRFs has a similar convex dual as that of the M 3 N, and the only difference is the loss they try to optimize, namely, 2 -CRFs minimize the log-loss while M 3 N minimizes the structured hinge loss.

Figure 3 shows the average weights of different models doing 10-fold CV on the first data set and the weights of the CRF model (first plot) that generates this data set. For CRFs, 2 -CRFs, LapM 3 NandM 3 N, all the weights are non-zero, although the weights of LapM 3 N are generally much smaller because of the shrinkage effect [27]. For 1 -M 3 Nand -CRFs, the estimates are sparse. Both of them can discard all the noise features when choosing an appropriate regular-ization constant. As shown in [27], 1 -CRFs are sensitive to the regularization constant. As we shall see the 1 -M with the EM-style algorithm is very robust. Note that all the models have quite different average weights from the model that generates the data. This is because we use a stochastic procedure (i.e., Gibbs sampler) to assign labels to the gen-erated data samples. In fact, if we use the CRF model to predict on its generated data, the error rate is about 0.5. Thus, the learned models, which get higher accuracy, are different from the model that generates the data. The OCR data set is partitioned into 10 subsets for 10-fold CV as in [20, 19]. We randomly select N samples from each fold and put them together to do 10-fold CV. We vary N from 100, 150, 200, to 250, and denote the selected data sets by OCR100, OCR150, OCR200, and OCR250, respectively. Figure 3: The average weights of different models. Figure 4: Evaluation results on OCR data sets with different numbers of selected data.
 On these data sets and the web data as in Section 5.4, our implementation of the cutting-plane method for 1 -M 3 Nis extremely slow. The warm-start simplex method of MOSEK does not help either. For example, if we stop the algorithm with 600 iterations on OCR100, then it will take about 20 hours to finish the 10 fold CV. Even with more than 5 thou-sands of constraints in each training, the performance is still bad (the error rate is about 0.45). The projected sub-gradient and the EM-style algorithm both are efficient. The EM-algorithm yields slightly better performance than the projected sub-gradient, as we shall see.

The results are shown in Figure 4, which are achieved bytheEM-stylealgorithm. Wecanseethatasthenum-ber of training data increases, all the models get lower error rates and smaller variances. Generally, the 1 -M 3 Nperforms comparably with the LapM 3 N, while consistently outper-forms all the other models. M 3 N outperforms the standard, un-regularized, CRFs and the 1 -CRFs. Again, 2 -CRFs perform comparably with M 3 N. This is reasonable due to the understanding of their only difference on the loss func-tions [9] as we have stated.
We have presented three algorithms to learn an 1 -M 3 N, including the projected sub-gradient, cutting-plane, and the EM-style algorithm. We empirically compare them on the synthetic and OCR data sets. We focus on: (1) effectiveness Figure 5: The error rates and training time (CPU-seconds) of different algorithms on the first synthetic data set against the regularization constants. in recovering sparse patterns; (2) prediction accuracy; and (3) time efficiency. For the EM-algorithm, we keep C fixed.
Table 1 shows the numbers of non-zero average weights learned by different algorithms with different regularization constants doing 10-fold CV on the first synthetic data set. The rows indicated by  X  X otal X  show the numbers of non-zero weights for all the 200 state-feature functions and the rows with  X  X rrelevant X  show the numbers of non-zero weights for the 140 state-feature functions based on the 70 irrelevant in-put features. In the EM-algorithm, we set  X  (or  X  )tobezero if it is less than 10  X  4 . We can see that the EM-algorithm has similar numbers (in reverse orders because of different effects of their regularization constants) of non-zero weights as the cutting-plane method. However, the sub-gradient method keeps many features, whose weights are small but not ex-actly zero, and truncating the feature weights with the same threshold as in the EM-algorithm doesn X  X  change the sparse pattern much. Perhaps tuning the learning rate could make this tail of very small features disappear. Note that for dif-ferent algorithms, the regularization parameters are gener-ally incomparable. In these experiments, we select a set of values around the best one we have tried for each algorithm.
Figure 5 shows the error rat es and training time of the three algorithms for the 1 -M 3 N doing 10-fold CV on the first synthetic data set. We can see that both the sub-gradient and cutting-plane methods are sensitive to their regularization constants (see exact values in Table 1). For the sub-gradient method, the time depends largely on the -ball X  X  radius. For larger balls, the projection will be eas-ier. Consider the extreme case that the 1 -ball is big enough to contain the model weights (a point in the space R K ), then the projection is not needed. So, the training time decreases when  X  increases (i.e., the radius of 1 -ball increases). For the cutting-plane method, the time is mainly dependent on the LP solver, e.g., MOSEK as we use. As C gets bigger, the training time increases ve ry fast because more features have non-zero weights (see Table 1). For the EM-algorithm, both the error rate and training time are stable. We use 15 EM-iterations in these experiments and each iteration takes about 3.7 cpu-seconds, smaller than the time of the Figure 6: The error rates and training time (CPU-seconds) of different algorithms on the OCR100 data set against the regularization constants. sub-gradient method. Since the number of features (204 in total) is small, the projection to an 1 -ball can be efficiently done by using the algorithm [8]. Therefore, the overall train-ing time of the projected sub-gradient algorithm is smaller than those of the other two methods. The best performance of all the three algorithms on this data set is comparable.
Figure 6 shows the training t ime and error rates of the sub-gradient and EM-style algorithms on the OCR100. The cutting-plane method is not scalable on this data set, as we stated in Section 5.2. We can see that both algorithms are robust in the performance on this data set. The best per-formance of the EM-algorithm is slightly better than that of the sub-gradient method. This may be due to an improper learning rate in the sub-gradient method. For the training time, the sub-gradient method is again sensitive to its reg-ularization constant, while the EM-algorithm is very stable. As we have stated, the decrease in the training time of the sub-gradient method is due to the fact that projection to a large 1 -ball is easier than projection to a smaller ball.
Web data extraction is a task to identify interested in-formation from web pages, as extensively studied in [25]. Each sample is a data record or an entire web page which is represented as a set of HTML elements. One striking characteristic of web data extraction is that various types of structural dependencies between HTML elements exist, e.g. the HTML tag tree or the Document Object Model (DOM) structure is itself hierarchica l. In [25], hierarchical CRFs are shown to have great promise and achieve better performance than flat models like linear-chain CRFs [16]. One method to construct a hierarchical model is to first use a parser to construct a so called vision tree. Then, based on the vision tree, a hierarchical model can be constructed accordingly to extract the interested attributes. See [25] for an example of the vision tree and the corresponding hierarchical model. In these experiments, we identify four attributes X  Name , Image , Price ,and Description for each product item. We use the data set that is built with web pages generated by 37 different templates [25]. For each template, there are 5 pages for training and 10 for testing. We assume that data records are given, and c ompare different models on the accuracy of extracting attr ibutes in the given records. There are 1585 data records in the 185 training pages and Figure 7: Average F1 and block instance accuracy with different numbers of training data. 3391 data records in the 370 testing pages. We use the two comprehensive evaluation measures, i.e. average F1 and block instance accuracy [25]. Average F1 is the average value of the F1 scores of the four attributes, and block instance accuracy is the percent of data records whose Name , Image , and Price are all correctly identified.

We randomly select m =5 , 10 , 15 , 20 , 30 , 40 , or , 50 percent of the training records as training data, and test on all the testing records. For each m , 10 independent experiments are conducted and the average performance is summarized in Figure 7. We can see that: first, the models (especially the max-margin models, i.e., M 3 N, 1 -M 3 N, and LapM 3 N) with regularization (i.e., 1 -norm, 2 -norm, or the entropic regularization of LapM 3 N) can significantly outperform the un-regularized CRFs. Second, the max-margin models gen-erally outperform the conditional likelihood-based models. Third, the primal sparse 1 -M 3 N performs comparably with the pseudo-primal sparse LapM 3 N, and outperforms all other models, especially when the number of training data is small. Finally, as in the previous experiments on OCR data, the -M 3 N generally outperforms the 1 -CRFs, which suggests the potential promise of the max-margin based 1 -M 3 N.
We have presented the 1 -norm max-margin Markov net-work ( 1 -M 3 N), which enjoys both the primal and dual spar-sity, and introduced three methods to learn an 1 -M 3 N, in-cluding a projected sub-gradient, a cutting-plane and a novel EM-style algorithm that is based on an equivalence between the 1 -M 3 N and an adaptive M 3 N. We conducted extensive empirical studies on both synthetic and real world OCR and web data. Our results show that: (1) the 1 -M 3 N can effec-tively select significant features; (2) the 1 -M 3 N can perform as well as the closely related pseudo-sparse LapM 3 Ninpre-diction accuracy, while consistently outperforms other com-peting models that enjoy either primal or dual sparsity; and (3) the EM-style algorithm is more robust than the other two in both prediction accuracy and time efficiency.
This work was done while J.Z. was a visiting researcher at CMU under a support fro m NSF DB I-0546594 an d DBI-0640543 awarded to E.X.; J.Z. and B.Z. are also supported by Chinese NSF Grant 60621062 and 60605003; National Key Foundati on R&amp;D Projects 2003CB317007, 2004CB318108 and 2007CB311003; and Basic Re search Founda tion of Ts-inghua National Lab for Info Sci &amp; Tech. [1] Y. Altun, I. Tsochantaridis, and T. Hofmann. Hidden [2] G. Andrew and J. Gao. Scalable training of [3] P. Bartlett, M. Collins, B. Taskar, and D. McAllester. [4] K. Bennett and O. Mangasarian. Robust linear [5] D. P. Bertsekas. Nonlinear Programming .Athena [6] S. Boyd and L. Vandenberghe. Convex Optimization . [7] S. Boyd, L. Xiao, and A. Mutapcic. Subgradient [8] J. Duchi, S. Shalev-Shwartz, Y. Singer, and [9] A. Globerson, T. Y. Koo, X. Carreras, and M. Collins. [10] Y. Grandvalet. Least absolute shrinkage is equivalent [11] Y. Grandvalet and S. Canu. Adaptive scaling for [12] T. Hastie, R. Tibshirani, and J. Friedman. The [13] T. Joachims, T. Finley, and C.-N. Yu. Cutting-plane [14] A. Kaban. On Bayesian classification with laplace [15] J. E. Kelley. The cutting-plane method for solving [16] J. Lafferty, A. McCallum, and F. Pereira. Conditional [17] S.-I. Lee, V. Ganapathi, and D. Koller. Efficient [18] R. M. Neal. Bayesian learning of neural networks. In [19] N. D. Ratliff, J. A. Bagnell, and M. A. Zinkevich. [20] B. Taskar, C. Guestrin, and D. Koller. Max-margin [21] C. H. Teo, Q. Le, A. Smola, and S. Vishwanathan. A [22] M. E. Tipping. Sparse bayesian learning and the [23] I. Tsochantaridis, T. Hofmann, T. Joachims, and [24] M. J. Wainwright, P. Ravikumar, and J. Lafferty. [25] J. Zhu, Z. Nie, B. Zhang, and J.-R. Wen. Dynamic [26] J. Zhu, S. Rosset, T. Hastie, and R. Tibshirani. [27] J. Zhu, E. Xing, and B. Zhang. Lapalce maximum [28] J. Zhu and E. P. Xing. On primal and dual sparsity of [29] H. Zou. An improved 1-norm svm for simultaneous Proof: Our proof uses some techniques of the proof in [10]. We do re-parametrization by defining  X  k = 1  X  X  k w k ,and  X  and the problem P2 changes to: The Lagrangian for P3 is as follows:
Taking the derivation of L w.r.t  X  and  X  ,wegetthenor-mal equations:
With the definition of g (  X , X  ), we can get  X  X  (  X , X  )  X  X  of P3, and let (  X   X ,  X   X ,  X  v,  X   X  ) be the dual optimum. Using the optimality condition that  X  X   X  X  =0and  X  X   X  X  = 0 and doing same algebra, we can get: By the Complementary slackness theorem, diag(  X   X  ) X   X  =0. Thus, we have:
The equality constraint k  X  2 k = K implies that:
To get the optimality conditions for the original problem, we let  X  w be the optimum solution of P2. From the definition of  X  and  X  ,weget  X  k, |  X  w k | =  X   X  k |  X   X  k | .Thus,
From the first equation in (7) and using the optimality condition of  X  X   X  X  = 0, we can get: We consider two cases. First, if  X   X  k =0,thenwehave  X  w k = X   X  k = 0. Second, if  X   X  k =0,thenweget:
From Eq. (10) and (11), we get:  X   X  2 k = K |  X  w k | K Therefore, the optimality conditions are: which can easily be shown to have the same form as the op-timality conditions of the following problem:
The last step is to show that the optimum lagrange mul-tipliers in P3 and P4 are the same. Eq. (11) implies that: Substituting this result into the Lagrangian L and using the equality constraint K i =1  X  2 k = K and Complementary slack-ness theorem that  X   X   X   X  = 0, we can get: which is the Lagrangian of the problem P4 evaluated at the optimal solution. Therefore, the optimum dual variables  X   X  are the same for both P3 and P4, and the above optimality conditions are the optimality conditions of P4.
 Similar to the re-formulation of the 1 -M 3 N, the problem P4canbeformulatedastheproblem: which is an 1 -M 3 NproblemasinP1 .
