 We develop and evaluate an approach to causal modeling based on time series data, collectively referred to as X  X rouped graphical Granger modeling methods. X  Graphical Granger modeling uses graphical modeling techniques on time series data and invokes the notion of  X  X ranger causality X  to make assertions on causality among a potentially large number of time series variables through inference on time-lagged ef-fects. The present paper proposes a novel enhancement to the graphical Granger methodology by developing and ap-plying families of regression methods that are sensitive to group information among variables, to leverage the group structure present in the lagged temporal variables according to the time series they belong to. Additionally, we propose a new family of algorithms we call group boosting, as an im-proved component of grouped graphical Granger modeling over the existing regression methods with grouped variable selection in the literature (e.g group Lasso). The introduc-tion of group boosting methods is primarily motivated by the need to deal with non-linearity in the data. We perform empirical evaluation to confirm the advantage of the grouped graphical Granger methods over the standard (non-grouped) methods, as well as that specific to the methods based on group boosting. This advantage is also demonstrated for the real world application of gene regulatory network discovery from time-course microarray data.
 I.2.6 [ Artificial Intelligence ]: Learning Algorithms Temporal Causal modeling, Boosting, Variable Group Selec-tion, Granger Causality, Graphical Modeling
Granger causality [11] is an operational definition of causal-ity well known in econometrics, which states that one time series  X  X auses X  another, if the former contains additional information for predicting the future values of the second series, conditioned on the past values of the first. In [1] we demonstrated that by combining the notion of Granger causality with regression algorithms for variable selection (e.g. Lasso [18]) and applying them to perform graphical modeling over the lagged temporal variables, effective meth-ods for modeling causality involving many time series vari-ables can be obtained. These methods, which are referred to as graphical Granger modeling methods, have received con-siderable attention recently in areas such as computational biology and data mining [7, 8, 15].

Existing algorithms for graphical Granger modeling, how-ever, have not dealt with an important aspect of the prob-lem, which can provide additional structural constraints on the graphical models to be estimated  X  the group structure among the lagged temporal variables naturally imposed by the time series they belong to. This observation suggests the application of variants of regression methods that make use of group information among variables, such as group Lasso [19, 21, 2, 22], into the domain of graphical Granger mod-eling. The present paper aims to quantify the advantage of leveraging group structure among the temporal variables in Granger graphical modeling, as well as to develop novel algo-rithmic extensions in this general class of methods. Specif-ically, we propose a new family of algorithms, group boost-ing and adaptive group boosting, as components of grouped graphical Granger modeling methods. These new methods are motivated primarily by their robustness and flexibility in applicability, for instance when dealing with non-parametric models. We prove that Group Boosting is actually  X  X quiv-alent X  to Group Lasso for the special case of linear models and orthonormal data.

We conduct empirical evaluation to confirm the advantage of the grouped graphical Granger methods over the standard (non-grouped) methods, as well as to quantify the relative performance of various algorithms we consider. Specifically, we conduct a systematic set of experiments using synthetic data, in which we randomly generate a true causal graph, generate time series data from it, and then apply the various alternative algorithms to estimate and infer the true causal structure. We employ precision, recall and related measures in order to quantify the accuracy of our algorithms as that of causal modeling, as opposed to methods of regression or statistical graphical modeling.
The experimental results show that indeed the grouped graphical Granger methods attain significantly higher accu-racy over the corresponding standard (non-grouped) meth-ods. They also demonstrate the advantage of the group boosting based methods, when the data are generated from a non-linear model. Our experiments show that this advan-tage is observed in a real world application, namely that of gene regulatory network discovery from time-course mi-croarray data, confirming the thesis that the new methods are more robust in real world applications, due to the non-linearity of data. Given the relative efficiency of these meth-ods, improved accuracy and robustness, these results suggest that the proposed methodology provides a practically viable approach to performing causal modeling on real world time series data.
We begin by introducing the notion of  X  X ranger Causal-ity X  [11]. This notion was introduced by the Nobel prize winning economist, Clive Granger, and has proven useful as an operational notion of causality in time series analysis in the area of econometrics. It is based on the intuition that a cause should necessarily precede its effect, and in particular that if a time series variable causally affects another, then the past values of the former should be helpful in predicting the future values of the latter, beyond what can be predicted based solely on their own past values.

More specifically, a time series x is said to X  X ranger cause X  another time series y , if the accuracy of regressing for y in terms of past values of y and x is statistically significantly better than that of regressing just with past values of y . Let { x t } T t =1 denote the time series variables for x and { y the same for y . The so-called Granger test first performs the following two regressions: where d is the maximum  X  X ag X  allowed in past observations and then applies an F-test, or some other statistical test, to determine whether or not (1) is more accurate than (2) with a statistically significantly advantage. In this paper, we often use the term  X  X eature X  to mean a time series (e.g. x ) and use temporal variables or lagged variables to refer to the individual variables (e.g. x t ).
The notion of Granger causality, as introduced above, was defined for a pair of time series. We are interested in cases in which there are many time series variables present and we wish to determine the causal relationships between them. For this purpose, we naturally turn to graphical modeling over time series data to determine conditional dependencies
The sensitivity of the result of Granger causality test to the choice of the maximum lag is a topic of active research, but is beyond the scope of the present paper. between the temporal variables, and obtain insight and con-straints on the causal relationships between the time series.
A particularly relevant class of methodology is those that apply regression algorithms with variable selection to de-termine the neighbors of each variable, such as the Lasso. That is, one can view the variable selection process in re-gression for y t in terms of y t  X  1 , . . . , y t  X  d , x z y against x , z , etc. By extending the pairwise Granger test to one involving an arbitrary number of time series, it makes sense to say that x Granger causes y , if x t  X  d is selected for any time lag d in the above variable selection. Where and to the extent that such regression based variable selection coin-cides with the conditional dependence between the variables (cf. [14]), the above operational definition can be interpreted as the key building block of the graphical Granger model. We demonstrated in [1] that the resulting graphical Granger modeling techniques are more effective in modeling causality than alternative canonical methods (e.g. Granger pairwise).
An important point that should be considered in design-ing graphical Granger modeling methods is the following: the question we are interested in is whether there exists any time lag d such that x t  X  d provides additional infor-mation for predicting y t . Emphatically, the question is not, whether for a particular lag d , x t  X  d provides additional infor-mation for predicting y t . That is, as a method of graphical Granger modeling, the relevant variable selection question is not whether an individual lagged variable is to be in-cluded in regression, but whether the lagged variables for a given time series, as a group, are to be included. We thus argue that a more faithful implementation of graphi-cal Granger modeling methods should take into account the group structure imposed by the time series in the variable selection process. This is the motivation for us to use re-gression techniques performing group variable selection. For example, a natural choice is the group Lasso method, which performs variable selection with respect to loss functions that penalize intra-group and inter-group variable inclusion differently. We also propose new algorithms for group vari-able selection based on boosting, Group Boosting and Adap-tive Group Boosting, motivated by the fact that boosting algorithms in general are more efficient and flexible when dealing with non-linear regression models. We note that, with the specific way we incorporate the group boosting procedure in our methodology, the non-linearity being dealt with is within the respective groups (times series). That is we consider models of the type y t  X  f ( y t  X  1 , . . . , y g ( x t  X  1 , . . . , x t  X  d ) + h ( z t  X  1 , . . . , z may be non-parametric functions (e.g. trees).

The foregoing argument leads to the generic procedure of grouped graphical Granger modeling method, exhibited in Figure 1. Note that we let x j denote a feature (or time series) for any j , and x j t the lagged variable at time t for x We now describe possible instances of the REG procedure.
Here and in subsequent discussions, we state the instances of the REG procedure as generic regression methods (i.e., Grouped Graphical Granger Modeling 1. Input: Time series data { X t } t =1 ,..,T where each X 2. Initialize the adjacency matrix for the p features, i.e. 3. For each feature y  X  V , run REG on regressing for Figure 1: Generic Group Graphical Granger Mod-eling Method we temporarily abandon the time series framework, and switch to the standard regression notation). The grouped graphical Granger modeling method is to use these regres-sion methods as the sub-procedure REG , in the obvious manner. That is, a feature is included in the model if the corresponding variable group is selected by REG .
Let us now consider linear regression models. Let Y = ( Y 1 , . . . , Y n ) T be a n  X  1 response vector and X = [ X a n  X  p predictor matrix, where X i = ( X i, 1 , . . . , X 1 , . . . n. Typically the pairs ( X i , Y i ) are assumed to be in-dependently identically distributed (i.i.d.) but most results can be generalized to stationary processes given a reason-able decay rate of dependencies (e.g. certain conditions on the mixing rates).

We are interested in selecting the most important pre-dictors. Hence the ordinary least squares estimate (OLS) would be inappropriate, and procedures performing coeffi-cient shrinkage and variable selection are desirable. We note that this consideration was overlooked in the original formu-lation of Granger causality. The use of regularized REG pro-cedure, in the graphical Granger modeling framework, has the advantage of taking into account the additional model complexity resulting from the inclusion of temporal variables of a feature in addition to those of the target feature. A pop-ular method for variable selection is the Lasso [18], which is defined as: where  X  is a penalty parameter. Here the l 1 norm penalty induces sparsity, that is  X   X  j (  X  ) = 0 for some j 0 s .
It is well known that the Lasso tends to over-select vari-ables. To address this issue, Zou [20] proposed the Adaptive Lasso, a two-stage procedure solving where  X   X  init is an initial root-n consistent estimator, e.g. that obtained by OLS. The division by |  X   X  init ,j | in the penalty term has the effect of enforcing further sparsity, by discour-aging the inclusion of those variables with small estimated coefficients in the first stage. In particular, notice that if  X   X  ables selected in the second stage will always be a subset of those from the first stage.

Recently, a grouped version of the Lasso algorithm has been developed, motivated by the fact that, in many situ-ations, natural groupings exist between variables, and vari-ables belonging to the same group should be either selected as a whole or not. Here, we apply this idea to the nat-ural grouping structure arising in time series data, where the lagged variables from the same time series belong to the same group. In the following subsections we describe vari-ous procedures for group variable selection, beginning with Group Lasso [19], followed by the novel methods: Group Boosting and Adaptive Group Boosting.
Given J groups of variables which partition the set of predictors, the group lasso estimate of Yuan and Lin [19] solves where  X  G j = vect(  X  k ; k  X  G j ) and G j denotes the set of group indices. When compared to Lasso, note that the l 1 penalty here is not with respect to the amplitude of the individual regression coefficients, but rather with respect to the amplitude of groups of coefficients (i.e the l 2 norm of each group of coefficients).
We now proceed to describe the boosting based algorithms for group variable selection that we propose.

Boosting methods in general have been proven to have excellent performance when dealing with high-dimensional data, and variants of boosting have been proposed recently to improve performance in terms of variable selection. For instance L 2 -boosting with componentwise linear least squares [4] picks, at each iteration, one single variable and a correspond-ing regression coefficient, and is shown to approximate the Lasso solution when the design matrix X is orthonormal. Sparse Boosting [3] is also based on squared loss, but re-sults in even sparser solutions by minimizing at each itera-tion a penalized version of the residual least squares. In the spirit of Adaptive Lasso, B  X  uhlmann [5] introduced the Twin Boosting algorithm, a two stage procedure. Consistency of L -boosting with componentwise linear least squares was proven for high-dimensional linear models in [6]. Experi-ments in [6] further illustrates that, though lasso and boost-ing are related, they are not identical. Group Boosting (General Weak Learner) 1. Initialization:  X  F 0  X   X  Y = 1 n 2. For m = 1 , . . . , M Figure 2: Group boosting for general weak learners.
We extend this work and propose here boosting meth-ods that perform group variable selection, which we call, respectively, Group Boosting and Adaptive Group Boost-ing. These are the  X  X roup X  versions of L 2 -Boost and Twin Boosting, respectively. We present in Figure 2 the Group Boosting algorithm with a general weak learner, and l 2 loss (generalization to other loss functions is straightforward). We then present the counterpart to adaptive group lasso, which we call Adaptive Group Boosting, given in Figure 3. Here note that  X  denotes the set of groups of variables. For a group G  X   X , X G is restriction of the matrix X to the columns corresponding to the variables in G , and X G ,i denotes the i th row of matrix X G . M G is the set of base models used by boosting involving the variables in G only. Possible choices for M G include regression trees where the node variables involve those in G only and linear models X G  X  G where  X  G is a vector of length |G| .

The Adaptive Group Boosting algorithm of Figure 3 ex-tends the Twin Boosting algorithm of [5] to handle groups of variables. The algorithm has two stages. The first stage consists in applying the Group Boosting algorithm of Fig-ure 2. The second stage is similar to the first stage, but leverages the model F init , output at the first stage, in its search for the best group (see Eq. (4)). The rationale for doing so is as follows. Eq.(4) comes from the fact that arg min f k U  X  f k 2 = arg max f ( 2U T f  X  X  f k 2 ) , combined with the fact that we wish to add a scaling factor accounting for the fitted function from the first round. The scaling W G can be seen as the normalized projection of the fitted func-tion F init against the candidate base model. This setup is adapted from [5](eq.(8)). Note that such scaling scheme bears some similarity with the Adaptive Lasso procedure described at the beginning of Section 3.2, where an initial estimator is also leveraged in a second stage.

The advantage of our group boosting methods lies not only in their computational attractiveness, but also in the fact that, contrary to Lasso-based methods, they are not limited to (generalized) linear models, and can deal with general non-parametric models and data of mixed types (e.g., con-tinuous, categorical or ordinal) in the component models for the respective groups. Such advantage may be critical in many practical situations, as we indeed verify in the exper-iments section, where the true model may be non-linear. Adaptive Group Boosting (General Weak Learners) First stage 1. Initialization:  X  F 0  X   X  Y = 1 n 2. For m = 1 , . . . , m 1 Second stage 1. Initialization:  X  F 0  X   X  Y = 1 n 2. For m = 1 , . . . , m 2 Figure 3: Adaptive Group boosting for general weak learners. In this section, we show that Group Boosting and Group Lasso are  X  X quivalent X  for the special case of linear models with orthonormal design matrices. Note, however, that both procedures are generally distinct.

Consider the orthonormal linear model with n variables, i.e., Y = X  X   X  +  X , where X  X  R n  X  n , X T X = XX T = I. As-ables with finite variance  X  2 . Consider J groups G 1 , . . . , G Under this setting, the following proposition characterizes the Group Lasso solution.

Proposition 1. For an orthonormal linear model, the solution of the group lasso problem of Eq.3,  X   X  =  X   X  group is given by
Proof. Let  X   X  =  X   X  group lasso problem. Proposition 1 in [19] states that  X   X  satisfies the following necessary and sufficient conditions We have
In addition the following holds. where the last equivalence follows by noticing that k  X   X   X  = k X T G proposition.

The following proposition establishes the X  X quivalence X  X e-tween Group Boosting with linear base models and Group Lasso for orthonormal linear models, by showing that under the same condition of orthonormality the Group Boosting solution path approximately coincides with that given by Proposition 1, up to a difference in the choice of  X  .
Proposition 2. Let  X   X  GroupBoost ( m,  X  ) be the estimate ob-tained by running group boosting with linear base models, step size  X  , and up to iteration m. For any sequence (  X  for each k , there is a corresponding iteration m such that for each group G j
Proof. We use the same arguments as in the proof of [3](Theorem 2) extending them so that they apply to groups rather than single variables. For linear base models, the base procedure estimation for a given group G j can be represented as  X  residuals and  X  U the fitted values. Then one can show by a similar reasoning as in [3] that the corresponding group-boosting operator at iteration m is
B m = I  X  ( I  X   X  H G 1 ) m 1 ( I  X   X  H G 2 ) m 2  X  X  X  ( I  X   X  H where m j is the number of times the group G j is selected, and therefore m =  X  Y = B m Y. B m can be diagonalized so that B m = X  X  m X T with where D m j = (1  X  (1  X   X  ) m j ) I ( |G squares at iteration m is given by RSS m = k Y  X  X  m Y k 2 k ( I  X   X  m ) X T Y k 2 , (since X is orthonomal) and thus de-creases monotonically in m . In addition RSS m  X  RSS m +1 decreases monotonically in m as well, since at each round the group that decreases the most the residual sum of squares is selected, and due to the fact that ( I  X   X  m ) G i = (1  X   X  ) tolerance  X  2 such that RSS k  X  RSS k +1 &gt;  X  2 ,  X  k &lt; m and RSS m  X  RSS m +1  X   X  2 . Let Z i = ( X T Y ) i . The remainder of the proof follows from that of [3](Theorem 2) by sub-stituting Z i ,  X  i , | Z i | with Z G j ,  X  G j , k Z G i showing that the Group Boosting solution satisfies the con-dition of Proposition 1, for  X  =  X  k (1 +  X  j )).
We now discuss the parameter tuning of the various REG procedures. For Lasso-based methods, this involves select-ing the penalty parameter  X . In practice,  X  can be chosen by cross-validation or using a model selection criterion. For instance the BIC criterion (see for instance [18]) is to min-imize over  X  where df M is the degrees of freedom [10]. For Lasso we use the following approximation. where I denotes the indicator function and p is the number of features. For Group Lasso we use the estimate proposed by [19]: df where  X   X  =  X   X  GroupLasso (  X  ) , p j is the size of group G  X   X 
OLS is the ordinary least squares solution when using all the variables.

For boosting-based variable selection procedures, param-eter tuning consists in determining where to stop the algo-rithm, e.g how to select m, m 1 , m 2 in Figures 2 and 3. For the boosting methods with non linear base learning meth-ods, the stopping points are evaluated by cross-validation. For linear weak learners, we can use a more computation-ally efficient criterion, such as the BIC criterion of Eq. 7. In order to do so, we need to evaluate the degrees of freedom of group boosting.

Let  X  G be the selected group at a given round. For linear weak learners, the base procedure estimation at the observed predictors X i can be written as Here U denotes the current residuals and  X  U the fitted values.
Then one can show by a similar reasoning as in [4] that the group-boosting procedure up to iteration m can be written as  X 
Y = B m Y, where B m = I  X  ( I  X   X  H  X  G and where  X  G j denote the group selected at round j.
Ignoring the nonlinearity introduced by the data-driven selection of the groups  X  G k , we can use as our estimate for the degrees of freedom. Then we pick the stopping point by minimizing
We first conducted systematic experimentation using syn-thetic data in order to test the performance of the pro-posed methods of group graphical Granger modeling, pri-marily against that of the non-grouped variants. The exper-iments we ran can be categorized into two types, depending on whether the data are generated using a linear model, or a non-linear model.
 As models for linear data generation, we employed the Vector Auto-Regressive (VAR) models (cf. [9]). The VAR model is a special class of time series graphical models with linear Gaussian models used as the component models of conditional densities. Specifically, if we let X t denote the vector of all features at time t , a VAR model is defined as below.
 Here the A matrices are coefficient matrices over the fea-tures.

In each of our experiments, we randomly generate an ad-jacency matrix over the features that determines the struc-ture of the true VAR model, and then randomly assign the coefficients (the A l matrices, l = 1 , . . . , d ) to the edges in the graph, for each lag l . We then apply the obtained model on a random initial vector to generate time series data { X t } t =1 ,...,T of a specified length T . It is worth em-phasizing that the adjacency matrix is over the features, corresponding to the idea that the true causal structure is at the level of the (time persistent) features, rather than the lagged variables.

There are a number of parameters that control this process of data generation (c.f. [1]), some of which are listed below, along with their settings in our experiments. The affinity , which is the probability that each edge is included in the graph, was set at 0.2; The sample size per feature per lag which is the total data size per feature per maximum lag allowed, was set at 10. We sampled the coefficients of the VAR model according to a normal distribution with mean 0 and standard deviation 0.25, the noise standard deviation was set at 0.1, and so was the standard deviation of the initial distribution.

As non-linear models for data generation, we employed the so-called additive non-linear Vector Auto-Regressive (VAR) models. In particular, we used sums of polynomials of de-gree 3 in our experiments (e.g. y t = a ( x 1 t  X  1 ) 2 + b ( x d ( x 3 t  X  1 ) +  X  X  X  ) and sums of sines and cosines (e.g. y of random generation, as well as control and fixed parame-ters, was used in the non-linear case as in the linear case. For convenience we refer to the aforementioned models as  X  X olynomial VAR X  and  X  X inusoidal VAR X .

We evaluate the performance of all methods using the so-called F1 measure, viewing the causal modeling problem as the classification problem of predicting the inclusion of the edges in the true graph (or the corresponding adjacency ma-trix). Recall that, given precision P and recall R , the F measure is defined as follows, and hence strikes a balance in the trade-off between the two measures.

We now present and discuss the results of our experimen-tation. Table 1 summarizes the results of our experiments on linear data. In the table, the average F 1 values over 18 runs are shown, along with the standard error. These results clearly indicate that there is a significant gap in performance between the proposed methods based on group variable se-lection methods and their X  X on-grouped X  X ounter-parts. One can also see that for these linear data, the method based on Adaptive Group Boosting is competitive with that based on Group Lasso.

We exhibit in Figure 4 some sample output graphs along with the true graph. In this particular instance, it is rather striking how the non-grouped methods tend to over-select, whereas the grouped methods manage to obtain nearly per-fect graphs.

Next we focus exclusively on the group-based methods and study the impact of dealing with non-linear data. Specifi-cally, we compare the performance of grouped methods us-ing three different REG procedures: (1) Group Lasso, (2) Adaptive Group Boosting with linear weak learner, which we refer to as GrpBoost-L , and (3) Adaptive Group boosting with generalized additive models (third order polynomial) as weak learner, which we refer to as GrpBoost-NL .

Table 2 exhibits the results of this comparison, namely, the average F 1 scores over 20 runs along with the corresponding standard error on synthetic data generated from X  X olynomial VAR X  and  X  X inusoidal VAR X  models.

For the simulations with  X  X olynomial VAR X , the three group-based methods perform similarly, with the method based on GrpBoost-NL having a slight edge. This is not surprising, since in this case, assuming linear models is a fairly good first order approximation of the true model, i.e., a polynomial VAR of degree 3. So using either Group Lasso or GrpBoost-L as REG procedure is a reasonable choice.
In the case of X  X inusoidal VAR, X  X owever, the performance of linear group-based methods (i.e., Group Lasso and GrpBoost-L) degrades considerably, while that of GrpBoost-NL (i.e., Group Boosting with polynomial additive models) is less affected, hence improving its relative performance signifi-cantly. This is to be expected, since in this case, assuming a linear model is not appropriate, while polynomial models are able to better approximate the sinusoidal VAR.
The set of experiments on non-linear data we just de-scribed illustrates the advantage and flexibility of group boost-ing based methods over group lasso based methods. One might argue that, in certain cases, an alternative solution to using GrpBoost-NL consists in (1) expanding the time series data, for instance using polynomial expansions such as x t ( x t ) 2 , ( x t ) 3 , etc.; (2) feeding the expanded dataset to the Group Graphical Granger modeling method, using Group Lasso or GrpBoost-L as REG and imposing that the time points belonging to a given time series and its various ex-pansions belong to the same group. We emphasize, however, that in real datasets one does not have prior knowledge of the underlying structure, and it is most likely impractical to include in advance all factors that capture the non-linearity in the data as features, with respect to which the data be-come linear. Hence, to some extent, the performance of  X  X inear X  methods over the original features mirrors the an-ticipated performance of the linear methods in a typical real world scenario. The boosting approach is more flexible, as it affords the freedom to consider arbitrary weak learners, pos-sibly involving non-parametric models (e.g. regression trees), allowing us to handle non-linearity in a more automated and efficient manner.
We also applied our methodology to the problem of gene regulatory network discovery from micro-array data. We used a real time course micro-array dataset [16], which con-sists of gene expression measurements for a human cancer cell, taken at 48 time points for a subset of 9 genes. We com-pare the discovered interactions to those previously reported in the BioGRID database (www.thebiogrid.org). For more details on the data and experimental set up, see [13]. It is important to note that the list of interactions reported in the database is far from being exhaustive, so caution should be exercised when interpreting the results of such comparison. Our results are presented in Table 3. Note that the variable selection accuracy of our methods is higher than that reported by previous work on the same data set ( F 1 = 0 . 40 in [16]). In addition, for our methods, using GrpBoost-NL as subprocedure REG leads to significantly higher accuracy than using group lasso, which is consistent with the non-linearity of gene interaction processes. The causal graphs output by our methods are presented in Fig-ure 5 along with the reference graph constructed using the BioGRID database.
We have proposed a host of novel methods for temporal causal modeling, collectively referred to as grouped graph-ical Granger modeling methods. This work opens up a new direction of research with many challenges, including in particular the development of concrete, improved algo-rithms within the general methodology, as we initiated in the present paper with the introduction of the Group Boost-ing algorithm. Some of the other interesting future research agenda include: Exploring interactions with other approaches to structural learning in Bayesian and causal networks (e.g. PC [17]); Improving the theoretical guarantee on our algo-rithms; and further validation of the proposed approach on a variety of real world applications.
 We thank Andrew Arnold for his help on some of the exper-iments. [1] Arnold, A., Liu, Y. and Abe, N. (2007), Temporal [2] Bach, F. (2008). Consistency of the group Lasso and [3] B  X  uhlmann, P. and Yu, B. (2006). Sparse Boosing [4] B  X  uhlmann, P. and Yu, B. (2003). Boosting with the [5] B  X  uhlmann, P. (2007). Twin Boosting: improved [6] B  X  uhlmann, P. (2006). Boosting for high-dimensional [7] Dahlhaus, R. and Eichler, M. (2003), Causality and [8] Eichler, M. (2007), Granger-causality and path [9] Enders, W. (2003). Applied Econometric Time Series, [10] Hastie, T., Tibshirani R., Friedman JH (2001). The [11] Granger, C. (1980). Testing for causlity: A personal [12] Hurvich, C., Simonoff, J. and Tsai, C.-L. (1998). [13] Lozano, A., Abe, N., Liu, Y, Rosset S. (2009). [14] Meinshausen, N. and B  X  uhlmann, P. (2006). [15] Mukhopadhyay, N. D. and Chatterjee, S. (2007), [16] Sambo F., Di Camillo B, Toffolo G. (2008) CNET: an [17] Spirtes P., Glymour C., and Scheines, R. Causation, [18] Tibshirani, R. (1996). Regression shrinkage and [19] Yuan, M. and Lin, Y. (2006) Model selection and [20] Zou, H. (2006) The Adaptive Lasso and its Oracle [21] Zhao, P, Rocha, G. and Yu, B. (2006). Grouped and [22] Zhou, N. and Zhu, J. (2007) Group variable selection Granger methods on synthetic linear VAR data.

