 This paper introduces a nonlinear logistic regression model for classification. The main idea is to map the data to a fea-ture space based on kernel density estimation. A discrimina-tive model is then learned to optimize the feature weights as well as the bandwidth of a Nadaraya-Watson kernel density estimator. We then propose a hierarchical optimization al-gorithm for learning the coefficients and kernel bandwidths in an integrated way. Compared to other nonlinear mod-els such as kernel logistic regression (KLR) and SVM, our approach is far more efficient since it solves an optimiza-tion problem with a much smaller size. Two other major advantages are that it can cope with categorical attributes in a unified fashion and naturally handle multi-class prob-lems. Moveover, our approach inherits from logistic regres-sion good interpretability of the model, which is important for clinical applications but not offered by KLR and SVM. Extensive results on real datasets, including a clinical pre-diction application currently under deployment in a major hospital, show that our approach not only achieves supe-rior classification accuracy, but also drastically reduces the computing time as compared to other leading methods. H.2.8 [ Database Management ]: Database Applications-Data Mining; J.3 [ Computer Applications ]: Life and medical Sciences Nonlinear classification; logistic regression; density estima-tion; medical prediction
Classification is a central and critical task in the area of data mining. Many classification models have been pro-posed, such as support vector machine (SVM), logistic re-gression (LR), kernel logistic regression (KLR), decision tree (DT), and naive Bayes (NB) classifier. There are a few key aspects regarding any classifier: 1) nonlinear separation abil-ity, 2) support for mixed data types (numerical and cate-gorical), 3) efficiency, 4) interpretability of model, and 5) support for multiway (as opposed to binary) classification. Few methods can competently achieve all of them.

An application that motivates this research is patient clas-sification for early clinical warning, an NIH project currently under clinical trial at the Barnes Jewish Hospital, one of the largest hospitals in the United States. As we reported ear-lier [13, 14], we classify patients for categorical outcomes of certain disease or clinical event based on real-time data such as vital signs. This application entails mixed data types and requires good interpretability of model. Our previous study has chosen LR as the classifier for this application [13, 14].
Kernel-based SVM has good nonlinear separation abil-ity. However, its output model is hard to interpret, which severely limits its use in some domains such as clinical warn-ing. Moreover, the outcome of SVM is score-based rather than confidence-based. The score output from SVM for one task is not comparable to another task and often makes little sense to end users. Finally, SVM is inherently a binary clas-sifier. Although there have been efforts to extend SVM to multiway classification, it is known that these methods have many limitations [1]. For example, the popular one-versus-the-rest method suffers from issues such as: the training is based on imbalanced datasets, and an instance may be as-signed to multiple classes.
 LR uses a linear weighted sum of the input attributes. LR has some advantages over SVM as it is often more effi-cient, and it is easier to extend LR to multiway classifica-tion. Moreover, LR has good interpretability: its confidence-based output is meaningful and comparable, and the weights associated with each feature can indicate the most impor-tant factors. However, LR is a linear classifier with a linear decision boundary. Extending LR by the kernel trick re-sults in KLR [11, 20], which enables nonlinear classification. Unlike SVM, KLR cannot be formulated as a quadratic pro-gramming problem and iterative nonlinear optimization al-gorithms such as quasi-Newton methods are needed. As a result, the time complexity for training KLR is high. More-over, KLR no longer offers good interpretability.

SVM, LR and KLR rely on numerical attributes and can-not naturally handle categorical attributes. When handling categorical attributes, they need to first transform those at-tributes to numerical features. A typical way is using m numbers to represent an m -category attribute [6]. Only one of the m numbers is 1 with the others being 0. However, this
Figure 1: ICU transfer rate for various age groups. kind of preprocessing dramatically increases the number of features.

In this paper, we propose a novel density-based logistic re-gression (DLR) model that well addresses all the five aspects to some extent. A key drawback of LR is that it assumes a monotonic relationship between every numerical attribute and the class probability, due to its linear formulation. How-ever, such monotonicity is often false. For example, Fig-ure 1 shows the ICU transfer rate versus the patients X  age group. We can see that there is a non-monotonic relation-ship. Based on kernel density estimation (KDE), DLR can model non-monotonic and nonlinear relationships. More-over, unlike LR, DLR can handle datasets with mixed data types since its nonlinear transformation can be applied to numerical and categorical attributes in a unified way. DLR also inherits the advantages of LR, including time efficiency, good interpretability, and support for multiway classifica-tion, which are not offered by other nonlinear methods such as SVM and KLR.

This paper contains the following contributions. 1) We propose DLR, a novel nonlinear classification model, by integrating Bayesian analysis and kernel density estima-tion into LR. DLR is much more efficient than other non-linear models and can naturally handle mixed data types. It also offers good interpretability and support for multiway classification. 2) We develop a hierarchical optimization algorithm for training DLR, which automatically learns free parameters in the model under a maximum likelihood framework. This optimization formulation not only learns the coefficients in DLR, but also provides a way to automatically select the kernel bandwidth in the Nadaraya-Watson estimator, which is absent in previous work. This makes DLR a self-tuning model without any free parameters. 3) We extend the DLR model to multiway classification based on the same theory for binary LR. The hierarchical optimization algorithm for binary DLR can also be applied to the multiway case. 4) We conduct extensive evaluation on a large collection of biomedical datasets, mixed-type datasets, and a real pa-tient dataset for clinical prediction. The results show that DLR compares favorably against other nonlinear methods including SVM and KLR in terms of classification quality. Moreover, DLR has much better efficiency and scalability  X  requiring drastically less, often orders of magnitude lower, training time than other kernel-based methods.
In this section, we first discuss the basics and limitations of LR and then describe our new DLR model.
Assume we are given a dataset D = { x i ,y i } , i = 1 ,  X  X  X  ,N , x i  X  R D , and y i  X  { 0 , 1 } . Let the input vector be x = ( x 1 ,  X  X  X  ,x D ), and the class label y be binary: y can be either 1 or 0. LR is based on the following probability model: where w is a vector of weights that need to be learned. Note that x 0 = 1 represents a constant term. LR uses a maximum likelihood optimization to learn the weight vector w .
Define Lemma 1. LR models the following distribution of data: Proof. Based on the Bayesian rule, we find the probability The lemma follows by comparing (1) with (5).

From Lemma 1, we see that in a LR model: a) if w d &gt; 0, then p ( y = 1 | x ) and  X  ( x ) increase as x d increases, and b) if w d &lt; 0, then p ( y = 1 | x ) and  X  ( x ) decrease as x d
Therefore, a severe drawback of LR is that it is reasonable only if there is a monotonic relationship between p ( y = 1 | x ) and x d . However, this condition is often violated. For exam-ple, as we show in Figure 1, the probability of ICU transfer is not in a monotonic relationship with the age attribute. KLR and SVM can model non-monotonic relationships be-tween p ( y = 1 | x ) and x d . However, KLR and SVM have many other limitations such as high computational cost and low interpretability. We derive an attribute transformation for LR based on Bayesian rules. For simplicity of presentation, we first dis-cuss the case where the class label y is binary: y  X  { 0 , 1 } and extend it to the multiway case later. The proposed DLR model follows the parametric form of LR in (1), but transforms each attribute x d to a feature  X  d ( x ): p ( y = 1 | x ) =  X  ( w T  X  ) = 1 where  X  0 = 1. For each d = 1 ,  X  X  X  ,D , the proposed DLR feature transformation  X  d is The first term in (7) relates to the likelihood of y = 1 given x of the dataset. Hence,  X  d ( x ) gives an unbiased measurement of the quantity sought after by the LR formulation based on the Bayesian explanation.
 Lemma 2. If all the attributes of x = ( x 1 ,  X  X  X  ,x conditionally independent given the label y , then: Proof. Given that all variables are conditionally indepen-dent, we know p ( x | y ) = Q D d =1 p ( x d | y ). It follows that Theorem 1. For a dataset, if all the attributes of x are conditionally independent given the label y , then the DLR function (6) with w 0 = 0 and w d = 1 for d = 1 ,  X  X  X  ,D models the true distribution of data.
 Proof. Following a similar proof as in Lemma 1, we know that DLR models the following distribution of data: It remains to show that (10) is true for the given dataset. Based on conditional independence and Lemma 2, we have
Theorem 1 shows that, under the conditional indepen-dence assumption, DLR with w 0 = 0 and w d = 1 for d = 1 ,  X  X  X  ,D perfectly models the distribution of data.
We also observe some connections between the naive Bayes (NB) model and DLR. NB first models p ( x d | y ) and then uses the Bayesian rule to figure out p ( y | x ), while DLR directly models p ( y | x d ). Although DLR makes the same assumption of conditional independence as NB does, it is not rigidly tied to this assumption as NB does. NB can be viewed as a spe-cial case of DLR. DLR is more general as it incorporates the discriminative ability of LR and the generative power of NB. We have proposed the transformation  X  for DLR in (7). Now we estimate the probabilities in (7). Given training data D = { x i ,y i } , i = 1 ,  X  X  X  ,N , we need to estimate  X  for each d . Divide D into two subsets D 1 and D 0 , which contain data samples with y = 1 and y = 0, respectively.
Two quantities are needed for computing  X  d ( x ) in (7): p ( y | x d ) and p ( y ). p ( y ) can be estimated by simply counting the proportion of y in D , i.e.
To estimate p ( y | x d ), we distinguish the cases of categorical and numerical attributes.

If an attribute x d takes categorical values, p ( y = k | x can be estimated by the proportion of samples with y = k among all the samples whose d th attribute is x d . Thus, it can be easily computed using simple counting: where D x d = { x i | x i,d = x d ,i = 1 ,  X  X  X  ,N } is the set of sam-ples in D whose d th attribute is x d . Note that we use x denote the d th attribute of the i th sample x i . Substituting (11) and (12) into (7), we have:
If an attribute x d takes numerical values, we propose to use a Nadaraya-Watson type kernel density estimator to es-timate p ( y = k | x d ) ,k = 0 , 1.

According to the Nadaraya-Watson estimator [1], we have: where K ( x ) is a kernel function satisfying K ( x )  X  0 and R
K ( x ) dx = 1, and h d &gt; 0 is a parameter called the band-width of the kernel density function. In this paper, we choose the Gaussian kernel for K ( x ), namely, Substituting the estimates in (14) into (7), we have our transformation for the d th dimension: Using the Gaussian kernel, we have:  X  d ( x ) = ln (17) gives the full closed form of  X  d ( x ) for a numerical x
In essence, we use a Bayesian explanation of LR to de-rive a  X  X easonable X  feature transformation in (7), and then use a kernel density estimator to compute the conditional probabilities in (7).
Now we develop an algorithm for learning the parame-ters in the DLR model, including the weights w and the bandwidth h d in the Nadaraya-Watson estimator for each numerical attribute x d .
 (6). The objective of our optimization is to minimize the negative logarithm of the likelihood without any constraints, also known as the cross-entropy error function: where h is the vector of all the h d , 1  X  d  X  D , where x is a numerical attribute (since categorical attributes do not need a Nadaraya-Watson estimator).

We minimize E ( w , h ) by performing gradient descent in the w and h spaces, based on the training set and validation set, respectively. We first compute the derivative of E with respect to h d . The derivatives of E with respect to w are well studied in LR and easy to compute.
In each Nadaraya-Watson estimator for x d , d = 1 ,  X  X  X  ,D , we need to set its bandwidth h d . A common method in pre-vious works use rules-of-thumb to set a heuristic h d values. A popular one is the Silverman X  X  rule of thumb [16]: where  X  is the standard deviation of x d .

Although such rules-of-thumb often give solid performance, based on the DLR framework, we can in fact derive a novel way to automatically choose optimal h d . Such automatic tuning is absent in previous work. We propose to find the h d that minimizes E in (18). For this minimization, we first need to find  X  X   X  X   X  d ( x ) = ln where Thus, the derivative of  X  d ( x ) over r d is Moreover, for j = 0 , 1, Now we calculate the derivatives of ln b i and ln(1  X  b i r which are needed in  X  X   X  X  and  X  ln(1  X  b i )
Using (27) and (25), we get the following derivative of (18): Then, since r d =  X  1 2 h 2
Finally, we substitute (23) and (24) into (29) and get the full closed-form expression of  X  X   X  X 
We have obtained first-order derivative  X  X   X  X  ever, the second-order derivatives for h are hard to compute. And optimizing both w and h on the same set would simply result in a trivial solution where all h are 0. Therefore, it is difficult to perform Newton X  X  method in the joint space of w and h . To address this issue, we propose to learn w and h separately by a hierarchical optimization framework, shown in Algorithm 1, which contains two levels of optimization: an outer loop which optimizes h using simple gradient de-scent on validation set, and an inner loop which optimizes w using Newton X  X  method on training set under fixed h .
Similar to tuning free hyperparameters in SVM [2], we di-vide the dataset into the training set, validation set, and test Algorithm 1 Hierarchical optimization for DLR learning 1: Initialize h using (19) 2: repeat . outer loop: optimize h 3: Assemble the feature matrix  X  under h 4: repeat . inner loop: fix h and optimize w 6: until w converges 7: for d = 1 to D do . fix w and update h d 8: if x d is a numerical attribute then 10: end if 11: end for 12: until h converges set. In the outer level, at each iteration, the feature matrix  X  , composed of  X  ( x i ) for i = 1 ,  X  X  X  ,D , is updated based on the new h (Line 3). If a variable x i is categorical,  X  ( x is computed by (13). For a numerical variable x d , we use the kernel density estimation in (16) to compute its feature  X  ( x i ). Then, entering the inner level, we fix h and optimize w by Newton X  X  method using the training set (Lines 4-6). In fact, given  X  , we can use any standard LR package to im-plement Lines 4-6 and leverage its mature implementation. Finally, we optimize h for numerical attributes by perform-ing gradient descent along the direction given in (29) using the validation set (Lines 7-11).

From Algorithm 1, we can derive two variants of DLR. We call the complete algorithm DLR-h since it automatically chooses h . If we fix h at its initial rules-of-thumb values given in (19) and skip the optimization steps in Lines 7-11, we call this algorithm DLR. In the experimental result section, we will evaluate both DLR and DLR-h. It is straightforward to extend DLR to multiway problems. We outline the main steps here. Assume there are C classes. We have, for k = 1 ,  X  X  X  ,C , the DLR model is as follows: where w k = ( w k, 1 ,  X  X  X  ,w k,D ) is the weight coefficient vector for class k and  X  k = (  X  k, 1 ,  X  X  X  , X  k,D ) is feature transforma-tion for class k , defined as: If x d is a numerical attribute, we estimate p ( y = k | x Nadaraya-Watson estimator, where D k  X  X  is the subset of data in class k .

Like binary DLR, multiway DLR learns w and h by min-imizing the negative logarithm of likelihood: where 1 y i = k is 1 when y i = k and 0 otherwise. Let b p ( y i = k | x i ), we have: As in (23) and (24), we have
Some simple calculation based on (33) and (34) gives: multiway DLR model.
In this section, we discuss the DLR model from the per-spective of kernel methods and illustrate that the DLR ker-nel is a valid and good kernel. We also point out its differ-ence from some existing kernels.

It is known that the LR and SVM models can be studied under a unified view [1, 11]. They both pursue the goal of minimizing a regularized error function as follows: The regularized LR model adopts the logistic loss where and the SVM model utilizes the hinge loss, i.e. Using the Representer Theorem, both models can be com-bined with the kernel trick to gain the ability of nonlinear separation.

From this perspective, the proposed DLR model can be viewed as taking the logistic loss as in LR. However, DLR uses the following kernel: where the transformation  X  d here takes the form in (7) and k ( x , x 0 ) =  X  d ( x )  X  d ( x 0 ).

To some extent, a kernel defines the similarity between two data samples. For example, the Gaussian kernel inter-prets similarity based on distance. According to (7) and as discussed in section 2.2, each  X  d indicates the  X  X ikelihood X  of y being labelled 1 given x d . Thus, the kernel in the DLR model defines the similarity of two instances by their  X  X ikeli-hood X  of having the same label. Specifically, k d ( x , x sents the similarity of the d th attribute of these two samples, and the overall similarity is the sum of similarities at each dimension. We can observe this fact by distinguishing the following cases:
Obviously, the DLR kernel in (38) is a valid kernel since it is the inner product in the feature space and thus the kernel matrix is always positive definite. For a perfect kernel, we need k ( x , x 0 ) to satisfy the condition that k ( x , x a kernel strictly satisfies this condition, all the instances in the dataset would be perfectly classified without any error. From the above analysis, we see that the DLR kernel closely correlates to this condition, as it can be seen as a probabilis-tic implementation of this condition. Thus, the DLR kernel is not only a valid kernel, but also a good kernel.
Different from most existing kernels like the polynomial kernel and the Gaussian kernel, our kernel considers the label information by discriminative conditional probability, which leads to a better measurement of similarity, while ex-isting kernels such as the Gaussian kernel do not consider the label information in y .

We can observe from (38) that the DLR kernel can be expressed in an additive form of kernels operating on each dimension, known as the additive kernel [12]. Thus, any classifiers using the DLR kernel inherit the good properties of additive kernels such as fast training and evaluation.
LR can also be studied from a Bayesian perspective, al-though exact Bayesian inference is infeasible and some ap-proximation such as Laplace approximation is needed [9]. A dual algorithm for KLR using ideas similar to the SMO algorithm for SVM is proposed in [11]. The import vector machine (IVM) incorporates the loss function of KLR in a SVM framework [20]. IVM shares some advantages with LR, including the abilities to estimate the underlying prob-ability and generalize to multi-class problems. Compared to these complex methods such as Bayesian logistic regression and IVM, our approach requires only a simple preprocess-ing to transform the input variables and requires solving a single unconstrained optimization problem as LR does. It also retains the advantages of LR over SVM.

Raina et al. proposed a hybrid generative/discriminative model for classification [15]. They also arrived at a form that transforms each variables x d into a feature  X  d = ln( p ( x 1)  X  ln( p ( x d ,y = 0). However, it only considers the case where x d takes discrete values. Moreover, the probabilities p ( x d ,y = 1) and p ( x d ,y = 0) are learned using a NB model, which requires many samples and does not work for continu-ous x d . In contrast, we use a smooth kernel density estima-tion which accommodates continuous x d and requires fewer samples. We also optimize the bandwidth h d automatically.
There are other related probabilistic kernels most of which aim at combining generative models with discriminative ones. Jaakkola and Haussler proposed using generative techniques in discriminative classification models [8]. They proposed to use the Fisher score from generative models to generate a kernel, which is in turn used in discriminative models. However, such generative kernels are only suitable for some domain-specific tasks and different tasks require different generative models. For example, the Fisher kernel of sequen-tial data such as DNA is often derived from a hidden Markov model (HMM) [7], while the the kernel for text segmentation is often derived from LDA [17]. A more general probabilistic kernel is the marginalized kernel [10] for classifying graphs. However, it still requires a domain-specific generative model. A discriminative kernel is introduced in [19] where the map-still relies on a particular generative model such as HMM to which  X  belongs. Compared to these model-driven ker-nels [4], our DLR kernel does not assume any underlying models for data samples and uses the Nadaraya-Watson ker-nel density estimator to approximate p ( y | x ), which results in a general classification approach that can be potentially used in a wide range of applications.

Our DLR model can also be considered a special case of the generalized additive model (GAM) [5]. GAM specifies as the link function and f d is the basis function. The pro-posed DLR model is equivalent to using ln p ( y =1 | x ) as the link function and w d  X  ( x d ) as the basis function. In particular, DLR is closely related to additive logistic regres-sion (ALR) [5], a GAM model for nonlinear classification. They use the same link function. However, rather than using splines as basis functions in ALR, we adopt the logarithm of kernel smooth functions as in (7). In addition, ALR is pro-hibitively time-consuming for high dimensional datasets due to the slow convergence of its backfitting algorithm. Also, It cannot naturally handle mixed data types as DLR does.
We conduct extensive experiments to evaluate DLR. We evaluate two versions of DLR: DLR with a fixed h given in (19) (denoted as DLR), and DLR with automatic tuning of h (DLR-h). For comparison, we also consider seven other clas-sification methods, including logistic regression (LR), SVM with polynomial kernel (SVM-p) and RBF kernal (SVM-r), least square SVM [18] with polynomial kernel (LSSVM-p) and RBF kernal (LSSVM-r), and kernel logistic regressions with polynomial kernel (KLR-p) and RBF kernal (KLR-r). For all the datasets, we perform a standard normalization of the features. The normalization is especially helpful for SVMs. All algorithms are implemented in Matlab. The op-timization in LR and DLR used the minFunc function in Matlab. The SVM package is implemented by Bottou and Lin in the Matlab Bioinformatics Toolbox, and the LSSVM package is the LS-SVMlab Toolbox. The regularization and kernel width parameters of SVM are also tuned on the vali-dation set. All experiments are performed on a desktop with 2.67GHz CPU and 2G memory running Windows 7.
For sanity check and illustration, we test on a simple 2-D dataset as shown Figure 2. It contains 1200 points, cho-Figure 2: A simple 2-D dataset. The two classes are colored differently. Figure 3: Probability output of LR. The decision boundary is at p = 0 . 5 . sen from 4 multivariate normal distributions with a mean variances of former two distributions are  X  = [1 , 0; 0 , 100] while the last two distributions are  X  = [100 , 0; 0 , 1]. The two classes are marked in different colors.

We plot the decision boundary of LR and DLR in Fig-ure 3 and Fig. 4, respectively. We can see that the decision boundary of LR is linear, leading to a low accuracy of 51.3%. On the other hand, DLR gives a very nice nonlinear decision boundary and has an accuracy of 89.3%.

We also illustrate the process of Algorithm 1 which au-tomatic tunes the kernel bandwidth h . As we explained before, a benefit of DLR compared to SVM is that it can naturally handle multi-class classification. In Figure 5, we illustrate the change of decision boundaries by tuning h on a three-class dataset. We can see that Algorithm 1 quickly converges to a near optimal h in only three iterations.
We test all the methods on five biomedical datasets from the UCI repository [3]. They have binary classes and numer-ical features. For each dataset, we run 100 experiments with different random 70/30 split for training/testing and report the averaged results. Table 1 and 2 compare the accuracy and AUC of various methods, respectively. We observe that DLR and DLR-h perform better than other methods in most datasets. We also see that, while DLR gives consistently good performance using the rule-of-thumb value of h , DLR-h achieves better accuracy than DLR on all the datasets.
Table 3 shows the training times. LR has the least running time while DLR is the second fastest and drastically more Figure 4: Probability output of DLR. The decision boundary is at p = 0 . 5 . efficient than KLR and SVM. DLR-h is slower than the DLR model since it tunes h , but it is still much faster than KLR and SVM in most cases. The main reason for the efficiency of DLR is that it optimizes a problem with D unknowns while kernel-based SVM and KLR search in a space with N unknowns, and typically D N .
Another advantage of DLR is its ability to naturally han-dle categorical variables. We evaluate our algorithms on three datasets with categorical features from the UCI repos-itory. For DLR and DLR-h, a categorical variable x is trans-formed into a numerical feature  X  ( x ) based on density esti-mation in (16). For other methods, we use a typical way to handle categorical variables. For each variable that has m categories, we transform it into m numbers with only one of the numbers being 1 and the others being 0. Tables 4, 5, and 6 show the accuracy, AUC, and training time of various methods on these datasets, respectively. DLR and DLR-h have the same performance on tic-tac and monk datasets since both have categorical features only and there is no h to tune. SPECFT heart data have mixed types. We can see that DLR and DLR-h are high competitive with, if not better than, all other methods. However, a huge advantage of DLR and DLR-h is their time efficiency  X  they are orders of magnitude faster than other kernel based classifiers.
Our research is motivated by a project in collaboration with the Barnes-Jewish Hospital (BJH), one of the largest hospitals in the US. Our task is to predict potential ICU transfers for hospitalized patients, based on 34 vital signs. We use a number of techniques to process the features, and the details of this process are reported in [13,14]. In a clinical trial at BJH, three algorithms, LR, SVM-r and DLR-h, are tested on more than 18,458 patients admitted from 01/14/11 to 04/23/12. Table 7 shows the results in terms of a few met-rics that are important for clinical practice. We can see that DLR-h improves LR and SVM-r on most metrics. In par-ticular, it significantly improves AUC, which measures the overall performance. Specificity and sensitivity are tradeoff that can be tuned in DLR-h by changing the threshold. We set DLR-h to have a specificity that is very close to SVM-r X  X . To study the scalability, we test different randomly chosen training datasets with various sizes. Figure 6 compares the training time with respect to the size of training data. We can see that DLR-h is much more efficient than SVM-r. Table 7: Prediction performance on clinical data.
 Figure 6: Training time on the clinical data. Note that the y-axis is in log scale.
In this paper, we have proposed a novel DLR model for classification. DLR integrates kernel density estimation and the discriminative power of logistic regression. DLR uses a novel nonlinear feature transformation derived from a Bayesian explanation of its parametric form, and a Nadaraya-Watson kernel density estimator for assessing the conditional prob-abilities in the transformation. We have also derived a hier-archical optimization algorithm for learning the model coef-ficients and kernel bandwidths in an integrated way. DLR competently supports nonlinear separation, efficient train-ing, mixed data types, multiway classification, and good in-terpretability, a combination of advantages that is rarely found in existing methods. Extensive results on real-world numerical and categorical data show that, compared to other leading methods, DLR gives comparable and often better classification quality while being orders of magnitude more efficient.

We believe that, because of its unmatched performance, versatility, efficiency, and interpretability, DLR will become a popular general-purpose classification approach for many real applications.
 This work is supported by the CNS-1017701 and CCF-1215302 grants from the National Science Foundation of USA. [1] C. M. Bishop. Pattern Recognition and Machine [2] O. Chapelle, A. Polytechnique, and N. Cristianini. [3] A. Frank and A. Asuncion. UCI machine learning [4] T. G  X  artner. A survey of kernels for structured data. [5] T. Hastie, R. Tibshirani, and J. H. Friedman. The [6] C. W. Hsu, C. C. Chang, and C. J. Lin. A practical [7] T. Jaakkola, M. Diekhans, and D. Haussler. Using the [8] T. Jaakkola and D. Haussler. Exploiting generative [9] T. Jaakkola and D. Haussler. Probabilistic kernel [10] H. Kashima, K. Tsuda, and A. Inokuchi. Marginalized [11] S. S. Keerthi, K. Duan, S. K. Shevade, and A. N. Poo. [12] S. Maji and A. C. Berg. Max-margin additive [13] Y. Mao, W. Chen, Y. Chen, C. Lu, M. Kollef, and [14] Y. Mao, Y. Chen, G. Hackmann, M. Chen, C. Lu, [15] R. Raina, Y. Shen, A. Ng, and A. McCallum.
 [16] B. W. Silverman and P. J. Green. Density Estimation [17] Q. Sun, R. Li, D. Luo, and X. Wu. Text segmentation [18] J. A. K. Suykens, T. V. Gestel, J. D. Brabanter, B. D. [19] K. Tsuda, M. Kawanabe, G. R  X  atsch, S. Sonnenburg, [20] J. Zhu and T. Hastie. Kernel logistic regression and
