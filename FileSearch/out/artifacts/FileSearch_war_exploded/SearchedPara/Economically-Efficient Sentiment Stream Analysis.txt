 Text-based social media channels, such as Twitter, produce torrents of opinionated data about the most diverse topics and entities. The analysis of such data (aka. sentiment analysis) is quickly becom-ing a key feature in recommender systems and search engines. A prominent approach to sentiment analysis is based on the applica-tion of classification techniques, that is, content is classified ac-cording to the attitude of the writer. A major challenge, however, is that Twitter follows the data stream model, and thus classifiers must operate with limited resources, including labeled data and time for building classification models. Also challenging is the fact that sentiment distribution may change as the stream evolves. In this paper we address these challenges by proposing algorithms that se-lect relevant training instances at each time step, so that training sets are kept small while providing to the classifier the capabilities to suit itself to, and to recover itself from, different types of senti-ment drifts. Simultaneously providing capabilities to the classifier, however, is a conflicting-objective problem, and our proposed al-gorithms employ basic notions of Economics in order to balance both capabilities. We performed the analysis of events that rever-berated on Twitter, and the comparison against the state-of-the-art reveals improvements both in terms of error reduction (up to 14%) and reduction of training resources (by orders of magnitude). H.3.1 [ Information Storage and Retrieval ]: Content Analysis; I.5.2 [ Pattern Recognition ]: Classifier Design and Evaluation Algorithms, Experimentation, Measurement, Performance Sentiment Analysis; Economic Efficiency; Streams and Drifts
The need for real-time text analytics is clear and present given the ubiquitous reach of social media sites like Facebook and Twit-ter. Specifically, recognizing customer sentiment in real-time and enabling advertising on-the-fly have the potential to be a break-through technology [20]. Early examples of such technology in use were demonstrated in this year X  X  National Football League X  X  Superbowl (a premier sporting event in the USA) where a well known manufacturer of Oreo cookies took advantage of a third quarter blackout (and associated Twitter sentiment) to embed a con-textual advertisement. Another example at the same event was the advertisement for a Hollywood movie, where, based on the initial advertisement which happened before the start of the first quarter (and associated Twitter sentiment), the decision on which of several possible advertisements to run later on in the program was appar-ently taken as a runtime decision. Examples like these are likely to occur more frequently due to lightweight and easy communi-cation mechanisms, such as Twitter microblogging, which makes people eager not only to exchange information, but also to convey their opinions and emotions. People watch events together on tele-vision, while tweeting out about things happening around them. As a result, opinionated content is created almost at the same time the event is happening in the real world, and becomes available shortly after. The analysis of such content (aka. sentiment analysis) in order to exploit the aggregate sentiment of the online crowd goes beyond advertising, and is becoming crucial to recommender sys-tems and search engines.

There is a growing trend in performing sentiment analysis us-ing classification-related techniques: a process that automatically builds a classification model by learning, from a set of previously labeled data (i.e., the training-set), the underlying characteristics that distinguish one sentiment from another (i.e., happiness, mad-ness, surprise, suspicion). The success of these classifiers rests on their ability to judge attitude by means of textual-patterns present in the data, which usually appear in the form of (idiomatic) expres-sions and combinations of words. Sentiment analysis over Twitter real-time messages, however, is particularly challenging, because: (i) Twitter follows the data stream model 1 , requiring classifiers to operate with limited computing and training resources, and (ii) ei-ther sentiment distribution or the characteristics related to certain
There are three main source streams in Twitter. The Firehose pro-vides all status updates from everyone in real-time. Spritzer and Gardenhose are two sub-samples of the Firehose. The current sam-pling rates are 5% and 15%, respectively. sentiments may change over time in almost unforeseen ways (i.e., sentiment drift).
 A possible strategy to cope with the aforementioned challenges is to employ selective sampling algorithms in order to focus only on the most relevant training examples/messages at each time step and to creating training sets from which classifiers are built. Such train-ing sets are kept as small as possible to ensure fast learning times, since a new classifier must be built at each time step, after a new target message arrives. Also, messages should be selected so that the resulting training set provides sufficient resources to enable the resulting classifier to be effective under the occurrence of drifts. In order to provide sufficient training resources while keeping sets small, our algorithms select training messages by taking into ac-count two important properties, that we define as adaptiveness and memorability. Informally, adaptiveness enables the classifier to adapt itself to drifts, and thus, improving adaptiveness involves in-corporating fresh messages into the current training set, while dis-carding obsolete ones. Memorability, on the other hand, involves retaining messages belonging to pre-drift distributions, therefore enabling the classifier to recover itself from drifts.

We hypothesize that adaptiveness and memorability are both nec-essary to make classifiers robust to drifts. However, given their an-tagonistic natures, improving both properties may lead to a conflicting-objective problem, in which the attempt to improve memorabil-ity further may result in worsening adaptiveness. Thus, we tackle the problem by proposing selective sampling algorithms based on multi-objective optimization, that is, we propose to select training messages so that the resulting classifier achieves a proper balance between memorability and adaptiveness. Our algorithms are based on central concepts in Economics, namely Pareto and Kaldor-Hicks efficiency criteria [19,22,28]. The Pareto Efficiency criterion infor-mally states that  X  X hen some action could be done to make some-one better off without hurting anyone else, then it should be done. X  This action is called Pareto improvement, and a system is said to be Pareto-Efficient if no such improvement is possible. The Kaldor-Hicks criterion is less stringent and states that  X  X hen some action could be done to make someone better off, and this could compen-sate those that are made worse off, then it should be done. X  The main contribution of this paper is to exploit the intuition behind the aforementioned concepts for devising new algorithms for senti-ment stream analysis. In practice, we claim the following benefits and contributions:
To evaluate the effectiveness of our algorithms, we performed experiments using Twitter data collected from three important events in 2010, spanning different sentiments expressed in different lan-guages. Results show that our algorithms make classifiers extremely effective, with gains in prediction performance that are up to 14% when compared against the state-of-the-art. Further, the amount of training resources needed is decreased by two orders of magnitude.
In the data stream model, data arrives at high speed and algo-rithms must work in real time and with limited resources. Further, in some domains, algorithms must deal either with burst detec-tion [42] and concept drift (i.e., data which nature or distribution change over time).  X liobait  X  e [35] categorizes such drifts as sudden, gradual, incremental and recurring. When data distribution or na-ture change over time, its relevance must be recalculated to avoid harming the model. This kind of data stream is known as evolving data streams.

Many techniques have been proposed to allow accurate classi-fication in evolving data streams. N X  X ez et al. [27] proposed a method for keeping a variable training window by adjusting inter-nal structures of decision trees. An ensemble of Hoeffding trees have been proposed in [5], each tree is limited to a small subset of attributes. Gama et al. [17] proposed a mechanism to discard old information based on sliding windows. Bifet et al. [6, 7] pro-posed an adaptive sliding window algorithm, called ADWIN, suit-able for data streams with sudden drifts. The approach presented in [24] suggests that a time-based forgetting function, which makes more recent observations more significant, provides adaptiveness to the classifier. Klinkenberg [23] compares example selection, of-ten used in windowing approaches with example weights. Experi-ments show that both approaches are effective. In [30] the authors proposed an approach based on a training augmentation procedure, which automatically incorporates relevant training messages into the training-set.

Some works have focused on feature similarity, such as Torres et al. [31] that studied different methods for data stream classifi-cation and proposed a new way of keeping the representative data models based on similarity measures. Feng et al. [16] extracted the concept from each data block using feature similarity probabilities. Masud et al. [25] proposed a novel technique to overcome the lack of labeled examples by building models from unlabeled instances and a small amount of labeled ones. Zhu et al. [41] employed ac-tive learning to produce a classifier ensemble that selects labeled instances from data streams to build classifiers. Also, in [37, 38] active learning approaches are presented for data streams that ex-plicitly handle concept drifts. They are based on uncertainty [21], dynamic allocation of labeling efforts over time, and randomization of the search space.  X liobait  X  e et al. [36] proposed a system that im-plements active learning strategies, extending the Massive Online Analysis (MOA) framework [8].

Works above cited attempt to face concept drift in data stream through manipulation of classifiers, with mechanisms such as train-ing windows and decay functions, active learning and sampling. In this paper we present new algorithms that select high-utility ex-amples in order to provide adaptiveness and memorability to the classifier. In order to balance adaptiveness and memorability, we formalized this issue as a multi-objective problem. The sample se-lection is performed using economic efficiency criteria: Pareto and Kaldor-Hicks. We did not find in the recent literature approaches that employ multi-objective models based on economic efficiency criteria to deal with issues in the data stream environment.
In this section we present novel selective sampling approaches for learning classifiers to distinguish between different sentiments expressed in Twitter messages. We start by discussing models based on specialized association rules. Then we present measures for adaptiveness and memorability, and describe the message utility space. Finally, we discuss Pareto and Kaldor-Hicks criteria, and algorithms that select training messages using these criteria.
In our context, the task of learning sentiment streams is precisely defined as follows. At time step n , we have as input a training set referred to as D n , which consists of a set of records of the form &lt; d,s i &gt; , where d is a message (represented as a list of terms), and s i is the sentiment implicit in d . The sentiment vari-able s draws its values from a pre-defined, fixed and discrete set of possibilities (e.g., s 1 , s 2 , ... , s k ). The training set is used to build a classifier relating textual patterns in the messages to their corre-sponding sentiments. A sequence of future messages referred to as T = { t n ,t n +1 ,... } , consists of messages for which only their terms are known, while the corresponding sentiments are unknown. The classifier obtained from D n is used to score the sentiments for message t n in T . Messages in T are eventually incorporated into the next training set.

There are countless strategies for devising a classifier for sen-timent analysis. Many of these strategies, however, are not well-suited to deal with data streams. Some are specifically devised for offline classification [12, 14], and this is problematic because pro-ducing classifiers on-the-fly would be unacceptably costly. In such circumstances, alternate classification strategies may become more convenient [33].
Next we describe classifiers composed of association rules, and how these rules are used for sentiment-scoring. Such classifiers are built on-the-fly [32,34], being thus well-suited for sentiment stream analysis, as shown in [30].
 Definition 1 . A sentiment rule is a specialized association rule X  X  X  X  s i , where the antecedent X is a set of terms (i.e., a termset), and the consequent s i is the predicted sentiment. The domain for X is the vocabulary of the training set D n . The support of X is de-noted as  X  ( X ) , and is the number of messages in D n having X as a subset. The confidence of rule X  X  X  X  s i is denoted as  X  ( X  X  X  X  s and is given as  X  ( X  X  s i )  X  ( X ) .
 We denote as R ( t n ) the classifier obtained at time step n , by ex-tracting rules from D n . Basically, the classifier is a poll of rules, and each rule {X  X  X  X  s i } X  X  ( t n ) is a vote given for sentiment s Given message t n , a rule is a valid vote if it is applicable to t Definition 2 . A rule {X  X  X  X  s i }  X  R ( t n ) is said to be applicable to message t n  X  X  if all terms in X are in t n .

We denote as R a ( t n ) the set of rules in R ( t n ) that are applica-ble to message t n . Thus, only rules in R a ( t n ) are considered as valid votes when scoring sentiments in t n . Further, we denote as R a ( t n ) the subset of R ( t n ) containing only rules predicting sen-timent s i . Votes in R s i a ( t n ) have different weights, depending on the confidence of the corresponding rules. The weighted votes for sentiment s i are averaged, giving the score for s i with regard to t
Finally, the scores are normalized, thus giving the likelihood of sentiment s i being the attitude in message t n : The simplest approach to rule extraction is the offline one. In this case, rule extraction is divided into two steps: support counting and confidence computation. Once the support  X  ( X ) is known, it is straightforward to compute the confidence  X  ( X  X  X  X  s i corresponding rules [40]. There are several smart support-counting strategies [1, 18, 40], and many fast implementations [3] that can be used. We employ the vertical counting strategy, which is based on the use of inverted lists [39]. Specifically, an inverted list associated with termset X , is denoted as L ( X ) , and contains the identifiers of the messages in D n having termset X as a subset. An inverted list L ( X ) is obtained by performing the intersection of two proper subsets of termset X . The support of termset X is given by the cardinality of L ( X ) , that is,  X  ( X ) = |L ( X ) | .

Usually, the support for different sets of terms in D n are com-puted in a bottom-up way, which starts by scanning all messages in D n and computing the support of each term in isolation. In the next iteration, pairs of terms are enumerated, and their support val-ues are calculated by performing the intersection of the correspond-ing proper subsets. The search for sets of terms proceeds, and the enumeration process is repeated until the support values for all sets of terms in D n are finally computed. Obviously, the number of rules increases exponentially with the size of the vocabulary (i.e., the number of distinct terms in D n ), and computational cost re-strictions have to be imposed during rule extraction. Typically, the search space for rules is restricted by pruning rules that do not ap-pear frequently in D n (i.e., the minimum support approach). While such restrictions make rule extraction feasible, they also lead to lossy classifiers, since some rules are pruned and therefore are not included into R ( t n ) .
 Online Rule Extraction . An alternative to offline rule extraction is to extract rules on-the-fly. Such alternative, which we call on-line rule extraction, has several advantages [30]. For instance, it becomes possible to efficiently extract rules from D n without per-forming support-based pruning. The idea behind online rule extrac-tion is to ensure that only applicable rules are extracted by project-ing D n on a demand-driven basis. More specifically, rule extraction is delayed until a message t n  X  T is given. Then, terms in t used as a filter which configures D n in a way that only rules that are applicable to t n can be extracted. This filtering process produces a projected training-set, denoted as D  X  n , which contains only terms that are present in message t n .
 Lemma 1 . All rules extracted from D  X  n are applicable to t Proof . Since all training messages in D  X  n contain only terms that are present in message t n , the existence of a rule X  X  X  X  s from D  X  n , such that X * t n , is impossible.

Lemma 1 implies that online rule extraction assures that R ( t R a ( t n ) . The next theorem states that search space for rules in-duced by D  X  n is much narrower than the search space for rules in-duced by D n . Thus, rules can be efficiently extracted from D matter the minimum-support value (which can be arbitrary low). Theorem 1 . The number of rules extracted from D  X  n polynomially with the number of distinct terms in D n .
 Proof . Let k be the number of distinct terms in D n . Since an arbi-trary message t n  X  T contains at most l terms (with l k ), then any rule applicable to t n can have at most l terms in its antecedent. That is, for any rule {X  X  X  X  s i } , such that X  X  t n , |X|  X  l . Con-sequently, the number of possible rules that are applicable to t l + l 2 + ... + l l = O (2 l ) O ( k l ) . Thus, the number of appli-cable rules increases polynomially in k .
 Extending Classifiers Dynamically . Let R = {R ( t 1 )  X  X  ( t  X  ...  X  R ( t n ) } . With online rule extraction, R is extended dy-namically as messages t i  X  X  are processed. Initially R is empty; a classifier R t i is appended to R every time a message t cessed. Producing a classifier R ( t i ) involves extracting rules from the corresponding training-set. This operation has a significant computational cost, since it is necessary perform multiple accesses to D i . Different messages in T = { t 1 ,t 2 ,...,t m } may demand different classifiers {R t 1 , R t 2 ,..., R t m } , but different classifiers may share some rules (i.e., {R t i  X  X  t j }6 =  X  ). In this case, mem-orization is very effective in avoiding work replication, reducing the number of data access operations. Thus, before extracting rule X  X  X  X  s i , the classifier first checks whether this rule is already in R . If an entry is found, then the rule in R is used instead of extracting it from the training-set. If it is not found, the rule is extracted from the training-set and then it is inserted into R .
Our approach to sentiment stream analysis is based on select-ing high-utility messages to compose the training set at each time step. Training sets must provide adaptiveness and memorability to the corresponding classifiers. Improving adaptiveness and mem-orability simultaneously, however, is a conflicting-objective prob-lem. Instead, our approaches create training sets that balance be-tween adaptiveness and memorability. Specifically, at each time step, candidate messages are placed into an n-dimensional space, in which each dimension corresponds to a utility measure which is either related to adaptiveness or memorability.
 At each time step, the classifier must score sentiments that are ex-pressed in the target message. Some of the utility measures we are going to discuss next are based on the distance to the target mes-sage. By minimizing such distance we are essentially maximizing adaptiveness, since the selected messages are similar to the target message. As for memorability, we are going to discuss a utility measure based on randomly shuffling candidate messages:
Each candidate message is judged based on these three utility measures. The need to judge one situation better than another moti-vates much of Economics, and next we discuss concepts from Eco-nomics and how they can be applied to select messages to compose the training set.
When the society is economically efficient, any changes made to assist one person would harm another. The same intuition could be exploited for the sake of selecting messages to compose the training set at each time step. In this case, a training set is economically efficient if it is only possible to improve memorability at the cost of adaptiveness, and vice-versa [26, 29].

There is an alternative, less stringent notion of efficiency, which is based on the principle of compensation [13]. Under new ar-rangements in the society, some may be better off while others may be worse off. Compensation holds if those made better off under the new set of conditions could compensate those made worse off. Next we discuss algorithms that exploit these two notions of eco-nomic efficiency in order to select messages to compose the train-ing sets.  X   X   X  (Distance in space) Adaptiveness Messages that are candidate to compose the training set at time step n are placed in a 3-dimensional space, according to their utility measures, as shown in Figure 1. Thus, each message a is a point in such utility space, and is given as &lt; U s ( a ) ,U t ( a ) ,U Definition 3 . Message a is said to dominate message b iff both of the following conditions are hold: Therefore, the dominance operator relates two messages so that the result of the operation has two possibilities as shown in Figure 2: (i) one message dominates another or (ii) the two messages do not dominate each other. Figure 2: The dominance operator: neither a or b dominates each other, but b dominates c .
 Definition 4 . Training set P n = { d 1 ,d 2 ,...,d m } is said to be Pareto-efficient at time step n , if P n  X  D n and there is no pair of messages ( d i ,d j )  X  X  n for which d i dominates d j .
 Messages that are not dominated by any other message, lie on the Pareto frontier [28]. Therefore, by definition, the Pareto-efficient training set at time step n , P n , is composed by all the messages lying in the Pareto frontier that is built from D n . There are efficient algorithms for building and maintaining the Pareto frontier, and we employed the algorithm proposed in [11] which ensures O ( |D complexity. We denote the process of exploiting Pareto-efficient training sets as Pareto-Efficient Selective Sampling, or simply PESS. Figure 3 shows an illustrative example of a Pareto frontier built from arbitrary points in the utility space.
 The PESS strategy follows a stringent criterion, which tends to se-lect only few messages to compose the training sets. As a result, the training sets may become excessively small and prone to noise. The Kaldor-Hicks criterion, on the other hand, follows a cost-benefit analysis and circumvents the small training set problem by stating that efficiency is achieved if those that are made better off could in theory compensate those that are made worse off. Thus, under the Kaldor-Hicks criterion, an utility measure can compensate other utility measures, and therefore, this criterion selects messages that are located inside a region which is below the Pareto frontier. To define this region we must first define the overall utility of a mes-sage.
 Definition 5 . Assuming that all measures are equally important, the overall utility of an arbitrary message d i  X  X  n is: That is, the overall utility of a message is given as the sum of its utility measures. Also, the baseline message, which is denoted as d , is defined as: That is, the baseline is the message lying in the frontier for which the overall utility assumes its lowest value.

The Kaldor-Hicks region is composed of messages for which the overall utility is not smaller than the baseline overall utility. Such baseline utility is the utility associated with the message lying in the Pareto frontier for which the overall utility is the lowest. Definition 6 . Training set K n = { d 1 ,d 2 ,...,d m } is said to be Kaldor-Hicks-efficient at time step n , if P n  X  K n  X  D there is no message d i  X  X  n such that U ( d  X  ) &gt; U ( d
We denote the process of exploiting Kaldor-Hicks-efficient train-ing sets as Kaldor-Hicks-Efficient Selective Sampling, or simply KHSS. Figure 4 shows an illustrative example of a Kaldor-Hicks region built from arbitrary points in the utility space.
In this section we empirically analyze the performance of our classifiers. We employ the mean squared error (MSE) as the basic evaluation measure in our experiments, since we are primarily in-terested in evaluating sentiment scoring given by Equation 2. The MSE measure is given as: where s i is the correct sentiment associated with message t T , and  X  p ( s i | t i ) is the sentiment score assigned by the classifier to message t i  X  X  .

To evaluate the amount of computing resources used as the stream evolves, we employ the RAM-Hours measure [9], where every RAM-Hour equals a GB of RAM deployed for 1 hour of execution. We also evaluate the amount of training resources used over time, as the number of messages labeled during the process. We used Ho-effding Adaptive Trees [4, 10] (abbreviated as HAT), Active Clas-sifier [37, 38] (abbreviated as AC), and Incremental Lazy Associa-tive Classifier [30] (abbreviated as ILAC) as baselines. All datasets used in our experiments were manually labeled by three to five human annotators. We expended significant time, effort, and re-sources to obtain high quality (labeled) data from Twitter streams, which shall be made available at publication time.

All experiments were performed on a 1.93 GHz Core i7 ma-chines with 8GB of memory, using the MOA system [8], an envi-ronment for running experiments with evolving data streams. Our evaluation follows the Test-Then-Train methodology, in which each individual message in T is used to test the classifier and then it be-comes available for training. Finally, we consider three possible settings:
In the following we describe our evaluation scenarios and discuss the performance of the classifiers.
The presidential election campaigns were held from June to Oc-tober 2010. the candidate Dilma Rousseff launched a Twitter page during a public announcement, and she used Twitter as one of the main sources of information for her voters. The campaign attracted more than 500,000 followers and as a result Dilma was the second most cited person on Twitter in 2010. The election came to a sec-ond round vote, and Dilma Rousseff won the runoff with 56% of the votes.
 We collected 66,643 messages in Portuguese referencing Dilma Rousseff in Twitter during her campaign. We labeled these mes-sages in order to track the population sentiment of approval during this period. As shown in Figure 5 (a), approval varied significantly over the time due to several polemic statements and political at-tacks, and our goal is to score approval during her campaign.
Figure 5 (b) shows the results in terms of MSE obtained for the evaluation of the classifiers in this dataset. All classifiers evalu-ated in this experiment operate on an instance basis. The x-axis represents different time steps (i.e., each message that passes in the stream), while the y-axis shows the MSE so far. As it can be seen, a better approximation is obtained using our proposed algo-rithms, namely PESS and KHSS. AC and ILAC were very com-petitive during all the campaign. Both PESS and KHSS algorithms started much better than the other competing algorithms, but slowly converges to the baseline numbers as the stream evolves.

Figure 5 (c) shows results concerning the proposed algorithms when operating in batch mode. The figure shows the number of messages that were labeled during the process as a function of  X  , the minimum similarity threshold discussed in Section 3.1. Basi-cally, we calculate the Jaccard coefficient associated with each pos-sible pair of messages in the batch, and if the coefficient is greater than  X  , the corresponding messages are merged into a new one. The process continues merging similar messages until no pair of messages are similar enough, and the process stops. At the end, only the merged messages are labeled. Clearly, higher values of  X  implies that less messages are merged, and thus incurring more la-beling effort. Further, as the figure shows, the dependence between labeling effort and  X  tends to be linear.

By varying  X  , we also study the trade-off between labeling ef-fort and MSE. As shown in Figure 5 (d), MSE decreases if more labeling effort is spent during the process. Specifically, best results are achieved when about 40% of the messages in the stream are la-beled during the process. Although both PESS and KHSS require the same amount of training resources, KHSS provides slightly bet-ter MSE numbers. Furthermore, smaller batch sizes incur in less labeling effort for this dataset.

We assume that HAT requires only the target message for updat-ing its tree model, and thus we consider that the training set is com-posed only by the target message. The AC algorithm requires much more messages within each training set. An abrupt decrease in the number of training messages is always observed after drifts. The proposed PESS algorithm requires very small training sets, since the Pareto frontier at each time step is composed by few messages, but these messages are still able to make the classifier robust to drifts as the stream evolves. Further, despite being less stringent than PESS, the proposed KHSS algorithm also requires small train-ing sets, as shown in Figure 5 (e).

Figure 5 (f) shows RAM-Hours numbers for the algorithms. AC, as well as PESS (instance) and KHSS (instance), are clearly the best performers in terms of amount of computing resources re-quired. Also, resources required during the process significantly increases when PESS and KHSS operate on batch mode, but still, ILAC is the worst performer.
Every year, TIME magazine selects the person (or a group of per-sons) that has mostly influenced during the year. The chosen person for 2010 was Mark Zuckerberg. The reader choice, however, was Julian Assange, with an overwhelming superiority of votes. We collected 5,616 messages in English referencing Julian As-sange and Mark Zuckerberg from 1-15-2010 to 12-21-2010. We la-beled them in order to track diverse sentiments regarding the mag-azine X  X  decision. Sentiments include (dis)approval, surprise (since the reader choice was pointing to Julian Assange), and even fury.
Figure 6 (a) shows the results in terms of MSE. As it can be seen, a better approximation is obtained by HAT and ILAC. For this dataset, AC was not effective in the first time steps. At the end of the process, both PESS (instance) and KHSS (instance) algo-rithms achieved competitive numbers when compared against the best performers.
 Figure 6 (b) shows the trade-off between labeling effort and MSE. Again, MSE numbers decrease as more labeling effort is spent dur-ing the process. This trend is particularly evidenced for smaller batch sizes. Further, the KHSS algorithms shows a better trade-off between labeling effort and MSE. Finally, Figure 6 (c) shows RAM-Hours numbers for the evaluated algorithms. The AC algo-rithm, as well as PESS (instance) and KHSS (instance) are, again, extremely competitive in terms of amount of computing resources required. Further, the amount of resources required during the pro-cess significantly increases when PESS and KHSS operate on batch mode, but still, ILAC is the worst performer.
The 2010 Soccer World Cup involved 32 teams. The Brazilian team was defeated by the Dutch team on 07-02-2010, after a con-troversial match. The Brazilian team scored first, but soon after the Dutch team scored twice and won the match. A specific player, Felipe Melo, had decisive participation (for better and worse) in all three goals. Specifically, Figure 7 (a) shows how the appreciation for Felipe Melo varied during the match.
 We collected 3,214 messages in Portuguese referencing Felipe Melo that were posted in Twitter as the match was happening. We labeled them in order to track the appreciation for the participation of Fe-lipe Melo.

Figure 7 (b) shows the results in terms of MSE.As it can be seen, the AC algorithm achieved the worst MSE numbers for this dataset. On the other hand, HAT, ILAC, as well as PESS (in-stance) and KHSS (instance) showed extremely competitive num-bers. This is expected, since this dataset contains three sudden drifts (as shown in Figure 7 (a)), and HAT, ILAC, PESS (instance) and KHSS (instance) were all able to ensure adaptiveness. For this dataset, memorability is not mandatory (as the sentiment distribu-tion never returns to a pre-drift distribution), and thus PESS (in-stance) and KHSS (instance) were not able to provide significant improvements, although being the best performers overall.
Figure 7 (c) shows a X-Y scatter plot correlating  X  and label-ing effort. The correlation is almost linear. The trade-off between labeling effort and MSE is shown in Figure 7 (d). Clearly, MSE de-creases with the effort spent to label messages. Figure 7 (e) shows the number of messages composing the training set at each time step. As in previous cases, AC and ILAC require much more train-ing resources than other competing algorithms. PESS (instance) as well as KHSS (instance) require much less training messages, again, showing that the selective sampling strategy is effective in producing small and effective sets at each time step.
 Finally, Figure 7 (f) shows RAM-Hours numbers. In this case, AC, as well as PESS (instance) and KHSS (instance), are clearly the best performers in terms of amount of computing resources re-quired. Further, the amount of resources required during the pro-cess significantly increases when PESS and KHSS operate on batch mode, but still, as in other datasets, ILAC is the worst performer.
This paper focused on sentiment analysis on Twitter streams. We have introduced new algorithms for active training-set formation, which we denote as Pareto-Efficient Selective Sampling (PESS) and Kaldor-Hicks Selective Sample (KHSS). The proposed algo-rithms provide the resulting classifier with memorability and adap-tiveness. We formalized the selective sampling process as a multi-objective optimization procedure, which finds a proper balance be-tween adaptiveness and memorability. Adaptiveness is assessed by computing the distance in time and space between the target mes-sage and the candidate ones. Also, candidate messages are ran-domly shuffled, thus providing memorability to the resulting clas-sifier. The message utility space is composed by such dimensions, and we compute the Pareto Frontier in this space in order to pick up messages satisfying the Pareto improvement condition, find-ing a proper balance between adaptiveness and memorability. The Kaldor-Hicks criterion enables memorability to compensate adap-tiveness, or vice-versa. A systematic evaluation involving recent events demonstrated the effectiveness of our algorithms.

As future work, we intend to extend our strategies for algorithms that do not depend on manual labeling. Adriano Veloso, Adriano Pereira, Wagner Meira Jr., and Renato Ferreira would like to acknowledge grants from CNPq, CAPES, Fapemig, Finep, and InWeb  X  the Brazilian National Institute of Science and Technology for the Web. Srinivasan Parthasarathy would like to acknowledge NSF grant IIS 1111118 and a Google re-search award. Roberto Oliveira Jr. would like to acknowledge that some aspects of this work was conducted while he was a visiting researcher in Srinivasan Parthasarathy X  X  lab at Ohio State Univer-sity. [1] R. Agrawal, T. Imielinski, and A. Swami. Mining association [2] R. Baeza-Yates and B. R-Neto. Modern Information [3] R. Bayardo, B. Goethals, and M. Zaki, editors. Workshop on [4] A. Bifet and E. Frank. Sentiment knowledge discovery in [5] A. Bifet, E. Frank, G. Holmes, and B. Pfahringer. Ensembles [6] A. Bifet and R. Gavald X . Learning from time-changing data [7] A. Bifet and R. Gavald X . Adaptive learning from evolving [8] A. Bifet, G. Holmes, R. Kirkby, and B. Pfahringer. MOA: [9] A. Bifet, G. Holmes, B. Pfahringer, and E. Frank. Fast [10] A. Bifet, G. Holmes, B. Pfahringer, and R. Gavald X . [11] S. B X rzs X nyi, D. Kossmann, and K. Stocker. The skyline [12] L. Breiman, J. Friedman, R. Olshen, and C. Stone.
 [13] J. Chipman. Compensation principle. In S. N. Durlauf and [14] C. Cortes and V. Vapnik. Support-vector networks. Machine [15] R. Durstenfeld. Algorithm 235: Random permutation.
 [16] L. Feng, F. Chen, and Y. Yao. A concept similarity based [17] J. Gama, R. S. ao, and P. Rodrigues. Issues in evaluation of [18] J. Han, J. Pei, Y. Yin, and R. Mao. Mining frequent patterns [19] J. Hicks. The foundations of welfare economics. The [20] R. Hof. Real-time advertising has arrived, thanks to Oreo and [21] C. Jin, K. Yi, L. Chen, J. Yu, and X. Lin. Sliding-window [22] N. Kaldor. Welfare propositions in economics and [23] R. Klinkenberg. Learning drifting concepts: Example [24] I. Koychev. Gradual forgetting for adaptation to concept [25] M. Masud, J. Gao, L. Khan, J. Han, and B. Thuraisingham. [26] M. Moreira, J. dos Santos, and A. Veloso. Learning to rank [27] M. N. nez, R. Fidalgo, and R. Morales. Learning in [28] F. Palda. Pareto X  X  Republic and the new Science of Peace . [29] M. Ribeiro, A. Lacerda, A. Veloso, and N. Ziviani.
 [30] I. Santana, J. Gomide, A. Veloso, W. M. Jr., and R. Ferreira. [31] D. Torres, J. Ruiz, and Y. Sarabia. Classification model for [32] A. Veloso, W. M. Jr., and M. Zaki. Lazy associative [33] A. Veloso, W. Meira Jr., M. Gon X alves, H. de Almeida, and [34] A. Veloso, M. Otey, S. Parthasarathy, and W. Meira Jr. [35] I.  X liobait  X  e. Learning under concept drift: an overview. [36] I.  X liobait  X  e, A. Bifet, G. Holmes, and B. Pfahringer. MOA [37] I.  X liobait  X  e, A. Bifet, B. Pfahringer, and G. Holmes. Active [38] I.  X liobait  X  e, A. Bifet, B. Pfahringer, and G. Holmes. Active [39] M. Zaki and K. Gouda. Fast vertical mining using diffsets. In [40] M. Zaki, S. Parthasarathy, M. Ogihara, and W. Li. New [41] X. Zhu, P. Zhang, X. Lin, and Y. Shi. Active learning from [42] Y. Zhu and D. Shasha. Efficient elastic burst detection in data
