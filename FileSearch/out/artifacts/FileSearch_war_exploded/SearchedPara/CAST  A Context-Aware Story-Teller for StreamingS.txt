 Online social streams such as Twitter timelines, forum discussions and email threads have emerged as important channels for informa-tion propagation. Mining transient stories and their correlations im-plicit in social streams is a challenging task, since these streams are noisy and surge quickly. In this paper, we propose CAST , which is a context-aware story-teller that discovers new stories from so-cial streams and tracks their structural context on the fly to build a vein of stories. More precisely, we model the social stream as a capillary network, and define stories by a new cohesive subgraph type called ( k,d ) -Core in the capillary network. We propose de-terministic and randomized context search to support the iceberg query, which builds the story vein as social streams flow. We per-form detailed experimental study on real Twitter streams and the results demonstrate the creativity and value of our approach. H.2.8 [ Database Management ]: Database Applications X  Data Min-ing ; H.3.3 [ Information Storage and Retrieval ]: Information and Retrieval X  Search process Measurement, Experimentation StoryVein; Context-aware; Story-teller; Social content
In the current social web age, the fast surge of online streaming social content, such as Twitter/Facebook timelines, blog posts, fo-rum discussions and email threads, may easily lead to  X  X nformation anxiety X , which is the gap between the information we receive and the information we are able to digest [11]. To relieve the informa-tion anxiety, there is an emergent need to organize streaming social content in a structural way and mine the  X  X eal gold X  out of noisy buzzes. There are many previous studies [14, 18, 22, 28] focus-ing on detecting new emerging events from social streams. They serve the need for answering  X  what X  X  happening now?  X . However, in reality, events usually do not happen in isolation, and existing studies fail to track the correlation between them. For example,  X  X rimea votes to join Russia X  (on May 6, 2014) and  X  X residen-t Yanukovych signs compromise X  (on February 21, 2014) are two separate events, but they are actually highly correlated under the same topic  X  X kraine Crisis X . In this paper, our goal is to design a context-aware story-teller for streaming social content, which not merely detects trending stories in a given time window of observa-tion, but also builds the  X  X ontext X  of each story by measuring its correlation with other stories on the fly. As a result, our story-teller has advantages on responding advanced user queries like  X  tell me related stories  X , which is crucial to help digest social streams.
Building a context-aware story-teller over streaming social con-tent is a highly challenging problem, as explained below:  X  The effective organization of social content . It is well known  X  Identification of transient stories from time window . Story  X  Story context search on the fly . Story correlation computation
To the best of our knowledge, no training data set for the context-aware story-telling on streaming social content is publicly avail-able, which renders the existing works [20, 24] on Story Link De-tection (SLD) not applicable, because SLD is trained on well-written news articles. Furthermore, we cannot apply topic tracking tech-niques [10, 8] to story context search, because topic tracking is usually formulated as a classification problem [2], with an assump-tion that topics are predefined before tracking, which is unrealistic for story context search on social streams.

To address above challenges, we propose CAST in this paper, which is a Context-Aware Story-Teller specifically designed for streaming social content. CAST takes a noisy social stream as the input, and outputs a  X  X tory vein X , which is a human digestible and evolving summarization graph by linking highly correlated stories. The major workflow of CAST is illustrated in Figure 1. First of all, we treat each post as a node and add a  X  X apillary X  edge between two posts if they are similar enough. For example,  X  X ustralian au-thorities update search for MH370 X  and  X  X ew MH370 search area is closer to Australia X  are two similar posts and we add a capil-lary edge between them. By this way, a capillary network will be constructed for social posts in the same observing time window. Second, we propose a new notion of ( k,d ) -Core to define a tran-sient story, which is a cohesive subgraph in a capillary network. In Figure 1: Illustrating the workflow of StoryVein , which has three major steps: (1) capillary network construction, (2) tran-sient story discovery and (3) story context search.
 a ( k,d ) -Core, every node has at least k neighbors and two end n-odes of every edge have at least d common neighbors. We propose two algorithms, Zigzag and NodeFirst , for the efficient discovery of maximal ( k,d ) -Cores from capillary network. After that, we define the iceberg query as finding highly correlated stories for a given story. Two approaches are proposed, deterministic context search (DCS) and randomized context search (RCS), to implement the iceberg query with high efficiency. The story vein is constructed based on the iceberg query and serves as the backend of context-aware story-teller on social streams, which discovers new stories and recommends related stories to users at each moment. Typi-cal users of CAST are daily social stream consumers, who receive overwhelming (noisy) buzzes and wish to digest them by an intu-itive and summarized representation in real time.

The main contributions of this paper are summarized below:  X  We introduce an efficient organization of streaming social con- X  We define a new cohesive subgraph called ( k,d ) -Core to rep- X  We propose deterministic and randomized context search to  X  Our experimental study on real Twitter streams shows that S-
Related work is discussed in Section 6. We conclude the paper in Section 7. Major notations are shown in Table 1. Social streams such as those found in microblogging sites like Twiter and discussion forums are typically noisy, are of large vol-ume, and surge quickly. In this section, we describe how we model social streams and formalize the main problem studied in this pa-per. We use the term post to denote each object in a social stream.
We begin by formalizing the social stream, social capillary and network as follows.

D EFINITION 1. (Social Stream) A social stream Q is a first-in-first-out queue of posts ordered by time of arrival, in which each post p is represented as a triple ( p T ,p  X  ,p u ) , where p content of p , p  X  is the time stamp, and p u is the author of the post.
The next definition assumes that we have a measure s ( p i gauge the similarity between a pair of posts p i and p j . Examples of post similarity are given below.

D EFINITION 2. (Social Capillary and Network) Given two posts p i ,p j in a social stream Q and a threshold (0 &lt;&lt; 1) , there is a social capillary between p i and p j if the post similarity s ( p i ,p j )  X  . The capillary network corresponding to Q is denot-ed as G ( V,E ) , where each node p  X  V is a post in Q , and each edge ( p i ,p j )  X  E is a social capillary with weight s ( p
Intuitively, a social capillary connects two posts if they are simi-lar enough. The capillary network can be viewed as a structural rep-resentation of the original unstructured social stream, by organiz-ing meaningful information from noisy buzzes. Specifically, posts with few capillaries can be essentially treated as noise and ignored. There are several key problems to tackle towards the effective mod-eling of social streams, as discussed below.
 Entity Extraction. Our goal is to design a processing strategy that can quickly judge what a post talks about and is robust enough to the informal writing style. In particular, we focus on entities in a post, since entities depict the story. However, traditional Named Entity Recognition tools [19] only support a narrow range of enti-ties like Locations, Persons and Organizations. Also, simply treat-ing each post text as a bag of words [17] will lead to loss of ac-curacy, since different words have different weights in describing a story. To broaden the applicability, we treat each noun in the post text as a candidate entity. For example, given a tweet  X  X Pad 3 battery pointing to thinner, lighter tablet? X , the entities are  X  X Pad X ,  X  X attery X  and  X  X ablet X . Technically, we obtain an entity list p a post p using a Part-Of-Speech Tagger 1 .
 Post Similarity. Traditional similarity measures such as TF-IDF based cosine similarity, Jaccard Coefficient and Pearson Correla-tion [17] only consider the post content. However, time stamps should play an important role in determining post similarity, since posts created closer together in time are more likely to discuss the same story than posts created at very different moments. We define the similarity between a pair of posts p i and p j by combing both content similarity and temporal proximity, that is where s T ( p T i ,p T j ) is a set-based similarity measure (e.g. Jaccard Coefficient [17]), and s  X  ( | p  X  i  X  p  X  j | ) is a distance measure mono-tonically decreasing with | p  X  i  X  p  X  j | /  X  t , where  X  t is the time win-dow sliding step size, typically in hours. Both s T ( p T s t s e Capillary Network Construction. In this paper, we assume that the social stream Q is monitored by a sliding time window of obser-vation. Since new posts flow in and old posts fade out at each mo-ment, the capillary network G ( V,E ) in the time window is dynam-ically updated, with new nodes/edges added and old nodes/edges removed. Removing a node and associated edges from G ( V,E ) is an easy operation. Let us investigate the case when a node is added.
POS Tagger, http://nlp.stanford.edu/software/tagger.shtml When a new post p flows in, we need to construct the capillaries be-tween p and nodes in V , following Definition 2. In a real scenario, since the number of nodes | V | can easily go up to millions, it is impractical to compare p with every node in V to verify the satis-faction of capillary. To solve this problem, we build inverted index on (entity, post) pairs and only check similarities between p and those posts sharing common entities with p .
In this paper, we propose CAST , an effective context-aware story-teller for social streams. As social posts flow in, CAST is able to discover new stories and track the  X  X ein X  between stories, in which each story is a group of highly similar posts telling the same thing in the social stream, and each vein is a correlation link be-tween two stories. We define a story vein as follows, where we use Cor ( S i ,S j ) to denote the correlation between stories S
D EFINITION 3. (Story Vein) Given a social stream Q and a threshold  X  ( 0 &lt; X &lt; 1 ), the output of CAST can be represent-ed by a directed graph G ( V , E ) , where each node S  X  V story, and each edge ( S i ,S j )  X  E means the story correlation Cor ( S i ,S j )  X   X  and S i happens earlier than S j .
The motivation behind CAST is, real world stories are not iso-lated but commonly correlated with each other. Intuitively, the con-text of S in G ( V , E ) is the neighboring stories of S . In particular, we use upper and lower context to represent the stories connected by incoming and outgoing edges of S , respectively. The target of CAST is to discover new stories and build the contexts of these sto-ries. As the social stream Q gets updated over time, the story vein G (
V , E ) will be also updated in real time. The dynamic nature of context-aware story-telling raises challenges both in the discovery of new stories and in the tracking of story context. In the follow-ing, we will address these challenges by discussing transient story discovery in Section 3 and story context tracking in Section 4.
In this section, we describe the motivation and algorithms for identifying transient stories as a new kind of cohesive subgraph, ( k,d ) -Core, from the capillary network.
Edge weights in a capillary network have very natural semantics: the higher the post similarity, the more likely two posts are to talk about the same story. It is well-known that a cohesive subgraph is a sub-structure with high edge density and very low internal com-mute distance [27]. Suppose S ( V S ,E S ) is a cohesive subgraph in G ( V,E ) . Since nodes in S ( V S ,E S ) have a high density to connect with each other, it is very likely that all nodes in S ( V the same content and tell the same story. Based on this observa-tion, we model a story in social stream as a cohesive subgraph in capillary network in this paper.
 There are many alternative ways to define a cohesive subgraph. Clique may be the best candidate in spirit, because any two nodes have a sufficiently high similarity and thus share the same content. However, in real world data sets, cliques are too restrictive for story definition and this calls for some relaxations. There are several choices for clique relaxations, e.g., quasi-clique, k -Core, k -Plex, etc [13]. Given a connected subgraph S ( V S ,E S ) , supposing N ( p ) is the neighbor set of node p  X  V S in S ( V S ,E S ) , these clique relaxations are defined as:  X  Quasi-clique: | N ( p ) | X   X  ( | V S | X  1) for every post p  X  k -Plex: | N ( p ) | X | V S | X  k for every post p  X  V S ;  X  k -Core: | N ( p ) | X  k for every post p  X  V S .

Notice that finding the maximum clique, quasi-clique or k -Plex from a graph is an NP-Hard problem [7, 4]. Even worse, [9] proved that there are no polynomial algorithms that provide any reasonable approximation to the maximum clique problem. Since a clique is a special case of quasi-clique or k -Plex, these hardness results car-ry over to maximum quasi-clique and maximum k-Plex problems. There are a lot of heuristic algorithms that provide no theoretical guarantee on the quality [7, 1, 4]. Since most of them are based on local search [7], they do not scale to large networks, because local search involves the optimization of solutions by iteratively moving to a better neighbor solution in an exponential search space.
The good news is that k -Cores can be exactly found in polyno-mial time. By adjusting k , we can generate k -Cores with desired edge density 2 | E S | / | V S | . For example, increasing k will improve edge density because the minimal edge degree is increased and is decreased in the same time. However, k -Cores are not always capable of capturing our intuition on stories: although each post has at least k neighbors, the fact that two posts are connected by a single capillary may be not a strong enough evidence to prove that they tell the same story. Sometimes, posts only share some common words but discuss different stories, e.g.,  X  X oogle acquires Motorola Mobility X  and  X  X ell Mobility acquires Virgin Mobile X . We show an example of 3-Core in Figure 2(a), where p 1 and p are connected but they belong to two separate cliques. To address this challenge, we make a key observation that the existence of more common neighbors between two capillary-connected posts suggest-s a stronger commonality in story-telling . Supposing p i connected by a capillary and there exists post p l  X  N ( p then we call p l a witness for the post similarity s ( p i ture this intuition in the following definition, where we formalize a story as a ( k,d ) -Core.

D EFINITION 4. (Story) A story in social stream Q is defined by a maximal ( k,d ) -Core S ( V S ,E S ) in capillary network G ( V,E ) associated with Q , where k,d are numbers with k&gt;d&gt; 0 and  X  S ( V S ,E S ) is a connected subgraph;  X  For every post p  X  V S , | N ( p ) | X  k ;  X  For every edge ( p i ,p j )  X  E S , | N ( p i )  X  N ( p Why ( k,d ) -Core? In ( k,d ) -Cores, we use k to adjust the edge density, and use d to control the strength of similarity witness. It is easy to see that a maximal ( k,d ) -Core is a subgraph of a maxi-mal k -Core. However, compared with k -Cores, ( k,d ) -Cores have a more cohesive internal structure enhanced by at least d common neighbors as witnesses of commonality between two nodes con-nected by an edge. This enhancement makes posts in ( k,d ) -Core more likely to tell the same story. We show an example of 3-Core and (3,1)-Core in Figure 2(b). As we can see, p 4 is in 3-Core but not in (3,1)-Core, because the similarity between p 1 and p witnessed by other posts. Besides, [21] defines a kind of cohesive subgraph called k -Dense, in which every edge has at least k wit-nesses. It is easy to infer that a k -Dense is a ( k +1 ,k ) -Core. Thus, k -Dense is a special case of ( k,d ) -Core, but provides no flexibility to adjust k and d independently. Therefore, our proposed ( k,d ) -Core is better than k -Core and k -Dense in capturing a story. Non-overlap Property. Similar to maximal k -Cores, a maximal ( k,d ) -Core cannot be a subgraph of any other ( k,d ) -Cores. Maxi-mal ( k,d ) -Cores have the following important property.
P ROPOSITION 1. The maximal ( k,d ) -cores of a graph are pair-wise disjoint. Figure 2: (a) A 3-Core without similarity witness between p and p 2 . (b) The illustration of generating a (3 , 1) -Core from a 3-Core.

We now discuss the computation of ( k,d ) -Cores. We first review the k -Core generation process [13]: given a network G ( V,E ) , it-eratively remove the nodes with degree less than k from G ( V,E ) , until all the remaining nodes have a degree at least k . The result will be a set of maximal k -Cores, which are obtained in polyno-mial time. The k -Core generation process forms the basis of our ( k,d ) -Core generation algorithms. We propose the first solution for ( k,d ) -Core generation in Algorithm 1. We call it a Zigzag algo-rithm, because the basic idea is to repeatedly change the property of the current network between two states: the first state is k -Core set G 1 obtained by removing nodes recursively, and the second state is ( d +1 ,d ) -Core set G 2 obtained by removing edges recursively. This Zigzag process will terminate if each connected component in the result set is a ( k,d ) -Core, or the result set is empty. It is easy to see Algorithm 1 takes polynomial time and the result is exact.
We notice that the transition costs between two states are not symmetric: the computational costs of G 1 and G 2 are very differ-ent. If we can reduce the frequency of the transition with a high-er overhead, the overall performance of Zigzag can be optimized. The following lemma formalizes the property.

P ROPOSITION 2. Given a network G ( V,E ) with | V | &lt; and integers k , d ( k&gt;d ), for each iteration in Zigzag , the com-putation of k -Core set G 1 is more efficient than the computation of ( d +1 ,d ) -Core set G 2 .

Proposition 2 suggests that if we reduce the computation fre-quency of the ( d +1 ,d ) -Core set G 2 , the performance will be im-proved. Following this, we propose Algorithm 2 to improve the performance of Algorithm 1, by applying node deletions as much as possible. The heuristic is, whenever an edge is deleted, we check whether this deletion makes the degree of its end nodes smaller than k , and if it does, a recursive node deletion process starting from end nodes will be performed (Line 7). We call Algorithm 2 NodeFirst because it greedily invokes the node deletion process when possi-ble. Since NodeFirst avoids to perform a complete edge deletion process as Zigzag did, the network converges very fast to the set of maximal ( k,d ) -Cores. The following propositions indicate that Algorithm 1: ( k,d ) -Core Generation: Zigzag Input : G ( V,E ) ,k,d
Output : All maximal ( k,d ) -Cores Algorithm 2: ( k,d ) -Core Generation: NodeFirst Input : G ( V,E ) ,k,d
Output : All maximal ( k,d ) -Cores NodeFirst produces exactly the same results as Zigzag , but its per-formance is better than Zigzag .

P ROPOSITION 3. Given a network G ( V,E ) and numbers k , d ( k&gt;d ), both Zigzag and NodeFirst generate the same result, which is the set of all maximal ( k,d ) -Cores in G ( V,E ) .
P ROPOSITION 4. On each iteration, NodeFirst is more effi-cient than Zigzag .

Transient stories we identified from the capillary network may be highly correlated, e.g.,  X  X he launch of Blackberry 10 X  and  X  X lack-Berry Super Bowl ad X . In this section, we exploit and integrate sig-nals from different measures in story correlation computation, and propose deterministic context search (DCS) to construct the veins for a story S on the fly. The performance of DCS is proportional to the size of S  X  X  neighboring posts. In the case when the neighboring post size is huge, we propose randomized context search (RCS) to boost the performance.
Recall that stories correspond to ( k,d ) -cores in the capillary net-work and let S i ( V i ,E i ) and S j ( V j ,E j ) denote two stories. Here we introduce different types of correlation dimensions, which cap-ture the story correlation from different perspectives, and quantify the correlation by a value in [0 , 1] . Notice that node overlap is a very common evidence to assess the correlation between two sub-graphs [25]. However, since Proposition 1 shows that ( k,d ) -Cores generated by Zigzag or NodeFirst are pairwise disjoint in nodes, node overlap is not useful in story correlation computation. Dimension 1: Content Similarity. By viewing a story as a doc-ument and a post entity as a term, existing document similarity measures can be used to assess the story correlation. However, TF-IDF based Cosine similarity fail to be effective, since TF vec-tors of stories tend to be very sparse. We exploit another popular measure, LDA-based symmetric KL-divergence [20]. Supposing d is the document representation of story S and  X  ( d ) is the topic distribution of d produced by LDA, we have where KL (  X  ( d i )  X  ( d j )) = x  X  x ( d i )log  X  x ( d i Dimension 2: Temporal Proximity. Stories that happen closer in time are more likely to correlate together. Given a story S we can get the histogram of post volume along time dimension on each bin with size equal to the time window sliding step  X  t . Sup-posing Dist is the normalized distribution (with the sum equal to 1) of story S  X  X  histogram, we take the complement of L 1 an example to measure the temporal proximity between S i and S where len is the length of the observation time window. Dimension 3: Edge Connectivity. Capillaries associated with posts of two stories can be used to determine the story correlation. This approach calculates the strength of edge connectivity between posts of S i and S j in capillary network. These edges serve as the bridge between two stories, and the connectivity strength can be measured by various ways, e.g., the Jaccard Coefficient based on the portion of bridge edges: where E ( A, B ) is the edge set between node set A and B . Baseline: Hybrid Correlation Model. As a baseline approach, story correlation can be computed by a hybrid model on all corre-lation dimensions, with the form:
It is easy to see that 0  X  Cor H ( S i ,S j )  X  1 . One drawback of the hybrid correlation model is the performance. First, LDA com-putation for large corpus is expensive [26]. Second, to construct the context of story S in the story vein, the hybrid correlation mod-el needs to compute Cor H ( S, S ) between S and every other story S in G ( V , E ) , which is redundant and time-consuming.
Story context search aims at finding the neighbors of a story in the story vein G ( V , E ) efficiently. However, simply applying the hybrid correlation model shown in Eq.(5) results in an inefficient pairwise computation. To overcome the challenge, we introduce iceberg query on story vein, as stated below.
 D EFINITION 5. (Iceberg Query) Given a story vein G ( V , a threshold  X  (0 &lt; X &lt; 1) and a story S ( S  X  V ) , the iceberg query for S is to find the subset of all stories V S  X  V , where for each S  X  V S , Cor ( S, S )  X   X  .

Iceberg query is the key technique to support CAST . Iceberg query grows the story vein on the fly: for each new story S , it can quickly build possible correlation links between S and the remain-ing stories. Iceberg query does not need to explore all stories in to generate the exact V S , which improves the performance.
In this section, we propose two approaches to implement iceberg query on story vein. The first approach is deterministic, which inte-grates content similarity, temporal proximity and edge connectivity together by aggregating all capillaries between two stories. The second approach is randomized, which improves the performance of the deterministic approach by filtering the flow from S to S . They are discussed separately below.
 Deterministic Context Search. The basic idea of deterministic context search (DCS) follows a propagation and aggregation pro-cess. To initialize, we treat story S ( V S ,E S ) as source and every other story S ( V S ,E S ) as target . In the propagation phase, we start from each post in source and broadcast the post similarity to neighboring posts along capillaries. In the aggregation phase, we aggregate the post similarity received by posts in each target , and denote it by PA D ( S, S ) . In detail, we have
Since s ( p, p ) is computed by combining content similarity and temporal proximity (see Eq.(1)), PA D ( S, S ) naturally integrates the above discussed three story correlation dimensions. The corre-lation between S and S is assessed by the ratio between the post similarity aggregated by S and propagated by S : where PA D ( S, G ) is the post similarity sum of all capillaries as-sociated with posts in S . When Cor D ( S, S )  X   X  , there will be a story link between S and S in story vein G ( V , E ) . Let S  X  de-note the average time stamp of S , i.e., S  X  = 1 | V S | p S  X  &lt;S  X  , the edge direction is from S to S . Otherwise, the edge direction is from S to S .

The sketch of DCS is shown in Algorithm 3. Notice that post similarity is not aggregated on every story. Some stories may re-ceive zero or little post similarity, and can be omitted. On average, a source story visits at most min {| V | , | S V | X | E | | in DCS. Compared with the hybrid correlation model, which re-quires to access the whole capillary network G ( V,E ) in pairwise computation, DCS improves the performance significantly. Randomized Context Search. To improve the performance of the deterministic context search further, we propose randomized con-text search (RCS) based on random walk. The motivation behind RCS is, in the propagation phase, it is preferable to only visit the neighboring posts with a high post similarity. Given a post p in sto-ry S , suppose max( p ) and min( p ) are the maximum and minimum Algorithm 3: Deterministic Context Search (DCS) Input : G ( V,E ) , G ( V , E ) , new story S ( V S ,E S ) ,  X 
Output : In-neighbor set N I ( S ) , out-neighbor set N O post similarity associated with p , respectively. Technically, we use  X  ( 0  X   X   X  1 ) as a throttle to propagate high post similarities: on-ly if s ( p, p )  X  min( p )+  X  (max( p )  X  min( p )) , RCS propagates s ( p, p ) to p by a uniformly random probability.

There are two special cases in RCS:  X   X  =0 : Post p propagates to every neighboring post.  X   X  =1 : Post p only propagates to the neighboring post with
We empirically choose  X  =0 . 5 . Suppose N is the total number of simulations we run for the source story S . On each simulation, a random surfer starts from a post p in S and randomly visits a neighbor p if s ( p, p )  X  min( p )+  X  (max( p )  X  min( p )) . The aggregated post similarity from source S to target S on simulation i can be described as
The aggregated post similarity by S on N simulations is denot-ed by PA R ( S, S ) :
We show the sketch of RCS in Algorithm 4. Similar to DCS, the correlation between S and S is computed by Cor R ( S, S )=
PA R ( S,G ) . The vein threshold and direction is determined by the same way of DCS. Clearly, in RCS, a source story visits at most N neighboring posts and usually N | V | . The value of N is decided by the time budget of story context search in real CAST system. Compared with DCS, RCS achieves better performance by accessing smaller number of neighboring posts connected by capillaries with higher post similarity.
Since story vein is described in graph notation, it becomes very important to explain the story vein to end users who may be non-technical. There are various options to present a ( k,d ) -Core in hu-man consumable form. The first option is annotating a ( k,d ) -Core by the most representative posts inside, e.g., posts with the highest edge degree. The second option is rendering a ( k,d ) -Core into a Algorithm 4: Randomized Context Search (RCS) Input : G ( V,E ) , G ( V , E ) , new story S ( V S ,E S ) ,  X  ,  X  , n
Output : In-neighbor set N I ( S ) , out-neighbor set N O word cloud. Since we extracted entities from each post, the fre-quency of an entity in a ( k,d ) -Core can be used to indicate the font size of that entity in the word cloud. In this paper, we provide both options to aid human perception. We will show some interpretation examples in experimental study. All experiments are conducted on a computer with Intel 2.66 GHz CPU, 8 GB RAM, running 64-bit Windows 7. All algorithms are implemented in Java.
 Parameter Setting. We empirically set the threshold =  X  =0 . 2 for the construction of capillary network and story vein. We set  X  =0 . 5 to filter out capillaries with low post similarity in RCS. All data sets are crawled from Twitter.com via Twitter4J 2 Although our story teller CAST works regardless of the domain, we make the data set domain-specific in order to facilitate evalua-tion of the generated results.
 CNN-News . By simulating a social news stream, we collect tweets created in the first half year of 2014 from CNN channels, which include @cnn, @cnnbrk, @cnnmoney, @cnnlive and @cnni. This data set has 10872 tweets and serves for the quality analysis. Tech-Lite . We built a technology domain dataset called Tech-Lite by aggregating timelines of users listed in Technology category of  X  X ho to follow X  3 and their retweeted users. Tech-Lite has 352,328 tweets, 1402 users and the streaming rate is 11700 tweets/day. Tech-Full . Based on the intuition that users followed by users in Technology category are most likely also in the technology domain, we obtained a larger technology social stream called Tech-Full by collecting all the timelines of users that are followed by users in Technology category. Tech-Full has 5,196,086 tweets, created by 224,242 users, whose streaming rate is about 7216 tweets/hour.
Both Tech-Lite and Tech-Full span from Jan. 1 to Feb. 1, 2012.
Twitter4J. http://twitter4j.org/ http://twitter.com/who_to_follow/interests Table 2: The number of edges in the capillary network, and the number of stories and correlation links in the story vein as the changing of temporal proximity functions. We define a story as a (5 , 3) -Core.

Rank HashtagPeaks MentionPeaks EntityPeaks 1 #CES  X  X  X  X  google 2 #SOPA  X  X  X  X  X  X  X  X  X  ces 3 #EngadgetCES  X  X  X  X  X  apple 4 #opengov  X  X  X  X  X  X  video 5 #gov20  X  X  X  X  X  X  X  X  sopa 6 #CES2012  X  X  X  X  X  twitter 7 #PIPA  X  year 8 #opendata  X  X  X  X  X  X  X  X  X  X  X  facebook 9 #StartupAmerica  X  app 10 #win7tech  X  X  X  X  X  X  X  X  X  X  X  X  X  X  iphone Precision 0.5  X  X  X  0.7 Recall 0.4  X  X  X  0.5 Figure 3: Top 10 results of HashtagPeaks, MentionPeaks and EntityPeaks on Tech-Lite dataset. The ground truth for pre-cision and recall is top 10 major stories selected from main stream technology news websites.
Post similarity computation shown in Eq. (1) influences the structure of capillary network directly. We can tune the content similarity function s T ( p T i ,p T j ) and temporal proximity function s ( | p  X  i  X  p  X  j | ) to make the capillary network concise and expressive. Many set-based similarity measures such as Jaccard coefficient [17] can be used to compute the similarity s T ( p T i ,p T j ) between posts. Since entity usually appears once in one tweet, similarity measures such as Cosine similarity and Pearson correlation [17] will degen-erate to a form very similar to Jaccard, so we use Jaccard as our similarity function and omit further discussion of alternatives.
Temporal proximity function s  X  ( | p  X  i  X  p  X  j | ) determines how sim-ilarity to older posts is penalized, compared to recent posts. We compared three different functions: (1) reciprocal fading ( X  X eci-Fading X ) with D ( | p  X  i  X  p  X  j | )= 1 | p  X  ( X  X xp-Fading X ) with D ( | p  X  i  X  p  X  j | )= e  X  X  p  X  i  X  ( X  X o-Fading X ) with D ( | p  X  i  X  p  X  j | )=1 . For any posts p in the old part of time window severely (see Table 2): the number of capillaries and stories generated by Exp-Fading is lower than by other approaches. Since No-Fading does not penalize old posts in the time window, too many capillaries and stories will be generated without considering recency. Reci-Fading is in between, which is a more gradual penalty function than Exp-Fading. In the rest of ex-periments, we use Exp-Fading, since it generates the most concise capillary network with an emphasis on recent posts.
In this section, we evaluate the quality of CAST on two tasks: story discovery and context search, by comparing with baselines. Ground Truth. We build the ground truth of tech stories in Jan 2012 by browsing news articles on main stream technology web-sites like CNET.com, TechCrunch.com, and ReadWrite.com, etc. We pick headlines with high occurrence and build a ground truth of top 10 stories. They include CES related stories, SOPA &amp; PIPA related stories, Facebook IPO, Yahoo co-founder Jerry Yang resig-nation, RIM CEO changes, etc.
 Baseline 1: Peak Detection. Some recent work on event detec-tion [14, 18, 23] can be used for story discovery, and they share the same spirit that aggregates the frequency of topic-indicating phras-es at each moment to build a histogram and generates stories by detecting volume peaks in the histogram. We design three base-lines to capture the major techniques used by these approaches.  X  HashtagPeaks : aggregate frequent hashtags;  X  MentionPeaks : aggregate frequent mentions.  X  EntityPeaks : aggregate frequent entities.

We show top 10 results of HashtagPeaks, MentionPeaks and En-tityPeaks in Tech-Lite dataset with time stamps between Jan 1 and Feb 1, 2012 in Figure 3. Their precision and recall are also list-ed by comparing with the ground truth. Notice that multiple peaks may correspond to the same story in ground truth, where the pre-cision and recall differ. As we can see, MentionPeaks is the worst because most of these mentions are not topic-indicating phrases. Hashtags are the  X  X witter X  way to indicate an event, but it requires manual assignation by human. EntityPeaks is the best out of three baselines, since entities are extracted from the social stream pre-processing stage to annotate stories. Although these baselines are able to indicate some words about a story, they X  X e not qualified for a successful story-teller because the internal and external structure of the story is missing. In particular, they cannot show how users interact with each other, how posts are clustered, and how the story is distributed along time dimension.
 Baseline 2: Topic Modeling. Topic modeling is a typical way to detect topics from text document corpus. As we mentioned earli-er, existing work on topic detection and tracking falls into a clas-sification problem, and prevalent topic modeling models like La-tent Dirichlet Allocation (LDA) [6] are mainly trained on formal-writing news articles. We design a baseline using LDA, and treat top 100 posts of each topic as a story. Post time stamps are totally ignored in LDA. We set the topic number as 50, and after rendering topics to stories, we show the top 20 stories in Table 3. As we can see, the topic cohesion of LDA-detected stories is not very high: posts sharing some common words are very easily classified into the same topic, even though they are not exactly talking about the same story. Besides, LDA model cannot deal with noisy posts, so the quality is compromised in social streams.
 CAST on Tech-Lite. Recall that our proposed story-teller CAST uses a ( k,d ) -Core to represent a story, which is a cohesive sub-graph in the capillary network. Figure 4 shows top 10 transient sto-ries generated by CAST . Each story is represented as a (5,3)-Core. It is worth noting that there are various ways to present stories on the user interface. In this experiment, we render a story into an en-tity cloud. Each cloud is annotated by a short description beside. Interestingly, we observe the curve of tweet frequency goes down on every weekend, which reflects the living habit in reality. Com-pared with the ground truth, our story-teller achieves a precision of 0.9 and a recall of 0.8. For precision, the only case we fail is that  X  X ug/hope new year X  is not a story in ground truth. The reason may be lots of people tweeting about  X  X appy new year X  but this is not formally reported in technology news websites, since it is a well-known fact. For recall, the ground truth story not appearing in our top 10 is  X  X pple iBooks 2 Release X  on Jan 19. This story is ranked on the 13th position in our story-teller. the bottom is the breakdown of tweet frequency on each day in Jan 2012. Table 3: Top 20 stories detected by LDA from Tech-Full and described by high frequency words. We treat top 100 posts of each topic as a story.

We can see that stories may be correlated with each other. In Fig-ure 4, there are multiple stories about CES (Consumer Electronic Show) 2012, held from Jan 8 to Jan 13. Meanwhile, stories related to SOPA &amp; PIPA are highly correlated, even though each of them tells a relatively different story.

When inspecting the top 50 transient stories returned by CAST , we observe two major advantages compared with news reports. First, our story-teller can identify stories that are hardly noticeable in news websites. For exmple,  X  X upert Murdoch joins Twitter X  is a story widely spread in social streams, but not commonly reported by news articles. Second, the formation of a transient story in so-cial streams generally occurs earlier than the first publishing of the corresponding news articles. [14] also observed this phenomenon, and the typical time lag is around 2.5 hours.
 CAST on Tech-Full. Figure 6 shows an example to illustrate the s-tory vein on Tech-Full. To help understand, we group stories dense-ly connected by vein links in rectangles, where each rectangle can be viewed as a story series. As we can see,  X  X ES X  and  X  X OPA &amp; PIPA X  are two very different story series, but they can be connected by indirect links via posts such as  X  X ES 2012: Microsoft Keynote With Steve Ballmer X  and  X  X icrosoft opposes SOPA X . CAST will Figure 5: A fragment of story vein tracked from CNN-News, which has a time span from January 1 to June 1, 2014. track the correlation between stories and build a highly informative story vein for story-teller.
 CAST on CNN-News. CNN-News simulates the daily social stream received by a Twitter user and has a time span of six months. We show a fragment of the story vein tracked from CNN-News in Fig-ure 5. In this example,  X  X H370 lost and search X  is a story series with an internal structure, by organizing individual stories together. Users can click on each story to see the details, or traverse along story veins to read highly related stories happening earlier or later. Compared with Twitter Trends Search 4 which shows trending top-ics, CAST has distinct advantages on providing both the internal structure and the external context of a specific story.
In this section, we test the performance of our proposed algo-rithms. All tests are performed on Tech-Full dataset with the time window set to one week. On average, the capillary network has a node size 710,000 and edge size 4,020,000.
 Story Identification. Recall that both k -Core and ( k,d ) -Core gen-erations have polynomial time complexity. We test the running time of generating all maximal k -Cores and ( k,d ) -Cores from the capil-lary network and show the result in Figure 7(a). Let k = d +1 .As d increases, the running time of k -Core generation drops. The rea-son is, although a bigger k means more nodes need to be removed at each iteration, the total number of iterations drops even more quickly. The running time for ( k,d ) -Cores is nearly stable, but much higher than k -Core generation. This observation implies that NodeFirst should be much faster than Zigzag , since NodeFirst performs k -Core generation whenever possible. This conclusion is supported by the experimental results in Figure 7(a). https://twitter.com/search-home Figure 6: An example to illustrate our context-aware story-teller. Each tag cloud here is a single story identified from the capillary network. Sets of stories with higher correlation are grouped together in rectangles to aid readability.

The relationship between d and the number of connected com-ponents we can find from the capillary network is discovered in Figure 7(b). We see that with increasing d , both the numbers of ( d +1) -Cores and ( d +1 ,d ) -Cores drop. Since a ( d +1 ,d ) -Core is at least a ( d +1) -Core but more cohesive, we may find multiple ( d +1 ,d ) -Cores from a ( d +1) -Core by removing edges that do not satisfy the ( k,d ) -Core definition. Thus, given the same capil-lary network, the number of ( d +1 ,d ) -Cores is usually higher than the number of ( d +1) -Cores.
 Iceberg Queries. In CAST , we use iceberg query to construc-t possible vein links between a given story and all other stories in the capillary network. As discussed in Section 4, we treat the given story as source and all other stories as targets, and the story corre-lation threshold  X  will govern the construction of story vein links.
In Figure 7(c), we treat each story in the capillary network as the query and show the total running time of iceberg queries for all stories. As d increases, we observe that the number of ( d +1 ,d ) -Cores decrease gradually from Figure 7(b). This will make the total size of transient stories smaller and naturally the total running time of iceberg queries decrease. In experiments, we can see that the performance of RCS is remarkably better than the performance of Table 4: The accuracy of RCS, as the number of simulations n grow. We define n as a number proportional to the neighboring post size of story S and measure the accuracy based on DCS. DCS. For the quality, we show the accuracy of RCS in Table 4, as the number of simulations n grow. We define n as a number proportional to the neighboring post size of story S and as we can see, 20% simulations alrady produce an accuracy higher than 95%.
The related work of this paper falls into one of the following four categories.
 Social Stream Mining. Mining social streams has gained popular-ity in recent years due to the rise of social media like Twitter and Facebook. The most relevant work in this category is event detec-tion. Weng et al. [28] build signals for individual words by ap-plying wavelet analysis on the frequency based signals of words to detect events from Twitter. A framework for tracking short, distinc-tive phrases (called  X  X emes X ) that travel relatively intact through on-line text was developed in [14]. Twitinfo [18] represents an event it discovers from Twitter by a timeline of related tweets. [22] investigated the real-time interaction of events such as earthquakes in Twitter and proposed an algorithm to monitor tweets and to de-tect a target event based on classifiers. Compared with the above, our approach represents a transient story as a cohesive subgraph in post network, which has a compact internal structure to describe the same story. [23] and [3] take a different approach by represent-ing an event as a small weighted graph of entities. One drawback is that since entity occurrences in multiple posts are aggregated into a single entity node and post similarities are aggregated numeri-cally on edge weights, post attributes like time stamps cannot be reflected on entity graphs.
 Topic Detection and Tracking. Topic detection and tracking is an extensively studied field [16], with the most common approach-es based on Latent Dirichlet Allocation (LDA) [6]. Techniques on topic detection and tracking cannot be applied to story correlation tracking, because they are usually formulated as a classification problem [2], with an assumption that topics are predefined before tracking, which is unrealistic for social streams. Recent works [10, 8] prey to this problem. Besides, the lack of training data set for story correlation tracking on noisy social streams renders the ex-isting works [20, 24] on Story Link Detection (SLD) inapplicable, because SLD is trained on well-written news articles.
 Cohesive Subgraph. There are several choices for clique relax-ations, e.g., quasi-clique, k -Core, k -Plex, etc [13]. Out of them, finding the maximal quasi-clique and k -Plex are NP-Hard problem-s [7, 4] and furthermore, the inapproximability result of cliques [9] carries over to maximal quasi-cliques and k-Plexes. k -Cores can be computed in polynomial time, but in terms of semantics, they lack evidence to show that every two nodes in a k -Core are very likely describing the same story. In this paper, we define a new type of cohesive subgraph called ( k,d ) -Core for better capture of stories. Correlation Computation. The correlation between nodes in a graph is a well-studied problem, with popular algorithms such as HITS, Katz and Personalized PageRank [5]. For the correlation be-tween cohesive subgraphs defined as ( k,d ) -Cores, traditional mea-sures like Jaccard coefficient, Cosine similarity and Pearson X  X  Cor-relation [17] are not effective, because maximal ( k,d ) -Cores do not overlap on node sets. Recently, a propagation and aggregation process can be used to simulate the information flow between n-odes, which was studied in the context of top-k structural similarity search in [12] and authority ranking in [15]. In this paper, we pro-pose our context search for iceberg query based on the propagation and aggregation process on the subgraph level.
In this paper, we focus on two problems: (1) Efficiently identify transient stories from fast streaming social content; (2) Perform Ice-berg query to build the structural context between stories. To solve the first problem, we transform social stream in a time window to a capillary network, and model transient stories as ( k,d ) -Cores in the capillary network. Two polynomial time algorithms are proposed to extract maximal ( k,d ) -Cores. For the second problem, we pro-pose deterministic context search and randomized context search to support the iceberg query, which allows to perform context search without pairwise comparison. We perform detailed experimental study on real Twitter streams and the results demonstrate the ef-fectiveness and value of our proposed context-aware story-teller CAST . In future work, we are interested in mining opinions from transient stories, e.g., the sentiment on a political event. Besides, we are interested in various visualization techniques to present tran-sient stories to end users in a friendly way. [1] J. Abello, M. G. C. Resende, and S. Sudarsky. Massive [2] J. Allan, editor. Topic detection and tracking: event-based [3] A. Angel, N. Koudas, N. Sarkas, and D. Srivastava. Dense [4] B. Balasundaram, S. Butenko, and I. V. Hicks. Clique [5] P. Berkhin. Survey: A survey on pagerank computing. [6] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet [7] M. Brunato, H. H. Hoos, and R. Battiti. On effectively [8] Z. Gao, Y. Song, S. Liu, H. Wang, H. Wei, Y. Chen, and [9] J. Hastad. Clique is hard to approximate within n 1  X  [10] Y. He, C. Lin, W. Gao, and K.-F. Wong. Tracking sentiment [11] B. Kovach and T. Rosenstiel. Blur: How to Know What X  X  [12] P. Lee, L. V. S. Lakshmanan, and J. X. Yu. On top-k [13] V. E. Lee, N. Ruan, R. Jin, and C. C. Aggarwal. A survey of [14] J. Leskovec, L. Backstrom, and J. M. Kleinberg.
 [15] P. Li, J. X. Yu, H. Liu, J. He, and X. Du. Ranking individuals [16] N. Liu. Topic detection and tracking. In Encyclopedia of [17] C. D. Manning, P. Raghavan, and H. Schuetze. Introduction [18] A. Marcus, M. S. Bernstein, O. Badar, D. R. Karger, [19] D. Nadeau and S. Sekine. A survey of named entity [20] T. Nomoto. Two-tier similarity model for story link [21] K. Saito, T. Yamada, and K. Kazama. The k-dense method to [22] T. Sakaki, M. Okazaki, and Y. Matsuo. Earthquake shakes [23] A. D. Sarma, A. Jain, and C. Yu. Dynamic relationship and [24] C. Shah, W. B. Croft, and D. Jensen. Representing [25] D. Shahaf, J. Yang, C. Suen, J. Jacobs, H. Wang, and [26] D. Sontag and D. Roy. Complexity of inference in latent [27] N. Wang, S. Parthasarathy, K.-L. Tan, and A. K. H. Tung. [28] J. Weng and B.-S. Lee. Event detection in twitter. In ICWSM ,
