 Recently, learning to rank algorithms have become a popular and effective tool for ordering objects (e.g. terms) according to their degrees of importance. The contribution of this paper is that we propose a simple and fast learning to rank model RankBayes and embed it in the co-training framework. The detailed proof is given that Na X ve Bayes algorithm can be used to implement a learning to rank model. To solve the problem of two-model inconsistency, an ingenious approach is put fo rward to rank all the phrases by making use of the labeled resu lts of two RankBayes models. Experimental results show th at the proposed approach is promising in solving ranking problems. H.3 [ Information storage and retrieval ]: Content Analysis and Indexing Algorithms, Experimentation Learning to rank, Na X ve Bayes, Co-training framework, Keyphrase extraction Ranking, which is to order objects according to their degrees of importance, is a key problem to many applications, such as information retrieval, document summarization, collaborative filtering and so on. Recently th e learning to rank methods are widely used for ranking tasks [1]. The commonly used approaches are pairwise, which model all the object pairs with a binary classification problem and establish the relative importance relations between two objects [2]. To get a better performance, however, pairwise learning to rank approaches still need a larger corpus of training data. Joachims collected the training data automatically by analyzing the c lick-through logs and used them to train a pairwise learning to rank method  X  Ranking SVM in search engine [4]. For the keyphrase extraction task, it is difficult to get a complete ranking of all phrases as training corpus, though a few documents labeled with ke yphrases can be collected. To apply the learning to ranking st rategy, we can collect all the obvious each keyphrase has higher relative importance than non-keyphrases. Jiang also verifies th at the learning to rank method is more appropriate for keyphrase extraction than the conventional classification methods [3]. To make full use of the training data and further enhance the learning to rank model, the intro duction of co-training framework [1] where two base rankers learn each other, is a better option. In this paper, we propose to appl y the co-training idea on two learning to rankers for the keyphrase extraction task. However, the co-training learning to rank framework should consider two problems: co-training speed and two-model inconsistency. It is known that the co-training process needs a number of iterations to reach the convergence. Most learning to rank models like RankSVM and RankNet, which respectively use SVM and Artificial Neutral Network, cost a great deal of time in each iteration. To speed up the co-training process, we put forward a simple learning to rank model -RankBayes, which adopts Naive Bayes to rank the relative importance of any two objects. Herbrich mentioned that the traditional Bayesian approaches may not be fit for a learning to rank model, because the properties of transitivity and asymmetry may be violated due to the problem of stochastic transitivity [2]. However, for our specific keyphrase extraction task, we can give a detailed proof that the Na X ve Bayes algorithm can be used to implement a learning to rank model, meaning that it will not yield inconsistent relative importance relations. Next, we propose the CoRankBayes model, which embeds two RankBayes models under the co-training framework. Two RankBayes models may pr oduce inconsistency among the relative importance relations. Fo r example, for two objects a and b , we use the symbol  X  to denote the relative importance relation, and the two models may respectively generate a  X  b , and b  X  a which are inconsistent to rank thes e two objects. In this paper, we propose an efficient approach, which can tackle the inconsistent relative importance relations and make use of the labeled results calculated by the two RankBayes models to rank all the objects. For the keyphrase extraction task, point-wise ranking has been the research focus. Turney implemented a system called GenEx and adopted the genetic algorithm to rank the importance of each phrase based on frequency and position features [6]. Witten implemented another system named KEA with the Bayesian approach [7]. Due to the difficulty of obtaining training corpus, pairwise learning to rank methods have recently attracted the researchers X  attention. Jiang brought the Ranking SVM algorithm to the keyphrase extraction task and achieved satisfying results [3]. Ranking SVM was first used in the field of information retrieval, which demonstrates that the lear ning to rank strategy can well model the ranking orders and optim ize the retrieval quality [2, 4]. Also in the field of information retrieval, Tan further proposed to use the ranking SVM models in th e co-training framework (RSCF) [5], which however does not involve much the problems of time efficiency and two-model inconsistency. To improve the co-training efficiency, Zhao proposed the feature fusion and sample selection strategies in the co-training Rank SVM framework [8]. In this paper, we will first introduce the co-training learning to rank framework into the keyphrase extraction task. With the research focus on the co-training speed and two-model inconsistency, we propose the CoRankBayes model. In this section, we introduce the general architecture of CoRankBayes as illustrated in Figure 1. The basic idea of CoRankBayes does not make much difference to a standard co-training algorithm, where two models learn from each other by making use of both unlabeled data and labeled data. Next, we will address the consistency of RankBa yes, the selection of the most confident labeled pairs to update L 1 and L 2 during the co-training process, and the rank-ordering appr oach using two sets of relative importance relations. Algorithm 1 : CoRankBayes 
Input : A set of labeled pairs L, A set of unlabeled pairs U ( including unlabeled training data and test data ) Output: absolute importance ordering list L 1 = L 2 = L ; U 1 = U 2 = U ; 
Loop for K iterations 1. Train RankBayes model  X  1 from L 1 , RankBayes model 2. Allow  X  1 to label the unlabeled pairs in U 1 ,  X  2 3. Add  X  1  X  X  N most confidently labeled pairs to update L 4. Add  X  2  X  X  N most confidently labeled pairs to update L
Loop end 5. Allow  X  1 to label all the pairs in U 1 , and add them to L 6. Order the ranks for all the phrase according to L 1 and L We choose the Na X ve Bayes algorithm in the pairwise learning to rank model due to its pretty go od performance and fast training speed. What we need do is to prove it won X  X  lead to ranking inconsistency. We define the phrase set as  X   X  X   X   X   X   X   X   X   X   X ,  X   X   X   X   X   X   X ,...,  X   X   X   X   X   X   X  X  X  where  X   X  phrase x i , if we suppose there are D features such as position, term frequency and so on. The vector difference  X  usually used as the input of a binary classifica tion function to determine the relative importance of two phrases x conform to the Na X ve Bayes algorithm, through discretizing each dimension of the vector difference, we construct a set of new binary features F . For example, the maximum and minimum values of term frequency are 50 and 0 respectively, the frequency difference is thereby ranged from -50 to 50, and a binary feature can be designed as fd _10 , whose value is 1 if the term frequency difference equals to 10, 0 otherwise. Next, for the n phrases in X , we construct a set  X   X   X  X  X  X   X , X   X   X   X   X   X   X   X   X ,..., | F |-dimension binary feature vector. Then we construct the classification function  X  :  X   X   X   X   X 1,1  X  , where 1 means that phrase x i has higher relative importance than x the relative importan ce relation) while -1 means the opposite ( x x ). Next we need to prove that the  X  relation determined by the Na X ve Bayes classification function is partial ordering. With the Naive Bayes algorithm, we get: where c means the class being 1,  X   X , X   X  X  X  X  the number of all binary features . We obtain for the log odd-ratio: where  X  X  means not being classified to class c . It follows that class c is chosen if the log value is greater than 0, otherwise  X  X  . According to the log equation, we can get a separating hyperplane: (3) where  X   X   X   X   X  weights all the features with Since the phrase pairs are selected randomly, the probability of written as: Equation (4) shows that the Na X ve Bayes classifier is linear for a pairwise learning to rank task. And we prove that the  X  relation satisfies the properties of asymme try and transitivity respectively as follows:  X   X  X  X   X  X  X  X   X  X  X   X  X  X  X   X   X  X  X   X  X  X  X   X  X  X  X   X  X  X   X  X  X  X   X  X  X  X   X  X  X  X   X  X  X  X   X   X  X  X   X  X  X  X   X  X  X  X   X  X  X  X   X  X  X   X  X  X  X   X  X  X  X   X  X  X   X  X  X  X   X  X  X  X   X  X  X   X  X  X  X   X  X  X  X   X  X  X  X   X  X  X  X  That is to say, the  X  relation is quasi-linear, which can sort all the objects naturally and consistently. During the co-training process, two RankBayes models update their training set L 1 and L 2 iteratively by adding some phrase pairs with relative importance relation labeled by the other RankBayes model, as illustrated in Step 3 and Step 4 of Figure 1. With L L updated, two RankBayes models  X  2 and  X  1 are respectively tuned. First of all, we should sele ct those most confidently labeled phrase pairs for refining the RankBayes models. As in Equation (2), during the k th iteration,  X  1 labels the phrase pair  X  and  X  2 labels  X   X , X   X  X  X  X   X   X   X   X   X   X   X   X  with: where  X   X , X   X  X  X  X   X   X   X   X   X   X   X   X  and  X   X , X   X  X  X  X   X   X   X   X   X   X   X   X  respectively by the two RankBayes models. We call  X  negative) denotes one relative im portance relation, and absolute value denotes the confidence degree of labeling the relation. Thus, we list the phrase pairs in decreasing order by the absolute values of the confidence scores, and select the top N phrase pairs for updating L 1 or L 2 . With training data updated, the tw o models changed iteration by iteration. Thus, even for one mode l, we cannot assure that the labeled results from different iterations are consistent. Then, in the as the training data of the next iteration. Since the updating of L and L 2 are the same, we take L 2 as example. With the assumption those phrase pairs when they are not consistent w ith those pairs already in L 2 . In our detailed implementation, we check each relative importance pair x i  X  x j produced by  X  1 using the recursive algorithm in Figure 2. Algorithm 2 : CheckInconsistency ( x i  X  x j , L 2 ) importance relation x i  X  x j . Return True , if inconsistency exists; False , otherwise. 1. If x i  X  x j  X  L 2 return True 2. Loop for all phrase t x in { x t | x t  X  x i  X  L 2 3. If CheckInconsistency ( x t  X  x j , L 2 ) = True 4. return True 5. Loop end 6. Loop for all phrase t x in { x t | x j  X  x t  X  L 2 7. If CheckInconsistency( x i  X  x t , L 2 ) = True 8. return True 9. Loop end 10. return False After the co-training process, the performance of the two RankBayes models reaches their stable states, and both L include the phrase pairs most confidently labeled by  X  As not all the unlabeled pairs enter into L 2 and L 1 , we use  X   X  to respectively label the phrase pairs left in U 1 and U the labeled results to L 2 and L 1 , just as Step 5 describes in Figure 1. Now, we know L 1 and L 2 respectively include  X  X  phrase pairs with confidence scores, which may have much inconsistency for the final ranking of all the phrases. Here we inconsistency and rank the n phrases. First, in L respectively regularize the confidence scores by dividing the maximum absolute value of the confidence scores. It follows that the regularized values range from -1 to 1. Take any phrase pair &lt; x , x j &gt; for example, its regularized confidence scores in L values have the same sign: positive means that both the two models agree x i  X  x j and negative means x j  X  x the one with the larger absolute value as the confidence score. If these two values have differen t signs meaning the two models disagree with each other, we reduce the confidence score by directly averaging these two values. Finally, we compute a new confidence score  X   X   X  X , X  X  for each pair &lt; x i , x According to the final confidence scores, we calculate an importance score for each phrase. For a phrase pair &lt; x  X   X  X , X  X  , we think that  X   X   X  X , X  X  contributes to the importance scores of both x i and x j . Then the importance score of x adding  X   X   X  X , X  X  and the importance score of x j subtracted from  X   X   X  X , X  X  . In the same way, all the phrase pairs are traversed and each phrase gets its importance score, according to which all phrases are ranked and those higher ranked phrases are selected as keyphrases. We use two datasets to evaluate our approach: Dataset 1 contains 244 scientific articles from Task 5 (named: Automatic keyphrase extraction from scientific articles) of SemEval-2 1 , where 8 to 37 keyphrases are given as the standard answer for each document and the average number of the keyphrases is 15.1 per document; Dataset 2 is composed of 90 pi eces of news, which we collect from well-known English media and manually label 1 to 8 keyphrases in each news documen t with the average number 4.6 keyphrases per document. To represent phrases, a set of commonly used features are designed as illustrated in Table 1. And we use Weka 2 to discretize the continuous features for getting binary features used in CoRankBayes. Feature Description TF Term Frequency IDF Inverse document frequency TF  X  IDF Product of TF and IDF ISF Inverse sentence frequency IfCap If a candidate is capitalized SentPos Position of a candidate in the sentence DocPos Position of a candidate in the document IfTitle If the title contains the candidate The micro-averaged Precision, Recall, F-measure metrics are adopted for evaluation. According to SemEval-2, we respectively evaluate the top 5(@5), top 10 (@10), top 15 (@10) candidates in the ranking list. terminology so that we use the Stanford Parser 3 to parse the noun http://semeval2.fbk.eu/semeval2.php?location=tasks http://www.cs.waika to.ac.nz/ml/weka/ http://nlp.stanford.edu/ software/lex-parser.shtml phrases (tagged as NN, NNS, NNP and NNPS 4 ) for ranking. All the scientific articles are divided into: 144 articles as training data and 100 articles as test data. From the training data, we obtain 202,344 &lt;keyphrase non-keyphrase&gt; pairs as the initial labeling data for CoRankBayes. In e ach iteration, the number ( N ) of phrase small and is assigned an experience value 100. We try to keep the independence of two RankBayes models, and choose 4 features (TF, IDF, TF  X  IDF, IfCap) for  X  1 and 3 features (SentPos, DocPos, IfTitle) for  X  2 . For comparison, a RankBayes model and a linear Ranking SVM model are implemented to extract keyphrases using all the 7 features, and the two baselines (using Na X ve Bayes and Maximum Entropy approaches respectively) provided by SemEval-2 are also gi ven. In total, 1,830,371 phrase pairs need to be labeled where only 202,344 pairs have been labeled for all the models. Table 2 compares their training time test time and F@15. We can see that the performance of RankBayes is similar to that of RankSVM since they both in learning to rank models. And Co RankBayes reaches a satisfying performance with a tolerable speed after it further improves the RankBayes under the co-training framework. CoRankBayes 0 957 21.04% RankBayes 53 348 19.96% RankSVM 3,434 8,581 19.89% Baseline1 --14.70% Baseline2 --14.70% To observe the co-training process, we control the iteration times K . Figure 3 displays the F@5, F@10, F@15 performance of the CoRankBayes model with K increasing. We can see that each RankBayes model improves itself by learning each other, and the combination approach furthermore promotes the performance. For news documents, the phrases are identified with the help of a dictionary which collects all the Wikipedia terms. The word sequences that occur in the dictionary are identified as phrases and a finite-state automaton is used to accomplish this task to avoid the imprecision of preprocessing by POS tagging or chunking. We choose 3 features (IDF, TF  X  IDF, ISF) for  X  1 and 3 features http://www.comp.leeds.ac.uk/ amalgam/tagset s/upenn.html Hardware: Inter  X  Core TM i5-540M Processor, 4GB RAM (IfCap, DocPos, IfTitle) for  X  2 . 30 documents serve as training labeled data of CoRankBayes. Since the average of keyphrase number is no more than 5, we on ly evaluate the top 5 candidates. Table 3 compares CoRankBayes with other approaches, which shows RankBayes also has a sim ilar performance with RankSVM and CoRankBayes can be easily tran sferred to other corpus, with a comparable performance. In this paper, we propose the Co RankBayes model: a feasible co-training framework which combines two Na X ve Bayes based learning to rank models. We prove that the Na X ve Bayes algorithm can be used to implement a lear ning to rank model with a fast speed. To solve the two-model inconsistency, an ingenious approach is proposed to rank all th e phrases by using the labeling confidence of two RankBayes models. Experimental results also show CoRankBayes is promising in solving ranking problems. This research was supported in part by the National Science Foundation of China grants (No: 60875042 and 90920011), National Social Science Founda tion grant (No: 10CYY023). [1] Blum, A. and Mitchell, T.. 1998. Combining Labeled and [2] Herbrich, R., Graepel, T., and Obermayer, K.. 2000. Large [3] Jiang, X., Hu, Y., and Li, H. 2009. A Ranking Approach to [4] Joachims, T. 2002. Optimizing search engines using [5] Tan, Q., Chai, X., Ng, W. and Lee, D.L. 2004. Applying Co-[6] Turney, P.D. 1999. Learning to Extract Keyphrases from [7] Witten, I.H., Paynter, G.W., Frank, E., Gutwin, C., and [8] Zhao, C., Yan, J. and Liu, N. 2008. Improve Web Search 
