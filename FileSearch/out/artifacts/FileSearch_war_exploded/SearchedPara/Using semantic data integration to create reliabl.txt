 Beata Jankowska n 1. Introduction of hypotheses obtained as an effect of reasonings performed in the system. Next, this quality depends on the speed of reasoning: if real-time constraints are to be guaranteed, the system reaction time has to be short. Unfortunately, systems cannot simulta-neously meet the both requirements. In the paper we will focus our attention only on the problem of securing the correctness of hypotheses generated by expert systems, exactly -by rule-based systems. The problem of the speed of reasoning was analyzed, among others, in Jankowska (2006) .
 2006 ) belong to the class of expert systems with a separate knowledge base. In RBSs, the knowledge is represented by means  X  X  X xperienced X  X  in an ultimate sense, i.e. it is uncertain, then the facts and rules should be stated by means of certainty factors ( Shafer, 1976 ) or by means of fuzzy notions ( Zadeh, 1965 ). The performance of system X  X  inference engine is the same in each RBS.
From these considerations it follows that we can influence the RBS quality only by equipping it with a valuable knowledge base. Such a base should always guarantee making all correct hypotheses, i.e. hypotheses that match current facts, and ordering them from most probable to least probable ones. The knowledge can be acquired to the knowledge base by a direct transfer -from an expert in the domain, from books and journals, and also from other expert systems. Such knowledge is acquired by means of classical techniques that enable classifying individuals from the domain, building domain ontologies, but also modeling protocols of activ-ities done while solving problems from the domain. In case of empirical knowledge, special techniques of machine learning have to be used. They consist in mining and processing data from different sources. The well known inductive learning enables discovering knowledge (learning rules) from training examples (facts that are defined by means of data sets). 1.1. Related work
Inductive learning methods received a great deal of attention and were reported in many papers and surveys. Commonly known algorithms of discovering association rules ( Agrawal et al., 1993 ; Agrawal and Srikant, 1994 ; Zaki, 2000 ; Han et al., 2004 ) are based on using factors of support and confidence -the most popular measures of significance and interest. The two factors do not occur in the association rules explicitly -they are hidden. Moreover, they do not differentiate between average and very strong rules, stating deep relations between variables under consideration. In order to distinguish very strong association rules from the set of all discovered ones, an extra analysis is needed learning is used when designing rough set decision rules from data written in the form of attribute tables ( Pawlak, 1982 ; Predki et al., 1998 ). The decision rules can be generated as certain or approximate ones. However, the level of uncertainty cannot be exposed, same as in the previous case of association rules. There are different algorithms of managing incomplete data, i.e. data with missing attribute values, when designing rough set decision rules ( Grzymala-Busse and Grzymala-Busse, 2007 ).

Since early nineties of the 20th century, various data integra-tion techniques have been developed very intensively. The pro-posed solutions are based on relational data model and SQL query language ( Miller et al., 2001 ; Haas et al., 2005 ; Chen, 2005 )or XML data model and XPath language ( Arenas and Libkin, 2005 ; Pankowski, 2008 ). While it is true that the goal of such integration is obtaining  X  X  X ull data X  X  only, yet these data present an excellent and easy source of valuable knowledge.

In the last decade, also ontological sciences developed very rapidly. There are many languages and many tools (e.g. Baader Ontology, 2010 ) that help us to correctly categorize or concep-tualize domains  X  to understand the semantics of individuals, categories and various relations between them. They can be useful for processing heterogeneous data from the domain.
Although the most intensive development of rule-based sys-tems (RBSs) fell on seventies and eighties of the 20th century, a new interest in them has arisen lately. It emerged, among others, from the direction of integrating RBSs with systems based on description logics ( Rosati, 2006 ). The two distinct methods of knowledge representation and distinct reasoning styles are inte-grated loosely as well as tightly. In loosely coupled systems, e.g. Nalepa and Furmanska (2010) , reasoning tasks, after having been recognized, are delegated to appropriate subsystems. For a defining one general reasoner that supports the two reasoning styles and manages the knowledge characteristic of the two systems. The third solution, called embedded integration, e.g. Bragaglia et al. (2010b) , somewhat combines the two previous methods of integration. It consists in implanting one X  X  reasoner behavior to the other of the two systems. As an effect, we obtain a system with homogeneous knowledge representation and homo-geneous style of reasoning. Such a solution, combining the simplicity of loosely coupled systems and efficiency of tightly coupled systems seems to be promising.

Some authors give weight to Attributive Logic ( Ligeza, 2006 )as the factor of RBSs development. The more expressive version ALSV of this logic operates on set values. It was proved that ALSV can be successfully implanted to RBSs ( Nalepa and Ligeza, 2009 ). 1.2. Contribution
An inductive learning process similar to those done for association rules and rough set decision rules can be performed with reference to production rules. Although there is a great similarity between decision rules and production rules, there is also an important difference: the latter ones can be exactly differentiated with regard to their  X  X  X upport X  X  and  X  X  X onfidence X  X . The two factors are explicitly specified inside production rules with uncertainty, which is a generalized form of production rules. As the factor of rule X  X  support will correspond to the rule X  X  priority, and the rule X  X  priority is the most often criterion for resolving conflicts in the agenda, so it will be guaranteed that the rules will be fired in an order from the strongest to the weakest. That is why, the reasonings performed in RBSs with uncertainty produce reliable hypotheses (even in such RBSs that are not locally confluent, i.e. can produce different final hypotheses depending on the order of rules X  firing).

In this paper a method of designing a high quality knowledge base for RBS with uncertainty is presented. The core of the method is an algorithm that discovers production rules from heterogeneous data, stored in different repositories. The data are represented by means of different tuples (lists of predefined attributes), formulated based on somewhat different domain taxonomies/ontologies. Together with the data, also mappings between the tuples have to be provided. The strength of the mentioned tuple formats lies in that the attributes take set values, not individual values.

In the algorithm X  X  performance, two kinds of relations are ana-lyzed and used: topological relations between data elements and sets, and taxonomical relations between data individuals and categories.
That is why, the performance is driven both by data syntax and data semantics. Both kinds of relations have been formally defined within the framework of the algebra of aggregate data (see Section 3). The formal definition of  X  X  X uality metrics for a knowledge base of
RBS with uncertainty X  X  enables to somehow evaluate the quality of knowledge bases obtained on the algorithm X  X  output.

A prototype algorithm version was presented in Jankowska and Szymkowiak (2010) . The new algorithm detects inconsisten-cies and subsumptions between production rules being designed and it protects from putting inconsistent or subsumed rules into the output knowledge base. The algorithm enables to discover most of reliable production rules that result from the input data.
This way, it guarantees a high quality of the output knowledge base, confirmed by a high value of its metrics.

The proposed solution is a systemic one. The rules applied in the process of data integration will be next formulated in the form of appropriate production rules and, implanted to the knowledge base, they will participate in reasoning processes, together with the whole group of domain production rules. 1.3. Organization
In Section 2 we introduce the problem studied in this paper, of aggregate data is given and some algebra of aggregate data is presented. Section 4 introduces a number of definitions that are necessary to explain the syntax, semantics and properties of the knowledge base of RBS with uncertainty. The algorithm GKB for designing knowledge bases for RBSs with uncertainty is proposed in Section 5. Properties of the knowledge bases being designed are the subject of considerations of Section 6. Section 7 concludes the paper and presents directions of future research. 2. Problem specification
Let us assume that we have a number of repositories storing domain aggregate data, written in similar although heteroge-neous formats. Here is an example (1) of such an aggregate data d , formulated on the assumption of some ontology O e for medical experiments. The data describes the result of an experiment carried out to verify the effectiveness of using short-acting beta2-agonists in children with acute asthma exacerbation. The outcomes tested were the necessity of hospitalization and the risk of the next asthma exacerbation in 72 h: d  X  o General_Diagnosis : f pediatric_asthma g = 17 , Current_Health_State : f acute_asthma_exacerbation g = 17 , Standard_Drug : f short_acting_beta2_agonist g = 17 ,
Additional_Drug : f inhaled_anticholin_multi_doses g = 17 , co_intervention : f systemic_corticosteroid g = 17 , age_range :  X  3 ; 17 = 17 , severity_of_diagn_illness : f mild , moderate g = 17 , symptoms : f coughing g = 17 , treatment_effects : f no_hospital_admission g = 16 , adverse_effects : f vomiting g = 3 , relapse : f next_asthma_exacerbation_in_72h g = 1 4  X  1  X  reveal a maximum possible number of dependencies in the domain. In order to be suitable for use in a knowledge base of
RBS with uncertainty, the dependencies should have a form of rules with uncertainty. In addition, if RBS has to be reliable, then the rules have to be of high quality. Remaining in the subject of allergology, we can imagine the following exemplary rule with uncertainty (2):
R 1  X  it happens with Gpm  X  0 : 64 : from 3 to 17 years, suffering fro m pediatric_asthma of mild or moderate intensity, with acute_asthma_exacerbation and associated coughing, in case of administering sh ort_acting_beta2_agonist, inha-led_anticholinergic_multi_doses and systemic_corticosteroid to him, the adverse effect of vomiting is probable to degree 0.17. 3. Algebra of aggregate data knowledge bases for RBSs with uncertainty from aggregate data, first, let us formally define the notion of aggregate data and, next, let us consider an algebra for aggregate data.
 Definition 1. By an aggregate data built according to a schema feature structure d of the following form: d  X f necessary_attributes : optional_attributes : attributes of primary meaning for the data d ; a 1 , a 2 , stand for the so-called non-key attributes -attributes of secondary meaning for the data d ; v A 1 , y , v Am , v a 1 stand for sets of individuals; q A 1 , y , q Am , q a 1 , for qualifiers of the corresponding sets v Ai and v aj , determining the character of the sets: is attached to each set representing the conjunction of its elements, -to each set representing the disjunction of its elements; c A , c c butes A i and a j , i.e. the numbers of those objects from among all objects caught in the data d that take the given values v on the considered attributes A i ( a j ). Besides, each set v is in normal form, i.e. if it has a form { e 1 , e 2 , y , e the following constraint must be satisfied: : ( (1 r i , j r n ) ( i a j 4 e i r O e j ) -see Definition 2.

The attributes used in the schema S correspond with chosen elementary categories defined by the ontology O . The set of all elementary categories under O will be denoted by A O . Key attributes A i are all those attributes from the schema S that formed the base for data aggregation. By virtue of the assumption joined in this aggregate one. For this reason, the counting c same for all the key attributes and it is not less than any counting aggregate data we will use the shortened record: /
The given above data d 1 (1) is formulated in accordance with the requirements of Definition 1. The attributes used in d ones form the set K 1 :
K  X f General_Diagnosis , Current_Health_State ,
It is worth noticing that the notion of aggregate data is general enough to represent also individual data, describing single per-sons, events or transactions (e.g. data of an individual patient, stored in his record kept by a GP surgery).

Definition 2. Let VAL O stands for a set of all individuals assumed by the domain ontology O . By an elementary taxonomical relation defined on the set VAL O we mean a general-specific relation r VAL O VAL O .

Let us assume that among other elements of the set VAL O are acute_asthma_exacerbation and acute_asthma_attack. Between the two elements, the obvious relation (6) holds: acute_asthma_exacerbation r VALO acute_asthma_attack  X  6  X  under the ontology O . We say that v 1 q 1 is a subset of v conditions is fulfilled:
In the light of (6), the following relations (7) hold: f acute_asthma_exacerbation g D q O f acute_asthma_attack , fever g f mild , moderate g D q O f moderate g X  7  X 
As it was considered in Jankowska (2009) , aggregate data and the operations performed on these data can be given an algebraic X  X axonomical interpretation. First, each attribute A has its model in the form of a power set of a qualified subdomain qVAL A , characteristic of the assumed ontology O , qVAL A D
VAL O { , }. Exactly speaking, a qualified set value assigned or . As a consequence, a data schema S  X  ( A 1 , A 2 , y , A y , a n ) has its model in a form of the Cartesian product CVAL of power sets of qualified subdomains qVAL A1 , y , qVAL A m qVAL a1 , y , qVAL a n :
This way, an aggregate data can be seen as a list of qualified set values, which are subsets of qVAL A1 , y qVAL A m , qVAL a1 qVAL a n , with their countings. An ordered pair ( v A q A written for simplicity as v A q A / c A , as it was in (4).
For each attribute A (and its qualified subdomain qVAL A ), a simple algebra S A can be defined, with a partial order relation D c A ,anoperationofunion [ c A , and an operation of intersection \ c defined on the Cartesian product of the power set of qVAL Definition 4. By a partial order relation on the set of qualified set values with their countings 2 qVAL A N , characteristic of the assumed ontology O , we mean a relation D c A that is defined as follows: D c From this definition it follows that the countings c A 1 and c have no influence on the relation X  X  holding.
 Lemma 1 obviously holds: Lemma 1. The relation D c A is a partial order relation. Definition 5. By union [ c A and intersection \ c A operations on the set of qualified set values with their countings 2 qVAL A N , char-acteristic of the assumed ontology O , we mean such extensions of the classical union [ and intersection \ operations from the set theory that fulfill the following conditions: [ c \ c where the values of v A 3 , q A 3 , v A 4 and q A 4 are as follows:
Obviously, only some of the above argument configurations have practical meaning.
 Lemma 2. The operations of union [ c A and intersection \ c the following conditions: 8 X  v 8 X  v A simple consequence of the lemmas 1 and 2 is theorem 1. Theorem 1. The algebra S A  X  ( 2 qVAL A N , D c A , [ c A lattice.

The discussed partial order relation D c A , the union operation [ c
A and the intersection operation \ c A can be simply generalized to a partial order relation D c S , a union operation [ c intersection operation \ c S , respectively, concerning data of a schema S  X  ( A 1 , y , A m , a 1 , y , a n ), built under the assumed ontology O . From now on, we will denote by VAL S the following Cartesian product: Definition 6. By a partial order relation on the data set VAL we mean a relation D c S that is defined as follows: 8 X  v  X  / A 1 : v A 11 , ... , A m : v A 1 m , a 1 : v a 11 , ... , a v  X  / A 1 : v A 21 , ... , A m : v A 2 m , a 1 : v a 21 , ... , a  X  v s 1 D c S v s 2  X  2 8 X  1 r i r m , 1 r j r n  X  X  v A 1 i D Lemma 3. The relation D c S is a partial order relation. Definition 7. By union and intersection on the data set VAL follows: 8 X  v  X  / A 1 : v A 11 , ... , A m : v A 1 m , a 1 : v a 11 , ... , a v  X  / A 1 : v A 21 , ... , A m : v A 2 m , a 1 : v a 21 , ... , a  X  v [ c S v s 2  X  X  / A 1 : [ c A 1  X  v A 11 , v A 21  X  , ... , A 8 X  v  X  / A 1 : v A 11 , ... , A m : v A 1 m , a 1 : v a 11 , ... , a v  X  / A 1 : v A 21 , ... , A m : v A 2 m , a 1 : v a 21 , ... , a  X  v \ c S v s 2  X  X  / A 1 : \ c A 1  X  v A 11 , v A 21  X  , ... , A Theorem 2. The algebra A S  X  ( VAL S , D c S , [ c S , \ c product P (1 r k r m + n ) S A k of similar algebras S A1 y , S a n , is a lattice.

Each domain ontology O offers a set of elementary categories (i.e. attributes) A O for optional use in data schemas being built under this ontology. When designing a data schema, one must decide which of these attributes should be put in the schema, and also how to divide them into key and non-key ones. Two schemas
S and S 2 built under the same ontology O are fully compatible if and only if they are based on the same subset of attributes SA
O D A O . On the other hand, if we make the open world assump-tion, then the absence of attributes from the set A O \ SA schema S proves the ignorance of their values. Formally, we can assume that each such attribute takes an unknown value { } or { e , e 2 , y , e n } , where e 1 , e 2 , y , and e n exhaust the whole attribute X  X  domain. As a consequence, we can believe that all schemas built under the same ontology O are partially compa-tible. Then, the above defined operations of union and intersec-tion can be performed not only on fully homogeneous data matching the same schema S , but also on partially heterogeneous data matching any of the schemas built under the same ontology
O . The division of attributes into key and non-key ones is of no importance for schemas compatibility (yet, it is necessary to formally define this division for new schemas, which are obtained as a result of performing union and intersection operations; to satisfy this requirement, we assume that in each resultant schema, the set of its key attributes consists of those attributes that are key ones in both the schemas matched by data arguments). Concluding, from now on we will use the symbols
D c , [ c O and \ c O instead of the previous ones D c S , [ c and \ c S .

Now let O stand for a reference domain ontology, which is the base for storing and processing data from the domain. Let D stand for a set of domain data matching any of the schemas built under the ontology O . In the light of the above considerations, the data from D O can be compared, joined or processed in any other way by means of a partial order relation D c O , an operation of union [ c O and an operation of intersection \ c O . Next, let D y , D
O n stand for sets of remaining domain data of schemas built under domain ontologies O 1 , O 2 , y , and O n , respectively. The last data are not compatible with the data from D O . To be able to perform operations on all these domain data, one has to transform data from the sets D O1 , D O2 , y , and D O n to their equivalent forms matching any of the reference schemas built under the ontology O . For this purpose, it is necessary and sufficient to define n functions map 1 , map 2 , y , map n dependencies between the reference attributes from the set A function map i ,1 r i r n , maps the domain A O to the correspon-dent codomain EXP O i , standing for a set of expressions built over the alphabet A O i , using the operators + and , and the parenth-eses  X ( X  and  X ) X . In addition, the set EXP O i contains an empty symbol e . The equality map i ( A )  X  e expresses the fact that the attribute A from the reference ontology O has no counterpart in the ontology O i . For a change, if it holds: map i ( A )  X  exp exp informs us how to obtain the qualified set value v A q counting c A -from set values of attributes of the data under the ontology O i by means of the union operation [ c A (operator +) and the intersection operation \ c A (operator ). As a consequence, by means of map i we can transform any data from the set D O i equivalent form from the set D O . In this new data, many unknown attribute values can occur.
 under the reference ontology O based on the set of attributes
A O  X  { A 1 , A 2 , y , A z }, or under one of the ontologies O , can be modeled by means of the following algebra A D :
A D  X  X  D , D c D , [ c D , \ c D  X  X  15  X  D c D , the union operation [ c D and the intersection operation \ c are defined as follows: where f 1 ( f 2 ) stands for the identity function in case of v ( v d 2 A D O ) and for an appropriate mapping map i in case of v 4. Knowledge base of RBS with uncertainty uncertainty. Next, let us formulate the constraints that a knowl-edge base of RBS with uncertainty should satisfy to be considered internally consistent and not redundant.

Definition 8. By an uncertain fact under the ontology O , we mean an equality formula defining for a chosen attribute from the set
A O -its value and the level of certainty of value correctness. The formula has the following form:
A  X  v A q A with Fpm  X  p A  X  17  X  where A stands for an attribute from the set A O , A A A O stands for a qualified set value of the attribute A , v A A v A q A , p A A [0;1].
 symptoms  X f coughing g with Fpm  X  0 : 9  X  18  X  episodes.

Definition 9. By a rule with uncertainty under the ontology O ,we mean an implication formed by the conjunction of premises as the antecedent and one conclusion as the consequent, enriched by two specific certainty factors, of the following form: it happens with Gpm  X  p R : stands for the reliability of the conclusion C given the premises P
P , y and P n .

Definition 10. A database of RBS with uncertainty under the ontology O is any (including empty) set of uncertain facts that are built under O .

Definition 11. A knowledge base of RBS with uncertainty under the ontology O is any non-empty set of rules with uncertainty built under O .

Definition 12. Any two rules with uncertainty R i and R j ontology O of the following forms:
R  X  it happens with Gpm  X  p R 1 : R j  X  it happens with Gpm  X  p then C 1 with Cpm  X  p C 1 then C 2 with Cpm  X  p C 2 we call as contradictory, and we write this fact as R i # only if the following conditions are fulfilled:
A knowledge base KB  X  { R i } i of RBS with uncertainty under the ontology O is internally consistent if and only if for any two rules
R , R j A KB it holds that : ( R i # O R j ).

For example, the contradiction # O holds between the above rule R 1 (2) and the following rule R 2 (21), R 1 # O R 2
R  X  it happens with Gpm  X  0 : 60 :
Definition 13. Let R i and R j stand for rules with uncertainty under the ontology O . Let they have forms as in (20). We say that
R subsumes R j , and we write this fact as R i 5 O R j , if and only if 8 X  v d 1 , v d 2 A D  X   X  A
 X  X  v d 2  X  ;...; A z : f 2  X  A z  X  X  v d 2  X  S  X   X  A
 X  X  v
 X  ;...; A z : f 2  X  A z  X  X  v d 2  X  S  X   X  A
 X  X  v  X  ;...; A z : f 2  X  A z  X  X  v d 2  X  S  X  X  16  X  the following conditions are fulfilled:
A knowledge base KB  X  { R i } i of RBS with uncertainty under the ontology O is not redundant if and only if there does not exist any pair of rules with uncertainty R i , R j A KB such that R R a R j .

For example, the subsumption 5 O holds between the follow-ing rule R 3 (22) and the given above rule R 1 (2), R 3 5 R  X  it happens with Gpm  X  0 : 76 :
The absence of a premise concerning the attribute age among the premises of R 3 means that none constraints are imposed on its value, i.e. the rule R 3 is applicable for each value coming from the attribute X  X  domain.
 Definition 14. A knowledge base KB  X  { R i } i of RBS with uncer-tainty is in a normal form (in short  X  the knowledge base is normal) if and only if it is internally consistent and not redundant. Definition 15. A proper subbase for the knowledge base KB is its subset KB 0 D KB consisting of all those rules from KB that are not subsumed by any other rules from KB and that are not contra-dictory to any other non-subsumed rules from KB .

For each knowledge base KB it exists exactly one proper subbase KB 0 . Obviously, KB 0 is a normal knowledge base. What is more, it is the least normal base that can be obtained from KB as a result of normalization (a minimum possible number of minimal rules with uncertainty).
 Definition 16. The quality of a normal knowledge base of RBS with uncertainty can be measured by a metrics qm. The metrics for a knowledge base KB  X  { R i } i is calculated from formula (23): qm  X  KB  X  X  ln 9 f R i g i 9 Gpm a  X f R i g i  X  X  23  X  where 9 { R i } i 9 stands for the cardinality of the set { R 5. Algorithm for designing a knowledge base for RBS with uncertainty
According to the previous assumptions, for any aggregate data built under the ontology O , the set of its key attributes should contain these and only these attributes from A O that formed the base for data aggregation. These attributes must necessarily be the common ones. Also some of the non-key attributes can be the common ones. Such a situation may be an effect of pure chance, but also the effect of an additional -conscious or unconscious -assumption of aggregation. This circumstance should be taken into account when machine learning of rules with uncertainty from aggregate data is performed.

From now on, any data d vir created by means of integrating k ( k Z 1) real data d 1 , d 2 , y , and d k from repositories in the way: d  X  d 1 \ c D d 2 \ c D ... \ c D d k  X  24  X  we will call as virtual data. To abbreviate data record, we will specify explicitly only those from among its attributes that have their values non-unknown: d
From such virtual data, production rules with uncertainty are designed. However, in order to be usable for this designing, a virtual data must have at least one proper attribute. For the  X  any common non-key attribute a rg ,1 r g r n , is a proper one if and only if the set { d 1 , d 2 , y , d k } is productive for a abbreviated data record for d 0 vir given as (26): vir  X  d 1 \ c D d 2 \ c D ... \ c D d k \ c D d k  X  1  X  26  X  comprises the same set of key attributes and the same set of common non-key attributes as the record for d vir , and stores the same qualified set values as the record for d vir at all pairs of their common attributes;  X  any non-common attribute a rg ,1 r g r n , is a proper one if and only if  X  none of the key attributes A rg ,1 r g r m is a proper one.
From the virtual data (25), we can obtain each such (and only such) a rule with uncertainty (27): it happens with Gpm  X  p R in which  X  C is one of proper data attributes (see above);  X  X  P 1 , P 2 , y , P k } (with optional C ) is a complete set of common (key and non-key) data attributes,  X  qualified set values v P 1 q P 1 , v P 2 q P 2 , y , v Pk same as their counterparts in the data record d vir ( p R p
C in the rule are obtained according to the current algorithms for calculating certainty factors Gpm and Cpm  X  see Section 7).
To summarize, machine learning of rules with uncertainty consists in, first, producing virtual data from real data stored in the given repositories and, next, designing rules from these data. There is no other source for deriving rules than the virtual data.
On the other hand, each virtual data contains maximum possible number of elementary data and for this reason it guarantees that the rules will be of high quality.

The proposed algorithm GKB enables obtaining a normal knowledge base for RBS with uncertainty from heterogeneous data matching schemas built under the reference ontology O and consist in: implementation.
 , ,  X  a set of heterogeneous data, stored in repositories D O y , and D O n , built in accordance with the domain ontologies O , O 1 , O 2 , y , and O n , respectively.

Output:  X  a normal knowledge base KB for RBS with uncertainty, derived by machine learning from the data stored in the repositories
D O , D O1 , D O2 , y , and D O n ;  X  set I of such rules with uncertainty that -although correctly derived -were not included into KB (the rules were confirmed to be inconsistent one to another).

In the above implementation, two groups of functions have been used:  X  functions implementing relations and operations on sets of elements: IsEmpty, Contains, PowerSet, Add, Remove, Sum and Subtract, of obvious meaning,
Implementation (written in C# language): i , d h , A ig )) {flag1  X  0; break;} ); );}  X  functions operating on data and rules: key, non-key, common, transform, preserves, integrate, formulate, are_contradict, and are_inconsist.

If the semantics of the functions key, non-key, common, are_contradict and subsumes is obvious, still the remaining ones need some explanation. On the abstract level, they can be defined as follows: has also two hidden arguments: the number from 1 to n , identifying which repository the data actually comes from and also the set of n mappings, each of them between the schema of data from D O and the schema of data from some repository D O i . Having an argument data, the function transform searches for its equivalent in the schema of D O ;  X  preserves: D O D O A O -{true, false}; preserves( d 1 , d set values substituted for the attribute A in the data d 1 respectively;  X  integrate: D O D O -D O ; integrate( d 1 , d 2 )  X  d 1  X  formulate: D O 2 A O A O -R O , where R O stands for a set of such rules with uncertainty that can be obtained by machine learning from the set of domain data D O . The function formulate is responsible for deriving a production rule from a virtual data d vir (the first function X  X  argument) obtained as a result of semantic integration of data from the given repositories. The second and third arguments of the function represent a set of all common attributes of the virtual data d vir (optionally  X  with the exception of a chosen non-key attribute), and a non-common attribute of d vir (optionally -the chosen common non-key attribute of d vir ), respectively; the two arguments guarantee that the constraints imposed on the process of machine learning of rules with uncertainty (see the beginning of the current section) will be satisfied. The auxiliary functions gGpm and gCpm calculate the values of rule X  X  factors. The detailed form of the function formulate depends on the target rule language.

Now, let us consider an example illustrating the algorithm operation. Let us assume that D O , D O1 , D O2 , y ,and D repositories containing, among others, three data which -after transforming them to equivalent forms matching schemas built under O -take the shape of the data d 1 (3), d 2 (28) and d d  X  o General_Diagnosis : f pediatric_asthma g = 18 , d  X  o General_Diagnosis : f pediatric_asthma g = 89 , We remark again that K 1  X  {General_Diagnosis, Current_Health_State, Standard_Drug, Additional_Drug}.

Let us now assume that in course of the algorithm X  X  operation: at point 3b a set E 11 has been established to {age_range, symptoms}, and at point 3c an attribute f 111 has been set to adverse_effects.
An attempt to integrate the initial data d 1 with the data d d to integrate d 1 with d 2 (the relations [3;17] D q O [4;15] ; {coughing} D q O {coughing, wheezing} are obvious; the relation {acute_asthma_exacerbation} D q O {acute_asthma_attack} is a con-sequence of acute_asthma_exacerbation r O acute_asthma_attack; and the non-common attribute adverse_effects has its qualified set data d 1,2 with d 3 is impossible (both because of not holding the relation [3;17] D q O [1;18] , and because of adverse_effects having integrating d 1 with d 2 and d 3 under the criterion K 1  X  E follows: d  X  o General_Diagnosis : f pediatric_asthma g = 35 , Current_Health_State : f acute_asthma_exacerbation g = 35 , Standard_Drug : f short_acting_beta2_agonist g = 35 ,
Additional_Drug : f inhaled_anticholin_multi_doses g = 35 , co_intervention : f no_corticosteroid g = 35 , age_range :  X  3 ; 17 = 35 , severity_of_diagn_illness : f mild , moderate g = 35 , symptoms : f coughing g = 35 , treatment_effects : f no_hospital_admission g = 34 , adverse_effects : f vomiting g = 7 4  X  30  X 
Let us further denote by C 1,2 -a set of all common attributes of d , C 1,2  X  {General_Diagnosis, Current_Health_State, Standard_Drug,
Additional_Drug, co_intervention, age_range, severity_of_diagn_ill-ness, symptoms}. After executing the function forms for the data d , the set of its common attributes C 1,2 , and the non-common attri-bute adverse_effects (31), we obtain the known rule R 1 (Section 2): formulate  X  d 1 , 2 , C 1 , 2 , adverse_effects  X  X  R 1  X  31  X 
Obviously, the data d curr obtained after finishing the iterations of the loop 3c (in the last example  X  data d 1,2 ) satisfies the constraints imposed on the form of virtual data. Also, the process of obtaining rules with uncertainty from these data is in accor-dance with the request. As a consequence, lemma 4 holds:
Lemma 4. All generated by the algorithm GKB rules with uncer-tainty are correct with respect to the constraints imposed on the machine learning process.

The algorithm GKB does not enforce the order in which the initial integration. As the operation \ c O is commutative, as this order has any two criteria of integration K i  X  E ij  X  f ijk and K i also the result of integration will be the same in both cases. These important observations are summarized in lemma 5.
 from the given repositories that are integrable with the initial data d i , the final result of integration does not depend either on d or on the integration criterion.

Let us assume that in the course of executing GKB for the initial data d 1 , among others, the two integration criteria were d is integrable with d 2 as well as with d 3 . On the basis of lemma 5, the final result d 1,2,3 of integration must be the same in both the cases, besides -irrespective of an order of joining d to d 1 (32): both integration criteria is owed to the fact that the set { d , d 3 } is productive not only for the common attribute symptoms (an obvious case of criterion K 1  X  X  13  X  f 133 ) but also for non-common attribute treatment_effects (criterion K 1  X  E 12  X  f result, the following rules R 4 (33) and R 5 (34) can be obtained:
R 4  X  it happens with Gpm  X  0 : 71 :
R 5  X  it happens with Gpm  X  0 : 75 : 6. Properties of the knowledge bases designed by GKB not subsumed by any other rule R s put earlier into the knowledge base KB . If the constraint is not satisfied, then the rule R removed from KB or this newly obtained rule R ijk will not be added to KB , respectively. What is more, after finishing the loop 3a, all pairs of contradictory rules R s and R t from the obtained knowledge base KB will be deleted from KB and put into a set of so-called questionable rules I . The described algorithm X  X  behavior implies the truthfulness of lemma 6.

Lemma 6. Each knowledge base KB obtained on the output of the algorithm GKB is a proper subbase for a set of all the rules created during the algorithm operation.
 can be measured by means of the metrics qm( KB )  X  ln 9 { R
Gpm a ({ R i } i ).Letusestimatethevalueofthemetricsforaknowledge base designed by the algorithm GKB from the aggregate data given imposes some additional constraints on the process of integrating data from the input. For example, if we assume that among the input data there are d i and d j such that K i a ( K i \ K possible to create a virtual data d i,j ,andnext-arule R i,j if only this virtual data contains at least one (non-key) proper attribute. Meanwhile, in the algorithm GKB the integration d will never come into effect. As a result, the prospective rule R never be created. Thus, the knowledge base KB designed by the algorithm GKB can be a proper subset of a knowledge base KB which is a proper subbase for the full set of rules generable by machine learning from the given data.
 On the other hand, each rule R i generated by the algorithm
GKB must be very reliable. Let us remark that it is created from the greatest possible virtual data, aggregating the most possible number of elementary data inside. If we assume that the main factor of rule X  X  reliability depends logarithmically on the number of elementary data aggregated inside, then this fact speaks for a high average rule X  X  reliability.

Let us remark at last that the algorithm X  X  complexity depends polynomially on the number of input aggregate data. Besides, it depends exponentially on the maximum number max A of non-key attributes occurring in these data. Assuming that this number is not high (at most -about 10), we can regard c  X  2 max constant factor of the algorithm X  X  complexity.

The above considerations are summarized by the following theorem.

Theorem 3. If repositories D O , D O1 , D O2 , y , and D O n aggregate data in total, then the algorithm GKB will design in polynomial time O(c n 2 ) such a knowledge base KB , which is a proper subbase for the set of all rules created during the algorithm operation. With regard to the limitations imposed on the process of rules generation, a number of correct rules may not be created and put into KB . Thus, the metrics qm( KB ) can differ from the best possible result that can be obtained by means of an algorithm of exponential complexity O(2 n ).

The process of deleting from KB all such rules that are contradictory to one another is carried out with the object of obtaining a normal knowledge base. However, it is worth noticing that the rules are deleted in excess of need. In most cases, one can isolate such subsets of set I that can be included into KB under preserving its normality. The choice of such a subset can be done automatically or semi-automatically, with the aid of a user. If the user can choose from two contradictory rules a right one, then it is better to choose the subset semi-automatically. In the opposite case, an algorithm guaranteeing the greatest improvement of qm( KB ) should be used. 7. Conclusions and future research
The proposed algorithm GKB for machine learning of rules with uncertainty from aggregate data has three important proper-ties. First, it operates on data whose components take not individual but set values. A formal representation of such data can be built on the grounds of Feature Structures theory ( Ait-Kaci, with Set Values ( Ligeza, 2006 ).

Next, the algorithm performs both syntax and semantic data analysis. It can manage such heterogeneous data that are built based on the same set of individuals, but can be categorized in many different ways. The semantics of the data is given by means of:  X  mappings between taxonomies used for producing domain data (exactly speaking -between categories of a reference taxonomy and categories used by the remaining taxonomies),  X  a subsumption relation, defined on the whole set of domain individuals,  X  qualifiers, giving (conjunctive or disjunctive) interpretation to set values substituted for data attributes and  X  data attribute dividers, determining for data attribute sets -subsets of their key attributes.

At last, the algorithm GKB operates on aggregate data, each one representing in a tight form a number of elementary, individual data. Correct implementation of data operations is owed partially to data attribute dividers. They directly specify constraints imposed on the form of elementary data subjected to aggregation.

The mentioned properties make the algorithm universal and expand the area of its applications. Besides, operating on aggre-gate data instead of individual ones speeds up the algorithm X  X  execution.

The algorithm X  X  operation is based on a specific semantic integration of aggregate data from the domain. A formal basis for this integration is provided by a lattice whose carrier set is a set of heterogeneous aggregate data given by lists of attributes of set values. On this data set a relation of subsumption, and operations of union and intersection are defined (we use them widely in the algorithm GKB). After having done integration, we obtain virtual data, which next becomes an argument for the function creating a rule with uncertainty. Putting the rule into a knowledge base being designed depends on the fact whether it does not violate this knowledge base normality.

For measuring the knowledge base quality, a special metrics qm has been proposed. A number of remarks have been made on the factors affecting this metrics. These are the number of created rules with uncertainty and the average rule X  X  reliability. The problems of calculating the reliability of a rule as a whole, and also the reliability of its conclusion given its premises were discussed, among others, in Szymkowiak and Jankowska (2010) . If the formula for calculating the last factor is out of the question (Cpm  X  L / N , where L stands for the counting of the attribute f from the virtual data d curr , N is the counting of each common attribute from d curr and L / N is the point estimate of Cpm value), yet the formula for calculating the rule X  X  reliability needs a deep reflection. It is beyond any doubt that this factor should depend on the number of elementary data  X  X  X aught X  X  in the data d which is the base for creating a rule. It should also reflect the quality of the data d curr , defined as an average quality of its attribute. The attribute X  X  quality, in turn, depends on the ratio l and the greatest set value, respectively, substituted for this attribute in any aggregate data used to create d curr or in the data d attribute X  X  quality. A low value of the ratio testifies to the great diversity within the data used to create d curr .

Regardless of striving to obtain a high metrics qm( KB ), one should also take care of internal relations between the rules, exactly of relations between the rules X  reliabilities. From among all the rules of KB , one should pick up the ones of the greatest importance. Such rules should be fired first while the reasonings. Due to this reason, the considered rules should have their original reliabilities raised to a high degree.

When analyzing similarities between production rules with uncertainty and association rules (see Section 1), we come to the conclusion that the factor Cpm from rules with uncertainty is a counterpart of the confidence from association rules, and the factor Gpm-a counterpart of the support from these rules. After having purposely raised the factor Gpm, we obtain a counterpart of the so-called strong association rule. An association rule can be recognized as a strong one if a strong dependence between the rule X  X  antecedent and the rule X  X  consequent is found, and if this dependence is proved to be not random. The dependence A -can be recognized as not random if the dependence : B -: A can also be found. Transferring these observations to RBSs with uncertainty, a production rule with uncertainty R can be recog-nized as a very reliable one if its conclusion has Cpm of a very low or a very high value which distinguishes this rule:  X  from the remaining rules with any premises and the same conclusion as the one used in R or  X  from the remaining rules with the same premises and a conclusion with the same attribute but with a value other than the one used in R .
 A detailed proposal of the method for calculating the factors Cpm and Gpm will be the subject of a forthcoming study. The proposed algorithm GKB is under implementation now.
We still have in prospect to improve its efficiency. The ratio of the number of rules created during algorithm X  X  operation to the size of the final knowledge base should excuse the algorithm X  X  behavior (we mean here a time-consuming process of refining the base from redundant and contradictory rules).
 As it was said, the proposed solution is a systemic one.
Production rules equivalent to those used in the algorithm performance will be put into the knowledge base, together with all the domain production rules. As an effect, a specific RBS with uncertainty will be obtained. Due to the method of its construc-tion, it should be considered as a system integrated by embedding into the classical RBS with uncertainty -some domain taxonomy and the reasoning style from Attributive Logic with Set Values.
Instead of often used fuzzy values and fuzzy reasoning, e.g. Bragaglia et al. (2010b) , it will operate on crisp values and it will perform an approximate reasoning.

As it was noticed by Cantrill (2010) , the fast development of information technology does not reflect itself in a significant progress in applying computers in medicine, especially in patient care. The commonly known reasons for this situation are both hardware limitations, and unreliability and low efficiency of software products. However, subjective reasons should also be taken into account. We mean here the unwillingness of most doctors to cooperate with knowledge engineers. In this situation, a medical system designer should appreciate each accessible source of real medical data. Such the data are, among others, enclosed in Clinical Trials Registers (CTRs) that the doctors publish in medical e-journals. The algorithm GKB has been prepared to operate on aggregate data; so it will be able to use CTRs in its performance.

As opposed to association rules and decision rules, production
In case of the considered RBS, it will be defined by means of certainty factors Cpm and Gpm. Owing to them, a set of the designed diagnostic and therapeutic rules will enable not only making but also ranking reliable medical hypotheses. Such an aiding system should protect the doctors against routine behaviors.
In the nearest future, a particular attention will be also paid to the project of a good ontology for medical experiments. The quality of designed knowledge bases will depend not only on the number, form and variety of given aggregate data but also on this ontology.
 Acknowledgments The research has been partially supported by the Polish
Ministry of Science and Higher Education under Grant N516 369536.
 References
