 Clustering is a basic data mining task that discovers similar groups from given multi-variate data. Validation of a clustering result is a fundamental but difficult clusters in the observed data[3,7,14]. Up until now, various validity measures have been proposed from different aspects, and they are mainly separated into two types whether based on internal or external criteria[7,10,8]:  X  Internal criteria evaluate compactness and separability[3] of the clusters  X  External criteria evaluate how accurately the co rrect/desired clusters are This paper focuses on using external criteria, that is provided by human inter-pretation of data. It is more beneficial to u se external criteria when class labels are available.

In order to understand obtained clusters better, this work introduces a neigh-borhood relation among clusters. A neighborhood relation is useful especially in case of micro-clusters or, i.e., cluster number is larger than class number. Global structure of clusters, which means not only individual (local) clusters, can be evaluated with neighborhood relation of classes within each cluster.
The basic policies of introducing the neighborhood relation is as follows: 1. A data object which belongs to the same class should be in neighbor over 2. A weighting function is introduced into basic statistics that are commonly Above mentioned conventional indices do not consider neighboring clusters, while very few works introduce inter-cluster connectivity for prototype based clustering[11]. The inter-cluster connectivity is introduced by the first and the second best matching units, but this work is based on internal criterion. The contribution of this work is to introduce neighborhood relation over clusters into conventional external cluster validity indices.

The reason why we assume the situation to evaluate an unsupervised learning by class labels is as follows. The fundamental difficulty of unsupervised learning is that the features and the distance metric are derived from observation and assumption, there is no information from human interpretation of data. On the other hand, it is often the case that a small number of samples, or data from the same domain, or simulated samples are available with class labels. In such cases, an external validity measure works as a preliminary evaluation instead of evaluating unlabeled target data.

This paper presents how to introduce the weighting function to smooth the conventional clustering validity measure s. In the experiment, we revealed the op-timal smoothing radius and also examined several parameters, prototype (micro-cluster) number, and class overlapping degree. We revealed the properties of our extended measure and showed potential to validate a clustering result consider-ing neighborhood relation of clusters. R , a clustering produces a cluster set C = { C i } K i =1 with a cluster assignment c ( i )  X  C for each object x i .
 class assignment for x i . Classes are provided independent from a clustering. distance between clusters that can be computed either Euclidean-based or topology-based distance.
 Ex) Inter-cluster distance. Euclidean-based distan ces can be given by single Note that though a neighbor graph is normally obtained independent from a clustering process, some method produ ces cluster (vector quantization) with topology preservation such as Self-Organizing Map(SOM)[6], which is also used in this experiment.
 The objective of this work is to evaluate density of class T within intra-cluster There are two types of cluster validity measures, namely set-based and pairwise-based measures 1 . These two types of measures can be extended in different manners. 3.1 Extension of Set-Based Cluster Validity Measures First, the way to extend set-based cluster validity measures[10,12] such as cluster purity and entropy are described in this section. The properties of each measure were studied in the literature[1].

By considering neighborhood relation of clusters, the neighbor class distribu-tion should be taken into account to the degree of certain class contained in a cluster, that is, the data points of the same class in the neighbor clusters should have a high weight, while those of distant clusters should have a low weight based on the inter-cluster distance as the diagram is shown in Fig. 1. Let f ( u ; l ) be a density distribution of class label l  X  T at u  X   X  ,where  X  denotes a data space, and h ( u , v ):  X   X   X   X  R be a weighting function based on the neighborhood relation. Based on the above concept, a class density N are smoothed by the weighting function h ( u , v ) as follows: Also N denotes the total number of objects; N =# { x k | x k  X  S } .Eqs.(1)to(3) can be rewritten as follows: Here, h i,j canbeusedanymonotoni cally decreasing function, for example, the often encountered Gaussian function: h i,j =exp(  X  d i,j / X  2 ), where d i,j denotes inter-cluster distance and  X  ( &gt; 0) is a smoothing (neighborhood) radius.
Thus, weighted cluster purity and entropy, for example, are defined using the weighted statistics of eqs. (4), (5), and (6) as follows: weighted Cluster Purity (wCP) weighted Entropy (wEP) 3.2 Extension of Pairwise-Based Cluster Validity Indices This section describes an extension of pairwise-based cluster validity measures[1,14]. Table 1 shows a class and cluster confusion matrix of data pairs, where a, b, c, d are the number of data pairs where x i and x j do or do not belong to the same class/cluster.

Here, we introduce likelihood ( c ( i )= c ( j )) indicating a degree that a data pair x i and x j belongs to the same cluster instead of the actual number of data pairs. The likelihood is given by the inter-cluster distance of the data pair as shown in Fig. 2(a). The same weighting function as in sec. 3.1 is available for the are replaced by summation of the likelihoods as follows: With these extended a ,b ,c and d , weighted pairwise accuracy and pairwise F-measure are defined as follows: weighted Pairwise Accuracy (wPA) weighted Pairwise F-measure (wPF) 3.3 Weighting Function For the weighting function for smoothing in the set-based and the likelihood in pairwise-based measures, any monotonically decreasing function h i,j  X  0is feasible, including the Gaussian or a rectangle function. Note that the extended measures are exactly the same as the original measures when h i,j =  X  i,j (  X  is the Kronecker delta).

The neighborhood radius effects the degree of smoothing and likelihood. Fig. 3 illustrates that the measure evaluates individual clusters, that is the original values, as the radius becomes zero (  X   X  0). On the other hand, as the radius becomes larger (  X   X  X  X  ), the data space is smoothed by almost the same weights, and all micro-clusters are treated as one big cluster. The way to find the optimal radius is described in section 3.4.
 3.4 Optimal Smoothing Radius Our smoothed measures include a neighborhood relation in the conventional cluster validity measures. In order to evaluate a neighbor component within the measure, we defined as: Definition 4. (neighbor component) The quantity within the smoothed cluster validity value (Eval) that are caused by the neighborhood relation. Here, Eval refers to the output value of wCP, wEP, wPA, or wPF in this paper.
Then, the neighbor component ( NC ) can be computed by comparing Eval s with randomized neighborhood relation.
 where Eval rnd ( n ) denotes an average of Eval when inter-cluster distances are n times shuffled, and when n  X  X  X  this value converges to Eval with the average of imizes the neighbor component is the optimal one, i.e.,  X   X  =argmax  X  NC (  X  ). Then, the optimal evaluation value can be Eval  X  = Eval (  X   X  ). This section describes the experiment t o clarify the properties of the proposed smoothed validity measures.
 4.1 Settings of Clustering and Neighborhood Relation 1. kmc-knn 2. SOM 4.2 Datasets 1. Synthetic data 2. Real-world data 4.3 Effect of Smoothing Radius -Finding the Optimal Radius Fig. 5 shows the evaluation values of the smoothed validity measures for the synthetic data using kmc-knn. The larger value is the better except entropy. The values are average of 100 runs of randomized initial values.

Firstly, the total evaluation values ( Eval ) provides always better value than that of random topology ( Eval rnd ) where neighborhood relation of the proto-types is destroyed. This means that the p roposed measures evaluate both cluster validity and neighborhood relation of the clusters.
Secondly, as the smoothing radius becomes close to zero (  X   X  0), the extended measure evaluates individual clusters without neighborhood relation. Whereas, as the radius becomes larger (  X   X  X  X  ), the extended measure treats whole data as one big cluster as mentioned before. Therefore, the solid and the broken lines gradually become equal as the radius becomes close to zero or becomes much larger.

Thirdly, the neighbor component has a monomodality against the radius in all measures, since there exists an appropriate radius to the average class dis-tribution. Since the smoothed measure is a composition of cluster validity and neighborhood relation, the radius that gives the maximum Eval does not al-ways match with that of neighbor component, for instance, wCP, wEP, and wPF in Fig. 5. Therefore, the neighbor component should be examined to find the appropriate radius. Also the appropriate radius depends on function of the measure such as purity, F-measure, or entropy. This means that the user should use different radius for each measure.

These three trends appear also in SOM (omitted due to page limitation). 4.4 Effect of Prototype Number The effect of prototype number is examined by changing k 1=10 , 25 , 50 (Fig. 6). In wPF, k 1 = 25 provides the highest neighbor component (0.116 at  X  =1 . 4) among three (Fig. 6(b)). wPF can suggest an optimal prototype number in terms of maximizing the neighbor component in the measure, which means neighbor relation of class distribution is maximized. However, the larger k 1 the better in wCP (Fig. 6(a)). This is because the function of cluster purity given by eq. (7), that is, the smaller number of elements in some cluster tends to give better purity. 4.5 Effect of Class Overlap The effect of class overlap is examined (Fig. 7) by changing distance between class centers  X  d =  X  x 2  X   X  x 1 from 2.0 to 3.0 in the synthetic data. Observing Fig. 7, the lower class overlap is, the better the neighbor component and the total values. However, the optimal radii are nearly the same even in different class overlap. This means that our mea sure can determine the optimal radius independent to class overlap, and can evaluate volume of overlap. 4.6 Real-World Data Fig. 8 shows the result for real-world data using SOM. Though there exists an optimal radius, the optimal radii vary depending on dataset, i.e., the number of classes and the class distribution. This result indicates that depending on dataset and measure, a user should use different radius that gives the maximum volume of neighbor component. This paper proposed a novel and generic smoothed cluster validity measures based on neighborhood relation of cluste rs with external criteria. The experi-ments revealed the existence of an optimal neighborhood radius which maxi-mizes the neighbor component. A user should use an optimal radius depending on a function of measure and a dataset. Our measure can determine the optimal radius independent to class overlap, and can evaluate volume of class overlap. In addition, feature selection, metric learning[13,15], and a correlation index for multilabels to determine the most relevant class are promising future directions for this work.
 Acknowledgment. This work was supported by KAKENHI (21700165).

