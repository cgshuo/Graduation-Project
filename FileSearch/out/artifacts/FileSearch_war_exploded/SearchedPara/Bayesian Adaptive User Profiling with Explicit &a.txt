 Research in information retrieval is now moving into a per-sonalized scenario where a retrieval or filtering system main-tains a separate user profile for each user. In this framework, information delivered to the user can be automatically per-sonalized and catered to individual user X  X  information needs. However, a practical concern for such a personalized system is the  X  X old start problem X : any user new to the system must endure poor initial performance until sufficient feed-back from that user is provided.

To solve this problem, we use both explicit and implicit feedback to build a user X  X  profile and use Bayesian hierarchi-cal methods to borrow information from existing users. We analyze the usefulness of implicit feedback and the adap-tive performance of the model on two data sets gathered from user studies where users X  interaction with a document, or implicit feedback , were recorded along with explicit feed-back. Our results are two-fold: first, we demonstrate that the Bayesian modeling approach effectively trades off be-tween shared and user-specific information, alleviating poor initial performance for each user. Second, we find that im-plicit feedback has very limited unstable predictive value by itself and only marginal value when combined with explicit feedback.
 H.3.3 [ Information Storage and Retrieval ]: Informa-tion Search and Retrieval  X  Information filtering, relevance feedback Algorithms, Human Factors Information Retrieval, User Modeling, Bayesian Statistics, Implicit Feedback Copyright 2006 ACM 1-59593-433-2/06/0011 ... $ 5.00.
Although ad hoc retrieval systems have become part of the daily life of internet users or digital library users, it is clear that there is great variety in users X  needs. Thus, such systems can not offer the best possible service since they are not tailoring information to individual user needs.
IR research is now moving into a more complex environ-ment with user centered or personalized adaptive informa-tion retrieval, information filtering and recommendation sys-tems as major research topics. As opposed to traditional ad hoc systems, a personalized system adaptively learns a pro-file for each user, automatically catering to the user X  X  specific needs and preferences [1].

To learn a reliable user specific profile, an adaptive sys-tem usually needs a significant amount of explicit feedback (training data) from the user. However, the average user doesn X  X  like to answer a lot of questions or provide explicit feedback on items they have seen. Meanwhile, a user does not want to endure poor performance while the system is  X  X raining X  and expects a system to work reasonably well as soon as he/she first uses the system. Good initial per-formance is an incentive for the user to continue using the system. Thus an important aspect of personalization is to develop a system that works well initially with less explicit user feedback.

Much prior research has been carried out exploring the usefulness of implicit feedback [10] because it is easy to col-lect and requires no extra effort from the user. On the other hand, user independent systems perform reasonably well for most users, mainly because the system parameters have been tuned to a reasonable value based on thousands of existing users. These observations suggest at least two possible ap-proaches to improve the early stage performance of a per-sonalized system: using cheap implicit feedback from the user and borrowing informatio n from other users. Both ap-proaches may help the system reduce its uncertainty about the user, especially when the user just starts using the sys-tem and has not provided much explicit feedback.

This paper explores both directions under a single, uni-fied Bayesian hierarchical modeling framework. Intuitively, combining the two approaches under this framework may achieve a nice tradeoff between bias and variance trade off, especially for a new user. This is because including im-plicit feedback decreases the bias and increases variance of the learner, while borrowing in formation from other users through a Bayesian prior in our framework has the inverse effect. We demonstrate, empirically, that the hierarchical model controls the tradeoff between shared and personal in-formation, thereby alleviating poor initial performance. We also evaluate the long-term usefulness of different types of feedbackforpredictingauser X  X ratingforadocument.

The paper is organized as follows. We begin with a brief review of related work in Section 2. We then describe the Bayesian hierarchical modeling approach to learn user spe-cific models (Section 3) and introduce a computationally efficient model, the Hierarchical Gaussian network, as an example to be used in our experiments (Section 4). Section 5 and 6 describe the experimental methodology and results on two real user study data set. Section 7 concludes.
Implicit feedback is a broad term including any kind of natural interaction of a user with a document [16]. Exam-ples of implicit feedback are mouse and keyboard usage, page navigation, book-marking, and editing. While the focus of much emerging research, there is still some debate over the value of implicit feedback. For example, it has been ob-served in numerous controlled studies [3, 12, 6, 4] that there is a clear correlation between the time a user spends view-ing a document and how useful they found that document. However, Kelly, et al. [9] demonstrate, in a more natural-istic setting, that this correlation varies significantly across tasks and conclude that the usefulness of implicit feedback is conditioned heavily on what type of information the user is seeking. Standard machine learning techniques such as SVMs [8], neural networks [12], and graphical models [4][18] have been used to explore the usefulness of implicit feedback, with varying degrees of success. In our work we use a hi-erarchical Bayesian hierarchical model to integrate implicit feedback.

The idea of borrowing information from other users to serve the information needs of a user is widely studied in the area called collaborative filtering and Bayesian hierar-chical modeling has applied in this context [17]. The major differences in our work are: 1) we focus on developing com-plex user models that go beyond relevance based content model by including other explicit feedback (such as topic familiarity and readability) and implicit feedback (such as user actions and system context); 2) we focus on the adaptive learning environment and analyze the online performance of the learning system. This analysis makes it clear how an adaptive system can benefit from the hierarchical Bayesian framework; and 3) we use a Gaussian network (Gaussian user models with Gaussian prior), while the prior work uses different functional forms, such as multinomial user models with Dirichlet prior [17]. Our method is more computation-ally efficient and also has less privacy concerns while sharing information across users.
One concern about using user-specific models, or profiles, is that there is an initial period where the user must endure poor performance. On the other hand, existing user inde-pendent systems seem to work well by tuning parameters for the general public. This motivates us to borrow information from existing users to serve a new user. Here we adopt a principled approach for doing this based on Bayesian hier-archical models.

Let f u represent the model of a user u .Overtime,the user interacts with the system and provides explicit and im-plicit feedback about documents the user has read. Let a d -dimensional random variable x u represent the information about a document for user u . Each dimension of x u corre-sponds to a feature, and the features could be user actions on the document, the document relevance scores the system derives from the user X  X  explicit feedback on other documents, user-independent features of the document, and so on. The user u may provide a rating y u toadocument 1 x u according to the user model f u .Fornow,a model is a function that takes information about a document and returns an estima-tion of whether the user likes the document or not (rating): f u : x u  X  y u . The functional form for f u varies for dif-ferent learning system, and we delay this discussion until Section 4. We make no assumption about the distribution of documents.

A Bayesian based learning system begins with a certain prior belief, P ( f |  X  ), about the distribution of user models. In the simplest terms the hiera rchical Bayesian framework canbewrittenas and is also illustrated in Figure 1 Note that this is a very general framework and can accommodate any class of func-tions for which a reasonable prior can be specified. This includes any function parameterized by real number (e.g. SVMs, neural networks with fixed topology) and regression trees [2].

A personalized system tries to learn the user model f u for auser u . As the user uses the system, the system receives a sample of document-rating pairs, D u = { ( x u i ,y 1 ...N u } ,where N u is the number of training examples for user u . Using Baye X  X  Rule, the system can update its belief about the user model based on the data and get the posterior distribution over user models:
To find the maximum a posteriori (MAP) model, f u MAP , we can ignore the denominator since it is the same for all models. Then we have: Equation 1 shows clearly how the fitness of a model is de-composed into the model X  X  prior probability and the data likelihood given the model.

Incorporating a prior distribution automatically manages the trade off between generic user information and user spe-
This paper sometimes refers to x u as simply a  X  X ocument X  even though it has features of the user X  X  interaction with it. cific information based on the amount of training data avail-able for the specific user. When a user u starts using a system, the system has almost no information about the user. Because N u for the user is small, the prior distribu-tion p ( f |  X  ) is a major contributor when estimating the user model  X  f (first term in Equation 1). If the system has a reliable prior, it can perform well even with little data for the user. How can we find a good prior for f ?Wetreat user u  X  X  model f u as a sample from the distribution P ( f Given a set of user models, we can choose  X  to maximize their likelihood.

As the system gets more training data for this particu-lar user, f u MAP is dominated by the data likelihood (second term in Equation 1) and the prior learned from other users becomes less important. As a result, the system works like a well tuned non-personalized system for a new user, but keeps on improving the user model as more feedback from the user is available. Eventually, after a sufficient amount of data is collected, the user X  X  profile should reach a fixed point.
Specific functional forms for f u and P ( f |  X  ) are needed to build a practical learning system. For our work, we let f be a simple linear regression function and let w be the parameter of f . We chose to represent the prior distribution as a mul-tivariate normal, i.e. P ( f |  X  )= P ( w |  X  )= N ( w ;  X ,  X ). The motivation for this choice is the fact that the self-conjugacy of the normal distribution simplifies computation. For sim-plicity, we assume that the covariance matrix  X  is diagonal, i.e. the components of the model are independent. Thus we have: where  X  is independent zero mean Gaussian noise. One can also view y as a random sample from a normal distribution N ( y ; x T  X  w u , X  2 u ). It is possible to extend the hierarchical model to include a prior on the variance of the noise  X  u and learn it from the data. For simplicity, we set  X  2 u constant value of 0 . 1 for all u in our experiments.
Based on Equation 1, the MAP estimate of the user model w u for a particular user u is [7]: w u MAP = argmax w log P ( w |  X  )+
Equation 2 also shows the natural tradeoff that occurs between the prior and the data. As we collect more data for user u , the second term is expected to grow linearly with N u . This term, which correspo nds to the mean square error ( L 2 loss) of the model w on the training sample, will eventually dominate the first term, which is the loss associated with the deviation from the prior mean.

The variance of the prior  X  also affects this tradeoff. For instance, if the values on the diagonal of  X  are very large for all j , the prior is close to a uniform distribution and Figure 1: Illustration of dependencies of variables in the hierarchical model. The rating, Y ,foradoc-ument, X , is conditioned on the document and the model, W , associated with that user. Users share information about their model through the use of a prior,  X  . reverse situation, the prior dominates the data and the MAP estimate will be closer to  X  .

So far, we have discussed the learning process of a single user and assume the prior is fixed. However, the system will have many users, and the prior should be learned from the other users data. We use an initially uniform prior when the system is launched 2 , i.e. P (  X ,  X ) is the same for all  X  , X .
As users join the system, the prior distribution p ( f |  X  )is updated. At certain point when the system has K existing users, the system has a set of K user models w u : u =1 ...K . Based on the evidence from these existing users, we can infer the parameters  X  =(  X ,  X ) of our prior distribution: This is, in essence, the mechanism by which we share user information: the prior mean is the average of that param-eter for all other user models. Similarly, the prior variance captures how consistent a parameter is for all users.
Given a large pool of data, we need to find the MAP estimation of all f u and  X  in Figure 1. A simple approach is to treat it as one large optimization problem where all parameters are optimized simultaneously. If there are K users with data sets  X  D = { D 1 ,...,D K } then we can write
It is, of course, possible to extend the hierarchical model to include a prior distribution for the parameters  X  and  X . down the objective function as: where  X  w = { w 1 ,...,w K } . This objective function corre-sponds to the posterior distribution over user models and the parameters of the prior distribution; it is the product of the posterior distributions of each user model, but with  X  and  X  also maintained as free variables. Importantly, this function is concave and standard optimization algorithms exist for finding its global maxima. The problem with this approach, from a system X  X  perspective, is that we cannot update one user X  X  model without updating everyone else X  X . Fortunately, a user X  X  model is independent of other users X  models given the prior  X  , and the model prior  X  learned from many existing users is unlikely to change much when a new user joins the system. Message passing algorithms, or belief propagation , take advantage of this independence. Specifically, if we assume  X  is fixed then we can update w independently of any other user X  X  data.

Although privacy is not a major focus of this paper, this learning process alleviates a common concern with sharing data among users. In a distributed environment where each individual user/peer keeps his/her own data and user model, w u is the sufficient statistic summarizing all the data about a user u , and only this information needs to be propagated to a central server that keeps the prior. The central server ag-gregates all the user models and summarizes them as (  X ,  X ) and shares only this information with each individual user.
Some experiments are carried out to understand the Bayesian hierarchical user modeling approach using Gaussian net-works and value of implicit feedback. The following two data sets are used in our experiments: [3] Claypool, et al. developed a web browser (called Cu-[18] Zhang modified the Curious Browser and used it for Table 1: Listing of the datasets used in the exper-imental evaluation. The third (I) and fourth (D) columns indicate whether the dataset contains im-plicit and explicit feedback, respectively. In both studies the users rated the documents on a scale of 1 to 5; a rating of 1 indicated the document was not of interest and a rating of 5 indicated it was.

All the features were normalized to zero mean and unit variance. These two data sets were collected through two user studies carried out by different research groups. They enable us to evaluate our tasks on data collected in very different experimental settings. There are no queries in both tasks. Compared to the traditional evaluation data set used in TREC, these data sets are very noisy and many of the features are not very informative.

In order to evaluate the usefulness of different types of features we formed 4 data sets, listed in Table 1 in total by taking subsets of features. Let D be the set of features the system gets right after the document arrives and before the user reads the document. This includes three features the system derived from the explicit feedback ( X  X elevance score X ,  X  X eadability score X ,  X  X opic familiar before X ) as well as the document length, server speed, and number of in-links to the news server. Since the first three features in D are learned from the user X  X  explicit feedback, we also refer to D as explicit feedback. Let I refer to implicit feedback ac-quired after the user reads the document. Claypool X  X  study included only features in set I and Zhang X  X  study included both. It is important to note that our use of implicit feed-back departs from a realistic application setting: we can X  X  very well decide to deliver a document based on the user X  X  in-teraction with it. This information is only available after the document has been delivered. Regardless, it is worthwhile to explore the predictive abilities of such a cheap resource.
Our aim is to see how a system that has already estab-lished itself can accommodate a new user, not to emulate a system starting from scratch. For each user, we simulate the following setting: all other users are already present on the system and then the user in question begins to use the system. This approach is very similar to the  X  X eave one out X  technique, and each time we leave one user out to test the system perform on that particular user over time. The original order in which the user viewed the documents is pre-served. The system receives the user rating right after the user finishes reading a document, and this training example is used to immediately update the user X  X  model.

The experiments compare the following four separate mod-els (Following each model name is its corresponding symbol in the experimental results figures that we will discuss in Section 6): Prior ( ) Model based on the hierarchical framework de-No Prior ( ) Linear model trained with only this user X  X  Generic (  X  ) Linear model trained with data available from Moving Average (  X  X  X  ) This simplistic model simply re-
We also include a simplistic baseline measure which corre-sponds to rating every document as 5. In both user studies, users only provide feedback on documents they viewed/clicked and so the baseline measure loosely corresponds to inter-preting click-throughs, the most common type of implicit feedback, as positive feedback.

For each model, at every time step, we evaluate it on the next document in the sequence 3 . Since the objective of the system is to minimize the squared error ( L 2 loss), we use it as our evaluation measure.
Table 2 reports the macro-average performance of each model. In all cases the hierarchical model X  X  (Prior) im-provement over the user-independent model (No Prior) and non-personalized model (Generic) is statistically significant. For all data sets from the Zhang study it also performs sig-nificantly better than the Moving Average. Note that in both cases where only implicit feedback is used, Z I and C ,
If the model X  X  prediction of whether the user likes a docu-ment or not is higher than 5 or smaller than 1, the value is set to 5 or 1. This clearly violates some of the assumptions underlying our model, but it is an simple step that anyone developing such a system would take.
 Prior 0.887 0.880 1.017 1.440 No Prior 1.021 1.118 1.141 2.762 Generic 1.027 1.012 1.157 1.648 Moving Average 1.037 1.446 Baseline 3.191 4.692 Table 2: Performance of different models aver-aged over time and user (in that order). For all datasets except C the hierarchical model performed better than the remaining methods according to a Wilcoxon signed rank test with 95% confidence. On the Claypool dataset the 0 model did not signifi-cantly outperform the Moving Average. the Moving Average performs better than the No Prior and Generic models.

Since a major focus of our study is the  X  X old start X  prob-lem, we did some further analysis to study the performance as a function of time or, equivalently, the number of train-ing examples. The error of each model X  X  performance on the Z ID data set is in Figure 2. The performance of each model at each time point is the average of all users, and the performance of a user at a time point is estimated us-ing the average over a 50 document window; this gives us a smoother picture of the general trends in performance. The number of labeled documents from each user varies. If a user X  X  number of labeled data is smaller than the number indexed by the x-axis, the user performance can not be es-timated and not included in the average on that time point. Thus the number of users is 15 at the very beginning when the number of training examples is small, and the plot stops when there are fewer than 5 users left for evaluation.
Figure 2 shows that the hiera rchical user model outper-forms the No Prior model and the Generic user model for the first 100 documents. Compared to No Prior, the use of a prior learned from other users helps the initial performance. The figure also shows that the prior and non-prior models converge as more data from a user is available. These ob-servations are what we expected. We truncate the x-axis to 100 to see more detail in the performance in the earlier stage of learning (Figure 3). Here the performance of a user at a time point is estimated using the average over a 10 document window.

Figures 2, 3 show the Generic model performs very simi-larly to the hierarchical model for the first 200 documents. This suggests that the effect of  X  X ersonalization X  does not occur until quite a bit of user feedback is gathered. In both studies, each user is interested in several topics (as compared to TREC style single topic user profile) and, therefore, there is a necessary increase in model complexity. As a result, a significant amount of data is needed for effective personal-ization.

The simple Moving Average model performs surprisingly well, significantly better than the baseline and among the best at the early stage of learning. This demonstrates the importance of user bias in rating, which is the only thing the moving-average tries to model. The Moving Average performance seems to degrade over time, indicating some kind of shift in how the users are rating the document. This may happen if the user suddenly finds a good news source Figure 2: Mean squared error for Z ID of model with Prior, with No prior, Generic user-independent model, and the Moving Average estimator. The bot-tom part of plot shows the results of a 95% confi-dence t-test comparing the Prior model to the oth-ers. A solid block indicates that the Prior performed better, an empty block indicates it performed worse, and no block means the difference in performance was statistically insignificant. or if the user has a new interest. User bias in rating has been previously observed and explicitly modeled [14]. Our results not only confirm the prior work but also suggest a need for dynamic modeling of user-bias.

Figure 4 shows each model X  X  performance on the Z I data set. Compared to Figure 2, the prediction power of each model using only implicit feedback is much worse than us-ing all information. The hierarchical model is not signifi-cantly different than the Moving Average when the number of training data is smaller than 200, while No Prior per-forms even worse than the Moving Average. Figure 4 also shows that the Prior and No Prior models are better than the Moving Average when sufficient training examples (more than 200) are available. This suggests that a large amount of implicit feedback from a particular user could be useful, which is consistent with Teevan, et al.[15].
 Results for Claypool X  X  data set ( C )areshowninFigure5. The range of the x-axis is much smaller than that of Figure 2, because the number of examples per user was far fewer in this study. The hierarchical user model performs similar to the moving-average, while the non prior model performs worse than. This might due to the fact that the system has not accumulated enough implicit feedback to make them useful. Thus, the relative performance of the models is sim-ilar to that of the early stage of Z I , which also only contains implicit feedback. Compared with Z I , the absolute perfor-mance on this data set is worse. The task of Claypool X  X  user study was unrestricted, thus the correlation between implicit feedback and user rate is smaller than Z I .Thisisnotsur-prising since previous studies on the usefulness of implicit feedback have reported both negative and positive results [9][13]. In particular Kelly and Belkin showed there to be very little correlation across tasks between time spent view-Figure 3: Mean squared error for the first 100 doc-uments in Z ID . See Figure 2 caption.
Figure 4: Results for Z I . See Figure 2 caption. ing a page and how useful they thought the page was. Our results support Kelly X  X  conclusions despite the fact that over the entire data set C , with all user X  X  aggregated, there is a slightly positive correlation between the time spent viewing a page and user rating.

Finally, in order to compare the performance across dif-ferent feature sets we give a side-by-side plot of the hierar-(Figure 6). It shows that the implicit feedback is unreliable by itself and has only marginal value when combined with explicit feedback.
In order to better understand why implicit feedback does not help at the early stage of learning and becomes useful at the later stage when there is a large amount of data, we can decompose the generalization error of a learning algorithm into3parts: In general, one wants to use a learning algorithm with both low bias and low variance. However, there is a natural  X  X ias-variance trade-off X  for any learning algorithm[5]. Bias-Variance Dilemma As the complexity of the learn-The purpose of user profile learning is to find a predictor with a good balance between bias and variance. Adding im-plicit feedback increases the complexity of the learning algo-rithm, reducing the bias and increasing the variance. When there are very few training s amples, the variance may be the dominant contributor to the generalization error, thus a less complex learning algorithm with less variance such as Moving Average is preferred to a more complex learning algorithm such as No Prior. When there are many train-ing samples, the bias can be the dominant contributor to the generalization error, thus No-Prior is preferred. One needs to be very careful while using implicit feedback. Al-though informative, implicit feedback may hurt the perfor-mance when the amount of training data is small and the learning algorithm is not well regularized. The hierarchi-cal Bayesian modeling approach uses the prior learned from other users as a regularizer to control the model complex-ity and balances bias and variance based on the amount of training data available. Thus, it consistently works well.
This work is motivated by the fact that there is substan-tial variation in behavior across users; an example of this is a user X  X  mouse-keyboard preference. It is illustrative to look at the behavior of a  X  X ifferent X  user X  X  model over time. The average correlation coefficient between time-spent-on-mouse and the rating given to a document across users is 0.01. The Figure 6: Squared error of hierarchical model using different feature sets. Z I includes only implicit indi-cators, Z D only includes document content features, and Z ID includes both. correlation coefficient between these two variables for user 15 is 0.18, the highest among all users . The weight associ-ated with this feature for user 15, over time, is illustrated in Figure 7. Both the hierarchical and non-hierarchical model demonstrate an adaptive behavior; the generic model does not change much at all. But observe that the hierarchical model is far more stable in the early stages of training. In the later stages it seems to have converged with the non-hierarchical model.

All models used in this paper implicitly assume that user behavior is consistent over time. In reality this is hardly ever maintained. The fact that the performance of all mod-els does not increase monotonically as the number of training data increases (Figure 2, 4, 5) seems to suggest shifting user behavior. And, in fact, we observed dramatic shifts, over time, in the correlation between certain features and doc-ument rating. How to model these shifts is an interesting, and important, topic for future work in personalization.
We used linear model to represent a user model, because 1) the earlier work on text classification shows that a well regularized linear model works very well for text classifica-tion compared to other state of art linear models such as SVM or logistic regression [11]; 2) it is computationally effi-cient. The drawback to our approach is that it assumes the relationship between various information about the docu-ment and the user rating is well approximated with a linear function. The assumption may be wrong, and the linear model is far from the optimal modeling approach for our task. A first step towards increasing performance would be to try an alternative set of models, such as regression trees, or mapping the features to a non-linear space.

It is worth mentioning that our simulation of the online learning is different from the real scenario, and only docu-ments a user clicked and evaluated are used for training and testing in our experiments. The significant improvement of our approach over the baseline demonstrates that all learn-ing approaches are better than treating click through as pos-itive feedback. Further investigation into the documents not clicked is a critical step to fill the gap between the paper and Figure 7: Weight of time-on-mouse, over time, for user 15. arealsystem.
Variationinhowusersinteractwithdocumentsmotivates the need for user specific modeling (personalization). A per-sonalized system needs to work well for a new user although the information about the user is limited. This paper ex-plores the usefulness of cheap implicit feedback and borrow-ing information from other users to improve early perfor-mance. We use hierarchical Bayesian models as a unified framework to achieve the two goals. Based on the condi-tional independence relationship implied by the hierarchical model, we use an efficient training technique to learn a user specific model over time.

Ontwodatasetscollectedfr om user studies carried out by two different research groups, we demonstrate that the hierarchical Bayesian modeling approach effectively trades off between information from o ther users (prior) and user-specific information (data) based on the number of training data from the current user, alleviating poor initial perfor-mance for each user. Second, we evaluate the benefits of utilizing implicit feedback in user profiling and find that it is only marginally useful, even when combined with explicit feedback. Compared to the baseline of treating every clicked document as relevant, all learning techniques studied in pa-per perform significantly better.

The personalization problem is far from solved and our work is just one step in this direction. The work described here poses more questions and challenges in building a per-sonalized adaptive retrieval/filtering system. Our results suggest that the performance of a system is influenced by some other hidden factors, such as the shifting of user be-havior or feature noise, both of which could overwhelm the benefit of the modeling approach. This research was funded in part by Google Research Award. Any opinions, findings, conclusions or recommen-dations expressed in this paper are the authors X , and do not necessarily reflect those of the sponsors.
