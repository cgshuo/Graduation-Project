 Personalisation is an important area in the field of IR that attempts to adapt ranking algorithms so that the results returned are tuned towards the searcher X  X  interests. In this work we use query logs to build personalised ranking models in which user profiles are constructed based on the represen-tation of clicked documents over a topic space. Instead of employing a human-generated ontology, we use novel latent topic models to determine these topics. Our experiments show that by subtly introducing user profiles as part of the ranking algorithm, rather than by re-ranking an existing list, we can provide personalised ranked lists of documents which improve significantly over a non-personalised baseline. Fur-ther examination shows that the performance of the per-sonalised system is particularly good in cases where prior knowledge of the search query is limited.

The vocabulary problem, where people use the same terms to describe different needs, is a well-known issue affecting IR systems and was identified early in the field X  X  development. Despite this, most IR systems treat all users equally and attempt to return an optimal ranked link for the  X  X verage user X . Recent years have seen a slight increase in the use of personalisation to improve search results and a correspond-ingly gradual increase in our understanding of how to tackle the problem. By understanding more about the user issuing a query, we can tailor the ranked list such that the likelihood of highly ranked URLs being relevant is increased. Much early work was unsuccessful and studies have subsequently shown that great care must be taken when applying person-alisation so as to avoid damaging an already near-optimal ranked list [11, 5].
 To construct a personalised ranking we require some knowl-edge of that user X  X  previous search behaviour. This  X  X ser profile X  should represent the topical interests of the user and can be built by considering searches made by the user prior to the current one, more specifically the query terms used and URLs clicked. In early work these profiles were simply the raw terms of prior queries or the content of clicked docu-ments, however these profiles often proved to be ineffective, perhaps being too fine-grained given the limited amount of data available. To deal with the sparsity, profiles can be based on the topics of each document, using categorisations from human-derived resources such as the ODP [10]. This approach is problematic as many URLs may not be present in the online categorisation scheme and requires that people determine the correct categories for each URL, an expensive and error-prone process.

Click-through data, in the form of query logs, is an abun-dant source of information regarding search behaviour and is therefore often used for personalised search [13]. Query logs generally take the form of triples, consisting of a user ID, a search query and a clicked URL. Each clicked URL is assumed to be either a vote confirming its relevance or a preference for that URL over other un-clicked URLs pre-sented higher in the list. In this work we assume each click on a URL represents an implicit vote of its relevance to the query and that the query words used to search for it repre-sent its content. This allows us to construct representations of URLs, build personalised search models and then fairly evaluate their performance, since the query logs represent user-specific relevance judgements in context.

We use query logs to build personalised models in which user profiles are constructed based on the representation of clicked URLs over a topic space. However, instead of employing a human-generated ontology we use latent top-ics. The topic space is therefore extracted directly from the query log itself and there is no need for human intervention to define the topics. Our experiments show that by subtly introducing user profiles as part of the ranking algorithm, we can provide personalised ranked lists of documents which improve significantly over a non-personalised baseline, espe-cially in cases where prior knowledge of the search query is limited.
The idea of using prior interactions with the search system to construct user profiles has been tackled in a number of different ways, differing based on the length of profile data used and how this data is turned into a suitable user profile. Some approaches consider only information from the current search session [15], others attempt to identify longer-term user interests [8] and some even combine short and long-term profiles [1]. Short-term data is often too sparse to permit robust personalisation and only delivers solid improvements late in long search sessions, which are rare. In this work we focus on long-term click data as it provides a richer source of information about the user X  X  interests and preferences.
Once interaction data has been chosen it must be con-verted into a representation of the user X  X  interests. Some ap-proaches use vectors of the original terms [8], often weighted in some fashion while others map the user X  X  interests onto a set of topics, which are either defined by the users them-selves [9] or extracted from large online ontologies, such as the ODP [10]. Dou et al. [5] investigated methods for cre-ating user profiles and generating personalised rankings us-ing query logs by using a set of pre-defined interests and a K-nearest neighbour approach for clustering similar users. Here we take a similar approach by reducing the dimen-sionality of the data, however we do not rely on predefined categories but rather derive topics from the data as part of the estimation process. Dou et al. found that personalisa-tion is not appropriate for all users and/or queries and may even harm performance, for example for highly unambiguous queries (e.g. navigational queries such as  X  X oogle X ), where the original ranking is close to optimal for all users. For queries which are both unambiguous and common, optimal results can be obtained by simply ranking URLs in order of their prior probability of being clicked for that query. How-ever, this approach is clearly not feasible for most queries, where either scant or no prior click data is available.
Teevan et al. [12] confirmed these results and investi-gated for which queries personalisation most improved per-formance, finding that query ambiguity provides a good in-dication of how much benefit will be gained from person-alisation. For low ambiguity queries (where all users find the same results relevant), personalisation can have a neg-ative impact on performance. This work indicates that one must be careful when personalising to ensure that too much weight is not given to the user profile in deference to the unpersonalised URL score. In later work Teevan et al. [11] demonstrate that the potential that each user/query pair holds for effective personalisation can in some cases be pre-dicted a-priori, allowing the system to select between per-sonalised and unpersonalised rankings.
The key idea of personalisation is to use the terms from the query the user submitted and the specific URL(s) they clicked on in the results list to build a topic level description of the user. The clicked URLs should then represent solu-tions to the actual information need that the user expressed via the query. For example, given an ambiguous query such as  X  X ava X , a user interested in computers is likely to click very different URLs to a user who is interested in coffee.
If we have observed a user/query pair before then we may wish to assume that the user will click on the same URL(s) as before. If the query has been submitted many times before, but by other users, then we may still be able to use this infor-mation to provide a good ranking. Consider an unambiguous query such as  X  X acebook X  where almost all users will want to click on the same URL. Here, a sensible option is to rank the documents in descending order of prior click frequency [5]. Unambiguous queries can be identified by a measure known as click entropy : H q = P d  X  D ( q )  X  P ( d | q ) log D ( q ) is the set of clicked documents for query q and P ( d | q ) is the (relative) frequency with which document d was clicked amongst all the clicked documents for query q . The range of possible entropy values depends on the query, making it problematic to compare click entropy values across queries. One way to deal with this issue is to report normalised en-tropy values instead, where we limit the range of values to measure for click entropy in our experiments.

Queries with low click entropy are good candidates for the simple  X  X ollaborative X  ranking method mentioned above, conversely, queries with high click entropy are more compli-cated and thus the ideal ranking will likely depend on the user who submitted the query. A second indicator of query  X  X ifficulty X  is the length of the query since longer queries by their very nature contain more (discerning) information and are therefore less likely to be ambiguous. It has been shown that the difficult queries (those which are short and/or have high click entropy) are the ones for which personalisation can potentially deliver significant ranking improvements [11].
If we do not have sufficient prior click data to build a ranked list then we rank the URLs in decreasing order of probability of being relevant to the query. An unperson-alised ranking can be generated by ranking documents in descending order of similarity to the query, or equivalently in descending order of likelihood that the query was gener-ated by the document. In order to do this we must first have some representation of each document in the collection. As we use query logs as a source of data for both training and testing our models, we construct document representations from these logs. To do so for a document d , we consider all of the terms of the queries in the log which resulted in the user clicking on d , conflating these terms over all users and queries. This follows the theory that queries should be ran-dom draws from the Language Models of the documents for which they are relevant and it has been shown that queries and URL content are strongly correlated [3].

Due to the relatively sparse nature of the language mod-els derived from the query logs and the success of using such methods on short documents in the literature [7, 14], we in-vestigate the use of topic models to represent the documents over a reduced-dimensionality latent topic space. We take a similar approach to many personalisation models in the lit-erature [10], namely that lower-dimensional categories are a better representation of a document X  X  topical coverage than its raw terms. However, instead of obtaining topic alloca-tions from an online ontology, which may have poor cover-age, low levels of granularity and a lack of novel vocabulary, we derive topics from the data itself.
Topic models attempt to probabilistically uncover the un-derlying semantic structure of a collection of documents based on analysis of only the vocabulary words present in each resource, this latent structure is modelled over a num-ber of topics which are assumed to be present in the collec-tion. In this section we briefly discuss a latent topic model that extends Latent Dirichlet Allocation (LDA) [2, 6] and was first applied to query-log based personalised retrieval, without success, by [4]. We use the model to derive topic allocations for each of the documents and to determine each user X  X  topical interest profile. Finally, we discuss variations on the basic model that allow for successful personalisation.
Figure 1 shows a graphical model diagram for a person-alisation topic model which involves an observed document d , a latent topic variable z , an observed word w and an ob-served user u . This structure is repeated for all words in a user X  X  query, all queries by the user and all users in the log. Here we make the modelling assumption that the user, as well as the word, is dependant on the topic. That is, given the topic distribution of the document, there will be a number of words chosen at random from those topics to describe that document and there will be a number of users who chose to click on that document. These users will be  X  X hosen X  according to the topics covered by the document, encoding the idea that users probabilistically choose doc-uments based on their own topical interests and how well these match to the document X  X  topical coverage.

The parameters of this model are a probability vector over topics for each document  X  d , a probability vector over words for each topic  X  z and a probability vector over users for each topic  X  z . Symmetric Dirichlet priors with hyperpa-rameters  X  ,  X  and  X  are placed over the three distributions in order to prevent them from overfitting the data. The hy-perparameters essentially act as pseudo counts allowing the model to fall back on uniform distributions in the event of sparse data. Given the prior distributions, expected values for the parameters under their respective posterior distribu- X   X  noting the number of times the topic z appears together with the word w , document d and user u respectively. N z and N d are the number of times topic z and the document d occur in total. W is the vocabulary size, Z is the number of topics and U is the number of users.

Exact inference for topic models is intractable, however a number of methods of approximating the posterior dis-tribution have been proposed including mean field varia-tional inference [2] and Gibbs sampling [6]. In this work we apply Gibbs sampling and average parameter estimates over consecutive samples from the Markov chain. Using this model we can construct a ranking formula which considers the probability of each document given both the words in a query and the profile of the searcher. However, as out-lined earlier in the paper, personalisation must be applied in a very subtle manner. By directly including the user in the model we consider his/her topical interests to be equally important when describing a document he/she has clicked as the document X  X  own description. In [4] we demonstrated that this assumption is too strong and so instead we con-sider a different model which does not explicitly include the user in the topic sampling but instead calculates a topic dis-tribution for each user after the sampler has converged.
This alternative approach is depicted in Figure 2 where we see that the user does not play a part in the sampling. After the Markov chain has converged, samples from the chain are used (as per normal) to calculate the 3 posterior means. The estimates for each user X  X  interests over the topic space (actually distribution over users for each topic  X  z ) are still obtained, however the sampler does not use these estimates to calculate the conditional distribution over topics when sampling the topic to assign to each word position.
The intuition here (i.e. calculating P ( u | z ) and not vice-versa) is that we wish to capture the idea that a user clicks on a document given a specific query due, in part, to his/her interests which are expressed over the topic space. We know from our estimates for  X  d which topics are covered by a doc-ument and therefore by multiplying this with P ( u | z ) we can express (a quantity proportional to) the probability that the user u would have clicked on this document, given the user X  X  interests. This means that if the model is confronted with a new query in which none of the constituent terms have been used by the user previously, it should still be able to map the query onto the user X  X  topic-based profile. This would clearly not be the case if were to instead use the raw (unigram) terms to build the user profiles.
Given a query q we wish to return to the user a ranked set of documents ( d  X  X  ) according to their likelihood given the query under the model, which in the case of an unper-sonalised (LDA) model can be estimated as follows:
P ( d | q )  X  P ( d ) P ( q | d ) = P ( d ) Y Notice that the ranking formula is the product of 2 distinct parts; a prior document probability P ( d ), and the probabil-ity of the query given the document P ( q | d ), the latter being estimated using parameters from the topic model. In our experiments we use the click information to get a Dirichlet smoothed estimate of P ( d ) based on the relative frequency of clicks on that particular URL in the query log:
For the personalised ranking model, we also know which user issued the query and can therefore include that user X  X  preferences into the ranking formula. We do that by simply ranking documents according to their likelihood given both the query and the user as follows:
Now the estimate of the probability of a document in-cludes the probability of the user clicking it, given its simi-larity to the user X  X  interests over the topic space. Note that we have extend this basic personalisation model by introduc-ing an additional parameter  X  in the range zero to one, which we use to weight the probability of a user given a particular topic P ( u | z ). This new parameter is of critical importance since it allows us to control, in a coherent and discriminative fashion, the amount of influence that the user X  X  topical inter-ests have on the overall ranking. The intuition behind the introduction of this parameter is that documents likely tell us more about their own topic distribution than the users who click on them do.
We now discuss our experiments, comparing the perfor-mance of our personalisation approach with an unperson-alised baseline model.
Figure 2: Model used to estimate parameters.
To evaluate our models on real-world data where each query was made in context we made use of the AOL Query Log dataset. The log contains the queries of 657,426 anony-mous users over a 3 month period from March to May, 2006. It is, as far as we know, the only publicly available dataset of sufficient size to perform our analysis. We protected user privacy by analysing results only over aggregate data.
To clean the data we first selected those queries which re-sulted in a click on a URL, then selected only URLs that more than 100 users had clicked at least once. We then selected only those users with more than 100 remaining queries, ensuring that all users in the dataset have a rea-sonably large number of queries from which to build profiles and that the documents constructed for each URL are of a reasonable size. Queries were tokenised, all punctuation was removed and Porter X  X  stemming algorithm was applied. We did not remove any stopwords but did remove any single-ton terms as it is not possible that such a term would exist in both training and testing sets and therefore they would be useless for ranking. The resulting reduced data set is described in more detail in Table 1. Table 1: Statistics of dataset used for experiments.
We separated the dataset into training and testing subsets by retaining the last 5% of entries for each user for testing. This ensures the test and training sets are distributed over users in the same way and follow the correct chronological or-der: i.e. all entries used for testing were submitted after the last training data point. As input to our ranking algorithm we use the test set queries and we use the URL that was actually clicked for each query as the relevance judgement. This method allows us to fairly evaluate our personalisa-tion model as only the user(s) who originally submitted the queries can really say whether a given URL is truly relevant to them or not. We believe this will accurately reflect the performance of a live system and is likely to actually give a slight under-estimate of the true performance.

We evaluate ranking performance using success at rank k (S@k) 1 and the mean reciprocal rank (MRR). We are pri-marily interested in how well these models rank URLs we report the S@k and MMR up to rank 10 as they are the most commonly reported in other literature since people tend to only pay attention to the first page of results in a ranked list. To determine how well our method is working in comparison to the LDA baseline, we also report a 3rd metric that we re-fer to as Personalisation Gain (P-gain) which compares the number of times the personalisation algorithm improves the ranking with the number of times it worsens it. This can be simply expressed as the following:
Therefore a value of 0 indicates no overall change in the rankings due to personalisation, a positive value indicates an improvement, a negative value indicates a degradation.
We experimented with a large range of settings for both the number of topics in each model and the hyperparameter settings for each prior. The models were not sensitive to the setting of the hyperparameters and we therefore set the con-centration parameters  X  ,  X  and  X  to be 50.0, 50.0 and 0 . 1 W respectively, which is common in the literature [6]. For sam-pling we use the collapsed Gibbs sampler [6], running the chain for 400 iterations in total, as this appeared to consis-tently give good convergence in terms of model likelihood. We discarded the first 300 samples of the chain as  X  X urn-in X  and averaged estimates over the final 100 samples.

When using hidden topic models an important consider-ation is how complex the model should be in terms of the number of topics. Each topic model can be viewed as being a class of an infinite number of different models, where the complexity in number of topics is in the range { 1 ,...,  X  X  . We estimated parameters for the topic models over differ-ent numbers of topics to see how retrieval performance was effected and found that improvements began to level off af-ter around 125 topics. As a result we make use of models consisting of 150 topics for all of the following analysis.
Table 2:top shows the results of the ranking experiments for the 2 models. A cursory glance suggests that the im-provements delivered by the personalised model are not par-Since there is one clicked URL per query, P@k is equal to S@k/k and thus is not reported separately. LDA 0.2122 0.4283 0.2765  X  PTM 0.2146 0.4316 0.2791 0.0466  X  LDA 0.2341 0.4766 0.1403  X 
PTM 0.2646  X  0.4991  X  0.1599  X  0.1962  X  % improv. 11.5% 4.5% 12.3%  X 
Table 2: Ranking performance on the test data. ticularly large; P-gain shows that, on average, the person-alised model is improving upon the baseline in 4.66% of cases. However, looking at the results more closely we found that in many cases (91,280) there was no difference in the rank of the relevant document returned by the 2 models. Since these queries are quite common and the performance metrics are based on taking averages over all queries, this strongly dilutes the impact of the personalised model. We will refer to such queries as same-rank queries , queries where the rank position of the relevant document was different will be referred to as different-rank queries .

The click entropy of the same-rank queries was signifi-cantly lower than that of the queries where this was not the case. Furthermore, the different-rank queries were signifi-cantly more likely to be novel (i.e. not observed in the train-ing data); overall 45.8% of same-rank queries were present in the training data, compared with 25.7% of different-rank queries. These observations have 2 important outcomes: 1) for many same-rank queries we can rely on the prior click data to deliver accurate ranking results and 2) a much larger proportion of the same-rank queries have low click entropy, meaning that they are poor candidates for personalisation. Figure 3:  X  in rank position between the 2 models.

We now focus on the  X  X arder X  different-rank queries, which constitute 14.25% of the total queries. Table 2:bottom shows the ranking results for different-rank queries. For these queries, the personalised model is able to deliver much bet-ter results in comparison to the non-personalised baseline, registering an improvement in rank in 19.62% of cases. In fact the difference in performance over all metrics is signifi-cant 2 (p-value 0.01). The improvements are particularly noticeable in the lower ranks, resulting in a significant in-crease in S@1 and MRR.

The ranking performance of the 2 models can be better compared by considering the difference in the ranks of the
As determined by 2 sample proportion z-test. relevant document. Figure 3 shows the distribution of the difference in the ranking of the relevant document for each query between the 2 models. The darker bars show the num-ber of queries where the ranking was improved, the lighter bars where it deteriorated,  X  X ther X  refers to all rank changes greater than 5. The ratio between improved and deterio-rated queries increases with the change in rank position: at a rank change of 1 the ratio is only 1.33:1, however it is as high as 1.91:1 when we look at queries where the change in rank was greater than 5. This indicates that for a number of queries the personalisation is able to move the relevant doc-ument much higher in the rankings, however the opposite case occurs very infrequently.
Query length 1 2 3 4 &gt; 4 # better 615 1,893 1,685 1,242 1,449 # worse 203 1,082 1,145 953 1,243 P-gain 0.504 0.273 0.191 0.132 0.077 Table 3: Counts of better and worse ranks and P-gain values for queries of different lengths. Person-alisation performance is query length dependent.

Table 3 details how the performance of our model changes as the length of the queries change. The performance gain of the personalised model is clearly much better for shorter queries, particularly for queries of length 1 or 2, however as the query length increases, the performance of the person-alised model -relative to the unpersonalised one -decreases. As a result, by focusing purely on queries of length 3 or less, we can achieve a p-gain of 0.265. Regardless of query length, the personalised model is still able to outperform the baseline, however the number of queries for which it is able to produce a better ranking decrease as query length increases. This ties in with the idea that personalisation is much more effective for ambiguous queries where there is likely to be more variation between different users. In the case of longer queries, the extra information included in the query reduces the uncertainty and renders the user profile information much less useful. The general performance of both models decreases as the query length increases (i.e. as the queries become increasingly less ambiguous). This is an important observation as queries tend to be short and therefore better performance is obtained for the most com-mon query lengths. In our testing data set queries of length 3 or less account for 70.69% of all queries.

Entropy 0-0.2 0.2-0.4 0.4-0.6 0.6-0.8 0.8-1.0 # better 398 429 669 605 630 # worse 236 262 319 343 267 P-gain 0.256 0.242 0.354 0.276 0.405 Table 4: Personalisation performance depends on query ambiguity as this table demonstrates.

For the more common queries we can measure the query difficulty more directly in terms of the click-entropy (see sec. 3.1). When initially looking at the click entropy, the re-sults were surprising. In general as the click entropy of the query increases, the relative performance of the personalised model appears to decrease. However, quite a large propor-tion of the entropies were calculated based on a very small Figure 4: The effect of varying the  X  parameter in the personalised ranking algorithm. number of data points (for over 10% the entropy was calcu-lated with &lt; 5 data points). An entropy calculation based on such a small sample it is very unlikely to approximate the  X  X rue X  value over the greater user population. To account for this we consider only the queries for which we have 20 or more data points. Table 4 shows how the performance of the models changed as the (normalised) click entropy of the queries increased. By restricting our analysis to only queries that were well represented in the training set we of course reduce the number of data points quite significantly, how-ever the numbers are still large enough to identify general trends. Although the trend is not nearly as clear as it was for the query lengths, we can see that as the click entropy of the query increases, so too does the relative performance of the personalised model (correlation = 0.71).
The parameter  X  controls the amount of influence the user profile has on the document scores, we tested this param-eter within values in the range 0 ... 0 . 5, where  X  = 0 col-lapses the model back to LDA. The effect on performance, in terms of P-gain, over all queries (dashed line) and over just the different-rank queries (solid line) is shown in fig-ure 4. For different-rank queries as the parameter value is decreased, the performance seems to increase. However as  X  decreases the total number of different-rank queries also decreases, since the differences between the 2 models are becoming increasingly smaller. Therefore it is not ideal to optimise  X  based purely on performance on these queries as we also want to ensure a positive impact on as many queries as possible. For example in setting  X  to 0 . 025, which appears to yield the best performance, the number of different-rank queries is reduced to just 5,331 (5.2% of the total). If  X  is instead optimised for performance over all queries (  X  = 0.175) then the improvement over the subset of different-rank queries is still very high, however the size of this set is increased to 14,656 (14.25%). Note that we have not in-cluded points in the plot for  X  = 0 because in this case the algorithm simply collapses back the unpersonalised model and all p-gains are 0.
This work has presented a new approach to query log-based personalisation using latent topic modelling to de-scribe both the clicked URLs and the interests of users over the same topic space. We note that getting topic-modelling based personalisation to work successfully required signifi-cant alterations to the basic topic model (LDA). Firstly the different parameters of the model needed to be estimated sequentially (rather than contemporaneously), and secondly an additional parameter controlling the influence of the user profile on the ranking needed to be introduced and carefully tuned. By testing this new approach on real click log data we have shown that it is applicable in a  X  X eal world X  scenario and have shown that it is able to outperform the unperson-alised model in all cases, particularly in the case of difficult queries. These difficult queries -where personalisation per-formance is best -can be quite easily identified, for example by using simply the query length. Since queries are normally short, they account for a large percentage of the total. We believe that there are further gains to be achieved by taking into account to what extent the user profile differs from that of the  X  X verage user X , whereby the more particular the in-terests of the user, the more likely personalisation is to have a positive effect. More generally, we would like to estimate the extent to which the user profile reduces the ambiguity of the query and use that to decide for which query-user pairs to personalise the results. We leave this for future work.
