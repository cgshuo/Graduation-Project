 Partial Least Squares (PLS) is, in its general form, a family of techniques for analyzing relations between data sets by latent variables. It is a basic assumpti on that the information is overrepresented in the data sets, and that these therefore can be reduced in di mensionality by the latent variables. Exactly how these are found and how the data is projected vari es within the approach, but they are often maximizing the covariance of two projected expressio ns. One of the appealing properties of PLS, which has made it popular, is that it can handle data sets with more dimensions than samples and massive collinearity between the variables.
 The basic PLS algorithm considers two data sets X and Y , where samples are arranged in rows, and consists on finding latent variables which account for the co variance X T Y between the data sets. This is done either as an iterative procedure or as an eigenva lue problem. Given the latent variables, the data sets X and Y are then transformed in a process which subtracts the inform ation contained in the latent variables. This process, which is often referr ed to as deflation , can be done in a number of ways and these different approaches are defining the many v ariants of PLS.
 Among the many variants of PLS, the one that has become partic ularly popular is the algorithm presented in [17] and studied in further details in [3]. The a lgorithm described in these, will in this paper be referred to as PLS2, and is based on the following two assumptions: First, that the latent variables of X are good predictors of Y and, second, that there is a linear relation between the late nt variables of X and of Y . This linear relation is implying a certain deflation scheme , where the latent variable of X is used to deflate also the Y data set. Several other variants of PLS exist such as  X  X LS Mode A X  [16], Orthonormalized PLS [18] and PLS-SB [11]; see [ 1] for a discussion of the early history of PLS, [15] for a more recent and technical descript ion, and [9] and for a very well-written contemporary overview.
 No matter how refined the various early developments of PLS be come, they are still linear projec-tions. Therefore, in the cases where the variables of the inp ut and output spaces are not linearly related, the challenge of the data is still poorly handled. T o counter this, different non-linear ver-sions of PLS have been developed, and these can be categorize d on two fundamentally different approaches: 1) The modified PLS2 variants in which the linear relation between the latent variables is substituted by a non-linear relation; and 2) the kernel va riants in which the PLS algorithm has been reformulated to fit a kernel approach. In the second appr oach, the input data is mapped by a non-linear function into a high-dimensional space in whic h ordinary linear PLS is performed on the transformed data. A central property of this kernel appr oach is, as always, the exploitation of the kernel trick, i.e., that only the inner products in the tr ansformed space are necessary and not the explicit non-linear mapping. It was Rosipal and Trejo who fir st presented a non-linear kernel variant of PLS in [7]. In that paper, the kernel matrix and the Y matrix are deflated in the same way, and the PLS variant is thus more in line with the PLS2 variant than with the traditional algorithm from 1975 (PLS Mode A). The non-linear kernel PLS by Rosipal and Tr ejo is in this paper referred to as simply KPLS2, although many details could advocate more det ailed nomenclator.
 The appealing property of kernel algorithms in general is th at one can obtain the flexibility of non-linear expressions while still solving only linear equatio ns. The downside is that for a data set of l samples, the kernel matrices to be handled are l  X  l , which, even for a moderate number of samples, quickly becomes a problem with respect to both memory and com puting time. This problem is present not only in the training phase, but also when predict ing the output given some large training data set: evaluating thousands of kernels for every new inpu t vector is, in most applications, not acceptable. Furthermore, there is, for these so-called den se solutions in multivariate analysis, also the problem of overfitting. To counter the impractical dense solutions in kernel PLS, a few solutions have been proposed: In [2], the feature mapping directly is a pproximated following the Nystrom method, and in [6] the underlying cost function is modified to impose sparsity.
 In this paper, we introduce a novel kernel PLS variant called Reduced Kernel Orthonormalized Partial Least Squares (rKOPLS) for large scale feature extr action. It consists of two parts: A novel orthonormalized variant of kernel PLS called KOPLS, and a sp arse approximation for large scale data sets. Compared to related approaches like [8], the KOPL S is transforming only the input data, and is keeping them orthonormal at two stages: the images in f eature space and the projections in feature space. The sparse approximation is along the lines o f [4], that is, we are representing the reduced kernel matrix as an outer product of a reduced and a fu ll feature mapping, and thus keeping more information than changing the cost function or doing si mple subsampling.
 Since rKOPLS is specially designed to handle large data sets , our experimental work will focus on such data sets, paying extra attention to the prediction of m usic genre, an application that typically involves large amount of high dimensional data. The abiliti es of our algorithm to discover non-linear relations between input and output data will be illustrated , as will be the relevance of the derived features compared to those provided by an existing kernel PL S method.
 The paper is structured as follows: In Section 2, the novel ke rnel orthonormalized PLS variant is introduced, and in Section 3 the sparse approximation is pre sented. Section 4 shows numerical results on UCI benchmark data sets, and on the above mentione d music application. In the last section, the main results are summarized and discussed. Consider we are given a set of pairs {  X  ( x a function that maps the input data into some Reproducing Ker nel Hilbert Space (RKHS), usually matrices  X  = [  X  ( x two matrices, each one containing n being the projection matrices of sizes dim ( F )  X  n (kernel) Multivariate Analysis (MVA) algorithms is to sear ch for projection matrices such that the projected input and output data are maximally aligned. For i nstance, Kernel Canonical Correlation Analysis (KCCA) finds the projections that maximize the corr elation between the projected data, while Kernel Partial Least Squares (KPLS) provides the dire ctions for maximum covariance: where  X   X  and  X  Y are centered versions of  X  and Y , respectively, I is the identity matrix of size n and the T superscript denotes matrix or vector transposition. In thi s paper, we propose a kernel extension of a different MVA method, namely, the Orthonorma lized Partial Least Squares [18]. Our proposed kernel variant, called KOPLS, can be stated in the k ernel framework as Note that, unlike KCCA or KPLS, KOPLS only extracts projecti ons of the input data. It is known that Orthonormalized PLS is optimal for performing linear r egression on the input data when a bottleneck is imposed for data dimensionality reduction [1 0]. Similarly, KOPLS provides optimal minimizes the sum of squares of the residuals of the approxim ation of the label matrix: where k X k ilarly to other MVA methods, KOPLS is not only useful for mult i-regression problems, but it can also be used as a very powerful kernel feature extractor in su pervised problems, including also the multi-label case, when Y is used to encode class membership information. The optimal ity condi-tion suggests that the features obtained by KOPLS will be mor e relevant than those provided by other MVA methods, in the sense that they will allow similar o r better accuracy rates using fewer projections, a conjecture that we will investigate in the ex periments section of the paper. Coming back to the KOPLS optimization problem, when project ing data into an infinite dimensional space, we need to use the Representer Theorem that states tha t each of the projection vectors in U can be expressed as a linear combination of the training data . Then, introducing U =  X   X  T A into (2), where A = [  X  the i th projection vector, the maximization problem can be refor mulated as: where we have defined the centered kernel matrices K inner products in F are involved 1 . Applying ordinary linear algebra to (4), it can be shown tha t the columns of A are given by the solutions to the following generalized eige nvalue problem: There are a number of ways to solve the above problem. We propo se a procedure consisting of iteratively calculating the best projection vector, and th en deflating the involved matrices. In short, the optimization procedure at step i consists of the following two differentiated stages: This iterative algorithm, which is very similar in nature to the iterative algorithms used for other MVA approaches, has the advantage that, at every iteration, the achieved solution is optimal with respect to the current number of projections. The kernel formulation of the OPLS algorithm we have just pre sented suffers some drawbacks. In particular, as most other kernel methods, KOPLS requires th e computation and storage of a kernel matrix of size l  X  l , which limits the maximum size of the datasets where the algo rithm can be applied. In addition to this, algebraic procedures to solve the generalized eigenvalue problem (5) normally require the inversion of matrix K A will in general be dense rather than sparse, a fact which impl ies that when new data needs to be projected, it will be necessary to compute the kernels betwe en the new data and all the samples in the training data set.
 Although it is possible to think of different solutions for e ach of the above issues, our proposal here is to impose sparsity in the projection vectors representat ion, i.e., we will use the approximation U =  X  T R B , where  X  R is a subset of the training data containing only R patterns ( R &lt; l ) and B = [  X  1 ,  X  X  X  ,  X  n strategies can be followed in order to select the training da ta to be incorporated into the basis  X  we will rely on random selection, very much in the line of the s parse greedy approximation proposed in [4] to reduce the computational burden of Support Vector M achines (SVMs).
 Replacing U in (2) by its approximation, we get an alternative maximizat ion problem that constitutes the basis for a KOPLS algorithm with reduced complexity (rKO PLS): where we have defined K keep the algorithm as simple as possible, we decided not to ce nter the patterns in the basis  X  simulation results suggest that centering  X  to the standard KOPLS algorithm, the projections for the rKO PLS algorithm can be obtained by solving The iterative two-stage procedure described at the end of th e previous section can still be used by simple replacement of the following matrices and variables : To conclude the presentation of the rKOPLS algorithm, let us summarize some of its more relevant properties, and how it solves the different limitations of t he standard KOPLS formulation: Table 1 compares the complexity of KOPLS and rKOPLS, as well a s that of the KPLS2 algorithm. Note that KPLS2 does not admit a compact formulation as the on e we have used for the new method, since the full kernel matrix is still needed for the deflation step. The main inconvenience of rKOPLS in relation to KPLS2 it that it requires the inversion of a mat rix of size R  X  R . However, this normally Table 1: Summary of the most relevant characteristics of the proposed KOPLS and rKOPLS algo-rithms. Complexity for KPLS2 is also included for compariso n purposes. We denote the rank of a matrix with r (  X  ) .
Table 2: UCI benchmark datasets. Accuracy error rates for a l inear  X  -SVM are also provided. pays off in terms of reduction of computational time and stor age requirements. In addition to this, our extensive simulation work shows that the projections pr ovided by rKOPLS are generally more relevant than those of KPLS2. In this section, we will illustrate the ability of rKOPLS to d iscover relevant projections of the data. To do this, we compare the discriminative power of the featur es extracted by rKOPLS and KPLS2 in several multi-class classification problems. In particula r, we include experiments on a benchmark of problems taken from the repository at the University of Cali fornia Irvine (UCI) 2 , and on a musical genre classification problem. This latter task is a good exam ple of an application where rKOPLS can be specially useful, given the fact that the extraction of fe atures from the raw audio data normally results in very large data sets of high dimensional data. 4.1 UCI Benchmark Data Sets We start by analyzing the performance of our method in six sta ndard UCI multi-class classification problems. Table 2 summarizes the main properties of the prob lems that constitute our benchmark. The last four problems can be considered large problems for M VA algorithms, which are in general not sparse and require the computation of the kernels betwee n any two points in the training set. Our first set of experiments consists on comparing the discri minative performance of the features calculated by rKOPLS and KPLS2. For classification, we use on e of the simplest possible models: we compute the pseudoinverse of the projected training data to calculate  X  B (see Eq. (3)), and then classify according to  X   X  0  X  B using a  X  X inner-takes-all X  (w.t.a.) activation function. For the kernel MVA algorithms we used a Gaussian kernel using 10-fold cross-validation (10-CV) on the training set to estimate  X  . To obtain some reference and 10-CV was carried out for both the kernel width and  X  .
 Accuracy error rates for rKOPLS and different values of R are displayed in the first rows and first columns of Table 3. Comparing these results with SVM (under t he rbf-SVM column), we can Table 3: Classification performance in a benchmark of UCI dat asets. Accuracy rates (%) and stan-dard deviation of the estimation are given for 10 different r uns of rKOPLS and KPLS2, both when using the pseudoinverse of the projected data together with the  X  X inner-takes-all X  activation func-tion (first rows), and when using a  X  -SVM linear classifier (last rows). The results achieved by a n SVM with linear classifier are also provided in the bottom rig ht corner. conclude that the rKOPLS approach is very close in performan ce or better than SVM in four out of the six problems. A clearly worse performance is observed in the smallest data set ( vehicle ) due to overfitting. For letter , we can see that, even for R = 1000 , accuracy rates are far from those of SVM. The reason for this is that SVM is using 6226 support vect ors, so that a very dense architecture seems to be necessary for this particular problem.
 To make a fair comparison with the KPLS2 method, the training dataset was subsampled, selecting at random l 0 samples, with l 0 being the first integer larger than or equal to  X  R  X  l . In this way, both rKOPLS and KPLS2 need the same number of kernel evaluati ons. Note that, even in this case, KPLS2 results in an architecture with l 0 nodes ( l 0 &gt; R ), so that projections of data are more expensive than for the respective rKOPLS. In any case, we mus t point out that subsampling was only considered for training the projections, but all training d ata was used to compute the pseudoinverse of the projected training data. Results without subsamplin g are also provided in Table 3 under the l 0 = l column except for the letter data set which we were unable to process due to massive memory problems.
 As a first comment, we have to point out that all the results for KPLS2 were obtained using 100 projections, which were necessary to guarantee the converg ence of the method. In contrast to this, the maximum number of projections that the rKOPLS can provid e equals the rank of the label matrix, i.e., the number of classes of each problem minus 1. In spite o f using a much smaller number of projections, our algorithm performed significantly better than KPLS2 with subsampling in four out of the five largest problems.
 As a final set of experiments, we have replaced the classificat ion step by a linear  X  -SVM. The results, which are displayed in the bottom part of Table 3, are in gener al similar to those obtained with the pseudoinverse approach, both for rKOPLS and KPLS2. However , we can see that the linear SVM is able to better exploit the projections provided by the MVA me thods in vehicle and letter , precisely the two problems where previous results were less satisfact ory.
 Based on the above set of experiments, we can conclude that rK OPLS provides more discriminative features than KPLS2. In addition to this, these projections are more  X  X nformative X , in the sense that we can obtain a better recognition accuracy using a smaller n umber of projections. An additional advantage of rKOPLS in relation to KPLS2 is that it provides a rchitectures with less nodes. 4.2 Feature Extraction for Music Genre Classification In this subsection we consider the problem of predicting the genre of a song using the audio data only, a task which since the seminal paper [14] has been subje ct of much interest. The data set we Accuracy rates analyze has been previously investigated in [5], and consis ts of 1317 snippets each of 30 seconds distributed evenly among 11 music genres: alternative, cou ntry, easy listening, electronica, jazz, layer3) encoded music with a bitrate of 128 kbps or higher, do wn sampled to 22050 Hz, and they are processed following the method in [5]: MFCC features are extracted from overlapping frames of the song, using a window size of 20 ms. Then, to capture temp oral correlation, a Multivariate Autoregressive (AR) model is adjusted for every 1.2 seconds of the song, and finally the parameters of the AR model are stacked into a 135 length feature vector fo r every such frame.
 For training and testing the system we have split the data set into two subsets with 817 and 500 songs, respectively. After processing the audio data, we have 5738 8 and 36556 135-dimensional vectors in the training and test partitions, an amount which for most kernel MVA methods is prohibitively large. For the rKOPLS, however, the compact representation is enabling usage of the entire training data.
 In Figure 1 the results are shown. Note that, in this case, com parisons between rKOPLS and KPLS2 are for a fixed architecture complexity ( R = l 0 ) , since the most significant computational burden for the training of the system is in the projection of the data . Since every song consists of about seventy AR vectors, we can measure the classification accura cy in two different ways: 1) On the level of individual AR vectors or 2) by majority voting among the AR vectors of a given song. The results shown in Figure 1 are very clear: Compared to KPLS2, t he rKOPLS is not only consistently performing better as seen in Figure 1(a), but is also doing so with much fewer projections. The strong results are very pronounced in Figure 1(b) where, for R = 750 , rKOPLS is outperforming ordinary KPLS, and is doing so with only ten projections compared to fif ty projections of the KPLS2. This demonstrates that the features extracted by rKOPLS holds mu ch more information relevant to the genre classification task than KPLS2. In this paper we have presented a novel kernel PLS algorithm, that we call reduced kernel orthonor-malized PLS (rKOPLS). Compared to similar approaches, rKOP LS is making the data in feature space orthonormal, and imposing sparsity on the solution to ensure competitive performance on large data sets.
 Our method has been tested on a benchmark of UCI data sets, and we have found that the results were competitive in comparison to those of rbf-SVM, and supe rior to those of the ordinary KPLS2 method. Furthermore, when applied to a music genre classific ation task, rKOPLS performed very well even with only a few features, keeping also the complexi ty of the algorithm under control. Because of the nature of music data, in which both the number o f dimensions and samples are very large, we believe that feature extraction methods such as rK OPLS can become crucial to music information retrieval tasks, and hope that other researche rs in the community will be able to benefit from our results.
 Acknowledgments This work was partly supported by the Danish Technical Resea rch Council, through the framework project  X  X ntelligent Sound X , www.intelligentsound.org ( STVF No. 26-04-0092), and by the Spanish Ministry of Education and Science with a Postdoctoral Felow ship to the first author.
 [1] Paul Geladi. Notes on the history and nature of partial le ast squares (PLS) modelling. Journal [2] L. Hoegaerts, J. A. K. Suykens, J. Vanderwalle, and B. De M oor. Primal space sparse ker-[3] Agnar Hoskuldsson. PLS regression methods. Journal of Chemometrics , 2:211 X 228, 1988. [4] Yuh-Jye Lee and O. L. Mangasarian. RSVM: reduced support vector machines. In Data [5] Anders Meng, Peter Ahrendt, Jan Larsen, and Lars Kai Hans en. Temporal feature integration [6] Michinari Momma and Kristin Bennett. Sparse kernel part ial least squares regression. In [7] Roman Rosipal and Leonard J. Trejo. Kernel partial least squares regression in reproducing [8] Roman Rosipal, Leonard J. Trejo, and Bryan Matthews. Ker nel pls-svc for linear and nonlinear [9] Kramer N. Rosipal R. Overview and recent advances in part ial least squares. In Subspace, [10] Sam Roweis and Carlos Brody. Linear heteroencoders. Te chnical report, Gatsby Computa-[11] Paul D. Sampson, Ann P. Streissguth, Helen M. Barr, and F red L. Bookstein. Neurobehav-[12] Bernhard Schoelkopf and Alexander Smola. Learning with kernels . MIT Press, 2002. [13] John Shawe-Taylor and Nello Christiani. Kernel Methods for Pattern Analysis . Cambridge [14] George Tzanetakis and Perry Cook. Music genre classific ation of audio signals. IEEE Trans-[15] Jacob A. Wegelin. A survey of partial least squares (PLS ) methods, with emphasis on the [16] Herman Wold. Path models with latent variables: the NIP ALS approach. In Quatitative so-[17] S. Wold, C. Albano, W. J. Dunn, U. Edlund, K. Esbensen, P. Geladi, S. Hellberg, E. Johans-[18] K. Worsley, J. Poline, K. Friston, and A. Evans. Charact erizing the response of pet and fMRI
