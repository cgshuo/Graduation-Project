 Dalian Maritime University text size continues to increase, the hapax/vocabulary ratio would approach 1. 1. Introduction
Words in English texts have a very peculiar distribution. On the one hand, between 50 X 100 top frequency words typically account for about 50% of the words in any text (Kennedy 1998); on the other, generally, about half of the words of the vocabulary of a text occur only once in the text (Baayen 1996; Kornai 2002). These lexical singletons are referred to as hapax legomenon (plural form: hapax legomena), hapax for short. between the number of hapaxes and vocabulary size (hereinafter referred to as HVR )is widely used in studies such as vocabulary growth (Tweedie and Baayen 1998), vocabu-lary richness and author identification (Holmes 1991), language typology (Popescu and
Altmann 2008), the degree of analytism (Popescu, Ma  X  cutek, and Altmann 2009), and so on.
 only two of them, the and a ( n ) would account for over 5% of the total word tokens of a text. However, the seemingly constant and high HVR of a text or collection of texts would decrease; as text length approaches infinity, all the words in the language would have occurred, and the number of hapaxes would approach zero. But the known facts so far do not seem to corroborate this intuition. For example, in Lewis Carroll X  X  26,505-word Alice X  X  Adventures in Wonderland , 44% of the vocabulary are hapaxes (Baayen 2001); in Mark Twain X  X  71,370-word The Adventures of Tom Sawyer , the percentage is 49.8% (Manning and Sch  X  utze 2001); in the 43-million-word Merc Corpus, this percentage is 56.6% (Kornai 2002). There seems to be no explanation for this strange behavior of hapax legomena in the literature.
 pragmatic information on roughly half of the vocabulary. The following questions ensue: What are the factors behind this enigmatic distribution of HVR ? Is it possible to substantially reduce HVR by increasing the size of a corpus? If so, how large should such a corpus be? These questions are the focus of this article.
 hapaxes H , and text length N was examined in the 100-million-word British National
Corpus (BNC). In the study, the orthographic word concept is adopted as a working def-inition, that is, a word (also called word token ) is a string of contiguous alphanumeric alphanumeric character set  X  is defined as
The word  X  is defined as wordssuchas Language , LANGUAGE ,and language are regarded as the same). So there words having the same stem, the same major part-of-speech, and the same word-sense of different lemmas within the text or corpus. 2. Data and Analysis
To study the dynamic relationship between N , V ,and H , the entire BNC was divided into equi-sized text blocks automatically by the computer. The size of each of the text ing the growth curves of V , H ,and HVR of a large corpus, text blocks much smaller than this (such as the average size of the text blocks of the one-million-word Brown and
LOB corpora, which is about 2,000 words) would considerably increase the number of text blocks and would therefore result in a longer computing time. However, the actual size of each of the text blocks is N  X  4, 200 words because of the removal of textual tags such as cPUN . The total number of such text blocks is 23,709. These text chunks were subsequently tokenized and lemmatized; characters that are not included in  X  were ignored, except for word-linking hyphens, which were replaced with white spaces.
These processed text blocks were then recombined into 1 0test sets, each having 2,371 text blocks, totaling about 10,000,000 word tokens, with the exception of the tenth set, which has 2,37 0text blocks. The formation of each set was done by random sampling without replacement from the 23,709 text blocks. During the formation of each set, as the text blocks were continuously sampled and pooled one by one to form a set, the growth of V , H , and the corresponding HVR were computed along the way. The V , H , and HVR of each of the ten sets are close to the means, which are 103,588.9, 42,384.6, and 0.4091, respectively.
 (See the right panel, Figure 1.) The HVR curves drop sharply initially, then stop drop-632 to the end. The number of word tokens at which HVR is the lowest and from which it displays a general upward trend until the end is referred to as POR (Point Of Return). Table 1 shows the initial HVR s( N  X  4, 200), the minimum HVR s, the final HVR s, V and
H at minimum HVR s, and POR s. The minimum HVRs of the ten sets are fairly close, around 0.3928, and so are the final HVR s, around 0.4091, although the POR s have a wide dispersion, from 1,515,629 to 4,382,688, averaging 2,957,179.5. It seems to defy common sense that POR s should exist in all the ten sets, and are much smaller than the sizes of the sets.
 the growth of V , H ,and HVR of the entire BNC were computed at an interval of 21 text blocks, about 89,000 word tokens. There are 1,129 such intervals, each formed by random sampling without replacement from the 23,709 text blocks. The purpose of reforming the 23,709 text blocks into 1,129 larger text chunks was to reduce computing time. The result is shown in Figure 2. The vocabulary size of the entire BNC is 346,578, and the number of hapaxes is 154,403. The initial HVR is 0.4583, the minimum HVR is
Average 0.5714 0.3928 0.4091 21,564.3 54,860.4 2,957,179.5 Q1 0.3899, and the final HVR is 0.4455. The POR is 3,820,340. The HVR curve quickly reaches end.
 hapaxes were randomly sampled and examined. These sampled hapaxes can be roughly divided into three major types: within-dictionary words, out-of-dictionary words, and typos. The total number of within-dictionary words is 232. They consist of 137 general imentary , bitstream , etc.); and two Early-modern English words ( auncyent , howshold ).
The total number of out-of-dictionary words is 718. They consist of 416 personal, organizational, and place and brand names of various national origins ( Khachaturyan , challeneger ,etc.).
 average length of the hapaxes is 7.74 characters, compared with the average length of word in the BNC as well), and the shortest has two. The following are some examples: 1. krumfettmahanamanahulamoranosecanosemahanaritiraoosalalarama 2. kahahahahahahahahahahahahehehehehehehehehehehehehahahahaha 3. llanfairpwllgwyngyllgogerychwyrndrobwllllantysiliogogogoch 4. 5cggaggggccctagagggccctagagggccccccaaaaacccccaaaaacccccc3 5. 01001101010101110101010010110101110010 (38 characters) 6. ywtfghikmccowlxpmtxwkirlmhjdb (29 characters) 634 7. antidisestablishmentarianism (28 characters) 8. tetramethylindocarbocyanine (27 characters) 9. whothinkstorideanangeldown (26 characters) 10. verfassungsschutzbericht (24 characters) 11. yyyyyyyyyyyyyyyyyyy (2 0characters) 12. actggaagggttagtttg (18 characters) 13. yslrwqieiifkvwksl (17 characters) 14. 5dddddddddddddd6 (16 characters) 15. chippingdale (12 characters) 16. reporeted (9 characters) 17. kwaouvi (7 characters) 18. baaba (5 characters) 19. akfu (4 characters) 20. xw (2 characters) many of the names, personal or non-personal, and foreign words appear to be random possible word in the English language has 104 characters, because there are 36 charac-ters in  X  (ignoring case), the maximum possible vocabulary size V language would be guage plus those formed by compounding, derivation, and higher-probability alphanu-meric strings would account for only a very tiny fraction of V blocks were sampled from the BNC, many of these words would have occurred more than once, but extremely low probability within-dictionary words and out-of-dictionary alphanumeric strings would accumulate. When N reached POR , the accumulation was large enough to reverse the downward trend of HVR ,andthe HVR curve started to go upwards. This suggests that as N approaches infinity, almost all the within-dictionary words would have appeared more than once, and the vocabulary growth would mainly be the growth of hapaxes formed by random alphanumeric strings, and the HVR curve would gradually approach its horizontal asymptote 1. That is, 3. Computer Simulation
A computer simulation was designed and performed to test this hypothesis. In the simulation, Lewis Carroll X  X  Through the Looking-Glass and What Alice Found There was used as a miniature language source that contains all the within-dictionary-words of the miniature language. The novel has 30,566 word tokens and 2,754 word types. A simulation corpus was built by repeated sampling with replacement from the miniature language source. The size of the sample was 100, each consisting of 94 words randomly drawn from the source, with six low-probability random alphanumeric strings added.
These strings were automatically generated by the computer with lengths between six and nine characters, and would roughly simulate the number of low-probability hapaxes in a natural English text of about 4,000 words, as there are about six low-probability hapaxes in a 4,200-word text chunk of the BNC. The following are some of these computer-generated random alphanumeric strings: sygtxue , ungwba , aruvfyr9 , layyieubk . As the size of the simulation corpus increased, V , H ,and HVR were computed. The simulation corpus finally consisted of 639,506 such samples, with N = 63,950,600,
V = 3,838,940, H = 3,835,368, the initial HVR = 0.8554, POR = 20,300, the HVR at POR = 0.6004, and the final HVR = 0.9991. The result is shown in Figure 3. At N = 192,000 the
HVR curve looks similar to that of the BNC; after the HVR curve reaches POR ,itstarts to go up and gradually approaches its asymptote. If we were able to build a corpus with
N hundreds of times larger than the BNC, its HVR curve would be similar to that of the simulation corpus. 4. Conclusion
This study reveals that HVR is not constant at all; it follows a U-shaped pattern. In the case of the BNC, initially, as N increases, HVR decreases; after N reaches POR ,
HVR starts to increase. This HVR distribution is due to the reoccurrence of hapaxes consisting of within-dictionary words and high probability alphanumeric strings, and the accumulation of extremely low probability within-dictionary words and out-of-dictionary alphanumeric strings. If N approaches infinity, HVR would approach its horizontal asymptote 1.
 reached the stage where hapaxes of extremely low probability within-dictionary words and out-of-dictionary alphanumeric strings substantially contribute to the vocabulary growth, and this may imply insufficient coverage of the core vocabulary of the text 636 for such a corpus, as shown in Section 2. On the other hand, a corpus with a size much larger than that of the BNC would have higher HVR ; such a corpus would have more lexical noise, with the within-dictionary words far outnumbered by out-of-dictionary alphanumeric strings, leaving sparseness of lexical, semantic, syntactic, discoursal, and pragmatic information on more than half of the vocabulary of the corpus practically unsolved.
 References
