
Tag recommendation is a major aspect of collaborative tagging systems. It aims to recommend suitable tags to a user for tagging an item. One of its main challenges is the effectiveness of its recommendations. Existing works focus on techniques for retrieving the most relevant tags to give beforehand, with a fixed number of tags in each recommen-ded list. In this paper, we try to optimize the number of recommended tags in order to improve the efficiency of the recommendations. We propose a parameter-free algorithm for determining the optimal size of the recommended list. Thus we introduced some relevance measures to find the most relevant sublist from a given list of recommended tags. More precisely, we improve the quality of our recommenda-tions by discarding some unsuitable tags and thus adjusting the list size.

Our solution is an add-on one, which can be implemen-ted on top of many kinds of tag recommenders. The expe-riments we did on five datasets, using four categories of tag recommenders, demonstrate the efficiency of our technique. For instance, the algorithm we propose outperforms the re-sults of the task 2 of the ECML PKDD Discovery Challenge 2009 1 . By using the same tag recommender than the win-ners of the contest, we reach a F1 measure of 0.366 while the latter got 0.356. Thus, our solution yields significant impro-vements on the lists obtained from the tag recommenders.
H.3.3 [ Information Search and Retrieval ]: Informa-tion Filtering
Algorithms, Experimentation 1. http ://www.kde.cs.uni-kassel.de/ws/dc09/ Tag Recommender Systems, Quality of recommendations, Optimization
Collaborative tagging is the practice of allowing users to annotate content. Users can organize, and search content with annotations so called tags. Nowadays the growth in popularity of social media sites has made the area of recom-mender systems for social tagging an active and growing topic of research [6].

Tag recommenders aim to suggest the most suitable tags to a user when tagging an item. They are a salient part of the web 2.0 where applications are user-centered. One of their main challenges is the effectiveness of their recommen-dations. People generally focus on techniques that enable retrieving the best suitable tags to give beforehand, with a fixed number of tags at each recommendation.

We follow another direction in order to improve tag re-commendation accuracy. We aim to dynamically adjust the number of tags to recommend. In other words, consider L
N = { t 1 ,t 2 ,...,t N } the list of N tags to be recommen-ded to a user, the goal is to substitute L N by one of its sublists that is more accurate, and provide a better quality of recommendation.

We consider all the sublists of L N in increasing size (i.e., the prefixes : L 1 , L 2 , ...L N  X  1 and L N )soastokeepthe tag order as illustrated below 2 . We introduce a relevance measure for the recommended lists Rel ( L N | u, i ), which estimates the probability that a user u will use all the recommended tags in L N for the tagging of an item i . Based on this measure, we compute the best list (the one having the optimal size, bls ) that will be finally recommended to the user. We define the optimal size as follows : with Existing approaches use linear combinations to compute the global average number of tags per post, the one related to combinations are then used to infer a fixed list size. Such approaches need some calibrations which are generally dif-ficult to set, and they lack of dynamicity which limits their accuracy.
 The algorithm we propose here enables adjusting dynami-cally the size of the recommended list of tags, and then in-creases the accuracy of the recommendations. It is a parameter-free algorithm that adjusts the list of recommended tags by discarding those which are estimated irrelevant. Our me-thod looks like an add-on filter on top of a tag recommen-der. It estimates the sublist which gives the best accuracy. We present in the paper two relevance measures and the al-gorithm that we use to retrieve the optimal sublist from a given tag recommendation list.

To evaluate the efficiency of our approach we implement it on top of four tag recommenders, from different approaches. One of our candidates is the pairwise interaction tensor factorization model (PITF) of Rendle and Schmidt-Thieme which won the task 2 of the ECML PKDD Discovery Chal-lenge 2009 [9]. It is still considered in the literature as one of the best tag recommenders. We took also the well-known tri-partite graph-based algorithm FolkRank [4] and the  X  Most Popular Tag  X  recommendation approach [6]. The last candi-date is a network-based tag recommender we developed [3], and which computes the list of tags based on the opinions of the users X  neighborhood and their tagging posts. The experiments we did on five datasets demonstrate the efficiency of the optimization technique we propose in this paper.
 The remainder of this paper is organized as follows. In Section 2 we present some preliminaries and describe briefly the four tag recommender candidates. Section 3 details the related work and presents our approach for finding the best size of a tag recommendation list to keep. In Section 4, we present our experiments and Section 5 concludes the paper.
A folksonomy is a system of classification that allows its users creating and managing tags to annotate and categorize content. It is related to the event of social tagging systems. A folksonomy F can be defined as a collection of a set of users U , a set of tags T ,asetofitems I , and a ternary relation between them S  X  U  X  I  X  T as F := ( U, I , T , S ). A tagging triple ( u, i, t )  X  S means that user u has tagged an item i with the tag t . A user can tag an item with one or more distinctive tags from T . We assume that a user can tag an item with a given tag at most once.

The interest of a tag t for a given user u to annotate an item i is generally estimated by a score score ( t | purpose of a tag recommender is to compute the top-K hi-ghest scoring tags for a post ( u, i ) which represents its re-commendations.
 For convenience, let us introduce some definitions and pro-perties of a folksonomy :
In the following we describe how our four tag recommen-der candidates model the scores associated to the tags.
Factorization models are known to be among the best per-forming models. They are a very successful class of models for recommender systems where they outperform the other approaches.
 We chose the pairwise interaction tensor factorization model (PITF) of Rendle and Schmidt-Thieme in our experimenta-tion due to its efficiency [9]. Indeed, it took the first place of the ECML PKDD Discovery Challenge 2009 for graph-based tag recommendation.

PITF proposes to infer pairwise ranking constraints from the set of tagging triples S . It captures the interactions bet-ween users and tags as well as between items and tags. Its model equation is given by : Where  X  U ,  X  I ,  X  T U and  X  T I are feature matrices which capture the latent interactions.

The main assumption of PITF is that within a post T ( u, i ), atag t can be preferred over another tag t iff the tagging triple ( u, i, t )  X  S (i.e., has been observed) and not ( u, i, t ). PITF models these preferences in Equation 3 such as the score of a tag which is more preferred than another one is greater.
FolkRank is a tripartite graph-based tag recommender de-signed in the spirit of PageRank [4, 5]. It assumes that a tag becomes important when it is used by important users or to tag important items. It also takes the same principle for users and items. Therefore FolkRank represents a folkso-nomy F as a graph where the vertices are mutually reinfor-cing each other by spreading their weights.

Let G F =( V, E ) be this graph. Its ver tices are the users, items and tags (i.e. V = U  X  I  X  T ) and the edges defined between them such as if a tagging triple ( u, i, t ) { u, i } , { u, t } , { i, t }  X  E .
 Let v i a vertex of this graph, i.e. v i  X  V .Wedenoteby N ( v i ) the set of neighbors of v i and by w ( v i ,v j )theweight of the edge between the vertices v i and v j . The weight of an edge is equal to the number of times its two vertices appear together in the tagging triples in S . From that, the degree of a vertex v i is defined as follows : FolkRank ranks the vertices according to their importances that it computes as follows :
PR ( v i )=  X  rence value of the vertex v i . Hence a straightforward idea for tag recommendation is to set a high preference to the consi-dered user and item (item to be tagged by the user), and then compute ranking values using PageRank as in Equa-tion 5. The parameter  X  determines the influence of p ( v Its value is between 0 and 1.

To recommend some tags for a user u andanitem i ,Fol-kRank uses two random surfer models on the graph, s (0) and s (1) , to infer the importance of each vertex of the graph. The first surfer, s (0) , set the same preference value to all the vertices ( p ( v )=1 ,  X  v  X  V ) while the second, s (1) , set their preference values to 0 except for the user u and the item i for which p ( u )=1and p ( i )=1.

After running the two surfers, the difference s := s (1)  X  is computed. Then the tags are ranked according to their importance value s , and the first ones are recommended to user u .
The rational of Most Popular Tags X  Recommenders (MPTR) is that when a tag t is popular (i.e., frequently used) for an item i and/or by a user u ,thattag t may be a relevant recommendation for u aiming to tag i .Hence,MPTRmo-del these popularities in their scoring models. A well-known MPTR model  X  also known as most popular  X  -mix  X  consists in adding the tag popularities (i.e. the ones of the user with those of the item) after normalizing and weighting them as follows : The parameter  X  in Equation 6 allows to tune the relative importance of the item or user-related tag popularity with respect to the second. When  X  = 1 we keep only the tags which are most specific to the item. On the other hand, when we set it to 0, we consider only the user X  X  popular tags. By default we fix  X  to 0 . 5 in our experimentation. Thus, we consider the user tags as important as those associated to the item. The advantage of MPTR is that they are fast to compute, while giving good predictions [6].
One weakness of MPTR (which entirely relies on popula-rity measures) is that they are not able to decide between tags with close popularities. Furthermore, some particular users can have their own vocabulary (i.e., tags) and the po-pular tags of the item may not be relevant for them. Thus, having reliable opinions about the tags from some trusted neighbors, in addition to the popularities of tags, may be a great asset to make better recommendations.
 FasTag [3] uses such an approach. It models the relevance score of a tag t for a user u and an item i (i.e. score ( t a popularity-dependent component, based a user X  X  trust in his neighbors in the network. Let us denote by score MPTR the scoring model of MPTR in Equation 6, the scoring one of FasTag is defined as where  X  ( t | u, i ) represents the opinion of the user X  X  neighbors about tag t . It is a normalized sum of the user X  X  trust values associated to his neighbors who already tagged this item with t . The rationale of this score function is to estimate a relative popularity of a tag depending on the trust neighbors of the user : i.e., the more a user trusts a neighbor, the more this neighbor X  X  opinion contributes in the user recommenda-tions. The sum 1 +  X  ( t | u, i ) enables taking into account the isolated users (when  X  ( t | u, i )=0).

We chose FasTag as a candidate for network-based tag recommenders, since it is fast and efficient as shown in [3]. Its scoring model is not only based on the trust associated propagation, following a natural interpretation that trust is, at some extent, transitive. See [10, 11] for more details on trust propagation models.
In this section, we present two ways to choose the best recommendation list size. The first one is based on existing works employing linear combination techniques, and the se-cond one is our proposal to optimize dynamically the size of the recommended list.
To choose the best list size (bls) of tags to recommend, usual approaches use some linear combinations of the global average number of tags per post, the one related to a user and/or the one specific to an item [9, 8, 7]. Therefore, we take as baseline the following general linear combination model bls =min( K,  X  +(  X  G  X   X  G )+(  X  u  X   X  u )+(  X  i  X   X  i ) )(8) where K stands for the maximum number of tags to re-commend, i.e. the maximal list size ;  X  G the global average number of tags per post ;  X  u the average number of tags per user and per post, and  X  i the average number of tags per item and per post. The rest stands for parameters which al-low us to make a lot of possible combinations. For instance, if we set the parameter  X  u to 1 and all the other parame-ters to zero, we obtain as a list size the average number of tags per user and post. We denote in the following the linear combination method by LC_bls .

For our experiments, we apply a grid search in order to find optimal parameters to keep. We test K  X  1 , 000 combi-nations of these parameters each time a top-K query is asked (i.e., each time we look for a list of K tags maximum). For instance, we make 10,000 combinations for the top-10 query and 5,000 for the top-5 one. We vary the parameter  X  from 0to K  X  1, each time by a step of 1. And using nested loops, we vary each of the other parameters from 0 to 1 by a step of 0 . 1. Thus, we test enough combinations with different va-lues of parameters. At the end, we keep the combination that gives the best result (in terms of reached F1-measure). blsC denotes the algorithm we propose to find the best ding to a user u , for the tagging of an item i .And L { t 1 ,t 2 ,...,t N } is an ordered list of N tags. We define the relevance of the list L N ,forbothauser u andanitem i ,as follows : justment of the list relevance. It allows us to promote longer Subsection 3.2.2. Rel ( t | u, i ) is the relevance of a tag t for a user u andanitem i . Intuitively, it measures the probability that user u will tag the item i with the tag t .

Let Max be a maximal list size and Rel ( L Max | u, i )the relevance of this list ( L Max ). We look for the best list size starting from Max down to 1. At each step, we compute the relevance of the current list and update the best list size as shown in Algorithm 1. In case we obtain the same relevance Algorithm 1: blsC : Best list size Computation Input : L Max /* Initial recommended tags list*/
Output : bls /* Suggested number of tags to keep */ 1 bls  X  Max /* bls : Best list size */ 2 blR  X  Rel ( L Max | u, i ) /* blR : Best list relevance */ 3for N =( Max  X  1) to 1 do 4if Rel ( L N | u, i ) &gt;blR then 5 bls  X  N 6 blR  X  Rel ( L N | u, i ) 7end 8end 9return bls for two different lists, we choose the longest one.
To compute the relevance of a tag, we propose two solu-tions. In the first one, we distinguish the known tags from the others and assign them different relevance values. In the second solution, we link the relevance values of the tags to some statistics we obtain from the available data (our trai-ning sets).
Making a distinction between the known tags (those al-ready used by the user and associated to the item) from the others may be useful to determine the relevance of a recom-mended list of tags. Our intuition is that the tags already linked to the user u andalsototheitem i are more relevant to recommend than the others.

Let P N = L N  X  T ( u )  X  T ( i ) be the sublist of L N containing the known tags. We assign a unique high relevance value, Rel max , to the tags in P N and an unique low one, Rel min to the other tags. By fixing the weight of each list to one, we can rewrite the Equation 9 as follows : After simplification it becomes :
From Equation 11, one can see that the relevance of a recommendation list depends mainly on the ratio between | P
N | and N . The relevance is an increasing function, having its inputs in the interval [0 ,N ]. It reaches its maximum when | P
N | = N . Thus, we can simply consider the relevance of a recommendation list as follows : With this formula, we can easily adjust the list size in order to obtain the best relevance value. Rel ( L N | u, i ) can be seen as a density measure of known tags in the recommended list. Then, the blsC is just seeking for the best density. Among the weaknesses of the relevance measure given in Equation 12 we can mention the fact that it fails to give a sublist when all the tags in the recommendation list are known (in P N ). We propose a relevance measure which faces the drawbacks of the previous one by taking into account the popularity of the tags for both the user and the item. Naturally, we expect that the relevance of a tag decreases according to its rank in a recommended list (high relevance for the first tag and low relevances for the last ones). This intuition is confirmed by our experimentations, on all the datasets we used and for all the tag recommenders we tested, as shown in Figure 1. We limit our tests to the top-10 lists of recommended tags. Figure 1: Relative relevance of a tag according to its position in a recommended list and the relevance of the first tag
From these observation, we see the need to use weighted means when evaluating the relevance of recommended lists. Indeed, since a recommendation list is ordered, the first tag is generally more relevant than the others (the second one and so on). Thus, using the average of the relevances of the tags contained in a list may still lead to a singleton, i.e. L as the best list.
 To take into account this natural decrease of the relevances of tags according to their positions, we define the weight  X  ( L N | u, i ) of a recommendation list L N as follows : This weight function estimates the importance of a list size compared to the maximal possible list size. It allows us to penalize short recommendation lists while promoting long lists. Therefore, from Equation 9 we derive a new list rele-vance measure : As one can see, we do not compute the mean of the rele-vance of tags but a relative list relevance according to the greatest possible list size. The comparison of the relevance of all the sublist is done as described in Algorithm 1. We just compute the most relevant list size from Max down to 1 by using this new relevance measure given in Equation 15. We denote this second proposal blsC_v2 . The latter can over-come the weak points of blsC . Indeed, even if all the tags in a recommendation list are in P N , blsC_v2 does not rely on their presence but only on their relevance. Thus, it is able to decide when blsC fails.
We demonstrate in this section the effectiveness of our proposals. We led a set of experiments with four tag recom-mender candidates on five publicly available datasets. In the next two subsections, we shortly describe these datasets and the evaluation measures and methodology we used. Then we present the results we obtained.
We chose five datasets from four online systems : deli-cious 3 , Movielens 4 ,Last.fm 5 , and BibSonomy 6 . We took the ones of delicious, movielens, and last.fm from HetRec 2011 [2] and the two other ones from Bibsonomy : a post-core at level 5 and a one at level 2 [1, 6]. We call them respectively Bibson 5and dc 09. dc 09 is the one of the task 2 of ECML PKDD Discovery Challenge 2009 7 . This task was especially intended for me-thods relying on the graph structure of the training data only. The user, item, and tags of each post in the test data are all contained in the training data, a post-core at level 2.Letusremindthatapost-coreatlevel p is a subset of a folksonomy with the property that each user, tag and item occur at least p times in the tagging triples of S .Table1 presents some details of these datasets.
To evaluate our solution, we used a variant of the leave-one-out hold-out estimation called LeavePostOut [6]. In all datasets except dc 09, we picked randomly for each user u , one item i that he had tagged before. Thus, we created a test set and a training one. The task of our recommender was then to predict the tags the user assigned to the item. 3. http://www.delicious.com 4. http://www.grouplens.org 5. http://www.lastfm.com 6. http://www.bibsonomy.org 7. http://www.kde.cs.uni-kassel.de/ws/dc09/ Dataset % of users with at most #tags per post 1 2 3 4 5
Bibson5 5.17 21.55 50.0 69.82 81.89 dc09 6.49 28.27 52.82 73.58 85.23 delicious 2.89 19.86 41.32 61.72 76.17 Last.fm 12.57 46.32 66.50 78.65 86.47 Movielens 65.37 92.57 97.63 98.95 99.76 Dataset % of items with at most #tags per post 1 2 3 4 5
Bibson5 0.27 9.418 32.40 61.21 80.60 dc09 11.46 25.45 45.47 64.47 78.83 delicious 11.10 27.08 46.61 63.72 76.40 Last.fm 19.77 51.87 79.50 90.03 94.08 Movielens 41.76 81.41 92.72 96.39 97.86 On each dataset, we ran the choosed tag recommenders. We successively obtained from them a top-1 , then a top-2 and so on, up to a top-10 . Then, we applied our algorithms LC_bls , blsC and blsC_v2 on the tag recommendation lists in order to get better sublists. The performance evaluation was based on the the F1-measure.

Let us notice that for all the experiments, we fixed the parameter  X  of MPTR and FasTag to 0 . 5(seeEquation6). Similarly, we set the parameter  X  of FolkRank to 0 . 7(see Equation 5) as in [5]. For PITF we used the software 8 and the parameters given by the winners of the task 2 of the ECML PKDD Discovery Challenge 2009, and we did not rely on assembling factor models as they did in [8]. We only computed one model with 64 factors as the dimensionality and a regularization value of 5  X  10  X  5 .Weranthelearning phase for 2 , 000 iterations. We present in this part the results of our experimentation. We aim to point out that our proposal outstrips the methods based on linear combinations. As we said, we show here the effectiveness of our proposals. We implemented them on top of the four tag recommenders. Figures 2 to 6 show the F1 quality of the original recommen-dation lists given by each tag recommender and the ones of each optimization method (i.e., blsC , blsC_v2 and LC_bls ). The x-axis of each of these figures gives the original number of tags to recommend before length optimization.

In almost all these figures, we see that the quality of the adjusted lists is increasing while the one of those with fixed sizes decreases when they exceed a certain size (specific to 8. http://bit.ly/1qL6NeF each dataset). This demonstrates the importance of giving optimal recommendation list size.

Second, in the most cases blsC_v2 outperforms blsC and linear combinations. For instance, it outstrips the results of the task 2 of the ECML PKDD Discovery Challenge 2009. With the same tag recommender than the winners, we reach an F1 measure of 0.366 while they got 0.356. They used a linear combination to adjust the size of the recommended lists (see [8]). blsC_v2 yields 3% of improvement over their F1 score on the dataset of the challenge.

Tables 2 and 3 compare the percentage of cases where each of the optimization methods gives the best contribution. Li-near combinations overpass our proposals in only 9.19% of cases (see 2). On all the rest, blsC_v2 dominates with more than 60% of cases, and blsC follows with 30%. Linear combi-nations are mainly better on Movielens dataset, which was the worst dataset of our five ones. In this dataset around 65% of the users never used more than one tag in their posts, which is poor for accurate recommendations. Simi-larly 41.76% of the items have never been tagged with more that one tag (see Table 1). This explains why our methods which use the popularity of the tags fail to give better results than linear combinations.

Table 4 presents the average optimal list size given by the methods LC_bls , blsC and blsC_v2 . In addition to the fact that blsC_v2 leads to the best size for 90.85% of cases compared to LC_bls (see Table 3), we see it gives longer lists than the latter in 80% of cases.
We presented a new proposal that improves the accuracy of the obtained recommendations from a tag recommender system. Our solution optimizes the size of the recommended list in order to obtain a better recommendation quality. The Table 4: Average optimal list length with 10 tags at maximum experimentation we did shows the effectiveness of our ap-proach. Furthermore, our approach suits well in the context of recommendation diversity. Since it shortens the number of relevant tags to recommend, it frees some space that can be used for the diversification purposes. [1] D.Benz,A.Hotho,R.J X schke,B.Krause,F.Mitzlaff, [2] I. Cantador, P. Brusilovsky, and T. Kuflik. 2nd [3] M.Gueye,T.Abdessalem,andH.Naacke.Anefficient [4] A.Hotho,R.J X schke,C.Schmitz,andG.Stumme.
 [5] R.J X schke,L.Marinho,A.Hotho, [6] R.J X schke,L.Marinho,A.Hotho, [7] L. Marinho, C. Preisach, and L. Schmidt-Thieme. [8] S. Rendle and L. Schmidt-Thieme. Factor models for [9] S. Rendle and L. Schmidt-Thieme. Pairwise [10] M. Tahajod, A. Iranmehr, and N. Khozooyi. Trust [11] C.-N. Ziegler and G. Lausen. Propagation models for
