 Karthik Raman karthik@cs.cornell.edu Thorsten Joachims tj@cs.cornell.edu Pannaga Shivaswamy pannaga@research.att.com AT&amp;T Research, San Francisco, CA, USA Tobias Schnabel tbs49@cornell.edu Fachbereich Informatik, Universitaet Stuttgart, Stuttgart, Germany A growing number of interactive systems use machine learning to adapt their models to different environ-ments, different users, or different user populations. Examples of such systems range from search engines and recommender systems, to personal assistants and autonomous robots. Ideally, these system should learn directly from their users in a manner that is unobtru-sive, robust, and efficient.
 A model of such learning processes is Coactive Learning (Shivaswamy &amp; Joachims, 2012), combining a bound-edly rational model of user behavior with an online learning model that formalizes the goal of learning. In particular, Coactive Learning models the interaction be-tween the user and a learner using weaker assumptions about the user feedback than in standard supervised learning. At each step, the learner (e.g. search engine) receives a context (e.g. query) for which it predicts an object (e.g. ranking, say [ d 1 ,d 2 ,d 3 ,d 4 ,... ]). This object is presented to the user. If this object is sub-optimal, the user responds with a slightly improved object, but not necessarily the optimal object as typi-cally assumed in supervised learning. This means the user merely provides a preference, which can typically be inferred from implicit feedback (e.g., clicks on d 2 and d 4 imply that the user would have preferred the ranking [ d 2 ,d 4 ,d 1 ,d 3 ,... ]). The goal of learning is to minimize regret, which is the cumulative suboptimality of predictions over the life of the learning system. While learning algorithms exist for the Coactive Learn-ing model (Shivaswamy &amp; Joachims, 2012), we show in this paper that they can perform poorly in the presence of noise. To overcome this problem, we propose a new learning algorithm that is robust to noise and performs well even in an agnostic setting. Our algorithm  X  called the Perturbed Preference Perceptron  X  produces greatly improved generalization performance both in simula-tion experiments, as well as in a live user study on an operational search engine. Furthermore, we prove regret bounds for this algorithm that characterize its behavior and provide explicit guidance for its appli-cation in practice, especially for ranking problems in search and recommendation.
 Our work follows the coactive learning model proposed in (Shivaswamy &amp; Joachims, 2012), which we discuss in Section 3. Feedback in coactive learning lies be-tween the Multi-Armed Bandit problem (Auer et al., 2002b;a; Flaxman et al., 2005) (payoff only for selected action) and the Expert-Advice problem (Cesa-Bianchi &amp; Lugosi, 2006; Zinkevich, 2003) (payoff for all actions). However, the coactive learner never observes absolute payoffs, but merely a preference between two actions. This aspect of preference feedback is similar to the du-eling bandits model (Yue et al., 2009; Yue &amp; Joachims, 2009), but the algorithm choses both actions in the dueling bandits model, while the user and algorithm chose one action each in the coactive learning model. Coactive learning also differs from other preference learning problems. For example, in ordinal regression (Crammer &amp; Singer, 2001) a training example ( x,y ) provides an absolute rank y . Ranking with pairwise preferences (Herbrich et al., 2000; Freund et al., 2003; Chu &amp; Ghahramani, 2005) is another popular problem. However, existing algorithms require an iid sample in a batch setting, while coactive learning works with no-iid data in an online setting. Listwise approaches to ranking (see (Liu, 2009)) differ from coactive learning as they require the optimal ranking for a query, not just a preference between typically suboptimal rankings. Partial monitoring games (Bart  X ok et al., 2010) also differ from coactive learning, as they require that loss and feedback matrices are revealed to the learning algorithm. Furthermore, partial monitoring games have no explicit notion of context that is available at the beginning of each round.
 The work in this paper is based on perturbing the output of a predictor for improved feedback. In infor-mation retrieval, this idea has been proposed for at least two purposes. First, search results from two re-trieval functions are interleaved (Chapelle et al., 2012) to elicit unbiased user preferences. Second, the  X  X air-Pairs X  perturbation strategy (Radlinski &amp; Joachims, 2006) was proposed for de-biasing click data in search. We use the FairPair idea, and provide the first explo-ration and learning algorithm for this type of feedback. The coactive learning model, as proposed in (Shiv-aswamy &amp; Joachims, 2012), is used in the rest of this paper. At each iteration t , the user states a context x t (e.g., query) and the learning algorithm makes a prediction y t  X  Y (e.g., ranking) in response. The user draws some utility U ( x , y ) from this prediction, Algorithm 1 Preference Perceptron.

Initialize w 1  X  0 for t = 1 to T do and provides an improved prediction  X y t  X  Y as feed-back. Denoting the optimal prediction for iteration t as y t = argmax y  X  X  U ( x t , y ), the quality of the users X  feed-back  X y t in response to y t is characterized as expected  X  -informative ,
E  X y t [ U ( x t ,  X y t )]  X  U ( x t , y t )+  X  ( U ( x t The expectation is under P x t [  X y t | y t ]. The definition characterizes by how much the feedback provided, in expectation, is an  X  -factor improvement over the pre-sented object relative to the maximum possible im-slack  X  t . Characterizing the feedback from boundedly rational users through Eq. (1) is sensible: a boundedly rational user may satisfice and not search the full space Y for the optimal y  X  (captured by  X  ), and may make imperfect assessments of utility (captured by  X  t ). We define the (average) regret of a learning algorithm after T iterations as: The goal of a coactive learning algorithm is to mini-mize regret. In the rest of this paper, we assume a w  X   X  R N is an unknown vector. Here,  X  ( x , y )  X  R N represents the joint feature vector of context x and object y . We assume that this vector is bounded, i.e,  X  x , y ; k  X  ( x , y ) k ` weight vector w  X  are never revealed to the learning algorithm. They are only used in their evaluation. This paper focuses on coactive learning for ranking. However, the model itself is more general and has applications in machine translation, robotics, etc. The Preference Perceptron (Shivaswamy &amp; Joachims, 2012) is a simple algorithm for coactive learning. Nev-ertheless, it can be shown to have tight regret bounds when the user feedback has no noise. However, we will show in the following subsection that it can fail catastrophically in noisy environments.
 The Preference Perceptron (Algorithm 1) maintains a weight vector w t which is typically initialized to 0 . At each time step t , the algorithm observes the context x t and presents an object y t that maximizes w &gt; t  X  ( x t over y  X  X  . The algorithm then observes user feedback  X y t and updates the weight vector w t in the direction  X  ( x t ,  X y t )  X   X  ( x t , y t ).
 Theorem 1 (Shivaswamy &amp; Joachims, 2012) The ex-pected average regret of the preference perceptron can be upper bounded, for any  X   X  (0 , 1] and any w  X  as The above bound is tight in the noise-free case and does not make any assumptions, as any user behavior can be characterized by appropriate values of  X  and  X  t . In this paper, we focus on rankings of documents D = { d 1 ,d 2 ,...,d n } as outputs y  X  X  . For such rankings, we construct the feature vector  X  ( x , y ) as the discounted vectors  X  ( x ,d ), where y ( i ) is the i-th document in the ranking. The  X  i are decreasing position discounts, such that sorting by document utility U ( x ,d ) = w &gt;  X  ( x ,d ) provides a ranking of maximum U ( x , y ) for a given w . 4.1. Instability: User Study on Search Engine We implemented the Preference Perceptron on the full-text search engine of arxiv.org , constructing feedback rankings  X y t from y t by moving the clicked documents to the top (more details in Section 7). Unfortunately, the Preference Perceptron did not learn a good ranking function in this online experiment, and Figure 1 shows the comparison against a hand-tuned baseline using Interleaving (Chapelle et al., 2012). The black line shows that the Preference Perceptron ( i.e., PrefP[top] ) only barely improves over the baseline (a value of 1 would indicate equivalence).
 Figure 2 gives some insight into this disappointing performance. It shows that the learned rankings do not stabilize and that the learning process oscillates. In particular, even after thousands of updates, the top 10 documents of the same query before and after 100 update steps only overlap by 4 documents on average. Figures 1 and 2 also show (in red) the behavior of the algorithm we introduce in this paper, the Perturbed Preference Perceptron for Ranking ( 3PR ). Note that it achieves substantial improvements over the baseline and that it does not oscillate. 4.2. Instability: Illustrative Example Why did the Preference Perceptron oscillate? Con-sider the following toy problem, where the goal is to learn rankings using the feature vector construction from above. In this toy example, document utility is independent of the context x and only document d 1 has utility U ( x ,d 1 ) = 1, all others have utility  X  1. Feature vectors  X  ( x ,d ) have 2 binary features that exactly reflect utility ( i.e.,  X  ( x ,d 1 ) = [1 , 0] while  X  i  X  [2 ,n ] :  X  ( x ,d i ) = [0 , 1]). Now let us consider the following simple user model: Each iteration, users view the current y t (i.e., documents ranked by w &gt; t  X  ( x They examine each y ( i ) in order, click the first one they deem to have utility 1, and then stop. However, users being an imperfect judge of utility, make each +1/  X  1 judgment with only 80% accuracy. We construct the feedback ranking  X y t from y t by swapping the clicked document into rank 1.
 Let us analyze the behavior of the Preference Per-ceptron on this toy example. In fact, let us assume that the algorithm is initialized with the weight vector w 1 = [1 ,  X  1], which correctly ranks d 1 first. If the user correctly clicks y (1) , the Preference Perceptron makes no change to w t . However, whenever the user selects an incorrect y ( i ) below (for which there is a  X  20% chance), the first weight decreases and the sec-ond weight increases. Eventually, the ranking will  X  X lip X  and d 1 will move to the last position. Even if the sys-tem eventually recovers from this catastrophic failure, the same sequence of events will lead to d 1 being placed at the bottom again. Thus, the system oscillates. The gravity of the problem can be seen in the following simulation results. For n = 10 documents and DCG discounting for  X  i (see Section 6), the average rank of d 1 within the first 1000 iterations of the Preference Perceptron is 9 . 36 (1 is best, 10 is worst). In fact, y (1) is in the worst position for most of the time steps, since it takes a low-probability event of 0 . 2 9 to correct the ranking, but a high-probability event of 0 . 2 almost immediately flips it back. Note that  X  X veraging X  does not fix this oscillation problem, since it is not a result of unbiased noise. In fact, an Averaged Perceptron (Collins, 2002) showed an average rank of 9 . 37. How can we prevent these oscillations to ensure conver-gence and improve regret? The key problem in the toy example from above is that the user feedback incurs large slack  X  t in (1) when d 1 is in the top position  X  even though it is perfectly  X  -informative without slack in all other cases. We now develop the Perturbed Pref-erence Perceptron to handle this bias in the feedback and guarantee stability.
 To motivate the algorithm, consider what happens if we run the Preference Perceptron, but present the user a perturbed ranking where, with 50% probability, we swap the top two documents. Even for the optimal weight vector w  X  , note that feedback on the perturbed ranking is now expected  X  -informative without slack under the given user model. This stabilizes the learn-ing process, since preferences now often reinforce w  X   X  namely whenever the relevant document d 1 is at rank two and the user clicks on it. Running the simulation from Section 4.2 using the perturbed rankings greatly improves the average rank of y (1) from 9 . 36 to 2 . 08. 5.1. Perturbed Preference Perceptron Following the idea of using perturbation to combat feed-back bias, Algorithm 2 defines the Perturbed Preference Perceptron . It is analogous to the conventional Prefer-ence Perceptron with two changes. First, the algorithm accepts a subroutine Perturb (  X y t ) for perturbing the perturbed object y t is presented to the user, the user X  X  preference feedback  X  and the subsequent update  X  is relative to y t , not  X y t . 5.2. Theoretical Analysis We now characterize the regret of the Perturbed Prefer-ence Perceptron as a function of the perturbation strat-egy. The following theorem bounds the expected regret of the Perturbed Preference Perceptron in terms of two quantities. First, consider expected  X  -informativeness of the user feedback analogous to Eq. (1), Algorithm 2 Perturbed Preference Perceptron.
 Input: Perturb (  X  X  X  ), GetFeedback (  X  X  X  ) w 1  X  0 { Initialize weight vector } for t = 1 to T do Note that the feedback  X y t is relative to the perturbed y , and that expectation is taken over perturbations. Second, consider affirmativeness w.r.t. a perturbed y t , Affirmativeness reflects the relationship between noise in the user feedback and noise from perturbation rela-tive to the current model w t . Positive affirmativeness indicates that the user feedback typically confirms the ordering based on the current w t , while negative affir-mativeness indicates the opposite. Based on these two quantities, we state the following regret bound. Theorem 2 The expected average regret of Algo-rithm 2 for a perturbation strategy satisfying the fol-lowing bound on the average affirmativeness, 1 T can be upper bounded as All proofs are provided in the supplementary material. Note that the average affirmativeness defined in (5) is a quantity that can be estimated by the learning algo-rithm, implying a dynamic strategy that determines how to perturb. Note further that in the bound  X  is always zero in the absence of perturbation, which recovers the conventional Preference Perceptron and its regret bound as a special case. The above bound can be substantially tighter than that of the conven-tional Preference Perceptron, since it allows trading-off between  X  and P  X  t . In the toy example from above perturbation reduced P  X  t to zero at a modest increase in  X .
 We also state two corollaries that give bounds on the regret w.r.t. an additive/multiplicative bound on the amount of perturbation.
 Corollary 3 The expected average regret of Algo-rithm 2 for a perturbation strategy satisfying can be upper bounded as Corollary 4 The expected average regret of Algo-rithm 2 for a perturbation strategy satisfying for 0  X   X   X  1 , can be upper bounded as E [ REG T ]  X  Corollary 3 follows immediately from Theorem 2, and Corollary 4 follows the structure of the proof in (Raman et al., 2012) for (unperturbed) coactive learning with approximate inference.
 The bounds presented above not only provide a the-oretical sanity check for Algorithm 2, but also give explicit guidelines for designing effective perturbation strategies that we will exploit in Section 6. Ranking is one of the most common learning tasks for online systems, since it is the basis for search and recommendation. These systems are ideally suited for coactive learning, since they can easily sense user interactions that provide (noisy) feedback. We now develop perturbation and feedback strategies for the Perturbed Preference Perceptron that ensure stable learning of ranking functions.
 For a perturbed ranking y , let  X y be a feedback ranking that is derived from interactions (e.g., clicks) in y . Our goal is a perturbation and feedback strategy such that  X y fulfills Eq. (4) with large  X  and small  X  . Let us consider some properties such a strategy should have. First, it is desirable to perturb uniformly throughout the ranking, so that any user experiences the same amount of perturbation no matter how deep they ex-plore. Second, we would like to make only local per-turbations to minimally alter the ranking. Third, the Algorithm 3 Perturbation and feedback for the Per-turbed Preference Perceptron for Ranking (3PR). Function FORMPAIRS ()
With prob 0 . 5: return ( { 1 , 2 } , { 3 , 4 } , { 5 , 6 } X  X  X  ) else: return ( { 1 } , { 2 , 3 } , { 4 , 5 } , { 6 , 7 } X  X  X  )
Function PERTURB (  X y ,p ) y  X   X y { Initialize with top-scoring ranking }
Pairs  X  FORMPAIRS () for i = 0  X  X  X  len ( Pairs ) do return ( y , Pairs )
Function GET-FEEDBACK ( y ,clicks,Pairs )  X y  X  y { Initialize with presented object } for i = 0  X  X  X  len ( Pairs ) do return  X y construction of the feedback ranking  X y should be robust to noisy clicks, limiting the increase in  X  in Eq. (4). These desiderata naturally lead to the perturbation and feedback strategy in Algorithm 3, which fol-lows the FairPairs method proposed in (Radlinski &amp; Joachims, 2006). The top-scoring ranking  X y (e.g., pairs of documents (e.g., [( d 1 ,d 2 ) , ( d 3 ,d 4 ) , ( d and each pair is flipped with probability p to produce the perturbed ranking y (e.g., y = [( d clicks on the bottom document of a pair, top and bottom document are swapped to produce the feed-back ranking  X y (e.g., for clicks on { d 1 ,d 4 ,d 6 } in y , we construct  X y = [( d 1 ,d 2 ) , ( d 4 ,d 3 ) , ( d 6 ,d 5 Algorithm 2 using the functions from Algorithm 3 the Perturbed Preference Perceptron for Ranking (3PR) . We now establish regret bounds for the 3PR algorithm, using the joint feature map  X  ( x , y ) for queries x and rankings y described in Section 4. In particular, we use position-discounting factors  X  i = 1 log DCG metric (Manning et al., 2008).
 Proposition 5 The 3PR with swap prob. p has regret:  X  On the one hand, the 3PR algorithm provides the first exploration strategy with a regret bound for FairPair feedback. On the other hand, the regret bound implies that the swapping of pairs does not need to necessarily be  X  X air X  (i.e., p = 0 . 5). For example, consider a dy-namic swap strategy that, at iteration t , determines its perturbation based on the cumulative affirmativeness R t = P mum perturbation D t = w &gt; t  X  ( x t ,  X y t )  X  w &gt; where y 0 t is the ranking obtained by swapping all pairs in  X y t . Note that D t is an apriori bound on the maxi-mum affirmativeness of the user feedback at iteration t . Based on these observable quantities, we propose the following dynamic adaptation rule for the swap probability with the following regret bound.
 Proposition 6 For  X   X  0 , dynamically setting the swap prob. of 3 PR to be p t  X  max (0 ,  X   X  t  X  R t D  X   X T To investigate the real-world effectiveness of the 3PR algorithm compared to the conventional Preference Perceptron ( PrefP ), we performed a user study (al-ready alluded to in Section 4.1) on the full-text search engine of arxiv.org . Results were collected in two subsequent runs, one for each method. We used a query-document feature vector  X  ( x ,d ) of length 1000, which included various query-dependent features (e.g., query-title match) and query-independent features (i.e. the age of a document).
 Users coming to the search engine were randomly as-signed one of two groups with equal probability. For users assigned to the learning group, we used the clicked documents of their query to construct the feedback rankings as described previously (i.e. move clicked documents to top for PrefP , and paired feedback with swap probability 0 . 5 for 3PR ). For users assigned to the evaluation group, the ranking induced by the cur-rent weight vector was compared to a baseline ranking that was generated with manually selected weights. We employed Balanced Interleaving (Joachims, 2002; Chapelle et al., 2012) for this comparison, which is a paired and blind test for eliciting a preference between two rankings. We record how often a user prefers a learned ranking over the baseline (i.e. wins a pairwise comparison). Both learning algorithms were initialized to start with the weights of the baseline ranker. Figure 1 shows the results of the experiment, plotting the win/loss ratio of each learning method over the baseline. While PrefP initially performs well, its win ratio eventually hovers only slightly above 1. The 3PR method, on the other hand, converges to a win-ratio of 1.9, which is large (and highly significant according to a Binomial Sign Test) compared to the experiments in (Chapelle et al., 2012). Finally, Figure 3 shows the average affirmativeness  X  from Theorem 2. It shows that  X  is positive and stabilizes, indicating that the amount of perturbation was appropriate. To get more detailed insights into the empirical per-formance of the proposed methods, we also conducted offline experiments on benchmark datasets.
 First, we use the Yahoo! learning to rank dataset (Chapelle &amp; Chang, 2011) (abbreviated Websearch ), which consists of roughly 28k queries and 650k doc-uments ( i.e., URLs). For each query-url pair in the dataset, there is a joint feature vector  X  ( x ,d ) of 700 features and a relevance rating in the range 0-4. In each iteration, the system is given a query and presents a ranking. In total, the system was run for over 28k iterations. All our results are averaged over 20 different runs (by randomizing the query stream order). Second, we simulate two news recommendation tasks, using the RCV1 (Lewis et al., 2004) and the 20 News-groups datasets (abbreviated News ). The RCV1 corpus contains over 800k documents that each belong to one or more of 103 topics, while the News dataset contains 20k documents that each belong to one of 20 topics. We used TFIDF features, with a total feature set of size 3k for RCV1 and 1k for News 1 . In these experiments, we simulated user interests by equating users with single topics. The algorithms were run for 50K iterations for RCV and 10K for News (by cycling through the data), and the results are averaged over all users (i.e. topics). We assume the following model of user interaction. The user scans the ranking from the top down to the tenth result and clicks on up to five results. To study the sta-bility of the different algorithms, clicks are corrupted by noise. For RCV1 and News, a user goes down the ranking and clicks on relevant documents, but with  X  chance of incorrectly assessing the relevance of a doc-ument (  X  = 0 . 2). On the search dataset, the user X  X  relevance assessment are corrupted by adding indepen-dent Gaussian noise (  X  = 1) to the true relevance of each document; the user then clicks on the 5 documents with highest (corrupted) relevance in the top 10. 8.1. What is the Generalization Performance First, let us compare the 3PR against alternative algo-rithms, including the conventional Preference Percep-tron where clicked documents are moved to the top of the feedback ranking ( PrefP[top] ). We also consider a variant of the conventional Preference Perceptron that uses the same paired feedback as 3PR , but has swap probability zero ( PrefP[pair] ).
 To compare with a regularized batch learner, a rank-ing SVM with move-to-top feedback was trained at (10,100,1k,10k,20k) iterations using a setup similar to (Shivaswamy &amp; Joachims, 2012). Between training steps, the current predictor is used to present rankings. For this experiment, we retrospectively pick the best C value (per run) and report the NDCG@5 corresponding to that C ( i.e., we are biasing in favor of the SVM). As a (rough) upper bound, we consider a Structured Perceptron (Collins, 2002) that is trained with the opti-mal y  X  without added noise. This simulates clean and exhaustive expert feedback, which is typically unob-tainable in practice. As a lower bound, we report the performance of uniformly random lists of documents. The results for this experiment are shown in Figure 4. It can be seen that the 3PR achieves a significantly higher NDCG@5 compared to other online algorithms PrefP[top] and PrefP[pair] at the end of the runs. In fact, PrefP[top] fails catastrophically on two of the datasets like in the toy example from Section 4.2. PrefP[pair] is more stable, but shows similar deteri-oration as well. An interesting extension could be the combination of aggressive move-to-top feedback in early iterations with more conservative 3PR updates later. Due to the biased training data that violates the iid model, the SVM performs poorly. We conjecture that more frequent retraining would improve performance, but be orders of magnitude more computationally ex-pensive (especially with realistic model selection). Presented y .717  X  .002 .286  X  .028 .386  X  .035 Predicted  X y .723  X  .002 .291  X  .028 .397  X  .035 The Structured Perceptron learns faster than 3PR , but despite receiving much stronger training data (optimal y  X  without feedback noise) its eventual performance is worse than 3PR on two tasks. This may be surprising at first glance. However, it is known that Perceptron-style algorithms tend to work less well on multiclass/ structured problems without good linear fit, and that they can even degenerate (Lee et al., 2004; Chen &amp; Xiang, 2006). Intriguingly, the 3PR seems less affected by this problem. 8.2. How does the Perturbed Ranking In the case of 3 PR , the algorithm first computes the argmax ranking  X y but then presents the perturbed ranking y . While the previous section showed the NDCG@5 of the presented rankings y , Table 1 shows the NDCG@5 for both y and  X y . As expected, the presented rankings are of slightly lower quality than  X  y due to perturbation. However, this small loss in quality leads to a big gain in the overall learning process in the long run  X  as demonstrated by the poor performance of PrefP [ pair ]. 8.3. How much Perturbation is Needed? While complete lack of perturbation leads to diver-gence, it is unclear whether a swap probability of 0 . 5 is always optimal. Intuitively, we expect that with low noise, smaller perturbation suffices to achieve high performance, while at higher noise levels, perturbation probabilities need to be higher to overcome the noise. Figure 5 explores the effect of different perturbation rates p in Alg. 3 on the performance of the 3PR . It ap-pears that a swap probability of more than 0 . 5 usually hurts. While 0 . 5 typically performs reasonably well, 0 . 25 produces the best performance on RCV1. 8.4. Can we Automatically Adapt the Ideally, we would like to automatically select an appro-priate swap probability. Note that this does not need to be a single fixed number, but can change over the learning run. Proposition 6 defined such a perturbation strategy that accounts for the current affirmativeness and adjusts the swap probability to optimize the re-gret bound in Theorem 2. The results of this dynamic strategy using  X  = 0 are also included in Figure 5. As we see from the figure, the method is able to adjust the swap rates to achieve performance among the best. Figure 6 shows how the swap probability chosen by the dynamic strategy varies. It can be observed that the swap probability first increases and then eventually decreases to exploit more often. When changing the noisiness of the user feedback, we find that the strategy automatically accounts for larger noise by increasing the swap rate relative to the low noise setting. 8.5. How does Noise Affect the Perturbed Our motivation for the 3PR algorithm was the inability of PrefP[top] to handle the noise in the feedback it en-countered in the user study. Therefore, all benchmark experiments we reported included feedback noise as described in the beginning of Section 8. But how does the 3PR algorithm perform without added noise? Figure 7 compares the performance of 3PR to that of PrefP[top] and PrefP[pair] with and without user feedback noise. Even with no feedback noise, 3PR outperforms PrefP[top] and is at least comparable to PrefP[pair] . Furthermore, the performance of 3PR de-clines much less when noise is introduced, as compared to the other algorithms.
 Note that  X  no noise  X  is somewhat of a misnomer. While we did not add any noise, even the expert provided ratings probably contain some amount of noise. More-over, any feedback that cannot be explained by a linear model appears as noise to the algorithm, which is likely to be a substantial source of noise in any real-world application. The 3PR algorithm handles this gracefully. We presented the Perturbed Preference Perceptron, an online algorithm for learning from biased and noisy preferences in the coactive learning model. Unlike ex-isting methods, the algorithm is stable and does not oscillate. The key idea lies in a controlled perturba-tion of the prediction, and we give theoretical regret bounds that characterize the behavior of the new al-gorithm. Focusing on learning to rank, we develop perturbation strategies for ranking and show that the new algorithm substantially outperforms existing meth-ods in an online user study, as well as in benchmark experiments. This work was supported in part by NSF Awards IIS-0905467, IIS-1142251, and IIS-1247696. Auer, P., Cesa-Bianchi, N., and Fischer, P. Finite-time analysis of the multiarmed bandit problem. Machine Learning , 47(2-3):235 X 256, 2002a.
 Auer, P., Cesa-Bianchi, N., Freund, Y., and Schapire, R. The non-stochastic multi-armed bandit problem. SIAM Journal on Computing , 32(1):48 X 77, 2002b. Bart  X ok, G  X abor, P  X al, D  X avid, and Szepesv  X ari, Csaba.
Toward a classification of finite partial-monitoring games. In ALT , pp. 224 X 238, 2010.
 Cesa-Bianchi, N. and Lugosi, G. Prediction, learning, and games . Cambridge University Press, 2006. Chapelle, O. and Chang, Y. Yahoo! learning to rank challenge overview. JMLR -Proceedings Track , 14: 1 X 24, 2011.
 Chapelle, O., Joachims, T., Radlinski, F., and Yue,
Yisong. Large-scale validation and analysis of in-terleaved search evaluation. ACM Transactions on Information Systems (TOIS) , 30(1):6:1 X 6:41, 2012. Chen, D. and Xiang, D. The consistency of multicate-gory support vector machines. Adv. Comput. Math , 24(1-4):155 X 169, 2006.
 Chu, W. and Ghahramani, Z. Preference learning with gaussian processes. In ICML , 2005.
 Collins, M. Discriminative training methods for hid-den markov models: theory and experiments with perceptron algorithms. In EMNLP , 2002.
 Crammer, K. and Singer, Y. Pranking with ranking. In NIPS , 2001.
 Flaxman, A., Kalai, A. T., and McMahan, H. B. Online convex optimization in the bandit setting: gradient descent without a gradient. In SODA , 2005.
 Freund, Y., Iyer, R. D., Schapire, R. E., and Singer,
Y. An efficient boosting algorithm for combining preferences. Journal of Machine Learning Research , 4:933 X 969, 2003.
 Herbrich, R., Graepel, T., and Obermayer, K. Large margin rank boundaries for ordinal regression. In
Advances in Large Margin Classifiers . MIT Press, 2000.
 Joachims, T. Optimizing search engines using click-through data. In KDD , 2002.
 Lee, Yoonkyung, Lin, Yi, and Wahba, Grace. Multicat-egory support vector machines. Journal of the Amer-ican Statistical Association , 99(465):67 X 81, 2004. Lewis, D. D., Yang, Y., Rose, T. G., and Li, F. RCV1:
A new benchmark collection for text categorization research. JMLR , 5:361 X 397, 2004.
 Liu, T-Y. Learning to rank for information retrieval. Foundations and Trends in Information Retrieval , 3, March 2009.
 Manning, C., Raghavan, P., and Sch  X utze, H. Introduc-tion to Information Retrieval . Cambridge University Press, 2008.
 Radlinski, F. and Joachims, T. Minimally invasive randomization for collecting unbiased preferences from clickthrough logs. In AAAI , pp. 1406 X 1412, 2006.
 Raman, K., Shivaswamy, P., and Joachims, T. Online learning to diversify from implicit feedback. In KDD , 2012.
 Shivaswamy, P. and Joachims, T. Online structured prediction via coactive learning. In ICML , 2012. Yue, Y. and Joachims, T. Interactively optimizing information retrieval systems as a dueling bandits problem. In ICML , 2009.
 Yue, Y., Broder, J., Kleinberg, R., and Joachims, T.
The k-armed dueling bandits problem. In COLT , 2009.
 Zinkevich, M. Online convex programming and gener-
