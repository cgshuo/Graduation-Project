 1. Introduction
Finding high quality web pages is one of the most important challenging issues for any web search engine. Ideally, the relevance of pages is defined based on user preferences. Therefore, the problem of ranking is to sort web pages based on user requests or preferences. Definitely, to make the web more interesting and productive, we need a good and efficient ranking algorithm to present more appropriate results for users. Usually, there are thousands or even millions of relevant pages for each query. Nevertheless, users typically consider only the top 10 or 20 results. Therefore, we have to focus on the most valu-able and appealing pages.

There are currently two major categories of ranking algorithms based on content (classical IR) and connectivity (link-based algorithms).

In classical Information Retrieval (Baeza-Yates &amp; Ribeiro-Neto, 1999 ), the system tries to find documents corresponding to the user query. IR Algorithms usually work based on matching words in documents. In other words, for each query the documents with the more similar content to the query will be selected as the more relevant ones. Examples of the content based ranking algorithms are TF-IDF (Salton &amp; Buckley, 1988 ), BM25 (Robertson, Walker, HancockBeaulieu, Gatford, &amp; Payne, 1995), etc.

However, the web consists of a large number of unstructured documents linked together, creating a massive graph. Fur-thermore, queries are generally short (2.4 terms in average (Zhang &amp; Moffat, 2006 )) and vocabulary is huge while in classical
Information Retrieval (IR) usually the number of documents is not huge and queries are long. These differences pose new challenges to IR. In addition, since the contents of the web are published in a distributed manner, these contents are often inconsistent and include a lot of misinformation. Therefore, application of classical IR methods to the web content may result in problems such as low precision.

To remedy these issues, new connectivity-based algorithms have been proposed that use links between web pages in addition to content relevancy. Previous studies indicate that algorithms using hyperlinks for ranking yield satisfactory re-sults ( Henzinger, 2001 ). The main strength of these algorithms comes from using the votes of other pages to rank current pages. In other words, links carry information which can be used to evaluate the importance of pages and the relevancy of them to the user X  X  query. Instances of connectivity-based ranking algorithms are PageRank ( Page, Brin, Motwani, &amp; Wino-grad, 1998 ), HITS ( Kleinberg, 1999 ) and DistanceRank ( Zareh Bidoki &amp; Yazdani, 2008 ).

Although these algorithms are appropriate in some situations, on average their precision is low compared to that of con-the system and the basic objective is to satisfy her by providing a good ranking. However, in the above ranking algorithms, there is not any position for the user; directly or indirectly. Thus, there seems to be room for better algorithms that take the role of the user into account. Clearly, users do not randomly click on links, but make a somewhat informed choice. While click-through data is typically noisy and clicks are not exactly the same as relevance judgments, they contain some useful information indicating likely relevance (Joachims et al., 2007 ). Currently there are some ranking solutions that consider and the interesting results have been found.

As mentioned above, three factors content, connectivity, and user behavior (and/or combination of them) have already been used in many different works (some of which are cited in this paper) to produce high quality rankings. In this paper, we propose an adaptive ranking algorithm based on this triple. For this purpose we use click-through data to implicitly take user behavior into consideration. We call this algorithm A3CRank in which the  X  X  X  X  stands for Adaptive and the 3  X  X  X  X  X  are
Content, Connectivity and Click-through data. In this algorithm, we use click-through data to combine the results of both the content based and the connectivity-based algorithms. For this combination, we have used some content based methods such as TF-IDF and DFR_BM25 (He &amp; Ounis, 2005 ) in addition to PageRank as a connectivity-based ranking algorithm.
Roughly speaking, we are going to aggregate some ranking algorithms using user click-through data, to achieve a better ranking algorithm and bring higher quality pages to the top of the result list. A major property of our method is its adapt-ability. Depend on the user X  X  need, the method adapts itself to the environment to present an appropriate ranking for the user X  X  satisfaction.

We use a goodness factor for each ranking algorithm which shows the satisfaction degree of an average user for each algo-rithm. The goodness factor is computed via an iterative process, using user clicks and reinforcement learning ( Sutton &amp; Barto, 1998). Furthermore, we have used the OWA 1 operator ( Yager, 1988 ) to merge the results of the various ranking algorithms.
We have used 130 queries on the University of California at Berkeley X  X  web to evaluate our algorithm. The queries have been issued by 10 computer science students to the system for evaluation. Our algorithm outperforms Ranking SVM ( Herb-rich, Graepel, &amp; Obermayer, 2000 ) in the standard criterions such as P@ n and NDCG.

The next section discusses our solution, A3CRank. Experimental analysis and comparison with some of the well-known algorithms come in Section 3. Section 4 reviews related work. Finally, our conclusion and future work agenda are presented in Section 5. 2. A3CRank
In this paper, we use a number of ranking algorithms and are going to merge results of algorithms such as PageRank ( Page view of our method. In our method, the behavior of the user is utilized to increase the quality of the merging process. In other words, the algorithm adapts itself with user click-through-data as time passes. Thus, the system will satisfy users X  needs to find high quality pages more conveniently. As mentioned earlier, we call this algorithm A3CRank, because it is an Adaptive algorithm based on the Content, Connectivity, and Click-through data triplet.

Naturally, each ranking algorithm has its own pros and cons. By combining different ranking algorithms, there is a chance to overcome their shortcomings and achieve a better algorithm. For example, PageRank which is a connectivity-based algo-rithm is based on page popularity which finds old high quality pages better. Thus a new born high quality page with low popularity (low in-degree) will have a small PageRank and will not reach to the top list. While content sensitive algorithms (in classical IR) such as TF-IDF and BM25 cannot find high popular pages which are voted by users using links. Thus with an appropriate aggregation, the weaknesses of each algorithm can be mitigated. We will show some of the benefits of combin-ing algorithms in the experimental results presented in the next section.

Roughly speaking, we needed to find out how to aggregate the results by using user behavior data to find the real quality of the pages and also to satisfy the user needs. Our problem is similar to a reinforcement learning problem ( Sutton &amp; Barto, 1998). In the reinforcement learning problem, the learning is done from interaction.  X  X  X he learner and decision-maker is called the agent. The thing it interacts with, comprising everything outside the agent, is called the environment. X  The agent interacts with the environment by selecting some actions and the environment will present the agent with rewards based on the chosen action. The agent and environment interact together at a sequence of discrete time steps t =0,1,2, ... . At each time step, the agent receives some representation of the environment X  X  state s and selects an action a t . The agent is going to maximize received rewards from the environment.

In our method, we consider the search engine as an agent and the users as the environment. Fig. 2 shows the relation between the search engine and the users. The search engine, the agent, in response to the user X  X  query ( q ), sends a ranked list of results as an action. According to the quality of these results, the user will click on items in the ranked list as the re-ward. Furthermore, at each time step t the ranked list and the query together comprise the state s engine agent aims to maximize the number of clicks on its high quality returned results. In other words, the agent must pre-pare more appropriate results for the user, in order to receive better rewards.

Our algorithm, A3CRank is composed of four stages: 1. Computing the goodness factor for each algorithm using click-through data gathered from user behavior. 2. Utilizing the goodness factor of each algorithm to compute the weight of each resulting page from that algorithm. 3. Computing the OWA vector for aggregation. 4. Computing the final weight of each result using the OWA vector.

These stages iterate for all queries. Finally, the system parameters such as goodness factors and weights will be found. In the following, we explain these stages in detail.

In the first stage, we use a factor called the goodness factor ( gf ) which is computed for every algorithm by reinforcement learning ( Sutton &amp; Barto, 1998 ) using user click data. The goodness factor shows the degree of suitability of each algorithm for users in average. This factor helps us to find the quality of pages in each algorithm. In other words, the algorithm with higher gf will return higher quality pages. We use this factor in the next stage to compute the weight of each resulting page returned by each algorithm. The gf is computed as shown in Eq. (1), where a is the learning rate of the system (Eq. (2)) and the factor c i (gamma) is the quality of the clicked pages for i th query (the query number i ) and comes from Eq. (3).
The value of the learning rate a comes from Eq. (2) where itr shows time (or iteration number) and b is a static value to control the regularity of the learning rate. In the next section, we will also compute all goodness factors when alpha is con-stant. In the initial state of the algorithm, the gf  X  X  of the algorithms are not known, so initially, we set a to one and, then, decrease it exponentially to near zero. Furthermore, in the first iteration we set all goodness factors to 1/m equally where m is the number of the underlying ranking algorithms.
In Eq. (3), which is similar to the NDCG measure (Jarvelin &amp; Kekalainen, 2000 ), T is the number of clicked pages and t depicts the order of clicking on a page with rank j ( j th page). Because we do not have r in the NDCG measure, we replace it with the order of clicks by the user. That is, we interpret that the pages receiving earlier clicks have higher relevancy. Naturally, we can use other factors such as P@ n for optimization. After finishing the learning process, the selected criteria will be optimized.

The second stage is to assign a weight to each result of every algorithm. The weight of resulting page d for algorithm i is dependent on its gf and the page X  X  rank. This weight is computed as depicted in Eq. (4), where n is the number of results for each query returned by algorithm i and R(d ) shows the rank (order) of d in the results. Thus, n might be different for every algorithm and it depends on both the query and algorithm. Furthermore, R by algorithm i .
Therefore, we will have a matrix of weights containing the weight of each resulting page returned by each algorithm. In this matrix the rows represent the various algorithms and the columns represent the returned pages. Naturally, if resulting page d is not included in algorithm i then w id is set to zero. Then we trace the matrix column by column to store a vector for descending order.

The third stage is computing the OWA (Yager, 1988 ) weight vector for each resulting page vector. This operator maps a vector of size n to a single value. For example if A =[ a vector in which P j o j = 1, the result of aggregation is f  X  a main property of the OWA operator is the reordering of A so that each weight o vector A instead of a i . A nice property of the OWA operator is that it includes the Max, Min and arithmetic mean operators for the appropriate selection of the vector O respectively, O =[l0, ... ,0] be easily shown that the result of any OWA operator is between the max and min values. Yager introduced a measure called  X  X  X rness X  that specifies the type of aggregation. Most notably, the flexibility of the OWA operator is realized through the or-ness, which controls the aggregation process.

We use the Optimistic Exponential OWA operator to find the weights of each vector as the following: where parameter k belongs to the unit interval 0 6 k 6 1. Now, we have an n dimensional OWA vector in that o with the i th position in each sorted page vector. Experimentally, we have found that k = 0.3 is suitable for the results aggregation. Currently, OWA is one of the best aggregation operators. We chose this operator in our work for two reasons. First, in
OWA, the weights have nothing to do with the magnitudes of the aggregates a and an orness degree. This is desirable since in the IR field the order of documents, and not their values, is important. Thus, this operator should be beneficial to ranking. Secondly, the weights for entities are dynamic and they are not specified con-stantly for each ranking algorithm. Rather, we assign weights to the order of algorithms. Therefore, by using this operator all of the algorithms will have equal chance to affect the final aggregation value.

The final stage is the weight computation of each resulting page as in Eq. (5), where m depicts the number of participating ranking algorithms.
At this point, the results are presented to the user in descending order of their w
The above four stages will iterate for all queries and in the end we will have the learned gf of each algorithm. After that, the learned goodness factors will be used for the combination of ranking algorithms. Naturally, gf is not static forever and web content is constantly prone to change. In response to these changes (detected during the crawling and indexing pro-cesses) gf will be computed again. However, except in the first run, the initial gf for each algorithm is not reset to 1/ m and always the latest calculated value is employed.
 Our combined algorithm has some advantages such as: It is scalable, allowing for the addition of any new algorithm easily.
 It is adaptive to the users X  behaviors and preferences.

It is possible to personalize the ranking scheme for each user separately by assigning the personalized gf s to each user/ algorithm.
 Instead of using coarse-grained features (ranking algorithms), we can use fine-grained features such as TF, IDF, in-link, etc.
Obviously, each ranking algorithm is composed of these fine-grained features. 3. Experimental results
Since our algorithm is based on user clicking behavior and reinforcement learning, its evaluation is hard and complicated. We used University of California at Berkeley X  X  web site with five million web pages to evaluate our algorithm. About 130 queries in two categories of computer science and biology, have been chosen. Most of these queries have been extracted from the AOL data set ( Pass, Chowdhury, &amp; Torgeson, 2006 ) of queries that users have used in the past to reach Berkeley X  X  web. These queries have been selected randomly from about 1000 queries. We asked 10 com-puter science students to enter these queries into the system and click on the results to gather the click-through data information. We decided to use this method since we did not have access to server logs of Berkeley X  X  web sites. The selected well-known algorithms for aggregation are PageRank, TF-IDF and DFR_BM25. We used the Terrier Information
Retrieval platform from the University of Glasgow (Terrier, 2007 ) to compute the value of content based ranking algo-rithms. It is notable that the results of this algorithm are restricted to the experimental environment we have adopted.
Furthermore, as we do not have the whole web graph, the PageRank information is quite limited to the Berkeley X  X  web graph.

We compared our algorithm with BM25, Ranking SVM 2 (RankSVM), ( Herbrich et al., 2000 ), a powerful learning based meth-od, and also the simple linear combination of the three mentioned features based on their average value, called AvregaeCom, as baselines. BM25 is a strong content based ranking algorithm that computes the relevance score of a document d for a query Q based on probability theory ( Robertson et al., 1995 ). To Compute the score, this algorithm uses some features like term fre-quency, document length, average document length, document frequency, the number of documents in the whole collection, and also some constants. In DFR_BM25, Dirichlet Priors normalisation is applied on term frequency in BM25, providing a robust and effective performance ( He &amp; Ounis, 2005 ).

Ranking SVM uses a support vector machine (SVM) to learn an algorithm for combining features. These features could be simple features such as TF and IDF, or more complex functions such as BM25 and PageRank. Ranking SVM takes pairs of doc-uments and explicit user preferences for those documents (from a ranked list of user preferences), and tries to learn the order of document pairs. Due to the space limitation we do not explain the details of these baseline algorithms.
RankSVM also uses above three features in the learning process to obtain a model. Then, we use the model in the test process. We employ the RankSVM both on the normalized training (all values are normalized between 0 and 1) and un-nor-malized training data, called RankSVM-Norm and RankSVM-UNorm respectively.

For the evaluation of our algorithm we conducted two stages: training and testing the system. About 60 queries were used in the training phase and the rest were used for the testing phase. In the training phase, users worked with the system by issuing queries and clicking on the results. In this stage, parameters like the goodness factor of algorithms were learned by the system. Fig. 3 demonstrates the steps of the training stage: 1. Set the goodness factors for each algorithm equally to 1/ m which m is the number of algorithms to be combined (here it is 3). 2. The system receives a query from the user. 3. Results of all ranking algorithms are sorted and merged by Eq. (5) (the final weight of each resulting page is computed). 4. The results are presented to the user, sorted by their weights. 5. Gather user click-through data for each query. 6. Compute the goodness factor of each algorithm using Eq. (1). 7. If any other query exists go to step 2.
 The above steps iterate for all training queries. Now we start the test (evaluation) process using the remaining 70 queries.
About 30 results from each algorithm, related to the test queries, were judged in six levels rating (0 = detrimental, 1 = bad, 2 = fair, 3 = good, 4 = excellent, and 5 = definitive) 3 by the pervious users.

For comparison, we have used three metrics  X  X  X recision at position n  X  (P@ n ), Mean Average Precision (MAP), and  X  X  X ormal-ized Discount Cumulative Gain X  (NDCG) (Jarvelin &amp; Kekalainen, 2000). Precision at n measures the relevancy of the top n results of the ranking list with respect to a given query (Eq. (6)).

The AP (average precision) is computed for a single query and is defined as the average of the P@ n values for all relevant documents.
 where N is the number of retrieved documents, and rel ( i ) is either 1 or 0, indicating whether the i th document is relevant to the query or not. Mean Average Precision (MAP) is the mean of average precisions over a set of queries.
Since P@ n and AP can only handle cases with binary judgments  X  X  X elevant X  or  X  X  X rrelevant X , to do a better evaluation, we also used NDCG which handles multiple levels of relevance. Eq. (8) shows how NDCG is computed. In this equation, r ( j ) indi-cates the rating (0 = detrimental, 1 = bad, 2 = fair, 3 = good, 4 = excellent, and 5 = definitive) at rank j .
Figs. 4 and 5 present the comparisons of the A3CRank algorithm with other algorithms in terms of the P@ n and the NDCG measures respectively for test queries. As the figures show, our learning algorithm outperforms RankSVM in both normalized and un-normalized states. For example, A3CRank achieves a 20%, 25% and 29% improvement over RankSVM-Norm, Rank-
SVM-UNorm and AverageCom respectively in terms of average NDCG. This achievement resides in the combination of both content based and connectivity-based algorithms using user behavior data.
 Fig. 6 shows the MAP of the above algorithms on the test set. This figure confirms the above results and shows clearly that A3CRank outperforms RankSVM and AverageCom.

We also did a paired t -test of A3CRank over RankSVM, AverageCom, and BM25 in terms of MAP and average NDCG. Table 1 shows the results. As the table shows, A3CRank has a statistically significant improvement ( p &lt; 0.05) over all the other algorithms in both metrics.

We computed the goodness factor of each algorithm for both a dynamic (Eq. (2)) and a constant alpha. The computed goodness factors with a dynamic alpha are 0.09, 0.38 and 0.53 for PageRank, TF-IDF and DFR_BM25 respectively ( Fig. 7 ). Fur-thermore, when alpha is constant and set to 0.3 these values are 0.17, 0.30 and 0.53 respectively (Fig. 8 ). Also their averages during the learning process are 0.12, 0.35 and 0.53 respectively. Figs. 7 and 8 show the track of the system to achieve final goodness factors for these queries. According to this figures, the BM25 and the PageRank algorithms have the highest and the lowest values respectively. We believe, not having the whole web graph and the limited number of query categories could be the reasons of low value for PageRank. Similar results have also been found in the literature for precision and NDCG values (Najork, Zaragoza, &amp; Taylor, 2007 ). Now we can be confident that by aggregation of some ranking algorithms controlled by user behavior we achieve more satisfactory results.

To illustrate that the learning process used by A3CRank will find the high quality pages in earlier positions on average for different queries, we show the difference between the result quality of each query (Eq. (9)) before and after the learning showed to the users and r j depicts the relevancy degree of the page with rank j (judged by the user). To compute the rank quality of query q , RQ q , we sum up the multiplication of relevancy degrees of items of the result list in their position values (1 ( j 1)/ n ). For example, for the first result of the query q , having the highest rank, its quality value is r second one its quality value is r 2 n 1 n , etc. Eq. (10) shows differences between query q after ( RQ learning process. As Fig. 9 shows, for most of the queries, the quality difference of results is positive. This means that after the learning process, our algorithm, on average, will find the high quality pages of different queries in earlier positions of the list.
 3.1. Analysis of experiments
As Figs. 4 and 5 show, superiority of A3CRank is due to its adaptation to the user queries and clicks. By using the past user clicks (click-through data), the system will learn how to find the best combination of different algorithms continuously.
In the proposed method, in each step of updating the goodness factors, a new query is tried. However, we have learned from TREC that no search system is the best for all queries. Thus we can expect the weight of each ranking algorithm to change when a new query is issued. This is clearly shown in Figs. 7 and 8, in which the goodness factor zigzags across the queries. Although, by keeping alpha constant in Fig. 8 , the zigzags are softer than Fig. 7 , it still does not reach a stable point. This is surprising and shows the adaptiveness of the system. Because new queries have different natures and so we need different goodness factors. We can easily conclude from the above that the final goodness factor of each algorithm over a certain amount of queries is a weighted average of the queries X  gammas. If we expand Eq. (1) we will have the equation below which is a type of weighted average over all gammas.
Another feature of A3CRank is its adaptation to the environment. In other words, the algorithm tries to find an optimum point for combination of three criteria. We can see in Figs. 7 and 8 , how the algorithm tries to find the best point for aggre-gation in the zigzag path. The mystery of outperforming other algorithms, showed in Figs. 4 and 5, lies in two properties: the adaptive learning and the use of the OWA for results aggregation. The reason of choosing OWA has been explained in the previous section.
 As Figs. 4 and 5 show, A3CRank has higher P@ n and NDCG@ n compared to the other algorithms for the first 10 results.
Since web users often click on the first pages of the list of search engine results, this method is suitable for the web environment. 4. Background and related work
Similar work has been done in the literature of ranking in search engines. For example in (Yeh, Lin, Ke, &amp; Yang, 2007) and (Pahikkala, Tsivtsivadze, Airola, Boberg, &amp; Salakoski, 2007 ) two combinational ranking algorithms using neural networks and genetic algorithms have been proposed. In these methods many ranking algorithms have been used as features for combi-nation. These methods use explicit feedback of users for combination of features. These feedbacks and features are from Web Track 2003 and 2004 and they are available from a data set called LETOR (LETOR, 2007 ).

In ( Joachims, 2002 ) by utilizing the user click-through data and support vector machine (SVM) for training, a learning method for ranking has been proposed. It has been shown that this method effectively adapts with the retrieval function of a search engine to a particular group of users. After training, with a couple of hundred queries, it outperforms Google in terms of retrieval quality.

Similar to research presented here are the work done in (Qin, Liu, Zhang, Chen, &amp; Ma, 2005; Shakery &amp; Zhai, 2006 ). In these works, both content and connectivity (links) have been used without considering user clicks. They proposed a new general relevance propagation model for combining link and content. In their work instead of transmitting the rank of nodes in the web graph, relevancy between the query and the document (that node) will be propagated. It has been shown exper-imentally, that combining link and content generally results in better performance than using only content.
In ( Xue, 2004 ) an iterative algorithm by utilizing the user click-through data to improve search performance has been proposed. This algorithm aims to find hidden relations and similarities between queries and documents to add these queries to documents X  metadata (document extension). Experimentally, by using a large set of the MSN search log, they achieved significant improvements on search performance.

In ( Agichtein et al., 2006 ) by using many click-through data features as user feedback in the ranking process both directly and indirectly, they found very interesting results. They used 3000 user queries for evaluation and found a 31% increase in ranking quality in comparison to other ranking algorithms.

Also, similar work has been done in the meta search engines area. For example in (Keyhanipour, Moshiri, Kazemian, Pir-oozmand, &amp; Lucas, 2007 ), a solution for aggregation of results in meta search engines that uses click-through data and also the OWA operator for merging has been proposed. Also this method is adaptive and they have achieved interesting results through the aggregation of nine search engines.

In the above methods that have used click-through data, user clicks are considered only as a feature for ranking, not for adaptability with the environment. While, in our method in addition to aggregation of some ranking algorithms by clicks, we have used the clicks to attain an adaptive ranking. 5. Conclusion and future work
In this paper we have proposed a combined algorithm based on 3  X  X  X  X  X : Content, Connectivity and Clicks. We have titled this algorithm A3CRank. A3CRank try to adapt itself with user needs using user click-through data. Our algorithm aggregates the results of a number of ranking algorithms. We have used a few ranking algorithms (both content and connectivity-based) such as TF-IDF, DFR_BM25 and PageRank for combination.

For each ranking algorithm a factor called the goodness factor has been defined. It shows the degree of the satisfaction of the average user with that algorithm. Each goodness factor is computed using a reinforcement learning method according to the quality of clicked documents. Every clicked document X  X  quality is computed by its rank and the order in which it was chosen for clicking.

Then, the results of all algorithms are merged by the OWA operator, which is one of the best known aggregation opera-tors. The weights for resulting pages are computed using the goodness factor of each algorithm and the page X  X  rank. Also, we have used the Optimistic Exponential OWA operator to find the weights of the OWA vector.
 We used University of California at Berkeley X  X  Web and 130 related queries for evaluation of A3CRank. We have compared
A3CRank with other algorithms in P@ n and NDCG measures and found interesting improvements on our experimental environment.

The proposed algorithm has some useful features like scalability and adaptability. It is scalable in that we can add any new algorithm easily and also adaptive in that it adapts itself with user needs.

It is notable that the results and conclusion of this algorithm are restricted to the experimental environment we have adopted. We only used two types of queries that could introduce a bias in the evaluation process. The evaluation on more wise range of queries in order to have a variance of the degree of users X  expertise similar to reality remains as future work. Currently, we have selected a few coarse-grained features such as PageRank popularity and BM25 relevancy for merging.
We plan to merge some fine-grained features like TF, IDF or in-degree that the above algorithms are composed from, using the mentioned method, as future work. Obviously, after the learning process and computation of goodness factors we will have a new ranking algorithm composed of some fine-grained features.
 Acknowledgement
The authors would like to thank Mohammad Keyhani and Information Processing &amp; Management journal X  X  reviewers for valuable comments.
 References
