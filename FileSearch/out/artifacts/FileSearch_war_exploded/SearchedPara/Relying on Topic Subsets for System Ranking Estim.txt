 Ranking a number of retrieval systems according to their re-trieval effectiveness without relying on costly relevance j udg-ments was first explored by Soboroff et al [6]. Over the years, a number of alternative approaches have been proposed. We perform a comprehensive analysis of system ranking estima-tion approaches on a wide variety of TREC test collections and topics sets. Our analysis reveals that the performance o f such approaches is highly dependent upon the topic or topic subset, used for estimation. We hypothesize that the per-formance of system ranking estimation approaches can be improved by selecting the  X  X ight X  subset of topics and show that using topic subsets improves the performance by 32% on average, with a maximum improvement of up to 70% in some cases.
 Categories and Subject Descriptors: H.3.4 Information Storage and Retrieval: Information Search and Retrieval General Terms: Experimentation.
 Keywords: Evaluation, System Ranking Estimation.
Estimating the performance of retrieval systems without recourse to relevance judgments was first explored by Sobo-roff et al [6]. The motivation stems from the high costs involved in the creation of test collections. Moreover, in a dynamic environment such as the Web regular evaluation of search engines with manual assessments is not feasible [6]. In recent years, a number of system ranking estimation ap-proaches have been proposed [1, 5, 6, 7, 8] that attempt to rank a set of retrieval systems without the need for manual relevance judgments. In each of the these approaches, the retrieval results of the full TREC topic set are relied upon to form an estimate of system performance. However, in [4] it was found that some topics of a topic set are better suited than others to differentiate the performance of re-trieval systems. While the work in [4] was performed in a more general evaluation context, here we explore the appli-cation of this observation in the context of system ranking estimation. In this paper, we hypothesize, that if the  X  X igh t X  subset of topics (i.e. those that best differentiate the perf or-mance of retrieval systems) is used, the current methods for estimating system ranking without relevance judgment can be substantially and significantly improved. To this aim, we implemented four different approaches to system ranking es-timation and compare their performance in a comprehensive analysis. We empirically determine the extent of the topic dependent performance and perform a range of experiments to evaluate the degree to which topic subsets can improve the performance of system ranking estimation approaches. In contrast to previous work, the evaluation is conducted on a wider variety of test collections, including more recen t ones. Our results show that the quality of system ranking estimation methods varies considerably depending on the se t of topics, and subset of topics.

The paper is organized as follows: first, in Sec. 2, a brief overview of related work is given. Then, in Sec. 3, we de-scribe the motivation for our experiments. The experimenta l setup is outlined in Sec. 4, followed by the empirical analys is in Sec. 5. We conclude with a summary in Sec. 6.
Soboroff et al [6] proposed to rely on automatically de-rived pseudo relevance judgments to rank retrieval systems instead of costly manual relevance assessments. For each topic, the top retrieved documents across the retrieval sys -tems to rank are pooled. A number of documents are sam-pled from the pool and used as pseudo relevant documents. The subsequent evaluation of each system is then performed with pseudo relevance judgments in place of relevance judg-ments. Although the reported correlations with the ground truth, that is the ranking of systems based on mean average precision (MAP), were significant, the performances of the best systems were consistently underestimated. It was sug-gested in [1] that this observation is due to the best systems being too different from the average.

The exploitation of pseudo relevant documents was also investigated by Nuray &amp; Can [5]. In contrast to [6], not all available systems participate in the creation of pseudo relevance judgments, only those that are the most different from the average. Additionally, the rank a document is re-trieved at is taken into account. The reported results are generally higher than those reported in [6] for the topic set s evaluated. However, in this work, we perform an extensive evaluation across more test collections, and show that this approach does not always deliver better performance.
Other methods that estimate a ranking of systems based on document overlap such as [8, 7], have not been proven to be as successful as [6] in our experiments when considering all available systems for ranking.

The aforementioned methods have all assumed that all topics are useful in estimating the ranking of systems. How-ever, recent research on evaluation which relies on manual judgments to rank systems has found that only a subset of topics is needed [4].
To explore the relationship between a set of topics and a set of systems, Mizarro &amp; Robertson [4] took a network analysis based view. They proposed the construction of a complete bipartite Systems-Topic graph where systems and topics are nodes and a weighted edge between a system and a topic represents the retrieval effectiveness of the pair.
Network analysis can then be performed on the graph, in particular, Mizarro &amp; Robertson [4] employed HITS, a method that returns a hub and authority value for each node. While the study in [4] was more theoretic in nature, a recent follow up on this work by Guiver et al [3] showed ex-perimentally that when selecting the right subset of topics , the resulting relative system performance is very similar t o the system performance on the full topic set, thus allowing to reduce the number of topics required.

The finding that individual topics vary in their ability to indicate system performance provided the basis for our work as it implies that there might exist a subset of topics that is as suited to estimate system performance as the full set of topics. While the motivation in [3, 4] is to reduce the cost of evaluation by reducing the topic set size, we are motivated by the fact that system ranking estimation does not perform equally well across all topics.

We examine the following research question: By reducing the topic set size, can the performance of current system ranking estimation methods be improved?
We conduct our analysis on eight different topic sets: TREC-{ 6,7,8 } (TREC Volumes 4+5 minus CR corpus), TREC-{ 9,10 } (WT10g corpus) and TB-{ 04,05,06 } (GOV2 corpus) 1 . Each topic set contains 50 topics, the number of retrieval systems to rank varies between 58 and 129 (Table 1).
The estimation methods we evaluate are: the data fusion ( DF ) approach by Nuray &amp; Can [5], the random sampling ( RS ) approach by Soboroff et al [6], the document similarity ( ACSim ) and the document score approach ( ACScore ) by Diaz [2]. The latter two approaches were originally applied to rank systems for a single topic, not across a set of topics. The main motivation for evaluating specifically those ap-proaches is their mix of information sources. In particular , RS relies on document overlap, while DF and ACScore take the rank and retrieval score, respectively, a system assign s to a document into account. Finally, the ACSim approach goes a step further and considers the content similarity be-tween ranked documents.
 DF: The variation of the data fusion approach, that per-formed best in [5] and which we utilize (Condorcet voting 1 In previous work, the topic sets TREC-{ 6,7,8 } have mostly been evaluated. No results have been reported yet for topic sets of GOV2. and biased system selection), has three parameters to set. We determined each topic set X  X  parameters by training on the remaining topic sets available for that corpus. RS: We follow the methodology of [6] and rely on the 100 top retrieved documents per retrieval system. We pool the results of all systems that are to be ranked. As in [6], due to the inherent randomness of the process, we perform 50 trials. In the end, we average the pseudo AP values for each pair of topic and system.
 ACScore: This approach, proposed in [2] 2 , is based on document overlap and the score, a retrieval system assigns t o each document. Essentially, a retrieval system is estimate d to perform well, if its documents X  scores are close to the average scores across all systems.
 ACSim: The second approach from [2] 3 we evaluate, is based on the notion that well performing systems are likely to fulfill the cluster hypothesis, while poorly performing s ys-tems are not.

The evaluation is performed by reporting the rank cor-relation coefficient Kendall X  X  Tau  X   X  [  X  1 , 1], between the ground truth ranking, that is the ranking of retrieval sys-tems according to MAP, and the system ranking produced by the ranking estimation methods.
In Sec. 5.1, we compare the four ranking estimation ap-proaches on the full topic sets. In Sec. 5.2, we will show that the system ranking cannot be estimated equally well for each topic of a topic set. Finally, in Sec. 5.3, we perform a number of motivational experiments to determine if it is possible to exploit this observation.
In Table 1, the results of the evaluation of the four sys-tem ranking estimation methods are shown. DF performs well on TREC-{ 6,7 } , the poor result on TREC-8 is due to an extreme parameter setting found during training. RS on the other hand outperforms DF on both the topic sets of WT10g and GOV2. Relying on content similarity does not aid, shown by ACSim performing worse than ACScore . Conversely, the knowledge of the retrieval scores assigned to a document by a system, as exploited by ACScore , leads to a slightly worse performance than RS , which considers doc-ument overlap only. If we consider the mean of Kendall X  X   X  across all topic sets, RS shows the most consistent perfor-mance, followed by ACScore .

Of note is the high correlations that the four approaches achieve on the topic sets of WT10g in comparison to TREC Volumes 4+5. System ranking estimation is harder on TREC Volumes 4+5 due to the greater number of manual runs available for those topic sets. Manual runs are often very different from automatic runs. We also observe that the DF approach, which is designed to prefer those unusual systems , does not perform significantly better than RS on TREC-{ 7,8 } . An explanation can be found in the indiscriminate mixing of best and worst systems by the DF approach.
We also observed that the problem of underestimating the ranking of the very best systems, decreases considerably fo r the topic sets of GOV2 compared to TREC Volumes 4+5 and WT10g. While the best performing retrieval system 3 referred to as  X  (  X y , y (rank 1 in the ground truth) was estimated by the RS ap-proach to be ranked at rank 57, 74, 113, 76 and 83 for TREC-{ 6,7,8,9,10 } respectively, the analogous estimated ranking on GOV2 is 30, 32 and 20 for TB-{ 04,05,06 } . Table 1: System ranking estimation on the full set of topics. Reported is Kendall X  X   X  . Underlined is the best performing approach per topic set. All corre-lations reported are significant ( p &lt; 0 . 005 ). Column 2 shows the number of retrieval systems to rank.

In contrast to earlier work, these experiments show that when evaluating a broader set of topic sets of more recent test collections the RS method consistently delivers the best overall system rankings.
In this Section, we show that the ability of system ranking estimation approaches to rank the systems correctly, differ s significantly between the topics of a topic set. We set up the following experiment: for each topic, we evaluated the estimated ranking of systems by correlating it against the ground truth ranking that is based on average precision . Note, that this is different from the ground truth ranking based on MAP. We are not interested in how well a single topic can be used to approximate the ranking of systems over the entire topic set, we are interested in how well the system ranking estimation approach performs for each individual topic. In Table 2, for each topic set, the minimum (worst topic) and maximum (best topic)  X  reached are shown.
The results are similar across all topic sets and system ranking estimation methods: the spread in correlation be-tween the best and worst topic are extremely wide; in the worst case, there is no correlation (  X   X  0) between the ground truth and the estimated ranking, whereas in the best case the estimated ranking is highly accurate and with few exceptions  X  &gt; 0 . 7. These findings form the motivation for the next section.
We now investigate if we can we improve the accuracy of an estimator when dealing with a subset of topics by for instance removing those topics, the system ranking approac h performs most poorly on. Since each of the evaluated topic sets consists of 50 topics we test subsets of cardinality 1 to 50. As it is not feasible to test all possible subsets per cardinality c , for each c we randomly sample 10000 subsets of topics. In total, we test five topic selection strategies, two based on random sampling and three iterative ones: worst sampled subset :  X  of the subset resulting in the average sampled subset : average  X  across all sampled greedy approach : an iterative strategy; at cardinality c , median AP : an iterative strategy; at cardinality c the estimation accuracy : an iterative strategy; at cardinal-
For the topic subsets of each cardinality, we determine the correlation between the estimated ranking of systems (base d on this subset) and the ground truth ranking of systems based on MAP. In contrast to Section 5.2, now we are indeed interested in how well a subset of one or more topics can be used to approximate the ranking of systems over the entire topic set.

We should stress, that the latter 3 strategies all require knowledge of the true relevance judgments. This experiment was set up, to determine if it is advantageous at all to rely on subsets instead of the full topic set. These strategies we re not designed to find a subset of topics automatically.
Exemplary, the results of this analysis are shown for two at subset sizes between 5 and 15, yields significantly higher correlations than the baseline (i.e. the correlation at the full topic set size of 50). After a peak, the more topics are added to the topic set, the lower the correlation. The worst subset strategy on the other hand shows the potential danger of choosing the wrong subset of topics - X  is significantly lower than the baseline for small cardinalities.

When considering the medianAP strategy of first adding easy topics, gains in correlation over the baseline are visi -ble, but they are topic dependent and far less pronounced than the best possible improvement (greedy approach). As hypothesized, adding topics according to the estimation ac -curacy does indeed lead to improved performance, although the problem remains that without knowing the ground truth it is difficult to estimate what the accuracy of the estimated ranking will be.

In Table 3 the results across all topic sets and approaches are summarized. Across all topic sets and all system rank es-timation approaches there exist indeed subsets of topics th at would greatly improve the performance of system ranking estimation algorithms. Consider for instance, the results of the DF approach on topic set TB-04: with the right topic subset, a rank correlation of  X  = 0 . 898 can be reached, a 54% increase over the performance on the full topic set.
In this work, we have reported the first experiments in topic subset selection for the system ranking estimation ta sk. Contrary to prior work, when widening the number of topic best sampled subset, which is therefore not listed separate ly approaches. reported are significant ( p &lt; 0 . 005 ). sets and TREC corpora, we found the initially proposed random sampling approach [6] to be the most stable and the best performing method. Furthermore, we found that the ability of system ranking estimation approaches to estimat e the ranking of systems for each individual topic varies wide ly within a topic set. Using different topic subset selection strategies, we confirmed the hypothesis that system ranking estimation approaches can be substantially improved, if th e  X  X ight X  X opic subsets is used during the estimation. Howeve r, this work can only be seen as a first step; for these insights to be useful in practice, automatic methods are required that can identify the best subsets of topics for system ranking estimation. This direction will be explored in future work.
Acknowledgment: We would like to thank Guido Zuc-con for his feedback and comments on this work.
