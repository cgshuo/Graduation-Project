 bouchard@cs.berkeley.edu slav@google.com Many natural language processing applications, from discriminative training [18, 9] to minimum-risk decoding [16, 34], require the computation of expectations over large-scale combinatorial spaces. Problem scale comes from a combination of large constant factors (such as the massive grammar sizes in monolingual parsing) or high-degree algorithms (such as the many dimensions of regions of the search space are skipped on the basis of some computation mask. For example, in monolingual parsing, entire labeled spans may be skipped on the basis of posterior probabilities in a coarse grammar [17, 7]. Conditioning on these masks, the underlying dynamic program can be made to run arbitrarily quickly.
 Unfortunately, aggressive pruning introduces biases in the resulting expectations. As an extreme example, features with low expectation may be pruned down to zero if their supporting struc-tures are completely skipped. One option is to simply prune less aggressively and spend more time on a single, more exhaustive expectation computation, perhaps by carefully tuning various thresholds [26, 12] and using parallel computing [9, 38]. However, we present a novel alternative: randomized pruning. In randomized pruning, multiple pruning masks are used in sequence. The re-sulting sequence of expectation computations are averaged, and errors average out over the multiple computations. As a result, time can be directly traded against approximation quality, and errors of any single mask can be overcome.
 Our approach is based on the idea of auxiliary variable sampling [31], where a set of auxiliary variables formalizes the idea of a pruning mask. Resampling the auxiliary variables changes the improve the mask for subsequent iterations. In other words, pruning decisions are continuously revisited and revised. Since our approach is formally grounded in the framework of block Gibbs sampling [33], it inherits desirable guarantees as a consequence. If one needs successively better Figure 1: A parse tree (a) and the corresponding chart cells (b), from which the assignment vector (c) is extracted. Not shown are the labels of the dynamic programming chart cells. approximations, more iterations can be performed, with a guarantee of convergence to the true expectations.
 method would be useless if it did not outperform previous heuristics in the time range bounded by the exact computation time. Here, we investigate empirical performance on English-Chinese bitext parsing, showing that bias decreases over time. Moreover, we show that our randomized pruning computation times. Our technique is orthogonal to approaches that use parallel computation [9, 38], and can be additionally parallelized at the sentence level.
 In what follows, we explain the method in the context of parsing to make the exposition more concrete, and because our experiments are on similar combinatorial objects (bitext derivations). in which randomized pruning will be most advantageous will be those in which high-order dynamic programs can be vastly sped up by masking, yet no single aggressive mask is likely to be adequate. 2.1 The need for expectations Algorithms for discriminative training, consensus decoding, and unsupervised learning typically involve repetitively computing a large number of expectations. In discriminative training of proba-of a tree-valued random variable T given a yield y ( T )= w is modeled using a log-linear model : P and f ( t, w )  X  R K is a feature function. Training such a model involves the computation of the following gradient in between each update of  X  (skipping an easy to compute regularization term): where { w i : i  X  I} are the training sentences with corresponding gold trees The first term in the above equation can be computed in linear time, while the second requires a cubic-time dynamic program (the inside-outside algorithm), which computes constituent posteriors for all possible spans of words (the chart cells in Figure 1). Hence, computing expectations is is computationally very expensive, limiting previous work to toy setups with 15 word sentences [18, 32, 35], or necessitating aggressive pruning [26, 12] that is not well understood. 2.2 Approximate expectations with a single pruning mask imated with a pruning mask which allows the omission of low probability constituents. Formally, a pruning mask is a map from the set M of all possible spans to the set (a) S NP P She . . 0 1 2 3 4 5 .
T given a yield y ( T )= w is modeled using a log-linear model : = t | y ( T )= w )=exp {  X   X  ,f ( t, w )  X  X  X  log Z (  X  , w ) } , in which  X   X  R K is a parameter vector t, w )  X  R K is a feature function. Training such a model involves the computation of the  X  log )= w { w : i  X  I} are the training sentences with corresponding gold trees Figure 1: A parse tree, from which the assignment variables are extracted. A linearization into an assignment vector is shown at the right. approximations, more iterations can be performed, with a guarantee of convergence to the true expectations.
 method would be useless if it did not outperform previous heuristics in a time range bounded by the exact computation time. Here, we investigate empirical performance on English-Chinese bitext parsing, showing that bias decreases over time. Moreover, we show that our randomized pruning computation times. Our technique is orthogonal to approaches that use parallel computation [9, 38], and can be additionally parallelized at the sentence level.
 In what follows, we explain the method in the context of parsing because it makes the exposition more concrete, and because our experiments are on similar combinatorial objects (bitext deriva-tions). Note, however, that the applicability of this approach is in no way limited to parsing. The settings in which randomized pruning will be most advantageous will be those in which high-order dynamic programs can be vastly sped up by masking, yet no single aggressive mask is likely to be adequate. 2.1 The need for expectations Algorithms for discriminative training, consensus decoding, and unsupervised learning typically involve repetitively computing a large number of expectations. In discriminative training of proba-of a tree-valued random variable T given a yield y ( T ) = w is modeled using a log-linear model : P ( T = t | y ( T ) = w ) = exp { X   X , f ( t, w )  X   X  log Z (  X , w ) } , in which  X   X  and f ( t, w )  X  R K is a feature function. Training such a model involves the computation of the following gradient in between each update of  X  : where { w i : i  X  I} are the training sentences with corresponding gold trees { t The first term in the above equation can be computed in linear time, while the second requires a cubic-time dynamic program (the inside-outside algorithm), which computes constituent posteriors for all possible spans of words (the chart cells in Figure 1). Hence, computing expectations is indeed the bottleneck here. While it is not impossible to calculate these expectations exactly, it is computationally very expensive, limiting previous work to toy setups with 15 word sentences [18, 32, 35], or necessitating aggressive pruning [26, 12] that is not well understood. Figure 2: An example of how a selection vector s and an assignment vector a are turned into a pruning mask m . 2.2 Approximate expectations with a single pruning mask imated with a pruning mask , which allows the omission of low probability constituents. Formally, whether a given span is to be ignored. It is easy to incorporate such a pruning mask into existing dynamic programming algorithms for computing expectations: Whenever a dynamic programming the computation (see Algorithm 3 for a schematic description of the pruned inside pass). However, exact, introducing a systematic error and biasing the model in undesirable ways. 2.3 Approximate expectations with a sequence of masks can combine several masks. Given a sequence of masks, m (1) computing a sequence of masks such that this average not only has theoretical guarantees, but also operationally.
 The masks are defined via two vector-valued Markov chains: a selection chain with current value de-s assignment vector a then determines, for each span, whether it would be forbidden if selected (or negative , a  X  =  X  ) or required ( positive , + ) to be a constituent.
 Our masks m = m ( s, a ) are generated deterministically from the selection and assignment vectors. is also a vector with coordinates corresponding to spans: m mask is illustrated on a concrete example in Figure 2. 1 We can now summarize how randomized pruning works (see Algorithm 1 for pseudocode). At chain on selection vectors as k  X  . Once a new mask m has been precomputed from the current selection vector and assignments, pruned inside-outside scores are computed using this mask. The selected auxiliary variable, i.e. T | ( A  X  = a ) . If a =  X  , sampling from T | A same dynamic program as for exact sampling, except that a single cell in the chart is pruned (the cell  X  ). The setting where a = + is more interesting: in this case significantly more cells can be pruned. Indeed, all constituents overlapping with  X  are pruned. This can lead to a speed-up of up to a multiplicative constant of 8 = 2 3 , when the span  X  has length |  X  | = Consider now the problem of jointly resampling the block containing T and a collection of excluded where S = A  X  = a  X  :  X  /  X  s is a configuration of the excluded auxiliary variables and C = A a dynamic program (described in Algorithm 3). The product of indicator functions shows that once a reading from the sampled tree t whether  X  is a constituent, for each  X  /  X  s .
 Given a selection vector s , we denote the induced block Gibbs kernel described above by k Since this kernel depends on the previous state only through the assignments of the auxiliary vari-ables, we can also write it as a transition kernel on the space { + ,  X  X  ments: k s ( a, a 0 ) . 3.2 The selection chain variables. This mechanism corresponds to picking which Gibbs operator k sition in the Markov chain on assignments described above. We will denote the random variable corresponding to the selection vector s at state ( i ) by S In standard treatments of MCMC algorithms [33, 22], the variables S ther independent (a mixture of kernels), or deterministic enumerations (an alternation of ker-nels). However this restriction can be relaxed to having S k for kernel selection. 3 The choice of k  X  is important. To understand why, recall that in the situation where ( A single cell in the chart is pruned, whereas in the case where ( A can be ignored. The construction of k  X  is therefore where having a simpler model or heuristic at so that better speedup can be achieved. Note that the algorithm can recover from mistakes in the simpler model, since the assignments of the auxiliary variables are also resampled. Another issue that should be considered when designing k  X  (repeating the same set of selections). To see why, note that if ( s, a ) = ( s paying the computational cost of a second iteration.
 The mechanism we used takes both of these issues into consideration. First, it uses a simpler model (for instance a grammar with fewer non-terminal symbols) to pick a subset M that have high posterior probability. Our kernel k  X  is restricted to selection vectors s such that next one, s 0 , as follows: after picking a random subset R  X  s of size Provided that the chain is initialized with | s | = 2 | M large portion of the state at every iteration (more precisely, | s  X  s the previous selection vector, but not on the assignment vector.
 such schemes will not converge to the correct distribution in general. Counterexamples are given in the adaptive MCMC literature [2]. 3.3 Accelerated averaging samples. In other words, how the variable E is updated in Algorithm 1.
 estimating expected sufficient statistics f is to average  X  X ard counts, X  i.e. to use the estimator: S For general Metropolis-Hastings chains, this is often the only method available. On the other hand, in our parsing setup X  X nd more generally, with any Gibbs sampler X  X t turns out that there is a more This is what we do when we add E m f to the running average in Algorithm 1.
 Suppose we have extracted samples X (1) , X (2) , . . . , X S X ( i +1) and add f ( X ( i +1) ) to a running average.
 R While we used the task of monolingual parsing to illustrate our randomized pruning procedure, the technique is most powerful when the dynamic program is a higher-order polynomial. We therefore demonstrate the utility of randomized pruning on a bitext parsing task. In bitext parsing, we have sentence-aligned corpora from two languages, and are computing expectations over aligned parse trees [6, 28]. The model we use is most similar to [3], but we extend this model and allow rules to mix terminals and non-terminals, as is often done in the context of machine translation [8]. These more challenging setup.
 In the terminology of adaptor grammars [19], our sampling step involves resampling an adapted derivation given a base measure derivation for each sentence. Concretely, the problem is to sample from a class of isotonic bipartite graphs over the nodes of two trees. By isotonic we mean that the Figure 4: Because each sampling step is three orders of magnitude faster than the exact computation (a,b), we can afford to average over multiple samples and thereby reduce the a fixed pruning scheme (c). Our auxiliary variable sampling scheme also substantially outperforms the tic-tac-toe pruning heuristic (d). edges E of this bipartite graph should have the property that if two non-terminals  X ,  X  aligned in the sampled bipartite graph, i.e. (  X ,  X  0 )  X  E and (  X ,  X  where  X   X   X  denotes that  X  is an ancestor of  X  . The weight (up to a proportionality constant) of can check that this yields a dynamic program of complexity O ( p factor (we follow [3] and use b = 3 ).
 machine translation research. Several researchers have found that state-of-the-art performance can be attained using grammars that mix terminals and non-terminals in their rules [8, 14]. Second, the randomized pruning method is most competitive in cases where the dynamic program has a suffi-ciently high degree. We did experiments on monolingual parsing that showed that the improvements were not significant for most sentence lengths, and inferior to the coarse-to-fine method of [25]. The bitext parsing version of the randomized pruning algorithm is very similar to the monolingual case. Rather than being over constituent spans, our auxiliary variables in the bitext case are over induced alignments of synchronous derivations. A pair of words is aligned if it is emitted by the same synchronous rule. Note that this includes many-to-many and null alignments since several or zero lexical elements can be emitted by a single rule. Given two aligned sentences, the auxiliary To compare our approximate inference procedure to exact inference, we follow previous work [15, 29] and measure the L 2 distance between the pruned expectations and the exact expectations. 4.1 Results We ran our experiments on the Chinese Treebank (and its English translation) [39], limiting the product of the sentence lengths of the two sentences to p  X  q  X  130 . This was necessary be-cause computing exact expectations (as needed for comparing to our baseline) quickly becomes prohibitive. Note that our pruning method, in contrast, can handle much longer sentences with-out problem X  X ne pass through all 1493 sentences with a product length of less than 1000 took 28 minutes on one 2.66GHz Xeon CPU.
 We used the BerkeleyAligner [21] to obtain high-precision, intersected alignments to construct the high-confidence set M 0 of auxiliary variables needed for k struct the support of the selection chain S ( i ) .
 For randomized pruning to be efficient, we need to be able to extract a large number of samples within the time required for computing the exact expectations. Figure 4(a) shows the average time required to compute the full dynamic program and the dynamic program required to extract a sin-gle sample for varying sentence product lengths. The ratio between the two (explicitly shown in Figure 4(b)) increases with the sentence lengths, and reaches three orders of magnitude, making it possible to average over a large number of samples, while still greatly reducing computation time. We can compute expectations for many samples very efficiently, but how accurate are the approxi-mated expectations? Figure 4(c) shows that averaging over several masks reduces bias significantly. but remains roughly constant when we average multiple samples. To determine the number of sam-ples in this experiment, we measured the time required for exact inference, and ran the auxiliary variable sampler for half of that time. The main point of Figure 4(c) is to show that under realis-tic running time conditions, the bias of the auxiliary variable sampler stays roughly constant as a function of sentence length.
 for each bispan. This figure of merit incorporates an inside score and an outside score. To compute this score, we used a product of the two IBM model 1 scores (one for each directionality). When a bispan figure of merit falls under a threshold, it is pruned away.
 In Figure 4(d), each curve corresponds to a family of heuristics with varying aggressiveness. With sampler, it is controlled by letting the sampler run for more iterations. For each algorithm, its coordinates correspond to the mean L 2 bias and mean time in milliseconds per sentence. The plot shows that there is a large regime where the auxiliary variable algorithm dominates tic-tac-toe for this task. Our method is competitive up to a mean running time of about 15 sec/sentence, which is well above the typical running time one needs for realistic, large scale training. There is a large body of related work on approximate inference techniques. When the goal is to maximize an objective function, simple beam pruning [10] can be sufficient. However, as argued in [4], beam pruning is not appropriate for computing expectations because the resulting approximation is too concentrated around the mode. To overcome this problem, [5] suggest adding a collection of samples to a beam of k -best estimates. Their approach is quite different to ours as no auxiliary variables are used.
 Auxiliary variables are quite versatile and have been used to create MCMC algorithms that can exploit gradient information [11], efficient samplers for regression [1], for unsupervised Bayesian inference [31], automatic sampling of generic distribution [24] and non-parametric Bayesian statis-is widely used for image segmentation [27]. Mask-based pruning is an effective way to speed up large dynamic programs for calculating feature expectations. Aggressive masks introduce heavy bias, while conservative ones offer only limited speed-ups. Our results show that, at least for bitext parsing, using many randomized aggressive masks generated with an auxiliary variable sampler is superior in time and bias to using a single, more conservative one. The applicability of this approach is in no way limited to the cases consid-ered here. Randomized pruning will be most advantageous when high-order dynamic programs can be vastly sped up by masking, yet no single aggressive mask is likely to be adequate.
