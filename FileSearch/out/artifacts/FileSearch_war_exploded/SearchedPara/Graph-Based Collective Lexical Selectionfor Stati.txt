 Lexical selection, which selects appropriate trans-lations for lexical items on the source side, is a cru-cial task in statistical machine translation (SMT). The task is closely related to two factors: 1) asso-ciations of selected translations with lexical items on the source side, including corresponding source items and their neighboring words, and 2) depen-other items on the target side.

As translation rules and widely-used n-gram language models can only capture local associ-ations and dependencies, we have witnessed in-creasing efforts that attempt to incorporate non-local associations/dependencies into lexical selec-tion. Efforts using source-side associations mainly focus on the exploitation of either sentence-level context (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or the utilization of document-level context (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Xiong et al., 2013). In contrast, target-side dependencies attract little attention, although they have an important im-pact on the accuracy of lexical selection. The most common practice is to use language mod-els to estimate the strength of target-side depen-dencies (Koehn et al., 2003; Shen et al., 2008; Xiong et al., 2011). However, conventional n-gram language models are not good at capturing long-distance dependencies. Consider the exam-ple shown in Figure 1. As the translations of pol-are far from each other, our baseline can only appropriately translates the other two words as  X  problem  X  and null , respectively, even with the support of an n-gram language model. If we could model long-distance dependencies among target translations of source words  X  w ` ent  X   X   X ( issue ), lation errors could be avoided.

In order to model target-side global dependen-cies, we propose a novel graph-based collective lexical selection framework for SMT. Specifically,  X  First, we propose a translation graph to model  X  Second, we introduce a collective lexical se- X  Finally, we incorporate confidence scores
We validate the effectiveness of our graph-based lexical selection framework on a hierarchi-cal phrase-based system (Chiang, 2007). Exper-iment results on the NIST Chinese-English test sets show that our approach significantly improves translation quality.

We begin in Section 2 with the construction of translation graph for each translated sentence. Then, we propose a graph-based collective lexical selection framework for SMT in Section 3. Ex-periment results are reported in Section 4. We summarize and compare related work in Section 5. Finally, Section 6 presents conclusions and di-rections for future research. Formally, a translation graph is a weighted graph G = ( N,E ) . In the node set N , each node repre-sents either a source word or a target translation that contains one or multiple target words. In the edge set E , an edge linking a source word to a tar-get translation is referred to as a source-target as-sociation edge , and an edge connecting two target translations is called as a target-target relatedness edge . In Section 2.1 and 2.2, we will answer the following two questions on the translation graph:  X  Which source words and their translations  X  How can we measure the strength of the 2.1 Graph Nodes For a source sentence, the most ideal translation graph is a graph that includes all source words and their candidate translations. However, this ideal graph has two problems: intensive compu-tation for graph inference and difficulty in mod-eling dependencies between function and content words. In order to get around these two issues, we only consider lexical selection for source content
We first identify source-side content word pairs using statistical metrics, and then keep word pairs with a high relatedness strength in the translation graph. To be specific, we use pointwise mutual in-formation (PMI) (Church and Hanks, 1990) and co-occurrence frequency to measure the related-ness strength of two source-side words s and s 0 within a window d s . Content word pairs will be kept when their co-occurrence frequencies are more than cf times in our training corpus and PMI values are larger than pmi . In this process, we remove noisy word pairs using the following heuristic rules: (1) As an adjective only has rela-tions with its head nouns or dependent adverbs, we remove all word pairs where an adjective is paired with words other than its head nouns or dependent adverbs; (2) We apply a similar con-straint to adverbs too, since the same thing hap-pens to an adverb and its head verb or head ad-jective. For example, in the Chinese sentence in Figure 1, the adjective  X  xi  X  angt  X  ong  X  is only related to the noun  X  l `  X ch  X  ang  X  although it also frequently co-occur with  X  w ` ent  X   X   X .

After identifying source-side content word pairs, we collect all target translations of these content words from extracted bilingual rules ac-cording to word alignments. These content words and target translations are used to build a transla-tion graph, where each node represents a source-side content word or a candidate target transla-tion. Note that there may be hundreds of differ-ent translations for a source word. For simplicity, we only consider target translations from transla-tion options that are adopted by the decoder after rule filtering. Let X  X  revisit the example in Figure 2, we include the following target translations in the translation graph:  X  problem  X ,  X  question  X ,  X  issue  X ,  X  hold  X , null ,  X  possess  X ,  X  stance  X  and  X  position  X . 2.2 Edges and Weights In this section, we introduce how we calculate weights for two kinds of edges in a translation graph. 2.2.1 Source-Target Association Edge Connecting a source-side content word and its candidate target translations, a source-target asso-ciation edge provides a way to propagate transla-tion association evidence from a source word to its candidate translations. Obviously, the stronger the association between a source word and its can-didate translation, the more evidence the corre-sponding edge will propagate. For each source-side content word, we obtain its candidate trans-lations via the kept word alignments. Following Xiong et al. (2014), we allow a target translation to be either a phrase of length up to 3 words or null when s is not aligned to any word in the cor-responding bilingual rule. We define the weight of the edge from a source-side content word s to its target translation  X  t as follows: where N ( s ) denotes the set of candidate target translations of s kept on the translation graph, and TP ( s,  X  t ) measures the probability of s be-ing translated to  X  t . It is very important to note that there is no evidence propagated from a target translation to a source word, as source-target asso-ciation edges only go from a source-side content word to its translations.

We compute TP ( s,  X  t ) according to the principle of maximal likelihood as follows: where count ( s,  X  t ) indicates how often s is aligned to  X  t in the training corpus. Using this method, we can compute the translation probabilities of the source-target association edges in Figure 2 as  X  hold  X )=0.22 and TP ( X  l `  X ch  X  ang  X ,  X  position  X )=0.47. 2.2.2 Target-Target Relatedness Edge Connecting two target translations of different re-lated source content words, a target-target relat-edness edge enables translation graph to capture dependencies between translations of any two dif-ferent source words.

Computing the weight of a target-target related-ness edge is crucial for our method. Intuitively, the stronger co-occurrence strength two transla-tions have, the more evidence should be propa-gated between them. Therefore we calculate the weight of a target-target related edge based on the co-occurrence strength of two translations linked by the edge. Formally, given the translation  X  t of source-side content word s and the translation  X  t 0 of source-side content word s 0 , the weight of the edge from  X  t to  X  t 0 is defined as follows: where N (  X  t ) denotes the set of candidate transla-strength of relatedness between  X  t and  X  t 0 which is calculated as the average word-level relatedness over all content words in these two translations  X  t and  X  t 0 .

As for the word-level relatedness RS ( t,t 0 ) for a content word pair ( t,t 0 ) , we estimate it with the following two approaches over collected co-occurring word pairs within a window of size d t : (1) RS ( t,t 0 ) is computed as a bigram conditional probability p lm ( t 0 | t ) via the language model; (2) Following (Xiong et al., 2011) and (Liu et al., 2014), we employ PMI to define RS ( t,t 0 ) as Based on the translation graph, we propose a col-lective lexical selection algorithm to jointly iden-tify translations of all source words in the graph. 3.1 Problem Statement and Solution Method As stated previously, the translation of a source-side content word s should be: 1) associated with s ; 2) related to the translations of other source-side content words. Thus, in the translation graph, the translation of s should be a target-side node which has: 1) an association edge with the node of s ; 2) many relatedness edges with other target-side nodes that represent translations of other source words.

Let X  X  revisit Figure 2. If we know that the trans-tween (  X  X ssue X ,  X  X old X  ) and between (  X  X ssue X ,  X  X osition X  ) can provide evidences that  X  hold  X  and  X  position  X  are the correct translations of  X  ch  X   X y  X  ou  X  and  X  l `  X ch  X  ang  X , respectively. On the other hand, the candidate translation  X  problem  X  is less related to  X  hold  X  and  X  position  X , which may suggest that it is not likely to be the correct translation of sociation relation with  X  w ` ent  X   X   X . However, in the translation graph, the correct target translation of a source word depends on correct translations of other source words in the graph, and vice versa. So how do we find these correct translations?
We propose a Random Walk (Gobel and Jagers, 1974) style algorithm to solve this problem, aim-ing to use both local source-target associations and global target-target relatedness simultane-ously during translation. In our algorithm, we as-sign each node an evidence score in the transla-tion graph, which indicates either the importance of a source word (for a source word node) or the confidence of a target translation being a correct translation (for a target word node). Specifically, we perform collective inference on the translation graph as follows:  X  First, we set initial evidence scores for nodes  X  Second, evidence scores are simultaneously In the following sub-section, we describe the two steps in detail. 3.2 Details of Our Algorithm Using the algorithm shown in Algorithm 1, we iteratively derive evidence scores for candidate translations. 3.2.1 Notations For a translation graph with n nodes, we assign each node an index from 1 to n and use this index to represent the node. We also use the following two notations:  X  The evidence vector V : an n -dimensional  X  The evidence propagation matrix M : an 3.2.2 Algorithm In Algorithm 1, we jointly infer the evidence scores of all candidate translations in the follow-ing three steps.

In Step 1 , we calculate the evidence propaga-tion matrix M according to the method described in Section 2.2 (equations (1) and (3)) ( Lines 1-8 ).
In Step 2 , we adopt different methods to set the value of V (0) according to the node type. If the node corresponds to a source word, we set the ini-tial value using its importance score in the trans-lation graph, as implemented in (Han et al. 2011) ( Lines 9-10 ). We calculate the importance score of the source word s using tf.idf as follows: where N src is the set of source words in the trans-lation graph. If the node corresponds to a target translation, its initial evidence score is 0 ( Lines 11-12 ).

In Step 3 , evidences are simultaneously rein-forced by propagating them among semantically related translations ( Lines 13-19 ). Specific to our algorithm, we update them by propagating evi-dences according to different types of relations in the evidence propagation matrix M . Formally, the recursive update of the evidence vector is defined as follows: where r is the number of iterations.

One problem with the above equation is that some nodes in the translation graph do not have evidence outgoing edges, such as translation nodes containing only function words or the null node. The evidence will disappear when passing through these nodes. To solve this problem, we propagate evidence in the form of reallocation: we reallocate a fraction of evidence to the initial evidence vector evidence vector is formulated as follows: where  X   X  (0 , 1) is the fraction of the reallocated evidence. We keep updating the evidence vec-tor according to this equation ( Line 16 ), until the maximal number of iteration maxIter is reached or the Euclidean distance ( Line 17 ) between evi-dence vectors calculated in two consecutive itera-tions is less than a pre-defined threshold ( Line 15 ).

In this way, we jointly infer the evidence scores of all candidate target translations in the transla-tion graph. Table 1 gives the evidence scores of the example in Figure 2. We can find that our sys-tem enhanced with target translation dependencies is able to select correct translations. 3.2.3 Integration of Derived Evidence Score For each translated sentence, we may build multi-ple translation graphs. For each translation graph, we infer evidence scores of translations repre-sented by graph nodes using the above-mentioned algorithm before decoding. Then, for each can-didate translation of a source-side content word, we normalize its evidence score over the cor-responding translation graph to form an addi-tional lexical translation probability ( Lines 20-23 ). For instance, the normalized evidence score of  X  ch  X   X y  X  ou  X  translated into  X  hold  X  is calculated as 0 . 1218 / (0 . 1218 + 0 . 0244 + 0 . 0604)  X  0 . 5895 . In this way, for each bilingual rule with word align-ments, we will obtain a new lexical weight which can be used together with the original translation probabilities and lexical weight to improve lexical selection in SMT. 4.1 Setup Our bilingual training corpus is the combina-tion of the FBIS corpus and Hansards part of LDC2004T07 corpus ( 1 M parallel sentences, 54 . 6 K documents, with 25 . 2 M Chinese words and 29 M English words). We word-aligned them using GIZA++ (Och and Ney, 2003) with the op-tion  X  grow-diag-final-and  X . We chose the NIST evaluation set of 2005 (MT05) as the development set, and the sets of MT06/MT08 as test sets. We used SRILM Toolkit (Stolcke, 2002) to train one 5-gram language model on the Xinhua portion of Gigaword corpus.
 To construct translation graphs, we first used the cess (word segmentation, PoS tagging and so on) Chinese and English sentences, respectively. We used the Chinese part of our bilingual corpus and an additional Chinese LDC Xinhua news corpus ( 10 . 2 M sentences with 279 . 9 M words) as train-ing data to collect Chinese word pairs. We set window size d s = 15 , thresholds pmi = 0 , cf = 5 to identify Chinese related word pairs in the NIST translated sentences. Averagely, these three sets contain 13 . 5 , 10 . 3 and 9 . 5 content words used to build translation graphs per sentence, respec-tively. Using the English part of our bilingual cor-pus and the Xinhua portion of Gigaword corpus as training data, we set window size d t = 20 , and used the SRILM toolkit with Witten-Bell smooth-ing and PMI to calculate relatedness strengths for target-side translations. To avoid data sparseness, we build the graph using the surface forms of words while calculating the word relatedness at the lemma level. To achieve this, we converted each word into its corresponding lemma with the exception of adjectives and adverbs. In the proce-dure of collective lexical selection, the difference eration number maxIter 100 .

We reimplemented the decoder of Hiero (Chi-ang, 2007), a famous hierarchical phrase-based (HPB) system. HPB system is a formally syntax-based system and delivers good performance in various translation evaluations. During decod-ing, we set the ttable-limit as 20 , the stack-size as 100 . The translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). To alleviate the impact of the instability of MERT (Och, 2003), we ran it three times for each exper-iment and reported the average BLEU scores as suggested in (Clark et al., 2011). Finally, we con-ducted paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. 4.2 Our Method vs Other Methods In the first group of experiments, we investigated the effectiveness of our model by comparing it against the baseline as well as two additional mod-els: (1) lexicalized rule selection model (He et al., 2008) ( LRSM ), which employs local context to improve rule selection in the HPB system; (2) topic similarity model (Xiao et al., 2012) 5 ( TSM ), which explores document-level topic information for translation rule selection in the HPB system. Furthermore, we combined our model with the two models to see if we could obtain further im-provements. For this, we integrated the new lexi-cal weight learned by our model as a new feature into the LRSM / TSM system.

Table 2 reports the results. All models outper-form the baseline. Especially, our graph-based lexical selection model GM(PMI) achieves an av-erage BLEU score of 26.40 on the two test sets, which is higher than that of the baseline by 0.65 BLEU points. This improvement is statistically significant at p&lt; 0 . 01 . The BLEU score of our model is close to those of LRSM and TSM, which achieve an average BLEU score of 26 . 55 and 26 . 35 on the two test sets, respectively. As PMI is slightly better than LM in our model, we use PMI in experiments hereafter.

The combination of our model and LRSM is able to further improve translation quality in terms of BLEU. In this case, the average BLEU score of the improved system is 26 . 95 , with 0 . 4 BLEU points higher than LRSM. When combining our model with TSM, we obtain an average BLEU score of 26 . 80 , which is better than TSM by 0 . 45 BLEU points. The two improvements over LRSM and TSM are also statistically significant at p&lt; 0 . 05 . These experiment results suggest that exploring long-distance dependencies among tar-get translations is complementary to the previous lexical selection methods which focus on source-side context information.

In order to know how our approach improves the performance of the HPB system, we compared the best translations of the HPB system using dif-ferent models. We find that our approach really improves translation quality by utilizing target-side long-distance dependencies which are, on the contrary, ignored in previous methods.

For example, the source sentence  X ... ;  X  .  X  c 9  X  [ n ]  X  /  X  ...  X  |  X   X  {  X   X  ... X  is translated as follows:  X  Ref : ... musharraf and some tribal leaders in  X  Baseline : ... musharraf last september and  X  LRSM : ... musharraf last september and  X  LRSM+GM(PMI) : ... last september Here both the baseline and LRSM fail to ob-tain the right translation for the word  X  n  X  be-cause  X  palestine  X  has a higher probability than  X  pakistan  X  ( 0 . 0374 vs 0 . 0285 ). However, in our model, the long-distance dependencies between (  X  X usharraf  X ,  X  X akistan X  ) and (  X  X aliban X ,  X  X ak-istan X  ) help the decoder correctly choose the translation  X  pakistan  X  for  X  n  X .
 In yet another example, the source sentence  X  { F "  X   X   X  K [  X  ]  X   X  1  X  is translated as follows:  X  Ref : us hopes agreement on north korean nu- X  Baseline : us hoped that the dprk nuclear is- X  TSM : us hope that the full implementation of  X  TSM+GM(PMI) : us hope that the dprk Even with TSM, the HPB system did not trans-late  X   X   X  at all because translation rules  X  X 1 ||| X 2 implementation of X 1  X  are used to trans-late the source sentence by the baseline and TSM systems respectively. However, in the combined model TSM+GM(PMI), the differences in relat-edness scores between (  X  X uclear X ,  X  X greement X  ), (  X  X ssue X ,  X  X greement X  ) and (  X  X greement X ,  X  X m-plemented X  ) encourage the enhanced system to se-lect right translation for this word. 4.3 Effect of Reallocation Weight  X  .
 In Eq. (6), the reallocation weight  X  determines which part plays a more important role in our method. In order to investigate the effect of  X  on our method, we tried different values for  X  : from 0 . 1 to 0 . 5 with an increment of 0 . 05 each time. The experimental setup is the same as the previous experiments. Figure 3 shows the aver-age BLEU scores on the two test sets. Our sys-tem performs well when  X  ranges from 0 . 1 to 0 . 25 . The performance drops when  X  is larger than 0 . 25 . A small reallocation weight  X  reduces the impact of initial evidences and local source-side associa-tions in the collective lexical selection algorithm, but increases the impact of global dependencies of target-side translation, which are normally not considered in previous lexical selection methods. This performance curve on the values of  X  sug-gests that target-side global dependencies are im-portant for lexical selection. The collective inference algorithm is partially in-spired by Han et al. (2011) who propose a graph-based collective entity linking (EL) method to model global interdependences among different EL decisions. We successfully adapt this algo-rithm to lexical selection in SMT. Other related work mainly includes the following two strands. (1) Lexical selection in SMT . In order to cap-ture source-side context for lexical selection, some researchers propose trigger-based lexicon models to capture long-distance dependencies (Hasan et al., 2008; Mauser et al., 2009), and many more re-searchers build classifiers with rich context infor-mation to select desirable translations during de-coding (Chan et al., 2007; Carpuat and Wu, 2007; He et al., 2008; Liu et al., 2008). Shen et al. (2009) introduce four new linguistic and contextual fea-tures for HPB system. We have also witnessed in-creasing efforts in the exploitation of document-level context information. Xiao et al. (2011) impose a hard constraint to guarantee the trans-lation consistency in document-level translation. Ture et al. (2012) soften this consistency con-straint by integrating three counting features into decoder. Hardmeier et al. (2012, 2013) introduce a document-wide phrase-based decoder and inte-grate a semantic language model that cross sen-tence boundaries into the decoder. Based on topic models, Xiao et al. (2012) present a topic simi-larity model for HPB system, where each rule is assigned with a topic distribution. Also relevant is the work of Xiong et al. (2013), who use three different models to capture lexical cohesion for document-level machine translation. Compared with the above-mentioned studies, our method fo-cuses on the exploitation of global dependencies among target translations, which has attracted lit-tle attention before.

Different from exploring source-side context, other researchers pay attention to the utilization of target-side context information. The com-mon practice in SMT is to use an n-gram lan-guage model to capture local dependencies be-tween translations (Koehn et al., 2003; Xiong et al., 2011). Yet another approach exploring target-side context information is proposed by Shen et al. (2008), who use a dependency language model to capture long-distance relations on the target side. Moreover, Zhang et al. (2014) treat translation as an unconstrained target sentence generation task, using soft features to capture lexical and syntac-tic correspondences between the source and tar-get language. Recently, many researcher have proposed to use deep neural networks to model long-distance dependencies of arbitrary length for SMT (Auli et al., 2013; Kalchbrenner and Blun-som, 2013; Devlin et al., 2014; Hu et al., 2014; Liu et al., 2014; Sundermeyer et al., 2014). Our work is significantly different from these meth-ods. We use a graph representation to capture local and global context information, which, to the best of our knowledge, is the first attempt to explore graph-based representations for lexical selection. Furthermore, our model do not resort to any syn-tactic resources such as dependency parsers of the target language. (2) Random walk for SMT . Because of the advantage of global consistency, random walk al-gorithm has been applied in SMT. For example, Cui et al. (2013) develop an effective approach to optimize phrase scoring and corpus weighting jointly using graph-based random walk. Zhu et al. (2013) apply a random walk method to dis-cover implicit relations between the phrases of dif-ferent languages. Aiming to better evaluate trans-lation quality at the document level, Gong and Li (2013) run PageRank algorithm to assign weights to words in translation evaluation. Different from these studies, the key interest of our research lies in the lexical selection with random walk. This paper has presented a novel graph-based collective lexical selection method for SMT. We build translation graphs to capture local source-side associations and global target-side dependen-cies, and propose a purely collective inference al-gorithm to jointly identify target translations of source-side content words in translation graphs. Our method capitalizes on capabilities of transla-tion graphs to represent both local and global rela-tions on the source/target side. Experiment results demonstrate the effectiveness of our method.
In the future, we plan to further improve our model by capturing semantic relatedness among source words. Additionally, we also want to jointly model different levels of context informa-tion in a unified framework for SMT.
 The authors were supported by National Nat-ural Science Foundation of China (Grant Nos 61303082 and 61403269), Natural Science Foundation of Jiangsu Province (Grant No. BK20140355), Research Fund for the Doctoral Program of Higher Education of China (Grant No. 20120121120046), Research fund of the State Key Laboratory for Novel Software Technology in Nanjing University (Grant No. KFKT2015B11), the Special and Major Subject Project of the Industrial Science and Technology in Fujian Province 2013 (Grant No. 2013HZ0004-1), and 2014 Key Project of Anhui Science and Technology Bureau (Grant No. 1301021018). We also thank the anonymous reviewers for their insightful comments.

