 Mining user preferences plays a critical role in many impor-tant applications such as customer relationship management (CRM), product and service recommendation, and market-ing campaigns. In this paper, we identify an interesting and practical problem of mining user preferences: in a multidi-mensional space where the user preferences on some cate-gorical attributes are unknown, from some superior and in-ferior examples provided by a user, can we learn about the user X  X  preferences on those categorical attributes? We model the problem systematically and show that mining user pref-erences from superior and inferior examples is challenging. Although the problem has great potential in practice, to the best of our knowledge, it has not been explored systemat-ically before. As the first attempt to tackle the problem, we propose a greedy method and show that our method is practical using real data sets and synthetic data sets. H.2.8 [ Database Management ]: Database Applications X  Data Mining Algorithms, Experimentation Preferences, superior examples, inferior examples, skyline
Mining user preferences plays a critical role in many im-portant applications, such as customer relationship man-agement (CRM), product and service recommendation, and marketing campaigns. Although many existing studies have explored how to use preferences to improve service quality such as obtaining better query answering with user prefer-ences [14, 15, 3], effectively capturing user preferences still largely remains a challenging problem.

In this paper, we identify an interesting and practical problem: mining user preferences from superior and inferior examples . To motivate the problem, consider an application scenario where a realtor learns a customer X  X  preference and makes recommendations.

A customer X  X  preference on realties depends on many fac-tors, such as price, location, style, lot size, number of bed-rooms, age of the realty, developer, etc. Some attributes, such as price, are numeric and come with a well accepted preference (e.g., the cheaper in price, the better). On some categorical attributes such as location, style, and developer, the preferences often vary fro m customer to customer. More-over, a customer X  X  preferences on those categorical attributes are often oblivious or may not be obtained by a realtor in a complete and explicit way.

In order to recommend realties effectively, it is important that a realtor can capture customers X  preferences well. A typical scenario is that a realtor gives a list of realties as examples. Then, a customer often picks a small subset as superior examples which the customer wants to see. For each superior example o , according to the customer X  X  preferences, there does not exist another realty o which is as good as o in every aspect, and is better than o in at least one aspect. This condition is necessary for a superior example since, otherwise, the user should see o instead of o .

The superior examples differ from each other in some as-pects, which reflect the tradeoffs that the customer would like to consider. For example, a customer may pick two su-perior examples: one with a higher price in an area by the beach, and the other with a lower price in an area without a beach. The two superior examples indicate that the cus-tomer is willing to consider the tradeoff between price and the beach.

At the same time, the customer also often picks a small subset as inferior examples which the customer definitely does not want to see. For each inferior example o , according to the customer X  X  p references, there exists at least one realty o which is as good as o in every aspect, and is better than o in at least one aspect. In other words, the customer regards o a better choice than o .

What can the realtor learn about the customer X  X  prefer-ences from those superior and inferior examples? While many realtors learn customers  X  preferences in the above sce-narios using their professional experience, in this paper, we model the problem as mining user preferences from superior and inferior examples, and develop a data mining solution so that the learning procedure can be automated.

One may wonder whether obtaining superior and inferior examples from a user is feasible in real applications, since a user may not want to give examples by checking thousands of realties currently available in the market. We advocate that a practical solution to the user preference mining problem is the core of the interactive learning and recommendation procedure which can be heavily employed in e-business and many other applications.

For example, instead of asking a user to pick examples from thousands of realties in the market, a realtor can give a short-list of tens of realties. A user can indicate some su-perior and inferior examples in the short list. Using the pref-erences learnt from this short-list and the superior/inferior examples, a realtor can filter out those realties in the market definitely uninteresting to the user, and recommend those in the market definitely preferred by the user. Among the rest that whether the customer prefers or not is unknown, an-other short-list can be made and provided to the customer so that more superior and inferior examples can be obtained. By the interactive and iterative learning procedure, the cus-tomer can be provided more and more accurate recommen-dations as the preferences are learnt progressively.
Automation of the user preference mining procedure can easily find significant applications in many domains. For example, a realtor web site can be built based on the in-teractive and iterative user preference learning and realty recommendation procedure described above. Such recom-mendation services are highly feasible and useful in many web-based business applications.

Although of great potential, to the best of our knowledge, the problem of mining user preferences from superior and in-ferior examples has not been explored before. In this paper, we tackle the problem and make several contributions. First, we identify and model the problem systematically. Second, our theoretical problem analysis indicates that mining pref-erences from superior and inferior examples is challenging. Therefore, we need to develop heuristic methods that are ef-fective in practice. Third, we develop a greedy method and show the effectiveness and the efficiency of the method using both real data sets and synthetic data sets. Last, we discuss possible extensions of our method and several interesting and promising directions for future work.

The rest of the paper is organized as follows. In Sec-tion 2, we present a formal problem definition and study the complexity of the problem. We review the related work in Section 3. A greedy method is developed in Section 4. An extensive empirical evaluation using both real data sets and synthetic data sets is reported in Section 5. Section 6 concludes the paper.
In this section, we define the problem of user preference mining, and study the complexity of the problem. Table 1 summarizes some notions frequently used in this paper. Lim-ited by space, the proofs of the theoretical results are omit-ted here, but can be found in the full version of the paper [9]. Let O be a set of objects in a multidimensional space D = D 1  X  D 2  X  X  X  X  X  D d . O can be finite or infinite. A user preference  X  is a strict partial order on O .Thatis, Table 1: The summary of frequently used notions.

In many applications, due to the large size of O or some other constraints, it is often infeasible to learn the user pref-erence on O completely. Instead, the user preference can often be decomposed into preferences on attributes in D fol-lowing the principles of preferences in [14].

A user preference  X  i on an attribute D i  X  X  (1  X  i  X  d ) is a strict partial order on the domain of D i .Thatis,for two values u, v  X  D i ,if u  X  i v , the user prefers u than v . Again, we write u i v if u  X  i v or u = v .

The assumption of independent attributes is often made: a user X  X  preference in space D is a composition of her/his preferences on all attributes. The composition of a set of preferences { X  1 ,...,  X  d } on attributes D 1 ,...,D d is a strict partial order  X  =(  X  1 ,...,  X  d ) such that for any objects ery attribute D i  X  X  (1  X  i  X  d ), and o 1 .D i 0  X  i 0 o 2 .D holds on at least one attribute D i 0  X  X  . Asindicatedin[14], such an assumption often holds in practice.

Example 1 (Preliminaries). Consider a customer X  X  preference in choosing hotels. Suppose two factors matter: the price (the lower the better) and the star level (the higher the better). In the three hotels in Table 2, since Best View is more preferable than Cindy X  X  in both star level and price, Best View  X  Cincy X  X . Amazing is better than Best View in star level, but Best View is cheaper than Amazing. Thus, the customer does not prefer one than the other between Amazing and Best View.
On some attributes such as price, customers often have well accepted and consistent preferences. For example, all customers prefer low price. On some other attributes such as hotel brands, airlines, and locations, however, the prefer-ences may vary dramatically from one user to another.
An attribute is called determined if a preference is defined on the attribute for all users. An attribute is called undeter-mined if there is no preference defined on the attribute for all users. To keep our discussion simple, we focus on learning user preferences on undetermined categorical attributes.
In order to learn preferences on undetermined attributes, we need the input from a target user. In this study, we assume that a target user provides two types of input: a set S of superior examples and a set Q of inferior examples. An object o 1 is called a superior example if the target user specifies that, according to the customer X  X  preference, o 1 is not dominated by any other objects o .Anobject o 2 is an inferior example if the target user specifies that, according to the customer X  X  preference, o 2 is dominated by some other objects o .

In literature, a superior example is also called a skyline point or a maximal vector [16]. The skyline in a data set is the complete set of all skyline points. A point is inferior if it is not a skyline point. In a large data set, there can be many superior examples. Please note that, in our model, the target user is not required to specify all superior and inferior examples. Instead, a user may only provide a small number of superior and inferior examples. The data mining task is to learn the user X  X  preferences as much as possible using those examples.
 Without loss of generality, let D = D D  X  X  U such that D
D  X  X  U =  X  ,where D D = { D 1 ,...,D d } (0  X  d &lt;d )isthe set of determined attributes and D U = { D d +1 ,...,D d } the set of undetermined attributes. Let  X  D be the preference defined on D D . Given a set of objects O ,aset S  X  X  of superior examples and a set Q  X  X  of inferior examples such that S  X  Q =  X  , a set of preferences R = { X  d +1 ,...,  X  called a satisfying preference set ( SPS for short) if (1) (1  X  i  X  d  X  d ) is a preference on attribute D d + i ,and (2) according to the co mposite preference  X  =(  X  D ,  X  d ,...,  X  d ), every object o 1  X  S is not dominated by any other object o  X  X  , and every object o 2  X  Q is dominated by at least one other object o  X  X  .

Generally, given a set of superior and inferior examples, there may be no SPS, one SPS, or multiple SPSs.

Example 2 (SPS). Consider the set of objects in Ta-ble 3. Let A be a determined attribute where the preference is a 1  X  A a 2 .Let B and C be two undetermined attributes.
Suppose a user specifies the set of superior examples S 1 = { o 1 ,o 3 } and the set of inferior examples Q 1 = { o 2 } user does not label objects o 4 and o 5 .Object o 2 is inferior, and o 1 is the only object that can dominate o 2 in the data C . On the other hand, since o 3 is superior, if c 1  X  C c 2 ,then with respect to S 1 and Q 1 .
 Suppose another user specifies the set of superior examples S 2 = { o 1 } and the set of inferior examples Q 2 = { o 4 easy to verify that both { b 1  X  B b 2 ,c 1  X  C c 2 } and } are SPSs with respect to S 2 and Q 2 . In other words, the SPSs are not unique in this case.

Suggested by Example 2, we study two problems in this paper. The first problem is whether a SPS exists.
Problem 1 (SPS existence). Given a set of superior examples S and a set of inferior examples Q , determine whether there exists at least a SPS R with respect to S and Q .
 As elaborated by the second case in Example 2, multiple SPSs may exist with respect to a set of superior examples and a set of inferior examples. Then, how can we evaluate the quality of the SPSs?
To avoid overfitting, we advocate the minimal SPSs which are the simplest hypotheses that fit the superior and inferior examples. A preference is a strict partial order which can be represented as a binary relation. The complexity of a strict partial order can be measured b y the cardinality of the tran-sitive closure of the binary relation. The intuition is that a stronger preference relation sets preferences between more pairs of objects than a weaker preference relation. Tech-nically, let  X  be a strict partial order. The complexity of  X  is denoted by | E (  X  ) | ,where E (  X  )denotesthetransitive closure of  X  as a binary relation.

The second problem about preference mining is to find a minimal SPS.

Problem 2 (Minimal SPS). Forasetofsuperiorex-amples S and a set of inferior examples Q , find a SPS R = { X  d +1 ,...,  X  d } with respect to S and Q such that | E (  X  d +1 ,...,  X  d ) | is minimized. R is cal led a minimal SPS .
As described in Section 2.1, under the assumption of in-dependent attributes, the preference in a multidimensional space is the composition of the preferences in all attributes. The complexity of the preference in a multidimensional space can be derived by the following rule.

Theorem 1 (Multidimensional preferences). In space D = D 1  X  D 2  X  X  X  X  X  D d ,let  X  i (1  X  i  X  d ) be a preference on attribute D i ,and  X  =(  X  1 ,...,  X  d ) .Then, where | D i | is the number of distinct values in attribute D in the data set.
In this section, we study the complexity of the SPS ex-istence problem and the minimal SPS problem. When we consider the two problems with l undetermined attributes, we call them the l -d SPS existence problem and the l -d min-imal SPS problem, respectively.
 Lemma 1. The 2 -d SPS existence problem is NP-complete. Proof sketch. The SPS existence problem is in NP. We can prove that the 2-d SPS existence problem is NP-complete by polynomial time reducing the 3SAT problem [6].

Theorem 2. The existence problem is NP-complete, even when there is only one undetermined attributes (i.e., l =1 ). Proof sketch. We can prove that there is a polynomial time redution from the l -d SPS existence problem to the ( l + 1)-d SPS existence problem, and vice versa. Following Lemma 1, the SPS existence problem is NP-complete.
Clearly, the minimal SPS problem is more difficult than the SPS existence problem. For a set of preferences that does not satisfy the given superior and inferior examples, we define its complexity (Theorem 1) as infinity. Since the minimal SPS problem is not in NP, applying Theorem 2, the following theorem immediately follows.

Theorem 3. The minimal SPS problem is NP-hard.
User preferences have been well recognized important in many applications. Kie X ling [14] introduced an expressive theoretical framework for pre ferences. The framework con-siders preferences in a multidimensional space. A set of preference constructors are given for both categorical and numerical domains. Our study follows the preference con-struction framework in [14].

A statistical model of user preferences is presented in [12, 11]. In the statistical model, the frequency of an item in a data set depends on two factors: the user X  X  preference and the accessibility of the item. Moreover, a user X  X  preference on an item can be further modeled as a function on the features of the item as well as the user profile which can be approximated by the user X  X  behavior history data.
A framework of expressing and combining preferences is proposed in [1]. A user can assign a preference score to items and a model is applied to combine preferences. However, the model is predefined and is not learnt from data.

Different from [14, 12, 11, 1] which are dedicated to mod-eling user preferences, this s tudy focuses on mining prefer-ences from examples.

The problem of mining user preferences has been accessed by some previous studies from some angles different from this study. For example, Holland et al. [7] develop the data driven preference mining approach to find preferences in user session data in web logs. The central idea is that the more frequently an item appears, the more preferable the item is. However, the mining methodology in [7] may not be accurate since it does not consider the accessibility of data items which is important as indicated by [11].

Most recently, the problem of context-aware user prefer-ence mining is addressed in [8]. The major idea is that user preferences may be transient when the context (e.g., the top-ics in web browsing) changes. The data-driven approach [7] is extended accordingly. [7, 8] do not use any explicit preference examples provided by users. To this extent, [7, 8] are analogous to the unsuper-vised learning approaches in classical machine learning [19]. In this study, we advocate to use superior and inferior ex-amples in preference mining. Our method is analogous to the supervised learning approaches [19, 5].

There are also studies on supervised mining user prefer-ences. Cohen et al. [20] develop an approach to order objects given the feedback that an object should be ranked higher than another. Joachims [10] uses a support vector machine algorithm to learn a ranking function of web documents uti-lizing user query logs of the search engine. Both studies focus on mining the order of objects according to user pref-erences. However, in this paper, instead of ordering objects, we are interested in learning the preferences in attributes whicharethereasonwhyobjectsshouldberankedinsuch an order. We mine the preference set on each attribute un-derneath the preferences on object level.
 Table 4: A set of objects as the running example. Table 5: The conditions in the running example.
 Our problem is also related to the classification problem. But the existing classification methods cannot be applied to the preference mining problem. In the traditional clas-sification model, a set of training examples are labeled and the distributions of classes in the data space is learnt. The prediction is on the class of an unseen case. In this study, the superior and inferior examples are based on the dom-inance relation, and the relations between data points are learnt. The prediction is on, given two cases whose domi-nance relation is unknown, whether one case dominates the other.

User preferences are used in many applications, such as personalized recommendation systems [18] and preference queries on large databases [13, 3, 4, 15, 17].
As indicated in Section 2.3, the SPS existence problem is NP-complete and the minimal SPS problem is NP-hard. Any polynomial time approximation algorithm cannot guar-antee to find a SPS whenever a SPS exists. In other words, such an approximation algorithm may fail to find a SPS in some cases where a SPS does exist. Moreover, any poly-nomial time approximation algorithm cannot guarantee the minimality of the SPSs found.

In this section, we develop a greedy method to mine a simple SPS with respect to a set of superior examples S and a set of inferior examples Q . Our focus is on the practicality of the method  X  being simple and easily implementable.
For a set of preferences R on the undetermined attributes, an object q  X  Q is called satisfied if q is an inferior object with respect to composite preference (  X  D ,R ). Similarly, apoint s  X  S is called satisfied if s is a superior object with respect to (  X  D ,R ). In Section 4.1, we give a method to satisfy the inferior examples in Q . In Section 4.2, we describe how to satisfy the superior examples in S .
For two ob jects o 1 ,o 2  X  X  , consider the preference  X  D inates o 2 depends on the preferences on the undetermined attributes. o 1 is said to partially dominate o 2 . On the other
For each ob ject q  X  Q ,let P ( q )bethesetofobjectsin O that partially dominate q . If there exists an object p  X  such that p  X  D q and p.D U = q.D U ,then q is satisfied. In such a case, q is said to be trivially satisfied by  X  D .Wecan remove q from Q .

Suppose q is not trivially satisfied. For any set of pref-erences R on the undetermined attributes, if R satisfies q , there must exist at least one object p  X  P ( q ) such that p  X  R q ,where  X  R is the composite preference of R .Wecall C ( p )= p  X  R q a condition of q being an inferior example. Each conjunctive element ( p.D i  X  i q.D i )isa preference term (or term for short) of preference  X  i . A condition consists of preference terms on multiple undetermined at-tributes. And a preference term can appear in many condi-tions. A condition is satisfied if and only if all terms in the condition are satisfied.

In implementation, we build an in-memory R-tree on de-termined attributes to facilitate detecting whether an infe-rior object is trivially satisfied and computing the conditions of inferior objects.

Example 3 (Conditions). Consider the set of objects in Table 4 in space D = D 1  X  D 2  X  D 3  X  D 4 .Both D 1 and Q = { o 2 ,o 5 ,o 7 ,o 8 ,o 10 } of inferior examples. moved. Table 5 shows the conditions of the other 4 inferior examples.

Intuitively, in order to satisfy all inferior examples in Q , we have to satisfy at least one condition for each object in Q . We can select one condition for each inferior example, and then build a SPS by satisfying all selected conditions. The total solution space is of size tive method to find the minimal SPS enumerates all possible solutions and outputs one SPS with the minimum complex-ity. Clearly, an exhaustive method is computationally pro-hibitive in practice. To tackle the problem, we develop two greedy algorithms to find approximate minimal SPSs. Let R =(  X  d +1 ,...,  X  d ) be the SPS to be computed. Initially, we set  X  i =  X  for each undetermined attribute D ( d &lt;i  X  d ). We iteratively add a term t into a preference  X  i until all inferior examples in Q are satisfied. The util-ity of a term t on attribute D i is measured by two factors: (1) complexity increment CI ( t ) which is the increase of size of E ( R )if t is selected, and (2) inferior example coverage Cov ( t ) which is the number of interior examples newly sat-isfied if t is selected.

To keep the complexity of E ( R ) low, the smaller the com-plexity increment and the larger the inferior example cover-age, the better a term. We define a utility score of a term utility score in each iteration. Algorithm 1 describes the algorithm framework.
 Algorithm 1 The term-based greedy algorithm.
 1: initialize all  X  i =  X  ( d &lt;i  X  d ); 2: for all q  X  Q do 3: compute P ( q ); 4: if q is not trivially satisfied then 5: for all p  X  P ( q ) do 6: compute condition C q ( p ) 7: end for 8: else 9: Q = Q \{ q } 10: end if 11: end for 12: while Q =  X  do 13: update score ( t ) for each preference term t ; 14: find the term t with the largest score ( t )value; 15: if t conflicts with previous selected terms then 16: remove t ; 17: else 18: include t into R ; 19: for all q  X  Q do 20: for all p  X  P ( q ) do 22: Q = Q \{ q } ; break ; 23: end if 24: end for 25: end for 26: end if 27: end while 28: return R
Note that we cannot select a term which conflicts with the not be selected, because a partial order is anti-symmetric. Such a conflicting term is removed in lines 15 and 16.
For a preference on an attribute, if a preference term is selected, multiple pairs may be added into the transitive closure of the updated preference.

Example 4 (Implied preference terms). Suppose attribute is 2.

For a preference  X  i and a term t on attribute D i ( d &lt; i  X  d ), if a term t holds in preference transitive closure E (  X  i  X  X  t } ) but not in E (  X  i ), then t is called an implied preference term from t and  X  i . Imp ( t,  X  i )isthesetofall implied terms from t and  X  i . Trivially, t  X  Imp ( t,  X  i
As mentioned in Section 2, a preference on an attribute can be represented as a directed acyclic graph. We main-tain the transitive closure of  X  i on each undetermined at-tribute D i .Onceaterm t on D i is selected, we can compute Imp ( f,  X  i )intimeO( | D i | )where | D i | is the cardinality of D . Using Theorem 1, we can compute the complexity of the new preference and derive the complexity increment. Example 5 (complexity increment). In Table 4, R = { X  3 ,  X  4 } . | D 3 | =5and | D 4 | = 3. Suppose, before an iteration,  X  3 = { a 1  X  3 a 2 } and  X  4 =  X  . Applying Theorem 1, |
E ( R ) | =(1+5)  X  3  X  5  X  3=3. If a 3  X  3 a 1 on D 3 is selected in the iteration, as shown in Example 4, | E (  X  3 ) | =3. So |
E ( R ) | =(3+5)  X  3  X  5  X  3 = 9. Therefore, the complex-Similarly, CI ( b 1  X  4 b 2 )=6.

A condition C q ( p ) of an inferior example q consists of at most ( d  X  d ) terms, each on one undetermined attribute. We write t  X  C q ( p )if t is a conjunctive element of C q We also denote by | C q ( p ) | the number of terms in condition C ( p ). | C q ( p ) | X  d  X  d .Ifaterm t  X  C q ( p ) is selected, then coverage of term t over condition C q ( p )as Example 6 (Coverage of term over condition).
 C
How should we define the coverage of a term over an in-ferior example? The above example indicates that a term t can appear in more than one condition of an inferior ex-ample, and it can appear in the conditions of many inferior examples. Moreover, we also have to consider the terms implied from t . Let us see the example below.
 Example 7 (Coverage of implied terms). In thermore, 1 2 of o 5 will be satisfied.

To sum up the above discussion, if a term t and its implied terms appear in many conditions of an inferior example q , the coverage of t over q is the largest coverage of t and its implied terms over one condition of q . The reason is that we only need to satisfy one condition in order to satisfy an inferior example. Formally, Finally, the total inferior example coverage of t is the sum of the coverage of t over all inferior examples, that is, Example 8 (Inferior Example Coverage). Continue Example6,wehave Cov ( b 3  X  4 b 1 ) = max { 0 . 5 , 1 } +max = 2.

We elaborate the term-based greedy method in the fol-lowing example.
 Example 9 (The term-based greedy algorithm).
 We run the term-based greedy algorithm on our running ex-ample (Tables 4 and 5). Table 6 shows the utility score Table 6: A running example of term-based algo-rithm.
 Algorithm 2 The condition-based greedy algorithm. ( Cov/CI )ofeachtermineachiteration. Onceatermis selected in an iteration (marked by *), the conditions of all satisfied inferior examples are removed, and some terms are also removed if they do not appear in any surviving condi-
After iteration 3, all inferior examples are satisfied. Fi-nally, we obtain a SPS R = { X  3 ,  X  4 } where  X  3 = { a 1
The term-based algorithm selects one term in each iter-ation, eventually to satisfy at least one condition for each inferior example. Once the best term t is selected, the con-ditions containing t are likely to be satisfied very soon, since they have less terms left. However, if such a condition has aterm t with large complexity increment, then selecting t will result in large complexity of the final result.
For example, in iteration 2 in Example 9, the best term next iteration due to the fact that they are both in conditions C
To overcome the deficiency of the term-based algorithm, we develop a condition-based algorithm which selects the best condition in each iteration, instead of the best term. We define the inferior example coverage of a condition C ( p ) to be the sum of the inferior example coverages of all its terms. That is, We also apply Theorem 1 to compute the complexity incre-ment of C q ( p ) by selecting all terms of C q ( p ). Please note that CI ( C q ( p )) is not equal to the sum of the complexity increments of all its terms.
 The utility of a condition is defined as score ( C q ( p )) = Cov ( C q ( p )) /CI ( C q ( p )). Algorithm 2 shows the condition-based algorithm modified from Algorithm 1. We only modify lines 13 to 18.
Example 10 (The condition-based algorithm). We run the condition-based greedy algorithm on the running ex-ample. Table 7 shows the utility score of each condition in each iteration. Once a condition is selected in an iteration (marked by *), the conditions of all satisfied inferior exam-ples are removed.

All inferior examples are satisfied after iteration 3. We obtain a SPS with  X  3 = { a 3  X  3 a 2 ,a 4  X  3 a 2 } and  X  { b 3  X  4 b 1 ,b 3  X  4 b 2 } . | E ( R ) | = 20, which is smaller than that of the SPS obtained by the term-based algorithm.
In the greedy algorithms described before, when a term (or a condition) is selected, the updated preferences may make some superior objects dominated by other objects. We call this term (condition) a violating term (condition) .A violating term (condition) cannot be selected and has to be removed from further consideration.

Example 11 (Violating Condition). Suppose o 3 is indicated as a superior example of objects in Table 4. In iteration 2 in Example 10, if C o 5 ( o 1 ) is selected, a 3
We use superior objects as verifiers. In Algorithms 1 and 2, before we select the best term (condition) (line 18), we check whether it is a violating term (condition). If yes, it is removed.
We conducted extensive experiments to study the effec-tiveness and the efficiency of our two greedy algorithms, using both synthetic data sets and real data sets. All al-gorithms were implemented in C++ and compiled by GCC. We ran experiments on a Pentium 4 2.8GHz PC with 512MB memory running Red Hat Linux operating system.
A synthetic data set consists of d determined attributes and d  X  d undetermined attributes. Each determined at-tribute is numerical. We use the benchmark data genera-tor [2] to generate three types of distributions, anti-correlated , independent ,and correlated . The partial order of an undeter-mined attribute D 0 is generated by combining two indepen-dent numerical attributes D 1 and D 2 ,thatis, D 0 = D 1  X  Then the domain size of D 0 is | D 0 | = | D 1 | X | D 2 | .For objects in a space with 3 determined attributes following in-dependent distribution and 2 undetermined attributes. The domain size of an undetermined attribute is 50.

To choose superior and inferior examples, we first pre-define the preference on every undetermined attribute. Then based on these preferences, we compute all superior objects and inferior objects. Finally, superior and inferior examples are randomly drawn from the set of superior and inferior objects, respectively. The default numbers of superior and inferior examples are both 40. Given a data set, the com-plexity of the minimal SPS and the running time vary a lot with respect to the selected examples, therefore in every experiment, we run our algorithms on 10 sets of randomly generated examples for the specified number of examples, Table 7: A running example of condition-based al-gorithm. and report the average SPS complexity and average running time.

We first compare the term-based greedy algorithm (TG) and the condition-based greedy algorithm (CG) with an ex-haustive algorithm (EX). Then we study the effectiveness and efficiency of TG and CG.
EX enumerates all possible solutions and outputs one SPS with the minimum complexity. The time complexity of EX is O( n m inf p ), where n p is the average number of objects that partially dominate an object and m inf is the number of infe-rior examples. We can only run EX on very small data sets with very few examples. Figure 1 shows the running time of TG, CG, and EX on data sets with 200 objects. The number of inferior examples is varied in the experiments, and it is equal to the number of superior examples. Running time is plotted in logarithmic scale. As expected, the running time of EX increases exponentially with respect to the number of examples, while TG and CG are much faster than EX.
Objects in correlated data sets are much easier to be partially dominated than those in anti-correlated data sets. Hence, objects in correlated data sets have much more con-ditions than those in anti-correlated data sets, resulting in longer running time of EX on correlated data sets than that on anti-correlated data sets.
 ness of our greedy algorithms. complexity EX is the com-plexity of the actual minimal SPS computed by EX, while complexity G is the approximate result obtained by TG or CG. Apparently, ratio  X  1 and the smaller the value of ratio , the better the approximation. Figure 2 indicates that both TG and CG are very accurate ( ratio &lt; 1 . 1) on three types of data sets.
Since EX is too slow, we exclude it in the rest of ex-periments. To evaluate the effectiveness of our greedy al-Here, cardinality = space consisting of all undetermined attributes; while follow-ing Theorem 1, complexity = is the complexity of the obtained SPS  X  =(  X  d +1 ,  X  X  X  , Intuitively, a smaller ratio represents a smaller SPS, hence, a better result.

WeobserveinFigure3that ratio on anti-correlated data sets are larger than that on independent data sets, and even larger than that on correlated data sets. Because in anti-correlated data sets, inferior examples are more likely to be dominated by different objects, thus have different condi-tions and different preference terms. So more terms are se-lected. Moreover, the number of non-trivial examples (i.e., examples that are not trivially dominated) are few in corre-lated data sets, where the SPS complexity is even small.
Figure 3(a) shows that ratio rises when we increase the number of inferior examples from 20 to 100, since more pref-erence terms are selected to satisfy more examples. However, the increment is sub-linear due to that some examples may share the same preference terms. Figure 3(b) shows that ratio is not sensitive to the number of superior examples. Because a superior example is used to eliminate wrong pref-erence terms and its satisfaction does not increase the com-plexity. In Figure 3(c), we vary the domain size of undeter-mined attributes from 25 to 100. The complexity increases linearly.

Figure 3 also indicates that CG finds a SPS with smaller complexity than TG.
Figure 4 shows the running time of TG and CG on three types of data sets with the effec ts of different factors. Gen-erally, both TG and CG run faster on correlated data sets while slower on anti-correlated data sets, because many ex-amples in correlated data are trivially dominated.
Figure 4(a) shows the effect of increasing the size of data sets from 50 , 000 to 200 , 000. TG and CG has similar perfor-mance while CG is slightly faster than TG on anti-correlated data sets. The running time of both algorithms increases linearly.

Figures 4(b) and 4(c) vary the number of determined at-tributes from 2 to 5 and the number of undetermined at-tributes from 1 to 4, respectively. The running time in-creases linearly against the number of determined attributes while exponentially with respect to the number of undeter-mined attributes. When the number of undetermined at-tributes increases, the search space increases exponentially.
In Figures 4(d) and 4(e), we see that the running time rises linearly when the domain size of undetermined attributes and the number of inferior examples increase. Figure 4(f) shows that the number of superior examples has negligible effect on the efficiency of both algorithms.
In conclusion, both two greedy algorithms are effective and practical. The condition-based algorithm is more accu-rate than the term-based algorithm and it is also faster on anti-correlated data sets.
We use the NBA data set (downloaded from www.nba.com ) to evaluate the accuracy of our greedy algorithms. The data set contains the career average technical statistics of 3,924 players from 1946 to 2006. We select 5 attributes from the data set, the average points per game (PTS), the average steals (STL), the average blocks (BLK), the aver-age rebounds (REB), and the average assists (AST). Large values are preferred on all attributes.

In this experiment, we use PTS, STL, and BLK as 3 de-termined attributes, and REB and AST as 2 undetermined attributes. In common sense, the larger values of REB and AST are prefered. The idea is that we hide the actual prefer-ences on REB and AST. Then we use our greedy algorithm to mine the preferences with some inferior and superior ex-amples. We want to evaluate whether the mined prefer-ences are consistent with common sense. This is quantified ence terms and R actual consists of terms in R total which are consistent with common sense. Larger pct indicates higher accuracy.

We convert REB and AST into integers. REB has 23 distinct values while AST has 12. The inferior and superior examples are drawn from pre-computed inferior and superior objects using the actual preferences. Figure 5 shows pct with different sets of inferior and superior examples. The running time is negligible.

In Figure 5(a), we use 15 superior examples. pct increases when the number of inferior examples increases from 5 to 25. Figure 5(b) shows that the result becomes more accu-rate when the number of superior examples increases from 5 to 25. The number of inferior examples is 15. pct is around 90% when we have more than 15 inferior and superior ex-amples. Again, CG has better performance than TG. This experiment shows that in practice we often need a small number of inferior examples to learn user preferences accu-rately.
In this paper, we tackled a novel problem of mining user preferences using superior and inferior examples. We elabo-rated the applications of the problem and modeled the prob-lem systematically. We showed that both the SPS existence problem and the minimal SPS problem are challenging. As the first attempt to tackle the problem, we devised a greedy method. The empirical study using both real data and syn-thetic data indicated that our greedy method is practical.
