 QA is a task concerned with finding answers to natural language (NL) questions (e.g.,  X  X ow tall is the Tokyo Tower? X  and  X  X ho is George Bush? X ) fro m large text collections. To construct a good QA syste m , it is preferable to have an appropriate IR syste m for retrieving docu m ents that have descriptions relevant to providing answers. IR for QA (IR4QA) is a task that evaluates IR m odules fro m the viewpoint of QA syste m construction [1 X 3].
 use partial-m atch IR m odels, such as the probabilistic IR m odel, the language m odel, and the vector-space m odel [1 X 3].
 m ent retrieval in general and QA about particular NEs is that docu m ents that do not contain any infor m ation about the given NEs m ust be irrelevant. Therefore, it is preferable to use a Boolean IR m odel. However, because of variations in the to m ake an appropriate Boolean query at the initial retrieval stage. els for handling this type of proble m . The syste m constructs an appropriate Boolean query based on the co m parison between the initial query and pseudo-relevant docu m ents. It then calculates a penalty for retrieved docu m ents that do not satisfy the Boolean query.
 for Web docu m ents to beco m e suited to the QA task. Experi m ental results show that our approach is better than one that uses a probabilistic IR syste mm odel alone. ABRIR is an IR syste m that has the following co m bination of features fro m the probabilistic and Boolean IR m odels. 1. Refor m ulation of a Boolean query 2. Calculating a score based on the results of the probabilistic and IR m odel 2.1 Refor m ulation of the Boolean Query The following procedure is used to refor m ulate a Boolean query. Figure 1 shows an exa m ple of this process. 1. Selection of Boolean candidate words 2. Refor m ulation of the Boolean query based on the initial query 2.2 Modification of the Score Based on the Boolean Query The probabilistic IR m odel in ABRIR is al m ost equivalent to Okapi BM25 [5] with pseudo-relevance feedback and query expansion, and is i m ple m ented by using the generic engine for transposable association (GETA) tool 1 . calculate the score for each docu m ent: ter m in query Q , and is calculated using Robertson-Sparck Jones weights [5]: where N is the count of all docu m ents in the database, n is the count of all docu m ents containing T , R is the given nu m ber of relevant docu m ents, and r the nu m ber of occurrences of T in a docu m ent and in a query, respectively, and k ,k 3 ,and K are control para m eters.
 rather than 1 when a phrasal ter m is found.
 relevant docu m ent i , respectively.
 fro m the text. 1. Morphological analysis 2. Extraction of index ter m s 3. Extraction of phrasal ter m s between a relevant docu m ent set and a ter m .
 m ay be less appropriate than docu m ents that do satisfy the query, we give a penalty score to docu m ents that do not satisfy the Boolean query. abilistic IR m odel, we used the BM25 weighting for m ula to calculate the score portance of the word in the query. We used a control para m eter  X  to calculate the penalty score: For the OR operator, we used the highest penalty a m ong the OR ter m sasthe overall penalty.
 query ( X  X  X  and ( X  X  X  or  X  X  X )) shown in Figure 1. First, we calculate the penalty score for all individual words ( X  X , X   X  X , X  and  X  X  X ). We assu m e Penalty ( C )  X  receive Penalty ( A ). 3.1 Differences between WWW Docu m ent Retrieval and QA ABRIR, as discussed in the previous section, was developed for WWW docu m ent retrieval. Because the characteristics of docu m ent retrieval in WWW docu m ents para m eters when applying ABRIR to QA.
 1. Use of verbs as index ter m s 2. Handling NEs 3. Nu m ber of relevant docu m ents 4. Nu m ber of query expansion ter m s 3.2 Query Construction Using Synony m s and Variation Lists To m ake a good Boolean query, it is preferable to have an appropriate list of synony m s and variations of Japanese katakana descriptions.
 Dictionary Research (EDR) Institute, Ltd., [8] is used for finding synony m s. In this dictionary, each verb has one or m ore se m antic ids. All verbs that share a se m antic id with the original verb are candidate synony m s.
 erating varieties of description 3 . 1. Re m ove  X   X   X  X ro m the original keyword. 2. Re m ove s m all katakana (e.g.,  X   X  X  X  X  X  X  X  X  X  X  X  X   X ) fro m the original key-three candidates ( X   X  X  X  X  X   X ,  X   X  X  X  X  X   X , and  X   X  X  X  X  X  X   X ) are gener-ated.
 which can be described as follows. 1. Re m ove the question ele m ent fro m the query 2. Morphological analysis and NE tagging 3. Generation of the synony m and variation lists 4. Initial retrieval 5. Construction of a Boolean query 6. Construction of the Boolean query 7. Query expansion using pseudo-relevant docu m ents 8. Final retrieval 4.1 Experi m ental Setup The NTCIR-8 GeoTi m e Japanese m onolingual task data [12] was used to evalu-ate the proposed syste m . There are 24 QA topics about geographic and te m po-ral infor m ation in Japanese for Mainichi newspapers over the period 2002-2005, which co m prises 377,941 docu m ents. Sub m itted results are evaluated based on the view point of docu m ent based relevance judge m ent. They uses the sa m e techniques used for analyzing IR4QA runs [3].
 values are co mm on in WWW retrieval. We used k 1 =1 ,k 3 =7 ,K = dl avdl ,c = 0 . 3 ,and X  =0 . 7 for the probabilistic IR m odel. Here, dl isthelengthofadoc-of all docu m ents.
 ply recalculated the score values to retain the ordering of all docu m ent scores. NE-filter-Verb-penalty (NfVp). Boolean operators on NEs are used for fil-NE-penalty-Verb-penalty (NpVp). Boolean operators only are used for Baseline-Okapi (BO). No Boolean operators are used. Query expansion ter m s 4.2 Discussion of the Experi m ental Results Table 1 shows evaluation m easures for each version of the syste m and Figure shows nor m alized Discounted Cu m ulative Gain (nDCG) of each syste m per topic. NpVp, using descriptions only in the query, is the best-perfor m ed sys-te m for the NTCIR-8 GeoTi m e Japanese m onolingual task.
 18, 21, 22, 23, and 24), the syste m could not m ake a strict Boolean query for as for NpVp. For seven of the topics (Topic:Boolean m atched docu m ents 1:772, 3:1, 6:275, 9:6, 13:105, 19:329, and 20:945), the syste m did m ake an appropriate re m aining three topics (Topic:filter out/total rel 2:26/48, 14:2/2, and 25:1/3), the constructed Boolean queries were too strict, and so m e relevant docu m ents were filtered out. For exa m ple, topic 2,  X   X  X  X  X  X  X  X  X  X  X  X   X  (hurricane Katrina) is recognized as an NE. Therefore, articles with  X   X  X  X  X  X   X (Kat-rina) and without  X   X  X  X  X  X   X  (hurricane) were filtered out. The quality of Boolean query filter for NE is highly depends on the one of NE tagging sys-te m . The topic 14 includes an NE keyword  X   X  X  X  X   X  (Africa). However, the relevant docu m ents have the na m e of the African country  X   X  X  X  X  X  X  X  X  X   X  with the part-whole relationship when generating the related keyword list for the Boolean query. Topic 25 has an NE keyword  X   X  X  X  X  X   X  (off the coast of S m atra) but a relevant docu m ent that has exculded with Boolean filter has  X   X  X  X  X  X  X   X  (off the coast of S m atra island) and does not have  X   X  X  X  X  X   X  NfVp was worse than for NpVp.
 NpVp and BO (base line) to analyze the effectiveness of using a Boolean query. The t test and Wilcoxon Signed Rank test were used to co m pare the AP, the nor m alized Discounted Cu m ulative Gain (nDCG), and the Q m easure (Q). Fro m ferences for nDCG (0.018) and Q (0.040) are statistically significant and that for AP(0.055) is not significant. For the Wilcoxon Signed Rank tests at a signif-icance level of 0.01 for two-sided tests, the AP (0.0015), nDCG (0.0006), and Q (0.0024) results are statistically significant.
 where the results for NpVp were worse than for BO.
 about  X   X  X  X  X  X   X  (hurricane) without  X   X  X  X  X  X   X  (Katrina) get a si m ilar score to  X   X  X  X  X  X   X  (Katrina) without  X   X  X  X  X  X   X  (hurricane).
 cases, it is difficult to assure the quality of the generated query. by our syste m , we found there are m any relevant docu m ents that do not have keywords of each query. It is necessary to have such Boolean query m odification m echanis m for constructing IR4QA syste m . In addition, it is very difficult to construct a perfect Boolean query fro m given query and pseudo-relevant docu (De m ocratic Republic of the Congo) instead of  X   X  X  X  X   X . To deal with such relations, it is necessary to have a good query analyzer and a m echanis m to deal type question. In this paper, we propose to use ABRIR as an IR syste m for QA about particular NEs. Fro m an evaluation experi m ent using the NTCIR-8 GeoTi m e Japanese m onolingual task, we confir m that ABRIR can be used in a syste m that uses appropriate Boolean queries and penalties to outperfor m a baseline syste m (the probabilistic IR m odel, Okapi BM25).
 This research was partially supported by a Grant-in-Aid for Scientific Research (B) 21300029, fro m the Japan Society for the Pro m otion of Science. I would also like to thank organizers of the NTCIR-8 GeoTi m etaskandthereviewerofthe paper for their fruitful contribution.
