 REGULAR PAPER ArjanJ.F.Kok  X  Robert van Liere Abstract The object-oriented visualization Toolkit (VTK) is widely used for scientific visualization. VTK is a visualization library that provides a large number of functions for presenting three-dimensional data. Interaction with the visualized data is controlled with two-dimensional input devices, such as mouse and keyboard. Support for real three-dimensional and multimodal input is non-existent. This paper describes VR-VTK: a multimodal interface to VTK on a virtual environment. Six degree of freedom input devices are used for spatial 3D interaction. They control the 3D widgets that are used to interact with the visual-ized data. Head tracking is used for camera control. Pedals are used for clutching. Speech input is used for application commands and system control. To address several problems specific for spatial 3D interaction, a number of additional fea-tures, such as more complex interaction methods and enhanced depth perception, are discussed. Furthermore, the need for multimodal input to support interaction with the visualization is shown. Two existing VTK applications are ported using VR-VTK to run in a desktop virtual reality system. Informal user experiences are presented.
 Keywords Visualization  X  Virtual environments  X  3D interaction 1 Introduction Throughout the past years, various authors have reported on the benefits of a vir-tual reality (VR) interface to scientific visualization [ 4, 24]. The key argument is that a well-designed VR interface can provide the sensation of presence and stim-ulates participation with objects in a scene. Such a VR interface needs support for 3D interaction with six degree of freedom (6 DOF) trackers and support for system control, for example, by speech. Clearly, this may lead to multimodal in-terfaces in which the user has more control of how the scientific data is presented and explored.
 dard for scientific visualization toolkits [ 18]. It is an open source class library that contains a large number of functions for the presentation of scientific data. VTK uses a pipeline mechanism for rendering. Data is passed through various pipeline objects (e.g. filters and mappers) to obtain geometry that can be displayed by the renderer.
 widgets. Picking is used to select objects in the visualization, while widgets are used to interact with objects in specific ways. A widget has a visual representa-tion within the 3D visualization and defines the behavior that is executed when the widget is manipulated [ 5]. Simple examples of 3D widgets are the point wid-get for probing object information, the box widget for positioning, rotating, and scaling of objects, the spline widget for defining a spline by editing control points, etc.
 and keyboard. An important question is whether a user interface to VTK can be improved when widgets are controlled with different modalities in a virtual envi-ronment. For example, what is the benefit of using 6 DOF input devices to control widgets? In what way can other input modalities, such as speech and gestures be used? In addition to controlling a widget with one specific input modality, it can be beneficial to combine modalities [ 12]: Two or more input modalities can be com-bined to define one action, different input modalities can be used concurrently to execute concurrent actions, and one input modality can be used to enable/disable one of the other modalities.
 used for direct 3D manipulation using widgets and pickers, feet for clutch-ing, head for camera control, and speech for system control. In the next sec-tion, we position our solution against other approaches that have been taken to use VTK in a virtual environment. Section 3 shows how the VTK class hier-archy has been extended. In Sect. 4 we show how the support for direct ma-nipulation has been added. It includes the basic picking and widget manipula-tion, multimodal interaction with widgets, some enhanced interaction features, and ways to allow more accurate 3D interaction by means of better depth per-ception. Section 5 shows how a VR interface can be extended with visualiza-tion and application control. Section 6 gives examples of real applications that have been implemented using our interface. It includes a description of the used hardware platform. Finally, the last section consists of a discussion of our design. 2 Previous work We have chosen three points of view to position our solution against others. First, from an engineering point of view, in which we discuss how other researchers have approached the problem of using VTK in a virtual environment. Second, from a multi-modal interface point of view, in which we discuss how multi-modalities which we discuss various performance issues in 3D interaction. 2.1 VTK in virtual reality Several visualization systems researchers have investigated the integration of VTK with a virtual reality (VR) system.
 of visualization geometry is decoupled from the rendering of this geometry. VTK generates the geometry in the form of actors that consist of geometry and prop-erties. A new class, vtkActorToPF, transforms these actors into pfGeodes that are included in a Performer scenegraph. This scenegraph is rendered independently of VTK. The advantage of this method is the explicit decoupling of rendering from geometry creation, by which the rendering is not interrupted because of complex geometry creation. Only whenever geometry is completely created by VTK, it is transformed to Performer. Disadvantages are the fact that the VTK cameras and lights are not considered in the Performer environment and the fact that there is no support for direct interaction with the VTK pipeline from the rendering. Objects can only be manipulated in the Performer application.
 window [ 20, 27]. The renderer and render window are the objects that are respon-sible for rendering the objects to screen(s). By using derived classes of the ren-derer and render window that include virtual reality system dependent functions, rendering is done onto the VR system. All other functionality of VTK, includ-ing interaction using the mouse, remains available to the user. Advantages of this approach are the rather simple implementation and the fact that all VTK function-ality is available for interaction. All objects in a pipeline can directly be modified as result of a user action.
 tegrate VTK with VR there are two problems to solve:  X  Real-time update of the visualization : This is needed to get the impression  X  Efficient comfortable interaction : We want to directly manipulate the visu-time updates [ 4, 13, 27]. Our focus is on efficient and comfortable direct manipu-lation in 3D and on efficient visualization and application control. 2.2 Multimodal widgets Smalltalk X  X  Model-View-Controller abstraction can be used to model a VTK wid-get [ 6]. Every widget has a view (graphical representation on display), which provides the user with the feedback of the state of the widget. A box widget, for example, is represented as a wireframe of a box augmented with small handles for translation and scaling of the representation. The model of a widget encapsulates the widgets semantics, i.e. which operations are feasible. For example, the seman-tics of a box widget define that an object, controlled by this widget, is rotated when the user moves a plane of the widget, and that the object is scaled when a handle is dragged. The controllers are the devices used to manipulate the widget. A mouse can be used to drag the handles of the box widget representation. by extending the controllers, see Sect. 4.3 . We have explored the usage of 6 DOF devices, pedals and speech as controllers. The user uses two hands to control the widget. The non-dominant hand is used to control the position and orientation of 3D objects in the virtual environment. The dominant hand is used to control the handles of the widget. Having both hands occupied interacting with the data prevents the user from using a keyboard or mouse at the same time. Therefore, other input modalities have been added. Foot pedals are used to simulate button actions, such as button press and release. The 3D widget mechanism also allows us to use a gesture recognition interface as controller to resize or move an object using the box widget.
 usage of multimodal VTK widgets. Multimodal interfaces, however, have been applied for visualization applications before, e.g. [ 11, 12 , 21 ]. They all combine gestures with speech to navigate through the environment or to manipulate the virtual objects. As outlined above, our approach is different. In our multimodal interface, 3D interaction is driven by 3D widgets. These widgets are primarily controlled by direct 3D manipulation, but can also be controlled by other modali-ties. 2.3 Human factors From the user interface community it is known that point location is a basic task in the design and implementation of 3D user interfaces. Point location, the task of positioning the cursor on a point in 3D, is the primitive operation used for tasks such as object selection and object manipulation. Many user studies have been performed to investigate the efficiency of point location subject to different hard-ware conditions (stereoscopic displays, head tracked displays) [ 1, 3]. In addition, user studies have been done to understand the effect that different depth cues have on the user performance of basic 3D tasks. For example, studies have reported on the relative importance of auxiliary 3D depth cues (stereo, lighting conditions, perspective viewing and shadows) on interactive 3D tasks [ 7, 9, 25, 29]. VTK. We have developed the notion of cursor regions in order to aid the user to perform the VTK pick in a three-dimensional environment. In addition, several cues have been introduced to enhance depth perception. These enhancements will be discussed in Sect. 4.
 mance studies on the usage of 3D widgets. 3 VR-VTK architecture Basic assumption in developing a virtual reality version of VTK is that the user must be able to port his existing VTK applications with minimal effort. Technical aspects of VR systems (support for input devices like trackers, pedals, keyboard, speech, support for multi-threading, ... ) should be hidden from the user. layer contains a number of new VR-VTK classes, most of them derived from the existing VTK classes (for modification of functionality), and some of them really new classes (to add new functionality). Note that only classes involved in interaction and rendering have to be added to this layer. The visualization pipeline classes (data objects, filters, mappers, etc.) can be used without any modification. ated with an interactor (vtkRenderWindowInteractor). This interactor captures the windowing system specific events (e.g. mouse and keyboard events) in the render window, translates these system dependent events into system independent VTK events, and dispatches these VTK events. Interactor observers, such as interactor styles (vtkInteractorStyle) and widgets (vtk3DWidget), define the behavior asso-ciated with particular VTK events. They intercept these events and perform the defined action. For example, upon receipt of a mouse move event, the interactor style moves the camera of the renderer. 3.1 Render window and renderer In VR-VTK, for rendering, subclasses of vtkRenderWindow and vtkRenderer have been developed. They control the rendering of the scene for a VR system. a separate thread for rendering. Rendering by this thread is triggered by render events. These events are invoked by the head tracker. Also, the Render method of the render window, which is usually called when the scene is modified is over-ridden. Instead of directly rendering the scene into the render window, it now invokes a render event. A new event handling method of VR-vtkRenderWindow now does the actual rendering of the scene. Furthermore, the VR-vtkRenderer per-forms some extra operations to visualize the scene on the VR system properly. For example, it supports the rendering of shadows (see Sect. 4.5 ) and the rendering of the VR-VTK 2D widget interface (see Sect. 5.2 ). 3.2 Render window interactor Central component in the VR-VTK library is the VR-vtkRenderWindowInteractor. Its main task is to capture and translate events from the VR system into events that can be interpreted by the VTK pipeline, that means into VTK events. At creation of the VR-vtkRenderWindowInteractor several event handlers are created (see also Fig. 2):  X  Two tracker handlers (one tracker for each hand) : These handlers receive  X  A head tracking handler : This handler receives head tracking events from the  X  Three button handlers : These handlers receive events from button devices used  X  A window handler : This handler receives events from the window system  X  A speech handler : This handler receives speech events from an external au- X  A simulation handler : The simulation handler generates VTK timer events of interactions is feasible [ 22]. 3.3 Interactor observers The VR-vtkInteractorStyle defines the  X  X ook and feel X  of the interaction and vi-sualization environment. It is driven by VTK events (for example, those gener-ated by the interactor, such as VTK tracker, button and character events). Its main tasks are controlling the picking process and the enhanced interaction methods (see Sect. 4), controlling features to enhance depth perception (see Sect. 4.5 ), and controlling the visualization (see Sect. 5).
 interactor style, these widgets observe VTK events. In stead of acting on mouse move events, as the standard widgets do, the VR-VTK widgets act upon receipt of TrackerMoveEvents. They use the real 3D position and orientation information stored in the VR-vtkRenderWindowInteractor to perform their action (see also Sect. 4.3 ). 3.4 Platform independence VR-VTK is generally applicable. It is not limited to specific hardware (plat-form, trackers, etc.). All VR-VTK classes are independent of specific hard-ware. Most of them are controlled by VTK events only. Even the VR-vtkRenderWindowInteractor does not depend on the used hardware. This class is implemented using a VR library, currently PVR [ 26]. The VR-library hides all device details from VR-VTK. The VR-vtkRenderWindowInteractor class receives events from this library (instead of from the devices directly), and translates these to VTK-events. It is not much work to modify the VR-vtkRenderWindowInteractor class to use other VR libraries, such as the MR toolkit [ 22]andVRPN[ 23]. These VR libraries allow us to use VR-VTK on other VR platforms, such as the CAVE. New input devices can be added easily, without the need of modifying VR-VTK, as long as the VR library supports these devices. 4 3D interaction Interacting with a 3D visualization will typically include the manipulation of graphical objects. In general, direct manipulation can be decomposed in three tasks: start the interaction by selecting the object, perform the interaction by drag-ging the object and ending the interaction by releasing the object. VR-VTK pro-vides three basic mechanisms that are used to support these tasks: 3D cursor re-gions, 3D picking (both controlled by the VR-vtkInteractionStyle) and 3D wid-gets. 4.1 3D cursor regions VTK uses a 2D cursor as feedback to indicate the position of the 2D input device. The straightforward analog would be to use a 3D cursor for the posi-tion/orientation of the 3D spatial input device. However, to address the difficulties that users have with point location in 3D, VR-VTK uses the notion of a cursor region.
 selection. Objects that coincide with the region are objects that can be selected. Two regions are available: a cylinder and a sphere. Regions can be parameterized with two parameters:  X  An offset : This offset defines the distance from the user X  X  hand to the centre of  X  The size of the region : The size of the region defines the accuracy required for and a cylinder or sphere at the appropriate position for the size of the region, see Fig. 3. 4.2 3D pick The cursor region defines a picking volume: when picking, the object intersected by the cursor region that is closest to the center of this region is picked. is visible in the pixel under the 2D cursor. In 3D it is not always clear which object is picked: the cursor region may intersect several objects, or the selected object is (partly) hidden by other objects. Therefore, a continuous feedback is given by highlighting the selected object, by placing a bounding box around the selected object.
 ally use foot pedals or speech). Upon press, the application can activate a VR-VTK picker object and call the VR-VTK pick function. While the button remains pressed, the selected object is dragged: its position is updated using the position of the cursor region. Upon button release, the interaction stops. 4.3 3D widgets VTK provides a set of 3D widgets to allow many types of interaction and control of many shapes (points, lines, planes, volumes), e.g. vtkPointWidget, vtkLineWid-get, vtkBoxWidget, etc. [ 19]. These widgets in VTK are controlled by 2D mouse movements and mouse button events. The 3D widget receives VTK events invoked by the interactor, and takes appropriate action. lating 3D widgets. To interact with an object, a widget has to be linked to it. The type of widget determines the semantics of the action, i.e. which operations on the object are feasible. Each widget has some geometry; for example, spherical handles to define size and shape of the widget, and arrows to define orientation. Manipulation of the widget is done by interacting with this geometry.
 behavior of an object controlled by the widget. Operation of the widgets is con-trolled by events from the VR-vtkRenderWindowInteractor and the position and orientation of the cursor region. This information defines the modification of the geometry of the widget, which in turn defines the response of the object. vices) and different levels of control:  X  By 3D manipulation of the geometry of the widget: The 3D tracker controls  X  By speech in combination with tracking to control the geometry of the widget:  X  By speech to control the whole widget: A speech command that specifies a handles at the corners of the plane can be used to enlarge/reduce the size of the plane, the arrow handle to modify the orientation of the plane while the plane itself can be used to move the plane.
 plex widgets can be built from the basic widgets. VR-VTK contains, for example, a convex-hull widget (to define convex volumes) built from several point widgets. widgets. To avoid the scene to become cluttered with manipulable object repre-senting these widgets, VTK allows for selective enabling/disabling widgets (by pressing a key). A disabled widget disappears from the environment. In VR-VTK enabling/disabling widgets can also be done by speech commands. 4.4 Enhanced 3D interaction There are two ways to enhance 3D interaction for visualization:  X  World in hand : In scientific visualization it is often useful to position and ori- X  Two-handed input : Two-handed input can be beneficial for interaction tasks. These enhanced interaction options can be enabled by speech commands. 4.5 Enhanced depth perception One of the major problems with interaction in three dimensions is the lack of depth perception, even if stereoscopic rendering is used. It is often very hard to determine which objects are closer to the viewer than others. That makes it very hard to select an object: are we reaching far enough into the environment to select the object, or not?  X  Motion parallax : Motion parallax is a powerful depth cue. Moving the head  X  The stage : The stage consists of a checkerboard and two coordinate axes.  X  Shadows : A virtual light source is placed above the virtual world. Using this 5 Application control Besides direct 3D object manipulation, VTK applications need support for non-spatial input, e.g. for control tasks. Examples of these tasks in our VR-VTK en-vironment are: enabling all kind of options such as stereo, shadows, the world-in-hand and the stage, enabling widgets for 3D interaction (see Sect. 4.3 ), enabling the pipeline browser (see Sect. 5.2.2 ), zoom in and out, and setting properties for the cursor region. Examples of these tasks in a user application might be: enabling visualization methods, setting parameters, setting properties for objects, and mod-ifying color tables.
 using a 2D user interface (UI) toolkit, such as Tcl/Tk or Java Swing, operated by mouse and keyboard. However, for our 3D interface this is not the most suitable solution. Both hands are used for steering trackers, so we have to drop one of the trackers to be able to use the mouse or keyboard. Moreover, we have to move our attention from the 3D environment to the 2D GUI or the keyboard. Therefore, other solutions are required. 5.1 Speech input Speech has shown to be a useful modality for system control [ 2, 17]. Commands can be issued in parallel with spatial interaction while user X  X  attention can stay focused on the 3D environment. sic visualization control. It includes commands to control picking, setting the en-hanced 3D interaction methods, setting the enhanced depth cues and zooming. Furthermore, it contains commands to control the rendering process, such as en-abling stereo and making screen dumps. Important is the command to transfer interaction from interaction with the data to interaction with the UI-toolkit (see Sect. 5.2 )viceversa.
 cation. As the speech handler converts speech events into character events, the user only has to specify the mapping from speech command to a character. The application itself should listen to character events. 5.2 2D widgets in a 3D world Since many people have experience with 2D widget interfaces on the desktop, it is an obvious choice to use them in the virtual world as well. Mine et al. [ 14 ] discussed how these widget systems should be used integrated in a virtual envi-ronment, and concluded that hand-held widgets performed best. 5.2.1 VR-VTK UI Therefore, the VR-VTK UI toolkit consists of a virtual cube containing 2D wid-gets, and a virtual pen as selection device to manipulate the widgets. Each side of the virtual cube can be equipped with all kind of widgets, such as buttons, pop-up menus, sliders, labels, message boxes, etc. Having six sides on a cube makes it possible to arrange widgets in groups. Desktop applications often use a main menu and several windows. In our system the main menu and each window can be placed on another side of the cube.
 graspable input devices [ 10]. Instead of using the selection device, in future, it will also be possible to manipulate the 2D widgets by speech commands.
 switches from visualization interaction to UI manipulation. The widget interface is shown over the visualization. The user can now change some control parameters on the cube by manipulating the 2D widgets. Results of these changes are directly visible in the data visualization. When the specific button or speech command is issued again, the widget interface disappears and direct 3D interaction with the data visualization can proceed. 5.2.2 VTK pipeline browser The VR-VTK pipeline browser is a special GUI built with the VR-VTK UI toolkit. It enables the user to inspect and modify all parameters of all objects in the VTK pipeline. The pipeline browser has the same functionality as the tcl/tk pipeline browser [ 16]. However, our browser is rendered and controlled inside the same 3D space as the VTK visualization. pipeline shows a connected graph on one or more faces of the cube (the pipeline in-terface). It contains one widget (checkbutton) for each object (data, filter, mapper, actor, light, etc.) in the visualization pipeline. The other view shows the proper-ties/methods of VTK objects (the object interface). Users can use the pen to select one or more objects in the pipeline interface. A user interface for each selected object will be rendered on a face of the cube. The object X  X  set/get methods are automatically shown as sliders, (check) buttons or menus. Object properties can be modified by manipulating these widgets.
 pipeline can consist of many VTK objects, and each pipeline object can have many manipulable properties, the user can choose for a subset of these for display. 6 Examples We ported several existing VTK applications to run in our virtual reality system, the Personal Space Station (PSS), by applying our new multimodal VR interface. 6.1 The Personal Space Station The Personal Space Station (PSS) is a near-field, mirror-based desktop VR/AR environment [ 15], see Figs. 8 and 9. A head-tracked user is seated in front of a mirror, which reflects the stereoscopic images of the virtual world as displayed by the monitor. The user perceives the stereoscopic images of the display as a virtual world located in front of him. The user can reach under the mirror into the virtual world to interact with the virtual objects.
 graspable input devices (props). The interaction space is monitored by two or more cameras. Each camera is equipped with an infra-red (IR) pass filter in front of the lens and a ring of IR LEDs around the lens. Patterns of retroreflective markers are applied to the tracked input devices. The optical tracking system uses these marker patterns to identify and reconstruct the poses of the devices. In this way, graspable, wireless input devices can easily be constructed. equivalent to standard mouse buttons. A microphone is used for the speech recog-nition system. Furthermore, standard desktop input devices, such as a keyboard and mouse, can be used.
 directly manipulate VTK pickers and VTK widgets.
 ments, such as the CAVE. 6.2 Diffusion tensor imaging MR diffusion tensor imaging (DTI) measures the diffusion of water molecules in tissue. The diffusion anisotropy is an indicator of the underlying structure of the tissue, for example, of the fibers in the brain. The structure is fairly complex. It is difficult to understand and to get insight in the structure when, for example, the number of fibers is reasonably large. Therefore the use of 3D visualization and 3D interaction can help on getting a better understanding of such complex data. A visualization tool has been developed, which allows the visualization of the 3D structure that can be obtained from the diffusion tensors [ 28]. This tool is used for brain and muscle studies.
 teractive visualization is provided by:  X  Orthogonal slice planes : The slice planes visualize the anisotropy by mapping  X  Glyphing : The diffusion tensors are visualized by ellipsoid or cuboid glyphs.  X  Fiber tracking : The diffusion tensor data can be visualized by streamlines,  X  World in hand : The user can hold the data set (brains) in his hand and move  X  Speech input : Speech is used to define the interaction and visualization by  X  VTK pipeline browser : Several visualization parameters can be modified using trolling the visualization by speech and the pipeline browser will constantly update the visualization. 6.3 Bouncing balls The bouncing balls application simulates the behavior of moving balls inside a bounding box. Several parameters control the simulation: the number of balls, the radius of the balls, the size of the box, the damping factor of the medium in which the balls move, an attraction force among the balls, the field force that is applied on the balls, the position of each ball, etc. Each step of the simulation computes new positions for the balls and new forces of attraction among the balls. Balls are represented as spheres and small arrows which indicate the direction of the ball movements. The force field around a ball is represented with an isosurface. provided by:  X  Picking and moving balls : Balls can interactively be selected and moved to a  X  Application of a field force : The white arrow left to the data visualization is  X  Color mapping of the forces in the data set on a slice plane : A plane widget  X  Definition of the bounding box : The bounding box is defined by a box wid- X  Speech input : Speech is used to enable/disable the widgets.  X  VTK pipeline browser and VR-VTK UI : Numeric values for the simulation, widgets and/or changing the simulation parameters by speech or the widget cube. The visualization can be controlled using the pipeline browser. 7 Discussion We presented a new multimodal virtual reality interface to VTK. Main contribu-tions are the addition of real-3D direct manipulation with the visualization and the integration of visualization and system control by different modalities. visualized world in three dimensions. Our 3D cursor regions enable easy selection of objects, and overcome the problems of point selection in 3D. Manipulation of the visualization is provided by the use of 3D widgets. These widgets can be controlled by different modalities: from direct manipulation in 3D using 6 DOF trackers to speech only control.
 added: a stage, shadows, and two-handed manipulation. Application and visual-ization control is provided by the use of speech commands and a 2D widget system with a pipeline browser that is integrated in the virtual world. The pipeline browser offers the user an interface to the VTK objects in the visualization pipeline. We believe that these features are very useful for all interactive visualization systems. VTK classes in the application by their VR-VTK versions. The next step is to replace the existing 3D interaction by 3D-widget interaction (as most existing ap-plications do not use 3D-widgets). Furthermore, the interaction style (e.g., cursor region parameters, use of stage, shadows) must be specified. The result is a 3D interactive application, including basic system and visualization control provided by speech input and 2D widgets. To have the application controls, usually pro-vided by a 2D GUI, available in the 3D environment, these extra controls can be implemented as speech commands or as 2D widgets on the VR-VTK UI.
 Porting was easy. It required only slight modifications to the existing VTK code. the new VR interface.
 them to interactively define a region of interest with seed points for the generation of streamlines. Goal is to get a good insight into the data. This is one of the basic tasks in the DTI tool. The choice of seed points is very important: If too many points are chosen, the scene gets cluttered with streamlines, and if the number of seed points is too small, important features of the data can be missed.  X  Direct 3D interaction with the application is much easier than interaction in  X  The world-in-hand option is an essential interaction method to be able to  X  Speech is very useful for fast switching between the different interaction (by  X  Speech needs more feedback. The only feedback a user gets from a speech ments can be made. The use of a parallel version of VTK will make rendering faster, and will therefore improve the exploration abilities of VTK. More com-plex widgets (although not necessary in our current applications), for example, real two-handed widgets, will directly support more complex interaction tasks. The use of speech to control the 3D widgets, as presented in Sect. 4.3 , should be further elaborated and tested.
 References Author Biographies
