 In the traditional form ulation of sup ervised learning, data instances are view ed as vectors of features in some high-dimensional space. However, in man y real-w orld tasks, data instances have a complex pattern of missing features. While features may sometimes be missing due to measuremen t noise or corruption, di eren t samples often have di eren t sets of observ able features due to inher ent prop erties of the instances. For example, in the case of recognizing objects in natural images, an object is often classi ed using a set of image patc hes corre-sponding to parts of the object (lik e the license plate for cars); but some images may not con tain all parts, either because a part was not captured in the image or because the speci c instance does not have this part in the rst place.
 In other scenarios, some features cannot even be de ned for all instances. Suc h situations arise when the objects to be learned are organized based on a kno wn graph structure, since their features may rely on local prop erties of the graph. For example, we migh t wish to classify the attributes of a web-page given the attributes of neigh boring web-pages [8]. In analyzing genomic data, we may wish to predict the edges in net works of interacting proteins or chemical reactions [9, 15]. In these cases, the local neigh borho od of an instance in the graph often varies drastically , and it has already been observ ed that variation this could introduce statistical biases [8]. In the web-page task, for instance, a useful feature is the most common topic of other sites that point to a given page. When a page has no suc h paren ts, however, this feature is meaningless and should be considered structur ally absent . The common approac h for classi cation with missing features is imputation , a two phase pro cedure where the values of the missing attributes are rst lled in during a prepro cessing phase, after whic h a standard classi er is applied to the completed data [10]. Most Impu-tation techniques mak e most sense when the features are missing due to noise, esp ecially in the setting of missing at random (MAR, when the missingness pattern is conditionally indep enden t of the unobserv ed features given the observ ations), or missing completely at random (MCAR, when it is indep enden t of both observ ed and unobserv ed measuremen ts). In common practice of applying imputation, missing attributes in con tinuous data are often lled with zeros, or with the average of all of the data instances, or using the k nearest neighb ors (kNN) of eac h instance to nd a plausible value of its missing features. A second family of imputation metho ds builds probabilistic generativ e mo dels of the features using raw maxim um likeliho od or algorithms suc h as exp ectation maximization (EM) [4]. Suc h mo del-based metho ds allo w the designer to introduce prior kno wledge and are extremely useful when priors can be explicitly mo deled. These metho ds work very well for MAR data settings, because they assume that the missing features are generated by the same mo del that generates the observ ed features. However, mo del-based approac hes can be computationally exp ensiv e, and require signi can t prior kno wledge about the data. More imp ortan tly, they will pro duce meaningless completions for features that are structur ally absent . As an extreme example, consider two subp opulation of instances (e.g., animals and buildings) having no overlapping features (e.g., body parts, and architectural asp ects), in whic h lling missing values (e.g., the body parts of buildings) is clearly meaningless and may harm classi cation performance. As a result, for structurally absen t features, it would be useful if we could avoid unnecessary prediction of hypothetical unde ned values, and classify instances directly . We approac h this problem directly from the geometric interpretation of the classi cation task as nding a separating hyperplane in the feature space. We view instances with dif-feren t feature sets as lying in subspaces of the full feature space, and suggest a mo di ed optimization objectiv e within the framew ork of supp ort vector mac hines (SVMs), that ex-plicitly considers the subspace of eac h instance. We sho w how the linearly separable case can be ecien tly solv ed using con vex optimization ( second order cone programming , SOCP). The objectiv e of the non separable case is non-con vex, and we prop ose an iterativ e pro ce-dure that is found to con verge in practice. These approac hes may be view ed as mo del-free metho ds for handling missing data in the cases where the MAR assumption fails to hold. We evaluate the performance of our approac h in two real world applications: prediction of missing enzymes in a metab olic net work, and automobile detection in natural images. In both tasks, features may be inheren tly absen t due to the mec hanisms describ ed above, and our metho ds are found sup erior to other simple imputation metho ds. Let x 1 : : : x n be a set of samples with binary lab els y i 2 f 1 ; 1 g . Eac h sample x i is char-acterized by a subset of features F ( x i ), from a full set F of size d . A sample that has all features F ( x i ) = F , is view ed as a vector in R d , where the i th coordinate corresp onds to the i th feature. A sample x i with partially valid features can be view ed as embedded in the relev ant subspace R jF ( x i ) j R d . For simplicit y of notation, we treat eac h x i as if it were a vector in R d where only its F ( x i ) entries are valid and de ne the inner pro duct with another vector in R d as wx = P k : f learned classi er must be consisten t across instances, assigning the same weigh t to a given feature in di eren t samples, even if those instance do not lie in the same subspace. In the classical SVM approac h [14, 13], a linear classi er w is optimized to maximize the mar-gin, de ned as min i y i ( wx i + b ) = k w k , and the learning problem is reduced to the quadratic constrained optimization problem where b is a threshold, the 's are slac k variables necessary for the case when the training instances are not linearly separable, and C is the error penalt y. Eq. (1) can be extended to nonlinear classi ers using kernels [13]. Consider now learning suc h a classi er in the presence of missing data. At rst glance, it may app ear that since the x 's only a ect the optimization through inner pro ducts with w , missing features can merely be skipp ed (or equiv alen tly, replaced with zeros), thereb y preserving the values of the inner pro duct. However, this does not prop erly normalize the di eren t entries in w , and damages classi cation accuracy . The reason is illustrated in Fig. 1 where a single sample in R 2 has one valid and one missing feature. Due to the missing feature, measuring the margin in the full space 2 , underestimates the correct geometric margin of the sample in the valid space 1 . This is di eren t from the case where the feature exists but is unkno wn, in whic h the sample's margin could be either over-or under-estimated. In the next sections, we explore how this Eq. (1) can be solv ed while prop erly taking this normalization into accoun t. We start by reminding the reader about the geometric interpretation of SVM. The deriv ation of the SVM classi er [14] is motiv ated by the goal of nding a hyperplane that maximally separates the positiv e examples from the negativ e, as measured by the geometric is transformed into the quadratic programming problem of Eq. (1) in two steps. First, k w k , is tak en out of the minimization, yielding max w 1 k invariance is used: for every solution, there exists a solution that achiev es the same target function value, but with a margin that equals 1. This allo ws us to write the SVM problem as a constrained optimization problem: max w k w k 1 s.t. y i ( wx i ) 1. This is equiv alen t to minimizing k w k 2 with the same constrain ts, whic h equals the SVM problem of Eq. (1). In the case of missing features, this deriv ation no longer optimizes the correct geometrical margin (Fig. 1). To address this problem, we treat the margin of eac h instance in its own subspace, by de ning the instanc e mar gin for the i th instance as i ( w ) = y i w ( i ) x i instance margins, yielding a new optimization problem Unfortunately , since di eren t margin terms are normalized by di eren t norms k w ( i ) k , we can no longer tak e the denominator out of the minimization as above. In addition, eac h ecien t way. We now discuss two approac hes for solving this problem.
 In the linearly separable case, the optimization problem of Eq. (3) is equiv alen t to This is a con vex feasibilit y problem for any xed value of , whic h is a real scalar that corresp onds to the margin. It can be solv ed ecien tly using a bisection searc h over 2 R + , where in eac h iteration we solv e a con vex second order cone program (SOCP) [11]. Unfor-tunately , extending this form ulation to the non-separable while preserving the geometric margin interpretation case mak es the problem non-con vex (this is discussed elsewhere). A second approac h for solving Eq. (3) is to treat eac h instance margin individually . We represen t eac h of the norms k w ( i ) k as a scaling of the full norm by de ning scaling coecien ts s = k w ( i ) k = k w k , and rewriting Eq. (3) as The s i factors are scalars, and had we kno wn them, we could have solv ed a standard SVM problem. Unfortunately they dep end on w ( i ) and are unkno wn. This formalism allo ws us to use again the invariance to the rescaling of k w k and rewrite as a constrained optimization problem over s i and w . In the non-separable case, Eq. (5) becomes This constrained optimization problem is no longer a QP . In fact, due to the normalization constrain t it is not even con vex in w . One solution is a projected gradient approac h, in whic h one iterates between steps in the direction of the gradien t of the Lagrangian and pro jections suc h approac hes are guaran teed to con verge to local minima [2].
 We can use a faster iterativ e algorithm based on the fact that the problem is a QP for any given set of s i 's, and iterate between (1) solv e a QP for w given s i , and (2) use the resulting w to calculate new s i 's. This algorithm di ers from a pro jected gradien t approac h in that rather than taking a series of small gradien t steps, it tak es bigger leaps, and pro jects bac k to the constrained space after eac h step. Since the con vergence of this iterativ e algorithm is not guaran teed, we used cross validation to choose an early stopping point and found that the best solutions were obtained within 2-5 steps. Typically , the objectiv e impro ved on the rst 1-3 iterations, but then, in about 75% of the cases the objectiv e oscillated. In the remaining cases the algorithm con verged to a xed point. It is easy to see that a xed point of this iterativ e pro cedure achiev es an optimal solution for Eq. (6), since it achiev es a minimal k w k while obeying the s i constrain ts. As a result, when this algorithm con verges, the solution is also guaran teed to be a locally optimal solution of the original problem Eq. (3). The power of the SVM approac h can be largely attributed to the exibilit y and eciency of nonlinear classi cation allo wed through the use of kernels. The dual of the above QP can be kernelized as in a standard SVM, yielding where K ( x i ; x j ) is the kernel function that sim ulates an inner pro duct in the higher dimen-sional feature space. Classi cation of new samples are obtained as in standard SVM by calculating the margin ( x new ) = P j y j j 1 s Kernels in this form ulation operate over vectors with missing features, hence we have to dev elop kernels that handle them correctly . Fortunately , man y kernels only dep end on their inputs through their inner pro duct. In this case there is an easy pro cedure to construct a mo di ed kernel that tak es suc h missing values into accoun t. For example, for a polynomial the inner pro duct calculated over valid features h x i ; x j i F = P k : f This can be easily pro ved to be a kernel. (a) (b) (c) We evaluated our approac hes in three di eren t missingness scenarios. First, as a sanit y chec k, we explored performance when features are missing at random, in a series of ve standard UCI benc hmarks, and also in a large digit recognition task using MNIST data. In this setting our metho ds performed equally well as other approac hes (or sligh tly better). The full details of these exp erimen ts are pro vided in a longer version of this work. Second, we study a visual object recognition application where some features are missing because they cannot be located in the image. Finally , we apply our metho ds to a problem of biological net work completion, where missingness patterns of features is determined by the kno wn structure of the net work.
 For all applications, we compare our iterativ e algorithm with ve common approac hes for completing missing features. 1. Zero: Missing values were set to zero. 2. Mean: Missing values were set to the average feature values 3. Aggregated Flags: Features were anno-tated with an explicit additional feature noting whether a feature is valid or missing. To reduce the num ber of added features, we added a single ag for eac h group of features that were valid or invalid together across all instances. For example, In the vision application, all features of a landmark candidate are group ed together since they are all invalid if the matc h is wrong (see below). 4. kNN: Missing features were set with the mean value obtained from the K nearest neigh bors instances; neigh borho od was measured using a Euclidean distance in the subspace relev ant to eac h two samples, num ber of neigh bors was varied as K = 3 ; 5 ; 10 ; 20, and the best result is the one rep orted. 5. EM: Generativ e mo del in the spirit of [4]. A Gaussian mixture mo del is learned by iterating between (1) learning a GMM mo del of the lled data (2) re-lling missing values using clusters means, weigh ted by the posterior probabilit y that a cluster generated the sample. Covariances were assumed spherical. The num ber of clusters was varied as K = 3 ; 5 ; 10 ; 15 ; 20, and the best result is the one rep orted. 6. Geometric margin: Our non-separable approac h describ ed in Sec. 3. In all of the exp erimen ts, we used a 5-fold cross validation pro cedure and evaluated perfor-mance using a testing set that was not used during training. In addition, 20% of the training set was used for choosing optimization parameters, suc h as the kernel type, its parameters, and an early stopping point for the iterativ e algorithm. 4.1 Visual object recognition We now consider a visual object recognition task where instances have structurally missing features. In this task we attempt to determine if an object from a certain class (automobiles) is presen t in a given input image. The task of classifying images based on the object class that they con tain has seen much work in recen t years [1, 5],and discriminativ e approac hes have typically pro duced very good results [5, 12].
 Features in these metho ds are commonly constructed from regions of inter est (patc hes) in the image. These patc hes typically cover \landmarks" of the object, like the trunk or a headligh t for a car. A typical set of patc hes includes sev eral candidates for any object part, and some images may have more candidates for a given part than others. For example, a trunk of a car may not be found in a picture of a hatc h-bac k car, hence all its corresp onding features are considered to be structurally missing from that image. Our object mo del con tains a set of \landmarks", for whic h we nd sev eral matc hes in a given image (details are omitted due to lack of space). Fig. 2 sho ws examples of matc hes for the fron t windshield landmark. Because of the noisy matc hes, the highest scoring matc h often does not matc h the true landmark, and the num ber of high-qualit y matc hes (features) varies in practice. It is in precisely suc h a scenario that we exp ect our prop osed algorithm to be e ectiv e. In some cases, landmark mo dels could pro vide con dence levels for eac h matc h. These could in principle be used as additional features to help the classi ers give more weigh t to better matc hes, and are exp ected to impro ve classi cation when the con dence measure is reliable. While this is a poten tially useful approac h for the curren t application, this pap er tak es a di eren t approac h: it does not use any soft con dence values but rather treats the low-con dence matc hes as wrong, remo ving them from the data.
 Concretely , we located up to 10 candidate patc hes (21 21 pixels) that were promising (lik eliho od above a given threshold) for eac h of the 19 landmarks in the car mo del. For eac h candidate, we compute the rst 10 principal comp onen t coecien ts of the image patc h and concatenate these patc hes to form the image feature vector. If the num ber of patc hes for a given landmark is less than ten, we consider the rest to be structurally absen t. We evaluated performance for this task using two levels of a 5-fold cross validation pro cedure as explained above. We compared sev eral kernels and rep ort results using the kernel that fared best on the validation set, whic h was usually a second order polynomial kernel. Fig. 2c compares the accuracy of the di eren t metho ds. We found the geometric approac h to be signi can tly sup erior to all other metho ds. To further evaluate our metho d, we qual-itativ ely examined the classi cation results for sev eral images across the various metho ds. Fig. 2a sho ws the top 10 matc hes for the fron t windshield landmark for a represen tativ e \easy" test instance where all local features are appro ximately in agreemen t. This instance was correctly classi ed by all metho ds. In con trast, Fig. 2b sho ws a represen tativ e \hard" test instance where local features cluster into two di eren t groups. In this case, the cluster of bad matc hes was automatically excluded yielding missing features, and our geometric approac h was the only metho d able to classify the instance correctly . 4.2 Metab olic pathway reconstruction As a nal application, we consider the problem of predicting missing enzymes in metab olic path ways, a long-standing and imp ortan t challenge in computational biology [15, 9]. In-stances in this task have missing features due to the structure of the bio chemical net work. Cells use a complex net work of chemical reactions to pro duce their chemical building blo cks (Fig. 3). Eac h reaction transforms a set of molecular comp ounds (called substr ates ) into another set of molecules ( products ), and requires the presence of an enzyme to catalyze the reaction. It is often unkno wn whic h enzyme catalyzes a given reaction, and it is desirable to predict the iden tity of suc h missing enzymes computationally .
 Our approac h for predicting missing enzymes is based on the observ ation that enzymes in local net work neigh borho ods usually participate in related functions. As a result, neigh bor-ing enzyme pairs have non trivial correlations over their features that re ect their functional relations. Imp ortan tly, di eren t types of neigh borho od relations between enzyme pairs lead to di eren t relations of their prop erties. For example, an enzyme in a linear chain dep ends on the preceding enzyme pro duct as its substrate. Hence it is exp ected that the corresp ond-ing genes are co-expressed [9, 15]. On the other hand, enzymes in forking motifs (same substrate, di eren t pro ducts) often have anti-correlated expression pro les [7]. To preserv e the distinction between di eren t neigh bor relations, we de ned a set of net work motifs, including forks , funnels and line ar chains . Eac h enzyme is represen ted as a vector of features that measure its relatedness to eac h of its neigh bors. A feature vector has structurally missing entries if the enzyme does not have all types of neigh bors. For example, the enzyme PHA2 in Fig. 3 does not have a neigh bor of type fork , and therefore all features assigned to suc h a neigh bor are absen t in the represen tation of the reaction \Pr ephanate ! Phenylpyruvate" . We used the metab olic net work of S. cerevisiae , as reconstructed by Palsson and colleagues [3], after remo ving 14 metab olic currencies and reactions with unkno wn enzymes, leaving 1265 directed reactions. We used three data types: (1) A comp endium of 645 gene expression exp erimen ts; eac h exp erimen tal condition k con tributed one feature, the point-wise Pearson domain con ten t of eac h enzyme as found by the Prosite database. Eac h domain k con tributed x cellular localization con tributed one feature, the point-wise Hamming distance. In total, the feature vector length was about 3900. Path way reconstruction requires that we rank candidate enzymes by their poten tial to matc h a reaction. As a rst step towards this goal, we train a binary classi er, to predict if an enzyme ts its neigh borho od. We created a set of positiv e examples from the reactions with kno wn enzymes ( 520 reactions), and also created negativ e examples by plugging imp ostor genes into `wrong' neigh borho ods. We trained an SVM classi er using a 5-fold cross validation pro cedure as describ ed above. Figure 3 sho ws the classi cation error of the di eren t metho ds in the gene lling task. The geometric mar gin appr oach achiev es signi can tly better performance in this task. kNN achiev ed very poor performance compared to all other metho ds. One reason could be that the Euclidean distance is inappropriate for the curren t task and that a more elab orate distance measure needs to dev elop ed for this type of data. Learning metrics is a complicated task in general, and more so in the curren t problem since the feature vectors con tain entries of sev eral di eren t types, making it unlik ely that a naiv e distance measure would work well. Finally , the resulting classi er is used for predicting missing enzymes, by ranking all can-didate enzymes according to their matc h to a given neigh borho od. Evaluating the qualit y of ranking on kno wn enzymes (cross validation), sho ws that it signi can tly outp erforms previous approac hes [9] (not sho wn here due to space limitations). We attribute this to the abilit y of the curren t approac h to preserv e di eren t types of net work-neigh bors as separate features in spite of creating missing values. We presen ted a novel metho d for max-margin training of classi ers in the presence of missing features, where the pattern of missing features is an inheren t part of the domain. Instead of completing missing features as a prepro cessing phase, we dev elop ed a max-margin learning objectiv e based on a geometric interpretation of the margin when di eren t instances essen-tially lie in di eren t spaces. Using two challenging real life problems we sho wed that our metho d is signi can tly sup erior when the pattern of missing features has structure. The standard treatmen t of missing features is based on the concept that missing features exist but are unobserv ed. This assumption underlies the approac h of completing features before the data is used in classi cation. This pap er focuses on a di eren t scenario, in whic h features are inheren tly absen t. In suc h cases, it is not clear why we should guess hypothetical values for unde ned features, since the completed values are lled based on other observ ed values, and do not add information to our classi ers. In fact, by completing features that are not supp osed to be part of an instance, we may be confusing the learning algorithm by presen ting it with problem whic h may be harder than the one we actually need to solv e. Interestingly , the problem of classifying with missing features is related to another problem, where individual reliabilit y measures are available for features at eac h instance separately . This is a common case in analysis scien ti c measuremen ts, where the reliabilit y of eac h exp erimen t could be pro vided separately . For example, DNA micro-arra y exp erimen ts have inheren t measures of exp erimen tal noise levels, and biological variabilit y is often estimated using replicates. This problem can be view ed in the same framew ork describ ed in this pap er: the geometric margin must be de ned separately for eac h instance since the di eren t noise levels distort the relativ e scale of eac h coordinate of eac h instance separately . Relativ e to this setting, the completely missing and fully valid features discussed in this pap er are extreme points on the spectrum of reliabilit y. It will be interesting to see whic h asp ects of the geometric form ulation discussed in this pap er can be extended to this new problem.
