 Abstract In this work we suggest a novel Text Categorization (TC) scenario, motivated by an ad-hoc industrial need to assign documents to a set of predefined categories, while labeled training data for the categories is not available. The sce-nario is applicable in many industrial settings and is interesting from the academic perspective. We present a new dataset geared for the main characteristics of the scenario, and utilize it to investigate the name-based TC approach, which uses the category names as its only input and does not require training data. We evaluate and analyze the performance of state-of-the-art methods for this dataset to identify the shortcomings of these methods for our scenario, and suggest ways for overcoming these shortcomings. We utilize statistical correlation measured over a target corpus for improving the state-of-the-art, and offer a different classification scheme based on the characteristics of the setting. We evaluate our improvements and adaptations and show superior performance of our suggested method.
 Keywords Natural language processing Name-based Text Categorization Semantic similarity 1 Introduction Topical Text Categorization (TC), also known as text classification , is the task of automatically classifying documents into a set of predefined categories (classes, topics). TC has become one of the key techniques for handling and organizing text data, and is used in a variety of scenarios, such as classifying news stories, finding interesting information on the web and guiding user search through hypertext browsing.

In the research community the dominant approach to this task is based on supervised machine learning (Sebastiani 2002 ), utilizing a large number of labeled training documents. This approach complies with some industrial settings, e.g. with a typical scenario of large information providers, such as Reuters, where significant amounts of labeled data are created as part of the work flow. Indeed, such data has underlain the development and establishment of supervised TC. However, there is another typical setting, where TC is involved to support an ad-hoc industrial information need and labeled data is not easily available.

Our work was triggered by collaboration with partners from the movie recommendation industry, whose scenario represents this second typical setting. They aimed at classification of movie descriptions to enhance a video recommen-dation system with topical knowledge. Yet, manually annotating training data for of topics in order to express the information need. With the rapid growth of online information this setting is becoming more and more frequent and one can see many domains where this TC setting is prevailing, e.g. categorization of product reviews to maintain a review-crawling website (Hu and Liu 2004 ; Ruhl et al. 2006 ; Archak et al. 2011 ), categorization of social media content for analyzing opinions on different social and political issues (Tumasjan et al. 2010 ; Ritter et al. 2012 ;Li et al. 2012 ; Maynard and Funk 2012 ; Liu and Zhang 2012 ), categorization of requests towards a tech support center to simplify the treatment of standard issues (Eichler et al. 2012 , 2014 ), as well as TC in the legal domain, education, medicine, 2013 ; Saggion and Funk 2009 ; Scharl and Weichselbraun 2008 ).

In all these scenarios an approach which does not require manual data annotation would be extremely appealing, since the categories of interest are numerous and often dynamic, while modest performance (relative to supervised methods) is still useful. Motivated by this fact, there has been a line of work on keyword-based TC, aiming at a bootstrapping approach, where training data is generated automatically from unlabeled documents. For this purpose, first each category is represented by a handcrafted list of characteristic keywords, which should capture the category meaning. Then measuring similarity between documents and the category keywords is used to produce labeled training data. The manual effort is thus reduced to providing a keyword list per category (Mccallum and Nigam 1999 ). This step was even partly automated in Ko and Seo ( 2004 ) and Liu et al. ( 2004 ), by using clustering to generate candidate keywords to assist a lexicographer or a domain expert. Nevertheless, the method still requires manual specification as part of the classification process. Further research (Gliozzo et al. 2005 ; Barak et al. 2009 ; Gliozzo et al. 2009 ) succeeded in eliminating the requirement for manual specification of keywords by using the category name alone as the initial keyword, yet obtaining superior performance within the keyword-based approach. Our research continues this line of work, focusing on the fully unsupervised name-based approach. We believe that this approach is not only appealing from the practical point of view, but is also worthwhile research-wise, as it is interesting to know how far one can get based solely on category names.

We note that all of the aforementioned methods were developed and evaluated using standard research datasets, which are in line with the supervised scenario. However, it would be appropriate to presume that the ad-hoc industrial scenario described above raises different issues. In our work we aimed at exploring this second typical setting using its inherent data. For brevity, we will further address our setting as industry-oriented or industrial , as opposed to the common supervised setting in the research literature.

In this paper we describe our research on TC from category name within our industrial setting, and provide the following contributions: 1. We identify the main issues raised by TC in the setting of our scope, and 2. We evaluate and analyze the performance of state-of-the-art unsupervised TC 3. As a result of this analysis, we identify the shortcomings of the state-of-the-art 4. We develop a novel classification and evaluation scheme for our scenario, In Sect. 2 we provide relevant background on state-of-the-art methods. Section 3 describes the dataset used in this study and analyzes the state-of-the-art performance over this data. Next, in Sect. 4 we present our suggested improvements. The results method, showing superior results over the state-of-the-art. 2 Background This section first describes related work on name-based Text Categorization (Sect. 2.1 ), and next provides relevant background on the lexical reference framework employed in this work. (Sect. 2.2 ). 2.1 Categorization based on category name TC approaches that use only the category name as the input and require no manual effort during the classification process have been attempted rather rarely in the literature.

A bootstrapping approach introduced by Gliozzo et al. ( 2005 , 2009 ) included the following steps: (a) expanding the category names using Latent Semantic Analysis (LSA) (Deerwester et al. 1990 ), such that the categories are represented in LSA space, (b) separating relevant and non-relevant category information using statistics from unlabeled examples by a Gaussian Mixture algorithm, (c) classifying each unlabeled example to the most probable category and (d) training a SVM classifier on the set of labeled examples resulting from the previous step. They reported results on two datasets, 20-NewsGroups and Reuters-10 (the 10 most frequent categories in Reuters-21578), showing improvement relative to earlier keyword-based methods.

In Downey and Etzioni ( 2008 ) the Monotonic Feature (MF) abstraction was introduced, where the probability of class membership increases monotonically with the MF X  X  value. In document classification, the name of the class is a natural MF; the more frequently it is repeated in a document, all other factors being equal, the greater the likelihood that the document belongs to the class. They extended the experiments of Gliozzo et al. ( 2005 ), presenting theoretical and empirical results, showing that even relatively weak MFs can be used to induce a noisy labeling over examples, and these examples can then be used to train effective classifiers utilizing existing supervised or semi-supervised techniques. 2.1.1 Reference-and-context approach The most recent approach has been reported by Barak et al. ( 2009 ). They proposed a novel scheme that models separately two types of similarity. One type, termed Reference , regards words that refer specifically to the category name X  X  meaning, is termed Context and regards typical context words for the category that do not  X  X  Baseball  X  X . The distinction of reference and context keywords stems from the recognition that topics are not monolithic things. Almost any concept exhibits different facets, and for any particular application only some of them are relevant. Thus, reference words are particular to the topic alone, while the context keywords apply to the topic plus its immediate neighborhood in a generalization hierarchy.
The similarity scores obtained by their combined measure ( Combined ) were used to produce an initial labeled set of documents which was then used to train a supervised classifier in a bootstrapping step. They tested their method on the two corpora used in Gliozzo et al. ( 2005 ) and reported superior performance. Below we describe the method in more detail, since in our work we employ it as representing the state-of-the-art.
 Reference component The assumption underlying this component is that a relevant document for a category typically includes concrete terms referring specifically to the meaning of the category name. Referring terms were collected from WordNet (further described in Sect. 2.2 ).

The reference vector for a category consists of the category name ( seed term) and all its referring terms, equally weighted. Documents are represented as vectors in term space, and cosine similarity is used to produce the reference model score for each document-category pair. Context component Classifying by the reference model alone may yield false  X  X  Medicine  X  X , and (2) a passing reference, e.g. an analogy to cars in a software document should not yield classification of the document to the category  X  X  Cars  X  X .
In both these cases the overall context in the document is expected to be typical for the triggered category. Therefore the contextual similarity between a category and a document is measured utilizing LSA space, replicating the method in Gliozzo et al. ( 2005 , 2009 ). In LSA documents and categories are represented in a latent semantic space via a dimensionality reduction method which decreases the number of dimensions in the term-document matrix. It converts the co-occurrence data represented in the matrix to a representation of implicit semantic concepts. The original method of Gliozzo et al. ( 2005 ) included a Gaussian Mixture rescaling step for the context model, which was not found helpful by Barak et al. ( 2009 ).
Both the category names and the documents are thus represented as vectors in the latent space, and cosine similarity between them is used to obtain context model scores.
 Combined measure and the bootstrapping phase To combine the scores obtained by these two models multiplication was used in Barak et al. ( 2009 ). Multiplication reduces the score of documents that contain referring terms, but relate to irrelevant contexts. Moreover, when the reference model returns a zero score, the integrated score would also be zero. Ideally, given perfect reference knowledge, this means that when the text does not refer to the category topic, it would not be classified to that category even if it involves a related context.

The similarity scores obtained by the combined measure were used to produce an initial labeled set of documents for training a supervised classifier. They used the initial labeled set, in which each document is considered as classified only to the best scoring category, to train an SVM classifier for each category. For Reuters-10, classification was determined independently by the classifier for each category, allowing multiple classes per document. For 20-NewsGroups, the category that yielded the highest classification score was chosen (one-versus-all), fitting the single-label setting of this corpus. They experimented with two document representations for the supervised step: either as vectors in tf-idf weighted term space, or as vectors in LSA space.

As mentioned above, in this work we base our method on the state-of-the-art approach of Barak et al. ( 2009 ), described above. Next we elaborate more on the lexical reference framework and the resources of referring terms, providing the background needed to understand the improvements suggested in our work. 2.2 Lexical reference Thelexicalreference(LR)notionwasdefinedinGlickmanet al.( 2006 )todenotein-text references to the specific meaning of a target term. The LR relation between two terms hand-side (LHS) term would generate a reference, in some contexts, to a possible meaningoftheright-hand-side(RHS)term,e.g. X  Jaguar ) luxurycar  X .Inthisexample the LHS is a hyponym of the RHS. Indeed, the commonly used hyponymy, synonymy and some cases of the meronymy relations are special cases of lexical reference.
In the TC scheme described above, category names are expanded with referring terms: for each rule in which the RHS is the name of a specific category, the LHS term of this rule is added to the category X  X  reference vector. E.g. the aforesaid rule  X  physician ) medicine  X  would add the word  X  X  physician  X  X  to the vector representing the  X  X  Medicine  X  X  category.
 We note that this expansion mechanism is analogous to query expansion in Information Retrieval (Xu and Croft 1996 ). In both tasks seed terms (either category names in TC or original query terms in IR) are expanded with other related terms in order to achieve improved performance.

The methods for automatic query expansion split into two major classes: global methods and local methods. Global methods utilize large resources, such as the WordNetandWikipediaresourcesdescribedbelow,whichsupplyexpansiontermsfora variety of query words. Another possible source for global query expansion is based on Local methods extract expanding terms for a query from a small set of documents that initially appeared to match the query, rather than from large global corpora. In our work we utilize large global resources as our lexical reference resources. We use the WordNet and Wikipedia resources based on manually encoded information, which were found beneficial by Barak et al. ( 2009 ) (see Sect. 2.2.1 ), and suggest additional global resources based on word co-occurrence in corpora (see Sect. 4.1 ). 2.2.1 Lexical reference resources Lexical-semantic resources are commonly utilized by applied inference systems (Giampiccolo et al. 2007 ) and applications such as Information Retrieval, Question Answering and TC (Shah and Croft 2004 ; Pasca and Harabagiu 2001 ; Scott and Matwin 1999 ).Two suchexternalresourcesavailable online wereutilizedbyBaraket al. ( 2009 ) for LR rules extraction: the WordNet lexical ontology (Fellbaum 1998 ) and the LR resource of Shnarch et al. ( 2009 ) based on Wikipedia. Below we provide a short WordNet WordNet is a large lexical database of English, where terms are grouped into sets of synonyms ( synsets ), each expressing a distinct concept. Synsets are interlinked by means of relations, such as hyponyms ( is-a relation), meronyms ( is-part-of relation), antonyms and derivational relations.

TC is one of the many tasks for which WordNet is exploited as a source for lexical expansion. In several works WordNet synonyms and hypernyms were used to enhance feature data for supervised TC methods (de Buenaga Rodriguez et al. 1997 ; Scott and Matwin 1999 ; Fleischman and Hovy 2002 ; Mansuy and Hilderman 2006 ). In the work of Barak et al. ( 2009 ) referring terms were found in WordNet starting from manually specified relevant senses of each category name. They used synonyms, derivations, hyponyms and meronyms, and as a common heuristic considered only the most frequent senses (top four) of referring terms, avoiding rare senses that are likely to introduce noise when used for expansion. In our work we adopt their method to acquire LR rules from WordNet knowledge.
 Wikipedia Wikipedia is a collaborative online encyclopedia that covers a wide variety of domains and is constantly evolving based on the contribution of online from article definitions, as well as from Wikipedia redirect and hyperlink relations, resulting in a rule base 1 of about eight million rules of different types, e.g.  X  x11 ) X-Windows  X ,  X  Yamaha SR500 ) motorcycle  X ,  X  John Lennon ) music  X .
This resource was used by Barak et al. ( 2009 ) to extract referring terms capturing the traditional synonymy and hyponymy relations. In our research we experiment with adopting all of the rule types provided by the resource, since by definition they comply with the lexical reference relation in all its diversity. 2
As shown by Barak et al. ( 2009 ), as an encyclopedic resource containing cultural and day-to-day terms by its nature Wikipedia is complementary to the type of rules extracted from WordNet, which covers mostly dictionary-style terms. 3 Dataset and prior art performance The focus of this research is on name-based TC in a typical unsupervised industrial Yet, as stated above, state-of-the-art approaches were developed and evaluated using standard academic datasets, namely Reuters-10 and 20-NewsGroups.
In this section we first describe our dataset, which was built in collaboration with our industrial partners (see Acknowledgments). We describe its construction and annotation along with the creation of the taxonomy of topics (Sect. 3.1 ). Next, in Sect. 3.2 , we analyze the prior art performance for this dataset. Following this analysis, in Sect. 4 we suggest ways to overcome the observed limitations. 3.1 Dataset To conduct this research, we consulted our industrial partners to determine the typical issues of their setting, on which we focus, resulting in the following main characteristics:  X  Labeled data is not available or its generation is costly, yet the number of  X  The documents of interest usually constitute user-generated content (social  X  The number of categories is relatively high (couple hundreds vs. couple dozens  X  There are documents that belong to more than one category, and there can often Triggered by a video recommendation scenario, we used the Internet Movie Database (IMDB) in order to construct a dataset complying with the above characteristics. IMDB is an online database of information related to films, television programs etc. It contains plot summaries written collaboratively by internet volunteers and thus corresponds to the first two characteristics by providing considerable amounts of user-generated content. The IMDB corpus which we created for our research is a collection of 120,000 movie descriptions downloaded from the IMDB website. Each movie description (further termed document ) contains the movie title and plot summary information. Target categories of these documents are unknown. The IMDB corpus is thus a large collection of documents, which was not labeled to a given taxonomy of topics.

To answer the last two of the aforesaid characteristics, we constructed a realistic topical taxonomy of the domain and annotated a sample of the documents as the gold-standard test data for evaluation. The taxonomy and annotation were generated by a native English-speaking M.Sc. student, who did not take part in our research. The annotator researched the IMDB database, its structure and content to see which information can be useful for building the taxonom. Browsing the internet, she found media taxonomies, compared and combined them with frequent annotated IMDB keywords to create a new target taxonomy.

Figure 1 shows a part of the taxonomy. Appendix 1 includes our complete IMDB taxonomy. The taxonomy consists of 97 topical categories organized in a three level hierarchy structure, where each classification to a daughter category is considered as a classification to all its ancestors as well. For example, a document whose category  X  X  Football  X  X  is a daughter of the  X  X  Sport  X  X  category.

She then manually annotated 1970 movie descriptions with category labels from the taxonomy. While selecting the documents to be annotated we had to make sure issues which were taken into consideration are the distribution of genres in the IMDB database and the fact that some genres are better suited for the classification task than others. We selected 2/3 of the dataset from the group of genres which are more suited for topical classification (Biography, Documentary, History, Music, Sport, and War) and 1/3 from the rest of the genres. We also filtered descriptions with less than 150 characters. Appendix 2 includes the annotation guidelines. A couple of iterations were required to stabilize the annotation. We randomly divided the annotated set to development (50 %) and test (50 %) subsets. The collection X  X  gold standard is multi-label classified, hence each document may be classified to zero, one or more categories. The average number of labels per document is 1.26. 3 The dataset thus complies with the characteristics mentioned in the beginning of this section. The annotated dataset will be made publicly available.

In order to evaluate the inter-annotator agreement, 200 documents were randomly sampled from our dataset and annotated according to the guidelines by an additional annotator, resulting in kappa value of 0.73. 4 We then performed yet another annotation round, in which the second annotator was allowed to see the first annotator X  X  judgments and update her decision. The consequent changes raised the kappa to 0.85, mostly due to changing to a child-category (e.g.  X  X  Hip Hop  X  X  instead reconsidering passing references. 3.2 Prior art performance on the IMDB dataset As described above, the bootstrapping scheme suggested by Barak et al. ( 2009 ) and others (Ko and Seo 2004 ; Gliozzo et al. 2005 ) consists of training a supervised classifier with an initial labeled set, which was created by the previous unsupervised step. We replicated the method in Barak et al. ( 2009 ), explained in detail in Sect. 2.1.1 , as representing the state-of-the-art classifier.

Replicating Barak et al. ( 2009 ), the Combined scoring method was used to produce an initial labeled set of about 120,000 documents from the IMDB corpus. This set, in which each document is assigned only to the highest scoring category, 5 was used to train a SVM classifier for each category. Following Barak et al. ( 2009 ), input vectors in tf-idf weighted term space. The results are presented in Table 1 . The results on the 20-NewsGroups and Reuters-10 datasets, which were used for evaluation in prior works, are given for comparison.

The results in Table 1 show that, unlike the prior datasets, in our case bootstrapping is problematic. Bootstrapping yields even lower performance than the unsupervised step, which constitutes its input training set, as we further report in Sect. 3.2.1 . There might be several possible reasons for these poor results: 1. The documents are short and their quality is low, e.g. often only a single word 2. The way of training set construction, where each document which was not 3. The results of the initial unsupervised step are not good enough to provide the Below we analyze the performance of the unsupervised step for our dataset (Sect. 3.2.1 ) and advocate the idea that this step is to be the core of industry-oriented unsupervised Text Categorization. Later, in Sect. 5 we report in more detail the adjustments we made for applying the bootstrapping process to our data. 3.2.1 Unsupervised single-label classification This unsupervised step of the algorithm classifies each document to a single category, the category with the highest similarity to the document. We tested both components of the Combined scoring method in Barak et al. ( 2009 ), namely the Reference model and the Context model 6 (Sect. 2.1.1 ). We also examined the baseline of including only the category name in the reference vector ( Cat-Name ).
Table 2 presents the classification results obtained for these methods. The results on the 20-NewsGroups and Reuters-10 datasets are given for comparison.
As we can see from Table 2 , the bootstrapping step led to a dramatic deterioration of the performance. Although lower than for the prior datasets, the results of the unsupervised step are much more reasonable, and for the Combined scoring method are even comparable to the results over the 20-NewsGroups. This supports the hypothesis that noisy user-generated documents are less suited for supervised techniques. Moreover, the bootstrapping step actually contradicts the rationale of LR-based approaches. Lexical reference specifies an accurate semantic relation, which aims to identify whether the meaning of a certain category name is referenced by the document text. This measure aims at a more appropriate relation to base the TC assumption on, since it requires an actual reference to the category topic in the text rather than general context similarity. In contrast, in the bootstrapping step a supervised classifier is used to perform the final categorization step on the test corpus. The supervised classifier does not capture the exact semantic relation needed to assess classification decisions. It might model the broader context of the text and not the specific topic it discusses. Therefore, we suggest that when LR-based approaches are applied, it is desirable to avoid the bootstrapping step.
We thus focus our efforts on improving the quality of initial unsupervised categorization and on making the unsupervised step applicable independently, without necessarily involving the consequent bootstrapping step. Below we analyze the limited performance of the unsupervised step for our data in order to detect directions for improvement. 1. Limitations of the context model Below we detail two main error cases related Passing reference A dominant phenomenon which causes misclassification is passing references. Passing references occur when the category name or some of its referring terms appear in a document, but they do not relate to the main topic of the document. Table 3 shows several examples of documents which contain a passing reference to one of the IMDB collection categories.

In Barak et al. ( 2009 ) two mechanisms are used to overcome the passing reference phenomenon:  X  Lexical expansion of the category names by the reference model, which results  X  Use of context models. When a term which refers to a certain topic (category) As explained above, in our settings documents are both rather short and less formally focused on the main topic of the document, containing excessive facts, analogies, etc. Thus often documents do not contain multiple occurrences of referring terms, but do contain numerous passing references. The second mechanism therefore is the main way to cope with passing references in such texts. Yet, in many of the cases the currently used context model failed to recognize context irrelevancy. We believe that this is mostly due to the noisy character of the data, since the documents used to generate the LSA context model suffer from the same shortcomings as the documents in the classification set.
 Ambiguity of referring terms Ambiguity of category names within the collection is rare since they are typically chosen to be very precise and capture the full meaning behind the corresponding topic. However, by using reference expansions as part of the method, terms are being added to the category name to represent the category. One of the reasons for wrong classifications is ambiguity of these expanding terms. Table 4 shows several examples of documents assigned to a wrong category due to ambiguity of the referring terms, which appeared in the documents in a different sense than the one corresponding to the category topic.
The context model was supposed to recognize that the overall context in these documents is not typical for the triggered categories and assign low context scores to avoid these classifications. Yet, in many cases the context model failed to effectively balance the high reference score obtained by the triggered categories.
Thus, we set our first direction for improvement as strengthening the context component to cope with the noisy character of the data. 2. Limitations of the reference model As described in detail in Sect. 2.2 , referring Lack of referring terms Some of the documents were not classified to the correct category due to a lack of correct expansions. Table 5 shows examples of such missing referring terms.

We note that most of the lacking expansions detected in our analysis were not related to the category names via traditional semantic relations, such as synonymy or hyponymy, but rather via topical relatedness. This justifies the decision we explained in Sect. 2.2 , not to focus solely on synonymy and hyponymy as it was done in previous works. Yet, we see that including all of the relations from the Wikipedia-based resource was not enough.

In addition, we note that many of the missing expansions are domain-specific, e.g. the currently used lexical resources provide a number of rules allowing to relate the word  X  X  drug  X  X  with the  X  X  Medicine  X  X  category, while in the IMDB dataset documents with this term mostly belong to the category  X  X  Crime  X  X . We thus conclude that additional sources of expansions are needed, able to capture topical relatedness and cover the deficit of expansions related to the domain categories.
 Noisy expanding terms Both WordNet and Wikipedia added expansions which were only correct for rather infrequent senses of either a category name or an expanding term, which caused false classifications. Table 6 shows several examples of such expansions. Sometimes the term sense is so rare in the given domain that it even seems to be an incorrect expansion. E.g. as an expansion for the category  X  X  Business  X  X  the word  X  X  house  X  X  is used in its second most frequent WordNet sense, sense is extremely low and thus this expansion introduces noise to the reference model.

As explained earlier, context model is supposed to cope with ambiguous references. Yet, in contrast to the ambiguity described above, expansions that are incorrect or inapplicable for a given domain can be potentially weakened in the reference model, or filtered from it. 3. Limitations of the classification scheme The main drawback of the state-of-the-art classification scheme for the unsupervised step is single-label classification. Classifying each document to a single class has two major disadvantages:  X  It  X  X  X orces X  X  classification. Each document is classified to the category with the  X  It  X  X  X iscards X  X  classifications, since only the category with the maximal In previous works, where the unsupervised step was only used to generate labeled for each category. As advocated above, we intend to enable independent application of the unsupervised step and thus need a classification scheme for fully unsupervised multi-label classification. Moreover, since the phenomena described above are very frequent in our data, multi-label categorization of documents during the unsupervised step can be helpful in training set construction towards applying the bootstrapping step (although simple top-k multi-label classification did not improve the bootstrapping quality, as reported earlier). 4 Improvements for TC in an ad-hoc industrial scenario improvementswesuggestto adapt the state-of-the-artprocedure tooursettings. Wefirst a new context model (Sect. 4.1.1 ) and induce a new lexical reference resource global reference-context combination scheme (Sect. 4.2 ). Finally, in Sect. 4.3 we introduce a classification scheme geared for ad-hoc industrial taxonomies. 4.1 Utilizing statistical correlation Co-occurrence based methods are based on the assumption that words that occur frequently together in the same document are related to the same topic. Therefore word co-occurrence information can be used to identify topical semantic relation-ships between words.

This type of information was not used in previous works mainly due to the fact that it is traditionally considered to be noisy, as the lists of semantically related words contain both lexical references and general context terms. For example, for the word  X  X  drug  X  X  both  X  X  marijuana  X  X  (lexical reference) and  X  X  crime  X  X  (general context term) can be extracted. Yet, we see this source of information as highly suitable for our task, both to enhance the context model and to provide the missing type of lexical expansions, namely topically-related and domain-specific lexical references.

There are various metrics to measure the strength of the co-occurrence relationship between two words, all based on the frequencies of the words X  independent and co-occurring appearances in the corpus. In this work we used the Dice coefficient (Smadja et al. 1996 ), which produced better expansions as compared to PMI (Church and Hanks 1990 ) and the probabilistic metric of Glickman and Dagan ( 2005 ) in our preliminary manual analysis. 4.1.1 Co-occurrence context model (2) not in a different sense than the one referring to the category name.
This requirement can be captured by a set of terms which correspond to typical category contexts, even though they do not necessarily concretely refer to the category. Such terms frequently appear in the category context and therefore tend to co-occur with the category X  X  seed terms. Occurrence of such terms implies that the not refer to the category  X  X  Baseball  X  X , as they can appear within the context of several other sport categories. However, the presence of a significant amount of such context words in a document increases the likelihood that this document may be related to the baseball topic. On the other hand, the lack of any context word in a document decreases the likelihood that this document is relevant to the category X  X  topic. For that purpose, co-occurrence of terms can be the basis for context models.
In Barak et al. ( 2009 ) LSA was utilized to represent the context similarity of documents and categories. LSA offers a powerful context-similarity measure, but is somewhat crude and has difficulties to distinct between topically close categories. Moreover, LSA is complex to implement and computationally expensive. This becomes an important issue in our setting, where huge amounts of unlabeled documents are available and we are interested to use them all in order to learn a good model for each of the numerous categories in the taxonomy.

As our analysis in Sect. 3.2.1 revealed the need to enhance the existing context model, we suggest an additional context model based on the Dice coefficient metric. This model is simpler than LSA, yet easy to analyze and less computationally expensive: 1. We expand each category name by the top-k (k  X  100 in our case) co-occurring 2. To obtain the context model score, we calculate the cosine similarity score 3. Like Barak et al. ( 2009 ), we use multiplication as the integration method of the Looking at the expanded vectors, we observe that the dice-based context model indeed captures typical category contexts, for example, the category  X  X  Baseball  X  X  refereeing terms for  X  X  Baseball  X  X , while the others are only related context terms. 4.1.2 Co-occurrence expansions resource The analysis in Sect. 3.2.1 showed that there is a need in (1) lexical references corresponding to the topical relatedness relation and (2) domain-specific lexical references. Producing domain-specific lists of topically-related terms is a known characteristic of the co-occurrence expansion resources learned from a large domain corpus. Yet, as explained above, these term lists mix lexical references with general context terms. Below we suggest a procedure of filtering co-occurrence lists in order to obtain relatively precise lists of lexical references. We note that in this work we employ this filtering only for our co-occurrence lexical resource, yet it can be terms inapplicable for the given domain, such as the term  X  X  house  X  X  as an expansion for the  X  X  Business  X  X  category.

For each category, we start with a noisy expansion list made up of the top-100 words co-occurring with the category seeds in the IMDB corpus. Then, we apply the filtering steps described below. We used the annotated development set from our IMDB dataset for tuning the parameters.

Weight filtering We used the dice co-efficient score for term weighting and filtered terms whose weight with a given category name is lower than a threshold which was set to 0.05. The idea behind this step is that commonly lexical reference terms are higher associated with the corresponding category than general context terms, which occur frequently with other related categories as well. This higher association results in higher co-occurrence scores, thus removing terms below a certain score threshold is likely to retain lexical references and discard general context terms.

Seed filtering Often one category name appears as an expansion of another. We filtered these expansions since the seeds were chosen to be very precise and to capture the full meaning behind the topic and mostly fit only their original category.

Frequent term filtering Some terms are referred widely in the corpus and cannot be used to distinct between categories. For example, the term  X  X  film  X  X  is a good lexical reference for the  X  X  Cinema  X  X  category, yet it is used too often in the IMDB documents and yields more passing references than correct classifications (see
Table 3 ). We filtered these expansions by setting a threshold on the term frequency in the corpus. Terms which appear in more than 4 % of the documents in the corpus are omitted from the category expansions list.

Multiple expansions filtering Some terms expand more than one category and are therefore less distinctive. We attribute the term only to the category which gets the highest dice coefficient score with the term, which is:
This filtering is very important since assigning a term to more than one category produces a lot of noise, for example, the term  X  X  mob boss  X  X  originally expands both drugs.

Table 7 shows correct references which were found by our co-occurrence resource of lexical references. These expansions were not found nor by WordNet neither by Wikipedia. Using the statistical lexical-reference resource we found additional proper names of known personalities in the category areas, as well as concepts which are strongly related to the category names. 4.2 Combined scoring We have now three resources of referring terms: WordNet, Wikipedia and the co-occurrence resource (further termed Dice ), which have to be combined. In (Barak et al. 2009 ) the following combination scheme was used:
Union Referring terms are collected from all the resources. The term lists for each category are unified to a single list which is then used to represent the category vector. The weight of the seed term and the referring terms in the vector is equal, and set to 1. The cosine similarity measure is then used to measure the similarity between document vectors and category vectors.

The above combination scheme discards important information on agreement of different resources concerning specific terms. Noisy expansions introduced by one resource are unlikely to appear among the expansions introduced by another resource, while good lexical references might appear in more that one resource. Following this logic, we propose an alternative combination scheme, aiming to weaken the inappropriate expansions in the reference model:
Geometric mean First consider each resource separately, by representing each category by a separate term vector per resource and calculating the cosine similarity between these category vectors and the documents. Then combine the individual similarity scores using geometric mean (GM): where n is the number of combined resources ( n  X j X j ) and Sim x is the similarity score of resource x . In case that Sim x  X  0, we assign it a non-zero smoothing factor k , which we have set to 0.0001.

GM is lower when there is a high difference between the averaged numbers and higher when this difference is low. This mathematical property of the GM captures the agreement between the resources, since referring terms supported by more than one resource will yield higher similarity scores.

These combination schemes deal with combining references resources and return the similarity score of the reference model, which in turn should be combined with the context model score. Aiming to maximize the performance, we combined both of the context models in the following way: we first combined the reference model with the LSA context model using multiplication as done in Barak et al. ( 2009 ), and then combined the resulting score with the co-occurrence context model by multiplication with a smoothing factor as described earlier in Sect. 4.1.1 .
Overall, our primary method configuration, which is evaluated in Sect. 5 , contains three lexical reference resources, WordNet , Wikipedia and Dice , combined with two context models, further termed LSA and Dice-based context model. Further we describe the novel classification scheme we suggest for classification by a large ad-hoc industrial taxonomy. 4.3 Multi-label classification scheme The analysis in Sect. 3.2.1 showed that single-label classification used in the prior art for the unsupervised categorization task is not suitable in our setting. Unlike previous methods, which used separate supervised classifiers for each category to potentially assign a document to several categories, we suggest addressing multi-label classification as a ranking-oriented task, where (1) a ranked list of documents is created for each category by sorting the documents in descending order according to their categorization score and (2) the top ranked documents in each list are selected as positive for the corresponding category.

To set a cut-off point in the ranked document list for each category a threshold has to be determined. Thus, there is a common tradeoff between recall and precision, thresholds are learned automatically, we suggest that a threshold for each category will be tuned separately to maintain a certain precision level for each category. This cut-off approach better fits the typical industrial settings, where the user would not want to have precision that is lower than a certain predefined level, while manual tuning of parameters is considered acceptable to achieve this objective. 5 Results and analysis This section presents the evaluation of our suggested improvements described in Sect. 4 . We first evaluate our full scoring method and the individual contribution of its components (Sect. 5.2 ). Then we assess the quality of our novel classification methodology (Sect. 5.3 ) and, finally, in Sect. 5.5 describe and report the results of our experiments with the bootstrapping scheme. 5.1 Evaluation scheme In this section we describe the evaluation approach suitable for our classification scheme, and the rationale behind this approach.

Two basic evaluation measures in TC are recall and precision, which allow to measure performance over the entire list of documents classified to a category. Yet, they do not account for the quality of ranking the documents in the document list. As explained above, our scheme assumes that industrial users would prefer classified documents to be ranked according to their relevance to the category, instead of just being presented with an unordered document set.

A common way to depict the degradation of precision with the increase of recall as one traverses the ranked document list is to plot interpolated precision numbers against percentage recall. E.g. a percentage recall of 50 % is the position in the documents list at which 50 % of the relevant documents in the collection have been retrieved. The same plot expresses the notion of recall at precision ,or R@P , referring to the percentage of relevant documents which can be found at a certain precision level.

As explained above, our multi-label classification scheme requires specifying a cut-off point in the ranked document list for each category. We suggest using the R@P curve , which is an averaged recall X  X recision curve where each cut-off point corresponds to a certain precision level: 1. The precision level is presented in 1 = k intervals, where k is the desired number 2. For each category, we calculate the number of correct classifications in the 3. We then sum the number of these correct classifications for all the categories, R@P curve illustrates how much recall a classifier can provide under a certain precision level, thus corresponding with the industrial requirement of maintaining a certain predefined precision level. We use the R@P curve as our main evaluation measure in Sect. 5 . For completeness we also report the Mean Average Precision (MAP) values of the evaluated methods. 5.2 Evaluation of the scoring method In this section we evaluate our scoring method described in Sect. 4.2 . We note that in this evaluation we intend to assess the quality of our suggested scoring method per se, without considering our novel classification scheme. We thus compare the following algorithms:
Single-label combined The state-of-the-art single-label Combined method described in Sect. 2.1.1 , which was used in Barak et al. ( 2009 ) for the unsupervised categorization step. The single-label combined method classifies each document to the category with the highest similarity to the document.
Multi-label combined An adaptation of the single-label combined method for unsupervised multi-label classification. This method calculates classification scores according to the Combined scoring method of Barak et al. ( 2009 ). It then creates a ranked document list for each category by sorting the documents by their classification score and leaving only those with non-zero score, i.e. leaving the documents that contained at least one mention of the category X  X  referring terms.
Our method The algorithm follows the same classification scheme as in the multi-label combined method above, utilizing our scoring method instead of the Combined method of Barak et al. ( 2009 ).

Figure 2 presents the resulting R@P average curves. It shows that using our scoring method consistently outperforms other methods by several points. The recall of our method is higher, since we utilized a new additional statistical LR resource. In addition, the accuracy of the statistical LR resource with the additional dice-based context model yielded an increase in precision as well. Comparison between the single-label combined curve with the multi-label curves shows that single-label classification is limited and even a naive multi-label classification performs better.
Comparison between the two multi-label curves shows that integrating the statistical knowledge (dice-based context model and lexical resource) helped to improve the performance, showing an average recall improvement of 6.8 points.
To complete the evaluation, in Table 8 we present the MAP values of the compared algorithms. We see that ranking by our score achieves considerably the two-sided Wilcoxon signed rank sum test (Wilcoxon 1945 ). 8 5.2.1 Contribution of the method X  X  components As described in Sect. 4.2 , our scoring method adds two components to the Combined scoring method of Barak et al. ( 2009 ), namely a co-occurrence dice-based context model and a dice-based lexical references resource. In order to assess the contribution of each component individually we performed ablation tests. Furthermore, we compared the existing union-based resource combination scheme to the scheme based on geometric mean, which we presented in Sect. 4.2 . The results are shown in Fig. 3 .
 Context models The results in Fig. 3 show that combining different context models is beneficial, since each of them has some additional information relative to the others. We also see that our dice-based context model shows quite similar behavior to the state-of-the-art LSA model, except for one cut-off point, of 0.7 precision, where the LSA performs better.

The MAP of the LSA model is 0.56, while the dice-based model X  X  MAP is 0.55 context model is comparable to the much more complex LSA-based model. We note that in addition to being simpler and less computationally expensive, dice-base model is easy to analyze, which is advantageous both in industrial and academic settings.
 Resource combination scheme Comparison of the two different resource combi-nation schemes shows that using geometric mean (GM) is no better than the resources union suggested by Barak et al. ( 2009 ). We believe that the rationale behind the GM combination scheme is valid, although the suggested approach was ineffective in the IMDB case. Indeed, multiplication of vectors is one of the simple methods to compose their meanings, which does not always work well. We assume additional methods are needed to effectively reduce the influence of poor expansions. One of the possible directions could be investigating compositional models in the spirit of Mitchell and Lapata ( 2010 ), Baroni and Zamparelli ( 2010 ), Grefenstette and Sadrzadeh ( 2011 ) and Socher et al. ( 2012 ).
 Lexical resources The contribution of the Dice LR resource is important when recall is considered. Starting from the 0.6 precision cut-off point, the recall of our scoring method increases due to the Dice LR resource.
 In Fig. 4 we present a more detailed comparison of contribution of each of the LR resources used in our improved scoring method, namely WordNet, Wikipedia all resources. The context models were part of the the system in all of these ablation tests.

Analyzing our results, we can conclude that utilizing LR resources is valuable for user-generated data as well. Relying solely on the context models, similarly to Gliozzo et al. ( 2009 ), is obviously not enough for the TC task.

Wikipedia has been a potentially good resource for LR, providing the typical knowledge found in an encyclopedia. However, there is an overlap between the Dice and Wikipedia resources. When we use the Dice and WordNet resources, the precision is low.

WordNet is a different type of resource that has more impact on the classification results. Typical knowledge that can be found in a dictionary tends to occur less frequently in co-occurrence statistics collected from a corpus. WordNet expansions improve the ranking of the documents and increase the recall, maintaining a high rate of precision.

Dice expansion resource increases the recall where the precision is below 0.6, but when we use only the Dice LR resource, the recall at the same range is not maximized. The performance of the Dice LR resource is reasonable even as a single resource in cases where no additional resources are available, e.g. for non-English data. However, combining the Dice LR resource with other external resources, such as WordNet, leads to better performance. 5.3 Evaluation of the classification scheme In this section we evaluate our novel classification scheme described in Sect. 4.3 ,in which we suggested a new cut-off approach. This is different from other standard approaches in the points X  cut-off level. In our R@P average curve each cut-off point be a threshold on the classification scores or a certain percentage of the top ranked classifications (top-k %).

Figure 5 shows a comparison between our cut-off approach and the two other typical cut-off approaches. The curves for the standard approaches were obtained similarly to our approach described in Sect. 5.1 , as follows:  X  For each category, calculate the number of correct classifications that meet the  X  Sum the number of these correct classifications of all the categories.  X  To obtain the recall level for the given threshold, divide the sum by the total  X  To obtain the precision level for the given threshold, divide the sum of correct Figure 5 shows that our more expensive cut-off scheme indeed outperforms other standard cut-off schemes consistently by several percentage points. The performance of our cut-off scheme is better, since the cut-off of a certain precision level defines different thresholds over different categories, allowing to eliminate noisy assignments more carefully and thus obtain an improved output. The figure achieve 0.56 recall, and the F1 score of 0.58, which is comparable to previous results on academic datasets. 5.4 Error analysis and directions for future work The multi-label classification schemes classify each document into numerous categories. Any random mention of a referring term yields a classification with a positive score. The main issue is whether the scoring method succeeded in ranking the category X  X  documents properly. To analyze this issue and detect directions for future improvements, we sampled incorrect classifications corresponding to different levels of precision, according to our suggested classification scheme.
We were satisfied to see that at precision levels acceptable in industrial settings For example, if a video recommendation system will suggest a movie about an artist is possible since our improved document scoring complies with the lexical reference requirement formulated by Barak et al. ( 2009 ), according to which a document will obtain a non-zero score for a category only if it contains a lexical reference to the corresponding category name. Thus even irrelevant documents in the lists produced by our method for a definite category will hold a lexical reference to the category and their assignment to the given category will therefore be accountable. This characteristic is extremely important for real-life systems and it reinforces our preference to avoid the bootstrapping procedure in our setting, since bootstrapping re-assigns the documents without adhering to the lexical reference requirement as explained earlier in Sect. 3 . Overall, 44 % of the error cases in our sample can be considered  X  X  X xcusable errors X  X .

Concerning the types of errors, in general our system suffers, although to a lesser extent, from the same error types explained in Sect. 3.2.1 : passing reference, ambiguity of expanding terms, as well as incorrectness and deficit of expansions. The distribution of error types is similar to that obtained for the analysis in Sect. 3.2.1 . We thus conclude that our method reduces the overall quantity of errors of all types, since the analysis did not distinguish error types for which our method was especially effective or particularly ineffective, as compared to state of the art. Figure 6 presents the distribution of the error types in our sample.

The figure shows that over a half of the errors (37  X  19  X  56 % ) are passing references. Although 44 % were considered excusable, further reducing the number of such errors would definitely improve the performance. Our analysis showed that context models often fail to degrade erroneously high reference scores when documents contain multiple passing references, as well as ambiguous terms and matches of noisy expansions. A possible direction for improvement could be to further strengthen the context component by considering complementary sources of information and more powerful models, e.g. the Explicit Semantic Analysis (ESA) by Gabrilovich and Markovitch ( 2007 ) or Latent Dirichlet Allocation (LDA) by Blei et al. ( 2003 ).
 There is also room for improvement in the lexical reference component. The WordNet and Wikipedia LR resources include many frequent domain-inappropriate while the Dice-based LR resource suffers insignificantly from this problem. Thus, it would be reasonable to consider weighting schemes for expansion terms, in the spirit of Metzler et al. ( 2007 ), preferring domain expansions over expansions from generic lexical resources, as well as giving higher weights to category seeds which proved to be a highly reliable source of information. Preferring expansions supported by multiple resources could also be further inspected, following the rationale of our GM combination scheme, as suggested above in Section 5.2.1 . Incorporating supplementary sources of lexical references would also potentially improve the performance, as lack of expansion terms accounts for as much as 15 % of errors in our analysis.

The quality of the Dice expansions lists of category names that appear frequently in the IMDB corpus is relatively high, yet for category names which are not very frequent in the corpus the Dice resource added many noisy terms, e.g.  X  X  roommate  X  X , suggest adding documents for categories with a small number of documents in the IMDB corpus by crawling the web or using some other corpus. This line of research may be investigated further to enrich and optimize the Dice LR resource.
We examined two interesting issues considering category characteristics: (i) whether categories with more documents in the test set are ranked better than categories with fewer documents, and (ii) whether estimating the size of the category was negative. We found that the quality of the categories X  ranking does not depend on that express their specific topic meaning, such as  X  X  Football  X  X ,  X  X  Buddhism  X  X  and  X  X  Motorcycle  X  X , were ranked much better than category names that express more frequency of the category name in the whole corpus does not provide an accurate this category contains a relatively high number of documents. In this work we did not make any adjustments to the category names, but simply set the seeds to be the category names as they were given in the taxonomy. The reason for this policy was that we wanted our results to be replicable, so we did not use any prior knowledge on the resources behavior. Prior knowledge on seeds that get more effective expansions such LR resource might be helpful. By investing manual effort in tuning the category names, which would be acceptable in the industry, further improvement could be potentially achieved. 5.5 Bootstrapping results As explained above, prior art approaches (reference to the 3 papers) consist of training a supervised classifier with an initial labeled set created by a previous unsupervised step. In Sect. 3 we showed that bootstrapping performance on the IMDB corpus by the method of Barak et al. ( 2009 ) was very poor, yielding lower performance than the unsupervised classification that constitutes its input training set. Further, we advocated the hypothesis that in our scenario it would be preferable to avoid the bootstrapping step. In this section we describe the adjustments we made in order to apply the bootstrapping procedure, following the analysis in Sect. 3.2 , and further evaluate the results of the bootstrapping step.

To produce an initial labeled set to train a SVM classifier for each category, the unsupervised procedure was used by Barak et al. ( 2009 ) to assign each document in the unlabeled set to a single best-scoring category. Classification was determined independently by the classifier for each category, allowing multiple classes per document. In Sect. 4.3 we introduced our multi-label unsupervised classification scheme where each document may be classified to zero, one or more categories. Consequently, we present a different approach for producing an initial labeled set of documents using a multi-label classification scheme as our first step. Ideally, we would have wanted to set a high precision level and take the documents that meet this requirement as positive training examples for the supervised classifier. However, we lacked the human resources for manually tuning each of the categories. We therefore had to adopt a standard global cut-off scheme. We selected the percentage cut-off scheme, where a top percent of the highly ranked documents of each category is selected as positive examples for the category classifier.
Good negative examples for the supervised training process need to have two properties: (a) high confidence that these are indeed negative examples and (b) at least some of them should be close enough to the positive examples, containing passing references or sharing similar contexts. Since each document can be assigned to multiple categories, with a different score per assignment, lower-scoring classifications can potentially be a source of negative examples. Following this rationale we performed an analysis, which showed that 99 % of such low-rank classifications were inappropriate indeed. However, the documents did have terms pointing to the inappropriate category. Therefore, we sorted the list of categories to which a given document was assigned, and selected the document as a negative example for categories that were ranked lower than a certain rank in the list. The last issue was selecting the ratio between the positive and negative examples. Since we were unable to estimate the real portion of a category in the corpus, we selected the same number of examples for both the negatives and the positives for each category. We manually tuned both of our parameters based on experiments on the development set, represented the input examples vectors for the SVM supervised classifier in tf-idf weighted term space and used a common feature selection which removes the least common features in the corpus.

However, we did not obtain any reasonable results. Both the recall and precision were lower than 0.1. These experiment results confirm the validity of our hypothesis that when LR-based approaches are applied in an ad-hoc unsupervised scenario, it is desirable to avoid the bootstrapping step. 6 Conclusions In this work we suggested a novel TC scenario, interesting both in industry and research-wise. We identified and described the main characteristics of the scenario and constructed a representative dataset accordingly. We investigated the unsuper-vised TC approach suitable for this scenario, which uses category names as its only input and does not require additional supervision. The approach is based on the integration of reference models and context models. The proposed method integrates a new LR and a new context model into the scoring method proposed by Barak et al. ( 2009 ). We suggested a novel multi-label classification scheme with a corresponding evaluation approach, revealing a new perspective on the classification results.

Our investigation highlights the following main conclusions about the integration of the two models, about each of the new models and about the classification and evaluation scheme: 1. Our analysis confirmed the conclusion of Barak et al. ( 2009 ) that the reference 2. Utilizing statistical correlation from a target domain corpus is useful for both 3. Our dice-based context model is much simpler than the LSA context model, 4. Combining different LR resources provides a more complete perspective and 5. When a small degree of manual intervention is possible, which is usually the We detected several promising directions for improvements (see Sect. 5.4 ) and will make the dataset publicly available to encourage further research in this direction. Appendix 1 Our complete IMDB taxonomy Categories 1. Religion 1.1. Buddhism 1.2. Hinduism 1.3. Christianity 1.4. Islam 1.5. Judaism 2. Sport 2.1. Bicycle 2.2. Boxing 2.3. Fishing 2.4. Football 2.5. Golf 2.6. Hockey 2.7. Martial-arts 2.8. Athletics 2.9. Running 2.10. Shooting 2.11. Skiing 2.12. Soccer 2.13. Water sports 2.14. Tennis 2.15. Baseball 2.16. Wrestling 2.17. Basketball 2.18. Horseracing 2.19. Olympic games 3. Interests (NON-CAT) 3.1. Beach 3.2. Outdoor 3.3. Gardening 3.4. Pets 3.5. Fitness 3.6. Cookery 3.7. Fashion 3.8. Computing 3.9. Travel 3.10. Motoring 3.11. Trains 3.12. Airplanes 3.13. Ships 3.14. Radio 3.15. Business 3.16. Nature 3.17. Outer Space 3.18. The environment 3.19. Showbiz 3.20. Traditions 3.21. Infants 3.22. Military 3.23. Weather 4. Arts 4.1. Cinema 4.2. Advertising 4.3. Theater 4.4. Music 4.5. Dance 5. Science 5.1. Medicine 5.2. Technology 5.3. Psychology 6. Education 7. Miscellaneous (NON-CAT) 7.1. Crime (NON-CAT) 7.2. Literature 7.3. History 7.4. Political 7.5. Social (NON-CAT) 7.6. Legal 7.7. Communism 7.8. War 7.9. Aliens 7.10. Comic-book 7.11. Journalism 7.12. Mythology Appendix 2 The annotation guidelines You are given a list of films with their plot description and a taxonomy of film categories.

The taxonomy is made up of film subject matters and is arranged in a hierarchical in all cases except when a category is only present in order to group similar subject together in which case it is marked with the text (NON-CAT) next to it.
For example: A film categorized as dealing with  X  X ars X  will also be relevant to  X  X otoring X  but not to  X  X nterests X  as it is not a category. 3. Interests (NON-CAT) Note  X  X he taxonomy is not exhaustive, you may find that there is no category in the taxonomy which accurately fits the film even though you can think of a subject matter that does. If a broader category is present choose it, otherwise choose none.
For each film, you must decide which categories (if any) out of the taxonomy are relevant to it. You can choose as many or as few categories as you see fit, or none.
Note  X  X f you find more than one category, please put each category in a separate line (insert lines if necessary).

You must categorize according to the following guidelines: 1. Is the background story prominent X  X ot just a passing reference.
 2. You must not base your decision on prior knowledge of the film, only on References
 Abstract In this work we suggest a novel Text Categorization (TC) scenario, motivated by an ad-hoc industrial need to assign documents to a set of predefined categories, while labeled training data for the categories is not available. The sce-nario is applicable in many industrial settings and is interesting from the academic perspective. We present a new dataset geared for the main characteristics of the scenario, and utilize it to investigate the name-based TC approach, which uses the category names as its only input and does not require training data. We evaluate and analyze the performance of state-of-the-art methods for this dataset to identify the shortcomings of these methods for our scenario, and suggest ways for overcoming these shortcomings. We utilize statistical correlation measured over a target corpus for improving the state-of-the-art, and offer a different classification scheme based on the characteristics of the setting. We evaluate our improvements and adaptations and show superior performance of our suggested method.
 Keywords Natural language processing Name-based Text Categorization Semantic similarity 1 Introduction Topical Text Categorization (TC), also known as text classification , is the task of automatically classifying documents into a set of predefined categories (classes, topics). TC has become one of the key techniques for handling and organizing text data, and is used in a variety of scenarios, such as classifying news stories, finding interesting information on the web and guiding user search through hypertext browsing.

In the research community the dominant approach to this task is based on supervised machine learning (Sebastiani 2002 ), utilizing a large number of labeled training documents. This approach complies with some industrial settings, e.g. with a typical scenario of large information providers, such as Reuters, where significant amounts of labeled data are created as part of the work flow. Indeed, such data has underlain the development and establishment of supervised TC. However, there is another typical setting, where TC is involved to support an ad-hoc industrial information need and labeled data is not easily available.

Our work was triggered by collaboration with partners from the movie recommendation industry, whose scenario represents this second typical setting. They aimed at classification of movie descriptions to enhance a video recommen-dation system with topical knowledge. Yet, manually annotating training data for of topics in order to express the information need. With the rapid growth of online information this setting is becoming more and more frequent and one can see many domains where this TC setting is prevailing, e.g. categorization of product reviews to maintain a review-crawling website (Hu and Liu 2004 ; Ruhl et al. 2006 ; Archak et al. 2011 ), categorization of social media content for analyzing opinions on different social and political issues (Tumasjan et al. 2010 ; Ritter et al. 2012 ;Li et al. 2012 ; Maynard and Funk 2012 ; Liu and Zhang 2012 ), categorization of requests towards a tech support center to simplify the treatment of standard issues (Eichler et al. 2012 , 2014 ), as well as TC in the legal domain, education, medicine, 2013 ; Saggion and Funk 2009 ; Scharl and Weichselbraun 2008 ).

In all these scenarios an approach which does not require manual data annotation would be extremely appealing, since the categories of interest are numerous and often dynamic, while modest performance (relative to supervised methods) is still useful. Motivated by this fact, there has been a line of work on keyword-based TC, aiming at a bootstrapping approach, where training data is generated automatically from unlabeled documents. For this purpose, first each category is represented by a handcrafted list of characteristic keywords, which should capture the category meaning. Then measuring similarity between documents and the category keywords is used to produce labeled training data. The manual effort is thus reduced to providing a keyword list per category (Mccallum and Nigam 1999 ). This step was even partly automated in Ko and Seo ( 2004 ) and Liu et al. ( 2004 ), by using clustering to generate candidate keywords to assist a lexicographer or a domain expert. Nevertheless, the method still requires manual specification as part of the classification process. Further research (Gliozzo et al. 2005 ; Barak et al. 2009 ; Gliozzo et al. 2009 ) succeeded in eliminating the requirement for manual specification of keywords by using the category name alone as the initial keyword, yet obtaining superior performance within the keyword-based approach. Our research continues this line of work, focusing on the fully unsupervised name-based approach. We believe that this approach is not only appealing from the practical point of view, but is also worthwhile research-wise, as it is interesting to know how far one can get based solely on category names.

We note that all of the aforementioned methods were developed and evaluated using standard research datasets, which are in line with the supervised scenario. However, it would be appropriate to presume that the ad-hoc industrial scenario described above raises different issues. In our work we aimed at exploring this second typical setting using its inherent data. For brevity, we will further address our setting as industry-oriented or industrial , as opposed to the common supervised setting in the research literature.

In this paper we describe our research on TC from category name within our industrial setting, and provide the following contributions: 1. We identify the main issues raised by TC in the setting of our scope, and 2. We evaluate and analyze the performance of state-of-the-art unsupervised TC 3. As a result of this analysis, we identify the shortcomings of the state-of-the-art 4. We develop a novel classification and evaluation scheme for our scenario, In Sect. 2 we provide relevant background on state-of-the-art methods. Section 3 describes the dataset used in this study and analyzes the state-of-the-art performance over this data. Next, in Sect. 4 we present our suggested improvements. The results method, showing superior results over the state-of-the-art. 2 Background This section first describes related work on name-based Text Categorization (Sect. 2.1 ), and next provides relevant background on the lexical reference framework employed in this work. (Sect. 2.2 ). 2.1 Categorization based on category name TC approaches that use only the category name as the input and require no manual effort during the classification process have been attempted rather rarely in the literature.

A bootstrapping approach introduced by Gliozzo et al. ( 2005 , 2009 ) included the following steps: (a) expanding the category names using Latent Semantic Analysis (LSA) (Deerwester et al. 1990 ), such that the categories are represented in LSA space, (b) separating relevant and non-relevant category information using statistics from unlabeled examples by a Gaussian Mixture algorithm, (c) classifying each unlabeled example to the most probable category and (d) training a SVM classifier on the set of labeled examples resulting from the previous step. They reported results on two datasets, 20-NewsGroups and Reuters-10 (the 10 most frequent categories in Reuters-21578), showing improvement relative to earlier keyword-based methods.

In Downey and Etzioni ( 2008 ) the Monotonic Feature (MF) abstraction was introduced, where the probability of class membership increases monotonically with the MF X  X  value. In document classification, the name of the class is a natural MF; the more frequently it is repeated in a document, all other factors being equal, the greater the likelihood that the document belongs to the class. They extended the experiments of Gliozzo et al. ( 2005 ), presenting theoretical and empirical results, showing that even relatively weak MFs can be used to induce a noisy labeling over examples, and these examples can then be used to train effective classifiers utilizing existing supervised or semi-supervised techniques. 2.1.1 Reference-and-context approach The most recent approach has been reported by Barak et al. ( 2009 ). They proposed a novel scheme that models separately two types of similarity. One type, termed Reference , regards words that refer specifically to the category name X  X  meaning, is termed Context and regards typical context words for the category that do not  X  X  Baseball  X  X . The distinction of reference and context keywords stems from the recognition that topics are not monolithic things. Almost any concept exhibits different facets, and for any particular application only some of them are relevant. Thus, reference words are particular to the topic alone, while the context keywords apply to the topic plus its immediate neighborhood in a generalization hierarchy.
The similarity scores obtained by their combined measure ( Combined ) were used to produce an initial labeled set of documents which was then used to train a supervised classifier in a bootstrapping step. They tested their method on the two corpora used in Gliozzo et al. ( 2005 ) and reported superior performance. Below we describe the method in more detail, since in our work we employ it as representing the state-of-the-art.
 Reference component The assumption underlying this component is that a relevant document for a category typically includes concrete terms referring specifically to the meaning of the category name. Referring terms were collected from WordNet (further described in Sect. 2.2 ).

The reference vector for a category consists of the category name ( seed term) and all its referring terms, equally weighted. Documents are represented as vectors in term space, and cosine similarity is used to produce the reference model score for each document-category pair. Context component Classifying by the reference model alone may yield false  X  X  Medicine  X  X , and (2) a passing reference, e.g. an analogy to cars in a software document should not yield classification of the document to the category  X  X  Cars  X  X .
In both these cases the overall context in the document is expected to be typical for the triggered category. Therefore the contextual similarity between a category and a document is measured utilizing LSA space, replicating the method in Gliozzo et al. ( 2005 , 2009 ). In LSA documents and categories are represented in a latent semantic space via a dimensionality reduction method which decreases the number of dimensions in the term-document matrix. It converts the co-occurrence data represented in the matrix to a representation of implicit semantic concepts. The original method of Gliozzo et al. ( 2005 ) included a Gaussian Mixture rescaling step for the context model, which was not found helpful by Barak et al. ( 2009 ).
Both the category names and the documents are thus represented as vectors in the latent space, and cosine similarity between them is used to obtain context model scores.
 Combined measure and the bootstrapping phase To combine the scores obtained by these two models multiplication was used in Barak et al. ( 2009 ). Multiplication reduces the score of documents that contain referring terms, but relate to irrelevant contexts. Moreover, when the reference model returns a zero score, the integrated score would also be zero. Ideally, given perfect reference knowledge, this means that when the text does not refer to the category topic, it would not be classified to that category even if it involves a related context.

The similarity scores obtained by the combined measure were used to produce an initial labeled set of documents for training a supervised classifier. They used the initial labeled set, in which each document is considered as classified only to the best scoring category, to train an SVM classifier for each category. For Reuters-10, classification was determined independently by the classifier for each category, allowing multiple classes per document. For 20-NewsGroups, the category that yielded the highest classification score was chosen (one-versus-all), fitting the single-label setting of this corpus. They experimented with two document representations for the supervised step: either as vectors in tf-idf weighted term space, or as vectors in LSA space.

As mentioned above, in this work we base our method on the state-of-the-art approach of Barak et al. ( 2009 ), described above. Next we elaborate more on the lexical reference framework and the resources of referring terms, providing the background needed to understand the improvements suggested in our work. 2.2 Lexical reference Thelexicalreference(LR)notionwasdefinedinGlickmanet al.( 2006 )todenotein-text references to the specific meaning of a target term. The LR relation between two terms hand-side (LHS) term would generate a reference, in some contexts, to a possible meaningoftheright-hand-side(RHS)term,e.g. X  Jaguar ) luxurycar  X .Inthisexample the LHS is a hyponym of the RHS. Indeed, the commonly used hyponymy, synonymy and some cases of the meronymy relations are special cases of lexical reference.
In the TC scheme described above, category names are expanded with referring terms: for each rule in which the RHS is the name of a specific category, the LHS term of this rule is added to the category X  X  reference vector. E.g. the aforesaid rule  X  physician ) medicine  X  would add the word  X  X  physician  X  X  to the vector representing the  X  X  Medicine  X  X  category.
 We note that this expansion mechanism is analogous to query expansion in Information Retrieval (Xu and Croft 1996 ). In both tasks seed terms (either category names in TC or original query terms in IR) are expanded with other related terms in order to achieve improved performance.

The methods for automatic query expansion split into two major classes: global methods and local methods. Global methods utilize large resources, such as the WordNetandWikipediaresourcesdescribedbelow,whichsupplyexpansiontermsfora variety of query words. Another possible source for global query expansion is based on Local methods extract expanding terms for a query from a small set of documents that initially appeared to match the query, rather than from large global corpora. In our work we utilize large global resources as our lexical reference resources. We use the WordNet and Wikipedia resources based on manually encoded information, which were found beneficial by Barak et al. ( 2009 ) (see Sect. 2.2.1 ), and suggest additional global resources based on word co-occurrence in corpora (see Sect. 4.1 ). 2.2.1 Lexical reference resources Lexical-semantic resources are commonly utilized by applied inference systems (Giampiccolo et al. 2007 ) and applications such as Information Retrieval, Question Answering and TC (Shah and Croft 2004 ; Pasca and Harabagiu 2001 ; Scott and Matwin 1999 ).Two suchexternalresourcesavailable online wereutilizedbyBaraket al. ( 2009 ) for LR rules extraction: the WordNet lexical ontology (Fellbaum 1998 ) and the LR resource of Shnarch et al. ( 2009 ) based on Wikipedia. Below we provide a short WordNet WordNet is a large lexical database of English, where terms are grouped into sets of synonyms ( synsets ), each expressing a distinct concept. Synsets are interlinked by means of relations, such as hyponyms ( is-a relation), meronyms ( is-part-of relation), antonyms and derivational relations.

TC is one of the many tasks for which WordNet is exploited as a source for lexical expansion. In several works WordNet synonyms and hypernyms were used to enhance feature data for supervised TC methods (de Buenaga Rodriguez et al. 1997 ; Scott and Matwin 1999 ; Fleischman and Hovy 2002 ; Mansuy and Hilderman 2006 ). In the work of Barak et al. ( 2009 ) referring terms were found in WordNet starting from manually specified relevant senses of each category name. They used synonyms, derivations, hyponyms and meronyms, and as a common heuristic considered only the most frequent senses (top four) of referring terms, avoiding rare senses that are likely to introduce noise when used for expansion. In our work we adopt their method to acquire LR rules from WordNet knowledge.
 Wikipedia Wikipedia is a collaborative online encyclopedia that covers a wide variety of domains and is constantly evolving based on the contribution of online from article definitions, as well as from Wikipedia redirect and hyperlink relations, resulting in a rule base 1 of about eight million rules of different types, e.g.  X  x11 ) X-Windows  X ,  X  Yamaha SR500 ) motorcycle  X ,  X  John Lennon ) music  X .
This resource was used by Barak et al. ( 2009 ) to extract referring terms capturing the traditional synonymy and hyponymy relations. In our research we experiment with adopting all of the rule types provided by the resource, since by definition they comply with the lexical reference relation in all its diversity. 2
As shown by Barak et al. ( 2009 ), as an encyclopedic resource containing cultural and day-to-day terms by its nature Wikipedia is complementary to the type of rules extracted from WordNet, which covers mostly dictionary-style terms. 3 Dataset and prior art performance The focus of this research is on name-based TC in a typical unsupervised industrial Yet, as stated above, state-of-the-art approaches were developed and evaluated using standard academic datasets, namely Reuters-10 and 20-NewsGroups.
In this section we first describe our dataset, which was built in collaboration with our industrial partners (see Acknowledgments). We describe its construction and annotation along with the creation of the taxonomy of topics (Sect. 3.1 ). Next, in Sect. 3.2 , we analyze the prior art performance for this dataset. Following this analysis, in Sect. 4 we suggest ways to overcome the observed limitations. 3.1 Dataset To conduct this research, we consulted our industrial partners to determine the typical issues of their setting, on which we focus, resulting in the following main characteristics:  X  Labeled data is not available or its generation is costly, yet the number of  X  The documents of interest usually constitute user-generated content (social  X  The number of categories is relatively high (couple hundreds vs. couple dozens  X  There are documents that belong to more than one category, and there can often Triggered by a video recommendation scenario, we used the Internet Movie Database (IMDB) in order to construct a dataset complying with the above characteristics. IMDB is an online database of information related to films, television programs etc. It contains plot summaries written collaboratively by internet volunteers and thus corresponds to the first two characteristics by providing considerable amounts of user-generated content. The IMDB corpus which we created for our research is a collection of 120,000 movie descriptions downloaded from the IMDB website. Each movie description (further termed document ) contains the movie title and plot summary information. Target categories of these documents are unknown. The IMDB corpus is thus a large collection of documents, which was not labeled to a given taxonomy of topics.

To answer the last two of the aforesaid characteristics, we constructed a realistic topical taxonomy of the domain and annotated a sample of the documents as the gold-standard test data for evaluation. The taxonomy and annotation were generated by a native English-speaking M.Sc. student, who did not take part in our research. The annotator researched the IMDB database, its structure and content to see which information can be useful for building the taxonom. Browsing the internet, she found media taxonomies, compared and combined them with frequent annotated IMDB keywords to create a new target taxonomy.

Figure 1 shows a part of the taxonomy. Appendix 1 includes our complete IMDB taxonomy. The taxonomy consists of 97 topical categories organized in a three level hierarchy structure, where each classification to a daughter category is considered as a classification to all its ancestors as well. For example, a document whose category  X  X  Football  X  X  is a daughter of the  X  X  Sport  X  X  category.

She then manually annotated 1970 movie descriptions with category labels from the taxonomy. While selecting the documents to be annotated we had to make sure issues which were taken into consideration are the distribution of genres in the IMDB database and the fact that some genres are better suited for the classification task than others. We selected 2/3 of the dataset from the group of genres which are more suited for topical classification (Biography, Documentary, History, Music, Sport, and War) and 1/3 from the rest of the genres. We also filtered descriptions with less than 150 characters. Appendix 2 includes the annotation guidelines. A couple of iterations were required to stabilize the annotation. We randomly divided the annotated set to development (50 %) and test (50 %) subsets. The collection X  X  gold standard is multi-label classified, hence each document may be classified to zero, one or more categories. The average number of labels per document is 1.26. 3 The dataset thus complies with the characteristics mentioned in the beginning of this section. The annotated dataset will be made publicly available.

In order to evaluate the inter-annotator agreement, 200 documents were randomly sampled from our dataset and annotated according to the guidelines by an additional annotator, resulting in kappa value of 0.73. 4 We then performed yet another annotation round, in which the second annotator was allowed to see the first annotator X  X  judgments and update her decision. The consequent changes raised the kappa to 0.85, mostly due to changing to a child-category (e.g.  X  X  Hip Hop  X  X  instead reconsidering passing references. 3.2 Prior art performance on the IMDB dataset As described above, the bootstrapping scheme suggested by Barak et al. ( 2009 ) and others (Ko and Seo 2004 ; Gliozzo et al. 2005 ) consists of training a supervised classifier with an initial labeled set, which was created by the previous unsupervised step. We replicated the method in Barak et al. ( 2009 ), explained in detail in Sect. 2.1.1 , as representing the state-of-the-art classifier.

Replicating Barak et al. ( 2009 ), the Combined scoring method was used to produce an initial labeled set of about 120,000 documents from the IMDB corpus. This set, in which each document is assigned only to the highest scoring category, 5 was used to train a SVM classifier for each category. Following Barak et al. ( 2009 ), input vectors in tf-idf weighted term space. The results are presented in Table 1 . The results on the 20-NewsGroups and Reuters-10 datasets, which were used for evaluation in prior works, are given for comparison.

The results in Table 1 show that, unlike the prior datasets, in our case bootstrapping is problematic. Bootstrapping yields even lower performance than the unsupervised step, which constitutes its input training set, as we further report in Sect. 3.2.1 . There might be several possible reasons for these poor results: 1. The documents are short and their quality is low, e.g. often only a single word 2. The way of training set construction, where each document which was not 3. The results of the initial unsupervised step are not good enough to provide the Below we analyze the performance of the unsupervised step for our dataset (Sect. 3.2.1 ) and advocate the idea that this step is to be the core of industry-oriented unsupervised Text Categorization. Later, in Sect. 5 we report in more detail the adjustments we made for applying the bootstrapping process to our data. 3.2.1 Unsupervised single-label classification This unsupervised step of the algorithm classifies each document to a single category, the category with the highest similarity to the document. We tested both components of the Combined scoring method in Barak et al. ( 2009 ), namely the Reference model and the Context model 6 (Sect. 2.1.1 ). We also examined the baseline of including only the category name in the reference vector ( Cat-Name ).
Table 2 presents the classification results obtained for these methods. The results on the 20-NewsGroups and Reuters-10 datasets are given for comparison.
As we can see from Table 2 , the bootstrapping step led to a dramatic deterioration of the performance. Although lower than for the prior datasets, the results of the unsupervised step are much more reasonable, and for the Combined scoring method are even comparable to the results over the 20-NewsGroups. This supports the hypothesis that noisy user-generated documents are less suited for supervised techniques. Moreover, the bootstrapping step actually contradicts the rationale of LR-based approaches. Lexical reference specifies an accurate semantic relation, which aims to identify whether the meaning of a certain category name is referenced by the document text. This measure aims at a more appropriate relation to base the TC assumption on, since it requires an actual reference to the category topic in the text rather than general context similarity. In contrast, in the bootstrapping step a supervised classifier is used to perform the final categorization step on the test corpus. The supervised classifier does not capture the exact semantic relation needed to assess classification decisions. It might model the broader context of the text and not the specific topic it discusses. Therefore, we suggest that when LR-based approaches are applied, it is desirable to avoid the bootstrapping step.
We thus focus our efforts on improving the quality of initial unsupervised categorization and on making the unsupervised step applicable independently, without necessarily involving the consequent bootstrapping step. Below we analyze the limited performance of the unsupervised step for our data in order to detect directions for improvement. 1. Limitations of the context model Below we detail two main error cases related Passing reference A dominant phenomenon which causes misclassification is passing references. Passing references occur when the category name or some of its referring terms appear in a document, but they do not relate to the main topic of the document. Table 3 shows several examples of documents which contain a passing reference to one of the IMDB collection categories.

In Barak et al. ( 2009 ) two mechanisms are used to overcome the passing reference phenomenon:  X  Lexical expansion of the category names by the reference model, which results  X  Use of context models. When a term which refers to a certain topic (category) As explained above, in our settings documents are both rather short and less formally focused on the main topic of the document, containing excessive facts, analogies, etc. Thus often documents do not contain multiple occurrences of referring terms, but do contain numerous passing references. The second mechanism therefore is the main way to cope with passing references in such texts. Yet, in many of the cases the currently used context model failed to recognize context irrelevancy. We believe that this is mostly due to the noisy character of the data, since the documents used to generate the LSA context model suffer from the same shortcomings as the documents in the classification set.
 Ambiguity of referring terms Ambiguity of category names within the collection is rare since they are typically chosen to be very precise and capture the full meaning behind the corresponding topic. However, by using reference expansions as part of the method, terms are being added to the category name to represent the category. One of the reasons for wrong classifications is ambiguity of these expanding terms. Table 4 shows several examples of documents assigned to a wrong category due to ambiguity of the referring terms, which appeared in the documents in a different sense than the one corresponding to the category topic.
The context model was supposed to recognize that the overall context in these documents is not typical for the triggered categories and assign low context scores to avoid these classifications. Yet, in many cases the context model failed to effectively balance the high reference score obtained by the triggered categories.
Thus, we set our first direction for improvement as strengthening the context component to cope with the noisy character of the data. 2. Limitations of the reference model As described in detail in Sect. 2.2 , referring Lack of referring terms Some of the documents were not classified to the correct category due to a lack of correct expansions. Table 5 shows examples of such missing referring terms.

We note that most of the lacking expansions detected in our analysis were not related to the category names via traditional semantic relations, such as synonymy or hyponymy, but rather via topical relatedness. This justifies the decision we explained in Sect. 2.2 , not to focus solely on synonymy and hyponymy as it was done in previous works. Yet, we see that including all of the relations from the Wikipedia-based resource was not enough.

In addition, we note that many of the missing expansions are domain-specific, e.g. the currently used lexical resources provide a number of rules allowing to relate the word  X  X  drug  X  X  with the  X  X  Medicine  X  X  category, while in the IMDB dataset documents with this term mostly belong to the category  X  X  Crime  X  X . We thus conclude that additional sources of expansions are needed, able to capture topical relatedness and cover the deficit of expansions related to the domain categories.
 Noisy expanding terms Both WordNet and Wikipedia added expansions which were only correct for rather infrequent senses of either a category name or an expanding term, which caused false classifications. Table 6 shows several examples of such expansions. Sometimes the term sense is so rare in the given domain that it even seems to be an incorrect expansion. E.g. as an expansion for the category  X  X  Business  X  X  the word  X  X  house  X  X  is used in its second most frequent WordNet sense, sense is extremely low and thus this expansion introduces noise to the reference model.

As explained earlier, context model is supposed to cope with ambiguous references. Yet, in contrast to the ambiguity described above, expansions that are incorrect or inapplicable for a given domain can be potentially weakened in the reference model, or filtered from it. 3. Limitations of the classification scheme The main drawback of the state-of-the-art classification scheme for the unsupervised step is single-label classification. Classifying each document to a single class has two major disadvantages:  X  It  X  X  X orces X  X  classification. Each document is classified to the category with the  X  It  X  X  X iscards X  X  classifications, since only the category with the maximal In previous works, where the unsupervised step was only used to generate labeled for each category. As advocated above, we intend to enable independent application of the unsupervised step and thus need a classification scheme for fully unsupervised multi-label classification. Moreover, since the phenomena described above are very frequent in our data, multi-label categorization of documents during the unsupervised step can be helpful in training set construction towards applying the bootstrapping step (although simple top-k multi-label classification did not improve the bootstrapping quality, as reported earlier). 4 Improvements for TC in an ad-hoc industrial scenario improvementswesuggestto adapt the state-of-the-artprocedure tooursettings. Wefirst a new context model (Sect. 4.1.1 ) and induce a new lexical reference resource global reference-context combination scheme (Sect. 4.2 ). Finally, in Sect. 4.3 we introduce a classification scheme geared for ad-hoc industrial taxonomies. 4.1 Utilizing statistical correlation Co-occurrence based methods are based on the assumption that words that occur frequently together in the same document are related to the same topic. Therefore word co-occurrence information can be used to identify topical semantic relation-ships between words.

This type of information was not used in previous works mainly due to the fact that it is traditionally considered to be noisy, as the lists of semantically related words contain both lexical references and general context terms. For example, for the word  X  X  drug  X  X  both  X  X  marijuana  X  X  (lexical reference) and  X  X  crime  X  X  (general context term) can be extracted. Yet, we see this source of information as highly suitable for our task, both to enhance the context model and to provide the missing type of lexical expansions, namely topically-related and domain-specific lexical references.

There are various metrics to measure the strength of the co-occurrence relationship between two words, all based on the frequencies of the words X  independent and co-occurring appearances in the corpus. In this work we used the Dice coefficient (Smadja et al. 1996 ), which produced better expansions as compared to PMI (Church and Hanks 1990 ) and the probabilistic metric of Glickman and Dagan ( 2005 ) in our preliminary manual analysis. 4.1.1 Co-occurrence context model (2) not in a different sense than the one referring to the category name.
This requirement can be captured by a set of terms which correspond to typical category contexts, even though they do not necessarily concretely refer to the category. Such terms frequently appear in the category context and therefore tend to co-occur with the category X  X  seed terms. Occurrence of such terms implies that the not refer to the category  X  X  Baseball  X  X , as they can appear within the context of several other sport categories. However, the presence of a significant amount of such context words in a document increases the likelihood that this document may be related to the baseball topic. On the other hand, the lack of any context word in a document decreases the likelihood that this document is relevant to the category X  X  topic. For that purpose, co-occurrence of terms can be the basis for context models.
In Barak et al. ( 2009 ) LSA was utilized to represent the context similarity of documents and categories. LSA offers a powerful context-similarity measure, but is somewhat crude and has difficulties to distinct between topically close categories. Moreover, LSA is complex to implement and computationally expensive. This becomes an important issue in our setting, where huge amounts of unlabeled documents are available and we are interested to use them all in order to learn a good model for each of the numerous categories in the taxonomy.

As our analysis in Sect. 3.2.1 revealed the need to enhance the existing context model, we suggest an additional context model based on the Dice coefficient metric. This model is simpler than LSA, yet easy to analyze and less computationally expensive: 1. We expand each category name by the top-k (k  X  100 in our case) co-occurring 2. To obtain the context model score, we calculate the cosine similarity score 3. Like Barak et al. ( 2009 ), we use multiplication as the integration method of the Looking at the expanded vectors, we observe that the dice-based context model indeed captures typical category contexts, for example, the category  X  X  Baseball  X  X  refereeing terms for  X  X  Baseball  X  X , while the others are only related context terms. 4.1.2 Co-occurrence expansions resource The analysis in Sect. 3.2.1 showed that there is a need in (1) lexical references corresponding to the topical relatedness relation and (2) domain-specific lexical references. Producing domain-specific lists of topically-related terms is a known characteristic of the co-occurrence expansion resources learned from a large domain corpus. Yet, as explained above, these term lists mix lexical references with general context terms. Below we suggest a procedure of filtering co-occurrence lists in order to obtain relatively precise lists of lexical references. We note that in this work we employ this filtering only for our co-occurrence lexical resource, yet it can be terms inapplicable for the given domain, such as the term  X  X  house  X  X  as an expansion for the  X  X  Business  X  X  category.

For each category, we start with a noisy expansion list made up of the top-100 words co-occurring with the category seeds in the IMDB corpus. Then, we apply the filtering steps described below. We used the annotated development set from our IMDB dataset for tuning the parameters.

Weight filtering We used the dice co-efficient score for term weighting and filtered terms whose weight with a given category name is lower than a threshold which was set to 0.05. The idea behind this step is that commonly lexical reference terms are higher associated with the corresponding category than general context terms, which occur frequently with other related categories as well. This higher association results in higher co-occurrence scores, thus removing terms below a certain score threshold is likely to retain lexical references and discard general context terms.

Seed filtering Often one category name appears as an expansion of another. We filtered these expansions since the seeds were chosen to be very precise and to capture the full meaning behind the topic and mostly fit only their original category.

Frequent term filtering Some terms are referred widely in the corpus and cannot be used to distinct between categories. For example, the term  X  X  film  X  X  is a good lexical reference for the  X  X  Cinema  X  X  category, yet it is used too often in the IMDB documents and yields more passing references than correct classifications (see
Table 3 ). We filtered these expansions by setting a threshold on the term frequency in the corpus. Terms which appear in more than 4 % of the documents in the corpus are omitted from the category expansions list.

Multiple expansions filtering Some terms expand more than one category and are therefore less distinctive. We attribute the term only to the category which gets the highest dice coefficient score with the term, which is:
This filtering is very important since assigning a term to more than one category produces a lot of noise, for example, the term  X  X  mob boss  X  X  originally expands both drugs.

Table 7 shows correct references which were found by our co-occurrence resource of lexical references. These expansions were not found nor by WordNet neither by Wikipedia. Using the statistical lexical-reference resource we found additional proper names of known personalities in the category areas, as well as concepts which are strongly related to the category names. 4.2 Combined scoring We have now three resources of referring terms: WordNet, Wikipedia and the co-occurrence resource (further termed Dice ), which have to be combined. In (Barak et al. 2009 ) the following combination scheme was used:
Union Referring terms are collected from all the resources. The term lists for each category are unified to a single list which is then used to represent the category vector. The weight of the seed term and the referring terms in the vector is equal, and set to 1. The cosine similarity measure is then used to measure the similarity between document vectors and category vectors.

The above combination scheme discards important information on agreement of different resources concerning specific terms. Noisy expansions introduced by one resource are unlikely to appear among the expansions introduced by another resource, while good lexical references might appear in more that one resource. Following this logic, we propose an alternative combination scheme, aiming to weaken the inappropriate expansions in the reference model:
Geometric mean First consider each resource separately, by representing each category by a separate term vector per resource and calculating the cosine similarity between these category vectors and the documents. Then combine the individual similarity scores using geometric mean (GM): where n is the number of combined resources ( n  X j X j ) and Sim x is the similarity score of resource x . In case that Sim x  X  0, we assign it a non-zero smoothing factor k , which we have set to 0.0001.

GM is lower when there is a high difference between the averaged numbers and higher when this difference is low. This mathematical property of the GM captures the agreement between the resources, since referring terms supported by more than one resource will yield higher similarity scores.

These combination schemes deal with combining references resources and return the similarity score of the reference model, which in turn should be combined with the context model score. Aiming to maximize the performance, we combined both of the context models in the following way: we first combined the reference model with the LSA context model using multiplication as done in Barak et al. ( 2009 ), and then combined the resulting score with the co-occurrence context model by multiplication with a smoothing factor as described earlier in Sect. 4.1.1 .
Overall, our primary method configuration, which is evaluated in Sect. 5 , contains three lexical reference resources, WordNet , Wikipedia and Dice , combined with two context models, further termed LSA and Dice-based context model. Further we describe the novel classification scheme we suggest for classification by a large ad-hoc industrial taxonomy. 4.3 Multi-label classification scheme The analysis in Sect. 3.2.1 showed that single-label classification used in the prior art for the unsupervised categorization task is not suitable in our setting. Unlike previous methods, which used separate supervised classifiers for each category to potentially assign a document to several categories, we suggest addressing multi-label classification as a ranking-oriented task, where (1) a ranked list of documents is created for each category by sorting the documents in descending order according to their categorization score and (2) the top ranked documents in each list are selected as positive for the corresponding category.

To set a cut-off point in the ranked document list for each category a threshold has to be determined. Thus, there is a common tradeoff between recall and precision, thresholds are learned automatically, we suggest that a threshold for each category will be tuned separately to maintain a certain precision level for each category. This cut-off approach better fits the typical industrial settings, where the user would not want to have precision that is lower than a certain predefined level, while manual tuning of parameters is considered acceptable to achieve this objective. 5 Results and analysis This section presents the evaluation of our suggested improvements described in Sect. 4 . We first evaluate our full scoring method and the individual contribution of its components (Sect. 5.2 ). Then we assess the quality of our novel classification methodology (Sect. 5.3 ) and, finally, in Sect. 5.5 describe and report the results of our experiments with the bootstrapping scheme. 5.1 Evaluation scheme In this section we describe the evaluation approach suitable for our classification scheme, and the rationale behind this approach.

Two basic evaluation measures in TC are recall and precision, which allow to measure performance over the entire list of documents classified to a category. Yet, they do not account for the quality of ranking the documents in the document list. As explained above, our scheme assumes that industrial users would prefer classified documents to be ranked according to their relevance to the category, instead of just being presented with an unordered document set.

A common way to depict the degradation of precision with the increase of recall as one traverses the ranked document list is to plot interpolated precision numbers against percentage recall. E.g. a percentage recall of 50 % is the position in the documents list at which 50 % of the relevant documents in the collection have been retrieved. The same plot expresses the notion of recall at precision ,or R@P , referring to the percentage of relevant documents which can be found at a certain precision level.

As explained above, our multi-label classification scheme requires specifying a cut-off point in the ranked document list for each category. We suggest using the R@P curve , which is an averaged recall X  X recision curve where each cut-off point corresponds to a certain precision level: 1. The precision level is presented in 1 = k intervals, where k is the desired number 2. For each category, we calculate the number of correct classifications in the 3. We then sum the number of these correct classifications for all the categories, R@P curve illustrates how much recall a classifier can provide under a certain precision level, thus corresponding with the industrial requirement of maintaining a certain predefined precision level. We use the R@P curve as our main evaluation measure in Sect. 5 . For completeness we also report the Mean Average Precision (MAP) values of the evaluated methods. 5.2 Evaluation of the scoring method In this section we evaluate our scoring method described in Sect. 4.2 . We note that in this evaluation we intend to assess the quality of our suggested scoring method per se, without considering our novel classification scheme. We thus compare the following algorithms:
Single-label combined The state-of-the-art single-label Combined method described in Sect. 2.1.1 , which was used in Barak et al. ( 2009 ) for the unsupervised categorization step. The single-label combined method classifies each document to the category with the highest similarity to the document.
Multi-label combined An adaptation of the single-label combined method for unsupervised multi-label classification. This method calculates classification scores according to the Combined scoring method of Barak et al. ( 2009 ). It then creates a ranked document list for each category by sorting the documents by their classification score and leaving only those with non-zero score, i.e. leaving the documents that contained at least one mention of the category X  X  referring terms.
Our method The algorithm follows the same classification scheme as in the multi-label combined method above, utilizing our scoring method instead of the Combined method of Barak et al. ( 2009 ).

Figure 2 presents the resulting R@P average curves. It shows that using our scoring method consistently outperforms other methods by several points. The recall of our method is higher, since we utilized a new additional statistical LR resource. In addition, the accuracy of the statistical LR resource with the additional dice-based context model yielded an increase in precision as well. Comparison between the single-label combined curve with the multi-label curves shows that single-label classification is limited and even a naive multi-label classification performs better.
Comparison between the two multi-label curves shows that integrating the statistical knowledge (dice-based context model and lexical resource) helped to improve the performance, showing an average recall improvement of 6.8 points.
To complete the evaluation, in Table 8 we present the MAP values of the compared algorithms. We see that ranking by our score achieves considerably the two-sided Wilcoxon signed rank sum test (Wilcoxon 1945 ). 8 5.2.1 Contribution of the method X  X  components As described in Sect. 4.2 , our scoring method adds two components to the Combined scoring method of Barak et al. ( 2009 ), namely a co-occurrence dice-based context model and a dice-based lexical references resource. In order to assess the contribution of each component individually we performed ablation tests. Furthermore, we compared the existing union-based resource combination scheme to the scheme based on geometric mean, which we presented in Sect. 4.2 . The results are shown in Fig. 3 .
 Context models The results in Fig. 3 show that combining different context models is beneficial, since each of them has some additional information relative to the others. We also see that our dice-based context model shows quite similar behavior to the state-of-the-art LSA model, except for one cut-off point, of 0.7 precision, where the LSA performs better.

The MAP of the LSA model is 0.56, while the dice-based model X  X  MAP is 0.55 context model is comparable to the much more complex LSA-based model. We note that in addition to being simpler and less computationally expensive, dice-base model is easy to analyze, which is advantageous both in industrial and academic settings.
 Resource combination scheme Comparison of the two different resource combi-nation schemes shows that using geometric mean (GM) is no better than the resources union suggested by Barak et al. ( 2009 ). We believe that the rationale behind the GM combination scheme is valid, although the suggested approach was ineffective in the IMDB case. Indeed, multiplication of vectors is one of the simple methods to compose their meanings, which does not always work well. We assume additional methods are needed to effectively reduce the influence of poor expansions. One of the possible directions could be investigating compositional models in the spirit of Mitchell and Lapata ( 2010 ), Baroni and Zamparelli ( 2010 ), Grefenstette and Sadrzadeh ( 2011 ) and Socher et al. ( 2012 ).
 Lexical resources The contribution of the Dice LR resource is important when recall is considered. Starting from the 0.6 precision cut-off point, the recall of our scoring method increases due to the Dice LR resource.
 In Fig. 4 we present a more detailed comparison of contribution of each of the LR resources used in our improved scoring method, namely WordNet, Wikipedia all resources. The context models were part of the the system in all of these ablation tests.

Analyzing our results, we can conclude that utilizing LR resources is valuable for user-generated data as well. Relying solely on the context models, similarly to Gliozzo et al. ( 2009 ), is obviously not enough for the TC task.

Wikipedia has been a potentially good resource for LR, providing the typical knowledge found in an encyclopedia. However, there is an overlap between the Dice and Wikipedia resources. When we use the Dice and WordNet resources, the precision is low.

WordNet is a different type of resource that has more impact on the classification results. Typical knowledge that can be found in a dictionary tends to occur less frequently in co-occurrence statistics collected from a corpus. WordNet expansions improve the ranking of the documents and increase the recall, maintaining a high rate of precision.

Dice expansion resource increases the recall where the precision is below 0.6, but when we use only the Dice LR resource, the recall at the same range is not maximized. The performance of the Dice LR resource is reasonable even as a single resource in cases where no additional resources are available, e.g. for non-English data. However, combining the Dice LR resource with other external resources, such as WordNet, leads to better performance. 5.3 Evaluation of the classification scheme In this section we evaluate our novel classification scheme described in Sect. 4.3 ,in which we suggested a new cut-off approach. This is different from other standard approaches in the points X  cut-off level. In our R@P average curve each cut-off point be a threshold on the classification scores or a certain percentage of the top ranked classifications (top-k %).

Figure 5 shows a comparison between our cut-off approach and the two other typical cut-off approaches. The curves for the standard approaches were obtained similarly to our approach described in Sect. 5.1 , as follows:  X  For each category, calculate the number of correct classifications that meet the  X  Sum the number of these correct classifications of all the categories.  X  To obtain the recall level for the given threshold, divide the sum by the total  X  To obtain the precision level for the given threshold, divide the sum of correct Figure 5 shows that our more expensive cut-off scheme indeed outperforms other standard cut-off schemes consistently by several percentage points. The performance of our cut-off scheme is better, since the cut-off of a certain precision level defines different thresholds over different categories, allowing to eliminate noisy assignments more carefully and thus obtain an improved output. The figure achieve 0.56 recall, and the F1 score of 0.58, which is comparable to previous results on academic datasets. 5.4 Error analysis and directions for future work The multi-label classification schemes classify each document into numerous categories. Any random mention of a referring term yields a classification with a positive score. The main issue is whether the scoring method succeeded in ranking the category X  X  documents properly. To analyze this issue and detect directions for future improvements, we sampled incorrect classifications corresponding to different levels of precision, according to our suggested classification scheme.
We were satisfied to see that at precision levels acceptable in industrial settings For example, if a video recommendation system will suggest a movie about an artist is possible since our improved document scoring complies with the lexical reference requirement formulated by Barak et al. ( 2009 ), according to which a document will obtain a non-zero score for a category only if it contains a lexical reference to the corresponding category name. Thus even irrelevant documents in the lists produced by our method for a definite category will hold a lexical reference to the category and their assignment to the given category will therefore be accountable. This characteristic is extremely important for real-life systems and it reinforces our preference to avoid the bootstrapping procedure in our setting, since bootstrapping re-assigns the documents without adhering to the lexical reference requirement as explained earlier in Sect. 3 . Overall, 44 % of the error cases in our sample can be considered  X  X  X xcusable errors X  X .

Concerning the types of errors, in general our system suffers, although to a lesser extent, from the same error types explained in Sect. 3.2.1 : passing reference, ambiguity of expanding terms, as well as incorrectness and deficit of expansions. The distribution of error types is similar to that obtained for the analysis in Sect. 3.2.1 . We thus conclude that our method reduces the overall quantity of errors of all types, since the analysis did not distinguish error types for which our method was especially effective or particularly ineffective, as compared to state of the art. Figure 6 presents the distribution of the error types in our sample.

The figure shows that over a half of the errors (37  X  19  X  56 % ) are passing references. Although 44 % were considered excusable, further reducing the number of such errors would definitely improve the performance. Our analysis showed that context models often fail to degrade erroneously high reference scores when documents contain multiple passing references, as well as ambiguous terms and matches of noisy expansions. A possible direction for improvement could be to further strengthen the context component by considering complementary sources of information and more powerful models, e.g. the Explicit Semantic Analysis (ESA) by Gabrilovich and Markovitch ( 2007 ) or Latent Dirichlet Allocation (LDA) by Blei et al. ( 2003 ).
 There is also room for improvement in the lexical reference component. The WordNet and Wikipedia LR resources include many frequent domain-inappropriate while the Dice-based LR resource suffers insignificantly from this problem. Thus, it would be reasonable to consider weighting schemes for expansion terms, in the spirit of Metzler et al. ( 2007 ), preferring domain expansions over expansions from generic lexical resources, as well as giving higher weights to category seeds which proved to be a highly reliable source of information. Preferring expansions supported by multiple resources could also be further inspected, following the rationale of our GM combination scheme, as suggested above in Section 5.2.1 . Incorporating supplementary sources of lexical references would also potentially improve the performance, as lack of expansion terms accounts for as much as 15 % of errors in our analysis.

The quality of the Dice expansions lists of category names that appear frequently in the IMDB corpus is relatively high, yet for category names which are not very frequent in the corpus the Dice resource added many noisy terms, e.g.  X  X  roommate  X  X , suggest adding documents for categories with a small number of documents in the IMDB corpus by crawling the web or using some other corpus. This line of research may be investigated further to enrich and optimize the Dice LR resource.
We examined two interesting issues considering category characteristics: (i) whether categories with more documents in the test set are ranked better than categories with fewer documents, and (ii) whether estimating the size of the category was negative. We found that the quality of the categories X  ranking does not depend on that express their specific topic meaning, such as  X  X  Football  X  X ,  X  X  Buddhism  X  X  and  X  X  Motorcycle  X  X , were ranked much better than category names that express more frequency of the category name in the whole corpus does not provide an accurate this category contains a relatively high number of documents. In this work we did not make any adjustments to the category names, but simply set the seeds to be the category names as they were given in the taxonomy. The reason for this policy was that we wanted our results to be replicable, so we did not use any prior knowledge on the resources behavior. Prior knowledge on seeds that get more effective expansions such LR resource might be helpful. By investing manual effort in tuning the category names, which would be acceptable in the industry, further improvement could be potentially achieved. 5.5 Bootstrapping results As explained above, prior art approaches (reference to the 3 papers) consist of training a supervised classifier with an initial labeled set created by a previous unsupervised step. In Sect. 3 we showed that bootstrapping performance on the IMDB corpus by the method of Barak et al. ( 2009 ) was very poor, yielding lower performance than the unsupervised classification that constitutes its input training set. Further, we advocated the hypothesis that in our scenario it would be preferable to avoid the bootstrapping step. In this section we describe the adjustments we made in order to apply the bootstrapping procedure, following the analysis in Sect. 3.2 , and further evaluate the results of the bootstrapping step.

To produce an initial labeled set to train a SVM classifier for each category, the unsupervised procedure was used by Barak et al. ( 2009 ) to assign each document in the unlabeled set to a single best-scoring category. Classification was determined independently by the classifier for each category, allowing multiple classes per document. In Sect. 4.3 we introduced our multi-label unsupervised classification scheme where each document may be classified to zero, one or more categories. Consequently, we present a different approach for producing an initial labeled set of documents using a multi-label classification scheme as our first step. Ideally, we would have wanted to set a high precision level and take the documents that meet this requirement as positive training examples for the supervised classifier. However, we lacked the human resources for manually tuning each of the categories. We therefore had to adopt a standard global cut-off scheme. We selected the percentage cut-off scheme, where a top percent of the highly ranked documents of each category is selected as positive examples for the category classifier.
Good negative examples for the supervised training process need to have two properties: (a) high confidence that these are indeed negative examples and (b) at least some of them should be close enough to the positive examples, containing passing references or sharing similar contexts. Since each document can be assigned to multiple categories, with a different score per assignment, lower-scoring classifications can potentially be a source of negative examples. Following this rationale we performed an analysis, which showed that 99 % of such low-rank classifications were inappropriate indeed. However, the documents did have terms pointing to the inappropriate category. Therefore, we sorted the list of categories to which a given document was assigned, and selected the document as a negative example for categories that were ranked lower than a certain rank in the list. The last issue was selecting the ratio between the positive and negative examples. Since we were unable to estimate the real portion of a category in the corpus, we selected the same number of examples for both the negatives and the positives for each category. We manually tuned both of our parameters based on experiments on the development set, represented the input examples vectors for the SVM supervised classifier in tf-idf weighted term space and used a common feature selection which removes the least common features in the corpus.

However, we did not obtain any reasonable results. Both the recall and precision were lower than 0.1. These experiment results confirm the validity of our hypothesis that when LR-based approaches are applied in an ad-hoc unsupervised scenario, it is desirable to avoid the bootstrapping step. 6 Conclusions In this work we suggested a novel TC scenario, interesting both in industry and research-wise. We identified and described the main characteristics of the scenario and constructed a representative dataset accordingly. We investigated the unsuper-vised TC approach suitable for this scenario, which uses category names as its only input and does not require additional supervision. The approach is based on the integration of reference models and context models. The proposed method integrates a new LR and a new context model into the scoring method proposed by Barak et al. ( 2009 ). We suggested a novel multi-label classification scheme with a corresponding evaluation approach, revealing a new perspective on the classification results.

Our investigation highlights the following main conclusions about the integration of the two models, about each of the new models and about the classification and evaluation scheme: 1. Our analysis confirmed the conclusion of Barak et al. ( 2009 ) that the reference 2. Utilizing statistical correlation from a target domain corpus is useful for both 3. Our dice-based context model is much simpler than the LSA context model, 4. Combining different LR resources provides a more complete perspective and 5. When a small degree of manual intervention is possible, which is usually the We detected several promising directions for improvements (see Sect. 5.4 ) and will make the dataset publicly available to encourage further research in this direction. Appendix 1 Our complete IMDB taxonomy Categories 1. Religion 1.1. Buddhism 1.2. Hinduism 1.3. Christianity 1.4. Islam 1.5. Judaism 2. Sport 2.1. Bicycle 2.2. Boxing 2.3. Fishing 2.4. Football 2.5. Golf 2.6. Hockey 2.7. Martial-arts 2.8. Athletics 2.9. Running 2.10. Shooting 2.11. Skiing 2.12. Soccer 2.13. Water sports 2.14. Tennis 2.15. Baseball 2.16. Wrestling 2.17. Basketball 2.18. Horseracing 2.19. Olympic games 3. Interests (NON-CAT) 3.1. Beach 3.2. Outdoor 3.3. Gardening 3.4. Pets 3.5. Fitness 3.6. Cookery 3.7. Fashion 3.8. Computing 3.9. Travel 3.10. Motoring 3.11. Trains 3.12. Airplanes 3.13. Ships 3.14. Radio 3.15. Business 3.16. Nature 3.17. Outer Space 3.18. The environment 3.19. Showbiz 3.20. Traditions 3.21. Infants 3.22. Military 3.23. Weather 4. Arts 4.1. Cinema 4.2. Advertising 4.3. Theater 4.4. Music 4.5. Dance 5. Science 5.1. Medicine 5.2. Technology 5.3. Psychology 6. Education 7. Miscellaneous (NON-CAT) 7.1. Crime (NON-CAT) 7.2. Literature 7.3. History 7.4. Political 7.5. Social (NON-CAT) 7.6. Legal 7.7. Communism 7.8. War 7.9. Aliens 7.10. Comic-book 7.11. Journalism 7.12. Mythology Appendix 2 The annotation guidelines You are given a list of films with their plot description and a taxonomy of film categories.

The taxonomy is made up of film subject matters and is arranged in a hierarchical in all cases except when a category is only present in order to group similar subject together in which case it is marked with the text (NON-CAT) next to it.
For example: A film categorized as dealing with  X  X ars X  will also be relevant to  X  X otoring X  but not to  X  X nterests X  as it is not a category. 3. Interests (NON-CAT) Note  X  X he taxonomy is not exhaustive, you may find that there is no category in the taxonomy which accurately fits the film even though you can think of a subject matter that does. If a broader category is present choose it, otherwise choose none.
For each film, you must decide which categories (if any) out of the taxonomy are relevant to it. You can choose as many or as few categories as you see fit, or none.
Note  X  X f you find more than one category, please put each category in a separate line (insert lines if necessary).

You must categorize according to the following guidelines: 1. Is the background story prominent X  X ot just a passing reference.
 2. You must not base your decision on prior knowledge of the film, only on References
