 In implicit feedback datasets, non-interaction of a user with an item does not necessarily indicate that an item is irrel-evant for the user. Thus, evaluation measures computed on the observed feedback may not accurately reflect per-formance on the complete data. In this paper, we discuss a missing data model for implicit feedback and propose a novel evaluation measure oriented towards Top-N recommenda-tion. Our evaluation measure admits unbiased estimation under our missing data model, unlike the popular Normal-ized Discounted Cumulative Gain (NDCG) measure. We also derive an efficient algorithm to optimize the measure on the training data. We run several experiments which demonstrate the utility of our proposed measure.
 Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval] Keywords: Recommender Systems; Ranking; Evaluation
Personalized recommendation of relevant content is a com-mon task in many retrieval systems. Many collaborative filtering approaches [3] attempt to identify user preferences based on explicit feedback such as user ratings. However, im-plicit feedback [1], in which a user X  X  preferences are expressed through item interactions such as views or purchases, is of-ten more common than explicit feedback.

In both explicit and implicit feedback systems, the pres-ence of missing data poses a challenge to the evaluation of a recommendation system. In explicit feedback datasets, rat-ings can be Missing-not-at-Random (MNAR)[8], so systems trained only on observed ratings may give biased predic-tions. On the other hand, in implicit feedback datasets, non-interaction of a user with an item does not necessarily indi-cate that the item is irrelevant for the user. If we view un-observed but relevant user-item pairs as missing data, then measures which do not take the missing data mechanism into consideration may also exhibit bias when evaluated on the complete data.

To address the MNAR problem in explicit feedback sys-tems, a missing data model was proposed in [8], and it was shown that the Top-N recall and the Area-under-the-Top-N-curve(ATOP) measures evaluated on the observed data provided an unbiased estimate of performance on the com-plete data under the missing data model. Due to the close relationship between ATOP and AUC, surrogate loss func-tions to minimize AUC on the training data were proposed.
However, the Top-N recall is known to be difficult to maxi-mize directly, while it has been shown in several recent works ([10, 7, 4]) that optimizing for AUC may not yield the best results on performance measures such as NDCG or MAP which focus on the top of the ranking. Thus, there is a need for a performance measure which admits efficient optimiza-tion and is aligned with top-of-the-ranking metrics.
In this work, we first present a missing observation model for implicit feedback data. Next, we present a new perfor-mance measure, the Average Discounted Gain (ADG), which focuses on top-of-the-ranking performance and can be esti-mated without bias on the observed relevance data under our missing data model. Finally, we present an efficient op-timization algorithm to optimize the ADG, and evaluate our proposed method on several datasets.
In our setting, we assume that we are given a set of users U = { u 1 ,u 2  X  X  X  u m } and a set of candidate items I = { i ,i 2  X  X  X  i n } . We are also given implicit feedback in the form of a user-item relevance matrix X  X  R |U| X |I| where Accordingly, we can define the lists of relevant and irrelevant items for each user:
Due to the scarcity of resources (for example, time, money or both), users may not be able to consume all items in I in which they are interested. We therefore assume that each user has a partially observed prior relevant set P + which contains all items in I which are relevant to the user. We can then view O + u  X  X  + u as a subset of items that a user has chosen to consume (i.e., interact sufficiently with so that that the observed items O + u are a simple random sample of unknown size drawn from P + u . Equivalently, for a given user u , each item in P + u has the same (but unknown) probability of being in O + u . It may be argued that in real-world settings, such a missing data model may be unrealistic; however, se-lecting test-set items uniformly from O + u to evaluate implicit feedback methods is a common practice (e.g. [6, 1]).
Our model has close connections to the model in [8] which was originally proposed for explicit data. In fact, when the non-relevant explicit feedback is discarded, the model in [8] is mathematically equivalent to our model, albeit with a different underlying interpretation. In this section, we will present the Average Discounted Gain, a new evaluation measure which can give us an unbi-ased estimate of performance on P + u under our missing data model. We first assume that we are given user and item sets U and I , and for each user are given relevant/irrelevant item sets R + u and R  X  u . We also assume a prediction func-tion f  X  ( u,i ) parameterized by  X  which assigns a score to each user-item pair ( u,i ). In this paper, we learn a k -dimensional vector for each user and item, as well as a per-item bias: and define For a given user u , we define the rank of item i under the prediction function f  X  ( u,i ) as is the number of items (both relevant and irrelevant) with a higher predicted score than item i for a given u . Then, we can define the ADG:
Definition 1. The Average Discounted Gain (ADG) is de-fined as where R + u is the set of all relevant items to the user u . Using this definition, we define the ADG on the observed and complete data respectively:
Theorem 1. Under the assumption that O + u is a simple random sample from P + u , ADG obs is an unbiased estimator
Proof. Given a fixed  X  , each relevant item i p  X  P + associated with a discounted gain value 1 log which depends only on the rank of i p . Now note that every observed item i o  X  O + u has the same rank, and therefore the same discount value, as the corresponding item in P + Thus if O + u is a random sample from P + u , then the mean dis-counted gain (i.e. ADG) can be estimated without bias. In the next section, we show how ADG is related to the NDCG measure, and also show that under our missing data model, NDCG obs is a biased estimator of NDCG comp .
The (binary) NDCG with logarithmic discount factor for a user u can be defined as is equivalent to the NDCG with a different per-user weight-ing function; thus, we expect that the ADG will focus on the top of the ranking just like the NDCG.

Theorem 2. Under the assumption that |O + u | is a simple random sample from P + u , NDCG obs is an unbiased estimator of NDCG comp only when |O + u | = |P + u | .
 Proof. First note that then E h which is only unbiased when |O + u | = |P + u | .
 Since O + u  X  X  + u , this means that NDCG obs will always be a biased estimate of NDCG comp unless the user consumes all items in P + u . We now present an efficient algorithm to optimize the ADG for a given dataset. Since the ADG is bounded be-tween 0 and 1, instead of maximizing the ADG, we will minimize (1  X  ADG). First, we note that where It can be shown (omitted for brevity) that  X  k  X  X  1  X  X  X |I|} ,
C ( k ) = approximation
C (rank( i + ))  X  X (see [10] for a related derivation) where
V u,i + = { i  X   X  ( I\ i + ) : f  X  ( u,i  X  )  X  f  X  ( u,i + Finally, we substitute Eq. (5) into Eq. (3), to get the final optimization problem: We follow a similar procedure to [10] to derive a stochastic gradient descent algorithm, and also use the early-stopping technique in [4] to speed up the optimization process. The pseudocode for the full algorithm is given in Algorithm 1. Algorithm 1 The OPT-ADG algorithm Input: user set U , item set I , relevance sets {O + u : u  X  X } 1: repeat 2: Sample u uniformly from U , i + uniformly from O + u 3: N = 0 4: violatorFound = False 5: repeat 6: Sample i  X  uniformly from I\ i + 7: if f  X  ( u,i + )  X  f  X  ( u,i  X  ) &lt; 1 then 8: violatorFound = True; v = i  X  9: break 10: end if 11: N = N + 1 12: until N &gt; = |I| X  1  X  13: if violatorFound then 14: Take gradient step on 15: end if 16: until max iterations exceeded
One purported advantage of measures like the ATOP and the ADG is that their performance on O + u gives us an un-biased estimate of performance on P + u (henceforth, we shall refer to them as unbiased-to-missing-data (UBM) measures).
However, in practice, we cannot directly make use of this property if the ranking model to be evaluated is trained on data in O + u , since items in O + u are no longer a random sample with respect to the ranking model. Thus, we cannot extrapolate performance on P + u by measuring performance on O + u , as this is analogous to guessing test set performance based on training performance in a classification setting.
Nevertheless, we note that UBM measures still retain a nice property: if a priori , some relevant items per user are held out (i.e. not used for training by the ranking model) in disjoint test and validation sets which are both uniform random samples from O + u , then we can expect the valida-tion and test set performance to be similar regardless of the number of validation or test items held out .

Our claim is easy to prove: If we denote the relevant items in the validation set as R + u, val and the relevant items in the test set as R + u, test , then we can view both R + u, val as uniform random samples from the set R + u, val  X  R Therefore from Theorem 1, we can expect that if we eval-uate the validation and test sets with respect to any UBM measure, they would both yield unbiased estimates of perfor-Using the same logic, if we are given an observation set O and prior observation set P + u , then splitting O + u into O and O u,test , training a ranker on O u,train then evaluating O u,test on any UBM measure should yield similar perfor-mance to the same UBM measures given to {P + u \ O which are exactly the unknown but relevant items we want to predict. Here, the intuition is somewhat analagous to the generalization ability of classifiers in classical machine learn-ing settings when the validation and test set come from the same distribution.

Another desirable property of UBM methods is allowing us to make statements about the absolute performance of ranking models: For example, ADG can be interpreted as the mean discounted gain of relevant items. In contrast, we cannot predict the absolute performance of a ranking model on non-UBM measures such as NDCG on {P + u \O + u } without knowing |{P + u \O + u }| .
To evaluate the performance of our proposed measure, we conducted experiments on 3 datasets: Amazon Games , a subset of customer reviews from the Video Games category on Amazon, MovieLens 10M data, and last.FM listening data for 110000 users from the Million Song Dataset Chal-lenge hosted on Kaggle. For the Amazon Games and Movie-Lens data, we binarized the data and treated the 4 and 5 star reviews as relevant and the rest as irrelevant. For the last.FM data, we considered a song relevant to a user if the user listened to it at least 3 times, and irrelevant otherwise. Due to the sparsity of each dataset, we also densified the data by retaining the most popular items and users with the most reviews. Our datasets are summarized below.
To show the utility of optimizing for ADG over AUC, we implemented two similar matrix factorization methods, MF-ADG and MF-AUC, both with f  X  ( u,i ) defined as in Eq. (1). MF-ADG uses Algorithm 1, while MF-AUC tries to optimize the empirical AUC for each user by solving min where [  X  ] + = max(0 ,  X  ).

For each user, 10% of the relevant items were used for the validation set, while 20% of the relevant items were used for the test set, and both were uniformly sampled from O + This process was repeated four times to create four folds and the mean performance was reported. For fairness, both methods were initialized with the same random parameters, and each algorithm was run for 1000000 iterations. We reg-ularized the ` 2  X  norms of both the user and item latent vec-tors, and used a single regularization parameter  X  whose optimal value was determined by performance on the vali-dation set. The number of latent factors per item and user was fixed at 50, and for MF-ADG, the value of  X  was fixed at 100. For each method, we report three UBM measures, ATOP, ADG, Recall@10) and also two popular ranking mea-sures, Mean Average Precision (MAP) and NDCG. As there was negligible difference between ATOP and AUC in our experiments ( &lt; 0.1%) we chose to only report ATOP in the paper. As a baseline, we also computed the rank-k SVD on the user-item relevance matrix for different values of k for each dataset, but do not report the results as the perfor-mance even for the best value of k was significantly worse than both MF-ADG and MF-AUC on all metrics.
Table 1 shows the performance of both methods on all experiments. For each dataset, MF-ADG performed better than MF-AUC on all ranking measures except ATOP, which is expected because of the close link between the ATOP ob-jective and AUC optimization. This supports our claim that optimizing ADG on the training set improves performance at the top of the ranking.
 Table 1: Mean performance on all datasets across 4 folds. The number in brackets is the standard error of the mean. Methods which performed significantly better are bolded.

Table 2 shows the mean performance of both methods on the test and validation subsets of the MovieLens dataset re-spectively. As discussed in Section 5, we can see that the UBM measures (ATOP, REC@10 and ADG) show broadly consistent performance across the test and validation sets, while the MAP and NDCG measures vary greatly. Similar observations were made for the other two datasets which we have omitted due to space constraints. This supports our claim that measuring these performance measures on a val-idation set can allow us to make confident predictive state-ments about the performance of the model on the unseen test data, even when the number of test items is unknown.
Many predictive models have been proposed for both ex-plicit feedback [3] and implicit feedback [1, 6]. [5, 2] have studied the MNAR assumption in terms of model fitting with different missing data models but the evaluations do not take the missing data models into account. Further-more, to the best of our knowledge, no one has formally proposed a missing data model for implicit feedback mod-els. Our work is most closely related to [8, 9].
In this work, we proposed a missing data model for im-plicit feedback and a novel evaluation measure which allows unbiased estimation with respect to our missing data model. We also showed that ranking models trained to maximise our evaluation measure have improved performance on top-of-the-ranking measures. In future work, we plan to explore different models of missing data generation.
 M easure T est V alid Diff % T est V alid Di ff% A TOP 0 .8855 0 .8849 -0.06 0 .8821 0 .8817 -0.05 A DG 0 .1714 0 .1709 -0.29 0 .1768 0 .1768 -0.00 R EC@10 0 .0945 0 .0945 0 .00 0 .1025 0 .1030 0 .49 M AP 0 .0775 0 .0586 -24.38 0 .0858 0 .0657 -23.43 N DCG 0 .3718 0 .2957 -20.47 0 .3820 0 .3046 -20.42 Table 2: Test vs. validation performance on Movie-Lens dataset. Performance on UBM measures is consistent across test and validation sets. The authors acknowledge support from Yahoo!, Inc., the Sloan Foundation, and NSF Grants CCF-0830535 and IIS-1054960. Daryl Lim was supported by a fellowship from the Agency for Science, Technology and Research (A*STAR), Singapore. [1] Y. Hu, Y. Koren, and C. Volinsky. Collaborative [2] Y. Kim and S. Choi. Bayesian binomial mixture model [3] Y. Koren, R. M. Bell, and C. Volinsky. Matrix [4] D. Lim and G. Lanckriet. Efficient learning of [5] B. M. Marlin and R. S. Zemel. Collaborative [6] S. Rendle, C. Freudenthaler, Z. Gantner, and [7] Y. Shi, A. Karatzoglou, L. Baltrunas, M. Larson, [8] H. Steck. Training and testing of recommender [9] H. Steck. Evaluation of recommendations: [10] J. Weston, S. Bengio, and N. Usunier. Large scale
