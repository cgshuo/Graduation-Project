 We present a clustering algorithm for discovering rare yet significant recurring classes across a batch of samples in the presence of random effects. We model each sample data by an infinite mixture of Dirichlet-process Gaussian-mixture models (DPMs) with each DPM representing the noisy re-alization of its corresponding class distribution in a given sample. We introduce dependencies across multiple samples by placing a global Dirichlet process prior over individual DPMs. This hierarchical prior introduces a sharing mech-anism across samples and allows for identifying local real-izations of classes across samples. We use collapsed Gibbs sampler for inference to recover local DPMs and identify their class associations. We demonstrate the utility of the proposed algorithm, processing a flow cytometry data set containing two extremely rare cell populations, and report results that significantly outperform competing techniques.
The source code of the proposed algorithm is available on the web via the link: http://cs.iupui.edu/~dundar/ aspire.htm .
 I.5.3 [ Pattern recognition ]: Clustering X  algorithms hierarchical Dirichlet process, random effects, batch cluster-ing, recurring classes, rare classes, anomaly detection  X  Corresponding author.

Rare-class discovery is a difficult machine-learning prob-lem that occurs in various practical settings, including vi-sual surveillance and monitoring, quality control, astronomy, physics, and  X  last but certainly not least  X  life sciences. A solution to the detection of rare classes is essential for rapid identification of samples with anomalous patterns of data. In this context a normal sample can be considered to be a composition of data points each originating from a pre-defined , i.e., known, class. Unlike normal samples, anoma-lous samples contain data points originating from classes not known beforehand and thus are considered undefined . An anomalous sample may contain data points from both de-fined and undefined classes; however, the points belonging to undefined classes are usually far less frequent than those originating from predefined ones, hence the term rare classes. Predefined classes are recurring and form reproducible pat-terns (in terms of class membership proportions) across all normal samples, whereas rare classes do not necessarily re-cur, and when they do, they may form varying patterns of class proportions in each anomalous sample. Therefore, anomalous samples can be as different from each other as they are from normal samples, in terms of the specific sub-set of rare classes present and their membership proportions.
We assume that data for each sample are generated by local distributions of classes present in that sample. The total number of classes across all samples is not known. The number and the specific subset of classes locally realized in each sample are also not known. Ideally, local distribu-tions of a given class across all samples should be identical, as they are snapshots of the same underlying model. How-ever, random effects that arise from various sources affecting sample-to-sample heterogeneity cause local distributions of the same class to vary significantly from one sample to other. This makes automated matching of local distributions across samples an arduous task, which is further complicated when some classes are represented by only a small number of data points. As a result, identifying the subset of classes present in each sample and recovering true class distributions be-come impractical without modeling random effects. Thus, the main objective of this study reaches beyond clustering on a per-sample basis, but addresses the issue of grouping local clusters across multiple samples to identify subsets of classes present in each sample. This goal is achieved un-der the severe constraint imposing the model in which some classes are rare, local class distributions vary from sample to sample owing to random effects, and classes may disappear altogether from some samples.
Our research has been motivated mainly by a practical problem related to automated clinical diagnostics, involving flow cytometry (FC) data analysis.

FC is a single-cell screening, analysis, and sorting tech-nology that plays a crucial role in research and clinical im-munology, hematology, and oncology. The power of FC lies in its ability to quantify phenotypic characteristics of indi-vidual cells in a high-throughput manner. This unique ca-pability allows FC to study complex inter-cellular networks, such as the immune system as it responds to various exter-nal perturbants, including pathogens, chemical compounds (drugs), or vaccination. The cellular phenotypes are defined in FC by combinations of morphological features (measured by elastic light scatter) and abundances of surface and intra-cellular markers. Each biological sample contains multiple, functionally distinct cell types, or  X  X ell populations X  in FC vernacular. These populations form multidimensional clus-ters in the space defined by measured biological features. Although the characteristics of cell populations present in normal samples are generally known, the number of popu-lations and the proportions of cells present in them could be substantially different in anomalous (often diagnostically relevant) samples.

Given the rapid increase in FC data abundance and the unsatisfactory level of engagement from the machine-learning community, FC researchers have been organizing the annual FlowCAP (Flow Cytometry Critical Assessment of Popula-tion Identification Methods) competition in order to increase awareness and elicit help from data scientists. The problem that our study tackles is related to the rare-class classifica-tion challenge introduced in FlowCAP 2012 [2]. The data set used in this challenge was produced by multiple lab-oratories participating in the External Quality Assurance Program Oversight Laboratory (EQAPOL) project [1].

The data sets containing two biologically important rare-cell populations represent samples that were subject to sev-eral potential sources of variation, including natural biologi-cal variability, different stimulation levels, and data acquisi-tion in different laboratories. The challenge provided several data sets representing three biological samples, at three lev-els of stimulation, collected in fifteen FC laboratories across the US. For the purpose of method verification the data points (individual cells analyzed by FC) belonging to two rare classes were manually labeled by experts. The remain-ing data points, considered  X  X ormal X  (and hence not inter-esting from the perspective of rare-class discovery), were all labeled as a single predefined abundant class. A typical sam-ple contained about three hundred thousand data points of which only less than one percent belonged to one of the two rare classes.

The FlowCAP challenge framed this problem in a stan-dard supervised classification setting in which the contes-tants were provided with half the samples as training data and were required to build classifier models subsequently assessed by the organizers using the remaining test data.
Although we appreciate the complexity and the difficulty of the challenge, we believe it represented the best-case sce-nario and a relatively easy problem setting. Therefore, our problem formulation presented in this report differs signif-icantly from the FlowCAP challenge description. We rec-ognize that biologically the rare classes may emerge as a result of various external perturbants, some of which may be unknown a priori . Thus, defining rare classes in an exhaustive fashion may not be realistic. In other words, defining rare classes on the basis of a small available sub-set present in the training data inevitably leads to classi-fiers that are biased towards those particular types of rare classes. Such models may not generalize well when applied to future samples in which rare classes may originate ow-ing to other biological mechanisms. Therefore, our prob-lem formulation requires that rare class discovery be per-formed in the absence of labeled data points representing these classes in the training sets. Herein, we present a non-parametric Bayesian algorithm called ASPIRE (a nomalous s ample p henotype i dentification with r andom e ffects) that identifies biologically significant phenotypes across a batch of samples in the presence of random effects.
We model each sample data by a mixture of potentially infinitely many Dirichlet-process Gaussian-mixture models (DPMs) with each individual DPM modeling the local dis-tribution of a single class. Under fairly weak assumptions and given enough components, finite mixtures of Gaussian distributions can model a given density arbitrarily closely [9]. The DPM itself is a mixture of potentially infinitely many Gaussian distributions with the actual number of mix-ture components determined directly from the data during inference. Thus, modeling local class distributions by DPMs offers the flexibility needed to accommodate class data that may arise in samples subjected to significant sources of vari-ations.

As local distributions of a given class are noisy realizations of the true class distribution we introduce a sharing mech-anism to create dependencies across DPMs associated with the same class. This is achieved by centering the base distri-butions of DPMs associated with the same class on a unique global parameter, which itself is distributed according to a higher level DPM. This global DPM not only associates lo-cal distributions of a given class with one another but also models the number and proportions of classes in each sam-ple.
 We use a collapsed Gibbs sampler to perform inference. Model learning, which is performed in a single unified pro-cess, involves three main tasks: recovering DPMs in each sample, finding class associations of DPMs, and identifying the total number of classes and their proportions in each sample.

ASPIRE is capable of identifying recurring classes (both normal and rare) in a completely unsupervised way across a batch of samples that are significantly perturbed by random effects and can characterize normal as well as anomalous states given only very weak assumptions regarding sample characteristics and origin.
Existing lines of work that can be adapted to solve the described problem can be broadly grouped into three cate-gories.

The first approach involves pooling data from all sam-ples and applying a standard clustering algorithm to cluster pooled data. Such an approach will have limited success with most real biological data sets because in the presence of random effects, local distributions belonging to one class may significantly overlap with local distributions of another class. The degree of overlap will be more severe in the pres-ence of rare classes. As a result, clusters recovered this way are unlikely to have any meaningful correspondence with the true class distributions.

The second approach involves identifying clusters on a per-sample basis and then matching local clusters across samples to recover actual class distributions. Although this technique may perform better than the first solution oper-ating with pooled data, the cluster-matching part will re-main a big challenge in the presence of random effects and rare classes. As a result, local distributions correspond-ing to larger classes may not be recovered as a whole and clusters corresponding to rare classes may be incorrectly matched with the distributions of other dominant classes, failing to indicate rare classes. FLAME (fl ow a nalysis with a utomated m ultivariate e stimation) [11] is a well-known spe-cialized FC algorithm that can be considered an example belonging to this category. FLAME fits a mixture model into each sample data with four possible choices of den-sity functions (Gaussian, skewed-Gaussian, t-distribution, skewed-t-distribution) available for individual mixture com-ponents. Local modes are pooled and then clustered to ob-tain a global template of meta-clusters. Local clusters are then assigned to these meta-clusters using graph-matching techniques. FLAME is somewhat similar to ASPIRE in the narrow sense that both techniques model individual sample data by a mixture model. However, there are significant differences in model learning. FLAME divides model learn-ing into three tasks: clustering data in individual samples, finding the optimal number of local clusters in each sample, and matching local clusters across samples to recover classes. These three tasks are performed by FLAME independently in a sequential manner. Unlike FLAME, the model learn-ing by ASPIRE is performed as a single unified process. Thus, ASPIRE can take advantage of recurring patterns of similarities across samples. For example, groups of isolated data points forming rare classes that would be ignored as outliers by clustering followed by cluster matching can be successfully identified as a rare class when these two tasks are performed simultaneously.

The third approach involves performing sample cluster-ing jointly with cluster matching. The proposed ASPIRE model, the hierarchical Dirichlet-process Gaussian-mixture model (HDPM) [5], and HDPM with random effects (HDPM-RE) [8] all belong to this category. Thanks to their non-parametric nature, the number of local clusters and classes can arbitrarily grow in all three models to better accommo-date data as needed. Both HDPM and HDPM-RE model individual sample data by a single DPM. HDPM uses the standard hierarchical Dirichlet process prior [13], assuming exact sharing of class parameters across all samples and ig-noring the presence of random effects. In the presence of ran-dom effects this assumption leads to the creation of several extraneous classes. HDPM tackles this problem by post-processing the results to combine local clusters sharing a common mode. However, such a post-processing technique may have limited success, as local clusters of a given class may not necessarily share the same mode. Unlike HDPM, HDPM-RE assumes that local clusters are noisy realizations of true class distributions and probabilistically models the deviations of the local cluster means from the mean of the corresponding class distribution.

One key limitation of HDPM-RE is the assumption that local class distributions can be effectively captured using a single Gaussian distribution. This assumption is often vio-lated in many real-world settings because different sources of variation introduced at different stages of the data col-lection and processing pipeline create class data that may not be closely approximated by a single Gaussian distribu-tion. In the case of HDPM-RE, additional local clusters of a given class are treated as if they belong to another class, thereby splitting a single class into multiple subclasses. Un-like HDPM-RE, which uses a single Gaussian distribution for each local distribution of a class, ASPIRE uses a single DPM for each local distribution, allowing for an arbitrarily large number of Gaussian distributions for modeling of lo-cal class data. Individual DPMs across samples are linked through class-specific global parameters, which are in turn distributed according to a higher-level DPM model. In ad-dition to modeling random effects, ASPIRE offers a more flexible data model that can recover class distributions with arbitrary shapes, avoiding the creation of artificial classes.
The rest of this report is organized as follows. In Section 2 we compare data models for DPM, HDPM, HDPM-RE, and ASPIRE. In Section 3 we discuss model inference for ASPIRE. In Section 4 we demonstrate the performance of ASPIRE with two experiments and compare results with three other competing techniques. In Section 5 we conclude by summarizing our contributions and offering future re-search directions.
We describe the technical details of our data model in four incremental stages. In the first stage we assume that each sample is modeled by a single DPM and that DPMs across multiple samples are independent. In the second stage we introduce dependencies across DPMs and impose exact shar-ing of mixture components corresponding to classes across samples. This is equivalent to the HDPM model. In the third stage we tackle random effects by relaxing the exact sharing of mixture components to allow local clusters to in-herit noisy realizations of classes in individual samples. This is equivalent to the HDPM-RE model. In the fourth stage we describe the proposed data model for ASPIRE, which mod-els each sample by a potentially infinite mixture of DPMs.
We denote point i in sample j by x ji  X  &lt; d , where i = { 1 ,...,n j. } and j = { 1 ,...,J } , n j. is the number of points in sample j , and J is the total number of samples. In the DPM model x ji is associated with a mixture component defined by  X  ji = {  X  ji ,  X  ji } , which in turn is generated i.i.d. from a DP as follows: G j are random probability measures distributed i.i.d. ac-cording to a DP with a base distribution G 0 and a precision parameter  X  .
  X  Using the stick-breaking construction according to [7], we can express G j as where
The points  X  jt are called the atoms of G j . Note that unlike continuous distributions, the probability of sampling the same  X  jt twice from G j is not zero and is proportional to  X  . Thus, G j is considered a discrete distribution and offers a clustering property, as the same  X  jt can be sampled for different  X  ji . In this model  X  is the parameter that controls the prior probability of assigning a point to a new cluster and thus plays a critical role in the number of clusters generated.
For the base distribution G 0 , from which  X  jt are drawn, we define a bivariate prior: where  X  0 is the prior mean and  X  0 is a scaling constant that controls the deviation of the cluster means from the prior mean. The smaller the  X  0 , the larger the separation will be between the cluster means. The parameter  X  0 is a positive definite matrix that encodes our prior belief about the expected  X , i.e., E ( X ) =  X  0 m  X  d  X  1 . The parameter m is a scalar that is negatively correlated with the degrees of freedom. In other words the larger the m , the less  X  will deviate from E ( X ), and vice versa. The plate model for independent modeling of samples using one DPM for each sample is available in Figure 1a.
In the previous section we introduced a clustering prop-erty across points in an individual sample by placing a DP prior over G j as in (2). Since G j is a discrete distribution, this prior enables sharing of the same cluster parameter by different points. When dealing with multiple samples, in addition to sharing of clusters by points formed within indi-vidual samples, a higher level of sharing occurs. Each local cluster in an individual sample is associated with a class. Thus, as we cluster points in each sample we also need to group local clusters into appropriate classes so that we can identify class associations of local clusters. This grouping can be achieved by introducing dependencies across individ-ual DPMs by placing a hierarchical DP prior over G 0 [13]. The HDPM for joint clustering and cluster matching across multiple samples becomes where  X  is the precision parameter for the higher-level DP prior and H is defined as in (4).
Using the stick-breaking construction we can express G 0 as where With this update, instead of letting G 0 be distributed ac-cording to (4) as in the independent modeling of samples we let H be distributed according to (4) and let the atoms of G 0 be distributed according to H . The distinct set of parameters  X  k corresponding to classes is sampled from H and local cluster parameters are sampled from G j . Since G j is a discrete distribution with its atoms sampled from G , and G 0 is a discrete distribution with its atoms sampled from H , each local cluster in turn inherits one of the  X   X  of classes and m j. is the number of local clusters in sample j .

Therefore, this model not only groups data points within each sample into clusters, but also groups local clusters across samples into classes. In other words, clustering and cluster matching are simultaneously addressed and depend on one another. The plate model for HDPM is available in Figure 1b.
In the standard HDPM the same parameters are inher-ited by all local realizations of a class. However, owing to the potential random effects this surmise may be unrealis-tic. Therefore, to account for random effects the HDPM-RE model [8] would be more suitable for the discovery of recur-ring classes. HDPM-RE presumes that sample data are gen-erated by noisy versions of parameters defining classes. This change can be incorporated into the data model by updating the model in (5) as follows: where G 0 j is a discrete distribution whose atoms are noisy versions of the corresponding atoms in G 0 . With this change in the model each individual sample now inherits different noisy realizations of global parameters. The plate model for HDPM-RE is available in Figure 1c.
Both HDPM and HDPM-RE assumes that local distri-butions of classes can be closely approximated by a sin-gle Gaussian distribution. This assumption is often quite restrictive for many practical settings, as local class data, which are produced subject to random effects, may emerge in the form of skewed as well as multi-mode distributions. As a result, fitting a single Gaussian distribution for local class distributions creates artificial classes that may not be easily distinguished from other significant classes.
ASPIRE uses a potentially infinite mixture of DPMs to model each sample data where individual DPMs are linked together through a hierarchical DP prior. This hierarchi-cal prior not only identifies local DPMs associated with the same class through sharing of a global parameter but also models the specific subset of classes present and their pro-portions in each sample.

We update our indexing notation from previous sections to introduce an additional subscript k to account for multiple DPMs in each sample. We denote point i of class k in sample j by x jki  X &lt; d , where i = { 1 ,...,n jk. } , k = { 1 ,...,K } , and j = { 1 ,...,J } , n jk. is the number of points from class k in sample j , K is the total number of classes, and J is the total number of samples. The proposed ASPIRE data model is as follows. where  X  k are global parameters each of which is associated with a different class. Individual DPMs associated with the same class inherit the same  X  k across samples. The nota-tion F  X  k indicates a distribution F centered at  X  k and de-fines class-specific base distributions of individual DPMs. Although F  X  k is same for all DPMs associated with the same class, local clusters between samples are generated i.i.d. given  X  k of corresponding DPMs. Thus, each local realization of a given class is modeled by a different DPM, allowing for the modeling of sample-to-sample variations in a systematic manner. The plate model for ASPIRE is avail-able in Figure 1d.

For the sake of simplicity and to preserve conjugacy we assume that the covariance matrices of all local clusters as-sociated with the same class are identical and limit the sus-ceptibility of local clusters to noise with their mean vectors. More specifically,  X  jki  X  G jk ,  X  jki =  X  k , and F  X  k as follows.
 Note that the covariance matrix of the base distribution F is a function of  X  k ; hence conjugacy of the model is pre-served. Conjugacy of the model is important as it enables us to implement a collapsed version of the Gibbs sampler as discussed in the next section. The scaling constant  X  adjusts the degree of deviation of local means from the cor-responding global mean. A smaller  X  1 results in a situation where local realizations of global means deviate significantly from one sample to another, suggesting significant random effects. On the other hand, a larger  X  1 value limits these deviations, resulting in few to no random effects.
Posterior inference for the proposed model in (8) can be performed by a Gibbs sampler by iteratively sampling lo-cal cluster indicator variables t = n { t jki } n jk. i =1 class indicator variables c = n { c jkt } m jk t =1 K k =1 state of all other variables. Including  X  in the Gibbs sampler significantly increases the size of the state space and severely retards the convergence of the Gibbs sampler to the equilib-rium distribution. Fortunately, our model uses a conjugate pair of H and p (  X |  X  jkt ), which allows us to integrate out  X  analytically. Thus, in the following we omit the discussion of sampling of  X  and describe sampling for only t and c .
When sampling the local cluster indicator variable t jki for x jki we first remove x jki from its current cluster and update the corresponding predictive distribution p ( x jki | D .c Then, we evaluate the likelihood of x jki  X  X  belonging to an ex-isting cluster by evaluating p ( x jki | D .c jkt . ,D jkt clusters in sample j , and its likelihood of originating from a new cluster by evaluating the predictive distribution for an empty cluster, i.e., p ( x jki ). Finally, we sample t on the normalized values of the product of prior probabil-ities and the corresponding likelihood values. This can be expressed by the following equation: where t  X  jki is the set of all cluster indicator variables, ex-cluding the one for point i of class k in sample j , D ... the set of all points across all samples, D .c jkt . denotes the subset of points sharing class c jkt across all samples, D denotes the subset of points in sample j belonging to cluster t of class k , excluding point i , m jk is the number of clusters associated with class k in sample j , and n  X  jki jkt is the number of data points in cluster t of class k in sample j , excluding point i .
 As we model local clusters by Gaussian distributions with Gaussian and inverted Wishart priors defined over their mean vectors and covariance matrices, respectively, the predictive distribution p ( x jki | D .c jkt . ,D jkt ) turns out to be in the form of a Student-t distribution, the derivation of which is pro-vided in Section 3.1.

When sampling the class indicator variable c jkt for cluster t of class k in sample j we remove points D jkt from D .c and update the parameters of the predictive distribution for class c jkt . Then, we evaluate the joint likelihood of cell data in D jkt for existing classes as well as for a new class. Finally, we sample c jkt based on the normalized values of the product of prior probabilities of classes and the corresponding joint likelihood values. This can be expressed by the following formula: where D  X  jkt .k. denotes the subset of points across all samples associated with class k , excluding points in cluster t in sam-ple j . The predictive distribution p ( x | D .k. ) is also obtained in the form of a Student-t distribution, and can be readily obtained from p ( x jki | D .c jkt . ,D jkt ) by setting D set. The details are provided in Section 3.1.

Sampling both t jki and c jkt requires evaluating the predic-tive distribution for a new, i.e., an empty, cluster. The pre-dictive distribution for a new cluster is denoted by p ( x in (10) and (11). This distribution can be obtained from p ( x | D .k. ) by setting D .k. an empty set. The details are also provided in Section 3.1.

During a single run of the ASPIRE algorithm one sweep of the Gibbs sampler involves two main iterative loops. In the first loop t jki are sampled for all points across all samples. In the second loop c jkt are sampled for all local clusters across all samples. The Gibbs sampler is run for a thousand sweeps and the state with the maximum Gibbs likelihood is recorded for final evaluation.
ASPIRE uses the following data model: For the above model the posterior predictive distribution of a local cluster can be derived by evaluating the following integral.
To evaluate the integral in (13) we need the posterior dis-tribution of the parameters p (  X  jkt ,  X  k | D .c jkt . ,D can be expressed as follows using the Bayes theorem. where  X  x jkt and A jkt are the sample mean and the scatter matrix for cluster t of class k in sample j , respectively and A k is the scatter matrix of class k . These statistics are defined as in (15). In order to evaluate (14) we first need to obtain where p ( X  k | A k ) where Once the distributions in (17)-(20) are substituted into (14) a closed-form expression for p (  X  jkt ,  X  k | D .c jkt . obtained. When we substitute this solution into (13) we Student-t distribution with three parameters. The location vector ( X   X  ), the scale matrix (  X   X ), and the de-grees of freedom ( v ) are given below. Location vector: Scale matrix:  X   X  = Degrees of freedom: The predictive distribution of a class can be readily obtained from p ( x jki | D .c jkt ,D jkt ) by setting D jkt an empty set. This is equivalent to dropping terms related to local clusters in equations (24), (25), and (26). Finally, the predictive distri-bution of an empty cluster can be obtained from p ( x jki by setting D .k. an empty set. This is equivalent to dropping terms in p ( x jki | D .k. ) related to classes.
We report results of experiments performed with two dif-ferent data sets. The first experiment demonstrated the functionality of the algorithms tested using simulated data, while the second experiment utilized real FC data.

Aside from the proposed ASPIRE algorithm, three other techniques were considered: DPM, HDPM, and HDPM-RE. In Section 1.3 we described three different approaches to the clustering problem set forth in this study. The first method uses standard clustering algorithms applied to pooled data, the second approach performs clustering and cluster match-ing in a sequential way, and the third performs clustering jointly with cluster matching. Among the three benchmark techniques DPM belongs to the first category; HDPM and HDPM-RE along with ASPIRE belong to the third category. We chose the well-known FC algorithm FLAME to represent the second category. Unfortunately the implementation of FLAME available through GenePattern [12] produced er-rors during processing of many of the samples in the two data sets, so we were forced to exclude FLAME from this analysis. For HDPM we used the software provided by the authors in [5]. For the other three algorithms we used our own implementations. Each algorithm is run for a thousand sweeps, and the state with the best likelihood is recorded for subsequent analysis.

The F 1 score is used as the performance measure for com-paring performances of these four techniques. As one-to-many matchings are expected between true and recovered classes, the F 1 score for each class is computed as the max-imum of the F 1 scores for all recovered classes, similar to [3].
 Table 1: F1 scores achieved and the number of classes recovered by each of the four techniques on the artificial data set.
 Method 1 (98.7%) 2 (0.3%) 3 (1%) # Classes DPM 1.00 0.75 0.56 5 HDPM 0.84 0.74 0.66 11 HDPM-RE 0.68 0.94 0.85 7 ASPIRE 1.00 1.00 0.90 3 Table 2: Number of points available from three classes in the FC data set before and after subsam-pling. Numbers in parentheses indicate percentage of the total number of points in the corresponding set.
We generated twenty samples, each with five thousand data points in a two-dimensional feature space, using the model in (8) and the following values of the model param-eters:  X  0 = 0 . 01,  X  1 = 0 . 2, m = 20,  X  0 = [0 0] T ,  X   X  = 0 . 2,  X  = 0 . 2, where I denotes the identity matrix. After all data points were sampled, three classes were produced by this model with overall class proportions of 0.987, 0.003, and 0.01, which indicates that two of the three recurring classes can be considered rare. For the pooled data, distributions of local clusters and the true values of the global parameters, i.e.,  X  k , are shown in Fig. 2a by dashed and solid contours, respectively. The ellipses correspond to data distributions that are at most four standard deviations from the mean. Individual data points are shown by black dots.
 We ran all four techniques (ASPIRE, DPM, HDPM, and HDPM-RE) on this data set and plotted contours repre-senting recovered classes in Figures 2b, 2c, 2d, and 2e, re-spectively. F1 scores obtained for each class and numbers of classes recovered by all four techniques are included in Table 1. Results suggest that ASPIRE not only correctly predicts the true number of classes but also estimates global param-eters with almost no bias, which in turn produces almost perfect F1 scores for each class. DPM produces a reason-able number of classes but estimates global parameters with a large bias. HDPM fails to consistently match local clus-ters across samples and substantially overpredicts the actual number of classes. HDPM-RE performs better compared to DPM and HDPM but generates several artificial classes, a direct result of modeling local class data by a single Gaus-sian distribution.
We evaluated the performance of ASPIRE in discovering rare classes with a FC data set used in the FlowCAP 2012 competition [2]. The data set contained FC measurements of multiple aliquots of three biological samples exposed to three Table 3: F 1 scores achieved and the number of classes recovered by each of the four techniques on the entire FC data set. Results for ASPIRE are av-erages over ten repetitions. Numbers in parenthesis indicate standard deviations.
 tuning. The parameter  X  1 models the deviation of cluster means from their corresponding class mean in the genera-tive model. Thus, increasing  X  1 while  X  0 and s are fixed potentially increases the number of classes generated. The parameter  X  0 models the deviation of cluster means from the prior mean in the generative model. Thus, increasing  X  0 while  X  1 and s are fixed potentially increases the num-ber of clusters generated. The parameter s models the ex-pected size of clusters. Increasing s potentially increases the number of clusters generated. These three parameters were coarsely tuned using a generic 5-parameter peripheral-blood immunophenotyping data set previously collected and ana-lyzed in our lab as part of an earlier study without retuning them for the FC data used in this experiment. The following values were used:  X  0 = 0 . 05,  X  1 = 0 . 1, s = 10.
F 1 scores computed for all three classes are shown in Ta-ble 3. Results for ASPIRE are averages of ten repetitions. As the run time for ten repetitions of the other algorithms would take on the order of weeks, we included results of a single run for these algorithms. Results in Table 3 fa-vor methods modeling random effects (HDPM-RE and AS-PIRE) over those that do not (DPM and HDPM) in terms of higher F 1 scores achieved for both rare classes. Between techniques that model random effects ASPIRE significantly outperforms HDPM-RE in terms of producing a more real-istic number of classes and higher F 1 scores for all classes. ASPIRE models local realizations of classes by an infinite mixture of Gaussians, which allows for associating multi-ple clusters with individual classes during inference. The other three techniques use a single Gaussian distribution to model local realization of classes. If a local distribution of a class cannot be effectively modeled by a single Gaussian distribution, these techniques tend to produce multiple local clusters all of which are assigned to a distinct class. As a result ASPIRE tends to generate a fewer number of classes and achieves higher F 1 scores compared to the other three techniques.

We also compared ASPIRE with a supervised classifier to find out how F 1 scores would improve if a subset of the labeled data were to be used during training. We used all samples belonging to one of the biological samples for train-ing and sequestered all samples for the other two biological samples for testing. The support vector machine toolbox in [6] was used to train and test a supervised classifier on this data. Parameters of this classifier are extensively tuned to optimize test performance. These results along with the re-sults obtained by ASPIRE on the test data are shown in Ta-ble 4. Results suggest that ASPIRE can predict rare classes Table 4: F 1 scores achieved by ASPIRE and SVM on the test portion of the FC data set. Results are av-erages over ten repetitions. Numbers in parenthesis indicate standard deviations.
 with F 1 scores comparable to those of a supervised classi-fier without using any labeled data. The F 1 score achieved by ASPIRE for the normal class is worse than that of the supervised classifier, mainly because the normal class is a combination of multiple uninteresting subclasses for which ASPIRE produces multiple classes to more effectively model the underlying class distribution. However, we do not be-lieve this is a major limitation, as in most practical set-tings labeled data are present for normal classes as these are classes that are known and predefined. On the other hand, for rare classes, labeled data may not exist because rare classes are usually not known a priori and cannot be predefined. Under such circumstances training a supervised classifier that requires labeled data for all classes may not be very realistic. On the other hand, ASPIRE can cluster data in a fully unsupervised manner and with the help of a limited amount of labeled data from normal classes results can be post-processed to distinguish unknown classes from known ones.

For ASPIRE, one sweep of the Gibbs sampler involves two main iterative loops. In the first loop, cluster indicator vari-ables are sampled for all data points across all samples. In the second loop, class indicator variables are sampled for all local clusters across all samples. As the first loop iterates over all points across all samples it is usually more com-putationally expensive than the second loop. Fortunately, during the sampling of the cluster indicator variables class parameters are fixed. This allows us to sample cluster indi-cator variables independently for each sample during a sin-gle sweep and leads to improvement in processing time on multi-processor machines. The actual run time for ASPIRE to process the FC data set containing 1.9 million points is about five and eleven hours with and without parallelization, respectively, on an eight-core workstation. The reduction in the overall computational time is not proportional to the number of processors, as the computational gain by paral-lelizing the first loop will be limited after a certain point by the computational time of the second loop.
We introduced ASPIRE as a new method for discover-ing recurring yet significant rare classes in the presence of random effects and showed experimental results that clearly favor ASPIRE over other benchmark techniques. We believe that ability to recover rare classes in FC data sets obtained in fifteen different laboratories convincingly demonstrates that automated identification of anomalous samples in research or diagnostic settings is indeed feasible.

Labeled information about normal, i.e., known classes, can be directly incorporated into the learning process by adopting a restricted Gibbs sampler scheme similar to the one introduced in [4]. Our research was mainly driven by a rare-class discovery problem in a clinical setting. However, ASPIRE is a general clustering technique that can be used in other disciplines to discover classes with recurring nature irrespective of whether they are rare or normal. ASPIRE can also be utilized for problems involving the detection of group anomalies [10, 14].

ASPIRE is implemented in C ++ . The source code is avail-able on the web via the link http://cs.iupui.edu/~dundar/ aspire.htm .
This research was sponsored by the National Science Foun-dation (NSF) under Grant Number IIS-1252648 (CAREER), by the National Institute of Biomedical Imaging and Bio-engineering (NIBIB) under Grant Number 5R21EB015707, and by the PhRMA Foundation (2012 Research Starter Grant in Informatics). The content is solely the responsibility of the authors and does not represent the official views of NSF, NIBIB or PhRMA. [1] External quality assurance program oversight [2] FlowCAP -flow cytometry: Critical assessment of [3] N. Aghaeepour, G. Finak, FlowCAP Consortium, [4] F. Akova, Y. Qi, B. Rajwa, and M. Dundar.
 [5] A. J. Cron, C. Gouttefangeas, J. Frelinger, L. Lin, [6] N. Djuric, L. Lan, S. Vucetic, and Z. Wang.
 [7] H. Ishwaran and L. F. James. Gibbs sampling methods [8] S. Kim and P. Smyth. Hierarchical Dirichlet processes [9] G. McLachlan and D. Peel. Finite Mixture Models . [10] K. Muandet and B. Sch  X  olkopf. One-class support [11] S. Pyne, X. Hu, K. Wang, E. Rossin, T.-I. Lin, L. M. [12] M. Reich, T. Liefeld, J. Gould, J. Lerner, T. P., and [13] Y. Teh, M. Jordan, M. Beal, and D. Blei. Hierarchical [14] L. Xiong, B. Poczos, and J. Schneider. Group anomaly
