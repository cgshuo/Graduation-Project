 Active learning is a paradigm in which the learner has the ability to sequentially select examples for labeling. The selection process can take advantage of information gained from previously ob-a paradigm in which the learner has no control over the labeled examples it is given. In machine learning, active learning has been a topic of intense interest. In certain machine learning problems it has been shown that active learning algorithms perform much better than passive learning, with superior convergence bounds (see [1, 4] and references therein) and/or superior empirical perfor-machines and humans.
 To our knowledge, no previous work has attempted to quantify human active learning performance often cast the learner as a passive learner, who observes some object (typically represented as a determine how the label should generalize. Anyone who has ever interacted with a three-year-old observe their environment, and certainly they pay attention when adults label objects for them  X  but they also ask a lot of questions. Active querying provides children with information that they would otherwise be less likely to encounter through passive observation; and so, presumably, such active querying has important implications for category learning.
 Early research in human concept attainment suggested that learners do benefit from the opportunity Figure 1: The two-category learning task with boundary  X  and noise level .
 categorization has ignored active learning. Furthermore, a rich literature on decision-making and scientific inference has produced conflicting claims regarding people X  X  capacities to select optimal learning examples [7, 10, 12, 13, 14, 15, 16, 17, 20]. Most famously, people make inappropriate queries to assess simple logical hypotheses such as  X  X f p then q  X  (frequently examining q instances pessimistic views of the human ability to choose relevant queries are based on faulty task analyses; and that, when the learning task is properly construed, humans do an excellent, even optimal job of proper metric for assessing performance, there is significant opportunity to benefit from the formal descriptions characteristic of machine learning research. The current study exploits one such analy-regarding human performance: [Q1] Do humans perform better when they can select their own examples for labeling, compared to passive observation of labeled examples? [Q3] If they do not, can machine learning be used to enhance human performance? [Q4] Do the answers to these questions vary depending upon the difficulty of the learning problem? The goal of this paper is to answer these questions in a quantitative way by studying human and machine performance in one well-understood classification task. Answers to these questions have important theoretical and practical implications for our understanding of human learning and cog-nition. As previously noted, most theories of human category learning assume passive sampling of the environment. Some researchers have argued that the environment provides little information regarding the category structure of the world, and so conclude that human category learning must be subject to strong initial constraints [6, 3, 9]. If, however, human learning benefits from active querying of the environment, it is not clear that such conclusions are justified. From an applied perspective, if machines can be shown to aid human learning in certain predictable circumstances, this has clear implications for the design of intelligent tutoring systems and other machine-human hybrid applications. theoretical understanding of both active and passive machine learning, offering an ideal test-bed for assessing active learning in humans. The task is essentially a two-category learning problem (binary consists of n sample and label pairs; { ( X Y probability 1  X  and equal to the other category with probability , where 0  X  &lt; 1 / 2 . In other words, each label more probably is correct than incorrect, and is the probability of an incorrect illustrates this model. Furthermore assume that, given X At this point we have not specified how the sample locations X major difference between passive and active learning. In the passive learning setting the sample learning setting the learner can choose the sample locations in a sequential way depending on the past, that is X takes into account past experiences and proposes a new query X If = 0 , that is when there is no label noise, the optimal methodologies for passive and active between the rightmost location where a label of zero was observed and the leftmost location where a label of one was observed. If the n sample locations are (approximately) evenly distributed between 0 the optimal strategy is a deterministic binary bisection: begin by taking X Y 2 = 1 interval of possible values of  X  is halved at every observation. Therefore after n samples the error exponentially with the number of samples, is much better than passive learning, where the error can decay only polynomially.
 If &gt; 0 there is uncertainty in our label observation process and estimating  X  becomes more del-icate. Under passive learning, the maximum likelihood estimator yields the optimal rate of error convergence. Furthermore it is possible to show a performance lower bound that clarifies what is the best possible performance of any passive learning algorithm. In particular we have the following result. where  X   X  passive learning procedures. This is a so-called minimax lower bound, and gives an indication of the best achievable performance of any passive learning algorithm. That is, no passive algorithm can learn more rapidly. This bound can be easily shown using Theorem 2.2 of [18], and the performance of the maximum likelihood estimator is within a constant factor of (1).
 suitable for our purposes. The key idea stems from Bayesian estimation. Suppose that we have a prior probability density function p before, we start by making a query at X label Y observed. Given these facts we can update the posterior density by applying Bayes rule. In this case we obtain p sample location X X median of the posterior distribution. We continue iterating this procedure until we have collected n samples. The estimate  X   X  illustrates the procedure. Note that if = 0 then this probabilistic bisection is simply the binary bisection described above.
 The above algorithm works extremely well in practice, but it is hard to analyze. In [2] a slightly modified method was introduced, which is more amenable to analysis; the major difference involves a discretization of the possible query locations. For this method it can be shown [2] that Note that the expected estimation error decays exponentially with the number of observations, as opposed to the polynomial decay achievable using passive learning (1). This shows that the accuracy Furthermore no active (or passive) learning algorithm can have their expected error decaying faster than exponentially with the number of samples, as in (2). Equipped with the theoretical performance of passive learning (1) and active learning (2), we now describe a behavioral study designed to answer Q1-Q4 posed earlier. The experiment is essentially a human analog of the abstract learning problem described in the previous section in which the learner tries to find the boundary between two classes defined along a single dimension, a setting used to demonstrate semi-supervised learning behavior in humans in our previous work [21]. We are particularly interested in comparing three distinct conditions: select the queries, and is instead presented sequentially with examples { X at random from [0 , 1] , and their noisy labels { Y |  X   X  n  X   X  | of learning from passive observation of random samples, their boundary estimates should approach the true boundary with this polynomial rate too.
 Condition  X  X uman-Active X  . This is the active learning condition where the human subject, at iteration i , selects a query X She then receives a subsequent noisy label Y examples by selecting informative queries then the rate of error decrease should be exponential, following (2).
 Condition  X  X achine-Yoked X  . This is a hybrid human-machine-learning condition in which the human passively observes samples selected by the active learning algorithm in [2], observes the noisy label generated in response to each query, and is regularly asked to guess, without feedback, where the boundary is  X  as though the machine is teaching the human. It is motivated by question Q3: Can machine learning assist human category learning? Materials. Each sample X is a novel artificial 3D shape displayed to the subject on a computer screen. The shapes change with X smoothly in several aspects simultaneously. Figure 3 shows a few shapes and their X values. A difference of 0.06 in X value corresponds roughly to the psychological  X  X ust Noticeable Difference X  determined by a pilot study. For implementation reasons our shapes are discretized to a resolution of about 0.003 in X values, beyond which the visual difference is too small to be of interest.
 alien snakes (category zero), and smooth eggs ( X close to 1) most likely hatch alien birds (category the egg shape (decision boundary) at which it switches from most likely snakes to most likely birds. Human-Active (14 subjects), Machine-Yoked (6 subjects). Machine-Yoked receives approximately this condition. In all conditions, subjects were explicitly informed of the one dimensional nature of 0.2, 0.4 with order determined randomly for each participant. For each session and participant the true decision boundary  X  was randomly set in [1 / 16 , 15 / 16] to avoid dependencies on the location of the true boundary. The experiment thus involved one between-subject factor (learning condition) and one within-subjects factor (noise level ).
 At iteration i of the learning task, a single shape at X wheel to scroll through the range of shapes. Once the participant found the shape she wished to query ( X to the noisy label), followed by a  X  X ontinue X  button to move on to the next query. In the Random and Machine-Yoked conditions, each sample X intervention, and a short animation was displayed showing shapes smoothly transitioning from X to
X i +1 in order to match the visual experience in the Human-Active condition. Once the transition was completed, the outcome (label) for X button to observe the next sample and outcome. In all conditions, the computer generated the noisy label Y either a snake picture ( Y to their estimate of the boundary location rather than simply searching locally around the current shape (total 15 re-starts over 45 queries; 45 re-starts would be too tedious for the subjects).  X  X oundary queries, X  the computer began by displaying the shape at X = 1 / 2 , and the participant used the mouse wheel to change the shape until it matched her current best guess about the boundary shape. Once satisfied, she clicked a  X  X ubmit boundary X  button. We thus collect  X   X  for each session. These boundary estimates allowed us to compute mean (across subjects) human estimation errors |  X   X  compare these means (i) across the different experimental conditions and (ii) to the theoretical pre-dictions in (1)(2). Figure 4 shows, for each condition and noise level, how every participant X  X  boundary guesses ap-proach the true boundary  X  . Qualitatively, human active learning (Human-Active) appears better than passive learning (Random) because the curves are more concentrated around zero. Machine-assisted human learning (Machine-Yoked) seems even better. As the task becomes harder (larger noise ), performance suffers in all conditions, though less so for the Machine-Yoked learners. These conclusions are further supported by our quantitative analysis below.
 It is worth noting that the behavior of a few participants stand out in Figure 4. For example, one subject X  X  boundary guesses shift considerably within a session, resulting in a rather zigzagged curve in (Human-Active, = 0 . 1 ). All participants, however, perform relatively well in at least some noise settings, suggesting that they took the experiment seriously. Any strange-looking behavior likely reflect genuine difficulties in the task, and for this reason we have not removed any apparent outliers in the following analyses. We now answer questions Q1 X  X 4 raised in Section 1. [Q1] Do humans perform better when they can actively select samples for labeling compared to passive observation of randomly-selected samples? [A1] Yes  X  at least for low noise levels. For higher noise the two are similar.
 To support our answer, we show that the human estimation error |  X   X  Active condition than Random condition. This is plotted in Figure 5, with  X  1 standard error bars. When noise is low, the Human-Active curve is well below the Random curve throughout the session. between human boundary guess and true boundary  X   X  ing (Human-Active) is better than passive learning (Random), and machine-assisted human learning (Machine-Yoked) is even better. As the task becomes harder (larger noise ), all performances suffer. Figure 5: Human estimate error |  X   X  iteration n . The error bars are  X  1 standard error. Human-Active is better than Random when noise is low; Machine-Yoked is better than Human-Active when noise is high.
 That is, with active learning the subjects quickly come up with better guesses and maintain this ad-vantage till the end. Human-Active performance deteriorates with higher noise levels, however, and at the highest noise levels is appears indistinguishable from performance in the Random condition. [Q2] Can humans achieve the full benefit of active learning suggested by learning theory? [A2] Human active learning does have exponential convergence, but with slower decay con-stants than the upper bound in (2) . Human passive learning, on the other hand, sometimes does not even achieve polynomial convergence as predicted in (1) , and in no condition does the rate approach optimal performance.
  X  =  X  1 / 2 log 1 / 2 + p (1  X  ) is determined by the noise level . The larger the decay con-slope  X   X  . To determine whether human error decays exponentially as predicted, and with a compa-as Figure 6 (Upper) shows it to be. This exponential decay of error offers further evidence that hu-man active learning exceeds passive learning performance, where error can only decay polynomially (Figure 6, Lower). The speed (decay constant) of the exponential decay in human active learning is, Figure 6: (Upper) Human active learning decreases error exponentially, as indicated by the linear distribution of log( |  X   X  the Random condition is slower than O (1 /n ) , since the slopes are shallower than -1 on log( |  X   X  (the y -axis) versus log( n ) (the x -axis). statistical learning theory for lower noise levels.
 active learning. For comparison, we computed the decay constant in the theoretical bound. Table 1 compares these decay constants under different noise levels. It is clear that human active learning X  X  error decays at a slower rate, especially when the noise is low.
 For passive learning, the minimax lower bound (1) has a polynomial decay of O (1 /n ) , which is a line with slope -1 on a plot of log( |  X   X  log-log plot from human passive learning in the Random condition does seem to fit a line, but the at a much lower rate than formal analysis suggests is possible. [Q3] Can machine learning be used to enhance human learning? [A3] Apparently in high noise levels  X  But what really happened? As shown in Figure 5, the Machine-Yoked curve is no different than Human-Active in low noise levels, but substantially better in high noise levels. It is important to remember that Machine-Yoked is human performance, not that of the machine learning algorithm. The results seem to indicate that humans can utilize the training data chosen by a machine active learning algorithm to enhance their performance in settings where humans are not generally performing well. Upon closer inspection, however, we noticed that almost all subjects in the Machine-Yoked condition used the following strategy. They quickly learned that the computer was generating training examples that soon con-verge to the true boundary. They then simply placed their boundary guess at (or near) the latest training example generated by the machine. This  X  X emorizing X  strategy worked very well in our Instead, they likely learned to trust and depend upon the computer. In view of this, we consider Q3 inconclusive, but hope these observations provoke thoughts on how to actually improve human learning. [Q4] Do answers to the above questions depend upon the difficulty of the learning task? [A4] One form of difficulty, the label noise level , has profound effects on human learning. Specifically, the advantage of active learning diminishes with noise; and at high noise levels active learning arguably has no advantage over passive learning for humans in this setting. Formal analysis suggests that the advantage of active over passive sampling should diminish with increasing noise; but it also suggests that some benefit to active sampling should always be obtained. An important goal for future research, then, is to understand why human performance is so adversely affected by noise. We have conducted behavioral experiments to compare active versus passive learning by humans in a simple classification task, and compared human performance to that predicted by statistical learning theory. In short, humans are able to actively select queries and use them to achieve faster category learning; but the advantages of active-learning diminish under higher noise conditions and do not approach theoretical bounds. One important conclusion from this work is that passive learning may not be a very good model for how human beings learn to categorize. Our research also raises several passive and active performance has been formally characterized. The drawback is that the task is not especially natural. In future work we plan to extend the current approach to learning situations more similar to those faced by people in their day-to-day lives.
 Acknowledgments: This work is supported in part by the Wisconsin Alumni Research Foundation, and NSF Grant 0745423 from Developmental Learning Sciences.

