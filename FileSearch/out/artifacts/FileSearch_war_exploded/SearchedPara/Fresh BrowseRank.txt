 In the last years, a lot of attention was attracted by the prob-lem of page authority computation based on user browsing behavior. However, the proposed methods have a number of limitations. In particular, they run on a single snapshot of a user browsing graph ignoring substantially dynamic nature of user browsing activity, which makes such methods recency unaware. This paper proposes a new method for comput-ing page importance, referred to as Fresh BrowseRank. The score of a page by our algorithm equals to the weight in a stationary distribution of a flexible random walk, which is controlled by recency-sensitive weights of vertices and edges. Our method generalizes some previous approaches, provides better capability for capturing the dynamics of the Web and users behavior, and overcomes essential limitations of BrowseRank. The experimental results demonstrate that our method enables to achieve more relevant and fresh rank-ing results than the classic BrowseRank.
 Categories and Subject Descriptions: H.3.3 [Informa-tion Storage and Retrieval]: Information Search and Re-trieval General Terms: Algorithms, Experimentation, Performance Keywords: Page authority, BrowseRank, freshness, web search, learning
Web page authority scores are regarded as one of the most useful features for ranking algorithms. There are many ap-proaches to evaluation of page importance. Among them there are those which analyze the user browsing histories. Particularly, BrowseRank algorithm [1] measures the page X  X  importance by its probability in the stationary distribution of a continuous-time Markov process on the user browsing graph. Despite undeniable advantages of BrowseRank and its generalizations (see Section 2), these algorithms miss the temporal aspect of the Web. Newer pages are probably more relevant to recency sensitive queries than the older ones, and therefore the temporal aspect of document relevance relates to the to the ability of a browsing based page authority measure to distinguish between relevant and irrelevant doc-uments. With regard to web pages devoted to periodical events such as news, government law issues, sport reviews or conference homepages, a surfer looks for content likely related to newer facts and events. By this reason the prob-lem of constructing recency sensitive (or  X  X resh X ) BrowseR-ank should be of great interest for commercial web search engines.

In this work we present the algorithm called Fresh BrowseR-ank . We make our algorithm fresh-aware by weighting pages according to their freshness. In order to show the effective-ness of Fresh BrowseRank, we compare it with the classic BrowseRank on a large sample of user browsing graph and demonstrate that our algorithm is better performing than BrowseRank. To the best of our knowledge the proposed algorithm is the first fresh-aware ranking algorithm based on the user browsing history.

The remainder of the paper is organized as follows. Sec-tion 2 contains a short review of related approaches to mea-suring page importance. In Section 3 we describe a frame-work necessary to introduce our algorithm. We define Fresh BrowseRank in Section 4. In Section 5 we introduce the method we employ for tuning of parameters of our algo-rithm. The experimental results are reported in Section 6. In Section 7 we discuss potential applications and future work.
A widely used approach to page importance computation is analysis of the web as a graph of pages connected with hyperlinks. The most acknowledged link-based method is PageRank (Page et al. [2]). According to this algorithm the score of a page p equals its weight in the stationary dis-tribution of Discrete-Time Markov process which models a random walk on the hyperlink graph. Such approaches of computing web page authority have some weaknesses. Par-ticulary, a significant portion of pages and hyperlinks are not reliable and not used by real users. The other items are used with essentially different level of activity. Therefore, a random surfer modeled by using a plain hyperlink graph is not realistic. In 2008 Liu et al. [1] proposed BrowseRank, which computes the page importance using user browsing behavior data. The browsing graph contains richer informa-tion such as staying times on web pages by users, number of transitions between web pages by users. Unlike PageRank algorithm based on a discrete random work, BrowseRank exploits a Continuous-Time Markov Process driven by user browsing statistics.This model provides a more realistic view on pages X  importance. However, a proper web ranking al-gorithm should possess some properties that BrowseRank algorithm still does not. Liu et al. in [3], [4] proposed differ-ent generalizations of the algorithm which overcome some of its imperfections. In [3] the authors introduce different ways of estimating the distribution of the staying time. In [4] a Markov Skeleton Process is used to model the random walk conducted by the web surfer. They show that this framework can cover many existing algorithms (among them PageR-ank, Usage-based PageRank [5], TrustRank [6], BrowseRank Plus, MobileRank [7] as its special cases. The parameters of these algorithms can be chosen in such a way that they be-come classical BrowseRank. There are also other approaches of computing page authority using user behavior data which do not exploit random walk (see, for example, [8]).
An important problem which is not solved by all the men-tioned algorithms is the recency ranking problem . As it is shown in [9], the user browsing graph is changing signifi-cantly day by day. Therefore, pages with a high BrowseR-ank score a few days ago can be not authoritative in the present.

Yu et al. [10] were among the first to study algorithms of link analysis sensitive to the temporal aspects of the Web. The authors modified PageRank by weighting each hyperlink according to its age. Other algorithms of recency-sensitive link-based ranking were studied by Amitay et al. in [11], Berberich et al. in [12] and [13], Yang et al. in [14], Dai and Davison in [15]. The algorithm T-Fresh from the last paper generalizes the ideas from the other above-mentioned papers. However, T-Fresh has a number of shortcomings which are overcome by the algorithm APR introduced by Zhukovskii et al. in [16]. Both methods are based on graph weightings that capture freshness of pages and hyperlinks. Dai and Davison constructed two different factors of page importance that depend on pages X  freshness and links X  fresh-ness respectively. In contrast to T-fresh, the freshness mea-sure of APR depends on combined pages X  and links X  activi-ties. This approach allows to control influence of both types of activities on the assigned score. Following similar princi-ples, we propose a solution to overcome the problem of re-cency insensitivity of BrowseRank as we show in Section 6. To the best of our knowledge this paper is the first to pro-pose a recency-sensitive variation of BrowseRank.
Popular search engines suggest users to install browser toolbars that, among other useful features, monitor user browsing sessions in order to use this aggregated information later on to improve their search quality. The anonymized information describing user behavior (visited pages, times of visiting, submitted queries etc.) is stored in the brows-ing log. We define the sessions and the user X  X  browsing graph in the same way as Liu et al. [1]. Let S be a user session. We denote its pages by p 1 ( S ) ,p 2 ( S ) ,...,p For each i  X  { 1 , 2 ,...,k ( S )  X  1 } toolbar makes a record p ( S )  X  p i +1 ( S ). We call pages p i ( S ) ,p i +1 boring elements of the session S .

For each page p from the browsing log, we denote the num-ber of sessions this page starts with by s ( p ). For each pair of neighboring elements { p i ,p i +1 } from a session we denote the number of sessions containing such a pair of neighboring elements by I ( p i ,p i +1 ).
 We define the user browsing graph G = ( V,E ) as follows. The set of vertices V consists of all the web pages mentioned in the browsing log and contains one additional vertex x . The set of directed edges E represents all the ordered pairs of neighboring elements { p 1 ,p 2 } . The set E contains also additional edges from the last pages of all the sessions to the vertex x . Let  X  ( x ) = 0. For any page p such that p  X  x  X  E denote the number of sessions finished with p by I ( p,x ).

Let us remind the definition of BrowseRank. Reset proba-bility  X  ( p ) is a probability of choosing page p while we start a new browsing session. It is proportional to the number of sessions s ( p ) starting from the page p . Therefore  X  ( x ) = 0. Transition probability w ( p 1  X  p 2 ) is a probability of clicking a hyperlink p 1  X  p 2 . It equals to I ( p 1 ,p 2 ) / P The estimated staying time Q ( p ) of the page p is defined in [1]. BrowseRank of p equals Q ( p )  X  ( p ), where (the last equations hold for p = x as well),  X   X  ( p ) =  X  (1  X   X  ( x )) +  X  ( x ).

Note that the transition probabilities do not depend on the freshness of links and the user chooses old and fresh links with the same probabilities. Therefore, we make the transition probabilities fresh by weighting the links accord-ing to the freshness scores of the destination pages. In the next session we introduce this freshness measure.
Let us define the freshness measure of a page. We use the framework from [16]. Consider two moments of time  X ,T ,  X  &lt; T . We divide the interval [  X ,T ] into K parts: [ t p from V denote by t ( p ) the date when it was created. We consider vertex x to be created at moment  X  .

Let i  X  { 1 , 2 ,...,K } . Let p  X  V be a vertex created before moment t i . We define the freshness score F i ( p ) of p in the i -th period of time in the following way. 1) We define the initial value F 0 i ( p ) capturing freshness of page p and its links as follows: where a 0 ,b 0 , are nonnegative parameters (which are learned according to the method described in Section 6); n i ( p ) = 1 if the vertex p is created in the i -th period, otherwise n i 0; m i ( p ) is the number of visits of page in the i -th period. We set F 0 i ( x ) = 0. 2) Initial measure F 0 i ( p ) is spreading over vertices by out-going edges:  X  X  i ( p ) =  X  F 0 i ( p )+ where  X   X  [0 , 1], W i ( p ) is a score assigned by the  X  X ocal X  freshness measure to the vertex p in the i -th period. This measure is defined in the same way as initial measure F (the values of the parameters may be different from those of the parameters in equation (1)),
W i ( p ) = a 1 n i ( p ) + b 1 m i ( p ) + X We want to  X  X pread freshness measure X  through outgoing links from a page even if there are no fresh links among them. Therefore we increase the weight of the page by 1 (the last summand in 3) if it was created before moment t The system described in Equation 2 illustrates the influence of neighbors on the freshness measure of the page. 3) Finally, freshness measure F i is defined as follows: F =  X  F i  X  1 ( p ) +  X  X  i ( p ) . The measure decreases exponentially as time goes if there are no activities concerned with the vertex p (the parameter  X  is from (0 , 1)). Indeed F i ( p ) =  X   X  X  0 ( p ) if there were no such activities during the period [  X ,t i ].

In Equation 2 all considered vertices and edges are sup-posed to be created before the time moment t i .
 Let the freshness measure assign to page p in the graph G the score F K ( p ). We make transition probability fresh by replacing I ( p 1 ,p 2 ) with I ( p 1 ,p 2 ) F K ( p 2 Fresh transition probability  X  F ( p 1  X  p 2 ) of edge p 1 equals to I ( p 1 ,p 2 ) F K ( p 2 ) / P p : p Fresh BrowseRank of p equals Q ( p )  X  F ( p ), where  X  F ( p ) =  X   X  ( p )  X  ( p ) + (1  X   X  ) X
Parameter Description
In Table 1 we give the description of all the parameters of the algorithm.
Let f q ( p ) be the value of our feature for a page p and a query q . We make it query-dependent by combining FBR linearly with a query-dependent feature.

The training data contains a collection of queries and sets of pages V 1 q ,V 2 q ,...,V k q for each query q which are or-dered from the most relevant (or fresh) to irrelevant (or not fresh) pages. In other words, V 1 q is the set of all pages with the highest score selected from among k labels, pages from the set V k q have the lowest score. For any two pages p 1  X  V i q ,p 2  X  V j q let h ( i,j,f q ( p 2 )  X  f q ( p 1 a penalty we get if the position of the page p 1 to our ranking algorithm is higher then the position of the page p 2 but i &lt; j . The function h is a loss function. We consider loss with margins b ij &gt; 0, where 1 6 i &lt; j 6 k : h ( i,j,x ) = max { x + b ij , 0 } 2 . Let  X  be a vector of parameters of FBR. We minimize
F (  X  ) = X by a gradient based optimization method. A fresh ranking algorithm requires frequent tuning of parameters. A simple choice of values of parameters from a large set takes a lot of time. Thus applying such simple choice makes it impossible to get a fresh score improving a search quality. Let us in-troduce the way we found the derivative  X  X  F  X  X  of our knowledge we are the first to get derivatives of the stationary distribution of Markov process when its transi-tion probabilities are functions of a stationary distribution of another Markov process (such nested Markov processes are widely used, see [15], [16]). It is easy to find the deriva-of linear equations. we get the derivative  X  X   X  X  ( q  X  p ) by finding  X  X  k  X  X  ( p ) from the following equation: Let us introduce the systems of linear equations with the lutions of the same equations). The first equations of the systems are the same with the equations of (5). We just should choose the considered parameter instead of  X  . It re-where W i (  X  p  X  p ) = W i ( p )  X   X  X  i  X   X  X  i Parameters  X ,T,K are chosen among a small number of vari-ants. We consider the period of time [  X ,T ] of the length which is equal to 1 week. The parameter K is chosen in such a way that the length of one period [ t i  X  1 among 1 day, 6 hours, 3 hours and 1 hour.
All experiments are performed with pages and links crawled in December 2012 by a popular commercial search engine. We utilize all the records from the toolbar that were made from 10 December 2012 to 16 December 2012. There are 113M pages and 478M transitions in the log. For ranking evaluation we sampled a set of queries from the queries sub-mitted by real users during the period from 14th till 17th December. A query is a pair &lt; text of query, time of submit-ting &gt; . For each query a set of URLs was judged by profes-sional assessors hired by the search engine. When an asses-sor judged a pair &lt; query, URL &gt; he assigned a label based on both the freshness of the page in respect to the query time and the topical relevance of the page to the query. Due to specifics of the task, we consider only recency-sensitive queries, which our algorithm is focused on. Hence, in order to be relevant for such queries, the documents need to be both fresh and topically relevant at the same time. Assessors are instructed to consider documents to be fresh at least to some degree if only the last access to the document according to the toolbar logs took place not earlier than 3 days before the time of the judged query. Finally, the above-described procedure resulted into 200 judged recency-sensitive queries (with 3000 judged query-URL pairs) for our evaluation.
The relevance score is selected from among the editorial labels: Perfect, Excellent, Good, Fair, Bad . We divide our data into two parts. On the first part (75% of the dataset) we train the parameters and on the second part we test the algorithms. We compare Fresh BrowseRank with classic BrowseRank in the following way used also by Dai and Davi-son [15] and Zhukovskiy et al. [16]. These algorithms are combined linearly by ranks with BM25. Then parameters for Fresh BrowseRank are chosen by maximizing the loss-function in the way described in Section 5. The parameter of linear combination with BM25 is chosen by maximizing the Normalized Discounted Cumulative Gain (NDCG) met-ric. We obtain the following parameters.
 The parameter K is chosen from the set { 7 , 28 , 56 , 168 } . In these cases the lengths of periods [ t i  X  1 ,t i ] equal to 1 day, 6 hours, 3 hours and 1 hour correspondingly.
 Table 2 demonstrates the ranking performance on metrics NDCG@5 and NDCG@10 over the algorithms.

In this paper we introduce the new recency-sensitive al-gorithm of user X  X  behavior analysis Fresh BrowseRank. We compare it with BrowseRank. We test our algorithm on a large data set and demonstrate that our algorithm is bet-ter than BrowseRank. Besides our results being interesting on their own, they may be used by commercial web search engines for improving a search quality. It seems natural to propose other recency-sensitive algorithms of user X  X  behavior analysis based on our framework. By this reason we believe that our approach will be the basis for the new research in this area.
 It would be interesting to study the other aspects of the Web the user X  X  behavior analysis algorithms are missing. For example, it is natural to introduce the layer of query nodes into the graph.

We also introduce the method of training parameters of nested Markov processes. It would be interesting to exploit these algorithms for other algorithms based on such pro-cesses.
