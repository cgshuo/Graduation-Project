 1. Introduction
Citations have long been used to characterize and obtain information on different types of documents: for example for scientific papers, the number of citations received has been used to establish the impact of a paper ( Cole, 2000; Garfield &amp; Merton, 1979 ); for web pages the number and quality of incoming links contributes to ranking in information retrieval systems ( Brin &amp; Page, 1998 ). Internet technology makes it possible to track citations bi-directionally in a network, and links can be given an associated meaning. For web pages, anchor text is used to ascertain the content of a page. More recently citation text has been used also for summarization of scientific articles: the set of citations to a target article  X  sentences about the cited paper, called citances ( Nakov, Schwartz, &amp; Hearst, 2004 )  X  are used to ascertain its important contributions, and to form a summary ( Divoli, Nakov, &amp; Hearst, 2012; Elkiss et al., 2008; Mei &amp; Zhai, 2008; Mohammad et al., 2009; Qazvinian &amp; Radev, 2008; Qazvinian et al., 2013 ).

While in the last decade several studies have explored citations for summarization of scientific articles, we suggest that they suffer a limitation in that they extract a summary only from the set of citing sentences. We propose that the citances (or anchor text) should not be the only text used as an indication of the content of the target document, rather the citances, full text, and where available the summaries of the citing documents should all be taken into account to build a global view of the target paper, and thus a better summary. In this paper we introduce a new family of methods that use both incoming and  X  outgoing citations, and combine citances with other elements of cited and citing documents, including the full text of the target document.

We apply our approach to a particular summarization problem: creating catchphrases for legal case reports. The field of law is one where automatic summarization can greatly enhance access to legal repositories, as legal professionals are con-fronted by large bodies of documents that must be scrutinized with great precision ( Moens, 2007 ). Legal cases, rather than summaries, often contain a list of what the legal profession call catchphrases. Catchphrases present the important legal points maries, catchphrases give a quick impression of what the case is about, but they have an indicative function rather than infor-mative: they indicate all the legal points considered instead of just summarizing the key point(s) of a decision.
Despite the large quantity of material stored in textual format, and the strong need for intelligent text processing, little has been done for automatic summarization of legal documents. Given the importance of citations in the legal system, it is also surprising that citation analysis has not been incorporated in previous attempts at summarization of case reports. In the work describe here, we use the network of citations ( Zhang &amp; Koppaka, 2007 ) to assign catchphrases using both incoming and outgoing citations, and considering not only citation sentences but also citation catchphrases and full text sentences. Our methods substantially improve performance over full-text-only methods. We also show that this approach is suitable for sci-entific-article summarization, so the one method can be used for both.

The paper is organized as follows: related work on citation summarization is presented in Section 2 , legal catchphrases are described in Section 3 , and our evaluation framework is presented in Section 4 . We first outline and evaluate a range of full-text-only baseline techniques, developed to identify important sentences in legal cases (Section 5 ). Traditional fre-quency-based approaches, popular in other domains such as news, perform quite poorly for legal cases. We also introduce a centrality-based approach using HITS ( Kleinberg, 1999 ). We then move to citation-based methods and show how these substantially increase performance. We introduce a family of methods which use citation sentences, summaries of related papers and full text, both for incoming and outgoing citations (Section 6 ). In Section 7 we compare all the techniques and show how the citation-based methods obtain higher performance when compared to methods based only on the full text of documents. We then apply our methods to the domain of scientific articles in Section 8 , and show how we obtain results comparable to state-of-the-art tools. 2. Related work The use of citations for summarization has been mainly applied to scientific articles. Bradshaw (2003) proposed Reference-
Directed Indexing (RDI), in which citations are used to determine the content of articles for information retrieval. Nakov et al. (2004) introduced the term  X  X  citances  X  X , defined as the sentences surrounding citations, and pointed out the possibility of using citances directly for text summarization, as they provide information on the important facts contained in a paper.
Elkiss et al. (2008) provided a quantitative analysis of the advantages of using citation contexts in applications such as summarization and information retrieval. In particular, they examined the relationship between the abstract and the citation contexts for a given scientific paper. Their experiments showed that citation contexts tend to have further focused informa-tion that is not present in the abstract, therefore citation contexts can be utilized as a different kind of summary, supplemen-tary to the abstract. These results are also supported by Divoli et al. (2012) , who compared citances to abstracts of biomedical papers, finding that the set of citances for a target article covers all information found in its abstract, and provides about 20% more concepts (provided that the article has already accumulated enough citations).

A first method for citation summarization was given by Mei and Zhai (2008) , who proposed the use of citation contexts to produce an impact-based summary of a single research paper, using language modeling methods; i.e. the set of citing sen-tences is used to understand the impact, and sentences that talk about that impact are searched in the original paper and extracted.

Qazvinian and Radev (2008) presented an application of citation analysis to summarization, where they sought to extract, from among the set of sentences that constitute the citation summary, a subset that gives the main contributions of that paper. The proposed method, C-LexRank, first clusters citing sentences using Tf-idf vector similarity to find the different con-tributions of the target paper. Then LexRank ( Erkan &amp; Radev, 2004 ) is used to extract sentences from each cluster. Follow-up studies explored different ways of selecting sentences from the citation summaries: Qazvinian, Radev, and Ozgur (2010) identified important keyphrases from the set of citation sentences (statistically significant N-grams extracted using point-wise Kullback X  X eibler divergence), then a greedy algorithm picked sentences that covered more (non-redundant) nuggets.
Mei, Guo, and Radev (2010) introduced DivRank, based on a reinforced random walk in an information network. This ranking method focused on balancing the prestige and the diversity of the top ranked vertices. Qazvinian and Radev (2011) tried to maximize diversity using an approach based on distributional similarity. They applied this idea not only to scientific citations but also to a set of news articles. The work of Abu-Jbara and Radev (2011) aimed at producing more readable and cohesive summaries. Mohammad et al. (2009) focused on multi-document summarization, to obtain a survey on a given topic, given a collection of research papers. The authors compared summaries obtained from the full text of the papers, the abstract of the papers, and the citances of the papers. Qazvinian et al. (2013) compared different methods for creating tech-nical summaries from citations, both for single-document summarization (using C-LexRank) as well as multi-document sur-veys of scientific paradigms.
Wan, Paris, and Dale (2009, 2010) implemented a tool (Citation-Sensitive In-Browser Summariser) that uses these ideas to support researchers in browsing the literature. Whenever a citation to another document is found, the tool fetches addi-tional details from the cited paper and makes a short summary of it, tailored to the citation context. Aljaber, Martinez, Stokes, and Bailey (2011) focused on feature engineering for using citation contexts to assign MeSH terms to biomedical articles.
They found that a high number of novel terms can be found from the citation context, many of which are semantically related to terms in the original document. The use of citation contexts to improve information retrieval of scientific papers (2007) proposed a citation-based technique to decide scientific attribution. Some work on identifying non-explicit citing sen-tences (sentences which talk about a citation but do not contain the explicit citation) are ( Kaplan, Iida, &amp; Tokunaga, 2009;
Qazvinian &amp; Radev, 2010 ), while Athar and Teufel (2012) examine the citation context (explicit and implicit citing sentences) in relation to citation sentiment detection.

In the legal field, the application of NLP techniques to citation analysis has not received the same attention as in other fields. Zhang and Koppaka (2007) group citations in sub-networks, each focusing on one specific legal issue. Other research-ers have investigated extractive summarization of legal cases, but without using any citation information; examples include
Salomon ( Frank Schilder, 2006; Uyttendaele, Moens, &amp; Dumortier, 1998 ), the work of Hachey and Grover (2006) and Ashley and Br X ninghaus (2009) and ProdSum ( Yousfi-Monod, Farzindar, &amp; Lapalme, 2010 ). None of this system can be downloaded to directly compare it to our approach. The only corpus for legal summarization available is the HOLJ corpus ( Grover, Hachey, &amp; Hughson, 2004 ), which however does not deal with catchphases, and does not contain citations.

In this work, we investigate further possibilities for using citations for summarization. We demonstrate the effectiveness of citation-based summarization of legal texts, where citations have an important role. Other than the exploration of a new domain, our work introduces several novelties which can be applied to other domains such as scientific articles. Firstly, our work considers both incoming and outgoing citations, while work on scientific articles is usually confined to incoming cita-tions. This is particularly important if we want to summarize new documents, which have not yet been cited. Secondly, while most approaches generate summaries directly from citation text only, we also show how to use citations to form a summary, extracting sentences from the full text. Thirdly, we propose to use catchphrases of citing and cited cases for summarization.
This is not possible for scientific articles as catchphrases are not given, but we show how to apply a similar idea to articles using the abstracts of cited papers. We reported our early work on using citations for legal summarization in ( Galgani,
Compton, &amp; Hoffmann, 2012a ). This paper extends that work introducing new summarization techniques based on HITS, pre-senting a more comprehensive experimental section, and investigating the application of our methods to other domains, in particular to scientific papers summarization. 3. Legal catchphrases and citations
We developed our approach for the summarization problem of creating catchphrases for legal case reports. When exam-ining Australian case reports, we found that, rather than summaries, cases often contain manually created lists of catch-phrases , i.e. phrases that present the important legal points of the case. or digest) can be seen as a summary of the case, which presents all the legal points raised, discussed and decided in the judg-ment, rather than just summarizing the key point(s) of the decision. The main motivation for automatic generation of legal catchphrases is that most Australian Court decisions lack any form of summary or catchphrases. So we use cases that come with human-made catchphrases to build a corpus, and guide our automatic approaches to be applied to cases which do not come with given catchphrases. Automatically generated catchphrases could also aid the manual creation of catchphrases for new court documents.

In Australia documents that record court decisions are made publicly available by AustLII, mation Institute ( Greenleaf, Mowbray, King, &amp; Van Dijk, 1995 ) and similar Legal Information Institutes (LIIs) have been set up in other countries. AustLII is one of the largest sources of legal material on the Internet, with over four million documents.
We created a citation summarization corpus of 2816 cases accessing case reports from the Federal Court of Australia (FCA), for the years 2007 to 2009, which contain court-given catchphrases. We extracted the full text and the catchphrases of every doc-ument. Each document contains on average 221 sentences and 8.1 catchphrases. The average number of words for each docu-ment is 7479, while for the set of catchphrases the average is 70.7 words per document. The resulting average compression ratio is 98.4%, this value is higher than compression ratios commonly found both in the legal and other domains (see for example ( Ceylan, Mihalcea,  X zertem, Lloret, &amp; Palomar, 2010 )). Each catchphrases is 10.4 words long on average, while a sentence is 33.8 words on average.

Looking at the list of catchphrases, we found that usually there are some high-level catchphrases (such as  X  X  Corporations  X  X  catchphrases, however, are longer and more specific regarding the facts or issues of the case (i.e.  X  X  where trustee in bankruptcy has decided not in interests of bankrupt estate to prosecute the appeals  X  X ). Examples of legal catchphrases and citations are given in Table 1 . We excluded the first-level catchphrases from the analysis (for example Bankruptcy ), which simply indicate the category of the case ( Galgani, Compton, &amp; Hoffmann, 2012c ).
In total we collected 19,251 catchphrases, giving 16,412 different phrases. Of these phrases, 15,303 (93.2% of the different phrases, or 79.5% of the total) appear in only one document in the corpus. Only 1109 phrases appear more than once. Given that re-use of catchphrases across different cases is very limited, rather than choosing catchphrases from a list of already seen phrases, it seems reasonable to approach the problem by looking for a set of text fragments (in the document itself or in its citations) that provide the same information contained in the catchphrases.

In order to investigate citation-based summarization, we downloaded citation data from LawCite. that cite the case, and legislation cited by the case. For each case, we queried LawCite, obtained these lists and downloaded the full texts and the catchphrases (where available) from AustLII, of both cited (previous) cases and more recent cases that cite the current one (citing cases).

Of the 2816 cases in our FCA summarization corpus, 1904 are cited at least by one other case (on average by 4.82 other cases). 4 We collected the catchphrases of these citing cases and searched the full text to extract the location(s) where the cita-tion is explicitly made (citances). For each of the 1904 cases we collected on average 21.17 citing sentences, and we extracted from the same citing cases an average of 35.36 catchphrases. For 2563 cases, we also extracted on average 67.41 catchphrases from previous cases referenced by the judge (cited cases). From cited cases, citances cannot be extracted as there is no direct mention of the target case in the text.

We refer to a document from our FCA corpus for which we want to create catchphrases as the target document , to sen-tences collected from cases that cite it as its citances ( Schwartz &amp; Hearst, 2006 ) and to the catchphrases of the citing and cited cases (but not of the target case) as citphrases . The latter can be divided into citing and cited citphrases, but for most of the discussion we will consider their union (see Fig. 1 for a schematic representation and Table 1 for an example). We use the term citations to refer to both citphrases and citances together.

We have made the corpus of catchphrases and citations available online in XML format
To our knowledge there is no other freely available corpus of legal summaries and citations. 4. Evaluation methodology
We propose a simple method to evaluate candidate catchphrases automatically by comparing them with the court-given catchphrases from our AustLII corpus (our gold standard), in order to quickly estimate the relative performance of a number of methods on a large number of documents. While human-based evaluation is considered more accurate, we needed to compare several methods on a large set of documents. It was not feasible to use experts for such repeated tasks so we devised an approximate automatic method. Our goal was to assess methods relative to each other, rather than to make an assessment of the quality of the catchphrases the methods found, as this would be possible only with a human based evaluation.
 Our system extracts text fragments as candidate catchphrases. We propose an evaluation method which is based on
ROUGE scores for extracted fragments compared to the given catchphrases. ROUGE ( Lin, 2004 ) includes several measures to quantitatively compare system-generated to human-generated summaries, counting the number of overlapping n-grams of various lengths, word pairs and word sequences between the summaries. We initially experimented with different scores from the ROUGE family. All the results presented are based on ROUGE-1, as the ranking of methods did not change using different scores. As we have only one gold-standard reference per case, using ROUGE-1 is equivalent to computing a bag-of-word overlap between the system and gold-standard summaries.

We wanted to reward sentences that contain an entire individual catchphrase. Fig. 2 illustrates how a sentence that con-tains small pieces of different catchphrases is not as useful as one that contain a full catchphrase. For this reason we evaluate sentence-catchphrase pairs individually. We devised the following evaluation procedure: we compare each extracted sen-tence with each catchphrase individually, using ROUGE. If the recall (on the catchphrase) is higher than a threshold t , the catchphrase-sentence pair is considered a match , and the sentence is considered relevant . For example if we have a 10-word catchphrase, and a 15-word candidate sentence, if they have 6 words in common we consider this as a match using ROUGE-1 with threshold 0.5, but not a match with a threshold of 0.7 (requiring at least 7/10 words from the catchphrase to appear in the sentence). Using other ROUGE scores (ROUGE-SU or ROUGE-W), the order and sequence of tokens are also considered in defining a match. This match definition is tighter than just considering any word overlap between all sentences and all catchphrases, as in our procedure the matching phrase must occur within a single sentence and a single catchphrase. Once the matches between single sentences and catchphrases are defined, for a document and a set of extracted (candidate) sen-tences, we define precision and recall as:
The recall is the number of gold-standard (court-given) catchphrases matched by at least one system-extracted sentence, divided by the total number of gold-standard catchphrases; i.e. the proportion of the target catchphrases where the system found a suitably matching sentences. The precision is the number of system-extracted sentences which match at least one gold-standard catchphrase, divided by the number of system-extracted sentences; i.e. the proportion of the system-extracted sentences which correctly matched a catchphrase. It should be noted that although these definitions of precision and recall have the same general sense as the standard definitions, two different types of instances are used in the calcula-tions: catchphrases for recall and sentences for precision. This means, for example, that the number of matched catchphrases does not necessarily equal the number of relevant sentences, as it is possible for one sentence to match more than one catch-phrase, or for multiple sentences to match the same catchphrase.

The main limitation of the evaluation method is that results depend on the choice of the threshold t . This threshold is not related to tuning the system (e.g. balancing precision vs. recall) but rather on setting the evaluation standard. Table 2 shows how changing the threshold results in more (higher t ) or less (lower t ) strict match conditions. If we choose a higher value for t , certain methods will have lower values for both precision and recall, than with a lower value of t . However this is true for all methods and so does not limit the effectiveness of the chosen evaluation procedure; for any criterion used, we would expect performance of the methods relative to each other to be reasonably consistent. Section 7 shows that the ranking of methods is invariant across different thresholds.

In Fig. 3 we can see the number of matching sentences for each catchphrase. Using ROUGE-1 with threshold 0.5 ( Fig. 3a ), there are a number of catchphrases (1601) which cannot be matched by any sentences, and a similar number (1931) of catchphrases which have 30 or more similar sentences. A large number of catchphrases have only one (2343), two (1964) or three (1564) matching sentences, with a long tail. Using a more strict matching criterion, ROUGE-1 with threshold 0.7 ( Fig. 3b ), the number of catchphrases which cannot be matched rises to 7373, and the tail of catchphrases with 30 or more matching sentences shrinks to 367.

This evaluation method lets us compare the performance of different extraction methods automatically, by giving a sim-ple but reasonable measure of how many of the desired catchphrases are generated by the system, and how many of the sentences extracted are useful. This is different from the use of standard ROUGE scores, where precision and recall do not relate to the number of catchphrases or sentences, but to the number of smaller units such as n-grams, skip-bigrams or sequences, which makes it more difficult to interpret the results for our application.

A limitation of our method for computing matches is that it does not take into account the length of the sentence: a long sentence and a short sentence which contain the same catchphrase are rewarded the same. In composing a summary, if two sentences contain the same information, ideally we would select the shorter. However, penalizing longer sentences may not be a good strategy, because many catchphrases are actually quite long and are contained only in long sentences. In Section 7 we will introduce a way to account for sentence length in the evaluation.

Using ROUGE to match extracted catchphrases with the given catchphrases may give different results from asking a human expert for their opinion of the extracted catchphrases, as ROUGE simply relies on both documents having words in common. However, ROUGE was shown by Lin (2004) to have a high correlation with human judgement for both single-and multi-document summaries, and worked especially well on short headline-like summaries with a correlation of up to 0.98. Since its introduction ROUGE has been the main tool for automatic summarization evaluation in public competitions such as DUC (Document Understanding Conference 2004 X 2007) and TAC (Text Analysis Conference 2008 X 2011) and has become a de facto standard for automatic evaluation of summaries. In our application ROUGE should at least demonstrate differences between methods in extracting catchphrases of the type that have some words in common with the given catch-phrases. We use ROUGE only to provide a comparative evaluation of the different methods.

We have also used the same corpus to evaluate a different (knowledge-based) approach to legal summarization. During that evaluation, we asked legal experts to rate a random sample of extracted sentences as  X  X  X ot Useful X  X ,  X  X  X seful X  X  and  X  X  X ery
Useful X  X . Sentences with a ROUGE match with catchphrases (relevant sentences) were rated statistically significantly higher than sentences without any match: 38% of relevant sentences was rated  X  X  X ery Useful X  X  and 45%  X  X  X seful X  X , while only 13% of the irrelevant one was rated  X  X  X ery Useful X  X  and 29%  X  X  X seful X  X , the result of this evaluation can be found in ( Galgani, 2012;
Galgani, Compton, &amp; Hoffmann, 2014 ). This promising result suggests that ROUGE matching has at least some level of correspondence with human matching. In the same study we also asked legal experts to compare the system X  X  to manu-ally-created catchphrases and to assess the comprehensiveness of 5-sentence summaries. The evaluation method was val-idated in two ways: first showing that it gives the same ranking between two systems (Figs. 11 and 12 in Galgani et al., 2014 ), secondly showing that random sentences with a ROUGE match are rated better that sentences without a match ( Fig. 13 of that study). 5. Full-text extraction methods
We initially developed a range of extraction methods that rely only on the full text of each case to extract potential catch-phrases. We looked for different kinds of indicators of saliency to locate important fragments of text; these methods apply some common summarization techniques but were also motivated by a manual analysis of some cases. The aim was to bet-ter understand the complexity of the task and to obtain a baseline to which citation based methods could be compared.
We used the corpus of cases and their provided catchphrases to experiment with several ways of identifying relevant words to score sentences in the full text. We use term to refer to single tokens and in the computation for the various methods all terms were stemmed with the Porter stemmer ( Porter, 1980 ) and stop word filtered. We developed six different frequency-based scores: Fcfound , FcfoundFreq , AvgFreq , TFIDF , Threshold and CoOccur . We also developed a centrality-based score: Centrality .

The Fcfound score of a term t is defined as the ratio between the number of documents in which the term appears both in the catchphrases provided and in the text of the case, divided by the number of documents which contains the term:
The Fcfound score of a term is computed using our corpus of catchphrases (2816 documents). The score of each term does not depend on the specific document considered.

FcfoundFreq is the previous Fcfound score, multiplied by the number of occurrences ( Freq ) of the term t in the particular document doc :
AvgFreq is the ratio between the number of occurrences of the term t in the present document and the average number of occurrences of the term in the collection: TFIDF is the standard Tf-idf measure: where NDocs tot is the total number of documents in the collection (2816), and NDocs  X  t  X  is the number of documents that con-tain the term t .

Threshold : using our corpus of catchphrases and documents, for each term we select a threshold for term counts in the documents that best predicts whether the term will appear in the catchphrases of that document. For each term the thresh-the observed frequencies of the term in those documents and selecting the value that gives the smallest error on the exam-ples available. Accordingly, for each sentence in a document we will give a score of 1 to those terms whose frequency is higher than the corresponding threshold, and 0 to the others.

CoOccur is computed, for each document, as follows:
The rationale behind this method is that some words (the 10 most frequent terms in the document) might indicate which are the  X  X  X mportant X  X  sentences of the document, so we look at terms that appear more frequently in those sentences rather than in other parts of the document.

All these methods assign scores to terms in a document. These computer scores are the weights for the model that is used for novel documents (out-of-vocabulary words are ignored). Because our goal is to rank sentences for extraction, we wish to assign scores to sentences rather than terms. We experimented with both averaging the score for each term of the sentence or taking the n top ranked words and looking for sentences that contain them. The first approach gave better results, so all the results presented here are based on it.

The last method, Centrality , is based on graph analysis rather than frequency computations. It does not give weight to terms but directly to sentences. To compute the ranking of sentences we use the link analysis algorithm HITS (Hyperlink-Induced Topic Search) ( Kleinberg, 1999 ).

HITS assigns two scores to each node of a graph, an authority score and a hub score. These scores are defined recursively: a higher authority score occurs if the node is pointed to by nodes with high hub scores, a higher hub score occurs if the node points to many nodes with high authority scores. HITS and other graph based methods, such as PageRank ( Brin &amp; Page, 1998 ), have been adapted to sentence ranking: a graph is built where nodes are sentences, and edges connect related sentences (see
Fig. 4 ), then HITS or PageRank are used to compute importance weights for nodes ( Mihalcea &amp; Tarau, 2005; Wan &amp; Yang, 2008 ).

To establish links between similar sentences, we counted the number of terms shared by the two sentences. Thus two sentences are not connected if they do not have any term in common, otherwise the edge between them is weighted with the number of common terms. We also experimented with using Tf-idf cosine as a similarity measure but it did not increase performance. This link definition implies that edges are undirected, thus HITS X  hub and authority scores are the same. We chose to use HITS rather than PageRank as it is suited to the bipartite citation graph which we use for citation based sum-marization (introduced in the next section). We did not experiment with using PageRank directly, but in Section 7 we use
TextRank ( Mihalcea &amp; Tarau, 2004 ) as a baseline system, which is based on PageRank. 6. Citation-based extraction methods
This section presents methods for extracting candidate catchphrases based on citation analysis. Given a target document, for which we want to generate a summary (catchphrases), and the set of its citances and citphrases extracted from con-nected documents (see Fig. 1 ), we developed two families of methods to form a summary for the target document: (1) we rank and extract text from citances and citphrases as the summary, or (2) we use the text from citances and citphrases to identify relevant sentences in the target documents, and extract those sentences as the summary. 6.1. Ranking citations We experimented with using citphrases and citances directly as candidate catchphrases for the target documents.
Extracting a summary from the set of citances is the most common method found in the literature on citation-based summarization. We rank all citations to find the most  X  X  X entral X  X  ones, to identify concepts that are repeated across different citations. The proposed methods thus belong to the class of centrality-based summarization techniques.

For each target case, we take all the citphrases and citances, both from citing and cited cases, and we build a similarity graph ( Fig. 5 ). Then we extract the most central citations (either citphrases or citances). We build two graphs, one for citphrases and one for citances, as scores are computed separately. We propose three centrality scores: 6.2. Using citations to rank sentences
We experimented with using citation text to rank sentences of the target case, as opposed to using citations directly as candidate catchphrases. For each sentence in the target document, we measure similarity with the citations (either citphras-es or citances), and rank the sentences in order of decreasing similarity: a sentence which has high similarity with many citations is preferred. We rely on the idea that citations indicate the main issues of the case, and that they can be used to identify the sentences that describe these relevant issues. We again build two graphs (one for citphrases and one for citanc-es) which show similarity between citations and sentences.

Differently from the previous graphs, this is a bipartite graph with directed edges which point from citations to sentences (see Fig. 6 ). We rank sentences in this graph, using citations, in the three following ways: 7. Results
In this section we compare all the proposed methods and discuss the results obtained on legal cases. Before examining the results for individual methods we need to fix a ROUGE matching criterion, according to our evaluation method described in
Section 4 . In order to compute precision and recall, we need to set the threshold t which defines matches between catch-phrases and sentences. Fig. 7 shows precision and recall estimates for different values of t (the methods evaluated are those already introduced and some baselines, which will be explained later). We can see from the figure that a low threshold cor-responds to high precision and recall, while when using a high threshold (corresponding to a strict matching) both precision and recall decrease. It may seem counter-intuitive that both precision and recall increase and decrease together, but this is simply because, as previously defined, they refer to different types of instances: recall is the proportion of catchphrases matched by sentences while precision is the proportion of sentences that match catchphrases. Fig. 8 shows Mean Average
Precision ( Manning, Raghavan, &amp; Sch X tze, 2008 ), which does not require a summary length to be fixed. This Figure confirms
ROUGE scores.
The important result of Figs. 7 and 8 is that changing the threshold does not change the relative ranking of the methods: the best method is the same for each threshold, and so on. Therefore to assess methods against each other, the choice of threshold is not critical. In all the result presented we will use a threshold of 0.5, as it gives the most spread among the meth-ods. For brevity we omit results with other thresholds. The same reasoning applies to the difference between using ROUGE-1 or ROUGE-SU6. It does not change the relative ranking of the systems and so we will use ROUGE-1 from now on. 7.1. Full text methods
We applied and evaluated the methods on our corpus of 2816 case reports. For each document every method ranks all the sentences, then for different numbers of extracted sentences we measure precision (number of relevant sentences) and recall (number of catchphrases covered) using ROUGE matches. Fig. 9 shows precision and recall of all the methods, for different numbers of selected sentences, averaged over all the documents, using ROUGE-1. For comparison the method Random is also included, which is a random selection of sentences from each document. Centrality , the graph method, exceeds the other methods for both precision and recall. Among the frequency based methods, Threshold and CoOccur obtain the highest precision, with Fcfound having slightly higher recall but lower precision. Ranking based only on TFIDF fails to outperform even a random selection of sentences.

The Fcfound , FcfoundFreq and Threshold methods are supervised methods which rely on statistics from the known catchphrases. Therefore, to obtain fair estimates for these three methods we need to apply them to an unseen test set.
We built a test set of unseen documents, downloading from AustLII new case reports from the Federal Court of Australia decisions in 2006 (1068 cases with court-created catchphrases). Fig. 10 shows all the methods evaluated on the test set of 10,168 FCA cases from the year 2006. We can observe that the values of precision and recall are almost identical to those computed on the training set (compare with Fig. 9 ). This result suggests that the statistics computed on catchphrases from the training corpus are general enough to be applied to new documents without loss of performance, and the methods do not overfit the training set. For this reason all the following results will be discussed in relation to a single set of cases (2007 X  2009). We particularly note that evaluation on a separate test set only applies to these three methods. The citation methods do not use any information from the documents on which they are evaluated, although these are the X  X  X raining X  X  documents for the overall study. These three methods are also the weakest even when evaluated on their own training data; they are only included for comparison as the paper focuses on the performance of the citation methods. The other frequency based methods, centrality and the citation-based methods are unsupervised (they do not use known catchphrases) and thus it is reasonable to evaluate them on the 2007 X 2009 catchphrases. As for the fact that the results for the Fcfound , FcfoundFreq and Threshold methods are almost identical for test and training data, we suggest the following hypothesis: these methods simply identify important terms, and given the way in which law depends so much on precedence it is likely the same terms will be as important in unseen documents as in the seen documents from which they were derived.
 7.2. Citation based methods
In Section 6 we have introduced two groups of methods to create a summary using citations; the first extracts a summary directly from the citations, the second uses citations to extract a summary from the full text. The performance of the first group, selecting a summary from the citations, is presented in Fig. 11 a for citphrases and in Fig. 11 b for citances. In both plots we used ROUGE-1 with threshold 0.5 as matching criterion; other criteria give the relative results. The plots show that the use of different similarity measures (AVG, THR or HITS), using different ROUGE scores (ROUGE-1 or ROUGE-SU), does not influence the results substantially. The plots also show that we can find  X  X  X ood X  X  citances and citphrases, that are similar to the target catchphrases; however, the recall fails to grow over a certain limit (around 0.6 for citances and 0.5 for citphras-es) even when increasing the number of extracted citations, thus suggesting that we cannot cover all aspects of a case relying only on citances or citphrases. It should be noted that citance methods can only be applied to cases with some incoming citations, while citphrases methods can be applied both to incoming and outgoing citations. As we selected only cases with at least 10 citing cases, citance methods are evaluated only on 943 cases, while citphrase methods are evaluated on 2344 cases (those with at least 10 citphrases).

The results for using citations to extract sentences are shown in Fig. 12 a for citphrases and 12 b for citances, where again we use ROUGE-1, with threshold 0.5 as matching criterion. As was the case with citation-only methods, the use of different similarity measures (ROUGE-1 or ROUGE-SU6 and AVG, THR or HITS) does not produce substantial differences: all the meth-ods in the same group have similar performances. We can see also that these methods obtain a higher recall then those based on citations only. 7.3. Overall evaluation
To better understand the performance of the proposed methods, we introduce three baseline methods representing state-of-the-art summarization systems: Mead, LexRank and SumBasic. Mead ( Radev, Jing, Stys, &amp; Tam, 2004 ) is a state-of-the-art general purpose summarizer. In Mead we can set different summarization policies: Mead Centroid (the original Mead score) builds a Tf-idf vector representation of the sentences in the document, then it extracts the sentences most similar to the cen-troid of the document in the vector space (the centroid consists of words that have the highest Tf-idf scores). The centroid feature is used in linear combination with the position feature (the position of the sentence in the document). Position is the normalized value of the position of a sentence in the document such that the first sentence of a document gets the maximum position value of 1, and the last sentence gets the value 0.

A variation of Mead is LexRank ( Erkan &amp; Radev, 2004 ), which builds a network in which nodes are sentences and edges between nodes are weighted with lexical (Tf-idf) cosine similarity. The method then performs a random walk to find the most central nodes in the graphs (using an adaptation of the PageRank algorithm) and takes them as the summary. We downloaded the Mead toolkit 6 and applied both methods to our corpus: using the  X  X  -score  X  X  option to obtain a ranking of all sentences. 7
SumBasic ( Nenkova &amp; Vanderwende, 2005 ) is a basic summarization strategy developed for multi-document summari-zation. It relies on the observation that words occurring frequently in the document occur with higher probability in human-created summaries than words occurring less frequently. The algorithm consists of extracting sentences which contain the most frequent words, discounting words that are already extracted, in a greedy fashion.

Among full-text only methods we selected Centrality and FcFound as those with highest performance. We used the fol-lowing citation methods:
The four citation methods use ROUGE-1 AVG as the similarity measure to rank text fragments, as it was shown that vary-ing the similarity measures (AVG, THR or HITS) does not impact the performance.
 The selected methods are compared in Fig. 13 , and the scores for 10-sentence extracts for all the methods are compared in Table 3 . The figure shows that CpSent is the best method both for precision and recall, followed by Centrality and CsSent .
This result suggests that using citation data to select sentences from the target document can significantly improve perfor-mance over using only citation text or only the full text of the case, as our new method CpSent, combining citphrases and sentences, outperforms all other approaches. This result has two consequences: firstly it demonstrates the importance of using citations in legal text summarization (which has not been done before), secondly it shows that in this domain previ-ously proposed citation summarization methods (which extract summaries from citances only) are inadequate; both cita-tions and full text elements need to be combined to obtain the best results.
 CpSent and CsSent cover a larger number of catchphrases and at the same time fewer irrelevant sentences are extracted.
CpOnly and CsOnly have lower precision and recall, confirming the hypothesis that citation data alone, while useful in iden-tifying some key issues of the target case, does not cover all the main aspects of the case, and should be used in combination with the full text of the target document to obtain better summaries.

Regarding the use of different types of citations, citphrases give higher performance than citances when used to select sentences; however, the opposite is true when citations are used directly to form the summary. The difference in perfor-mance may be influenced by the fact that citphrases are generally present in larger numbers than citances and in many cases, citances may be insufficient to cover all the main issues. Another difference is that citphrases are taken from both previous cited cases and later citing cases, while citances can be extracted only from the latter. An example of sentences extracted using CpSent is given in Fig. 14 .

Mead Centroid and LexRank also obtain a good level of performance. It is not surprising that the two Mead methods per-form similarly to the Centrality method, particularly for recall, as they use similar centrality scores. However both Mead Cen-troid and LexRank combine centrality-based weighting with position of sentences, while all our methods rely on a single feature. The other baseline summarizer, SumBasic , performs poorly, obtaining the lowest recall and precision. This confirms that relying only on frequency of terms does not give good results on this data (the method TFIDF was shown to fail to out-perform Random in Fig. 9 ). SumBasic was originally designed for multi-document summarization of news articles, where repetition is likely to be more significant than in legal cases.

The methods based on citphrases (CpSent and CpOnly) were evaluated using two kinds of citphrases, those from citing documents (cases that cite the target case) and those from cited documents (cases cited by the target case). In Fig. 15 we evaluate the two kinds of citphrases separately. The difference between using citing citphrases or cited citphrases is minimal in terms of recall and precision, and using both kinds of citations together improves the results, especially in the case of the
CpOnly method. This has important implications for the power of these methods: as opposed to most citation-based sum-marization approaches, our methods can be applied also to new cases that have not been cited yet, if the documents they cite have catchphrases. Thus it is possible to use citations to summarize new documents not yet cited, using only outgoing citations.
 One of the limitations of our evaluation method is that it does not take into account the length of the extracted sentences.
To account for sentence length, we modified the presentation of results computing precision and recall for different sum-mary sizes, expressed as number of words. In Fig. 16 precision and recall for the methods are plotted against the number of words in the extracted sentences on the x -axis, rather than the number of sentences themselves. It can be seen that there are some differences between Figs. 13 and 16 , in particular the recall of Fcfound is higher than Centrality and CsSent, and very close to the top recall reached by CpSent. This suggests that the methods based on HITS centrality, both with citations (CpSent and CsSent) or without citations (Centrality), tend to have a high recall in part because they select longer sentences, so that if we limit the summary length by imposing a maximum number of words, other methods may perform better. In general longer sentences may perform better than shorter ones just because they have a higher probability of containing a catchphrase, but it is not clear if long sentences convey the same information as a given short catchphrase, or the key infor-mation is somehow lost if the sentence is too long (each catchphrases is 10.4 words long on average, while a sentence is 33.8 words on average). The purpose of summaries is to compress information, and thus long sentences may include unnecessary information. This problem is usually approached applying sentence compression after the summarization stage ( Zajic, Dorr, hand it is appropriate to penalize long sentences in the evaluation (if two sentences contain the same information  X  having the same units in common with catchphrase  X  we prefer to select the shorter one); on the other hand methods that penalize longer sentences, with a bias towards selecting short sentences, also have problems, as many catchphrases are quite long and are covered only by long sentences. 8. Citation-based summarization of scientific articles
All the methods we proposed should generally be applicable to other domains and different kinds of text. The frequency-based methods require a large corpus to compute meaningful statistics, and some of them used information about known catchphrases to identify relevant text. We found that frequency-based methods, which are state-of-the-art in other domains such as news summarization ( Nenkova &amp; Vanderwende, 2005; Vanderwende, Suzuki, Brockett, &amp; Nenkova, 2007 ), are not effective for legal cases; for example, SumBasic failed to outperform even a random selection of sentences. We achieved bet-ter performance with citation-based methods. Although these methods do not require a training corpus, they can be applied only in domains where documents cite each other. In order to assess the wider applicability of these methods, we investi-gated their application to another domain where citations are widely used, scientific articles, which has been the most pop-ular domain for citation-based summarization research.

We downloaded a corpus of scientific papers with citations. The ACL Anthology Network than 18,000 papers in the area of computational linguistics, and includes citation data for these papers ( Radev,
Muthukrishnan, &amp; Qazvinian, 2009 ). It also contains a small set of 25 highly cited articles which are manually annotated with tion summarization, and it has been used in other summarization studies (such as Qazvinian &amp; Radev, 2011; Qazvinian et al., 2010; Qazvinian et al., 2013 ). We use this smaller data set in the following evaluation.

The corpus contains, for each of the 25 papers, a list of citation sentences (citances) from later articles that refer to the paper. It also contains a list of non-overlapping contributions for each paper, which are called facts . Each fact is identified by one or more nugget(s) , phrases that represent facts. One fact can be represented by different nuggets, for example Fact 3
Table 4 . The nuggets were created by manual annotators, who were asked to  X  X  X ead the citation summary (which corresponds to what we call citances of each paper) and extract a list of the main contributions of that paper, as perceived by reading the citation summary X  X  . The authors asked the annotators  X  X  X o focus on the citation summary to do the task and not on their back-ground on this topic X  X  ( Qazvinian &amp; Radev, 2008 ). Nuggets were manually matched to facts, and only facts shared by at least two annotators were retained. Each paper has on average 5.52 facts, and each fact is represented by 2.65 nuggets on average.
The nuggets can be put in correspondence with the citances in which they appear, to build a citance-fact occurrence matrix. This matrix is not released with the dataset, the authors of the paper stating that  X  X  to detect which nuggets/facts a cita-tion contains, one should perform basic string matching  X  X . We used ROUGE-1 with threshold 0.9 (using stemming) to detect which citances contain which facts. A fact that occurs in many citances is considered more important than one that appears in less citances: matching facts to citances thus gives a score to every fact. For the only paper provided as an example (P99-1065) in ( Qazvinian &amp; Radev, 2008 ), there are some differences in the computation of matches between our results and those given in the paper, which are probably due to a different way of performing string matching and a slightly different version of the corpus. 10
While in legal cases we used sets of catchphrases as target summaries, this dataset uses facts, which can be considered very short catchphrases. The main difference is that while catchphrases aim at representing the main issues of the document, the facts in the 25 papers from the AAN corpus are identified from the citation summaries, and thus they cover the main contributions of a document as seen from the citations, rather than as seen in the document itself. We use the data in the following way: we use facts to evaluate the methods in the same way we use catchphrases, we obtain from the corpus citances for each paper, and from the full AAN corpus the corresponding full text of the papers. In order to apply our methods we need one more element, citphrases. From the AAN corpus we can get the list of articles cited by each paper, or which cite the paper; however, unlike the 25 papers, these are not annotated with important facts. In scientific papers the use of catch-phrases is limited to the occasional presence of keywords (an exception being the MESH terms for biomedical papers). The abstract of a paper, as an author-made short summary of the main points of a document, can be compared to the set of catch-phrases of a legal case, but differences exist. For example abstracts tend to focus more on results than methods. For each target document we extract the abstracts of all cited and citing articles (seen as a short summary of each article) and use the sentences in the abstract as citphrases.

We pre-process the documents in the same way as with the legal cases, applying stemming and using stop-words to filter the tokens in the full text, the citances, the nuggets (corresponding to our target catchphrases) and the abstracts of cited/cit-ing papers (corresponding to our citphrases). We set up the evaluation framework, to identify how many of the facts an extracted sentence covers, using the same ROUGE procedure described for the legal corpus. Given that the facts are usually quite short, we used ROUGE-1 with threshold 0.9 to compute matches. A fact is considered matched by a sentence if any of the nuggets is contained in the sentence. 8.1. Results We applied the citation-based methods as described in Section 6 , using both citances and citphrases, to compute CsSent,
CsOnly, CpSent and CpOnly scores, using both ROUGE and HITS. Fig. 17 presents the evaluation of these four citation-based methods, and of three full-text methods: Centrality, LexRank and Random. We can see from the figure that CsOnly , which ranks and extracts citances, outperforms all the other methods both for precision and recall. The second best performing method is the other method which relies on citances, CsSent , which uses the text of citances to extract relevant sentences from the full text. On this data CsOnly outperforms any other method because the nuggets are extracted by the annotators directly from the citances, i.e. only facts explicitly mentioned in the citances are contained in the gold standard summaries.
Thus methods based on citances have an obvious advantage. In contrast, in the case of legal catchphrases, the phrases are created by the judge at the same time as the full text. Citances could not be used by the judge as by definition they come only from citing documents written later.

In order to compare our approach with that of ( Qazvinian &amp; Radev, 2008 ), we also downloaded C-LexRank, the method described in that paper, which was released by the authors as a Perl script. related citances (using Tf-idf cosine similarity) and then picks representative citances from each cluster using LexRank. We applied both C-LexRank and LexRank to the citances for each case. In Fig. 18 we compare different ways of ranking citances. Our methods based on ROUGE and HITS perform very similarly to LexRank and C-LexRank, with no method clearly outperforming the others. We can see that the performance of the methods is also similar to a random ordering on citances.

In ( Qazvinian &amp; Radev, 2008 ) the pyramid score ( Nenkova &amp; Passonneau, 2004 ) is used for evaluation. In pyramid eval-the pyramid if it occurs in i citances). The score of a summary is given by the sum of the weights of the facts matched by the summary, divided by the sum of the weights of an optimal summary. The pyramid score of a summary ranges from 0 to 1, with a high score indicating more heavily weighted facts. Along with precision and recall, we also computed pyramid scores for 5-sentence extracts, reported in Table 5 . All the five methods (including random) which rank citances obtain similar pyr-amid scores, with the best method being our AVG ROUGE CsOnly method. However, given the small size of the dataset, the differences between these five ways of extracting citances are not statistically significant ( p -value &gt; 0.05).
The pyramid scores that we compute differ from those reported in ( Qazvinian &amp; Radev, 2008 ), which reports a pyramid score of 0.41 for random, 0.71 for LexRank and 0.75 for C-LexRank; whereas our results for random are similar to the other methods. 12 In the following papers by same group, random is also similar to the other methods. Qazvinian et al. (2010) describe a method to extract significant ngrams (keyphrases) and then pick sentences that cover many ngrams, reporting results higher than C-LexRank on the same dataset; however the pyramid score of the method is not given. Qazvinian and Radev (2011) report (for the same dataset) scores of 0.716 for Random, 0.728 for LexRank and 0.781 for C-LexRank, while the new method proposed in that paper, based on word distributional similarity, obtains a pyramid score of 0.813. In this last study, however, the sum-maries are cut at a maximum of 150 words, rather than using a limit of 5 sentences. These scores confirm our findings that all methods that rank citances obtain similar performance. Using an updated release of the dataset, containing 30 papers,
Qazvinian et al. (2013) compare C-LexRank to other methods: random, MMR ( Carbonell &amp; Goldstein, 1998 ), LexRank, DivRank ( Mei et al., 2010 ) and MASCS ( Zajic et al., 2007 ). C-LexRank obtains the highest pyramid score both for 100-words ( p = 0.647) and 200-words summaries ( p = 799), but again is not statistically significantly superior to the other methods including random. Overall, this experiment shows that our methods can be successfully applied to scientific paper summarization, with
CsOnly performing comparably to and perhaps outperforming both the other methods and competitive baselines. However, we suggest that the dataset used for evaluation, chosen because it has been used to evaluate similar approaches and thus it enables direct comparison, suffers from two limitations: the small size of the corpus means that results are not statistically significant; the very specific way of creating reference summaries (extracted directly from citances) suggests that results may not generalize to different summarization tasks. Differences between legal and scientific documents, both in their use of language in general and in the purpose and significance of citations, mean that different methods have to be tuned to be ported from one domain to the other. In this experiment we only want to show that bidirectional citations as well as combinations of citations and sentences are potentially useful also for domains besides the legal one. 9. Conclusion
In this paper we explored citation-based summarization for legal documents. To enable this investigation we created a corpus of legal documents which contains the full text of the documents, summaries (catchphrases) and citations (citances and citphrases). 13 The overall conclusion from our work is that citations are important in the legal system and should be used for summarization. Secondly, citations to as well as from the target document can be very useful. More generally we suggest that citation-inspired methods may have wider scope than simply the domain of scientific publications.

We introduced several full-text-only methods based both on frequency computations and on centrality measures. We then proposed an innovative approach to using citations for summarization, developing four different methods: CpOnly (cit-phrases  X  elements of the summary from the citing/cited document  X  form the summary), CsOnly (citances  X  sentences from the citing document  X  form the summary), CpSent (citphrases are used to rank sentences in the target document) and CsSent (citances are used to rank sentences in the target document). These methods introduce three important contributions: (1) they use older documents cited by the target document (outgoing citations) for summarization (as opposed to using only incoming citations  X  newer documents citing the target, as usually applied to scientific papers), which allows us to summa-rize a new case which has not yet been cited without any significant loss of performance; (2) citphrases are taken from cited and citing cases and shown to improve performance compared to using citing sentences (which are usually used in citation summarization); (3) the methods use ROUGE scores or HITS to match the citation text with the full text of the target doc-ument to find important sentences, rather than extracting the summary directly from the citation text.

When evaluating the extraction methods on our legal corpus, we found that the citation-based CpSent and CsSent meth-ods (using citphrases and citances to identify sentences in the target document) and the HITS-based Centrality method gave the best results, outperforming both frequency-based methods and some baseline methods that represent more complex state-of-the-art general-purpose summarizers. The best method CpSent (extracting the sentences which are most similar to the citphrases) obtains an average recall of 68.6% and an average precision of 65.5% when extracting the 10 best sentences for each document. The fact that CpSent obtains the highest performance clearly underlines the importance of (1) going beyond citances and including other citation elements (citphrases); (2) combining citations and full text elements (citations used to rank sentences). Furthermore, we showed in Fig. 15 how using bi-directional citations (both cited and citing cases) allows us to summarize new documents not yet cited and further improves performance when both kinds of citations are available. Techniques that are successful in other domains, such as Tf-Idf and SumBasic, perform poorly in the legal domain.
To investigate the applicability to other types of text, these citation-based methods were also applied to a dataset of sci-entific articles, showing performance comparable to state-of-the-art techniques. The best performance was obtained by the method CsOnly, as the way in which this corpus was constructed favors the use of citances to create summaries. Given the good performance obtained on this different dataset, we conclude that the proposed citation-based methods, developed for the legal domain, are general enough to be applied to different kinds of text to obtain different kinds of summaries; they perform equally as well as scientific citation summarization tools, while outperforming the same methods (LexRank) on legal text summarization.

The results suggest that for different domains, and for different summarization goals, different subsets of methods should be preferred. The power of the proposed methods is that they comprise different strategies, such as using different combi-nations of citations and full-text elements, that allow us to target diverse kinds of documents and summaries. It seems that the most important factor is not which method is used to rank the elements (i.e. how to choose the best citances), but which elements we should use (e.g. using citances rather than sentences from the full text). However, to date most research effort seems to have gone into selecting the best possible subset of citations, rather than considering where citations should be preferred and where they should be complemented with other elements.

Finally, all the different proposed methods use diverse kinds of information to identify relevant text. It therefore seems reasonable to assume that a combination of different methods in a single approach might increase the performance when compared to using a single method. Using citations to select sentences is a first step in this direction. To our knowledge, no previous summarization system which uses a combination of textual elements from citations, abstracts and full texts has been proposed. This is probably due to the fact that a complex way of combining the different information is required.
In ( Galgani, Compton, &amp; Hoffmann, 2012b ) we proposed a knowledge-based approach to build summarizers which combine different information about cases (frequency, centrality, citations, etc.), but more research is needed on how best to achieve this. We also expect that using discourse analysis in combination with our approach is likely to allow a better assessment of the purpose of a citation and may be a promising path that might yield further improvements in areas such as scientific papers. Future research is needed to shed light on this.
 Acknowledgements
The authors would like to thank Philip Chung, Andrew Mowbray and Graham Greenleaf from AustLII for their valuable help and constructive comments.
 References
