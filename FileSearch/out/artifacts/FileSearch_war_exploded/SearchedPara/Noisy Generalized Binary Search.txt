 This paper studies learning problems of the following form. Consider a finite, but potentially very large, collection of binary-valued functions H defined on a domain X . In this paper, H will be called the hypothesis space and X will be called the query space . Each h  X  H is a mapping from X to { X  1 , 1 } . Assume that the functions in H are unique and that one function, h  X   X  H , produces the correct binary labeling. The goal is to determine h  X  through as few queries from X as possible. For each query x  X  X , the value h  X  ( x ) , corrupted with independently distributed binary noise, is ob-served. If the queries were noiseless, then they are usually called membership queries to distinguish them from other types of queries [Ang01]; here we will simply refer to them as queries. Problems of this nature arise in many applications , including channel coding [Hor63], experimental design [R  X  en61], disease diagnosis [Lov85], fault-tolerant computing [FRPU94], job scheduling [KPB99], image processing [KK00], computer vision [SS93, GJ96], computational geometry [AMM + 98], and active learning [Das04, BBZ07, Now08].
 Past work has provided a partial characterization of this problem. If the responses to queries are noiseless, then selecting the optimal sequence of queries from X is equivalent to determining an optimal binary decision tree, where a sequence of queries defines a path from the root of the tree mination of the optimal tree is NP-complete [HR76]. However, there exists a greedy procedure that yields query sequences that are within an O (log |H| ) factor of the optimal search tree depth [GG74, KPB99, Lov85, AMM + 98, Das04], where |H| denotes the cardinality of H . The greedy procedure is referred to as Generalized Binary Search (GBS) [Das04, Now08] or the splitting al-gorithm [KPB99, Lov85, GG74]), and it reduces to classic binary search in special cases [Now08]. The GBS algorithm is outlined in Figure 1(a). At each step GBS selects a query that results in the most even split of the hypotheses under consideration into two subsets responding +1 and  X  1 , respectively, to the query. The correct response to the query eliminates one of these two subsets from further consideration. Since the hypotheses are assumed to be distinct, it is clear that GBS terminates in at most |H| queries (since it is always possible to find query that eliminates at least
Generalized Binary Search (GBS) initialize: i = 0 , H 0 = H . while |H i | &gt; 1 1) Select x i = arg min x  X  X  | P h  X  X  2) Obtain response y i = h  X  ( x i ) . 3) Set H i +1 = { h  X  X  i : h ( x i ) = y i } , i = i + 1 .
 one hypothesis at each step). In fact, there are simple examples demonstrating that this is the best one can hope to do in general [KPB99, Lov85, GG74, Das04, Now08]. However, it is also true that in many cases the performance of GBS can be much better [AMM + 98, Now08]. In general, the number of queries required can be bounded in terms of a combinatorial parameter of H called the extended teaching dimension [Ang01, Heg95] (also see [HPRW96] for related work). Alternatively, there exists a geometric relation between the pair ( X , H ) , called the neighborly condition, that is sufficient to bound the number of queries needed [Now08].
 The focus of this paper is noisy GBS. In many (if not most) applications it is unrealistic to assume that the responses to queries are without error. Noise-tolerant versions of classic binary search have been well-studied. The classic binary search problem is equivalent to learning a one-dimensional binary-valued threshold function by selecting point evaluations of the function according to a bisec-tion procedure. A noisy version of classic binary search was studied first in the context of channel coding with feedback [Hor63]. Horstein X  X  probabilistic bisection procedure [Hor63] was shown to be optimal (optimal decay of the error probability) [BZ74] (also see[KK07]).
 One straightforward approach to noisy GBS was explored in [Now08]. The idea is to follow the GBS algorithm, but to repeat the query at each step multiple times in order to decide whether the response is more probably +1 or  X  1 . The strategy of repeating queries has been suggested as a general approach for devising noise-tolerant learning algorithms [K  X  a  X  a06]. This simple approach has been studied in the context of noisy versions of classic binary search and shown to be suboptimal [KK07]. Since classic binary search is a special case of the general problem, it follows immediately that the approach proposed in [Now08] is suboptimal. This paper addresses the open problem of determining an optimal strategy for noisy GBS. An optimal noise-tolerant version of GBS is developed here. The number of queries an algorithm requires to confidently identify h  X  is called the query complexity of the algorithm. The query complexity of the new algorithm is optimal, and we are not aware of any other algorithm with this capability.
 It is also shown that optimal convergence rate and query complexity is achieved for a broad class of geometrical hypotheses arising in image recovery and binary classification. Edges in images and decision boundaries in classification problems are naturally viewed as curves in the plane or sur-faces embedded in higher-dimensional spaces and can be associated with multidimensional thresh-old functions valued +1 and  X  1 on either side of the curve/surface. Thus, one important setting for GBS is when X is a subset of d dimensional Euclidean space and the set H consists of multidimen-sional threshold functions. We show that our algorithm achieves the optimal query complexity for actively learning multidimensional threshold functions in noisy conditions.
 The paper is organized as follows. Section 2 describes the Bayesian algorithm for noisy GBS and presents the main results. Section 3 examines the proposed method for learning multidimensional threshold functions. Section 4 discusses an agnostic algorithm that performs well even if h  X  is not in the hypothesis space H . Proofs are given in Section 5. In noisy GBS, one must cope with erroneous responses. Specifically, assume that the binary response y  X  X  X  1 , 1 } to each query x  X  X  is an independent realization of the random variable Y satisfying P ( Y = h  X  ( x )) &gt; P ( Y =  X  h  X  ( x )) , where h  X   X  H is fixed but unknown. In other words, the response is only probably correct. If a query x is repeated more than once, then each response is an independent realization of Y . Define the noise-level for the query x as  X  x := P ( Y =  X  h  X  ( x )) . Throughout the paper we will let  X  := sup x  X  X   X  x and assume that  X  &lt; 1 / 2 .
 A Bayesian approach to noisy GBS is investigated in this paper. Let p 0 be a known probability mea-sure over H . That is, p 0 : H  X  [0 , 1] and P h  X  X  p 0 ( h ) = 1 . The measure p 0 can be viewed as an initial weighting over the hypothesis class, expressing the fact that all hypothesis are equally reason-is updated according to normalized to satisfy P h  X  X  p i +1 ( h ) = 1 . The update can be viewed as an application of Bayes rule and its effect is simple; the probability masses of hypotheses that agree with the label y i are boosted relative to those that disagree. The parameter  X  controls the size of the boost. The hypothesis with to zero as quickly as possible by strategically selecting the queries. A similar procedure has been shown to be optimal for noisy (classic) binary search problem [BZ74, KK07]. The crucial distinction here is that GBS calls for a fundamentally different approach to query selection.
 The query selection at each step must be informative with respect to the distribution p i . For example, informative due to the large disagreement among the hypotheses. This suggests the following noise-tolerant variant of GBS outlined in Figure 1. This paper shows that a slight variation of the query selection in the NGBS algorithm in Figure 1 yields an algorithm with optimal query complexity. It is shown that as long as  X  is larger than the noise-level of each query, then the NGBS produces decreasing sequence (see Theorem 1). The main interest of this paper is an algorithm that drives the error to zero exponentially fast, and this requires the query selection criterion to be modified slightly. To see why this is necessary, suppose that at some step of the NGBS algorithm a single hypothesis (e.g., h  X  ) has the majority of the probability mass. Then the weighted prediction will be almost equal to the prediction of that hypothesis (i.e., close to +1 or  X  1 for all queries), and therefore the responses to all queries are relatively certain and non-informative. Thus, the convergence of the algorithm could become quite slow in such conditions. A similar effect is true in the case of noisy (classic) binary search [BZ74, KK07]. To address this issue, the query selection criterion is modified via randomization so that the response to the selected query is always highly uncertain. In order to state the modified selection procedure and the main results, observe that the query space X can be partitioned into equivalence subsets such that every h  X  H is constant for all queries in each such subset. Let A denote the smallest such partition. Note that X = S A  X  X  A . For every A  X  A and h  X  H , the value of h ( x ) is constant (either +1 or  X  1 ) for all x  X  A ; denote this value by h ( A ) . As first noted in [Now08], A can play an important role in GBS. In particular, observe that the query selection step in NGBS is equivalent to an optimization over A rather that X itself. The randomization of the query selection step is based on the notion of neighboring sets in A . Definition 1 Two sets A,A 0  X  A are said to be neighbors if only a single hypothesis (and its complement, if it also belongs to H ) outputs a different value on A and A 0 .
 The modified NGBS algorithm is outlined in Figure 2. Note that the query selection step is identical to that of the original NGBS algorithm, unless there exist two neighboring sets with strongly bipolar weighted responses. In the latter case, a query is randomly selected from one of these two sets with equal probability, which guarantees a highly uncertain response.
 Theorem 1 Let P denotes the underlying probability measure (governing noises and algorithm ran-domization). If  X  &gt;  X  , then both the NGBS and modified NGBS algorithms, in Figure 1(b) and { a n } n  X  0 is a monotonically decreasing sequence.
 The condition  X  &gt;  X  ensures that the update (1) is not overly aggressive. We now turn to the matter of sufficient conditions guaranteeing that P ( b h n 6 = h  X  )  X  0 exponentially fast with n . The exponential convergence rate of classic binary search hinges on the fact that the hypotheses can be ordered with respect to X . In general situations, the hypothesis space cannot be ordered in such a fashion, but the neighborhood graph of A provides a similar local structure.
 Definition 2 The pair ( X , H ) is said to be neighborly if the neighborhood graph of A is connected (i.e., for every pair of sets in A there exists a sequence of neighboring sets that begins at one of the pair and ends with the other).
 In essence, the neighborly condition simply means that each hypothesis is locally distinguishable from all others. By  X  X ocal X  we mean in the vicinity of points x where the output of the hypothesis changes from +1 to  X  1 . The neighborly condition was first introduced in [Now08] in the analysis hypothesis spaces consisting of multidimensional threshold functions. If ( X , H ) is neighborly, then the modified NGBS algorithm guarantees that P ( b h i 6 = h  X  )  X  0 exponentially fast. Theorem 2 Let P denotes the underlying probability measure (governing noises and algorithm ran-domization). If  X  &gt;  X  and ( X , H ) is neighborly, then the modified NGBS algorithm in Figure 2 generates a sequence of hypotheses satisfying The exponential convergence rate 1 is governed by the key parameter 0  X  c  X  &lt; 1 . The minimizer in (2) exists because the minimization can be computed over the space of finite-dimensional probability mass functions over the elements of A . As long as no hypothesis is constant over the whole of X , the value of c  X  is typically a small constant much less than 1 that is independent of the size of H (see [Now08, Now09] and the next section for concrete examples). In such situations, the convergence rate of modified NGBS is optimal, up to constant factors. No other algorithm can solve the noisy GBS problem with a lower query complexity. The query complexity of the modified NGBS algorithm can be derived as follows. Let  X  &gt; 0 be a prespecified confidence parameter. The number optimal query complexity. Intuitively, O (log |H| ) bits are required to encode each hypothesis. More formally, the classic noisy binary search problem satisfies the assumptions of Theorem 2 [Now08], and hence it is a special case of the general problem. It is known that the optimal query complexity for noisy classic binary search is O (log |H|  X  ) [BZ74, KK07].
 We contrast this with the simple noise-tolerant GBS algorithm based on repeating each query in the standard GBS algorithm of Figure 1(a) multiple times to control the noise (see [K  X  a  X  a06, Now08] for related derivations). It follows from Chernoff X  X  bound that the query complexity of determining the queries at each step to guarantee that the labels determined for all n 0 queries are correct with prob-ability 1  X   X  . If ( X , H ) is neighborly, then GBS requires n 0 = O (log |H| ) queries in noiseless conditions [Now08]. Therefore, under the conditions of Theorem 2, the query complexity of the simple noise-tolerant GBS algorithm is O (log |H| log log |H|  X  ) , a logarithmic factor worse than the optimal query complexity. We now apply the theory and modified NGBS algorithm to the problem of learning multidimensional threshold functions from point evaluations, a problem that arises commonly in computer vision [SS93, GJ96, AMM + 98], image processing [KK00], and active learning [Das04, BBZ07, CN08, Now08]. In this case, the hypotheses are determined by (possibly nonlinear) decision surfaces in Note that hypotheses of this form can be used to represent nonlinear decision surfaces by applying a nonlinear mapping to the query space.
 Theorem 3 Let H be a finite collection of hypotheses of form sign (  X  a,x  X  + b ) , for some constant c &lt;  X  . Then the hypotheses selected by the modified NGBS algorithm with  X  &gt;  X  satisfy Based on the discussion at the end of the previous section, we conclude that the query complexity of the modified NGBS algorithm is O (log |H| ) ; this is the optimal up to constant factors. The only other algorithm with this capability that we are aware of was analyzed in [BBZ07], and it is based on a quite different approach tailored specifically to linear threshold problem. We also mention the possibility of agnostic algorithms guaranteed to find the best hypothesis in H even if the optimal hypothesis h  X  is not in H and/or the assumptions of Theorem 2 or 3 do not hold. The best hypothesis in H is the one that minimizes the error with respect to a given probability mea-sure on X , denoted by P X . The following theorem, proved in [Now09], demonstrates an agnostic algorithm that performs almost as well as empirical risk minimization (ERM) in general, and has the optimal O (log |H| / X  ) query complexity when the conditions of Theorem 2 hold.
 Theorem 4 Let P X denote a probability distribution on X and suppose we have a query budget of n . Let h 1 denote the hypothesis selected by modified NGBS using n/ 3 of the queries and let h 2 denote the hypothesis selected by ERM from n/ 3 queries drawn independently from P X . Draw the remaining n/ 3 queries independently from P  X  , the restriction of P X to the set  X   X  X  on which h 1 h 2 on these queries. Select b h = arg min { b R  X  ( h 1 ) , b R  X  ( h 2 ) } . Then, in general, where R ( h ) , h  X  H , denotes the probability of error of h with respect to P X and E denotes the expectation with respect to all random quantities. Furthermore, if the assumptions of Theorem 2 hold with noise bound  X  , then 5.1 Proof of Theorem 1 C n  X  [0 ,  X  ) reflects the amount of mass that p n places on the suboptimal hypotheses. First note that Next, observe that Note that because p 0 is assumed to be uniform, C 0 = | H |  X  1 . A similar conditioning tech-nique is employed for interval estimation in [BZ74]. The rest of the proof entails showing that E weighted proportion of hypotheses that agree with y i . The factor that normalizes the updated dis-P Denote the reciprocal of the update factor for p i +1 ( h  X  ) by where z i ( h  X  ) = h  X  ( x i ) y i , and observe that p i +1 ( h  X  ) = p i ( h  X  ) / X  i . Thus, this, we will assume that p i is arbitrary.
 For every A  X  A and every h  X  H let h ( A ) denote the value of h on the set A . Define  X  + A = all h  X  X  . Let A i denote that set that x i is selected from, and consider the four possible situations: To bound E [  X  i | p i ] it is helpful to condition on A i . Define q i := P x,y | A +1 , then Define  X  + i ( A i ) :=  X  + A  X  &lt; 1 . Define to obtain the bounds Since both  X  + i ( A i ) and  X   X  i ( A i ) are less than 1 , it follows that E [  X  i | p i ] &lt; 1 . 5.2 Proof of Theorem 2 The proof amounts to obtaining upper bounds for  X  + i ( A i ) and  X   X  i ( A i ) , defined above in (4) and (5). For every A  X  A and any probability measure p on H the weighted prediction on A is defined following lemma plays a crucial role in the analysis of the modified NGBS algorithm.
 Lemma 1 If ( X , H ) is neighborly, then for every probability measure p on H there either exists a set A  X  A such that | W ( p,A ) |  X  c  X  or a pair of neighboring sets A,A 0  X  A such that W ( p,A ) &gt; c  X  and W ( p,A 0 ) &lt;  X  c  X  .
 Proof of Lemma 1 : Suppose that min A  X  X  | W ( p,A ) | &gt; c  X  . Then there must exist A,A 0  X  A such that W ( p,A ) &gt; c  X  and W ( p,A 0 ) &lt;  X  c  X  , otherwise c  X  cannot be the minimax moment c The neighborly condition guarantees that there exists a sequence of neighboring sets beginning at A point in the sequence, it follows that there exist neighboring sets satisfying the claim. this implies that b i  X  c  X  , and according the query selection step of the modified NGBS algorithm, A Hence, both  X  + i ( A i ) and  X   X  i ( A i ) are bounded above by 1  X   X  0 (1  X  c  X  ) / 2 .  X 
A &gt; (1 + b i ) / 2 and  X  E [  X  i | p i ,A i  X  X  A,A 0 } ] &lt; 1  X   X  0 / 4 . If h  X  ( A ) =  X  1 on A and h  X  ( A 0 ) = +1 , then applying (5) on A and (4) on A 0 yields A and (5) on A 0 to obtain not belong to H , then p i (  X  h  X  ) = 0 . Hence,
E [  X  i | p i ,A i  X  X  A,A 0 } ]  X  since the bound is maximized when p i (  X  h  X  ) = 0 . Now bound E [  X  i | p i ] by the maximum of the conditional bounds above to obtain and thus it is easy to see that 5.3 Proof of Theorem 3 First we show that the pair ( R d , H ) is neighborly (Definition 2). Each A  X  A is a polytope in R d . These polytopes are generated by intersections of the halfspaces corresponding to the hypotheses. Any two polytopes that share a common face are neighbors (the hypothesis whose decision boundary defines the face, and its complement if it exists, are the only ones that predict different values on these two sets). Since the polytopes tessellate R d , the neighborhood graph of A is connected. Next consider the final bound in the proof of Theorem 2, above. We next show that the value of c  X  , defined in (2), is 0 . Since the offsets b of the hypotheses are all less than c in magnitude, it follows that the distance from the origin to the nearest point of the decision surface of every hypothesis is at most c . Let P r denote the uniform probability distribution on a ball of radius r centered at the origin in R d . Then for every h of the form sign (  X  a,x  X  + b ) and lim r  X  X  X  R X h ( x ) dP r ( x ) = 0 and so c  X  = 0 .
 Lastly, note that the modified NGBS algorithm involves computing P h  X  H p i ( h ) h ( A ) for all A  X  X  at each step. The computational complexity of each step is therefore proportional to the cardinality of A , which is equal to the number of polytopes generated by intersections of half-spaces. It is known that |A| = P d i =0 | H | i = O ( | H | d ) [Buc43]. [AMM + 98] E. M. Arkin, H. Meijer, J. S. B. Mitchell, D. Rappaport, and S.S. Skiena. Decision trees [Ang01] D. Angluin. Queries revisited. Springer Lecture Notes in Comp. Sci.: Algorithmic [BBZ07] M.-F. Balcan, A. Broder, and T. Zhang. Margin based active learning. In Conf. on [Buc43] R. C. Buck. Partition of space. The American Math. Monthly , 50(9):541 X 544, 1943. [BZ74] M. V. Burnashev and K. Sh. Zigangirov. An interval estimation problem for controlled [CN08] R. Castro and R. Nowak. Minimax bounds for active learning. IEEE Trans. Info. [Das04] S. Dasgupta. Analysis of a greedy active learning strategy. In Neural Information [FRPU94] U. Feige, E. Raghavan, D. Peleg, and E. Upfal. Computing with noisy information. [GG74] M. R. Garey and R. L. Graham. Performance bounds on the splitting algorithm for [GJ96] D. Geman and B. Jedynak. An active testing model for tracking roads in satellite [Heg95] T. Heged  X  us. Generalized teaching dimensions and the query complexity of learning. [Hor63] M. Horstein. Sequential decoding using noiseless feedback. IEEE Trans. Info. Theory , [HPRW96] L. Hellerstein, K. Pillaipakkamnatt, V. Raghavan, and D. Wilkins. How many queries [HR76] L. Hyafil and R. L. Rivest. Constructing optimal binary decision trees is NP-complete. [KK00] A. P. Korostelev and J.-C. Kim. Rates of convergence fo the sup-norm risk in image [KK07] R. Karp and R. Kleinberg. Noisy binary search and its applications. In Proceedings [KPB99] S. R. Kosaraju, T. M. Przytycka, and R. Borgstrom. On an optimal split tree problem. [Lov85] D. W. Loveland. Performance bounds for binary testing with arbitrary weights. Acta [Now08] R. Nowak. Generalized binary search. In Proceedings of the 46th Allerton Conference [Now09] R. Nowak. The geometry of generalized binary search. 2009. Preprint available at [R  X  en61] A. R  X  enyi. On a problem in information theory. MTA Mat. Kut. Int. Kozl. , page 505516, [SS93] M.J. Swain and M.A. Stricker. Promising directions in active vision. Int. J. Computer
