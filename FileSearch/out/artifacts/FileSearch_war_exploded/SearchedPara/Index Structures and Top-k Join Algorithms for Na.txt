 For supporting keyword search on structured data, current solutions require large indexes to be built that redundantly store subgraphs called neighborhoods. Further, for explor-ing keyword search results, large graphs have to be loaded into memory. We propose a solution, which employs much more compact index structures for neighborhood lookups. Using these indexes, we reduce keyword search result explo-ration to the traditional database problem of top-k join pro-cessing, enabling results to be computed efficiently. In par-ticular, this computation can be performed on data streams successively loaded from disk (i.e., does not require the en-tire input to be loaded at once into memory). For supporting this, we propose a top-k procedure based on the rank join operator, which not only computes the k -best results, but also selects query plans in a top-k fashion during the pro-cess. In experiments using large real-world datasets, our so-lution reduced storage requirements and also outperformed the state-of-the-art in terms of performance and scalability. Categories and Subject Descriptors: H.2.4 [Database Management]: Systems General Terms: Algorithms, Performance Keywords: graph database, keyword search, top-k process-ing
Today, we find on the Web not only textual documents but also a large amount of structured data. This includes metadata associated with Web pages, semi-structured data, and highly-structured information made publicly available such as Linked Data 1 .

Especially for lay users, keyword search has been regarded as an effective mechanism because it helps to circumvent the complexity of structured query languages, and hide the underlying data representation. Without knowledge of the query syntax and data schema, users can obtain possibly http://linkeddata.org/ complex structured results, including tuples from relational databases, XML data, data graphs, and RDF resources[4, 17]. As opposed to document retrieval, results in this struc-tured data setting may encompass several resources that are connected over possibly very long paths (e.g. joined database tuples, XML trees, connected RDF resources).
There are two directions of research that aim at support-ing this. First, there are schema-based approaches imple-mented on top of off-the-shelf databases ([5, 12, 13, 15]). These approaches find candidate networks, which represent valid join sequences that are employed as queries to retrieve the final results. The main advantage here is that the power and optimization capabilities of the underlying database en-gine can be fully utilized. Second, there are schema-agnostic approaches ([9, 4, 11]), which operates directly on the data. Since they do not rely on a schema, the applicability of these approaches is not limited to structured data. For instance, semi-structured RDF data [17] as well as the combination of structured, semi-structured and unstructured data [11] have been supported.

In this paper, we focus on the latter type, also called native approaches . Fig. 1 shows a data example, describing peo-ple ( p 1  X  p 4) and their relationships. For computing results of a query such as Q 1 = { miller, corp } , a native approach firstly finds matching elements in the data graph, which are miller = { p 1 , p 4 } and corp = { o 1 , o 2 } . Next, struc-tures connecting these so-called keyword elements are ex-plored. These structures constitute the query results, which are called Steiner trees [3], or Steiner graphs ([17, 11]).
Since the search space may be large, two main directions have been investigated for optimization. One is based on materializing paths [4]. The fastest solution proposed re-cently, called EASE, is based on materializing even more complex structures in the form of graphs, which capture entire neighborhoods of data elements [11]. The other di-rection is to improve the efficiency of the exploration al-gorithm ([9, 3]). The main drawbacks of current state-of-the-art is that materialized indexes [11], which redundantly store neighborhood information may become too large. Fur-ther, existing search strategies for efficient result exploration assume the entire data to be available in memory, a require-ment that is too limited when the amount of data is large.
The index structures and algorithms proposed so far are very specific to the keyword search problem. In this work, we aim to reduce this problem to more standard database ones to leverage the vast body of existing work. In particular, we propose the concept of native keyword search databases , which solves the keyword search problem using two standard database operations, data access and join . Instead of loading the data entirely at once and exploring results in memory, keyword search is supported through a series of fine granular data access (path lookups) and join operations (path and graph joins). For supporting these operations, we build upon existing work on index structures and top-k join processing. The main contributions can be summarized as follows:
We evaluated our approach by comparing it to state-of-the-art native keyword search approaches ([11, 9]), using several large real-world datasets. Our solution reduced stor-age requirement (up to 86%) and improved scalability (by several factors) and performance (over 50% on average).
Outline. In Section 2 we present our data model and a formal definition of keyword search. Section 3 describes our extension of the 2-hop cover concept. In Section 4 we show how keyword queries can be answered using top-k join processing on the proposed data structures. Related work is presented in Section 5. In Section 6 we present the result of our evaluation. We conclude in Section 7. This work deals with general graph structured data:
Definition 1 (Graph Structured Data). Data is modeled as a directed, labeled graph G ( N, E ) called data graph , where N = N E ] N A is a set of nodes , conceived as the disjoint union of entities N E and attribute values N A , and E = E R ] E A is a set of directed edges , where E are edges between entities called relations , and E A are edges between an entity and an attribute value called attributes . The length of a path denotes the number of edges in the path. We use distance between u, w to denote the length of the shortest path between u and w . Two graph elements (nodes or edges) are connected if they appear together in a path (no matter the directions of edges in that path). In addition to graph-structured data such as RDF, this model can also be used to capture relational databases: a tuple cap-tures an entity and its attribute values; and tuples with a foreign key relationship are connected via a relation edge.
Query keywords match entities X  attribute value nodes, or labels of attribute or relation edges. Thus, keyword matches can always be associated with some entities. We call these matching entities keyword elements :
Definition 2 (Keyword Element). Given a keyword query K consisting of keywords k 1 , . . . , k n , a node n is a keyword element for k  X  K iff there is e ( n k , a )  X  E and k matches any of the elements e, n k or a (i.e., k matches edges that contain n k in the  X  X ubject X  position). The set of all keyword elements for k is N k .

For processing a keyword query, we firstly find keyword elements in the data graph. Then, we search for substruc-tures, which connect the keyword elements. The most com-monly used substructures are Steiner trees [9], i.e., minimal rooted trees in the data graph, which contains at least one keyword element for every keyword in the query. Instead of rooted trees, general graphs have also been used [17, 11]. This notion of Steiner graph is also employed in this paper:
Definition 3 (Keyword Query Answer). An answer to a keyword query K = { k 1 , . . . , k n } also called Steiner graph is a subgraph of G denoted as G S = ( N S , E S ) , which satisfies the following conditions: We call such a graph a d -length Steiner graph when direct paths , i.e., paths that connect exactly two keyword elements n , n j s.t. there is no other keyword element n k in the path between n i and n j , have length d or less (we use n i ! to denote these paths).

This notion captures the standard semantics of keyword query answers [9, 17]. Limited by its internal index struc-ture, EASE [11] departs from this and employs an alter-native semantics: while it also searches for Steiner graphs, it implicitly assumes one  X  X enter keyword element X  that is connected to all other keyword elements over a maximum distance d . While this leads to a more restricted search space, results corresponding to the standard semantics may be missed. In this work, we support the standard seman-tics. The d -length restriction only implies that for every keyword element n i in the Steiner graph, there is at least one other keyword element connected to it via a path of length d or less (i.e., the directly connected one), while all other keyword elements may be (indirectly) connected to it via longer paths.

The computation of top-k Steiner graphs typically re-quires a monotonic function for ranking. Widely used in keyword search is the score of a node calculated using a probabilistic IR model. Scores which measure nodes X  pres-tige have also been incorporated  X  PageRank for instance [6]. Besides, keyword search solutions generally rest on the assumption that more compact Steiner graphs more likely match the user needs, i.e., the length of paths connecting two keyword elements has a negative effect on the rank. The solution proposed here is orthogonal to the ranking function being used. For ease of exposition, we use path length as the only metric:
Definition 4. Let G S be a Steiner graph and P be the set of direct paths that connect its keyword elements, the rank of G S is determined by Score ( G S ) = P p  X  P Score ( p ) , where The solution we propose is based on two main operations. For data access , a specialized index is employed to retrieve connecting paths for every pair of keyword elements. In a top-k fashion, these paths are then joined to construct the best ranked Steiner graphs.
A 2-hop cover is a compact representation of connections in a graph that has been previously used to answer reachabil-ity and distance queries [16, 1]. In this work, we propose to leverage this concept for retrieving paths between two key-word elements. Our data graph is directed. However, while it is essential to find paths during the computation of Steiner graphs, the direction of the edges that establish these paths are irrelevant (have to be preserved only for outputting the actual results). For the efficient retrieval of paths, we pro-pose the d -length 2-hop cover, which in contrast to the 2-hop cover employed in existing work, only considers paths up to length d . First, we define the neighborhood of a node u that captures paths from u to nodes in its vicinity:
Definition 5 (Neighborhood). A neighborhood la-bel (or short: neighborhood) N B u  X  G of a node u  X  N (also called the center node of N B u ) is the union set of nodes and edges that are connected to u via some paths. The set of paths between u and a node w  X  N B u is denoted as P ( u, w ) , The combined information is represented as a path entry ( u, s, w ) (or short: ( u, w ) ), where u is the center node of N B u , w  X  N B u and s = Score ( P ( u, w )) . A d -neighborhood of node u is the set of nodes and edges connected to u via paths of length d or less.

Definition 6 ( d -length 2-hop Cover). A labeling of a graph G = ( N, E ) , which consists of a set of neighborhood labels is a d -length 2-hop cover , if the following two condi-tions hold for any two nodes u, v  X  N E : 1) if there is path of length d or less between u and v then N B u  X  N B v 6 =  X  ; 2) all paths of length d or less between the center nodes u and v are of the form  X  u, . . . , w, . . . , v  X  , where w  X  N B ( w is called a hop node ).

This cover is used to find all paths of length d or less be-tween two nodes by forming the intersection of their neigh-borhoods, i.e., the set of paths between u and v is P ( u, v ) = { p uw ++ p wv | w  X  N B u  X  N B v , p uw  X  P ( u, w ), p wv  X  P ( w, v ), Length ( p uw p wv )  X  d } .
The problem of constructing a 2-hop cover of minimal size has been reduced to the minimum set cover problem that is NP-hard [2]. Consequently, approximative algorithms are necessary for dealing with large graphs [16, 1]). The basic idea is as follows: in a greedy manner, one neighborhood is selected at every iteration to prune redundant paths in other neighborhoods. The goal is to reduce redundancy as much as possible, while preserving the 2-hop cover property.
We adopt existing algorithms in two directions. (1) Firstly we define a trivial d -length 2-hop cover for the keyword search setting, which is used as a basis for later pruning. (2) Then, the d -length restriction is taken into account during the pruning of paths. The detailed algorithm, complexity and proofs are presented in our report [10].

Basically, we observe that a trivial d -length 2-hop cover is simply the set of d -neighborhoods :
Theorem 1. The set of d -neighborhoods constructed for every node u  X  N in G ( N, E ) is a valid d -length 2-hop cover.
Proof Sketch: By definition, a d -neighborhood of a node u contains all nodes in G ( N, E ) reachable from u via a path of length d or less. Thus, for all u, v  X  N , u  X  N B u v  X  N B v , if u ! d v then u  X  N B v and v  X  N B u . Hence, N B u  X  N B v must be not empty (this ensures condition 1). Further, we know the set N B u  X  N B v contains hop nodes for all paths of length 2  X  d or less, which include all paths of length d or less (this ensures condition 2).

Then, the following intuition is employed for pruning re-dundant paths captured by this trivial d -length 2-hop cover: Given two nodes u, v  X  N that are connected via the set of paths P uv =  X  u, . . . , w, . . . , v  X  of length d or less, N B contains some  X  X artial paths X  P wv =  X  w, . . . , v  X  that are also in N B v , and vice versa N B v contains the parts P  X  u, . . . , w  X  that are also in N B u . Then, these parts are re-dundant in the sense that after removing P wv from N B and P uw from N B v , all the paths in P uv are still preserved and can be computed via hop nodes in N B u  X  N B v .
The approximate algorithm for this consists of | N  X  1 | iterations. At each step, a neighborhood N B i is selected (based on  X  X runing power X  simply measured in terms of node counts), marked as complete, and used for pruning other neighborhoods not completed yet. Neighborhood pruning is discussed in the example below.

Example 1. Fig. 2 shows the two not yet pruned neigh-borhoods of p 2 and p 3 . Here, we prune N B p 3 using N B starting a simultaneous breadth-first search towards the leaf nodes from p 2 . All edges in N B p 3 that also occur in N B are marked and later removed from N B p 3 . The part pruned this way is highlighted in Fig. 2.

Note that the procedure here operates at the level of paths while existing work on keyword search applies pruning at the level of neighborhoods, i.e., a neighborhood is discarded only when it is completely covered by one another [11].
We store neighborhoods as well as the actual paths. We define the path and path entry indexes, which are used dur-ing the computation of Steiner graphs.

Definition 7 (Path Entry Index). The path entry index I P E maps nodes u  X  N to a list of path entries ( u, s, w ) , sorted by s = Score ( P ( u, w )) , where w  X  N B
Definition 8 (Path Index). The path index I P maps a path entry ( u, w ) to a list of paths P ( u, w ) , sorted by Score ( p ) .

Example 2. Tab. 1 shows an index entry for node p 2 from the running example. It contains path entries from p 2 to all hop nodes in the neighborhood with their associ-ated scores. Recall that this is simply the maximal score of paths from p 2 to a hop node. Tab. 2 shows an excerpt of the path index for nodes in the neighborhood of p 2 . For example, scores) from p 2 to o 1 . In this case there are two such paths.
In this section we present the process of answering key-word queries using a d -length 2-hop cover.
Given a keyword query K and its keyword elements N K , the goal is to find Steiner graphs. The basic idea is to use the pruned neighborhoods of the d -length 2-hop cover to find paths between every pair of keyword elements and itera-tively join them until they all are connected. For this, we (1) firstly perform data access operations to retrieve the neigh-borhoods for every keyword, called keyword neighborhoods , (2) then perform neighborhood join to merge two keyword neighborhoods to obtain a keyword graph and then succes-sively (3) apply graph joins to combine a keyword neighbor-hood with a keyword graph.

Definition 9 (Keyword Neighborhood). Given a keyword k and its keyword elements N k , the keyword neigh-borhood N B k of k is the union set of path entries retrieved from the index I P E . It captures the neighborhoods of all key-word elements in N k , i.e., N B k = S n  X  N
Definition 10 (Neighborhood Join). Given two key-word neighborhoods N B k 1 , N B k 2 , the neighborhood join 1
NB combines two path entries ( n k 1 , w ) in N B k 1 and ( n in N B k 2 that match on w , i.e., N B k 1 1 NB N B k 2 = { ( n + ( n k 2 , w ) | ( n k 1 , w )  X  N B k 1 , ( n k 2 , w )  X  N B + ( n k 2 , w ))  X  d } . The result ( n k 1 , w ) ++ ( n a path between n k 1 and n k 2 with w being the hop node. A join of 2 or more path entries such as ( n k 1 , w ) ++ ( n form a keyword graph G K = ( N K , P E K ) with N K subdi-vided into the two disjoint sets of center nodes N C K and hop nodes N H K . For every path entry ( n, w )  X  P E K , we have n  X  N C K and w  X  N H K .
 Based on this 1 NB operation, we provide Theorem 2 for computing results of 2-keyword queries.

Theorem 2. Given a 2-keyword query K = { k 1 , k 2 } and the retrieved keyword neighborhoods N B k 1 , N B k 2 , the key-word graphs resulting from N B k 1 1 NB N B k 2 are d -length Steiner graphs for K .

Proof Sketch: In this 2-keywords setting, the set of all d -length Steiner graphs for K = { k 1 , k 2 } is exactly the set of all paths of length d or less between the pairs of nodes n 1  X  N k 1 , n 2  X  N k 2 . By definition of the d -length 2-hop cover, the intersection N B n 1  X  N B n 2 contains the hop nodes for alls path of length d or less between n 1 and n neighborhood join N B k 1 1 N B k 2 leverages this, joining all paths in N B k 1 with paths in N B k 2 on the hop node w  X  N B n 1  X  N B n 2 . The result comprises all paths connecting the pairs of nodes n 1  X  N k 1 , n 2  X  N k 2 that are of length d or less, i.e., all d -length Steiner graphs for K . and p 4 and (intermediate) results.

A keyword graph resulting from the neighborhood join contains only path entries in N B k 1 and N B k 2 , which con-nect the two keyword elements n k 1 and n k 2 . Thus, only hop nodes in these paths remain, while other nodes in the neighborhood of n k 1 and n k 2 have been eliminated during the process. For the subsequent graph join 1 G , which takes a keyword graph G K and a keyword neighborhood N B k as inputs, these hop nodes still have to be considered because they might help in connecting G K with N B k . In order to include these candidates during the execution of 1 G , a key-word graph is expanded with path entries discarded previ-ously to obtain a keyword graph neighborhood: Definition 11 (Keyword Graph Neighborhood).
 Given a keyword graph G K = ( N C K ] N H K , P E K ) , the key-word graph neighborhood N B G K is a set of graph entries constructed from G K and a path entry ( n, w ) , i.e., N B { G K ++ ( n, w ) | ( n, w )  X  I P E ( n ) , n  X  N C K } , where every G + ( n, w ) forms a graph entry denoted as G + K .
 As a result of this expansion, each graph entry G + K contains a  X  X ree X  path entry ( n, w ), whose hop node w is yet not connected. Similar to 1 NB , this hop node is used as the join attribute of the 1 G operation:
Definition 12 (Graph Join). Given a keyword neigh-borhood N B k i and a keyword graph neighborhood N B G the result of a graph join 1 G is the combination of path en-tries ( n k i , w )  X  N B k i and graph entries G + K ( N N B G K defined as N B k i 1 G N B G K = { G + K ( N K , P E + ( n k i , w ) |  X  ( n k j , w )(( n k j , w )  X  P E K ) } . The result of this operation is also a keyword graph, which however, connects more than two keyword elements via a number of path entries. For each entry, we use the path index to retrieve all paths between its center and hop nodes and join them along the edges of the keyword graph to con-struct the final Steiner graphs.

Example 3. Fig. 3 shows an example for processing the 2-neighborhoods of nodes p 2 , p 4 . The keyword neighborhoods have three hop nodes in common: p 3 , p 4 , o 1 . The figure shows the keyword graph resulting from a join combination sulting keyword graph neighborhood. Two Steiner graphs are generated from this keyword graph. A query K can be processed according to a query plan . It is a sequence of join operators ( 1 1 NB , 1 2 G starting with the operator 1 NB joining two keyword neigh-borhoods (called base inputs), followed by 1 G operators that combine a base input with a graph. Since the base inputs represent elements retrieved for the keywords, a plan also represents a particular order of query keywords.

The principle difference to standard database join pro-cessing is that in keyword search, connections between two keywords are not known in advance. This is why typically, connections have to be explored at runtime by traversing edges of the data graphs that are loaded into memory. For this, efficient techniques for searching results in the data graph [9, 3], or pruning joined neighborhoods [11] have been proposed. For  X  X earching X  keyword search results via join processing, we observe there is no single optimal plan but different plans might produce different results, or even no results at all, as illustrated by Example 4.

Example 4. Consider query Q 3 from Fig. 1. For d = 2 , keywords steve and malta are connected and mary is con-nected to steve , but not to malta . Fig. 4 shows two query plans for query Q 3 . The first join on the keyword neighbor-hoods of mary and malta produces empty results as there are no paths of length d or less. However, results can be pro-duced when the keyword neighborhoods of steve and malta or steve and mary are joined first.

Now, we provide Theorem 3 to compute keyword search results for queries with more than 2 keywords, using an in-tegrated query plan :
Theorem 3. Let G be the data graph, K be an n -keyword query, and P ( K ) represents all permutations of keywords in the set K where each p  X  P ( K ) stands for a query plan, key-word graphs resulting from executing all p  X  P ( K ) capture all Steiner graphs for K that can be obtained from G .
Proof Sketch: Let N K = { N k 1 , . . . , N k i } be the set of keyword elements obtained for K . We must generate (1) all graphs from G , which (2) contain an element n k i  X  N K for every k i  X  K , (3) all these keyword elements are pairwise connected, and (4) direct paths between two keyword ele-ments are of length d or less. The integrated plan contains different orders of joins. Every join is either 1 NB or 1 both operations rest on the same concept of d-length 2-hop cover (i.e., join two path entries on a hop node), the result-ing paths are of length d or less (4). Every plan joins every keyword element with one another, ensuring they all are pairwise (directly or indirectly) connected (3). Also, since every plan contains all keywords, (2) is trivially satisfied. Taken (2+3+4) together, we can conclude that every plan (i.e., order of join) results in Steiner graphs of one particular structure. Since the integrated plan contains all possible or-ders, Steiner graphs of all possible structures that may exist in the data graph are taken into account (1).

As illustrated in Example 5, the integrated query plan consists of operators at levels l = 0 , . . . , | K | . Level 0 rep-resents data access operators, employing | K | base inputs. Level 1 to | K  X  1 | contain join operators. Only 1 NB oper-ators are needed at level 1. Subsequent levels consist exclu-sively of 1 G operators, which always combine a graph with a base input such that there is a correspondence between level and the number of inputs, i.e., every join operator at level l consumes exactly l + 1 base inputs. That is, operators at level | K  X  1 | have inputs for all query keywords in K . In fact, every operator at this level can be seen as represent-ing one particular plan (i.e., particular order of keywords). Level | K | has a single union operator that combines results from different plans.

The total number of join operators can be computed by taking all permutations of the set of base inputs K as the total number of join order plans and multiply this by | K | X  1, which is the number of join operators for every plan. More precisely, the permutations of the set K represent only the upper bound. The number of join operators at level 1 is in fact N (1) = C ( | K | , 2) = | K | ( | K | X  1) / 2, which denotes all 2-combinations of the set | K | . At this level, where only operators are applied, the order of the base inputs is not rel-evant (and thus it suffices to consider all 2-combinations in-stead of permutations). The order of the subsequent 1 G op-erations is however relevant and distinguishes one plan from another. At each subsequent level l &gt; 1, l -permutations of the set | K | have to be considered. These different orders however, share overlapping parts. To eliminate this redun-dancy and minimize the number of join operators, we em-ploy join operators whose outputs can be connected to more than one subsequent operators at the next level. Based on this, we construct a join operator at level l &gt; 1 by combin-ing inputs of a possibly  X  X hared X  operator of the previous level with a base input not processed yet. Since the num-ber of base inputs not processed by an operator at level l  X  1 is | K | X  l , the number of join operators at level l is N ( l ) = N ( l  X  1)( | K | X  l ) (indicating all combinations of in-puts from previous join operators and base inputs not pro-cessed yet). Thus, the total number of join operators is N ( K ) = N (1) + P | K | X  1 l =2 ( | K | X  l ) N ( l  X  1).
Complexity Given the number of join operators, the up-per bound on complexity of keyword query processing can be established as follows. In worst case, the result of a join is the cross product of its inputs. Here, input size is given by
Figure 5: Excerpt of an integrated query plan. the number of path entries in each keyword neighborhood, which can be calculated as | N k | X  b d for each k  X  K , where b is the average branching factor of the pruned d -neighborhood: b gives the number of paths in the neighborhood while | N k denotes the total number of neighborhoods that can be ob-tained for a keyword k . The upper bound can then be cal-culated as N ( K )  X  max {| N k | X  b d | k  X  K } 2 . Note that due to pruning at the level of paths, the branching factor of the neighborhoods is smaller than the one of the original graph.
Example 5. Fig. 5 shows an excerpt of the integrated query plan for a query of size 4. One subplan for the join order  X  steve 1 NB mary 1 G corp 1 G malta  X  is emphasized. The figure also shows that intermediate results such as the one produced by steve 1 NB mary is used for this as well as for one another plan.
Different top-k query processing techniques have been pro-posed to reach early termination after obtaining the top-k results [8]. Of particular interest in this context is the top-k join, which produces ranked join results without consuming all inputs [7].

A top-k join operator, also called rank join , takes two inputs that are sorted by the scores of their elements and iteratively processes them as data streams (i.e., no random access) to produce k top results. Crucial in top-k processing is the threshold T , an upper bound maintained for every join operator to capture the maximum score that can be achieved using as yet unprocessed input tuples. A join result can be reported if its score is higher than T . For a join between the inputs 1 and 2, T = max( Agg ( s max 1 , s 2 ) , Agg ( s where Agg (  X  ) returns the score for the joined result, s denotes the best score of all tuples of input i , and s i the score of the tuple lastly seen from i . The last seen score and threshold decrease as new tuples are processed (because they arrive at a decreasing order of scores).

This concept of rank join is utilized for top-k query pro-cessing in a pull-based architecture: each operator in the query plan has a next method that is called to produce the next result (i.e., a reportable result whose score exceeds T ); a tree-shaped query plan is employed where the root oper-ator calls the next method of lower level operators, which in turn call next on the base inputs. Processing inputs this way is guaranteed to preserve the top-k property, i.e., the resulting k results have best scores. This follows directly from the notion of threshold, as results reported by the next methods have scores greater than T , and thus are guaran-teed to have scores greater than all other result candidates that can be produced using remaining tuples.

Top-k Processing We adopt this top-k query processing to our setting, as early termination can avoid the processing of a possibly large amount of join operators in the integrated query plan. Further, implementing 1 G and 1 NB as rank join naturally enables us to treat data as input streams. In-stead of loading large graphs into memory [11] at once, we use rank join to incrementally access and process streams of path entries. We note that the only difference to the stan-dard query plan is that in the integrated query plan, the root is a union operator. We apply the notion of threshold to obtain a rank union operator in order to preserve the top-k property. The threshold of a union operator  X  is defined as T (  X  ) = max { T 1 | 1  X   X  1 } , where T 1 is the threshold of a rank join operator 1 , and  X  1 is the set of all rank join operators that feed into  X  . A result for  X  can be reported when its score is higher or equal to T (  X  ). Processing the integrated query plan with the rank version of our opera-tors (i.e., operators with thresholds for result output) using the pull-based procedure discussed above [8], yields top-k results. We provide the following Theorem to capture this:
Theorem 4. Given a query K , the data G and a pre-defined parameter k , an integrated query plan consisting of rank operators produces a sorted list of k Steiner graphs { G S 1 , . . . , G S k } for K s.t. there exist no other Steiner graph G i in G with Score ( G S i ) &gt; Score ( G S k ) .

The proof for this essentially exploits the top-k property of the underlying rank operators. The intuition is this: the integrated query plan can be decomposed into a number of subplans, each represents a different join order for the key-words in K . Because every such subplan corresponds exactly to the notion of query plan used previously [8], the estab-lished top-k property holds in this case, providing the guar-antee that every subplan yields the k best Steiner graphs (of one particular structure reflected by the join order). The rank union operator reports a top-k result only when its score equals or is higher than the maximum of the thresholds of the underlying rank join operators (subplans). This max-imum threshold can be seen as a global threshold applied to results of all subplans. The top-k property is preserved be-cause by definition, no subplan can produce further results with scores exceeding this global threshold.

Push-based Architecture The pull-based architecture [8] is  X  X riven X  by the final results. When called by the next method, each operator consumes its inputs until it is able to produce one result. Such an architecture is problematic in keyword query processing, as some operators of the inte-grated query plan may produce no results, which can only be determined after completely reading at least two base in-puts. Detecting and avoiding these  X  X roken operators X  (and the subplans resulting from them) can improve efficiency. To this end, we propose a push-based architecture, which is  X  X riven X  by the data access operators instead of the results. These operators push their outputs into the join operators they are connected to, from where keyword graphs propa-gate upwards through the query plans. In contrast to pull-based architectures, control and data flow are in the same rather than opposite direction.

Operator Ranking Using such a push-based architec-ture, we can apply operator ranking to accommodate knowl-edge related to which operators are broken, which ones pro-duce more results, or which ones produce results earlier than others. The ranking of operators determines its order dur-ing execution. That is, we propose not only to rank inputs and (partial) results, but also the join operators themselves. We associate rank join operators with a global score, which is estimated based on current results ( R ), and upper bound estimates for subsequent join operations to be performed on the remaining base inputs ( N B k ).
 Definition 13 (Operator Rank). Given the results R of a 1 Rank operator and the remaining set N B K of key-word neighborhoods not included in the inputs to 1 Rank , the global score of 1 Rank is defined as S = max { Score ( G K R } + P NB
Query Execution The global score is used to guide query execution. When a join operator has results that can be pushed to the next operator, it is associated with an (up-dated) global score and becomes active. If there are no active join operators, the lower level data access operators are ac-tivated in a round-robin fashion, each pushing a path entry into all rank 1 NB operators they are connected to. If there are active operators, the operator with the highest global score pushes its results to subsequent operators until its re-sult stack is empty, or another join operator has a better global score.

Detailed Algorithms Alg. 1 shows the query execution algorithm. First, the keyword elements N k for all k  X  K are retrieved and their keyword neighborhoods are created. Priority queue J keeps track of all currently active join op-erators, sorted by their global scores. While the number of results is lower than k , the algorithm gets the best result (via the topR method) from the topmost push rank join (PRJ) operator op in the queue J . By calling the push method of every connected operators (retrieved via nextLevelOps ), this result is propagated to the next level. Now we check if the second best result (the one not been pushed yet) passes the threshold to determine if op is still active, and add it back to the queue if this is the case. If no active operators are available, the base inputs are activated until at least one operator becomes active or all inputs are exhausted. Algorithm 1: EvaluateQuery( K, I P E , k ) Input : Keyword query K , k
Data : Queue J of active join operators, the integrated 1 Retrieve N k for all k  X  K 2 Retrieve N B k  X  N B K for each N k 3 while number of final results &lt; k do 4 if | J | &gt; 0 then 5 op = J .pop() 6 N Ops = IQP .nextLevelOps( op ) 7 foreach nop  X  N Ops do nop . push ( op .topR) 8 op .active =  X  op.secR ( Score ( op.secR ) &gt; op.T ) 9 if op .active then Add op to J 10 else 11 Select next input N B k 12 N Ops = IQP .nextLevelOps( N B k ) 13 foreach nop  X  N Ops do nop . push 14 Add all active operators to J
The employed PRJ shown in Alg. 2 is based on the hash rank join [7]: it also maintains a hash table for each input ( H 1 , H 2 ) for efficient lookup and computation of the join results J , and a threshold T (updated via the threshold method). Additionally, the PRJ operator also maintains the global score S . As previously defined, S is computed from the score of the currently best result and the maximum score of the combination of remaining base inputs that have not been joined during the process, i.e., are not part of the current results.
 Algorithm 2: PRJ: Push( r ) Input : Pushed input element r
Data : Queue R of sorted results, hash tables H 1 , H 2 // Use m,n to identify hash tables and inputs 1 if r is left input then m = 1, n = 2 2 else m = 2, n =1 3 Insert r into hash table H m 4 T = threshold() 5 J = probe H n for valid joined results with keys of r 6 forall the j  X  J do insert j into R 7 if | R | &gt; 0 then 8 if Score ( this .topRes ) &gt; T then 9 this .active = true 10 Update S
Example 6. Fig. 6 shows a PRJ operator at level l , join-ing neighborhoods of p 2 and p 4 . The inputs, i.e., path en-( ... ) from other operators, are pushed from operators at level l  X  1 . The two hash tables contain 4  X  2 elements already processed. One output with score 2 . 5 has been pushed to the next level. A candidate result with score 2.0 is in the queue. Because the last seen score is 0.5, T = 2 . 0 + 0 . 5 = 2 . 5 . As the result has a score less than threshold T , it cannot be pushed to subsequent operators yet (is thus inactive). This may change subsequently because T may decrease when lower input scores are processed. The query execution controls the push operation: an operator is only activated to push its results if its global score S is the highest among all active operators.

Note that the proposed modifications to the basic strat-egy, i.e., push-based execution in combination with operator-ranking, only aims at improving the efficiency. It has no effect on the top-k property. Essentially, because the same threshold checks are carried out, Theorem 4 must also hold in this case.
We discussed research work on neighborhood indexing [16, 1] and join processing [8, 7] that underlies our approach. We also provided an overview of existing keyword search solu-tions, which can be categorized into the categories of (1) schema-based ([5, 12, 13, 15, 14]) and (2) schema-agnostic ([9, 4, 11]) approaches. A similar distinction was made in a recent survey of keyword search in relational databases [18]. The capabilities of the underlying database engine can be fully utilized by the former approaches, while the applicabil-ity of the latter approaches is not limited to structured data that have well defined schemas. Our work follows the line of schema-agnostic approaches, aiming at providing keyword search support for different types of data, from structured to semi-structured up to unstructured data [11]. In particular, we compute Steiner graphs by operating directly on the data graph [11, 9, 17]. We make two kinds of contributions that are substantially different from previous work along this line: (1) previous strategies for searching Steiner graphs load the data graph into memory, and then find Steiner graphs by traversing graph edges [9], or pruning them [11]. Instead, we propose the use of join operations for this. (2) Further, for efficient data access, we propose an index that stores materialized paths in the neighborhoods of graph elements. While the concept of neighborhood is conceptually similar to the notion used by EASE [11], the data structure to im-plement it is more efficient because the employed strategies for neighborhood pruning is based on the more fine-grained notion of paths, instead of graphs. Further, this implementa-tion enables us to conceive a neighborhood as a set of paths, enabling the search for Steiner graphs to be conducted via path-based join operations.
Systems We compare our approach with an implementa-tion of EASE [11], a state-of-the-art native keyword search solution. We also implemented the bidirectional search al-gorithm (BDS) [9]. We used two variants of our approach: KJ features operator ranking and KJU does not.

For all approaches, a Lucene keyword index is used to re-trieve the top 300 matching keyword elements for each key-word of the query. The time needed for this is not counted as it is the same for all systems. For BDS, the exploration is performed directly on the data graph to obtain top-k Steiner graphs. For EASE, first all maximal 2-radius graphs, which contain at least one keyword element for every keyword, are identified. The union of these graphs computed for all query keywords is then loaded into memory, and pruned succes-sively to obtain Steiner graphs.

All systems were implemented in Java 1.6 on top of Or-acle Berkeley DB (Java Edition). Experiments were per-formed on a Linux system with two Intel Xeon 2.80GHz Dual-Core processors, a Samsung HE322HJ SATA 320GB disk and 8GB of main memory, 4GB of which were assigned to the Java VM. Operating system caches were cleared after each query run.
 Data and Queries We used a crawl of the Billion Triple Challenge 2009 2 (BTC) dataset, and 3 DBLP datasets from the SP2Bench benchmark 3 (DBLP) that vary in size, which we employ for the scalability experiment. Tab. 3 shows de-tailed statistics about the datasets and the size of the in-dexes for KJ (path entry index I P E and path index I P ) and EASE. For each dataset, we created nine keyword queries of length 2 ( Q 1  X  Q 3 ), 3 ( Q 4  X  Q 6 ) and 4 ( Q 7  X  Q 9 shows example queries for both datasets.
 Table 3: Dataset and index statistics (index sizes in MB).

DBLP BTC  X  X iller journal X   X  X vent movie X   X  X ress article 1988 X   X  X lbum queen magic X   X  X ournal medical article 1979 X  Table 4: Example queries for DBLP and BTC datasets.

Index Statistics For each dataset we created the path entry and path indexes with d = 2 for our approach and an index containing maximal 2-radius graphs for EASE. As discussed, the scale-free nature of Web data graphs makes values d &gt; 2 impractical because neighborhoods become as large as the entire data graph. Note that unlike EASE, this is not such a strong limitation because paths between 2 keyword elements can still have lengths up to | K  X  1 | X  d (instead of 2  X  d ).

Due to more fine-grained pruning, the KJ indexes were much smaller than the EASE indexes: for the DBLP datasets, the size of the KJ indexes was between 50% (DBLP1) and 21% (DBLP10) the size of the corresponding EASE indexes. For the BTC dataset, the difference was even larger: the KJ index was only 14% the size of the EASE index. http://vmlion25.deri.ie/ http://dbis.informatik.uni-freiburg.de/index.php? project=SP2B Figure 8: Evaluation results: load and process times for different keyword query lengths.

Overall Performance Fig. 7(a)+(b) show total query times for all queries on the DBLP10 and BTC datasets (missing values indicate timeouts). The performance of BDS was worse than all other systems on average (up to one or-der of magnitude for some queries). This suggests that the materialization strategies employed by other systems largely improve online performance. For both datasets, the perfor-mance of KJ was better than EASE for all queries. For DBLP10, and k = 10, the average total query times for KJ and EASE were 5.4s and 8.9s, respectively (BDS: 105.5s). On the BTC dataset with k = 10, the total times were 3.7s and 9.1s (BDS: 147.9s). This represents an improvement of 39% and 59%, respectively. However, the performance advantage of KJ compared to EASE decreases with longer queries. Whereas for | K | = 2 on DBLP10 with k = 10, EASE was on average 2.39 times slower than KJ, this factor decreased to 1.53 and 1.51 for | K | = 3 and | K | = 4, respec-tively. We will show later that the reason for that lies in the increased number of joins needed to process the search space resulting from the standard semantics of keyword search re-sults that is larger than the one of EASE.

Top-k Processing Fig 7(c) shows query performance for different values of k for the BTC dataset. Clearly, query times for all systems increase with higher values of k .
Operator Ranking We compare KJ and KJU to ex-amine the effect of operator ranking. Fig 7(c) shows that, on average, KJ with operator ranking was faster than KJU without operator ranking. We can see that the difference in query time between KJ and KJU decreases with larger values of k . For k = 1 KJU took 17% longer than KJ, whereas for k = 50 the difference was only 4%. This shows that operator ranking enables faster early result reporting, but also that the benefit decreases when more results are re-quested, which can be explained by the additional overhead introduced by operator ranking during query execution.
Scalability Fig. 7(d) shows overall query times for the three different DBLP datasets. The query time is split into the time for data access (load) and the time for join pro-cessing (process). We can see that data access makes up the main share of total time. For all datasets, KJ out-performs EASE. With respect to scalability, we found that the difference in performance between EASE and our sys-tem increases with larger datasets. On DBLP1, the smallest dataset, EASE was worse by a factor of 1.44. This increases with larger versions of the DBLP dataset, e.g. EASE was worse by a factor of 1.66 on DBLP10.

For an examination of the separate impact of data access (load) and data processing (process), Figure 8 illustrates a decomposition of query time.

Data Access Fig. 8 shows access times at different query lengths | K | for DBLP with k = 10. For all systems, access times increase with longer queries. Again, we can see the positive effect of operator ranking, as access times for KJ do not increase as sharply as for KJU. For | K | = 2, both KJ and KJU exhibit similar access times, whereas for | K | = 4 KJU is 1.6 times worse than KJ. Access times for EASE are worse than KJ for all query lengths. This confirms that the more fine-grained path-level pruning implemented by KJ helps to focus on smaller neighborhoods. Further, this efficiency gain can also be attributed to the fact that instead of loading all entire neighborhoods (i.e., all matching 2-radius graphs), KJ only loads path entries necessary to compute top-k results.
Data Processing Fig. 8(b) shows processing times at different values for | K | . Clearly, processing times increase for larger values of | K | . However, this effect is more pro-nounced for KJ and KJU than for EASE. From | K | = 2 to | K | = 4, KJ X  X  processing times increase by a factor 12.2 where EASE X  X imes increase only by 1.6. This result is not surprising, given our approach supports the standard seman-tics whereas EASE assumes a center node. With more com-plex queries, the search space for KJ becomes larger. This is reflected in a higher number of query plans and join oper-ators that have to be processed. Regardless of query length, EASE always operates on the union of the 2-radius graphs, and obtain results using the same pruning procedure.
Here, we can more clearly see the benefit of operator rank-ing: KJU X  X  processing times are higher than KJ X  X  times; this difference increases with | K | , indicating that operator rank-ing is beneficial when the queries are more complex.
We proposed a native keyword search database solution based on the basic operations data access and join. We in-troduced the d -length 2-hop index and a top-k procedure for implementing this database style processing. In experi-ments using large scale datasets, we showed that our solution consistently outperformed state-of-the-art solutions, i.e., it reduced storage requirement up to 86%, improved scalabil-ity by several factors, and improved performance by more than 50% on average. Further, our approach exhibits two main qualitative advantages. (1) While the exisiting solu-tion requires large graphs to be loaded entirely into memory at once, the database style processing we propose operates data streams that are loaded successively. (2) Also, the se-mantics of the results supported is more general, leading to more results formed by longer paths. Whereas this work focuses on the efficiency of top-k keyword search process-ing, the question of how differences in semantics and scoring functions affect the quality of results is considered as the main future work.
 Acknowledgements. The authors would like to thank J  X  org Diederich for providing the DBLP query logs. Research reported in this paper was supported by the German Federal Ministry of Education and Research (BMBF) in the iGreen project (grant 01IA08005K).
