 Traditional approaches to system management have been larg ely based on domain experts through a knowledge acquisition pro cess that translates domain knowledge into operating rules and p olicies. This has been well known and experienced as a cumbersome, la-bor intensive, and error prone process. In addition, this pr ocess is difficult to keep up with the rapidly changing environments. In this paper, we will describe our research efforts on establishin g an in-tegrated framework for mining system log files for automatic man-agement. In particular, we apply text mining techniques to c atego-rize messages in log files into common situations, improve ca tego-rization accuracy by considering the temporal characteris tics of log messages, develop temporal mining techniques to discover t he re-lationships between different events, and utilize visuali zation tools to evaluate and validate the interesting temporal patterns for system management.
 I.2 [ Artificial Intelligence ]: Learning; I.5.4 [ Pattern Recogni-tion ]: Applications Algorithms, Experimentation, Theory System Management, Log Categorization, Event Relationshi p, Temporal Pattern Copyright 2005 ACM 1-59593-135-X/05/0008 ... $ 5.00.
When problems occur, traditional approaches for trouble sh oot-ing rely on the knowledge and experience of domain experts to fig-ure out ways to discover the rules or look for the problem solu tions laboriously. It has been estimated that, in medium and large compa-nies, anywhere from 30% to 70% of their information technolo gy resources are used in dealing with problems [19]. It is unrea listic and inefficient to depend on domain experts to manually deal w ith complex problems in ever-changing computing systems.

Modern computing systems are instrumented to generate huge amounts of system log data. The data in the log files describe t he status of each component and record system operational chan ges, such as the starting and stopping of services, detection of n etwork applications, software configuration modifications, and so ftware execution errors. Analyzing log files, as an attractive appr oach for automatic system management and monitoring, has been enjoy ing a growing amount of attention. However, several new aspects of the system log data have been less emphasized in existing ana lysis methods from data mining and machine learning community and pose several challenges calling for more research. The aspe cts in-clude disparate formats and relatively short text messages in data reporting, asynchronism in data collection, and temporal c haracter-istics in data representation.

First, the heterogeneous nature of the system makes the data more complex and complicated [6]. As we know, a typical compu t-ing system contains different devices (e.g., routers, proc essors, and adapters) with different software components (e.g., opera ting sys-tems, middleware, and user applications), possibly from di fferent providers (e.g., Cisco, IBM, and Microsoft). These various com-ponents have multiple ways to report events, conditions, er rors and alerts. The heterogeneity and inconsistency of log formats make it difficult to automate problem determination. For example , there are many different ways for the components to report the star t up process. Some might log  X  X he component has started X , while o thers might say that  X  X he component has changed the state from star ting to running X . This makes it difficult to perform automated ana lysis of the historical event data across multiple components whe n prob-lems occur as one need to know all the messages that reflect the same status, for all the components involved in the solution [24]. To enable automated analysis of the historical event data ac ross multiple components, we need to categorize the text message s with disparate formats into common situations. Second, text mes sages in the log files are relatively short with a large vocabulary s ize [22]. Hence, care must be taken when applying traditional documen t pro-cessing techniques. Third, each text message usually conta ins a timestamp. The temporal characteristics provide addition al context information of the messages and can be used to facilitate dat a anal-ysis.

In this paper, we will describe our research efforts to addre ss the above challenges in mining system logs. In particular, we pr opose to establish an integrated framework for computing system m an-agement by acquiring the needed knowledge automatically fr om a large amount of historical log data, possibly from differen t types of information sources such as system errors, resource perfor mance metrics, and trouble ticket text records. Specifically, we w ill apply text mining techniques to automatically categorize the tex t mes-sages into a set of common categories, incorporate temporal infor-mation to improve categorization performance, develop tem poral mining techniques to discover the relationships between di fferent events, and utilize visualization tools to evaluate and val idate the interesting temporal patterns for system management.
 The architecture of the proposed framework is summarized in Figure 1. The rest of the paper is organized as follows: Secti on 2 applies text mining techniques to categorize text messages into a set of common categories, Section 3 proposes two approaches of incorporating temporal information to improve the categor ization performance, Section 4 describes our techniques for discov ering the relationships between different events, Section 5 pres ents our experimental results, and finally Section 6 provides conclu sions and discussions.

The disparate logging mechanisms impede problem investiga -tion because of no standard approach for classifying them [2 4]. In order to create consistency across similar fields and improv es the ability to correlate across multiple logs, it is necessary t o transform the messages in the log files into a common base [2] (i.e., a set of common categories). In this paper, we first manually determi ne a set of categories as the basis for transformation. The set of cat-egories is based on the CBE (Common Base Event) format estab-lished by IBM initiative [24]. CBE provides a finite set of can onical situations after analyzing thousands of log entries across multiple IBM and non-IBM products. Although we use CBE as an inter-nal presentation here, the problem and the proposed approac h are generic and can be extended for other data formats as well. Sp ecif-ically, the set of categories includes start , stop , dependency , create , connection , report , request , configuration , and other .
Given the set of common categories, we can then categorize th e messages reported by different components into the prescri bed cat-egories. This can be viewed as a text categorization problem where the goal is to assign predefined category labels to unlabeled doc-uments based on the likelihood inferred from the training se t of labeled documents [21].

In our work, we use naive Bayes as our classification approach as it is among the most successful known algorithms for learn ing in text categorization [20]. System log files usually have a lar ge size of vocabulary and most of the system log messages contain a fr ee-format 1024-byte ASCII description of the event [22]. Basic ally Naive Bayes assumes that the text data is generated by a param etric model, and uses training data to calculate Bayes-optimal es timates of the model parameters [15]. Then, it classifies test data us ing the generative model by calculating the posterior probabil ity that a class would have generated the test data in question. The mo st probable class is then assigned to the test data [16].
The classification performance achieved by the Naive Bayes classifier in our practice is not satisfactory as will shown i n Sec-tion 5. In many scenarios, text messages generated in the log files usually contain timestamps. The temporal characteristics provide additional context information of the messages and can be us ed to facilitate data analysis. As an example, knowing the curr ent status of a network component would help forecast the possib le types of messages it will generate. These structural constr aints can be naturally represented using naive Bayes algorithm and hi dden Markov models [7]. In this section, we describe two approach es of utilizing the temporal information to improve classificati on perfor-mance. Our experiments in Section 5 show that both approache s greatly improve the categorization accuracy. A preliminar y report of the work has been presented in [17].
To facilitate the discussion, Table 1 gives an example of the sys-ample, four columns, i.e., Time , EventSource , Msg and State are listed. Time represents the generated timestamp of log messages, EventSource identifies the component which reports the message, Msg lists the content of text message, and State labels the current category.

If a sequence of log messages are considered, the accuracy of categorization for each message can be improved as the struc ture relationships among the messages can be used for analysis. F or ex-ample, the components usually first start up a process, then s top the process at a later time. That is, the stop message usually occurs af-ter the start message. For another example, as shown in Table 1, the component Inventory Scanner will report dependency messages if it cannot find some features or components. This happens especi ally just right after it generates the create message X   X  X annot create *  X  (* is the abbreviation of arbitrary words. It represents cer tain com-ponents or features here). Thus it is beneficial to take the te mporal information into consideration.

In order to take the relationships between adjacent message s into account, we make some modifications to the naive Bayes al -gorithm. Suppose we are given a sequence of adjacent message s D = ( d 1 , d 2 ,  X  X  X  , d T ) . let Q i be the category labels for message \ invdelta.tmp. dependency \ invdelta.tmp. dependency d (i.e., Q i is one of C 1 , C 2 ,  X  X  X  C L ). Now we want to classify Note that Assuming that d i +1 is conditionally independent of Q Q once Q i is determined, P ( d i +1 | Q i ) is then fixed. Hence maximizing P ( Q i +1 | d i +1 , Q i ) is equivalent to maximizing P ( Q i +1 | Q i ) P ( d i +1 | Q i +1 ) . Observe that P ( d in order to maximize P ( d i +1 | Q i +1 ) , it is enough to maximize P ( Q i +1 | d i +1 ) . Therefore, in summary, we assign category Q i +1 which is argmax j ( P ( C j | d i +1 )  X  P ( C other words, we aim at maximizing the multiplication of text clas-sification probability P ( C j | d i +1 ) and state transition probability P ( C j | Q i ) . The transition probability P ( C j | Q from training log files.
Hidden Markov Model(HMM) is another approach to incorpo-rate the temporal information for message categorization. The tem-poral relations among messages are naturally captured in HM M [9]. The Hidden Markov Model is a finite set of states, each of which is associated with a (generally multidimensional) probabi lity distri-bution. Transitions among the states are governed by a set of prob-abilities called transition probabilities [18]. In a parti cular state an outcome or observation can be generated, according to the as so-ciated probability distribution. The model describes a pro babilis-tic generative process whereby a sequence of symbols is prod uced by starting in some state, transitioning to a new state, emit ting a symbol selected by that state, transitioning again, emitti ng another symbol and so on until a designated final state is reached.
In our experiment, the category labels we specified in Sectio n 2.1 are regarded as states. The emitting observation symbols ar e the log messages corresponding to their state labels. HMM expli citly considers the state transition sequence. It can also be used to com-pare all state paths from the start state to the destination s tate, and then choose the best state sequence that emits a certain obse rvation sequence. So it works well in labeling continuous log messag es. When one log message has been assigned several competitive s tate labels by text classification, HMM selects a certain label by travers-ing the best state sequence. The state transition probabili ty is calcu-lated from the training log data sets, for example, in our exp eriment the probability of state create to state dependency is 0.4470. The probability of emitting messages can be estimated as the rat io of the occurrence probabilities of log messages to the occurre nce of their states in the training data. Viterbi algorithm is used to find the most possible state sequence that emits the given log mes sages, that is, finding the most possible state labels to assign to th ese log messages [8].
Once the messages in the log file are transformed into common categories, it is then possible to analyze the historical ev ent data across multiple components to discover interesting patter ns embed-ded in the data. Each message in log files usually has a timesta mp and we will try to find the mining temporal patterns. Temporal patterns of interest appear naturally in the system managem ent ap-plications [4]. Specifically, a computer system problem may trigger a series of symptom events/activities. Such a sequence of sy mptom events provides a natural signature for identifying the roo t cause. As summarized in [4, 5]: a problem manifests itself as a seque nce of events propagating from origin and low layer to high softw are layer through the dependency tree. Thus knowing the tempora l pat-terns can help us pinpoint the root cause and take proper acti ons.
Once we categorize each message into a set of categories (or event types), each message can then be represented as a three -tuple &lt; s, c, t &gt; where s is the category label or event type, source component of this log message, and t is the timestamp. In order to discover event relationships among components, we map each distinct pair of &lt; s, c &gt; into a unique event type event sequence we obtained is then is then a collection of all events occurred within some time range (say, between 0 and { &lt; e 1 , t 1 &gt;, &lt; e 2 , t 2 &gt;, . . . , &lt; e [0 , T ] , and t i  X  t i +1 .

As a first attempt, we focus on pairwise temporal patterns. Pa ir-wise patterns can be easily interpreted by domain experts, a nd be easily visually presented. The problem of mining event rela tion-ships can be stated as
P ROBL E M 1. Given event sequence D , find all pairwise statis-tically dependent patterns that can be characterized as fol lowing We refer this type of patterns as t-patterns .
Previous work of temporal mining focuses on frequent item-sets with a predefined time window [14]. It fails to address tw o important aspects often required by applications. First, t he fixed time-window scheme can not explore precise temporal inform a-tion within a window, and misses the opportunity to mine temp o-ral relationship longer than the window size. In our practic e on system management applications, the temporal relationshi ps dis-covered have time distances ranging from one second to one da y. Second, as well-known for transaction data, frequent patte rn frame-work misses significant, but infrequent patterns. In most sy stem management applications, frequent patterns are normal ope rations and service disruptions are usually infrequent but signific ant pat-terns.

To address the above two problems, we develop algorithms for discovering temporal patterns without predefined time wind ows. The problem of discovering temporal patterns is divided int o two sub-tasks: (1) using  X  X heap statistics X  for dependence tes ting and candidates removal (2) identifying the temporal relations hips be-tween dependent event types. The dependence problem is form u-lated as the problem of comparing two probability distribut ions and is solved using a technique reminiscent of the distance meth ods used in the spatial point process, while the latter problem i s solved using an approach based on Chi-Squared tests. The statistic al prop-erties provide meaningful characterizations for the patte rns and are usually robust against noise. A preliminary report of the wo rk has been presented in [10].
Let T a and T b be two point processes for event a and b respec-tively. We would like to check whether T b is dependent on In this paper, we use distance methods instead of quadrat met h-ods(e.g., time window segment). Our approach is motivated b y the methods of finding spatial associations between spatial point processes developed in statistical community [1, 3]. The id ea is simple: if the occurrence of b is predictable by the occurrence of then the conditional distribution which models the waiting time of event type b given event type a  X  X  presence would be different from the unconditional one.

Define the distance from a point z to the point process T to its nearest neighbor in T a which occurs after z .

D E FINIT ION 1. The unconditional distribution of the waiting time of event b is defined as F b ( r ) = P{ d ( x, T b )  X  r } . The distribution can be interpreted as the probability of ha ving event type b within time r .

D E FINIT ION 2. The conditional distribution for the waiting time of event b with respect to event a is defined as: with respect to the process a .
 The conditional distribution can be regarded as the conditi onal probability distribution given there is an event of Next we define the dependent relationship between two event t ypes based on the two distributions.

D E FINIT ION 3. Given two event types a and b , Denote their arrival times as T a and T b . We say that b is directly dependent on a , denoted by a  X  b , if the two distributions F b ( r ) and different.
Estimating the Two Distributions. Since the true distributions are not available, we have to first estimate them. Under the st a-tionarity assumption, the distribution of d ( x, T b ) does not depend on the location of x . The assumption of stationary is a pragmatic simplification which justifies the use of relatively simple e stimation methods [3]. The two distributions can be estimated as follo ws: let T the information in [0 , T  X  ] for estimation. F b ( r ) can be estimated by measuring the distance d ( x, b ) from each of several arbitrary test points to the nearest event occurrence in b and forming an empir-ical distribution. Similarly, F a the distance from each occurrence a i of the point process nearest arrival of event type b , and forming an empirical distribu-tion function.
 Let t i = b i  X  b i  X  1 denote the inter-arrival time for 1 , ..., n b and b 0 = 0 . Denote ( t (1) , t (2) , . . . , t permutation of t i  X  X  where t ( i )  X  t ( i +1) . The estimate of the observed proportion of distance values satisfying d ( x, T  X 
F ( r ) = Let n  X  the time range [0 , T  X  ] . For each point a i , 1  X  i  X  n d denote the distance from a i to the nearest b occurring after b , i.e., d i = d ( a i , T b ) . Let D ab = ( d 1 ,  X  X  X  , d ( d (1) , d (2) , . . . , d ( n  X  a ) ) as the ordered permutation of d values satisfying d ( a i , T b )  X  r : Note that the estimate of the unconditional distribution is a piece-wise linear function of r , while the estimate of the conditional one is a step function of r .

Dependency Tests On First Moments. Under the null hypothe-sis that there is no dependency between event type a and b pected that F b = F a scale data, we propose to test their difference in first momen t. It is easy to see that if F b = F a terexamples can be constructed in which a and b are dependent but the first moments of two distribution functions are the same. So the testing based on first moment difference is conservative and will generate false positives. However the conservative proper ty does not affect our two-stage approach since stage one only serve s as a preprocessing step to reduce the candidate space.

It is easy to check that the first moments for the two distri-butions  X  F b ( r ) and  X  F a mean of d i  X  X . Under the hypothesis that a and b are independent, all the d i  X  X  are independently distributed as  X  F b ( r ) Limiting Theorem, where var (  X  F b ) denotes the variance of distribution  X  equal to
Given a confidence level  X  , we could compute the correspond-ing confidence interval [  X  Z  X  /  X  n, Z  X  /  X  n ] where z sponding 1  X   X  quantile and n is the sample size. If the value of  X  two distributions are different and hence b is dependent on
After preprocessing via  X  X heap statistics X  in stage one, th e next task is identifying the dependence between the candidate pa irs and finding the waiting period (or lag) between two dependent eve nts. Let  X  be the time tolerance accounting for factors such as phase shifts and lack of clock synchronization.

D E FINIT ION 4. Given b is dependent on a , we say that the waiting period of b after a is p if the distance sequence ( d 1 ,  X  X  X  , d n  X  a ) , has a period p with time tolerance  X  The discovery of the waiting periods is carried out using the Chi-Squared test based approach first introduced in [13]. Consid er an arbitrary element  X  in D ab and a fixed  X  . Let C  X  be the total num-ber of elements of D ab that fall into the interval [  X   X   X ,  X  +  X  ] Intuitively, if  X  is not a period p , C  X  should be small; otherwise it should be large. The idea here is to compare C  X  with the number of elements in [  X   X   X ,  X  +  X  ] that would expected from a random sequence. The procedure for identifying the relationship i s essen-tially a one dimensional clustering process.
The log files used in our experiments are collected from sever al different machines with different operating systems in the School of Computer Science at Florida International University. W e use logdump2td (NT data collection tool) developed by Event Min ing Team at IBM T.J. Watson research center. The raw log files has 1 4 columns, which are sequence, time, EventType, ReportTime, Log-TypeName, LogTypeID, EventSourceName, EventSource, Host -Name, HostID, severity, Category, and Msg. To preprocess te xt messages, we remove stop words and skip html labels. We use ac -curacy, defined as the the proportion of messages that are cor rectly assigned to a category, to evaluate the classification perfo rmance and use the random 30-70 data split for training and testing. The Naive Bayes classifier is built on the Bow (A Toolkit for Statistical Language Modeling, Text Retrieval, Classifica tion and plementation of HMM tool is based on the package UMDHMM
The experimental results on message categorization are sum ma-rized in Table 2. In summary, our experiments show that both t he modified Bayes algorithm and HMM enhance the classification a c-curacy and HMM achieves the best performance in our practice . However, constructing HMM requires more computational cos ts. Intuitively, the modified Bayes algorithm considers only ad jacent state transitions while HMM considers all possible state pa ths. The choice between the two approaches for incorporating tempor al in-formation is problem-dependent in practice.
 Table 2: Performance Comparisons on Message Categorizatio n
We perform simulation studies to evaluate the effectivenes s of our approach for estimating two interarrival distribution s. Three sets of experiments are conducted. These simulation experi ments illustrate the relationships between temporal dependence and com-parisons of two distributions. (a) Two Independent
Poisson Process
Independent Poisson Process : Two independent Poisson pro-cesses are generated. Figure 2(a) plots the arrivals of two g ener-ated processes and Figure 2(b) plots the two cumulative dist ribu-tion functions(CDFs). As we would expect, they are very clos e to each other.

Branching Poisson Process : Branching Poisson process is a pro-cess model for generating dependent process types. The even t se-quence T a , which is simulated by a Poisson process with arrival rate 3 , is the main process. The event sequence T b is generated as the subsidiary process with the following parameters: the n umber 2 The tool can be downloaded at http://www-2.cs.cmu.edu/  X  mccallum/bow/ . 3 The package can be downloaded at http://www.cfar.umd.edu/  X  kanungo/software/software.html . (a) Branching Poisson
Process (a) Neyman-Scott Pro-cess of subsidiary events K  X  P oisson (1) and the inter-arrival time between each subsidiary event T  X  exp(0 . 8) , where exp stands for exponential distribution. Figure 3(a) plots the arriva ls of the two processes and Figure 3(b) plots the two CDF X  X . The differ ence between the two CDF X  X  clearly indicates the dependence betw een the two processes.

Neyman-Scott Process : Neyman-scott process is a typical clus-ter process in which the daughter process is generated aroun d the parent process. Like the example in branching Poisson proce ss, we first generate the parent process T a as a Poisson Process with ar-rival rate 3 and the daughter process T b as follows: the number of daughters  X  P oisson (1) and the distance between the daughter event and parent  X  exp(0 . 7) . Figure 4(a) plots the arrivals of the two processes and Figure 4(b) plots the two CDF X  X . The differ ence between the two CDF X  X  clearly indicates the dependence betw een the two dependent processes.
We apply the techniques for discovering t-patterns on our lo g files. By reviewing the discovered patterns with operation s taff, we find that many of the t-patterns we discovered are closely rel ated to underlying problems. We discover several patterns of int erest. For example, the  X  X ort up X  and  X  X ort down X  are temporal patte rns that signify a mobile user X  X  login and logout;  X  X outer link d own X  followed by  X  X outer down X  in 10 seconds signifies a typical ro uter down sequence: a router down results in its link down. Furthe r, a sequence of four SNMP requests (5, 10, 15, seconds apart in se-quences) raises up a security concern of port scans. Another pat-tern discovered about a performance measurement crossing c ritical thresholds and resetting leads to the resolution of a wrong t hreshold setting problem.

Visualization can help the users understand and interpret p atterns in the data. For example, a scatter plot can help network oper ators to identify patterns of significance from a large amount of mo nitor-ing data. We also use visualization tools such as EventBrows er [12] to interpret and validate interesting patterns.
In this paper, we present our research efforts on establishi ng an integrated framework on mining log files for computing syste m management by exploring the synergy of text mining, tempora l data mining and visualization. There are several natural av enues for future research. First, instead of manually determinin g the set of common categories, we could develop techniques to automa ti-cally infer them from historical data. Second, in practice, the num-ber of common categories for can be significantly large. To re solve this issue, it is useful to utilize the dependence relations hips among different categories. Third, another natural direction is to use level-wise approach to extend the pairwise patterns to the pattern s of length greater than two.
