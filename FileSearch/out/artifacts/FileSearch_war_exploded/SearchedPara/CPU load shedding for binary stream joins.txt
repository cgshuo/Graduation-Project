 REGULAR PAPER Bugra Gedik  X  Kun-Lung Wu  X  Philip S. Yu  X  Ling Liu Abstract We present an adaptive load shedding approach for windowed stream joins. In contrast to the conventional approach of dropping tuples from the input streams, we explore the concept of selective processing for load shedding. We allow stream tuples to be stored in the windows and shed excessive CPU load by performing the join operations, not on the entire set of tuples within the win-dows, but on a dynamically changing subset of tuples that are learned to be highly beneficial. We support such dynamic selective processing through three forms of runtime adaptations : adaptation to input stream rates, adaptation to time corre-lation between the streams and adaptation to join directions. Our load shedding approach enables us to integrate utility-based load shedding with time correlation-based load shedding. Indexes are used to further speed up the execution of stream joins. Experiments are conducted to evaluate our adaptive load shedding in terms of output rate and utility. The results show that our selective processing approach to load shedding is very effective and significantly outperforms the approach that drops tuples from the input streams.
 Keywords Data streams  X  Stream joins  X  Load shedding 1 Introduction With the ever increasing rate of digital information available from online sources and networked sensing devices [ 20 ], the management of bursty and unpredictable data streams has become a challenging problem. It requires solutions that will enable applications to effectively access and extract information from such data streams. A promising solution for this problem is to use declarative query pro-cessing engines specialized for handling data streams, such as data stream man-agement systems (DSMS), exemplified by Aurora [ 5], STREAM [ 1], and Tele-graphCQ [ 8].
 ing more important with the increasing need for fusing data from various types of sensors available, such as environmental, traffic, and network sensors. Here, we list some real-life applications of stream joins. We will return to these examples when we discuss assumptions about the characteristics of the joined streams.  X  Finding similar news items from two different sources : Assuming that news  X  Finding correlation between phone calls and stock trading : Assuming that  X  Finding correlated attacks from two different streams : Assuming that alerts ied [ 13, 14, 18]. This is mainly due to the fact that traditional join algorithms need to perform a scan on one of the inputs to produce all the result tuples that match with a given tuple from the other input. However, data streams are unbounded. Producing a complete answer for a stream join requires unbounded memory and processing resources. To address this problem, several approaches have been pro-posed.
 dows. In a windowed stream join, a tuple from one stream is joined with only the tuples currently available in the window of another stream. A sliding window can be defined as a time-based or count-based (tuple-based) window. An example of a time-based window is  X  X ast 10 s X  tuples X  and an example of a count-based window is  X  X ast 100 tuples. X  Windows can be either user defined, in which case we have fixed windows, or system-defined and thus flexible , in which case the system uses the available memory to maximize the output size of the join. Another way of han-dling the problem of blocking joins is to use punctuated streams [24], in which punctuations that give hints about the rest of the stream are used to prevent block-ing. The two-way stream joins with user defined time-based windows constitute one of the most common join types in the data stream management research to date [ 2, 13, 18].
 is usually needed in stream processing systems. Several factors may contribute to the demand for CPU load shedding, including (a) bursty and unpredictable rates of the incoming streams; (b) large window sizes; and (c) costly join conditions. Data streams can be unpredictable in nature [ 19] and incoming stream rates tend to soar during peak times. A high stream rate requires more resources for performing a windowed join, due to both increased number of tuples received per unit time and the increased number of tuples within a fixed-sized time window. Similarly, large window sizes imply that more tuples are needed for processing a windowed join. Costly join conditions typically require more CPU time.
 dowed stream joins, aiming at maximizing both the output rate and the output utility of stream joins. The proposed approach is applicable to all kinds of join conditions, ranging from simple conditions such as equi-joins defined over single-valued attributes (e.g., the phone calls and stock trading scenario) to complex con-ditions such as those defined over set-valued attributes (e.g., the correlated attacks scenario) or weighted set-valued attributes (e.g., the similar news items scenario). 1.1 Summary of contributions Our adaptive load shedding approach has several unique characteristics. existing approaches, our adaptive load shedding framework follows a selective processing methodology by keeping tuples within the windows, but processing them against a subset of the tuples in the opposite window.
 join operations to three dynamic stream properties: (i) incoming stream rates, (ii) time correlation between streams and (iii) join directions. The amount of selective processing is adjusted according to the incoming stream rates. Prioritized seg-ments of the windows are used to adapt join operations to the time-based correla-tion between the input streams. Partial symmetric joins are dynamically employed to take advantage of the most beneficial join direction learned from the streams. integration of the three adaptations with the utility-based load shedding. Maximiz-ing the utility of the output tuples produced is especially important when certain tuples are more valuable than others.
 were conducted to evaluate the effectiveness of our adaptive load shedding ap-proach. Our experimental results show that the three adaptations can effectively shed the load in the presence of any of the following; bursty and unpredictable rates of the incoming streams, large window sizes, or costly join conditions. 2 Related work Based on the metric being optimized, related work on load shedding in windowed stream joins can be divided into two categories.
 duced. Different tuples may have different importance values based on the appli-cation. For instance, in the news join example, certain type of news, e.g., security news, may be of higher value, and similarly in the stock trading example, phone calls from insiders may be of higher interest when compared to calls from regulars. In this case, an output from the join operator that contains highly valued tuples is more preferable to a higher rate output generated from lesser valued tuples. The drop tuples from the input streams with low utility values. We refer to this type of load shedding as utility-based load shedding ,alsoreferredtoas semantic load shedding in the literature.
 tuples produced [ 10, 18, 22]. This can be achieved through rate reduction on the source streams, i.e., dropping tuples from the input streams, as suggested in Car-ney et al. [ 7] and Kang et al. [ 18 ]. The work presented in Kang et al. [ 18]inves-tigates algorithms for evaluating moving window joins over pairs of unbounded streams. Although the main focus of Kang et al. [ 18] is not on load shedding, sce-narios where system resources are insufficient to keep up with the input streams are also considered.
 eral, including memory allocation among query operators [ 3] or inter-operator queues [ 17], load shedding for aggregation queries [ 4], and overload-sensitive management of archived streams [ 9].
 ple dropping for CPU-limited scenarios and memory allocation among windows for memory-limited scenarios. However, dropping tuples from the input streams without paying attention to the selectivity of such tuples may result in a suboptimal solution. Based on this observation, heuristics that take into account selectivity of the tuples are proposed in [ 10].
 in Srivastava and Widom [ 22 ] for performing memory-limited stream joins. This work is based on the observation that there exists a time-based correlation be-tween the streams. Concretely, the probability of having a match between a tuple just received from one stream and a tuple residing in the window of the opposite stream, may change based on the difference between the timestamps of the tuples (assuming timestamps are assigned based on the arrival times of the tuples at the query engine). Under this observation, memory is conserved by keeping a tuple in the window since its reception until the average rate of output tuples generated using this tuple reaches its maximum value. For instance, in Fig. 1 case I, the tuples can be kept in the window until they reach the vertical line marked. This effectively cuts down the memory needed to store the tuples within the window and yet produces an output close to the actual output without window reduction. the beginning of the window, the age-based window reduction can be effective for shedding memory load. A natural question to ask is:  X  X an the age-based window reduction approach of Srivastava and Widom [ 22]beusedtoshedCPUload? X  This is a valid question, because reducing the window size also decreases the number of comparisons that have to be made in order to evaluate the join. How-ever, as illustrated in Fig. 1 case II, this technique cannot directly extend to the CPU-limited case where the memory is not the constraint. When the distribu-tion does not have its peak close to the beginning of the window, the window reduction approach has to keep tuples until they are close to the end of the win-dow. As a result, tuples that are close to the beginning of the window and thus are not contributing much to the output will be processed until the peak is reached close to the end of the window. This observation points out two important facts. First, time-based correlation between the windowed streams can play an important role in load shedding. Second, the window reduction technique that is effective for utilizing time-based correlation to shed memory load is not suitable for CPU load shedding, especially when the distribution of the streams is unknown or unpre-dictable.
 proach that is capable of performing selective processing of tuples in the stream windows by dynamic adaptation to input stream rates, time-based correlations be-tween the streams, and profitability of different join directions. To the best of our knowledge, our load shedding approach is the first one that can handle arbitrary time correlations and at the same time support maximization of output utility. tics follow the input-triggered approach, in which the output of the join is a time ordered series of tuples, where an output tuple is generated whenever an incoming tuple matches with an existing tuple in the join windows. The same approach is used in most of the related work described in this section [ 10, 13, 14, 18, 22]. A different semantics for the windowed stream joins, one based on the negative tuples approach, is used in the Nile stream engine [ 11]. In Nile, at any time the answer of the join is taken as the answer of the snapshot join whose inputs are the tuples in the current window for each input stream. This is more similar to the problem of incremental materialized view maintenance. The extension of our se-lective processing approach to negative tuples is an interesting research direction. However, it is beyond the scope of this paper. 3Overview Unlike the conventional load shedding approach of dropping tuples from the input streams, our adaptive load shedding encourages stream tuples to be kept in the windows. It sheds the CPU load by performing the stream joins on a dynamically changing subset of tuples that are learned to be highly beneficial, instead of on the entire set of tuples stored within the windows. This allows us to exploit the char-acteristics of stream applications that exhibit time-based correlation between the streams. Concretely, we assume that there exists a non-flat distribution of proba-bility of match between a newly received tuple and the other tuples in the opposite window, depending on the difference between the timestamps of the tuples. exist between the streams as a result of differences between the communication overhead of receiving tuples from different sources [ 21]. Second and more im-portantly, there may exist variable delays between related events from different sources. For instance, in the news join example, different news agencies are ex-pected to have different reaction times due to differences in their news collection and publishing processes. In the stock trading example, there will be a time delay between the phone call containing the hint and the action of buying the hinted stock. In the correlated attacks example, different parts of the network may have been attacked at different times. Note that, the effects of time correlation on the data stream joins are to some extent analogous to the effects of the time of data creation in data warehouses, which are exploited by join algorithms such as Diag-Join [ 15]. 1 ity match distributions corresponding to the time correlations. Instead, they are learned by our join algorithms. However, we do assume that these distributions are usually not uniform and thus can be exploited to maximize the output rate of the join.
 source is sufficient, we want to point out two important observations. First, with increasing input stream rates and larger stream window sizes, it is quite com-mon that CPU becomes limited before memory does. Second, even under limited memory, our adaptive load shedding approach can be used to effectively shed the excessive CPU load after window reduction is performed for handling the memory constraints. 3.1 Technical highlights Our load shedding approach is best understood through its two core mechanisms, each answering a fundamental question on adaptive load shedding without tuple dropping.
 we can process  X  given a window of stream tuples. The factors to be considered in answering this question include the performance of the stream join operation under current system load and the current incoming stream rates. In particular, par-tial processing dynamically adjusts the amount of load shedding to be performed through rate adaptation.
 should we process  X  given the constraint on the amount of processing, defined at the partial processing phase. The factors that influence the answer to this question include the characteristics of stream window segments, the profitability of join directions, and the utility of different stream tuples. Selective processing extends partial processing to intelligently select the tuples to be used during join process-ing under heavy system load, with the goal of maximizing the output rate or the output utility of the stream join.
 we first briefly review the basic concepts involved in processing windowed stream joins, and establish the notations that will be used throughout the paper. 3.2 Basic concepts and notations A two-way windowed stream join operation takes two input streams denoted as S 1 and S 2 , performs the stream join and generates the output. For notational con-venience, we denote the opposite stream of stream i ( i = 1 , 2) as stream i .The sliding window defined over stream S i is denoted as W i , and has size w i in terms of seconds. We denote a tuple as t and its arrival timestamp as T ( t ) . Other nota-tions will be introduced in the rest of the paper as needed. Table 1 summarizes the notations used throughout the paper.
 streams and processing them against tuples in the opposite window. Figure 2 il-lustrates the process of windowed stream joins. For a newly fetched tuple t from stream S i , the join is performed in the following steps.
 the end of window W i are checked in order and removed if they have expired. A tuple t o expires from window W i iff T  X  T ( t o )&gt;w i ,where T represents the current time. The expiration check stops when an unexpired tuple is encountered. The tuples in window W i are sorted in the order of their arrival timestamps by de-fault and the window is managed as a doubly linked list for efficiently performing insertion and expiration operations. In the third and last step, tuple t is processed against tuples in the window W i , and matching tuples are generated as output. pseudo-code, in practice buffers can be placed in the inputs of the join operator, which is common practice in DSMS query networks and also useful for mask-or weighted set-valued attributes, the following additional details are attached to the processing steps, assuming a tuple is a set of items (possibly with assigned weights). First, the items in tuple t are sorted as it is fetched from S i . The tuples in W i are expected to be sorted, since they have gone through the same step when they were fetched from S i . Then, for each tuple t a in W i , t and t a are compared by performing a simple merge of their sorted items. Equality, subset, superset, overlap and inner product joins all can be processed in a similar manner. For in-dexed joins, an inverted index is used to efficiently perform the join without going through all the tuples in W i . We discuss the details of indexed join in Sect. 4.2 . 4 Partial processing  X  how much can we process? The first step in our approach to shedding CPU load without dropping tuples is to determine how much we can process given the windows of stream tuples that participate in the join. We call this step the partial processing based load shedding. For instance, consider a scenario in which the limitation in processing power re-quires dropping half of the tuples, i.e. decreasing the input rate of the streams by half. A partial processing approach is to allow every tuple to enter into the win-dows, but to decrease the cost of join processing by comparing a newly fetched tuple with only a fraction of the window defined on the opposite stream. put tuples produced by the join operator, when compared to tuple dropping or window reduction approaches. However, as we will describe later in the paper, it forms a basis to perform selective processing, which exploits the time-based cor-relation between the streams, and makes it possible to accommodate utility-based load shedding, in order to maximize the output rate or the utility of the output tuples produced.
 processing: (1) the current incoming stream rates, and (2) the performance of the stream join operation under current system load. Partial processing employs rate adaptation to adjust the amount of processing performed dynamically. The perfor-mance of the stream join under the current system load is a critical factor and it is influenced by the concrete join algorithm and optimizations used for performing join operations.
 details of utilizing indexes for efficient join processing. Finally, we describe how to employ rate adaptation in conjunction with indexed join processing. 4.1 Rate adaptation The partial processing-based load shedding is performed by adapting to the rates of the input streams. This is done by observing the tuple consumption rate of the join operation and comparing it to the input rates of the streams to determine the fraction of the windows to be processed. This adaptation is performed periodically, at every T r seconds. T r is called the adaptation period . We denote the fraction parameter as r , which defines the fraction of the windows to be processed. In other words, the setting of r answers the question of how much load we should shed.
 Algorithm 1 Rate adaptation parameter r is set to 1. Every T r seconds, the average rates of the input streams S 1 and S 2 are determined as from streams S 1 and S 2 since the last adaptation step are determined as  X  1 and  X  2 . Tuples from the input streams may not be fetched at the rate they arrive due to an inappropriate initial value of the parameter r or due to a change in the stream rates since the last adaptation step. As a result,  X  =  X  1 +  X  2 ( X  percentage of the input tuples fetched by the join algorithm. Based on the value of  X  , the fraction parameter r is readjusted at the end of each adaptation step. If  X  is smaller than 1, r is multiplied by  X  , with the assumption that comparing a tuple with the other tuples in the opposite window has the dominating cost in join processing. Otherwise, the join is able to process all the incoming tuples with the the fraction boost factor . This is aimed at increasing the fraction of the windows processed, optimistically assuming that additional processing power is available. If not, the parameter r will be decreased during the next adaptation step. Higher values of the fraction boost factor result in being more aggressive at increasing the parameter r .
 of the streams, but large enough not to cause overhead and undermine the join processing. Based on Nyquist X  X  theorem [ 6] and control theory, the adaptation pe-riod can be taken as half the period of the most frequent rate bursts in the streams. However, if this value increases the percentage of CPU time spent for performing the adaptation step beyond a small threshold (  X  0.5%), then T r should be set to its lower bound. This lower bound is the smallest value which results in spending percentage of the CPU time for adaptation. 4.2 Indexed join and partial processing Stream indexing [ 12, 25] can be used to cope with the high processing cost of the join operation, reducing the amount of load shedding performed. However, there are two important points to be resolved before indexing can be employed together with partial processing and thus with other algorithms we introduce in the following sections. The first issue is that, in a streaming scenario the index has to be maintained dynamically (through insertions and removals) as the tuples enter and leave the window. This means that the assumption made in Sect. 4.1 about finding matching tuples within a window (index search cost) being the dominant cost in the join processing, no longer holds. Second, the index does not naturally allow processing only a certain portion of the window. We resolve these issues in the context of inverted indexes that are predominantly used for joins based on set or weighted set-valued attributes. The same ideas apply to hash-indexes used for equi-joins on single-valued attributes. Our inverted-index implementation reduces to a hash-index in the presence of single-valued attributes. Here, we first give a brief overview of inverted indexes and then describe the modifications required to use them in conjunction with our load shedding algorithms. 4.2.1 Inverted indexes An inverted index consists of a collection of sorted identifier lists. In order to in-sert a set into the index, for each item in the set, the unique identifier of the set is inserted into the identifier list associated with that particular item. Similar to inser-tion, removal of a set from the index requires finding the identifier lists associated with the items in the set. The removal is performed by removing the identifier of the set from these identifier lists. In our context, the inverted index is maintained as an in-memory data structure. The collection of identifier lists are managed in a hash table. The hash table is used to efficiently find the identifier list associ-ated with an item. The identifier lists are internally organized as sorted (based on unique set identifiers) balanced binary trees to facilitate both fast insertion and removal. The set identifiers are in fact pointers to the tuples they represent. which is usually accelerated through the use of a heap. Same type of processing is used for all types of queries we have mentioned so far. Specifically, given a query set, the identifier lists corresponding to items in the query set are retrieved using the hash table. These sorted identifier lists are then merged. This is done by inserting the frontiers of the lists into a min heap and iteratively removing the top-most set identifier from the heap and replacing it with the next set identifier (new frontier) in its list. During this process, the identifier of an indexed set, sharing k items with the query set, will be picked from the heap k consecutive times, mak-ing it possible to process relatively complex overlap and inner product 2 queries efficiently [ 16]. 4.2.2 Time ordered identifier lists Although the usage of inverted indexes speeds up the processing of joins based on set-valued attributes, it also introduces significant insertion and deletion costs. This problem can be alleviated by exploiting the timestamps of the tuples that are being indexed and the fact that these tuples are received in timestamp order from the input streams. In particular, instead of maintaining identifier lists as balanced trees sorted on identifiers, we can maintain them as linked lists sorted on times-tamps of the tuples (sets). This does not affect the merging phase of the indexed search, since a timestamp uniquely identifies a tuple in a stream unless different tuples with equal timestamps are allowed. In order to handle the latter, the iden-tifier lists can be sorted based on (timestamp, identifier) pairs. This requires very small reordering, as the event of receiving different tuples with equal timestamps is expected to happen very infrequently, if it happens at all. Figure 4 provides an illustration of timestamp ordered identifier lists in inverted indexes. 2. It facilitates piggybacking of removal operations on insertion and search oper-3. Timestamp sorted identifier lists make it possible to end the merging process, 5 Selective processing  X  what should we process? Selective processing extends partial processing to intelligently select the tuples to be used during join processing under heavy system load. Given the constraint on the amount of processing defined at the partial processing phase, selective pro-cessing aims at maximizing the output rate or the output utility of the stream joins. Three important factors are used to determine what we should select for join pro-cessing: (1) the characteristics of stream window segments, (2) the profitability of join directions, and (3) the utility of different stream tuples. We first describe time correlation adaptation and join direction adaptation, which form the core of our selective processing approach. Then we discuss utility-based load shedding. The main ideas behind time correlation adaptation and join direction adaptation are to prioritize segments (basic windows) of the windows in order to process parts that will yield higher output (time correlation adaptation) and to start load shedding from one of the windows if one direction of the join is producing more output than the other (join direction adaptation).
 Algorithm 2 Time correlation adaptation 5.1 Time correlation adaptation For the purpose of time correlation adaptation, we divide the windows of the join into basic windows . Concretely, window W i is divided into n i basic windows of size b seconds each, where n i = 1 + w i / b . B i , j denotes the j th basic window in W i , j  X  X  1 ,..., n i ] . Tuples do not move from one basic window to another. basic windows slide discretely b seconds at a time. The newly fetched tuples are inserted into the first basic window. When the first basic window is full, meaning that the newly fetched tuple has a timestamp that is at least b seconds larger than the oldest tuple in the first basic window, the last basic window is emptied and all the basic windows are shifted, last basic window becoming the first. The newly fetched tuples can now flow into the new first basic window, which is empty. The basic windows are managed in a circular buffer, so that the shift of windows is a constant time operation. The basic windows themselves can be organized as either linked lists (if no indexing is used) or as inverted/hashed indexes (if indexing is used).
 called the time correlation adaptation period . During the time between two con-secutive adaptation steps, the join operation performs two types of processing. For a newly fetched tuple, it either performs selective processing or full processing . Selective processing is carried out by looking for matches with tuples in high pri-ority basic windows of the opposite window, where the number of basic windows used depends on the amount of load shedding to be performed. Full processing is done by comparing the newly fetched tuple against all the tuples from the opposite window. The aim of full processing is to collect statistics about the usefulness of the basic windows for the join operation. This cannot be done with selective pro-cessing, since it introduces bias as it processes only certain segments of the join windows and cannot capture changing time correlations.
 and in lines 1 X 5 of Algorithm 3. Full processing is only done for a sampled subset of the stream, based on a parameter called sampling probability , denoted as  X  .A newly fetched tuple goes through selective processing with probability 1  X  r  X   X  . Algorithm 3 Tuple processing and time correlation In other words, it goes through full processing with probability r  X   X  . The fraction parameter r is used to scale the sampling probability, so that the full processing does not consume all processing resources when the load on the system is high. The goal of full processing is to calculate for each basic window B i , j , the expected number of output tuples produced from comparing a newly fetched tuple t with a tuple in B i , j , denoted as o i , j . These values are used later during the adaptation Here, s i j is the index of the basic window which has the j th highest o value among the basic windows within W i . Concretely: s This means that B i , s 1 andsoon.
 tive processing, s j i values are used to guide the load shedding. Concretely, in order to process a newly fetched tuple t against window W i , first the number of tuples from window W i , that are going to be considered for processing, is determined by calculating r  X  X  W i | ,where | W i | denotes the number of tuples in the window. The fraction parameter r is determined by rate adaptation as described in Sect. 4.1 . Then, tuple t is processed against basic windows, starting from the highest prior-ity one, i.e. B i , s 1 searched for matches completely, if adding | B of tuples used so far from window W i to process tuple t does not exceeds r  X  X  W i | . Otherwise an appropriate fraction of the basic window is used and the processing is completed for tuple t . 5.1.1 Impact of basic window size The setting of basic window size parameter b involves trade-offs. Smaller values also introduce overhead in processing. For instance, recalling Sect. 4.2.1 ,inan window. Although the lists themselves are shorter and the total merging cost does not increase with smaller basic windows, the cost of looking up the identifier lists from the hash tables increases with increasing number of basic windows, n i . dent on the time correlation between the streams, is utilized for a given value of the basic window size parameter b , under a given load condition. We use r to de-note the fraction of tuples in join windows that can be used for processing. Thus, r is used to model the current load of the system. We assume that r can go over 1, in which case processing resources are abundant.
 dow W i ,where T 2 T match with a tuple t in W i that has a timestamp T ( t )  X  X  T  X  T 1 , T  X  T 2 ] .Note that, due to discrete movement of basic windows, a basic window covers a time varying area under the match probability distribution function. This area, denoted as p i , j for basic window B i , j , can be calculated by observing that the basic win-b + ( j  X  1 )  X  b ) ] on the time axis ( [ 0 ,w first basic window is full. Then, we have basic windows whose tuples are all considered for processing is denoted as c e .The fraction of tuples in the last basic window used, that are considered for processing, is denoted as c p . c p is zero if the last used basic window is completely processed. We h ave denoted as p u , can be calculated as extent of a on the time axis. Then we can calculate the optimality of p u , denoted as  X  , as follows: ing the overhead of small basic windows). Otherwise, the expected output rate is  X  times the optimal value, under current load conditions ( r ) and basic window size setting ( b ). Figure 5 plots  X  (on z -axis) as a function of b /w i (on x -axis) and r (on y -axis) for two different match probability distributions, the bottom one being more skewed. We make the following three observations from the figure:  X  Decreasing availability of computational resources negatively influences the  X  The increasing skewness in the match probability distribution decreases the  X  Smaller basic windows sizes provide better join optimality, when the avail-tion of the join windows can be processed, we have to pick a small number of basic windows that are around the peak of the match probability distribution. When the granularity of the basic windows is not sufficiently fine grained, the limited re-sources are spent processing less beneficial segments that appear within the basic windows. This effect is more pronounced when the match probability distribution is more skewed. As a result, small basic window sizes are favorable for skewed probability match distributions and heavy load conditions. We report our experi-mental study on the effect of overhead, stemming from managing large number of basic windows, on the output rate of the join in Sect. 6. 5.2 Join direction adaptation Due to time-based correlation between the streams, a newly fetched tuple from stream S 1 may match with a tuple from stream S 2 that has already made its way into the middle portions of window W 2 . This means that, most of the time, a newly fetched tuple from stream S 2 has to stay within the window W 2 for some time, before it can be matched with a tuple from stream S 1 . This implies that one direction of the join processing may be of lesser value, in terms of the number of output tuples produced, than the other direction. For instance, processing a newly fetched tuple t from stream S 2 against window W 1 will produce fewer output tuples when compared to the other way around, as the tuples to match t has not yet arrived at window W 1 . In this case, symmetry of the join operation can be broken during load shedding, in order to achieve a higher output rate. This can be achieved by decreasing the fraction of tuples processed from window W 2 first, and from W 1 later (if needed). We call this join direction adaptation .
 Specifically, two different fraction parameters are defined, denoted as r i for win-dow W i , i  X  X  1 , 2 } . During join processing, r i fraction of the tuples in window W i are considered, making it possible to adjust join direction by changing r 1 and r 2 . This requires replacing r with r i in line 3 of Algorithm 3 and line 5 of Algorithm 2. isons performed per time unit should stay the same when compared to the case where there is a single r value as computed by Algorithm 1. The number of tu-ple comparisons performed per time unit is given by 2 i = 1 ( r i  X   X  i  X  ( X  i  X  w i )) , since the number of tuples in window W i is  X  i  X  w i . Thus, we should have pected number of output tuples produced from comparing a newly fetched tuple with a tuple in W i , denoted as o i ,for i = 1 and 2. This can be computed as o r = min ( 1 , r  X  w 1 + w 2 The generic procedure to set r 1 and r 2 is given in Algorithm 4.
 portion of one of the windows is more valuable than all portions of the other window. This may not be the case for applications where both match probabil-application scenario, a two-way traffic flow between two points implies both di-rections of the join are valuable. We introduce a more advanced join direction adaptation algorithm, that can handle such cases, in the next subsection as part of utility-based load shedding. 5.3 Utility-based load shedding So far, we have targeted our load shedding algorithms toward maximizing the number of tuples produced by the join operation, a commonly used metric in the Algorithm 4 Join direction adaptation literature [ 10, 22]. Utility-based load shedding, also called semantic load shed-ding [ 23], is another metric employed for guiding load shedding. It has the benefit of being able to distinguish high utility output from output containing a large number of tuples. In the context of join operations, utility-based load shedding promotes output that results from matching tuples of higher importance/utility. In this section, we describe how utility-based load shedding is integrated into the mechanism described so far.
 join algorithm. For instance, in the news join example, certain type of news, e.g., security news, may be of higher value, and similarly in the stock trading example, phone calls from insiders may be of higher interest when compared to calls from tuples are considered important. In some cases, data mining operators can also be used to mark tuples with their importance types. As long as each tuple has an assigned utility value, our utility-based load shedding algorithm can maximize output utility while working in tandem with the selective processing based load shedding algorithm that has rate, time correlation, and join direction adaptation. a tuple t ,where Z ( t ) = z  X  Z ,as V ( z ) . Type domains and their associated utility values can be set based on application needs. In the rest of the paper, the utility value of an output tuple of the the join operation that is obtained by matching tu-to the output. Our approach can also accommodate other functions, like average (0 . 5  X  ( V ( Z ( t a )) + V ( Z ( t b )) ) ). We denote the frequency of appearance of a tuple of type z in stream S i as  X  i , z ,where z  X  Z  X  i , z = 1.
 parameter for each different type of tuple fetched from a different stream, denoted as r i , z ,where z  X  Z and i  X  X  1 , 2 } . The motivation behind this is to do less load shedding for tuples that provide higher output utility. The extra work done for such tuples is compensated by doing more load shedding for tuples that provide lower output utility. The expected output utility obtained from comparing a tuple t of type z with a tuple in window W i is denoted as u i , z , and is used to determine r z  X  X . Sect. 5.1.1 . The number of basic windows from W i whose tuples are all consid-tuples in the last basic window used from W i , that are considered for processing, processed. Thus, we have Then, the area under f i that represents the portion of window W i processed for a tuple of type z , denoted as p u ( i , z ) , can be calculated as follows: formally as subject to the processing constraint: Although the formulation is complex, this is indeed a fractional knapsack problem and has a greedy optimal solution. Assuming that some buffering is performed outside the join, this problem can be reformulated as follows. Consider I i , j , z as an item that represents processing of a tuple of type z against basic window B i , j .Item I j , z has a volume of made per time unit to process incoming tuples of type z against tuples in B i , j )and a value of  X  1  X   X  2  X   X  i , z  X  b  X  u i , z  X  p time unit, from comparing incoming tuples of type z with tuples in B i , j ). The aim is to pick the maximum number of items, where fractional items are acceptable, so that the total value is maximized and the total volume of the picked items is at most  X  1  X   X  2  X  r  X  (w 2 + w 1 ) . r i , j , z  X  X  0 , 1 ] is used to denote how much of ( n these variables as, r i , z = n i j = 1 r i , j , z .
 volume ratios, v i , j , z = u i , z  X  p estimate of p able per unit volume. However, since the number of items is large, the sort step is costly, especially for large number of basic windows and large sized domains. A Algorithm 5 Join direction adapt, utility-based shedding is described in Algorithm 5, which replaces Algorithm 4. Algorithm 5 makes use of the s j i values that define an order between value over volume ratios of items for a fixed-type z and window W i . The algorithm keeps the items representing differ-ent streams and types with the highest value over volume ratios (2  X  X  Z | of them), in a heap. It iteratively picks an item from the heap and replaces it with the item having the next highest value over volume ratio with the same stream and type subscript index. This process continues until the capacity constraint is reached. During this process r i , z values are calculated progressively. If the item picked rep-resents window W i and type z ,then r i , z is incremented by 1 / n i unless the item is picked fractionally, in which case the increment on r i , z is adjusted accordingly. 6 Experiments The adaptive load shedding algorithms presented in this paper have been imple-mented and successfully demonstrated as part of a large-scale stream processing prototype at IBM Watson Research. We report four sets of experimental results to demonstrate the effectiveness of the algorithms introduced in this paper. The first set of experiments illustrates the need for shedding CPU load for both indexed (using inverted indexes) and non-indexed joins. The second set demonstrates the performance of the partial processing-based load shedding step  X  keeping tu-ples within windows and shedding excessive load by partially processing the join through rate adaptation. The third set shows the performance gain in terms of out-put rate for selective processing, which incorporates time correlation adaptation and join direction adaptation. The effect of basic window size on the performance is also investigated experimentally. The final set of experiments presents results on the utility-based load shedding mechanisms introduced and their ability to max-imize the output utility under different workloads. Note that the overhead cost associated with dynamic adaptation has been fully taken into account and it man-ifests itself in the output rate of the join operations. Hence, we do not separately show the overhead cost. 6.1 Experimental setup The join operation is implemented as a Java package, named ssjoin.* ,and is customizable with respect to supported features, such as rate adaptation, time correlation adaptation, join direction adaptation, and utility-based load shedding, as well as various parameters associated with these features. Streams used in the experiments reported in this section are timestamp ordered tuples, where each tuple includes a single attribute, that can either be a set, weighted set, or a single value. The sets are composed of variable number of items, where each item is an integer in the range [ 1 ,..., L ] . L is taken as 100 in the experiments. Number of items contained in sets follow a normal distribution with mean  X  and standard deviation  X  . In the experiments,  X  is taken as 5 and  X  is taken as 1. The popularity of items in terms of how frequently they occur in a set, follows a Zipf distribution with parameter  X  . For equi-joins on single-valued attributes, L is taken as 5000 with  X  = 1and  X  = 0.
 time shift parameter denoted as  X  and cycle period parameter denoted as  X  .Cycle period is used to change the popularity ranks of items as a function of time. Ini-tially at time 0, the most popular item is 1, the next 2, and so on. Later at time T , the most popular item is a = 1 + L  X  T mod  X   X  , the next a + 1, and so on. Time shift is used to introduce a delay between matching items from different streams. Applying a time shift of  X  to one of the streams means that the most popular item is a = 1 + L  X  ( T  X   X ) mod  X   X  at time T , for that stream. delay of  X  = 5 8  X   X  is applied to stream S 2 and  X  = 2  X  w ,where w 1 = w 2 = w . The two histograms represent two different scenarios, in which  X  is taken as 0 . 6 and 0 . 8, respectively. These settings for  X  and  X  parametersarealsousedinthe rest of the experiments, unless otherwise stated. We change the value of param-eter  X  to model varying amounts of skewness in match probability distributions. Experiments are performed using time varying stream rates and various window sizes.
 T inputs of the join operation. We report results from overlap and equality joins. Other types of joins show similar results. The experiments are performed on an IBM PC with 512 MB main memory and 2.4 GHz Intel Pentium4 processor, using Sun JDK 1.4.2.
 load shedding by randomly dropping tuples from the input buffers and performing separately from our selective join framework and does not include any overhead due to adaptations. 6.2 Processing power limitation We first validate that the processing power happens to be the limiting resource, for both indexed and non-indexed join operations. Graphs in Fig. 7 plot input tuple drop rates (sum of the drop rates of the two streams) for non-indexed ( left )andin-dexed joins ( right ), as a function of window size for different stream rates. The join operation performed is an overlap join with a threshold value of 3. It is observed from the figure that, for a non-indexed join, even a low stream rate of 50 tuples/s results in dropping approximately half of the tuples, when the window size is set to 125 s. This corresponds to a rather small window size of around 150 KBytes, when compared to the total memory available. Although indexed join improves performance by decreasing tuple drop rates, it is only effective for moderate win-dow sizes and low stream rates. A stream rate of 100 tuples/s results in dropping approximately one quarter of the tuples for a 625 s window (approx. 10 min). This corresponds to a window size of around 1.5 MBytes. As a result, CPU load shed-ding is a must for costly stream joins, even when indexes are employed. 6.3 Rate adaptation We study the impact of rate adaptation on output rate of the join operation. For the purpose of the experiments in this subsection, time shift parameter is set to zero, i.e.  X  = 0, so that there is no time shift between the streams and the match probability decreases going from the beginning of the windows to the end. A non-indexed overlap join, with threshold value of 3 and 20 s window on one of the streams, is used.
 time. The rate of the streams stay at 100 tuples/s for around 60 s, then jump to 500 tuples/s for around 15 s and drop to 300 tuples/s for around 30 s before going back to its initial value. Figure 8 also shows (on the right y -axis) how fraction parameter r adapts to the changing stream rates.
 with and without rate adaptation, respectively. No rate adaptation case represents random tuple dropping. It is observed that rate adaptation improves output rate when the stream rates increase. That is the time when tuple dropping starts for the non-adaptive case. The improvement is around 100% when stream rates are 500 tuples/s and around 50% when 300 tuples/s. The ability of rate adaptation to keep output rate high is mainly due to the time aligned nature of the streams. In this scenario, only the tuples that are closer to the beginning of the window are useful for generating matches and the partial processing uses the beginning part of the window, as dictated by the fraction parameter r .
 riodshowninFig. 9 as a function of skewness parameter  X  , for different window sizes. It shows that the improvement in output rate, provided by rate adaptation, increases not only with increasing skewness of the match probability distribution, but also with increasing window sizes. This is because larger windows imply that more load shedding has to be performed. 6.4 Selective processing Here, we study the impact of time correlation adaptation and join direction adap-tation on output rate of the join operation. For the purpose of the experiments in this subsection, time shift parameter is taken as  X  = 5 8  X   X  . A non-indexed overlap join, with threshold value of 3 and 20 s windows on both of the streams, is used. Basic window sizes on both windows are set to 1 s for time correlation adaptation. Figure 11 also shows (on the right y -axis) how fraction parameters r 1 and r 2 adapt to the changing stream rates with join direction adaptation. Note that the reduction in fraction parameter values start with the one ( r 2 in this case) corresponding to the window that is less useful in terms of generating output tuples when processed against a newly fetched tuple from the other stream. time with three different join settings. It is observed that, when the stream rates increase, the time correlation adaptation combined with rate adaptation provides improvement on output rate (around 50%), when compared to rate adaptation only case. Moreover, applying join direction adaptation on top of time correlation adap-tation provides additional improvement in output rate (around 40%).
 skewness parameter  X  , for different join settings. This time, the overlap threshold is set to 4, which results in lower number of matching tuples. It is observed that the improvement in output rates, provided by time correlation and join direction adaptation, increase with increasing skewness in match probability distribution. The increasing skewness does not improve the performance of rate adaptive-only case, due to its lack of time correlation adaptation which in turn makes it unable to locate the productive portion of the window for processing, especially when the time lag  X  is large and the fraction parameter r is small. 6.4.1 Indexed joins The idea of selective processing applies equally well to indexed joins, as it applies to non-index joins, as long as the cost of processing a tuple against the window de-fined on the opposite stream monotonically increases with the increasing fraction of the window processed. This assumption is valid for most type of joins, such as the inverted-index supported set joins discussed before. One exception to this is the hash-table supported equi-joins, since probing the opposite window takes constant time for equi-joins with hash tables. As a result, no gain is to be expected from using selective processing for equi-joins.
 with all adaptations applied, over random dropping in terms of the output rate of the join as a function of stream rates, for equi-joins and overlap joins with and without indexing support. For the non-indexed scenarios windows of size 20 s are used, whereas for indexed scenarios windows of size 200 s are used to increase the cost of the join to necessitate load shedding (recall Fig. 7). join or overlap join), selective processing provides up to 500% higher output rate, the improvement being higher for overlap joins, as they are more costly to eval-uate. Second, for overlap joins with inverted indexes the improvement provided by selective processing is not observed until the stream rates get high enough, 300 tuples/s in this example. This is intuitive, since with indexes the join is eval-uated much faster and as a result the load shedding starts when the stream rates are high enough to saturate the processing resources. From the figure, we ob-serve up to around 100% improvement over random dropping for the case of indexed overlap join. The relative improvement in output rate is likely to in-overlap join suggests in Fig. 15 . Third, we see no improvement in output rate when indexed equi-join is used. As the zoomed-in portion of the figure shows, there is no deterioration in the output rate either. As a result, selective sampling is performance-wise indifferent in the case of equi-joins, compared to random dropping. 6.4.2 Basic window size We study the impact of basic window size on output rate of the join operation. The graphs in Fig. 14 plot average join output rate as a function of basic win-dow size, for different  X  values. The graphs on the left represents a non-indexed overlap join, with threshold value of 3 and 20 s windows, respectively, on both of the streams. The graphs on the right represents an indexed overlap join, with threshold value of 3 and 200 s windows, respectively, on both of the streams. For the indexed case, both identifier sorted and time sorted inverted indexes are used. The  X  X one X  value on the x -axis of the graphs represent the case where basic win-dows are not used (note that this is not same as using a basic window equal used. ever, there are two interesting observations for the indexed join case. First, for very small basic window sizes, we observe a drop in the output rate. This is due to the overhead of processing large number of basic windows with indexed join. In particular, the cost of looking up identifier lists for each basic window that is used for join processing, creates an overhead. Further decreasing basic window size does not help in better capturing the peak of the match proba-bility distribution. Second, identifier sorted inverted indexes show significantly lower output rate, especially when the basic window sizes are large. This is be-cause identifier sorted inverted indexes do not allow partial processing based on time. 6.5 Utility-based load shedding We study the effectiveness of utility-based load shedding in improving the output utility of the join operation. We consider three different scenarios in terms of set-ting type frequencies; (i) uniform, (ii) inversely proportional to utility values, and (iii) directly proportional to utility values. For the experiments in this subsection, we use a non-indexed overlap join, with threshold value of 3 and 20 s windows on both of the streams. Five hundred tuples per second is used as the stream rate. The graphs in Fig. 16 plot the improvements in the output utility of the join, compared to the case where no utility-based load shedding is used, as a function of skewness in utility values. Both joins are rate, time correlation, and join direction adaptive. In this experiment, there are three different tuple types, i.e. | Z |= 3.Foraskew-from the figure that, the improvement in the output utility increases with increas-ing skewness for uniform and inversely proportional type frequencies, where it stays almost stable for directly proportional type frequencies. Moreover, the best improvement is provided when item frequencies are inversely proportional to util-ity values. Note that this is the most natural case, as in most applications, rare items are of higher interest.
 the output utility. It plots the improvement in the output utility as a function of type domain size, for different value difference factors . A value difference fac-tor of x means the highest utility value is x times the lowest utility value and the utility values follow a Zipf distribution (with parameter log | Z | x ). The type frequencies are selected as inversely proportional to utility values. It is observed from the figure that, there is an initial decrease in the output utility improvement with increasing type domain size. But the improvement values stabilize quickly as the type domain size gets larger. Same observation holds for different amounts of skewness in utility values. 7Conclusion We have presented an adaptive CPU load shedding approach for stream join op-erations. In particular, we showed how rate adaptation, combined with time-based correlation adaptation and join direction adaptation, can increase the number of output tuples produced by a join operation. Our load shedding algorithms em-ployed a selective processing approach, as opposed to commonly used tuple drop-ping. This enabled our algorithms to nicely integrate utility-based load shedding with time correlation-based load shedding in order to improve output utility of the join for the applications where some tuples are evidently more valued than others. Our experimental results showed that (a) our adaptive load shedding al-gorithms are very effective under varying input stream rates, varying CPU load conditions, and varying time correlations between the streams; and (b) our ap-proach significantly outperforms the approach that randomly drops tuples from the input streams. References Author Biographies
