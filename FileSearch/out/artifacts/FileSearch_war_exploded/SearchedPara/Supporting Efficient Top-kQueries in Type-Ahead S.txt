 Type-ahead search can on-the-fly find answers as a user types in a keyword query. A main challenge in this search paradigm is the high-efficiency requirement that queries must be answered within milliseconds. In this paper we study how to answer top-k queries in this paradigm, i.e., as a user types in a query letter by letter, we want to efficiently find the k best answers. Instead of inventing completely new algorithms from scratch, we study challenges when adopting existing top-k algorithms in the literature that heavily rely on two basic list-access methods: random access and sorted access. We present two algorithms to support random access efficiently. We develop novel techniques to support efficient sorted access using list pruning and materialization. We extend our techniques to support fuzzy type-ahead search which allows minor errors between query keywords and answers. We report our experimental results on several real large data sets to show that the proposed techniques can answer top-k queries efficiently in type-ahead search.
 H.3.3 [ Information Search and Retrieval ]: Retrieval models Algorithms, Experimentation, Performance Type-ahead search, top-k search, fuzzy search
To give instant feedback when users formulate search queries, many information systems support autocomplete search, which shows results immediately after a user types in a partial keyword query. As an example, almost all the major search engines nowadays automatically suggest possible keyword queries as a user types in partial keywords. Most autocomplete systems treat a query with multiple keywords as asingle string , and find answers with text that matches the string exactly. To overcome this limitation, a new type-ahead search paradigm has emerged recently [2, 13]. Using this paradigm, a system treats a query as a set of keywords, and does a full-text search on the underlying data to find answers including the keywords. We treat the last keyword in the query as a partial keyword the user is completing. For instance, a query X  graph sig  X  X n a publication table can find publication records with the keyword X  graph  X  X ndakeyword that has  X  sig  X  as a prefix, such as  X  sigir  X ,  X  sigmod  X , and  X  signature  X . In this way, a user can get instant feedback after typing keywords, thus can obtain more knowledge about the underlying data to formulate a query more easily.
Ji et al. [13] extended type-ahead search by allowing minor errors between queries and answers. As a user types in query keywords, the system can find relevant records with keywords similar to the query keywords. This feature is especially important when the user has limited knowledge about the exact representation of entities she is looking for. For instance, if a user types in a partial query X  chritos falut  X , the system can find records approximately matching the two keywords despite the typo in the query, such as a record with keywords  X  X hristos Faloutsos X . Clearly these features can further improve user search experiences.

In this paper we study how to answer ranking queries in type-ahead search on large amounts of data. That is, as a user types in a keyword query letter by letter, we want to on-the-fly find the most relevant (or  X  X op-k  X ) records. One approach first finds records matching those query keywords, and then computes their ranking scores to find the most relevant ones. This approach is not efficient when there are a large number of candidate answers to compute and store. Existing type-ahead search approaches assume an index structure with a trie for the keywords in the underlying data, and each leaf node has an inverted list of records with this keyword, with the weight of this keyword in the record [13, 19]. As an example, Table 1 shows a sample collection of publication records. For simplicity, we only list some of the keywords for each record. Figure 1 shows the corresponding index structure. (More details about the index are in Section 3.)
Suppose a user types in a query  X  graph icdm li  X . For exact search, we find records containing the first two keywords and a word with prefix of  X  li  X , e.g., record r 5 . For fuzzy search, we compute records with keywords similar to query keywords, and rank them to find the best answers. For each complete keyword, we find keywords similar to the query keyword. For instance, both keywords  X  icdm  X  X nd X  icdl  X  are similar to the second query keyword. The last keyword Table 1: Publication records with sample keywords.  X  li  X  is treated as a prefix condition, since the user is still typing at the end of this keyword. We find keywords that have a prefix similar to  X  li  X , such as  X  lin  X ,  X  liu  X , and  X  lui  X . We access the inverted lists of these similar keywords to find records and rank them to find the best answers for the user.
A key question is:  X  X ow to access inverted lists on trie leaf nodes efficiently to answer top-k queries? X  Instead of inventing completely new algorithms from scratch, we study how to adopt a plethora of algorithms in the literature for answering top-k queries by accessing lists (e.g., [21, 12]). These algorithms share the same framework proposed by Fagin [6], in which we have lists of records sorted based on various conditions. An aggregation function takes the scores of a record from these lists and computes the final score of the record. There are two methods to access these lists: (1) Random Access : Given a record id, we can retrieve the score of the record on each list; (2) Sorted Access : We retrieve the record ids on each list following the list order.
In this paper we study technical challenges when adopting these algorithms, and focus on new optimization opportunities that arise in our problem. In particular, we study how to support the two types of access operations efficiently by utilizing characteristics specific to our index structures and access methods. We make the following contributions: 1) In Section 3, we present a forward-list-based method for supporting random access on the inverted lists, and develop a heap-based method and list-materialization techniques to support sorted access efficiently. 2) In Section 4 we study fuzzy type-ahead search. We propose a list-pruning technique to improve the performance of sorted access, and study how to improve the techniques based on forward lists and list materialization for fuzzy search. Due to the challenging nature of the problem, our extensions are technically nontrivial. 3) In Section 5 we present our experimental results on real large data sets to show the efficiency of our techniques. We have deployed several systems using this paradigm, which have been used regularly and well accepted by users due to its friendly interface and high efficiency 1 . Type-Ahead Search :Let R be a collection of records such as the tuples in a relational table. Let D be the set of words in R .Let Q be a query the user has typed in, which is a sequence of keywords w 1 ,w 2 ,...,w m . We treat the last keyword w m as a partial keyword the user is completing, and other keywords as complete keywords the user has completed As a user types in a keyword query letter by letter, type-ahead search on-the-fly finds records that contain the first m  X  keywords and a word with the last keyword as a prefix. Without loss of generality, each string in the data set and a query is assumed to use lower-case letters. For example, in Suppose a user types in a query X  icdm gra  X . We treat X  icdm  X  as a complete keyword and X  gra  X  X s a partial keyword. Records r , r 4 , r 5 ,and r 6 are potentially relevant answers. For example, r 0 contains complete keyword  X  icdm  X  X ndword  X  graph  X  X ithaprefixof X  gra  X . When the user types in more letters and submits query  X  icdm graph li  X , we treat  X  icdm  X  and X  graph  X  X s complete keywords and X  li  X  X s a partial keyword. Records r 4 and r 5 are potentially relevant answers. Top-k Answers : We rank each record r in R based on its relevance to the query. Given a positive integer k ,our goal is to compute the best k records in R ranked by their relevance to Q . Notice that our problem setting allows an important record to be in the answer, even if not all query keywords appear in the record (the  X  X R X  semantics). Thus the algorithms in [13] cannot be used directly in our problem. Ranking : In the literature there are many algorithms for answering top-k queries by accessing lists (e.g., [21, 12]). These algorithms share the same framework proposed by Fagin [6], in which we have lists of records sorted based on various conditions, such as term frequency and inverse document frequency ( X  X f*idf X ). Each record has a score on a list, and we use an aggregation function to combine the scores of the record on different lists to compute its overall relevance to the query. The aggregation function needs to be monotonic , i.e., decreasing the score of a record on a list cannot increase the record X  X  overall score. This approach has the advantage of allowing a general class of ranking functions. In this paper, we focus on an important class of ranking functions with the following property: the score F ( r, Q ) of a record r to a query Q is a monotonic combination of scores of the query keywords with respect to the record r . Formally, we compute the score F ( r, Q )intwo steps. In the first step, for each keyword w ,wecomputea scoreofthekeywordwithrespecttotherecord r , denoted by F ( r, w ). In the second step, we compute the score F ( r, Q ) by applying a monotonic function on the F ( r, w ) X  X  for all the keywords w . The intuition of this property is that the more relevant an individual query keyword is to a record, the more likely this record is a good answer to this query. For example, we compute the score of a record to query  X  icdm graph li  X  X y aggregating the scores of each of keywords with respect to the record.

Each complete keyword w has a weight associated with arecord r , denoted by W ( r, w ). This weight could depend Figure 2: Type-ahead search for Q = w 1 ,w 2 ,...,w m . on the keyword, such as the tf*idf value of the keyword in the record. As a specific case, it can also be independent from the keyword. For instance, if a record is a URL with tokenized keywords, its weight could be a rank score of the corresponding Web page. If a record is an author, we can use the number of publications of the author as a weight of this record. For the last partial keyword w m , there could be multiple complete words. We compute the relevance score of w m in the record, i.e., F ( r, w m ), based on the following property: F ( r, w m ) is the maximal value of the W ( r, d ) weights for all the keywords d with respect to w m in r , where d is a keyword in record r and has a prefix of w m . This property states that we only look at the most relevant keyword in a record to the partial keyword when computing the relevance of the keyword to the record. It means that the ranking function is  X  X reedy X  to find the most relevant keyword in the record as an indicator of how important this record is to the partial keyword. As we can see in Section 3, this property allows us to do effective pruning when accessing the multiple lists of a query keyword. The following is an example function.
 where
F ( r, w i )= W ( r, w i ) if 1 In Figure 1, consider query  X  icdm graph li  X  and record r F ( r 5 ,  X  icdm  X ) = W ( r 5 ,  X  icdm  X )=8and F ( r 5 ,  X  graph  X ) = W ( r 5 ,  X  graph  X ) = 9. The partial keyword  X  li  X  X astwo complete words  X  lin  X  X nd X  liu  X . F ( r 5 ,  X  li  X ) = max  X  lin  X ), W ( r 5 , X  liu  X ) } =8. F ( r 5 , X  icdm graph li  X ) = 25.
In this section, we study efficient list-access methods to support exact type-ahead search, i.e., no mismatches between query keywords and answers.
 Indexing : We construct a trie for the data keywords in the data D . A trie node has a character label. Each keyword in D corresponds to a unique path from the root to a leaf node 3 on the trie. For simplicity, a trie node is mentioned interchangeably with the keyword corresponding to the path from the root to the node. A leaf node has an inverted list of IDs of pairs rid,weight ,where rid is the ID of a record containing the leaf-node string, and weight is the weight of the keyword in the record. Figure 1 shows the index structure in our running example. For instance, for the leaf node of keyword  X  graph  X , its inverted list has five elements. p h The first element  X  r 5 , 9  X  indicates that the record r this keyword, and the weight of this keyword in this record is  X 9 X , i.e., W ( r 5 ,  X  graph  X ) = 9.
 Searching : We compute the top-k answers to a query Q in two steps. As illustrated in Figure 2, in the first step, for each complete keyword w i (1  X  i  X  m  X  1), we get its inverted list. For the last partial keyword, we locate the trie node of w m and retrieve the inverted lists of the trie node X  X  leaf descendants. For example, in Figure 1, consider a query X  icdm li  X . The partial keyword X  li  X  X as two leaf-node keywords:  X  lin  X  X nd X  liu  X . In the second step, we access the inverted lists to compute the k best answers.
Many algorithms have been proposed for answering top-k queries by accessing sorted lists [12, 6]. When adopting these algorithms to solve our problem, we need to efficiently support two basic types of access used in these algorithms: random access and sorted access on the lists.
To support random access, we construct a forward index in which each record has a forward list of IDs of its keywords. We assume each keyword has a unique ID with respect to its leaf node on the trie, and the IDs of the keywords follow their alphabetical order. Figure 3 shows the forward lists. The element  X  1 , 9  X  X ntheforwardlistofrecord r 5 shows that this record has a keyword with ID 1 and weight 9, which is keyword  X  graph  X  as shown on the trie.

Given a record and a complete keyword, we can get the corresponding weight by doing a binary-search on the forward list. For example, to get the weight of keyword  X  icdm  X  X ith ID 6 in r 5 , we can do a binary search on r 5  X  X  forward list and get the corresponding weight 8. For the partial keyword, as it has multiple complete words, we need first locate its trie node and then enumerate its leaf-descendants to get the corresponding weights. This method could be expensive if the trie node has many leaf-descendants. To improve the performance, we can use an alternative method. For each trie node n , we can maintain a keyword range [ l n ,u n ], where l and u n are the minimal and maximal keyword IDs of its leaf nodes, respectively [13]. An interesting observation is that a complete word with n as a prefix must have an ID in this keyword range, and each complete word in the data set with an ID in this range must have a prefix of n .In Figure 3, the keyword range of node  X  g  X  X s[1 , 4], since 1 is the smallest ID of its leaf nodes and 4 is the largest one.
Based on this observation, this method verifies whether record r contains a keyword with a prefix of w m as follows. We first locate the trie node w m and then check if there is a keyword ID on the forward list of r in the keyword range [ l w m ,u w m ]. Since we can keep the forward list of r sorted, this checking can be done efficiently. For instance, consider query  X  graph icdm l  X . For the first element on the inverted list of  X  graph  X , r 5 , 9 , we can check whether Figure 4: A heap-based method to compute the virtual sorted list of partial keyword  X  l  X . r 5 contains other two keywords as follows. For complete keyword  X  icdm  X  X ithID6,wedoabinarysearchon r 5  X  X  forward list and get weight 8. For partial keyword  X  l  X  X ith keyword range [7 , 9], using a binary search on r 5  X  X  forward list 1 , 9 ; 2 , 8 ; 3 , 4 ; 6 , 8 ; 7 , 3 ; 8 , 8 , we find keyword IDs 7 and 8 in this range. Thus we know that the record indeed contains keywords with prefix  X  l  X , and compute the corresponding score F ( r 5 ,  X  l  X ) = max F ( r 5 ,  X  lin  X ) ,F ( r F ( r 5 ,  X  lui  X ) =8.Thus F ( r 5 ,  X  graph icdm l  X ) = 25.
To support sorted access, we can keep the elements on the inverted lists sorted based on their weights in a descending order. Thus, for the complete keyword, we can get an ordered list. For the partial keyword w m , it has multiple leaf descendants and corresponding inverted lists. We use U ( w m )todenote the union of those inverted lists, called union list of w need to support sorted access on U ( w m ) to retrieve the next most relevant record ID for w m . Fully computing U ( w m using the keyword lists could be expensive in terms of time and space. In this section, we propose two techniques to support sorted access efficiently.
We can support sorted access on U ( w m ) by building a max heap on the inverted lists of its leaf nodes. In particular, we maintain a cursor on each inverted list. The max heap initially consists of the record IDs pointed by the cursors so far, sorted on the weights of the keywords in these records. Notice that each inverted list is already sorted based on the weights of its keyword in the records. To retrieve the next best record, we pop the top element from the heap, increment the cursor of the list of the popped element by 1, and push the new element of this list to the heap. When popping all elements from the heap, we can get a sorted list for the partial keyword. For example, consider the partial keyword  X  l  X . It has three complete keywords  X  lin  X ,  X  liu  X , and  X  lui  X . We can compute its union list as shown in Figure 4. Note that since our method does not need to compute the entire list of U ( w m ), U ( w m )isa virtual sorted list of partial keyword w m . On top of the inverted lists of complete keywords and the max heap of the partial keyword, we can adopt an existing top-k algorithm to find the k best records.

As an example, suppose we want to compute the top-1 best answer for query  X  graph icdm l  X  using sorted access only. We get the first elements of  X  graph  X  X nd X  icdm  X , r and r 4 , 9 , pop the top element of the max heap in Figure 4, r , 9 , and compute an upper bound on the overall score of Figure 5: Benefits of materializing the union list U ( v ) for node v with respect to partial keyword w m . an answer, i.e., 27. We get the next elements of  X  graph  X  X nd  X  icdm  X , r 4 , 7 and r 5 , 8 . We increment the cursor of the list that produces the top element, push it into the heap, and retrieve the next top element: r 5 , 8 . Based on the accessed elements, we have 1) The score of record r 5 is 9+8+8 = 25; 2) The maximal score of record r 3 is 7 + 8 + 9 = 24, and that of r 4 is 7 + 9 + 8 = 24, while those of other records are at most 7 + 8 + 8 = 23. Thus, record r 5 is the best answer.
We can further improve the performance of sorted access for the partial keyword w m by precomputing and storing the unions of some of the inverted lists on the trie. Let v be a trie node, and U ( v ) be the union of the inverted lists of v  X  X  leaf nodes, sorted by their record weights. If a record appears more than once on these lists, we choose its maximal weight as its weight on list U ( v ). For example, U ( X  li  X ) = { r , 8 , r 7 , 8 ; r 4 , 7 , r 6 , 5 , r 9 , 4 , r 2 , 3 , r using a max heap to retrieve records sorted by their scores for the partial keyword, this materialized list could help us build a max heap with fewer lists and reduce the cost of push/pop operations on the heap. Therefore, this method allows us to utilize additional memory space to answer top-k queries more efficiently. For instance, consider the index in Figure 1 and a query  X  icdm gr  X . For the partial keyword  X  gr  X , we access its data keywords  X  graph  X ,  X  gray  X ,  X  gross  X , and  X  group  X , and build a max heap on their inverted lists based on record scores with respect to this query keyword. If we materialize the union lists of  X  gra  X  X nd X  gro  X , we can use their materialized lists, saving the time to traverse the four leaf nodes and some push/pop operations on the heap.
We next give a detailed cost-based analysis to quantify the benefit of materializing a node on the performance of operations on the max heap of w m , for exact type-ahead search. Let B be a budget of storage space we are given to materialize union lists. Given a trie node v ,let U ( v )bethe union of inverted lists of leaf nodes in the subtrie of v .Our goal is to select trie nodes to materialize their union lists for maximizing the performance of queries. The following are naive algorithms for choosing trie nodes:  X  Random : We randomly select trie nodes.  X  TopDown : We select nodes top down from the trie root.  X  BottomUp : We select nodes bottom up from leaf nodes.
Each naive approach keeps choosing trie nodes to materialize their union lists until the sum of their list sizes reaches the space limit B . One main limitation of these approaches is that they do not quantitatively consider the benefits of materializing a union list. To overcome this limitation, we propose a cost-based method called CostBased to do list materialization. Its main idea is the following.

For simplicity we say a node has been  X  materialized  X  X f its union list has been materialized. For a query Q with a prefix keyword w m , suppose some of the trie nodes have their union lists materialized. Let v be such a materialized node. If we can use U ( v ) to construct the heap of w m ,we need not visit v  X  X  descendants and access the inverted lists of v  X  X  leaf descendants, and thus achieve the benefit of reducing the time of traversing the subtrie rooted at v and push/pop operations on the max heap of w m . We say the materialized node v is usable for partial keyword w m .

Next we discuss how to check whether a node v is usable for partial keyword w m .If v is not a descendant of w m , materializing v is unusable to w m ; otherwise, if no node on the path from v to w m (including w m ) has been materialized, materializing v is usable to w m . Notice that if v has a materialized ancestor v on the path from v to w m ,then we can use the materialized list U ( v ) instead of U ( v ), and the list U ( v ) will no longer be usable to w m . To summarize, a materialized node v is usable for partial keyword w m if, 1. v is a descendant of w m ;and 2. v has no materialized ancestor between v and w m .
For example, consider a query X  icdm g  X , materializing node  X  l  X  X s unusable for partial keyword X  g  X  X s X  l  X  X s not a descendant of  X  g  X . Materializing  X  gr  X  X susablefor X  g  X  X f X  g  X  X snot materialized. If X  gr  X  X s materialized, then materializing X  gra  X  is unusable for  X  g  X  as we will use the materialized list of  X  gr  X  to build the max heap of  X  g  X , instead of using  X  gra  X .
If v is usable for w m , materializing U ( v ) has the following benefits for the heap of w m . (1) We do not need to traverse the trie to access these leaf nodes and use them to construct the max heap; (2) Each push/pop operation on the heap is more efficient since it has fewer lists. Here we present an analysis of the benefits of materializing the usable node v . In general, for a trie node v ,let T ( v ) denote its subtrie and |
T ( v ) | denote the number of nodes in T ( v ). The total time of traversing this subtrie is O | T ( v ) | .

Now we analyze the benefit of materializing node v .As illustrated in Figure 5, suppose v has materialized descendants. Let M ( v ) be the set of highest materialized descendants of v . These materialized nodes can help reduce the time of accessing the inverted lists of v  X  X  leaf nodes in two ways. First, we do not need to traverse the descendants of a materialized node d  X  M ( v ). We can just traverse | T ( v ) | X  d  X  M ( v ) trie nodes. Second, when inserting lists to the max heap of w m , we insert the union list of v into the heap and need not insert the union list of each d  X  M ( v ) and the inverted lists of d  X  N ( v ) into the heap, where N ( v ) denotes the set of v  X  X  leaf descendants having no ancestors in M ( v ). Let S ( v )= M ( v )  X  N ( v ). We quantify benefits of materializing node v : 1. Reducing traversal time : Since we do not traverse v  X  X  2. Reducing heap-construction time : When constructing 3. Reducing sorted-access time : If we insert the union list
The following is the overall benefit of materializing v for the partial keyword w m : where A v is the number of sorted accesses on U ( v ). A v be computed using the number of records in the union list U ( v ), and the number of keywords in the query.
The analysis above is on a query workload. If there is no query workload, we can use the trie structure to count the probability of each node to be queried and use such information to compute the benefit of materializing a node. In this paper, we employ a no query workload setting.
In this section, we first define the problem of top-k queries in fuzzy type-ahead search [13]. We then develop new techniques to support efficient list access to answer such queries by extending techniques developed in exact search.
As a user types in a query letter by letter, fuzzy type-ahead search on-the-fly finds records with words similar to the query keywords. For example, consider the data in Table 1. Suppose a user types in a query  X  graph grose  X . We return r as a relevant answer since it has a keyword X  gross  X  X imilar to query keyword  X  grose  X . We use edit distance to measure the similarity between strings. Formally, the edit distance between two strings s 1 and s 2 , denoted by ed ( s 1 , s minimum number of single-character edit operations (i.e., insertion, deletion, and substitution) needed to transform s to s 2 . For example, ed ( gross , grose )=1.
 Similarity Function :Let  X  be a function that computes the similarity between a data string s and a query keyword w in Q = w 1 ,w 2 ,...,w m . An example is: where | w | is the length of the query keyword w . We normalize the edit distance based on the query-keyword length in order to allow more errors for longer query keywords. Our results in the paper focus on this function, and they can be generalized to other functions using edit distance.

Let d be a keyword in the data set D .Foreachcomplete keyword w i ( i =1 , 2 ,...,m  X  1) in the query, we define the similarity of d to w i as: Since the last keyword w m is treated as a prefix condition, we define the similarity of d to w m as the maximal similarity of d  X  X  prefixes using function  X  , i.e.: Let  X  be a similarity threshold. We say a keyword d in D is similar to a query keyword w if Sim ( d, w )  X   X  .Wesaya prefix p of a keyword in D is similar to the query keyword w m if  X  ( p, w m )  X   X  . We want to find the keywords in the data set that are similar to query keywords, since records with such a keyword could be of interest to the user.
Legend: Figure 6: Keywords similar to those in query Q = w 1 ,w 2 ,...,w m . Each query keyword w i has similar keywords on leaf nodes. The last prefix keyword w m has similar prefixes.
 Let  X ( w i )( i =1 ,...,m )denotethesetofkeywordsin D similar to w i ,and P ( w m ) denote the set of prefixes (of keywords in D ) similar to w m . We compute the top-k answers to the query Q in two steps. In the first step, for each keyword w i in the query, we first compute an edit-distance upper bound based on the similarity function, i.e., (1  X   X  )  X  X  w i | , and then compute the similar keywords  X ( w i )and similar prefixes P ( w m ) on the trie (shown in Figure 6). Ji et al. [13] developed an efficient algorithm for incrementally computing these similar strings as the user modifies the current query. A similar algorithm is developed in [5]. In the second step, we access the inverted lists of these similar data keywords to compute the k best answers.

For example, assume a user types in a query  X  grose li  X  letter by letter on the data shown in Table 1. Suppose the similarity threshold  X  is 0 . 45. The set of prefixes similar to the partial keyword  X  li  X  X s P ( X  li  X ) = { l , li , lin , liu , lu , lui , i } , and the set of data keywords similar to the partial keyword  X  li  X  X s X ( X  li  X ) = { lin , liu , lui , icdl , icdm particular,  X  lui  X  is similar to  X  li  X  X ince Sim ( lui , li )=1 keyword X  grose  X  X s X ( X  grose  X ) = { gross } . Then we compute top-k answers using the inverted lists of those words in  X ( X  grose  X ) and  X ( X  li  X ).
 Ranking : We still assume the ranking function has the first property described in Section 2, which computes the score F ( r, Q ) by applying a monotonic function on the F ( r, w for all the keywords w i in the query. Given a complete keyword w i and a record r , for exact search, we can use the weight of w i in r , i.e., W ( r, w i ), to denote their relevancy F ( r, w i ). But for fuzzy search, the keyword w i can be similar to multiple keywords in the record r , and different similar words have different similarities to w i and different weights in r . A question is how to compute the relevance value of keyword w i in record r , F ( r, w i ).

Let d be a keyword in record r such that d is similar to the query keyword w i , i.e., d  X   X ( w i ). We use F ( r, w denote the relevance of this query keyword w i in the record with respect to keyword d . The value should depend on both the weight of d in r , i.e., W ( r, d ), as well as the similarity between w i and d , i.e., Sim ( d, w i ). Intuitively, the more similar they are, the more relevant w i is to r in terms of d . For instance, F ( r, w i ,d )= Sim ( d, w i )  X  W ( r, d )isanexample ranking function to evaluate the relevancy of w i in the record with respect to keyword d . We use the following function with the second property in Section 2 to compute F ( r, w
We first study how to support efficient random access for fuzzy type-ahead search. For simplicity, in the discussion we focus on how to verify whether the record has a keyword with a prefix similar to the partial keyword w m . With minor modifications the discussion extends to the case where we want to verify whether r has a keyword similar to a complete keyword w i (1  X  i  X  m  X  1).

In each random access, given an ID of a record r ,wewant to retrieve information related to a query keyword w i ,which allows us to retrieve W ( r, d ) for each of w i  X  X  similar word d so as to compute the score F ( r, w i ). In particular, for a keyword w i in the query, does the record r have a keyword similar to w i ? One naive way to get the information is to retrieve the original record r and go through its keywords. This approach has two limitations. First, if the data is too large to fit into memory and has to reside on hard disks, accessing the original data from the disks may slow down the process significantly. This costly operation will prevent us from achieving an interactive-search speed. The second limitation is that it may require a lot of computation of string similarities based on edit distance, which could be time consuming. In this section, we present two efficient approaches for solving this problem.
 Method 1: Probing on Forward Lists : This method verifies whether record r contains a keyword with a prefix similar to w m as follows. For each prefix p on the trie similar to w m (computed in the first step of the algorithm as discussed above), we check if there is a keyword ID on the forward list of r in the keyword range [ l p ,u p ] of the trie node of p as discussed in Section 3.
 Method 2: Probing on Trie Leaf Nodes : Using this method, for each prefix p similar to w m , we traverse the subtrie of p and identify its leaf nodes. For each leaf node d , we store the fact that for the query Q ,thiskeyword d has a prefix similar to w m in the query. Specifically, we store We store the query ID in order to differentiate it from other queries in case multiple queries are answered concurrently. We store the similarity between w m and p to compute the score of this keyword in a candidate record. In case the leaf node has several prefixes similar to w m , we only keep their maximal similarity to w m . For each complete keyword w , we also store the same information for those trie nodes similar to w i . Therefore, a leaf node might have multiple entries corresponding to different keywords in the same query. We call these entries for the leaf node as its collection of relevant query keywords . Notice that this structure needs very little storage space, since the entries of old queries can be quickly reused by new queries, and the number of keywords in a query tends to be small. We use this additional information to efficiently check if a record r contains a complete word with a prefix similar to the partial keyword w m .We scan the forward list of r . For each of its keyword IDs, we locate the corresponding leaf node, and test whether its collection of relevant query keywords includes this query and a p h the keyword w m . If so, we use the stored string similarity to compute the score of this keyword in the query.
Figure 7 shows how we use this method in our running example, where the user types in a keyword query q 1 = lin , grose . When computing the similar words of X  grose  X , i.e.,  X  gross  X , we insert the query ID (shown as  X  q 1  X ), the partial keyword X  grose  X , and the corresponding prefix similarity to its collection of relevant query keywords. To verify whether record r 5 has a word with a prefix similar to X  grose  X , we scan its forward list. Its third keyword is  X  gross  X . We access its corresponding leaf node, and see that the node X  X  collection of relevant query keywords includes  X  grose  X . Thus we know that r 5 indeed contains a keyword similar to  X  grose  X , and can retrieve the corresponding prefix similarity. Comparison: The time complexity of the forward-list based method (Method 1) is O G  X  log ( | r | ) , where G is the total number of similar prefixes of w m and similar complete words of w i  X  X  for 1  X  i  X  m  X  1, and | r | is the number of distinct keywords in record r . Since the similar prefixes of w m could have ancestor-descendant relationships, we can optimize the step of accessing them by considering the  X  X ighest X  ones. The time complexity of the second method is The first term corresponds to the time of traversing the subtries of similar prefixes, where T ( p ) is the subtrie rooted at a similar prefix p . The second term corresponds to the time of probing the leaf nodes, where | Q | is the number of query keywords. Notice that to identify the answers, we need access the inverted lists of complete words, thus the first term can be removed from the complexity. Method 1 is preferred for data sets where records have a lot of keywords such as long documents, while Method 2 is preferred for data sets where records have a small number of keywords such as relational tables with relatively short attribute values. Heap-Based Method: For a query keyword w ,wewant to support sorted access that can access record IDs based on the relevance of w to these records. As w has multiple similar words, we can support sorted access efficiently by building a max heap on the inverted lists of such similar words, as described in Section 3. Notice that, in exact search, each leaf node has the same similarity to w ; but for fuzzy search, different leaf nodes could have different similarities. Thus, when pushing a record r from an inverted list of a similar word d to the heap, we maintain r, F ( r, d ) in the heap. We push/pop the record on the heap with the maximal F ( r, d ).
Consider the query  X  icdm li  X . Figure 8 shows the two heaps for the two keywords. For illustration purposes, for Figure 8: Max heaps for the query keywords  X  icdm  X  and  X  li  X . Each shaded list is merged from the underlying lists. It is  X  X irtual X  since we do not need to compute the entire list. each keyword we also show the virtual merged list of records with their scores, and this list is only partially computed during the traversal of the underlying lists. Each record on a heap has an associated score of this keyword with respect to the query keyword, computed using Equation 4.
 List Pruning: As there may be a large number of similar words for a query keyword, especially for the partial keyword, it could be expensive to construct a heap on the fly. We further improve the performance of sorted access on the virtual sorted list U ( w ) by using the idea of  X  X n-demand heap construction, X  i.e., we want to avoid constructing a heap for all the inverted lists of keywords similar to a query keyword. Suppose w has t similar words. Each push/pop operation on the heap of these lists takes O ( log ( t )) time. If we can reduce the number of lists on the heap, we can reduce the cost of its push/pop operations. We have two observations about this pruning method. (1) As a special case, if those keywords matching query keywords exactly have the highest relevance scores, this method allows us to consider these records prior to considering other records with mismatching keywords. (2) The pruning can be more powerful if w is the last partial keyword w m ,sincemanyof its similar keywords share the same prefix p on the trie.
Consider query  X  icdm li  X , Figure 8 illustrates how we can prune low-score lists and do on-demand heap constructions. The prefix  X  li  X  has several similar keywords. Among them, the two words  X  lin  X  X nd X  liu  X  have the highest similarity value to the query keyword, mainly because they have a prefix matching the keyword exactly. We build a heap using these two lists. To compute the top-1 best answer, the lists of  X  lui  X ,  X  icdm  X , and  X  icdl  X  are never included in the heap since their upper bounds are always smaller than the scores of popped records before the traversal terminates.
We next introduce how to do list pruning for the max-heap based methods in fuzzy type-ahead search. Given a keyword w ,let d 1 ,...,d t be its similar words and L 1 ,...,L t corresponding inverted lists, respectively. We need not use all the inverted lists to build the max heap of w . Instead, we use those with higher similarities to w to  X  on-demand build the max heap  X . We first sort these inverted lists based on the similarities of their keywords to w , without loss of generality, suppose Sim ( d 1 ,w ) &gt;...&gt; Sim ( d t ,w ). We first construct the max heap using the lists with the highest similarity values and then include other lists on-demand.

Suppose L i is a list not included in the heap so far. We can derive an upper bound u i on the score of a record from L i (with respect to the query keyword w ) using the largest weight on the list and the string similarity Sim ( d i ,w ). Let r be the top record on the heap, with a score F ( r, w ). If F ( r, w )  X  u i , then this list does not need to be included in the heap, since it cannot have a record with a higher score. Otherwise, this list needs to be included in the heap. Based on this analysis, each time we pop a record from the heap and push a new record r , we compare the score of the new record with the upper bounds of those lists not included in the heap so far. For those lists with an upper bound greater than this score, they need to be included in the heap from now on. Notice that this checking can be done very efficiently by storing the maximal value of these upper bounds, and ordering these lists based on their upper bounds. The pruning power can be even more significant if the keyword w is the partial keyword w m , since many of its similar keywords share the same prefix p on the trie similar to w m . We can compute an upper bound of the record scores from these lists and store the bound on the trie node p . In this way, we can prune the lists more effectively by comparing the value F ( r, w ) with this upper bound stored on the trie, without needing to on-the-fly compute the bound. List Materialization: For fuzzy search, the partial keyword w m has multiple similar prefixes and each similar prefix has multiple similar words. The max heap of w m is built on top of inverted lists of such similar words. Let d be such a similar word. Recall that the value F ( r, w m ,d ) of a record r on the list of a similar word d with respect to w m is based on both W ( d, r )and Sim ( d, w m ). Let v be a materialized node. To use U ( v ) to replace the lists of v  X  X  leaf nodes in the max heap, the following two conditions need to be satisfied:  X  All the leaf nodes of v have the same similarity to w m  X  All the leaf nodes of v are similar to w m , i.e., their
When the conditions are satisfied, the sorting order of the union list U ( v ) is also the order of the scores of the records on the leaf-node lists with respect to w m . A materialized node v that satisfies the two conditions must be a descendant of a similar prefix of partial keyword w m .Wecanprove this by contradiction. Suppose node v is not a descendant of any similar prefix of partial keyword w m .Thennode v and its ancestors are not similar prefixes of w m ,thatis the leaf nodes of v are not similar keywords of w m .Thisis contradicted with the second condition. Thus a materialized node v that satisfies the two conditions must be a descendant of a similar prefix of partial keyword w m .

Suppose p 1 ,p 2 ,...,p n are similar prefixes of w m .We check whether their materialized descendants satisfy the two conditions as follows. Consider a materialized node v which has ancestors among p 1 ,p 2 ,...,p n .Ifnode v has no descendants that are similar prefixes of w m , v must satisfy the two conditions; otherwise suppose p j is a descendant of v that is a similar prefix of w m and has the largest similarity to v among all such descendants. Without loss of generality, let p i be an ancestor of v and has the largest similarity with v among all similar prefixes. If Sim ( v,p j )  X  Sim (( v,p i ), v satisfies the two conditions; otherwise v will not. Thus we can find usable materialized nodes to construct the max heap of w m and use our proposed techniques in Section 3.2.2 to do a cost-based analysis to select high-quality nodes for materialization.
We implemented our proposed techniques and compared with existing methods on three real data sets. (1)  X  X BLP X : It included computer science publication records 4 . (2) X  X RL X  It included 10 million URLs. (3)  X  X nron X : It was an email collection 6 . Table 2 shows details of the data.
For the DBLP data set, we selected 1000 real queries from the logs of our deployed systems and each query contained 1-6 keywords 7 . For the other two data sets, we generated 1000 queries with keywords randomly selected from the set of words used in the collection. We assumed the letters of a query were typed in one by one. For each keystroke, we measured the time of computing the top-k answers to this query. For exact search, we measured the total running time. For fuzzy search, we measured the time in two steps: in step 1 we computed keywords on the trie similar to the query keywords (using the algorithm described in [13]); in step 2 we found the top-k answers using the inverted lists of these similar keywords. Unless otherwise specified, k = 10. We compared our method with state-of-the-art method [13]. We implemented the NRA algorithm described in [6] if we only do sorted access, and the Threshold Algorithm ( X  X A X ) if we can do both sorted access and random access.
All the indexes were built off-line and pre-loaded and full-resident in memory during all querying operations. All experiments were run on a Ubuntu Linux machine with an Intel Core processor (X5450 3.00GHz and 4 GB RAM). Sorted Access Only : We implemented the following methods. (1) BinaryProbe [13]: We considered the inverted lists of the complete query keywords, and the union of the inverted lists for the complete keywords of the partial keyword. We chose the shortest list, and for each of its record IDs, we did binary probings on other lists. (2) NRA(Heap): We implemented the NRA algorithm using the heap-based technique. (3) NRA(Heap+Materialization 8 ): We implemented the NRA algorithm using the heap-and-materialization-based techniques. Figure 9 shows the results on the Enron dataset, which showed that our method improved search efficiency. For instance, for queries with a partial keyword of length 2, NRA(Heap) reduced the query time of BinaryProbe from 128 ms to 10 ms. NRA(Heap+Materialization) further reduced the time to 2 ms. This is because 1) BinaryProbe first computed all results and then ranked them; 2) BinaryProbe on-the-fly computed the union list of the partial keyword. NRA(Heap) used the max heap to generate a sorted partial list and NRA(Heap+Materialization) used materialized lists to save push/pop operations on the heap.
 Sorted Access + Random Access : We implemented the following methods. (1) BinaryProbe (Forward List)[13], we chose the shortest list, and for each of its record IDs, we verified whether the record ID contained other keywords (a) Varying Data Size (b) Varying prefix length Figure 9: Exact search using sorted access (Enron). using the forward list. (2) TA(Forward List+Heap): We implemented the TA algorithm using forward list for random access and max heap for sorted access. (3) TA(Forward List+Heap+Materialization): We implemented the TA algorithm using forward list, max heap, and list materialization. Figure 10 shows the results on the DBLP dataset. We can see that the random-access techniques indeed improved efficiency. Sorted Access Only : We first evaluated the effect of the list-pruning technique. Figure 11 shows the experimental results (including two steps). We can observe that list pruning indeed improved search efficiency. For the Enron dataset with 0.5M records, the method with pruning can reduce the time from 30 ms to 17 ms. The pruning technique was more effective on the Enron dataset than on the other two datasets mainly due to two reasons. First, the Enron dataset had more trie nodes due to its large number of distinct keywords in the emails. Thus a query keyword can have more similar prefixes on the trie. Second, the Enron dataset had fewer records, and the inverted lists were relatively shorter. During the list traversal, the NRA algorithm visited fewer records, and its higher score of the top record from the max heap helped us prune more lists.
 List Materialization : We evaluated the improvement on sorted access using list materialization for fuzzy type-ahead search. We measured the amount of storage space for storing materialized lists as a percentage of the total size of the inverted lists on the trie. We varied this amount, and measured the average time of finding the top-10 answers using the NRA algorithm. Figure 12 shows the results. We can see that list materialization improved the search performance.
We implemented the different methods for list materialization, namely Random , TopDown , BottomUp ,and CostBased as discussed in Section 3.2.2. Figure 13 shows the results. Among the three naive methods, Random gave the best results. The CostBased algorithm outperformed all the naive methods. This is because CostBased selected high-quality nodes for materialization using a cost-based analysis. Sorted Access + Random Access : We implemented the TA algorithm using the two methods for random access and list pruning for sorted access (described in Section 4). Figure 14 shows the scalability results on the three datasets. The two random-access methods scaled well. Method 2 (probing on trie leaf nodes) outperformed Method 1 (probing on forward lists). This is because for the three data sets, there were many prefixes similar to the partial keyword, and Method 1 needed to consider all similar prefixes for each record on forward lists.
There are many studies on autocomplete and phrase prediction for user queries [22, 15, 9, 23, 7]. Google instant search was (a) Varying Data Size (b) Varying prefix length Figure 10:Exact search using random access(DBLP). launched to support type-ahead search. It first suggested relevant queries based on user profiles and query logs and then answered the top queries. Chaudhuri et al. [5] studied how to find similar strings interactively as users type in a query string, using an approach similar to that in [13, 20]. They did not study the case where a query has multiple keywords that need list-intersection operations. The search paradigm studied in this paper is different since we support fuzzy, full-text search as users type in queries.
Bast et al. proposed techniques to support type-ahead search in their CompleteSearch systems [2, 3, 1]. Another study [19] is about type-ahead search on relational data graphs. Ji et al. [13] developed algorithms for fuzzy type-ahead search. Our work extends these studies by developing efficient algorithms to support top-k search.

Khoussainova et al. [14] proposed to suggest relevant SQL snippets as users type in SQL queries. Li et al. [18] studied how to use SQLs to support type-ahead search in databases. Feng et al. [8] studied fuzzy search on XML data. There have been many studies on supporting fuzzy search (e.g., [10, 17, 4, 11, 24, 16]). However these algorithms are inefficient for type-ahead search since they have low pruning power for short strings (partial keywords). The experiments in [13, 5] showed that these approaches are not as efficient as trie-based methods for fuzzy type-ahead search. Theobald et al. [25] proposed a heap-based method for query expansion. They used WordNet words and only utilized sorted access. We consider both sorted access and random access.
In this paper we studied how to efficiently answer top-k queries in type-ahead search. We focused on an index structure with a trie of keywords in a data set and inverted lists of records on the trie leaf nodes. We studied technical challenges when adopting existing top-k algorithms in the literature: how to efficiently support random access and sorted access on inverted lists? We presented two algorithms for supporting random access, and proposed optimization techniques using list pruning and materialization to support sorted access. Our techniques can be easily extended to support large datasets through data partition. For example, we have built a system to search on 20 million MEDLINE publication records using two machines.

