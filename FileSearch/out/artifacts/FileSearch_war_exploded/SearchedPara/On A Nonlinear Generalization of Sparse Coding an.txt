 Yuchen Xie yxie@cise.ufl.edu Qualcomm Technologies, Inc., San Diego, CA 92121 USA Jeffrey Ho jho@cise.ufl.edu Baba Vemuri vemuri@cise.ufl.edu University of Florida, Gainesville, FL 32611 USA Dictionary learning has been widely used in machine learning applications such as classification, recogni-tion, image restoration and others (e.g., (Aharon et al., 2006)). Under this model, each data point is as-sumed to be generated linearly using only a small number of atoms, and this assumption of linear spar-sity is responsible for much of its generalization power and success. The underlying linear process requires that the data points as well as the atoms are vectors in a vector space R d , and the dictionary is learned from the input data using the vector space structure of R d and its metric (typically the L 2 -norm). How-ever, for many applications such as those in direc-tional statistics (e.g., (Mardia &amp; Jupp, 1999)), ma-chine learning (e.g., (Yu &amp; Zhang, 2010)), computer vision (e.g., (Turaga et al., 2008)), and medical im-age analysis, data and features are often presented as points on known Riemannian manifolds, e.g., the space of symmetric positive-definite matrices (Fletcher &amp; Joshi, 2007), hyperspheres for parameterizing square-root densities (Srivastava et al., 2007), Stiefel and Grassmann manifolds (Mardia &amp; Jupp, 1999), etc.. While these manifolds are equipped with metrics, their lack of vector space structures is a significant hin-drance for dictionary learning, and primarily because of this, it is unlikely that the existing dictionary learn-ing methods can be extended to manifold-valued data without serious modifications and injections of new ideas. This paper takes a small step in this direction by proposing a principled extension of the linear sparse dictionary learning in Euclidean space R d to general Riemannian manifolds.
 By the Nash embedding theorem (Nash, 1956), any abstractly-defined Riemannian (data) manifold M can be isometrically embedded in some Euclidean space R , i.e., M can be considered as a Riemannian sub-manifold of R d . It is tempting to circumvent the lack of linear structure by treating points in M as points in the embedding space R d and learning the dictionary in R . Unfortunately, this immediately raises two thorny issues regarding the suitability of this approach. First, for most manifolds, such as Grassmann and Stiefel manifolds, there simply does not exist known canoni-cal embedding into R d (or such embedding is difficult to compute), and although Nash X  X  theorem guarantees the existence of such embedding, its non-uniqueness is a difficult issue to resolve as different embeddeings are expected to produce different results. Second, even in the case when the existing methods can be applied, due to their extrinsic nature (both the vector space struc-ture and metric structure in R d are extrinsic to M ), important intrinsic properties of the data manifold are still very difficult to capture using an extrinsic dictio-nary. For example, a linear combination of atoms in R d does not in general define a proper feature (a point in M ), and the inadequacy can be further illustrated by another simple example: it is possible that two points x,y  X  M have a large geodesic distance separating them but under an embedding i : M  X  R d , i ( x ) ,i ( y ) are near each other in R d . Therefore, sparse coding us-ing dictionary learned in R d is likely to code i ( x ) ,i ( y ) (and hence x,y ) using the same set of atoms with sim-ilar coefficients. This is undesirable for classification and clustering applications that use sparse coding co-efficients as discriminative features, and for dictionary learning to be useful for manifold-valued data, sparse coding must reflect some degree of similarity between the two samples x,y  X  X  as measured by the intrinsic metric of M .
 While the motivation for seeking an extension of the existing dictionary learning framework to the more general nonlinear (manifold) setting has been outlined above, it is by no means obvious how the extension should be correctly formulated. Let x 1 ,  X  X  X  ,x n denote a collection of data points on a Riemannian manifold M . An important goal of dictionary learning on M is to compute a collection of atoms { a 1 ,  X  X  X  ,a m }  X  M , also points in M , such that each data point x i can be generated using only a small number of atoms (spar-sity). In the Euclidean setting, this is usually formu-lated as where D is the matrix with columns composed of the atoms a i , w i the sparse coding coefficients for x i and Sp ( w ) the sparsity-promoting regularizer. One im-mediate technical hurdle that any satisfactory gener-alization must overcome is the generalization of the corresponding sparse coding problem (with a fixed D above), and in particular, the crucial point is the proper generalization of linear combination x i = Dw i for data and atoms belonging to the manifold M that does not support a (global) vector space structure. Once the nonlinear sparse coding has been properly generalized (Section 3), the dictionary learning algo-rithm can then be formulated by formally modifying each summand in Equation 1 so that the sparse coding of a data x i with respect to the atoms { a 1 ,  X  X  X  ,a m } X  M is now obtained by minimizing (log denoting the Riemannian logarithm map (do Carmo, 1992)) with the important affine constraint that P m j =1 w ij = 1, where w i = ( w i 1 ,...,w im ) T . Mathematically, the lack of global vector space structure on M is partially compensated by the local tangent space T x M at each point x , and global information can be extracted from these local tangent spaces using Riemannian geometry operations such as covariant derivatives, exponential and logarithm maps. We remark that this formula-tion is completely coordinate-independent since each log x a direct generalization of its Euclidean counterpart this is the individual summand in Equation 1. Computa-tionally, the resulting optimization problem given by Equation 2 can be effectively minimized. In partic-ular, the gradient of the objective function in some cases admits a closed-form formula, and in general, it can be evaluated numerically to provide the input for gradient-based optimization algorithms on manifolds, e.g., (Edelman et al., 1998).
 Before moving onto the next section, we remark that our context is very different from the context of man-ifold learning that is perhaps better known in the ma-chine learning community. For the latter, the manifold M on which the data reside is not known and the focus is on estimating this unknown M and characterizing its geometry. For us, however, M and its geometry (Riemannian metric) are known, and the goal is not to estimate M from the data but to compute a dictio-nary as a finite subset of points in M . This section presents a brief review of the necessary background material from Riemannian geometry that are needed in the later sections and we refer to (Spi-vak, 1979) for more details. A manifold M of dimen-sion d is a topological space that is locally homeo-morphic to open subsets of the Euclidean space R d With a globally defined differential structure, mani-fold M becomes a differentiable manifold. The tan-gent space at x  X  M , denoted by T x M , is a vector space that contains all the tangent vectors to M at x . A Riemannian metric on M associates to each point x  X  M an inner product  X  X  ,  X  X  x in the tangent space T x M . Let x i ,x j be two points on the mani-fold M . A geodesic  X  : [0 , 1]  X  M is a smooth curve with vanishing covariant derivative of its tangent vec-tor field, and in particular, the Riemannian distance between two points x i ,x j  X  M , dist 2 M ( x i ,x j ), is the infimum of the lengths of all geodesics joining x i ,x j . Let v  X  T x M be a tangent vector at x . There exists a unique geodesic  X  v satisfying  X  v (0) = x with ini-tial tangent vector v , and the Riemannian exponential map (based at x ) is defined as exp x ( v ) =  X  v (1). The inverse of the exponential map exp x is the log map, denoted as log x : M  X  T x M . We remark that in general the domain of exp x is not the entire tangent space T x M and similarly, log x is not defined on all of M . However, for technical reasons, we will assume in the following sections that M is a complete Rieman-nian manifold (Spivak, 1979) such that log x is defined everywhere in M for all x  X  M , and the subtle tech-nical point when exp , log are not defined everywhere will be addressed in a future work. Under this as-sumption, there are two important consequences: 1) the geodesic distance between x i and x j can be com-puted by the formula dist M ( x i ,x j ) = k log x and 2) the squared distance function dist 2 M ( x,  X  ) is a smooth function for all x  X  X  . In linear sparse coding, a collection of m atoms a ,  X  X  X  ,a m , are given that form the columns of the (overcomplete) dictionary matrix D . The sparse cod-ing of a feature vector x  X  R d is determined by the following l 0 -minimization problem: where the function F D : R m  X  R d is defined as F
D ( w ) = Dw . In the proposed nonlinear generaliza-tion of sparse coding to a Riemannian manifold M , the main technical difficulty is the proper interpreta-tion of the function F D ( w ) : R m  X  X  in the manifold setting, where the atoms a 1 ,  X  X  X  ,a m are now points in M and D now denotes the set of atoms since it is no longer possible to stack together the atoms to form a matrix as in the linear case.
 Moving to the more general manifold setting, we have forsaken the vector space structure in R d , and at the same time, we are required to work only with notions that are coordinate independent (do Carmo, 1992). This latter point is related to the fact that on M there does not have a special point such as the origin (zero vector) in R d , and the subtlety of this point seems to have been under-appreciated. In particular, the notion of sparsity that we are accustomed to is very much de-pendent on the choice (location) of the origin. Sparsity, Coordinate Invariance and Affine Constraint To illustrate the above point, let x  X  R d be a sparse vector (according to D ) such that x = a 1 + a 2 + a 3 , i.e., x can be reconstructed using only three atoms in D . Changing the coordinates by trans-lating the origin to a new vector t , each atom a i be-comes a i  X  t , and similarly for x , we have x  X  t . Under this new coordinates with a different origin, x  X  t can no longer be reconstructed using the three (translated) atoms a 1  X  t,a 2  X  t,a 3  X  t , and most likely, in this new coordinates, the same point cannot be reconstructed by a small number of atoms, i.e., it is not a sparse vector with respect to the dictionary D . This is not surprising because linear sparse coding considers each point in R d as a vector whose specification requires a reference point (the origin). However, in nonlinear setting, each point cannot be considered as a vector and therefore, must be considered as a point, and this particular viewpoint is the main source of differences between linear and nonlinear sparse coding.
 Fortunately, we can modify the usual notion spar-sity using affine constraint to yield a coordinate-independent notion of sparsity: a vector is a (affine) sparse vector if it can be written as an affine linear combination of a small number of vectors: x = w 1 a 1 + w 2 a 1 + ... + w s a s , w 1 + ... + w s = 1 . It is immediately clear that that the notion of affine sparsity is a coordinate-independent notion as the lo-cation of the origin is immaterial (thanks to the affine constraint), and it is this notion of affine sparsity that will be generalized. We note that the affine constraint was also used in (Yu et al., 2009; Yu &amp; Zhang, 2010) for a related but different reason. We also remark that the usual exact recovery results (e.g., (Elad, 2010)) that es-tablish the equivalence between the l 0 -problem above and its l 1 -relaxation remain valid for affine sparsity since the extra constraint introduced here is convex. Re-interpreting F D It is natural to require that our proposed definition for F D on a manifold M must reduce to the usual linear combination of atoms if M = R . Furthermore, it is also necessary to require that F
D ( w )  X  M and it is well-defined and computable with certain intrinsic properties of M playing a crucial role. An immediate candidate would be where  X  D,w ( x ) is the function of weighted sum of squared distances to the atoms: While this definition uses the Riemannian geodesic dis-tances, the intrinsic quantities that the learning algo-rithm is supposed to incorporate, it suffers from two main shortcomings. First, since there is no guarantees that the global minimum of  X  D,w must be unique, F
D ( w ) is no longer single-valued but a multi-valued function in general. Second, much more importantly, F
D ( w ) may not exist at all for some w (e.g., when it contains both positive and negative weights w i ). How-ever, if we are willing to accept a multi-valued gener-alization F D ( w ), then the second shortcoming can be significantly remedied by defining where CP ( X  D,w ( x )) denotes the set of critical points of  X  D,w ( x ), points y  X  M with vanishing gradient:  X   X  D,w ( y ) = 0.
 We remark that under the affine constraint w 1 +  X  X  X  + w m = 1, not all w i can be zero simultaneously; there-fore,  X  D,w ( x ) cannot be a constant (zero) function, i.e.,  X  D,w ( x ) 6 = M . Since a global minimum of a smooth function must also be its critical point, this implies that the existence of critical points is less of a problem than the existence of a global minimum. This can be illustrated using the simplest nontrivial Riemannian manifold of positive reals M = R + con-sidered as the space of 1  X  1 positive-definite matrices equipped with its Fisher Information metric (see next section): for x,y  X  X  , For any two atoms a 1 ,a 2  X  X  , the function does not have a global minimum if w 1 w 2 &lt; 0. How-ever, it has one unique critical point a w 1 1 a w 2 In particular, for any number of atoms a 1 ,...,a m and w  X  R m satisfying the affine constraint,  X  D,w ( x ) has one unique critical point Furthermore, we have Proposition 1 If M = R d , then F D ( w ) is single-valued for all w such that w 1 +  X  X  X  + w m = 1 and The proof is straightforward since the unique critical point y is defined by the condition  X   X  D,w ( y ) = 0 and  X 
D,w ( y ) = w 1 k y  X  a 1 k 2 +  X  X  X  + w m k y  X  a m k 2 . Thanks to the affine constraint, we have and F D ( w ) = y .
 While the acceptance of multi-valued F D ( w ) may be a source of discomfort or even annoyance at first, we list the following five important points that strongly suggest the correctness of our generalization: 1. F D ( w ) = y incorporates the geometry of M in 2. The generalization reduces to the correct form for 3. F D ( w ) is effectively computable because any crit-4. The above point also shows the viability of us-5. While F D ( w ) is multi-valued, it is continuous in We believe that in generalizing sparse coding to Rie-mannina manifolds, it is not possible to preserve every desirable property enjoyed by the sparse coding in the Euclidean space. In particular, in exchange for the multi-valued F D ( w ), we have retained enough useful features and properties that will permit us to pursuit dictionary learning on Riemannian manifolds. In this section, we present a dictionary learning al-gorithm based on the nonlinear sparse coding frame-work described above. Given a collection of features x ,...,x n  X  R d , existing dictionary learning meth-ods such as (Olshausen &amp; Field, 1997; Lewicki &amp; Se-jnowski, 2000; Aharon et al., 2006) compute a dictio-nary D  X  R d  X  m with m atoms such that each feature x i can be represented as a sparse linear combination of these atoms x i  X  Dw i , where w i  X  R m . Using l 1 regularization on w i , the learning problem can be suc-cinctly formulated as an optimization problem (Mairal et al., 2010; Yang et al., 2009): where  X  is a regularization parameter. There are several notable recent extensions of this well-known formula, and they include online dictionary learning (Mairal et al., 2010) and dictionary learning using dif-ferent regularization schemes such as group-structured sparsity (Szabo et al., 2011; Huang et al., 2011) and local-coordinate constraints (Wang et al., 2010). For our nonlinear generalization, the main point is to make sense of the data fidelity term k x i  X  Dw i k 2 2 in the man-ifold setting. Formally, this is not difficult because, using previous notations, the data fidelity term can be defined analogously using We remark that F D ( w i ) is a multi-valued function and the above equation should be interpreted as finding an point  X  x i  X  F D ( w i ) such that dist M ( x i ,  X  x i i.e.,  X  x i is a good approximation of x i . However, the dis-tance dist M ( x i ,  X  x i ) is generally difficult to compute, and to circumvent this difficulty, we propose a heuris-tic argument for approximating this distance using a readily computable function. The main idea is to note that  X  x i a critical point of the function  X  D,w i ( x ) and the equation  X   X  D,w i ( X  x i ) = 0 implies that since the LHS is precisely the gradient near  X  x i , the tangent vector (at x i ) P m j =1 w ij log should be close to zero. In particular, this immediately suggests 1 using as a substitute for the distance dist M ( x i ,  X  x i ) 2 This heuristic argument readily leads to the following optimization problem for dictionary learning on M : where W  X  R n  X  m and w ij denotes its ( i,j ) compo-nent. Similar to Euclidean dictionary learning, the optimization problem can be solved using an iterative algorithm that iteratively performs 1. Sparse Coding: fix the dictionary D and opti-2. Codebook Optimization: fix the sparse coeffi-The first step is the regular sparse coding problem, and because the optimization domain is in R n  X  m , many fast algorithms are available. The codebook optimiza-tion step is considerably more challenging for two rea-sons. First, the optimization domain is no longer Eu-clidean but the manifold M . Second, for a typical Rie-mannian manifold M , its Riemannian logarithm map is very difficult to compute. Nevertheless, there are also many Riemannian manifolds that have been ex-tensively studied by differential geometers with known formulas for their exp/log maps, and these results sub-stantially simplify the computational details. The fol-lowing two subsection will present two such examples. For optimization on the manifold M , we use a line search-based algorithm to update the dictionary D , and the main idea is to determine a descent direction v (as a tangent vector at a point x  X  X  ) and perform the search on a geodesic in the direction v . The for-mal similarity between Euclidean and Riemannian line search is straightforward and tansparent, and the main complication in the Riemannian setting is the compu-tation of geodesics. We refer to (Absil et al., 2008) for more algorithm details and convergence analysis. 4.1. Symmetric Positive-Definite Matrices Let P ( d ) denote the space of d  X  d symmetric positive-definite (SPD) matrices. The tangent space T X P ( d ) at every point X  X  P ( d ) can be naturally identified with Sym( d ), the space of d  X  d symmetric matrices. The general linear group GL ( d ) acts transitively on P ( d ) : X  X  X  X  GXG T , where X  X  P ( d ) and G  X  GL ( d ) is a d  X  d invertible matrix. Let Y,Z  X  T M P ( d ) be two tangent vectors at M  X  P ( d ), and define an inner-product in T M P ( d ) using the formula where tr is the matrix trace. This above formula de-fines a Riemannian metric on P ( d ) that is invariant un-der the GL ( d )-action (Pennec et al., 2006; Fletcher &amp; Joshi, 2007), and the corresponding geometry on P ( d ) has been studied extensively by differential geometers (see (Helgason, 2001) and (Terras, 1985)), and in information geometry, this is the Fisher Information metric for P ( d ) considered as the domain for parame-terizing zero-mean normal distributions on R d (Amari &amp; Nagaoka, 2007). In particular, the formulas for com-puting geodesics, Riemannian exponential and log-arithm maps are well-known: The geodesic passing through M  X  P ( d ) in the direction of Y  X  T M P ( d ) is given by the formula where Exp denotes the matrix exponential and G  X  GL ( d ) is a square root of M such that M = GG T . Consequently, the Riemannian exponential map at M which maps Y  X  T M P ( d ) to a point in P ( d ) is given by the formula and given two positive-definite matrices X,M  X  P ( d ), the Riemannian logarithmic map log M : P ( d )  X  T M P ( d ) is given by where Log denotes the matrix logarithm. Finally, the geodesic distance between M and X is given by the formula dist( M,X ) = k log M ( X ) k = The above formulas are useful for specializing Equa-tion 7 to P ( d ): let X 1 ,...,X n  X  P ( d ) denote a collec-tion of d  X  d SPD matrices, and A 1 ,...,A m  X  P ( d ) the m atoms in the dictionary D . We have
X where G i  X  GL ( d ) such that G i G T i = X i . With l regularization, dictionary learning using Equation 7 now takes the following precise form for P ( d ): where L ij = log( G  X  1 i A j G  X  T i ). The resulting optimiza-tion problem can be solved using the method outlined previously. 4.2. Spheres and Square-Root Density We next study dictionary learning on spheres, the most well-known class of closed manifolds. In the context of machine learning and vision applications, spheres can be used to parameterize the square roots of density functions. More specifically, for a probability density function p and its (continuous) square root  X  = we have By expanding  X  using orthonormal basis functions (e.g., spherical harmonics if S is a sphere), the above equation allows us to identify  X  as a point on the unit sphere in a Hilbert space (see e.g., (Srivastava et al., 2007)). In other words, for a collection of density func-tions, we can consider them as a collection of points in some high-dimensional sphere, a finite-dimensional sphere spanned by these points in the unit Hilbertian sphere. Under this identification, the classical Fisher-Rao metric (Rao, 1945) for the density functions p cor-responds exactly to the canonical metric on the sphere, and the differential geometry of the sphere is straight-forward: Given two points  X  i , X  j on a d -dimensional unit sphere S d , the geodesic distance is just the angle between  X  i and  X  j considered as vectors in R d +1 The geodesic started at  X  i in the direction v  X  T  X  i ( S is given by the formula and the exponential and logarithm maps are given by the formulas: where u =  X  j  X  X   X  i , X  j  X   X  i . We remark that in order to ensure that the exponential map is well-defined, we require that | v |  X  [0 , X  ), and similarly, the log map log  X  point of  X  i ( x 6 =  X   X  i ).
 Using these formulas, we can proceed as before to spe-cialize Equation 7 to the sphere S d : Let x 1 ,...,x n de-note a collection of square-root density functions con-sidered as points in a high-dimensional sphere S d , and a ,...,a m  X  S d the atoms in the dictionary D . W is an n  X  m matrix. Using l 1 regularization, Equation 7 takes the following form for S d : min s.t. where u ij = a j  X  X  x i ,a j  X  x i . The resulting optimiza-tion problem can be efficiently solved using the method outlined earlier. This section presents the details of the two classifica-tion experiments used in evaluating the proposed dic-tionary learning and sparse coding algorithms. The main idea of the experiments is to transform each (manifold) data point x i into a feature vector w i  X  R m using its sparse coefficients w i encoded with respect to the trained dictionary. In practice, this approach offers two immediate advantages. First, the sparse feature w i is encoded with respect to a dictionary that is com-puted using the geometry of M , and therefore, it is expected to be more discriminative and hence useful than the data themselves (Yang et al., 2009). Sec-ond, using w i as the discriminative feature allows us to train a classifier in the Euclidean space R m , avoid-ing the more difficult problem of training the classi-fier directly on M . Specifically, let x 1 ,...,x n  X  M denote the n training data. A dictionary (or code-book) D = { a 1 ,...,a m } with m atoms is learned from the training data using the proposed method, and for each x i , we compute its sparse feature w i  X  R m us-ing the learned dictionary D . For classification, we train a linear Support Vector Machine (SVM) using w ,...,w n as the labelled features. During testing, a test feature y  X  X  is first sparse coded using D to ob-tain its sparse feature w , and the classification result is computed by applying the trained SVM classifier to the sparse feature w . The experiments are performed using two public available datasets: Brodatz texture dataset and OASIS brain MRI dataset, and training and testing data are allocated by a random binary par-tition of the available data giving the same number of training and testing data.
 For comparison, we use the following three alternative methods: 1) Geodesic K-nearest neighbor (GKNN), 2) SVM on vectorized data and 3) SVM on fea-tures sparse coded using a dictionary trained by KSVD (Aharon et al., 2006). GKNN is a K -nearest neighbor classifier that uses Riemannian distance on M for determining neighbors, and it is a straight-forward method for solving classification problems on manifolds. In the experiments, K is set to 5. For the second method (SVM), we directly vectorize the man-ifold data x i to form their Euclidean features (without sparse coding)  X  w i and an SVM is trained using these Euclidean features. The third method (SVM+KSVD) applies the popular KSVD method to train a dictio-nary using the Euclidean features  X  w i , and it uses a linear SVM on the sparse features encoded used this dictionary. All SVMs used in the experiments are trained using the LIBSVM package (Chang &amp; Lin, 2011). We remark that the first method (GKNN) uses the distance metric intrinsic to the manifold M with-out sparse feature transforms. The second and third methods are extrinsic in nature and they completely ignore the geometry of M . 5.1. Brodatz Texture Dataset In this experiment, we evaluate the dictionary learn-ing algorithm for SPD matrices using Brodatz texture dataset (Brodatz, 1966). Using a similar experimen-tal setup as in (Sivalingam et al., 2010), we construct 16-texture and 32-texture sets using the images from Brodatz dataset. Some sample textures are shown in Figure 1. Each 256  X  256 texture image is partitioned into 64 non-overlapping blocks of size 32  X  32. In-side each block, we compute a 5  X  5 covariance ma-trix FF &gt; (Tuzel et al., 2006) summing over the block, that each covariance matrix is positive-definite, we use FF &gt; +  X E , where  X  is a small positive constant and E is the identity matrix. For k-class problem, where k=16 or 32, the size of the dictionary D is set to 5 k . The texture classification results are reported in Ta-ble 1. Our method outperforms the three compara-tive methods. Among the three comparative methods, sparse feature transform outperforms the one without (KSVD+SVM vs SVM) and the intrinsic method has advantage over extrinsic ones (GKNN vs. SVM and KSVD+SVM). Not surprisingly, our method utilizing both intrinsic geometry and sparse feature transform outperforms all three comparative methods. 5.2. OASIS Dataset In this experiment, we evaluate the dictionary learn-ing algorithm for square-root densities using the OA-SIS database (Marcus et al., 2007). OASIS dataset contains T1-weighted MR brain images from a cross-sectional population of 416 subjects. Each MRI scan has a resolution of 176  X  208  X  176 voxels. The ages of the subjects range from 18 to 96. We divide the OASIS population into three (age) groups: young sub-jects (40 or younger), middle-aged subjects (between 40 and 60) and old subjects (60 or older), and the classification problem is to classify each MRI image according to its age group. Sample images from the three age groups are shown in Figure 2, and the subtle differences in anatomical structure across different age groups are apparent. The MR images in the OASIS dataset are first aligned (with respect to a template) using the nonrigid group-wise registration method de-scribed in (Joshi et al., 2004). For each image, we obtain a displacement field, and the histogram of the displacement vectors is computed for each image as the feature for classification (Chen et al., 2010). In our experiment, the number of bins in each direction is set to 4, and the resulting 64-dimensional histogram is used as the feature vector for the SVM+KSVD and SVM methods, while the square root of the histogram is used in GKNN and our method. The dictionar-ies (KSVD+SVM and our method) in this experi-ment have 100 atoms. We use five-fold cross valida-tion and report the classification results in Table 2. The pattern among the three comparative methods in the previous experiment is also observed in this ex-periment, confirming the importance of intrinsic ge-ometry and sparse feature transform. We note that all four methods produce good results for the two bi-nary classifications involving Old group, which can be partially explained by the clinical observation that re-gional brain volume and cortical thickness of adults are relatively stable prior to reaching age 60 (Mortamet et al., 2005). For the more challenging problem of clas-sifying young and middle-aged subjects, our method significantly outperforms the other three, demonstrat-ing again the effectiveness of combining intrinsic geom-etry and sparse feature transform for classifying man-ifold data. We have proposed a novel dictionary learning frame-work for manifold-valued data. The proposed dic-tionary learning is based on a novel approach to sparse coding that uses the critical points of functions constructed from the Riemannian distance function. Compared with the existing (Euclidean) sparse cod-ing, the loss of the global linear structure is compen-sated by the local linear structures given by the tan-gent spaces of the manifold. In particular, we have shown that using this generalization, the nonlinear dic-tionary learning for manifold-valued data shares many formal similarities with its Euclidean counterpart, and we have also shown that the latter can be considered as a special case of the former. We have presented two experimental results that validate the proposed method, and the two classification experiments pro-vide a strong support for the viewpoint advocated in this paper that for manifold-valued data, their sparse feature transforms should be formulated in the context of an intrinsic approach that incorporates the geome-try of the manifold.
 Acknowledgement This research was supported by the NIH grant NS066340 to BCV.

