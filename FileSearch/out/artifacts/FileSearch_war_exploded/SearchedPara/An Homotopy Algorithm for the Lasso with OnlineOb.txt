 [2], and machine learning communities. The ` 1 penalty indeed leads to sparse solutions, which is a desirable property to achieve model selection, data compression, or for obtaining interpretable results. In this paper, we focus on the problem of ` 1 -penalized least-square regression commonly R Lasso optimization problem is given by y .
 The ` 1 -regularized least-square problem can be formulated as a convex quadratic problem (QP) with linear equality constraints. The equivalent QP can be solved using standard interior-point methods (IPM) [3] which can handle medium-sized problems. A specialized IPM for large-scale problems was recently introduced in [4]. Homotopy methods have also been applied to the Lasso to compute feature-sign search [12], bound optimization methods [13] and gradient projection algorithms [14]. We propose an algorithm to compute the solution of the Lasso when the training examples ( y Hence we use the previously computed solution as a  X  X arm-start X , which makes our method partic-ularly efficient when the supports of  X  ( n ) and  X  ( n +1) are close.
 In Section 2 we review the optimality conditions of the Lasso, which we use in Section 3 to derive our algorithm. We test in Section 4 our algorithm numerically, and show applications to compres-sive sensing with sequential observations and leave-one-out cross-validation. We also propose an algorithm to automatically select the regularization parameter each time we observe a new data point.  X  = 0 for some i . Hence there is a global minimum at  X  if and only if the subdifferential of the set conditions for the Lasso are given by optimality conditions as Note that if we know the active set and the signs of the coefficients of the solution, then we can compute it in closed form. 3.1 Outline of the algorithm Suppose we have computed the solution  X  ( n ) to the Lasso with n observation and that we are given the augmented problem. We introduce the following optimization problem
Step 1 Vary the regularization parameter from  X  n to  X  n +1 with t = 0 . This amounts to comput-
Step 2 Vary the parameter t from 0 to 1 with  X  =  X  n +1 . We show in Section 3.2 how to compute 3.2 Algorithm derivation and signs of the solution at t = 1 , and therefore can compute the desired solution  X  ( n +1) . We suppose as in Section 2 and without loss of generality that the solution at t = 0 is such that t  X  [0 , t  X  ) , the solution of (2) has the same support and the same sign as  X  (0) . P ROOF . The optimality conditions of (2) are given by ( v 1 , w 2 ( t ) x Solving for  X  1 ( t ) using the first equation gives change signs. We also have we obtain the desired result.
 show how to compute the value of the transition point.
 Let  X  X = set. We use the Sherman-Morrison formula and rewrite (4) as that where  X  e =  X  X 1  X   X  1  X   X  y . We can rewrite (5) as will become 1 in absolute value as soon as to derive the proposed algorithm.
 Algorithm 1 RecLasso: homotopy algorithm for online Lasso 1: Compute the path from  X  ( n ) =  X  (0 ,  X  n ) to  X  (0 ,  X  n +1 ) . 4: Update v 1 ,  X  X 1 and x n +1 , 1 according to the updated active set.
 The initialization amounts to computing the solution of the Lasso when we have only one data point v = sign ( yx ( i 0 ) ) . We have We illustrate our algorithm by showing the solution path when the regularization parameter and t are successively varied with a simple numerical example in Figure 1. 3.3 Complexity point. The size of this matrix is bounded by q = min( n, m ) . As the update to this matrix after a Figure 1: Solution path for both steps of our algorithm. We set n = 5 , m = 5 ,  X  n = . 1 n . All the values of X , y , x n +1 and y n +1 are drawn at random. On the left is the homotopy when the regularization parameter goes from  X  n = . 5 to  X  n +1 = . 6 . There is one transition point as  X  2 becomes inactive. On the right is the piecewise smooth path of  X  ( t ) when t goes from 0 to 1 . We active with their signs unchanged. The three transition points are shown as black dots. to compare it with the complexity of recursive least-square, which corresponds to  X  n = 0 for all n than updating the solution to the non-penalized least-square problem.
 Suppose that we applied Lars directly to the problem with n + 1 observations without using knowl-set is 0 to  X  n +1 . Let k 0 be the number of transition points. The complexity of this approach is number of transition points. 4.1 Compressive sensing Let  X  0  X  R m be an unknown vector that we wish to reconstruct. We observe n linear projections y has been shown in the noiseless case that it is sufficient to use n  X  k log m such measurements. This approach is known as compressive sensing [16][17] and has generated a tremendous amount of Pursuit (BP) problem signal as measurements arrive, as opposed to waiting for a specified number of measurements. Al-gorithms to solve BP with sequential measurements have been proposed in [18][19], and it has been shown that the change in the active set gives a criterion for how many measurements are needed to recover the underlying signal [19].
 solve the Basis Pursuit DeNoising problem instead [20]. Hence, our algorithm is well suited for compressive sensing with sequential and noisy measurements. We compare our proposed algorithm to Lars as applied to the entire dataset each time we receive a new measurement. We also compare our method to coordinate descent [11] with warm start: when receiving a new measurement, we initialize coordinate descent (CD) to the actual solution.
 We sample measurements of a model where m = 100 , the vector  X  0 used to sample the data has 25 The reconstruction error decreases as the number of measurements grows (not plotted). The param-eter that controls the complexity of Lars and RecLasso is the number of transition points. We see when the support of the solution does not change much there are typically less than 5 transition points for RecLasso. We also show in Figure 2 timing comparison for the three algorithms that we have each implemented in Python. We observed that CD requires a lot of iterations to converge to convergence. Our algorithm is consistently faster than Lars and CD with warm-start. where at each iteration we receive a new measurement. On the left is the comparison of the number of transition points for Lars and RecLasso, and on the right is the timing comparison for the three algorithms. The simulation is repeated 100 times and shaded areas represent one standard deviation. 4.2 Selection of the regularization parameter We have supposed until now a pre-determined regularization schedule, an assumption that is not practical. The amount of regularization depends indeed on the variance of the noise present in the data which is not known a priori. It is therefore not obvious how to determine the amount of regularization. We write  X  n = n X  n such that  X  n is the weighting factor between the average mean-squared error and the ` 1 -norm. We propose an algorithm that selects  X  n in a data-driven manner. The problem with n observations is given by We have seen previously that  X  (  X  ) is piecewise linear, and we can therefore compute its gradient We propose the following update rule to select  X  n +1 (  X  1 , 0 to update the regularization parameter before introducing the new observation by varying t from 0 to 1 . We perform the update in the log domain to ensure that  X  n is always positive. We performed simulations using the same experimental setup as in Section 4.1 and using  X  = . 01 . We show in Figure 3 a representative example where  X  converges. We compared this value to the one we would the test set. We obtain a very similar result, and understanding the convergence properties of our proposed update rule for the regularization parameter is the object of current research. 4.3 Leave-one-out cross-validation parameter  X  is tied to the amount of noise in the data which we do not know a priori. A standard use n  X  1 data points to solve the Lasso with regularization parameter ( n  X  1)  X  and then compute point serves as the test set. Hence the best value for  X  is the one that leads to the smallest mean prediction error.
 Our proposed algorithm can be adapted to the case where we wish to update the solution of the Lasso parameter from n X  to ( n  X  1)  X  . We then compute the second homotopy by varying t from 1 to 0 which has the effect of removing the data point that will be used for testing. As the algorithm is very similar to the one we proposed in Section 3.2 we omit the derivation. We sample a model with n = 32 and m = 32 . The vector  X  0 used to generate the data has 8 non-zero elements. We add Figure 4 the histogram of the number of transition points for our algorithm when solving the Lasso with n  X  1 data points (we solve this problem 10  X  n times). Note that in the majority cases there are very few transition points, which makes our approach very efficient in this setting. Figure 3: Evolution of the regularization param-eter when using our proposed update rule.
 We have presented an algorithm to solve ` 1 -penalized least-square regression with online observa-tions. We use the current solution as a  X  X arm-start X  and introduce an optimization problem that al-lows us to compute an homotopy from the current solution to the solution after observing a new data computational advantage as compared to Lars and Coordinate Descent with warm-start for applica-tions such as compressive sensing with sequential observations and leave-one-out cross-validation. We have also proposed an algorithm to automatically select the regularization parameter where each new measurement is used as a test set. Acknowledgments We wish to acknowledge support from NSF grant 0835531, and Guillaume Obozinski and Chris Rozell for fruitful discussions.

