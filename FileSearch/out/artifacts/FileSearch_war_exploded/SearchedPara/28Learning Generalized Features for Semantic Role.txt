 Semantic Role Labeling (SRL) is a kind of shallow semantic parsing task and its goal is to recognize the related phrases and assign a joint structure (WHO did WHAT to WHOM, WHEN, WHERE, WHY, HOW) to each predicate of a sentence [Gildea and Juafsky 2002; Gildea and Palmer 2002]. SRL representations have many potential applications in natural language processing (NLP) and have been shown to bene-fit question and answering [Narayanan and Harabagiu 2004], information extraction [Christensen et al. 2010; Surdeanu et al. 2003], and machine translation [Liu and Gildea 2010; Wu and Fung 2009; Xiong et al. 2012; Zhai et al. 2012, 2013], and so on.
The SRL task is usually treated as a supervised problem. Therefore, a huge set of features are crucial to the performance of SRL systems. The features often used include lexicalization features (e.g., the last word of the argument ) and syntax features (e.g., the syntax tag of the argument ). But these features, especially the lexicalization features, often do not generalize well. For instance, if we want to classify the argument  X  X n winter X  of Table I, the lexical information  X  winter  X  is crucial to discriminate the argument. Unfortunately, there is only the arguments like  X  X n spring X  and  X  X n summer X  in the training set except  X  X n winter. X  In other words, the feature of the last word  X  winter  X  does not occur in the training set, which may cause the classifier to classify  X  X n winter X  incorrectly. The example shows the weak generalization power of the common features. However, intuitively,  X  X n winter X  is very similar to the arguments  X  X n spring X  and  X  X n summer X  in the training set and thus these arguments can activate some identical features. If we can learn these features, then they should have strong generalization powers and will be helpful to classify the arguments.

Previous works have shown that the out-of-vocabulary features like the above can be reduced by using word clusters, as has been done by Koo et al. [2008] and Deschacht and Moens [2009]. They introduced lexical intermediaries by learning word clusters from a large unannotated corpus. They evaluated their method in dependency parsing and found the substantial gains can be obtained. Similarly, Roth and Woodsend [2014] investigated distributed word representations for improving SRL. The advantage of the distributed word representations is that words with similar meanings will have similar representations, which can provide a more robust input signal to the classifier. Both word clusters and distributed representations can be thought as a generalization for the lexicalization features and they both produce positive effects to some extent. But, for the SRL task, an evident truth is that one word has a specific representa-tion or cluster but this does not mean that it should be assigned a specific label. We show some examples to further illustrate. In the examples below, there are the same arguments  X  the child , X  but they are assigned different semantic roles because of dif-ferent syntactic positions. Therefore, the lexicalization representations is insufficient for discriminating an argument. An important factor ignored by these methods is the syntactic information. (a) The child [A0] broke the window. (b) The mother blamed the child [A1] for breaking the window.
 Differing from Koo et al. [2008], Roth and Woodsend [2014], and F  X  urstenau and Lapata [2009] addressed the out-of-vocabulary features with a semi-supervised learn-ing. Their main idea is generating new training instances from an unlabeled corpus by automatically annotating. They first represented labeled and unlabeled sentences as graphs and formalized the search as a graph alignment problem in which graph alignment is scored using a function based on lexical and syntactic similarity. Then, they chose the top K similar sentences for each labeled sentence and automatically an-notated these unlabeled sentences according to the labeled sentences. Although both the lexicalization and the syntactic information are incorporated in their method, the generalization power is also limited. For example, to correctly label  X  X n winter X  of Table I, their method has to add training samples which could offer the feature of the last word  X  winter . X  Moreover, these training samples should be labeled as AM-TMP by automatically annotating. Therefore, these rigorous conditions limit the generalization power of their method.

Differing from the above methods, this article makes efforts to learn generalized fea-tures for SRL. The motivation of our work is that arguments that are lexically similar are likely to represent the same semantic role, and, similarly, arguments occurring in similar syntactic positions are likely to bear the same semantic role. Thus, the key idea of this work is to make a group of similar arguments activate one feature and another group of similar arguments activate another feature. In specific, we use a clustering method to reach the goal. We embed the information of lexicalization and syntax into a feature vector for each argument and then cluster all feature vectors of training set. For an unseen argument (e.g.  X  X n winter X ), it will go to the same cluster as the similar arguments (e.g.  X  X n spring X ) in the training set. Therefore, compared with the common features such as  X  the last word of the argument , X  the learned clusters can be expected to be a kind of generalized feature that should be helpful for SRL.

We have conducted experiments on two standard benchmarks: Chinese PropBank and English PropBank. We evaluate our method under various conditions. The experi-mental results show that compared with the baseline system, the accuracy of our model improves significantly on Chinese PropBank and English PropBank.

The remainder of this article is organized as follows. Section II gives an introduction to the corpus used in our work and the baseline system. The proposed method is presented in Section III. The experiments and results are presented in Section IV. Section V discusses the related works. Finally, the conclusion is presented in Section VI. This section introduces the corpora used in the experiments and the baseline system. Charles Fillmore proposed a theory of meaning called Frame Semantics in which ab-stract actions or common situations are defined as a frame and elements related with a frame are defined as semantic roles. Inspired by his theory, two handcrafted corpora, FrameNet [Baker et al. 1998] and PropBank [Gildea and Palmer 2002], have been constructed. Compared with FrameNet, PropBank provides unified roles for different predicates and has received the most attention from the research community, and thus we adopt it in this work. In the following, we use an example from the official anotation guidelines for English PropBank 1 to give a brief illustration about Propbank. [ Mr. Bush ] A 0 met [ him ] A 1 [ privately ] AM-MNR ,[ in the White House ] AM  X  LOC ,[ on The above sentence describes a frame scene of  X  meet  X  X nwhich X  Mr. Bush,  X  labeled as A0, is the agent,  X  him,  X  labeled as A1, is the patient and other roles provide adjuncts for the scene such as manner (MNR), locative (LOC), temporal (TMP). From the simple example, we can see that the unified annotation of Propbank can provide rich semantic information about a sentence in a concise way. An SRL system usually takes a parse tree (either handcrafted parse trees or auto parse trees generated by an automatic parser) as the input and assigns appropriate semantic roles to the constituents in the parse tree, which are semantic arguments to the predicate in question. Most SRL systems work in stages, which consist of a pruning stage, an argument identification stage, and an argument classification stage. In the pruning stage, the goal is to eliminate the obvious non-argument candidates. The heuristic pruning method in Xue [2008] has been widely accepted by the SRL community and thus we implemented his method in this article. This article mainly focuses on argument classification. Argument classification, which assigns appropriate semantic roles to candidates identified in the argument identification stage, is a natural multi-category classification problem. 2.2.1. Classifier. In the literature, many classification techniques, such as SVM [Pradhan et al. 2005], Perceptron [Carreras et al. 2004], and Maximum Entropy [Xue and Palmer 2003; Xue 2008], have been successfully used to solve the tasks of argu-ment identification and argument classification. In the work, we used a Maximum Entropy classifier with a tunable Gaussian prior. As a discriminative model, the Max-imum Entropy classifier is scalable to handle a large set of features. Moreover, the Maximum Entropy classifier does multi-category classification naturally and thus can be straightforwardly applied in the problems here. Zhang Le X  X  MaxEnt toolkit 2 is used for the specific implementation. 2.2.2. Features. Features are crucial to the performance of SRL systems. Many works [Xue and Palmer 2004; Xue 2008] have studied what features are discriminative for se-mantic role labeling. In their works, the authors experimentally proved that the stages of argument identification and classification require different features. Therefore, we follow their suggestions and utilize two groups of features: one for argument identifica-tion and the other for argument classification. The following lists describe the features in detail.

The features for argument identification are as follows:  X  X redicate lemma  X  X ath from node to predicate  X  X ead word  X  X ead word X  X  part-of-speech  X  X erb class  X  X redicate and Head word combination  X  X redicate and Phrase type combination  X  X erb class and Head word combination  X  X erb class and Phrase type combination
All of the above features are also used in argument classification. In addition, there are some other features:  X  X osition: the relative position of the candidate argument compared with the predicate  X  X ubcat frame: the syntactic rule that expands the parent of the verb  X  X he first and the last word of the candidate  X  X hrase type: the syntactic tag of the candidate argument  X  X ubcat frame+: the frame that consists of the NPs In this section, we will introduce our method in detail. Our main idea is that we first finely construct a feature vector for all samples of training set and then use K-means to cluster all feature vectors. Then the learned clusters are added into the classifier as a new feature. When an unseen sample is to be classified, we first obtain the nearest cluster by computing the distances between it to the centers of all clusters and choose the cluster as the new feature. Semantic Role Labeling is a kind of shallow semantic parsing for natural sentences. For a sentence, all its roles construct the skeleton of its semantics. But how to represent a role of a sentence using a feature vector is an open and hard problem. A feasible way is to use all features in Section II to represent a role but most features such as the first and the last word of the argument are very sparse. If we represent a role in this way, then the length of the feature vector will be up to 100,000. It is also very inefficient to cluster vectors with such a high dimension if no special technique is done. Moreover, the feature vector is too sparse to learn good cluster information. Intuitively, lexicalization and syntax information are two key elements for representing a role. In the following section, we will introduce how to use the two elements to represent the roles of a sentence. 3.1.1. Lexicalization Information. We represent the lexicalization information of an argu-ment using the technique of distributed word representation. Distributed represen-tations represent words in some low-dimensional space, where each dimension might correspond to some syntactic or semantic property of a word. Distributed word repre-sentation recently has caught much attention and has wide applications. These word representations can be used to create novel features [Miller et al. 2004; Koo et al. 2008; Nguyen and Grishman 2014; Roth and Woodsend 2014; Sun et al. 2011; Turian et al. 2010] and can also be treated as model parameters to build representations for higher-level structures in some compositional embedding models [Collobert et al. 2011; Collobert 2011; Hermann et al. 2014; Socher et al. 2012; Socher et al. 2013]. In practical applications, distributed word representation have boosted the performance of many NLP tasks, such as syntax parsing [Collobert 2011; Turian et al. 2010], semantics pars-ing [Hermann et al. 2014; Socher et al. 2012; Socher et al. 2013], question answering [Bordes et al. 2014], and machine translation [Devlin et al. 2014].

While an argument usually consists of multiple words, in our method, we compute additive compositional representations of all words of an argument. This is the sim-plest method of Mitchell and Lapata [2010], where the composed representation is the uniformly weighted sum of each single representation.

Although compositional models aim to learn the higher-level structure representa-tions, composition of distributed representations alone may not capture the important syntactic or semantic patterns. Most research on distributed representations evalu-ate their methods just by conducting word-level experiments such as word similarity, which only show the power of distributed representations at generalizing the lexical-ization information. Therefore, there is still not sufficient evidence to demonstrate its advantages at incorporating syntax or semantics which is crucial to SRL. We take the examples of  X  X he child X  in Section I to further illustrate. The two arguments  X  the child  X  have the same lexicon but their roles differ. In this case, the syntactic information is crucial to correctly classifying the two arguments. If only lexicalization information is considered, the classifier may classify these arguments incorrectly. The example clearly shows the deficiency of using lexicalization information alone at representing an argument. 3.1.2. Syntactic Information. Just as the above states, syntactic information is an impor-tant factor for representing an argument. But what kind of syntax is the most effective to represent an argument of a sentence is still controversial. For example, the syntactic path is a kind of syntactic information to represent an argument, but the syntactic path is too sparse to learn helpful information. For an unseen argument, its syntactic path may not be found in the training set. Moreover, the syntactic path is also redundant, for example, there are many inessential components like  X  X P  X  NP, X  which means that the parent of the argument is also an NP. In the work, we experimentally choose the flat tree to represent the syntactic information. Our method is a variant of Argument Keys in Titov and Klementiev [2012]. In the following, we will illustrate how to represent an argument using a flat tree.

A flat tree, as opposite to a hierarchical tree, reflects the syntactic structure of a sentence in a flat way. Compared with a hierarchical tree, a flat tree provides a more compact way to understand a sentence. The flat tree used in the work slightly differs from the common one and it consists of syntactic tags of the core arguments and the predicate in a sentence. The flatting process of a syntactic tree is as follows: (a) For a given predicate, collect all core arguments and the predicate itself. (b) Arrange the syntactic tag of the above nodes into a line according to the position We take some examples to give a further description. There are two sentences in Figure 1. After flattening, the structure of the left sentence becomes  X  X P-VV-NP X  and the right sentence becomes  X  X P-VV. X  It is worth noting that the adjunct arguments like the argument  X  X uddenly X  in the right sentence are removed from the flat tree. There are three reasons for removing adjunct arguments. First, the core arguments like  X  X  X 0 X  and  X  X 1 X  are key elements to convey the meanings of a sentence while ad-junct arguments are used to reveal the peripheral information such as Time, Location, Manner, and Extent. Second, in most cases, the adjunct arguments like  X  X uddenly X  can be correctly classified just using the lexicalization information. Third, removing the adjunct arguments helps control the dimensionality of syntactic information.
After the whole flat tree is obtained, we incorporate the flat tree with the position of the argument in question to represent the argument. For the arguments  X  X ary X  and  X  X he door X  in the left sentence of Figure 1, we represent them by using  X  X P  X  -VV-NP X  and  X  X P-VV-NP  X  , X  respectively, in which the symbol  X   X   X  indicates the argument in question. We can see that this kind of representation can capture all core elements related to the predicate which suggests some global information can be encoded in the representation. Moreover, compared with other syntax representations like syntactic path, this kind of representation is very concise and compact.

The two sentences of Figure 1 are both in the active voice, but some sentences in the passive voice may also occur. We find that the roles in the sentences with different voices, especially for A0 and A1, have different preference for the position in the flat tree. For instance, there are two sentences below and one is in the passive voice. (a) Mary [A0] opened the door. (b) The door was opened by Mary [A0].

The two sentences have the same meaning, which suggests that the same roles of the two sentences should have the same representation. But for  X  X ary X  in sentence (a) the representation is  X  X P  X  -VV-NP X  and for  X  X ary X  in sentence (b) the representation is  X  X P-VV-NP  X  . X  Therefore, we design two rules to handle the sentences in passive voice. These two rules are designed as follows: (i) The subject 3 of the predicate in the passive voice is put after the predicate. (ii) If a prepositional phrase leaded by  X  X y X  directly follows the predicate, then it will After processing the flat tree of the sentence (b) by using the two rules, we obtain the same representations for the corresponding roles like  X  X ary X  in the two sentences.
The representation constructed above is discrete. We then transform it into a feature vector by using a one-hot representation. We collect the flat representations for all arguments in the training set into a list. The feature vector has the same length as the list and only one dimension is on. It is worth noting that, according to the above illustration, the adjunct arguments do not have the syntax part and thus its syntax part is all off. According to the above description, we can obtain the feature vectors in the lexicaliza-tion part and the syntax part for an argument. The feature vector in the lexicalization part is denoted F lex and the feature vector in the syntax part is denoted F syn . Then we concatenate F lex and F syn into a single vector F to represent this argument. Intuitively, the arguments with the same roles have similarity in the lexicalization and the syntax, which indicates that their feature vectors F should be very similar and close. There-fore, the idea of our method is to cluster all feature vectors of arguments in training set. The clusters learned should generalize better than the common features.
In particular, we use the K-means algorithm to implement clustering. There are no special requirements for the clustering algorithm, so other clustering algorithms can also be adopted. But K-means is a very simple and efficient clustering algorithm, and thus it is adopted here. The K-means algorithm consists of the following four steps: (i) Select K vectors as the initial centroids for K clusters. (ii) Assign each vector to the cluster with the nearest distance. (iii) Compute each cluster X  X  centroids by averaging all its vectors. (iv) Repeat Step (ii) and (iii) until convergence.

In Step (ii), a distance function is required to measure the distance of a vector to the centroid of a cluster. The Euclidean distance is adopted as the distance function. Its definition is as follows: in which F and O denote the vector representations for an argument and a cluster X  X  centroid, and the subscripts lex and syn denote the lexicalization part and the syntax part of F and O , respectively. In Equation (1), the distance of F and O consists of two sub-distances, dis lex and dis syn , which denote the distance of F and O in the lexical-ization and syntax parts, respectively. A hyper-parameter  X  is added to trade off the distances in the lexicalization and syntax.
 We extract the feature vectors of all arguments in the training set and implement K-means clustering on them following the above steps. After K-means clustering is done, we obtain the centroids of K clusters. We number all clusters for distinction. Each argument of training set will be assigned to the nearest cluster and the number of the cluster will be added into the classifier as a new feature. For an argument of test set, we extract its feature vector, and the number of its nearest cluster is the new feature. To evaluate the performance of our approach, we have conducted experiments on two standard benchmarks: Chinese PropBank and English PropBank. The experimental settings for Chinese and English are given as follows: Chinese: We use Chinese Proposition Bank 1.0 as the evaluation corpus in the Chinese SRL system. All data are divided into three groups, whereby 648 files (from chtb_081.fid to chtb_899.fid) are used as the training set and 40 files (from chtb_041.fid to chtb_080.fid) constitute the development set. The test set consists of 72 files (chtb_001.fid to chtb_040.fid and chtb_900.fid to chtb_931.fid). This data setting is the same as in Xue [2008]. We adopt Berkeley Parser 4 to carry out auto parsing for SRL and the parser is retrained on the training set.

English: We choose English Propbank as the evaluation corpus in the English SRL system. According to the traditional partition, the training set consists of the annota-tions in Sections 2 to 21, the development set is Section 24, and the test set is Section 23. This data setting is the same as in Toutanova et al. [2008]. We adopt Charniak Parser 5 to carry out auto parsing for SRL and the parser is retrained on the training set.
 Distributed word representation
To construct vector representations of words, we run the public word2vec 6 tool on a public large corpus the Xinhua portions of English Gigaword (LDC2003T05) and Chinese Gigaword corpus (LDC2003T09), using the CBOW model with a window of 10 and hierarchical softmax. The dimension size of word vectors is set 500.
 Evaluation metric
For fair comparison, we use the widely accepted criteria: F score ( F 1 ). Following Zhang et al. [2004], we conducted the statistical significance tests with 1,000 resampling size and 95% confidence interval. In the following tables, an asterisk is used to indicate that the difference between the proposed approach and the corresponding baseline system was statistically significant.
 Hyper-parameters tuning
There are two hyper-parameters, K and  X  , in our method and we tune them on the development set. We evaluate our method on the test set of Chinese Propbank and English Propbank under various experimental conditions. The conditions are set as follows:
Auto: This condition adopts completely auto modules in all stages of SRL, which includes auto parse trees as the input, the argument identification stage, and the stage that separates the core arguments from the adjunct arguments. Because the flat tree defined in the work consists of the syntax tags of core arguments, it is necessary to judge whether an argument is a core one. Here, we use the argument classification module of the baseline system to automatically perform judgement.

Gold: The difference of this condition with Auto is that it adopts the gold syntax trees as the input. In this condition, we also determine automatically whether an argument is a core one.

Known: The difference of this condition with Gold is that it ignores the argument identification stage. In other words, this condition assumes that all argument candi-dates are known and it focuses on the argument classification stage. In this condition, we also determine automatically whether an argument is a core one.

All-Known: The difference of this condition with Known is that it assumes whether an argument is a core one is also known.
 Table II shows the results of contrast systems and our method in various conditions. The contrast systems contain Baseline , which is described in Subsection 2.2; Roth ,in which we implement the method of Roth and Woodsend (2014) in our Baseline system; and Koo , in which we implement the method of Koo et al. (2008) in our Baseline system. Our method contains three systems:  X  X ex, X   X  X yn, X  and  X  X ex+Syn. X   X  X ex X  denotes the system where the clusters are learned only from lexicalization information,  X  X yn X  denotes the system where the clusters are learned only from syntactic information, and  X  X ex+Syn X  denotes the system where the clusters are learned from both.
From the results of Table II, we can get the following observations:  X  X ompared with Baseline,  X  X oth, X   X  X oo, X  and  X  X ex X  improve the performance of SRL systems because they could learn helpful information from lexicalization information of large unlabelled data. However, the improvement is very limited, only about 0.2% X  0.3% under the Auto condition, which indicates that learning informative features only from lexicalization information is insufficient for discriminating semantic roles.  X  X ompared with other systems, both Chinese and English SRL systems with  X  X ex+Syn X  get better performance, especially in the condition of  X  X ll-Known, X  where the F 1 score improves significantly by 1.5 points and 1.6 points on Chinese PropBank and English PropBank, respectively.
  X  X ur method get better performance in the condition of  X  X ll-Known X  than in other conditions. The reason is that there are some argument identification errors and core argument identification errors in other conditions. These errors can generate cascading errors in constructing the flat tree, which will cause negative effects on the overall performance.

In the  X  X ex+Syn X  system, we further investigate every feature X  X  importance to the performance by removing each feature independently and the experimental condition is  X  X ll-Known X  of the Chinese SRL system. We adopt two strategies to remove features: (I) Do not train a new model and directly remove one feature from the system, and (II) differing from the strategy I, the strategy retrains the classifier after removing one feature. The results are shown in Table III.

In Strategy I, the model has been trained by using all features and when testing we remove one feature at a time to investigate its importance to the whole performance. We find that, after being removed, most features cause performance drop while the proposed feature,  X  X ex+Syn, X  causes the most severe drop, which indicates that the proposed feature is crucial to the trained model. An interesting point is that  X  X irst word X  and  X  X ast word X  do not cause any performance drop but slight improvement. We think the reason is these lexicalization features can be covered by the proposed feature since the lexicalization information has been encoded into the proposed feature.
In Strategy II, after removing one feature, we retrain the classifier. To our surprise, after being removed, almost every feature helped improve the performance. However, after the proposed feature is removed, the performance drops 1.6 points to 92.78%, which indicates that the common features can harm the performance in some cases due to the weak generalization power but the proposed feature is indispensable for argument classification.

The results in Table III also show that more features do not always bring better performance and feature selection is important. Here, we adopt a simple greedy strat-egy for feature selection. Although the best solution may not be found, this strategy possibly can produce a good local optional solution. Each time we choose one of the feature templates and remove it from the system. In one iteration, the one, after which it is removed, the performance is the highest, will be removed. Then we continue the above iteration process until the performance on the development set does not improve. Through the greedy feature selection, the results of Strategy I and Strategy II achieve 94.60% and 95.20%, which are higher than the results of all features. Moreover, we find that the removed features in the greedy selection are lexical features like  X  X redicate, X   X  X irst word, X   X  X ast word, X  and so on, which often generalize poorly. We tune the hyper-parameter K of the K -means algorithm on the development set. The experimental condition is  X  X ll-Known X  of the Chinese SRL system and the hyper-parameter  X  is set to a default value 1.0. Figure 2 shows the results of varying K on the development data. We can see that the proposed method performs better than the baseline system when K is less than 60. The best results are achieved when K is 30. But if K is bigger than 60, the performance of our method is lower than the baseline system, which suggests that a large clustering number causes the drop of our feature X  X  gener-alization power. Thus, the K in the experiments on the Chinese PropBank is set to 30. The K of the experiments on the English PropBank is also tuned by the same method. We tune the hyper-parameter  X  of K -means algorithm on the development set. The ex-perimental condition is  X  X ll-Known X  of Chinese SRL system and the hyper-parameter K is set to 30 according to the above subsection. In our method,  X  is introduced to trade off between the lexicalization information and syntactic information in weighing the similarity of two arguments. When  X  is assigned 0, this means that only the lex-icalization information is utilized to represent an argument and we can see that the performance of our method under this case is slightly better than the baseline system. With the growth of  X  , the syntactic information takes more weight in computing the similarity of two arguments and our method performs much better than the case of  X  equalling to 0, which again demonstrates that the syntactic information is crucial to representing an argument. However, the performance drops slightly if  X  is bigger than 1.0. Thus, the K in the experiments on the Chinese PropBank is set to 1.0. The  X  of the experiments on the English PropBank is also tuned by the same method. K-means clustering is usually thought of as an unstable algorithm and the clustering results are easily influenced by the initial values. Here, we investigate whether the unstable clustering results will influence the performance of the SRL system. The experimental condition is  X  X ll-Known X  of the Chinese SRL system and the English SRL system. Figure 4 shows the results of running 20 times K-means clustering separately. We can see that, although the clustering results are unstable, the results of SRL systems are stable and the gap between the maximum value and the minimum value is lower than 0.1 points. An SRL system often suffers severe performance drops on out-of-domain test data due to the diversity of features of different domains. Intuitively, the common features learned from in-domain data generalize more poorly in out-of-domain test data than in in-domain data. We further investigate whether the proposed method helps relieve the problem of domain adaptation. The results for the out-of-domain experiments are summarized in Table IV. The evaluation corpus utilized is the Brown part of the English PropBank. From the results of Table IV, we can see that the performance of baseline is lower by about 10 points in out-of-domain data than in in-domain. However, after the generalized features are added, the accuracy improves significantly by 1.1 points and 3.6 points under Known and All-Known, respectively. Moreover, the absolute gains are slightly higher than in the in-domain setting. Therefore, the proposed features help relieve the diversity of different domains. The publicly available system Mate 7 ranks at the top in the SRL-only closed challenge of the Conference on Computational Natural Language Learning (CoNLL) 2009 shared tasks and represents the current state of the art for SRL. Thus, we integrate the generalized features into Mate to further evaluate our method. In the experiments, we evaluate our methods on the Chinese and English portions of CoNLL 2009 shared tasks, and the same training, development, and test sets provided in the CoNLL shared tasks are utilized for a fair comparison. We report labelled semantic F 1 score as computed by the official scorer. 8 The results of in-domain tests are shown in Table V. We can see that our method helps improve the F 1 scores of both the Chinese SRL system and the English SRL system, especially in the condition of  X  X ll-Known, X  where the F 1 score improves significantly by 1.8 points and 1.5 points on the Chinese and English portions, respectively. However, the improvement in the condition of  X  X uto X  is very limited, which is caused by the cascading errors in constructing the flat tree. In the future, we will exploit more robust syntactic representation of an argument. The annotated corpuses Framenet and Propbank have greatly boosted the develop-ment of Semantic Role Labeling. Gildea and Jurafsky [2002] first presented an auto SRL system based on a statistical classifier that is trained on a hand-annotated corpora FrameNet. In their pioneering work, they used a gold or autoparsed syntax tree as the input and then extracted various lexical and syntactic features to identify the semantic roles for a given predicate. After Gildea and Jurafsky [2002], there have been a large number of works on automatic semantic role labeling. Based on a basic discriminative model, Punyakanok et al. [2004] constructed an integer linear programming architec-ture in which the dependency relations among arguments are implied in the constraint conditions. Toutanova et al. [2008] proposed a joint model to explore relations of all arguments of the same predicate. In addition, there have been many extensions in machine learning models [Moschitti et al. 2008], feature engineering [Xue and Palmer 2004], and inference procedures [Punyakanok et al. 2004; Toutanova et al. 2008; Yang and Zong 2014; Yang et al. 2015; Yang et al. 2016; Zhuang and Zong 2010a; Zhuang and Zong 2010b].

This article focuses on how to boost the generalization power of an SRL system. There have been some related works. Koo et al. [2008] showed that the out-of-vocabulary fea-tures can be reduced by using word clusters and in dependency parsing substantial gains can be obtained by using their method. Roth and Woodsend [2014] investigated distributed word representations that can provide a more robust input to the classifier. However, the syntactic information is ignored by these methods because the syntactic information is crucial to classifying an argument. F  X  urstenau and Lapata [2009] pro-posed a semi-supervised method in which they automatically annotated most similar unlabeled data for a labeled data and a new classifier is retrained on all these data. But, the generalization power of their method is limited because only some lexical fea-tures can be learned by using their method. Differing from these methods, we induce the generalized feature from similar arguments which are helpful for SRL. This article proposes a simple method to learn generalized features for SRL. In current systems for SRL, people usually define a large set of features to achieve good perfor-mance. However, most of these features, like  X  X irst word X  and  X  X ast word, X  generalize poorly when predicting an unseen argument. To relieve the problem, some researchers have tried adding auto labeled data or using distributed word representation learned from unlabeled data and they reported better performance. But the generalization power of these methods is limited. Intuitively, arguments occurring in similar syntac-tic positions are likely to bear the same semantic role, and, analogously, arguments that are lexically similar are likely to represent the same semantic role. Therefore, differing from previous works, we propose a simple method to induce generalized features from similar arguments. In specific, we embed the lexicalization and syntax information of each argument into a feature vector, and then the K-means algorithm is utilized to make clustering for all feature vectors. This makes the similar arguments go to the same cluster, and thus the learned clusters can be thought as a kind of generalized feature. We evaluate our method on two standard benchmarks: Chinese Propbank and English Propbank. The results demonstrate that our method can significantly improve the SRL performance. Meanwhile, the proposed method can help relieve the problem of domain adaptation.

