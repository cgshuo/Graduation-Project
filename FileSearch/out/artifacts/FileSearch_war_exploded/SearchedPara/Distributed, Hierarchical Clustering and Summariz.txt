 Many data-centric sensor network applications are not only interested in raw sensory read-ings of individual nodes, but are also interested in the patterns, outliers, and summaries of network-wide sensory data. For example, on the left of Fig.1 shows part of the deployment of a sensor network at the Intel Berkeley Lab together with a snapshot of the temperature sensor readings of individual nodes [3]. If we cluster the sensor nodes by their temperature readings and report the data range and average of each cluster, it gives a clear overview of teractive data analysis, this kind of clustering and summary information is useful for data-online clustering and summarization in sensor networks. 
Since both clustering and summarization are computation-intensive tasks over a approach has two major drawbacks: one is timeliness and the other network power ef-ficiency. In consideration of these drawbacks, we take a distributed approach to clus-tering and summarization in sensor networks. 
When considering distributing the clustering and summarization task to individual on each node and the multi-hop communication scheme of sensor networks. As a result, we first treat each sensor node as an initial cluster and then let geographically adjacent clusters gradually merge based on their readings. This bottom-up process is efficient because spatial correla tions exist in real world sensory data so that computa-tion and communication happens among proximate clusters mostly. In addition, summary information are also computed and maintained for the resulting hierarchy of clusters. As such, we call our method Distributed, Hierarchical Clustering and Summarization (DHCS). 
Specifically, we propose a summary structure called Cluster Distribution Feature, or CDF, for each cluster. This feature includes both the data range and the statistical features of a cluster, and can be incrementally maintained along the hierarchy. Subse-quently, we design a dissimilarity metric called expansion , to compute the dissimilar-ity between two clusters based on CDF. Furthermore, the computation complexity of tributed manner. With these and other information, the DHCS algorithm clusters sen-sor nodes and computes summaries for clusters in a distributed, hierarchical manner. 
Research on scalable clustering algorithms has been fruitful [2, 10, 11]. Unfortu-nately, these traditional clustering methods are infeasible in sensor networks, because they are mostly centralized. DHCS is distributed, thus can fully utilize nodes X  compu-tation ability. More, DHCS considers both data similarity and spatial proximity, whereas previous work has only considered da ta similarity. In recent years, there has network topology information. In comparison, DHCS is more data-centric and brings more opportunities for data reduction. Kotodis introduces the idea of snapshot queries towards data-centric sensor networks [7]. However, those representative nodes can only represent their one-hop neighbors, which limits the reduction ratio. then the dissimilarity metric expansion based on CDF, and finally the compact cluster. 2.1 Cluster Distribution Feature Assume N nodes are scattered in a re gion and each node can sense d attributes, such as temperature and light. Assume the value of each attribute can be normalized by some sponds to a d -dimensional normalized data vector. A cluster of N nodes corresponds to N d -dimensional data points, { X i }, where i = 1, 2, ..., N . 
A Cluster Distribution Feature summarizes the sensory data distribution infor-Range and Cluster Feature. Definition 1. Assume that there are N nodes in a cluster, each of which can sense d at-dius of the smallest sphere into which all X i fall. space can be a CDR. In this paper we choose sphere for simplicity and intuitiveness. In the following of this paper, we use CDR and SCDR interchangeably. 
We adopt Cluster Feature (CF) from Birch [10] to describe the statistical features the square sum of the N data points. CF facilitates the computation of the mean, de-viation and other statistical features. More, it can be incrementally maintained [10]. Having the definitions of CDR and CF, we define the Cluster Distribution Feature. Definition 2. Given N nodes in a cluster, the Cluster Distribution Feature ( CDF ) of the cluster is a tuple CDF = &lt;CDR, CF&gt;. Proposition 1. (CDF Additivity) Assume that the CDF of cluster A is CDF A LS C that is formed by merging A and B , is the smallest sphere in the data space that can enclose the CDR of A and the CDR of B . That is,  X  when ( dist + R B )  X  R A , Center C = Center A , R C = R A ; 
Addition of CF: the following additive law [10] holds: These CDF vectors as summaries are not only efficient but also accurate for calculat-ing the dissimilarity metric that we need for making clustering decisions in DHCS. 
Next we define the dissimilarity metric between two clusters, expansion . 2.2 Expansion ss ( N and R B . That is, expansion = R C -max ( R A , R B ). Essentially, expansion describes how much the CDR will expand after merging of two clusters. The smaller the expansion , the more similar the two clusters. 2.3 Compact Cluster Assume that the normalized vector ) ,..., , ( threshold , where any two nodes within a cluster . Assume that hopcount threshold is the maximum toler-able hop count between any two nodes within a cluster. The compact cluster is defined as follows: Given the difference threshold ) ,..., , ( pact cluster C is a non-empty subset of D satisfying the following conditions:  X   X 
Geographical proximity. Two conditions must hold. First, any two nodes within C can communicate with each other, possibly through intermediate nodes; if interme-diate nodes are needed, they must be also in C . Second, the hop count between any two nodes in C should be no greater than hopcount . Different from traditional clustering methods, DHCS clusters nodes based on their current data values as well as their geographical proximity. Adjacency is defined as: nodes.  X  communicate with each other directly (within one hop).  X  node n p in C i and node n q in C j , n p and n q are adjacent . DHCS produces compact clusters and their summaries, CDF vectors, in a bottom-up each cluster, DHCS divides the entire region into several sub-regions and keeps multi-resolution summaries for each sub-region. These summaries are organized in trees. Definition 6. A summary tree of a compact cluster C is a tree structure of the sensor nodes in C satisfying the following condition: the nodes in any sub-tree form a com-the CDF of C i . Given the difference and hopcount thresholds, DHCS produces compact clusters and their summaries in a distributed, bottom-up manner. Initially, each node treats itself as an active cluster. Then, similar adjacent clusters are merged into larger clusters round other as the most similar neighbor. A compact cluster produced through merging must satisfy the thresholds. DHCS terminates when no merging happens any more. The fi-nal clusters, which cannot be merged any more, are called steady clusters. 
In each round, each CH (short for cluster head) represents its cluster to coordinate with other clusters. In order for a CH to route to other CH s in its adjacent clusters ef-ficiently, we maintain the adjacency information of a cluster in its CH and adapt DSR (Dynamic Source Routing) [6] for DHCS. Thus, a CH keeps the CDF and adjacency information of its cluster. In DHCS, there are three kinds of nodes by their states: -ACTIVE nodes: the CHs of the active clusters; -PASSIVE nodes: the nodes that are not CH of any cluster; -STEADY nodes: the CHs of the steady clusters. 
Initially each node will be ACTIVE . ACTIVE nodes become PASSIVE or STEADY along the merging of clusters. DHCS terminates when there is no ACTIVE node. The STEADY nodes represent the final compact clusters. 
Each round has four stages: advertising, inviting, accepting and merging. In the advertising stage, clusters exchange CDFs with neighbors. Then adjacent clusters may try to reach an agreement about merging by shaking hands in inviting and accepting stages. In the merging stage, new clusters are generated. globally unique hardware ID [8], which we use as the node ID . Cluster ID is defined as the ID of its CH . (1) Advertising CDF: Each ACTIVE node n i advertises the CDF of its cluster to the ACTIVE nodes of all its adjacent clusters simultaneously. After exchanging CDFs, each ACTIVE cluster determines the most similar neighbor by computing expansion one whose ID is the largest. If a cluster cannot be merged any more given the thresh-olds, or if a cluster receives no messages, the state of its CH turns into STEADY . (2) Sending invitation: For the purpose of coordination and avoiding redundant invi-tations, we take the following policy when sending invitations. Assume that n i consid-has more nodes than n j ; or (b) the two clusters have the same number of nodes, and n i merging succeed. (3) Accepting an invitation: Suppose n j receives several invitations. Assume that n j tion, adjacency information of n j is piggybacked in the accepting message, and then n j sets its state as PASSIVE . By shaking hands, some pairs of adjacent clusters agree to merge. dren. The CHs of these newly generated clusters finish the cluster merging by the fol-lowing operations: (1) Compute the new CDF by the addition of the original two CDFs; and (2) Maintain the adjacency information of the new cluster. 
After DHCS terminates, nodes X  information about their parents, children and CDF will form several summary trees. Summary trees organize nodes considering data cor-nance mechanism of summary trees for future work. We build a preliminary simulation environment to evaluate the performance of DHCS. First comes the effectiveness, then the efficiency. The following two datasets are used: z The real geographical data set downloaded from Climatic Research Unit [12]. z Synthetic spatially-correlated data: In 4.2, we use the tool in [5] to generate larger nodes. d is the transmission range. A large d allows a large number of adjacent nodes for a node. dt is the difference threshold. Assume that the communication is reliable. 
We define two metrics to evaluate the quality of clusters: The reduction ratio is de-fined as N / N C , where N is the number of nodes and N C is the number of clusters. The cluster head to estimate those of any other nodes in the cluster. We consider only the impact of dt by setting the hopcount threshold to be sufficiently large. (1) Effectiveness of DHCS We vary dt from 1 to 5 and d from 1 to 3. We can see in Fig.2, DHCS achieves the with the increase of dt and d , due to the increase of cluster size. Fig. 3 shows that the average absolute deviation is significantly smaller than dt used, about 1/6 of dt . Addi-average absolute deviation are mainly influenced by data correlation and dt . (2) Efficiency of DHCS The most important metric for sensor networks is power efficiency. Therefore, we evaluate the efficiency of DHCS by the number of messages transmitted. We generate w*w datasets with w varied from 40 to 160, with step 20. For the centralized cluster-ing, the main cost is in collecting data. 
Suppose the sink node resides at the center of the upper side of a w * w square, then the depth of the routing tree is about w/d and on average w/(2d) messages per node for col-
When d is 2 in Fig. 4(a), DHCS is worse than centralized clustering. The main rea-convergence of clustering. Fortunately, this downside is compensated by the scalabil-ity of DHCS. As shown in the figures, DHCS will eventually outperform centr-experiment in larger network limited by our simulation platform. Fig. 4(b) shows that, when d is 1, the cost of DHCS is rather stable, about 30 messages per node. In con-sages when w is 160. The larger the network is, the better DHCS performs. We propose DHCS, a method of distributed, hierarchical clustering and summariza-well as their geographical proximity in a distributed, bottom-up manner. The resulting hierarchy of clusters and their summaries can provide quick overviews about the net-work and facilitate interactive data exploration at multiple resolutions. 
Future work includes extending DHCS to merging more than two clusters at a time, designing maintenance mechanisms for the clustering and summary informa-tion, and evaluating DHCS in real sensor networks. 
