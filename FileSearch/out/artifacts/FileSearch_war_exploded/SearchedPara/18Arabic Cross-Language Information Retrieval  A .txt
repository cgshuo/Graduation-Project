 Today, the majority of the retrieved documents on the Internet are in English, whereas many Web users may be interested in a variety of other languages. Thus, there is a growing need for appropriate tools that are able to recover relevant information in mul-tilingual document collections. With these increasing needs, it is important to develop tools for cross-language information retrieval (CLIR), which is a subfield of informa-tion retrieval (IR) that deals with searching relevant information stored in a language other than that of the user X  X  query [Gey et al. 2005]. However, CLIR tasks have been introduced for many decades. In the early 1970s, experiments for searching documents across languages were performed by Salton [1973]. Different new research on CLIR has been assessed in several competitions, such as TREC, 1 CLEF, 2 and NTCIR, 3 and many papers have been published in their proceedings or in specialized journals. Indeed, each of these competitions focused on a set of languages other than English. For example, TREC has been interested in French, Italian, German, Spanish, Chinese, and Arabic. However, CLEF has expressed interest in French, Italian, German, Spanish, Swedish, Dutch, Finnish, and Russian. NTCIR has focused only in Extreme Orient languages, such as Japanese [Duc et al. 2012], Chinese [Zhang et al. 2005; Gao et al. 2006; Zhou et al. 2008], and Korean [Kwok et al. 2005].

In this article, we review existing approaches in the field of Arabic CLIR and their crucial utility in the recent innovative research directions in the open area of IR. Arabic is one of the six official languages of the United Nations. According to the Cairo Demographic Center, it is the mother tongue of about 300 million people over the world. It is written from the right to the left, and its basic alphabet contains 28 letters. However, adaptations of the Arabic script for further languages removed and added some letters, similar to Persian, Sindhi, Urdu, among others, all of which have extra letters [Habash 2010]. Additionally, this alphabet can be extended to 90 elements by writing additional shapes, marks, and vowels. Arabic words are classified into three main parts of speech: nouns, verbs, and particles. In fact, the majority of the Arabic words are morphologically derived from tri, quad, or pent-literal roots.

Due to its richness, treating Arabic language in IR faces several problems, which are partially solved. Consequently, there is a growing need for Arabic IR tools, which are highly solicited both in research and commercial development. The number of Arabic-speaking Internet users in 2011 was about 86 million, representing about 23.9% of the Arab world population. 4 Actually, Google and Bing are the most popular Web search engines for Arabic-speaking users, and there is an increasing number of Arabic CLIR tools, most of them being noncommercial. Indeed, standards for the evaluation of avail-able tools were introduced by TREC and CLEF starting in 2000. Unfortunately, TREC has not initiated tasks on the Arabic language since the 2001 and 2002 competitions. In addition, no new large publicly available test collections were produced beyond 2002, and the old collection is stale. That is why we discuss, at the end of this article, how a new standard collection for Arabic IR and CLIR can encourage researchers to continue scientific research dedicated to the Arabic language.
 To the best of our knowledge, the existing surveys in the field of CLIR [Gey et al. 2005; Kumar 2012; Iswarya and Radha 2012; Zhou et al. 2012; Farag and N  X  urnberger 2012, 2013] are limited to languages other than Arabic. For example, Kishida [2005] provides a survey of the principal technical issues in CLIR for Asian languages, including cognate matching, translation types, translation techniques, language-specific issues such as tokenization and segmentation, stopword lists, stemming, decompounding, part-of-speech tagging, and techniques for combining multiple language resources, among others. Sujatha and Dhavachelvan [2011] surveyed CLIR for Indian language and foreign languages using the CLEF 2007 dataset. Furthermore, European language CLIR approaches are mainly reviewed and tested in CLEF competitions. Nevertheless, there is a previous review on Arabic IR by Abu El-Khair [2007] in the ARIS&amp;T journal. In fact, Abu El-Khair discussed CLIR only very briefly, and his survey is now outdated. Moreover, Alqudsi et al. [2012] proposed a survey for Arabic machine translation (MT) in which they recapitulated the most important methods used in MT from Arabic to English, and identified and discussed their advantages and disadvantages. Zhou et al. [2012] surveyed techniques used in translating queries and/or documents in the context of CLIR, but they omitted approaches related to the Arabic language as a particular case compared to other languages. Recently, Darwish and Magdy [2013] provided a detailed review for Arabic IR in which specialized retrieval problems are surveyed and discussed, specifically Arabic-English CLIR, Arabic document image retrieval, Arabic social search, Arabic Web search, question answering, image retrieval, and Arabic speech search. Moreover, Zitouni [2014] provides a recent survey dedicated to NLP of Semitic languages such as Arabic, Hebrew, Maltese, and Amharic. However, the Arabic CLIR field is not yet fully reviewed in all of these surveys and reviews. For all of these reasons, in this article we propose an Arabic CLIR review in which we identify and discuss problems related to the Arabic language and their impact in any CLIR task. Consequently, we propose some solutions to the current problems and some perspectives as future work to boost scientific research in this field. In CLIR tasks, either the documents or the queries are translated. Nevertheless, the majority of approaches focus on query translation, as document translation is com-putationally expensive. There are three main approaches to CLIR: dictionary-based methods, MT methods, and parallel or comparable corpora-based methods. Approaches combining these resources try to improve the quality of query translation and then have a good effect on CLIR effectiveness. We briefly recall these approaches in the following discussion. However, Arabic CLIR translation techniques are detailed in Section 3.3 to compare and prove that efforts on diverse sides of Arabic CLIR remain to be incomplete and strictly lacking behind efforts in other languages.
 Dictionary-based methods [Aljlayl and Frieder 2001; Pirkola et al. 2001; Levow 2003; Hedlund et al. 2004] are the general approaches for CLIR when no commercial MT system with a recognized quality is available. Information retrieval systems (IRS) use the so-called bag-of-words architecture, in which the documents and the queries are represented by sets of words (or phrases) during an indexing procedure. Therefore, the queries can be simply translated by replacing every word by its corresponding translation extracted from a bilingual term list or a bilingual dictionary.
Levow [2003] proposed three expansion methods in Chinese-English CLIR tasks. The first one consists of an expansion before translation, the second one after translation, and the third combining both cases. She assessed the impact of pre-and posttranslation in document expansion for cross-language spoken document retrieval in Mandarin Chi-nese. The experiments showed that posttranslation expansion produced better results by enhancing retrieval effectiveness rather than the pretranslation expansion alone or in combination. In fact, the pretranslation expansion approach in the Chinese-English case suffers from two key factors of segmentation and translation in Chinese orthog-raphy that limit the effectiveness of CLIR. However, the posttranslation expansion approach was exempt from this constraint.

Pirkola et al. [2001] presented and discussed CLIR research done at the University of Tampere (UTA). They especially focused on problems related to dictionary-based CLIR in which they proposed the structured query model by Pirkola [1998] and detailed results for four diverse language pairs (Finnish to English, English to Finnish, Swedish to English, German to English) to assess the effectiveness of query structuring. Later, Darwish and Oard [2003] proposed and assessed the probabilistic structured query methods. Their goal was to compute a weight for each query term to estimate both term frequency and document frequency. Experimental results showed significant enhancement in CLIR effectiveness based on optical character recognition (OCR). Wang and Oard [2006] exploited both synonymy and bidirectional translation probabilities to improve CLIR effectiveness. They confirmed that results using similarity-based synonymy combined with bidirectional translation knowledge out-performed some query translation techniques. Hedlund et al. [2004] summarized the fundamental structure and analyzed the experimental results of the dictionary-based UTACLIR system performed at CLEF 2000 X 2002.

Pirkola et al. [2002] proposed a new n -gram X  X ased string matching approach called the targeted s -gram matching technique . In this method, and on the basis of charac-ter contiguity in words, the n -grams are classified into categories. In addition, they provide a comparative study of s -gram and n -gram techniques using adjacent charac-ters as n -grams. In their assessment process, the authors used a target word list of 119,000 Finnish words to match English, German, and Swedish query keys against their Finnish spelling variants and Finnish morphological variants. The experiments showed that the targeted s -gram matching technique achieved better results than the conventional n -gram matching one, both for cross-and monolingual word form vari-ants. The authors also analyzed and discussed the sensitivity of s -gram performance with regard to both the length of the longest common subsequence (LCS) of the variants and the query key length.

Pirkola et al. [2003] exploited query structuring in CLIR in different manners. First, they took advantage of an electronic dictionary to translate English queries into Finnish to search documents from a Finnish newspaper database of 55,000 articles. Then, us-ing the syn-operator of the InQuery retrieval system, queries were structured by mixing the Finnish translation equivalents of the same English query key. They showed that the structured queries performed clearly better than the unstructured ones. Second, the authors assessed and discussed the impact of compound-based structuring using a proximity operator for the translation equivalents of query language compound com-ponents. They confirmed that their approach was not useful in syn-based queries and reduced the retrieval effectiveness. Third, the authors used the Boolean and-operator in a query structuring approach to allocate more weight to keys translated through n -gram matching. This method produced better results when applied to proper names, which are frequently nonidentical spelling variants in diverse languages. This allows n -gram X  X ased translation of out-of-dictionary names.

Toivonen et al. [2005] proposed a two-step fuzzy translation method for cross-lingual spelling variants to overcome some challenges caused by technical terms and proper names in dictionary-based CLIR. Technical terms and proper names in different lan-guages have many spelling variants because they frequently share the same Greek or Latin origin. In the first step, the authors applied transformation rules generated automatically using translation dictionaries, and thus the source words became more similar to their target language equivalents. In the second step, a fuzzy matching model was used to translate the intermediate forms (generated from the first step) into a target language. The experiments were performed using five source languages and English as a target language. They confirmed that the two-step methods performed better than fuzzy matching alone.

MT techniques [Guessoum and Zantout 2004; Alqudsi et al. 2012] aim to translate speech or text from a source natural language to another target language. In the context of CLIR tasks, the main aim is to analyze the context of the query before translating its terms from one language to another. Modern translation approaches are statistical and could be improved through syntactic analysis. However, syntactic and semantic ambiguities (sentences having many parse trees, words having many senses, etc.) decrease MT performance. However, MT-based CLIR approaches try to profit from the advanced research on MT and the accessibility of several commercial products to improve CLIR systems.
 MT systems have shown their effectiveness by supporting query translation in CLIR. For example, the Maryland Interactive Retrieval Advanced Cross-Language Engine MIRACLE system [Oard et al. 2008] provided two query translation techniques: (i) an automatic MT-based translation, in which the tool translates each query terms and then provides the corresponding searched documents, and (ii) a user-assisted query transla-tion, in which the user can reformulate or refine his original query if he is not satisfied with the results. MIRACLE query language is always English, but its design can be easily adapted to other languages. However, MIRACLE provides the opportunities to benefit from the existing English language resources independently of document lan-guage. The tool exploited only a simple bilingual term list, but it can work with any ad-ditional resources. Despite its advantages, MIRACLE suffers from some drawbacks. For example, (i) in the automatic MT-based translation, the user cannot improve the query translation task before starting the search process, (ii) the tool does not provide any contextual information illustrating the translation in the user X  X  language, and (iii) the user should check the meanings of all possible translations to identify the suitable one.
Wu et al. [2008] took advantage of Google Translate 5 to run their query translation task in CLIR. Their results confirmed the effectiveness of MT, supported by a relevance feedback process, in both long and short Web query translation. Moreover, a query picking technique has been employed by Hefny et al. [2011] to estimate the effectiveness of cross-language and multilingual search. Indeed, the authors used Arabic queries to retrieve the English Web using the Bing 6 online translator. This technique showed a statistically significant effect on a multilingual retrieval task by performing some improvements over a monolingual search.

Several works have benefited from MT techniques in their query translation ap-proaches [Wu et al. 2008; Udupa et al. 2009]. For example, Wu et al. [2008] indicated that MT achieved better results than the dictionary-based translation. This conclusion has been confirmed in the case of Web search as a result of the following assumptions [Darwish and Magdy 2013]. First, the MT technique provides a unique translation for each word without any synonym. Consequently, the rankers of Web search engines are not changed. Second, after the translation process, MT tries to optimally reorder words X  translations, for which Web search engines are naturally susceptible.

The Linguistic Data Consortium (LDC) has provided parallel text useful to train MT systems [Habash and Sadat 2006; Sajjad et al. 2013]. In addition, multilingual broad-cast news retrieval [Olive et al. 2011] has required the integration of an MT system in the context of the Global Autonomous Language Exploitation (GALE) 7 program. For more detail, Darwish and Magdy [2013] have discussed this in an Arabic speech search framework.
 In the 1990s, MT-based CLIR approaches suffered from several other limitations. First, MT systems have had serious difficulties in appropriately generating syntactic and semantic analysis, especially for short texts. Second, linguistic analysis is com-putationally expensive, which decreases search performance. In fact, MT was more troubled in CLIR than it is today. Furthermore, the current MT is mostly statistical (SMT), its quality is quite high, and CLIR results are better. This has been noticed in the CLIR evaluation of CLEF around 2008 X 2010. For more detail, we refer the reader to CLEF publications.
 Today, Google and Bing are among the most popular available online translators. Actual queries from query logs are exploited to assess commercial Web search engines such as Google and Bing. For more detail, Lewandowski [2012] provides a study about Web search engine research.
 The corpus-based methods [McNamee and Mayfield 2002b; Yang et al. 2005; Talvensaari 2008; Farag and N  X  urnberger 2008] exploit a set of multilingual term lists extracted from parallel or comparable corpora. Indeed, parallel document collections are composed of identical pairs or a set of documents in diverse languages. Nonethe-less, a comparable corpus contains similar documents in different languages, where the documents X  pairs are conceptually similar.

Yang et al. [2005] proposed an approach mixing categorization-based 8 and corpus-based approaches for CLIR. They also assessed many high-performing corpus-based learning approaches and an MT-based method using Systran . A new corpus (Springer) of medical queries and documents in German and English has been exploited in bilin-gual search tests. Their results have shown that the categorization-based method un-derperformed the corpus-based approaches because of the loss of detailed information from category-level indexing. However, it outperformed the MT-based method. Globally, mixing the output of categorization-based retrieval and corpus-based retrieval provided a significant enhancement in CLIR effectiveness over using each method alone.
Talvensaari [2008] studied and assessed the effects of aligned corpus quality and size in corpus-based CLIR. In fact, the effectiveness of a corpus-based CLIR system mainly depends on three qualities of translation corpora: (i) size of the corpus, (ii) quality of the alignments, and (iii) topical nearness to the translated queries. The varying alignment quality of corpora (alternating from noisier comparable corpora to a clean parallel corpus) and their sizes affect the translation of different domain topics. Experiments utilizing the three qualities confirmed that topical nearness was the most relevant factor, outweighing both other factors. Consequently, when parallel corpora are not available for a given domain, it is recommended to use noisy comparable corpora as complimentary resources.

Corpus-based translation outperformed dictionary-based translation in many re-search works (e.g., Azarbonyad et al. [2013]). Nonetheless, building parallel corpora is quite expensive and complicated, as finding parallel corpora for certain languages is not a trivial task. In addition, quality and coverage are the major challenges with both dictionary-based and corpus-based translation. In fact, CLIR system performance can significantly decrease due to the poor quality of dictionaries and corpora. Words that do not exist in the corpus or in the dictionary and out-of-vocabulary (OOV) words are the main issues with coverage. Some languages still suffer from missing translation for OOV words. Saralegi and de Lacalle [2010] suggested two techniques based on the query translation process, or on both query and document translations, and using bilingual lexicons or parallel corpora to provide translations for such words.
Approaches combining translation resources . As each method of translation has its limits, it is natural to think about their combination to benefit from the advantages of each. A combination of translation methods using different CLIR resources signif-icantly improves CLIR performance. In the following, we identify two cases where resources X  combinations can improve CLIR task:  X  Increase the resource coverage : A resource translation taken separately may not cover all query terms. This scenario often happens with named entities where bilingual dictionaries do not always cover this type of words. In such a case, it is essential to combine the resource translation with others to compensate for missed terms.  X  Refinement of translations and translation probabilities : A resource translation does not cover all words with the same degree of confidence. For some words the trans-lations can be accurate, whereas for others they are inappropriate. The same goes for weight translation. Sometimes different resources offer the same translation but with different probabilities. Thus, a judicious combination of resources allows one to refine the correct translations with reasonable probability.

In a CLIR task, most experiments have combined techniques based on knowledge bases with techniques based on parallel texts [Xu and Weischedel 2005]. In other cases, such as Kadri [2008], a simple combination based on bilingual dictionaries and bilingual terms X  lexicons was used to cover the named entities X  translations. In these studies, a linear combination of various translation resources is generally applied by assigning a confidence weight to each of the translation resources.

Moreover, there are many approaches in the literature that have been proposed for reducing term ambiguity and phrasal translation. Indeed, many authors suggested that combining multiple resources, such as other dictionaries or term lists generated from parallel (or comparable) corpora, can overcome the limits caused by the coverage of the dictionaries. For example, Yahya et al. [2013] proposed and evaluated a CLIR approach based on domain ontology that used Quranic concepts for disambiguating the translation of a given query and enhancing dictionary-based query translation. In their experiments, the authors took benefits from Quranic concepts as a resource for cross-language query translation accompanied by dictionary-based translation and Quranic ontology written in the English and Malay languages as a bilingual parallel corpus. The results showed that the new CLIR technique carries important enhancement in retrieval effectiveness for English document collections but is lacking for Malay document collections.

Research works on cross-language Web search are limited compared to monolin-gual Web retrieval. However, some encouraging results have been performed on cross-language Web retrieval. For example,  X  X ranslation quality X  and the  X  X ase of search X  are the means that Kishida [2008] identified to estimate the performance of cross-language query. The problem of when we should translate query terms or not was tackled by Lee et al. [2010]. Hefny et al. [2011] studied the utility of query logs and examined their impact on cross-language search effectiveness. In fact, the authors confirmed that a given source language query can probably provide better results if it will be supported by its equivalent query identified in the set of query logs. This article is organized as follows. Initially, we focus on Arabic IR specificities and tasks (Section 2), in which we recall Arabic language specificities and formalize the Arabic IR tasks. In Section 3, we detail the Arabic CLIR task. We first summarize the Arabic CLIR tracks and standard resources. Second, the translation techniques in Arabic CLIR with their main approaches are discussed. Third, the evaluation of the existing Arabic CLIR systems and their translation knowledge are recapitalized in this section. In Section 4, we explore open problems and future directions and conclude in Section 5. In this section, we present and discuss Arabic language specificities that may affect CLIR results. In addition, we present Arabic IR tasks in which we expose some problems related to the Arabic language, such as spelling normalization and mapping, transliteration, tokenization, stemming, and stopwords removal.
 The Arabic language has some peculiarities compared to other languages. In the eval-uation process of their research works related to the Arabic language, most authors explain the limits of their Arabic IRS by the morphology of Arabic [Abdelali et al. 2004b], which is far more complex than that of other languages, such as French or English. In other words, this language has a complex derivational morphology based on roots and patterns [Abdelali et al. 2004b]. A root is composed of a set of bare conso-nants (regularly three), which are combined into patterns to build words (i.e., a method of constructing words from a basic root). To convert a root such as  X   X  ( X  X tb X  in the standard transliteration) into a  X  X tem X  such as  X   X  ( X  X tAb, X  which means  X  X ook X ), the Arabic language takes advantage of template-based character insertions. Then, word forms can be built by adjunction of some suffixes or/and prefixes, and regular modifiers can be added to the end or the beginning of a word to finally generate a limited group of compound forms (e.g.,  X   X   X  X ktAbAn, X  which means  X  X nd two books X ). However, the same Arabic token can be generated from many roots. Consequently, disambiguation problems are the main challenges of the token-level analysis.

The morphological complexity of Arabic is a challenging task for Arabic IRS due to the high degree of inflection in this language [Bounhas et al. 2011a]. This complexity is caused by four factors. First, there is no space between words and pronouns. Second, the form of a letter depends on the preceding and the following letters. Third, some glyphs are ambiguous, such as the case for ALIF  X   X   X , as many writers ignore to write  X  X AMZA X (  X  ). Thus,  X  X LIF X  may be interpreted as  X   X   X  X r X   X   X  X r X   X   X . Fourth, the prefixes and the suffixes could be a combination of two, three, or even four grammatical tokens, as in the case of  X   X  ( X  X hey will teach it to you X ) or  X   X  ( X  X ou teach them X ) or  X   X  ( X  X o have them to teach it to you X ).

In the Arabic language, the verb plays a larger inflectional role than in other lan-guages. Consequently, Abdelali et al. [2004b] confirmed that the Arabic language is not an analytic language but rather is inflectional. Moreover, each Arabic word is gener-ated from roots on behalf of lexical and semantic relating elements. Additionally, the order of words within a sentence is possible in many ways in the Arabic language [Attia 2008]. Thus, there are many opportunities to join particles and affix pronouns to Arabic words. Therefore, there are several transformational mechanisms, such as omission, extraposition and fronting, or syntactic replacement (e.g., the use of an agent noun in place of a verb), that can mislead a good interpretation of the sentence.
Despite many advances in Arabic NLP, much work is still required. However, related works have investigated some of these issues, which will be detailed in the following section. In the following, we present and discuss some problems especially related to Arabic IR tasks, such as complex morphology, Arabic spelling normalization and mapping, transliteration, tokenization, stemming, and stop-word removal. 2.2.1. Handling Morphological Complexity. To overcome the problems caused by the com-plex morphology of Arabic in CLIR, Beesley [1996, 1998b] proposed the finite-state transducer of Xerox. Beesley [1998b] also proposed, discussed, and evaluated a two-level finite-state morphology that produced many promising analyses but occasionally was extremely improbable. Three methods were proposed to solve this problem. The first method was based on corpus statistics and/or a contextual method, in which the mainly possible results have been generated following a profound full analysis [Darwish 2002]. In the second method, and to generate stems similar to English or French stem-ming, some rule-based approaches have been used to remove regular suffixes and prefixes from morphology. However, Aljlayl and Frieder [2002] confirmed that this method was like a  X  X ight stemming X  in Arabic, because the obtained  X  X tems X  occasion-ally disagree with results generated by a full linguistic analysis. The third method was a corpus-based clustering method, in which there is a possibility to find words in some other way, such as using a  X  X ight stemmer. X  Then, these words were classi-fied into classes based on their distributional features [De Roeck and Al-Fares 2000]. Khoja [2001] developed her rule-based morphological analyzers, and Buckwalter [2002] proposed and tested his table-based morphological analyzer. 9 Levow et al. [2005] investigated the first two methods to test their dictionary-based CLIR approach. They built four types of terms in their experimentations: (i) lightly stemmed words, using the  X  X l-stem X  rule-based system, in which affixes were easily and automatically exposed (e.g.,  X  X lkitab X  ( ) will be  X  X itab X  ( )); (ii) linguistic stems, in which affixes were removed via the mainly possible analysis using the Sebawai morphological analyzer [Darwish 2002]; (iii) linguistic roots, in which the root was generated (e.g.,  X  X lkitab X  will be  X  X tb X ) using the mainly possible Sebawai analysis; and (iv) token, in which just white spaces were exposed.

Moreover, Darwish et al. [2005] studied the impact of improved morphological anal-ysis, particularly context-sensitive morphology, on Arabic monolingual IR. Their ex-perimental results showed that context-sensitive morphology slightly improved Arabic IR over non X  X ontext-sensitive morphology. Indeed, the IR effectiveness enhancement is almost statistically significant (3%) compared to other statistical morphological an-alyzers and light stemming. Recently, Darwish and Ali [2012] exploited the Wikipedia hypertext to page title pairs to train a model of character-level morphological transfor-mation. Results confirmed that the method outperformed the best statistical stemming technique and yielded statistically significant enhancements in Arabic IR.
Despite the fact that these approaches have been tested as a relative solution to support Arabic IRS, the optimal solution has not been reached by any of these methods. Indexing using roots improves IR recall at the expense of precision. Alternatively, more relevant results have been provided using word and stem-based approaches, but they may miss relevant documents [Abu-Salem et al. 1999]. To overcome some drawbacks of existing approaches, some highly accurate tools are now available, such as MADA [Habash and Rambow 2005; Roth et al. 2008; Habash et al. 2009; Habash et al. 2013], AMIRA [Diab et al. 2007], and MADAMIRA [Pasha et al. 2014], which can stem with nearly 99% accuracy. Such tools should overcome the limits caused by linguistic resources such as corpora, lexicons, and tools detailed in the following. 2.2.2. Spelling-Related Issues. Arabic spelling is challenging. For example, the variation of the Arabic letter YEH (  X  ) to ALEF MAKSURA (  X  ) at the end of a word is too regular due to the similarity between the two letters. Consequently, these variations generally result in an invalid word, which can decrease search effectiveness. In addition, there is another kind of orthography variation related to some glyphs mixing HAMZA or MADDA with ALEF (e.g.,  X  ,  X  ,and  X  ), which are occasionally written as a plain ALEF (  X  ), due to their resemblance in appearance. However, the written word and the intended one are both valid words in most cases. In some cases, it is not easy to identify the intended orthography without using the context.

To solve the problem of some glyphs, two approaches are proposed. First, in the denormalization approach, a normalized form of an Arabic word may be denormalized into multiple different forms depending on context. For example, the normalized word  X  qrAn X  has two denormalized forms:  X  qr|n X  (Qur X  X n) and  X  qrAn X  [El Kholy and Habash 2010; Moussa et al. 2012]. Second, in the normalization approach, all occurrences of the diacritical ALEFs are replaced by the plain ALEF. This approach is easier than the denormalization approach, but it can increase ambiguity. 2.2.3. Transliteration. In Arabic IR, transliteration is a preprocessing step, but it involves phonetically mapping from one character set. Using the most common Buckwalter [2002] representation scheme is more of an encoding scheme than a transliteration one that has any bearing on IR. In fact, Buckwalter [2002] proposed an Arabic transliteration system, which surveys the standard encoding varieties made for representing Arabic characters for computers. This system provides some advantages, such as (i) it was written in ASCII characters, and (ii) it provided a strict transliteration based on a  X  X ne-to-one X  technique. Nevertheless, it is not intuitively easy to read Buck-walter transliteration. Consequently, this tool has mainly been used in many Arabic NLP and IR works and in many linguistic resources proposed by the LDC.

Users who cannot read the Arabic script may also benefit from the transliteration scheme proposed by Habash et al. [2007], in which the authors took advantage of the definition of the terms transliteration and transcription proposed by Beesley [1998a]. This particular definition of transliteration is occasionally called an orthographical transliteration or a strict transliteration [Beesley 1998a]. However, transcription means an orthography that symbolizes the morphophonology of a language. To overcome the limits of their transliteration, Habash et al. [2007] proposed to extend the Buckwalter transliteration scheme by including non-ASCII characters of which the pronunciation is easier to remember. In fact, opposed to ASCII characters, non-ASCII ones are less ac-cessible through standard American keyboards, which is a strong compromise between ease of readability and typing/coding simplicity.

To the best of our knowledge, there is no standard transliteration scheme to follow in the natural language processing research community working on Arabic. Additionally, it is not yet clear how this step may affect IR results. 2.2.4. Tokenization: n-Gram X  X ased Approaches. In IR tasks, an n -gram can be defined as follows. It is a neighboring sequence of n items extracted from a given sequence of text or speech. According to the application, the items can be letters, syllables, phonemes, words, or base pairs. In general, the n -grams are gathered from a text or speech corpus. In the following, we detail some n -gram works related to Arabic IR tasks, such as (i) search of Arabic texts, (ii) transliteration, and (iii) root identification.
In TREC-2001, Mayfield et al. [2001] exploited and discussed the character n -grams for Arabic search. They concluded that the character n -grams of length 4 were the most efficient. Then, to solve some morphology problems related to Arabic text, Xu et al. [2002] tested and discussed approaches related to infix morphology, specifically misspelled words, and broken plurals. The n -gram X  X ased technique is widely used to assess several tokenization methods for searching scanned Arabic texts. For example, Darwish and Oard [2002b] suggested that character n -grams of lengths 3 or 4 were the most fruitful methods.

In TREC-2002, McNamee et al. [2002] took advantage of the fruitful performance of n -gram X  X ased retrieval for Arabic. They investigated a hybrid approach using to-kenization methods in the same term space, in which they included space-delimited words, and they exploited n -grams of more than one length. In their experiments, the authors were obliged to replace or eliminate certain Arabic characters or symbols that did not exist in the set of 28 available letters. For example, they eliminated HAMZA (  X  ), MADDA (  X  ), and any remaining Arabic letters. They also mapped ALEF MAKSURA (  X  )toYEH(  X  ) and TEH MARBUTA (  X  )toTEH(  X  ).

Alternatively, McNamee and Mayfield [2002a] mentioned that using multiple-length n -grams in Asian language retrieval can be rather successful, with improvements of up to 10%, as measured by mean average precision (MAP), over using single-length n -grams. McNamee et al. [2002] started by generating many indexes to access and compare different approaches of tokenization. They built the set of all 3-grams, 4-grams, and 5-grams that can be identified from a given input series. However, to represent a text in a single term space, the authors proposed some official runs taking into account both words and 3-, 4-, and 5-grams. Consequently, the index data structures tripled the disk space consumed compared to the use of 4-grams alone.
 McNamee et al. [2002] chose their approach in TREC-2001 for experimentation in TREC-2002. Indeed, in TREC-2001, the authors tested and evaluated many knowledge 25 available Arabic topics. A pseudorelevance feedback process has been also exploited. They concluded that when a hybrid indexing method is used, the plain 4-grams did quite well, but slightly superior effectiveness was obtained.

Rammel et al. [2011] exploited and assessed an indexing method and retrieval sys-tems using n -grams applied to Arabic legal language used in official Lebanese govern-ment journal documents. Indeed, the authors exploited n -grams as a representation technique based on words and characters. They confirmed that indexing Arabic docu-ments using trigrams achieved better results than using the vector space model with three similarity measures: TF  X  IDF weighting, Dice X  X  coefficient, and the cosine coeffi-cient. Unfortunately, the n -grams methods are inadequate for indexing and retrieving legal Arabic documents. However, the linguistic approach, using a legal thesaurus or ontology for juridical language, seems to be effective in overcoming this limit.
Therefore, n -gram models have many advantages. For example, one advantage is c omprehensiveness : the terminology of character tokens is generally known in advance and is much smaller than any word terminology. For that reason, the problem of sparse of data is much less grave in character n -gram models. A second advantage is language autonomy and simplicity :the n -gram X  X ased approach may be applied to any language, even for nontextual documents such as music and images. A third advantage is ro-bustness : character-level n -gram models are quite insensible to orthography errors and deviations, principally in contrast to word characteristics.

Moreover, AbdulJaleel and Larkey [2003] proposed and tested a statistical translit-eration technique for English-Arabic CLIR called the selected n-gram model . This technique requires no linguistic knowledge or heuristics of either language. Indeed, the training process passed by two steps: (i) select the suitable n -gram fragments to be added to the unigram record for the source language, and (ii) select the trans-lation model over this record. The authors took advantage of a test set of named entities extracted from the Arabic AFP corpus to assess the  X  X impler handcrafted model X  and the  X  X tatistically trained model X . Results revealed that these two models outperformed two online translation sources ( Tarjim and Al-Misbar 10 ). They also con-firmed the effectiveness of these tools at the TREC-2002 CLIR competition [Larkey et al. 2002a].
 Mustafa [2005] studied the character contiguity in n -gram X  X ased word matching. Indeed, the author proposed, evaluated, and compared two n -gram approaches for Arabic root-driven string searching. The first one was the contiguous n -gram, and the second one was the hybrid n -grams mixing contiguous and noncontiguous characters. In his assessment, Mustafa collected about 160KB of Arabic text (with no diacritics) from a larger set of documents of social sciences and humanities and enclosed 24,750 textual words (including redundant words), with an average word size of 5.56 letters. Results using stemming showed that the hybrid approach outperformed the contiguous one. Hmeidi et al. [2010] proposed and tested an approach for the extraction of roots from Arabic words using bigrams. They exploited two similarity measures: Dice X  X  measure of similarity and the Manhattan distance measure. In their evaluation and testing, the authors took advantage of a corpus of 242 abstracts from the proceedings of the Saudi Arabian national computer conferences and the Holy Quran (containing about 78,000 words). The first corpus includes some modern Arabic words and some words borrowed from foreign languages, as well as the original Arabic words. The Holy Quran contains mainly traditional Arabic words. The experiments showed that an approach mixing Dice X  X  measure with n -grams outperformed the Manhattan distance measure. 2.2.5. Stemming. The Arabic language is challenging to stem due to many factors detailed throughout Section 2. To improve the analysis process for IR tasks, it is nec-essary to choose between stems or roots as the suitable indexing unit [Larkey and Connell 2001; Larkey et al. 2002b]. In fact, significant research efforts in developing morphological analysis and stemming tools for Arabic have been performed. According to related works, there are four different approaches to Arabic stemming: (i) manually built dictionaries; (ii) light stemming, which eliminates suffixes and prefixes; (iii) mor-phological analyzers, which try to identify roots; and (iv) rule-based stemmers. More detail regarding each approach can be found in Oard et al. [2000].

Manually built dictionaries . Manually built dictionaries of terms with stemming information have been used by many researchers. For example, Buckwalter [2002] built a table-based stemmer, in which he defined tables of authorized groupings using the lexicons of Arabic suffixes, prefixes, and stems. In TREC-2001, Xu et al. [2001b] used this stemmer within the BBN group.

Light stemmers. Light stemmers are based on stripping off a small set of prefixes and/or suffixes without needing to find roots, recognize patterns, or deal with in-fixes. Light stemming can fail to conflate other forms that should go together. It can also suitably conflate several alternatives of terms into wide stem groups. For in-stance, some past tense verbs do not get conflated with their present tense forms, and some plurals forms for adjectives and nouns do not get conflated with their singular forms.

Some authors used light stemming techniques, such as De Roeck and Al-Fares [2000] and Aljlayl and Frieder [2002]. In fact, three Arabic light stemmers were tested in TREC-2001. The first one, denoted  X  X l-Stem X , was proposed by the authors at the University of Maryland and modified by Leah Larkey from the University of Mas-sachusetts (UMass) [Darwish et al. 2001; Darwish and Oard 2002b]. The second one, denoted  X  X Mass Stemmer, X  was detailed in Larkey and Connell [2001]. The third one was an extended version of the UMass Stemmer, in which two further prefixes, identi-fied through failure analysis using the TREC-2001 topics, were handled. The authors confirmed no reliable differences between results of these three stemmers. The light stemming approach has mainly been used for recognizing the stems of Arabic words. In fact, since the TREC-2002 CLIR track, several works have been proposed and tested in light stemming. That is why Al-Nashashibi et al. [2010] proposed a comparative study of the existing light stemmers of the Arabic language in terms of theoretical principles, algorithms, and affixes lists and their impact on IR effectiveness. Experiments have shown that the performance of the stemmers Aljlayl-1, Aljlayl-2, and Aljlayl-3 could be influenced by different affixes lists. However, Al-Stem and Aljlayl-2 stemmers provided diverse results, because they use different stemming algorithms for removing affixes, even with similar affixes lists. In the case of expansion experiments, the Aljlayl-3 stemmer returned better results than the other stemmers. However, in the case of nonexpanded results, the light10 stemmer produced better results than the other stemmers.

Morphological analyzers. Some morphological analyzers try to find the root or any number of possible roots for each word. However, Beesley [1996] mentioned that other morphological analyzers provided a more exhaustive grammatical analysis of the word. In fact, many morphological analyzers were developed for Arabic, such as those of Beesley [1996], Khoja and Garside [1999], Khoja et al. [2001], and Darwish et al. [2001], but many of them have not been tested in IR, which would help to evaluate their practical effectiveness. Moreover, Darwish proposed an Arabic morphological analyzer that was used by some participants of TREC-2001 [Darwish et al. 2001; Mayfield et al. 2001]. Larkey et al. [2002b] benefited from a simple morphological analyzer developed by Khoja and Garside [1999] to improve stemming for Arabic IR.

Kadri and Nie [2006a] proposed a stemming approach that allows finding a stem of an Arabic word. Given that Arabic words are mainly presented as a chain of antefix, prefix, core, suffix, and postfix, this linguistic-based method is based on this composition of Arabic words. In fact, this approach was analogous to that proposed by Larkey and Connell [2001], Chen and Gey [2002], and Darwish and Oard [2002a, 2002b]. In fact, the authors started from the idea that the stems encode the basic semantics of words. This approach was compared to a light stemming method. These two stemming techniques were assessed using TREC-2001 and -2002 collections, and the experiments confirmed that the new stemming approach performed better than light stemming. Despite its advantages, this technique has several disadvantages. A first disadvantage is error related to cutting some affixes, which should not be cut. In fact, this method should be improved at the level of corpus statistics, which would help to surmount this type of indistinctness. A second disadvantage is the problem of ambiguity related to the difficulty of choosing the suitable stem if there are several possible candidates for a given word.

Rule-based stemmers . Some efforts have been dedicated to overcome some of Khoja and Larkey stemmer drawbacks. For example, Eldesouki et al. [2009] proposed, tested and compared five techniques for extracting Arabic roots. Indeed, the fifth technique is a rule-based approach, whereas the remaining four are based on a positional letter-ranking method where such a technique is investigated along with a modification and two proposed variants. To correct irregular words, the authors took advantage of a dedicated algorithm applied for all techniques. In their assessment, they used an in-house text collection to compute the accuracy of these techniques by comparing the generated roots to a predefined list of roots built from this collection. Due to the correc-tion algorithm, the positional letter ranking X  X ased algorithms were enhanced by 7% to 10%, whereas the rule-based technique was improved by almost 14%. The adjusted positional letter ranking technique was slightly higher than the rule-based one, but it outperformed all other proposed techniques in terms of accuracy. Nevertheless, when the correction algorithm supported the rule-based technique, its accuracy outperformed the 10 other algorithms.

Ababneh et al. [2012] proposed a rule-based light stemmer in which they took ad-vantage of the Larkey stemmer by supporting it with a new algorithm that was useful for deciding if a certain chain of characters is part of the original word or not. This al-gorithm uses a set of rules and mainly contributes to finding solutions to some cases of ambiguity. Moreover, this stemmer provides the possibility of grouping words sharing the same sense in a regular form thanks to its ability to manipulate the majority of broken plural forms and reducing them to their singular form.

Discussion . According to the preceding state-of-the-art tools, several linguistic and light stemmers have been proposed and tested for the Arabic language, but they still suffer from many drawbacks. For example, the Khoja stemmer has been used to extract the root of the word, whereas the Larkey stemmer has been used to just truncate the words X  affixes. However, the Khoja stemmer still suffers from many weaknesses and problems. First, the stemmer sometimes fails to eliminate the affixes of the word, and consequently it fails to select the suitable root. Second, the stemmer provides, in several cases, a new word that is the root that is very general; therefore, it will decrease retrieval effectiveness. This problem is related to the fact that root extraction stemmers are not convenient with regard to the Arabic language from an IR system point of view. Third, to guarantee that recently revealed words are suitably stemmed, the root dictionary involves a continue updating. However, the Larkey stemmer is more practical from an IR system point of view, taking into consideration the Arabic language. Despite this advantage, the stemmer still suffers from some limits: (i) in several cases, the stemmer fails to assemble words that have the same sense in one compacted form, because it is not susceptible to the plural form of irregular nouns in Arabic language, and (ii) the stemmer fails in many cases when it truncates a chain of characters that matches one of the affixes that is actually part of the original word, and consequently this will provide a new word.

Many works showed that there is a little difference between using stems and roots when they assessed their influence to IR effectiveness. For example, no consistent comparison between roots and stems were identified in TREC competitions [Darwish et al. 2001]. Moreover, when they combined roots and stems, Larkey et al. [2002b] showed a slight increase in Arabic IR effectiveness.

Abu-Salem [2004] proposed and discussed a comparative study of stemming and n -gram matching for term conflation in Arabic text. First, the author achieved good results when testing a mixed stemming approach in which a word, a stem, or a root was used for a query term based on the highest average inverse document frequency ( idf ) value, using more documents and queries. In fact, Abu-Salem confirmed that the root with a weighting approach performed better results. In addition, the mixed stemming enhanced binary weighting search results in all cases, although it did not increase the effectiveness greater than the weighted stems or roots. The n -gram matching has mainly been exploited for term conflation, such as in the case of retrieving from the Web without any knowledge about whether query terms are stored in documents, as a compound or in the form of several words, as plural or singular, in new or old spelling, or probably in incorrect orthography.

In this comparative study of stemming and n -gram matching, Abu-Salem [2004] sug-gested that (i) all proposed n -gram methods performed better results than the word, stem, and root index methods for binary weighting scheme; (ii) the  X  X tem-NGram-Stem X  search performed better results than the digrams method 11 when both the documents and query words are not stemmed; (iii) the  X  X tem-NGram-Stem X  search approach performed better results than the trigrams method (Stem-NGram-Word) when the documents words are not stemmed and query words are stemmed; and (iv) the  X  X tem-NGram-Word X  method performed better results than the digrams method when both the documents and query words are not stemmed.
 Recently, Algarni et al. [2014] proposed and assessed a root stemmer for the Modern Standard Arabic (MAS) language to enhance Arabic IR effectiveness. The so-called simple Arabic stemmer (SAS) took advantage of the internal morphological structure of Quranic words, such as roots, patterns, and affixes, to accomplish the derivation of Arabic morphological rules. Then, a dedicated lexicon was generated to enclose the root of the majority of MSA vocabulary. The new SAS performed with higher precision compared to Khoja and Sebawai root stemmers using the TREC collection. 2.2.6. Stopwords Removal. Stopwords are characterized by their short meaning and do not specify topic matter even though they have a syntactic function. In fact, sev-eral Arabic NLP applications need an effective stopword removal algorithm, such as question answering systems, stemming and stem weighting, spelling normalization, and IRS. The IR task has mainly been influenced by such words in many ways. On the one hand, these words appear with a very high frequency and tend to decrease the effect of frequency differences between less common words, thereby affecting the weighting process. As well, this process is also related to the document length, which is decreased with the elimination of stopwords. Consequently, the search effectiveness will decrease if the stopwords removal task is not done well. Indeed, the presence of stopwords may result in a large amount of unproductive processing due to their rela-tively empty semantic content and frequency of occurrence. Moreover, the performance of the indexing process in IR is very sensitive to stopwords elimination, because 30% to 50% of the tokens in large text corpora can represent stopwords.

The process of stopwords removal from text requires building a list of such words, called a stoplist . There are two kinds of stoplists: domain dependent and domain in-dependent. In the domain-dependent method, stoplists are generated using corpus statistics to find those words occurring with greatest frequency or by specifying certain syntactic classes for inclusion. A hybrid approach mixing the corpus statistics with certain syntactic classes has also been proposed to generate stoplists.

Today, stoplist approaches need to be improved for several languages, including Ara-bic. Existing stoplists for all languages are available at the IR Multilingual Resources at UniNE 12 site. In fact, Arabic stopwords can accept prefixes and suffixes. There-fore, stemming is a required step in the generation of these stopwords. Certain Arabic stemmers, such as Sebawai, have stopword lists and some stemming incorporations in search engines (e.g., Lemur 13 and Solr). Further sources for Arabic stopwords are (i) the Arabic stopword list from UniNE, (ii) the dialectal stopword list [Darwish et al. 2012], (iii) the Wael Salloum 14 stopword list, (iv) the Al-Mostabaadat 15 Arabic stopword list, and (v) the Anton Balucha 16 stopword list. Existing stopword lists are not exhaustive, as they are built using specific grammatical classes of Arabic, and they are missing many word categories, such as pronouns and determiners. In the following, we detail and discuss the general and the corpus-based stoplist approaches related to the Arabic language.

General stoplists. The assessments of Arabic NLP and IR approaches generally de-pend on the general Arabic stoplists used. For example, Khoja [2001] built a sophisti-cated stoplist with 168 words (part of the Lemur Toolkit) to assess her Arabic stemmer. This stoplist is also exploited by Larkey and Connell [2001] and Larkey et al. [2002a, 2002b]. Today, Arabic language research needs a standard general stoplist taking into consideration the specific characteristics and structure of this language. This stoplist should contain all of the words evaluated as stopwords to guarantee its exhaustiveness. The diverse syntactic classes of Arabic should be used to generate this stoplist in a sys-tematic approach. Moreover, several word categories should be used in building this general stoplist, such as pronouns, verbal pronouns, relative pronouns, interrogative pronouns, conditional pronouns, prepositions, adverbs, transformers (verbs, letters), and referral name determiners [Abdul-Rauf 1996].
Corpus-based stoplists. In this case, stoplists are built from the test corpus. In fact, corpus statistics are also exploited to make decisions about the number of words in this stoplist, called the cut-off point . The suitable number used as a cut-off point is arbitrarily fixed following an examination of the corpus word frequency statistics. The final stoplist is created within the words appearing more than this number of times in the corpus.

In the TREC-2002 competition, Savoy and Rasolofo [2002] used the same technique as used in Savoy [2002] to build their corpus-based stoplist. This technique consists of using a document collection to extract the most frequently occurring words to generate a preliminary list. Then, cardinals, some nouns, and adjectives are removed from this list because they are useful in some IR queries. Instead, other words, such as prepositions, conjunctions, possessive pronouns, and personal pronouns, are added to this list because they are not suitably  X  X nformation-bearing X  for use in queries.
The TREC-2002 Arabic stoplist was generated from the LDC corpus and contains 347 words, but it suffers from many limits. First, it contained some words preceded by the letter WAW (  X  ) in 17 words, including 11 duplicates. This letter may cause several problems related to ambiguity, as it is used in many Arabic words and can precede all words in the language without exception. This ambiguity can be solved thanks to the use of an adequate Arabic stemmer. Second, many other single letters, along with the WAW (  X  ), are removed, such as HAMZA (  X  ), ALIF (  X  ,  X  ), BA (  X  ), and HA (  X  ), because these letters can be written as disconnect characters even when they form part of a word. Consequently, the meaning of the word will be wrong if this kind of character is eliminated. Third, more than 30 words in the list appeared in some of the queries, but they are evaluated as stopwords due to their higher frequency in the corpus.
This domain-dependent list improved Arabic IR effectiveness using the LDC corpus in TREC-2002, but there is no guarantee that it will remain effective for other collec-tions. Stopwords generated from a corpus can be a relevant query X  X  word if we use other types of corpus. This is the main challenge of the corpus-based stoplist approaches.
Many existing stopword elimination approaches used a dictionary containing a list of stopwords. However, these approaches are computationally expensive in terms of the complexity and the required storage space. To overcome some of these limits, Al-Shalabi et al. [2004] benefited from a finite-state machine (FSM) to propose and assess an effective stopword removal algorithm for the Arabic language. They built a stoplist containing more than 1,000 words extracted from many resources, such as the list compiled by Bonnie Glover Stalls and Yaser Al-Onaizan, and another from translating English stoplists to Arabic. In their assessments, the authors reported good results that achieved about 98% accuracy using a set of data chosen from the Holy Quran (7,030 words and 3,235 stopwords), and another set of 242 Arabic abstracts chosen from the proceedings of Saudi Arabian national computer conferences (47,897 words containing 12,891 stopwords).

Abu El-Khair [2006] proposed, tested, and discussed a comparative study of the ef-fectiveness of three stoplists for Arabic IR: a general stoplist, a corpus-based stoplist, and a mixed stoplist. He took advantage of three weighting schemes: inverse document frequency weight ( idf ), probabilistic weighting, and statistical language modeling. To achieve the best IR effectiveness, Abu El-Khair mixed statistical approaches with lin-guistic approaches and compared their influence on the search process. In his assess-ments, the author used the LDC Arabic Newswire dataset with the Lemur Toolkit. He confirmed that the best match (BM) weighting scheme (BM25), already available in the Okapi IRS, obtained better results than the other weighting algorithms. When using the BM25 weight, the involvement of stoplists enhanced search effectiveness in terms of MAP, and the general stoplist MAP was better than both the corpus-based and mixed stoplists. Moreover, when each stoplist (general, corpus-based, or mixed) was combined with the BM25 weight, its corresponding MAP was enhanced by 10.44% (with the general stoplist), 7.67% (with the corpus-based stoplist), and 9.49% (with the mixed stoplist).

Alhadidi and Alwedyan [2008] proposed and assessed a hybrid stopword elimination approach for the Arabic language based on an algorithm and a dictionary. They tested their approach using a set of data extracted from the Jordanian Alrai newspaper and another set of 242 Arabic abstracts selected from the proceedings of the Saudi Arabian national computer conferences. Alajmi et al. [2012] proposed and examined a statistical method to build their Arabic stoplist. In their assessments, the authors used a corpus of 1,002 documents containing several kinds of Arabic texts. The document collection includes more than 700,000 words and contains 140,781 different words. The generated stoplist was compared to a general stoplist and performed good results by enhancing an artificial neural network classifier. In fact, the results showed that the top 200 words generated in the list (manually edited) reached 96% accuracy in terms of classification rate against 90% accuracy when using the generalized list. 2.2.7. Discussion. Since Arabic IR tasks depend on many factors, we have discussed the following problems related to Arabic language. We focused on spelling normalization and mapping. However, we give special interest to the problem of choosing the  X  X ndexing unit. X  For example, Abdelali [2004] proposed a classification of existing Arabic IRS into two classes. In morphology-based IR , the Arabic search engines exploit a linguistic-based approach supported by a huge base of linguistic processors. Several techniques for incorporating morphology such as stem [Darwish et al. 2005], root-based approaches, and light stemming [Abdelali et al. 2004b] have been used. As a result, precision and recall have been improved because of stemmers. In addition, the light stemmer was more effective for CLIR than several morphological stemmers that tried to find the root for each word [Gey and Oard, 2001; Larkey et al. 2002b]. However, full form X  X ased IR approaches use the exact query term, with a possibility to enlarge their retrieved documents to the variations of the input query term. Examples of full form X  X ased IR include Sakhr 17 and Ayna 18 Web search engines, as well as Unicode multilingual engines like Google, 19 alltheweb, 20 and UCLIR [Abdelali et al. 2004a]. We especially focus in this review on Arabic CLIR by studying previous tracks, used resources, and translation techniques. In addition, we expose and discuss a comparative study of some existing Arabic CLIR systems. There are a few evaluation tracks dedicated to Arabic IR and CLIR compared to other languages. For example, TREC has mainly focused on English; CLEF has been in-terested in European languages, 21 and FIRE has concentrated on Indian languages. However, the Arabic language has been involved in some of the tasks in these cam-paigns, such as the TREC 2001/2002 CLIR track, the CLEF 2008/2009 INFILE track, and the CLEF 2012/2013 QA4MRE track. Moreover, poorly studied areas in Arabic IR and CLIR are treated by other research groups, such as the U.S. DARPA GALE and BOLT programs. 3.1.1. TREC 2001/2002 CLIR Track. In TREC-2001 [Voorhees and Harman 2001], the LDC built and evaluated the Arabic data collection called Arabic Newswire Part 1 , 22 catalog number LDC2001T55. The assessor who developed the topic built the full topic statement in both English and Arabic after selecting 25 topics from all candidate topics. Then, these 25 English topics were translated into French. The evaluation set was built using three runs from every team and the top 70 documents from each run. The average size of the sets was 910 documents. Unfortunately, the TREC-2001 topics and their relevance judgments are suspect because of two factors: (i) only one of the participating teams had suggested a large number of relevant documents to the pool, and (ii) when that run was removed, there was a large drop in MAP on the rest of the runs [Gey and Oard 2001].

In the TREC-2002 CLIR track [Voorhees 2002], the majority of participants used an extended version of their TREC-2001 methods assessed using the same document col-lection with extra topics. However, the monolingual Arabic runs were also maintained to compare their effectiveness to the CLIR task. The TREC-2002 CLIR track was an ad hoc retrieval task in which the documents are in one language (Arabic) and topics are in a different language (English) [Oard and Gey 2002]. Fifty topics were built (first in English and then in Arabic) for the track, using the regular topic development pro-cedure except that the topic process took place at the LDC as in TREC-2001. During the evaluation, the top 100 documents from each run were selected. As in TREC-2001, a binary evaluation (relevant/not relevant) was used by the LDC evaluators to referee each document in the sets. The average size of the sets was 769 documents.
The TREC-2002 test collection does not have similar concerns as those for TREC-2001. In fact, only 5 topics had at least 40% of the relevant documents retrieved by one group. Thus, over the 50 topics, the average number of relevant documents is 118.2, with a minimum of 3 relevant documents and a maximum of 523 relevant documents. The MAP scores decreased by less than 2%, with a maximum change of 5.7%, which is similar to the TREC ad hoc collections. Although the 2002 teams used more runs and went deeper into the ranked list, the average size of the TREC-2002 teams (118.2) was smaller than the average size of the 2001 teams (164.9). Therefore, in TREC-2002, there was great similarity among results generated by different systems than in TREC-2001. Indeed, the track provided an ordinary set of resources, such as bilingual dictionaries and stemmers available to participants; common resources may decrease the differences between the systems. In addition, the topic set in 2002 was fundamentally simpler than the one in 2001, although the effectiveness of the greatest automatic systems was somewhat lesser in 2002, which would propose a contrary interpretation. Actually, the TREC-2002 collection is the best available large Arabic IR test collection for the news domain. 3.1.2. CLEF 2008/2009 INFILE Track. The Information Filtering Evaluation (INFILE) track in CLEF 2008/2009 [Besanc  X on et al. 2008, 2009] aims to apply information fil-tering in three languages: English, French, and Arabic. Indeed, the goal of the track was to assess the cross-language adaptive filtering systems by quantifying the ability of automated systems to identify and exclude irrelevant documents from a document collection given a query. The document collection is different from the TREC 2001/2002 collection. It enclosed 300,000 documents (100,000 articles from each of the three lan-guages) from the AFP newswire published between 2004 and 2006 and 50 related topics distributed as follows: 30 topics for news and events, and 20 topics for scientific and technological subjects. Four different search engines are exploited by the track organiz-ers, using different combinations of the topic fields, to generate relevance judgments by retrieving the document collection. Indeed, 28 different runs are used to build a set of search results, which were manually evaluated for relevance. Since the 2 years of the CLEF 2008/2009 track, no participation in the Arabic task has been received, whereas task data has been available for use in Arabic IR and CLIR testing. 3.1.3. CLEF 2012/2013 QA4MRE Track. In CLEF 2012/2013, seven languages, including Arabic, were involved in the Question Answering for Machine Reading Evaluation (QA4MRE) task initiated in 2012 [Pe  X  nas et al. 2012, 2013]. Indeed, documents con-cerned with a set of four specific topics including Alzheimer X  X , AIDS, climate change, and music and society were exploited to answer a multiple choice question (MCQ). The user was to choose only the correct answer among a set of five available answers. The main goal was to exploit NLP and IR approaches to analyze the question and search for the correct answer from the document collection. In this track, 160 questions were prepared (40 for each topic), in which NLP techniques were used in some of them before obtaining their answers. In fact, inferences from multiple sentences and paragraphs or inference resolution for some named entities were applied to some questions before getting their corresponding answers. The Arabic collection contains a set of documents related to the four topics distributed as follows: 120,600 documents for AIDS, 19,300 for Alzheimer X  X , 15,700 for music and society, and 10,200 for climate change. Unfor-tunately, the Arabic task was limited to two runs in CLEF 2012 achieving the lowest average scores between all runs in the seven languages that were assessed in the track [Pe  X  nas et al. 2012]. For more detail, Pe  X  nas et al. [2013] reviewed the QA4MRE challenge that was run as a lab at CLEF 2011 X 2013. 3.1.4. GALE Program. Arabic IR and CLIR evaluations are part of the Global Au-tonomous Language Exploitation (GALE) program. In fact, the main objectives of the U.S. DARPA GALE program are to transcribe, translate, analyze, and distill textual and speech data in Arabic and Chinese to make their information available to human queries [Olive et al. 2011]. The program aims to enhance MT and combine it with au-tomatic speech translation. GALE has generated new tools, such as MADA for dialects [Habash et al. 2013], and uses its training corpora for transcription and translation covering both MSA and dialectal data. The goal of distillation is to extract the most useful pieces of information related to a given query, generate a  X  X unctional X  presen-tation of such information, and eliminate redundancy. The GALE program involves Arabic speech search and question answering detailed in Darwish and Magdy [2013]. For further description of the work done in the GALE program, we refer the reader to Olive et al. [2011]. 3.1.5. BOLT Program. In 2011, the U.S. DARPA started the Broad Operational Lan-guage Translation (BOLT) 23 program to establish new methods for automated transla-tion and linguistic analysis that can be investigated in informal text and speech. The goal was to support communication with non X  X nglish-speaking persons and to detect relevant information in foreign language sources by (i) allowing multiturn communi-cation in text and speech with non X  X nglish speakers; (ii) assisting English speakers in recognizing all genres of foreign language sources, such as informal conversation, chat, and messaging; and (iii) investigating natural language queries to help English speakers to easily select targeted information in foreign language sources. The work in the BOLT program has been divided into two phases. In 2012, phase 1 exploited discussion forum threads in English, Arabic, and Chinese to experiment with ques-tion answering tasks. This phase used 43,000 Arabic forum threads (most of them were Egyptian dialect) and 154 topics, and 26 of them were proposed to search Arabic documents. In 2013, phase 2 consisted of clustering retrieved results topically, tem-porally, or geographically. This phase used 150 topics, and 22 of them were proposed to search Arabic documents. These topics identified the preferred language of the re-trieved documents. Further details about the BOLT program are given in Darwish and Magdy [2013]. To show the effectiveness of their CLIR tools, every participating group in TREC-2001 and -2002 competitions used some standard linguistic resources. We ones most used are summarized as follows:  X  X n Arabic  X  X ight stemmer, X  which was proposed in cooperation between Kareem
Darwish at the University of Maryland and Leah Larkey at the University of Mas-sachusetts. This stemmer benefited from a truncation rule-based method to eliminate a small set of prefixes and suffixes.  X  X  bidirectional Arabic-English bilingual dictionary, which was provided by David Smith of Tufts University. This dictionary was rekeyed from Salmone X  X  Advanced
Learner X  X  Arabic-English Dictionary .  X  X wo tables of translation probabilities, one for English-to-Arabic and the other for
Arabic-to-English. In fact, to align the Arabic stems produced by the light stemmer with English stems produced using the Porter stemmer [Porter 1980], the partici-pating groups took advantage of the Giza++ 24 implementation of IBM Model 1. The documents on which this alignment was performed were obtained from the United Nations by Jinxi Xu at BBN, and the alignments were produced by Alex Fraser of
USC-ISI while working at BBN.  X  X  Web-based bidirectional Arabic-English MT system called Tarjim .

In the same way, Darwish and Oard [2002b] combined these resources in the following manner: (1) They used transitive translation based on the translation probability table. After (2) Using both the Salmone dictionary and MT systems to generate translations, the (3) Finally, in the case of English-to-Arabic translation, the obtained probabilities were
Therefore, the translation process can be improved in two interesting ways due to the use of several sources of evidence. First, no single source offers a truthful indi-cation of translation probabilities. Second, no single source supplies a complete set of translations. Thus, CLIR effectiveness is improved when translation knowledge from diverse sources have been mixed.
In addition, existing Arabic IRS suffer from many shortcomings caused by the lack of suitable resources useful in the real world X  X  evaluation tasks. We mainly talk about corpora and lexicons. Indeed, existing Arabic corpora are limited to the LDC collec-tion and the Al-Hayat newspaper collection from the European Language Resources Distribution Agency. The LDC collection contains a large-scale resource used for the TREC evaluations in 2001 and 2002 sessions. However, these collections suffer from the few explorations and their boundaries in terms of representativeness and richness. Thus, the language used is MSA, which was discussed in Abdelali [2004] in terms of localization. The collection was compiled in the Middle East office of the AFP in Cypress. An evaluation of the corpus for its quality using Zipf X  X  law 25 shows that its representativeness and comprehensiveness make it a satisfactory collection.
Nevertheless, this collection suffers from many limitations. First, the spelling of translated or transliterated proper names in general tends to be inconsistent in Arabic. Such a drawback decreased the effectiveness of some participant Arabic IRS. Second, this collection represents a very small subset of actual MSA texts in terms of morphol-ogy, syntax, and composition style. In fact, the AFP corpus has some unique example of style and does not take into account the huge diversity of styles that exist from the diverse Arabic countries.

To guarantee the suitable matching in CLIR among the user X  X  information need proposed in one language and document concepts expressed in a second one, we mainly need translation knowledge, which provides the fundamental link between them. Monolingual and bilingual dictionaries are thus one of the main resources that influence the effectiveness of retrieval. Nevertheless, these kinds of resources can dif-fer from handcrafted dictionaries to MT dictionaries for a particular theme or practice. Several Arabic IRS used many available online dictionaries [Larkey and Connell 2001; Larkey et al. 2002b], such as Tarjim , Al -Misbar ,and Ectaco. 26 In addition, Zajac et al. [2001] suggested that these dictionaries are not suitable for special applications, which push to develop and test specific-task lexicons. Gey et al. [1998] confirmed that the performance of CLIR tasks showed good effectiveness using methods based on off-the-shelf MT systems [Ahn et al. 2004] alone or combined with other translation resources. However, these tools suffer from the few number of language pairs for which such systems exist. A relative solution to this problem consists of concentration on the easy form of a translation lexicon (i.e., bilingual term lists), which are more accessible for many language pairs and can be built quite simply for others. In fact, a bilingual term list is an unordered set of query-language/document-language term translation pairs, often with no translation preference or part-of-speech information [Iswarya and Radha 2012]. As detailed earlier, there are three main approaches to translation in CLIR [Farag and N  X  urnberger 2013]: (i) translation by a bilingual machine-readable dictionary (MRD), (ii) parallel or comparable corpora-based methods, and (iii) MT. Recently, approaches combining these resources have achieved some improvement in CLIR performance. In the following, we focus our discussion on the applications of these approaches to Arabic CLIR. 3.3.1. Dictionary-Based Approaches. Aljlayl and Frieder [2001] focused on Arabic-English CLIR principally using an MRD. The main problem is related to the translation ambiguity associated with these resources. In fact, they proposed and evaluated three approaches. The first one is the every-match (EM) method, in which they evaluated the impact of simple word-by-word translation on Arabic-English retrieval performance. However, this method suffers from ambiguous translations, as many extraneous terms are added to the original query. To disambiguate the query translation, the second one was the first-match (FM) method, in which only the first match translation per query term was retained instead of using all of the listed translations. Finally, to overcome the limits of these two methods, the authors proposed the two-phase (TP) method, which used some, but not all, of the translations of a given Arabic term. Their experi-ments showed that good retrieval effectiveness can be accomplished without complex resources using the TP method for Arabic-English CLIR. They also exploited short, medium, and long queries from TREC [Aljlayl et al. 2001] to empirically assess the effectiveness of the MT-based approach. They concluded that query translation for Arabic-English CLIR through MRD is cost effective compared to the other methods, such as latent semantic indexing (LSI), parallel corpus, and MT.

Hasnah and Evens [2001] proposed and evaluated an Arabic-English CLIR system using a bilingual dictionary. Indeed, their goal was to solve the problems of translation ambiguity. Arabic-English and English-Arabic CLIR experiments were performed via AL-MUTARJIM AL-ARABEY, a machine-readable Arabic-English bilingual dictionary from ATA Software LLC, which contains about 40,000 words exploited to translate the English query words into Arabic words and vice versa.

However, we should recognize that a dictionary usually gives general translations, and it suffers from other problems, such as missing words, missing word forms, and lack of proper nouns. Nevertheless, the availability of bilingual dictionaries for many language pairs and the simplicity of using the dictionary for translation make it a good choice for CLIR. In addition, query translation based on bilingual dictionaries can lead to decreased CLIR performance compared to monolingual performance because of many factors, such as (i) lack of specialized dictionaries, (ii) the introduction of ambiguity, and (iii) the inability to translate the concepts expressed by compound terms.
Kadri [2008] confirmed that the missing coverage of some terms remains the major challenge related to CLIR based on bilingual dictionary approaches. This case appears clearly for a query containing technical terms for which a general dictionary can re-spond favorably. These types of dictionaries normally contain no specialized vocabulary. The users can also enter abbreviations, acronyms, and new terms in their queries that are not included in dictionaries.

In fact, word-by-word translation methods cannot always succeed in correct transla-tion of compound words or phrases that contain more than one word. This problem can be solved using idiomatic or terminological dictionaries. Unfortunately, it is difficult to find this kind of broad coverage dictionary (e.g., for the English-Arabic pair). 3.3.2. Parallel Corpora-Based Approaches. Dictionary-based methods in Arabic CLIR suf-fer from several drawbacks. First, the majority of available Arabic language lexicons have a limited coverage compared to other languages. Second, it is not easy to choose the suitable translation among the set of alternatives proposed by the dictionary in the query translation process. To overcome some of these limitations, Kadri and Nie [2004] took advantage of statistical translation models trained on a corpus of parallel texts gathered automatically from the Web to improve query translation in an Arabic CLIR system. They also incorporated into this system other translation methods based on bilingual dictionaries. In addition, the authors focused on morphological processing for stemming Arabic words. Results of the TREC-2001 collection showed that mixing several translation tools offers good effectiveness of CLIR.

Abu-Salem and Chan [2006] proposed and assessed an English-Arabic CLIR ap-proach based on an automatic building of a translation matrix. The method used a variation of the anticipated mutual information measure (EMIM) to translate a set of English-Arabic parallel sentences. In their evaluation, the authors assessed the translation matrix by measuring the effectiveness of CLIR compared to its monolin-gual equivalent. The experimental results depend on the type of indexing method and the number of translation terms. In the majority of cases, using this matrix for query translation provided better results in the English-Arabic CLIR task compared to its monolingual Arabic retrieval.

Farag and N  X  urnberger [2008] proposed an Arabic-English word translation dis-ambiguation approach using parallel corpora and matching schemes. In fact, their approach took advantage of a huge bilingual corpus and statistical co-occurrence to identify the appropriate sense for the query translations terms. Indeed, the authors benefited from the cohesion between a given query term and its possible translations in the training corpus and a particular similarity score measure to select the suitable translation of each query term. However, the correct match was mainly influenced by the specific properties of the Arabic language.

More precisely, Farag and N  X  urnberger [2008] exploited a naive Bayesian algorithm in which they benefited from the distribution of words and related words in paral-lel corpora, with regard to the morphological inflection change across the target and the source languages. In their evaluation process, the authors concluded that their approach was worse in short queries (particularly the queries composed of only two words) but was better in the case of long queries. The unfortunate performance was caused by the features that are extracted from the few query terms that frequently appear in the context of dissimilar senses.

Additionally, the authors evaluated the performance of the algorithm based on build-ing two classifiers from different subsets of features and mixtures of them. The results showed that when considering only the basic word form, the performance of the algo-rithm was poor because an Arabic word can be represented not just in its basic form but in several inflectional forms. Thus, the authors proposed to use more training sentences that will be helpful to the disambiguation algorithm.

Recently, Farag and N  X  urnberger [2013] proposed a new Arabic CLIR system called (Mult)iSearcher , supported by an automatic translation process, which reduces the user X  X  task by verifying all alternatives for every query term. In fact, statistical meth-ods were used to automatically improve both the translation and the disambiguation steps. For example, using mutual information, the tool provides the user with ranked translation alternatives to their contextual information given in the user X  X  own lan-guage [Farag et al. 2011; Farag and N  X  urnberger 2013]. Consequently, the user can have a certain degree of confidence about the translation. As well, (Mult)iSearcher benefited from parallel corpora that are more available and with free access contrary to WordNet. In fact, the huge free texts available on the Internet are relevant for such tools. Moreover, bilingual dictionaries mainly suffer from the difficulty in identify-ing acronyms, numbers, technical terms, and named entities. Therefore, using named entity recognition approaches can enhance the performance of the (Mult)iSearcher tool.

Approaches based on statistical/probabilistic methods applied on parallel text with the intention of selecting correct translations provide good performance, but they are challenged by several obstacles. First, the translation association created among the parallel words in the text is generally domain restricted, which means that the accuracy decreases outside the domain. Second, the problem of unavailability of parallel texts in different pairs of languages, especially for the Arabic language, remains a serious challenge.
 3.3.3. MT-Based Approaches. The application of MT-based approaches to CLIR systems suffers from many challenges related to the specificities of Arabic sentences. In fact, the Arabic sentence can be expressed in different subject-verb-object (SVO) permutations having the similar significance. Consequently, and according to different word orders (SVO, VSO, VOS, and SOV), Arabic sentences can be classified into four types using these three elements. Therefore, it is not easy to find an MT system that gathers all CLIR users X  needs. In addition, it is not yet obvious whether MT-based approaches can convince CLIR users X  requirements in terms of translation quality and retrieval time. We believe that several types of MT methods exist, some of which are appropriate for MT-based CLIR.
 Contrary to the English-Arabic CLIR tracks (English queries are used to search Arabic documents) proposed in TREC-2001 and -2002, there are many other works on Arabic-English CLIR tracks (Arabic queries are used to search English documents). For example, Aljlayl et al. [2002] proposed and assessed the use of an MT-based approach for query translation in an Arabic-English CLIR system using the TREC-7 and TREC-9 topics and collections. To study the impact of context in successful MT, the authors studied the effect of query length on the performance of the MT. The proposed CLIR tool took advantage of the first commercial Arabic-to-English MT system (ALKAFI) developed by CIMOS Corporation. It also used the Porter stemmer [Porter 1980, 2006] and the K-stem algorithm.

The assessment was performed using the AIRE search engine. The goal was to eval-uate the effect of the context on the quality of the translation by using different query lengths. They concluded that the fewer source terms used to form a context, the better the retrieval precision and effectiveness. Nevertheless, their approach suffers from the problem of semantics, because MT systems cannot distinguish among cases that are syntactically and lexically ambiguous. Consequently, these systems need some level of semantic representation to accomplish high-quality translation. Finally, the authors proposed that the accuracy of the MT system can be improved by well-formed source queries. The latter can be obtained after an expansion process using pseudorelevance feedback to highlight the context of the source query.

Espa  X  na-Bonet et al. [2009] proposed an Arabic-to-English standard phrase-based statistical MT tool. To address the semantic ambiguity of Arabic, the tool was extended by integrating a local discriminative phrase selection model. Phrases are translated using linguistic information and context achieved by a local classifiers training process. Consequently, the accuracy in phrase selection was considerably increased. In fact, the global MT task took advantage of the discriminative learning. Experimental results suggested an important enhancement in the full translation task at the semantic, syntactic, and lexical levels.

Alqudsi et al. [2012] proposed a full survey for Arabic MT, in which all the existing approaches were presented and discussed and many challenges are proposed. In this survey, the authors provided some reasons for the general use of MT and discussed the features of Arabic and major MT methods. They concluded that the majority of MT systems concentrate on the translation of official texts and news. However, open domain translation has not been widely investigated.

Today, translation is a crucial requirement on the Internet, and statistical machine translation (SMT) has grown speedily. Nevertheless, these MT systems still do not meet CLIR users X  requests. Hence, most of Arabic CLIR communities need an efficient Arabic-to-English and English-to-Arabic MT system. This system could improve Arabic CLIR effectiveness if it would take into account specifics of the Arabic language. 3.3.4. Approaches Combining Arabic Translation Resources. Recently, T  X  ure and Boschee [2014] proposed a query-specific combination approach for CLIR. Indeed, the authors started from the idea combining various techniques into a single  X  X tructured query X  to improve CLIR effectiveness. This approach was assessed using four different CLIR tasks: TREC-2002 English-Arabic CLIR; NTCIR-8 English-Chinese Advanced Cross-Lingual Information Access (ACLIA); and two forum post retrieval tasks as part of the DARPA BOLT program X  X nglish-Arabic (BOLT ar ) and English-Chinese (BOLT ch ). The query language is English in all cases. This new technique yields statistically important enhancements over other state-of-the-art combination approaches, such as uniform and task-specific weighting. Moreover, it achieves a higher effectiveness than three advanced query translation techniques, all based on one-best MT systems. The authors confirmed that combining translations improves CLIR using a set of modern CLIR tasks containing a diversity of text domains and two target languages.
Mallat et al. [2014] proposed an automatic query translation approach that has been assessed in an Arabic-French CLIR context. Indeed, the authors took advantage of both a bilingual dictionary and a parallel corpus. The lexical query disambiguation process is based on a matching between two semantic networks (SNs) to select the suitable translation for each source query term. The first SN contains all possible senses of each query term. The second SN, contextually enriched, encloses a set of sentences extracted from the retrieved documents. Experimental results showed the positive impact of the contextual enrichment on the quality of translation.

To the best of our knowledge, there is no publicly published work on Arabic Web retrieval except that of Hefny et al. [2011]. Indeed, Hefny et al. took advantage of 200 cross-language queries played within the online CLIR system Bing to test Arabic-English cross-language Web retrieval. Nevertheless, their collection is not public. More-over, the relative number of indexed Arabic pages in a Web search engine was identified using search queries containing stopwords. In this context, Google is expected to index about 3 billion Arabic pages, whereas Bing can only index about 210 million pages. Further discussion of challenges that need to be tackled for Arabic retrieval is given in Darwish and Magdy [2013].

Figure 1 summarizes the available CLIR translation techniques from the literature review that were investigated to improve CLIR systems. In fact, Arabic CLIR transla-tion techniques are different from those of other languages, as each language has its own features, such as agreement features and morphosyntactic features. Additionally, challenges and ambiguities differ from one language to another.
 Unfortunately, this summary does not cover all Arabic CLIR translation techniques. Various works on Arabic CLIR not mentioned in this review have been published in several research fields and communities. For example, Arabic MT-based techniques are fully surveyed by Alqudsi et al. [2012]. In addition, Arabic IR including CLIR has mainly been reviewed by Darwish and Magdy [2013]. These works are relevant sources for recognizing additional papers. The effectiveness of Arabic CLIR systems seems to be analogous to the effectiveness of existing CLIR systems of other languages. The effectiveness of each participating Arabic CLIR system using this test collection was published by TREC. In the first year (2001), they initiated Arabic for cross-language and monolingual assessment. The 10 participants in TREC-2001 used many different resources, which are summarized in Table I [Gey and Oard 2001; Abdelali et al. 2004b].

The groups who used stem-or root-based methods did not prove how their approach resolved ambiguity. In fact, it is rare to find a text marked with diacritics (short vowels) in MSA, which causes a lot of ambiguity. To solve this problem, Beesley [2001] and Gal [2002] proposed approaches based on automatic restoration of Arabic diacritics.
A comparative study of the effectiveness of these systems is given by Abdelali et al. [2004b]. The published results for TREC-2001 showed the accomplishments of the diverse contributors. Despite that the effectiveness showed suitable results, it is clear that the LDC test collection used for the assessment is very particular. However, no one is sure if this effectiveness will be attained with a different test collection.
In the following, we especially focus on Arabic translation knowledge and its influence on Arabic IR effectiveness. Table II summarizes and compares the nine teams participating in the Arabic TREC-2002 competition. Indeed, we recall their different Arabic translation resources and their approaches in CLIR. We note that every team submitted one or more CLIR runs using different  X  X un IDs X  to easily distinguish between them. For example, the University of California at Berkeley (UC Berkeley) submitted four CLIR runs using the following Run IDs: BKYCL1, BKYCL2, BKYCL3, and brkcl4, whereas the University of Massachusetts (U Mass) submitted five different runs: UMassM, UMassX2, UMassX6, UMassX2n, and UMassX6n. The performance of the CLIR runs proposed by the UC Berkeley team are not given using the MAP metric, as was used for the majority of the participant teams; instead, the UC Berkeley team computed their performance in terms of the percentage of their monolingual runs (%MONO).

The contribution of Hummingbird was limited to publish the precision of Arabic di-agnostic runs. Indeed, the best result was registered for the run  X  X LSE-td-Y01 X , with a MAP score of 0.365. However, the University of Neuchatel (U Neuchatel) team used an evaluation based on the mean reciprocal rank (MRR) of the first correct answer found by the system [Savoy and Rasolofo 2002]. Additionally, the University of Mary-land (U Maryland) and the University of Neuchatel were the unique teams that used transliteration.

To measure the effectiveness of the runs, MAP becomes the standard evaluation metric in the CLIR tracks. In TREC-2002, the best runs were cross-language. The UMassX6n run from the University of Massachusetts was the best cross-language run, with a MAP score of 0.3996. However, the UniNE3 run from the University of Neuchatel was the best monolingual run, with a MAP score of 0.3807. These two runs were the best according to precision at document cut-off level 10 as well; however, in this case, their order was upturned: UMassX6n had a P(10) score of 0.488 and UniNE3 a score of 0.516. The University of Massachusetts submitted one monolingual run (MAP: 0.3619, P(10): 0.432), but there was not an analogous cross-language run from the University of Neuchatel. Therefore, it is not possible to confirm, from this data, whether monolingual access is better for high-precision results [Voorhees 2002].
 There are many other Arabic CLIR systems published outside TREC competitions. For example, Kadri [2008] suggested an English-Arabic CLIR tool that improved Arabic CLIR research by two contributions. First, this CLIR tool used a new stemming ap-proach based on linguistic rules to find the core of a given Arabic word. The technique achieved better results compared to a light stemming approach. Second, this CLIR tool benefited from confidence factors to propose a new technique for the combination of translation resources [Kadri and Nie 2006a, 2006b, 2007, 2008].
 Elghazaly and Fahmy [2009] proposed an English-Arabic CLIR approach for Arabic OCR-degraded text. The proposed model solves some problems related to the transla-tion and transliteration ambiguities using an orthographic query expansion. It has a high precision in translating the queries from English to Arabic and a high degree of precision in handling OCR errors.

T  X  ure et al. [2012] proposed an approach based on the combination of three kinds of statistical translation techniques: (i) context-independent token translation imple-mented using statistically aligned tokens in parallel text, (ii) token translation using phrase-dependent contexts achieved using aligned statistical phrases, and (iii) token translation using sentence-dependent contexts accomplished using those same aligned phrases together with an n -gram language model. Experimental results performed us-ing an English-Arabic task from TREC-2002, an English-Chinese task from NTCIR-8, and an English-French task from CLEF 2006, with topics in English and documents in Arabic, Chinese, and French, confirmed that no one technique is optimal for all queries. However, combination-of-evidence techniques using these three types of sta-tistical translation improve retrieval effectiveness. In addition, the best results were attained when they performed a linear interpolation of all three approaches. The opti-mal combination was resource dependent, showing a need for further efforts on robust tuning to the types of individual collections. In this section, we discuss some of the open fields in Arabic CLIR. First, we focus on Arabic cross-language social search. Second, we concentrate on multicriteria Arabic CLIR. Third, we explore the area of Arabic cross-language Web search. Finally, we discuss how a new test set can improve Arabic IR and CLIR tracks. We start by detailing the characteristics of each of these areas and discuss their related challenges. Then, we suggest some possible solutions that can be promising for these new future directions. The number of Arabic Internet users has considerably increased, and many of them are at least bilingual. Consequently, Arabic cross-language social search can be promising in diverse domains. Indeed, the language barrier between social Arabic text and other languages can be decreased using many techniques, such as status updates on social networking sites and microblogs.

Moreover, there is an increasing number of research works that are dedicated to improving the effectiveness of Arabic social search, such as Ounis et al. [2011], Darwish et al. [2012], Magdy [2013], and Magdy et al. [2012]. However, these efforts are not sufficient, as this field still suffers from several drawbacks involving new open problems that need to be tackled.

The effectiveness of Arabic social search is mainly influenced by the dialectal Arabic stemmer. In fact, the majority of available Arabic stemmers are dedicated to MSA. Consequently, there is an urgent need for an effective stemmer for dialectal Arabic. However, stemming dialectal Arabic is not an obvious task, as it mainly depends on many features, such as (i) the nonexistence of spelling standards of the dialects and (ii) the changeable linguistic features of different dialects.

Many research works have been dedicated to suggesting spelling standards of the dialects. For example, Chiang et al. [2006] expressed interest in parsing Arabic dialects using both morphological analyzers and parsers for dialects. Habash et al. [2012] pro-posed a conventional orthography for dialectal Arabic to standardize the spelling of dialects. Pasha et al. [2013] suggested a tool supporting dialectal Arabic IR using a query expansion process based on a thesaurus of equivalent words across dialects. Indeed, Arabic microblog search can significantly benefit from these efforts since it mainly has used dialects. TREC-2001 and -2002 have been the unique standard large collection for Arabic CLIR tasks, which contains news documents issued only from the AFP. Unfortunately, these collections are not suitable for multicriteria IR, as it is based only on one relevance criterion: topicality. However, there are many other domains containing a great number of documents that can be promising in studying Arabic documents in terms of other criteria, such as quality and reliability. For example, the Prophetic traditions (Hadith) collection has a large size and is available in different languages, such as Arabic, English, and French. This collection has been exploited in reliability evaluation by Bounhas [2012]. Indeed, each Prophetic Hadith contains two parts: (i) the text of the Hadith, and (ii) the chain of narration enclosing a list of linked Arabic names and the transmission manner. Bounhas et al. [2010, 2015b] exploited these chains of narrators in named entity and identity recognition. The authors proposed identity recognizer tools that link names found in chains of narrators to the biographies of the corresponding persons to identify sources of unreliability of the Hadith. Shatnawi et al. [2011] studied the automatic verification of authenticity using 17,000 Hadiths. In addition, ZAD [Darwish and Oard, 2002a] is one of the few other publicly available collections built from religious texts. Related works using the Hadith collection are detailed in the Section 4.4.

The Wikipedia Web site encloses a large number of pages in different languages, including Arabic, which represent about 252,000 pages [Denoyer and Gallinari 2006]. These pages can be considered semistructured documents, as they contain titles, out-lines, categories, info-boxes, pictures, references, internal and external links, and cross-language links. Therefore, there is a real opportunity for building a standard test collection for Arabic IR and CLIR using Wikipedia pages. As well, authors of Wikipedia pages, cross-language links, and those in the References section can be ex-ploited in reliability evaluation tasks. For example, Strassel et al. [2008] took advantage of Wikipedia links to propose their cross-document entity matching approach assessed in an automatic content extraction framework. Indeed, Wikipedia-based search tasks are improved thanks to such matching supported by Wikipedia links. In this context, Zeng et al. [2006] combined the credibility of the previous versions of an article with the quality of the added/removed content of its last version, which depends on the author X  X  reputation (registered, unregistered, or blocked). Three possible degrees of reliability (featured, good, and cleanup) are attributed using a Bayesian network tested on a set of articles generated from the Wikipedia internal review system. There are a few works in the literature dealing with cross-language Web search. In this context, the cross-language query performance prediction was studied by Kishida [2008], based on the translation quality and the so-called ease of search. The question of when to translate query words or not was tackled by Lee et al. [2010]. Then, the effectiveness of a cross-language retrieval task was examined by Hefny et al. [2011] using a query picking method based on query logs. The authors confirmed that any source language query can provide better results if it is reinforced by its corresponding equivalent query selected from the set of query logs.

Today, Arabic Web search efforts are limited to the commercial sector. In fact, Google and Bing are the leaders, as they are the most used Arabic Web search engines. Unfortunately, many efforts are invested in finding some specialized commercial Arabic search engines, such as onkosh, Araby, and Ayna, but most of them have been suspended. In fact, Arabic cross-language Web search requires that several research efforts be enhanced. These efforts should focus on crawling techniques, indexing, and search interface.

First, to crawl Web pages in diverse languages, a cross-language Web search en-gine should use language-independent algorithms. In fact, the Arabic Web topology is different from the English one. Consequently, a dedicated Arabic Web page crawler should recognize, prioritize, and scrape its users X  relevant Web. To do this, many techniques, such as metafeature extraction, page cleaning, and page prioritization, should be involved in such an Arabic crawler. Second, the Web page indexing step is a language-dependent process. Thus, an Arabic cross-language Web search engine should involve diverse indexing techniques dedicated to different languages, such as page filtering, page segmentation, tokenization, and stemming. Third, search interface is also a language-dependent task, as it involves many features that differ from one language to another, such as layout design, spell checking, query expansion, query suggestions, and displaying search results.

To the best of our knowledge, the ClueWeb09 collection 27 encloses more than 29.2 million Arabic pages crawled from the Arabic Web [Callan et al. 2009], which represents the biggest publicly available Arabic Web dataset. Unfortunately, these huge Arabic collections suffer from the lack of associated Arabic topics and their cor-responding relevance judgments on Arabic documents [Darwish and Magdy 2013]. The most crucial resources for IR and CLIR research are the test corpora that are sen-sible and represent, in general, a constraint in the assessment step. In fact, the Arabic language suffers from few available corpora compared to other languages. Additionally, the majority of Arabic test corpora are generated using newspaper texts [Maamouri and Cieri 2002]. We detail and compare in Table III the available Arabic IR and CLIR corpora.
 According to the preceding detailed literature review, the LDC Arabic Newswire Part 1 from TREC-2001 and -2002 is the most used collection in the majority of Arabic works published since the early 2000s. Nevertheless, these works have mainly focused on CLIR and ad hoc retrieval. Other works have been interested in different Arabic retrieval aspects, such as question answering, speech search, Web search, social search, image retrieval, and filtering. Unfortunately, Arabic IR and CLIR efforts suffer from several deficiencies compared to efforts invested in other languages.

In addition, ZAD is a very small collection of 2,730 documents issued from only one book. Moreover, Shatnawi et al. [2011] built a small collection using about 17,000 Prophetic traditions (Hadith) to exploit it in the automatic authenticity verification.
Most of these corpora (and especially Arabic Newswire Part 1) are limited to specific types of modern texts and are highly ambiguous, as they are not vocalized. The next generation of Arabic IR systems will need to consider working with richer data. Current data found on the Internet could be a starting point for building considerable resources adequate to these systems and reflect their ability to handle real data and text circu-lated by the different media resources. Initial work done by Goweder and De Roeck [2001] and Abdelali et al. [2004b] demonstrated the feasibility of considering resources from daily newspapers published throughout the Arab world to construct well-balanced corpora that could be used for the task of improving Arabic IR and CLIR, as well as many other tasks. It is important to get a concrete evaluation for the morphological analyzers used as the backbone in many IR and CLIR systems. This will lead to a real assessment of the effectiveness of these tools and help to improve them.
 We focus here on classical Arabic texts, which have been ignored in previous Arabic IR and CLIR test corpora. Thus, we exploit a set of Arabic stories documents called Hadith that have been the issue of several recent works [Harrag et al. 2009, 2013; Lahbib et al. 2013, 2014; Soudani et al. 2014a, 2014b, 2014c; Ayed et al. 2012a, 2012b, 2014a, 2014b; Faidi et al. 2014; Bounhas 2012; Bounhas et al. 2010, 2011a, 2011b, 2014a, 2014b, 2015a, 2015b]. In fact, Hadiths are more than religious texts; they also contain general and universal knowledge, as they talk about all of the real-life issues, and most of them remain valid in any time and place. There are many reasons for this choice: (1) The Hadith corpus is bigger than TreeBanks and the TREC collection. We can find (2) The books of Hadith are widely used in IR and knowledge extraction because of (3) This corpus is well structured, and the titles of chapters and subchapters represent (4) Unlike Arabic Newswire Part 1, which contains only Arabic documents, several (5) The Hadith corpus is one of the few vocalized Arabic corpora, thus reducing ambi-
Most of the Hadiths are assigned to the prophet (Peace Be Upon Him) act/saying, which may make it not an ideal source for modeling. However, many Hadiths are speeches of other persons, like the companions of the prophet (PBUH), and many nar-rators add other expressions, especially when they describe stories and not speeches, therefore enriching the corpus and making it less specific.

Among the full Hadith corpus, we are first interested in the well-recognized six encyclopedic books, which are organized by theme: Sahih Al-Bukhari, Sahih Muslim, Sunan Abi Dawud, Sunan Ettermidhi, Sunan Ibn Majah, and Sunan Annasaii. These books are very rich in scope and vocabulary, which makes them a good test corpus for Arabic IR and CLIR.

In fact, Hadith terms are not only limited to classical Arabic; most of them have been used in MSA. However, some other new concepts that are used today are absent in the Hadith corpus. Unfortunately, such corpus cannot cover all classic and modern Arabic terminology, and the problem of coverage remains the major challenge for any CLIR corpus.

For all of these reasons, we propose building a new IR and CLIR standard collection from Hadith books. This collection will enclose a set of standard topics and a set of books aligned at the Hadith level. Although the Hadiths have been translated in many languages, we are focusing on the Arabic-English pair, as the English translations are the most available. This requires the following tasks: (i) structuring the Hadith corpus, (ii) topic development procedure, (iii) IR models running and sampling, and (iv) dissemination.
 We estimate that building a new test set using the Hadith corpus can encourage Arabic IR and CLIR research works. In fact, it can provide new opportunities for Arabic IR and CLIR competitions via calls for workshops, conferences, or journal papers using this standard. Nevertheless, there are many challenges in this project implementation, such as:  X  Size of the corpus : The Hadith corpus is gigantic, including thousands of Hadiths distributed in several languages in heterogeneous versions. All of this data should be aligned and rechecked manually.  X  Missing data and translations : Every Hadith has its corresponding English version, but some of its words are not well translated; many of them are just transliterated, which may challenge IR and CLIR systems. In fact, when we started by aligning the first book, we remarked that some Hadiths were not translated.

Consequently, the standardization of the Hadith collection requires great efforts and hard work to be performed before convincing the IR and CLIR community about the utility and quality of this resource. In this article, we reviewed Arabic CLIR, including the specificities of the Arabic lan-guage, its preprocessing techniques, related work in different areas of Arabic CLIR, open problems, and future directions for Arabic CLIR. It is a hard task, as it deals with the full complexities of the Arabic language and aims to identify relevant documents from multilingual collections. Research in the field of Arabic CLIR has been conducted and tested since the early 2000s. As shown in the previous section, so many issues hinder current systems from demonstrating real effectiveness. For example, the prob-lem of spelling normalization and mapping is not yet resolved. Thus, Arabic letters are always ambiguous and hard to interpret. In addition, the discussion about the indexing unit (i.e., root, stem, light stem, or n -gram) is always open. Additionally, Arabic transla-tion knowledge is very poor because of the lack of high-coverage bilingual dictionaries and corpora, which are useful for translation and/or word sense disambiguation. This is why the issue of user intervention in query translation has not yet been discussed for Arabic yet many CLIR tools have been developed for other languages [Farag and N  X  urnberger 2012].

We suggested some open domains of research in Arabic CLIR that need to be ex-plored to enhance CLIR systems. The suggested Arabic CLIR tasks (social search and Web search) were reported and explained through certain examples. In addition, we propose for each task some test collections that can be suitable for their assessment. Consequently, we considered that the Arabic CLIR area may be significantly enhanced when researchers in this field focus and investigate more with regard to these tasks.
Finally, we discussed how a new test set can improve Arabic IR and CLIR. Indeed, we proposed to transform the Hadith corpus into a standard test collection for Arabic CLIR. Such a collection should contain, in addition to documents (in our case, the Hadiths), typical queries proposed by experts. Then, we should develop several indexing and matching models (e.g., Elayeb et al. [2009, 2011, 2014], Ben Khiroun et al. [2011, 2012, 2014b], and Ben Romdhane et al. [2013]), which give different results for each query. We will also vary the indexing units, thus testing and assessing several tokenization and stemming techniques and tools. A call for participation will be announced to invite researchers in the field to test their models. We are developing a collaborative platform following the TREC guidelines as described in Ben Khiroun et al. [2014a]. The results of all models will be aggregated using a sampling technique before manual validation.
Such a project requires great effort, as the amount of data to be gathered, aligned, and verified is huge. As well, Arabic texts are highly ambiguous even when vocalized, whereas Arabic NLP tools are not so mature to help efficient information mining. We mainly need to develop new analysis and disambiguation approaches. We also need to test different approaches with regard to the different steps. For example, we will test and compare several disambiguation, indexing, and retrieval methods. Hence, this project is challenging and multidisciplinary, involving many fields. It also requires cooperation between specialists from different fields, such as linguistics, Hadith, and computer sciences.

