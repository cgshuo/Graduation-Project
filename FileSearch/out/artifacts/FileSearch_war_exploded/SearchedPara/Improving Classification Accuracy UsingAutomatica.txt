 Classification is a core task in knowledge discovery and data mining, and there has been substantial research effort in developing sophisticated classification models. In a parallel thread, recent work from the NLP community suggests that for tasks such as natural language disambiguation even a simple algorithm can outperform a sophisticated one, if it is provided with large quantities of high quality training data. In those applications, training data occurs naturally in text corpora, and high quality training data sets running into billions of words have been reportedly used.

We explore how we can apply the lessons from the NLP community to KDD tasks. Specifically, we investigate how to identify data sources that can yield training data at low cost and study whether the quantity of the automatically extracted training data can compensate for its lower qual-ity. We carry out this investigation for the specific task of inferring whether a search query has commercial intent. We mine toolbar and click logs to extract queries from sites that are predominantly commercial (e.g., Amazon) and non-commercial (e.g., Wikipedia). We compare the accuracy obtained using such training data against manually labeled training data. Our results show that we can have large ac-curacy gains using automatically extracted training data at much lower cost.
 H.2.8 [ Database management ]: Database applications -data mining Algorithms, Experimentation
Work done when the author interned at Microsoft Re-search.

Classification lies at the core of many knowledge discov-ery and data mining (KDD) applications whose success de-pends critically on the quality of the classifier. There has been substantial research in developing sophisticated classi-fication models and algorithms with the goal of improving classification accuracy, and currently there is a rich body of such classifiers.

On the other hand, there is work coming out of the NLP community that suggests that for tasks such as natural lan-guage disambiguation a simple algorithm can outperform a sophisticated algorithm if it is provided with more training data [4, 10, 13]. In the application settings considered in these papers, the training data occurs naturally in text cor-pora, and high quality training data sets running into billions of words have been used.

This paper explores how we can apply the lessons from the NLP community in KDD settings. In particular, we address the question of how to obtain massive labeled data sets cheaply. Contrary to the NLP setting, such almost-for-free data is not readily available, and it is often noisy. We investigate whether we can still get high accuracy when there is no one good source for training data, and the extracted data contains inconsistencies.

To keep the discussion concrete, we consider the specific task of inferring whether a query posed to a search engine has commercial intent (i.e., the user plans to buy a tangible product). Because of large variations in the search queries, success in this task requires large amounts of training data X  queries labeled as commercial or non-commercial. However, manual labeling of queries is time-consuming and expensive. As the queries change with the passage of time, new labeling is constantly required. Consequently, there is never enough training data. Moreover, there is often inconsistency in the labels assigned even by experts [1].

In our study, we extract training data by mining toolbar and click logs. We collect queries posed to commerce-focused portals (Amazon, Craigslist) as the source of positive ex-amples. We also mine negative examples from click logs by obtaining queries that frequently lead to non-commercial portals (Wikipedia). Several issues arise immediately:
We address these issues and show how to leverage good but imperfect sources to quickly and cheaply generate mas-sive amounts of training data as frequently as needed. In the end, we show that this data yields high accuracy classi-fication results. Our findings are applicable to any domain where it is possible to find data sources that are related to the target class. For example, consider the task of deciding if a query has  X  X ocal X  intent (i.e., it reflects the intent to pur-chase goods from a local store). We could then use a site such as YellowPages.com as a source of local queries.
As we have already argued, for most classification tasks obtaining large amounts of manually labeled training data is expensive and often simply infeasible. The problem of train-ing data sparsity has been studied in the machine learning community, where two research directions have been pur-sued: i) designing complex feature representations to rem-edy feature sparseness, and ii) leveraging unlabeled data to compensate for the limited amounts of labeled data [17]. The latter approaches are often collectively referred to as semi-supervised learning (SSL).

We wish to draw a distinction between SSL and our ap-proach of automatically extracting large amounts of labeled data. SSL starts with as much manually labeled data as it is feasible to obtain. SSL then tries to leverage unlabeled data, which is assumed to be available in abundance for free, to learn a better classification model. While SSL algorithms may predict labels for the unlabeled data either during or af-ter the learning process, the quality of these predicted labels depends strongly on the amount of initially labeled data. If little to no reliable labeled data is initially available, an SSL approach may not succeed, even if it has access to large amounts of unlabeled data. In contrast, the goal of our ap-proach is to select the right data sources from which labeled data can be extracted quickly and in a straightforward man-ner. In fact, the labels are known inherently from the sources which guarantees obtaining large amounts of labeled data with ease. Thus, our approach does not require any manu-ally labeled data but instead aims at selecting the sources from which to obtain labeled data. In the empirical evalua-tion in Section 5 we compare our approach against an SSL approach, namely a self-training algorithm.

The rest of the paper is organized as follows. In Section 2, we describe our methodology for collecting automatically labeled data: selecting data sources, extracting data, and identifying those data points that are useful for training. In Section 3 we describe the problem of commercial intent iden-tification for search engine queries. In Section 4, we show how we employed the methodology presented in Section 2 to automatically extract labeled data for the commercial intent identification task. Section 5 presents an empirical evalua-tion of the approach, including a comparison against clas-sifiers trained on manually labeled data. Section 6 reviews related work, and Section 7 gives concluding remarks.
In this section, we discuss desirable properties of potential sources of training data, followed by a method for selecting which data from those sources should be extracted and au-tomatically labeled.

We propose that the sources of training data should satisfy the following properties: We now describe how we can use these properties as a guide for selecting good sources of training data.
 Popularity. The popularity property essentially states that among different possible sources we should select those that contain the largest amount of data. For example, if we are interested in extracting tagged images for image classifica-tion, we should extract images from sites like Flickr 1 , one of the popular destinations on the web for posting and tagging images. For the commercial intent classification task, we may want to extract queries posed to popular commercial portals such as Amazon.

For many classification tasks, it is possible to resort to publicly available statistics in order to choose sources that satisfy the popularity requirement. Internet survey com-panies such as Hitwise 2 provide Web traffic reports, which can be used for selecting popular sources. In Section 4 we describe how we selected the popular sources for the com-mercial intent classification task.
 Orthogonality. The orthogonality property essentially states that the sources we select should be diverse so that they provide different types of training data. For example, for the commercial intent classification task, we want to have examples related to different types of products that a user may query about. One data source may be good in pro-viding training examples of queries for electronics products, while another one may be good for queries on used furniture. Again, we can use external sources for identifying orthogo-nal sources. Web directories, or Internet survey companies such as Hitwise categorize Web sites according to the ser-vices they provide. We can make use of this categorization in order to select sources that span different categories. Separation. The separation property states that the train-ing examples must unambiguously reflect the intended mean-ing of their source. For example, the query  X  X orld War I X  can be commercial if it is posed to the Amazon portal, where it refers to a book. However, it is clearly not commercial in any other context. We now propose a technique to enforce http://www.flickr.com http://www.hitwise.com the separation property, which relies on the fact that the feature space of two distinct classes is different.
Our focus is on classification problems where the target class is relatively rare. In other words, the positive class rep-resents a concept for which we are given a definition (e.g., commercial intent), while the negative class includes every-thing else. Thus, our technique for enforcing separation is based on conservatively refining the set of positive examples, while using as many negative examples as possible.
The first step of the technique is to obtain candidate pos-itive examples from the positive sources and negative exam-ples from the negative sources. For instance, queries from Amazon and Wikipedia can serve as positive and negative examples, respectively, for commerce intent classification task. In order to make refinement easier, the second step is to separate the candidate positive examples into groups if possible (e.g., by clustering or using available metadata), and compute the frequency distribution of features for each group. The last step is to compare the distributions of each group against the distribution of the negative examples, and keep only those groups whose distribution is highly divergent with respect to the negative distribution. Groups of exam-ples too similar to the negative class are discarded. Note that for some applications or some data sources, it may not be sensible to subdivide the candidate positive examples into groups. In this case, we can compare the distribution of the entire set of positive examples to the distribution of the neg-ative examples.

The above refinement process involves comparing feature frequency distributions between sets of examples. The fre-quency distribution of features, p ( w | S ), in a set S (e.g., a group of candidate positive examples or all negative exam-ples) is defined as the fraction of times that the feature ap-pears in that set: We measure similarity between distributions using Jensen-Shannon (JS) divergence. This symmetrized and smoothed version of the Kullback-Leibler (KL) divergence [6] provides a good estimate of the true divergence, as it takes into ac-count the non-overlapping features of the two distributions under consideration. The KL-divergence between two dis-tributions P and Q is computed as: and the corresponding The Jensen-Shannon divergence is defined as: where M is the  X  X verage distribution X  computed as M = ( P + Q ). In Section 4, we provide an example to illustrate the refinement process based on JS divergence.
For the investigation of the central thesis of the paper, we choose to study the task of identifying search queries with commercial intent. From a technical point of view, this task is interesting for the following reasons:
Additionally, this task has important practical value. A large fraction of online commercial transactions are initiated with a query to a generic search engine. Thus, in order to customize the user experience around shopping, it is fun-damental for the search engines to detect the commercial intent of their users.
 Informal Definition of Commercial Intent
For the purposes of this paper, we say that a query has commercial intent if most of the users who type the query have the intention to buy a tangible product . Examples of tangible products include items such as books, furni-ture, clothing, jewelry, household goods, vehicles, etc. Ser-vices are not included.For example,  X  X edical insurance X  and  X  X leaning services almaden X  are queries that are not consid-ered commercial.

Intention to buy means that the user intends to perform a commercial transaction in which she will acquire the prod-uct. It includes cases in which a user researches the product before buying it. For example,  X  X igital camera reviews X  and  X  X igital camera price comparison X  reflect intention to buy. Intention to buy also means that money will be spent on the product and thus excludes products that can be obtained for free. For example, X  X ree ringtones X  X oes not have commercial intent because the user wants to get the product for free.
It is apparent that this definition is very broad and rather ambiguous, which adds to the complexity of the task. the category (in this case,  X  X lectronics X ) and the query.
In this section, we show how we applied the label gener-ation process to the commercial intent identification task. We show the sources that we chose; we justify why they satisfy the properties that we propose in Section 2; and we present the methods that we used to extract commercial queries from toolbar logs and non-commercial queries from click logs.
 Choosing Data Sources
We used the Hitwise Web traffic report as the basis for identifying data sources for the commercial intent identifi-cation task. Based on this report, we chose Amazon and Craigslist as sources of commercial queries, and Wikipedia as a source of non-commercial queries. We can use the Hitwise report to justify why these sources satisfy the de-sired properties. First, they are popular. According to Hit-wise X  X  September 2007 survey, Amazon has 29% of the entire Web traffic in the  X  X epartment Stores X  category; Craigslist has 52% of the traffic in the  X  X lassified Ads X  category; and Wikipedia has 27% of the traffic in the  X  X eference X  cate-gory. Second, Amazon and Craigslist have considerable de-gree of orthogonality because the kinds of products offered by  X  X lassified Ads X  and  X  X epartment Stores X  are quite dif-ferent. For example, Amazon provides a wide array of elec-tronic products, while Craigslist does not have so many of them. Craigslist does have many offers about cars, how-ever, which Amazon lacks. Finally, Craigslist and Amazon are likely to be separable from Wikipedia since the former focuses on commercial content, while the latter focuses on informational and mostly non-commercial content.
 Extracting Commercial Queries
We extracted the commercial queries from the toolbar log of a popular Web browser. We chose to use the toolbar log for two reasons. First, the queries that users pose at a portal are influenced by the type of content the site serves. Thus, if a user accesses a commercial portal and types a query in the search box, then she is likely to have commercial intent. We will shortly show how we used the toolbar logs to obtain the queries that are typed directly on the search box of com-mercial portals such as Amazon and Craigslist. Second, we can use the toolbar logs to obtain additional metadata asso-ciated with the queries. In particular, we extracted queries together with category assignments (i.e., the sales depart-ments related to the queries). The categories were used to divide the queries into groups and apply the technique pre-sented in Section 2. They were also used to discard groups that were not fully aligned with the definition of commercial intent (e.g, the category  X  X ickets X  in Craigslist).
To illustrate our use of the toolbar logs, consider a user who types the query  X  X une X  on the search box of Amazon (Figure 1a). Once the query is submitted, the following URL is generated:
It is easy to see that the query can be directly parsed out from the URL. Thus, given access to the URLs that record the activity of the users on the Amazon site, we can extract all the queries that are typed in the search box. All search engine companies provide browser toolbars and save toolbar logs that record such URLs.

It is also possible to use the toolbar logs to obtain the categories associated with the queries, which can be used to separate the queries into groups. Continuing with the  X  X une X  example, suppose that the user selects the category  X  X lectronics X  before typing  X  X une X  (Figure 1b). Then, the following URL is generated: http://amazon.com/...electronics&amp;field-keywords=zune
Again, both the category and the query can be extracted from the URL. Motorcycles/Scooters 0.563 Photo and Video 0.553 Music Instruments 0.551 CDs/DVDs/VHS 0.441 Extracting Non-commercial Queries
For the non-commercial queries from Wikipedia, we did not use the toolbar logs; the typical entry point to Wikipedia is a general search engine rather than the search box on the Wikipedia home page. Because of this, we employed the click logs of a search engine to mine non-commercial queries. In particular, we selected all the queries for which a substantial fraction of the clicks led to a Wikipedia entry (specifically, we set this fraction to one-fourth of all clicks for the query).
 Enforcing Separation
To enforce the separation property, we selected the cate-gories from Amazon and Craigslist with the highest JS diver-gence with respect to Wikipedia. In Table 1, we show the categories with the highest and lowest JS divergences for Amazon and Craigslist versus Wikipedia. Notice that cat-egories  X  X ooks X ,  X  X VDs X , and  X  X HS X  have low divergence in both commercial sources, which is consistent with our intuition that the queries for these categories (mostly spe-cific book or film titles) are ambiguous, as their vocabu-lary can be easily confused with the vocabulary of general non-commercial queries. In contrast, the high divergence categories contain words that refer to brand names, mod-els, etc., which are typically not part of the vocabulary of non-commercial queries.

Note that for Craigslist, there is a sharp decrease in di-vergence from X  X Ds/DVDs/VHS X  X o X  X ollectibles X . We used this observation to prune out the lowest divergence queries, namely X  X ooks X  X nd X  X Ds/DVDs/VHS X . A similar argument was used in the Amazon dataset to prune out all the cat-egories related to books, music, and movies. In the next section, we empirically show that removing the queries from these categories leads to a significant improvement in the performance of the resulting classifier.
In this section, we test the hypothesis that automatically extracted labeled data can be used to build high accuracy classifiers. We show the cost-effectiveness of our approach by comparing the performance of classifiers trained with automatically extracted data to the performance of classi-fiers trained with manually labeled data. We also compare against the case in which a semi-supervised learning tech-nique (in particular, self-training [15]) is used to leverage unlabeled data. The other goal of the evaluation is to val-idate the importance of enforcing the properties proposed in Section 2. To do so, we study the effect of training sets extracted from different data sources (some satisfying the properties, others not) on the performance of the resulting classifiers. Classifier
All experiments use a logistic regression classifier trained using a set of N labeled training examples D = { ( x i , y where x i is a feature vector with corresponding label y i For the commercial intent query classification task, x i is a fixed-length feature vector computed from a text query. We considered only binary features that represent the pres-ence/absence of unigrams and bigrams in the text of the query (including special begin/end bigrams). The value y  X  X  X  1 , +1 } is a class label encoding membership ( +1) or non-membership (  X  1) of x i in the commercial intent class. The logistic regression classifier computes the probability of data point x belonging to class +1 as: We find the optimal w , b using the Orthant-Wise Limited-Memory Quasi-Newton method, which enables handling large feature spaces and a large number of training points [2]. Performance Metric
We define precision and recall as follows. Let C be the set of queries that have true commercial intent (as decided by manual labeling). Also, for a particular threshold  X   X  [0 , 1] on the probability output by the logistic regression classi-fier, let Z  X  define the set of queries that have probability of having commercial intent greater than  X  . Then, we define precision and recall at threshold level  X  as: Precision at .5 recall Figure 2: Comparison of performance of classifiers trained using manually labeled and automatically extracted queries, adding a source at a time.
For many applications of practical importance (such as showing relevant content for commercial intent queries), the goal is to maximize precision for a fixed recall level. Thus, in the following we set the recall to 0 . 5, which is reasonable for such applications, and we report precision for the value of  X  such that the recall @  X  is 0 . 5. We also report the area under the curve (AUC) using a Reimann integral of the entire precision-recall curve.
 Test Set
The test set consists of 5,000 queries randomly sampled from a search engine query log. In order to assign labels to these queries, we used the Amazon Mechanical Turk Plat-form. 3 Mechanical Turk is a tool that enables requesters to pose tasks (known as Human Intelligence Tasks or  X  X ITs X ) to be answered by a community of workers. Since the quality of the labels depends heavily on the design of the HITs, we placed particular emphasis on the HIT design. In particular we asked three questions wherein the  X  X urker X  was asked to weigh plausible commercial and non-commercial intentions for each query and decide the one she thinks is dominant. Each query was shown to five  X  X urkers, X  and the final label was assigned to the query using majority voting.
The goal of the experiments in this section is to show the cost-effectiveness of using automatically extracted training data, as opposed to manually labeled data. To understand the performance of the classifiers trained with manually la-beled data, we constructed training sets by randomly sam-pling a query log. The labels were obtained using the same process explained above for labeling the queries in the test set, which involves using the Mechanical Turk Platform. We considered training sets of sizes 1K, 5K, 10K, 15K, and 20K. We created six training sets for each size, randomly sampling https://www.mturk.com/mturk/welcome. Precision at .5 recall Figure 3: Comparison of classifier performance in terms of data set size. from a pool of 25K queries, and trained six classifiers for each size of the training set. In the testing phase, we report per-formance using the mean value and error bars corresponding to one standard deviation.

For the automatically extracted data, we constructed train-ing sets using the procedure of Section 4. We used train-ing sets with queries from Amazon and Wikipedia (roughly 1.5M from each), and training sets that contain queries from Amazon, Craigslist and Wikipedia (1.5M for Amazon and Craigslist, and 3M for Wikipedia). Again, we created six training sets in each case by randomly sampling from queries extracted from the corresponding sources, and we report the mean over the six runs and error bars corresponding to one standard deviation.

Figure 2 shows the performance of the logistic regression classifiers trained with the different types of data. We con-sider manually labeled training data of different sizes and plot the precision corresponding to a 50% recall in each case. We use horizontal lines to show the performance of the classifiers trained with automatically extracted data. Note that we have used unusually large amounts of man-ually labeled data to get a feel for the asymptotic region. Such large manual data sets are generally too expensive and time-consuming to construct.
 The precision at 50% recall of the classifier trained with Amazon and Wikipedia queries is 0.58. This outperforms the classifiers trained with up to 8.5K manual labels. When we add an additional source of commercial queries (i.e., we use both Amazon and Craigslist), the precision of the classi-fier trained with automatically generated labels jumps from 0.58 to 0.73. The classifier trained with manually labeled data now needs more than 20K labels to catch up. The same results are obtained in terms of AUC, which is 0.55 for the classifier trained with Amazon, 0.64 for the classifier trained with Amazon and Craigslist, and 0.63 for the classi-fier trained with 20K manual labels. It is interesting to note what happens for the classifiers trained with the amounts of manually labeled data typically used in practice (1K-5K range). In this case,the performance gap with respect to the classifiers trained with automatically extracted data is significantly large. Precision at .5 recall Figure 4: Performance comparison of classifiers trained using automatically extracted queries and using semi-supervised learning with manually la-beled queries.

At this stage, it is natural to wonder whether it is possible to boost the performance of the classifiers trained with auto-matically extracted data by adding some manually labeled data to the training sets. The answer is yes, but only to a small extent. In particular, we ran an experiment where we consider training sets that consist of a mix of manually la-beled queries (from 1K to 25K) and automatically extracted labels. The results are shown in Figure 3. It can be seen that adding manual labels helps to boost the performance of the classifier only when large amounts of labeled data are used; and that even for the largest amount of extra man-ual labels, the gains are small. In particular, the precision increases from 0.73 to 0.74, which is within the margin of error (the standard deviation is 0.013).

Another natural question is whether it would be possible to boost the performance of the manually labeled training sets by employing semi-supervised learning techniques. To address this question, we considered the case in which we start with classifiers trained with manually labeled data and then apply self-training , a semi-supervised learning tech-nique, to exploit unlabeled queries from a query log. In our self-training experiment, we started with the manually labeled queries, and at every round of training, we added the most reliably predicted unlabeled examples to the train-ing set with their putative labels. In particular, for every round of training, we obtained predictions for 5,000 unla-beled queries sampled randomly from the query log of a ma-jor search engine. We sorted the probabilities and labeled the top 5% and bottom 10% of the sorted queries as pos-itive and negative examples, respectively. We investigated other schemes, such as hard thresholding of the probabilities, and found the scheme used here to work best. The results are given in Figure 4. We can see that self-training pro-vides only a marginal improvement in precision, and that a large amount of manually labeled data is still necessary to catch up with the classifiers based on automatically ex-tracted data. Figure 5: Precision-recall curves for the classifiers trained with queries from all categories and with queries only from selected categories.
In this section, we present experiments aimed at validat-ing the importance of enforcing the properties proposed in Section 2. To validate the separation property, we consider the effect of pruning the low-divergence categories as ex-plained in Section 4. In particular, we consider two clas-sifiers trained with the same number of queries and from the same sources (Amazon, Craigslist, and Wikipedia). The difference between the two training sets is that one contains queries sampled only from the categories that we selected in order to satisfy the separation property; the other contains queries sampled from all categories, including the low diver-gence ones. In Figure 5, we show the precision-recall curve for both classifiers. It can be observed that our proposed strategy of removing the low divergence categories leads to considerable improvement in precision, especially around re-call of 50%.

We also evaluate the importance of the orthogonality prop-erty. The reason why we chose Amazon and Craigslist in our study was based on the fact that they belong to orthogonal categories in the Hitwise report:  X  X epartment Stores X  and  X  X lassified Ads. X  In order to validate the orthogonality prop-erty, we consider what happens when more than one source from the same category is used. In particular, Amazon is the site with the highest traffic for the  X  X epartment Stores X  category. The second and third most popular sites in the same category are Walmart and Target. We now consider what happens when we use Amazon and one of the other two department stores as sources. We can observe in Fig-ure 6 that adding queries from Walmart or Target does not improve performance with respect to the classifier trained using only queries from Amazon. On the other hand, we can see that when queries from the orthogonal source Craigslist are added, precision improves substantially.
In using automatically extracted training data, we face the problem of potentially noisy data (for example, a query about a commercial product that shares a name with a non-Precision at .5 recall Figure 6: Precision at 50% recall for various combi-nation of sources for commercial queries. commercial object). Previous work on inferring the relia-bility of training data has relied on measuring similarities between data points. Some approaches assign probabilis-tic weights to data points based on, for instance, Gaussian mixture models [14] or feedback from a neural network [16]. In [12], the purity of neighborhood graphs (constructed us-ing data features) is used to remove noisy data points. In contrast, we exploit metadata (categories) and background knowledge to reduce the number of undesirable queries.
Most work on query intent identification has used small amounts of labeled data. For example, 6,000 manually la-beled queries are used in [3] to learn to classify a query as having informational, navigational, or transactional in-tent, and only 1,408 labeled queries are used in [7] to train a commercial intent detection classifier. To offset the prob-lems with small training sets, there has been work on using semi-supervised learning. For instance, both [5] and [11] apply semi-supervised learning for the task of web query classification. While [5] uses query logs as unlabeled data, [11] uses the query-click graph. While semi-supervised tech-niques assume the presence of an initial high-quality seed set of manual labels, our approach, in contrast does not re-quire any manual labeling. Instead, our approach focuses on identifying data sources that can be leveraged to obtain large-scale labeled data for the classification task at hand.
Related, but not directly relevant, is the line of work on mining search engine query logs to obtain training data for learning ranking models [1, 8, 9]. Their task is different and the source of the training data is closely tied to the task at hand. In particular, the click logs collected through the usage of the ranker is utilized to provide implicit feedback to the ranker. In our case the data sources we considered are not directly tied to the classification task.
We showed how to leverage good, but imperfect, Web sources to quickly and cheaply generate massive training sets as frequently as needed, in a manner that yields high accuracy classifiers. These techniques may obviate the need for expensive, time-consuming manual training sets for some tasks.

In this paper, we used the task of commercial intent iden-tification to validate our proposal for extracting massive amounts of training data. It is natural to think of other tasks that can benefit from a similar approach. Examples of such tasks include identification of geographical queries where positive queries can arise from popular mapping sites, while negative queries can be the log queries that never (or with very small probability) lead to clicks on mapping sites.
There are several directions for future work. These in-clude exploring other types of sources of labeled data, new techniques for handling noise in the sources, and algorithms to automatically select the thresholds to prune out low-divergence classes. [1] R. Agrawal, A. Halverson, K. Kenthapandi, [2] G. Andrew and J. Gao. Scalable training of [3] R. A. Baeza-Yates, L. Calder  X on-Benavides, and C. N. [4] M. Banko and E. Brill. Scaling to very very large [5] S. Beitzel, E. Jensen, O. Frieder, and D. Grossman. [6] T. Cover and J. Thomas. Elements of information [7] H. Dai, Z. Nie, L. Wang, L. Zhao, J. Wen, and Y. Li. [8] T. Joachims. Optimizing search engines using [9] T. Joachims, L. A. Granka, B. Pan, H. Hembrooke, [10] M. Lapata and F. Keller. Web-based models for [11] X. Li, Y. Wang, and A. Acero. Learning query intent [12] F. Muhlenbach, S. Lallich, and D. A. Zighed. [13] P. Nakov and M. A. Hearst. Using the web as an [14] U. Rebbapragada and C. E. Brodley. Class noise [15] D. Yarowsky. Unsupervised word sense disambiguation [16] X. Zeng and T. R. Martinez. A noise filtering method [17] X. Zhu. Semi-supervised learning literature survey.
