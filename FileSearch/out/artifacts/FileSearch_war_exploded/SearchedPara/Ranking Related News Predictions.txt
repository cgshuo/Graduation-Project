 We estimate that nearly one third of news articles contain references to future events. While this information can prove crucial to understanding news stories and how events will develop for a given topic, there is currently no easy way to access this information. We propose a new task to ad-dress the problem of retrieving and ranking sentences that contain mentions to future events, which we call ranking related news predictions . In this paper, we formally define this task and propose a learning to rank approach based on 4 classes of features: term similarity, entity-based similarity, topic similarity, and temporal similarity. Through extensive evaluations using a corpus consisting of 1.8 millions news articles and 6,000 manually judged relevance pairs, we show that our approach is able to retrieve a significant number of relevant predictions related to a given topic.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Retrieval models ; H.3.4 [ Information Storage and Retrieval ]: Systems and Software X  Perfor-mance evaluation (efficiency and effectiveness) Algorithms, Experimentation, Performance News predictions, Future events, Sentence retrieval and rank-ing
Predicting the future has long been the holy grail in the financial world. The leaders of large organizations need to Work performed while intern at Yahoo! Research analyze information related to the future in order to iden-tify the key challenges that can directly affect their organi-zations. However, it is not just businesses that care about the future -all people have anticipation and curiosity about the future. Canton [8] describes the future trends that can influence our lives, our jobs, our businesses, and even our world. These include the energy crisis, the global financial crisis, politics, health care, science, securities, globalization, climate changes, and technologies. When people read news stories on any of these topics whether it is an article about war in the middle east or the latest health care plan, they are naturally curious about potential future events. How long will the war last? How much will it cost? What happens if we do nothing at all? This obsession with the future is also reflected in the news articles themselves -our analysis of one year worth of news from over 100 sources indicates that nearly one third of news articles contain at least one statement made about a future date.

Accessing this information in an intuitive way would greatly improve how people read and understand news. In this pa-per, we define a new task we call ranking related news pre-dictions that directly addresses this problem by finding all predictions related to a news story in a news archive and ranking them according to their relevance to the news story. This task is motivated by the desire of news sites to in-crease user engagement by providing content that directly addresses the information needs of users. By providing links to relevant content, new sites can keep users on their site longer thus increasing the likelihood that users will click on revenue generating links and also improving user satisfac-tion. For a wide range of news events from natural disasters to political unrest in the middle east, the information need -the question most on people X  X  minds -is what is going to happen next. This new task is a first step toward helping people answer this very question by finding and linking to predictions that are relevant to the user.

Our query is extracted from a news article currently read by a user, and is composed of a bag of entities or terms . Using an automatically-generated query, predictions are re-trieved, ranked over the time dimension, and presented to the user. Note that there are a number of future-related in-formation analyzing tools including Recorded Future 1 , and Time Explorer [22]. Recorded Future extracts predictions from different sources (news publications, blogs, trade pub-lications, government web sites, and financial databases). A user creates a query by selecting a topic of interest (e.g. a topic about X  X inancial Markets X ), and then specifying an en-tity (people, companies, or organizations) from a set of X  X re-defined X  en tities. The system will then retrieve predictions related to the selected topic and entity. A major difference with our system is that Recorded Future requires a query specified in advance, while our system automatically creates a query for the user based on the news article being read and it is not limited to  X  X redefined X  entities. Time Explorer is a search engine that allows users to see how topics have evolved over time and how they might continue to evolve in the future. The system extracts predictions from document collections and allows users to search for them using ad-hoc queries. However, neither Time Explorer nor Recorded Fu-ture provide details of how predictions are ranked nor do they evaluate performance in a formal setting as we do here.
The main contributions of this paper are: 1) the first for-malization of the ranking related news predictions task, 2) an evaluation dataset with over 6000 relevance judgments from the New York Times Annotated Corpus 2 with queries that are selected from real-world future trends [8] 3) a learned ranking model incorporating four classes of features includ-ing term similarity, entity-based similarity, topic similarity, and temporal similarity 4) an in-depth analysis of feature se-lection to guide further research in the ranking related news predictions task.
 The organization of the rest of the paper is as follows. In Section 2, we explain our system architecture, and outline the models for annotated documents, predictions as well as queries. In Section 3, we propose four classes of features used for learning a ranking model. In Section 4, we describe our ranking model. In Section 5, we evaluate the proposed ranking model. In Section 6, we give an overview of related work. Finally, in Section 7, we conclude and outline future work.
In this section, we outline the system architecture, and give the formalization of the models for annotated docu-ments, predictions, and queries.
Figure 1 depicts our system which retrieves a set of predic-tions (sentences containing future dates) related to a given news article. Predictions can be extracted from a temporal document collection  X  any collection that contains times-tamped documents, e.g., personal emails, news archives, company websites and blogs. In this work, we automat-ically extract predictions from news archives using differ-ent annotation tools. Our document annotation process in-cludes tokenization, sentence extraction, part-of-speech tag-ging, named entity recognition, and temporal expression ex-traction. The result of this process is a set of sentences annotated with named entities and temporal expressions, which will be indexed as predictions for further processing or retrieval.

A key component of the annotation process is the extrac-tion of temporal expressions using a time and event recogni-tion algorithm. The algorithm extracts temporal expressions mentioned in a document and normalizes them to dates so they can be anchored on a timeline. As explained in [1], there are three types of temporal expressions: explicit, im-
Figure 1: Prediction retrieval system architecture. plicit and relative. An explicit temporal expression men-tioned in a document can be mapped directly to a time point or interval, such as, dates or years on the Gregorian calendar. For example,  X  X uly 04, 2010 X  or  X  X anuary 01, 2011 X  are ex-plicit temporal expressions. An implicit temporal expression is given in a document as an imprecise time point or interval. For example,  X  X ndependence Day 2010 X  or  X  X ew Year Day X  X  2011 X  are implicit expressions that can be mapped to  X  X uly 04, 2010 X  X r X  X anuary 01, 2011 X  X espectively. A relative tem-poral expression mentioned in a document can be resolved to a time point or interval using a time reference -either an explicit or implicit temporal expressions mentioned in a doc-ument or the publication date of the document itself. For example, the expressions  X  X his Monday X  or  X  X ext month X  are relative expressions which we map to exact dates using the publication date of the document.

Instead of having an explicit information need provided, we automatically generate a query. In this case, we assume that the user X  X  information needs lie in the news article being read by the user, and a query will be extracted from this news article (further details are given in Section 2.4). For a given news article, we retrieve predictions that are relevant to the news article, that is, relevant sentences containing future dates with respect to the publication date of the news article being read.

Retrieved predictions are ranked by the degree of rele-vance, where a prediction is  X  X elevant X  if it is future infor-mation about the topics of the news article . Note that we do not give any specific instructions about how the dates involved are related to relevance. Nevertheless, we hypothe-size that predictions extracted from more recent documents are more relevant. In this paper, we use a machine learning approach [20] for learning the ranking model of predictions. This involves identifying different classes of features (see Sec-tion 3) to measure the relevance of a prediction with respect to the news article.
Our document collection contains a number of news arti-cles defined as C = { d 1 , . . . , d n } . We treat each news article as a bag-of-words (an unordered list of terms, or features), d = { w 1 , . . . , w n } . time ( d ) is a function given the creation or publication date of d . Some of our proposed features are extracted from annotated documents, which are defined Table 1: Example of a prediction with field/value pairs.
 as follows. Each document d , has an associated annotated document  X  d , which will consist of three sets,  X  d e ,  X  set of named entities  X  d e = { e 1 , . . . , e n } , where each entity e  X  E and E is the complete set of entities (typed as per-son, location, and organization) in the collection; a set of annotated temporal expressions  X  d t = { t 1 , . . . , t m of sentences  X  d s = { s 1 , . . . , s z }
A prediction p can be viewed as a sentence containing field/value pairs of annotation information and we define d as the parent document where p is extracted from. We de-fine several fields for a prediction including id , parent id title , entity , future date , pub date , text , and con-text . The field id specifies a prediction X  X  unique number, parentid and title represent a unique number and the ti-tle of d p respectively entity contains a set of annotated entities p entity  X   X  d e , future date consists of  X  X uture X  tem-por al expressions p future annotated in p , pub date is the publication date of the parent document d p and text is a prediction X  X  text p txt or the sentence of p . Note that each prediction must contain at least one  X  X uture X  temporal ex-pression, that is, p future 6 =  X  . In addition, we explicitly model the context of the prediction p ctx , represented by the field context and defined as surrounding sentences of the main sentence [6]. In our work, we define the context p ctx as the sentence immediately before and the one immediately after p txt . Table 1 contains an example of a prediction with its field/value pairs.
As mentioned earlier, a query q is automatically generated from a news article being read d q ; q is composed of two parts: keywords q text , and the time of query q time . The keywords q text are extracted from d q in three ways resulting in three different types of queries.

The first type of query is a bag of entities , noted as X  X ntity query X  or Q E where its q text is composed of the top-m en-tities (ranked by frequency) extracted from d q . Intuitively, we want to know whether using only key entities frequently mentioned in the news article can retrieve relevant predic-tions with high precision or not. For example, given an ac-tual document about  X  X resident Bush and the Iraq war X , we extract Q E with q text = h George Bush , Iraq , America i . At retrieval time, q text will be matched with the entity field of the predictions.

The second query is denoted X  X erm query X  X r Q T where its q text is composed of top-n terms (ranked by term weighting, i.e., TF-IDF) extracted from d q . Q T is considered a bag of terms important to both d q (locally) and the whole collec-tion (globally). In contrast to the previous query type, Q aims at retrieving predictions related to the topics of news article, which can be represented as a set of informative is extracted from the same document used in the Q E exam-ple above. In this case, q text will be matched with the text field of the predictions.

The last type is called  X  X ombined query X  or Q C where its q text is a combination of both top-m entities and top-n terms formed by concatenating Q E and Q T . We discuss how we select top-m and top-n in Section 5.

The last component of the query is the temporal criteria or q time used for retrieving predictions on the time dimension; q time is composed of two different time constraints. The first constraint is specified in order to retrieve only predictions that are future relative to the publication date of query X  X  parent article, or time ( d q ). The second constraint indicates that those predictions must belong to news articles pub-lished before time ( d q ). Both time constraints will be repre-sented using a time interval, i.e., [ t b , t e ], where t ning time point and t e is an ending time point, and t e &gt; t In all cases, the first constraint is ( time ( d q ) , t max second constraint is [ t min , time ( d q )], where ( time ( d [ time ( d q ) , t max ]  X  X  time ( d q ) } , and t max and t imum time in the future and the minimum time in the past respectively. At retrieval time, the first constraint will be matched with the field future date of predictions, whereas the second constraint will be matched with the field pub date of predictions.
In this section, we present features used for learning a ranking model for related news predictions. The model will be described in Section 4. We propose several classes of fea-tures to capture the similarity between a news article query q and a prediction p , i.e., term similarity, entity-based simi-larity, topic similarity, and temporal similarity. The detailed description of each class will be given next.
Since a prediction is defined with multiple fields, we em-ploy the fielded searching provided with Apache Lucene search engine. The first term similarity feature retScore is the de-fault similarity scoring function 3 of Lucene, which is a vari-ation of the tf-idf weighting scheme.

A disadvantage of retScore is that it will not retrieve any predictions that do not match the query terms. This is-sue is exacerbated in sentence retrieval by the fact that we have to retrieve short fragments of text which might refer to the query terms using anaphora or other linguistic phenom-ena. One technique to overcome this problem is to use query expansion/reformulation using synonyms or different words with very similar meanings. It has also been shown that extending a sentence structure by its surrounding context sentences and weighting them using a field aware ranking function like bm25f consistently improves sentence retrieval [6]. Therefore, rather than reformulating a query, we will retrieve a prediction by looking at the context and title fields, in addition to the text field. Thus, even if the text field does not match exactly with a query term, p can receive a score if either the context or title field match the query term.

In our case, instead of weighting differently keyword matches in the title or body of a Web-page, we assign a different im-portance to matches in the sentence itself or its context. The second term similarity feature bm25f can be computed as follows. w here l f is the field length, avl f is the average length for a field f , b f is a constant related to the field length, k a free parameter and boost ( f ) is the boost factor applied to a field f . N P is the total number of predictions and n i is the number of prediction containing w i , and F = { text , context , title } . We discuss parameter settings in Section 5.1.
This feature class is aimed at measuring the similarity between q and p by measuring the similarity of the entities they each contain. Note that, this class is only applicable for a query consisting of entities, that is, Q E and Q C , and it is ignored for Q T . The first feature entitySim compares a string similarity between the entities of q and p entity the Jaccard coefficient, which can be computed as follows. where p entity is a set of entities, | q  X  p entity | and | q  X  p are the size of intersection and union of entities of q and p .
Thus, the higher the overlap between the entities of a prediction and the query, the higher the prediction will be ranked for the query. We also want to rank predictions by us-ing features that are commonly employed in an entity rank-ing task. For example, an entity is relevant if it appears in the title of a document, or it always occurs as a subject of sentence. We will employ entity ranking features by assum-ing that the more relevant entities a prediction contains, the more relevant it is. The entity-based features will be ex-tracted and computed relative to the parent document of a prediction ( d p ) or on the prediction itself ( p ). Features extracted from documents are title , titleSim , sen-Pos , senLen , cntSenSubj , cntEvent , cntFuture , cntEventSubj , cntFutureSubj , timeDistEvent , timeDistFuture and tagSim . Features extracted from predictions are isSubj and timeDist . The value of all features is normalized to range from 0 to 1, unless otherwise stated. First, the feature title indicates whether an entity e is in the title of d p . A value is 1 if e appears in the title of d p , or 0 if otherwise. titleSim is a string similarity between e and the title. senPos gives the position of the 1 st sentence where e occurs in d p . w here len ( d p ) gives the length of d p in words. pos ( s position of a sentence s y in d p . senLen gives the length of the first sentence of d that contains e . cntSenSubj is the number of sentences where e is a subject. We run a dependency parser over the sentences in order to determine whether an entity is a subject of not. where S e is a set of all sentences of e in d p . isSubj ( e, s 1 if e is a subject of s y . cntEvent is the number of event sentences (or sentences annotated with dates) of e . where E p d is a set of all event sentences in d p . isEqual ( s returns 1 if s z equals to s y . cntFuture is the number of sentences with a mention of a future date. cntEventSubj is the number of event sentences that e is a subject. Similarly, cntFutureSubj is the number of future sentences that e is a subject. timeDistEvent is a measure of the dis-tance between e and all dates in d p . where normDist ( e, s z ) = 1 |T dist ( w i , w j ) = | pos ( w i )  X  pos ( w i ) |  X  1. E sentences of e , and T s z is a set of all temporal expressions in s z . dist ( w i , w j ) is a distance in words between terms w and w j . maxDist ( s z ) is a maximum distance between terms in s z . timeDistFuture ( e, d p ) is a distance of e and all future dates in d p computed similarly to timeDistEvent . tagSim is a string similarity between e and an entity tagged in d p where N p d is a set of all entities tagged in d p . tagSim is only applicable for a collection provided with manually assigned tags (e.g., the New York Times Annotated Corpus). isSubj ( e, p ) is 1 if e is a subject with respect to a prediction p , and timeDist ( e, p ) is a distance of e and all future dates in p computed similarly to timeDistEvent . All features in this class are parameter-free.
This class of features is aimed to compare the similarity between q and p on a higher level by representing them using topics. Examples of topics are  X  X ealth care reform X ,  X  X inan-cial crisis X , and  X  X lobal warming X . Several works [7, 32] have proposed to model a document with a low dimensionality, or to use topics rather than terms. We will use latent Dirichlet allocation (LDA) [7] to model a set of topics. LDA is based on a generative probabilistic model that models documents as mixtures over an underlying set of topic distributions. In general, topic modeling consists of two main steps. The first step is to learn topic models from training data. LDA requires the parameter N z or the number of topics to be specified. After a model is trained, the next step is to in-fer topics from the learned topic model outputting a topic distribution for the prediction.

Wei and Croft [32] incorporated topic modeling for ad-hoc retrieval, and showed that linearly combining LDA with the query likelihood model outperformed non-topic models like the unigram model. We incorporate LDA into the retrieval process differently from Wei and Croft in two ways. First, instead of combining LDA scores with the original retrieval score, we represent q and p as vectors of topic distributions and compute the topic-based similarity using a cosine sim-ilarity between two vectors. Second, we explicitly take the time dimension into modeling topics because topics distribu-tions can evolve over time. Intuitively, topics keep changing over time according to different trends.

We apply topic modeling to future retrieval in three main steps: 1) learning a topic model, 2) inferring topic models, and 3) measuring topic similarity.

Learning a topic model. We take into account the time dimension for learning topic models. As shown in Figure 2, we create training data by partitioning the document collec-tion D N into sub-collections (or document snapshots) with respect to time. In other words, we group documents by year of publication, and randomly select documents as training data, called a training data snapshot D train ,t k at time t Note that, we can also use more sophisticated approaches for modeling topics over time as presented in [31]. However, we will leave this study for future work.

Topic model inference. Using learned models from the previous step, we determine the topics for q and p from their contents. This process is called topic inference , which rep-resents a query and a prediction by a distribution of topics (probabilities). For example, given a topic model  X  , a pre-diction p can be represented as p  X  = p ( z 1 ) , . . . , p ( z p ( z ) gives a probability of a topic z obtained from  X  . Because our topic models are learned from different time periods, a question is which model snapshot we use for inference. Note that, q and p must be inferred from the same model snap-shot in order to be comparable. We select a topic model for inferring in two ways. First, we select a topic model from a time snapshot time ( d q ) which corresponds to the publica-tion date of the news article parent of q . Second, a topic model is selected from a time snapshot t which corresponds to the publication date of the news article making predic-tion p , or the time ( d p ). Moreover, a prediction p will be inferred in three different ways depending on the contents used: 1) only text p txt , 2) both text p txt and context p and 3) the parent document d p . For a query q , the contents of its parent document d q will be used for inference.
In addition to using all N z topics for inference, we will also select only top-k topics ranked by the importance. The idea is that measuring the topic similarity using too many topics may not be as accurate as using only the most impor-tant topics. We use coverage and variation proposed in [29] for ranking topics. A topic coverage ( z ) assumes that top-ics that cover a significant portion of the corpus content are more important than those covering little content, while a topic variation  X  ( z ) considers topics that appear in all the documents to be too generic to be interesting, although they have significant content coverage. ( z ) and  X  ( z ) are com-puted using a mean and a standard deviation over topic distributions, and the final score for ranking topic is a mul-tiply of ( z ) and  X  ( z ). The calculation ( z ) and  X  ( z ) for a topic z at time t k is given as: where N D is the number of documents in a training set at in a document d i and len( d i ) is the document length of d A final score for ranking a topic z can be computed as: where the parameters  X  1 and  X  2 indicate the importance of ( z ) and  X  ( z ). If  X  1 = 1 and  X  2 = 0, the ranking is determined purely by topic coverage. On the contrary, if  X  1 = 0 and  X  2 = 1, the ranking emphasizes topic variance.
Measuring topic similarity. Given a topic model  X  , the topic similarity can be calculated using a cosine simi-larity between a topic distribution of query q  X  and a topic distribution of prediction p  X  as follows.
We denote a topical feature using LDA i,j,k , where i is one of the two different methods for selecting model snap-shot: i = 1 for selecting a topic model from a time snap-shot time ( d q ), and i = 2 for selecting from a time snapshot time ( d p ); j is one of the three different ways of using the contents for inference: p txt , p ctx , or d p . Finally, k refers to whether we use all of only top-k of topics for inference. Thus, this results in 12 (=3*2*2) LDA-based features in to-tal.
As mentioned earlier, we explicitly exploit temporal ex-pressions in ranking. To measure the temporal similarity between a query and a prediction, we employ two features proposed in previous work: TSU [16] and FS [15].

We will represent our model of time using a time interval [ b, e ] having a begin point b and the end point e . The actual value of any time point, e.g., b or e in [ b, e ], is an integer or the number of time units (e.g., milliseconds or days) passed (or to pass) a reference point of time (e.g., the UNIX epoch).
The fir st feature TSU is defined as the probability of gen-erating the time of query q time from the document creation date time ( d ). TSU can be computed as follows.
 where DecayRate and  X  are constants, 0 &lt; DecayRate &lt; 1 and  X  &gt; 0. is a unit of time distance. Intuitively, the prob-ability obtained from this function decreases proportional to the distance between q time and time ( d ), that is, a document with its creation date closer to q time will receive a higher probability than a document with its creation date farther
We apply TSU for measuring the temporal similarity be-tween q and p based on two assumptions. First, we assume that p is more likely to be relevant if its parent time time ( d is closer to the time of query article time ( d q ). Our first tem-poral feature, denoted TSU 1 , will be calculated similarly to Equation 16 resulting the following function.

The second assumption, denoted TSU 2 , is that a predic-tion is more likely to be relevant if its future dates p future are closer to the publication date of query article time ( d If there are more than one future dates associated to p , a final score will be averaged over scores of all future dates p future . The temporal distance of TSU 2 of q and p is defined as follows.
 where t f is a future date in p future and N f is the number of all future dates.

In addition to TSU 1 and TSU 2 , we can measure the tem-poral similarity between q and p using a fuzzy membership function, which is originally proposed by Kalczynski and Chou [15].

We adapt the original fuzzy set function in [15] by using its parent time time ( d p ) and the time of query article time ( d We denote this feature as FS 1 , and it can be computed as follows.
 time ( d q ), or 1 if time ( d p ) = time ( d q ).
We define the second temporal feature based on a fuzzy set by using the prediction X  X  future dates p future and the publica-tion date of query article time ( d q ). Similarly, if a prediction p has more than one future date, a final score will be av-eraged over scores of all dates p future . The second temporal feature FS 2 is defined as follows.
 N f is the number of all future dates in p future , and t f ture date, i.e., t f  X  p future . f 2 ( t f ) is equal to if t f 6 = time ( d q ), or 1 if t f = time ( d q ). n and m are con-stants.  X  1 and  X  2 are the minimum and maximum time of reference with respect to q time .  X  1 is calculated by subtract-ing the time offset s min from from q time , and  X  2 is calculated by adding the offset s max to q time .
Given a query q , we will rank a prediction p using a ranking model obtained by training over a set of labeled query/prediction pairs using a learning algorithm. An un-seen query/prediction pair ( q, p ) will be ranked according to a weighted sum of feature scores: where x i are the different features extracted from p and q , N is the number of features, and w i are the weighting co-efficients. The goal of the algorithm is to learn the weights w i using a training set of queries and predictions, in order to minimize a given loss function. Learning to rank algo-rithms can be categorized into three approaches: pointwise, pairwise, and listwise approaches [20]. The pointwise ap-proach assumes that retrieved documents are independent, so it predicts a relevance judgment for each document and ignores the positions of documents in a ranked list. The pairwise approach considers a pair of documents, and rele-vance prediction is given as the relative order between them (i.e., pairwise preference). The listwise approach considers a whole set of retrieved documents, and predicts the relevance degrees among documents. For a more detailed description of each approach, please refer to [20].
 We employ the listwise learning algorithm SVM MAP [34]. The algorithm trains a classifier using support vector ma-chines (SVM), and it determines the order of retrieved doc-uments in order to directly optimize Mean Average Preci-sion (MAP). In addition, we also experimented with other learned ranking algorithms: RankSVM [14], SGD-SVM [36], PegasosSVM [28], and PA-Perceptron [9]. However, these algorithms do not perform as well as SVM MAP in our exper-iments. Thus, we will only discuss the results obtained from SVM MAP in the next section.
In this section, we evaluate the retrieval effectiveness of our proposed ranking model using three different query for-mats. We will first describe the experimental settings fol-lowed by an explanation of the results and a detailed discus-sion. Temporal document collection. We used the New York Times Annotated Corpus for our document collection, which contains 1.8 million documents covering the period from January 1987 to June 2007. In order to extract pre-dictions and features, a series of language processing tools, including OpenNLP (for tokenization, sentence splitting and part-of-speech tagging, and shallow parsing), the SuperSense tagger (for named entity recognition) and TARSQI Toolkit (for extracting temporal expressions from documents). Given the importance of time to our system, we note that the tem-poral expression extraction of TARSQI has a reported per-formance of 0.81 F1 on the Time Expression Recognition and Normalization task 4 .

We employed the Apache Lucene search engine for both indexing and retrieving predictions. The statistics of ex-tracted data are as follows. There are 44,335,519 sentences and 548,491 are predictions. There are 939,455 future dates, and an average future date per prediction is 1.7 and the standard deviation is 0.92. Among 1.8 million documents, more than 25% of all documents contain at least one predic-tion (i.e., a reference to the future). In order to determine this percentage over a broader range of news sources, we performed the same analysis on 2.5 million documents from over 100 news sources from Yahoo! News for the one year period from July 2009 to July 2010 and found over 32% of the documents contained at least one prediction.

Future-related queries. There is no gold standard avail-able to evaluate the task of ranking related news prediction. We manually selected 42 query news articles from the New York Times that cover the future-related topics shown in Table 2. The actual queries ( Q E , Q T and Q C ) used for re-trieving predictions are extracted from these news articles.
Relevance assessments. Human assessors were asked to evaluate query/prediction pairs (e.g., relevant or non-relevant) using 5 levels of relevance: 4 for excellent (very relevant prediction), 3 for good (relevant prediction), 2 for fair (related prediction), 1 for bad (non-relevant prediction), and 0 for non prediction (incorrect tagged date). The last option was presented because there are predictions incor-rectly annotated with time (this is an error produced by the annotation tools). More precisely, an assessor was asked to give a relevance score Grade ( q, p, t ) where ( q, p, t ) is a triple of a query q , a prediction p , and a future date t in p . Consider the following prediction about the topic  X  X lobal warming X  and the publication date of the news article is 2007/02/21:
The prediction contains two future dates (as highlighted in bold). Hence, an assessor has to give judges to two triples corresponding to q , p and both future dates. A triple ( q, p, t ) is considered relevant if Grade ( q, p, t )  X  3, and it is con-sidered non-relevant if 1  X  Grade ( q, p, t )  X  2. Relevance level 0 is not included in the evaluation 5 . These judgments are normalized by a query/prediction pair ( q, p ) since we are interested in presenting a prediction for all future dates, re-gardless of their number. That is, a query/prediction pair ( q, p ) is relevant if and only if there is at least one rele-vant triple ( q, p, t ), and a prediction is non-relevant if all triples are non-relevant. Our assumption is that predictions extracted from more recent documents are more relevant.
In total, assessors judged 52 queries and for each one of them we retrieved up to 100 sentences that contained predic-tions. On average 94 sentences with future mentions were retrieved, with an average of 1.2 future dates per predic-tion. Finally, assessors evaluated 4,888 query/prediction pairs (approximately 6,032 of triples) 6 .

Our machine learning ranking models operate in a su-pervised manner, and as such, they need training data for learning. We created training data using cross validation by randomly partitioned query articles into N F folds. We used N F  X  1 query/prediction from other folds for training a ranking mode and the remaining fold for testing. We re-moved queries with zero relevant results, and we obtained N F = 3 , 4 , 5 for Q E , Q C , Q T respectively.

Parameter setting. We set the boost factors on an in-dependent experiment as boost ( text ) = 5 . 0, boost ( context ) = 1 . 0, and boost ( title ) = 2 . 0. We use the recommended values for the constants b = 0 . 75 for all fields, and k 1 = 1 . 2 [26]. For LDA-based features, we trained a yearly model snapshot by selecting 4% of all documents in each year. For each document, we filtered out terms occur-ring in less than 15 documents and the 100 most common terms. We learn a topic model for each document snapshot by employing Stanford Topic Modeling Toolbox 7 , and the number of topics for training LDA N z is fixed to 500 and the number of topics for inference k is 200. A learning algorithm we use is the collapsed variational Bayes approximation to the LDA objective (CVB0LDA) [2]. All other parameters are default values of the topic modeling. Using CVB0LDA Figure 3: P@10 and MAP performance of Q E (left) when v arying top-m entities, and Q C (right) when varying top-n terms. required high CPU and memory, but needed fewer iterations and had faster convergence rates than a collapsed Gibbs sampler [12], which requires less memory during training.
For both T SU 1 and T SU 2 , DecayRate = 0 . 5,  X  = 0 . 5 and = 2 y are used where y the number of years. For both F S 1 and F S 2 , n = 2, m = 2, s min = 4 y and s max = 2 y are used. So,  X  1 = time ( d q )  X  4 y and  X  2 = time ( d q ) + 2 y .
Methods for comparison. We experiment with the three different ways of constructing the query Q E , Q T , and Q
C . The baseline for retrieval is Lucene X  X  default ranking function and our queries incorporate two time constraints as explained in Section 2.4. We re-rank the baseline results us-ing SVM MAP yielding Re -Q E , Re -Q T and Re -Q C . For the application of ranking related news predictions, we prefer top-precision retrieval performance metrics over recall-based metrics: a user will be typically interested in a few top pre-dictions even though there are many predictions retrieved. Consequently, we envision a user interface that contains lit-tle space for displaying related predictions. Thus, we will measure the retrieval effectiveness by the precision at 1, 3 and 10 (P@1, P@3, and P@10 respectively), Mean Recipro-cal Rank (MRR), and Mean Average Precision (MAP). We report the average performance over N F folds to measure the overall performance, for each query type.
The three types of queries ( Q E , Q T , and Q C ) are com-posed of either top-m entities or top-n terms, or both. We first establish which are good m and n values for each one of the types. Instead of varying m and n in re-ranking, we select the m and n that give a reasonable improvement in a hold-out set (where we randomly divided queries into two folds). Therefore, we will use only one fixed version of m and n for the rest of our experiments. We select the values of m and n by performing a preliminary analysis as follows. First, by looking at P@10 and MAP, we select the value of m that yields the best performance using only Q E to retrieve predictions for each varying m . As shown in Figure 3 (left), 9  X  m  X  12 give almost no difference in terms of P@10. In spite of that, we choose the number of entities m = 11 because it is slightly better than the other values. Next, we find the optimal value of n by observing the performance of Q C when m is fixed to 11 and the value of n is varied. As depicted in Figure 3(right), there is very slight difference in P@10 for 9  X  n  X  11; We choose the number of terms n = 10 because it obtains the best in MAP among them.
The retrieval effectiveness of simple methods and their corresponding re-ranking methods are displayed in Table 3. These results are averaged over queries retrieving at least Table 3: The effectiveness of each method when us-ing all queries;  X  ,  X  indicates statistical improvement over the corresponding simple methods using t-test with significant at p &lt; 0 . 1 , p &lt; 0 . 05 respectively. one relevant prediction. In general, Q T gains the highest effectiveness in all measurements followed by Q C and Q E and the feature-based re-ranking approach improves the ef-fectiveness for all query types. In addition, Re -Q C has the highest effectiveness over other re-ranking methods for P@1 and P@3, while Re -Q T gains the highest effectiveness for the rest of all metrics.

Q E and Q C pose a problem in not retrieving any relevant result of our judged pool among the first 100 for a large number of queries, which makes it impossible for the ma-chine learning model to improve the ranking. However, we still want to compare the performance between the different variations of the query ( Q E , Q C , Q T ). Therefore, we use a subset of queries that contained at least one relevant result among all the different methods. The results are shown in Table 4 where we compare all other methods against Q E because we have observed that Q E performs worst among them. As seen from the results of each re-ranking method, our proposed features improve the effectiveness for all cor-responding simple methods. In particular, the re-ranking method Re -Q C outperforms the simple method Q E signif-icantly. However, Re -Q E did not provide a significant im-provement over Q E . The results show that, for the same set of queries, using entities alone are limited while terms alone are able to retrieve most of relevant predictions.
Interestingly, when looking at the same sub-set of queries with relevant predictions, the re-ranking approach Re -Q outperforms every other method, even if the plain retrieval Q
T is superior to Q C . This is an indicator that entity-based features are able to produce higher quality results but only for a certain type of topics. We performed an error analysis to determine why Q E is unable to retrieve relevant predic-tions. In general, Q E fails for a topic that cannot be rep-resented using only people, locations, or organizations. For example, for the topic about  X  X he Europeans agreement of gas emissions X , the top-5 Q E is h European Union , Brussels , Finland , Germany , Hungary i and the top-5 Q T is h european , emission , target , climate , brussels i . In this case, Q E is unable to represent the key terms  X  X mis-sion X  and  X  X limate X , and thus fails to retrieve many relevant predictions that match those terms.

Similarly, for the query topic about  X  X linton health care reform X , Q E is represented using the named entity Clinton (the terms  X  X ealth care X  and  X  X eform X  are not annotated as entities). When matching, all predictions containing the en-tity Clinton are matched which will return many documents that are not related to  X  X ealth care X  and  X  X eform X . Table 4: The effectiveness of each method when us-ing a subset of queries;  X  ,  X  ,  X  indicates statistical im-provement over the method Q E using t-test with sig-nificant at p &lt; 0 . 1 , p &lt; 0 . 05 , p &lt; 0 . 01 respectively. Table 5: Top-5 features with highest weights and low est weights for each query type.

We analyzed feature weights obtained from the learning algorithm SVM MAP in order to understand better what is the importance of the different features,. Note that, in or-der to compare the weights among different queries, we per-formed normalization by diving with the maximum value of all weights for each query. Column w i in Table 5 displays the top-5 features with highest and lowest weights for each query type.

At least two topic-based features of all query types are in the top-5 features with highest weight, and therefore topic-based features play an important role in the re-ranking model. Although retScore and bm25f measure the similarity on a term level, they help to re-rank predictions when incorpo-rated into the machine learning model. as seen in the top-5 features for Q T and Q C . The feature that received the highest importance value for the Q E type is tagSim , which measures the similarity between entities in a prediction and manually tagged entities. This indicates that tagged entities in a query document can precisely represent user informa-tion needs. The temporal features FS 1 and FS 1 also play an important role for Q E .

Features in top-5 features with lowest weights are those from the entity-based class. Recall that these features are extracted in order to measure the importance of entities an-notated in a prediction with respect to their respective par-ent documents. However, the results show that these fea-tures are not good enough for discriminating between rele-vant and non-relevant predictions.
Our related work includes sentence retrieval, entity rank-ing, temporal ranking, and domain-specific predictions.
Sentence retrieval is the task of retrieving a relevant sen-tence related to a query. Different application areas of sen-tence retrieval are mentioned in the book of Murdock [24] and references therein, including, for example, question an-swering [30], text summarization, and novelty detection. Sur-deanu et al. [30] applied supervised learning to rank a set of short answers (sentences) matched a given question (query) by using different classes features. Li and Croft [19] pro-posed to detect novelty topics by analyzing sentence-level information (sentence lengths, named entities, and opinion patterns). Generally, because sentences are much smaller than documents and thus have limited content compared to documents, the effectiveness of the retrieval of sentences is significantly worse. To address this problem, Blanco and Zaragoza [6] proposed to use the context of sentences in or-der to improve the effectiveness of sentence retrieval.
There have been a number shared tasks with the goal of furthering research in the area of entity ranking. For in-stance, the TREC 2008 Enterprise track was created with the objective to find experts (or people) related to a given topic of interest. The INEX Entity Ranking track [10] was launched with the task of finding a list of relevant entities (represented by Wikipedia articles) for a given topic. Re-cently, the TREC 2009 Entity track was introduced, and the task is to find related entities (represented by home-pages) given a topic (called a source entity). The difference between the TREC 2009 Entity and the previous tracks is that it allows a relation and a target entity type to be ex-plicitly specified. There are various approaches to ranking entities by using language models [4], voting models [21], and entity-based graph models [35].

Many ranking models exploiting temporal information have been proposed, including [5, 11, 18, 23]. Li and Croft [18] experimented with time-based language models by assigning a document prior using an exponential decay function of its creation date, such that the more recent documents obtain the higher probabilities of relevance. Diaz and Jones [11] build a temporal profile of a query from the distribution of document publication dates. They use time dependent fea-tures derived from these profiles that improve the ranking of temporal queries.

Berberich et al. [5] integrated temporal expressions into query-likelihood language modeling, which considers uncer-tainty inherent to temporal expressions in a query and in documents, i.e., two temporal expressions can refer to the same time interval even when they are not exactly equal. Metzler et al. [23] mined query logs to identify implicit tem-poral information needs and presented a time-dependent ranking model for certain types of queries.

There is much research in domain-specific predictions such as stock market predictions [27, 33] and recommender sys-tems [17, 25]. The first aims at predicting stock price move-ments by analyzing financial news, while the latter applies collaborative filtering algorithms for recommending books, videos, movie, etc. based on users X  interests.
 The future retrieval problem was first presented by Baeza-Yates [3]. He proposed to extract temporal expressions from news, index news articles together with temporal expres-sions, and retrieve future information (composed of text and future dates) by using a probabilistic model. A document score is given as a multiplication of a keyword similarity and a time confidence, i.e., a probability that the document X  X  events will actually happen. The limitation of this original work is that it is evaluated using a small data set and only a year granularity is used.
The more recent work on the future-related information retriev al is presented by Jatowt et al. [13]. In contrast to our work, they do not focus on relevance and ranking future-related information retrieval. They presented an analyti-cal tool for extracting, summarizing and aggregating future-related events from news archives, but did not perform an extensive evaluation, only calculating averaged precision on a small set of generated results.
In this paper, we demonstrated that future related infor-mation is abundant in news stories and defined the task of ranking related future predictions . The main goal of this task is to improve user access to this information by selecting the predictions from a news archive that are most relevant to a given news article. We created an evaluation dataset with over 6000 relevance judgments and addressed this task using a learning to rank methodology incorporating four classes of features including term similarity, entity-based similarity, topic similarity, and temporal similarity that outperforms a strong baseline system. Finally, we performed an in-depth analysis of feature importance. Possible future work includes time-dependent query classification using query logs, com-bining multiple sources (Wikipedia, blogs, home pages, and tweets) of future-related information, sentimental analysis for future-related information and evaluating the effective-ness of the predictions in a real-world news application.
We would like to thank Hugo Zaragoza for his help at the early stages of this paper. This work is partially supported by the EU Large Scale Integrated Project LivingKnowledge (contract no. 231126).
