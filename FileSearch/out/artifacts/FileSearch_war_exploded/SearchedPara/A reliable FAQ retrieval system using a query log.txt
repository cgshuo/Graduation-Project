 1. Introduction
FAQ (frequently asked question) retrieval plays an increasingly important role in e-commerce websites because FAQs accommodate both customer needs and business requirements. As a useful tool for information access, most commercial sites provide customers with a keyword search. However, sometimes the keyword search does not perform well in FAQ retrieval because FAQ collections consist of ordinary lists which have
Instead, the information suppliers construct question candidates in advance by using their own knowledge and answer the question candidates. However, the question candidates do not always satisfy users X  needs. Second, each FAQ consists of a small number of words, unlike ordinary documents. Information suppliers choose words in each FAQ according only to their knowledge. However, the words may not be properly used in the FAQs. These deficiencies may raise lexical disagreement problems in keyword search. For example, the query  X  X  X ow can I remove my login ID X  X  and the FAQ  X  X  X  method to secede from the membership X  X  have a very similar meaning, but there is no overlap between the words in the two sentences. This fact often makes keyword search systems misdirect users to irrelevant FAQs with one or more common words such as  X  X  X ow to create login ID X  X  and  X  X  X ow can I change my password X  X .
 The representative FAQ retrieval systems are FAQ Finder ( Hammond, Burke, Martin, &amp; Lytinen, 1995 ),
Auto-FAQ ( Whitehead, 1995 ), and Sneiders X  system ( Sneiders, 1999 ). FAQ Finder was designed to improve navigation through already existing external FAQ collections. To match users X  queries to the FAQ collections,
FAQ Finder uses a syntactic parser to identify verb and noun phrases. Then, FAQ Finder performs concept matching using semantic knowledge like WordNet ( Miller, 1990 ). Auto-FAQ matches users X  queries to prede-fined FAQs using a keyword comparison method based on shallow NLP (natural language processing) tech-niques. Sneiders X  system classifies keywords into three types: required keywords, optional keywords and irrelevant keywords. Sneiders X  system retrieves and ranks relevant FAQs according to the three types.
Although the representative systems perform well, it is a time and effort consuming job to construct these knowledge bases whenever application domains are changed. To reduce these bothersome efforts, we propose a FAQ retrieval system using only query logs as knowledge sources because the query logs have two charac-teristics: (1) we can easily collect users X  query logs, and (2) we can observe similar meanings of words in var-ious query logs.
 There have been numerous studies on how clustering can be employed to improve retrieval results ( Liu &amp; advance, independent of the user X  X  query, and clusters are retrieved based on how well their centroids match the user X  X  query. The query specific clustering methods group the set of documents retrieved by an IR system for a query. The main goal of the query specific clustering methods is to improve the rankings of relevant doc-uments on searching time. Some studies have shown that cluster-based retrieval did not outperform docu-ment-based retrieval, except with the small size of collection ( El-Hamdouchi &amp; Willet, 1989; Tombros system use query log clusters as a form of document smoothing ( Liu &amp; Croft, 2004 ) because the size of FAQ collections is probably much smaller than the size of ordinary document collections.

This paper is organized as follows. In Section 2 , we propose a cluster-based FAQ retrieval system using query logs as knowledge sources. In Section 3 , we explain experimental results. Finally, we draw some con-clusions in Section 4 . 2. FRACT: a FAQ retrieval and clustering techniques
If we can automatically group similar meanings of query logs, we may be able to use the query log clusters as informative knowledge sources in FAQ retrieval because the query log clusters will include various surface forms of sentences that reflect users X  preferences for specific words. Based on this assumption, we propose a cluster-based FAQ retrieval system called FRACT (Faq Retrieval And Clustering Technique). FRACT con-sists of two sub-systems: a query log clustering system and a cluster-based retrieval system. The query log clus-each FAQ as an independent category and classifies the query logs into the FAQ categories by using a vector similarity measure in latent semantic space. Based on the classification results, the query log clustering system groups the query logs and computes centroids of each query log cluster. When a user inputs his/her query, the cluster-based retrieval system calculates the similarities between the query and FAQs smoothed by the query FAQs.
 2.1. Vector representation of indexing terms
In linguistics, a syntagmatic lexical affinity, also termed a lexical relation, between two units of language stands for a correlation of their common appearance in the sentence ( Saussure, 1949 ). The observation of lex-ical affinities in large textual corpora has been shown to convey information on both syntactic and semantic ities are extracted from a text by paring it, since two words share a lexical affinity if they are involved in a modifier X  X odified relation.

In IR (information retrieval), sentences are generally represented as a set of unigrams, but the unigrams do not provide contextual information between co-occurring words. A possible solution to this problem is to sup-very efficient, and the number of dependency bigrams extracted by parsing is not enough to measure similarity between sentences if we do not have a large corpus. Therefore, we apply a sliding window technique ( Maarek,
Berry, &amp; Kaiser, 1991 ) to FRACT in order to robustly extract co-occurrence information. The sliding window technique consists of sliding a window over the text and storing pairs of words involving the head of the win-dow if it is a content word and any of the other content words of the window. The window slides word by word from the first word of the sentence to the last, the size of the window decreasing at the end of the sen-tence so as not to cross boundaries between sentences. The window size being smaller than a constant, the number of extracted bigrams is linear to the number of unigrams in the sentence. In the experiment, we set the window size to three. Fig. 1 shows the process of term extraction with examples.

After extracting terms, FRACT assigns weight scores to each term according to the weighting scheme of the 2-poisson model ( Robertson &amp; Walker, 1992 ), as shown in Eq. (1) .
In Eq. (1) , w ij is the weight score of the j th term in the i th document, number of documents, and df j is the number of documents including the j th term. k for performance tuning. 2.2. Query log clustering system using LSA The similarities between documents can be calculated by popular methods such as the cosine measure, the overlap between the words in the sentences. LSA (latent semantic analysis) is a method of extracting and rep-resenting the contextual-usage meaning of words by statistical computations ( Landauer, Foltz, &amp; Laham, 1998 ). Some researchers have shown that LSA can bridge some lexical gaps between two words by mapping all terms in the texts to a representation in so-called latent semantic space. Based on this fact, we apply the
LSA techniques to FRACT in order to increase the performance of query log classification. The LSA pro-cesses are as follows. First, FRACT constructs an m  X  n term-document matrix X of terms, n is the number of documents, and an element w ij ciation between the i th term and the j th document, as shown in Eq. (1) . Then, FRACT applies SVD to the term-document matrix X m  X  n , as shown in Eq. (2) .
 where U m  X  m is an m  X  m orthonormal matrix, and V n  X  n positive matrix whose nonzero values are s 11 , ... , s rr descending order s 11 P s 22 P P s rr &gt; 0. After applying SVD, FRACT reduces X top r -dimensions, and obtains a pseudo-document matrix, V semantic space, as shown in Eq. (3) . Fig. 2 illustrates a pseudo-document matrix that is generated according to Eq. (3) .

In Fig. 2 , the shaded portions of the matrices are what we use as the basis for its term and document vector representations. As shown in Fig. 2 , FRACT selects the top r elements among diagonal elements in S reduce the representation dimension. Therefore, the actual representations of the term and document vectors
When the pseudo-document matrix has been constructed, to increase the performance of query log classi-fication, FRACT compares FAQ vectors to query log vectors in the latent semantic space (not in the original vector space) by using cosine similarity measure, as shown in Eq. (4) ( Salton &amp; McGill, 1983 ).
In Eq. (4) , f i k and ql j k are the k th element values of the i th FAQ vector f r -dimensional pseudo-document matrix V n  X  r  X  S r  X  r . After comparing FAQ vectors to query log vectors,
FRACT classifies each query log vector into FAQ categories
FRACT generates centroid vectors of each FAQ category according to Eq. (5) and construct the centroid ma-trix C fn  X  r , where fn is the number of FAQ categories, by gathering the centroid vectors.
In Eq. (5) , c i is the centroid vector of cat i that is the category of the i th FAQ vector f vectors (i.e. a FAQ vector and query log vectors) belonging to cat centroid vectors from leaning excessively toward query logs that may be misclassified. Finally, FRACT re-stores the representation dimension to original m dimension, as shown in Eq. (6) .
In Eq. (6) , FRACT sets the element values smaller than zero to zero in C are not directly calculated according to the actual term occurrences but are estimated according to LSA techniques. 2.3. Cluster-based FAQ retrieval system
The similarity in vector space models is determined by using associative coefficients based on the inner product of the document vector and query vector, where word overlap indicates similarity. The inner product is usually normalized. The most popular similarity measure is the cosine coefficient, which measures the angle between the document vector and the query vector. In other words, given a query and FAQs, we can calculate ever, we may obtain very low similarities because there is often little overlap between the query vector and the
FAQ vector. To overcome this problem, FRACT smoothes representations of the FAQ vectors using the latent term weights and calculates cosine similarities between the queries and the smoothed FAQ vectors, as shown in Eqs. (7) and (8). and has a value between zero and one. max_val( x ) is a normalizing factor that represents the maximum one among the element values of the vector x . According to the normalizing scheme, we expect that the vectors will lean toward axes with high term weights and the elements with high term weights will take some advantages, as shown in Fig. 3 .

Based on Eq. (8) , we believe that FRACT can alleviate the lexical disagreement problems, as FRACT uti-lizes the centroids of FAQ categories, including much more terms than FAQs themselves, as smoothing fac-tors. For example, if FRACT has the latent centroid vector [ how:0.1, membership:0.5, method:0.4, remove:0.1, secede:0.9 ] associated with the FAQ  X  X  X  method to secede from the membership X  X , FRACT may return the FAQ  X  X  X  method to secede from the membership X  X  with a high rank when users input the query  X  X  X ow can
I remove my login ID X  X  because there are two common words between the query and the latent centroid although there is no common word between the query and the original FAQ. 3. Evaluation experiments 3.1. Data sets and experimental settings We collected 406 Korean FAQs from three domains; LGeShop ( www.lgeshop.com , 91 FAQs), Hyundai
Securities ( www.youfirst.co.kr , 81 FAQs) and KTF ( www.ktf.com , 234 FAQs). LGeShop is an internet shop-ping mall, Hyundai Securities is a security corporation, and KTF is a mobile communication company. For two months, we also collected a large amount of query logs that were created by commercial search engines installed in the above websites for FAQ retrieval. After eliminating additional information such as IP, date and time except users X  queries, we automatically selected 5845 unique query logs (1549 query logs from LGe-
Shop, 744 query logs from Hyundai Securities, and 3552 query logs from KTF) that consisted of two or more content words. Then, we manually classified query logs into the 406 FAQ categories and annotated each query log with the identification numbers of the FAQ categories. Finally, we constructed a test collection called KFAQTEC (Korean test collection for evaluation of FAQ retrieval systems). KFAQTEC consists of 406 FAQs and 5845 query logs. The number of content words per query is 5.337. Table 1 shows a sample of KFAQTEC.

The manual annotation was done by graduate students majoring in language analysis and was post-processed for consistency.

To experiment with FRACT from the various viewpoints, we reorganized KFAQTEC into three types of data sets; FAQSET-1, FAQSET-2 and FAQSET-3. In FAQSET-1, the query logs were divided into 10-folds, and each fold used 9/10 of the query logs and all FAQs for system building, and 1/10 of the query logs for system testing. Using FAQSET-1, we evaluated performances of FRACT, as compared with conventional IR systems. In FAQSET-2, the query logs were divided into 10-folds, and each fold used 9/10 of the query logs and fake FAQs (1/10 of the query logs) for system building, and real FAQs for system testing. The fake FAQs were used as both retrieval target sentences and categories for query log classification as if they were real
FAQs pre-constructed by information suppliers. Using FAQSET-2, we evaluated the robustness of FRACT according to word combinations of FAQs. In FAQSET-3, the query logs were divided into 10-folds, and each fold used various numbers of the query logs (from 1/10 to 9/10 of query logs) and all FAQs for system build-ing, and 1/10 query logs for system testing. Using FAQSET-3, we evaluated the performances of FRACT according to the sizes of training data for query log clustering. In all our experiments, we set k
Eq. (1) to 1.2 and 0.75 respectively according to Okapi BM-25 ( Robertson, Walker, Jones, Beaulieu, &amp; Gat-ford, 1994 ). We also set the reduced dimension r in Eq. (3) to 200, the threshold value h in Eq. (5) to 0.3, and the smoothing rate k in Eq. (7) to 0.7. 3.2. Evaluation methods To evaluate the performances of the query log clustering system, we computed the F 1 measure, as shown in Eq. (9) .

In Eq. (9) , P is the precision that means proportion of correct ones out of returned query logs. R is the recall rate that means proportion of returned query logs out of classification targets. To evaluate the performances of the cluster-based retrieval system, we computed the MRRs (Mean Reciprocal Rank) and the miss rates. The
MRR represents the average value of the reciprocal ranks of the first relevant FAQs given by each query, as shown in Eq. (10) .

In Eq. (10) , rank i is the rank of the first relevant FAQ given by the i th query, and num is the number of que-ries. The miss rate means the ratio of the cases that the searching engine fails to return relevant FAQs, as shown in Eq. (11) .
 3.3. Evaluation results
To evaluate seriousness of lexical disagreement problems in short document retrieval, we analyze the degrees of word overlaps between query logs and relevant FAQs given by the query logs, as shown in Table 2 .
 As shown in Table 2 , we found that 1338 query logs (22.9% of query logs) did not overlap with original
FAQs at all and 56.4% of query logs had overlaps less than one word. This fact reveals that the lexical dis-agreement problems often occur in short document retrieval.

To evaluate effectiveness of latent term weights, we implemented two versions of FRACT that perform query log classifications in different vector spaces (i.e. latent semantic space and original term-document space). Then, we compared the performances on each version of FRACT by using FAQSET-1, as shown in Table 3 .

As shown in Table 3 , FRACT using latent term weights highly outperformed the other FRACT using ori-ginal term weights. This fact reveals that the proposed latent term weights hold more effective information. To evaluate performances of FRACT, we compared FRACT with conventional IR systems by using FAQ-SET-1, as shown in Table 4 .

In Table 4 , OKAPI is the Okapi BM25 retrieval model ( Robertson et al., 1994 ), and LM is the KL-diver-gence language model using JM smoothing ( Zhai &amp; Lafferty, 2001 ). We implemented these IR systems using
LeMur Toolkit version 3.0 ( The Lemur project ). As shown in Table 4 , FRACT outperforms all comparison systems in both the average MRR and the average miss rate. Specifically, FRACT reduced the average miss rate by 0.196. In the experiment, it is difficult to compare FRACT directly with the other systems because the other systems do not use query log information. Even if direct comparisons are impossible, we think that the proposed method can be an effective solution to the lexical disagreement problems between queries and FAQs. As an additional experiment, we observed changes of ranks on the basis of top-10 in comparison with OKAPI.
As shown in Table 5 , FRACT made about 60.3 relevant FAQs ranked into top-10. Moreover, FRACT ranked about 129.3 relevant FAQs in top-10 that OKAPI could not find at all. This fact reveals that FRACT highly ranks relevant FAQs more than OKAPI.

To evaluate the robustness of FRACT according to the word combinations of FAQs, we compared the miss rates between FRACT and OKAPI by using FAQSET-2, as shown in Fig. 4 .
 As shown in Fig. 4 , FRACT is less sensitive to word combinations than the representative IR system, OKAPI.

To evaluate changes of system performances according to the number of query logs, we computed the per-formances of FRACT by using FAQSET-3, as shown in Fig. 5 .
 As shown in Fig. 5 , FRACT slowly increases in MRR while the number of query logs is growing. 3.4. Failure analysis
We analyzed the cases where FRACT failed to highly rank relevant FAQs. We found some reasons why the relevant FAQs were low ranked or missed. First, there were still the lexical disagreement problems between users X  queries and FAQs. FRACT could resolve some lexical disagreement problems because it used query log clusters in order to smooth the FAQs. However, we found many cases where there was very little overlap between the words in queries and the words in query log clusters. To solve this problem at a basic level, we need to study new methods that match users X  queries with FAQs on the semantic levels. Second, there were some cases where only one query was associated with several FAQs. In these cases, we could not select the
FAQs that were entirely relevant to those queries. To solve this problem, information suppliers should accu-rately construct initial FAQs and should constantly update the FAQs. Third, there were some cases where several relevant FAQs were much lower ranked, as compared with OKAPI. To solve this problem, we need to study new methods that effectively combine latent term weights with original term weights. Finally, there were some cases where irrelevant FAQs were returned because of syntactically inadequate bigrams. To solve this problem, we need to develop high-performance dependency parser and replace the simple bigrams with dependency bigrams. 4. Conclusion
In IR, query logs may be informative knowledge sources because we can observe various surface forms of sentences with similar meanings. Based on this merit of query logs, we proposed a high-performance FAQ retrieval system using query log clusters as smoothing factors. Using LSA techniques during the indexing time, the FAQ retrieval system effectively groups query logs and generates cluster centroids containing latent term weights. When a user inputs a query, the FAQ retrieval system smoothes retrieval target FAQs using the clus-FAQ retrieval system could resolve some lexical disagreement problems regardless of word combinations of
FAQs. We believe that the FAQ retrieval system is more practical and reliable than the previous FAQ retrieval systems because it does not require high-level knowledge sources like thesauruses and handcrafted rules. Acknowledgement
This study was supported by 2006 Research Grant from Kangwon National University. It was also par-tially supported by Kangwon Institute of Telecommunications and Information (KITI).
 References
