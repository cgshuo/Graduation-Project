 Statistical models with hidden or latent variables are of great importance in natural language processing, speech, and many other fields. The EM algorithm is a remarkably successful method for parameter esti-mation within these models: it is simple, it is often relatively efficient, and it has well understood formal properties. It does, however, have a major limitation: it has no guarantee of finding the global optimum of the likelihood function. From a theoretical perspec-tive, this means that the EM algorithm is not guar-anteed to give consistent parameter estimates. From a practical perspective, problems with local optima can be difficult to deal with.

Recent work has introduced polynomial-time learning algorithms (and consistent estimation meth-ods) for two important cases of hidden-variable models: Gaussian mixture models (Dasgupta, 1999; Vempala and Wang, 2004) and hidden Markov mod-els (Hsu et al., 2009). These algorithms use spec-tral methods: that is, algorithms based on eigen-vector decompositions of linear systems, in particu-lar singular value decomposition (SVD). In the gen-eral case, learning of HMMs or GMMs is intractable (e.g., see Terwijn, 2002). Spectral methods finesse the problem of intractibility by assuming separabil-ity conditions. For example, the algorithm of Hsu et al. (2009) has a sample complexity that is polyno-mial in 1 / X  , where  X  is the minimum singular value of an underlying decomposition. These methods are not susceptible to problems with local maxima, and give consistent parameter estimates.

In this paper we derive a spectral algorithm for learning of latent-variable PCFGs (L-PCFGs) (Petrov et al., 2006; Matsuzaki et al., 2005). Our veloped in this paper are quite general, and should be relevant to the development of spectral methods for estimation in other models in NLP, for exam-ple alignment models for translation, synchronous PCFGs, and so on. The tensor form of the inside-outside algorithm gives a new view of basic calcula-tions in PCFGs, and may itself lead to new models. For work on L-PCFGs using the EM algorithm, see Petrov et al. (2006), Matsuzaki et al. (2005), Pereira and Schabes (1992). Our work builds on meth-ods for learning of HMMs (Hsu et al., 2009; Fos-eral extensions: in particular in the tensor form of the inside-outside algorithm, and observable repre-consider spectral learning of finite-state transducers; Lugue et al. (2012) considers spectral learning of head automata for dependency parsing. Parikh et al. (2011) consider spectral learning algorithms of tree-structured directed bayes nets. Given a matrix A or a vector v , we write A  X  or v  X  for the associated transpose. For any integer n  X  1 , row or column vector y  X  R m , we use diag( y ) to refer to the ( m  X  m ) matrix with diagonal elements equal to y ments equal to 0 . For any statement  X  , we use [[ X ]] and 0 if  X  is false. For a random variable X , we use E [ X ] to denote its expected value.
 We will make (quite limited) use of tensors: m sor C , and a vector y  X  R m , we define C ( y ) to be the ( m  X  m ) matrix with components [ C ( y )] P y  X  R m to a matrix C ( y ) of dimension ( m  X  m ) .
In addition, we define the tensor C
Finally, for vectors x,y,z  X  R m , xy  X  z  X  is the is analogous to the outer product: [ xy  X  ] the the hidden variable for the left-hand-side of rule r Each h
Define a side of rule r to be the index of the rule above node i in the tree. Define L  X  [ N ] to be the set of nodes in the tree which are the left-child of some parent, and R  X  [ N ] to be the set of nodes which are the right-child of some parent. The probability mass function (PMF) over full trees is then
The PMF over s-trees is p ( r P
In the remainder of this paper, we make use of ma-trix form of parameters of an L-PCFG, as follows:  X  For each a  X  b c  X  R , we define Q a  X  b c  X  m  X  m to be the matrix with values q ( a  X  b c | h,a ) for h = 1 , 2 ,... m on its diagonal, and 0 values for its off-diagonal elements. Similarly, for each a  X  P , x  X  [ n ] , we define Q a  X  x  X  R m  X  m to be the matrix with values q ( a  X  x | h,a ) for h = 1 , 2 ,... m on its diagonal, and 0 values for its off-diagonal elements.  X  For each a  X  b c  X  R , we define S a  X  b c  X   X  For each a  X  b c  X  R , we define T a  X  b c  X   X  For each a  X  I , we define the vector  X  a  X  R m where [  X  a ] Given an L-PCFG, two calculations are central: 3. A vector c 1
The following theorem gives conditions under which the algorithms are correct: Theorem 1 Assume that we have an L-PCFG with that there exist matrices G a  X  R ( m  X  m ) for all a  X  N such that each G a is invertible, and such that: 1. For all rules a  X  b c , C a  X  b c ( y ) = 2. For all rules a  X  x , c  X  3. For all a  X  I , c 1 Then: 1) The algorithm in figure 2 correctly com-putes p ( r rithm in figure 3 correctly computes the marginals  X  ( a,i,j ) under the L-PCFG.

Proof: See section 9.1. A crucial result is that it is possible to directly esti-mate parameters C a  X  b c , c  X  conditions in theorem 1, from a training sample con-sisting of s-trees (i.e., trees where hidden variables are unobserved). We first describe random variables underlying the approach, then describe observable representations based on these random variables. 6.1 Random Variables Underlying the Approach Each s-tree with N rules r will use the s-tree in figure 1 as a running example.
Each node has an associated rule: for example, node 2 in the tree in figure 1 has the rule NP  X  D N . If the rule at a node is of the form a  X  b c , then there are left and right inside trees below the left child and right child of the rule. For example, for node 2 we have a left inside tree rooted at node 3 , and a right inside tree rooted at node 4 (in this case the left and right inside trees both contain only a single rule pro-duction, of the form a  X  x ; however in the general case they might be arbitrary subtrees).

In addition, each node has an outside tree. For node 2, the outside tree is child of node i , and the right child of node i respec-tively. (E.g., A  X  O is the outside tree at node i .  X  B is equal to 1 if node i is at the root of the tree (i.e., i = 1 ), 0 otherwise.

If the rule r the form a  X  x , we have random vari-ables R H
We assume a function  X  that maps outside trees o node in question, the word following the node in question, and so on. We also assume a function  X  As one example, the function  X  might be an indica-tor function tracking the rule production at the root of the inside tree. Later we give formal criteria for what makes good definitions of  X  ( o ) of  X  ( t ) . One requirement is that d  X   X  m and d  X  m .

In tandem with these definitions, we assume pro-for all a  X  N . We then define additional random variables Y where a Note that Y 6.2 Observable Representations Given the definitions in the previous section, our representation is based on the following matrix, ten-sor and vector quantities, defined for all a  X  N , for all rules of the form a  X  b c , and for all rules of the form a  X  x respectively: D d  X  a  X  x = E h [[ R 1 = a  X  x ]] Z  X  | A 1 = a i Assuming access to functions  X  and  X  , and projec-tion matrices U a and V a , these quantities can be es-set of s-trees (see section 7).
 Theorem 2 Assume conditions 1 and 2 are satisfied. For all a  X  N , define G a = ( U a )  X  I a . Then under the definitions in Eqs. 2-4: 1. For all rules a  X  b c , C a  X  b c ( y ) = 2. For all rules a  X  x , c  X  3. For all a  X  N , c 1
Proof: The following identities hold (see sec-tion 9.2):
Under conditions 1 and 2,  X  a is invertible, and ( X  identities in the theorem follow immediately. Figure 4 shows an algorithm that derives esti-mates of the quantities in Eqs 2, 3, and 4. As input, the algorithm takes a sequence of tuples ( r
These tuples can be derived from a training set consisting of s-trees  X   X   X  i  X  [ M ] , choose a single node j i uniformly at random from the nodes in  X  rule at node j j inside tree under the left child of node j is the inside tree under the right child of node j r node j
Under this process, assuming that the s-trees  X  ... X  M are i.i.d. draws from the distribution p (  X  ) over s-trees under an L-PCFG, the tuples ( r from the joint distribution over the random variables R
The algorithm first computes estimates of the pro-jection matrices U a and V a : following lemma 1, and then taking SVDs of each  X  a . The matrices are then used to project inside and outside trees applicable in deriving results for the case where  X   X  a is used in place of  X  a .

Proof sketch: The proof is similar to that of Foster under the assumptions of the theorem, the estimates  X  c ,  X  d  X  values being estimated. The second step is to show
The method described of selecting a single tuple ( r analysis underlying theorem 3. In practice, an im-plementation should most likely use all nodes in all trees in training data; by Rao-Blackwellization we know such an algorithm would be better than the one presented, but the analysis of how much better would be challenging. It would almost certainly lead to a faster rate of convergence of  X  p to p . method. The most obvious is parsing with L-cases where EM has traditionally been used, for ex-ample in semi-supervised learning. Latent-variable HMMs for sequence labeling can be derived as spe-cial case of our approach, by converting tagged se-quences to right-branching skeletal trees.

The sample complexity of the method depends on the minimum singular values of  X  a ; these singular values are a measure of how well correlated  X  and  X  are with the unobserved hidden variable H perimental work is required to find a good choice of values for  X  and  X  for parsing. This section gives proofs of theorems 1 and 2. Due to space limitations we cannot give full proofs; in-stead we provide proofs of some key lemmas. A long version of this paper will give the full proofs. 9.1 Proof of Theorem 1 First, the following lemma leads directly to the cor-rectness of the algorithm in figure 2: Lemma 2 Assume that conditions 1-3 of theorem 1 are satisfied, and that the input to the algorithm in figure 2 is an s-tree r to be the non-terminal on the left-hand-side of rule r , and t at its root. Finally, for all i  X  [ N ] , define the row It follows immediately that
This lemma shows a direct link between the vec-tors f i calculated in the algorithm, and the terms b i which are terms calculated by the conventional in-side algorithm: each f i is a linear transformation (through G a i ) of the corresponding vector b i . Proof: The proof is by induction.

First consider the base case. For any leaf X  X .e., for any i such that a
The inductive case is as follows. For all i  X  [ N ] such that a f vector with components  X  r P with components equal to  X  l P  X  diag(  X  l ) Q r i is a row vector with components equal to  X  r
But b i
P  X  diag(  X  l ) Q r i = b i and the inductive case follows immediately from Eq. 10.

Next, we give a similar lemma, which implies the correctness of the algorithm in figure 3:
