 For time-relevant multi-dimensional data sets (MDS), users usually pose a huge amount of data due to the large dimen-sionality, and approximating query processing has emerged as a viable solution. Speci fi cally, the cube streams handle MDSs in a continuous manner. Traditional cube approxi-mation focuses on generating single snapshots rather than continuous ones. To address this issue, the application of generating snapshots for cube streams, called SCS, is in-vestigated in this paper. Such an application collects data events for cube streams on-line and generates snapshots with limited resources in order to keep the approximated infor-mation in synopsis memory for further analysis. As com-pared to OLAP applications, the SCS ones are subject to much more resource constraints for both processing time and memory and cannot be dealt with by existing methods due to the limited resources. In this paper, the DAWA algo-rithm, standing for a hybrid algorithm of Dct for Data and discrete WAvelet transform, is proposed to approximate the cube streams. The DAWA algorithm combines the advan-tage of high compression rate from DWT and that of low memory cost from DCT. Consequently, DAWA costs much smaller working bu ff er and outperforms both DWT-based and DCT-based methods in execution e ffi ciency. Also, it is shown that DAWA provides answers of good quality for SCS applications with a small working bu ff er and short ex-ecution time. The optimality of algorithm DAWA is theo-retically proved and also empirically demonstrated by our experiments.
 Categories and Subject Descriptors: H.2.8 [Database Applications]: Data mining General Terms: Algorithms, Management Keywords: Cube Streams, OLAP, Data Cubes, Data Streams
Since the concepts of OLAP techniques and data cubes were introduced in [4], a large number of data warehouses and data cubes have been constructed and deployed in ap-plications. In addition, data cube has become an important component in most data warehouse systems and in Decision-Support-Systems. Answering range queries is one of the primary tasks in OLAP applic ations. For example, when exploring marketing databases, users may be interested in discovering the total purchases of several products at all branches in California during last three days.

In addition, some applications, such as phone call analy-sis in cellular phone companies or sales volume monitoring in retails, need a faster mechanism to process continuously incoming numbers. Those applications handle stream data in multidimensional form rather than the static ones han-dled by traditional OLAP applications. For those multi-dimensional data streams, or cube streams, OLAP queries require complex operations over gigabytes of data and take very long time to produce exact answers. Thus, the ap-proximate query processing over cube streams becomes a viable solution. Also, the volume of data is usually too huge to be stored on permanent devices or to be scanned thor-oughly more than once. It is hence recognized that both approximation and adaptability are key ingredients for ex-ecuting queries over rapid multidimensional data streams. Such applications collect data events for multidimensional data sets (MDSs) on-line and generate snapshots with lim-ited resources in order to keep the approximated information in synopsis memory for further analysis. Those applications can be regarded as generating snapshots for cube streams (SCS). Like OLAP applications, those SCS ones have a num-ber of scenarios where a user may prefer a snapshot in a few seconds rather than the exact answer that requires tens of minutes to compute [16]. Speci fi cally, the consideration for SCS applications is di ff erentfromthatfortraditionalOLAP applications in two important aspects. First, the resources for both processing time and memory are much more con-strained than that for o ff -line cube construction, i.e., cube streams must be processed e ffi ciently with a small work-ing bu ff er in order to deal with the rapid growth of data events. Secondly, those data events are time-relevant in na-ture and thus are appropriate to be modeled as brown noise [2]. As a result, an e ffi cient algorithm with the capability of compressing multidimensional data streams within small working bu ff er in one data scan is required to address such aproblem.

In this paper, we explore the generation of snapshots for cube streams in a small working bu ff er with one data scan and also the store of snapshots in synopsis memory. Since the original data sets grows continuously, the snapshots should be captured periodically for each given time period and be Figure 1: The system model for processing multi-dimensional data streams. processed rapidly. Figure 1 shows an extension of the com-putational model for data streams proposed in [8].
For an SCS application, the data events are collected pro-gressively and being processed in the working bu ff er un-til the pre-de fi ned time interval is reached. The sub-block segmented by a time interval of a cube stream is called a processing block (PB). Note that the processed data in work-ing bu ff er should be compressed further in order to keep as more snapshots of PBs as possible. Hence, the size of working bu ff er, denoted as B , is usually much smaller than the cardinality of a PB, denoted as N ,andthesizeofthe snapshot of each PB, denoted as b , should be much smaller than B . For each PB, the data set can be regarded as sta-tic OLAP data cubes and those techniques for maintaining aggregations of OLAP cubes can be applied. It is noted that traditional OLAP approaches su ff er from several criti-cal problems to process a PB and cannot comply with the requirements of SCS applications.
To deal with SCS applications, we devise an algorithm to approximate PBs. For static OLAP cubes, several ap-proaches are developed to compress data e ffi ciently in order to solve the problem of selectivity estimation [12, 17] and approximating query processing [16]. Most of those works are based on two approaches: Discrete Wavelet Transform (DWT) and Discrete Cosine Transform (DCT). For DWT-based approaches, the Haar wavelets is a mathematical tool for the hierarchical decomposition of functions with several successful applications in signal and image processing [3, 11].TheDCThasbeenwidelyusedintheimageandsignal processing areas in two-dimensional domain since it takes advantage of unequal distribution in frequency of natural signals, and therefore provides plausible e ffi ciency in capabil-ity of compression. It was empirically shown that those two approaches can compress the original data sets to a small amount of coe ffi cients and provide acceptable quality for range-sum queries. However, several limitations described later refrain both approaches from handling SCS applica-tions.

The Haar wavelets is shown to provide acceptable qual-ity of answers for range-sum queries with a high compression rate as long as the working bu ff er is large enough. The prob-lem for applying wavelets to SCS applications is that the cor-responding memory cost, which is as high as the cardinality of a PB, limits the data volume to be less than or equal to thesizeofworkingbu ff er. Consequently, one can segment the original cube stream into small PB in order to process them in the limited working bu ff er at the cost of degraded quality of answers. The alternative adopts the threshold-ing technique in order to accommodate larger PBs [17]. By dropping small coe ffi cients between each round of decompo-sitions, this technique can reduce the memory required to thesizeofaworkingbu ff er. However, the quality of an-swersalsodegradessigni fi cantly. The authors in [16] take the advantage of sparseness of real world data sets and store temporal results in secondary storage to avoid the shortcom-ings of thresholding. However, the intensive I/O between the working bu ff er and the secondary storage makes this approach not practical for SCS applications. Moreover, all the DWT-based approaches su ff er from the problems of di-mension ordering and the unnecessary decomposition on null cells (zeros). The disadvantages of DWT including problems of small PBs and coe ffi cients thresholding will be further in-vestigated in Section 2.

The work in [12] proposed a DCT-based approach which compresses information from a large number of small-sized histogram buckets using the disc rete cosine transform. Also, a mechanism for selecting a set of theoretically best DCT coe ffi cients for answering range-sum queries was proposed. The transformation of each DCT coe ffi cient is independent of one another and thus this approach is able to work in a small amount of main memory. However, its time complex-ity, estimated as O( nB ), is too high for SCS applications to process the PBs online.
In this paper, the DAWA algorithm, standing for an in-tegrated algorithm of Dct for Data and discrete WAvelet transform, is proposed to ap proximate the cube streams. Algorithm DAWA is processed in two phases for compress-ing PBs. The fi rst phase, called DCT-phase, partitions a PB into several sub-blocks, and then performs DCT to each of them by reciprocal zonal sampling. The second phase, called DWT-phase, retrieves the DCT coe ffi cients located at the same position of all the sub-blocks and then joins those coe ffi cients into several series. Those series, shown to be optimal, are decomposed by the Haar wavelets to further reduce the space required in order to save the information to synopsis memory. Also, a heuristic is applied to improve the accuracy of reconstruction . The superiority of algorithm DAWA over other approaches lies in its integration of the advantage of high compression rate from DWT and that of low memory cost from DCT, thus enabling DAWA to pro-vide answers of good quality for SCS applications under a small working bu ff er and execution time. The optimality of the algorithm is fi rst proved theoretically and empirically demonstrated by our experiments.

The contributions of this paper are many folds. We ad-dress the problem of approximating cube streams. Today X  X  DSS applications require faster techniques to handle the con-tinuously growing data cubes. The problem of handling those multidimensional data streams is formulated in this paper. We propose an e ffi cient algorithm and a heuristic to approximate cube streams under very restricted resources. In addition, the proposed algorithm is able to improve tradi-tional OLAP applications to ob tain faster and more accurate results.

The rest of the paper is organized as follows. Prelimi-naries are presented in Section 2. The theoretical basis and detail steps of algorithm DAWA are introduced in Section 3. Empirical studies are conducted in Section 4. This paper concludes with Section 5.
To solve the problems of approximating cube streams un-der limited working bu ff er and synopsis memory, several techniques are discussed in this section. A multidimen-sional data set (MDS) consists of categorical attributes, e.g., Site_type and Site_Region, which may serve as the dimen-sions, and numeric attributes, e.g., number of errors, serv-ing as the measures. Also, one kind of MDS, constructed with multidimensional data streams, is usually partitioned into blocks with a pre-de fi ned time period T and is then processed individually for snapshots generation since the working bu ff er is constrained. In particular, those parti-tioned blocks, being regarded as OLAP cubes, are referred to as Processing Blocks (abbreviated as PB).

In general, those measures are aggregated to the com-bination of dimension attributes using functions like sum, average, count, and variance. An important class of aggre-gation queries is the range-sum query which applies SUM operations over a set of continuous data cells [16]. For those range-sum queries over the cube stream, the answers can be estimated by summing up that of each PB. Without loss of generality,wefocusontheproblemofestimatingthesubto-tal of a PB. The Haar wavelets and its extension to multidi-mensional data sets are described in Section 2.1. The basics of DCT, including the reciprocal zonal sampling and integral techniques for answering range-sum queries, are introduced in Section 2.2.
The wavelet represents a function in terms of a coarse overall approximation and a series of detail coe ffi cients that revise the function from coarse to fi ne. Wavelets are con-structed in various forms such as orthonormal [9] and nonorthog-onal [7]. This technique is advantageous for its linear compu-tational complexity of O( N ) which is ideal for data streams.
The concept of wavelet transform can be best understood by the following example.
 Example 1: Suppose a numerical time series, S={56, 24, 56, 36}. For multi-resolution analysis, the values are pair-wisely averaged to get a low-resolution signal fi rst. There-fore,wehave{40,46},wherethe fi rst two values in the orig-inal signal, i.e., 56 and 24, are averaged to 40, and so on. To avoid losing any information in this averaging process, the di ff erence values which are {16, 10} should also be stored. As such, the original values can be reconstructed from these average and di ff erence values.

By performing this process recursively, the full decompo-sition can be obtained as in Table 1.

Note that wavelet coe ffi cients which correspond to di ff er-ent resolution scales are generated recursively. The decom-posed wavelet coe ffi cients for original series are
The Haar wavelets decomposition can be extended to mul-tidimensional data arrays by performing a series of one-dimensional decomposition. The detail steps of multidimen-sional decomposition can be found in [16, 17].

Also, the Haar wavelet has been proven to be e ffi cient for dealing with multidimensional data sets as long as the work-ing bu ff er is large enough to accommodate the entire data cube. Clearly, the trade-o ff between quality and space cost has to be considered. For SCS applications, the available space for the working bu ff er is usually much smaller than the size of the PB, i.e., B  X  N . Thus, only part of those coe ffi cients, being representative in general, can be kept and others have to be pruned. This technique, called threshold-ing, selects top-k coe ffi cients with largest absolute values and has been proven to result in minimum L 2 -norm error [6]. This approach is widely adopted for thresholding.
The thresholding provides a high compression rate and quality answers if it is applied after decompositions of along all dimensions are fi nished. However, the requirement for working bu ff er is infeasible. Instead, the thresholding is per-formed between two successive decompositions. This, how-ever, compromises the optimality of global thresholding. As a result, the quality of reconstruction is a ff ected signi fi cantly. Moreover, the results of decompositions depend on the di-mension order. Hence, the best selection of dimension order needs to be evaluated empirically.

In addition, the computational complexity is proportional to the cardinality of the PB denoted as N . For real world data sets, the number of non-zero data points, denoted as n ,ismuchsmallerthan N [5, 15]. Therefore, it is deemed ine ffi cient by performing the Haar wavelets decomposition on sparse datasets.
In order to avoid the interlaced decompositions and thresh-oldings,thesizeofaPBblockmustbereducedto fi tthe working bu ff er. However, it costs much more time to com-pute the answers for range-sum queries from numerous small blocks. Moreover, the selection of snapshot coe ffi cients may not be optimal due to the uneven distributions of di ff erent partitions, i.e., the size for a snapshot may be too large or too small for di ff erent PBs. Example 2 shows the problem caused by small PBs.
 Example 2: Figure 2(a) shows the data sequence for a real dataset consisting of stock indexes during 512 successive trading days. If only 25% of coe ffi cients can be retained for each block, the multiresolution decomposition will cause the error=0.8%.

It is shown in Figure 2(b), the average absolute relative error raises while the number of split segments increases. Due to those shortcomings of thresholding and small blocks, Figure 2: (a) the indexes chart (b) the average ab-solute relative error for various number of segments where 25% ceo ffi cients (128) are retained the DWT is not applicable to approximating cube streams directly. To remedy this, we shall devise a new approach to compressthePBtoamorecompactforminSection3.
TheDiscreteCosineTransform(DCT)hasbeenwidely used in the image and signal processing areas because of its capability of compressing information. Recent works have extended the technique to compress multidimensional his-togram [12] and OLAP Cubes. Let f be N 1  X  N 2  X  ...  X  N d d -dimensional data and F is the corresponding transformed coe ffi cient set. The d-dimensional DCT for a coe ffi cient F located at ( u 1 ,u 2 ,...u d ) is de fi ned as: c and the inverse-DCT for the data point f  X   X  m located at ( m 1 ,m 2 ,...m d ) is de fi ned as: where e f =
The number of DCT coe ffi cients increases exponentially as the dimensionality increases. Clearly, computing all coef-fi cients for possible selection is computationally prohibitive. Therefore, the DCT-based approaches choose and compute only the coe ffi cients that are estimated to be most repre-sentative. In practical OLAP systems, the data distribution should have correlation among data items, i.e., the frequency spectrum of the distribution should show large values in its low frequency coe ffi cients and small values in its high fre-quency coe ffi cients [2, 12]. In general, while the high fre-quency coe ffi cients are usually of less interest in OLAP, the low frequency coe ffi cients are of high interest since the latter correspond to range-aggregation queries [10].

To select representative coe ffi cients, several geometrical zonal sampling techniques, such as the triangular, the spher-ical, the rectangular and the reciprocal sampling, have been proposed [13]. The work in [12] has extended those tech-niques to multi-dimensional datasets and shown that the reciprocal zonal sampling technique is able to select the most representative coe ffi cients of DCT under the brown noise as-sumption. This sampling techniques selects coe ffi cients by the following constraint { k  X   X  u | aDCTcoe ffi cient located at ( u 1 ,u 2 ,...u d ) and u i frequency index in i -th dimension.

Since only the most e ffi cient coe ffi cients are needed to be transformed, the execution time of DCT for data is reduced to O( nk )where k is the number of selected coe ffi cients. As a result, the reciprocal zonal sampling technique improves the execution e ffi ciency signi fi cantly.
For range-sum queries, it is expensive to estimate the val-ues of individual data points and then compute the aggre-gation. Authors of [12] introduced the integral approach to compute the aggregation. For a d -dimensional range-sum query, it costs O( kd ) to compute the aggregation if k coe ffi cients are used. For example, the answer of the query requesting the sum of those cells located at a&lt;u and c&lt;u 2 &lt;d in an M  X  N data cube can be estimated as S =
To remedy the issues of space cost for DWT and time cost for DCT, we propose the DAWA algorithm, standing for an integrated algorithm of Dct for Data and discrete WAvelet transform, to approximate the snapshots for cube streams. The DAWA algorithm is able to generate a snapshot from aPBinasmallworkingbu ff er with one data scan in the computational complexity of only O ( nB  X  will be deducted in Section 3.2 and 3.3. The DAWA frame-workisprocessedintwophasesforgeneratingsnapshots. The fi rst phase, called DCT-phase, partitions a PB into several sub-blocks, called DAWA cells, and then performs DCT to each DAWA cell based on the optimal set of coef-fi cients selected by reciprocal zonal sampling. The second phase, called DWT-phase , groups those DCT coe ffi cients, whose characteristic in time-to-frequency transformation is close to each other, from each DAWA cell into several iso-frequency multidimensional data sets, denoted as IMS. As it will be shown in Section 3.1, the variance of data points in the same IMS is small, and the IMS can thus be further compressed by DWT e ffi ciently. Also,atechniques,called global thresholding, is developed based on the characteris-tics of IMS in order to improve the accuracy of algorithm DAWA.
The essence of DCT or DWT is based on the e ff ectiveness of power concentration of the transformations. Basically, brown noise and random walks are deemed the prevalent formats in real signals [14]. For brown noise, the energy is more concentrative in low frequencies since the data points are more related to each other than those in white noise generated by the pure random process. Also, the relation-ship between power density and frequencies can be explored mathematically and then provides the theoretical founda-tion for zonal sampling. Consequently, we have the following lemma.
 Lemma 1 : For a data series generated by random walk process { X n , 0  X  n&lt;N } and the corresponding DCT co-e ffi cients { F ( k ) , 0  X  k&lt;N } , the squared values of those coe ffi cients S ( k ) , regarded as the spectral densities of X ( n ) , are reciprocal proportional to the squared frequency indexes of coe ffi cients, i.e., S ( k )  X  1 k 2 .

Lemma 1 shows the advantage over information concen-tration of DCT. Also, note that the frequency of a coe ffi cient depends on its location in the transformed P B. Therefore, the reciprocal zonal sampling is optimal for information con-servation.
The idea of DCT-phase is to partition both the PB and the working bu ff er into k sub-blocks and then to perform DCT on each sub-block of the PB, called the DAWA cell, to result in B k coe ffi cients with reciprocal zonal sampling. The transformed coe ffi cients of a DCT are stored into one sub-block, called as T-DAWA cell, in the working bu ff er. In Figure 3:The concept of T D W : (8,1,0)  X  {(2,0,0),(2,1,0)} another words, a DAWA cell is transformed by DCT and stored in corresponding T-DAWA cell. The fl ow of DCT-phase is (1) partitioning a PB into k DAWA cells, (2) reading a multidimensional data point and translating its coordinate corresponding to the PB into that corresponding to DAWA cell,(3)performingDCTontheincomingdatapointand storing B k coe ffi cients to corresponding T-DAWA cell, and (4) repeating (2) and (3) until all data points in this PB are processed. Note that the DCTs over DAWA cells are symmetric, thus avoiding the problem of dimension order. Also, the number of resultant coe ffi cients equals to the size of working bu ff er. For ease of presentation, symbols used in the following sections are summarized in Table 2.
The transformation T D W is de fi ned to transform the coor-dinate p of the data point M p in the PB to a new coordinate { q,r } of the data point E q r in the DAWA cell D q . The con-cept of the transformation of the PB into to 18 sub-blocks is shown in Figure 3. For the original data point located at (X,Y,Z)=(8,1,0) in the PB, the corresponding DAWA cell is D (2 , 0 , 0) and the coordinate of E q r is (2,1,0). After performing T
W , each data point is related to one DAWA cell only. Since only the contribution of a data point to the corresponding T-DAWA cell rather than that to the entire working bu ff er needs to be calculated, the execution e ffi ciency can be im-proved signi fi cantly.

Intuitively, the time for performing DCT on k DAWA cells is 1 /k of that for performing on the entire PB with the same working bu ff er. However, partitioning too many DAWA cells will degrade the e ffi ciency of DWT-phase .ThemoreDAWA cells a PB has, the larger sequence the Haar wavelets have to decompose in DWT-phase . In the extreme case where the number of DAWA cells is the same as the number of data points of the PB, we have a pure DWT-based approach. On the other hand, the process becomes a DCT-based approach ifthevolumeofaDAWAcellisthesameasthatofthePB.
 Thestrategytooptimizethistrade-o ff is to partition each dimension to an appropriate size close to the square root of the dimension size, and consequently, this partitioning tech-nique is able to reduce the computational complexity from O ( nB ) to O ( nB  X  In general, the value of N islargeandthustheexecution e ffi ciency is improved signi fi cantly.

In addition to reduce the overall computational complex-ity of performing DCT, the e ff ectiveness of reciprocal zonal sampling is another factor for quality of compression. To show that the e ffi ciency will not be compromised by parti-tioning a PB into DAWA cells, the data distribution of data series in a DAWA cell is discussed fi rst.
 Lemma 2: For a data series generated by random walk process { X t , 0  X  t&lt;T } , the subset of X t , { X t 0 is also brown noise.
 From Lemma 2, the data series along each dimension in a DAWA cell is shown to be brown noise and then the power density can be estimated as S ( k )  X  1 k 2 accordingtoLemma 1. Therefore, performing recipr ocal zonal sampling on single DAWA cell is as e ff ective as that of performing on whole PB from a local perspective, i.e., the local optimal selection of coe ffi cients can be achieved. For the global optimization, the correlations between DAWA cells can further be utilized by performing DWT on those DCT coe ffi cients located at di ff erent T-DAWA cells.
The idea of DWT-phase is to collect coe ffi cient sets from di ff erent T-DAWA cells to be further compressed by DWT in order to take the advantage of the similarity between those distributions of each T-DAWA cell. Intuitively, the smaller entropy this set has, the higher compression rate the DWT can achieve. Consequently, the DWT prefers those coe ffi -cients whose absolute values are close to each other. In light of this, those DCT coe ffi cients located at the same coordi-nate u in each T-DAWA cell are collected as an IMS I u , standing for Iso-frequency Multidimensional dataSet, to be decomposed by DWT. In a T-DAWA cell, the absolute value of a DCT coe ffi cient is inversely proportional to where u i is the frequency index in i -th dimension. For ex-ample, the coe ffi cient E r (0 , 1) is expected as twice large in magnitude as E r (1 , 1) .Here,thecoe ffi cient whose index of each dimension equals to 0 is called the DC coe ffi cient and the corresponding IMS is denoted as I 0 1 .ForeachT-DAWA cell D i ,ifallcoe ffi cients in D i are adjusted by a scaling fac-tor where the scaling factor is the ratio of the DC coe ffi cient of D 0 to that of D i , the collected IMS will be much smoother. This technique, called scaling, converts a T-DAWA cell to a scaled T-DAWA cells and smoothens the magnitude of coef-fi cients. It is noted that those factors can be calculated from I and thus no extra storage space is needed. As a result, thedecomposedIMS,calledD-IMS,hasthesamestructure as that of an IMS while being much more representative of original information.

The processes of DWT-phase is (1) for each T-DAWA cell
The value of the DC coe ffi cient equals to the product of the average of all data points in the same DAWA cell and a constant D , multiplying all the coe ffi cients except the DC coe ffi cient of D i by the corresponding scaling factor, (2) collecting IMSs from T-DAWA cells and performing DWT on each of them, and (3) sorting all the decomposed coe ffi cients in each D-IMS, performing the heuristic and selecting coe ffi cients to store in synopsis memory. Since coe ffi cients in an IMS are collected from each T-DAWA cell, the working bu ff er B , which is partitioned into k parts, is able to accommodate D-IMSs.

To prove the optimality of IMSs for wavelets decompo-sition, the relationship between an IMS and reconstructed IMS needs to be further explored. For interest of brevity, we employ the case of four-points series to illustrate the nature of an IMS.
 Lemma 3 : For two four-points IMSs of IMS 1 = { X 1 0 , X X will raise if X 1 j and X 2 j are exchanged and the hard-thresholding pruning drops the coe ffi cient having minimum magnitude.
From Lemma 3, the IMS is shown to be the optimal se-lection for performing DWT. Since all the data sources for DWT-phase, which are generated from DCT-phase, are kept in the working bu ff er already, the DWT-phase can be per-formed without loss of information during the multidimen-sional decompositions. Therefore, the optimality of perform-ing thresholding can be maximized.

Figure 4 shows an example of the whole process for trans-forming a 4  X  4 PB to D-IMSs where the PB is partitioned into 4 DAWA cells. Those four T-DAWA cells are fi rst scaled and those coe ffi cients at the same coordinates in each scaled T-DAWA cell are collected to construct an IMS.

For DWT based approaches, it costs O(k) to decompose a data sequence whose size is k. Thus, the computational com-plexity of DWT-phase can be deducted as O( B k  X  k )=O( B ). To store large amount of PBs in synopsis memory, the size of each D-IMS must be as small as possible. Based on the theory of thresholding in DWT, only the most representa-tive coe ffi cients of D-IMSs are selected as the snapshot and then stored to the synopsis memory. In stead of selecting the DWT coe ffi cient set which is good for each individual IMS, a techniques, called global thresholding, is developed to improve, as a whole, the accuracy of algorithm DAWA. The heuristic to improve the performance of DWT-phase is to perform hard thresholding on all the coe ffi cients in the working bu ff er altogether rather than to perform that for each D-IMS individually. Assuming the size of available syn-opsis memory for a PB is limited to b , one should select the most b representative coe ffi cients. The DWT-phase adopts the global thresholding to select top-b coe ffi cients from all D-IMSs. It is noted that those D-IMSs, which have too few coe ffi cients after global thresholding, will degrade the qual-ity signi fi cantly. Thus, the thresholds, denoted as T noc set to fi lter out those low-quality D-IMSs.

The process of global thresholding is (1) sorting the co-e ffi cients of D-IMSs, (2) marking the top-b coe ffi cients, (3) dropping those D-IMSs which do not meet the constraints imposed by T noc and un-marking corresponding coe ffi cients, (4) marking more coe ffi cients having large absolute values from remaining D-IMSs until the number of marked coef-fi cients reaches the memory limit b . At last , the marked coe ffi cients are stored to the synopsis memory.
For [ B k ] D-IMSs, i.e., k T-DAWAcells,storedinthework-ing bu ff er B ,thesortingforeachD-IMScostsO( k log k ) in time complexity, and the step of merging [ B k ] sorted series to mark top-b coe ffi cients is of the complexity O( k  X  [ B fore, the computational complexity for global thresholding is O ([ B k ]  X  k  X  log k + k  X  [ B k ]) , and that of algorithm DAWA, which can be analyzed from the operations on DCT-Phase, DWT-Phaseandtheheuristic,isO( nB  X 
In order to evaluate the algorithm DAWA, three sets of empirical studies on both synthetic and real datasets are conducted. In Section 4.1, the scalability of algorithm DAWA with various settings of working bu ff er and synopsis mem-ory for a PB is evaluated. In Section 4.2, the performance of generating snapshots for PBs by DAWA is compared to that by the wavelet-based approach [17]. Moreover, the quality of answers of range-sum queries for both DAWA and wavelet-base approach is evaluated in Section 4.3.

The synthetic dataset, denoted as D-TPC, whose source is from the decision support benchmark, TPC-H [1], consists of 8 relations. Since the SCS applications on multidimen-sional data sets, a derived table consisting of 6 dimensions ( Suppliers , Nationkey , Orderstatus , Mkesegment , Mfgr and Brand ) is joined from 6 relations to compose the cube with 169,665 tuples and 2,250,000 cells. Also, a real world cube stream, denoted as D-TEL, is obtained from the error log of a large cellular phone company. Four common dimensions are considered where 1,258,368 cells and 120,478 tuples are involved in this cube. The sizes of PBs and DAWA cells for both data sets are illustrated in Table 3.

To simulate the range-sum queries, the range in each di-mension of a query is randomly decided with the criterion that the product of all ranges is close to the query selectivity 0 . 1 . Also, the working bu ff er size is controlled by the para-meter r w , which is de fi ned as r w = B N . The size of available synopsis memory is controlled by r s = b B .
To verify the scalability of algorithm DAWA, both D-TPC and D-TEL are compressed by algorithm DAWA with vari-oussettingsof r w and r s .

ItcanbeseenfromFigure5thattheexecutiontimeof algorithm DAWA for generating the snapshots for those PBs is proportional to the size of working bu ff er and the perfor-mance will not be degraded with the growing size of snap-shots, showing that the inclusion of the heuristic proposed in Section 3 does not a ff ect the execution e ffi ciency and the algorithm DAWA scales well with di ff erent size of snapshots.
To show the good scalability of DAWA in dealing with dif-ferent sizes of data sets, fi ve di ff erent size of small data sets are sampled from both D-TPC and D-TEL. Figure 6 shows that the linear relationship between execution time and the size of data set. That is, the execution time is proportional to the number of tuples of original data set. This conforms to the computational complexity O( Bn )where B is the size of working bu ff er and n is the number of non-null tuples in original data set.

Note that the r w and r s in this experiment are set over a wide range in order to evaluate the scalability of DAWA. Figure 5: Execution times for DAWA under (a) a constant r r =0.2 and various values of r s Figure 6:Execution times of di ff erent sizes of data sets The following experiments are performed with small r w and r to evaluate the the performance of algorithm DAWA in severely constrained environments.
To assess the performance of algorithm DAWA, the wavelet-base approach with hard-thresholding mechanism [17], ab-breviated as DWT-H, is implemented for comparison pur-poses. The execution times for generating snapshots of both algorithm DAWA and DWT-H are measured for D-TPC and D-TEL with various settings of r s .Tocomparetheperfor-mance of both algorithms fairly, the I/O times for scan data are excluded and the time cost for decomposition, trans-formation, and the heuristic are considered. As shown in Figure 5(a), the execution time for algorithm DAWA is al-most linear to the size of the working bu ff er. Therefore, r set to a constant ( 0 . 05 ) and the execution time for di ff erent sizes of working bu ff er can be estimated proportionally.
Figure 7 indicates that algorithm DAWA outperforms al-gorithm DWT-H in execution e ffi ciency by a margin about 3.5 times for the dataset D-TEL. Furthermore, in a large dataset D-TPC, algorithm DAWA is 8 times more e ffi cient than algorithm DWT-H. Consequently, the computational Figure 7: Execution times of generating snapshots with various values of r s Figure 8: 1-norm relative error rates of answers with various settings of r s complexity of DWT-H, estimated as O( N ) from Section 2, is much larger than that of algorithm DAWA.

Note that the execution times for DCT-based approaches [12] are not included since the computational complexity is too high to be practically used for SCS applications. For thecaseof r s =0 . 01 , the DCT-based approaches costs more than 8,500 seconds to fi nish the experiment, meaning that thetimecostis1,400timesasmuchasthatofDAWA.
To evaluate the quality of answers for range-sum queries, 100 queries for both D TEL and D TPC are generated. The settings of r w and r s are the same as those in Section 4.2.
As shown in Figure 8, the quality of answers from algo-rithm DAWA is much better than that from DWT-H. The error rates of the answers from DAWA are all below 5% for both D-TPC and D-TEL, showing that DAWA works well underasmallworkingbu ff er and is able to generate very small snapshots for original data sets with acceptable er-ror rates. In contrast, the DWT-H results high error rates, being as high as 14% and 58% for D-TEL and D-TPC re-spectively, due to insu ffi cient working bu ff er. Furthermore, the DWT-H needs to perform hard thresholding and drops coe ffi cients after decomposition for each dimension. Conse-quently, the thresholding mechanism for DWT-H degrades the quality of answers signi fi cantly. Moreover, the quality of answers may not be improved even if more storage space for synopsis memory is supplied.
In this paper, the problem of generating snapshots for cube streams is addressed. Also, an e ffi cient approach DAWA has been proposed for PBs. By utilizing the techniques of partitioning and selection of IMSs, DAWA is able to com-press a PB into highly representative coe ffi cients e ffi ciently during the DCT-phase and the DWT-phase. In addition, a heuristic has been developed to generate the fi nal results to be stored in synopsis memory and to improve the quality of answers. As shown by our experimental results, due to its advantage of combining the merits of DCT and DWT, DAWA not only generates snapshots for PBs very rapidly but also delivers answers of high quality for range-sum queries. The work was supported in part by the National Science Council of Taiwan, R.O.C., under Contracts NSC93-2752-E-002-006-PAE. [1] Transactional Processing Performance Council. TPC [2] R. Agrawal, C. Faloutsos, and A. N. Swami. E ffi cient [3] C.S.Burrus,R.A.Gopinath,andH.Guo.
 [4] E. Codd, S. Codd, and C. Salley. Providing [5] G. Colliant. Olap, relational, and multidimensional [6] D. L. Donoho. De-noising by soft-thresholding. IEEE [7] D. Gabor. Theory of communication. Journal of the [8] M. Garofalakis, J. Gehrke, and R. Rastogi. Querying [9] A. Haar. Theorie der orthogonalen [10] C.-T. Ho, R. Agrawal, N. Megiddo, and R. Srikant. [11] B. Jawerth and W. Sweldens. An overview of wavelet [12] J.-H. Lee, D.-H. Kim, and C.-W. Chung.
 [13] J. S. Lim. Two Dimensional Signal And Image [14] K. pong Chan and A. W.-C. Fu. E ffi cient time series [15] S. S. Indexing OLAP data. Data Engineering Bulletin , [16] J. S. Vitter and M. Wang. Approximate computation [17] J. S. Vitter, M. Wang, and B. Iyer. Data cube
