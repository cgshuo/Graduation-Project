 Whole page relevance defines how well the surface-level repre-sentation of all elements on a search result page and the corres-ponding holistic attributes of the presentation respond to users X  information needs. We introduce a method for evaluating the whole-page relevance of Web sear ch engine results pages. Our key contribution is that the met hod allows us to investigate as-pects of component relevance that are difficult or impossible to judge in isolation. Such aspect s include component-level informa-tion redundancy and cross-component coherence. The method we describe complements traditi onal document rele vance measure-ment, affords comparative relevance assessment across multiple search engines, and facilitates th e study of important factors such as brand presentation effects and component-level quality. H.3.4 [ Systems and Software ]: Performance evaluation (efficien-cy and effectiveness) Design, Experimentation, Measurement Web search relevance, measurement, evaluation Traditional information retrie val (IR) evaluation methodologies (e.g., [2][6]) judge the relevance of the individual documents from a ranked list returned for a que ry, and compute a single perfor-mance score that is averaged across many queries. This method of assessing a search engine X  X  result relevance manifests a high level of abstraction over the retrieval task by eliminating several sources of variability [5], enab ling important experimentation, measurement, and evaluation effo rts. However, this method ig-nores page elements other than the main document ranking, the surface-level representation of these elements, and holistic results-page characteristics such as cohe rence, diversity, and redundancy. Search engine result pages (SERPs) play a critical role in the Web search process. Web searchers first interact with the SERP re-turned for their query and then with the retrieved results. On the SERP, each result has a summary and may include multimedia or links to additional documents. Ar ound and interspersed within the ranked list are other page elemen ts such as suggested spelling corrections, suggestions of foll ow-on queries, results from alter-nate queries, advertising links , and mini-rankings from other sources such as news, image, and video search. Whole-page relevance (WPR) defines how well SERP compo-nents and the corresponding holistic attributes of the result page presentation respond to searchers X  information needs. Despite its importance, whole-page relevance is seldom considered in IR evaluation. User studies (e.g., [4]), log analysis of user interaction with SERP components (e.g., [1]) , and parallel A/B testing (e.g., [3]) can capture aspects of WPR but are limited in terms of factors such as scalability (user studies are costly and time consuming) or their ability to capture qualitative feedback (log analysis and A/B tests study only behaviors rather th an users X  rationales for them). This poster presents an evalua tion method for whole-page relev-ance. Our evaluation metaphor dr aws on teaching assessment and has judges consider the SERP responses to a query as though they were teachers grading school assi gnments from multiple students. While each student may have diffe rent styles and layout, overall they can be graded with respect to how well they address and satisfy the information needs represented in the assignment. As-signments can be graded both on component elements of their response (e.g., did they mention an important fact?) and on holis-tic aspects such as coherence, co mprehension, and use of authori-tative sources. This metaphor gives rise to our method X  X  name: the School Assignment Satisfaction Index (SASI). The SASI method is a framework for judging aspects of SERP component and holistic relevance. It differs from traditional Cran-field-style evaluations [2] in thre e important ways: (i) judging the surface-level presentation on the SERP rather than the full content of documents, (ii) judging all com ponents of the SERP rather than only the top-ranked algorithmic search results, and (iii) judging the SERP components in context rather than judging each docu-ment in isolation. Since SASI focuses on assessing the user X  X  experience when interacting with the search engine, SERPs are captured and presented to judges in their entirety. However, there is no interaction with the interf ace components or any hyperlink clicking which may take the ju dge away from the SERP and po-tentially introduce bias from la nding pages encountered. SASI focuses on judging only the surface-level representation of the page components, but an altern ative WPR judging system could also judge the landing pages of all linked items. Figure 1 shows our implementati on of a SASI judging interface, with a judging pane and a pane showing a Microsoft Bing SERP for the query [generic drugs]. During judging, the interface can highlight the element currently be ing judged, which in Figure 1 is page middle answer  X  a universal search result in the middle of the page. In the figure, the first of the top-10 results ( top algo ) has already been rated as good . Note that this is just one example of a judging interface, and others c ould be used. For example, al-though we adopted an emoticon-bas ed judging scale; numeric or label scales could also be used. The selection of a three-point scale is arbitrary; a two, four, or five point scale may be equally or more effective for providing SASI judgments. We have introduced SASI, a new evaluation method that delibe-rately considers only surface-level information on a search results page, enabling evaluation of whole-page relevance. While sharing some similarities with traditiona l search evaluation, the SASI method does not generate a reusable test collection because the judgments are not independent of other components shown on the page. Instead it gives vi sibility into different search system cha-racteristics, that are normally evaluated using user studies or A/B testing. SASI is an efficient form of experiment; initial testing with the judge interface described herein revealed that a whole SERP can be judged in the time it takes to judge two documents in Cranfield-style experiments. We have also performed further investigations of the utility of SASI for evaluating whole-page Web search relevance, revealing that it provides insights on likely user perceptions of relevance that are unavailable via traditional IR evaluation methods (e.g., com ponent-level quality and search engine branding effects). In targe ting whole-page relevance, SASI provides a useful complement to document relevance approaches for assessing search performance. In future work we will refine SASI to further our understandin g of whole-page relevance. [1] Clarke, C.L.A., Agichtein, E., Dumais, S., and White, R.W. [2] Cleverdon, C.W. (1960). ASLIB Cranfield research project on [3] Kohavi, R., Henne, R., and Sommerfield, D. (2007). Practical [4] Turpin, A., Scholer, F., J X rvelin, K., Wu, M., and Culpepper, [5] Voorhees, E.M. (2008). On test collections for adaptive in-[6] Voorhees, E.M. and Harman, D. (2005). TREC: Experiment 
