 Gibbs sampling is a workhorse for Bayesian inference but has several limitations when used for parameter estimation, and is often much slower than non-sampling inference meth-ods. SAME (State Augmentation for Marginal Estimation) [15, 8] is an approach to MAP parameter estimation which gives improved parameter estimates over direct Gibbs sam-pling. SAME can be viewed as cooling the posterior param-eter distribution and allows annealed search for the MAP parameters, often yielding very high quality estimates. But it does so at the expense of additional samples per itera-tion and generally slower performance. On the other hand, SAME dramatically increases the parallelism in the sam-pling schedule, and is an excellent match for modern (SIMD) hardware. In this paper we explore the application of SAME to graphical model inference on modern hardware. We show that combining SAME with factored sample representation (or approximation) gives throughput competitive with the fastest symbolic methods, but with potentially better qual-ity. We describe experiments on Latent Dirichlet Allocation, achieving speeds similar to the fastest reported methods (on-line Variational Bayes) and lower cross-validated loss than other LDA implementations. The method is simple to im-plement and should be applicable to many other models. G.4 [ MATHEMATICAL SOFTWARE ]: Algorithm de-sign and analysis; Parallel and vector implementations; Effi-ciency; I.2.6 [ ARTIFICIAL INTELLIGENCE ]: Learn-ing X  Parameter Learning Gibbs sampling; Latent Dirichlet Allocation; Simulated An-nealing c  X 
Many learning problems can be formulated as inference on a joint distribution P ( X,Z,  X ) where X represents observed data,  X  a set of parameters, and Z are latent variables. One generally wants to optimize over  X  while marginalizing over Z . The output of the algorithm is typically a value or symbolic distribution over  X  while the Z are often discarded.
Gibbs sampling is a very general approach to posterior es-timation for P ( X,Z,  X ), but it provides samples only rather than MAP estimates. But therein lies a problem: sampling is a sensible approach to marginal estimation, but can be a very inefficient approach to optimization. This is particu-larly true when the dimension of  X  is large compared to X (which is true e.g. in Latent Dirichlet Allocation and prob-abilistic recommendation algorithms). Such models have been observed to require many samples (thousands to hun-dreds of thousands) to provide good parameter estimates. Hybrid approaches such as Monte-Carlo EM have been de-veloped to address this issue -a Monte-Carlo method such as Gibbs sampling is used to estimate the expected values in the E-step while an optimization method is applied to the parameters in the M-step. But this requires a sepa-rate optimization strategy (usually gradient-based), a way to compute the dependence on the parameters symbolically, and analysis of the accuracy of the E-step estimates.
SAME (State Augmentation for Marginal Estimation) [15, 8] is a simple approach to MAP parameter estimation that remains within the Gibbs framework 1 . SAME replicates the latent state Z with additional states. This has the effect of  X  X ooling X  the marginal distribution on  X , which sharp-ens its peaks and causes  X  samples to approach local op-tima. The conditional distribution P ( Z | X,  X ) remains the same, so we are still marginalizing over a full distribution on Z . By making the temperature a controllable parameter, the parameter estimates can be annealed to reach better lo-cal optima. In both [15, 8] and the present paper we find that this approach gives better estimates than conventional Gibbs sampling. The novelty of the present paper is show-ing that SAME estimation can be very fast , and competitive with the fastest symbolic methods. Thus it holds the poten-tial to be the method of choice for many inference problems. To begin, we define a new joint distribution
SAME is a general approach to MCMC MAP estimation, but in this paper we will focus on its realization in Gibbs samplers which models m copies of the original system with tied parameters  X  and independent blocks of latent variables Z (1) ,...,Z ( m ) . The marginalized conditional P 0 ( X  | X ) = P ( X,  X ) /P ( X ). And where P ( X,  X ) = R Z P ( X,  X  ,Z ) dZ .
 So P 0 ( X  | X ) = P m ( X,  X ) /P ( X ) which is up to a constant factor equal to P m ( X  | X ), a power of the original marginal parameter distribution. Thus it has the same optima, in-cluding the global optimum, but its peaks are considerably sharpened. In what follows we will often demote X to a subscript since it fixed, writing P ( X  | Z,X ) as P X ( X  | Z ) etc.
This new distribution can be written as a Gibbs distribu-tion on  X , as
P m ( X  ,X ) = exp(  X  mg X ( X )) = exp(  X  g X ( X ) / ( kT )) (3) from which we see that m = 1 / ( kT ) is an inverse temper-ature parameter ( k is Boltzmann X  X  constant). Increasing m amounts to cooling the distribution.

Gibbs sampling from the new distribution is straightfor-ward given a sampler for the original, since the new model has the same form with a larger number of parameters. It is perhaps not obvious why sampling from a more complex system could improve performance, but we have added con-siderable parallelism since we can sample various  X  X opies X  of the system concurrently. It will turn out this approach is complementary to using a factored form for the poste-rior. Together these methods gives us orders-of-magnitude speedup over other samplers for LDA.

The rest of the paper is organized as follows. Section 2 summarizes related work on parameter inference for proba-bilistic models and their limitations. Section 3 introduces the SAME sampler. We discuss in Section 4 a factored approximation that considerably accelerates sampling. A hardware-optimized implementation of the algorithm for LDA is described in Section 5. Section 6 presents the experimental results and section 7 describes a generalization to the nested CRP processes. Finally Section 8 concludes the paper.
The Expectation-Maximization (EM) algorithm [7] is a popular method for parameter estimation of graphical mod-els of the form we are interested in. The EM algorithm alternates between updates in the expectation (E) step and maximization (M) step. The E-step marginalizes out the la-tent variables and computes the expectation of the likelihood as a function of the parameters. The E step computes a Q function Q ( X  0 |  X ) = E Z |  X  (log P X ( Z,  X  0 )) to be optimized in the M-step.

For EM to work, one has to compute the expectation (over Z |  X ) of the sufficient statistics of the likelihood func-tion. One also needs a method to optimize the Q function. In practice, the iterative update equations can be hard to derive. Moreover, the EM algorithm is a gradient-based method, and therefore is only able to find locally-optimal solutions.

Variational Bayes (VB) [13] is an EM-like algorithm that uses a parametric approximation to the posterior distribu-tion of both parameters and other latent variables, and at-tempts to optimize the fit (e.g. using KL-divergence) to the observed data. VB typically uses a coordinate-factored form for the approximate posterior. The factored form simplifies inference, but makes strong assumptions about the distri-bution (effectively eliminating interactions). While fast to compute, it often yields biased models.
Gibbs samplers [9] are an excellent match to inference on graphical models since they support simple, local simula-tion of each node conditioned by its Markov blanket. Gibbs sampling can give good (unbiased) parameter estimates for models like LDA [10, 12] but are typically much slower than competing methods (this is certainly true for LDA and re-lated models). Our results suggest this slow convergence is due in part to large variance in the parameters in conven-tional samplers.

Another part of the slow speed of Gibbs sampling for LDA has been the high cost of generating multinomial samples. This was addressed in [20], who used  X  X lias Sampling X  and Metropolis-Hastings to generate k similarly-distributed sam-ples in O ( m + k ) time, where m is the multinomial dimen-sion. In our work, by using replicated (SAME) sampling in blocks, we are able to compute counts of k samples directly in time O ( m ) in fully parallel fashion. As reported in the ex-periments section, this approach is substantially faster than [20]. Based on reported perplexity, analytical arguments, and our attempts to replicate [20], SAME sampling gener-ates more accurate parameter estimates as well.
Monte Carlo EM [16] is a hybrid approach that uses MCMC (e.g. Gibbs sampling) to approximate the expected value E
Z |  X  (log P X ( Z,  X  0 )) with the mean of the log-likelihood of the samples. The method has to optimize Q ( X  0 |  X ) using a numerical method (conjugate gradient etc.). Like standard EM, it can suffer from convergence problems, and may only find a local optimum of likelihood.
Belief propagation [17] and Expectation propagation [14] use local (node-wise) updates to infer posterior parameters in graphical models in a manner reminiscent of Gibbs sam-pling. But they are exact only for a limited class of mod-els. Recently variational message-passing [18] has extended the class of models for which parametric inference is possi-ble to conjugate-exponential family graphical models. How-ever similar to standard VB, the method uses a coordinate-factored approximation to the posterior which effectively eliminates interactions (although they can be added at high computational cost by using a factor graph). It also finds only local optima of the posterior parameters.
SAME estimation involves sampling multiple Z  X  X  indepen-dently and inferring  X  using the aggregate of Z  X  X .
We use the notation Z  X  i = Z 1 ,...,Z i  X  1 ,Z i +1 ,...,Z similarly for  X   X  i .
 Algorithm 1 Standard Gibbs Parameter Estimation 1: initialize parameters  X  randomly, then in some order: 2: Sample Z i  X  P X ( Z i | Z  X  i ,  X ) 3: Sample  X  i  X  P X ( X  i |  X   X  i ,Z ) Algorithm 2 SAME Parameter Estimation 1: initialize parameters  X  randomly, and in some order: 3: Sample  X  i  X  P X ( X  i |  X   X  i ,Z (1) ,...,Z ( m ) )
Sampling Z ( j ) i in the SAME sampler is exactly the same as for the standard sampler. Since the groups Z ( j ) are in-dependent of each other, we can use the sampling function for the original distribution, conditioned only on the other components of the same group: Z ( j )  X  i .

Sampling  X  i is only slightly more complicated. We want to sample from where Z = Z (1) ,...,Z ( m ) and if we ignore the normalizing constants: which is now expressed as a product of conditionals from the original sampler P X ( X  i |  X   X  i ,Z ( j ) ). Inference in the new model will be tractable if we are able to sample from a prod-for many distributions. e.g. for exponential family distri-butions in canonical form, the product is still an exponen-tial family member. A product of Dirichlet distributions is Dirichlet etc., and in general this distribution represents the parameter estimate obtained by combining evidence from independent observations. The normalizing constant will usually be implied from the closed-form parameters of this distribution. Adjusting sample number m at different iter-ations allows annealing of the estimate.
Certain distributions (including LDA) have the property that the latent variables Z i are independent given X and  X . That is P ( Z i | Z  X  i ,X,  X ) = P ( Z i | X,  X ). Therefore the Z can be sampled (without approximation) in parallel. Fur-thermore, rather than a single sample from P ( Z i | X,  X ) (e.g. a categorical sample for a discrete Z i ) we can construct a SAME Gibbs sampler by taking m samples. These samples will now have a multinomial distribution with count m and probability vector P ( Z i | X,  X ). Let  X  Z i ( v ) denote the count for Z i = v among the m samples, and P ( Z i = v | X,  X ) de-note the conditional probability that Z i = v .

We can introduce still more parallelism by randomizing the order in which we choose which Z i from which to sam-ple. The count m for variable Z i is then replaced by random variable  X  m  X  Poisson( m ) and the coordinate-wise distribu-tions of  X  Z i become independent Poisson variables: when the Z i are independent given X,  X , the counts  X  Z i fully capture the results of taking the m (independent) sam-ples. These samples can be generated very fast, and com-pletely in parallel. m is no longer constrained to be an in-teger, or even to be &gt; 1. Indeed, each sample no longer corresponds to execution of a block of code, but is simply an increment in the value m of the Poisson random number generator. In LDA, it is almost as fast to generate all m samples for a single word for a large m (up to about 100) as it is to generate one sample. This is a source of considerable speedup in our LDA implementation.

For distributions where the Z i are not independent, i.e. independent sampling as an approximation . This ap-proach is quite similar to the coordinate-factored approxi-mation often used in Variational Bayes. We leave the details to a forthcoming paper.
In order to generate samples exactly we need fast, re-peated multinomial sampling. Alias sampling is one ap-proach to doing this fast on a sequential machine [20], but is not evidently suitable for parallel implementation. Instead we implemented a fast multinomial sampler using parallel scan operations on the GPU. Let p i for i = 1 ,...,n denote a probability vector from which we will take m multinomial samples. In our implementation, there is no need for p i be normalized, i.e. to sum to 1.

The p i are arranged as leaves of a binary tree. At each internal node, we recursively compute the sum of the prob-abilities of all its descendents, which is equal to the sum of its two children. This requires log 2 ( n ) stages.
From the root of the tree, we start with a count of m assigned to the root. At this node we then draw a bino-mial sample m L from B ( n,p ) = B ( m,L/ ( L + R )) where L is total probability of the left child and R is the total prob-ability of the right child. The left child receives the count m
L while the right child receives m R = m  X  m L . We con-tinue recursively computing binomial samples using the left and right child weights and the count assigned to the node. Eventually, each leaf receives a count which its share of the multinomial count vector.

The complexity of this scheme is O ( n ) (proportional to the number of nodes in the tree). Although a practical GPU implementation uses the same number of threads per datum at each layer with fast shared memory. So the practical complexity is O ( n log n ), albeit with a very small constant. The current implementation of this scheme is within a factor of two of the Poisson method above. Our SAME LDA sampler implementation is described in Algorithm 3. Samples are taken directly from Poisson dis-tributions (line 7 of the algorithm) as described earlier.
LDA is a generative process for modeling a collection of documents in a corpus. In the LDA model, the words X = { x d,i } are observed, the topics Z = { z d,i } are latent variables and parameters are  X  = (  X , X  ) where  X  is docu-ment topic distributions and  X  is word topic distributions. Subscript d,i denotes document d and its i th word. Given X and  X , z d,i are independent, which also implies that we can sample from Z without any information about Z from previous samples. A similar result holds for the parameters, which can be sampled given only the current counts from the Z i . We can therefore alternate parameter and latent variable inference using a pair of samplers: P X ( Z |  X , X  ) and P
X (  X , X  | Z ). Such blocked sampling maximizes parallelism and works very well with modern SIMD hardware.

The sampling of z d,i  X  X  uses the Poisson formula derived earlier. The conditional distributions P X (  X , X  | z d,i tiple independent Dirichlet X  X . In practice, we collapse out (  X , X  ), so we in effect sample a new z t d,i given the z a previous iteration. The update sampler follows a Dirich-let compound multinomial distribution (DCM) and can be derived as, where W and K are numbers of words and topics respec-tively, c are counts across all samples and all documents which are defined as, where N d is the number of documents and superscript ( j ) of z denotes the j th sample of the hidden topic variable. In Equation 7, dot(s) in the subscript of c denotes integrat-ing (summing) over that dimension(s), for example, c k,d, P w =1 c k ,d,w . As shown in Equation 7, sample counts are sufficient statistics for the update formula.
For scalability (to process datasets that will not fit in memory), we implement LDA using a mini-batch update strategy. In mini-batch processing, data is read from a dataset and processed in blocks. Mini-batch algorithms have been shown to be very efficient for approximate inference [4, 3].

In Algorithm 3, D t is a sparse (nwords x ndocs) matrix that encodes the subset of documents (mini-batch) to pro-cess at period t . The global model (across mini-batches) is the word-topic matrix  X  . It is updated as the weighted av-erage of the current model and the new update, see line 14. The weight is determined by according to [12]. We do not explicitly denote passes over the dataset. The data are treated instead as an infinite stream and we examine the cross-validated likelihood as a function of the number of mini-batch steps, up to t max .
GPUs are extremely well-suited to Gibbs sampling by virtue of their large degree of parallelism, and also because of their extremely high throughput in non-uniform random Algorithm 3 SAME Gibbs LDA 1: for t = 0  X  t max do 2:  X   X  = 0;  X   X  = 0;  X  t = (  X  0 + t )  X   X  3:  X  = SDDMM (  X , X ,D t ) 4: for all document-word pair ( d,w ) in mini-batch D t 5: for k = 1  X  K parallel do 7: sample z  X  Poisson(  X   X  m ) 10: end for 11: end for 12:  X  =  X   X  +  X  ;  X  =  X   X  +  X  13: normalize  X   X  along the word dimension 14:  X  = (1  X   X  t )  X  +  X  t  X   X  15: end for number generation (thanks to hardware-accelerated tran-scendental function evaluation). For best performance we must identify and accelerate the bottleneck steps in the algo-rithm. First, computing the normalizing factor in equation 7 is a bottleneck step. It involves evaluating the product of two dense matrix A  X  B at only nonzeros of a sparse matrix C . Earlier we developed a kernel (SDDMM) for this step which is a bottleneck for several other factor models including our online Variational Bayes LDA and Sparse Factor Analysis [6].

Second, line 7 of the algorithm is another dominant step, and has the same operation count as the SDDMM step. We wrote a custom kernel that implements lines 4-11 of the al-gorithm with almost the same speed as SDDMM. Finally, we use a matrix-caching strategy [5] to eliminate the need for memory allocation on the GPU in each iteration. In this section, we evaluate the performance of the SAME Gibbs sampler against several other algorithms and systems. We implemented LDA using our SAME approach, VB online and VB batch, all using GPU acceleration. The code is open source and distributed as part of the BIDMach project [5, 6] on github 2 . We compare our systems with four other systems: 1) Blei et al X  X  VB batch implementation for LDA [2], 2) Griffiths et al X  X  collapsed Gibbs sampling (CGS) for LDA [10], 3) Yan et al X  X  GPU parallel CGS for LDA [19], 4) Li et al X  X  LDA via Walker X  X  alias method [20], 5) Ahmed et al X  X  cluster CGS for LDA [1]. 1) and 2) are both C/C++ implementations of the al-gorithm 3 on CPUs. 3) is the state-of-the-art GPU imple-mentation of parallel CGS and 4) is the most recent work on topic model inference and won the KDD X 14 best paper award. To our best knowledge, 3) and 4) are among the fastest single-machine LDA implementation to date. And 5) is the fastest cluster implementation of LDA.

All the systems/algorithms are evaluated on a single PC equipped with a single 8-core CPU (Intel E5-2660) and a https://github.com/BIDData/BIDMach/wiki 2) provides a Matlab interface for CGS LDA dual-core GPU (Nvidia GTX-690), except GPU CGS, Alias LDA and cluster CGS. Each GPU core comes with 2GB memory. Only one core of GPU is used in the benchmark. The benchmarks for GPU CGS are reported in [19]. They run the algorithm on a machine with a GeForce GTX-280 GPU. Alias LDA benchmarks are reported in [20]. The benchmarks we use for cluster CGS are reported on 100 and 1000 Yahoo cluster nodes [1].

Two datasets are used for evaluation. 1) NYTimes has approximately 300k New York Times news articles. There are 100k unique words and 100 million tokens. 2) PubMed contains about 8.2 million abstracts from collections of US National Library of Medicine. There are 140k unique tokens, and 730 million words.
We first compare the convergence of SAME Gibbs LDA with other methods. We choose m = 100 (the repeated sam-pling count) for the SAME sampler, because convergence speed only improves marginally beyond m = 100. However, runtime per pass over the dataset starts to increase beyond m = 100 while being relatively flat for m &lt; 100. The mini-batch size is set to be 1 / 20 of the total number of examples. We use the per word log likelihood ll = 1 N test log( P ( X (negative log of perplexity) as a measure of convergence.
Figures 1 and 2 show the cross-validated likelihood as a function of the number of passes through the data for both datasets, up to 20 passes. As we can see, the SAME Gibbs sampler converges to a higher quality result than VB online and VB batch on both datasets. The SAME sampler and VB online converged after 4-5 passes for the NYTimes dataset and 2 passes for the PubMed dataset. VB batch converges in about 20 passes.

All three methods above are able to produce higher quality than CGS within 20 passes over the dataset. It usually takes 1000-2000 passes for CGS to converge for typical datasets such as NYTimes, as can be seen in Figure 3.

Figure 3 plots log-likelihood against the number of sam-ples taken per word. We compare the SAME sampler with m = 100, m = 1 and CGS. To reach the same number of samples CGS and the SAME sampler with m = 1 need to run 100 times as many passes over the data as the SAME sampler with m = 100. That is Figure 3 shows 20 passes of SAME sampler with m = 100 and 2000 passes of the other two methods. At the beginning, CGS converges faster. But in the long run, SAME Gibbs leads to better convergence. Notice that SAME with m = 1 is not identical to CGS in our implementation, because of minibatching and the mov-ing average estimate for  X .
We further measure the runtime performance of different methods with different implementations for LDA. Again we fix the sample size for SAME GS at m = 100. For the table, all systems were run on the NYTimes dataset with K = 256, except Yan X  X  CGS [19] used K = 128 and AliasLDA [20] used K = 1024 4 . For most systems, we assume linear dependence
AliasLDA was run for around 150 iterations for the 6750 second measurement in [20], at which point its reported per-Blei et al on the dimension, so we added expected runtimes scaled to K = 256 in parenthesis. We report time per iteration and time to convergence (somewhat subjective since most mod-els didnt not use formal definitions of convergence). Note that one iteration of SAME uses 100 parallel samples and approximates many passes of conventional GS.

Results are illustrated in the Table 1. The SAME Gibbs sampler takes 90 seconds to converge on the NYTimes dataset. By comparison, the other CPU implementations take around 60-70 hours to converge, and Yan X  X  GPU implementation takes 5400 seconds to converge for K = 128. We also mea-sured SAME GS LDA to convergence with K = 128 (50 seconds) and K = 1024 (330 seconds) for direct comparison with Yan et al. and Li et al. Our system demonstrates at least an order of magnitude speedup (we believe closer to two when all models are trained to convergence). This is a substantial improvement over the state of the art.
Finally, we compare our system with the state-of-art clus-ter implementation of CGS LDA [1], using time to conver-gence. Ahmed et al. [1] processed 200 million news articles on 100 machines and 1000 iterations in 2mins/iteration = 2000 minutes overall = 120k seconds. We constructed a repeating stream of news articles (as did [1]) and ran for two iterations -having found that this was sufficient for news datasets of comparable size. This took 30k seconds, which is 4x faster, on a single GPU node. Ahmed et al. also processed 2 billion articles on 1000 machines to con-vergence in 240k seconds, which is slightly less than linear scaling. Our system (which is sequential on minibatches) simply scales linearly to 300k seconds on this problem. Thus single-machine performance of GPU-accelerated SAME GS is almost as fast as a custom cluster CGS on 1000 nodes, and 4x faster than 100 nodes.
The effect of sample number m is studied in Figure ?? and Table 2. As expected, more samples yields better con-vergences given fixed number of passes through the data. And the benefit comes almost free thanks to the SIMD par-allelism offered by the GPU. As shown in Table 2, m = 100 is only modestly slower than m = 1. However, the runtime for m = 500 is much longer than m = 100.

We next studied the effects of dynamic adjustment of m , i.e. annealing. We compare constant scheduling (fixed m ) with 1. linear scheduling: m t = 2 mt t 2. logarithmic scheduling: plexity was higher than the other models we studied and still decreasing. So the table gives a lower bound on its conver-gence time t max is the total number of iterations. The average sample size per iteration is fixed to m = 100 for all 4 configurations. As shown in Figure 5, we cannot identify any particular an-nealing schedule that is significantly better then fixed sample size. Invlinear is slighter faster at the beginning but has the highest initial sample number.
 Finally, we study the cooled distribution we sample from. It is hard to find the distributions for all the parameter con-figurations. Instead we use the empirical word-topic distri-butions (on NYTimes dataset) as a proxy. We compute the entropy of the distributions for different words, and differ-ent sample size m . The entropies are normalized by the entropy at m = 100 for word  X  X eb X . Lower entropy indi-cates that the empirical sampling distribution is more con-centrated and  X  X ooled X . Figure 6 shows the cooling effect. As we can see, the empirical entropies decreases with increas-ing m . The figure provides evidence that the distribution we sample from is cooled for larger m . However, sample size m beyond 100 offers only marginal contribution on reducing variance. It also shows that word  X  X eb X  has more spread-out topic distribution of  X  X chool X  and  X  X ook X .
Another popular topic model is based on the nested Chi-nese Restaurant Process [11]. In the CRP model, the topics mixed into each document are drawn from a topic hierarchy. The CRP model is illustrated in figure 7.

The parameters  X  ,  X  ,  X  and  X  play a similar role as in the standard LDA model, although  X  specifies a topic mixture over a set of topics drawn from the CRP for this document, rather than over all topics. In more detail, the model is implemented by (1) choosing a sequence of c i for each docu-ment using the CRP process. This sequence specifies a set of L topics which are represented in the document. (2) draw a vector of topic proportions  X  from an L -dimensional Dirich-let. (3) generate the words in the document (i.e. the z and w ) using the coordinates of  X  as the mixture coefficients.
The model was found to require tens of thousands of sam-ples for good model estimation [11], and so would benefit from acceleration. We note that as with standard LDA, the words are sampled repeatedly and independently for each document from the same distribution (keeping c ,  X  and  X  fixed). Similar to LDA, we can fold c ,  X  and  X  into the parameter vector  X , and apply SAME sampling by over-sampling the Z parameters. Because of independence of the Z given  X , we can again use multinomial sampling to extract counts of samples rather than distinct, individual samples. The method in [11] alternates draws of c and z and the multinomial counts can be used directly in the for-mula for the distribution of c in section 4 of [11]. While the application of SAME to nested CRP is straightfoward, a SAME implementation of the nested CRP process is a topic for future work. This paper described hardware-accelerated SAME Gibbs Parameter estimation -a method to improve and accelerate parameter estimation via Gibbs sampling. This approach re-duces the number of passes over the dataset while introduc-ing more parallelism into the sampling process. We showed that the approach meshes very well with SIMD hardware, and that a GPU-accelerated implementation of cooled GS for LDA is faster than other sequential systems, and com-parable with the fastest cluster implementation of CGS on 1000 nodes. The code is available at https://github.com/BIDData/BIDMach .

SAME GS is applicable to many other problems, and we are currently exploring the method for inference on gen-eral graphical models. The coordinate-factored sampling approximation is also being applied to this problem in con-junction with full sampling (the approximation reduces the number of full samples required) and we anticipate simi-lar large improvements in speed. The method provides the quality advantages of sampling and annealed estimation, while delivering performance with custom (symbolic) infer-ence methods. We believe it will be the method of choice for many inference problems in future. [1] A. Ahmed, M. Aly, J. Gonzalez, S. Narayanamurthy, [2] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [3] L. Bottou. Large-scale machine learning with [4] L. Bottou and O. Bousquet. The tradeoffs of large [5] J. Canny and H. Zhao. Bidmach: Large-scale learning [6] J. Canny and H. Zhao. Big data analytics with small [7] A. P. Dempster, N. M. Laird, D. B. Rubin, et al. [8] A. Doucet, S. Godsill, and C. Robert. Marginal [9] S. Geman and D. Geman. Stochastic relaxation, gibbs [10] T. L. Griffiths and M. Steyvers. Finding scientific [11] D.M. Blei and T.L. Griffiths and M.I. Jordan and J.B. [12] M. D. Hoffman, D. M. Blei, and F. R. Bach. Online [13] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and [14] T. Minka. Expectation propagation for approximate [15] C. Robert, A. Doucet, and S. Godsill. Marginal MAP [16] G. C. Wei and M. A. Tanner. A monte carlo [17] Y. Weiss. Correctness of local probability propagation [18] J. Winn and C. Bishop. Variational message passing. [19] F. Yan, N. Xu, and Y. Qi. Parallel inference for latent [20] A. Li, A. Ahmed, S. Ravi, and A. Smola Reducing the
