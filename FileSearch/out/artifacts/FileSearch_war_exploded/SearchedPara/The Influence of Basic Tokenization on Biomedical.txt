 Tokenization is a fundamental preprocessing step in Infor-mation Retrieval systems in which text is turned into index terms. This paper quantifies and compares the influence of various simple tokenization techniques on document re-trieval effectiveness in two domains: biomedicine and news. As expected, biomedical retrieval is more sensitive to small changes in the tokenization method. The tokenization strat-egy can make the difference between a mediocre and well performing IR system, especially in the biomedical domain. H.3.1 [ Content Analysis and Indexing ]: Indexing meth-ods Algorithms, Measurement, Performance, Experimentation Biomedical document retrieval, tokenization, lexical analysis
Tokenization is the process of converting a stream of char-acters into a stream of words or tokens. In the context of Information Retrieval the process is also known as lexical analysis and its goal is to identify candidate index terms.
For general IR purposes, a simple tokenizer extracting se-quences of letters will suffice. Additionally, the words can be conflated to a root form using a stemmer (e.g.  X  X cti-vation X  and  X  X ctivate X  can be stemmed to  X  X ctiv X ) and non-informative words can be discarded using a list of stopwords (e.g.  X  X he X ).

Handling biomedical literature is characterized by its prob-lems related to terminology [5]. Its terminology is inconsis-tently spelled, abbrevations are frequently used, words and abbreviations can have different meanings (homonymy) and Copyright 2007 ACM 978-1-59593-597-7/07/0007 ... $ 5.00. concepts are described in more than one way (synonymy). Tokenization plays an important role in handling inconsis-tently spelled terminology, especially for the purpose of key-word based searching. Biomedical terms contain digits, cap-italized letters within words, greek letters, roman digits, hy-phens and other special characters.

Many different tokenization approaches have been pro-posed and used, but few or limited comparative studies have been carried out on its actual influence on search perfor-mance. In this work we compare several simple tokenization approaches in the context of biomedical document retrieval. We focus on: case sensitivity, the use of digits, the treat-ment of special characters, removal of stopwords, stemming and the expansion of compound terms.
Most related research comes from the field of biomedical text mining in which named entity recognition is an impor-tant problem. Term variation is one of the most frequent causes of gene name recognition failures [2, 4, 5]. Many tokenization approaches have been proposed during The TExt Retrieval Conference Genomics tracks [3]. Zhou et al [9] applied conditional Porter stemming to prevent biomedical terminology from being stemmed incorrectly. Wan et al [7] separate sequences of either letters or digits as index terms. Urbain et al [6] normalize gene/protein terms with variants. Many systems use domain specific stop word lists and expand queries with lexical variants. Unfortunately often no comparisons are made to a baseline, making it dif-ficult to assess the added value of these approaches. Wang et al [8] studied the influence of handling hyphenation and Greek letters for preprocessing queries.
We use two IR models in our experiments: (1) probabilis-tic Language Modeling (LM) using Jelinek-Mercer smooth-ing and (2) the out-of-the-box TFIDF model from Lucene [1].
A biomedical and news corpus are used for the experi-ments. Firstly, the TREC 2006 Genomics collection and topics, which consists of around 160.000 full-text biomedi-cal articles from Highwire Press. The topics are used unal-tered in the experiments, that is maintaining phrases such as  X  X hat is the role of X . Secondly, the TREC ad hoc col-lection consisting of around 520.000 newswire documents (FBIS, FR, FT and LA) is used as a reference corpus, using topics 351 to 450 of the TREC 6, 7, 8 ad hoc evaluations. We only use the title section of the topic descriptions for our queries. Table 1 gives an overview of the 14 tested to-kenization methods. The methods and their performance measured in terms of (document based) mean average pre-cision will be discussed in the next section.
The baseline system ( LL ) converts the characters to lower-case and treats sequences of letters as terms. Both LM and TFIDF models perform around the median of TREC Ge-nomics 2006 submissions (31% MAP). Case folding shows to be beneficial: its case sensitive variant ( L ) performs slightly worse (-6% and -2% for LM and TFIDF respectively).
Adding digits to the index ( LLD , LD and SLLD ) also im-proves performance. What the best treatment is of digits depends on the IR model. TFIDF performs better using terms consisting of both letters and terms, where LM gives the best performance when sequences of either letters or digits ( SLLD ) are used as tokens.

Rewriting Greek letters to their full name ( TGLL ) did not show large differences, probably because only few topics con-tained references sensitive to this approach.

Porter stemming ( LLSP ) shows strong improvements on the news collection (19% and 21% respectively). Despite the inccorect stemming of some gene names, the performance on the Genomics collection increases slightly for LM (4%) and decreases for TFIDF (-1%).

Applying a general stopword ( LLSW ) list shows a strong improvement on the genomics collection, probably because the topic descriptions contain many stopwords. The results on the news collection show a modest improvement.
As expected, naively tokenizing (lowercased) sequences of non-whitespace characters ( W and LW ) performs poor.
The compound tokenizer ( COM ) tokenizes both compounds and its components. The sequence of lowercased charac-ters first is split on whitespace. This may result in strings containing special characters such as punctuation and hy-phens (e.g.  X  X d28-responsive X ). The special characters are stripped to form the first token (e.g. cd28responsive ). Fi-nally, the string is tokenized using the SLLD tokenizer; this results in additional tokens (e.g. cd 28 responsive ). The rationale behind this tokenizer is that one token represents a normalized form of the original compound in the text. The additional tokens will match lexical variations contain-ing the same components. The compound tokenizer shows a strong improvement over the baseline (15% and 17%).
The custom tokenizer ( CUS ) combines the tested tokeniz-ers: it uses the compound tokenizer, removes stopwords and performs Porter stemming. The resulting indices show a very strong improvement over the baseline (27% for LM and 25% for TFIDF). The same tokenizer also performs best on the news collection.

The results show that a small change in tokenization strat-egy can improve a mediocre 2006 TREC genomics submis-sion (MAP average: 29%) to the top quarter of the submis-sions (36%-54%). Normalization and splitting compounds to multiple terms shows to be very beneficial for the tested IR models which assume term independence in both queries and documents. We expect that incorporation of proximi-ties of related terms in the retrieval model will even further improve retrieval performance. This work was part of the BioRange programme of the Netherlands Bioinformatics Centre (NBIC), which is sup-ported by a BSIK grant through the Netherlands Genomics Initiative (NGI).
