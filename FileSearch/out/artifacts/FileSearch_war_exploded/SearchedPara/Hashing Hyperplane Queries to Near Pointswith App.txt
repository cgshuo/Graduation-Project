 example-based learning algorithms, content-based image o r audio retrieval, and quantization-based data compression. Often the search problem is considered in the domain of point data: given a forgoing a brute force scan through all database items, e.g. , [1, 2, 3, 4, 5, 6, 7]. By comparison, much less work considers how to efficiently ha ndle instances more complex than a database of points, which are nearest to a novel hyperplane query? This problem is critical to unlabeled pools, a fast (sub-linear time) hyperplane searc h method is needed. To this end, we propose two solutions for approximate hyperp lane-to-point search. For each, we provide bounds for the approximation error of the neighbors retrieved. Our first approach devises hyperplane distance, thereby making them searchable with e xisting approximate nearest neighbor method has stronger accuracy guarantees.
 lems with massive unlabeled pools on the order of millions of examples. We briefly review related work on approximate similarity sea rch, subspace search methods, and pool-based active learning.
 Approximate near-neighbor search. For low-dimensional points, spatial decomposition and tre e-methods break down for high-dimensional data, a number of approximate near neighbor methods have been proposed that work well with high-dimensional inp uts. Locality-sensitive hashing (LSH) of methods design Hamming space embeddings that can be index ed efficiently (e.g., [11, 12, 6]). However, in contrast to our approach, all such techniques ar e intended for vector/point data. A few researchers have recently examined approximate searc h tasks involving subspaces. In [13], a Euclidean embedding is developed such that the norm in the em bedding space directly reflects the principal angle-based distance between the original subsp aces. After this mapping, one can apply existing approximate near-neighbor methods designed for p oints (e.g., LSH). We provide a related bounds, and our embedding is more compact due to our proposed sampling strategy. Another method of specialized methods for the hyperplane search problem, and show that they handle high-dimensional data and large databases very efficiently.
 Margin-based active learning. Existing active classifier learning methods for pool-based selection unlabeled datasets make the cost of exhaustively searching the pool impractical. Researchers have previously attempted to cope with this issue by clustering o r randomly downsampling the pool [19, fewer points when making the next active label request, yet g uarantee selections within a known error of the traditional exhaustive pool-based technique.
 Other forms of approximate SVM training. To avoid potential confusion, we note that our prob-lem setting differs from both that considered in [23], where computational geometry insights are combined with the QP formulation for more efficient  X  X ore vec tor X  SVM training, as well as that considered in [19], where a subset of labeled data points are selected for online LASVM training. We consider the following retrieval problem. Given a databa se D = [ x R whose normal is given by w  X  R d . We call this the nearest neighbor to a query hyperplane (NNQH) each x The Euclidean distance of a point x to a given hyperplane h Thus, the goal for the NNQH problem is to identify those point s x where the goal is to maximize x T w or  X  x T w , respectively. Hence, existing approaches are not directly applicable to this problem.
 We formulate two algorithms for NNQH. Our first approach maps the data to binary keys that are locality-sensitive for the angle between the hyperplane no rmal and a database point, thereby per-mitting sub-linear time retrieval with hashing. Our second approach computes a sparse Euclidean ing approximate nearest-point methods.
 In the following, we first provide necessary background on lo cality-sensitive hashing (LSH). The nally, in Sec. 3.5, we explain how either method can be applie d to large-scale active learning. 3.1 Background: Locality-Sensitive Hashing (LSH) one need only search those database items with which a novel q uery collides in the hash table. B ( p, r ) denote the set of examples from S within radius r from p .
 Definition 3.1. [3] Let h H is called ( r, r (1 +  X  ) , p For a family of functions to be useful, it must satisfy p h h
H ( p ) , h , while for dissimilar points it is at most p k mapped to a series of l hash tables indexed by independently constructed g (NN) for q , meaning if q has a neighbor within radius r , then with high probability some example within radius r (1 +  X  ) is found.
 the Hamming distance over vectors. For that hash function,  X  = log p 1 define two locality-sensitive hash functions for the NNQH pr oblem. 3.2 Hyperplane Hashing based on Angle Distance (H-Hash) vectors are unit norm, then this means that for the  X  X ood X  (cl ose) database vectors, w and x are almost perpendicular. Let  X  Definition 3.1 to reflect how far from perpendicular w and x are: Consider the following two-bit function that maps two input vectors a , b  X  X  X  d to { 0 , 1 } 2 : where h independently from a standard d -dimensional Gaussian, i.e., u , v  X  X  (0 , I ) . We define our hyperplane hash (H-Hash) function family H as: Next, we prove that this family of hash functions is locality -sensitive (Definition 3.1). Claim 3.2. The family H is  X  r, r (1 +  X  ) , 1 d (  X  ,  X  ) , where r,  X  &gt; 0 .
 Proof. Since the vectors u , v used by hash function h query hyperplane vector w and a database point vector x , Next, we use the following fact proven in [25], where u is sampled as defined above, and  X  Using (4) and (5), we get: Hence, when  X   X  such that  X   X  x we use h retrieve examples for which we know only that x is  X / 2 or less away from w .
 With these functions in hand, we can now form hash keys by conc atenating k two-bit pairs from k plane to retrieve its closest points (see Sec. 3.1).
 The approximation guarantees and correctness of this schem e can be obtained by adapting the proof our LSH scheme will return a point within a distance (1 +  X  ) r , where r = min for all values of r,  X  . Furthermore, as p as values of r . See the supplementary material for more discussion on the b ound. 3.3 Embedded Hyperplane Hashing based on Euclidean Distanc e (EH-Hash) Our second approach for the NNQH problem relies on a Euclidea n embedding for the hyperplane and points. It offers stronger bounds than the above, but at t he expense of more preprocessing. dimensional vector by vectorizing the corresponding rank-1 matrix aa T : where a Hence, minimizing the distance between the two embeddings i s equivalent to minimizing | a T b | , our intended function.
 Given this, we define our embedding-hyperplane hash (EH-Hash) function family E as: where h Claim 3.3. The family of functions E defined above is  X  r, r (1 +  X  ) , 1 Proof. Using the result of [25], for any vector w , x  X  R d , Using (7) together with the definition of h x we have: Hence, when (  X  and p We observe that this p values close to twice those returned by H-Hash X  X  p Hence, the factor  X  = log p 1 detailed comparison of the two bounds.
 On the other hand, EH-Hash X  X  hash functions are significantl y more expensive to compute. Specif-use a form of randomized sampling when computing the hash bit s for a query that reduces the time to approximation problems (see [26]).
 Lemma 3.4. Let v  X  R d and define p v with probability p Then, for any y  X  R d ,  X  &gt; 0 , c  X  1 , t  X  c We defer the proof to the supplementary material. The lemma i mplies that at query time our hash function h tation by sampling O ( 1 for w . However, in this case, the computational requirements inc rease to O ( d While one could alternatively use the Johnson-Lindenstraus s (JL) lemma to reduce the dimension-dimensionality of a subspace represented by a hyperplane im plies the random projection dimension-on the sum of the number of database points and query hyperplanes. The latter is problematic when instance-dependent and incurs very little overhead for com puting the hash function. they define Euclidean embeddings for affine subspace queries and database points which could be used for NNQH, although they do not specifically apply it to hy perplane-to-point search in their work. Also, their embedding is not tied to LSH bounds in terms of the distance function (2), as we have shown above. Finally, our proposed instance-specific s ampling strategy offers a more compact representation with the advantages discussed above. 3.4 Recap of the Hashing Approaches To summarize, we presented two locality-sensitive hashing approaches for the NNQH problem. Our able two-bit hash functions together with a bound on retriev al time. Our second EH-Hash approach NNHQ to the Euclidean space nearest neighbor problem, for wh ich efficient search structures (in-cluding LSH) are available. While EH-Hash has better bounds t han H-Hash, its hash functions are where we randomly sample the given query embedding, reducin g the query time to linear in d . Note that both of our approaches attempt to minimize d hyperplane w . Since that distance is only dependent on the angle between x and w , any scaling of 3.5 Application to Large-Scale Active Learning in somewhat canned scenarios: the implementor has a moderat ely sized labeled dataset, and simply to compute exhaustively, and thus defeats the purpose of imp roving overall learning efficiency. lected for labeling: x  X  = argmin tables, and then at each active learning loop, we hash the cur rent classifier w as a query. 2 We demonstrate our approach applied to large-scale active l earning tasks. We compare our methods (H-Hash in Sec. 3.2 and EH-Hash in Sec. 3.3) to two baselines: 1) passive learning, where the next in (1) is computed over all unlabeled examples in order to find the true minimum. The main goal substantially greater efficiency.
 Datasets and implementation details. We use three publicly available datasets. 20 Newsgroups consists of 20,000 documents from 20 newsgroup categories. We use the provided 61,118-d bag-of-It is a manually labeled subset of the 80 Million Tiny Image da taset [28], which was formed by images from [28]. For both CIFAR-10 and Tiny-1M, we use the pr ovided 384-d GIST descriptors as across five such runs. We fix k = 300 , N  X  = 500 ,  X   X  = 0 . 01 .
 Newsgroups documents results. Figure 1 shows the results on the 20 Newsgroups, starting wit h yet require scanning an order of magnitude fewer examples (b ). Note, Random requires  X  0 time. haustive search. We also observe the expected trade-off: H-Hash is more efficient, while EH-Hash provides better results (only slightly better for this smal ler dataset).
 CIFAR-10 tiny image results. Figure 2 shows the same set of results on the CIFAR-10. The tre nds margin between active and random. Averaged over all classes , we happen to outperform exhaustive offs: EH-Hash has stronger guarantees than H-Hash (and thus retrieves lower w T x values), but is more expensive. Figure 3(a) shows example image selection r esults; both exhaustive search and our hashing methods manage to choose images useful for learning about airplanes/non-airplanes. to 1 and 5 seconds for the Newsgroups and Tiny image datasets, respect ively. (Note, however, that the advantage of our approximate methods: accounting for bo th types of cost inherent to training provide the best accuracy gains by minimizing both selectio n and labeling time. Tiny-1M results. Finally, to demonstrate the practical capability of our hyp erplane hashing ap-with 50 examples from CIFAR-10. The 1M set lacks any labels, m aking this a  X  X ive X  test of active learning (we ourselves annotated whatever the methods sele cted). We use our EH-Hash method, since it offers stronger performance.
 retrieves seemingly relevant instances. To our knowledge, this experiment exceeds any previous active selection results in the literature in terms of the sc ale of the unlabeled pool. Conclusions. We introduced two methods for the NNQH search problem. Both p ermit efficient we plan to further explore more accurate hash-functions for our H-hash scheme and also investigate sublinear time methods for non-linear kernel based active l earning.
 This work is supported in part by DARPA CSSG, NSF EIA-0303609 , and the Luce Foundation.
