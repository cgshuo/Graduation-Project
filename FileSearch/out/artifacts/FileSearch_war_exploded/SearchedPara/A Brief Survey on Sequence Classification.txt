 Sequence classification has a broad range of applications such as genomic analysis, information retrieval, health in-formatics, finance, and abnormal detection. Different from the classification task on feature vectors, sequences do not have explicit features. Even with sophisticated feature se-lection techniques, the dimensionality of potential features may still be very high and the sequential nature of features is difficult to capture. This makes sequence classification a more challenging task than classification on feature vec-tors. In this paper, we present a brief review of the existing work on sequence classification. We summarize the sequence classification in terms of methodologies and application do-mains. We also provide a review on several extensions of the sequence classification problem, such as early classification on sequences and semi-supervised learning on sequences. Sequence classification has a broad range of real-world appli-cations. In genomic research, classifying protein sequences into existing categories is used to learn the functions of a new protein [13]. In health-informatics, classifying ECG time se-ries (the time series of heart rates) tells if the data comes from a healthy person or comes from a patient with heart disease [59]. In anomaly detection/intrusion detection, the sequence of a user X  X  system access activities on Unix is mon-itored to detect abnormal behaviors [33]. In information re-trieval, classifying documents into different topic categories has attracted a lot of attentions [51]. Other interesting ex-amples include classifying query log sequences to distinguish web-robots from human users [58; 18] and classifying trans-action sequence data in a bank for the purpose of combating money laundering [42].
 Generally, a sequence is an ordered list of events. An event can be represented as a symbolic value, a numerical real value, a vector of real values or a complex data type. In this paper, we consider sequence data into the following sub-types. A sequence may carry a class label. For example, a time series of ECG data may come from a healthy or ill person. A DNA sequence may belong to a gene coding area or a non-coding area. Given L as a set class labels, the task of (conventional) sequence classification is to learn a sequence classifier C , which is a function mapping a sequence s to a class label l  X  L , written as, C : s  X  l, l  X  L . In (conventional) sequence classification, each sequence is associated with only one class label and the whole sequence is available to a classifier before the classification. There are also other application scenarios for sequence classification. For example, for a sequence of symptoms of a patient over a long period of time, the health condition of the patient may change. For a streaming sequence, which can be regarded as a virtually unlimited sequence, instead of predicting one class label, it is more desirable to predict a sequence of la-bels. This problem is considered in [24; 23] as the strong sequence classification task. In this paper, we will discuss several extensions of (conventional) sequence classification in Section 3.
 There are three major challenges in sequence classification. First, most of the classifiers, such as decision trees and neu-ral networks, can only take input data as a vector of features. However, there are no explicit features in sequence data. Second, even with various feature selection methods, we can transform a sequence into a set of features, the feature se-lection is far from trivial. The dimensionality of the feature space for the sequence data can be very high and the compu-tation can be costly. Third, besides accurate classification results, in some applications, we may also want to get an interpretable classifier. Building an interpretable sequence classifier is difficult since there are no explicit features. In this paper, we give a brief survey of the existing sequence classification methods. Since most of the existing works fo-cus on the task of conventional sequence classification, Sec-tion 2 is devoted to summarizing the major methods for this task. In Section 3, we discuss some extensions of the conventional sequence classification tasks, such as streaming sequence classification and early classification on sequences. In Section 4, we summarize sequence classification from the perspective of application domains, such as time series data, text data and genomic data. Section 5 concludes the paper. The sequence classification methods can be divided into three large categories. In the rest of this section, we will present some represen-tative methods in the three categories. Some methods may ride on multiple categories. For example, we can use SVM by either extracting features (Category 1) or defining a dis-tance measure (Category 2). Sequence classification using SVM will be summarized in Section 2.3. All methods dis-cussed in this section are for conventional sequence classifi-cation. Conventional classification methods, such as decision trees and neural networks, are designed for classifying feature vec-tors. One way to solve the problem of sequence classification is to transform a sequence into a vector of features through feature selections.
 For a symbolic sequence, the simplest way is to treat each element as a feature. For example, a sequence CACG can be transformed as a vector  X  A, C, C, G  X  . However, the sequen-tial nature of sequences cannot be captured by this trans-formation. To keep the order of the elements in a sequence, a short sequence segment of k consecutive symbols, called a k -gram, is usually selected as a feature. Given a set of k -grams, a sequence can be represented as a vector of the presence and the absence of the k -grams or as a vector of the frequencies of the k -grams. Sometimes, we also allow inexact matchings with gapped k -grams. By using k -grams as features, sequences can be classified by a conventional classification method, such as SVM [35; 36] and decision trees [12]. A summary of k -gram based feature selection methods for sequence classifications can be found in [16]. The size of candidate features which are all k -grams where 1  X  k  X  l is 2 l  X  1. If k is a large number, the size of the feature set can be huge. Since not all features are equally useful for classification, Chuzhanova et al. [12] use Gamma test to select a small informative subset of features from the k -grams. A genetic algorithm is used to find the local optimal subset of features.
 In contrast to k -gram based feature selections, Lesh et al. [30; 34] propose a pattern-based feature selection method. The features are short sequence segments which satisfy the fol-lowing criteria (1) frequent in at least one class (2) distinc-tive in at least one class and (3) not redundant. Criterion (2) means a feature should be significantly correlated with at least one class. The redundancy in Criterion (3) can be defined in the way of feature specification and feature gener-alization. An efficient feature mining algorithm is proposed to mine features according to the criteria. After selecting the features, Winnow [41] and naive bayes classifiers are used. The experimental results in [30] show that comparing to the method of considering each element as a feature, pattern-based feature selection can improve the accuracy by 10% to 15%.
 The challenge of applying pattern-based feature selection on symbolic sequences is how to efficiently search for the fea-tures satisfying the criteria. Ji et al. [22] propose an algo-rithm to mine distinctive subsequences with a maximal gap constraint. The algorithm, which uses bisect and boolean operations and a prefix growth framework, is efficient even with a low frequency threshold.
 Time series data is numeric. The feature selection tech-niques for symbolic sequences cannot be easily applied to time series data without discretization. Discretization may cause information lost. Ye et al. [65] propose a feature selec-tion method which can be applied directly on numeric time series. Time series shapelets, the time series subsequences which can maximally represent a class, is proposed as the features for time series classification. For a two-class clas-sification task, given a distance threshold, a shapelet is a segment of time series which can be used to separate the training data into two parts according to the distance to the shapelet, and maximizes the information gain. The dis-tance threshold and the shapelet are learned from the train-ing data to optimize the information gain. To construct a classifier, the shapelet selection process is integrated with the construction of the decision tree.
 Although subsequences are informative features, they can only describe the local properties of a long sequence. Ag-garwal et al. [5] develop a method to capture both the global and local properties of sequences for the purpose of classifi-cation. Aggarwal et al. [5] modify wavelet decomposition to describe a symbolic sequence on multiple resolutions. With different decomposition coefficients, the wavelet represents the trends in different range of intervals, from global to lo-cal. Using wavelet decomposition and a rule based classi-fier, the wavelet decomposition method outperforms the k-nearest neighbor classifier on a web accessing sequence data set and on a genomic sequence data set.
 In summary, the existing methods differ from each other on the following aspects. Sequence distance based methods define a distance func-tion to measure the similarity between a pair of sequences. Once such a distance function is obtained, we can use some existing classification methods, such as K nearest neighbor classifier (KNN) and SVM with local alignment kernel (to be discussed in Section 2.3 [49], for sequence classification. KNN is a lazy learning method and does not pre-compute a classification model. Given a labeled sequence data set T , a positive integer k , and a new sequence s to be classified, the KNN classifier finds the k nearest neighbors of s in T , kNN ( s ), and returns the dominating class label in kNN ( s ) as the label of s .
 The choice of distance measures is critical to the perfor-mance of KNN classifiers. In the rest of this section, we focus on summarizing different distance measures proposed for sequence data.
 For simple time series classification, Euclidean distance is a widely adopted option [26; 59]. For two time series s and s Euclidean distance is The Euclidean distance usually requires two time series to have the same length. Keogh et al. [26] show when applying 1NN classifier on time series, Euclidean distance is surpris-ingly competitive in terms of accuracy, compared to other more complex similarity measures.
 Euclidean distance is sensitive to distortions in time dimen-sion. Dynamic time warping distance (DTW) [28] is pro-posed to overcome this problem and does not require two time series to be of the same length. The idea of DTW is to align two time series and get the best distance by align-ing. One example of DTW is shown in Figure 1. Xi et al. [61] show that on small data sets, elastic measures such as dynamic time warping (DTW) can be more accurate than Euclidean distance. However, recent empirical results [15] strongly suggest that on large data sets, the accuracy of elastic measures converges with Euclidean distance. Dynamic time warping is usually computed by dynamic pro-gramming and has the quadratic time complexity. There-fore, it is costly on a large data set. Ratanamahatana et al. [48] propose a method to dramatically speed up the DTW similarity search process by using tight lower bounds to prune may calculations. Xi et al. [61] use numerical re-duction to speed up DTW computation. The idea is to reduce the number of the training examples used by a 1NN classifier and, at the same time, adjust the warping window dynamically.
 For symbolic sequences, such as protein sequences and DNA sequences, alignment based distances are popular adopted [25]. Given a similarity matrix and a gap penalty, the Needleman-Wunsch algorithm [44] computes an optimum global align-ment score between two sequences through dynamic pro-gramming. In contrast to global alignment algorithms, lo-cal alignment algorithms, such as the Smith-Waterman al-gorithm [53] and BLAST [6], measure the similarity between two sequences by considering the most similar regions but not enforcing the alignments on full length. SVM has been proved to be an effective method for sequence classification [43; 39; 35; 54; 55; 52; 13]. The basic idea of applying SVM on sequence data is to map a sequence into a feature space and find the maximum-margin hyperplane to separate two classes. Sometimes, we do not need to explic-itly conduct feature selection. A kernel function corresponds to a high dimension feature space. Given two sequences, x, y , some kernel functions, K ( x, y ), can be viewed as the similarity between two sequences [54]. The challenges of ap-plying SVM to sequence classification include how to define feature spaces or kernel functions, and how to speed up the computation of kernel matrixes.
 One of the widely used kernels for sequence classification is k-spectrum kernel or string kernel , which transforms a sequence into a feature vector. Leslie et al. [35] propose a k -spectrum kernel for protein classification. Given the protein animo acid alphabet of 20 elements  X  A, R, N, D  X  X  X  X  , the k -spectrum is all the possible sequences of length k that are composed by the elements in the alphabet. For example, if k = 3, the k -spectrum contains ARN, AND, DCN, and so on. Given the alphabet A , a sequence x is transformed into a feature space by a transformation function where  X  a ( x ) is the number of times a occurs in x . The kernel function is the dot product of the feature vectors, By using a suffix tree algorithm [35], K ( x, y ) can be com-puted in O ( kn ) time.
 Lodhi et al. [43] propose a string kernel for text classifica-tion. Similar to the k -spectrum kernel in [35], the string kernel also uses a k -length sub-sequences but allows gaps. By using an exponentially decaying factor of the length of span of the subsequences occurring in the text, the gap is penalized. The kernel function is the dot product of the feature vectors and can be efficiently computed by dynamic programming. Leslie et al. [36] extends the k -spectrum ker-nel to handle mismatching. Sonnenburg et al. [55] propose a fast k-spectrum kernel with mismatching .
 One disadvantage of kernel based methods is that it is hard to be interpreted and hard for users to gain knowledge be-sides a classification result. Sonnenburg et al. propose a method to learn interpretable SVMs using a set of a string kernels [54]. The ideas is to use a weighted linear combi-nation of base kernels. Each base kernel uses a distinctive set of features. The weights represent the importance of the features. After learning the SVM, users can have an insight into the importance of different features.
 String kernels or k -spectrum kernel can be viewed as a fea-ture based method. Saigo et al. [49] propose a local align-ment kernel for protein sequence classification which can be viewed as a distance based method. Although local align-ment distance can effectively describe the similarity between two sequences, it cannot be directly used as a kernel function because it lacks the positive definiteness property. Saigo et al. [49] modify the local alignment distance and form a valid kernel called local alignment kernel, which mimics the be-havior of the local alignment. The theoretical connection between the local alignment kernel and the local alignment distance is proved. Given two sequences x, y , the local align-ment kernel K ( x, y ) can be computed by dynamic program-ming.
 Other kernels used for sequence classification include polynomial-like kernels [52], kernels derived from probabilistic model (Fisher X  X  kernel) [52], and diffusion kernels [50]. One category of sequence classification methods is based on generative models, which assume sequences in a class are generated by an underlying model M . Given a class of se-quences, M models the probability distribution of the se-quences in the class. Usually, a model is defined based on some assumptions, and the probability distributions are de-scribed by a set of parameters. In the training step, the parameters of M are learned. In the classification step, a new sequence is assigned to the class with the highest like-lihood.
 The simplest generative model is the Naive Bayes sequence classifier [37]. It makes the assumption that, given a class, the features in the sequences are independent of each other. The conditional probabilities of the features in a class are learned in the training step. Due to its simplicity, Naive Bayes has been widely used from text classification [29] and genomic sequences classification [11].
 However, the independence assumption required by Naive Bayes is often violated in practice. Markov Model and Hid-den Markov Model can model the dependence among ele-ments in sequences [17].
 Yakhnenko et al. [64] apply a k -order Markov model to clas-sify protein and text sequence data. In the training process, the model is trained in a discriminative setting instead of the conventional generative setting to increase the classification power of the generative model based methods.
 Different from Markov Model, Hidden Markov Model as-sumes that the system being modeled is a Markov process with unobserved states. Srivastava et al. [56] use a profile HMM to classify biological sequences. A profile HMM usu-ally has three types of states, inserting, matching and delet-ing. Aligned training examples are used to learn the tran-sition probabilities between the states and emission prob-abilities. The learned HMM represents the profile of the training dataset. A profile HMM may also be learned from the unaligned sequences by gradually aligning each example with the existing profile. For each class, a profile HMM is learned. In the classification step, an unknown sequence is aligned with the profile HMM in each class by dynamic pro-gramming. An unknown sequence will be classified into the class which has the highest alignment score. In this section, we review some closely related or extended problems of conventional sequence classification. Those ex-tensions are proposed to address the challenges when apply-ing sequence classification to different real world application scenarios, such as classifying a sequence using its prefixes to achieve early classification, classifying sequences by using both labeled and unlabeled data, and predicting a sequence of labels instead of a single label for streaming sequences. For temproal symbolic sequences and time series, the values of a sequence are received in time stamp ascending order. Sometimes, monitoring and classifying sequences as early as possible is desired. For example, in a retrospective study of the infants admitted to a neonatal intensive care unit, it is found that the infants had abnormal heart beating time se-ries pattern 24 hour before the doctor finally diagnosed them with sepsis [21]. As another example, Bernaille et al. [9] show that by only observing the first five packages of a TCP connection, the application associated with the traffic flow can be classified. The applications of online traffic can be identified without waiting for the TCP flow to end. Gener-ally, early classification of sequences may have applications in anomaly detection, intrusion detection, health informat-ics, and process control.
 To the best of our knowledge, Diez et al. [14] first mentioned the concept of early classification of time series. They de-scribe a time series by some relative literals, such as  X  X n-crease X  and  X  X tay X , and some region literals, such as  X  X l-ways X  and  X  X ometimes X  over some intervals. Each literal and its associated position are viewed as a base classifier. Ada boost [19] is used to ensemble the base classifiers. The ensemble classifier is capable of making predictions on in-complete data by viewing unavailable suffixes of sequences as missing features.
 Anibal et al. [8] apply a case based reasoning method to classify time series to monitor the system failure in a simu-lated dynamic system. The KNN classifier is used to clas-sify incomplete time series using various distances, such as Euclidean distance and Dynamic time warping (DTW) dis-tance. The simulation studies show that, by using case based reasoning, the most important increase of classification ac-curacy occurs on the prefixes through thirty to fifty percent of the full length.
 Although in [14; 8], the importance of early classification on time series is identified and some encouraging results are shown, the study only treat early classification as a problem of classifying prefixes of sequences. Xing et al. [62] point out the challenge of early classification is to study the tradeoff between the earliness and the accuracy of classification. The methods proposed in [14; 8] only focus on making predic-tions based on partial information but do not address how to select the shortest prefix to provide a reliable prediction. This makes the result of early classification cannot be easily used by users for further actions Xing et al. [62] formulate the early classification problem as classifying sequences as early as possible while maintaining an expected accuracy. A feature based method is proposed for early classification on temporal symbolic sequences. The major idea is to first select a set of features that are fre-quent, distinctive and early, and then build an association rule classifier or a decision tree classifier using those features. In the classification step, an oncoming sequence is matched with all rules or branches simultaneously until on a prefix, a matching is found and the sequence is classified. In this way, a sequence is classified immediately once the user expected accuracy is achieved. The methods proposed in [62] show some successes in handling symbolic sequences by achieving competitive accuracies using only less than half of the length of the full sequences.
 One disadvantage of the methods in [62] is that it cannot handle numeric time series well. Since numeric time series need to be discretized online, the information loss makes some distinctive features not easy to capture. Xing et al. [63] propose an early classifier for numeric time series by utiliz-ing instance based learning. The method learns a minimal prediction length (MPL) for each time series in the training dataset through clustering and uses MPLs to guide early classification. As shown in Section 2, 1NN classifier with Euclidean distance is a highly accurate classifier for time se-ries classification. One interesting property of the method in [63] is that without requiring a user expected accuracy, the classifier can achieve early classification while maintain roughly the same accuracy as a 1NN classifier using full length time series. There are usually more unlabeled data than labeled data. Some unlabeled data shares common features with labeled data and also contains extra features which may provide a more comprehensive description of a class. Therefore, by incorporating unlabeled data, sometimes, a more accurate classifier may be built.
 For text classification, there is a large amount of unlabeled data. Nigam et al. [46] propose a semi-supervised classifi-cation method to label documents. Initially, a Naive Bayes classifier is used to classify unlabeled examples in the first round. Then, an Expectation-Maximization (EM) process is utilized to adjust the parameters of the Naive Bayes clas-sifier and re-classify the unlabeled data in an iteration. The process terminates when the classification result is stable. One document may belong to several categories and have multiple labels.
 Besides text classification, Zhong et al. [66] propose a HMM based semi-supervised classification for time series data. The method uses labeled data to train the initial parameters of a first order HMM, and then uses unlabeled data to adjust the model in an EM process. Wei et al. [59] adopt one nearest neighbor classifier for semi-supervised time series classifica-tion. The method is designed to handle the situation where only a small amount of labeled data in the positive class is available. In the training step, at the beginning, all the un-labeled data is regarded as negative. Then, a 1NN classifier is applied to classify unlabeled data in iteration until the the stopping criteria is met. Wei et al. [59] propose a heuristic stopping criteria. In the iteration of labeling more time se-ries as positive, they observe that the minimum distance in the positive class will first decrease and then experience a plateau, and at last decrease again. The iteration will stop when the minimum distance in the positive class starting to decrease after the plateau.
 Weston et al. [60] propose a semi-supervised protein classi-fication method by using SVM with a cluster kernel. The kernel function between two sequences is defined as the dis-tance between two clusters of sequences. The two clusters are the neighborhoods of the two sequences, and the distance of the two clusters is the average pair-wise inter-cluster dis-tance. The neighborhood of a sequence may contain labeled and unlabeled sequences. By using the cluster kernel, the information of the unlabeled data can be utilized. The re-sults show that by adding unlabeled data, the cluster kernel works better than only using labeled data. As discussed in Section 1, for streaming sequence classifica-tion, instead of predicting one class label, it is more desirable to predict a sequence of labels. Kadous [24; 23] identifies this problem as strong sequence classification task but does not provide a solution for this problem.
 A closely related problem considered in natural language processing is called labeling sequences [31; 7; 20]. The task is to label each element in a sequence. For example, given a sentence, where each word is treated as an element, se-quence labeling is to assign each word to a category, such as name identity, noun phrase, verb phrase etc. The straight-forward solution is to label each element independently. An advanced solution is to consider the labels of the elements in a sequence related to each other. Sequence labeling prob-lem has been solved by using conditional random fields [31]. The problem has also been tackled by other methods, such as using a combined model of HMM and SVM [7] and using a recurrent neural network [20]. Sequence classification has a broad range of applications. For different application domains, the classification task has different characteristics. In this section, we summarize and compare major methods applied in several application do-mains. In recent years, a large amount of DNA and protein se-quences are available in public databases, such as GenBank [3], EMBL Nucleotide Sequence Database [1] and the Entrez protein database [2]. To understand the functions of differ-ent genes and proteins, sequence classification has attracted a lot of attention in genomic research.
 Feature based methods are widely used for genomic sequence classification [35; 12; 13; 52]. k -grams [35; 36; 12] and pat-tern based feature selection [52] have been used on genomic sequences. After obtaining features, conventional classifiers, such as SVM [35; 36; 52], rule based classifier [5] and neural networks [10] can be applied to classify genomic sequences. To measure the distance between two genomic sequences, global alignment and local alignment are widely used meth-ods [32; 45]. After obtaining the distance function, KNN classifier can be used for genomic sequence classification [13]. By using a local alignment kernel [49], SVM can also be used to classify protein sequences without feature selection. Model based methods, such as profile HMM [56], are also important methods for genomic sequence classification. Deshpande et al. [13] compare the performance of SVM, HMM, and KNN methods for classifying genomic sequence data. They find that SVM outperforms in most cases and feature selection plays an important role in determining ac-curacies of SVM classifiers. She et al. [52] also conclude that SVM is the most effective method for protein classification. Besides accuracy, other challenges in genomic sequence clas-sification are to speed up classification in order to handling a large amount of data [55] and to train an interpretable classifier to gain knowledge about characteristics of genomic sequences [54]. Time series data is an important type of sequence data. In Time Series Data Library [4], time series data across 22 do-mains, such as agriculture, chemistry, health, finance,industry, are collected. UCR time series data archive [27] provides a set of time series datasets as a benchmark for evaluating time series classification methods.
 For simple time series data, to apply feature based methods, the feature selection is a challenging task since we cannot do feature enumeration on numeric data. Therefore, distance based methods are widely adopted to classify time series [61; 26; 59; 48]. It is shown that comparing to a wide range of classifiers, such as neural networks, SVM and HMM, 1-nearest neighbor classifier with dynamic time warping dis-tance is usually superior in classification accuracy [61]. To apply feature based methods on simple time series, usu-ally, before feature selection, time series data needs to be transformed into symbolic sequences through discretization or symbolic transformation [40]. Without discretization, Ye et al. [65] propose a method to find time series shapelets and use a decision tree to classify time series. Comparing to distance based methods, feature based methods may speed up the classification process and be able to generate some interpretable results.
 Model based methods are also applied to classify simple time series, such as HMM which is widely used in speech recog-nition [47].
 Multivariate time series classification has been used for ges-ture recognition [24] and motion recognition [38]. The multi-variate data is generated by a set of sensors which measure the movements of objects in different locations and direc-tions. For multivariate time series classification, Kadous et al. [24] propose a feature based classifier. A set of user-defined meta-features are constructed and a multivariate time series is transformed into a feature vector. Some uni-versal meta-features include the features to describe the trends of increases and decreases and local max or min val-ues. By using those features, multivariate time series with additional non-temporal attributes can be classified by a de-cision tree. One multivairate time series can be viewed as a matrix. Li et al. [31] propose a method to transform a multivariate time series into a vector through singular value decomposition and other transformations. SVM is then used to classify the vectors. Sequence classification is also widely used in information re-trieval to categorize text and documents. The widely used methods for document classification include Naive Bayes [29] and SVM [43]. Text classification has various extensions such as multi-label text classification [67], hierarchical text classification [57] and semi-supervised text classification [46]. Sebastiani et al. [51] provide a more detailed survey on text classification . In this paper, we provide a brief survey on sequence clas-sification. We categorize sequence data into five subtypes. We group sequence classification methods in feature based methods, sequence distance based methods and model based methods. We also present several extensions of the conven-tional sequence classification. At last, we compare sequence classification methods applied in different application do-mains.
 We notice that most of the works focus on the classification task on simple symbolic sequences and simple time series data. Although there are a few works on multiple variate time series and complex symbolic sequences, the problem of classifying complex sequence data is still open at large. Furthermore, most of the methods are devoted to the con-ventional sequence classification task. Streaming sequence classification, early classification, semi-supervised classifica-tion on sequence data and the combinations of those prob-lems on complex sequence data which have practical appli-cations, present challenges for future studies. [1] Embl nucleotide sequence database homepage: http: [2] Entrez protein database homepage: http://www.ncbi. [3] Genbank homepage: http://www.ncbi.nlm.nih.gov/ [4] Time series data library webpage: http: [5] C. C. Aggarwal. On effective classification of strings [6] S. F. Altschul, W. Gish, W. Miller, E. W. Myers, [7] Y. Altun, I. Tsochantaridis, and T. Hofmann. Hidden [8] B. Anibal, S. M. Aranzazu, and R. J. Jose. Early fault [9] L. Bernaille, R. Teixeira, I. Akodkenou, A. Soule, and [10] K. Blekas, D. I. Fotiadis, and A. Likas. Motif-based [11] B. Cheng, J. Carbonell, and J.Klein-Seetharaman. Pro-[12] N. A. Chuzhanova, A. J. Jones, and S. Margetts. Fea-[13] M. Deshpande and G. Karypis. Evaluation of tech-[14] J. J. R. Diez, C. A. Gonz  X alez, and H. Bostr  X om. Boosting [15] H. Ding, G. Trajcevski, P. Scheuermann, X. Wang, [16] G. Dong and P. Jian. Sequence Data Mining , pages 47 X  [17] R. Durbin, S. R. Eddy, A. Krogh, and G. Mitchison. [18] O. Duskin and D. G. Feitelson. Distinguishing humans [19] Y. Freund and R. E. Schapire. A decision-theoretic gen-[20] A. Graves, S. Fern  X andez, F. Gomez, and J. Schmid-[21] M. P. Griffin and J. R. Moorman. Toward the early [22] X. Ji, J. Bailey, and G. Dong. Mining minimal dis-[23] M. W. Kadous. Temporal classification: extending the [24] M. W. Kadous and C. Sammut. Classification of mul-[25] L. Kaj  X an, A. Kert  X esz-Farkas, D. Franklin, N. Ivanova, [26] E. Keogh and S. Kasetty. On the need for time se-[27] E. Keogh, X. Xi, L. Wei, and C. A. Ratanama-[28] E. J. Keogh and M. J. Pazzani. Scaling up dynamic [29] S.-B. Kim, K.-S. Han, H.-C. Rim, and S. H. Myaeng. [30] D. Kudenko and H. Hirsh. Feature generation for se-[31] J. D. Lafferty, A. McCallum, and F. C. N. Pereira. [32] T. W. Lam, W.-K. Sung, S.-L. Tam, C.-K. Wong, and [33] T. Lane and C. E. Brodley. Temporal sequence learning [34] N. Lesh, M. J. Zaki, and M. Ogihara. Mining features [35] C. S. Leslie, E. Eskin, and W. S. Noble. The spectrum [36] C. S. Leslie and R. Kuang. Fast string kernels using [37] D. D. Lewis. Naive (bayes) at forty: The independence [38] C. Li, L. Khan, and B. Prabhakaran. Real-time clas-[39] M. Li and R. Sleep. A robust approach to sequence [40] J. Lin, E. J. Keogh, L. Wei, and S. Lonardi. Experienc-[41] N. Littlestone. Learning quickly when irrelevant at-[42] X. Liu, P. Zhang, and D. Zeng. Sequence matching for [43] H. Lodhi, C. Saunders, J. Shawe-Taylor, N. Cristianini, [44] S. Needleman and C. Wunsch. A general method ap-[45] L. A. Newberg. Memory-efficient dynamic program-[46] K. Nigam, A. McCallum, S. Thrun, and T. M. Mitchell. [47] L. Rabiner. A tutorial on HMM and selected applica-[48] C. A. Ratanamahatana and E. J. Keogh. Making time-[49] H. Saigo, J.-P. Vert, N. Ueda, and T. Akutsu. Pro-[50] B. Sch  X olkopf, K. Tsuda, and J.-P. Vert. Kernel Meth-[51] F. Sebastiani. Machine learning in automated text cat-[52] R. She, F. Chen, K. Wang, M. Ester, J. L. Gardy, and [53] T. Smith and M. Waterman. Identification of com-[54] S. Sonnenburg, G. R  X atsch, and C. Sch  X afer. Learning [55] S. Sonnenburg, G. R  X atsch, and B. Sch  X olkopf. Large [56] P. K. Srivastava, D. K. Desai, S. Nandi, and A. M. [57] A. Sun and E.-P. Lim. Hierarchical text classification [58] P.-N. Tan and V. Kumar. Discovery of web robot ses-[59] L. Wei and E. Keogh. Semi-supervised time series clas-[60] J. Weston, C. S. Leslie, D. Zhou, A. Elisseeff, and W. S. [61] X. Xi, E. Keogh, C. Shelton, L. Wei, and C. A. [62] Z. Xing, J. Pei, G. Dong, and P. S. Yu. Mining sequence [63] Z. Xing, J. Pei, and P. S. Yu. Early classification on [64] O. Yakhnenko, A. Silvescu, and V. Honavar. Discrimi-[65] L. Ye and E. Keogh. Time series shapeletes: A new [66] S. Zhong. Semi-supervised sequence classification with [67] S. Zhu, X. Ji, W. Xu, and Y. Gong. Multi-labelled clas-
