 Many popular machine learning approaches such as RankSVM [1], AdaRank [2], cause it is used for all different queries. A single global ranking model is simple and such a query-dependent model a local model because it uses only local information to build a ranking function. The local model is good for some sort of queries but unsta-and the selection depends on the semantics of each query and documents. 
For example, a searcher wants to search information about Apple X  X  products such as iPad and Macbook. If the searcher uses keyword  X  X pple X  to search, it is safer to use a global model. Using  X  X ech-Apple X  local model or  X  X ruit-apple X  local model is risky word  X  X pple ipod X  to search, using the  X  X ech-Apple X  local model to rank documents is selecting good local models to rank documents. 
There are two possible ways to use local models. First, we can use the information of a query to find one best local model to rank documents for this query. Second, we can aggregate the results of many local models to balance the risk of using a bad local model and use the information of this query to determine the weights of aggregation. In these two alternatives, how to extract the information of a query is fundamental and crucial. We call this a  X  query representation  X  problem in this paper. Finding a good mation is interesting to a user for an unseen query. However, if the framework could represent queries correctly, it would improve the performance much. gate two different query representation schemes. We also analyze the upper-bound of selecting the best local model, estimate the challenging of these approaches, and dis-cuss the results from the viewpoint of query difficulty. The experimental results show that we propose the best aggregation approaches in LETOR4 datasets, and our ap-proach has a better performance to deal with hard queries.
 shows the performance of each approach and gives the detail analyses. Section 4 summarizes the results. A query dependent ranking framework consisting of training and testing steps de-scribes how to derive and use local models. Three query dependent ranking ap-proaches, named as Na X ve , SelectRanker and Transform , are proposed under the framework. In the following, we first introduce the framework and then describe each approach in details. 2.1 Framework A query dependent ranking framework is shown in Figure 1. The corresponding train-ing and testing steps are described as follows. Training Step. The first step of our framework is to build local models. For this pur-ters, and use RankSVM [1] to train local mode ls from the individual clusters. In other depending on which approach, i.e., the Na X ve , SelectRanker or Transform approaches is used, will be discussed later. similarity, which depends on the Na X ve , SelectRanker and Transform approaches, will be discussed later. The rank aggregation function is defined by Equation (1). where m is the number of related to test query  X   X  ,  X   X   X  i ,  X   X  is the weight of local and  X   X  , represents the predi a document d is calculated b 2.2 Three Approaches The first approach named N best local model to rank t h approaches, i.e., SelectRa n The Na X ve approach, whic h Equation (1) where the b e models have  X  weight  X   X  Equation (1) can be used t We specify these approach e Na  X   X  ve. Algorithm 1 select to determine the ranks of d in global model. The m input clusters in A First, the relevant query-do c these relevant q-d pairs a r Euclidean distance as a si m in cluster i , add all releva n the cluster is represented b y d pairs in this cluster, a n RankSVM with cluster i as SelectRanker. This approach aggregates scores from local models. The weight of cluster i is determined by the top-k most similar queries in this cluster. Algo-rithm 2 describes the procedure to calculate  X  weight  X   X  of cluster . Algorithm 2. SelectRanker approach Input: (1) Test query  X   X  Output:  X   X  : i =1, ... ,m 
The Kullback-Leibler divergence shown in Eq uation (2) is used to measure the dis-tance between local model and base model for a given query. Peng et al. [5] showed that KL divergence is useful in information retrieval. For the n retrieved documents of ment d in  X   X  and  X   X  , respectively. 
Step 3 in Algorithm 2 filters out non-relevant queries. Step 4 computes the average precision as the confidence of a query, and sums the confidences as the weight of this cluster. We choose BM25 as the base ranking model  X   X  in the experiments. 
After determining the weights of local models, we apply Equation (1) to merge the ranking scores of each local model to determine the final scores of documents. Algorithm 1. Na  X   X  ve approach 
Steps: (1) For test query  X   X  , use the global RankSVM model to find query  X  s top -t Output: r : the best local model, i.e., the loca l model of the most similar cluster. Transform. This approach aggregates scores from local models too, but it uses differ-ent scheme to represent queries and feeds the transformed queries into a clustering algo-two global ranking models to transform query X  X  representation. This is also motivated by Peng et al. [5]. Algorithm 3 describes how to determine the weight of each cluster experiments, we let  X   X  and  X   X  be BM25 and global RankSVM model. Then we cluster those 2-dimension query vectors using K-means clustering with Algorithm 3 to get the weights of the clusters for a test query  X   X  , and the documents distance between  X   X  and test query  X   X   X  . Algorithm 3. Transform approach Input: (1) Test query  X   X  Output:  X   X  : i =1, ... ,m 2.3 Time Complexity The time complexity of SelectRanker is the largest one of the three approaches. Com-dimensions of q-d pairs, #q -d is the maximum number of q-d pairs of queries in train-ing set, and m is the number of local models. The other two extra steps are steps 3 and 4 of Algorithm 2, which takes  X O X   X   X  X  X  X  X  X n X  . In general settings, m is a predefined constant. Thus it does not add too much cost in response time. 3.1 Dataset and Para meter Selection We adopt LETOR4 dataset [6] for our experiments. It contains two collections: MQ2007 and MQ2008. MQ2007 and MQ2008 have 1,692 and 784 queries with total mined by the validation set to have the best mean average precision (MAP), and are applied to the testing set. 3.2 Performance Comparison with LETOR4 Baselines We compare our aggregation approaches to LETOR4 rank aggregation baselines, and We can see that the proposed three approaches are better than all baselines. The two than Na X ve approach. We will analyze the results later in details. 
In Table 1, the four LETOR4 rank aggregation baselines adopted the approaches proposed by Qin et al. [7]. They used MQ2007-agg and MQ2008-agg while we use MQ2007 and MQ2008 in our experiments. MQ2007/MQ2008 and MQ2007-ent. MQ2007-agg and MQ2008-agg used order as feature, where the order is from MQ2007-agg and MQ2008-agg. Comparatively, we use score-based rank aggregation approaches in MQ2007 and MQ2008. The performance of our approaches is better. 
Although the performances of our approaches are far beyond the LETOR4 rank ag-gregation baselines, we want to know further the comparison to LETOR4 global models. We show the results in Table 2, where  X + X  after a score means our proposed approach outperforms the RankSVM baseline, and N.@k, MeanN., and AdaRank denote abbreviations of NDCG@k, MeanNDCG, and AdaRank-MAP algorithm, respectively. Although the results with + are better than the RankSVM baseline, they did not pass the significant test. 
We can see that the Transform approach has the best MAP and the SelectRanker approach has the best P@1, P@2 and P@5 in MQ2007. If the metric is NDCG, the SelectRanker approach has the best performance in MQ2007, and its performance in MQ2008 is better than RankSVM baseline, which is used to build our local models. 
In MQ2008, only the SelectRanker approach has the best P@2. If we compare our results to RankSVM, most performance of the SelectRanker and Transform approach-es are better than that of RankSVM. On the other hand, the Na  X   X  ve approach does not model did not perform well because of our poor query representation scheme. We show our analysis in next section. 3.3 Analysis of Selecting the Best Local Model good query representation scheme? If we have a better query representation scheme, the analysis will give us some interesting insights. The result is shown in Fig. 2. 
In Fig. 2, the x-axis is the number of clusters, and the line of RelevantOnly is the query representation scheme that uses only relevant documents. We can see that if we have a better query representation scheme, the Na X ve approach can beat RankSVM introduce a lot of noise, and query representation scheme is the main reason why the Na X ve approach did not perform well. 
We are also interested in the upper bound of selecting the best local model approach. In the Na X ve approach, if we can always select the best cluster that results in the best average precision (AP) of a query, what is the performance? The line of Up-perBound in Fig. 2 shows the situation. We can see that there is a lot of room for im-provement. But is it easy to achieve improvement? We further answer this question in Fig. 3. 
In Fig. 3, we fix the number of clusters to be 100. We calculate AP of a query by query among the 100 local models. If we always select the best AP, we get the upper performance of RankSVM baseline is around in selecting the 19-th best AP out of 100 clusters. In other words, if we can always select one of the top-19 clusters out of 100 We believe that it is achievable, but we leave it as future work. 3.4 Analysis in Terms of Query Difficulty TREC held a hard track 1 in 2003 and 2005. The goal of the hard track is to improve across the runs in previous TRECs. 
In this paper, the proposed algorithms heavily rely on the relevant documents of a query, so we define the difficulty of queries in terms of the number of relevant docu-ments. A hard query is defined to be a query with the number of relevant documents less than 4; an easy query is defined to be a query with the number of relevant docu-and 20 is regarded as a normal query. We compare the performance of RankSVM and our approaches at the three difficult levels. The query distribution with different num-ber of relevant documents of MQ2007 is shown in Fig. 4, and the results in terms of various difficulty levels are shown in Table 3. 
In Fig. 4, we can find there are lots of queries with zero relevant documents, so we could ignore these queries in Table 3. According to our statistics, the ratios of queries at the hard, normal, and easy levels in MQ2007 are 17%, 47% and 21%, respectively, and their distributions in MQ2008 are 38%, 31%, and 2%, respectively. 
In Table 3, we use bold to emphasize the cases in which our approaches are better than the baseline. We can find that our approaches perform worse than RankSVM in easy queries. But the performances of our approaches are better than RankSVM in hard and normal queries. This is an important property because doing well in hard and when the relevant documents are paucity, user may feel happier. We believe that this advantage comes from the nature of aggregation approaches. Aggregation approaches queries shown in Table 4. 
In Table 4, the AP in bold means our approach outperforms RankSVM, and under-line of word in query terms means the query term is misspelled. We can find that the performance of our approaches is better than RankSVM when query term lose alpha-can prevent very poor performance. On the other hand, an aggregation approach is not as good as the baseline when the baseline has very high AP. This is also the nature of aggregation approaches because some non-relevant local models are introduced. How to filter out non-relevant local models is the key to improve the aggregation approaches. Many learning to rank techniques have been proposed such as RankSVM [1], Ada-Rank [2], Topical RankSVM [8], ListNet [3] and RankBoost [9]. These algorithms can be classified in terms of different criteri a. The first four algorithms are classified RankBoost is classified as non-linear model because it combines base learners to rank documents. Researchers also classify the learning to rank algorithms into point-wise, pair-wise, and list-wise approaches [10] based on the way the learners use the training and integrates base learners X  results to produce final ranking, is a special type of algo-rithms among these classifications. 
Dwork et al. [11], Peng et al. [5] and Farah and Vanderpooten [12] proposed rank aggregation approaches to enhance web systems. The meta-search approaches [11] can be viewed as aggregation approaches, in which they aggregate results from differ-ent search engines. The difference between these aggregation approaches and our approaches is the use of base learners. They all use global models as base models. In general, if base learners have good results for all different kinds of queries, aggrega-aggregating global models. We leave this issue as future work. works used learning approaches to find best weighting scheme in clusters [8, 14] and pre-defined query categories [13]. 
Geng et al. [4] proposed a good local model approach named as query-dependent approach. Their approach of generating query representation is much like the algo-fixed number of local models instead. In this paper, we propose three query dependent ranking approaches to improve re-trieval performance, and the experiments show our approaches are better than all LETOR rank aggregation approaches and are comparable to global machine learning techniques. We conduct many detail analyses on the proposed approaches, and find that our approaches perform well in hard and normal queries. We also found that se-lecting the ideal local model has a very high upper bound, and there is a lot of room to improve this kind of approaches. Finally, our analyses showed that query representa-tion and filtering out non-relevant local models are two key issues to improve perfor-mance of using local models. 
Several issues need to be further studied in the aggregation framework. The alterna-the future work. Acknowledgment. Research of this paper was partially supported by National Science Council (Taiwan) under the contract NSC 98-2221-E-002-175-MY3. 
