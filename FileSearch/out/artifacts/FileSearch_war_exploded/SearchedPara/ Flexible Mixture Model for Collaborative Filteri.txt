 Luo Si LSI@CS.CMU.EDU Rong Jin RONG@CS.CMU.EDU The rapid growth of the information on the Internet demands intelligent information agent that can sift through all the available information and find out the most valuable to us. These intelligent systems can be categorized into two classes: c ollaborative filtering (Breese, Heckerman &amp; Kadie, 1998) and c ontent-based recommending (Basu &amp; Hirsh, 1998). The difference between collaborative filtering and content-based recommending is that: collaborative filtering only utilizes the ratings of training users in order to predict ratings for test users while content-based recommendation systems rely on the contents of items for predictions. Therefore, collaborative filtering systems have advantages in the environments where the contents of items are not available due to privacy issues or where that contents are difficult for a computer to analyze. In this paper, we only focus on the collaborative filtering problems. Most collaborative filtering methods fall into two categories: memory-based algorithms and model-based algorithms. Memory-based algorithms usually do not have a training phase. Instead, they simply store rating examples of users into a training database. In the predicting phase, the memory-based approaches first find users in the training database similar to the test user and then, predict the test user X  X  ratings based on the corresponding ratings of these similar users. On the contrary, model-based algorithms build models that are able to explain the training examples well and predict the ratings of test users using the estimated models. Both of the memory-based algorithms and the model-based algorithms have their advantages and disadvantages. Memory-based algorithms have much less off-line computation costs while the model-based algorithms may have less on-line computation costs. Though memory-based and model-based approaches differ from each other in many aspects, both of them assume that users with similar tastes should rate items similarly and therefore the idea of clustering is used in both approaches either explicitly or implicitly. For memory-based approaches, training users similar to the test user are grouped together and their ratings are combined to predict ratings for the test user. Meanwhile, model-based approaches cluster items and/or training users into classes explicitly and predict ratings of a test user by simply using the ratings of classes that fit in best with the test user and/or items to be rated. Thus, how to cluster users and items appropriately is a key issue in designing collaborative filtering systems, which can affect the scalability, robustness and performance. While theoretically interesting, model-based approaches have achieved mixed results in previous studies (Breese et al., 1998; O X  X onnor &amp; Herlocker, 2001). We suspect that this may be due to the inappropriate clustering algorithms used in their studies. More specifically, three issues of clustering algorithms are important for collaborative filtering: First, both users and items need to be clustered and more importantly, users and items are coupled with each other through the rating information. Therefore, a good clustering algorithm should be able to explicitly model both classes of users and items and be able to leverage their correlation. Secondly, many clustering techniques assume that each user or item belongs to a single class. However, since a user may have diverse interests and an item may have multiple aspects, it is desirable to allow both items and users to be in multiple classes simultaneously. In this paper, a flexible mixture model (FMM) is proposed in order to capture this idea. Thirdly, the assumption that users with similar tastes would have similar ratings may not necessarily be true because some users may tend to give a higher rating to all items than some others. In order to account for the variance in the rating patterns among the users with similar interests, we extend the flexible mixture model by introducing an additional new hidden  X  X reference X  node. Such an extension allows us to infer the preference values underlying the surface ratings, which can then be used as (presumably more reliable) evidence for clustering. For the simplicity of computation, we compute the preference information for each user using  X  X ecoupled models X  (DM), and then apply the proposed flexible mixture model to cluster over the estimated preference values instead of the original rating values. The rest of the paper is arranged as follows: Section 2 discusses previous work. The proposed flexible mixture model for collaborative filtering is presented in Section 3. The extended version of flexible mixture model with the decoupling of rating and preference is discussed in Section 4. Section 5 presents experiments results. Conclusions and future work are discussed in Section 6. Let us first introduce the annotations that will be used for the rest of this paper. Let } ,......, { items, } ,......, { the ratings information in the training database, ) ( y X be item x by user y , and y R 2.1 Memory-Based Algorithms Two commonly used memory-based algorithms are Pearson Correlation Coefficient algorithm (PCC) (Resnick et al., 1994) and Vector Space Similarity (VS) (Breese, Heckerman &amp; Kadie, 1998) algorithm. The main idea of these two algorithms is to calculate the similarities of the training users to the test user and the prediction of ratings is computed by performing a weighted average of deviations from the training users X  mean. The difference between them is on how to compute the similarities between users, where a Pearson correlation coefficient is used for measuring the user similarity in the PCC algorithm and a cosine similarity is computed in the VS algorithm. More details can be found in (Resnick et al., 1994; Breese, Heckerman &amp; Kadie, 1998). 2.2 Model-Based Algorithms Three model-based algorithms are discussed here: the aspect model (AM) (Hofmann &amp; Puzicha, 1999), two-sided clustering model (Hofmann &amp; Puzicha, 1999) and the Personality Diagnosis model (PD) (Pennock et al., 2000). Aspect Model (AM) The aspect model (Hofmann &amp; Puzicha, 1999) is a probabilistic latent space model, which models individual preferences as a convex combination of preference associated with each observation pair of a user and an item. The aspect model assumes that users and items are independent from each other given the latent class variable. Thus, the probability for each observed pair (x,y) is calculated as follows: where P(z) stands for class prior probability, P(x|z) and P(y|z) stand for class dependent distributions for items and users respectively. Essentially, the preference pattern of a user is modeled by a combination of typical preference patterns, which are represented by the distributions of P(z), P(x|z) and P(y|z). Note that the aspect model only introduces one set of latent variables  X  X  X  for the purpose of clustering and there is no explicitly grouping of either users or items. In the proposed model, we intentionally introduce two sets of latent variables in order to model the clusters of users and the clusters of items separately. Two of the choices (Hofmann &amp; Puzicha, 1999) to incorporate the ratings  X  r  X  into the aspect model are expressed in Equation (2) and (3), respectively. The corresponding graphical models for Equation (2) and (3) are shown in Figure 1 as model (a) and (b) respectively. According to the graphic models, the difference between these two methods is that, in the graphic model (a) (or Equation. (2)), the rating r conditioned only on the latent class variable  X  X  X , while the second model let the rating r (l) be conditioned on both the latent class variable  X  X  X  and the item x (l) . The second model is a refined version of the first model, but the number of parameters is much larger than the first model with the same number of aspects. Two-Sided Clustering Model A two-sided clustering model is proposed for collaborative filtering in (Hofmann &amp; Puzicha, 1999). This model assumes that each user should belong to exactly one group of users and the same is true for each item. Let } ,....., { variables iv I and ju J } 1 , 0 {  X  indicates whether the i th user belongs to the v th user class and the j th item belongs to the u th item class respectively. Then, the joint probability P(x,y,r) is defined as: C is the cluster association parameter. In order to be consistent with the above assumption, a global normalization constraint has to be made as (Hofmann &amp; Puzicha, 1999) pointed out that this model has a different spirit from the aspect model and is less flexible in modeling the preferences relationship between the users and items, and we believe the key reason is its strong assumption. However, this model does try to model the clustering of users and items separately, which appears to be a better modeling approach. Since previous experiments have shown that the performance of aspect model is substantially better than the two-sided clustering model, we will not compare our model with the two-sided clustering model. Personality Diagnosis Model (PD) In the personality diagnosis model (Pennock et al., 2000), the observed rating for the test user y t on an item x is assumed to be drawn from an independent normal distribution with the mean as the true rating as ) ( x R where the standard deviation  X  is set to constant 1 in our experiments. Then, the probability of generating the observed rating values of the test user by any user y in the training database can be written as: The likelihood for the test user y t to rate an unseen item x as category r can be computed as: The final predicted rating for item  X  x  X  by the test user will be the rating category  X  r  X  with the highest likelihood method is able to outperform several other approaches for collaborative filtering (Pennock et al., 2000). In this section, we introduce the flexible mixture model (FMM) and show how it can be applied to the collaborative filtering task. 3.1 Model Description The FMM for collaborative filtering is motivated by the following observations on the two-sided clustering model and the aspect model. Compared with the two-side clustering model, the aspect model has the flexibility of letting each user and item belong to multiple groups simultaneously while the two-sided clustering model restricts each user and item to be in exactly one cluster. This issue can be quite important for the collaborative filtering task because there may not be a set of underlying clusters for users and items that are exclusive from each other. Most likely, we will see overlapping clusters, which lead to multiple memberships for users and items. For example, the film  X  X ora! Tora! Tora! X  may be deemed as a  X  X ar movie X  by a young man due to its intensive war scenes while a veteran may treat it as a  X  X istorical film X  because of the historical events described in the movie. Clearly, the non-exclusive nature between the category  X  X istorical movie X  and  X  X ar movie X  leads to the multiple membership for this movie. On the other hand, the two-sided clustering model is able to explicitly model the clusters of users and items, which appears to make sense. Based on these observations, we propose the flexible mixture model (FMM) for collaborative filtering, which tries to address the two issues, namely allowing each user and item to be in multiple clusters and modeling the clusters of users and items separately. Let } ,....., {
Z indicates the class membership for user  X  y  X  and user classes; Latent variable x Z indicates the class multinomial distribution on the item classes; ) | ( X Y P(x|Z) P(Z) P(y|Z) describing the conditional probability of users Y given a specific user class y Z ; ) | ( x Z X P , 1 ( M X  X   X  ) 1 J Z is a multinomial distribution describing the conditional probability of items X given a specific item class multinomial distribution for the ratings  X  r  X  given a specific user class y Z and a specific item class x Z . With above annotation, the joint generation probability P(x,y,r) for FMM can be written as: The corresponding graphical model is shown in Figure 2. According to the model, the FMM differs from the aspect model in that it explicitly models the user classes and the items classes with two sets of latent variables {
Z }. The FMM model is different from the two-sided clustering model by the fact that it does not have the global normalization restriction in Equation (5). The graphical model most similar to the proposed FMM model is the product space mixture model (PSMM) ( Hofmann &amp; Puzicha, 1998), which was proposed for information retrieval. But the PSMM model only extends the aspect model by enforcing a decomposition of aspects that sum up to 1, while our FMM has two sets of latent variables  X  X x X  and  X  X y X , which normalized separately. 3.2 The Training Procedure The Expectation and Maximization (EM) (Dempster &amp; Rubin, 1977) algorithm is a well-known optimization algorithm, which alternates between two steps: In the expectation step, the joint posterior probabilities of the latent variables { Zx , Zy } are computed; in the maximization step, the model parameters are updated given the posterior probabilities estimated in the expectation step. More specifically, in the expectation step, the joint posterior probabilities are computed as: Then, the model parameters are updated in the maximization step as: In order to avoid the unfavorable local maximum problems, we use a general form of the EM algorithm named annealed EM algorithm (AEM) ( Hofmann &amp; Puzicha, 1998), which is an EM algorithm with regularization. In this algorithm, the training database is divided into two parts: the training data and the held-out data. In the expectation step, a variable  X  X  X  is introduced to control the training process as: When variable  X  X  X  goes to positive infinity, the posterior probability becomes a delta function and the clustering process becomes the hard case (each user and item belong to a single class). When variable  X  X  X  is set to 1, the AEM returns back to the original EM algorithm in Equation (10). Therefore, by varying the variable  X  X  X , we can adjust the clustering process. In our procedure, b is initially set to 1. We perform EM algorithm with early stopping if the performance on hold-out data deteriorates. Then the variable b is decreased ( b=0.9*b) and the EM is applied again until b is smaller than a lower bound (0.5). Finally a new model is trained over the whole training data (including the held-out dataset) with the current b value for several steps. A similar training procedure is applied to train the aspect models as described in Section 2.2. 3.3 The Prediction Procedure The ultimate goal of the collaborative filtering is to set of observed ratings of the test user t y : ) ( t y X = P(x|Zx) P(y|Zy) the number of given ratings of the test user t y . A  X  X old-in X  process can be used to make the prediction. The main idea of this process is to estimate the joint probability of the rating, item and the test user as ) ( ^ x R t y . The joint probability is calculated as: We have all the variables on the right hand side of Equation (17) from the training process except ) | ( y t which can be computed by simply treating the test user y as another user in the training database and run the EM algorithm as described above with all the parameters With the estimated joint probability ) , , ( r y x P prediction of rating on item  X  x  X  can be computed as: The other key issue with the collaborative filtering is that users with similar or even identical taste (preference) on the items may give very different surface ratings. For example, two movie viewers A and B may have exactly the same taste on the films, which means they both like the same set of films and disfavor another same set of films. But the viewer A can be quite strict on his rating standard and may rate most of his favorite films only as rating  X 3 X  and rate all of his disfavored films with rating  X 1 X . On the hand, the viewer B is a quite tolerating person and may rate most of his favorite films with highest rating  X 5 X  and even rate his disfavored films with rating  X 3 X . In order to account for the variance in the rating behavior among the users with similar interests, we extend the graphical model in Figure 2 by introducing a new latent node  X  X  X , which accounts for the  X  X rue X  preference values ,and node  X  X r X , which accounts for different rating behaviors. First, according to Figure 3, the user node  X  X  X  is determined jointly by nodes  X  X y X  and  X  X r X , namely users are distinguished from each other both by their interests encode in  X  X y X  and by their rating patterns encoded in  X  X r X . Secondly, the preference node  X  X  X  is determined jointly by the node  X  X x X  and node  X  X y X , e.g. P(v|Zx, Zy). Thirdly, the rating is generated by the preference node and the node  X  X r X . Therefore, it is not necessary that an item with a high rating be truly favored by the user due to the dependency of node  X  X x X  and  X  X y X . Finally, in the extended version of FMM model, two latent variables  X  X x X  and  X  X y X  are not coupled through the rating information  X  X  X  as in Figure 2. Instead, they are connected directly through the preference node  X  X  X  and the rating information  X  X  X  can influence the correlation between nodes  X  X x X  and  X  X y X  indirectly through the preference node  X  X  X . Let the preference variable  X  X  X  be a continuous random variable ranging from 0 to 1(e.g. 1 0  X   X  v ). With the new graphic model, the joint probability can be calculated as: where ) ( r Z P and ) , | ( v Z r P distributions, and ) , | ( Unfortunately, due to the introduction of the preference node  X  X  X  and the latent rating node  X  X r X , the inference and prediction processes are rather complex and time-consuming. Considering that the collaborative filtering task usually demands high efficiency, instead of using the above complex graphical model directly, we explore a simple model that is able to compute the preference values for a user given a set of rated items of that user. With this simple model, we are able to obtain the preference value  X  X  X  directly instead of having to infer it from the graphic model in Figure 3. Then, a FMM model similar to Figure 2 can be used for computing the ratings for the test user by simply replacing the rating node  X  X  X  with the preference node  X  X  X . 4.1 The Decoupled Models of Preferences and Ratings So we need to calculate the preference value on an item with specific rating  X  X  X  for a user who gives a set of rated of flexible mixture model to group rating patterns .
 P(x|Zx) items. Two factors can influence this value: 1) the percentage of items that have been rated no more than  X  r  X . The larger the number of items rated no more than  X  r  X , the more likely that the user prefers the item. 2) The percentage of items that have been rated as  X  r  X . The larger the number of items rated as  X  r  X , the less likely that the item is preferred by the user. Based on this intuition, we likelihood function for user y on an item, which he rates the rating count vector, which counts how many times the user y has rated items with specific ratings. ) ( _ r R P computed as: This procedure can be seen in another way as somehow normalizing the user ratings into user preference. When there are very few given ratings from the user, Equation (20) may become unreliable. A better solution is a smoothed version of the ) | ( y r Rating P = function, which utilizes the rating patterns of similar users. The similarity coefficient between the user y and y X  is defined as the probability of mistaking user y X  given the rating patterns of user y: The last step is derived by assuming a uniform distribution on P(y). And ) ' | ( y C P y is computed as: The smoothed version of ) | ( y r Rating P = is as follows: Plugging Equation (23) into Equation (20), we can convert a rating  X  r  X  given by a user  X  y  X  into the likelihood of being preferred. Now, let us consider the opposite, namely how to convert an estimated likelihood of being preferred ) ( x V y into an appropriate rating  X  r  X . We simply find the rank that leads to the preference probability 4.2 The Combination of the FMM and the DM The basic idea of combining FMM with the decoupled models (DM) is to, first convert the ratings in the training database into their corresponding preference values using the DM model. Then similar to the graphic model in Figure 2, a FMM is built over preference values instead of the ratings (e.g. replacing the rating node  X  X  X  with the preference node  X  X  X  in Figure 2). More specifically, the joint probability P(x,y,v) for a user  X  y  X , an item  X  x  X , and a preference value  X  v  X , is computed as: where v (l) is the preference value computed from Equation (20). Compared with Equation (10), Equation (25) contains term ) , | ( ) ( y x l Z Z v P instead of ) , | ( For simplicity, we assume ) , | ( ) ( y x l Z Z v P to be a normal distribution, i.e. The updating equations for the means and standard deviations are derived by AEM algorithm in Section 3.2. With this modified FMM model using preference value, we will be able to compute the estimated preference value  X  v  X  for an item x given the user y, e.g. ) ( rating is computed by converting the estimated preference value ) ( ^ x V y into rating ) ( ^ x R y using Equation (24). In this section, we will present experiment results in order to address two issues. 1) Is the FMM more effective than other collaborative filtering algorithms? In the experiment, we will compare the proposed FMM model to other popular algorithms. 2) Can the FMM be further improved by combining it with the decoupled models (DM) as discussed in Section 4? In the experiment, we will compare the performance of FMM with and without the decoupled models (DM). Two datasets of movie ratings are used in our a subset of 2000 users with more than 40 ratings was extracted. The details of these two datasets are listed in Table 1. To compare different algorithms in a large spectrum, we tried several different configurations. For the MovieRating testbed, we set the first 100 or 200 users to be the training users. For the EachMovie testbed, the first 200 or 400 users were used. Furthermore, 5, 10 or 20  X  X  X  X   X  items were provided as exposed items for a test user on both these two testbeds. As we believe that it is had for collaborative filtering system to collect huge amount of training data before it can provide recommendation service to the customers (so it is more important to evaluate the system performance with a limit number of training users), a relatively small number of training users were used in our experiments. But other experiments with more training users (300 and 400 for MovieRating, 800 and 1000 EachMovie) were conducted. As the limit of space, the results are not reported here but they are consistent with the results reported (the proposed FMM model got the best performance in all cases). The evaluation metric used in our experiments was the commonly used mean absolute error (MAE), which is the average absolute deviation of the predicted ratings from the actual ratings on items the test users have voted. where Test L is the number of the test ratings. 5.1 Experiment Results The first set of experiments is shown in Table 2 and Table 3. In addition to the proposed FMM model, two memory-based and three model-based approaches are evaluated. They are: the Pearson Correlation Coefficient method (PCC), the Vector Similarity method (VS), the aspect model using extension Equation (2) (AM_a), the aspect model with extension Equation (3) (AM_b), and the Personality Diagnosis model (PD). The two-sided clustering model is not included because previous studies have shown that its performance is substantially inferior to the aspect models. The number of user classes and the number of item classes in the FMM were set to 10 and 20 for users and items separately without much tuning. (Varying the number of classes from 5*10 to 20*40 gives us similar results to those reported in Table 2 and 3). According to Table 2 and 3, the proposed new FMM performs better than all the other algorithms on all different configurations in terms of the MAE. Furthermore, consistent with (Pennock et al., 2000), the PD method achieves the second best performance, and is generally better than the other four methods (except AM_b on Each Movie with 400 training users). The number of aspects in the AM_a and AM_b were turned for the best performance (shown in italic in Table 2 and 3). As for the same number of aspects, AM_b has much more parameters than AM_a, thus it can be seen that the optimal number of aspects in AM_b model is smaller than AM_a. Since the proposed FMM model is similar to the aspect model except for the explicitly modeling of user and item clusters, we attribute the good performance of FMM to its ability of modeling the classes of users and items separately. Furthermore, the difference between the proposed model and the two-sided clustering model suggests that it is beneficial not to assume that each user (item) belongs to a single class. The second set of experiments is shown in Table 4 and Table 5, where we compare the performance of FMM with and without the decoupled models (DM). It is clear that the combination of FMM model with DM model outperforms the basic FMM in all configurations. Therefore, it is important to cluster users with similar preference patterns instead of rating patterns. It is interesting to further explore efficient inference and prediction algorithms for the graphical model in Figure 3, which is able to simultaneously group users with similar preference patterns, rating patterns and items with similar characteristics and therefore may result in even more improvement in the prediction accuracy. Partition or clustering techniques have been studied intensively in the previous work for collaborative filtering. In this work, we proposed a formal graphical model for collaborative filtering, named flexible mixture model (FMM). The new model tries to address three issues in collaborative filtering: 1) explicitly modeling both classes of users and items by taking into account their correlations; 2) allowing each user and item to belong to multiple clusters simultaneously; 3) clustering users with similar preference patterns instead of rating patterns. Experiments on two common testbeds with several different configurations indicated that the proposed model is able to outperform five other algorithms for collaborative filtering task substantially. The combination method of the flexible mixture model and the decoupled models is rather preliminary. As future work, we plan to explore better efficient approximation algorithms for inference and prediction with the complex graphical model, which can simultaneously group users with similar preference patterns, rating patterns and items with similar characteristics. Finally, the proposed FMM model is only applied and evaluated on the problem of predicting item rating for new users; we hope to extend this model for other tasks such as recommending new items to known users in the further work. We thank ChengXiang Zhai for his helpful discussion, Paul Ogilvie and Jamie Callan for their useful comments and help to improve the English in this paper. Basu, C., &amp; Hirsh, H. (1998). Recommendation as classification: Using social and content-based information in recommendation. In the Proceedings of Fifteenth National Conference on Artificial Intelligence. Breese J. S., Heckerman D., Kadie C. (1998). Empirical Analysis of Predictive Algorthms for Collaborative Filtering. In the Proceeding of the Fourteenth Conference on Uncertainty in Artificial Intelligence. O X  X onnor, M. &amp; Herlocker, Jon. (2001). Clustering Items for Collaborative Filtering. In the Proceedings of SIGIR-2001 Workshop on Recommender Systems, New Orleans, LA. Dempster, A. P., Laird, N. M., &amp; Rubin, D. B. (1977). 
Maximum likelihood from incomplete data via the EM algorithm . Journal of the Royal Statistical Society, B39: 1-38. Hofmann, T., &amp; Puzicha, J. (1999). Latent Class Models for Collaborative Filtering. In the Proceedings of International Joint Conference on Artificial Intelligence. Hofmann, T., &amp; Puzicha, J. (1998). Statistical models for co-occurrence data (Technical report). Artificial Intelligence Laboratory Memo 1625, M.I.T. Pennock, D. M., Horvitz, E., Lawrence, S., &amp; Giles, C. L. (2000). Collaborative Filtering by Personality Diagosis: A Hybrid Memory-and Model-Based Approach. In the Proceeding of the Sixteenth Conference on Uncertainty in Artificial Intelligence. Resnick, P., Iacovou, N., Suchak, M., Bergstrom, P., &amp; Riedl, J. (1994). Grouplens: An Open Architecture for Collaborative Filtering of Netnews. In Proceeding of the ACM 1994 Conference on Computer Supported Cooperative Work. 
