 In sentiment analysis, aspect-level review analysis has been an im-portant task because it can catalogue, aggregate, or summarize var-ious opinions according to a product X  X  properties. In this paper, we explore a new concept for aspect-level review analysis, latent sentiment explanations , which are defined as a set of informative aspect-specific sentences whose polarities are consistent with that of the review. In other words, sentiment explanations best repre-sent a review in terms of both aspect and polarity. We formulate the problem as a structure learning problem, and sentiment expla-nations are modeled with latent variables. Training samples are automatically identified through a set of pre-defined aspect signa-ture terms (i.e., without manual annotation on samples), which we term the way weakly supervised .

Our major contributions lie in two folds: first, we formalize the use of aspect signature terms as weak supervision in a structural learning framework, which remarkably promotes aspect-level anal-ysis; second, the performance of aspect analysis and document-level sentiment classification are mutually enhanced through joint modeling. The proposed method is evaluated on restaurant and hotel reviews respectively, and experimental results demonstrate promising performance in both document-level and aspect-level sen-timent analysis.
 I.2.7 [ Artificial Intelligence ]: Natural language processing X  Text Analysis Opinion Mining; Sentiment Classification; Sentiment Analysis; Struc-tural Learning; Text Mining
The booming web gives an enormous impetus to the prosperity of online customer reviews. Such content tends to become a major resource from which users may find opinions or comments on the products or services they want to consume. However, users some-times might be overwhelmed, and not be able to read reviews one by one when facing a considerably large number of reviews. Users may be not satisfied with numerical review statistics since textual opinions are more helpful. To address this issue, aspect-level re-view analysis may be a better option.

Recently, aspect-level review analysis has shown advantages over traditional document-level sentiment analysis[8, 9, 14, 31, 33, 38]. Many existing research works[6, 7, 20, 22] addressed the task of aspect extraction, considering it as a prerequisite for aspect-based sentiment analysis. Generally, the aspect extraction process starts from some given seed words for each aspect. Then, words that either have strong associations with the extracted ones or satisfy certain predefined rules are selected. Many other approaches[2, 10, 20, 12] that extend the topic models[1] are also widely studied. The seed words are sometimes termed aspect signature terms , which can be obtained by some simple methods with a small amount of manual annotation. Thus, aspect signature terms makes it easy to scale to other domains or expand to new aspects when new prod-ucts or brands are introduced. For example, the word set { X  X alue X ,  X  X rice X ,  X  X ost X ,  X  X orth X  X  is a set of signature terms for aspect  X  X rice X  in hotel or restaurant review, while { X  X toryline X ,  X  X tory X ,  X  X ale X ,  X  X cript X ,  X  X toryteller X  X  signify the aspect  X  X tory X  in movie review.

However, it should be noted that those given aspect signature terms are not fully utilized in these approaches. Such prior knowl-edge is only employed for model initialization: aspect seed initial-ization or prior distribution for latent topics. As prior knowledge has long been shown to play an important role on human brain in understanding the world[30], many research works in data mining or machine learning attempt to promoting the performance by in-corporating prior knowledge. For instance, a variety of approaches have been proposed to encode prior knowledge into support vector machines[3, 5, 11, 34] and showed remarkable performance im-provement. In this paper, we address the problem of aspect-level sentiment analysis by making full use of these aspect signature terms.

We give an exemplar hotel review in Figure 1. It mentions sev-eral aspects including room, service, food, and price. Each aspect covers several sentences. As can be seen, the reviewer enjoyed the room and service of the hotel, but complained about the food, and gave a negative overall rating to the hotel. It X  X  worth noting that, though there are positive opinions on room and service, the major aspect (food) that the reviewer complained about leads to a nega-tive overall rating. We are inspired by the simple observation and propose the following conjectures:
In addition, statistics on more labeled data also reveals that aspect-associated sentences may act as sentiment explanations . We man-ually annotated about 450 restaurant reviews containing 4,405 sen-tences with 6 predefined aspects such as taste, ambience, etc. (see details in the experiment section). Each sentence of the review is labeled with aspect and polarity label. Figure 2 presents the as-pect distribution of positive and negative reviews. Positive reviews mention much more about taste than negative reviews (22.2% vs. 12.3%), which may imply that taste is a major factor for giving a positive overall rating. In comparison, service is mentioned much more frequently in negative reviews than in positive ones (21.1% vs. 5.5%). This gives the signal that restaurants with delicious food may receive more positive reviews, while a poor service may lead to more negative reviews.
 Figure 2: Different aspect distributions in positive and negative reviews
Furthermore, users are writing reviews to praise or criticize on some specific aspects about which they care, which results in a high correlation between aspects and opinions. Table 1 presents some statistics on the labeled data. Among sentences that are la-beled with positive or negative polarity, 93.48% (= 58 . 64% are tagged with aspect label. The statistics explains that aspect-level analysis shall be performed on the polarity part of the review, which is neglected in prior studies. However, it is difficult to obtain data with aspect labels to perform aspect-level analysis since aspect annotation is cost and time expensive. Table 1: Correlation between aspect and opinion (restaurant review)
To address the aforementioned issues, we present a structural learning model for jointly performing aspect-and document-level sentiment analysis. The major departure from prior studies is two-fold:
The proposed method is evaluated on restaurant and hotel re-views respectively, and experimental results show remarkable im-provements both in document-level sentiment classification and in aspect analysis. The remainder of this paper is organized as fol-lows: in Section 2, we briefly introduce related work on multi-level sentiment classification and aspect analysis. In Section 3 we present the formulation of the model. In Section 4 we discuss the experimental settings and results. Finally, we summarize our work in the last section.
There has been much work focused on multi-level sentiment classification. For document-level and sentence-level sentiment analysis, Mao and Lebanon[16] extended the standard conditional random fields to model the sequential flow of sentiment throughout the document. They also demonstrated that it is useful to employ local sequential sentiment representation for document-level senti-ment analysis. McDonald et al .[17] proposed a sentence-document model to perform fine-to-coarse sentiment analysis which aims to jointly classify sentiment on multiple levels of granularity. Also, they treated the inference of the sentence-level sentiment as a se-quential labeling problem. T X ckstr X m and McDonald[29] proposed to discover fine-grained sentiment with hidden-state CRF[23] only by the document-level coarse-grained supervision. Yessenalina et al .[35] deployed the framework of latent structural SVMs[36] for multilevel sentiment classification jointly. They both treated the sentence-level sentiment as latent variables, which is trained in a structural learning model. Many of the previous works[4, 16, 17, 29, 35] claimed that document-level sentiment analysis can benefit from finer level classification.

Many works promoted the performance of sentiment analysis by incorporating prior knowledge as weak supervision. Li and Zhang[13] introduced lexical prior knowledge to non-negative ma-trix tri-factorization. Shen and Li[26] further extended the matrix factorization framework to model dual supervision from document and word labels. Melville et al .[18] proposed a generative back-ground model to leverage lexical information in terms of word la-bels. Silva et al .[27] proposed a self-augmentation training proce-dure incorporating sentiment rules which can be easily obtained by projecting the training data for effective sentiment stream analysis.
There is also much work on aspect-level analysis, such as aspect rating, ranking, extraction, or summarization. Hu and Liu[8] ap-plied frequent itemset mining to extract product feature. Adjectives that are close to feature words are considered as opinion words. Reviews are then summarized according to product feature. Qiu et al .[22] proposed to iteratively extract aspect feature words and opinion words with predefined rules using  X  X ouble Propagation X . Hai et al .[7] proposed to incorporate statistical association analy-sis in a bootstrapping framework to mine aspect features. Snyder and Barzilay[28] employed the good grief algorithm for multiple aspect ranking. Their algorithm jointly learns ranking models for individual aspects by modeling the dependencies between assigned ranks.

Besides, various extensions of generative topic models are also widely studied for aspect analysis[2, 10, 12, 15, 19, 20]. Titov and McDonald[31] proposed a multigrain topic model to discover local rateable aspects. Wang et al .[33] proposed to predict aspect rating using the generative probabilistic model. Zhao et al .[38] employed MaxEnt-LDA for jointly modeling aspects and opinions. By mod-eling the aspect-document structure and document generative pro-cess, they all[31, 33, 38] used the mined aspect-specific knowledge for further aspect analysis.

However, previous research works do not fully utilize aspect-specific supervision (aspect signature terms). In this paper, we leverage such weak aspect-specific supervision to extract sentiment explanations for both document-level sentiment classification and aspect-oriented review analysis.
We define aspect as a set of signature terms that signify the oc-currence of a product X  X  property. For example, { X  X toryline X ,  X  X tory X ,  X  X ale X ,  X  X cript X ,  X  X toryteller X  X  defines the aspect  X  X tory X  for movie review. A sentence is considered as a sentiment explanation if it is describing a certain aspect and its polarity is coherent with that of the review.

We assume that each sentiment explanation contributes to the overall rating of a review. In this paper, we further assume that each sentiment explanation is only associated with one aspect. This is practical in our problem as we can extract sub-sentences by sep-arating the review document with punctuation marks such as semi-colon, period, exclamation point, or interrogation mark. Without ambiguity, we term the separated text segments as  X  sentence  X .
A review document mainly consists of two parts: opinion part and non-opinion part. Each sentence of the opinion part charac-terizes a certain aspect or describes some opinion, while the non-opinion part is about the background or factual information. Among the opinion part, we only consider the sentiment explanations of the review, i.e., those sentences whose polarity are consistent with the overall polarity. In other words, sentiment explanations are the in-formative part that best represent the original review in both aspect and polarity. Thus, aspect analysis shall be performed on the sen-timent explanations of a review. We believe that this would make the mined opinions more coherent, representative, and meaningful.
Let document be denoted by x , y  X  { +1 ,  X  1 } represents the positive/negative polarity of the document, and H is the set of informative sentences representing the sentiment explanations , in which each sentence is attached with a certain aspect a i { a 1 ,...,a k } . The task here, is to learn a function F ( x, ( y, H )) that jointly models the document polarity and the aspect assignment of the sentiment explanations, as follows: where P ( x ) is the power set of all the sentences in x , and each sentence in H is predicted with an aspect label. Let x j j -th sentence of document x , and a j is the attached aspect of x Note that document-level polarity is the only supervision we used while aspect-level annotation on sentence is not required.
In our model, we expect that each sentence in H characterizes one specific aspect. It should be noticed that for each aspect, we have a set of signature terms, which is critical in choosing an as-pect if discriminate models are employed. For example, if  X  X ost X  is observed in a sentence, it is highly probable that it is talking about the  X  X rice X  aspect.

To incorporate such weak supervision, we propose two types of loss functions for H : sentence-level loss and document-level loss. Sentence-level loss (SL)
The gold-standard aspect label of sentence x j is  X  a x j tence contains signature terms of the accordant aspect 1 , while the predicted aspect is a x j . This type of local loss measures the differ-ence of aspect between the predicted aspect a x j and the reference aspect  X  a x j , with respect to a subset H A of H . It is denoted by  X  where H A contains all the sentences automatically identified by aspect signature terms in H .

Sentence-level loss is proposed to captures the local aspect fea-ture, with the assumption that if sentence x j contains the signature terms of aspect a , the aspect label of x j is a .
 Document-level loss (DL)
From the global perspective, we have a set of aspects  X  A for a  X   X  A x , the document contains at least one aspect signature term for a . Once again,  X  A x can be obtained by a simple dictionary lookup process where the dictionary is the signature terms. The document-level loss measures the difference between the predicted aspect set A x and the reference aspect set  X  A x , as follows: Document-level loss ensures that the predicted aspect set is not  X  X ar from X  the aspect set obtained by signature terms.
For the decision function F ( x, ( y, H )) , we define the asymmet-ric loss function as
The gold-standard is obtained by an automatic dictionary lookup process; that is why we call it weak supervision . where  X  ,  X  are the coefficients that balance the aspect loss and document-level sentiment classification loss, and  X  y represents the gold-standard polarity of a review document.

Similar to Structural SVM [32], let  X ( x,y, H ) denote the joint feature map that outputs the features describing the quality of pre-dicting sentiment y using the sentence set H . In order to obtain a model that is jointly trained, and that satisfies the condition that the overall polarity of document should influence the sentiment of extracted informative sentences, the document polarity shall also be encoded in  X ( x,y, H ) . In spirit to Yessenalina et.al .[35], we propose the following formulation of the discriminate function
F ( x, ( y, H )) = ~w T  X ( x,y, H ) where N ( x ) is the normalizing factor,  X  pol ( x j ) and  X  resents the polarity and subjectivity features of sentence x tively. ~w doc denotes the weight vector modeling the overall polar-ity. ~w pol and ~w subj denote the weight for polarity and subjectivity features, respectively. More specifically, ~w pol a and ~w sent the vectors of feature weight for aspect a to calculate the polar-ity and subjectivity score, respectively. That is, ~w pol are weight matrix and have the form as follows Document Polarity Prediction
To predict document polarity, we have the document-level senti-ment classifier as In the experiment, we tune the size of H with respect to the number of sentences in x to obtain the optimal performance.
 Aspect Assignment of Extracted Latent Sentiment Explanation
For each sentence x j , we compute the joint subjectivity and po-larity score with respect to aspect a and label y as we then assign aspect a j to sentence x j if After sorting score ( x j , ( a j ,y )) in decreasing order and taking sum-mation by selecting the top |H| sentences (or fewer, if there are fewer than |H| that have positive joint score) as the total score for each y  X  X  +1 ,  X  1 } , we then predict y with the higher joint scores as the sentiment of the whole document.
With the problem formulation in previous section, the solution is to solve an optimization problem as follows: OP 1 : As OP 1 is non-convex, we employ the framework of structural SVMs with latent variables[36]using CCCP algorithm [37]. Ac-cording to the formulation, the true informative sentence set (sen-timent explanation) is never observed, and thus is modeled as a hidden or latent variable. Thus, we keep H i fixed to compute the upper bound for the concave part of each constraint, and rewrite the constraints as  X   X  max After that, we have y i completed with the latent variable H is observed. For each training example, starting with an initialized sentence set in which each sentence is tagged with an aspect label, the training procedure alternates between solving an instance of the structural SVM using the H i and predicting a new sentence set until the learned weight vector ~w converges.

In our work, we use bag-of-words features, and use the perfor-mance on a validation set to trigger the halting condition, which is a commonly adopted strategy when an optimization problem is non-convex.
The normalizing factor is set as N ( x ) = p |H| since Yesse-nalina et.al .[35] demonstrates that the square root normalization can be useful, where the size of the extracted sentiment explana-tions |H| will be further discussed in the experimental section. To analyze the aspect of each sentence, we need to give an initial guess of the aspect and polarity for each sentence.
 Sentence-level Polarity Initialization
To initialize the sentence level polarity, we employ a rule based method that counts positive and negative sentiment terms, with ad-versative relation considered. The decision rule is that if there are more positive terms than negative ones, the polarity of a sentence is positive, otherwise negative.
 Sentence-level Aspect Assignment Initialization
Obviously, if a signature term of aspect a occurs in sentence x we assign aspect a to x l , and add x l to an aspect specific sentence set S a . For sentence x l without any aspect term, we set a as the aspect label if We use cosine similarity and select the sentences whose polarity is consistent with the overall rating of a review as the initial guess of the sentiment explanation ( H ).
We crawled thousands of reviews from some social review sites such as dianping.com and daodao.com (Chinese version of tripAd-visor) to evaluate the proposed model. Each of these reviews has an overall rating ranging from one to five stars. We consider a review as positive if its rating is greater than or equal to 4 stars, or negative if less than or equal to 2 stars, and leave neutral reviews as future work. Table 2 presents some statistics of the training corpus. It should be noted that our model is trained on this dataset only with a handful set of aspect signature terms. To further evaluate aspect analysis, we also manually labeled 884 reviews, in which each sen-tence is labeled in terms of polarity and aspect. Table 3 shows the statistics about the evaluation data.
The training corpus is then split into 10 folds. Two folds are left out for test, 7 folds for training, and 1 fold for development, the performance is averaged over 5 runs. For each domain, we pre-defined several aspects, each of which is represented by some signature terms that can be easily obtained by manual annotation on top frequent aspect words. The average number of signature terms for the pre-defined aspects is around 10, and table 4 presents several samples of the aspect signature terms used in this paper.
To evaluate our model in terms of both document-level sentiment classification and aspect prediction on the extracted  X  X entiment ex-planations X , we design the following experiments: The Optimal Extraction Size
To perform document-level sentiment classification, we have to firstly determine the optimal number of extracted sentences (we term it extraction size ). For simplicity, we set  X  and  X  to 1 and choose  X  X ero/One loss X , which is the percentage of the wrong pre-dictions, as the measure to evaluate document-level sentiment clas-sification.
 Figure 3: Zero/One loss of document-level sentiment classifica-tion by varying the extraction size
Figure 3 presents the performance of document-level sentiment classification by varying the extraction size. Initially, the Zero/One loss decreases when the extraction size increases, indicating that for the majority of extracted sentences, their polarities are coherent with the corresponding review X  X  overall rating, which helps to im-prove the performance (we will further verify this later in Section 4.4.1). After the loss reaches a minimum, it increases when more sentences are extracted. The reason might be that sentences with inconsistent polarities (without opinion or even with opposite po-larity) are extracted as sentiment explanations, which leads to the performance degradation.

Another observation is that when the extraction size is around 50% , our model reaches the best performance (See the area tagged by the eclipse in Figure 3). As is shown in Table 1, the percentage of informative sentences is around 50%, and the coincidence may be the evidence that our model can extract those highly informative sentences. Due to these observations, we set the optimal extraction size to 50% to avoid losing possible opinionated sentences in the later experiments.
 Sentiment Explanations for Document-level Sentiment Classifi-cation
To justify whether using the extracted sentiment explanations can improve document-level sentiment classification, we compared our method to standard SVM[21] 2 . We set the extraction size to 50%. The major difference is that, in our model, document polarity is predicted over the extracted sentiment explanations , as illustrated in Equation 1, while standard SVM employs all of the sentences in a review. Table 5: Accuracy of document-level sentiment classification
Table 5 clearly shows that, our model remarkably outperforms the baseline on document-level sentiment classification. Note that the SVM model is a very strong baseline for this task, as discussed in [21].
 Impact of Encoding Weak Supervision on Document-level Senti-ment Classification
In our model,  X  and  X  are designed for aspect analysis. We shall first investigate if  X  and  X  affect the performance of document-level sentiment classification before further aspect analysis. It can be done as follows: firstly  X  is set to 1 by varying  X  , and secondly  X  is set to 1 by varying  X  , both under the optimal extraction size (50%). Figure 4 and Figure 5 show that, the performance of document-level sentiment classification is fairly stable when changing  X  or  X  respectively, which also implies the robustness of our model on document-level sentiment classification.
 Figure 4: Zero/One loss of document-level sentiment classifica-tion by varying  X  (extraction size 50%)
In this section, we present several case studies for both aspect representative sentences and document-level  X  X entiment explana-tion X  extraction . http://svmlight.joachims.org/ Figure 5: Zero/One loss of document-level sentiment classifica-tion by varying  X  (extraction size 50%) Aspect Representative Sentences
Our model aims to assign the extracted sentiment explanations with polarity and pre-defined aspect labels. Table 6 presents some samples of the extracted sentiment explanations . It shows that even though some of these sentences do not explicitly contain aspect signature terms, our model provides the correct prediction of as-pect assignments. Therefore, our model is capable of assigning the predefined aspect labels to the extracted sentences correctly. Aspect Aspect Representative Sentence Ambience Service Quality Document-level  X  X entiment Explanation X 
Here, we present an example of the extracted  X  X entiment expla-nation X  from the document perspective. Table 7 shows a sample of 4-star restaurant review, mentioning price, taste, ambience, and ser-vice. It can be seen that our model correctly assigns aspect labels to the three extracted sentences, all of which are coherent with the polarity of the review. Obviously, the reviewer praised the restau-rant on all the mentioned aspects except price, as  X  X oo expensive X  was even mentioned twice. However,  X  X ne flaw cannot obscure the great virtues X , the taste, service, and environment are so impressive that the reviewer gave a positive overall rating for this review. From this point of view, our model is capable of extracting sentences that best represents the document in terms of both polarity and aspect.
We present some quantitative analysis of our model in terms of both sentence-level polarity and aspect assignment.
In this section, we evaluate whether the polarity of the extracted sentences is coherent with the overall rating of the corresponding review. For each labeled review, we obtain the ground-truth  X  X en-timent explanations X  based on manual annotation. After that, we compare the predicted sentence set with the ground-truth  X  X enti-ment explanations X  in terms of precision, recall and F 1 score. Table 8: Performance of sentence-level polarity prediction
Table 8 shows the performance of sentence-level polarity predic-tion on different extraction size. It can be seen that the recall and F score increase rapidly when the extraction size grows from 20% to 60%, and the F 1 score then stays fairly stable. For both restaurant and hotel reviews, it only increases 6 percent when the extraction size increases from 60% to 90%. The precision shows that the ma-jority of the extracted sentences are coherent with review overall rating. It should be noted that the sentence polarity is only inferred based on the document polarity. The results demonstrate that our model is capable of extracting sentiment explanations that are co-herent with the polarity of a review.
Our model is also capable of predicting aspect labels to the ex-tracted informative sentences. To evaluate the performance of as-pect assignment, we compare the aspect labels predicted by our model with manual annotation for the extracted sentences on the labeled reviews. Previous experimental results show the optimal extraction size is around 50%. Therefore, we set the extraction size to 50%.
 Effectiveness of Encoding Aspect Signature Terms
We first evaluate whether the weak supervision (introduced by aspect signature terms) encoded in our model is effective for aspect analysis. We varied the value of parameter  X  and  X  to see how the encoded weak supervision impacts on the performance of aspect assignment. Note that we set  X  to 1 when evaluating the parameter  X  , and the evaluation for  X  is in the similar procedure. Figure 6 and Figure 7 show the precision of aspect assignment by varying  X  and  X  on restaurant and hotel reviews respectively. In addition, we also present the performance when  X  =  X  = 0 , indicating that aspect signature terms are only used for model initialization the dashed line shown in Figure 6 and Figure 7.
 Figure 6: Precision of aspect assignment on restaurant reviews by varying  X  and  X  Figure 7: Precision of aspect assignment on hotel reviews by varying  X  and  X 
The most common way that prior studies utilized aspect signature terms.
Figure 6 and Figure 7 both show that when  X  is fixed, the pre-cision of aspect assignment firstly increases with respect to  X  , and reaches the maximum value (  X  = 3 for restaurant and  X  = 3 . 5 for hotel), and then decreases slowly. The reason might be that sentence-level local loss may slightly affect the performance of document-level sentiment classification. We have similar obser-vation for  X  when  X  is fixed (the maximum value is reached when  X  = 0 . 5 for restaurant and  X  = 1 for hotel). In addition, it can also be observed that remarkable performance improvement can be ob-tained when aspect-specific knowledge is introduced (  X  &gt; 0 , X  &gt; 0 compared to  X  =  X  = 0 ), which demonstrates that it is effec-tive and necessary to encode aspect-specific prior knowledge. In real applications, the parameters  X  and  X  shall be tuned according to the data. For the case here, we may let  X  = 3 and  X  = 0 . 5 for restaurant reviews, and  X  = 2 . 5 and  X  = 1 for hotel reviews, or use other grid-search methods such as optimization approach to obtain an optimal setting.
 To evaluate the performance of aspect assignment, we employ SVM multiclass 4 as baseline. As for SVM multiclass , we se-lect sentences with aspect signature terms for model training in the training corpus (see Table 2), and each sentence is treated as a train-ing instance, in which the aspect signature term identifies the cor-responding aspect label.

Table 9 presents accuracy of the aspect assignment for the ex-tracted sentence. From the result, we can see that for restaurant re-views, our model outperforms much better than SVM multiclass , and for hotel reviews, our model shows a slight performance pro-motion.

As can be seen from Table 9, the performance difference on restaurant reviews is much more significant than that on hotel re-views. We take a further investigation on the data, and find that in restaurant domain, there are more  X  X oisy X  reviews than in hotel do-main. It should be noted that  X  X oisy X  means irrelevant, as nowhere in the entire review is a mention of restaurant or hotel related prop-erty. For example, the customer received a parking ticket as his car was parked illegally, and only for this reason, he gave the restau-rant a negative overall rating. Such unrelated reviews bring much noise during model training. As our model aims to extract infor-mative sentences corresponding to a certain aspect, the results may also indicate that our model is capable of extracting highly infor-mative sentences, without inclusion of those inconsistent sentences that are unrelated to any predefined aspects.
 Impact of the Number of Aspect Signature Terms
We now study how the size of the signature term set affects the performance. In comparison, we also choose the SVM multiclass as the baseline. After ranking all the aspect signature terms by document frequency on the training reviews in descending order, we select top 20%, 40%, 60%, 80%, and 100% of the total aspect signature terms to investigate how the performance changes with different sizes. Note that  X  and  X  are set to 1 as the default setting.
Figure 8 and 9 present the performance of precision on aspect as-signment over different size of aspect signature terms for restaurant and hotel review respectively. It can be seen that for restaurant re-http://svmlight.joachims.org/svm_multiclass.html Figure 8: Precision of aspect assignment on restaurant reviews by varying the seed size Figure 9: Precision of aspect assignment on hotel reviews by varying the seed size view, our model significantly outperforms the baseline, with grad-ual improvement when more signature terms are introduced. And for hotel review, our model outperforms the baseline when the seed size is over 60%. It should be noted that on average, 60% means for each aspect, only about 6 aspect signature terms are encoded in the model as weak supervision. Such a small number of aspect signature terms can be obtained at a fairly low cost for any domains in real application.
 Informativeness of the  X  X entiment Explanations X 
As mentioned before, restaurant reviews contain more noisy con-tent than hotel reviews. Topic Models such as LDA[1] are capable of modeling topics in a collection of documents, and many models made use of the latent topics for classification. Examples include the Labeled LDA[24] and Partially Labeled Dirichlet Allocation (PLDA)[25]. The PLDA model is capable of capturing the noisy content with the  X  X ackground topic X , while other topic is learnt with human-interpretable labels, which is quite close to the idea of using signature terms in our work. Hence, we now compare our model with PLDA model on the precision of aspect assignment. We treat each sentence in the training corpus (see Table 2) as a document, and train a PLDA model on the sentences with aspect signature terms that identify the corresponding aspect labels. We study the performance comparison by varying the size of aspect signature terms. Note that we also set  X  =  X  = 1 with 50% as the extraction size.
 Figure 10: Precision of aspect assignment on hotel reviews by varying the seed size
Figure 10 illustrates results of PLDA and our model. It can be seen that our model outperforms the PLDA model under all sizes of aspect signature terms. Noticing that the PLDA model outperforms SVM multiclass remarkably, partially due to the reason that noisy contents are grouped into the  X  X ackground topic X . From this point of view, our model is capable of extracting informative and repre-sentative sentences, filtering those sentences which are unrelated to any predefined aspects.
In this paper, we propose a structural learning model with a hand-ful set of aspect signature terms that are encoded as weak supervi-sion. Our model aims to extract latent sentiment explanations that are aspect-specific informative sentences whose polarity is consis-tent with the overall rating of a review.  X  Sentiment explanations  X  represent the original review in terms of both polarity and aspect, and are modeled with latent variables in this work. The proposed model is capable to perform both document-and aspect-level re-view analysis, and the performances of the two tasks are mutually enhanced through joint modeling. To summarize, the major contri-butions of this work are as follows: Experimental results on the extracted sentences also demonstrate that our model is capable of extracting latent sentiment explana-tions that are representative and informative in terms of both polar-ity and aspect. In addition, our model is a general approach which can be easily extended to other domain without re-implementation, except collecting a handful set of aspect signature terms as weak supervision.

As for future work, we plan to further improve aspect-level anal-ysis by incorporating context information of aspect signature terms to obtain better semantic coherence. It may also be interesting to predict aspect rating by performing regression for the latent vari-ables. We may also apply the results of our model for other senti-ment analysis task such as aspect-oriented summarization.
This paper was partly supported by the National Key Basic Re-search Program (also called 973 Program) with No.2013CB329403, the National Science Foundation of China with No.61272227 and Tsinghua University Initiative Scientific Research Program with No.20121088071. [1] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet [2] S. Brody and N. Elhadad. An unsupervised aspect-sentiment [3] D. Decoste and B. Sch X lkopf. Training invariant support [4] L. Fang and M. Huang. Fine granular aspect analysis using [5] G. M. Fung, O. L. Mangasarian, and J. W. Shavlik.
 [6] H. Guo, H. Zhu, Z. Guo, X. Zhang, and Z. Su. Product [7] Z. Hai, K. Chang, and G. Cong. One seed to find them all: [8] M. Hu and B. Liu. Mining and summarizing customer [9] M. Hu and B. Liu. Mining opinion features in customer [10] Y. Jo and A. H. Oh. Aspect and sentiment unification model [11] F. Lauer and G. Bloch. Incorporating prior knowledge in [12] P. Li, Y. Wang, W. Gao, and J. Jiang. Generating [13] T. Li, Y. Zhang, and V. Sindhwani. A non-negative matrix [14] B. Liu, M. Hu, and J. Cheng. Opinion observer: analyzing [15] B. Lu, M. Ott, C. Cardie, and B. K. Tsou. Multi-aspect [16] Y. Mao and G. Lebanon. Isotonic conditional random fields [17] R. McDonald, K. Hannan, T. Neylon, M. Wells, and [18] P. Melville, W. Gryc, and R. D. Lawrence. Sentiment [19] S. Moghaddam and M. Ester. Ilda: interdependent lda model [20] A. Mukherjee and B. Liu. Aspect extraction through [21] B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up? [22] G. Qiu, B. Liu, J. Bu, and C. Chen. Opinion word expansion [23] A. Quattoni, S. Wang, L.-P. Morency, M. Collins, and [24] D. Ramage, D. Hall, R. Nallapati, and C. D. Manning. [25] D. Ramage, C. D. Manning, and S. Dumais. Partially labeled [26] C. Shen and T. Li. A non-negative matrix factorization based [27] I. S. Silva, J. Gomide, A. Veloso, W. Meira, Jr., and [28] B. Snyder and R. Barzilay. Multiple aspect ranking using the [29] O. T X ckstr X m and R. McDonald. Discovering fine-grained [30] J. B. Tenenbaum, C. Kemp, T. L. Griffiths, and N. D. [31] I. Titov and R. McDonald. A joint model of text and aspect [32] I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. [33] H. Wang, Y. Lu, and C. Zhai. Latent aspect rating analysis on [34] X. Wu and R. Srihari. Incorporating prior knowledge with [35] A. Yessenalina, Y. Yue, and C. Cardie. Multi-level structured [36] C.-N. J. Yu and T. Joachims. Learning structural svms with [37] A. L. Yuille and A. Rangarajan. The concave-convex [38] W. X. Zhao, J. Jiang, H. Yan, and X. Li. Jointly modeling
