 Advances in data gathering, storage and distribution technologies introduce in-evitable challenges of extracting meaningful patterns from related databases at distributed sites for unbiased pattern discovery and rapid decision mak-ing [11, 13]. Consider retail stores such as Walmart [10], which has more than 3800 stores in the US alone (and many more in other countries), each of which produces a huge number of transactions on a daily basis. The transactions are typically stored locally, which leads to a large number of related transaction databases. Notice that items sold in diff erent countries/reg ions are rarely the same, such transaction databases are related to each other in terms of the data distributions and items representing th e databases. Developing effective data mining techniques to discover patterns from multiple related databases thus becomes crucial for these types of applications. Many methods exist for dis-covering patterns from databases sharing identical items, and existing solutions roughly fall into the following two categories: (1) collective mining  X  aggre-gating data from different databases to a centralized databa se for centralized processing [2, 5]; and (2) distributed mining  X  selecting important local rules from each individual database and synthesizing the local rules to form meaning-ful patterns over all related databases [7 X 9].

Compared to collective mining, distributed mining has the advantages of low transmission costs and low data privacy concerns [12, 16]. Th erefore it attracts much attention recently. However, existing rule synthesizing methods for dis-tributed mining commonly assumes that related databases are relevant, share similar data distributions, and have identical items. This is equivalent to the assumption that all stores have the same type of business with identical meta-data structures, which is hardly the case in practice. The irrelevance among related databases raises significant research issues to the rule synthesizing pro-cess, mainly from the following three challenges. (1) Databases with different items. Rule synthesizing is only able to synthesize rules containing items shared by all databases, but has to eliminate rules containing items unique in a few databases, even though the rules are locally significant. Therefore, in addition to discovering significant rules for all databases, solutions are needed to discover meaningful rules, which are locally significant and unique, for items which ap-pear only in a few databases. (2) Databases with similar items but different rules. Although many databases may contain similar items, the rules underneath the data may vary significantly. Under such scenarios, the rule synthesizing process should produce meaningful rules which are locally significant and globally infor-mative. (3) Customized rule synthesizi ng. For some applications, users may be either interested in rules related to so me specific items, or a few items with the highest supports. This assumption is esp ecially true when data bases are collected from the Web, journals, books, etc. Unde r such a scenario, the rule synthesizing process should generate meaningful rul es which make sense with respect to the users X  queries or constraints.

In this paper, we report our recent work i n addressing the above challenges, where the essential goal is to employ a two-step clustering based approach at the item and rule levels to ensure that im portant rules can be synthesized from related databases. For databases with different items (Challenge 1 above), clus-tering at the item level generates high-frequency items across all data collections. As a result, we only apply a rule synthesizing method on databases containing similar items. Such a clustering at the item level also helps to customize the rule synthesizing since the databases can be clustered according to the distance function built on the items specified by the users (Challenge 3). For databases sharing similar items but different rules (Challenge 2), the clusters generated from the item-level clustering are furthe r clustered using a rule based distance function. So databases are clustered according to the high-frequency association rules they contain. Thus the final clusters contain not only similar high-frequency items but also similar high-frequenc y rules associated with these items.
The remainder of the paper is structured as follows. In Section 2 we summarize related work. In Section 3 we present the proposed two-step clustering based rule synthesizing framework, followed by exp erimental compariso ns in Section 4. The concluding remarks are reported in Section 5. Existing research on association rule mining from related databases roughly falls into the following two categories: (1) collective mining  X  aggregating data from different sources to a centralized da tabase for centrali zed processing [2, 5]; and (2) distributed mining  X  selecting important local rules from each individual source and synthesizing rules to form meaningful patterns over all related databases [7 X 9].

For collective pattern mining, aggregating data from related sources is the most straightforward method but is not practical due to concerns such as band-width limitations, data ownership, and data privacy [16]. Alternatively, many methods exist for association rules mining without data integration [18 X 20]. Among them, count distribution, data distribution, and candidate distribution are three basic mechanisms [15]. For example, in [17], the authors proposed a fast distributed association rule mining method, where pattern pruning consid-ers both local supports and the polling results exchanged between sites. In their problem setting, a dedicated master site is used to control related databases to carry out pattern mining. The main theme of collective mining methods, from distributed association rule mining persp ective, is to discover association rules which are globally significant from all related databases X  point of view. The primary technical challenge is to minimize the communication cost between dis-tributed sites in order to accelerate the pattern pruning process. For all above methods, the underlying databases must have identical items. In comparison, our research does not intend to find rules globally significant, but focuses on discovering rules from databases with different items.

Different from collective mining, wher e both related databases and the mas-ter site are involved to achieve a mining goal, in distributed mining, the pri-mary focus is to combine mining results from distributed sites to synthesize new/significant rules [1, 21]. Under this framework, the related databases inde-pendently carry out the mining activities, and the results (rules) are aggregated to synthesize global interesting rules. Wu and Zhang [7, 8] proposed a weighting model to effectively synthesize local prom ising rules. After all promising rules are selected from different databases using normal association rule mining algo-rithms such as Aprior [4], a weight is the n assigned to each rule, according to the number of databases that contain the rule. Then a weight for each database is computed according to their rules and the rule weight values. Using the weights, a global support for each rule is comput ed and is used to determine signifi-cant rules overall all data collections. However, the weighting model assumes the databases are relevant and share highly-similar items and rules so it can not handle irrelevant databases properly. Since the model doesn X  X  do clustering, we call it simple synthesizing algorithm . The synthesized rules of this algorithm are always highly frequent globally.

Adhikari and Rao [9] proposed an algorithm to synthesize heavy association rules, which are the rules whose global supports are higher than a user given threshold. This criterion is the same as the measure defined in [7]. They also ob-served the cases that heavy association rules may not be shared by all databases. Therefore they defined a highly-frequent rule as the rule shared by at least n  X   X  1 databases and an exceptional rule as the rule shared by no more than n  X   X  2 databases, where n is the number of databases, and  X  1 and  X  2 are user defined thresholds. Then they synthesized the local rules and reported whether the rules are highly-frequent or exceptional. However, the algorithm still ignores the possi-ble irrelevances among the databases. Since it synthesizes only heavy association rules, it is not able to report rules for items unique to certain groups of databases, and it is not able to report association rules for items the user inputs. What X  X  more, the user needs to define both the highly-frequent and exceptional thresh-olds. As a conclusion, they addressed a different problem and their algorithm is not able to solve the challenges in Section 1. Intuitively, given a number of related databases, many rules may be frequent in only a few databases and a rule tends to be more significant if it is highly frequent in more databases. Accordingly, we define that a rule is meaningful if its support is higher than a given threshold t (we call t the support threshold ) in a cluster of at least k similar databases(we call k the cluster size threshold ), where t and k are given by users. Based on this definition, our two-step cluster-ing method mainly aims to maximize the chance of capturing meaningful rules for synthesizing databases with dissimilar items and data distributions. It helps detect data similarities at both item an d rule levels since databases may be dis-similar to each other at either level. Notice that if k = 1, the problem becomes simply mining normal frequent rules in individual databases, which collects sig-nificant rules from each database. If k equals the total number of databases, the simple synthesizing algorithm [7] works well since it always synthesizes rules globally, namely it considers all databases as in one cluster.

We define r-RepItem (Representative Items) as the r items with the highest supports in the list of items whose supports are greater than minsupport .The r-RepItem can also be r items of the user input if there is any. Then the item based similarity function is defined as where I i ,I j are r-RepItem sets for data sources D i ,D j . In the remaining of the paper, we will just use all items with supports higher than minsupport as the representative items.
 3.1 Item Based Clustering Algorithm We apply a maximal clique algorithm to generate appropriate clusters. We re-quire for any two databases D i ,D j in the same cluster, sim ( D i ,D j )  X   X  . The ba-sic idea is to build a graph G =( V , E ), where V = D 1 ,D 2 , ..., D m is the m databases while e i,j  X  E when sim ( I i ,I j )  X   X  , I i is a representative item set of D i ,and  X  is the threshold of similarity. We enumerate all maximal cliques from the graph, using the popular algorithm from Bron and Kerbosch [6]. These maximal cliques can overlap with each other such that the clustering always exits for any thresh-old  X  . This is useful when the user input threshold is allowed. If we do not allow clusters to overlap, namely if we require sim ( I i ,I j ) &lt; X  for D i and D j in dif-ferent clusters, the clusters may not ex ist for some threshold. Each clique will be assigned one cluster label. Since the maximal cliques are always the same for a given graph, the clustering result never changes with the order of input databases. We call this algorithm Maximal Clique Clustering Algorithm (MCCA) .

Our purpose is to find a clustering where databases from the same cluster are similar to each other while databases from different clusters are different from each other. Therefore a key challenge is to design an evaluation function which has a unique polar-point, namely a unique minimum or maximum value, for a set of given databases and thus ca n be used to select the best clustering automatically by the algorithm shown in Figure 1. We define a distance function as follows.
 Here C stands for clusters, C  X  stands for the clusters when the similarity thresh-old is  X  . C  X  i stands for the i th cluster when the similarity threshold is  X  . ISim stands for item based similarity, and dist ( C i ,C j ) is the distance between two clusters. The smaller the distance function distance ( C,  X  ) is, the better a clus-tering is. This function has at least one polar-point for a set of given databases since its value can never be infinite and its value decreases first and then in-creases. Therefore it can be used to sel ect the best clustering automatically.
The distance function in Equation 4 has a unique polar-point such that the algorithm shown in Figure 1 can be applied directly to select the best clustering. Lemma 1. Distance function distance ( C,  X  ) = Goodness ( C,  X  ) -has a unique polar-point. The proof of Lemma 1 is omitted due to space restrictions. Since we allow over-lapping among different clusters, a best clustering always exists.

Given that the distance function has a unique polar-point, we are able to find the polar-point automatically, using the algorithm shown in Figure 1. The algorithm initializes the similarity threshold as 1 and then keeps on decreasing the threshold by a small number  X  until it locates a small region that contains the polar-point. Then it uses binary search to find the polar-point. It is straight-forward to see the correctness of the algorithm. The distance function where ISim ( I i ,I j ) is used can be applied to the algorithm to select the best clustering. We call the algorithm using ISim ( I i ,I j ) RuleBasedClustering Algorithm. In the algorithm, we set  X  = maxSim/ 20 where maxSim is the maximum similarities among all databases, and the denominator is set to 20 empirically.

We observe that the maximal clique clustering algorithm may not be feasible to a large dense graph where the number of edges is close to O ( n 2 )and n is the number of nodes. This is because the nu mber of maximal cliques may increase exponentially with the number of edges. Based on this observation, we propose a Greedy Clique Clustering Algorithm (GCCA) .A maximum clique, which is of the largest size among all maximal cliques, is found at first, then all the nodes in the clique are removed from the graph. Then the next maximum clique is found in the remaining graph. The algorithm runs until there is no more node to remove. However, the greedy selection o f the maximum clique may not generate the best clustering since once the nod es of a maximum clique are removed, the process cannot be reverted. If there are several maximum cliques, we select the one with the smallest value as defined in Formulae 1 and 5, respectively. Since the characteristics of c lusters generated by GCCA are the same as those generated by MCCA , the new distance function can be applied to MCCA directly such that a best clustering at each step can be found automatically. One problem of GCCA is it may produce some small clusters whose sizes are smaller than the given cluster size threshold k since databases are removed at each step. Therefore the possibly important rules from these clusters may be missing. To address this problem, we take the rules from these clu sters to be synthesized using the method in [7], which always synthesizes rules globally without clustering. This guarantees that GCCA captures more meaningful rules than the method in [7]. 3.2 Rule Based Clustering Algorithm Once the representative item based clustering is done, we obtain a set of clusters where databases inside the same cluster contain similar representative items. However, even though the databases in one cluster are relevant to each other, namely they contain similar representative items, they may still contain very different rules related to these items. T hese rules will not be generated by the synthesizing algorithm even though they are actually very meaningful.
To solve this problem, we need to apply step two of our clustering algorithm, namely rule based clustering, on each cluster obtained from the item based clustering (we call these clusters Step 1 Clusters ). The clustering process is similar to the item based one, but with a different similarity function. We first need to select global representative it ems in each step 1 cluster. We can apply the weighting model on items for each step 1 cluster and select the first r items with the highest global weights. If the user inputs r items that they are interested in, these items will then be used instead of the r items with the highest global weights. Then for each such item, we mine association rules related to the item (namely rules contain the item) in the databases in each step 1 cluster. We then define a similarity function for these rules. Let D i ,D j be two databases, S ,S j be the sets of association rules from D i ,D j related to item d , respectively, and | S | be the number of association rules in set S . We define the rule based similarity between D i and D j as Here C stands for a cluster, and RSim stands for rule based similarity. The distance function of rule based clustering will be the same as the one from item based clustering while using rule based Value and dist functions. The Rule-BasedClustering algorithm is the same as the ItemBasedClustering algorithm with the only difference that RSim ( d, S i ,S j ) is used in the algorithm. Once the best clustering is selected, the weighting model will be applied to each subcluster to generate high-frequency rules relat ed to each global representative item. 3.3 Weighting Model for Rule Synthesizing After the two-step clustering, we have clusters of databases which share simi-lar items and similar rules. We then apply the weighting model [7] as shown in Figure 2 to synthesize rules on each clus ter. The model is shown to be effective in synthesizing rules from multiple databases. After all promising rules are se-lected from different databases using normal association rule mining algorithms such as Apriori [4] , a weight is then assigned to each rule, according to the number of databases that contain the rule. Then a weight for each database is computed according to their rules and the rule weight values. Using the weights, a global support for each rule is computed, and is used to determine significant rules over all data collections. We show our two-step clustering based algorithm in pesudocode in Figure 3. The algorithm selects best clustering using the above representative item-based distance function and then synthesize the items to generate representative items for each cluster. Then it further selects the best clustering for each Step 1 cluster using th e rule-based distance function. Finally it synthesizes the rules from databases in each final cluster. The functions Item-Synthesizing and RuleSynthesizing both use the weighting model on items and rules, respectively. Globally meaningful items and rules on each input cluster are then generated by these two functions. We implemented our two-step clustering based algorithm on a 2.8GHz P4 CPU with 512MB memory. To demonstrate the algorithm performance on real-world data, we applied it to several machine lea rning benchmark datasets from the UCI data repository [22]. In order to construct appropriate transaction databases, we consider each different value of each attribute including the class attribute as an item and each instance as a transaction. Therefore all the values in the same line are the items in the same transaction. We construct the transaction databases from three databases: Balance, BC (Breast Cancer) and Nursery. The characteristics of each database are summarized in Figure 4. 4.1 Experimental Results on Similar Databases We first conducted the experiments on the transaction database transferred from the Balance dataset. We randomly split the transaction database into 10 databases of roughly equal sizes. Then we apply the two-step clustering based algorithm on the 10 databases. We set the minsupport as 0.02. We also set the cluster size threshold as 2, given that there are only 10 databases. Therefore the synthesized rules are meaningful if their supports are higher than 0.02 in a cluster of at least two databases. We apply MCCA as our clustering method. We compared the number of meaningful rules captured from each database by four methods: regular Apriori on each database, synthesizing without cluster-ing [7], synthesizing with item-based clustering only and synthesizing with the two-step clustering based algorithm. For the Apriori algorithm, we simply se-lect significant rules whose supports are greater than the minsupport from each database.These numbers illustrate our method filters out lots of rules significant in only individual databases. Our algorithm runs for a few seconds. We show the experimental results in the left graph o f Figure 5. As we expected, the item-based clustering only algorithm and the two-step clustering based algorithm capture much more meaningful rules with high supports in at least two databases than the simple synthesizing method does. We can also observe that item-based only algorithm and two-step clustering based algorithm capture the same number of rules. This is because in each databas e the number of items is very small com-pared to the number of transactions (23 vs. 62  X  2 5 ). Once the databases are clustered based on item distance, the databases in the same cluster contain similar items. Since the item number is small, most transactions will contain similar items, which generates similar rules. Therefore these databases are similar on the rule level and the clusters remain unchanged after rule-based clustering. 4.2 Experimental Results on Dissimilar Databases In order to illustrate the benefits of our algorithm, we conducted the second set of experiments on the transaction database transferred from the BC dataset. Again, we split the transaction database into 10 databases of roughly equal sizes. However, for the second half of the databases, we injected random transactions containing different items selected from a randomly generated itemset. The in-jected transactions are of d ifferent sizes. The number of injected transactions for each database is the same as that of the original database. Therefore, after the injection, the injected databases are highly possibly less similar to the un-changed databases while the unchanged databases are still similar to each other. The minsupport for item-based clustering and rule-based clustering are fixed as 0.02 and 0.04, respectively. The cluster size threshold is again fixed as 2. We compare the numbers of meaningful rules captured from each database by the simple synthesizing method [7] and our two step clustering based method. We use GCCA for clustering. Our two-step clustering based algorithm ran for around 1 minute. We show the experimental results in the right graph of Figure 5. The item-based clustering correctly clusters the unchanged databases (databases 1 to 5) which are more similar to each other. For the databases with injected transactions (databases 6 to 10), since the transactions are injected randomly, these injected databases a re more likely to be different from each other but may still stand a chance to be similar to other databases (databases 6, 7, e.g.). For the clusters whose size is less than two, the two-step clustering based algorithm simply takes the rules synthesized by the simple synthesizing method. That X  X  why the number of synthesized rules for some databases such as databases 8, 9, 10, are the same for both methods. The two-step clustering based algorithm using GCCA always captures no less meaningful rules than the simple synthesizing method does. Those newly captured rules are all highly frequent in at least two similar databases. If we use MCCA , an optimal clustering will be found and we can capture more meaningful rules from those databases which are in clusters of size less than two by GCCA . Due to space restrictions, a comparison between MCCA and GCCA is omitted. 4.3 Scalability Assessment The last experiment is to show that our algorithm is able to scale up to a large number of databases. We conduct the experiment on the Nursery database. This database is much larger than the previous two databases. We randomly split the database into 300 databases of equal size. Again, we inject random transactions to half of them, however, unlike the previous experiment, we inject random trans-actions alternatively to the database, namely random transactions are injected for databases with each odd index in the range of [1, 300]. The two-step clus-tering algorithm ran for 104 minutes. We show the numbers of meaningful rules captured from each database by the four methods in Figure 6. To illustrate how the databases are clustered, we didn X  X  s et the cluster size threshold. Since the number of databases is relatively big, for illustration purpose, we use GCCA for clustering. As we can see in the first plot for the Apriori algorithm, since random transactions were injected to all databases with an odd index, these databases contain more rules, while the remaining unchanged databases contain less rules. A similar rule number distribution can be observed for the simple synthesiz-ing method where the number of meaningful rules captured in each database is much less than that by the Apriori algorithm. For Item-based only synthesiz-ing and two-step clustering based synthesizing, we re-arrange the indices of the databases such that the databases in the same cluster are indexed close to each other. Clearly we can see the databases are clustered into a few clusters where the distributions of the numbers of meaningful rules for the databases in each cluster are similar. Again, the number of meaningful rules captured by the two-step clustering based algorithm is more than the item-based only algorithm. One more observation is although we didn X  X  fix the cluster size threshold, it is easy to see that there are many clusters of sizes more than ten after the item-based only clustering and the two-step clustering. The numbers of highly frequent rules from them are many more than those from the simple synthesizing method. It indicates if we set the cluster size thre shold as 10, our method is able to capture much more meaningful rules. In this paper, we proposed a general rule synthesizing framework for association rule mining from multiple related databases. We argued that due to realities that (1) multi-related data coll ections may have different ite ms, (2) local patterns in the related databases may be largely different, an d(3)usersmayrequirea rule synthesizing process to be customi zed to some specific items, existing rule synthesizing methods are ineffective in solving all these challenges. Alternatively, we proposed a two-step clustering based rule synthesizing framework, which clusters the data at both item and rule levels, to synthesize association rules from multiple related databases. In our definition, a synthesized rule is meaningful if its support is frequent in a cluster of related databases. A synthesizing algorithm is then applied on the final clusters to find significant rules for representative items of the clusters. Experimental results and comparisons indicated that the proposed two-step clustering based rule synthesizing method is able to capture meaningful rules that are otherwise incapable of being synthesized by other methods.

