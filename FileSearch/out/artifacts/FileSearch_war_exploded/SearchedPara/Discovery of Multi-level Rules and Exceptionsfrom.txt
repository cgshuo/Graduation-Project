 Large amounts of data pose special difficulties for Knowledge Discovery in Databases. Efficient means are required to ease this problem; this can be achieved using sufficient statistics rather than individual level data. Sufficient statistics are especially appropriate for Knowledge Discovery from distributed databases, where it may be either too expensive or prohibited by confidentiality constraints to communicate individual level data across the network. The data of interest are of a type similar to those found in OLAP data cubes and Data Warehouses. They consist of both numerical and categorical attributes (dimensions) and are contained within a hierarchical structure. There are few algorithms to date that explicitly exploit this hierarchical structure when carrying out knowledge discovery on these data. Using sufficient statistics in the form of aggregate data and accompanying statistical metadata retrieved from a distributed database, we use multi-level models to identify and present relationships between a single numerical attribute and a combination of other attributes at various levels of the hierarchy. On the basis of these relationships, rules in conjunctive normal form are induced. We present to the user the significant attribute relationships and interactions via a graphical interface rather than the traditional statistical table. Exceptions to these rules are discovered, and the user may browse these exceptions at different levels of the hierarchy.
 Multi-level Statistical Models, Aggregates, Distributed Databases, Sufficient Statistics, Exception Discovery, Rule Discovery. Frequently data are distributed on different computing systems in various sites. Distributed Database Management Systems provide a superstructure which integrates either homogeneous or heterogeneous DBMS [1]. In Europe a convergence between Database Technology and Statistics has been particularly encouraged by the EU Framework IV initiative, with DOSIS projects IDARESA [2] and ADDSIA [3], which retrieve aggregate data from distributed statistical databases via the internet. These projects store data with a large amount of associated domain knowledge in the form of statistical metadata, which lends itself nicely to the task of knowledge discovery incorporating this domain knowledge.

In order to alleviate some of the problems associated with mining large amounts of individual level (micro) data, one option is to use a set of sufficient statistics in place of the data itself [4]. This is particularly important in the distributed database situation encountered in both IDARESA and ADDSIA, where issues associated with slow data transfer and confidentiality constraints may preclude the transfer of the individual level data [5]. In this paper we show how the same results can be obtained by communicating aggregate data from each site to a central site in place of the individual level data. This has two associated problems; firstly deciding on the sufficient statistics in the form of the aggregate data which need to be communicated from each site in order to compute a required model; secondly ensuring that these distributed sufficient statistics can be combined centrally in a meaningful way.

The types of data we deal with here are similar to the multidimensional data stored in a Data Warehouse (DW) [6,7]. These data consist of two attribute value types: measures or numerical data, and dimensions or categorical data. Some of the dimensions may also have associated hierarchies to specify grouping levels. This paper deals with such data in statistical databases, but should be easily adapted to either a typical or a distributed DW implementation [8].

In our database implementation, the individual level or micro data is stored in Fact and Dimension tables on the different sites using the STAR relational schema [6]. The aggregate data is stored in the form of MAMED Objects [3], consisting of two parts: a macro relation (containing the aggregate data) and its corresponding statistical metadata relations (containing statistical metadata for tasks such as dimension value re-classification, measure value conversion and statistical model computation). Using this aggregate data, it is possible, with models taken from the field of statistics, to study the relation between a single measure attribute and a combination of one or more explanatory attributes from various levels of a hierarchy. We demonstrate this approach using statistical multi-level models [9, 10, 11] which enable us to relax many of the constraints normally associated
Permission to make digital or hard copies of part or all of this work or personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to permission and/or a fee.

KDD 2000, Boston, MA USA  X  ACM 2000 1 -58113 -233 -6/00/0 8 ...$5.00 with statistical models. Thus in this paper, we illustrate how a user may obtain rules and exceptions from data which is distributed over a number of sites. This follows on from difficulties workers in statistical agencies were having in building models and obtaining knowledge from data which was geographically dispersed over databases in a number of countries. They were also faced with confidentiality considerations in communicating the individual level data from other statistical agencies and other approaches were required to solve this problem.
 The following section, Section 2, contains an extended example and an explanation of the multi-level problem structure. Section 3 shows how the data are retrieved and integrated in preparation for the multi-level model building phase. Multi-level modeling and computation are discussed in Section 4, along with the method of presenting the resulting discovered knowledge in Section 5. Section 6 concludes with related work, summary and possibilities for further work. To illustrate our approach we present an insurance example where the data are stored in databases on four different distributed sites in Ireland, England, France and Spain . These data are distributed horizontally with some attribute domain value mismatches over the different sites which need to be redressed before the modeling phase. The database schema for the micro data is shown in Figure 1 along with the concept hierarchy showing that Individual driver data are nested within Regions within Countries . The attributes of interest to the user are shown in Table 1. Mean Country Claim in Table 1 is a derived attribute, calculated when the data are aggregated.
 Figure 1. Star Schema of micro data and Concept Hierarchy
The process begins with the user browsing through textual statistical metadata held in XML [12] to select a set of measures (possibly specifying some derived measures) and dimensions of interest, along with an appropriate model. The user may also specify restrictions on the attribute values of interest (e.g. selecting " GENDER=Female" or " Cost of Claim &gt; X 5000" ).
 These details form the user's query on a client site. This query is then sent to a central domain server where it is decomposed in order to retrieve the necessary aggregate data from the distributed database. This is carried out by a Query Agent which sends out MAMED Object requests to the relevant distributed sites. The required data is then used to construct the multi-level model.
A multilevel problem is one that concerns the relationships between attributes that are measured at a number of different hierarchical levels. For the example data above, the user is attempting to discover rules and exceptions for the attribute " Cost of insurance claims " based on a number of individual level attributes, a number of attributes at a regional level and a further set of attributes at a country level indicated in Table 1.
 DRIVER TEST SCORE {Continuous} Individual
Once groupings exist (e.g. drivers from the same country or region) the individual tuples may not be independently and identically distributed (IID). Heterogeneity is very often present in relationships between hierarchical data held in databases, and frequently the statistical models with full IID assumptions do not explain this heterogeneity sufficiently. If this independence assumption is violated, the estimates of the standard errors of conventional statistical tests are much too small, resulting in many deceptive statistically significant results. This lack of independence between observations within groups is expressed as the intra-class correlation . It is a population estimate of the variance explained by the grouping structure.

Multilevel models [9] attempt to realistically model these situations where there is clustering of individuals within groups and attributes are measured at different grouping levels of the hierarchy. They essentially work by combining models which are built at each level of the hierarchy. In these models the assumption of independence of individuals is dropped, and relationships in the data are no longer assumed to be fixed over groups but are allowed to differ.

In this paper we use multilevel models with a number of goals in mind:  X  firstly to determine the direct relationships between a single  X  secondly to determine if the explanatory attributes at the  X  thirdly to improve prediction of cost claims within individual  X  fourthly to partition the variance components among levels  X  lastly to isolate groups at different levels of the hierarchy that
On the basis of the relationships discovered using such multi-level models, we induce rules in conjunctive normal form. Exceptions to these rules are also discovered, and the user may browse these exceptions at the different levels of the hierarchy. For example we may uncover a rule that states that cost claims for males in Ireland can be expected to lie within a given range. Following on from this, we may find an exception grouping of males in region 12 in Ireland where the claim costs lie outside this range in a statistically significant way. This may then lead the user to an investigation of this region and eventually to conclude that the cost of insurance for males in this region needs to be higher to cover the extra claim costs (due perhaps to higher than average car theft in the region). Without the statistical metadata in our system which indicates the hierarchical level of each attribute, the data could only be analysed at the individual level, with all higher level attributes stepped-down to this level. This metadata is even more important if the multilevel models are to be computed in an automated fashion. The data at any one site can consist of individual level  X  X icro X  data and/or aggregate  X  X acro X  data, along with accompanying statistical metadata [3] required for example for harmonisation of the data at the central domain server. This view of micro and macro data is similar to the base data and materialised views held in a Data Warehouse [6]. In addition, in the ADDSIA system [12], textual (passive) statistical metadata for use in documentation are held in XML files. These textual statistical metadata can be browsed by the user as an aid to choosing a relevant set of attributes to model. They may also contain domain knowledge to assist the user in the form of previously discovered relationships in the data. Storing the micro data in the relational STAR schema format [6] as shown in Figure 1 reduces the time required for query execution in producing the required macro relations [7]. The aggregate data in our system is stored in a single relational table format.

Graefe et al. [4] state that "most algorithms are driven by a set of sufficient statistics that are significantly smaller that the data". Their approach takes advantage of the query processing system of SQL databases to produce a set of sufficient statistics for classification, thereby avoiding the need to move the individual level data to the client. This results in a significant increase in performance. In the distributed database situation, where it may be too expensive or prohibited by confidentiality constraints to communicate the individual level data across the network, obtaining such a set of sufficient statistics is even more relevant. However in the distributed database scenario, we must not only find a set of sufficient statistics for the statistical models we wish to construct, but we must also ensure that they can be combined at the central site in a meaningful way.

For the statistical models we use, our sufficient statistics consist of aggregates in the form of counts, sums and sums of squares of measures and products of measures. These aggregates also serve as sufficient statistics to a number of other statistical procedures including Analysis of Variance Models, Regression Models and other linear statistical models. With this in mind, the data model for MAMED Objects was developed to store these sufficient statistics which are calculated from the micro data. A single MAMED Object consists of a macro relation with a number of accompanying statistical metadata relations [3]. The statistical metadata relations associated with a single macro relation in a MAMED Object [3] consist of a set of relational tables required for statistical model building and data harmonisation and integration at the central domain server. They include statistical population estimation information, reclassification information for measures and dimensions, and statistical modeling information. Within a MAMED Object a macro relation describes a set of macro objects (statistical tables) where C represent n dimensions and S 1 ,...S m are m sets of summary attributes. Each S i in the macro relation consists of aggregates in the form of count (N), sum (SM) and sums of squares (SS) for a measure or derived measure from the micro data. The aggregates in each set of summary attributes are functionally defined by a cross product of the dimension attribute domain values. As an example, Table 2 shows a macro relation with two dimension attributes ( gender and claim type ) and two measures ( age and a derived measure claimcost divided by yearly mileage ). The cross product of the dimension attribute values results in four records, each with its corres ponding set of summary attribute sets for each measure.

The concept which enables us to combine the summary attributes from the distributed macro relations to form a centralised set of sufficient statistics for the multi-level model is the additive property of these summary attributes [13]. This property allows us to combine aggregate data from the distributed sites seamlessly and is defined in (1) as follows: where  X  and  X  are macro relations which are domain compatible and  X  () is an application of an additive summary function (e.g. SUM) over a measure in  X  and  X  . Two macro relations are deemed to be domain compatible if they contain the same dimension attributes, each with identical domain sets, and the same summary measures, each defined on the same units. Thus an England 1998 macro relation which is domain compatible with the Ireland 1998 macro relation in Table 2, would also contain four entries and identical summary measures with the same units. The function MEAN is not an additive summary function and for this reason our summary measures do not include Mean values. However, with the Counts and Sums, we can for example obtain the mean age of all Gender=females over the two countries by dividing the sum of the appropriate Age_SM values for the two countries by the sum of their corresponding Age_N (Count) values.

To retrieve aggregate data in a domain compatible format from the distributed data sites, a complete set of operators has been developed to work with MAMED Objects [3]. These MAMED operators are implemented using SQL and operate simultaneously on a macro relation and on its accompanying statistical metadata relations. In this way the statistical metadata relations only contain information relevant to the attributes which are in the corresponding macro relation. Thus if a measure or a dimension is removed from a macro relation, information pertaining to those attributes is also removed from the statistical metadata relations.
Once the initial required MAMED Objects have been created at each site from the micro data (or from pre-existing MAMED Objects), they are communicated to the Domain Server. Here MAMED operators are applied to the macro relations using information in the statistical metadata relations so that the macro relations are domain compatible and ready to be integrated together. In many cases a dimension  X  s domain values need to be reclassified so that all the macro relations contain dimension attributes with the same domain set (e.g. the French database might classify Accidents in Claim Type as  X  Accident Type A  X  and  X  Accident Type B  X  separately. These need to be reclassified and aggregated using a MAMED operator to the single value  X  Accident  X  which is the appropriate classification used by the other Countries involved in the query). Another MAMED operator is used to convert all of the measures' values to common units (e.g. all claim costs need to be converted to the common currency of EUROs).

The final harmonised MAMED Objects from each site now contain macro relations which are domain compatible and can therefore be integrated into a single aggregate macro relation. The statistical metadata relations are also integrated accordingly, and the user can view these if required to see how the final macro relation was obtained. The final task is to apply the DATA CUBE operator [14] to all of the N, SM and SS summary attributes in the macro relation. The data is now in a suitable format for the statistical modeling phase.

In this way it is possible to combine aggregates over these summary attributes in distributed macro relations at a central site for our knowledge discovery purposes, once the data have been suitably harmonised using the statistical metadata relations. This also enables us to leverage the query processing system of SQL databases (including the Data Cube operator) to compute our sufficient statistics in the form of the aggregates held in the final macro relation. By communicating only these aggregate data, we overcome the problems associated the confidentiality constraints in communicating individual level data across the network. In our prototype the micro data and MAMED Objects are stored in INGRES DBMS on each site. Access to remote distributed servers is achieved via the Internet in a Java environment. A well-acknowledged three-tier architecture has been adopted for the design. The logical structure consists of a front-end user (the client), a back-end user (the server), and middleware which maintains communication between the client and the server. The distributed computing middleware capability remote method invocation (RMI) is used here. A query is transformed into a series of nested MAMED Operators and passed to the Query Agent for assembly into SQL and execution. The multilevel models used may be exemplified using a two level hierarchy consisting of individual drivers and regions from the more levels and more attributes at each level [15]. At the individual driver level we use Gender (X 1 ) and DriverTestScore (X 2 ) to predict Cost of Insurance Claim (Y ij ) for driver At the regional level we include attributes Region Type (Z Road Assessment Score (Z 2 ). The full multi-level may be explained by dividing up the model into equations (2), (3) and (4). Initially a separate model can be built for each region separate slopes (  X  0j ) and intercepts (  X  1j ,  X  2j ) as shown in (2). where  X  0. is the usual regression equation intercept,  X  usual regression slopes and  X  ij the usual residual error term. Y represents the claim cost of an individual driver i within region
The model in (2) which is termed a level 1 model, differs from a regression model in that both the intercepts and slopes are allowed to vary across regions. Thus across all regions, each of these slope and intercept coefficients have a mean and variance, and in the multilevel model the variation in these coefficients is modeled by introducing explanatory attributes at the Region Level (Z To model the variation in the intercept coefficients across regions, level 2 models are introduced as shown in (3) and (4). and for each slope coefficient (  X  1j ,  X  2j ) for h  X  {1,2} where  X  oo ,  X  ho are intercept coefficients,  X  coefficients and,  X  0j and  X  hj the residual errors at the region level. All level 2 residual errors are assumed to have a mean of zero and to be independent from the residual errors at level 1 in (2). Hence equation (3) states that the variation in the level 1 intercept coefficient (  X  0j ) can possibly be explained by the level 2 attributes Z and Z 2 , with any residual error variance being captured in  X  Equation (4) states that the variation in the level 1 slope coefficients (  X  hj ) can possibly be explained by attributes Z with any residual error variance being captured in  X  substituting Equations (3) and (4) into Equation (2) for  X   X  , we obtain a single complex equation giving us a multi-level model at two levels shown in Equation (5). This can be generalised to a case with more levels and more explanatory attributes at each level, as well as interaction terms at level 1. Equation (5) breaks the model into a fixed components part and a random components part. characteristics. The matrix notation for the two level multi-level model is Equation (5) is shown below. X is a matrix of the attribute and attribute interaction values, Y is a matrix of the corresponding cost claims,  X  is a matrix containing the model coefficients and V is a Block Diagonal covariance Matrix [15] containing the variance and covariance components of the model.

Maximum Likelihood [9] is used to obtain estimates of the fixed and random coefficients of the model. Computing the maximum likelihood estimates requires an iterative procedure, iterating from estimating the fixed components (  X  ), to using these estimates to estimate the random components ( V ), which in turn are used to estimate the fixed components in the next iteration. In this paper we use an iterative generalised least squares method (IGLS) [15] to compute these fixed and random components. This method begins by generating reasonable starting values for the  X  estimates, based on simple single-level ordinary least squares (OLS) regressions (Equation (6) with the V -1 matrix terms removed and higher level random effects (  X  0j and  X  equal to zero). The resulting  X  estimates from Equation (6) are used to obtain a matrix G of raw residuals (differences between the observed claim costs,Y ij , and predicted values computed using the  X  estimates for each individual). In the sec ond stage of an iteration, this matrix G is used in conjunction with structural information on the V matrix, to obtain estimates of the random components in the V matrix also using GLS [15] of a similar form to Equation (6). The iterations continue in this way using refined estimates of the random components ( V ) matrix in Equation (6) to obtain improved fixed components (  X  ) estimates until the process converges. In [16] we show how the aggregate data in the macro relations can be communicated as sufficient statistics in place of the individual level data to compute ANOVA models. Computing an ANOVA model requires only one MAMED Object to be retrieved from each distributed site. Here we show how this can be achieved for multilevel models, but we require more than one retrieval of MAMED Objects. While it is not the focus of this paper to describe the computational methods used in great depth, sufficient detail is included in order to provide an understanding of the process.

Both stages of an iteration in computing a multi-level model involve GLS as indicated in section 4.2 for the case where the individual level data is available centrally. To see how we can accomplish this by communicating only aggregate data to and from the central domain server, we take the simplest GLS model (equation (6)) consisting of only one X attribute. The generalised least squares normal equations (derived by calculus [17]) used to solve this simple GLS model are shown in equations (7) and (8). The V i are matrix elements from the V -1 matrix in equation (6). Using the least squares normal GLS equations (7 and 8), it is possible to obtain a more compact representation for the matrix equation in (6) in terms of aggregates. This is shown in equations (9) and (10). We call the two matrices in (9) and (10) aggregate matrices . The matrix elements in (9) and (10) consist of sums and sums of squares of attributes (and products of those attributes).

These aggregate matrices form the set of sufficient statistics for the GLS estimation procedure. Using our MAMED Operators as explained in Section 3, it is possible to assemble these aggregate matrix elements centrally from the original distributed individual level data, by communicating only the relevant aggregate data. When the retrieved aggregates are harmonised and integrated, they can be fitted into the matrices in equation (9) and (10) and then used to solve equation (6). This simple example can be generalised to the more complicated equations we face when solving a GLS for a mulit-level model and the result is the same: that we can solve a GLS equation using only aggregate data retrieved from the distributed sites. 
Such aggregate matrices can be used to compute both the fixed component  X  coefficient estimates and the random component V components for any IGLS iteration, since both stages employ the same GLS methods. However since the V i estimates in the V -1 matrix change after each iteration, the matrix aggregates represented in (9) and (10) need to be recalculated from the micro data each time. The same applies to the computation of an aggregate G matrix for the random component estimation part of an iteration. This implies that in the distributed case, a number of communications between the central domain server and the distributed sites are required for each iteration of the IGLS. This is the focus of the following section. Having shown in section 4.3 that the aggregate data from the macro relations can be used to solve a multilevel problem with IGLS, this must be applied to the distributed data. Each iteration of the IGLS requires a number of communications of data and requests for data between the distributed sites and the domain server. This series of steps is illustrated in Figure 2.
 Figure 2. Steps involved in IGLS from distributed database
Currently in our system, the user specifies the particular multilevel model of interest. An automated approach allows the versions require sums of products of individual elements of the
X matrix and the V -1 matrix which must be computed at the distributed sites. It is for this reason that the V -1 matrix must be communicated to the distributed sites. user to simply select attributes which may be of interest and the system automatically builds the best model from this given set of attributes. Work on the automated approach is at an early stage and is not described further here. Historically multilevel problems were handled by analysing all attributes at one single level of interest. This was accomplished by either stepping-up 3 or stepping-down to an appropriate hierarchy level and computing the model at that level using multiple regression techniques. Stepping-up a level implies carrying out the analysis at a level higher than the individual level. For example if we step-up to the region level, the analysis is carried out at this level and the means of the individual level attributes in each region are used in place of the individual level values. If the analysis is carried out at the individual level, stepping-down a level implies moving region and country level attribute values from being descriptors of these groups, to being descriptors of the individual tuples.

However, while these strategies would be very efficient computationally for our purposes as they do not require an iterative solution, they create two different sets of problems. Kreft [11] states that the within-group information can account for as much as 80-90% of the total variation in such data. Therefore by stepping-up to a higher hierarchy level, this information would be lost before the analysis has even begun. Stepping-down can lead to many apparent statistically significant relationships that are in reality questionable [10].

There can also be problems associated with analysing all of the data solely at one level and drawing conclusions about the relationships at another level. Robinson [18] shows that relationships between means of individual level attributes for example, can not be used as estimates for the corresponding individual-level relationships. This is known as the Robinson up" and "stepping-down" are "aggregation" and "dis-aggregation". We do not use the traditional terms here in order to avoid confusion as we use the term aggregation elsewhere in another context. effect or the ecological fallacy. Robinson also showed that drawing inferences at a higher level from an analysis at a lower level can be just as misleading. This is known as the atomistic fallacy. Therefore these techniques would be unsuitable in this case. Once the model parameters have been calculated and validated for appropriateness, the results are presented to the user. Our approach summarises the main details of this output in a format more suited to a user not overly familiar with statistical modeling and analysis. This begins with a graph as shown in Figure 3, containing only those explanatory attributes or effects from the various hierarchical levels which exhibit a statistically significant relationship with Cost of Insurance claim . The higher the bar, the greater the statistical significance of the relationship. Presenting the standardised effects initially in Figure 3 allows the user to compare the statistical significance of the attribute relationships against each other on a common scale. By clicking on the bottom left-hand button in Figure 3, the user may view the significant effects on an un-standardised scale. These values are more important in terms of the real effect of the significant attributes on the insurance cost claims. This information is also conveyed to the user in more detail in the form of rules. There is a significant cross level interaction effect indicated between the region level attribute region type and yearly mileage from the individual level. This means that the relationship between yearly mileage and insurance claim cost is also dependant on the associated value of region type .

The user can interact fully with this graphical output to interrogate the results at more detailed levels. This allows the user to understand an effect  X  s relationship with the cost of insurance claim in greater depth. By clicking on one of the level options in the legend, the user can view a tabular representation of the statistical significance of all the explanatory attributes included in the model at that level, or for all interactions included in the model. For significant dimension and measure attributes not involved in interactions, it is possible to isolate their effect on the cost of insurance claim while holding all of the other attributes values constant. For dimensions, this is accomplished by drilling down to the attribute domain value level, presented both graphically and in rule format (in conjunctive normal form). This is accomplished by clicking on the bar of an attribute of interest in Figure 3. As an example, by clicking on the Car-Class bar, the graph and corresponding rules in Figure 4 appear. This shows a breakdown of average insurance claim costs for each car-class. The graph illustrates this for each car-class in terms of deviance in claim cost from the overall average claim cost. The rules present this information with associated statistical confidence intervals. From the rules, it can also be seen that there is a statistical difference between cost of claims for car-class C and the other classes, while there is no statistically significant difference between the cost of insurance claims between drivers in car-classes A and B. This illustrates at a lower more detailed level, the relationship between car class and cost of insurance claims. The user could then, after further investigation, decide to charge drivers of car class A a higher premium to cover this extra cost.
Effects are interpreted differently for explanatory attributes which are measures. These are presented as rules where a one unit increment in the explanatory measure, holding all other attribute values constant, brings about a certain increase or decrease in the cost of insurance claim. These rules are also presented with an associated confidence interval. For attributes involved in an interaction effect, if all or some of the attributes are measures, the rules resulting are similar to those for measures (but having more conjunctions). If the interacting attributes in an effect are all dimensions, the rules are similar to those for a dimension effect (again having more conjunctions) and a graph similar to Figure 4 is also presented to the user, broken down for each combination of the cross pr oduct. As a final step, the user can view all resulting rules together. The combined set of rules summarise the discovered knowledge. In this way, the user can browse through the results, moving from a high-level graphical representation of the attribute level information, to lower more detailed levels.

Taking information from the final V matrix coefficients produced by the multilevel model, a graph of the statistically significant variation components within each level is presented to the user. For the insurance problem this can be divided into the following variation components: Type 1. % of the overall variation in cost of insurance Type 2. % of the overall variation in cost of insurance Type 3. % of the overall variation in cost of insurance
This information is also presented to the user in Figure 3. In the insurance data, 33% of the variation was of type 1, 24% was of type 2 and a large 43% of the variation was of type 3 between countries. These figures aid the user in determining where most of the variation in the cost of insurance claims arises. The final pieces of knowledge which are automatically presented to the user are exceptions to discovered rules. A rule is something which holds for a grouping of drivers or observations. An exception to a rule represents a single driver, or a minority grouping of the drivers covered by a rule, who behave in a manner which is significantly different from the majority of the grouping in some sense. More specifically, the term  X  exception  X  here is defined as an individual or aggregate measure value which differs in a statistically significant manner from its expected value calculated from the multilevel model. The exceptions (as is the case for the rules) are presented in conjunctive normal form, along with the actual cost of insurance claim (an average if it involves a group of drivers rather than an individual driver) and the expected cost (from the model) in terms of a confidence interval. A simple example is an exception grouping defined as:
This grouping can be deemed to be an exception as the actual claim cost lies outside the range as predicted by the multilevel model. This exception can be investigated by the user. One factor which is also important to a user interested in finding exceptions, is to know in what way they are exceptions. The user may possibly be given some guidance in this quest through an examination of the rules which are related to the exception. We
C ar-Class {Type A }  X  cost of insurance c laim between {1700.4 -2148.4}
C ar-Class {Type B }  X  cost of insurance c laim between {1161.4 -1582.4}
C ar-Class {Type C }  X  cost of insurance c laim between {1039.4 -1487.4} define a rule and an exception to be related if the rule antecedent is nested within the exception antecedent . If we assume that as well as the rule set in Figure 4, we also have a rule as shown in (12), it can be stated in this simple illustration that car-class is in some sense a causal factor in exception (11), as the actual cost of insurance claim 2100 ECU ex ceeds the range of car-class A's rule, but not that of the rule in (12). This may conveys more knowledge to the user about the exception. Further work is required on this last concept to automate the process in some suitable way. Before making any decisions, this exception should be investigated in detail by the user. If it is found to be a data error of some sort, it should be removed from the data and the model rebuilt. Otherwise it should remain in any model building. Due to the confidentiality constraints associated with our distributed data, it may not be permitted to discover such exceptions at the individual level of the data and therefore such exceptions may be discovered at aggregate levels only. As a final note, if the model is to be used for prediction purposes, it is beneficial to split the data into a test set and a training set similar to decision tree building in order to obtain a more robust model [17]. Multilevel models have also been used for consumer purchasing behaviour. [19] uses multilevel models to look at the purchase and use of personal care products in terms of product use profiles. Of primary interest was the variation in behaviour between individuals. In the area of supervised learning, a lot of research has been carried out on the discovery of rules in conjunctive normal form and some work is proceeding on the discovery of exceptions and deviations for this type of data [20, 21]. A lot less work in the knowledge discovery area has been carried out in relation to a measure attribute described in terms of either dimension attributes alone or a combination of dimensions and measures at levels in a hierarchy. Some closely related research involves a paper on exploring exceptions in OLAP data cubes [21]. The authors there use an ANOVA model to enable a user to navigate through exceptions using an OLAP tool, highlighting drill-down options which contain interesting exceptions.

Some work on knowledge discovery in distributed data has also been carried out. In the main, this has consisted either of distributing the data in a suitable way in order to increase the efficiency of a data mining algorithm, or using meta-learning to build a model at each site and attempt to combine the models rather than the data at a central site [4, 5, 22, 23, 24, 25]. Graefe et al.[4] have conducted research into sufficient statistics for the task of classification and the set of frequent itemset in association rule mining could also be considered to be a set of sufficient statistics. In summary, we have described a means of discovering rules and exceptions from hierarchically structured data returned from a number of databases distributed over the internet. We have shown how it is possible to use a set of sufficient statistics in place of the individual level data to build such multilevel models over a distributed database. We have also illustrated ways in which the knowledge represented in the model can be presented to the user at different levels of detail. Further work will involve the intelligent automatic building of a good multilevel model where only the attributes of interest are specified by the user. Since this will involve many more iterations than the current implementation, efficient means will be required for this process. One possibility is the use of random sampling of the distributed data to build initial models. Other possible algorithms and data mining tasks which could take advantage of the aggregates will also be investigated. This work has been partially funded by ADDSIA (Esprit project no. 22950) which is part of EUROSTAT's DOSIS initiative. [1] Bell, D., Grimson, J.: Distributed database systems. [2] M c Clean S., Grossman, W. and Froeschl, K.: Towards [3] Lamb, J., Hewer, A., Karali, I., Kurki-Suonio, M., [4] Graefe, G, Fayyad, U., Chaudhuri, S.: On the Efficient [5] Aronis, J., Kolluri, V., Provost, F., and Buchanan, B.: The [6] Chaudhuri, S., Dayal, U.: An Overview of Data [7] Shoshani, A.: OLAP and Statistical Databases: [8] Albrecht, J. and Lehrner, W.: On-Line Analytical [9] Goldstein,H.: Multilevel Statistical Models. New York, [10] Bryk, A.&amp; Raudenbush, S.: Hierarchical Linear Models. [11] Kreft, I., de Leeuw, J.: Introducing multilevel modeling. [12] Bi, Y, Murtagh F, and McClean, S.: Metadata and XML [13] Sadreddini M., Bell D., &amp; McClean SI.: A Model for [14] Gray, J., Bosworth, A., Layman, A., Pirahesh, H.: Data [15] Goldstein, H.: Multilevel mixed model analysis using [16] P  X  irc  X  ir, R., McClean, S., Scotney, B.: Automated [17] Neter, J.: Applied linear statistical models 3rd eds. [18] Robinson, W.S.: Ecological correlations and the behavior [19] Romaniuk, H., Skinner, C.J. and Cooper, P.J.: Modeling [20] Arning, A., Agrawal, R. and Raghavan, P.: A linear [21] Sarawagi, S., Agrawal, R., Megiddo, N.: Discovery-[22] H. Kargupta, B. Park, D.Hershberger and E. Johnson: [23] Cheung, D., Han, J., Ng, V., Fu, A., Fu, Y.: A Fast [24] Provost, F.: Distributed data mining: scaling up and [25] Stolfo, S., Prodromidis, A., Tselepis, S., Lee, W., Fan, D.,
