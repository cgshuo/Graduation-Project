 1. Introduction
Review assignment is a common task that many people such as conference organizers, journal editors, and grant admin-istrators would have to do routinely. The task usually has to do with assigning a certain number of experts to review a re-search paper or a grant proposal and judge its quality. Currently such assignments are mostly done manually by editors or conference organizers. Some conference systems support bidding on papers by reviewers, shifting some of the assignment task to the reviewers, but it is still mostly a time-consuming manual process. Manual assignment is not only time-consum-matching of expertise would presumably be more accurate for reviewers whose expertise is largely known to an editor or a conference program chair, but less accurate for reviewers not so familiar to the review assigner.

To help people who manage review assignments to improve their productivity and to potentially correct any bias in re-the problem is considered as a retrieval problem, where the query is a paper (or a grant proposal) to be reviewed and a candidate reviewer is represented as a text document. One main drawback of all these works is that a paper or proposal is matched as a whole component without taking into account the multiple subtopics.  X  search X  X . However, the existing methods may assign three reviewers that are all experts on machine learning but do not know well about Web search.
 review assignment: Redundancy Removal , Reviewer Aspect Modeling , and Paper Aspect Modeling . These proposed methods for multi-aspect review assignment are shown to increase the aspect coverage for automatic review assignment. However, in this work, the assignment of reviewers to each paper is done independently without considering the whole committee. This makes it hard to balance the review load among a set of reviewers, and may result in assigning too many papers to a reviewer with expertise on a popular topic. In many applications of review assignment, such as assigning conference papers to the reviewers on the program committee and assigning grant proposals to a panel of reviewers, there is a limit on the number of reviews to be done by each single reviewer. To balance the review load and conform to the review quota of each reviewer, it is necessary to set up the problem as to simultaneously assign papers to all the reviewers on a committee with consider-ation of review-load balancing. We call this problem Committee Review Assignment (CRA).
 expertise in matching papers with reviewers.

In this paper, we study a novel setup of the CRA problem where the goal is to assign a pool of reviewers on a committee to a set of papers based on multi-aspect expertise matching (i.e., the assigned reviewers should cover as many subtopics of the paper as possible) and with constraints of review quota for reviewers. We call this problem Constrained Multi-Aspect Com-mittee Review Assignment (CMACRA). Given the limited review capacity, limited reviewer expertise, and the need for matching papers with reviewers based on multiple aspects, the CMACRA problem is quite challenging.

We propose to solve the CMACRA problem by casting it as an integer programming problem and present an integer linear programming formulation of the problem. In our optimization setup, matching of reviewers with a paper is done based on matching of multiple aspects of expertise. Therefore, the assigned reviewers would not only have the required expertise to review a paper but also can cover all the aspects of a paper in a complementary manner. In addition, such an assignment does not unduly burden any individual reviewer with too many papers and at the same time a full set of reviewers is as-signed to each paper. All these preferences and requirements are captured through a set of constraints in the integer pro-gramming formulation, and the objective function maximizes average coverage of multiple aspects. The proposed modate any probabilistic or deterministic method for modeling multiple aspects to automate committee review assignments.

To evaluate the effectiveness of our algorithm, we use the measures and gold standard data introduced in our previous work ( Karimzadehgan et al., 2008 ). Our experiment results show that the proposed committee review assignment algorithm is quite effective for the CMACRA task and outperforms a heuristic greedy algorithm for assignment. The results also show atively large conference. Although we mainly evaluated our algorithm in the context of conference review assignment, it is general and can be applied to other review assignment tasks such as assigning grant proposals to a group of panelists for reviewing.
 The rest of the paper is organized as follows. We first discuss related work in Section 2 and the problem of Constrained
Multi-Aspect Committee Review Assignment in Section 3 . We then propose our algorithm to optimize the committee assign-our evaluation results. We conclude the paper in Section 7 . 2. Related work
Review assignment has been studied in several previous works. Dumais and Nielsen (1992) did an early study using La-tent Semantic Indexing (LSI). A recent work by Hettich and Pazzani (2006) is a recommendation system which recommends panels of reviewers for NSF grant applications by using TF X  X DF weighting measure. Basu et al. (1999) use Web to find ab-stracts of papers written by reviewers and then TF X  X DF weighting is used for ranking. Biswas and Hasan (2007) use topics defined based on a domain-ontology to represent papers and reviewers and use TF X  X DF weighting to rank reviewers. Mimno and McCallum (2007) propose an Author-Persona-Topic (APT) model, in which topical components are learned and then each author X  X  papers are divided into many  X  X  X ersonas X  X . Rodriguez and Bollen (2008) present a method that propagates a par-ticle swarm over the co-authorship network, starting with the authors cited by the submitted paper. Karimzadehgan et al. (2008) consider multiple aspects of papers and multiple expertise of reviewers and they match papers with reviewers based on aspect matching. Our work is different from all these works in that we consider the CMACRA problem, i.e., considering constraints associated with papers and reviewers and maximizing the coverage of multiple topics in a paper by the assigned reviewers.
 Committee review assignment has been studied in Benferhat and Lang (2001), Hartvigsen and Wei (1999), Merelo-
Guervos and Castillo-Valdivieso (2004), Sun, Ma, Fan, and Wang (2008), Taylor (2008) ; Benferhat and Lang (2001) present a heuristic approach in which they define regulations to reduce the set of feasible solutions, and they use preferences and constraints to order the feasible assignments and select the best one. Authors of Merelo-Guervos and Castillo-Valdivieso (2004) present another heuristic evolutionary algorithm by maximizing the match between the reviewers X  expertise and pa-per topics. Hartvigsen and Wei (1999) use minimum cost network flow. For each reviewer and paper, they define a weight denoting the degree of expertise of a reviewer for that paper. They then adopted the idea of finding the assignment by solv-ing a maximum weighted X  X apacitated transportation problem on the network. Taylor (2008) solves the global constrained optimization problem using Linear Programming. The information needed for optimization is based on the preferences indi-cated by the area chairs and affinities. However, their optimization framework cannot solve the constrained multi-aspect matching. Considering the constrained multi-aspect in the optimization framework makes the problem more challenging. bination of mathematical decision models with knowledge rules. Their aim is to maximize the total expertise levels of the erage of topics in the paper and reduce the redundancy in covering each topic by assigned reviewers.
Integer linear programming has been applied to solve several other information management tasks. Roth and tau Yih (2005) use integer programming for the inference procedure in conditional random field in order to support general struc-tures. In Gkoulalas-Divanis and Verykios (2006) and Menon, Sarkar, and Mukherjee (2005) , integer programming is used to secure sensitive knowledge from being shown in patterns extracted by frequent pattern mining algorithms. Clarke et al. (2006) use integer programming to compress sentences while preserving the meaning of the sentences. Berretta, Mendes, and Moscato (2005) use integer programming to set up a unified framework for feature selection process for molecular clas-sification of cancer.

Expert Finding is another area related to our work. In Expert Finding, a ranked list of experts with expertise on a given topic is retrieved. Several probabilistic models and language models are presented in Balog, Azzopardi, and de Rijke (2006), Fang and Zhai (2007) and Petkova and Croft (2006) . Some other systems use graph-based ranking algorithms for determining the connections between people. These methods ( Campbell, Maglio, Cozzi, &amp; Dom, 2003; Dom, Eiron, Cozzi, trieved documents, Macdonald and Ounis (2006) model the ranking of candidates as a voting process using the retrieved documents and documents in experts X  profile. They then apply data fusion techniques to generate the final ranking for ex-perts with this model.
 with a multinomial word distribution. We use topic models to discover multiple aspects of expertise of reviewers, which are then used in the proposed optimization framework to enable matching of reviewers and papers based on multiple aspects.
The basic idea of our work has been published in a short paper of CIKM ( Karimzadehgan &amp; Zhai, 2009 ). However, this expansion of the previous short paper to add a comprehensive review of related work, more complete description of the algorithms, new experiment results for inferred topics, complexity analysis of the algorithms, and a new set of scalability experiments and results. 3. Constrained Multi-Aspect Committee Review Assignment
Informally, the problem of Constrained Multi-Aspect Committee Review Assignment (CMACRA) is to reflect a very com-mon application scenario such as conference review assignment where the goal is to assign a set of reviewers to a set of pa-pers so that (1) each paper will be reviewed by a certain number of reviewers; (2) each reviewer would not review more than a specified number of papers; (3) the reviewers assigned to a paper have the expertise to review the paper; and (4) the com-edge, no previous work on review assignment has considered all these requirements. Indeed, in most previous work, the problem has been simplified as using each paper as a query to retrieve a set of right reviewers with expertise matching
We propose to solve this problem by casting it as an optimization problem. As a computation problem, CMACRA takes the following information as the input: A set of n papers: P X f p 1 ; ... ; p n g where each p j is a paper.

A set of m reviewers: R X f r 1 ; ... ; r m g where each r i
A set of reviewer quota limits: NR ={ NR 1 , ... , NR m } where NR
A set of numbers of reviewers to be assigned to a paper: NP ={ NP should be assigned to paper p j .
 And the output is a set of assignments of reviewers to papers, which can be represented as an n m matrix M with
M ij 2 {0,1} indicating whether reviewer r i is assigned to paper p
To respect the reviewer quota limits and to ensure that each paper gets the right number of reviewers, we require M to satisfy the following two constraints: Naturally, we assume that there are sufficient reviewers to review all the papers subject to the quota constraints. That is,
In addition, we would also like the review assignments to match the expertise of the assigned reviewers with the topic of the paper well, and ideally, the reviewers can cover all the subtopics of the paper. Formally, let s =( s subtopics that can characterize the content of a paper as well as the expertise of a reviewer, and s subtopics can either be from the list of topic keywords that are typically provided in a conference management system to
The subtopics in the first case are usually designed by human experts that run a conference such as program chairs, and both the authors and reviewers would be asked to choose some specific keywords to describe the content of the paper and the expertise of the reviewer, respectively. Thus we would have access to deterministic assignments of subtopics to the pa-pers and reviewers. In the second, a subtopic can be characterized by a word distribution, and in general, a paper and a re-viewer would get a probabilistic assignment of subtopics to characterize the content of the paper and the expertise of the reviewer. Since deterministic assignments can be regarded as a special case of probabilistic assignments when the probabil-ity is either 1.0 or 0.0, we thus only need to consider the probabilistic assignments of subtopics.

Thus we assume that we have two matrices P and R available, which represent our knowledge about the subtopics of the content of a paper and the subtopics of the expertise of a reviewer, respectively. P is a n K matrix where P indicating how likely subtopic s k represents the content of paper p how likely subtopic s k represents the expertise of reviewer r having deterministic assignments of subtopics to papers and reviewers.

Since there are potentially a very large number of possible committee assignments, our solution space is huge and a bruce force enumeration of all the possible solutions would not be feasible. Thus a main technical challenge in solving the CMACRA reviewers and the topics of papers with consideration of multiple aspects of topics and expertise. We discuss how we solve this challenge in the next section. 4. Algorithms for CMACRA
Our main idea for solving the CMACRA problem is to cast it as a tractable optimization problem, i.e., an integer linear pro-gramming problem. Before we present such an optimization algorithm, we first present a non-optimal heuristic greedy algo-rithm, which only works for the scenario of deterministic subtopic assignments to papers and reviewers. We would also use this algorithm as a baseline to evaluate our integer linear programming algorithm. 4.1. A greedy algorithm
A straightforward way to solve the CMACRA problem is to use a greedy algorithm, in which we would optimize the review assignments for each paper iteratively. The algorithm only works for the scenario of deterministic assignments of topics to papers and reviewers. It works as follows: reviewer that can cover most subtopics of the paper is assigned. In addition, the review quota and paper quota are checked, i.e., the number of papers assigned to each reviewer and the number of reviewers assigned to each paper. If the review quota scribes the algorithm more formally.

This greedy algorithm does not always lead to an optimal solution. Intuitively, since at each assignment stage, it greedily a subtopic quickly when processing the top-ranked papers in the ranked list. As a result, such reviewers would no longer be available later when we encounter a paper that really needs reviewers with such rare expertise. In the next subsection, we formulate the problem in a more principled way as an integer linear programming problem which can achieve optimized review assignments. 4.2. An integer linear programming algorithm
In this subsection, we propose a formulation to use Integer Linear Programming (ILP) ( Korte &amp; Vygen, 2006; Papadimi-solving this problem is natural because what we want to compute is binary assignments of papers to reviewers, and BIP can naturally compute such binary assignments to optimize a linear objective function subject to linear constraints. The main motivation for framing the problem with linear constraints and linear objective function is to ensure that the optimi-zation problem can be solved efficiently. 4.2.1. The ILP formulation
Linear Programming (LP) is a way to optimize a linear objective function, subject to linear equality and linear inequality ear Programming, if all the unknown variables are required to be integers, then the problem is called an Integer Linear Pro-gramming (ILP). Binary Integer Programming (BIP) is the special case of integer linear programming where variables are required to be zero or one.

We now show how the problem of CMACRA can be formulated as a Binary Integer Programming problem. First, we ob-serve that in our formal definition of the CMACRA problem in Section 3 , we have already naturally introduced several con-straints, and the problem can be cast as an optimization problem where we seek an optimized assignment matrix M that would satisfy all the constraints as well as optimize the multi-aspect matching of expertise of reviewers and the content of each paper. Thus M ij would naturally become variables in the definition of the ILP problem.

How should we define the objective function to be optimized? Intuitively, the function must capture our desire to match the expertise of reviewers with the content of a paper based on multiple subtopics. Unfortunately the variables M help us directly because they are not defined on subtopics. Thus, we need to introduce auxiliary variables to connect M subtopic assignments. However, it is not immediately clear how to define such auxiliary variables and formulate a linear objective function.

We propose to introduce the following set of auxiliary variables: where t jk 2 [0, NP j ] is an integer indicating the number of assigned reviewers that can cover subtopic s lows us to define the following linear objective function to maximize:
Intuitively, this objective function says that we would prefer assignments that maximize the coverage of all the subtopics (the inner sum) for all the papers (the outer sum).

Now we still need to connect t jk with the review assignment matrix M . Specifically, we need an upper-bound constraint for t jk so that we will not end up having a trivial solution that simply gives each t is intuitively the actual number of reviewers assigned to paper p topic assignment is completely binary, which means that the element values of both matrices P and R are binary, it is rela-tively easy to see that we should have the following set of n inequality linear constraints, each for a paper:
Recall that P jk refers to whether paper p j covers topic s interpreted as follows: For paper p j , if the paper covers subtopic s reviewers assigned to paper p j that can cover subtopic k . Note that R , and M lj = 1 iff reviewer r l is assigned to paper j , thus assigned to paper p j .

Since this upper-bound is itself an integer, and our objective function encourages choosing a larger value for t solution, we will end up having t jk  X  X  that would actually satisfy the equality.

What if our subtopic assignment is probabilistic or fuzzy? In such a case, the element values of P and R can be any positive real numbers. It turns out that the inequality above for t the equality. Specifically, we may regard the right hand side of the inequality as to compute the weighted combined coverage left hand side can also be interpreted as the desired coverage of subtopic s very much likely about subtopic s k , and thus we would demand more coverage about s makes sense in the case of non-deterministic assignments of subtopics to papers and reviewers as in the case of learning subtopics from the publications of reviewers.
 Adding additional constraints introduced in Section 3 , the complete ILP formulation of the CMACRA problem is shown in Fig. 2 .

Our objective function indicates that for each paper we want to maximize both the number of covered topics and the number of reviewers that can cover each topic in the paper. Constraint C1 shows that each variable is either one or zero where one means reviewer r i is assigned to paper p j . Constraint C2 indicates that t maximum NP j , where NP j is the number of reviewers that should be assigned to paper p paper p j will be assigned precisely NP j reviewers. Constraint C4 indicates that each reviewer r
Finally, constraint C5 requires that variable t jk be constrained by the actual coverage of subtopic s to paper p j according to M .
 If we have knowledge about conflict of interest of reviewers, we may further add the following additional constraint:
C6 : M ij = 0, if reviewer r i has conflict of interest with paper p 4.2.2. Solving the ILP
Once our problem is formulated as an ILP problem, we can potentially use many algorithms to solve it. In our experi-ments, we use the commercial ILOG CPLEX 11.0 package 2 to solve our CMACRA problem. ILOG CPLEX simplex optimizers
Mathur, 1989 ) algorithm which is an exact algorithm consisting of a combination of a cutting plane method with a Branch-and-Bound algorithm to solve integer linear programs. The idea of the  X  X  X ranch-and-Bound X  X  algorithm ( Salkin &amp; Mathur, algorithm ( Papadimitriou &amp; Steiglitz, 1982 ).

The  X  X  X ranch-and-Cut X  X  algorithm is essentially a  X  X  X ranch-and-Bound X  X  algorithm with an additional Cutting step. When the and add them to the LP relaxation of the subproblem. At this point, the problem is divided into two subproblems: one is to less than or equal to the next lesser integer. These new linear programs are then solved using the simplex method and the tion about the algorithm, a reader is referred to Salkin and Mathur (1989) . 4.3. Complexity analysis of the algorithms
The complexity of the greedy algorithm in the worst case is O ( n log n + n m log m ), where n and m are the number of papers and the number of reviewers, respectively. The n log n part is the sort time for papers according to the number of topics they have. In line 3 of the algorithm, for each paper, we also sort the reviewers according to covering most topics of the papers. So, the complexity for that part is: n m log m .
 complexity of our ILP algorithm is O (2 n m N n k ), where k is the number of subtopics, and N is the maximum number of reviewers that should be assigned to a paper, i.e., N = max two children based on rounding of a variable with a fractional part. So, at the worst case, we will see 2 for a general integer variable x with domain 0, ... , N , we are typically branching using cuts such as x 6 a 6 N . So, we also get two children, but it takes approximately log case. So we have 2 log 2 N  X  N paths in the tree from any node where we first start branching on x to any node where we are done branching on x (assuming we do all x branching before moving on to another variable). So the overall complexity is
O (2 n m ( N ) n k ). Although the worst case complexity of the optimized algorithm is exponential, it works empirically well assignments for reasonably large conferences. 4.4. Modeling and assigning subtopics
The proposed algorithms are based on the assumption that we have available a set of subtopics s and the assignments of authors and reviewers to choose subtopic keywords, in which case we generally would have a binary P and R .
In applications where we do not have such input from authors and reviewers, we may learn subtopics from the publica-tions of reviewers and compute probabilistic assignments of subtopics to papers and reviewers as has been done in our pre-can take non-binary element values of P and R . Indeed, the algorithm is completely general to take any meaningful positive weights in P and R as long as these weights are comparable (e.g., when they are all probability values).
In our experiments, we experiment with both application scenarios: (1) We have complete knowledge about subtopics and their assignments to papers and reviewers. (2) We have no such knowledge but we have publications of reviewers so we can learn subtopics and their assignments. In the second scenario, we can use the Probabilistic Latent Semantic Indexing (PLSA) ( Hofmann, 1999 ) approach to model and learn subtopics, i.e., to set matrices P and R as described in Karimzadehgan et al. (2008) . This method has been shown to outperform other general strategies in our previous work ( Karimzadehgan et al., 2008 ) for solving the multi-aspect review assignment.
 We now describe this approach in more detail.

We assume that we have available some text documents to represent the expertise of each reviewer, which can be, e.g., the publications of the reviewer. Formally, let T R  X f T r tise of m reviewers, respectively and T Q  X f T q 1 ; ... ; T
PLSA explicitly models subtopics in the publications of reviewers with mixture language models. If we can somehow di-rectly model the potential topic aspects and try to match a paper with reviewers based on their topic-aspect representation, we may potentially achieve better results.

To implement this idea, we assume that there is a space of K topic aspects, each characterized by a unigram language model. Let s =( s 1 , ... , s K ) be a vector of topics. s then learn these K topic aspects from this set of reviewer expertise documents using a topic model such as PLSA ( Hofmann, 1999 ), which is described below.

The log-likelihood of the whole collection according to PLSA is: where V is the set of all the words in our vocabulary, c  X  w ; T selecting topic aspect s a for document T r i . Intuitively, p ( a j h document of reviewer i . p ( w j s a ) is the probability of word w according to topic s
We may use Expectation X  X aximization (EM) algorithm ( Dempster, Laird, &amp; Rubin, 1977 ) to compute the maximum like-lihood estimate of all the parameters including s and h i
Once we obtain h i which is a distribution over all the possible topic aspects p ( a j h way to represent our document T r i in terms of topic aspects. With a similar model to the one presented above, we can also estimate the subtopic coverage in our query papers, T Q , with the main difference being that we have s already known so we only need to estimate h T Q which would give us a distribution over subtopics for each query paper (e.g., p  X  a j h query, where a =1, ... , K ). The distributions of topic aspects for each reviewer (i.e., p ( a j h used to fill out matrices R and P , respectively. 5. Experiment design
In this section, we describe the data set and evaluation measures. 5.1. Dataset
Evaluating of such a system is very challenging. Since the actual assignments of reviewers to papers are confidential, we cannot use such data to evaluate the effectiveness of our methods. Even if we had such information, it would not necessarily for matching reviewers with papers based on multiple aspects not only for retrieving experts/reviewers. The only data set available for evaluating multi-aspect review assignment is the one we created in our previous work ( Karimzadehgan et al., 2008 ). 3 We thus use this data set in our experiments. The details of the dataset is as follows:
This data set was constructed based on the abstract papers of ACM proceedings from years 1971 X 2006 from the ACM digital the prospective reviewers reduced the number of reviewers to 189. The papers in SIGIR 2007 are used to simulate papers that are to be reviewed. There are 73 papers with at least two aspects. We experimented with both abstracts and full papers.
To create a gold standard for evaluating our approaches, an information retrieval expert identified 25 major subtopics based on the topic areas in the Call for Papers of ACM SIGIR in most recent five years and session titles in the recent ACM relevant expertise/topic aspects to each paper and each reviewer. This serves as a gold standard to evaluate our methods. 5.2. Evaluation measures
We use the evaluation measures defined in the previous work ( Karimzadehgan et al., 2008 ). For completeness, we include the description of these measures taken from reference ( Karimzadehgan et al., 2008 ).

This can be captured by the Coverage measure which tells us whether we can cover all aspects of the query by the assigned reviewers. Consider a query with n A topic aspects A 1 ; ... ; A n assigned reviewers can cover. Coverage can be defined as the percentage of topic aspects covered by these n reviewers:
We would also prefer an assignment where each aspect is covered by as many reviewers as possible. Given the same level of coverage, we would prefer an assignment where each aspect is covered by as many reviewers as possible. Intuitively, this Confidence measure is defined to capture the redundancy of reviewers in covering each aspect and is defined as follows:
Let A 0 1 ; ... ; A 0 n cover aspect A 0 i , then Confidence measure is as follows:
The confidence values are normalized with the number of covered topic aspects n obtained by intentionally covering fewer aspects. This is why confidence alone would not be so meaningful and it should be combined with the coverage. In this sense, the relation between coverage and confidence is similar to that between precision and recall. A perfect assignment should have both high coverage and high confidence. One way to combine coverage and
Since a missed aspect would decrease the average confidence, it serves as a combination of coverage and confidence. Using the notations introduced earlier, the Average Confidence measure is defined as follows:
We normalize all three measures by considering their corresponding optimal values that can be gained from our gold use greedy algorithms to compute an approximate value.

For finding the optimal Coverage , we use an MMR-based approach when selecting the next wide-coverage reviewer; i.e., we first pick the reviewer that covers the most number of aspects of the paper, then we take away the covered aspects and can cover aspect A 1 but do not cover any of the remaining aspects of the paper (to minimize n
Confidence , we select each reviewer to work independently to cover most aspects of the paper. In all cases, we follow some to make the values more comparable across different query papers. 6. Experiment results
We first examine the question whether the ILP algorithm can effectively solve the CMACRA problem. We evaluate the ILP algorithm in the following two scenarios: 1. Known subtopic assignments: This is to simulate a common application scenario where reviewers are asked to choose from a set of pre-defined subtopics a subset to describe their expertise, and the authors are asked to do the same for their papers. In this case, we can set each element of matrices P and R to either 1 or 0 based on the selections made by review-ers and authors. we can use Probabilistic Topic Models to infer subtopics and matrices P and R based on publications of reviewers as done in for reviewers and papers, and the elements of both P and R will generally have a real value between 0 and 1.0.
Since our work is the first study of CMACRA, there is no existing baseline to be compared with. We thus want to see whether the ILP algorithm can achieve better aspect coverage than the heuristic greedy algorithm. Since the greedy algo-rithm only works for the scenario of known subtopics assignments, we only compare ILP with the greedy algorithm in the first scenario where we can use our gold standard data set to obtain subtopic assignments (i.e., matrices P and R ). 6.1. Known subtopic assignments Our data set has 189 reviewers and 73 papers, thus n = 73, and m = 189. The total number of topics is 25, thus K = 25.
Average numbers of topics covered by a reviewer and a paper are 5 and 3, respectively. In all the experiments, we assign three reviewers to each paper, which is meant to resemble a typical setup of conference review assignment. Since there for the objective function for the ILP algorithm, we generate 10 such solutions and average over all. We do the same for the greedy algorithm.

We compare the ILP algorithm with the greedy algorithm by varying parameter values in three different ways. The results plot the average performance over all the papers and also show standard deviations for different solutions with error bars. Please note that invisible error bars mean zero variance.

First, in Fig. 3 , we show the results from varying the number of reviewers and allowing each reviewer to review up to five the number of reviewers, the performance of both algorithms is getting better, which is expected because as the resources, i.e., the number of reviewers increase, better reviewers can be assigned to papers. In addition, the performance of the ILP algorithm is much better than the greedy algorithm.

Second, we fix the number of reviewers to 30, and vary the number of papers each reviewer can review. This is to simulate we are also increasing the resources, and as a result, the performance of both algorithms becomes better. Also, comparing two algorithms shows that the ILP algorithm has a better performance than the greedy algorithm. The reason is that the gree-dy algorithm does not always lead to an optimal solution; since at each assignment stage, it greedily assigns the best re-viewer that can cover most aspects of the paper, it may consume all the reviewers with rare expertise on a subtopic quickly. However, since our ILP algorithm is formulated to achieve the global optimization, the problem we just mentioned will not happen in the ILP algorithm.

Finally, we compare the two algorithms when we have very limited resources, i.e., the maximum number of reviewers is 10 for 73 papers. Again we randomly select 10 reviewers and we repeat the sampling process for 10 times and get the aver-age. Each paper gets three reviewers and the number of papers that each reviewer can get is calculated according to the number of reviewers that we have. For example, if we have five reviewers, each should get 44 papers. The results are shown it does global optimization. 6.1.1. Statistical significant tests
The performance results of the ILP algorithm shown in Figs. 3 X 5 are statistically significant for all cases compared to the for all three evaluation measures (Coverage, Confidence and Average Confidence).
All these results confirm that the ILP optimization algorithm achieves better performance than the baseline greedy algo-rithm in terms of all the three measures. 6.2. Inferred subtopic assignments
In this subsection, we look at the scenario when the subtopics are unknown, and focus on studying how to optimize per-formance when we use PLSA to predict subtopics. We follow the work in Karimzadehgan et al. (2008) and learn the subtopics for both papers and reviewers using the PLSA model. We further look into cases when we only need to infer one of the two matrices, i.e., one of the matrices are learned with PLSA model and the other one is gained from gold standard data.
While our ILP algorithm can be directly applied to the probabilistic assignments of subtopics given by PLSA, intuitively, not all the predictions are reliable, especially the low-probability ones. Thus we also experimented with pruning low prob-means when we only keep the top five probability values out of 25 learned topics and prune the rest. The figure shows the result of the Coverage measure. The figure suggests having more topics such as 15 and 25 for reviewers and fewer topics for paper, i.e., K P 4 and K 6 7 would help improve the performance. Additional observation from the figure is, having more top-ics for reviewers such as 25 leads to a more stable curve.

Fig. 6 (right) shows the performance of the ILP algorithm in Average Confidence. We can observe the same trend as in the curves for the Coverage. Since the variance of multiple solutions is small, to aid exposition, we do not show error bars in Fig. 6 .

Table 1 compares the results of applying ILP algorithm directly to the PLSA results (25 learned topics for both reviewer and paper) with the best result obtained from pruning low-probability topics. The best pruning result was obtained when statistical significance using a Wilcoxon Signed-Rank test ( Wilcoxon, 1945 ) indeed indicates that the difference between PLSA (Best Cutoff) and PLSA (No Cutoff) is statistically significant.

So far, we looked at the scenario when the subtopics for both papers and reviewers are unknown and used PLSA to predict subtopics. In order to have a better understanding of the our algorithm, we now further look into cases when we only need to infer one of the two matrices P and R . 6.2.1. Estimating subtopics for reviewers only with PLSA
In the first scenario, we learn 25 topics (the same number of topics as in gold standard data) using PLSA and only predict vert the topic weights learned with PLSA into binary values; for example, Cutoff probability values and set them to one and set the rest to zero.
The results are shown in Fig. 7 (left). The figure also shows the precision and recall curves for the estimated topics based on PLSA. Since we know the true aspects from the gold standard data and we also have the estimated topics with
PLSA model and Cutoff, we can easily calculate precision and recall at each given cutoff point. In these results, each paper gets three reviewers and each reviewer gets up to five papers to review. The interesting observation is that as Cutoff in-creases, the performance in the Coverage measure decreases. The reason can be explained based on precision and recall curves. When we have a perfect precision but low recall for reviewers, it means that we could not get true aspects with
PLSA model but we also do not have noise for reviewers X  topics. On the other hand, when we have a perfect recall but low precision, it means that we could get all the aspects for reviewers with PLSA model but we also have some noisy aspects which might mislead the optimized algorithm. In this figure, the performance is more sensitive to the precision, i.e., when the precision decreases the performance of the algorithm in the coverage measure also decreases, suggesting that we are probably introducing noise, i.e., we have considered a reviewer as an expert on a topic when the reviewer is not really an expert on the topic. 6.2.2. Estimating subtopics for papers only with PLSA
In the second scenario, we predict the topics for papers with PLSA model and the topics for reviewers are obtained from gold standard data. In this experiment, we also assign 3 reviewers to each paper and each reviewer gets up to five papers to Coverage measure also increases. This observation can be again explained with precision and recall curves for PLSA model. case, the Coverage measure would be high because with PLSA model we could estimate not only the true aspects in the paper but also some extra aspects which are noise. However, these noises do not hurt the performance much when we have en-ough resources, i.e., enough reviewers. On the other hand, when we have a perfect precision but low recall, it means that we would minimize the noise in the topics for the paper but we also miss some aspects. That is why the coverage would be low-the performance trend for the Coverage measure is the same as for recall curve. 6.3. Scalability of ILP using the 73 papers in gold standard data. In reality, the number of submitted papers to a conference is much larger. Unfor-tunately there is no data set with more papers available for us to use, thetic data to simulate the scenarios of conference review assignments with larger number of submissions.
Specifically, we first select the number of topic areas be 25 (the same number of topics as in the gold standard data). We then get the average number of expertise areas for reviewers from the gold standard data which is 5 and the average number of topics in our query papers (73 papers) which is three. These are used to specify how many topics out of 25 should be one for each reviewer and paper. So, we randomly choose five and three topics out of 25 to be the ones assigned to each reviewer and paper, respectively. In our experiments, we also vary the number of topics, i.e., 50 and 100. When we have 50 topics, we randomly select 10 and 6 out of 50 topics to be one for each reviewer and paper, respectively. Since the number of topics is reviewers and 3 to 6 for papers. The same is done when we have 100 topics.

The number of reviewers to be assigned to each paper is three (this is what is usually done in real conferences) and each reviewer can get up to six papers. Then, we vary the number of papers and reviewers. Consider that n is the number of pa-pers, the minimum number of reviewers which are needed is n  X  3/6 (three is the number of reviewers assigned to each paper and six is the maximum number of papers that each reviewer can review.).

Fig. 8 (left) shows the runtime of both the ILP and greedy algorithm as the number of papers increase. Please note that the runtime is only to generate one optimal solution. The algorithms run on Intel (R) Xeon (R) CPU, 1.6 GHZ, with 8 GB memory.
The runtime for both algorithms increases when we have a large number of papers and topics as expected. Since the runtime greedy algorithm separately in Fig. 8 (right) as well. Given the computational complexity of the ILP algorithm, this observation is intuitively expected; indeed, as we increase the number of papers, more time is needed to find the optimal assignment, because the number of variables is increased, as a result, the algorithm behaves exponentially to the number 50 keywords, the ILP algorithm is sufficiently efficient for use in real conferences with large number of submissions, e.g., 1000.

As expected, the greedy algorithm takes less time to run (less than a minute) but for the ILP algorithm, it takes more time to find an optimal solution for a large number of papers and topics. However, as discussed in Section 6.1 , the assignment accuracy and quality of ILP is significantly better than the greedy algorithm. Since assignment of reviewers in a conference management system is usually not required to be run in real time, spending more time to get an optimal solution is worth-while. Thus we can expect ILP to be more useful than the greedy algorithm. 6.4. Summary
Overall, our experiment results demonstrate the value of the ILP algorithm for multi-aspect committee review assign-ment. The main findings are:
The ILP algorithm is significantly better than the greedy algorithm according to Wilcoxon Signed-Rank test for all three evaluation measures, i.e., Coverage, Confidence and Average Confidence as we the results in Figs. 3 X 5 indicate. The reason is that the ILP globally optimizes the assignment however the greedy algorithm greedily assigns reviewer at each assign-ment stage which may consume all the reviewers with rare expertise on a subtopic quickly.

The ILP algorithm is scalable and it can be used for a large number of paper submissions and topics according to the results in Fig. 8 . The greedy algorithm takes less time to run, but this is at the cost of having much worse review assign-ment quality. Since assignment of reviewers in a conference management system is usually not required to be run in a real time, spending more time to an optimal solution is worthwhile as in the case of the ILP algorithm.
When predicting topics with PLSA model for both reviewers and papers and pruning the low-probability values helps improve the performance as shown in Table 1 . The reason is that when predicting subtopics with PLSA model, intuitively, not all the predictions are reliable, especially the low-probability ones, so they can pruned to further improve the performance.
 ally through applying the algorithms to review assignments in a real conference. 7. Conclusions and future work
Review assignment is an important but time-consuming task. Automatic assignment of reviews is interesting for multiple studied a novel setup of the problem, i.e., committee review assignment based on multiple subtopics, where the assigned reviewers would not only have the required expertise to review a paper but can also cover all the aspects of a paper in a complementary manner satisfying all constraints.

We proposed two general algorithms for solving this problem, including greedy algorithm and ILP algorithm. We system-atically tested the algorithms with previously created review-assignment data set. Experiment results show that the ILP algorithm is effective for increasing the coverage and confidence of topic aspects in committee assignment task, and outper-sions in a normal conference.

The proposed algorithms are general, thus they can be applied to all review assignment tasks where we need to assign a assignments and grant proposal assignments to a group of panelists.

Due to the lack of resources for evaluation, our conclusions are inevitably preliminary, thus an important future research direction is to further evaluate these algorithms with more data sets representing different application scenarios, ideally through applying the algorithms to review assignments for a real conference. Another interesting future research direction is to further extend our optimization formulation to include additional preferences such as the bids on papers entered by reviewers in a typical conference review system.

One limitation of our optimization formulation is that it maximizes only the coverage. Ideally, we would like to optimize both confidence and coverage, and further exploration of potentially better optimization algorithms would be a very inter-co-authors in both creation of the gold standard and automatic extraction of subtopics. Although this limitation is unlikely affecting much the hypotheses that we tested since the effectiveness of the proposed committee assignment algorithm is orthogonal to the optimization of topic inference, it would be interesting to further explore the use of author-topic model to model co-authors and infer topics more accurately ( Rosen-Zvi et al., 2004 ).
 Acknowledgment
We thank the anonymous reviewers for their useful comments. This material is based upon work supported by the Na-tional Science Foundation under Grant Numbers IIS-0713581 and CNS-1027965, by NIH/NLM grant 1 R01 LM009153-01. The first author was supported by a Google Ph.D. Fellowship. Any opinions, findings, conclusions, or recommendations expressed in this material are the authors X  and do not necessarily reflect those of the sponsors.
 References
