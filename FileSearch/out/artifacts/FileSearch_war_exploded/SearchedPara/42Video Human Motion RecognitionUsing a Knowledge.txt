 The development of technologies, such as high-powered computer systems, inexpensive disk storage, and broadband Internet make it easy to obtain and share a huge amount of multimedia data, such as images and videos, anywhere. In this kind of environment, it is desirable to have intelligent systems with the ability to detect and understand the multimedia contents, not simply to transmit and store such contents. Among the topics relating to intelligent systems dealing with multimedia data, video-based human motion/action recognition is a very interesting topic because it is a prerequisite for many useful applications in the fields, such as automatic visual surveillance, gaming, human-computer interaction (HCI), and so on. However, it is common knowledge that video-based human motion recognition is challenging because of a variety of environmental issues (e.g., changeable illuminations, noisy backgrounds, occlusions, and subject X  X  own characteristics). Fo r improving the accuracy of human action recognition, researchers have tried using multimodal learning approaches where they try to use data in another modalities, such as audio/speech, to help improve accuracy. In this article, we focus on human actions that are relevant to our day-to-day activities, such as walking, sitting down, standing up, running, etc. For recognizing these actions, we take a different approach: we use knowledge from another media, that is, 3D hu-man motion capture, and build a hidden Markov model (HMM)-based framework that can recognize human actions in video data. (i.e., regular 2D video sequences captured using normal Web cameras). Precision, as well as explicit knowledge of human body joint movements, are the advantages which 3D motion capture data possesses.
For extracting appropriate features from 2D video, we use a modified optical flow algorithm to extract the feature vectors f rom sequential video frames. The feature vectors extracted consist of positions, differential information, directions, and speeds from the moving body. Then, we show that there is a strong correlation between these 2D video features and the 3D motion capture features. Next, we propose a knowledge-based hybrid (KBH) methodology for hidden Markov models (HMMs) to learn from clean and precise data, such as 3D motion capture, as shown in Figure 1. The proposed framework makes use of the 3D motion capture data only during training. Once the probabilities have been estimated by using the KBH method, it works directly on the 2D video clips during the testing stage.

Through an exhaustive set of experiments, we show that the proposed approach, called the Knowledge-Based Hybrid HMM (KBH-HMM), outperforms HMMs trained by traditional learning algorithms, such as the Baum-Welch. For instance, results show that the KBH-HMM model can learn the probabilities faster compared to tradi-tional methods, such as the Baum-Welch method. Also, we show that the classification accuracy is higher for the KBH-HMM compared to HMM employing the Baum-Welch approach, as well as the linear SVM. Additionally, we compare the accuracy of the KBH-HMM modeled from both 2D and 3D feature data to the baseline model estimated by only 2D feature data, without learning by the Baum-Welch. We make an important contribution by developing a methodology that can classify one media-type content (2D video) by learning from the knowledge extracted from another media type (3D motion capture).  X  We make an accuracy improvement of 3 X 24% depending on the human motion to be recognized (average accuracy improvement was nearly 9%).  X  The proposed KBH-HMM method shows another significant contribution: it needs less than 100 milliseconds for estimating the probabilities compared with the few hours needed by the Baum-Welch method.

The faster estimation of probabilities can help in scenarios where new human mo-tion classes are introduced for recognition, as well as in cases where new training data needs to be introduced. It should be noted here that the proposed methodology is dif-ferent from other multimodal techniques in the sense that our proposed approach uses another media or modality only during training. The testing stage works on only one media data (2D video).

In this article, we present as follows. Section 2 reviews related work. Section 3 describes 3D motion capture and 2D video data characteristics. In Section 3.1, we show how to extract 3D data from 3D motion capture data. In Section 3.2, we explain how to get video-based human motion feature vectors. Section 3.3 describes how correlated the features from 3D motion capture and 2D video are. In Section 4.1, we explain the difference between the traditional HMMs and KBH HMMs in parameter learning. In Section 4.2, we show how to determine the model parameters for KBH-HMMs so that 2D video clips are correct ly classified in the testing stage. Section 5 describes the experimental procedure for testing video data and shows the experimental results from the video-based human motion recognition. In Section 6, we discuss applications, limitations, and future work. Finally, we conclude in Section 7. Video-based human motion recognition has been increasingly of interest in the areas of computer vision, machine learning, and vision-based computer graphics due to the high processing power of computers and inexpensive cost of video cameras. For video analysis related to video-based human motion recognition, there are three steps: de-tecting moving objects, tracking the objects frame by frame, and analyzing tracked objects. Among these steps, the object tra cking step precedes the human motion recog-nition, and hence a good object tracking technique is required for satisfactory results in the subsequent step of human motion recognition.

A variety of tracking methods can be applied in a specific application. Optical flow is one of the feature selections for tracking. The optical flow is the distribution of ap-parent velocities of movement of brightness patterns in an image [Horn and Schunck 1981]. This optical flow technique usually has been used for object detection and track-ing. Bobick and Davis [2001] presented a new view-based approach as a recognition method with motion history images (MHI) used for computing how motion images are moving. In MHI, the more recent motions are brighter than older motions.
Most recent works for video-based human mo tion recognition have introduced 3-dimensional scale invariant feature transform (SIFT) [Laptev and Lindeberg 2004; Laptev et al. 2007; Scovanner et al. 2007]. The 3D SIFT descriptors extended from original SIFT descriptors [Lowe 2004], which represent that local features in im-ages can be used with a bag-of-words [Csurka et al. 2004] approach for human action classification.

One of the most common approaches for video-based human motion recognition is the hidden Markov model (HMM). HMM has been used widely for speech recognition. In the 90s, HMM had been applied to computer vision applications. [Yamato et al. 1992] first applied HMMs for recognizing video human gestures, and they tried to recognize the actions for tennis by modeling HMMs. [Yang and Xu 1994] presented a method for developing a gesture-based system using a multidimensional hidden Markov model, and the method can be used in telerobotics and human computer inter-faces. They also obtained experimental results for continuous gesture recognition. [Sminchisescu et al. 2006] presented a framework for human motion recognition based on criminative conditional random field (CRF) [Lafferty et al. 2001] and max-imum entropy Markov models (MEMM) [Mccallum et al. 2000]. While HMMs have independence assumptions among observations, CRFs accommodate long-range de-pendencies between states and observations. They demonstrate that the framework can discriminate even subtle motions, such as normal walk and wander walk, and CRMs and MEMMs outperform HMMs.

As an application of vision-based human motion recognition, [Tao et al. 2005] pre-sented an intelligent video surveillance system for fall incident detections. They have used background modeling and subtractio n in HSV color space in video images and extracted the aspect ratio of a person for feature. [Cucchiara et al. 2007] proposed a multi-camera vision system for fall detection. They have used multiple cameras for solving the occlusion problem.

Video-based human motion can be taken conveniently from cameras for feature extraction. However, it can be easily affected by noisy environments or changeable illumination. On the other hand, 3D motion capture data have semantic character-istics. [Barbi  X  c et al. 2004] accurately segmented motion capture data into distinct behaviors in an unsupervised way, which is much simpler than video segmentation [Zeinik-Manor and Irani 2001]. [Li et al. 2007] achieved very high performance ratio for segmenting and classifying human motion capture data in real-time. [Tian and Sclaroff 2005] presented a new technique for recognizing hands signals using 3D motion capture data by minimizing the back projection error between 2D and 3D sequences using a dynamic time warping (DTW) algorithm.

There are numerous efforts of applying vision-based human motion recognition for game control/interface. The EyeToy for the Sony PlayStation 2 was a successful com-mercial product. 1 It is a camera device to allow players to interact with games. Instead of using a joystick, moving a player X  X  body or making a gesture can control his/her 3D avatar during gaming. Recently, Microsoft launched Kinect 2 that uses a 3D camera in-terface for controlling or interacting with the Xbox 360 by using recognition of a user X  X  gestures, face, and voice. 3D motion capture data is used widely in 3D computer animation (e.g., Polar Express and Monster House), films (e.g., Bear Wolf, King Kong, and The Lord of the Rings), video games (e.g., Tiger Woods PGA Tour and Enemy Territory) industry, and medical rehabilitation. Various and numerous motion capture data are freely available from motion capture repositories on the Web. 3 , 4 For example, in 3D computer animation, when a performer acts as the character in the scene, a motion capture system records the motion of the performer by sampling it at 120 frames per second. Motion capture systems use a variety of technologies for data capture: for instance, infrared camera-based systems (e.g., VICON motion capture system 5 ) capture the data from the reflective markers attached to a performer. By mapping the animation character to the motion capture data, the animation c haracter can perform the same actions as the real performer.

This 3D motion capture system generates data corresponding to 19 human body segments, each of which consist of six degrees of freedom (DOF) for translation (in 3 axes: T-X, T-Y, and T-Z) and rotation (in 3 axes: A-X, A-Y, and A-Z) information. The 19 human body segments typically represent pelvis, left femur, left tibia, left foot, left toes, right femur, right tibia, right foot, right toes, thorax, head, left clavicle, left humerus, left radius, left hand, right clavicle, right humerus, right radius, and right hand. A motion sequence is represented as a matrix where 114 columns are the body joint segments and rows corresponding to a time frame (see Table I).

If we are interested in specific body parts from the whole 3D motion capture data, we can choose related markers X  information. For example, we can divide 3D motion capture matrix into 3 submatrices if we want to analyze 3 subparts of a human body, such as torso, arms, and legs as illustrated in Figure 2.

In this article, we extract a submatrix that includes positional information (x, y, and z positional data) belonging to the torso, right arm, left arm, right leg, and left leg in the matrix, considering it as information that can represent the movement of the human body sufficiently. Then we use singular value decomposition (SVD) to reduce the dimensionality of a large motion capture matrix to singular values [Jin and Prabhakaran 2007]. Therefore, we have a five-dimensional matrix, that is, five singular value features per frame for one human motion clip. In Section 5.1, we varied the number of dimensions and five is chosen as the optimal value. In Figure 3, as an example of SVD, the singular value corresponding to the head segment shows a considerable variation when the head falls on the floor.

The 3D motion capture data has more semantic information and is much cleaner, as compared with 2D data extracted from video sequences which can be easily affected to environment. Hence, in this article, we make use of 3D motion capture data as a knowledg-base for human motion models, and through several experiments, we show that it leads to a significant performance improvement. 2D feature extraction from video se quences is carried out by OpenCV 6 optical flow algorithm [Mccane et al. 2001]. The optical flow algorithm used the Shi and Tomasi [1994] and PLK [Bouguet 2002] algorithms to compute the feature vectors using flow field variation between two adjacent frames. Ramadass et al. [2010] proposed a mod-ified version of the optical flow algorithm where they used a technique called frame jump to eliminate noisy and irrelevant features extracted from the optical flow algo-rithm. They show that the features extracted are cleaner than the existing feature extraction algorithms, such as motion history image (MHI), and are more suitable for human action classification. They use the Euclidean distance of the coordinate posi-tions to eliminate noisy and irrelevant features because of the complex images and backgrounds. The extracted features are of six dimensions that include x and y fea-ture coordinate positions, speed, and direction of the moving body. x and y coordinates of the features qualified by threshold of the Euclidean distance are the most basic el-ements among the six dimensional features from which additional features, such as speed or direction, can be calculated. Th e x and y coordinates are mean coordinate values of qualified features. where N is the total number of qualified features. These mean x and y coordinates are considered as representative values of the moving human body in 2D video sequences. In this article, we use the same six dimensional feature vectors extracted from video sequences. The 2D video data would be used for both learning and testing stages. While it is true that 3D motion capture data provides more semantic information on human motion, it is clear that the data format of 3D motion capture is very different from 2D video. Hence, we need to answer the question: how do we know if the features extracted from 3D motion capture and 2D video are related? Also, we need to ensure that features from 3D motion capture and 2D video are synchronized appropriately so that the learning can be meaningful.

For answering the preceding question, we analyze the relation between the features extracted from 3D motion and 2D video, as follows. The video features extracted from the 2D video, as explained in Section 3.2, are representative of x and y coordinates corresponding to the moving human body in 2D video sequence (since 2D video pre-processing involves background subtraction methods also). In a similar manner, 3D motion data has the x, y, and z positional data of the human body. Therefore 3D data in x-y-z coordinates can be mapped to the x-y coordinates in which x and y data from 2D features are located. Although 2D and 3D features are heterogeneous, we can find the correlation between 2D x and y coordinates data and 3D positional data in space.
As we explained about singular value features in Section 3.1, we can obtain singular values by where M i is a motion data submatrix corresponding to each body component ( t :torso, la :leftarm, ra :rightarm, ll : left leg, rl : right leg), and  X  i are singular values which are coefficients of each frame as the spatial feature. By the SVD decomposition, we have a reduced M f a 5 from M f a m (f: frames, m : 114 columns).

Next, we carried out empirical studies comparing the SVD features extracted from 3D motion capture data and the 2D features from 2D video by the modified optical flow algorithm, as shown in Figure 4. Based on Equations (1) and (2), we plotted the average x and y coordinates of the modified optical flow features extracted from the video sequence for fall down and sit down motions and compared them with 5 SVD values from the corresponding 3D mo tion data. As we can see from this fig-ure, there is a strong correlation between the features depending on which parts of the human body were active in the motion. Here, the human motion in 2D video was synchronized with the human motion in 3D motion capture. In the top figure, for the fall down action, a steep fall is seen in some time interval in both cases of torso/arms (3D feature data) and Y positional feature (2D feature data). Appendix B shows similar comparisons for the other motions used in the experiments. It should be observed that this synchronization is needed only during the training stage and is not difficult to achieve. It can be either done by capturing 3D motion capture and 2D video at the same time or by some simp le preprocessing to align the human mo-tion. In this article, we obtained 3D motion capture data separately from 2D video clips and used preprocessing, such as downsampling for synchronizing two different sequences. In this section, we focus on the next question that arises: if the features are related, how do we learn from the 3D motion capture features so that we can classify 2D video clips? In the traditional hidden Markov models (HMMs), the Baum-Welch algorithm is well known as an efficient method for finding unknown parameters, such as state transition and output probabilities, given an output sequence as training data. The Baum-Welch algorithm, a special case of the expectation-maximization (EM) algo-rithm iteratively performs an expectation (E) step and a maximization (M) step to find maximum likelihood estimates of parameters. However, the Baum-Welch algorithm has some disadvantages, for example, depending on random initial values, it may con-verge to a local maximum, even though some approaches X  X uch as random restart or simulated annealing X  X re proposed to escape a local maximum. Moreover, it requires computation time for reaching the local maximum because it needs iterative EM steps.
On the other hand, the knowledge-based hybrid (KBH) approach is a straight-forward way for parameter learning in HMMs, adopting hybrid motion data as knowledge (see Figure 5). The KBH direct ly computes unknown parameters given a hidden sequence, as well as an output sequence for HMM learning. Since the KBH employs the hidden sequences created by hybrid motion data in which 3D motion data correspond to 2D feature data through down-sampling, the HMMs learn by using the KBH method with semantic-rich and clean 3D motion data made of human efforts and should outperform those from the Baum-Welch with only 2D feature data. Before calculating the model parameters of HMMs, we need preprocessing, such as synchronizing two heterogeneous data sequences, so that the synchronized data sequence would be used as a hidden states sequence for determining the model parameters of HMMs in the KBH method. We can obtain great benefit from 3D motion capture data by mixing two heterogeneous data sequences.

At the preprocessing stage, the 3D motion capture data must be down-sampled to the 2D video feature data X  X  frame rate because the 3D motion capture data X  X  frame rate (120 fps) does not equal the 2D video feature data X  X  frame rate (30 fps). This expression shows the mapping of the n th video frame to the m th 3D motion cap-ture frame by the down-sampling rate, m =4( n  X  1) + 1. For example, the 3D motion ... , vf n ).

Figure 6(a) shows mapping video sequences to 3D motion data. In this figure, we have Tr number of 2D observation sequences and one 3D 5 SVD feature sequence ( Tr :1 mapping) at training time. These two types of sequences all belong to the same motion class. Since the size of the frames of each video data may be different according to the length of time taken, that is, the length can be shorter or longer than the length for 3D motion capture data, we always choose the minimum length (min( T i , h )) as the length of the hybrid motion data.

Given any frame, we simply combine 2D features ( x 1 , x 2 , x 3 , x 4 , x 5 , x 6 )with5SVDfea-
In Figure 6(b), (( v f i , 3 d  X  f )  X  hf i ) expresses the procedure combining 2D and 3D data into hybrid data. ( hf i  X   X  i ) represents producing hidden state sequences from hybrid data by kernel mapping function such as k -means clustering. At time toratthe j th frame, a hidden state (  X  i j ) can generate a 1D symbol q .Ifweuse the 11-dimensional features in the hybrid matrix are converted to one-dimensional features by a k -means algorithm, and the one-dimensional sequence is used as the hidden state sequence in HMMs. In Figure 6(c), ( O i  X   X  i ) shows one-to-one mapping between the i th observation and the hidden states. Here, O i is a one-dimensional observation sequence after the original six-dimensional features are converted by k -means.

Now we are making both observation and hybrid sequence data one-dimensional data sequences through k -means clustering. Since we are using discrete HMMs, all feature vectors should be partitioned into clusters using vector quantization like k -means clustering for computing the state probability. This is why we are using the k -means algorithm in the KBH approach.

The rest of this section describes how to calculate the model parameters based on the observation and hidden sequences already explained previously.

The learning in the traditional HMMs is to determine the model parameters  X  = ( A , B ,  X  ) given a sequence of observations [Rabiner 1989], where  X  ,A, and B are the initial state probabilities, state transition probabilities, and observation symbol probabilities, respectively. Similarly in the KBH method, we can directly computer the model parameter  X  from the observation and hidden state sequences.
 where f is the frequency (number of times) of the event seen;  X  i 1 is the hidden state at time t =1in i th video clip; and q  X  is the  X  th symbol, and  X   X  X  1 , 2 ,  X  X  X  , n } .
For initial state probabilities (  X   X  ), we compute the frequency of the  X  th symbol in the first frame through all of the training set divided by Tr , that is, the number of training video motion clips for a motion class.
 where q  X  is the  X  th symbol, and  X   X  X  1 , 2 ,  X  X  X  , n } .

For the state transition probabilities from  X  to  X  ( a  X  X  ), we divide the total number of times of transitions from  X  to  X  in hidden states through the training set ( Tr )bythe total number of transitions from  X  to all hidden state symbols through the training set ( Tr )ofthesameclass.
 where O  X  is the observation symbol and  X   X  X  1 , 2 ,  X  X  X  , m } .

For the emission probability, we divide the total frequency of q  X  emitting an obser-vation symbol ( O  X  ) given observation and hidden state sequences through training set ( Tr ) by the total frequency from q  X  to all observation symbols through training set ( Tr ) of the same class.
 Now we have an HMM according to an action. In order to classify eight actions, eight HMMs should be created through the training data of each class. For testing, given a model  X  i =( A , B ,  X  ), we compute P ( O |  X  i ), where O is an unknown observation sequence and 1  X  i  X  8. Then we choose arg max 1  X  i  X  8 P ( O |  X  i ). In this framework, vector quantization technique (VQ) is needed in order to take one value per frame from multidimensional features both in the hidden state se-quence and in the observation sequence. For this process, we use k -means clustering with k centroid points. Since an unsuitable choice of k may cause poor results, we choose the number k through experiment. To find an optimal value for k ,wevaried the number of k between 10 and 50. Figure 7 shows the accuracies with different numbers of clusters. From the result, we observe that as the number of clusters increases, so does the accuracy, but after 30, accuracy starts declining. Therefore, we partition the 11-dimensional features in the hybrid matrix into one of the 30 (= k ) clusters. We captured 762 video clips (resolution of 320  X  240 pixels) from five different subjects across eight different motion classes. We defined eight basic actions, such as crawling, falling down, jumping, picking up, running , sitting down, standing up, and walking (see Figure 8). We took one 3D motion capture data corresponding to each motion class by using the motion capture system based on the VICON optical system. Samples of video clips and 3D motion capture clips are posted on YouTube. 7 For video clips, basically we divide video motion clips into training and testing sets with the same ratio of 50%. We never used the training dataset for testing. With the training dataset, we learned the parameters for the HMM of each motion class. Thus, given the learned parameters  X  , the framework computes the most probable HMM with a video sequence O i in the testing dataset.

In the preceding HMM parameters computation, the KBH method computes the parameter without reestimation with both the hidden state sequence and observation sequence. In contrast, the Baum-Welch method performs the reestimation process with only the observation sequence. We have some options for the initial estimates of the parameters as random or uniform initial estimates. In this paper, we used a random initial estimate for the Baum-Welch method when we test for comparison of the KBH method with the Baum-Welch method. In this case, the Baum-Welch method starts with a random initial estimate and runs iteratively until it reaches the convergence rate. Although the Baum-Welch method leads to a satisfactory likelihood after repeating reestimation calculation, it may be the local maximum, not the global maximum. Moreover, it may generate ve ry low accuracy with noisy observation sequences. Because the KBH method starts computing with considerably reliable 3D motion capture data as hidden state sequences, the method can reach a high likelihood value and provide high accuracy, even if it runs with no iterative learning phase.

We have conducted a variety of experiments to test various aspects of the proposed framework.  X  Efficacy of the dimensionality reduction techniques and initial probability estimates  X  Comparing accuracies of different approaches  X  Utility of 3D motion capture data in the proposed approach  X  Modifying the dataset used for training and testing  X  Comparing training time needed for the proposed KBH-HMM and Baum-Welch
HMM  X  Comparing accuracy of KBH-HMM a nd support vector machine (SVM) (The specifications of the computer used in the experiment were CPU: AMD Turion TM 64  X  2 Mobile Technology TL-56 1.81GHz; Memory: 2.00GB of RAM.) In the proposed method, we used singular value decomposition (SVD) to reduce the dimensionality of the 3D feature data which is mixed with the 2D feature data; then, we again reduce the dimensionality of the 2D and 3D mixture using the k -means algo-rithm. An alternative approach can be to eliminate the dimensionality reduction using SVD and compute the singular value per frame using only the k -means algorithm. A second alternative is to check whether we need five different SVD values or whether we can use fewer SVD components.
 For examining these alternatives, we compare four cases X  X o SVD, 1D SVD, 3D SVD, and 5D SVD. In the case of not using SVD, 114 attributes including positional and angular information in 3D motion capture data are used directly as 3D feature vectors without applying to SVD. 1D SVD consists of one dimensional value which is head. 3D SVD has three dimensional values related to torso, arms, and legs. In Table III, there are four subcolumns X  X o SVD, 1D SVD, 3D SVD, and 5D SVD X  under the KBH column. Comparing the accuracy in No SVD with others in SVDs, the experiment result shows that using SVD g ives more accuracy compared to avoiding SVD.

We then tested the effect of initial probability estimates in the Baum-Welch algo-rithm. For this, we used both random initialization, as well as the KBH value itself, as the initial estimate. Here, the accuracy of KBH is higher than the Baum-Welch al-gorithm with the random initialization parameter and the Baum-Welch with the KBH initialization parameter (KBH+BW). The detailed accuracy of the KBH recognition method is shown in Table IV. In Table IV, we observe that misclassification of an ac-tion results from similar actions. For example, the misclassification rate of walk to run is 11.4%, and run is misclassified to walk at a rate of 11.1%. In Table IV and Figure 9, we can observe that the proposed KBH method is superior to the Baum-Welch method in recognition a ccuracy rate. Comparing the Baum-Welch algorithm with the KBH method, the overall average of all actions X  accuracy for the Baum-Welch method is 80.1%, while that of the KBH algorithm is 88.5%. In some specific motions, such as crawling, the results show that the accuracy improves by nearly 24% in the KBH method (Baum-Welch: 72.7%; KBH: 96.4%). We made another baseline model to prove the usefulness of the 3D motion capture data. The model is created by directly co mputing the parameters of the HMM using the hidden state sequence generated from only the 2D feature data without using 3D data. It is not calculated iteratively for the expectation-maximization process like the Baum-Welch method. We show a comparison between the KBH and the baseline mode not using 3D data in Table V. The total average accuracy of the baseline model is about 78%. It is obvious that the KBH using 3D data still outperforms the baseline model not using 3D data. Therefore, the results show that the knowl-edge extracted from 3D motion capture data significantly enhances human motion recognition. Additionally, we also conducted experiments with the dataset, splitting the training and testing dataset by subject (see  X  X ubject-fold X  in Table VI). After we first chose one of five subjects, the entire dataset, including the subject, are used as the testing dataset, but the rest are used for the training data set. Hence, the video clips of the subjects in the training dataset do not exist in the testing dataset.
 We carried out an additional evaluation method: 10-fold cross validation for KBH. We split 10% of the datasets of each of the eight motion classes for testing and the remaining 90% of the datasets for training. We repeated the experiment ten times by testing 10% of the different datasets of each of the eight motion classes, that is, by taking 10% of the datasets of each of the eight motion classes from the training datasets that were not tested before and combining the previously tested datasets in the training datasets (see  X 10-fold X  in Table VI). In the case of some of the actions, such as crawling and running, the Baum-Welch method gets stuck on a local maximum and does not tend to converge. This is also the reason why the Baum-Welch method takes hours to complete the learning, whereas KBH does the same in less than a second. Hence, we conducted several experi-ments on training times and the number of iterations needed for the Baum-Welch method.
 From the comparison between the KBH method and the Baum-Welch method in Figure 10, we can observe that Baum-Welch maximizes the likelihood by making estimations iteratively, but the KBH can generate better likelihood even without iterative calculations (as Baum-Welch does). For the falling down action shown in Figure 10, the probability of the likelihood by the KBH method is 1.21E-21. On the other hand, in the case of the Baum-Welch method, the probability converges to 6.00E X 26 after 74 iterations. Hence, the KBH method gives better likelihood than the Baum-Welch method, and thereby increasing the accuracy of classification of human actions. In Figure 11, the graph shows that the number of iterations can vary, since the Baum-Welch could be sensitive to initialization.

We carried out additional experiments in which we want to show that the parameter created from the KBH method can have influence on the results of the learning part in the Baum-Welch, because the parameter with initialization must be better than the random parameter. We took the HMM parameters for each action computed using the KBH method, and then we fed them as an initial parameter into the Baum-Welch al-gorithm. Such a mixed method results in the curves in Figure 12. Three curves result from the Baum-Welch, starting with the low likelihood value and, hence, needing more iteration for the maximum likelihood value. Thus, one of the curves even had 46 times the iteration. But in the case of the mixed method, because it starts with a high like-lihood value, it needs less time (such as eight times) and gets a good likelihood value. The results show why the KBH method can increase the accuracy of classification of human actions.

For all the experiments, we have used the same dataset for both training and test-ing. The only difference between them is the 3D feature vector inserted into the train-ing dataset of KBH during the training procedure to compute the HMM parameter in a non-iterative manner. In case of the Baum-Welch algorithm, it took more than four hours to learn the eight HMMs, and it needed 74 iterations for the fall down model among eight HMMs. On the other hand, the KBH method took less than just 100 milliseconds for learning the probabilities because the KBH method computes without reestimates.
 In Table VII, we compare the learning time between the KBH and the Baum-Welch. We show the time and the number of iterations taken during the learning procedure. Zero in a time column means that the time is close to zero. We compared the KBH approach to another existing classifier, a linear SVM, which is a classifier that generally achieves good performance. For the training of SVM, we modified 2D feature vectors which we used for the KBH method because SVM requires a different type of feature from HMMs. For a video clip, we calculate sta-tistical feature vectors, such as mean, standard deviation, variance, and histograms, based on 2D feature vectors extracted from the modified optical flow. The statistical features are modified, but they basically originated from the same 2D feature vec-tors as used in the earlier experiments. The feature vector consists of mean, stan-dard deviation, variance of each position, and speed feature values, and histograms of direction feature over the whole frames for a sample video clip. For this experi-ment, we derived 17 dimensional feature vectors, for example, x pos mean, x pos std, x pos var, y pos mean, y pos std, y pos var, direction0, direction1, direction2, direc-tion3, direction4, x speed mean, x speed std, x speed var y, speed mean, y speed std, and y speed var.

SVM took 6 minutes 41 seconds with the training dataset as the training time. As shown in Table VIII, the total average accuracy of the KBH method is higher than that of SVM, with 75.9%. Although results of some actions like crawl and sit down in SVM are better than in KBH, most accuracies are m uch worse. For example, accuracies for run and stand up are very low (44.4% and 57.1%, respectively). In this section, we discuss another state-of-the-art classifier, 3D SIFT with bag-of-words. Many spatial temporal feature-based methods have been applied to the problem of action detection [Niebles et al. 2006]. These methods rely on finding points of in-terests [Niebles et al. 2006] or just random selections of such points [Scovanner et al. 2007] and then applying various approaches to generate a feature descriptor for that location. Then these new found feature descriptors can be used to form visual words [Csurka et al. 2004; Laptev and Lindeberg 2004; Laptev et al. 2007; Lowe 2004; Niebles et al. 2006; Scovanner et al. 2007] and apply techniques from NLP or can be combined with other timeline-based approaches like HOF (histogram of optical flow), DTW, etc.
The visual words-based approach takes a long time to produce the initial dictionary of words that is required to perform proper classification. The strength in the method lies around the descriptor and its ability to describe a region of the video very robustly. Due to this, it is a good idea to use a large set of diverse words to generate the dic-tionary. Scovanner et al. [2007] and Niebles et al. [2006] have used clustering-based methods to form such dictionaries, and depending on the vastness and the scale of the number of features used as input to the process, it can take a lot of time to get the dictionary generated.

In this article, the KBH-HMM is designed to train, as well as classify, with a very small learning time ( &lt; 100 milliseconds), so that we can consider new sets of motion classes to be recognized. Though it is a good idea to compare KBH-HMM with approaches using a combination of bag-of-visual-words and SIFT, our prelimi-nary results of application of these methods on a subset of the dataset took several hours for learning (using latent semantic analysis). Hence, we did not carry out a complete performance comparison study of KBH-HMM and visual bag-of-words-based approaches. We can use our proposed approach in different areas such as surveillance. For instance, we can use it for remote monitoring of day-to-day activities of senior citizens or persons afflicted with disorders, such as Parkinson X  X  or Alzheimer X  X . One main disadvantage of using video-based monitoring in such personal environments is privacy. However, in our approach we have the reference template of 3D motion capture data that was used during the training stage. We can use the 3D motion capture data to drive a 3D avatar. For instance, if we detect a sit down action, the 3D avatar can sit down, or if a fall down action is detected, the 3D avatar can fall down, as shown in Figure 13. This will help to track the motion of person with disorders while ensuring privacy, since the monitoring can be based only on 3D avatars, that is, the 3D motion captured information can hide asubject X  X privacy. 8
In a similar manner, we can use this approach for a video gaming environment in which more challenging issues may happen in contrast with day-to-day activities (Sec-tion 6.2), as demonstrated in Figure 14. 9 It should be observed here that in Microsoft X  X  Kinect, the camera in the Xbox i s a 3D camera that provides a depth pixel apart from the regular video pixels. Our approach here uses a regular Web cam for the video data capture. All the experiments reported earlier were based on human actions associated with day-to-day living activities. In another set of experiments, we tested our KBH-HMM approach on another set of actions that typically occur in a video gaming environment such as tennis or boxing (backhand, forehand, left kick, right kick, left punch, right punch, and smash). Tables IX and X show the results of classifying these actions. Here, we extracted 2D feature vectors using the modified optical flow algorithm and classify the actions using the KBH-HMM method. Our observation is that the gaming type of actions are more challengeable compared t o those for day-to-day actions. One reason is that the actions considered for video gaming types of environments are far more subtle than the ones considered earlier. For instance, in actions such as left punch, right punch, left kick, and right kick, the arm and leg movements were towards the video camera.

Hence, the extracted 2D features were not very helpful in discriminating the ac-tions. One can realize the need for more information in these actions, such as the depth pixel obtained from 3D video cameras in the Xbox gaming system. For instance, the 2D features for left punch and the corresponding 3D motion capture data were not very similar, as shown in Figure 15. Figure 16 shows that the 2D features extracted from two different actions, such as left punch (top) and right punch (bottom), may not be distinguishable. The similarity of the features results in 54.3% of the left punch action in testing data is misclassified into right punch, as shown in Table X. Hence, we feel we may need a different feature extraction methodology for 2D video correspond-ing to these actions. We plan to investigate this as our future work. In this article, we contribute by showing how a machine learning algorithm can learn from a media type (3D motion capture) to better classify another related media type (2D video). We proposed an efficient frame work using the KBH method in which the semantic 3D motion capture data enhances video-based human motion recognition by combining 3D motion capture data with video feature data in order to create the hid-den states in the HMMs. Through the experiments, we compared learning by the traditional Baum-Welch algorithm with learning by the proposed KBM method. Since the KBH method computes the model parameter directly with the hidden state se-quence built from the knowledge base and the observation sequence, it can generate better recognition accuracy than the Baum-Welch method that may get stuck in a lo-cal maximum with noisy video feature data, even though it computes iteratively for reestimations. Moreover, since the KBH can avoid iterative calculations, it has a very short learning time compared with the traditional Baum-Welch algorithm. Addition-ally, through total accuracy, we show that the proposed KBH method outperforms the linear SVM.

The proposed framework has several applic ations that require human motion recog-nition, such as gaming, assisted/senior living environments, and surveillance. Though we have explained some limitations of our approach in identifying subtle actions in video gaming environments, it can still be useful in monitoring actions of daily living exhibited by persons with certain disorders or age without intruding on their privacy. It can also be useful in surveillance scenarios where nonsuspicious actions can be elim-inated using the proposed KBH-HMM method.

