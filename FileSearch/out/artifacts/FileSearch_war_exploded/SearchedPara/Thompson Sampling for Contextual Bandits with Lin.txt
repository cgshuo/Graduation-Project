 A.1. Posterior distribution computation  X  Pr( r i ( t ) |  X   X  ) Pr(  X   X  )  X  exp { X   X  exp { X   X  exp { X   X  exp { X   X  N ( X   X  ( t + 1) ,v 2 B ( t + 1)  X  1 ) .
 Therefore, the posterior distribution of  X  at time t + 1 is N ( X   X  ( t + 1) ,v 2 B ( t + 1)  X  1 ).
 A.2. Some concentration inequalities Formula 7.1.13 from Abramowitz &amp; Stegun (1964) can be used to derive the following concentration and anti-concentration inequalities for Gaussian dis-tributed random variables.
 Lemma 5. (Abramowitz &amp; Stegun, 1964) For a Gaus-sian distributed random variable Z with mean m and variance  X  2 , for any z  X  1 , Definition 9 (Super-martingale) . A sequence of ran-dom variables ( Y t ; t  X  0) is called a super-martingale corresponding to filtration F t , if for all t , Y t measurable, and for t  X  1 , Lemma 6 (Azuma-Hoeffding inequality) . If a super-martingale ( Y t ; t  X  0) , corresponding to filtration F satisfies | Y t  X  Y t  X  1 |  X  c t for some constant c t t = 1 ,...,T , then for any a  X  0 , The following lemma is implied by Theorem 1 in Abbasi-Yadkori et al. (2011): Lemma 7. (Abbasi-Yadkori et al., 2011) Let ( F 0 t ; t  X  0) be a filtration, ( m t ; t  X  1) be an R d -valued stochas-tic process such that m t is ( F 0 t  X  1 ) -measurable, (  X  1) be a real-valued martingale difference process such that  X  t is ( F 0 t ) -measurable. For t  X  0 , define  X  P the d -dimensional identity matrix. Assume  X  t is con-ditionally R -sub-Gaussian.
 1  X   X  0 , A.3. Proof of Lemma 1 Bounding the probability of event E  X  ( t ) : We use Lemma 7 with m t = b a ( t ) ( t ),  X  t = r a ( t ) b that effectively, F 0 t has all the information, including the arms played, until time t + 1, except for the reward of the arm played at time t + 1). By the definition of F Also,  X  t is conditionally R -sub-Gaussian due to the as-sumption mentioned in the problem settings (refer to Section 2.1), and is a martingale difference process: Also, this makes Note that B ( t ) = M t  X  1 , and  X   X  ( t )  X   X  = M  X  1 t  X  1 Let for any vector y  X  R and matrix A  X  R d  X  d , || y || denote p y T Ay . Then, for all i , | b i ( t ) T  X   X  ( t )  X  b i ( t ) T  X  | = | b i ( t ) T The inequality holds because M  X  1 t  X  1 is a positive defi-nite matrix. Using Lemma 7, for any  X  0 &gt; 0, t  X  1, with probability at least 1  X   X  0 , R q d ln T with probability 1  X   X  T 2 , for all i , This proves the bound on the probability of E  X  ( t ). Bounding the probability of event E  X  ( t ) : For all i , = | b i ( t ) T  X   X  ( t )  X  b i ( t ) T  X   X  ( t ) |  X  v q b i ( t ) T B ( t )  X  1 b i ( t )  X || Therefore, we can prove the statement of the lemma by proving that || 1 v B ( t ) 1 / 2 (  X   X  ( t )  X   X   X  ( t )) || mal variable, therefore using concentration of Gaus-sian random variables (Lemma 5),  X  d  X  A.4. Proof of Lemma 2 Given event E  X  ( t ), | b a  X  ( t ) ( t ) T  X   X  ( t )  X  b  X  Lemma 5, Where | Z t | = So A.5. Missing details from Section 3.2 use the following result, implied by the referred lemma in Auer (2002).
 Lemma 8. (Auer, 2002, Lemma 11). Let A 0 = A + xx T , where x  X  R d ,A,A 0  X  R d  X  d , and all the eigenvalues  X  j ,j = 1 ,...,d of A are greater than or A 0 can be arranged so that  X  j  X   X  0 Let  X  j,t denote the eigenvalues of B ( t ). Note that B ( t + 1) = B ( t ) + b a ( t ) ( t ) b a ( t ) ( t ) T Therefore, above implies This allows us to derive the given inequality after some algebraic computations following along the lines of Lemma 3 of Chu et al. (2011).
 To obtain bounds for the other definition of regret in Remark 1, we observe that because E [ r i ( t ) |F t  X  1 b F t  X  1 for this definition of regret( t ) is same as before. More precisely, for F t  X  1 such that E  X  ( t ) holds, inition 8 is a super-martingale with respect to this new definition of regret( t ) as well. Now, if | r b | Y Hoeffding inequality exactly as in the proof of Theorem 1 to obtain regret bounds of the same order as Theo-rem 1 for the new definition. The results extend to the more general R -sub-Gaussian condition on r i ( t ), using a simple extension of Azuma-Hoeffding inequality; we omit the proof of that extension.
 Below is a description of the algorithm for single pa-rameter setting which, instead of generating a single Algorithm 1, generates  X  i ( t ) ,i = 1 ,...,N as N inde-pendent samples with the same marginal distributions as b i ( t ) T  X   X  ( t ).
 Algorithm 2 Modified Thompson Sampling
Set B = I d ,  X   X  = 0 d , f = 0 d . for all t = 1 , 2 ,..., do end for In the regret analysis of this algorithm, we will be able to utilize the independence of the  X  i ( t ) X  X  to bound the probability of playing saturated arms in terms of the probability of playing optimal arm (see Lemma 11). In comparison, in the proof of Theorem 1, we bounded the probability of playing saturated arms in terms of the probability of playing unsaturated arms which includes the optimal arm. This difference in the analysis is the key to our improved regret bound for this algorithm.
 In the proof below, except when explicitly redefined, notations are as before (refer to notations table in Ap-pendix A).
 Definition 10. Define ` ( T ) and v as before, but redfine g ( T ) = p 4 ln( NT ) v + ` ( T ) .
 Definition 11. An arm i is called saturated at time Observe that by definition, a  X  ( t ) is an unsaturated arm at time t .
 Definition 12. Define event E  X  ( t ) as before, but re-define E  X  ( t ) as the event that Lemma 9. For all t , 0 &lt;  X  &lt; 1 , Pr( E  X  ( t ))  X  1  X  And, for all possible filtrations F t  X  1 , Pr( E  X  ( t ) |F Proof. The probability bound for E  X  ( t ) can be proven using a concentration inequality given by Abbasi-Yadkori et al. (2011), as before in the proof of Theorem 1. The probability bound for E  X  ( t ) can be proven us-ing the concentration inequality for Gaussian random variables from Abramowitz &amp; Stegun (1964) stated as Lemma 5 in Appendix A.2 .
 The next lemma lower bounds the probability that the Lemma 10. For any filtration F t  X  1 such that E  X  ( t ) is true, b inequality in Lemma 5, Where So The following lemma bounds the probability of playing a saturated arm in terms of the probability of playing the optimal arm.
 Lemma 11. Given any filtration F t  X  1 such that E  X  ( t ) is true, where p = 1 Proof.
 = Pr  X  a  X  ( t ) ( t )  X   X  j ( t ) ,  X  j 6 = a  X  ( t ) F  X  Pr  X  i  X  C ( t ) , X  a  X  ( t ) ( t )  X   X  i ( t ) ,  X  p  X  Pr ( a ( t )  X  C ( t ) F t  X  1 )  X  Pr E  X  ( t ) F t  X  1  X  p  X  Pr ( a ( t )  X  C ( t ) F t  X  1 )  X  The second equality follows from the independence holds because given the current distributions, which are fixed on fixing the filtration F t  X  1 , the algorithms samples  X  1 ( t ) ,..., X  N ( t ) are independently from their respective distributions. In the second last inequality, for the first term we used the lower bound provided by Lemma 10. For the second term, we used the ob-definition of these events and the definition of satu-rated arms, it holds that Therefore, given an F t  X  1 such that E  X  ( t ) is true,  X  for some saturated arm i can be larger than b a  X  ( t ) ( t ) only if E  X  ( t ) is false.
 Next, we establish a super-martingale process that will form the basis of our proof of the high-probability re-gret bound.
 Definition 13. Let X where p = 1 Lemma 12. ( Y t ; t = 0 ,...,T ) is a super-martingale process with respect to filtration F t .
 Proof. We need to prove that for all t  X  [1 ,T ], and any F If F t  X  1 is such that E  X  ( t ) is not true, then regret if an arm i is played at time t , then it must be true true, then, b i ( t ) T  X   X   X  i ( t )  X  g ( T ) s t,i is false. Also, by the definition of unsaturated arms, for every unsaturated arm i ,  X  i ( t )  X  g ( T ) s t,i these observations,
E [regret 0 ( t ) F t  X  1 ] = E  X  a ( t ) ( t ) I ( a ( t )  X  C ( t )) F t  X  1 The last inequality uses Lemma 11. In the first and the last inequality, we also used that for all i ,  X  i ( t )  X  1, Now, we are ready to prove Theorem 3.
 B.1. Proof of Theorem 3 We observe that the absolute value of the first three terms in the definition of X t bounded by 1 /p , and the last term is bounded by 2 /p , therefore the super-martingale Y t has bounded difference | Y t  X  Y t  X  1 | X  5 for all t  X  1, and thus apply Azuma-Hoeffding inequal-ity, to obtain that with probability 1  X   X  2 , = O ( can be derived along the lines of Lemma 3 of Chu et al. (2011) using Lemma 11 of Auer (2002). Also, because E  X  ( t ) holds for all t with probability at least Theorem 2 considers the setting where each arm i is associated with a parameter  X  i  X  R d , where possibly  X  i 6 =  X  i 0 for two different arms i and i 0 . In this case, Thompson Sampling would maintain a separate esti-would be updated only at the time instances when i is played. We appropriately modify some of the previous definitions: The posterior distribution for each arm i at time t would be N ( b i ( t ) T  X   X  i ( t ) ,v 2 b i ( t ) T B i the TS algorithm is now stated as follows.
 Algorithm 3 Thompson Sampling for Contextual bandits with N parameters
Set B i = I d ,  X   X  i = 0 d , i = 1 ,...,N , f i = 0 d . for t = 1 , 2 ,..., do end for The optimal arm a  X  ( t ) is now the arm that maximizes b ( t ) T  X  i , and the regret at time t is defined as The regret analysis closely follows the proof of Theo-rem 3, described in the previous section. Below, we describe only the changes required.
 The events E  X  ( t ) will now be defined with respect means. That is, Similarly, E  X  ( t ) will be the event that It is easy to observe that the statements of Lemma 9 and the super-martingale property established by Lemma 12 will hold as it is for these new defini-tions. The only difference will appear in the bound for P t s t,a ( t ) used in the proof of Theorem 3. For the case of N different parameters, we will get a bound of O (  X  Let n i ( T ) be the number of times arm i is played by time T . Then using Lemma 8, for any two consecutive time steps t,t 0 at which arm i is played, This allows us to derive the following lemma along the lines of Lemma 3 of Chu et al. (2011).
 Lemma 13. (Chu et al., 2011, Lemma 3) For i = 1 ,...,N , Using the above lemma, Therefore, following the same lines as proof of Theo-rem 3, we will get a regret bound of  X  O ( d q NT 1+ ). We provided a theoretical analysis of Thompson Sam-pling for the stochastic contextual bandits problem with linear payoffs. Our results resolve some open questions regarding the theoretical guarantees for Thompson Sampling, and establish that even for the contextual version of the stochastic MAB problem, TS achieves regret bounds close to the state-of-the-art methods. We used a novel martingale-based analy-sis technique which is arguably simpler than the tech-niques in the past work on TS (Agrawal &amp; Goyal, 2012; Kaufmann et al., 2012), and is amenable to extensions. In the algorithm in this paper, Gaussian priors were used, so that  X   X  ( t ) was generated from a Gaussian dis-tribution. However, the analysis techniques in this paper are extendable to an algorithm that uses a prior distribution other than the Gaussian distribu-tion. The only distribution specific properties we have used in the analysis are the concentration and anti-concentration inequalities for Gaussian distributed random variables (Lemma 5), which were used to prove Lemma 1 and Lemma 2 respectively. If any other dis-tribution provides similar tail inequalities, to allow us proving these lemmas, these can be used as a black box in the analysis, and the regret bounds can be re-produced for that distribution.
 Several questions remain open. A tighter analysis that can remove the dependence on is desirable. We be-lieve that our techniques would adapt to provide such bounds for the expected regret . Other avenues to ex-plore are contextual bandits with generalized linear models considered in Filippi et al. (2010), the setting with delayed and batched feedback, and the agnostic case of contextual bandits with linear payoffs. The ag-nostic case refers to the setting which does not make the realizability assumption that there exists a vector  X  i for each i for which E [ r i ( t ) | b i ( t )] = b i ( t ) knowledge, no existing algorithm has been shown to
 Shipra Agrawal shipra@microsoft.com Microsoft Research India Navin Goyal navingo@microsoft.com Microsoft Research India Multi-armed bandit (MAB) problems model the ex-ploration/exploitation trade-off inherent in many se-quential decision problems. There are many versions of multi-armed bandit problems; a particularly useful version is the contextual multi-armed bandit problem. In this problem, in each of T rounds, a learner is pre-sented with the choice of taking one out of N actions, referred to as N arms. Before making the choice of which arm to play, the learner sees d -dimensional fea-ture vectors b i , referred to as  X  X ontext X , associated with each arm i . The learner uses these feature vec-tors along with the feature vectors and rewards of the arms played by her in the past to make the choice of the arm to play in the current round. Over time, the learner X  X  aim is to gather enough information about how the feature vectors and rewards relate to each other, so that she can predict, with some certainty, which arm is likely to give the best reward by look-ing at the feature vectors. The learner competes with a class of predictors, in which each predictor takes in the feature vectors and predicts which arm will give the best reward. If the learner can guarantee to do nearly as well as the predictions of the best predictor in hindsight (i.e., have low regret), then the learner is said to successfully compete with that class.
 In the contextual bandits setting with linear payoff functions , the learner competes with the class of all  X  X inear X  predictors on the feature vectors. That is, a predictor is defined by a d -dimensional parameter  X   X  R d , and the predictor ranks the arms according to b  X  . We consider stochastic contextual bandit prob-lem under linear realizability assumption, that is, we assume that there is an unknown underlying parame-ter  X   X  R d such that the expected reward for each arm i , given context b i , is b T i  X  . Under this realizability as-sumption, the linear predictor corresponding to  X  is in fact the best predictor and the learner X  X  aim is to learn this underlying parameter. This realizability assump-tion is standard in the existing literature on contextual multi-armed bandits, e.g. (Auer, 2002; Filippi et al., 2010; Chu et al., 2011; Abbasi-Yadkori et al., 2011). Thompson Sampling (TS) is one of the earliest heuris-tics for multi-armed bandit problems. The first version of this Bayesian heuristic is around 80 years old, dating to Thompson (1933). Since then, it has been rediscov-ered numerous times independently in the context of reinforcement learning, e.g., in Wyatt (1997); Ortega &amp; Braun (2010); Strens (2000). It is a member of the family of randomized probability matching algorithms. The basic idea is to assume a simple prior distribution on the underlying parameters of the reward distribu-tion of every arm, and at every time step, play an arm according to its posterior probability of being the best arm. The general structure of TS for the contextual bandits problem involves the following elements: 1. a set  X  of parameters  X   X  ; 2. a prior distribution P (  X   X  ) on these parameters; 3. past observations D consisting of (context b , re-4. a likelihood function P ( r | b,  X   X  ), which gives the In each round, TS plays an arm according to its pos-terior probability of having the best parameter. A simple way to achieve this is to produce a sample of parameter for each arm, using the posterior distribu-tions, and play the arm that produces the best sam-ple. In this paper, we design and analyze a natural generalization of Thompson Sampling (TS) for con-textual bandits; this generalization fits the above gen-eral structure, and uses Gaussian prior and Gaussian likelihood function. We emphasize that although TS is a Bayesian approach, the description of the algo-rithm and our analysis apply to the prior-free stochas-tic MAB model, and our regret bounds will hold ir-respective of whether or not the actual reward distri-bution matches the Gaussian likelihood function used to derive this Bayesian heuristic. Thus, our bounds for TS algorithm are directly comparable to the UCB family of algorithms which form a frequentist approach to the same problem. One could interpret the priors used by TS as a way of capturing the current knowl-edge about the arms.
 Recently, TS has attracted considerable attention. Several studies (e.g., Granmo (2010); Scott (2010); Leslie (2011); Kaufmann et al. (2012)) have empiri-cally demonstrated the efficacy of TS: Scott (2010) provides a detailed discussion of probability match-ing techniques in many general settings along with fa-vorable empirical comparisons with other techniques. Chapelle &amp; Li (2011) demonstrate that for the basic stochastic MAB problem, empirically TS achieves re-gret comparable to the lower bound of Lai &amp; Robbins (1985); and in applications like display advertising and news article recommendation modeled by the contex-tual bandits problem, it is competitive to or better than the other methods such as UCB. In their exper-iments, TS is also more robust to delayed or batched feedback than the other methods. TS has been used in an industrial-scale application for CTR prediction of search ads on search engines (Graepel et al., 2010). Kaufmann et al. (2012) do a thorough comparison of TS with the best known versions of UCB and show that TS has the lowest regret in the long run.
 However, the theoretical understanding of TS is lim-ited. Granmo (2010) and May et al. (2011) pro-vided weak guarantees, namely, a bound of o ( T ) on the expected regret in time T . For the the ba-sic (i.e. without contexts) version of the stochastic MAB problem, some significant progress was made by Agrawal &amp; Goyal (2012), Kaufmann et al. (2012) and, more recently, by Agrawal &amp; Goyal (2013), who pro-vided optimal regret bounds on the expected regret. But, many questions regarding theoretical analysis of TS remained open, including high probability regret bounds, and regret bounds for the more general con-textual bandits setting. In particular, the contextual MAB problem does not seem easily amenable to the techniques used so far for analyzing TS for the basic MAB problem. In Section 3.1, we describe some of these challenges. Some of these questions and difficul-ties were also formally raised as a COLT 2012 open problem (Chapelle &amp; Li, 2012).
 In this paper, we use novel martingale-based analy-sis techniques to demonstrate that TS (i.e., our Gaus-sian prior based generalization of TS for contextual bandits) achieves high probability, near-optimal re-gret bounds for stochastic contextual bandits with lin-ear payoff functions. To our knowledge, ours are the first non-trivial regret bounds for TS for the contex-tual bandits problem. Additionally, our results are the first high probability regret bounds for TS, even in the case of basic MAB problem. This essentially solves the COLT 2012 open problem by(Chapelle &amp; Li, 2012) for contextual bandits with linear payoffs.
 Our version of Thompson Sampling algorithm for the contextual MAB problem, described formally in Sec-tion 2.2, uses Gaussian prior and Gaussian likelihood functions. Our techniques can be extended to the use of other prior distributions, satisfying certain condi-tions, as discussed in Section 4. 2.1. Problem setting There are N arms. At time t = 1 , 2 ,... , a context vector b i ( t )  X  R d , is revealed for every arm i . These context vectors are chosen by an adversary in an adap-tive manner after observing the arms played and their rewards up to time t  X  1, i.e. history H t  X  1 , where a (  X  ) denotes the arm played at time  X  . Given b ( t ), the reward for arm i at time t is generated from an (unknown) distribution with mean b i ( t ) T  X  , where  X   X  R d is a fixed but unknown parameter.
 E r i ( t ) { b i ( t ) } N i =1 , H t  X  1 = E [ r i ( t ) b An algorithm for the contextual bandit problem needs to choose, at every time t , an arm a ( t ) to play, using history H t  X  1 and current contexts b i ( t ) ,i = 1 ,...,N . Let a  X  ( t ) denote the optimal arm at time t , i.e. a  X  tween the mean rewards of the optimal arm and of arm i at time t , i.e., Then, the regret at time t is defined as The objective is to minimize the total regret R ( T ) = P t =1 regret( t ) in time T . The time horizon T is finite but possibly unknown.
 We assume that  X  i,t = r i ( t )  X  b i ( t ) T  X  is conditionally R -sub-Gaussian for a constant R  X  0, i.e., This assumption is satisfied whenever r i ( t )  X  [ b ( t ) T  X   X  R,b i ( t ) T  X  + R ] (see Remark 1 in Appendix A.1 of Filippi et al. (2010)). We will also assume (the norms, unless otherwise indicated, are ` 2 -norms). These assumptions are required to make the regret bounds scale-free, and are standard in the literature on this problem. If ||  X  ||  X  c, || b i ( t ) ||  X  c,  X  instead, then our regret bounds would increase by a factor of c .
 Remark 1. An alternative definition of regret that appears in the literature is We can obtain the same regret bounds for this alter-native definition of regret. The details are provided in the supplementary material in Appendix A.5. 2.2. Thompson Sampling algorithm We use Gaussian likelihood function and Gaussian prior to design our version of Thompson Sampling al-gorithm. More precisely, suppose that the likelihood of reward r i ( t ) at time t , given context b i ( t ) and pa-rameter  X  , were given by the pdf of Gaussian distri-bution N ( b i ( t ) T  X ,v 2 ). Here, v = R q 24 d ln( 1  X  (0 , 1) which parametrizes our algorithm. Let Then, if the prior for  X  at time t is given by rior distribution at time t + 1, as N ( X   X  ( t + 1) ,v 2 B ( t + 1)  X  1 ) (details of this computa-tion are in Appendix A.1). In our Thompson Sampling algorithm, at every time step t , we will simply generate a sample  X   X  ( t ) from the distribution N ( X   X  ( t ) ,v and play the arm i that maximizes b i ( t ) T  X   X  ( t ). We emphasize that the Gaussian priors and the Gaus-sian likelihood model for rewards are only used above to design the Thompson Sampling algorithm for con-textual bandits. Our analysis of the algorithm allows these models to be completely unrelated to the actual reward distribution. The assumptions on the actual reward distribution are only those mentioned in Sec-tion 2.1, i.e., the R -sub-Gaussian assumption. Algorithm 1 Thompson Sampling for Contextual bandits
Set B = I d ,  X   X  = 0 d , f = 0 d . for all t = 1 , 2 ,..., do end for Every step t of Algorithm 1 consists of gener-ating a d -dimensional sample  X   X  ( t ) from a multi-variate Gaussian distribution, and solving the problem arms N is large (or infinite), the above algorithm is efficiently solvable. This is the case, for example, when the set of arms at time t is given by a d -dimensional convex set (every vector in the convex set is a context vector, and thus corresponds to an arm). 2.3. Our Results Theorem 1. For the stochastic contextual ban-dit problem with linear payoff functions, with prob-ability 1  X   X  , the total regret in time T for Thompson Sampling (Algorithm 1) is bounded by  X  &lt; 1 . Here, is a parameter used by the Thompson Sampling algorithm.
 Remark 2. The parameter can be chosen to be any constant in (0 , 1) . If T is known, one could choose Remark 3. Our regret bound in Theorem 1 does not depend on N , and is applicable to the case of infi-nite arms, with only notational changes required in the analysis.
 In the main body of this paper, we will discuss the proof of the above result. Below, we state two ad-ditional results; their proofs require small changes to the proof of Theorem 1 and are provided in the sup-plementary material.
 The first result is for the setting where each of the N arms is associated with a different d -dimensional pa-rameter  X  i  X  R d , so that the mean reward for arm i alization of the basic MAB problem to d -dimensions. Thompson Sampling for this setting will maintain a separate posterior distribution for each arm i which would be updated only at the time instances when i is played. And, at every time step t , instead of a single sample  X   X  ( t ), N independent samples will have to be generated:  X   X  i ( t ) for each arm i . We prove the follow-ing regret bound for this setting.
 Theorem 2. For the setting with N different pa-rameters, with probability 1  X   X  , the total regret in time T for Thompson Sampling is bounded by O d q NT 1+ ln N ln T ln 1  X  , for any 0 &lt; &lt; 1 , 0 &lt;  X  &lt; 1 .
 The details of the algorithm for N -parameter setting and the proof of Theorem 2 appear in the supplemen-tary material in Appendix C.
 Note that unlike Theorem 1, the regret bound in The-orem 2 has a dependence on N , which is expected because Theorem 2 deals with a setting where there are N different parameters to learn. However, the bound in Theorem 2 has a better dependence on d . This improvement results from the independence of  X  On the other hand in Algorithm 1, used for the single parameter setting of Theorem 1, a single  X   X  ( t ) is gen-This motivates us to consider a modification of Al-gorithm 1 for the single parameter setting, in which the  X  i ( t ) X  X  are independently generated, each with highest value of  X  i ( t ) is played at time t . Although, this modified algorithm could be inefficient compared to Algorithm 1 if N is large (say exponential) compared to d , the better dependence on d in regret bounds could be useful if d is large.
 Theorem 3. For the modified algorithm in single pa-rameter setting, with probability 1  X   X  , the total regret in time T is bounded by O d q T 1+ ln N ln T ln 1  X  , for any 0 &lt; &lt; 1 , 0 &lt;  X  &lt; 1 .
 The details of the modified algorithm and the proof of the above theorem appears in the supplementary material in Appendix B. 2.4. Related Work The contextual bandit problem with linear payoffs is a widely studied problem in statistics and machine learning often under different names as mentioned by Chu et al. (2011): bandit problems with co-variates (Woodroofe, 1979; Sarkar, 1991), associative reinforce-ment learning (Kaelbling, 1994), associative bandit problems (Auer, 2002; Strehl et al., 2006), bandit prob-lems with expert advice (Auer et al., 2002), and linear bandits (Dani et al., 2008; Abbasi-Yadkori et al., 2011; Bubeck et al., 2012). The name contextual bandits was coined in Langford &amp; Zhang (2007).
 A lower bound of  X ( d by Dani et al. (2008), when the number of arms is al-lowed to be infinite. In particular, they prove their lower bound using an example where the set of arms correspond to all vectors in the intersection of a d -dimensional sphere and a cube. They also provide an upper bound of  X  O ( d slightly restrictive in the sense that the context vector for every arm is fixed in advanced and is not allowed to change with time. Abbasi-Yadkori et al. (2011) ana-lyze a UCB-style algorithm and provide a regret upper bound of O ( d log ( T ) the dependence on , our bounds are essentially away by a factor of d from these bounds.
 For finite N , Chu et al. (2011) show a lower bound of  X ( et al. (2011) analyze SupLinUCB, a complicated al-gorithm using UCB as a subroutine, for this prob-lem. Chu et al. (2011) achieve a regret bound of O ( q Td ln 3 ( NT ln( T ) / X  )) with probability at least 1  X   X  (Auer (2002) proves similar results). This regret bound is not applicable to the case of infinite arms, and assumes that context vectors are generated by an oblivious adversary. Also, this regret bound would give O ( d 2 of-the-art bounds for linear bandits problem in case of finite N are given by Bubeck et al. (2012). They provide an algorithm based on exponential weights, with regret of order actions. However, the exponential weights based algo-rithms are not efficient if N is large (sampling com-plexity of O ( N ) in every step). Also, their setting is slightly different from ours. The set of arms and the associated b i vectors are non-adaptive and fixed in advance. And, they consider a non-stochastic (adver-sarial) bandit setting where the reward at time t for arm i is b T i  X  t with  X  t chosen by an adversary. Very recent work Russo &amp; Roy (2013) provides near-optimal bounds on Bayesian regret in many general settings. This result is incomparable to ours because of the different notion of regret used.
 While the regret bounds provided in this paper do not match or better the best available regret bounds for the extensively studied problem of linear contextual bandits, our results demonstrate that the natural and efficient heuristic of Thompson Sampling can achieve theoretical bounds that are close to the best bounds. The main contribution of this paper is to provide new tools for analysis of Thompson Sampling algorithm for contextual bandits, which despite being popular and empirically attractive, has eluded theoretical analysis. We believe the techniques used in this paper will pro-vide useful insights into the workings of this Bayesian algorithm, and may be useful for further improvements and extensions. 3.1. Challenges and proof outline The contextual version of the multi-armed bandit problem presents new challenges for the analysis of TS algorithm, and the techniques used so far for analyz-ing the basic multi-armed bandit problem by Agrawal &amp; Goyal (2012); Kaufmann et al. (2012) do not seem directly applicable. Let us describe some of these dif-ficulties and our novel ideas to resolve them.
 In the basic MAB problem there are N arms, with mean reward  X  i  X  R for arm i , and the regret for playing a suboptimal arm i is  X  a  X   X   X  i , where a  X  is the arm with the highest mean. Let us compare this to a 1-dimensional contextual MAB problem, where arm i is associated with a parameter  X  i  X  R , but in addition, at every time t , it is associated with a context b i ( t )  X  so that mean reward is b i ( t )  X  i . The best arm a  X  ( t ) at time t is the arm with the highest mean at time t , and In general, the basis of regret analysis for stochastic MAB is to prove that the variances of empirical esti-mates for all arms decrease fast enough, so that the re-gret incurred until the variances become small enough, is small. In the basic MAB, the variance of the em-pirical mean is inversely proportional to the number of plays k i ( t ) of arm i at time t . Thus, every time the suboptimal arm i is played, we know that even though a regret of  X  i  X   X   X  i  X  1 is incurred, there is also an im-provement of exactly 1 in the number of plays of that arm, and hence, corresponding decrease in the vari-ance. The techniques for analyzing basic MAB rely on this observation to precisely quantify the exploration-exploitation tradeoff. On the other hand, the variance of the empirical mean for the contextual case is given optimal arm i is played, if b i ( t ) is small, the regret b improvement b i ( t ) 2 in B i ( t ).
 In our proof, we overcome this difficulty by dividing the arms into two groups at any time: saturated and unsaturated arms, based on whether the standard deviation of the estimates for an arm is smaller or larger compared to the standard deviation for the optimal arm. The optimal arm is included in the group of unsaturated arms. We show that for the unsaturated arms, the regret on playing the arm can be bounded by a factor of the standard deviation, which improves every time the arm is played. This allows us to bound the total regret due to unsaturated arms. For the saturated arms, standard deviation is small, or in other words, the estimates of the means constructed so far are quite accurate in the direction of the current contexts of these arms, so that the algorithm is able to distinguish between them and the optimal arm. We utilize this observation to show that the probability of playing such arms at any step is bounded by a function of the probability of playing the unsaturated arms.
 Below is a more technical outline of the proof of Theorem 1. At any time step t , we divide the arms into two groups:  X  saturated arms defined as those with g ( T ) s t,i &lt;  X  unsaturated arms defined as those with g ( T ) s t,i  X  ( g ( T ) &gt; ` ( T )) are constants (functions of T,d, X  ) de-fined later. Note that s t,i is the standard deviation of ation of the random variable b i ( t ) T  X   X  ( t ). bound the regret at any time t by g ( T )( s t,a  X  ( t ) + s Now, if an unsaturated arm is played at time t , then using the definition of unsaturated arms, the regret is lines of Auer (2002)), which allows us to bound the total regret due to unsaturated arms.
 For saturated arms, we prove that the probability of playing a saturated arm at any time t is within p of the probability of playing an unsaturated arm, where p = of history H t  X  1 and the contexts b i ( t ) ,i = 1 ,...,N at time t , and prove that for  X  X ost X  (in a high probability We use these observations to establish that ( X t ; t  X  0), where
X t ' regret( t )  X  g ( T ) p I ( a ( t ) is unsaturated) s is a super-martingale difference process adapted to fil-tration F t . Then, using the Azuma-Hoeffding inequal-ity for super-martingales, along with the inequality P high probability regret bound. 3.2. Formal proof For quick reference, the notations introduced below also appear in a table of notations at the beginning of the supplementary material.
 Definition 1. For all i , define  X  i ( t ) = b i ( t ) marginal distribution of each  X  i ( t ) is Gaussian with mean b i ( t ) T  X   X  ( t ) and standard deviation vs t,i s t,i is the standard deviation of estimate b i ( t ) T  X   X  ( t ) . Definition 2. Recall that  X  i ( t ) = b a  X  ( t ) ( t ) b ( t ) T  X  , the difference between the mean reward of op-timal arm and arm i at time t . .
 Definition 3. Define ` ( T ) = R q d ln( T 3 ) ln( 1  X  ) + 1 , Definition 4. Define E  X  ( t ) and E  X  ( t ) as the events respective means. More precisely, define E  X  ( t ) as the event that Define E  X  ( t ) as the event that Definition 5. An arm i is called saturated at time erwise. Let C ( t ) denote the set of saturated arms at time t . Note that the optimal arm is always unsatu-shifting from saturated to unsaturated and vice-versa over time.
 Definition 6. Define filtration F t  X  1 as the union of history until time t  X  1 , and the contexts at time t , i.e., F By definition, F 1  X  F 2  X  X  X   X  F T  X  1 . Observe that the following quantities are determined by the history H t  X  1 and the contexts b i ( t ) at time t , and hence are included  X   X   X  ( t ) ,B ( t ) ,  X  s t,i , for all i ,  X  the identity of the optimal arm a  X  ( t ) and the set  X  whether E  X  ( t ) is true or not,  X  the distribution N ( X   X  ( t ) ,B ( t )  X  1 ) of  X   X  ( t ) , Lemma 1. For all t , 0 &lt;  X  &lt; 1 , Pr( E  X  ( t ))  X  1  X  And, for all possible filtrations F t  X  1 , Pr( E  X  ( t ) |F Proof. The complete proof of this lemma appears in Appendix A.3. The probability bound for E  X  ( t ) will be proven using a concentration inequality given by Abbasi-Yadkori et al. (2011), stated as Lemma 7 in Appendix A.2. The R -sub-Gaussian assumption on rewards will be utilized here. The probability bound for E  X  ( t ) will be proven using a concentration inequal-ity for Gaussian random variables from Abramowitz &amp; Stegun (1964) stated as Lemma 5 in Appendix A.2 .
 The next lemma lower bounds the probability that  X  time t will exceed its mean reward b a  X  ( t ) ( t ) T Lemma 2. For any filtration F t  X  1 such that E  X  ( t ) is true, Proof. The proof uses anti-concentration of Gaussian mean b a  X  ( t ) ( t ) T  X   X  ( t ) and standard deviation vs provided by Lemma 5 in Appendix A.2, and the con-vided by the event E  X  ( t ). The details of the proof are in Appendix A.4.
 The following lemma bounds the probability of playing saturated arms in terms of the probability of playing unsaturated arms.
 Lemma 3. Given any filtration F t  X  1 such that E  X  ( t ) is true, where p = 1 Proof. The algorithm chooses the arm with the high-of the unsaturated arms (which include the optimal arm and other suboptimal unsaturated arms) must be played. Therefore, By definition, for all saturated arms, i.e. for all events E  X  ( t ) and E  X  ( t ) are true then, by the def-initions of these events, for all j  X  C ( t ),  X  j ( t )  X  b ( t ) T  X  + g ( T ) s t,j . Therefore, given an F t  X  1 that E  X  ( t ) is true, either E  X  ( t ) is false, or else for all j  X  C ( t ),  X  ( t )  X  b j ( t ) T  X  + g ( T ) s t,j  X  b a  X  ( t ) ( t ) Hence, for any F t  X  1 such that E  X  ( t ) is true,  X  p  X  The last inequality uses Lemma 2 and Lemma 1. Sub-stituting in Equation (1), this gives, which implies Definition 7. Recall that regret ( t ) was defined as, regret 0 ( t ) = regret ( t )  X  I ( E  X  ( t )) . Next, we establish a super-martingale process that will form the basis of our proof of the high-probability re-gret bound.
 Definition 8. Let where p = 1 Lemma 4. ( Y t ; t = 0 ,...,T ) is a super-martingale process with respect to filtration F t .
 Proof. See Definition 9 in Appendix A.2 for the defi-nition of super-martingales. We need to prove that for all t  X  [1 ,T ], and any F t  X  1 , E [ Y t  X  Y t  X  1 |F t  X  1 If F t  X  1 is such that E  X  ( t ) is not true, then regret regret( t )  X  I ( E  X  ( t )) = 0, and the above inequality holds trivially. So, we consider F t  X  1 such that E  X  ( t ) holds. if an arm i is played at time t , then it must be true that  X  i ( t )  X   X  a  X  ( t ) ( t ). And, if E  X  ( t ) and E true, then, b i ( t ) T  X   X   X  i ( t )  X  g ( T ) s t,i Therefore, given a filtration F t  X  1 such that E  X  ( t ) is is false. And, hence, In the first inequality we used that for all i ,  X  i ( t )  X  1. The second inequality used the definition of unsatu-1 to apply Pr E  X  ( t )  X  1 T 2 . The third inequality used Lemma 3, and also the observation that 0  X  s t,a  X  ( t )  X  || b Now, we are ready to prove Theorem 1.
 Proof of Theorem 1 We observe that the absolute value of each of the four terms in the definition of X t Y t has bounded difference | Y t  X  Y t  X  1 | X  8 p t  X  1. Thus, we can apply Azuma-Hoeffding inequality (see Lemma 6 in Appendix A.2), to obtain that with probability 1  X   X  2 , The second inequality used the observation that if an unsaturated arm is played, i.e., a ( t ) /  X  C ( t ), then, can be derived along the lines of Lemma 3 of Chu et al. (2011) using Lemma 11 of Auer (2002) (see Ap-pendix A.5 for details). Also, recalling the definitions of p,` ( T ), and g ( T ) (see the Table of notations in the beginning of the supplementary material), and substi-tuting in above, we get Also, because E  X  ( t ) holds for all t with probability for all t with probability at least 1  X   X  2 . Hence, with probability 1  X   X  , The proof for the alternate definition of regret men-tioned in Remark 1 is provided in Appendix A.5. Detailed concluding remarks appear in supplementary materials Sec. D.
 Abbasi-Yadkori, Yasin, P  X al, D  X avid, and Szepesv  X ari, Csaba. Improved Algorithms for Linear Stochastic Bandits. In NIPS , pp. 2312 X 2320, 2011.
 Abramowitz, Milton and Stegun, Irene A. Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables . Dover, New York, 1964. Agrawal, Shipra and Goyal, Navin. Analysis of Thompson Sampling for the Multi-armed Bandit Problem. In COLT , 2012.
 Agrawal, Shipra and Goyal, Navin. Further Optimal
Regret Bounds for Thompson Sampling. AISTATS , 2013.
 Auer, Peter. Using Confidence Bounds for
Exploitation-Exploration Trade-offs. Journal of Ma-chine Learning Research , 3:397 X 422, 2002.
 Auer, Peter, Cesa-Bianchi, Nicol`o, Freund, Yoav, and Schapire, Robert E. The Nonstochastic Multiarmed
Bandit Problem. SIAM J. Comput. , 32(1):48 X 77, 2002.
 Bubeck, S  X ebastien, Cesa-Bianchi, Nicol`o, and Kakade,
Sham M. Towards minimax policies for online linear optimization with bandit feedback. Proceedings of the 25th Conference on Learning Theory (COLT) , pp. 1 X 14, 2012.
 Chapelle, Olivier and Li, Lihong. An Empirical Eval-uation of Thompson Sampling. In NIPS , pp. 2249 X  2257, 2011.
 Chapelle, Olivier and Li, Lihong. Open Problem: Re-gret Bounds for Thompson Sampling. In COLT , 2012.
 Chu, Wei, Li, Lihong, Reyzin, Lev, and Schapire, Robert E. Contextual Bandits with Linear Payoff Functions. Journal of Machine Learning Research -Proceedings Track , 15:208 X 214, 2011.
 Dani, Varsha, Hayes, Thomas P., and Kakade, Sham M. Stochastic Linear Optimization under Bandit Feedback. In COLT , pp. 355 X 366, 2008.
 Filippi, Sarah, Capp  X e, Olivier, Garivier, Aur  X elien, and
Szepesv  X ari, Csaba. Parametric Bandits: The Gen-eralized Linear Case. In NIPS , pp. 586 X 594, 2010. Graepel, Thore, Candela, Joaquin Qui  X nonero, Borchert, Thomas, and Herbrich, Ralf. Web-Scale
Bayesian Click-Through rate Prediction for Spon-sored Search Advertising in Microsoft X  X  Bing Search Engine. In ICML , pp. 13 X 20, 2010.
 Granmo, O.-C. Solving Two-Armed Bernoulli Bandit Problems Using a Bayesian Learning Automaton. International Journal of Intelligent Computing and Cybernetics (IJICC) , 3(2):207 X 234, 2010.
 Kaelbling, Leslie Pack. Associative Reinforcement
Learning: Functions in k-DNF. Machine Learning , 15(3):279 X 298, 1994.
 Kaufmann, Emilie, Korda, Nathaniel, and Munos, R  X emi. Thompson Sampling: An Optimal Finite Time Analysis. ALT , 2012.
 Lai, T. L. and Robbins, H. Asymptotically effi-cient adaptive allocation rules. Advances in Applied Mathematics , 6:4 X 22, 1985.
 Langford, John and Zhang, Tong. The Epoch-Greedy
Algorithm for Multi-armed Bandits with Side Infor-mation. In NIPS , 2007.
 May, Benedict C. and Leslie, David S. Simula-tion studies in optimistic Bayesian sampling in contextual-bandit problems. Technical Report 11:02, Statistics Group, Department of Mathemat-ics, University of Bristol, 2011.
 May, Benedict C., Korda, Nathan, Lee, Anthony, and Leslie, David S. Optimistic Bayesian sampling in contextual-bandit problems. Technical Report 11:01, Statistics Group, Department of Mathemat-ics, University of Bristol, 2011.
 Ortega, Pedro A. and Braun, Daniel A. Linearly
Parametrized Bandits. Journal of Artificial Intel-ligence Research , 38:475 X 511, 2010.
 Russo, Daniel and Roy, Benjamin Van. Learn-ing to optimize via posterior sampling. CoRR , abs/1301.2609, 2013.
 Sarkar, Jyotirmoy. One-armed badit problem with co-variates. The Annals of Statistics , 19(4):1978 X 2002, 1991.
 Scott, S. A modern Bayesian look at the multi-armed bandit. Applied Stochastic Models in Business and Industry , 26:639 X 658, 2010.
 Strehl, Alexander L., Mesterharm, Chris, Littman,
Michael L., and Hirsh, Haym. Experience-efficient learning in associative bandit problems. In ICML , pp. 889 X 896, 2006.
 Strens, Malcolm J. A. A Bayesian Framework for Re-inforcement Learning. In ICML , pp. 943 X 950, 2000. Thompson, William R. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika , 25(3-4):285 X  294, 1933.
 Woodroofe, Michael. A one-armed bandit problem with a concomitant variable. Journal of the Ameri-can Statistics Association , 74(368):799 X 806, 1979. Wyatt, Jeremy. Exploration and Inference in Learning from Reinforcement . PhD thesis, Department of Ar-
