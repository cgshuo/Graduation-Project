 In this paper, we propose a new way to automatically model and predict human behavior of receiving and disseminating information by analyzing the contact and content of personal communications. A personal profile, called CommunityNet, is established for each individual based on a novel algorithm simultaneously. It can be used for personal social capital management. Clusters of CommunityNets provide a view of informal networks for organization management. Our new algorithm is developed based on the combination of dynamic algorithms in the social network field and the semantic content classification methods in the natural language processing and machine learning literatures. We tested CommunityNets on the Enron Email corpus and report experimental results including filtering, prediction, and recommendation capabilities. We show that the personal behavior and intention are somewhat predictable based on these models. For instance, "to whom a person is going to send a specific email" can be predicted by one X  X  personal social network and content analysis. Experimental results show the prediction accuracy of the pr oposed adaptive algorithm is 58% better than the social network-based predictions, and is 75% better than an aggregated model based on Latent Dirichlet Allocation with social network enhancement. Two online demo systems we developed that allow interactive exploration of CommunityNet are also discussed. I.2.6 [Artificial Intelligence]: Learning General Terms: algorithms, experimentation Keywords: user behavior modeling, personal information management, information dissemination what you know, but who you know [1]. A social network, the graph of relationships and interactions within a group of individuals, plays a fundamental role as a medium for the spread of information, ideas, and influence. At the organizational level, personal social networks are activated for recruitment, partnering, and information access. At the individual level, people exploit their networks to advance careers and gather information. hard to acquire, factor affecting companies' performance. Krackhardt [2] showed that companies with strong informal networks perform five or six times better than those with weak networks, especially on the long-term performance. Friend and advice networks drive enterprise operations in a way that, if the real organization structure does not match the informal networks, then a company tends to fail [3]. Since Max Weber first studied modern bureaucracy structures in the 1920s, decades of related social scientific researches have been mainly relying on questionnaires and interviews to understand individuals' thoughts and behaviors for sensing informal networks. However, data collection is time consuming and seldom provides timely, continuous, and dynamic information. This is usually the biggest hurdle in social studies. principle for advanced user interfaces that offer information management and communication services in a single integrated system. One of the most pronounced examples is the networking study by Nardi et al. [4], who coined the term intensional networks to describe personal social networks. They presented a visual model of user X  X  PSN to organize personal communications in terms of a social network of contacts. From this perspective, many tools were built such as LinkedIn [5], Orkut [6], and Friendster [7]. However, all of them only provide tools for visually managing personal social networks. Users need to manually input, update, and manage these networks. This results in serious drawbacks. For instance, people may not be able to invest necessary efforts in creating rich information, or they may not keep the information up-to-date as their interests, responsibilities, and network change. They need a way to organize the relationship and remember who have the resources to help them. We coin the terminology of managing these goals as personal social capital management 1 . technology, which can dynamically describe and update a person X  X  personal social network with context-dependent and temporal evolution information from personal communications. We refer to the model as a CommunityNet . Senders and receivers, time stamps, subject and content of emails contribute three key components  X  content semantics , temporal information, and social relationship . We propose a novel Content-Time-Relation (CTR) algorithm to capture dynamic and context-dependent information in an unsupervised way. Based on the CommunityNet models, many questions can be addressed by inference, prediction and filtering. For instance, 1) Who are semantically related to each other? 2) Who will be involved in a special topic? Who are the important (central) people in this topic? 3) How does the information flow? and 4) If we want to publicize a message, whom should we inform? topic detection and clustering is conducted on training emails in order to define topic-communities. Then, for each individual, CommunityNet is built based on the detected topics, the sender and receiver information, and the time stamps. Afterwards, these personal CommunityNets can be applied for inferring organizational informal networks and predicting personal behaviors to help users manage their social capitals. We incorporate the following innovative steps: 1) Incorporate content analysis into social network in an 2) Build a CommunityNet for each user to capture the context-3) Analyze people X  X  behaviors based on CommunityNet, 4) Show the potential of using automatically acquired personal corpus comprising the communication records of 154 Enron employees dating from Jan. 1999 to Aug. 2002. The Enron email dataset was originally made available to public by the Federal Energy Regulatory Commission during the investigation [9]. It was later collected and prepared by Melinda Gervasio at SRI for the CALO (A Cognitive Assistant that Learns and Organizes) project. William Cohen from CMU has put up the dataset on the web for research purpose [9]. This version of the dataset contains around 517,432 emails within 150 folders. We clean the data and extract 154 users from those 150 folders with 166,653 unique messages from 1999 to 2002. In the experiments, we use 16,873 intra-organizational emails which connect these 154 people. we develop an algorithm incorporating content-time-relation detection. Second, we generate an application model which describes personal dynamic community network. Third, we show how this model can be applied to organization and social capital management. To the best of our knowledge, this is among the first reported technologies on fusing research in the social network analysis field and the content analysis field for information management. We propose the CTR algorithm and the CommunityNet based on the Latent Dirichlet Allocation algorithm. In our experiments, we observed clear benefit of discovering knowledge based on multi-modality information rather than using only single type of data.
 we present an overview of related work. In Section 3, we present our model. We discuss how to use CommunityNet to analyze communities and individuals in section 4 and 5, respectively. In Section 6, we show two demo systems for query, visualization and contact recommendation. Finally, conclusions and future work are addressed in Section 7. been a subject of study for more than 50 years. An early sign of the potential of social network was perhaps the classic paper by Milgram [10] estimating that on average, every person in the world is only six edges away from each other, if an edge between i and j means " i knows j ". Lately, introducing social network analysis into information mining is becoming an important research area. Schwartz and Wood [11] mined social relationships from email logs by using a set of heuristic graph algorithms. The Referral Web project [12] mined a social network from a wide variety of publicly-available online information, and used it to help individuals find experts who could answer their questions based on geographical proximity. Flake et al. [13] used graph algorithms to mine communities from the Web (defined as sets of sites that have more links to each other than to non-members). Tyler et al. [14] use a betweenness centrality algorithm for the automatic identification of communities of practice from email logs within an organization. The Google search engine [15] and Kleinberg's HITS algorithm of finding hubs and authorities on the Web [16] are also based on social network concepts. The success of these approaches, and the discovery of widespread network topologies with nontrivial properties, have led to a recent flurry of research on applying link analysis for information mining. structural properties of social networks is the class of Exponential Random Graph Models (ERGMs) (or p* model) [17]. This statistical model can represent structural properties that define complicated dependence patterns that cannot be easily modeled by deterministic models. Let Y denote a random graph on a set of n nodes and let y denote a particular graph on those nodes. Then, the probability of Y equals to y is where () s y is a known vector of graph statistics (Density, Reciprocity, Transitivity, etc) on y,  X  is a vector of coefficients to model the influence of each statistics for the whole graph, T satisfy () 1 properties in a network to represent the complex structure. However, social networks evolve over time. Evolution property has a great deal of influence; e.g. , it affects the rate of information diffusion, the ability to acquire and use information, and the quality and accuracy of organizational decisions. researchers X  attentions recently. Given a snapshot of a social network, [19] tries to infer which new interactions among its members are likely to occur in the near future. In [20], Kubica et al. are interested in tracking changes in large-scale data by periodically creating an agglomerative clustering and examining the evolution of clusters over time. Among the known dynamical social networks in literature, Snijder's dynamic actor-oriented social network [18] is one of the most successful algorithms. Changes in the network are modeled as the stochastic result of network effects (density, reciprocity, etc. ). Evolution is modeled by continuous-time Markov chains, whose parameters are estimated by the Markov chain Monte Carlo procedures. In [21], Handcock et al. proposed a curved ERGM model and applied it to the new specifications of ERGMs This latest model uses nonlinear parameters to represent structural properties of networks. in analyzing longitudinal stream data. However, most of them are only based on pure network properties, without knowing what people are talking about and why they have close relationships. of modeling the contributions of different topics to a document is to treat each topic as a probability distribution over words, viewing a document as a probability distribution over words, and thus viewing a document as a probabilistic mixture over these document is formalized as: where i z is a latent variable indicating the topic from which the i th word was drawn and () | w under the j th topic. () a word from topics j in the current document, which varies across different documents. Latent Semantic Analysis (PLSA), in which, topics are modeled as multinomial distributions over words, and documents are assumed to be generated by the activation of multiple topics. Blei et al. [23] proposed Latent Dirichlet Allocation (LDA) to address the problems of PLSA that parameterization was susceptible to overfitting and did not provide a straightforward way to infer testing documents. A distribution over topics is sampled from a Dirichlet distribution for each document. Each word is sampled from a multinomial distribution over words specific to the sampled topic. Following the notations in [24], in LDA, D documents containing T topics expressed over W unique words, we can represent (|) P wz with a set of T multinomial distributions  X  over the W words, such that () and P(z) with a set of D multinomial distribution topics, such that for a word in document d , () Recently, the Author-Topic (AT) model [25] extends LDA to include authorship information, trying to recognize which part of the document is contributed by which co-author. In a recent unpublished work, McCallum et al. [26] further extend the AT model to the Author-Recipient-Topic model by regarding the sender-receiver pair as an additional author variable for topic classification. Their goal is role discovery, which is similar to one of our goals as discussed in Sec. 4.1.2 without taking the temporal nature of emails into consideration. by using sophisticated approximation either with variational Bayes or expectation propagation. To solve this problem, Griffiths and Steyvers [24] extended LDA by considering the posterior distribution over the assignments of words to topics and showed how Gibbs sampling could be applied to build models. Specifically, n is the number of times word w has been assigned to topic j in the vector of assignments z , () d j n is the number of times a word from document d has been assigned to topic j , n , () d n is a sum of () d j n . Further, one can estimate probability of using word w in topic j , and () d j  X  , the probability of topic j in document d as follows: their algorithm and show meaningful aspects of the structure and relationships between scientific papers. key factors for current data mining and knowledge management models. However, there are few papers addressing these three components simultaneously. .In our r ecent paper, we built user models to explicitly describe a person X  X  expertise by a relational and evolutionary graph representation called ExpertisetNet [27]. In this paper, we continue exploring this thread, and build a CommunityNet model which incorporates these three components together for data mining and knowledge management. propose a Content-Time-Relation (CTR) algorithm to build the personal CommunityNet . We also specifically address the prediction of the user X  X  behaviors as a classification problem and solve it based on the CommunityNet models. Definition 1. Topic-Community: A topic community is a group of people who participate in one specific topic.
 Definition 2: Personal Topic-Community Network (PTCN): A personal topic-community network is a group of people directly connected to one person about a specific topic. Definition 3. Evolutionary Personal Social Network: An evolutionary personal social network illustrates how a personal social network changes over time. Definition 4. Evolutionary Personal Topic-Community Network: An evolutionary network illustrates how a person X  X  personal topic-community network changes over time.
 Definition 5. Personal Social Network Information Flow: A personal social network information flow illustrates how the information flows over a person X  X  personal social network to other people X  X  personal social networks Definition 6: Personal Topic-Community Information Flow: A personal Topic-CommunityNet information flow illustrates how the information about one topic flows over a person X  X  personal social network to other people X  X  personal social networks. their communication records. The nodes of a network represent whom this person contacts with. The weights of the links measure the probabilities of the emails he sends to the other people: A basic form of the probability that an user u sending email to a recipient r is: We build evolutionary personal social networks to explore the dynamics and the evolution. The ERGM in Eq. (1) can be used to replace Eq. (6) for probabilistic graph modeling. A big challenge of automatically building evolutionary personal social network is the evolutionary segmentation, which is to detect changes between personal social network cohesive sections. Here we apply the same algorithm as we proposed in [27]. For each personal social network in one time period t , we use the exponential random graph model [17] to estimate an underlying distribution to describe the social network. An ERGM is estimated from the data in each temporal sliding window. With these operations, we obtain a series of parameters which indicates the graph configurations. information, and time stamps, and use these sources of knowledge to create a joint probabilistic model. An observation is (u, r, d, w, email d containing words w during a particular time period t . Conceptually, users choose latent topics z , which in turn generate receivers r , documents d , and their content words w during time period t . where , ur is a sender-receiver pair during time period t . , ur can be replaced by any variable to indicate the user X  X  behavior, as long as it is also assumed to be dependent on latent topics of emails. latent topics dynamically and at the same time track the emails related to the old topics. This is a problem similar to topic detection tracking [28]. We propose an incremental LDA (ILDA) algorithm to solve it, in which the number of topics is dynamically updated based on the Bayesian model selection principle [24]. The procedures of the algorithm are illustrated as follows: Incremental Latent Dirichlet Allocation (ILDA) algorithm: Input: Email streams with timestamp t Output: () , w jt  X  , () , d jt  X  for different time period t Steps: 1) Apply LDA on a data set with currently observed emails in a 2) When new emails arrive during time period k, use Bayesian 3) Repeat step 2) until no data arrive. Relation (CTR) algorithm. It consists of two phases, the training phase and the testing phase. In the training phase, emails as well as the senders, receivers and time stamps are available. 
P wzt and () ,|, old P ur zt are learnt from the observed data. In the testing phase, we apply ILDA to learn Based on () ,|, old P ur zt , which is learnt from the training phase, , ur can be inferred. Again, , ur represents a sender-receiver pair or any variable to indicate the user X  X  behavior, as long as it is dependent on the latent topics of emails. Content-Time-Relation (CTR) algorithm: 1) Training phase Input: Old emails with content, sender and receiver information, and time stamps old t Steps: a) Apply Gibbs Sampling on the data according to equation (3). b) Estimate () () , |, c) Estimate 2) Testing phase Input: New emails with content and time stamps new t Steps: a) Apply incremental LDA by Gibbs Sampling based on c) If there are new topics detected, update the model by on this model. For the CTR algorithm, sender variable u or receiver variable r is fixed. For instance, if we are interested in () |,, P rudt , which is to answer a question of whom we should send the message d to during the time period t . The answer will be arg max | , , arg max | , , | , , | , | , , where z / time period t . Another question is if we receive an email, who will be possibly the sender? arg max | , , arg max | , , | , , | , | , , Eq. (9) and Eq. (10) integrate the PSN, content and temporal analysis. Social network models such as ERGM in Eq. (1) or the model in Sec. 3.2 can be applied to the () ,|, P ur dt terms. LDA, AT and ART models. In CTR, the observed variables not only include the words w in an email but also the sender u and the timestamp on each email d . schemes in details. Specifically, we address the problem of predicting receivers and senders of emails as a classification problem, in which we train classifiers to predict the senders or receivers and other behavior patterns given the observed people X  X  communication records. The trained classifier represents a function in the form of: where () , Comm t i t  X  is the observed communication record senders or other user behavior patterns to be discriminated, and regarding which user behavior patterns gave rise to the observed communication records. The classifier is trained by providing the history of the communication records with known user behaviors. a given user, and build his/her personal social network. We choose those people with the highest communication frequency with this person as the prediction result. prediction, which is referred as LDA-PSN in the paper. Latent topics are detected by applying original LDA on the training set and LDA is used for inference in testing data without incorporating new topics when time passes by. The possible senders and receivers when new emails arrive, () ,|, new People are ranked by this probability as the prediction results. different topics during different time periods. This is the assumption we made for our predictive model based on CTR. time slidin g window, A : number of authors, ,  X   X  and 
P ur dt is estimated by applying the CTR model discussed in section 3.3. The prediction results are people with highest scores calculated by equation (9) and (10). a key piece of information from communication records --the dynamical nature of emails. Both personal social network and Topic-Community dynamically change and evolve. Only based on the training data which are collected in history will not get the optimal performance for the prediction task. Adaptive prediction by updating the model with newest user behavior information is necessary. We apply several strategies for the adaptive prediction. The first strategy is aggregative updating the model by adding new user behavior information including the senders and receivers into the model. Then the model becomes:  X  ,|, ,|, |, ,| |, P ur dt P ur z t P z dt P ur t P z dt where K is the number of old topics. Here, we always use the data during i t . current data and the previous data decays over time. The more recent data are more important. Thus, a sliding window of size n is used to choose the data for building the prediction model, in which the prediction is only dependent on the recent data, with the influence of old data ignored. Here in equation (12), algorithm. The CommunityNet model, which refers to the personal Topic-Community Network, draws upon the strengths of the topic model and the social network as well as the dynamic model, using a topic-based representation to model the content of the document, the interests of the users, the correlation of the users and the receivers and all these relationship changing over time. For prediction, CommunityNet incorporates the adaptive CTR model as described in Section 3.4.4. of topics, and the senders and receivers who participated in those topics. First, we analyze the topics detected from the Enron Corpus. Then, we study the topic-community patterns. to choose the number of topics. In the Enron intra-organization emails, there are 26,178 word-terms involved after we apply stop-words removal and stemming, We computed () | PwT for T values of 30, 50, 70, 100, 110, 150 topics and chose T = 100 with the maximum value of () () log | PwT for the experiment. have P(z|d) , which indicates how likely each document belongs to each topic. By summing up this probability for all the documents, we get the topic distribution of how likely each topic occurs in this corpus. We define this summed likelihood as  X  X opularity X  of the topic in the dataset. From this topic distribution, we can see that some topics are hot -people frequently communicate with each other a bout them, while some others are cold, with only few emails related to them. Table 1 illustrates the top 5 topics in Enron corpus. We can see that most of them are talking about regular issues in the company like meeting, deal, and document. Table 2 illustrates the bottom 5 topics in Enron corpus. Most of them are specific and sensitive topics, like  X  X tock X  or  X  X arket X . People may feel less comfortable to talk about them broadly. topic popularity for year 2000 and 2001, and calculate the correlation coefficients of these two series. For some topics, the trends over years are similar. Figure 3(a) illustrates the trends for two topics which have largest correlation coefficients between two years. Topic 45, which is talking about a schedule issue, reaches a peak during June to September. For topic 19, it is talking about a meeting issue. The trend repeats year to year. over 2000 to 2001. We can see that it reaches a peak from the end of year 2000 to the beginning of year 2001. From the timeline of Enron [29], we found that  X  X alifornia Energy Crisis X  occurred at exactly this time period. Among the key people related to this topic, Jeff Dasovich was an Enron government relations executive . His boss, James Steffes was Vice President of Government Affairs. Richard Schapiro was Vice President of Regulatory Affairs . Richard Sanders was Vice President and Assistant General Counsel . Steven Kean was Executive Vice President and Chief of Staff . Vincent Kaminski was a Ph.D. economist and Head of Research for Enron Corp . Mary Han was a lawyer at Enron X  X  West Coast trading hub . From the timeline, we found all these people except Vince were very active in this event. We will further analyze their roles in Section 5. Popularity (b) The trend of  X  X alifornia Power X  and most related keywords only under certain few topics. People in the same community under a topic would share the information. Thus, if there is something new about one topic, people in that topic-community will most likely get the information and propagate it to others in the community. Finally, many people in the community will get the information. will be possibly involved in an observed email, we collect the ground truth about who are the senders and receivers for the training set and testing set. We tried two strategies for this experiment. First is to randomly partition the data into a training set with 8465 messages and a testing set with 8408 messages. Prediction accuracy is calculated by comparing the inference results and the ground truth ( i.e ., receiver-sender pair of that email). We found that 96.8446% people stick in the old topics they are familiar with. The second strategy is to partition data by time: emails before 1/31/2000 as the training data (8011) and after that as the testing data (8862). We found 89.2757% of the people keep their old topics. Both results are quite promising. It is found that people really stick in old topics they are familiar with. CommunityNet . First, we show how people X  X  roles in an event can be inferred by CommunityNet . Then, we show the predicting capability of the proposed model in experiments. specifically on specific topics. Here we show it is possible to infer people X  X  roles by using CommunityNet . involved in  X  X alifornia Energy Crisis X . In reality, Dasovich, Steffes, Schapiro, Sanders, and Kean, were in charge of government affairs. Their roles were to  X  X olve the problem X . Mary Hain was a lawyer during the worst of the crisis and attended meetings with key insiders. We calculated the correlation coefficients of the trends of these people and the overall trend of this topic. Jeff Dasovich got 0.7965, James Steffes got 0.6501, Mary Hain got 0.5994, Richard Shapiro got 0.5604, Steven Kean got 0.3585 (all among the 10 highest correlation scores among 154 people), and Richard Sanders got 0.2745 (ranked 19), while Vince Kaminski had correlation coefficient of -0.4617 (Figure 4). We can see that all the key people except Vince Kaminski have strong correlation with the overall trend of  X  X alifornia Energy Crisis X . From their positions, we can see that all of them were sort of politicians while Vince Kaminski is a researcher. Thus, it is clear to see the difference of their roles in this topic. Popularity Figure 4. Personal topic trend comparison on  X  X alifornia Power X  possible to infer who will possibly be the receivers by a person X  X  own historic communication records and the content of the email-to-send. One possible application is to help people organize personal social capital. For instance, if a user has some information to send or a question to ask, CommunityNet can recommend the right persons to send the info or get the answer. training set with the emails from 1999 to 2000, and a testing set with the emails from 2001 to 2002. The testing set is further partitioned into sub-sets with emails from one month as a subset. With this, we have 15 testing sets. (We exclude the emails after March 2002 because the total number of emails after that is only 78.) One issue we want to mention is that the number of people from 1999 to 2000 is 138, while from 2001 to 2002 is 154. In this study, we test each email in the training set by using its content, sender, and time as prior information to predict the receiver, which is compared to the real receiver of that email. comparing the CTR algorithm, PSN, and the aggregated LDA-PSN model. The result shows that CTR beats PSN by 10% on accuracy. The aggregated LDA-PSN model performs even worse than PSN, because of the inaccurate clustering results. The performance gain is 21%. Moreover, intuitively, personal contacts evolve over time. Models built at a specific time should have decreasing predicting capability over time. In this figure, we obtain strong evidence of this hypothesis by observing that the performance of these models monotonically decays. This also implies our models well match the practice. measured by testing whether the  X  X eal X  receiver is among the the senders given a person X  X  CommunityNet and the content of the email. One possible application is to exclude spam emails or detect identification forgery. Figure 6 illustrates the prediction result, which also shows the prediction accuracy decays over time. from the results of 5.2 and 5.3, which reflects the changes of the nature of email streams. Here we apply adaptive prediction algorithms we mentioned in 3.4.3, in which we incrementally and adaptively estimate statistical parameters of the model by gradually forgetting out-of-state statistics. (b) Comparison of algorithms using Breese evaluation metrics algorithm and compares it to the CTR algorithm. For the data far away from the training data, the improvement is more than 30%. And, if we compare it to the PSN and LDA-PSN algorithms, the performance gains are 58% and 75%, respectively. Evaluation by this accuracy metric tells us how related the top people ranked in the prediction results are. To understand the overall performance of the ranked prediction results, we apply the evaluation metric proposed by Breese [30], and illustrate the overall comparison in Figure 7(b). This metric is an aggregation of the accuracy measurements in various top-n retrievals in the ranked list. Among all predictive algorithms, adaptive CTR models perform best and PSN performs worst. In adaptive CTR models, estimating from recent data of six months beats aggregative updating the model from all the data from the history. based on the CommunityNet. The first one is a visualization and query tool to demonstrate informal networks incorporation. The second one is a r eceiver recommendation tool which can be used in popular email systems. These demos can be accessed from http://nansen.ee.washington.edu/CommunityNet/. system of CommunityNet. The distance of nodes represents the closeness (measured by the communication frequencies) of a person to the center person. Users can click on the node to link to the CommunityNet of another person. This system can show personal social networks, which includes all the people a user contacts with during a certain time period. For instance, Figure 8(a) illustrates the personal social network of Vice President John Arnold from January 1999 to December 2000. During this period, there were 22 people he sent emails to, regardless what they were talking about. An evolutionary personal social network is illustrated in Figure 8(b), in which we show people X  X  personal social network changes over time. From Jan. 1999 to Dec. 2000, no new contact was added to John X  X  PSN. However, people X  X  relationship changed in 2000. A Personal Social Network Information Flow is illustrated in Figure 8(c), in which we show how the information flows through the network (here we illustrate the information in two levels.) (c) Personal Social Network Information Flow with two-level will contact with under a certain topic. On retrieval, keywords are several personal topic-community networks for John Arnold. First, we type in  X  X hristmas X  as the keyword. CommunityNet infers it as  X  X oliday celebration X  and shows the four people John contacted with about this topic. About  X  X tock X , we find John talked with five people on  X  X tock Market X  and  X  X ompany Share X  from Jan. 1999 to Dec. 2000. Personal Topic-Community network can be depicted by the system, too. 
Figure 9. Personal Topic-Community Networks when we type in whom to ask how to find an expert and who may tell him/her more details because of their close relationships. In our second demo, we show a CommunityNet application which addresses this problem. This tool can be incorporated with general email systems to help users organize their personal social capitals. First, after a user login a webmail system, he can type in content and/or subject then click on the "Show Content Topic-Community". This tool shall recommend appropriate people to send this email to, based on the learned personal social network or personal topic-community. The distances of nodes represent the closeness of the people to the user. Users can click on the node to select an appropriate person to send email to. If the center node is clicked, then a sphere grows to represent his ties to a group of experts. Click on  X  X ail To X , then the people in the sphere will be included in the sender list. can ask his closest friends whenever he has questions or wants to disseminate information. If he wants to inform or get informed on  X  X overnment X  related topics, the system will suggest him to send emails to Steffes, Allen, Hain, or Scott. The topics are inferred by matching the terms from the Subject as well as the content of the email. He can also type in  X  X an you tell me the current stock price? X  as the email content. This system will detect  X  X tock Market X  as the most relevant topic. Based on Dasovich X  X  CommunityNet , it shows three possible contacts. He then chooses appropriate contact(s). (b) Receiver recommendation for  X  X an you tell me the current and predict human behavior of receiving and disseminating information. We establish personal CommunityNet profiles based on a novel Content-Time-Relation algorithm, which incorporates contact, content, and time information simultaneously from personal communication. CommunityNet can model and predict the community behavior as well as personal behavior. Many interesting results are explored, such as finding the most important employees in events, predicting senders and receivers of emails, etc. Our experiments show that this multi-modality algorithm performs better than both the social network-based predictions and the content-based predictions. Ongoing work includes studying the response time of each individual to emails from different people to further analyze user X  X  behavior, and also incorporating nonparametric Bayesian methods such as hierarchical LDA with contact and time information. We would like to thank D. Blei, T. Griffiths, Yi Wu and anonymous reviewers for valuable discussions and comments. This work was supported by funds from NEC Labs America. 
