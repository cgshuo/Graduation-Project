 Text clustering is one of the most common themes in data mining field. As we know, text data is unstructured, which cannot be directly processed by the clustering algorithms. Thus text representation plays a vital role in text mining. Vector space model (VSM) borrowed from information retrieval is popularly used here. In VSM, a document is represented as a feature vector which consists of the words, phrases or other semantic units to identify the contents of the document. Different features have different weights according to their importance for the specific mining task. There are two open problems in VSM. One is how to select appropriate features. The other is how to weight these features. In this paper, we focus on solving the second problem, feature weighting.

Feature weighting approaches can be divided into two groups according to whether the approach makes use of the prior category label information, i.e. supervised feature weighting and unsupervised feature weighting [1]. In this pa-per, we aim to improve unsupervised feature weighting because our task is text clustering, where there is no known membership information in the data set.
Traditional feature weighting approaches (e.g. TF-IDF) directly compute the syntactic relationship between feature and document via some statistical meth-ods. Generally, it consists of two factors: fe ature frequency factor representing the content of a document and document frequ ency factor indicating the feature X  X  discriminating power [2]. However, they c onsider neither the semantic relation-ship between feature and document nor the relatedness between features.
To enrich text representation with semantic information, researchers have begun to take advantage of feature relatedness for re-weighting feature [3,4] and computing document similarity [5]. Jing et al. [3] and Wang et al. [4] embedded the semantic information to document representation by multiplying document-feature tf-idf matrix and feature relatedness matrix. Their new feature weight contains all the relatedness between each pair of features in the whole document collection. Huang et al. [5] designed a new document similarity measure by taking account of the feature relatedness betw een two documents. However, both these two approaches need to cost too much computational complexity.

In this paper, we propose a new seman tically enriched feature weighting method by taking account of the local feature relatedness  X  the relatedness between features within each individual document. In other words, the weight of each feature in document is calculated a ccording to the relatedness between the feature and its contextual content. Here, the features in the same document are taken as the contextual content. In this case, we can say the local feature relat-edness weight implies the semantic relationship between feature and document because the whole features of each docum ent can express the document X  X  topic. By this method, feature highly semantically related to document X  X  topic can be arranged with a higher weight. In addition, it takes much less computational complexity than the above two feature relatedness-based approaches.
In order to measure the feature relatedness, we adopt two methods, document collection-based implicit relatedness measure and Wikipedia link-based explicit relatedness measure. The former mines feature relatedness on the basis of feature distribution in document collection, while the later explicitly calculates semantic relatedness between features according to Wikipedia hyperlink structure.
The rest of the paper is organized as follows: Section 2 describes document representation and two feature relatedness measure methods. In Section 3, we propose the local feature relatedness-based weighting approach and combine syn-tactic and semantic factors in terms of d ocument similarity. Section 4 compares the new feature weighting method with two existing feature relatedness-based work. Section 5 describes the experimental methodology and discusses the ex-perimental results. Finally, we conclude the paper in Section 6. 2.1 Document Representation The original text documents cannot be d irectly processed by clustering algo-rithms. Therefore, documents need to be converted into a more manageable representation. Typically, a document is represented as a vector in feature space  X  a sequence of features [6]. The most r epresentation models use terms as the features. Terms can be words or n-grams appearing in document.

With the development of the semantics -based text representation, concepts from background knowledge are also used as features in some literature. For ex-ample, terms appearing in document are first mapped to their most related concepts indicated by the Wikipedia articles via the hyperlink structure in Wikipedia, as mentioned in [7], and then document representation is built by replacing the terms with those concepts. Concept VSM has been wide used in existing semantics-based text mining [4,5,8,9,10].

In this paper, we propose the weighting approach by regarding both terms and concepts as features separately. 2.2 Relatedness Measure Our purpose is to improve feature weighting with the aid of the relatedness between features. However, how to meas ure the feature relatedness is an open problem. In this section, we introduce two relatedness measures which will be used to improve feature weig hting respectively later.  X  Implicit Relatedness Measure: Feature relatedness is mined only from  X  Explicit Relatedness Measure: Semantic relatedness information is ex-3.1 Feature Weighting Based on Syntactic Information Feature weighting is usually implemented on the basis of syntactic relationship between feature and document via some statistical methods, inspired by infor-mation retrieval field. Salton et al. [2] di scussed several cons iderations. First, because terms that are frequently menti oned in an individual document appear to be useful to represent the content of t he document, terms frequency factor (e.g. normal raw Term Frequency, TF) is always used as part of the feature weighting system. Second, collection-dependent fact or (e.g. Inverse Document Frequency, IDF) is introduced to increa se the term X  X  discriminating power to pick up all the relevant documents from other irrelevant document. In general, these two factors are combined by a multiplication operation.

The most popular syntactic feature weighting strategy is TF-IDF [13] showed by Eq.(1). Here, tf ( d j ,t i ) is the frequency of term t i appears in document d j .The IDF factor varies inversely with the number df ( t i ) of documents which contain the term t i in a collection of N documents.
 TF-IDF indicates the syntactic relationship between feature and document, which is determined by taking just lexic al information into account. It can not reflect the semantic relationship betw een feature and document. Meanwhile, the weight of feature is independent on other features. 3.2 Feature Weighting Based on Local Feature Relatedness (LFR) To design a new weight which can represent the semantic relationship between term and document, we propose a semantically enriched feature weighting ap-proach by taking account of local feature relatedness  X  the feature relatedness within each individual document.

Human understand document X  X  topic through the document X  X  content  X  a sequence of terms, thus all the terms (se lected features) in one document can express the document X  X  meaning. The sem antic relationship between term and document can be calculate d by the total semantic relatedness between the term and all the terms (called contextual terms) in the document.

As the contextual terms, their import ance can be differentiate with the aid of syntactic information. In other words, contextual term with higher syntactic weight is more important to represent the document X  X  content. Thus relatedness Rel ( t k ,t i ) of each pair of terms is weighted by the syntactic weight of the con-textual term t k in document d j . The semantic feature weighting based on local feature relatedness is formulated as follows.
 where T d j means the term set of document d j , Rel ( t k ,t i ) is the relatedness between term t k and t i introduced in Section 2.2, W syn ( d j ,t k )representsthe syntactic weight of contextual term t k in document d j .

So far, we have given the new feature weighting approach. Based on this approach, term will be set to more higher weight if it is more semantically related to those important contextual terms in document. Here the importance of contextual terms for document X  X  topic d iscrimination is measured by syntactic feature weighting approach (e.g. TFIDF).
 Feature weighting based on local feature relatedness only needs to take O ( Nm 2 ), where N is the document number in the collection and m represents the average number of features in one document. Furthermore, in clustering task, the document similarity cosine measure takes O ( 1 2 N 2 m + Nm ). Here, 1 2 N 2 m is for computing the dot products of 1 2 N 2 pairs of document v ectors (the numer-ator of Eq.(3)), and Nm is for computing the L2-norms of N document vectors (the denominator of Eq.(3)). It is more fast than two existing relatedness-based document representation approaches which will be described in next section. 3.3 Combination of Syntactic and Semantic Factors In order to consider both syntactic and semantic information in text cluster-ing, we combine these two factors on the basis of document similarity following [5]. Syntactic (semantic) document similarity is computed by cosine measure as Sim ( d i ,d j )= Sim sem ( d i ,d j )when W ( d, t )= W sem ( d, t )
The overall document similarity is defined as linear combination of syntactic similarity Sim syn and semantic similarity Sim sem : To our knowledge, local feature relatedness has not been used for feature weight-ing. However, global feature relatedness and inter-document feature relatedness have appeared in the literature. 4.1 Feature Weighting Based on Global Feature Relatedness (GFR) Different from our feature weighting based on local feature relatedness, some researchers improved feature weighting by global feature relatedness  X  the re-latedness of all the feature pairs in the entire feature space. Jing et al [3] and Wang et al. [4] enriched semantic information into feature weight by multiplying document-term (concept) matrix by term (concept) correlation matrix. Their weight is calculated by Eq.(5).
 It is worth noting that T D are the term set in the entire document collection. Although both GFR and LFR utilize syntactic weight value and the feature relat-edness, GFR ignores the semantic relatedness within each individual document. This is main difference between these two weighting approaches.

GFR-based feature weighing is also a semantic feature weighting approach. In text clustering, it can be used to compute semantic document similarity, which is combined with syntactic document similarity accord to Eq.(3) and Eq.(4).
GFR needs to take O ( NMm ), where M represents the number of features (terms or concepts) in the document collect ion. Because almost all the features in the document collection have non-zero weight in each document when us-ing GFR-based feature weighting, the document similarity cosine measure takes O ( 1 2 N 2 M + NM ), which is different from O ( 1 2 N 2 m + Nm )ofLFR. 4.2 Document Similarity Based on Inter-document Feature In the literature, inter-document feat ure relatedness (the relatedness between features from different documents) was not used for feature weighting, but doc-ument similarity measure. Huang et al. [5] represented document by a set of concepts, each with a syntactic weight. Semantic relatedness between concepts was applied to the similarity between documents which is defined as: Where C d i and C d j are the concept sets in document d i and d j respectively. W represents the semantic relatedness between two concepts c k and c l ,itisequal to the corresponding Wikipedia articles X  relatedness introduced in Section 2.2. In contrast to inter-document feature relatedness, our local feature relatedness is also called intra-document feature relatedness.

Here, semantic document similarity is directly computed by Eq.(6). As pre-sented in [5], it can be further combined with syntactic similarity by Eq.(4).
To compute the semantic similarity between each two documents, it has to take O ( 2 2 N 2 m 2 ). The numerator 2 means two multiply operations are required in each loop body (see Eq. (6)). 5.1 Datasets The proposed weighting approach was tested on two real data, 20Newsgroups 1 and Reuters-21578 2 . We extracted two subsets from 20Newsgroups following [14]:  X  20NG-Multi5 was extracted from 5 categor ies (comp. graphics, rec. mo- X  20NG-Multi10 was extracted from 10 categories (alt. atheism, comp. sys. Other two data subsets were created from Reuters-21578 following [5]:  X  R-Min20Max200 consists of 25 categories with at least 20 and at most  X  R-Top10 contains 10 largest categories in the original data set. For efficiency, 20NG-Multi5, 20NG-Multi10 and R-Top10 were created by randomly picking 100 documents from eac h selected category. Table 1 lists the number of categories and documents conta ined in these subsets. In this paper, we only consider the single-label documents. Wikipedia is used as background knowl-edge base which contains 2,388,612 articles and 8,339,823 anchors in English.
In each data set, the terms were extracte d by preprocessing steps, selecting only alphabetical sequences, stemming th em, removing stop words and filtering them by the document frequency. The concepts were mapped from terms ac-cording to [7,11]. The number of terms and concepts extracted from each data set are showed in Table 1.
 5.2 Methodology In the experiments, we have two purposes. The first one is to compare the LFR-based feature weighting with existing work, including syntactic feature weighting, GFR-based feature weighting [3,4] and IFR-based document simi-larity measure [5] in clustering task. The second one is to validate whether the combination of syntactic and semantic f actors can achieve better performance. The best combined result is reported by changing the parameter  X  in Eq.(4) from 0.1 to 0.9, with 0.1 interval. Here, the syntactic weight is calculated by TFIDF. For simplicity, the abbreviations LFR, GFR and IFR are used to represent three feature relatedness-based approaches in the following parts.

We adopt Bisecting k -means (Bi-KM) algorithm which is proven to be the best document clustering technology [15]. In the experiments, Bi-KM in CLUTO 3 toolkit is used. The parameter of Bi-KM, the number of clusters, is set to be the true number of classes for each data set.

The input of Bi-KM is document similarity matrix. Different approaches tested in the experiments and their input matrices are described in Table 2. All the above approaches can be applied on Term VSM and Concept VSM. Meanwhile, the feature relatedness measure in GFR, LFR and IFR is calculated by two strategies: document collection-based implicit relatedness measure (Imp) and Wikipedia link-based explicit relatedness measure (Exp). Therefore, each approach in Table 2 is implemented in four schemas: Term+Imp, Concept+Imp, Term+Exp and Concept+Exp (In TFID F, Term+Imp is equal to Term+Exp and Concept+Imp is equal to Concept+Ex p, because TFIDF does not refer the feature relatedness measure.). 5.3 Evaluation Metrics Since the class label of each document is known in data sets, we use the external cluster validation method to evaluate the clustering results by calculating the correspondence between the clusters gen erated by clustering algorithm and the inherent classes of the data. Cluster quality is evaluated by F-Score [16] and the normalized mutual information ( NMI ) [17]. F-score combines the information of precision and recall. NMI is defined as t he mutual information between the cluster assignments and a pre-existing labeling of the dataset normalized by the arithmetic mean of the maximum possible entropies of the empirical marginal. Both the two metrics range from 0 to 1, and the higher their value, the better the clustering quality is. 5.4 Experiment Results Table 3 compares LFR with syntactic feature weighting (TFIDF) and other two feature relatedness-based approaches (GFR, IFR) in four schemas: Term+Imp, Concpet+Imp, Term+Exp and Concept+Exp. Table 4 shows the clustering results by combining syntactic factor with semantic factor in terms of docu-ment similarity. Bold-face number indicates the best result among different approaches.The X * X  X ndicatesthebestresultamongdifferentschemas.

From Table 3, we can see LFR achieves the better performance no matter which schema is adopted. The success of LFR is due to the use of the feature relatedness within each individual document. The relatedness between feature and its contextual features implies the semantic relationship between feature and document. IFR utilizes the semantic r elatedness between features from dif-ferent documents. LFR and IFR are desi gned from two different aspects, intra-document similarity and inter-document similarity. GFR updates feature weight by feature relatedness in the entire document collection. It neither implies the contextual information as LFR, nor implies the feature X  X  discriminating power as IFR. Thus GFR is worse than both LFR and IFR. Moreover, GFR is more easy to introduce noise because all the feature pairs are referred.
Comparing with syntactic feature weighting, LFR always surpasses TFIDF but GFR and IFR fail in some cases. What is the degree that LFR improves TFIDF? This is partly due to the quality of feature relatedness. In the exper-iments, feature relatedness threshold is changed from 0 to 0.3, we found the smaller threshold value can usually achieve the better performance but at the cost of time. In this paper, we report the results when threshold is set to 0. In the future, we expect to find more proper feature relatedness measure to achieve much better semantic feature weighting.

Comparing Table 4 with Table 3, we can see the clustering results with the combined method are always better than using TFIDF or LRF (GRF, IRF) solely. In other words, text clustering can be further impr oved by combining syntactic factor with semantic factor, which is enriched by feature relatedness. From Table 4, we can also see that TFIDF+LRF is usually better than both TFIDF+GRF and TFIDF+IRF.

Comparing the implicit relatedness measure with the explicit relatedness mea-sure, LFR with explicit schema surpasses implicit schema in most cases, while GFR and IFR with explicit schema is worse than implicit schema. This is an in-terest phenomenon. The explicit relatedness is calculated by mining the semantic information from Wikipedia, which contains abundant information from all kinds of domains. With aid of Wikipedia, two related terms (concepts) can get high relatedness indeed. However, terms (con cepts) which are unrelated in the data set might have also relatedness in Wikipedia. In contrast, implicit relatedness is computed according to the st atistical information fr om document collection it-self, which introduces less noise. From th e experimental results, we conclude that LFR is more robust because it refers to l ess feature pairs than GFR and IFR. Meanwhile, it takes proper advantage of feature relatedness than GFR and IFR. 5.5 Computational Complexity It is worth noticing that LFR is much faster than GFR and IFR. Given document-feature tf-idf weight matrix and feature relatedness matrix, we record the running time 4 of generating document similarity matrix using different ap-proaches in Figure 1, taking Concept+Exp schema as an example. In Figure 1, IFR indicates the time of computing document similarity matrix using Eq. (6); while LFR (GFR) includes the time of constructing LFR (GFR)-based document-feature weight matrix using Eq. (2) (Eq. (5)) and computing docu-ment similarity matrix using Eq.(3). We can see IFR is the most time-consuming among all the three approaches. This is because in IFR document semantic similarity refers to all the the feature pairs from different documents. GFR takes much more time than LFR. In GFR, the relatedness of all the feature pairs in the entire document collection are used to generate new document-feature weighting matrix which is very dense. In this process, too much multi-ply operation is required and dense matrix makes document similarity is slow to be computed. In contrast, LFR focuses on the feature relatedness within each individual document which greatly reduces the computational complexity. As mentioned earlier, the computational complexity of GFR, IFR and LFR is O ( NMm + 1 2 N 2 M + NM ), O ( 2 2 N 2 m 2 )and O ( Nm 2 + 1 2 N 2 m + Nm ) respectively. In general, M is 10 to 100 times of m .Thevalueof N and M is showed in Table 1. The obvious advantage in computational complexity makes our local feature relatedness-based feature weig hting more useful in practice. In this paper, we propose a semantic feature weighting approach by taking account of local feature relatedness  X  feature relatedness within documents. The feature relatedness can be computed by two strateg ies, document collection-based implicit relatedness measure and Wikipedia link-based explicit relatedness measure. From experimental results on real data, we conclude that: (1) The semantic feature weight-ing approach based on local feature relatedness surpasses the syntactic feature weighting (e.g. TFIDF); (2) The use of local feature relatedness surpasses global fea-ture relatedness and inter-document feature relatedness in clustering quality and computational complexity; (3) Clusterin g results can be furth er improved by com-bining syntactic factor and semantic factor which is enriched by feature relatedness. Acknowledgments. We thank Dr. David Milne for providing us with the Wikipedia Miner tool-kit. This work was supported by the Fundamental Research Funds for the Central Universities (2009YJS023, 2010RC029), the National Natu-ral Science Foundation of China (60905028, 60905029, 90820013, 60875031, 61033013), 973 project (2007CB311002, 2007CB307100, 2007CB307106) and the Project-sponsored by SRF for ROCS, SEM.

