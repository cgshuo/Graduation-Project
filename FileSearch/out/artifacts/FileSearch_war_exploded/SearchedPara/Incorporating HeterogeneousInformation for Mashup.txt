 Yao Wan 1( With the development of service oriented computing and the increasing demand complex demand of users, contributing to the boom of composite services and mashups. More and more developers are using mashup technologies to build their own services, and publishing them for other users to invoke. Statistics from ProgrammableWeb.com , a popular mashups and APIs management platform, show that the number of mashups has reached up to 6,094, and that the number of APIs has reached up to 10,634 by November 2014. In addition, the mashups are increasing on a daily basis. The rapid increase of mashups demands a sys-tematic approach to discover mashups with great accuracy and efficiency. mashup discovery, which can be roughly divided into three categories: the traditional web service discovery, mashup discovery and mashup components discovery. Firstly, some literatures such as [ 9 , 13 ] propose reasoning-based sim-ilarity algorithm to retrieve satisfied web services from the formalized descrip-tion languages such as OWL-S and WSMO. But these methods cannot be applied to mashup discovery seamlessly for the reason that there is no for-malized description for mashups. Secondly, in [ 10 , 12 ], some semantic-based approaches are used for mashup discovery, while only the semantic informa-tion of mashups is considered, neglecting the relationship between mashups and their components. Thirdly, many research works [ 3 , 11 , 14 ] focus on discovery and recommendation for mashup components. Furthermore, ProgrammableWeb.com has its own mashup search engine, but how the search algorithm works is unknown to the public. According to our study, the mashup search system of ProgrammableWeb.com only supports weak semantics searching and gives low recall, losing many relevant results with similar semantics. For example, when searching  X  X ilm X , many mashups about  X  X ovie X  will be lost, and when searching  X  X ellular phone X , no results about  X  X obile X  will be discovered.
 of their related components and the relationship between them should also be taken into consideration. Taking ProgrammableWeb.com as an example, it not only records the information of each mashup and API, but also the composition relationship between them. Figure 1 shows the detailed information of an mashup and API. Specifically, we can find that the tags, description of Mashup , as well as APIs of which the mashup is composed from the website. Based on the information of Fig. 1 , a composition network between mashups and their related APIs can be constructed. Links exist between mashups and APIs through the relation of  X  X nclude X  and  X  X ncluded by X . Besides, in our paper we will also explore the relationship between mashups. If two mashups consist of a common API, there should be a link between them. These two networks containing the two described kinds of links are called heterogeneous network in our paper. A sample heterogeneous network of ProgrammableWeb.com is shown in Fig. 2 . In this figure, G MA represents the network between mashups and APIs, G represents the network on the mashups layer.
 The aim of this paper is to improve the discovery of mashups by introducing the semantic information of mashups as well their components, and leveraging the heterogeneous information network between mashups and their components. We first construct a heterogeneous network on mashups and APIs. A probabilis-tic model is proposed to calculate the relevance score of each mashup, integrat-ing the semantic information of mashups and APIs, as well as the composition network between mashups and APIs. Furthermore, a regularized framework is proposed to ensure the consistency between mashups.
 The contribution of this paper is summarized as follows:  X  A probabilistic model is proposed to leverage the semantic information of mashups as well as their components. Besides, this model also integrate dis-covery and ranking process properly.  X  A heterogeneous information network between mashups and their components is constructed and a regularized framework with consistency hypothesis is proposed to ensure the similarity consistency between mashups.  X  We crawl 4,699 mashups and 937 related APIs from ProgrammableWeb.com to evaluate the performance of our approach. Comprehensive experiments show that our approach achieves a better performance comparing with ProgrammableWeb.com search engine and a baseline method.
 The remainder of this paper is organized as follows. Section 2 introduces the related work of this paper. A probabilistic model and a heterogeneous network based mashup discovery approach are shown in Sect. 3 . Section 4 describes the datasets we will use in our experiment and shows the experimental results and analysis. Finally, we conclude and give some future directions in Sect. 5 . In service oriented computing, discovery of appropriate web services has always been a hot research topic. A number of approaches have been proposed for service discovery. Many semantic-based approaches develop reasoning-based similarity algorithms to retrieve relevant web services described using semantic web lan-guages such as OWL-S and WSMO [ 9 , 13 ].
 With the increase of mashup services, those traditional methods of service discovery cannot be applied seamlessly to mashup discovery, since most of them only consider the information extracted from WSDL documents. Recently, more and more research works are focusing on mashup searching or mashup discovery. In [ 10 ], Li et al. provided a semantics extended framework to improve the preci-sion and recall of mashup discovery as well as to improve the performance of the mashup discovery processing time. Elmeleegy et al. [ 8 ] presented a recommen-dation tool named MashupAdvisor , which used a semantic matching algorithm and a metric planner to modify the mashup to produce the suggested output. Bianchini et al. [ 2 ] proposed a recommendation system to design mashup appli-cations based on the semantic description of mashup components, according to their similarities with designer X  X  requirements and their mutual coupling. posed. In [ 15 ], the authors combine current discovery techniques with social information as a mechanism to trade off exploration and exploitation. In [ 16 ], Zhou et al. provided an approach that learns a semantic Bayesian network with a semi-supervised learning method to build a web mashup network. Cao et al. [ 3 ] proposed a recommendation approach for mashup service that utilizes both users X  interests and the social network based on relationships among mashups, APIs and tags. The inspiration of our paper mainly stems from some research on expertise finding. In [ 7 ], Deng et al. proposed a joint regularized framework to improve expertise retrieval by modeling heterogeneous networks as regular-ization constraints. In [ 18 ], an incremental method based on multiple graphs was proposed for document recommendation in a digital library. Besides, [ 17 ] provides a strong theoretical support for learning with local and global consis-tency. Inspired by those works, our paper builds a heterogeneous social network between mashups and their components, and a regularized framework is pro-posed to ensure the consistency. 3.1 Baseline Model for Mashup Discovery The probabilistic model proposed in this subsection mainly follows the basic idea of a document-centric probatilistic model, which is proposed to estimate the expertise of a candidate by summing the relevance of its associated documents [ 7 ]. In the contex of mashup discovery, we denote the relevance score of candidate mashup m i related to a given query q as p ( m i | q ), and according to the document-centric model,the relevance score of a mashup related to a given query can be formulated as: where A m i denotes the APIs set of which mashup m i is composed of; p ( m denotes the probability of mashup m i being relevant to a given API a ; p ( q p ( q | m i ) denote the semantic similarities of API and mashup, respectively, for a given query; p ( a )and p ( m i ) can be seen as the quality of API a and mashup m , respectively;  X  (0  X   X   X  1) is a tuning parameter used to determine how much the relevance score of a candidate mashup relies on the candidate mashup itself and its API components. In this equation, the Bayes X  theorem is applied. Intuitively, we argue that when we calculate the relevance score of a mashup candidate, we should not only consider the information of the mashup, but also the information of its components.
 The right hand of Eq. ( 1 ) can be divided into two parts. The first term rep-resents the relevance score contributed by APIs by aggregating the relevance scores of APIs directly associated with a mashup. The second term denotes the relevance score from the mashup itself. Those two terms are combined by a tun-ing parameter  X  . When  X  = 0, only the information of mashup is considered, and when  X  = 1, only the information of mashup X  X  components is considered, otherwise, the semantic information of mashup as well as its components are integrated to improve the performance of mashup discovery.
 Specifically, in this model, p ( m i | a ) represents the association between the candidate and its components. Suppose that mashup m i consists of n then p ( m i | a )=1 /n m i if a is a component of m i , and zero otherwise. p ( q sures the relevance between q and API a , while p ( q | m between q mashup m i . These two probabilities can be determined by using the language model. In this paper, we use Latent semantic indexing (LSI) mothod [ 6 ] to calculate the semantic similarities between a query and APIs or mashups. Before applying LSI, some standard process of natural language processing such as case folding , tokenization , pruning , stemming and spell correcting ducted on all APIs and mashups.
 In addition, the prior probability p ( a )and p ( m ) can be viewed as the quality of an API and mashup respectively, which generally follow the uniform distribu-tion. Indeed, the quality of an API or mashup can also be set to be how popular the API or mashup is. In the context of our problem, since the popularity of mashups are difficult to measure, the qualities of mashups are set to be uni-form. However, in the information network of APIs and mashups, we define the popularity of a particular API as the number of times that it is used in the formation of mashups. For example, in Fig. 2 ,the  X  X witter API X  mashups, then the popularity of  X  X witter API X  is 4. From our analysis, we can find the distribution of API popularity is long tailed [ 1 ]. So we estimate p ( a )by the logrithm of the popularity of API.
 For simplicity, let x be the relevance vector between API a x = p ( q | a i ), y be the relevance vector between mashup m y = p ( q | m i ), Q A and Q M be the diagonal matrix which represent the quality of APIs and mashups respectively, and P MA be the composition matrix between mashups and APIs. The primary model as shown in Eq. ( 1 ) can be rewritten as: where z represents the relevance score vector of all candidate mashups, and  X  is the tuning parameter which controls the weight between the mashups and their components. 3.2 Heterogeneous Information Incorporation In the previous subsection, we utilize the description of mashups and their com-ponents to measure the relevance of candidate mashups for a given query. In addition to the textual document information, some information of the hetero-geneous network should also be considered. In this subsection, we will describe a mashup consistency hypothesis and enforce the hypotheses by defining regu-larization constraints.
 Mashup Consistency Hypothesis: If two mashups share many common ser-field should be similar in some sense. As shown in Fig. 2 , mashup m posed of  X  X oogle Maps API X  ,  X  X oogle Calendar API X  and  X  X witter API X  mashup m 4 is composed of  X  X witter API X  and  X  X oogle Calendar API X  two mashups share two common APIs, so we can consider that their functional-ity and quality are similar in some sense, and their relevance for a given query should also be similar.
 tion constraints. Suppose we are given a mashup graph G M is a weighted undirected graph. Suppose that the pairwise similarities among the mashups are described by matrix S M  X  R | M | X | M | measured based on G Thus, we formulate to minimize a regularization loss function as follows: where  X &gt; 0 is the regularization parameter. The first term of the loss function defines the mashup consistency , which prefers small difference in relevance scores between nearby mashups; the second term is the fitting constraint the difference between final scores z and the initial relevance scores z initial relevance score vector z 0 can be calculated according to Eq. ( 2 )inthe probabilistic model. Setting  X  X  ( z ) / X  z = 0, we can see that the solution z essentially the solution to the linear equation: where  X  =1 / (1 +  X  ). Since the matrix S M is usually very sparse, calculating the inversion of S M is of high time complexity. One solution to the above equation is using a powerful iterative method [ 17 ]: where  X  =1 / (1 +  X  ), z  X  = z  X  is the solution. Now the interesting question is how to calculate S M among set M . For graph data, a number of works [ 4 ] have been given on obtaining the similarity measures. For undirected graph, S M is simply the normalized adjacency matrix W : where W is the adjacency matrix of mashups in G M , W ij = 1 if node i is linked to node j , otherwise, W ij =0,and  X  is a diagonal matrix with  X  3.3 Implementation Figure 3 shows the framework of the proposed heterogeneous information net-work based mashup discovery, which integrates the semantic information of mashups and their components, as well as the similarity consistency between mashups. The proposed framework is comprised of three components: neous network construction , data processing ,and mashup ranking tion of heterogeneous network and data processing can be performed offline, while the mashup ranking part should be conducted online according to the query specified by a user.
 4.1 Experimental Setup Dataset. ProgrammableWeb.com is one of the most popular platforms that has collected lots of APIs and mashups used in Web and mobile applications. To evaluate our proposed approach, we crawl all the mashups and their related APIs from ProgrammableWeb.com as we can.
 collection. After the construction of the heterogeneous graph, we observe that there are many edges on the mashup graph, while relatively few edges in the graph of mashups and APIs. As for G M the density of matrix is nearly 17 . 0%, while for G A the density of matrix is only 0 . 18 %.
 Evaluation Metrics. For the evaluation, several categories of Web search eval-uation metrics are used to measure the performance of our proposed model from different aspects, including some relevance based metrics, ranking based met-rics and diversity based metrics. To measure the relevance of our search results, we use the precision at rank k ( P @ K ) which is widely used and is defined as: retrieved results that are relevant for the given query. From the ranking aspect, we use Mean Reciprocal Rank (MRR) to evaluate the ranking of our search results. A larger MRR value means a better result. The MRR is defined as: search results should not only have a high precision and reasonable ranking, but also have a high diversity. Following [ 5 ], we use the  X  -DCG metric to mea-sure the novelty and diversity of our retrieved results. The  X  -DCG is defined n is the total number of topics the searching results contains; J ( d result m i contains topic j , otherwise, J ( m i ,j )=1. i the degree of diversity and novelty of the searching result;  X  is a parameter. In our model,  X  is set to be 0 . 5. 4.2 Comparison In this subsection, comparisons between our method and the following approaches have been made to show the effectiveness of our proposed approach.  X  PW Search Engine :The ProgrammableWeb.com has its own search engine for mashups and APIs discovery. From our observation, we find that the search results is mostly based on the name, description and tags/categories of mashups/APIs. In addition, the search results are all ranked by their updated date.  X  MD-Sim : This method which has been used in many state-of-the-art works just utilizes the semantic information of mashups. The semantic similarities between mashups and query are calculated by the LSI method.  X  MD-Sim+ : Compared with the above two methods, our probabilistic model (MD-Sim+) employs the semantic information of mashups, as well as the semantic information of the related APIs which are the components of mashups. The qualities of related APIs are also introduced in this approach.  X  MD-HIN : This method is the extended version of the probabilistic model (MD-Sim+). It proposes a consistency hypothesis on mashups firstly, and a regularization constraints is employed.
 Before comparing the performance of the above four methods, several points should be made clear first. Since there is no published ground truth for compar-ison, we select twenty queries of different topics to evaluate the performance on the above metrics. According to the search results, if the categories of mashup are related to a query, then the mashup will be considered to be relevant with the query. We judge the degree of relevance by the number of followers of mashups. Since most mashups have more than one tags, we will use the tags to evaluate the diversity of results. The parameter settings of our approaches are  X  =0 . 4,  X  =0 . 5, # iteration = 100, and topic number of LSI is set as 20. The experi-mental results are shown in Table 1 , and the detailed investigations of parameter settings will be provided in Sects. 4.3 and 4.4 .
 Based on the results in Table 1 , we have the following observations:  X  From the perspective of P @ K , when the value of K is small, the perfor-mance of ProgrammableWeb.com search engine is a litter better than our approach (MD-HIN). While when the value of K is large, our approach has a great advantage over ProgrammableWeb search engine. This is because as
K increases, the ProgrammableWeb.com search engine is unable to discover so many mashups, which just depends on the utilization of the mashup infor-mation, while our approach can find more related mashups by incorporating the mashups and APIs X  information and leveraging the heterogeneous social network between them.
  X  From the perspective of ranking of results, our extented approach (MD-HIN) achieves better performance than the probabilistic approach (MD-SIM) since introducing the quality of APIs along with the similarity consistency on mashup social network, making sure that the mashups with similar quality will have similar ranking scores.  X  Among all the discovery methods, our proposed method (MD-HIN) generally achieves better performance on both P @20, MRR and  X  -DCG , indicating that integrating the semantic information of mashups with APIs, and consid-ering the similarity consistency on mashup social network will facilitate and improve the discovery of mashups. These experimental results demonstrate that our model leveraging the heterogeneous social network is practical and effective. 4.3 Impact of  X  In our model, the parameter  X  controls how much our method relies on the semantic information of mashups and their related APIs. To study the impact of  X  on P @20, MRR and  X  -DCG ,wevary  X  from 0 to 1 with a step value 0 . 1. The experimental results are shown in Fig. 4 . Figure 4 (a) shows that optimal  X  value settings can achieve better performance of mashup discovery, which demonstrates that fusing the information of mashups and their related APIs with our proposed approach will improve the discovery accuracy. As  X  increases, the P @20 value increases at first, but when  X  surpasses a certain threshold, the P @20 value decreases with further increase of the value of  X  . This phenomenon confirms the intuition that purely using the semantic information of mashups or purely employing the semantic information of their related APIs cannot generate better performance than fusing these two factors together. From Fig. 4 (b) and (c), we can find that the value of  X  also has an impact on the MRR and  X  -DCG although the impact is small. This demonstrates that when we introduce the semantic information and quality of APIs, the ranking and diversity of search results will be improved. 4.4 Impact of  X  In our model,  X  =1 / (  X  +1) where  X  is a regularization parameter which controls the difference between final score z and the initial score z of  X  on the metrics of our approach, we change  X  from 0 to 1 with a step value 0 . 1. We set  X  =0 . 4, Top-K=20, and # iteration = 100 in this experiment. From Fig. 5 (a), we can find that value  X  has a significant impact on the precision of the discovery results. When  X  increases to 1, the P @20 will decrease rapidly. This is because that when  X  is near to 1,  X  is near to 0, in this condition, we only consider the similarity constraints of mashups, neglecting the constraint that the final score value should be fitting to the initial value. Figure 5 (b) and (c) show that the value of  X  still has an effect on MRR and  X  -DCG , although the effect is not obvious. Based on some traditional semantic-based service discovery methods, we pro-pose an approach to improve mashup discovery by integrating the semantic information of mashups and their related APIs. Besides, a similarity consis-tency between mashups is proposed and a regularization framework is employed to achieve better performance. Comprehensive experiments on a real-world ProgrammableWeb.com dataset are conducted, and the extensive experimental analysis shows the effectiveness of our approach.
 Although the data crawled from ProgrammableWeb.com is sufficient for eval-uation purpose, we believe that there is a necessity for an experiment on tra-ditional Web services described by a standard language such as WSDL. In our future work, we plan to build a social network on traditional Web services and verify our model. In addition, we will extend our ground truth with more queries, and more information retrieval evaluation metrics will be introduced to enhance the quality of our work. Furthermore, we are going to conduct more research on the social network, a more complex social network including service users will be built.

