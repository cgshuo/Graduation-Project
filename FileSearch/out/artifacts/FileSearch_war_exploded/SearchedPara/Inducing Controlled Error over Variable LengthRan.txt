 The robustness of a modeling or prediction system is defined as the system X  X  ability to handle noise or error applied to its input, and operate within certain limits. For example, if we have a classification system that predicts a state, given a set of observations, we can measure its accuracy by examining how many of its predictions are correct. We can then measure the robustness of the system by applying a specified level of random noise to the observations, and then examine its change in accuracy. Of course, if we in creased the level of the added noise, we would expect the accuracy of the system t o decrease, but a more robust system would provide a slower decrease.

To determine the robustness level of a system, we must often perform a sim-ulation experiment, where we can control the noise. Noise is usually randomly sampled from a predefined distribution with carefully chosen parameters to en-sure the noise is consistent for the experi ment. For example, if our observations are elements of the real number set, we may generate noise using a Normal dis-tribution with zero mean and variance of one. If our observations are frequency values, we may generate noise using a Poisson distribution with mean 1. Fur-ther experiments can then be performed by adjusting the variance of the noise distribution and measuring the change in accuracy.

Many systems, such as collaborative filtering systems, meta search engines, query expansion systems, and rank aggregation systems require a set of ranked items as input.

Therefore, to measure the robustness of such systems, we can randomly per-mute the lists to obtain a given expected Kendall X  X   X  between the permuted and unpermuted lists. We can achieve this by performing a set number of ran-domly chosen adjacent transpositions, as long as all of the list lengths are the same. If the lists lengths are not the same, it is not clear how many adjacent transpositions we should apply to obtain a given expected Kendall X  X   X  across all lists.

In this article, we investigate how to compute the number of random adjacent transposition required to obtain a given expected  X  for a given list length. We find that this is a very computationally expensive task. We also propose an alternate method of inducing error in ranked lists called Gaussian Perturbation. We show that its parameter is a function of the rank correlation reduction caused by the error, and therefore can be used to induce controlled noise in ranked lists. We provide the following contributions:  X  An analysis of the relationship between the expected Kendall X  X   X  ,thenumber  X  A novel ranked list noise induction method called Gaussian Perturbation  X  An approximate version of Gaussian Perturbation to compute the required
The article will proceed as follows: Sect ion 2 reviews how we measure the ro-bustness of a system, and examines the form of Kendall X  X   X  . Section 3 examines the noise induced using random adjacent transpositions, introduces and anal-yses Gaussian Perturbation, and provides a faster approximation to Gaussian Perturbation. To assess the robustness of a prediction system using simulation, we must define a method of inducing controlled noise into the observation space. In this section, we will examine how to measure robustness, given a noise distribution. We will then examine how to use Kendall X  X   X  rank correlation to measure the induced noise. 2.1 Measuring System Robustness A robust system is one that can function within a set of predefined limits in a noisy (error prone) environment. Therefore a robust classification system would be able to provide a certain level of accuracy, when making predictions based on observations, with a given level of noise. For example, if our observation space is the one dimensional real line R , we might define an unknown true value as a  X  R , and an observation with added noise as  X  a = a +  X  where  X   X  N (0 , X  ), a sample from a Normal distribution, with mean 0 and standard deviation  X  .
In this situation the level of noise is controlled by the parameter  X  . As the dif-ference between the noisy and noise free observations increase, we would expect the system prediction accuracy to chan ge, but we would expect a more robust system X  X  behaviour to change less.

Analysis of robust system behaviour has provided us with strategies that allow prediction models to provide good accuracy using a wide range of observa-tion input data. Some well known general strategies are to use cross validation when training a model [12] and including a regularisation term in the optimisa-tion function (used in SVMs [13] and Lasso/Ridge regression [7]). Such systems introduce bias into the optimisation, in order to reduce the variance of the clas-sification, and hence in crease the robustness.

Similar methods can be used in clustering and dimension reduction. Com-pressive sampling has been used in image analysis [2], clustering [11] and outlier detection [1]. Each of these methods use l 1 regularisation to obtain sparse so-lutions to the dimension reduction, clustering, and outlier detection problems. These methods have also been applied to Principal Component Analysis [3], to obtain biased, but more robust principal components.

Now consider the observation space as the set of all ranked lists of multiple lengths. There are many systems that have an observation space of ranked lists, therefore it is important that we provide a method of measuring the robustness of this space. Systems using Collaborati ve filtering [9] use an observation space of ranked lists. These systems obtain ranked responses from the user community and aggregate them to assist in the decision making process. Query expansion [4] also requires ranked lists, where ranked documents are used to extract potential query terms. Meta search over multiple dat abases [8,6] obtain ranked results from various databases (e.g. airline travel prices, book prices, Web search results) and combines them to form a single result. Also, Rank aggregation systems [5,10] are used to combine multiple ranked lists into a single ranked list (e.g. results from tennis competitions to form an overall player ranking).

To test the robustness of methods applied in such systems, we need a method to induce noise into ranked lists. Furthermore, we must be able to control the amount of noise induced. Ranked lists contai n ordinal numbers, therefore the error term  X  would be the application of a random permutation of the list ele-ments (clearly, simply adding an error variate to the true ranking will not be appropriate for ordinal numbers). 2.2 Measuring Error in Ranked Lists The ranking of n items can be identified with an ordering of the integers 1 ,...,n . Thus the sample space for ranked lists can be considered S n ,thesetofpermu-tations of the integers 1 to n . We will talk about elements x =( x 1 ,...,x n )of S ,where x i is the rank of item i . Note that we cannot induce error on lists of length n = 1, therefore, we will only consider n&gt; 1 in this article.
Given two rankings x and y  X  S n , one way to measure the similarity of the two rankings is using Kendall X  X   X  rank correlation. Kendall X  X   X  is defined as: where the sum is over the set of all possible pairs of items being ranked, and and d ij =1  X  c ij . A pair of items are concordant ( c ij = 1) if their ordering in each of the two lists matches, otherwise they are discordant ( d ij =1).We can see that  X  = 1 if and only if x = y (all items are concordant), implying perfect correlation. Also  X  =  X  1 if and only if x = reverse( y ) (all items are discordant), implying perfect anti-correlation. The result of  X  = 0 implies no correlation between x and y . Therefore, Kendall X  X   X  can be simplified to:
We can use Kendall X  X   X  to measure the noise induced in a ranked list. Given ranked lists x , y  X  S n , the level of noise between x and y is measured using  X  ( x , y ). The greater the value of  X  , the lower the amount of noise.
Since Kendall X  X   X  is a measure of correlation, it is comparable across lists of different lengths. This means that if we induce noise on a list of length n 1 and on another of length n 2 , where the measure of noise using Kendall X  X   X  is the same for both, then we have induced the same level of noise for both lists. To examine the robustness of a system, we must examine how it performs when noise is introduced. If the system takes a set of ranked lists as input, we must induce controlled noise in the ranked lists. We showed that error in ranked lists can be measured using Kendall X  X   X  , which is a function of the permutation required to remove error from the erroneous ranked list. Therefore, to induce controlled noise, we must perfo rm controlled permutations.

Ideally, supposing the truth to be x  X  S n , we would weight all elements y  X  S n such that the expected  X  is as desired. We can then sample from the set S n with probability proportional to the assigned weights. However, there are n ! elements in S n and enumeration rapidly becomes impractical.

In this section, we examine two methods of sampling from S n to obtain an expected  X  ; one controlled by the number of transpositions t , the other controlled by the standard deviation  X  . 3.1 Using Adjacent Transpositions to Induce Controlled Noise When measuring error using Kendall X  X   X  , an obvious choice of inducing error is to perform adjacent transpositions. The set of adjacent transpositions is a generating set for the symmetric group, therefore we can obtain every possible ranking of n items using a finite sequence of adjacent transpositions.
To induce error in a ranked list x of length n , we randomly select x i ,where i  X  X  1 , 2 ,...,n  X  1 } and transpose it with the adjacent item x t random adjacent transpositions, we obtain a permuted list y , which when com-pared to the original list x , gives a value of  X  . If we repeat this random process many times, we find that we obtain a distribution over  X  , that is dependent on n and t .

To compute the expected Kendall X  X   X  after t random adjacent transpositions, we first construct the probability transition matrix containing the probability of moving from one state to another in one adjacent transposition. The state of the list is the order of the items in the list. Therefore, if there are n items in the list, there are n ! possible states. The probability transition matrix will be of size n !  X  n !, but each column will contain only n  X  1 nonzero elements (since for any list of n items, we can only move to n  X  1otherstatesusingasingle adjacent transposition). This gives us a probability transition matrix containing n !  X  ( n  X  1) nonzero elements.

For example, if n =3,wehavethe n ! = 6 possible states x 1 =(1 , 2 , 3), x 2 =(1 , 3 , 2), x 3 =(3 , 1 , 2), x 4 =(3 , 2 , 1), x 5 =(2 , 3 , 1) and x 6 =(2 , 1 , 3) giving a probability transition matrix T with n !( n  X  1) = 12 nonzero values: where t i,j , the elements of T , contain the probability of moving from state j to p 0 =[100000] . By taking a random walk of length 1, we compute our new state probability as p 1 = T p 0 =[00 . 50000 . 5] . A random walk of length 2 is computed as p 1 = T p 1 = T 2 p 0 =[0 . 500 . 2500 . 25 0 ] . A random walk of length n gives us the state distribution T n p 0 . Once we have the probability of each state after t random adjacent transpositions, we can compute the expected Kendall X  X   X  using: where p t,i is the i th element of p t . If we perform two random transpositions (a random walk of length 2), on a list of length n = 3, we can obtain a Kendall X  X   X  of 0 or  X  1 / 3, but the expected Kendall X  X   X  is: By representing the problem as a random walk on an undirected graph, we ob-tain additional information about its stationary distribution, being proportional to the degree of each vertex over an undirected graph. The degree of each vertex over the set of permutations is equal ( n  X  1), therefore the stationary distribu-tion is the Uniform. This implies that as the number of adjacent transpositions approaches infinity, each state is equally likely. Kendall X  X   X  is symmetric about 0 over all permutation states, therefore the expected Kendall X  X   X  approaches 0 as t approaches infinity for all lists of length n&gt; 1.

Given the task of randomly sampling ranked lists of length n with a given expected  X  error, it is not obvious how we should choose t . In fact there is no way to directly compute t , other than trial and error (set t and n and examine the associated expected  X  ). To achieve this task, we have provided Table 2 con-taining the computed expected  X  values of lists of length 2 to 9, using 1 to 50 random adjacent transpositions. So given E [  X  ]=0 . 5952, the table shows that we are required to perform 13 random adjacent transpositions for n =8.Ifwe induce error in lists of length 7 and 9, we find that 9 and 18 random adjacent transpositions will provide the wanted E [  X  ] respectively.
 We have also provided the computation time for each of the lists in Table 1. It is interesting to note how fast the number of nonzero elements and computa-tion time grows as n increases. Based on the trend, we found that the time is increasing double exponentia lly, meaning that the expected  X  for n =10would take approximately 100 days to compute. Clearly, it is not feasible to compute the expected  X  using this method for lists of length 10 or more. 3.2 Using Gaussian Perturbation to Induce Controlled Noise Rather than performing adjacent transpo sitions, controlled noise can be induced in ranked lists by perturbing the rank of t he elements. Perturbation requires the introduction of a latent value for each item i as a sample from a Normal distribution X i  X  N (  X  = x i , X  ), with mean equal to its rank x i and constant standard deviation  X  .

Once we have sampled a value for each item, we generate the new rank, by ordering the latent values. An example of this process is shown in Figure 1. We can see in this example four items (1 , 2 , 3 , 4), each having a Normal distribution with equal standard deviation, centred on its rank. A sample from these four list (1 , 3 , 2 , 4). Using this method of noise induction, it is more likely that each item moves a smaller number of ranks than one item move a large number, which is the typical form of error seen in ranked lists.

Using Gaussian Perturbation, we can compute the expected  X  as a function of n and  X  . The expected value of  X  is given as: showing that it is dependent on the expected concordance of each pair of items. Let us consider x the ranked list of length n , and the permuted list y ,based on latent values  X  y , which are realisations of Normal random variables  X  Y .Given two items i and j ,where x i &lt;x j in ranked list x , then the pair is concordant if y &lt;y j which happens if and only if  X  y i &lt;  X  y j .Now; from a Normal distribution with mean  X  = x i  X  x j , and standard deviation After standardising, we obtain: where Z  X  N (0 , 1) is the standard Normal distribution. Substituting equation 4 back into 3 gives: By noticing that x j  X  x i is an integer from 1 to n  X  1, and the sum involves several copies of each such integer, we can simplify the equation further: Since  X &gt; 0and n&gt; 1, each term of the sum in equation 5 is non-negative, are zero, implying that E [  X  | n,  X  ]  X  0 if and only if  X   X  X  X  .

Using equation 5, we can compute the value of  X  for a given E [  X  ]and n using a one-dimensional optimisation function. For example, if we want to induce noise so that E [  X  ]=0 . 6, we find that we must assign  X  =22 . 46, 44 . 48 and 110 . 54 for n = 100, 200 and 500 respectively.

Table 3 provides us with the computation time to perform the one-dimensional optimisation to compute  X  . The table shows us that the time to compute  X  increases linearly with n . When comparing this to Table 1, we see that Gaussian Perturbation has benefit over the Adjacent Transposition sampling method in terms of the parameter computation time. 3.3 Estimating  X  for Gaussian Perturbation In the previous section, we derived the equation for E [  X  ], dependant on n and  X  , and stated that we had to run a one dimensional optimisation over the function to compute  X  when given E [  X  ]and n . The computation of  X  would be faster if we had a formula that gives the appropriate  X  in terms of n and E [  X  ]. Unfortunately,  X  is embedded in the Normal cumulative density function (CDF) and summed n  X  1 times.

To allow inversion of equation 5 we have made use of the Shah piece-wise approximation [14] to the Standard Normal CDF: for x  X  0. The similarity of the Shah approximation to the Standard Normal CDF is shown in Figure 2.

If we assume that ( n  X  1) / ( the first piece of the Shah approximation x (4 . 4  X  x ) / 10+1 / 2. By making this substitution into equation 5 and simplifying, we obtain the expected  X  approxi-mation: whichisaquadraticequationintermsof1 / X  .Bysolvingfor  X  , we obtain: can compute an approximate  X  for all values of n when 0 &lt; E [  X  ] &lt; 0 . 6453, and greater values of E [  X  ]as n decreases. Also note that  X   X  X  X  as E [  X  ]  X  0, as shown in equation 5.

Of the two solutions, the negative form provides accurate estimates for most of the  X ,n combinations, but the positive form provides poor estimates. To examine the estimate X  X  accuracy, w e chose a desired value of E [  X  ], used equation 7 to compute the approximate  X  , then used equation 5 to compute the obtained E [  X  ]. Figure 3 shows the absolute difference in desired E [  X  ] compared to the obtained E [  X  ], when using an approximate  X  . We can see that that the  X  estimate is a good estimate since most of the error is smaller than 0 . 01.

When using equation 7 to compute  X  the computation time is less that 10  X  5 seconds for all n . To examine the robustness of systems that take ranked lists as input, we can induce noise by applying a set number of random adjacent transpositions, where the error is measured using Kendall X  X   X  rank correlation. For a fixed list length n , we can ensure a fixed expected  X  by keeping the number a random adjacent transpositions constant, allowing a consistent level of noise to be applied to all lists. If the lists are of varying length, it is not clear how many adjacent transposition should be applied to each list to obtain a consistent expected  X  over all lists.

In this article, we examined the r elationship between the expected  X  , the list length n and the number of random adjacent transpositions t . We found that it is possible to compute the expected  X  ,given n and t , but the computation time rapidly increases with n , making the computation infeasible for n&gt; 9.
We also proposed a new method of inducing noise in ranked lists called Gaus-sian Perturbation, which allows us to derive an equation for the expected  X  when given n and its parameter  X  . The parameter  X  can be computed in reasonable time for large lists (just over six minutes for lists of length 10 6 ), allowing us to compute  X  for various list lengths while keeping the expected  X  constant over all lists.

We also provided an approximate solution for  X  that requires less that 10  X  5 seconds of computation time for all n .

