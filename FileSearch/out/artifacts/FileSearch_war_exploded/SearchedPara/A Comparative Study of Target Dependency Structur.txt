 Target language side dependency structures have been successfully used in statistical machine trans-lation (SMT) by Shen et al. (2008) and achieved state-of-the-art results as reported in the NIST 2008 Open MT Evaluation workshop and the NTCIR-9 Chinese-to-English patent translation task (Goto et al., 2011; Ma and Matsoukas, 2011). A primary ad-vantage of dependency representations is that they have a natural mechanism for representing discon-tinuous constructions, which arise due to long-distance dependencies or in languages where gram-matical relations are often signaled by morphology instead of word order (McDonald and Nivre, 2011).
It is known that dependency-style structures can be transformed from a number of linguistic struc-tures. For example, using the constituent-to-dependency conversion approach proposed by Jo-hansson and Nugues (2007), we can easily yield de-pendency trees from PCFG style trees. A seman-tic dependency representation of a whole sentence, predicate-argument structures (PASs), are also in-cluded in the output trees of (1) a state-of-the-art head-driven phrase structure grammar (HPSG) (Pol-lard and Sag, 1994; Sag et al., 2003) parser, Enju 1 (Miyao and Tsujii, 2008) and (2) a state-of-the-art CCG parser 2 (Clark and Curran, 2007). The moti-vation of this paper is to investigate the impact of these non-isomorphic dependency structures to be used for SMT. That is, we would like to provide a comparative evaluation of these dependencies in a string-to-dependency decoder (Shen et al., 2008). 2.1 Dependency tree We follow the definition of dependency graph and dependency tree as given in (McDonald and Nivre, 2011). A dependency graph G for sentence s is called a dependency tree when it satisfies, (1) the nodes cover all the words in s besides the ROOT; (2) one node can have one and only one head (word) with a determined syntactic role; and (3) the ROOT of the graph is reachable from all other nodes.
For extracting string-to-dependency transfer rules, we use well-formed dependency structures, either fixed or floating, as defined in (Shen et al., 2008). Similarly, we ignore the syntactic roles both during rule extracting and target dependency language model (LM) training. 2.2 Dependency parsing Graph-based and transition-based are two predom-inant paradigms for data-driven dependency pars-ing. The MST parser (McDonald et al., 2005) and the Malt parser (Nivre, 2003) stand for two typical parsers, respectively. Parsing accuracy comparison and error analysis under the CoNLL-X dependency shared task data (Buchholz and Marsi, 2006) have been performed by McDonald and Nivre (2011). Here, we compare them on the SMT tasks through parsing the real-world SMT data. 2.3 PCFG parsing For PCFG parsing, we select the Berkeley parser (Petrov and Klein, 2007). In order to generate word-level dependency trees from the PCFG tree, we use the LTH constituent-to-dependency conversion tool 3 written by Johansson and Nugues (2007). The head finding rules 4 are according to Magerman (1995) and Collins (1997). Similar approach has been orig-inally used by Shen et al. (2008). 2.4 HPSG parsing In the Enju English HPSG grammar (Miyao et al., 2003) used in this paper, the semantic content of a sentence/phrase is represented by a PAS. In an HPSG tree, each leaf node generally introduces a predicate, which is represented by the pair made up of the lexical entry feature and predicate type fea-ture. The arguments of a predicate are designated by the arrows from the argument features in a leaf node to non-terminal nodes (e.g., t0  X  c3, t0  X  c16). Since the PASs use the non-terminal nodes in the HPSG tree (Figure 1), this prevents their direct us-age in a string-to-dependency decoder. We thus need an algorithm to transform these phrasal predicate-argument dependencies into a word-to-word depen-dency tree. Our algorithm (refer to Figure 1 for an example) for changing PASs into word-based depen-dency trees is as follows: 1. finding , i.e., find the syntactic/semantic head 2. mapping , i.e., determine the arc directions 3. checking , i.e., post modifying the dependency
Table 1 lists the mapping from HPSG X  X  PAS types to word-level dependency arcs. Since a non-terminal node in an HPSG tree has two kinds of heads, syn-tactic or semantic, we will generate two dependency graphs after mapping. We use  X  X AS+syn X  to repre-sent the dependency trees generated from the HPSG PASs guided by the syntactic heads. For semantic heads, we use  X  X AS+sem X .
 For example, refer to t0 = when in Figure 1. Its arg1 = c16 (with syntactic head t10), arg2 = c3 (with syntactic head t6), and PAS type = conj arg12. In Table 1, this PAS type corresponds to arg2  X  pred  X  arg1, then the result word-level de-pendency is t6(is)  X  t0(when)  X  t10(is).

We need to post modify the dependency graph af-ter applying the mapping, since it is not guaranteed to be a dependency tree. Referring to the definition of dependency tree (Section 2.1), we need the strat-egy for (1) selecting only one head from multiple heads and (2) appending dependency relations for those words/punctuation that do not have any head. When one word has multiple heads, we only keep one. The selection strategy is that, if this arc was deleted, it will cause the biggest number of words that can not reach to the root word anymore. In case of a tie, we greedily pack the arc that connect two words w i and w j where | i  X  j | is the biggest. For all the words and punctuation that do not have a head, we greedily take the root word of the sentence as their heads. In order to fully use the training data, if there are directed cycles in the result dependency graph, we still use the graph in our experiments, where only partial dependency arcs, i.e., those target flat/hierarchical phrases attached with well-formed dependency structures, can be used during transla-tion rule extraction. 2.5 CCG parsing We also use the predicate-argument dependencies generated by the CCG parser developed by Clark and Curran (2007). The algorithm for generating word-level dependency tree is easier than processing the PASs included in the HPSG trees, since the word level predicate-argument relations have already been included in the output of CCG parser. The mapping from predicate types to the gold-standard grammat-ical relations can be found in Table 13 in (Clark and Curran, 2007). The post-processing is like that de-scribed for HPSG parsing, except we greedily use the MST X  X  sentence root when we can not determine it based on the CCG parser X  X  PASs. 3.1 Setup We re-implemented the string-to-dependency de-coder described in (Shen et al., 2008). Dependency structures from non-isomorphic syntactic/semantic parsers are separately used to train the transfer rules as well as target dependency LMs. For intu-itive comparison, an outside SMT system is Moses (Koehn et al., 2007).

For Chinese-to-English translation, we use the parallel data from NIST Open Machine Translation Evaluation tasks. The training data contains 353,796 sentence pairs, 8.7M Chinese words and 10.4M En-glish words. The NIST 2003 and 2005 test data are respectively taken as the development and test set. We performed GIZA++ (Och and Ney, 2003) and the grow-diag-final-and symmetrizing strategy (Koehn et al., 2007) to obtain word alignments. The Berkeley Language Modeling Toolkit, berkeleylm-1.0b3 5 (Pauls and Klein, 2011), was employed to train (1) a five-gram LM on the Xinhua portion of LDC English Gigaword corpus v3 (LDC2007T07) and (2) a tri-gram dependency LM on the English dependency structures of the training data. We re-port the translation quality using the case-insensitive BLEU-4 metric (Papineni et al., 2002). 3.2 Statistics of dependencies We compare the similarity of the dependencies with each other, as shown in Table 2. Basically, we in-vestigate (1) if two dependency graphs of one sen-tence share the same root word and (2) if the head of one word in one sentence are identical in two depen-dency graphs. In terms of root word comparison, we observe that MST and CCG share 87.3% of iden-tical root words, caused by borrowing roots from MST to CCG. Then, it is interesting that Berkeley and PAS+syn share 74.8% of identical root words. Note that the Berkeley parser is trained on the Penn treebank (Marcus et al., 1994) yet the HPSG parser is trained on the HPSG treebank (Miyao and Tsujii, 2008). In terms of head word comparison, PAS+syn and PAS+sem share 79.1% of identical head words. This is basically due to that we used the similar PASs of the HPSG trees. Interestingly, there are only 59.3% identical root words shared by PAS+syn and PAS+sem. This reflects the significant difference be-tween syntactic and semantic heads.

We also manually created the golden dependency trees for the first 200 English sentences in the train-ing data. The precision/recall (P/R) are shown in Table 3. We observe that (1) the translation accura-cies approximately follow the P/R scores yet are not that sensitive to their large variances, and (2) it is still tough for domain-adapting from the treebank-trained parsers to parse the real-world SMT data. PAS+syn performed the best by avoiding the errors of missing of arguments for a predicate, wrongly identified head words for a linguistic phrase, and in-consistency dependencies inside relatively long co-ordinate structures. These errors significantly influ-ence the number of extractable translation rules and the final translation accuracies.

Note that, these P/R scores on the first 200 sen-tences (all from less than 20 newswire documents) shall only be taken as an approximation of the total training data and not necessarily exactly follow the tendency of the final BLEU scores. For example, CCG is worse than Malt in terms of P/R yet with a higher BLEU score. We argue this is mainly due to that the number of illegal dependency trees gener-ated by Malt is the highest. Consequently, the num-ber of flat/hierarchical rules generated by using Malt trees is the lowest. Also, PAS+sem has a lower P/R than Berkeley, yet their final BLEU scores are not statistically different. 3.3 Results Table 3 also shows the BLEU scores, the number of flat phrases and hierarchical rules (both integrated with target dependency structures), and the num-ber of illegal dependency trees generated by each parser. From the table, we have the following ob-servations: (1) all the dependency structures (except Malt) achieved a significant better BLEU score than the phrasal Moses; (2) PAS+syn performed the best in the test set (0.3376), and it is significantly better than phrasal/hierarchical Moses ( p &lt; 0 . 01 ), MST and CCG ( p &lt; 0 . 05 ); and (3) CCG performed as well as MST and Berkeley. These results lead us to argue that the robustness of deep syntactic parsers can be advantageous in SMT compared with tradi-tional dependency parsers. We have constructed a string-to-dependency trans-lation platform for comparing non-isomorphic tar-get dependency structures. Specially, we proposed an algorithm for generating word-based dependency trees from PASs which are generated by a state-of-the-art HPSG parser. We found that dependency trees transformed from these HPSG PASs achieved the best dependency/translation accuracies. We thank the anonymous reviewers for their con-structive comments and suggestions.

