 Taco Cohen T.S.C OHEN @ UVA . NL Max Welling M.W ELLING @ UVA . NL Machine Learning Group, University of Amsterdam Recently, the field of deep learning has produced some re-markable breakthroughs. The hallmark of the deep learn-ing approach is to learn multiple layers of representation of data, and much work has gone into the development of rep-resentation learning modules such as RBMs and their gen-eralizations (Welling et al., 2005), and autoencoders (Vin-cent et al., 2008). However, at this point it is not quite clear what characterizes a good representation. In this paper, we take a fresh look at the basic principles behind unsuper-vised representation learning from the perspective of Lie group theory 1 .
 Various desiderata for learned representations have been expressed: representations should be meaningful (Bengio &amp; Lecun, 2014), invariant (Goodfellow et al., 2009), ab-stract and disentangled (Bengio et al., 2013), but so far most of these notions have not been defined in a mathe-matically precise way. Here we focus on the notions of in-variance and disentangling, leaving the search for meaning for future work.
 What do we mean, intuitively, when we speak of invari-ance and disentangling? A disentangled representation is one that explicitly represents the distinct factors of varia-tion in the data. For example, visual data (i.e. pixels) can be thought of as a composition of object identity, position and pose, lighting conditions, etc. Once disentangling is achieved, invariance follows easily: to build a representa-tion that is invariant to the transformation of a factor of vari-ation (e.g. object position) that is considered a nuisance for a particular task (e.g. object classification), one can sim-ply ignore the units in the representation that encode the nuisance factor.
 To get a mathematical handle on the concept of disentan-gling, we borrow a fundamental principle from physics, which we refer to as Weyl X  X  principle, following Kanatani (1990). In physics, this idea is used to tease apart (i.e. disentangle) the elementary particles of a physical system from mere measurement values that have no inherent phys-ical significance. We apply this principle to the area of vi-sion, for after all, pixels are nothing but physical measure-ments.
 Weyl X  X  principle presupposes a symmetry group that acts on the data. By this we mean a set of transformations that does not change the  X  X ssence X  of the measured phe-nomenon, although it may change the  X  X uperficial appear-ance X , i.e. the measurement values. As a concrete example that we will use throughout this paper, consider the group known as SO (2) , acting on images by 2D rotation about the origin. A transformation from this group (a rotation) may change the value of every pixel in the image, but leaves in-variant the identity of the imaged object. Weyl X  X  principle states that the elementary components of this system are given by the irreducible representations of the symmetry group  X  a concept that will be explained in this paper. Although this theoretical principle is widely applicable, we demonstrate it for real-valued compact commutative groups only. We introduce a probabilistic model that de-scribes a representation of such a group, and show how it can be learned from pairs of images related by arbitrary and unobserved transformations in the group. Compact com-mutative groups are also known as toroidal groups, so we refer to this model as Toroidal Subgroup Analysis (TSA). Using a novel conjugate prior, the model integrates proba-bility theory and Lie group theory in a very elegant way. All the relevant probabilistic quantities such as normal-ization constants, moments, KL-divergences, the posterior density over the transformation group, the marginal density in data space, and their gradients can be obtained in closed form. 1.1. Related work The first to propose a model and algorithm for learning Lie group representations from data were Rao &amp; Ruder-man (1999). This model deals only with one-parameter groups, a limitation that was later lifted by Miao and Rao (2007). Both works rely on MAP-inference procedures that can only deal with infinitesimally small transformations. This problem was solved by Sohl-Dickstein et al. (2010) using an elegant adaptive smoothing technique, making it possible to learn from large transformations. This model uses a general linear transformation to diagonalize a one-parameter group, and combines multiple one-parameter groups multiplicatively.
 Other, non-group-theoretical approaches to learning trans-formations and invariant representations exist (Memisevic &amp; Hinton, 2010). These gating models were found to perform a kind of joint eigenspace analysis (Memisevic, 2012), which is somewhat similar to the irreducible reduc-tion of a toroidal group.
 Motivated by a number of statistical phenomena observed in natural images, Cadieu &amp; Olshausen (2012) describe a model that decomposes a signal into invariant amplitudes and covariant phase variables.
 None of the mentioned methods take into account the full uncertainty over transformation parameters, as does TSA. Due to exact or approximate symmetries in the data, there is in general no unique transformation relating two images, so that only a multimodal posterior distribution over the group gives a complete description of the geometric situa-tion. Furthermore, posterior inference in our model is per-formed by a very fast feed-forward procedure, whereas the MAP inference algorithm by Sohl-Dicksteint et al. requires a more expensive iterative optimization. 2.1. Equivalence, Invariance and Reducibility In this section, we discuss three fundamental concepts on which the analysis in the rest of this paper is based: equiv-alence, invariance and reducibility.
 Consider a function  X  : R D  X  X that assigns to each pos-sible data point x  X  R D a class-label ( X = { 1 ,...,L } ) or some distributed representation (e.g. X = R L ). Such a function induces an equivalence relation on the input space R
D : we say that two vectors x , y  X  R D are  X  -equivalent if they are mapped onto the same representation by  X  . Sym-bolically, x  X   X  y  X   X ( x ) =  X ( y ) .
 Every equivalence relation on the input space fully de-termines a symmetry group acting on the space. This group, call it G , contains all invertible transformations  X  : R D  X  R D that leave  X  invariant: G = {  X  | X  x  X  R D :  X (  X  ( x )) =  X ( x ) } . G describes the symmetries of  X  , or, stated differently, the label function/representation  X  is invariant to transformations in G . Hence, we can speak of G-equivalence : x  X  G y  X   X   X   X  G :  X  ( x ) = y . For example, if some elements of G act by rotating the image, two images are G -equivalent if they are rotations of each other.
 Before we can introduce Weyl X  X  principle, we need one more concept: the reduction of a group representation (Kanatani, 1990). Let us restrict our attention to linear representations of Lie groups:  X  becomes a matrix-valued function  X  g of an abstract group element g  X  G , such that  X  g,h  X  G :  X  g  X  h =  X  g  X  h . In general, every coordinate y of y =  X  g x can depend on every coordinate x j of x . Now, since x is G-equivalent to y , it makes no sense to consider the coordinates x i as separate quantities ; we can only con-sider the vector x as a single unit because the symmetry transformations  X  g tangle all coordinates. In other words, we cannot say that coordinate x i is an independent part of the aggregate x , because a mapping x  X  x 0 =  X  g x that is supposed to leave the intrinsic properties of x unchanged, will in fact induce induce a functional dependence between all supposed parts x 0 i and x j .
 However, we are free to change the basis of the measure-ment space. It may be possible to use a change of basis to expose an invariant subspace , i.e. a subspace V  X  R D that is mapped onto itself by every transformation in the group:  X  g  X  G : x  X  V  X   X  g x  X  V . If such a subspace ex-ists and its orthogonal complement V  X   X  R D is also an invariant subspace, then it makes sense to consider the two parts of x that lie in V and V  X  to be distinct, because they remain distinct under symmetry transformations.
 Let W be a change of basis matrix that exposes the invari-ant subspaces, that is, for all g  X  G . Both  X  1 g and  X  2 g form a representation of the same abstract group as represented by  X  . The group representations  X  1 g and  X  2 g describe how the individual parts x 1  X  V and x 2  X  V  X  are transformed by the elements of the group. As is common in group representation theory, we refer to both the group representations  X  1 g , X  2 g and the subspaces V and V  X  corresponding to these group repre-sentations as  X  X epresentations X .
 The process of reduction can be applied recursively to  X  1 and  X  2 g . If at some point there is no more (non-trivial) in-variant subspace, the representation is called irreducible . Weyl X  X  principle states that the elementary components of a system are the irreducible representations of the symme-try group of the system. Properly understood, it is not a physics principle at all, but generally applicable to any situ-ation where there is a well-defined notion of equivalence It is completely abstract and therefore agnostic about the type of data (images, optical flows, sound, etc.), making it eminently useful for representation learning.
 In the rest of this paper, we will demonstrate Weyl X  X  princi-ple in the simple case of a compact commutative subgroup of the special orthogonal group in D dimensions. We want to stress though, that there is no reason the basic ideas can-not be applied to non-commutative groups acting on non-linear latent representation spaces. 2.2. Maximal Tori in the Orthogonal Group In order to facilitate analysis, we will from here on con-sider only compact commutative subgroups of the special orthogonal group SO ( D ) . For reasons that will become clear shortly, such groups are called toroidal subgroups of SO ( D ) . Intuitively, the toroidal subgroups of general com-pact Lie groups can be thought of as the  X  X ommutative part X  of these groups. This fact, combined with their an-alytic tractability (evidenced by the results in this paper) makes them suitable as the starting point of a theory of probabilistic Lie-group representation learning.
 Imposing the constraint of orthogonality will make the computation of matrix inverses very cheap, because for or-thogonal Q , Q  X  1 = Q T . Orthogonal matrices also avoid numerical problems, because their condition number is al-ways equal to 1. Another important property of orthogo-nal transformations is that they leave the Euclidean metric invariant: k Qx k = k x k . Therefore, orthogonal matrices cannot express transformations such as contrast scaling, but they can still model the interesting structural changes in im-ages (Bethge et al., 2007). For example, since 2D image ro-tation and (cyclic) translation are linear and do not change the total energy (norm) of the image, they can be repre-sented by orthogonal matrices acting on vectorized images. As is well known, commuting matrices can be simultane-ously diagonalized, so one could represent a toroidal group in terms of a basis of eigenvectors shared by every element in the group, and one diagonal matrix of eigenvalues for each element of the group, as was done in (Sohl-Dickstein et al., 2010) for 1-parameter Lie groups. However, or-thogonal matrices do not generally have a complete set of real eigenvectors. One could use a complex basis instead, but this introduces redundancies because the eigenvalues and eigenvectors of an orthogonal matrix come in com-plex conjugate pairs. For machine learning applications, this is clearly an undesirable feature, so we opt for a joint block-diagonalization of the elements of the toroidal group:  X   X  = WR (  X  ) W T , where W is orthogonal and R (  X  ) is a block-diagonal rotation matrix 3 : The diagonal of R (  X  ) contains 2  X  2 rotation matrices In this parameterization, the real, orthogonal basis W iden-tifies the group representation, while the vector of rotation angles  X  identifies a particular element of the group. It is now clear why such groups are called  X  X oroidal X : the parameter space  X  is periodic in each element  X  j and hence is a topological torus. For a J -parameter toroidal group, all the  X  j can be chosen freely. Such a group is known as a maximal torus in SO ( D ) , for which we write T J = {  X  |  X  j  X  [0 , 2  X  ] ,j = 1 ,...J } .
 To gain insight into the structure of toroidal groups with fewer parameters, we rewrite eq. 2 using the matrix expo-nential: The anti-symmetric matrices A j = d d X  as the Lie algebra generators, and the  X  j are Lie-algebra coordinates.
 The Lie algebra is a structure that largely determines the structure of the corresponding Lie group, while having the important advantage of forming a linear space. That is, all linear combinations of the generators belong to the Lie al-gebra, and each element of the Lie algebra corresponds to an element of the Lie group, which itself is a non-linear manifold. Furthermore, every sub group of the Lie group corresponds to a sub algebra (not defined here) of the Lie algebra. All toroidal groups are the subgroup of some max-imal torus, so we can learn a general toroidal group by first learning a maximal torus and then learning a subalgebra of its Lie algebra. Due to commutativity, the structure of the Lie algebra of toroidal groups is such that any sub space of the Lie algebra is in fact a subalgebra. The relevance of this observation to our machine learning problem is that to learn a toroidal group with I parameters ( I &lt; J ), we can simply learn a maximal toroidal group and then learn an I -dimensional linear subspace in the space of  X  . In this work, we are interested in compact subgroups only which is to say that the parameter space should be closed and bounded. To see that not all subgroups of a maximal torus are compact, consider a 4D space and a maximal torus with 2 generators A 1 and A 2 . Let us define a subalgebra with one generator A =  X  1 A 1 +  X  2 A 2 , for real numbers  X  1 and  X  2 . The group elements generated by this algebra through the exponential map takes the form Each block R (  X  j s ) is periodic with period 2  X / X  j , but their direct sum R ( s ) need not be. When  X  1 and  X  2 are not commensurate, all values of s  X  R will produce differ-ent R ( s ) , and hence the parameter space is not bounded. To obtain a compact one-parameter group with parameter space s  X  [0 , 2  X  ] , we restrict the frequencies  X  j to be inte-gers, so that R ( s ) = R ( s + 2  X  ) (see figure 1). It is easy to see that each block of R ( s ) forms a real-valued irreducible representation of the toroidal group, making R ( s ) a direct sum of irreducible representations. From the point of view expounded in section 2.1, we should view the vector x as a tangle of elementary components u j = W T j matrix of W corresponding to the j -th block in R ( s ) . Each one of the elementary parts u j is functionally independent of the others under symmetry transformations.
 The variable  X  j is known as the weight of the representa-tion (Kanatani, 1990). When the representations are equiv-alent (i.e. they have the same weight), the parts are  X  X f the same kind X  and are transformed identically. Elementary components with different weights transform differently. In the following section, we show how a maximal toroidal group and a 1-parameter subgroup can be learned from cor-respondence pairs, and how these can be used to generate invariant representations. We will start by modelling a maximal torus. A data pair ( x , y ) is related by a transformation  X   X  = WR (  X  ) W where  X  N ( 0 , X  2 ) represents isotropic Gaussian noise. In other symbols, p ( y | x , X  ) = N ( y | WR (  X  ) W T x , X  We use the following notation for indexing invariant sub-u j = W T j x and v j = W T j y . If we want to access one of the coordinates of u or v , we write u j 1 = W T (: , 2 j  X  1) We assume the  X  j to be marginally independent and von-Mises distributed. The von-Mises distribution is an expo-nential family that assigns equal density to the endpoints of any length-2  X  interval of the real line, making it a suit-able choice for periodic variables such as  X  j . We will find it convenient to move back and forth between the con-ventional and natural parameterizations of this distribution. The conventional parameterization of the von-Mises distri-bution M (  X  j |  X  j , X  j ) uses a mean  X  j and precision  X  The function I 0 that appears in the normalizing constant is known as the modified Bessel function of order 0.
 Since the von-Mises distribution is an exponential fam-ily, we can write it in terms of natural parameters  X  (  X  where T (  X  j ) = (cos(  X  j ) , sin(  X  j )) T are the sufficient statistics. The natural parameters can be computed from conventional parameters using, and vice versa, Using the natural parameterization, it is easy to see that the prior is conjugate to the likelihood, so that the poste-rior p (  X  | x , y ) is again a product of von-Mises distributions. Such conjugacy relations are of great utility in Bayesian statistics, because they simplify sequential inference. To our knowledge, this conjugacy relation has not been de-scribed before. To derive this result, first observe that the likelihood term splits into a sum over the invariant sub-spaces indexed by j : p (  X  | x , y )  X  p ( y | x , X  ) p (  X  ) Both the bilinear forms v T j R (  X  j ) u j and the prior terms  X 
T (  X  ) are linear functions of cos(  X  j ) and sin(  X  j ) , so that they can be combined into a single dot product: which we recognize as a product of von-Mises in natural form.
 The parameters  X   X  j of the posterior are given by:  X   X  j =  X  j + where  X  j is the angle between u j and v j . Geometrically, we can interpret the Bayesian updating procedure in eq. 12 as follows. The orientation of the natural parameter vector  X  j determines the mean of the von-Mises, while its mag-nitude determines the precision. To update this parame-ter with new information obtained from data u j , v j , one should add the vector (cos(  X  j ) , sin(  X  j )) T to the prior, us-ing a scaling factor that grows with the magnitude of u j and v j and declines with the square of the noise level  X  . The longer u j and v j and the smaller the noise level, the greater the precision of the observation. This geometrically sensible result follows directly from the consistent applica-tion of the rules of probability.
 Observe that when using a uniform prior (i.e.  X  j = 0 ), the posterior mean  X   X  j (computed from  X   X  j by eq. 10) will be exactly equal to the angle  X  j between u j and v j . We will use this fact in section 3.1 when we derive the formula for the orbit distance in a toroidal group.
 Previous approaches to Lie group learning only provide point estimates of the transformation parameters, which have to be obtained using an iterative optimization proce-dure (Sohl-Dickstein et al., 2010). In contrast, TSA pro-vides a full posterior distribution which is obtained us-ing a simple feed-forward computation. Compared to the work of Cadieu &amp; Olshausen (2012), our model deals well with low-energy subspaces, by simply describing the un-certainty in the estimate instead of providing inaccurate es-timates that have to be discarded. 3.1. Invariant Representation and Metric One way of doing invariant classification is by using an in-variant metric known as the manifold distance . This metric d ( x , y ) is defined as the minimum distance between the or-bits O x = {  X   X  x |  X   X  G } and O y = {  X   X  y |  X   X  G } . Observe that this is only a true metric that satisfies the co-incidence axiom d ( x , y ) = 0  X  x = y if we take the condition x = y to mean  X  X quivalence up to symmetry transformations X  or x  X  G y , as discussed in section 2.1. In practice, it has proven difficult to compute this distance exactly, so approximations such as tangent distance have been invented (Simard et al., 2000). But for a maximal torus, we can easily compute the exact manifold distance: where  X   X  j is the mean of the posterior p (  X  j | x , y ) , obtained using a uniform prior (  X  j = 0 ). The last step of eq. 13 follows, because as we saw in the previous section,  X   X  j simply the angle between u j and v j when using a uniform prior. Therefore, R ( X   X  j ) aligns u j and v j , minimizing the distance between them.
 Another approach to invariant classification is through an invariant representation. Although the model presented above aims to describe the transformation between obser-vations x and y , an invariant-equivariant representation ap-pears automatically in terms of the parameters of the pos-terior over the group. To see this, consider all the transfor-mations in the learned toroidal group G that take an image x to itself. This set is known as the stabilizer stab G ( x ) of x . It is a subgroup of G and describes the symmetries of x with respect to G . When a transformation  X   X  G is applied to x , the stabilizer subgroup is left invariant, for if  X   X  stab G ( x ) then  X   X   X   X  x =  X   X   X   X  x =  X   X  x and hence  X   X   X  stab G (  X   X  x ) .
 The posterior of x transformed into itself, p (  X  | x , x , X , X  = 0 ) = Q j M (  X  j |  X   X  j ,  X   X  j ) gives a probabilistic description of the stabilizer of x , and hence must be invariant. Clearly, the angle between x and x is zero, so  X   X  = 0 . On the other hand,  X   X  contains information about x and is invariant. To see this, recall that  X   X  j = k  X   X  j k . Using eq. 12 we obtain  X   X  j = k u j k 2  X   X  2 = k W T j x k 2  X   X  2 . Since every transfor-mation  X  in the toroidal group acts on the 2D vector u j by rotation, the norm of u j is left invariant.
 We recognize the computation of  X   X  as the square pool-ing operation often applied in convolutional networks to gain invariance: project an image onto filters W : , 2 j  X  1 W : , 2 j and sum the squares. This computation is a direct consequence of our model setup. In section 3.3, we will find that the model for non-maximal tori is even more in-formative about the proper pooling scheme.
 Since we want to use  X   X  as an invariant representa-tion, we should try to find an appropriate metric on  X   X  -space. Let  X   X  ( x ) be defined by p (  X  | x , x , X  = 0 ) = Q j M (  X  |  X   X  j ,  X   X  j ( x )) . We suggest using the Hellinger dis-tance: H 2 ( X   X  ( x ) ,  X   X  ( y )) = 1 which is equal to the exact manifold distance (eq. 13) up to a factor of 1 2  X  2 . The first step of this derivation uses eq. 12 under a uniform prior (  X  j = 0 ), while the second step again makes use of the fact that  X   X  j is the angle between u and v j so that k u j kk v j k = u T j R ( X   X  j ) v j . 3.2. Relation to the Discrete Fourier Transform We show that the DFT is a special case of TSA. The DFT of a discrete 1D signal x = ( x 1 ,...,x D ) T is defined: where  X  = e 2  X i/D is the D -th primitive root of unity. If we choose a basis of sinusoids for the filters in W , then the change of basis performed by W is a DFT. Specif-ically, R ( X j ) = u j 1 and I ( X j ) = u j 2 .
 Now suppose we are interested in the transformation tak-ing some arbitrary fixed vector e = W (1 , 0 ,..., 1 , 0) to x . The posterior over  X  j is p (  X  j | e , x , X  j 1) = M (  X  j |  X   X  j ) , where (by eq. 12) we have  X   X  k u j k [cos(  X  j ) , sin(  X  j )] T ,  X  j being the angle between u and the  X  X eal axis X  e j = (1 , 0) T . In conventional coor-dinates, the precision of the posterior is equal to the mod-ulus of the DFT,  X   X  j = k u j k = | X j | , and the mean of the posterior is equal to the phase of the Fourier transform,  X   X  =  X  j = arg( X j ) . Therefore, TSA provides a proba-bilistic interpretation of the DFT coefficients, and makes it possible to learn an appropriate generalized transform from data. 3.3. Modeling a Lie subalgebra Typically, one is interested in learning groups with fewer than J degrees of freedom. As we have seen, for one pa-rameter compact subgroups of a maximal torus, the weights of the irreducible representations must be integers. We model this using a coupled rotation matrix, as follows: Where s  X  [0 , 2  X  ] is the scalar parameter of this subgroup. The likelihood then becomes y  X  X  ( y |  X  s x , X  2 ) . We have found that the conjugate prior for this likelihood is the generalized von-Mises (Gatto &amp; Jammalamadaka, 2007): p ( s ) = M + ( s |  X  + ) = exp  X  +  X  T + ( s ) / Z + where T + ( s ) = [cos( s ) , sin( s ) ,..., cos( Ks ) , sin( Ks )] This conjugacy relation p ( s | x , y )  X  exp ( X   X  +  X  T + obtained using similar reasoning as before, yielding the up-date equation, where  X   X  k is obtained from eq. 12 using a uniform prior  X  k = 0 . The sum in this update equation performs a pool-ing operation over a weight space , which is defined as the span of those invariant subspaces k whose weight  X  k = j . As it turns out, this is exactly the right thing to do in order to summarize the data while maintaining invariance. As explained by Kanatani (1990), the norm k  X   X  + j k of a linear combination of same-weight representations is always in-variant (as is the maximum of any two k  X   X  j k or k  X   X  + similarity to sum-pooling and max-pooling in convnets is quite striking.
 In the maximal torus model, there are J = D/ 2 degrees of freedom in the group and the invariant representation is D  X  J = J -dimensional (  X   X  1 ,...,  X   X  J ). This represen-tation of a datum x identifies a toroidal orbit, and hence the vector  X   X  is a maximal invariant with respect to this group (Soatto, 2009). For the coupled model, there is only one degree of freedom in the group, so the invariant repre-sentation should be D  X  1 dimensional. When all  X  k are distinct, we have J variables  X  + 1 ,..., X  + J that are invari-ant. Furthermore, from eq. 15 we see that as x is trans-formed, the angle between x and an arbitrary fixed refer-ence vector in subspace j transforms as  X  j ( s ) =  X  j +  X  for some data-dependent initial phase  X  j . It follows that  X   X  k ( s )  X   X  k  X  j ( s ) =  X  j (  X  k +  X  k s )  X   X  k (  X   X   X  k  X   X  k  X  j is invariant. In this way, we can easily con-struct another J  X  1 invariants, but unfortunately these are not stable because the angle estimates can be inaccurate for low-energy subspaces. Finding a stable maximal invariant, and doing so in a way that will generalize to other groups is an interesting problem for future work.
 The normalization constant Z + for the GvM has so far only been described for the case of K = 2 harmon-ics, but we have found a closed form solution in terms of the so-called modified Generalized Bessel Functions (GBF) of K -variables  X  + =  X  + 1 ,..., X  + K and parame-ters 5 exp (  X  i X  + ) = exp (  X  i X  + 1 ) ,..., exp (  X  i X  toli et al., 1991): We have developed a novel, highly scalable algorithm for the computation of GBF of many variables, which is de-scribed in the supplementary material.
 Figure 2 shows the posterior over s for three image pairs related by different rotations and containing different sym-metries. The weights W and  X  were learned by the pro-cedure described in the next section. It is quite clear from this figure that MAP inference does not give a complete description of the possible transformations relating the im-ages when the images have a degree of rotational sym-metry. The posterior distribution of our model provides a sensible way to deal with this kind of uncertainty, which (in the case of 2D translations) is at the heart of the well known aperture problem in vision. Having a tractable pos-terior is particularly important if the model is to be used to estimate longer sequences (akin to HMM/LDS models, but non-linear), where one may encounter multiple high-density trajectories.
 If required, accurate MAP inference can be performed us-ing the algorithm of Sohl-Dickstein et al. (2010), as de-scribed in the supplementary material. This allows us to compute the exact manifold distance for the coupled model. 3.4. Maximum Marginal Likelihood Learning We train the model by gradient descent on the marginal likelihood. Perhaps surprisingly given the non-linearities in the model, the integrations required for the evaluation of the marginal likelihood can be obtained in closed form for both the coupled and decoupled models. For the decoupled model we obtain: p ( y | x ) = tion constants of regular von-Mises distributions, the anal-ogous expression for the coupled model is easily seen to be equal to eq. 18, only replacing Q j I 0 ( X   X  j ) / I can be found in the supplementary material.
 The gradient of the log marginal likelihood of the uncou-pled model w.r.t. a batch X , Y (both storing N vectors in the columns) is: d d W where we have used ( P Q ) 2 j,n = Q j,n P 2 j,n and ( P ation. A , B ( x ) and B ( y ) are D  X  N matrices with elements where the  X   X  jn is the posterior precision in subspace j for image pair x ( n ) , y ( n ) (the n -th column of X , resp. Y ). The gradient of the coupled model is easily computed using the differential recurrence relations that hold for the GBF (Dattoli et al., 1991).
 We use minibatch Stochastic Gradient Descent (SGD) on the log-likelihood of the uncoupled model. After every pa-rameter update, we orthogonalize W by setting all singular values to 1 : Let U , S , V = svd ( W ) , then set W := UV . This procedure and all previous derivations still work when the basis is undercomplete, i.e. has fewer columns (filters) than rows (dimensions in data space). To learn  X  j , we esti-mate the relative angular velocity  X  j =  X  j / X  from a batch of image patches rotated by a sub-pixel amount  X  = 0 . 1  X  We trained a TSA model with 100 filters on a stream of 250 . 000 16  X  16 image patches x ( t ) , y ( t ) . The patches x ( t ) were drawn from a standard normal distribution, and y ( t ) was obtained by rotating x ( t ) by an angle s drawn uni-formly at random from [0 , 2  X  ] . The learning rate  X  was ini-tialized at  X  0 = 0 . 25 and decayed as  X  =  X  0 / T was incremented by one with each pass through the data. Each minibatch consisted of 100 data pairs. After learn-ing W , we estimate the weights  X  j and sort the filter pairs by increasing absolute value for visualization. As can be seen in fig. 3, the filters are very clean and the weights are estimated correctly except for a few filters on row 1 and 2 that are assigned weight 0 when in fact they have a higher frequency.
 We tested the utility of the model for invariant classifica-tion on a rotated version of the MNIST dataset, using a 1-Nearest Neighbor classifier. Each digit was rotated by a random angle and rescaled to 16  X  16 pixels, resulting in 60k training examples and 10k testing examples, with no rotated duplicates. We compared the Euclidean distance (ED) in pixel space, tangent distance (TD) (Simard et al., 2000), Euclidean distance on the space of to the exact manifold distance for the maximal torus, see section 3.1), the true manifold distance for the 1-parameter 2D rotation group (MD), and the Euclidean distance on the non-rotated version of the MNIST dataset (ED-NR). The results in fig. 4 show that TD outperforms ED, but is out-performed by MD-classifier is about as accurate as ED on a much sim-pler dataset, demonstrating that it has almost completely modded out the variation caused by rotation. We have presented a novel principle for learning disentan-gled representations, and worked out its consequences for a simple type of symmetry group. This leads to a com-pletely tractable model with potential applications to invari-ant classification and Bayesian estimation of motion. The model reproduces the pooling operations used in convolu-tional networks from probabilistic and Lie-group theoretic principles, and provides a probabilistic interpretation of the DFT and its generalizations.
 The type of disentangling obtained in this paper is contin-gent upon the rather minimalist assumption that all that can be said about images is that they are equivalent (rotated copies) or inequivalent. However, the universal nature of Weyl X  X  principle bodes well for future applications to deep, non-linear and non-commutative forms of disentangling. We would like to thank prof. Giuseppe Dattoli for help with derivations related to the generalized Bessel functions. Bengio, Y. and Lecun, Y. International Con-ference on Learning Representations, 2014.

URL https://sites.google.com/site/ representationlearning2014/ .
 Bengio, Y., Courville, A., and Vincent, P. Representation
Learning: A Review and New Perspectives. IEEE trans-actions on pattern analysis and machine intelligence , pp. 1 X 30, February 2013.
 Bethge, M., Gerwinn, S., and Macke, J. H. Unsupervised learning of a steerable basis for invariant image represen-tations. Proceedings of SPIE Human Vision and Elec-tronic Imaging XII (EI105) , February 2007.
 Cadieu, C. F. and Olshausen, B. A. Learning intermediate-level representations of form and motion from natural movies. Neural computation , 24(4):827 X 66, April 2012. Dattoli, G., Chiccoli, C., Lorenzutta, S., Maino, G.,
Richetta, M., and Torre, A. A Note on the Theory of n-Variable Generalized Bessel Functions. Il Nuovo Ci-mento B , 106(10):1159 X 1166, October 1991.
 Gatto, R. and Jammalamadaka, S. R. The generalized von
Mises distribution. Statistical Methodology , 4(3):341 X  353, 2007.
 Goodfellow, I. J., Le, Q. V., Saxe, A. M., Lee, H., and Ng,
A. Y. Measuring invariances in deep networks. Advances in Neural Information Processing Systems , 2009.
 Kanatani, K. Group Theoretical Methods in Image Under-standing . Springer-Verlag New York, Inc., Secaucus, NJ, USA, 1990. ISBN 0387512535.
 Memisevic, R. On multi-view feature learning. Interna-tional Conference on Machine Learning , 2012.
 Memisevic, R. and Hinton, G. E. Learning to Rep-resent Spatial Transformations with Factored Higher-
Order Boltzmann Machines. Neural Computation , 22 (6):1473 X 1492, June 2010. ISSN 1530-888X. doi: 10.1162/neco.2010.01-09-953.
 Miao, X. and Rao, R. P. N. Learning the Lie groups of visual invariance. Neural computation , 19(10):2665 X 93, October 2007.
 Rao, R. P. N. and Ruderman, D. L. Learning Lie groups for invariant visual perception. Advances in neural informa-tion processing systems , 816:810 X 816, 1999.
 Simard, P. Y., Le Cun, Y. A., Denker, J. S., and Victorri, B.
Transformation invariance in pattern recognition: Tan-gent distance and propagation. International Journal of Imaging Systems and Technology , 11(3):181 X 197, 2000. Soatto, S. Actionable information in vision. In 2009 IEEE 12th International Conference on Computer Vision , pp. 2138 X 2145. IEEE, September 2009.
 Sohl-Dickstein, J., Wang, J. C., and Olshausen, B. A. An unsupervised algorithm for learning lie group transfor-mations. arXiv preprint , 2010.
 Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.
Extracting and composing robust features with denois-ing autoencoders. Proceedings of the 25th international conference on Machine learning , pp. 1096 X 1103, 2008. Welling, M., Rosen-zvi, M., and Hinton, G. Exponential Family Harmoniums with an Application to Information
Retrieval. Advances in neural information processing
