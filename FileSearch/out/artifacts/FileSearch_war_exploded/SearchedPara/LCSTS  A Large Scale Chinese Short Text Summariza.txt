 Nowadays, individuals or organizations can eas-ily share or post information to the public on the social network. Take the popular Chinese mi-croblogging website (Sina Weibo) as an example, the People X  X  Daily, one of the media in China, posts more than tens of weibos (analogous to tweets) each day. Most of these weibos are well-written and highly informative because of the text length limitation (less than140 Chinese charac-ters). Such data is regarded as naturally annotated web resources (Sun, 2011). If we can mine these high-quality data from these naturally annotated web resources, it will be beneficial to the research that has been hampered by the lack of data. Figure 1: A Weibo Posted by People X  X  Daily.
In the Natural Language Processing (NLP) community, automatic text summarization is a hot and difficult task. A good summarization system should understand the whole text and re-organize the information to generate coherent, informative, and significantly short summaries which convey important information of the original text (Hovy and Lin, 1998), (Martins, 2007). Most of tradi-tional abstractive summarization methods divide the process into two phrases (Bing et al., 2015). First, key textual elements are extracted from the original text by using unsupervised methods or lin-guistic knowledge. And then, unclear extracted components are rewritten or paraphrased to pro-duce a concise summary of the original text by using linguistic rules or language generation tech-niques. Although extensive researches have been done, the linguistic quality of abstractive sum-mary is still far from satisfactory. Recently, deep learning methods have shown potential abilities to learn representation (Hu et al., 2014; Zhou et al., 2015) and generate language (Bahdanau et al., 2014; Sutskever et al., 2014) from large scale data by utilizing GPUs. Many researchers realize that we are closer to generate abstractive summariza-tions by using the deep learning methods. How-ever, the publicly available and high-quality large scale summarization data set is still very rare and not easy to be constructed manually. For exam-ple, the popular document summarization dataset human written English text summarizations. The problem is even worse for Chinese. In this pa-per, we take one step back and focus on construct-ing LCSTS , the L arge-scale C hinese S hort T ext S ummarization dataset by utilizing the naturally annotated web resources on Sina Weibo. Figure 1 shows one weibo posted by the People X  X  Daily. In order to convey the import information to the pub-lic quickly, it also writes a very informative and short summary (in the blue circle) of the news. Our goal is to mine a large scale, high-quality short text summarization dataset from these texts.
This paper makes the following contributions: (1) We introduce a large scale Chinese short text summarization dataset. To our knowledge, it is the largest one to date; (2) We provide standard splits for the dataset into large scale training set and human labeled test set which will be easier for benchmarking the related methods; (3) We explore the properties of the dataset and sample 10,666 instances for manually checking and scoring the quality of the dataset; (4) We perform recurrent neural network based encoder-decoder method on the dataset to generate summary and get promis-ing results, which can be used as one baseline of the task. Our work is related to recent works on automatic text summarization and natural language process-ing based on naturally annotated web resources, which are briefly introduced as follows.

Automatic Text Summarization in some form has been studied since 1950. Since then, most re-searches are related to extractive summarizations by analyzing the organization of the words in the document (Nenkova and McKeown, 2011) (Luhn, 1998); Since it needs labeled data sets for su-pervised machine learning methods and labeling dataset is very intensive, some researches focused on the unsupervised methods (Mihalcea, 2004). The scale of existing data sets are usually very small (most of them are less than 1000). For example, DUC2002 dataset contains 567 docu-ments and each document is provided with two 100-words human summaries. Our work is also related to the headline generation, which is a task to generate one sentence of the text it entitles. Colmenares et.al construct a 1.3 million financial news headline dataset written in English for head-line generation (Colmenares et al., 2015). How-ever, the data set is not publicly available. Naturally Annotated Web Resources based Natural Language Processing is proposed by Sun (Sun, 2011). Naturally Annotated Web Re-sources is the data generated by users for commu-nicative purposes such as web pages, blogs and microblogs. We can mine knowledge or useful data from these raw data by using marks generated by users unintentionally. Jure et.al track 1.6 mil-lion mainstream media sites and blogs and mine a set of novel and persistent temporal patterns in the news cycle (Leskovec et al., 2009). Sepandar et.al use the users X  naturally annotated pattern  X  X e feel X  and  X  X  feel X  to extract the  X  X eeling X  sentence collec-tion which is used to collect the world X  X  emotions. In this work, we use the naturally annotated re-sources to construct the large scale Chinese short text summarization data to facilitate the research on text summarization. A lot of popular Chinese media and organizations have created accounts on the Sina Weibo. They use their accounts to post news and information. These accounts are verified on the Weibo and la-beled by a blue  X  X  X . In order to guarantee the qual-ity of the crawled text, we only crawl the verified organizations X  weibos which are more likely to be clean, formal and informative. There are a lot of human intervention required in each step. The pro-cess of the data collection is shown as Figure 2 and summarized as follows: 1) We first collect 50 very popular organiza-tion users as seeds. They come from the domains of politic, economic, military, movies, game and etc, such as People X  X  Daily, the Economic Observe press, the Ministry of National Defense and etc. 2) We then crawl fusers followed by these seed users and filter them by using human written rules such as the user must be blue verified, the number of followers is more than 1 million and etc. 3) We use the chosen users and text crawler to crawl their weibos. 4) we filter, clean and extract (short text, summary) pairs. About 100 rules are used to ex-tract high quality pairs. These rules are concluded by 5 peoples via carefully investigating of the raw text. We also remove those paris, whose short text length is too short (less than 80 characters) and length of summaries is out of [10,30]. The dataset consists of three parts shown as Ta-ble 1 and the length distributions of texts are shown as Figure 3.

Part I is the main content of LCSTS that con-tains 2,400,591 (short text, summary) pairs. These pairs can be used to train supervised learning model for summary generation.

Part II contains the 10,666 human labeled (short text, summary) pairs with the score ranges from 1 to 5 that indicates the relevance between the short text and the corresponding summary.  X 1 X  denotes  X  the least relevant  X  and  X 5 X  denotes  X  X he most relevant X . For annotating this part, we recruit 5 volunteers, each pair is only labeled by one an-notator. These pairs are randomly sampled from Part I and are used to analysize the distribution of pairs in the Part I. Figure 4 illustrates examples of different scores. From the examples we can see that pairs scored by 3, 4 or 5 are very relevant to the corresponding summaries. These summaries are highly informative, concise and significantly short compared to original text. We can also see that many words in the summary do not appear in the original text, which indicates the significant difference of our dataset from sentence compres-sion datasets. The summaries of pairs scored by 1 or 2 are highly abstractive and relatively hard to conclude the summaries from the short text. They are more likely to be headlines or comments in-stead of summaries. The statistics show that the percent of score 1 and 2 is less than 20% of the Figure 3: Box plot of lengths for short text(ST), segmented short text(Segmented ST), sum-mary(SUM) and segmented summary(Segmented SUM). The red line denotes the median, and the edges of the box the quartiles. data, which can be filtered by using trained classi-fier.

Part III contains 1,106 pairs. For this part, 3 annotators label the same 2000 texts and we ex-tract the text with common scores. This part is independent from Part I and Part II. In this work, we use pairs scored by 3, 4 and 5 of this part as the test set for short text summary generation task. Recently, recurrent neural network (RNN) have shown powerful abilities on speech recogni-tion (Graves et al., 2013), machine transla-tion (Sutskever et al., 2014) and automatic dialog response (Shang et al., 2015). However, there is rare research on the automatic text summarization by using deep models. In this section, we use RNN as encoder and decoder to generate the summary of short text. We use the Part I as the training set and the subset of Part III, which is scored by 3, 4 and 5, as test set.

Two approaches are used to preprocess the data: 1) character-based method, we take the Chinese character as input, which will reduce the vocab-ulary size to 4,000. 2) word-based method, the Figure 5: The graphical depiction of RNN encoder and decoder framework without context. Figure 6: The graphical depiction of the RNN en-coder and decoder framework with context. text is segmented into Chinese words by using adopt two deep architectures, 1) The local con-text is not used during decoding. We use the RNN as encoder and it X  X  last hidden state as the input of decoder, as shown in Figure 5; 2) The context is used during decoding, following (Bah-danau et al., 2014), we use the combination of all the hidden states of encoder as input of the decoder, as shown in Figure 6. For the RNN, we adopt the gated recurrent unit (GRU) which is proposed by (Chung et al., 2015) and has been proved comparable to LSTM (Chung et al., 2014). All the parameters (including the embeddings) of the two architectures are randomly initialized and ADADELTA (Zeiler, 2012) is used to update the learning rate. After the model is trained, the beam search is used to generate the best summaries in the process of decoding and the size of beam is set to 10 in our experiment. RNN context Table 2: The experiment result:  X  X ord X  and  X  X har X  denote the word-based and character-based input respectively.
 Figure 7: An example of the generated summaries.
For evaluation, we adopt the ROUGE met-rics (Lin, 2004) proposed by (Lin and Hovy, 2003), which have been proved strongly correlated with human evaluations. ROUGE-1, ROUGE-2 and ROUGE-L are used. All the models are trained on the GPUs tesla M2090 for about one week.Table 2 lists the experiment results. As we can see in Figure 7, the summaries generated by RNN with context are very close to human written summaries, which indicates that if we feed enough data to the RNN encoder and decoder, it may gen-erate summary almost from scratch.

The results also show that the RNN with con-text outperforms RNN without context on both character and word based input. This result indi-cates that the internal hidden states of the RNN encoder can be combined to represent the context of words in summary. And also the performances of the character-based input outperform the word-based input. As shown in Figure 8, the summary generated by RNN with context by inputing the character-based short text is relatively good, while the the summary generated by RNN with context Figure 8: An example of the generated summaries with UNKs. on word-based input contains many UNKs. This may attribute to that the segmentation may lead to many UNKs in the vocabulary and text such as the person name and organization name. For exam-ple,  X   X   X   X   X   X   X  is a company name which is not in the vocabulary of word-based RNN, the RNN summarizer has to use the UNKs to replace the  X   X   X   X   X   X   X  in the process of decoding. We constructed a large-scale Chinese short text summarization dataset and performed RNN-based methods on it, which achieved some promising re-sults. This is just a start of deep models on this task and there is much room for improvement. We take the whole short text as one sequence, this may not be very reasonable, because most of short texts contain several sentences. A hierarchical RNN (Li et al., 2015) is one possible direction. The rare word problem is also very important for the gener-ation of the summaries, especially when the input is word-based instead of character-based. It is also a hot topic in the neural generative models such as neural translation machine(NTM) (Luong et al., 2014), which can benefit to this task. We also plan to construct a large document summarization data set by using naturally annotated web resources. This work is supported by National Natu-ral Science Foundation of China: 61473101, 61173075 and 61272383, Strategic Emerg-ing Industry Development Special Funds of Shenzhen: JCYJ20140417172417105, JCYJ20140508161040764 and JCYJ20140627163809422. We thank to Baolin Peng, Lin Ma, Li Yu and the anonymous reviewers for their insightful comments.
