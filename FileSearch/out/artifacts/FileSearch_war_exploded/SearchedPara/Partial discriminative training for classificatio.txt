 ORIGINAL PAPER Cheng-Lin Liu Abstract For character recognition in document analysis, some classes are closely overlapped but are not necessarily to be separated before contextual information is exploited. For classification of such overlapping classes, either dis-criminating between them or merging them into a meta-class does not satisfy. Merging the overlapping classes into a metaclass implies that within-metaclass substitution is consi-dered as correct classification. For such classification pro-blems, this paper proposes a partial discriminative training (PDT) scheme, in which, a training pattern of an overlap-ping class is used as a positive sample of its labeled class, and neither positive nor negative sample for its allied classes (those overlapping with the labeled class). In experiments of offline handwritten letter and online symbol recognition using various classifiers evaluated at metaclass level, the PDT scheme mostly outperforms ordinary discriminative training and merged metaclass classification.
 Keywords Character recognition  X  Overlapping classes  X  Discriminative training  X  Partial discriminative training 1 Introduction In document analysis applications involving character recog-nition, there is often the case that some patterns from different classes have very similar characteristics. In the feature space, such patterns of different classes correspond to co-incident or very close points, residing in an overlapping region. Such classes are called overlapping classes. A typical example pro-blem is handwritten alphanumeric recognition, where some classes such as letters  X  X  X ,  X  X  X  and numeral  X 0 X  have identi-cal shape, and it is neither possible nor necessary to separate them. Some other classes, such as upper-case letters  X  X  X ,  X  X  X  and  X  X  X , have many samples written in lower-case shapes (see Fig. 1 ). Thus, the upper-case letter and its corresponding lower-case have partially overlapping regions in the feature space. For such overlapping classes and all the pairs of upper-case/lower-case letters, it is not necessary to separate them at character level because it is easy to disambiguate according to the context, say, upper-case letters and lower-case letters rarely co-occur in the same word except the first letter.
There are generally two ways to deal with the overlapping classification problem. One way is to simply merge the over-lapping classes into a metaclass 1 and ignore the boundary between these classes. In metaclass classification, the sub-stitution between overlapping classes (within a metaclass) is considered as correct. In English letter recognition, the 52 letters can be merged into 26 case-insensitive classes. The 52-class letter classifier can also be evaluated at meta-class level by ignoring the substitution between the upper and lower cases of the same letter. Another way is to sepa-rate the overlapping classes by refining the classification boundary in the feature space, via extracting complemen-tary or more discriminative features, using multi-stage clas-sifier or combining multiple classifiers [ 1  X  4 ]. Particularly, using class verifiers [ 3 ] or pairwise classifiers [ 4 ]toverify or discriminate the top ranked classes output by a first-stage classifier can improve the classification accuracy. Neverthe-less, the accuracy of overlapping classes separation is limited by the inherent feature space overlap, and in some cases, such attempt of discrimination is not necessary due to the availa-bility of context.

The problem of overlapping classification is similar to multi-labeledclassification[ 5 , 6 ],whereapatternmaybelong to multiple classes. If we enhance the class label of a pat-tern from an overlapping class such that it belongs to the labeled class as well as the allied classes (those overlapping with the labeled class), the overlapping classification pro-blem becomes a multi-labeled one. In evaluation, the classi-fication of a pattern to any of its allied classes is considered correct.

Unlike that many works attempt to separate overlapping classes, this work attempts to improve the separation between metaclasses while ignoring within-metaclass substitution. To achieve this goal, this paper proposes a new scheme for trai-ning neural networks, support vector machines (SVMs), and other discriminative classifiers. Ordinary discriminative trai-ning tends to complicate the boundary between overlapping classes, and consequently, may deteriorate the generalized classification performance. In the proposed partial discrimi-native training (PDT) scheme, the pattern of an overlapping class is used as a positive sample of its labeled class, and neither positive nor negative sample of the allied classes. By contrast, in ordinary discriminative training, the pattern of a class is used as negative sample of all the other classes, and in multi-labeled classification (cross-training [ 5 ]), the pattern is used as positive sample of its allied classes.

The effect of PDT to improve the separation between metaclasses is helpful in three scenarios:  X  If some classes (such as letters  X  X  X  and  X  X  X ) are not sepa- X  When overlapping or partially overlapping classes can be  X  When pairwise (within-metaclass) classifiers are used to
To justify the effect of partial discriminative training, I evaluated various discriminative classifiers on two databases: the C-Cube handwritten letter database [ 7 , 8 ] and a subset of online handwritten symbols from the HANDS databases of TUAT [ 9 ]. The evaluated classifiers include neural net-works, SVMs, nearest prototype classifiers [ 10 ], and discri-minative learning quadratic discriminant function (DLQDF) [ 11 ]. The results show that the proposed PDT mostly outper-forms cross-training, ordinary discriminative training, and merged metaclass classification when evaluated at metaclass level.

In the rest of this paper, Sect. 2 briefly reviews the rela-ted works; Sect. 3 describes the proposed PDT scheme and Sect. 4 describes the specific classifiers; Sect. 5 presents the experimental results, and Sect. 6 offers concluding remarks. 2 Related works have been popularly applied to pattern recognition. A para-metric statistical classifier, which estimates the probability density function of each class without considering the boun-dary between classes, is ready for classification of overlap-ping classes. The overlap between classes will not affect the estimation of parameters of parametric statistical classifiers. In training neural networks, the connecting weights are itera-tively adjusted by optimizing an objective of minimum squa-red error or cross-entropy between class outputs and desired targets [ 13 ]. The overlap between classes will affect the com-plexity of decision boundary.

The support vector machine (SVM) [ 14 ]isanemerging classifierforsolvingdifficultclassificationproblems.Though multi-class SVM was proposed, binary SVM is popularly used, even for multi-class classification, by combining mul-tiple binary classifiers for one-versus-all, pairwise, or other binary coded tasks. In any case, the binary SVM is trained (coefficients of kernel functions estimated) by maximizing the margin between two classes. The overlap between two classes also affects the trained SVM.

Both neural networks and SVMs, as discriminative classi-fiers, attempt to separate different classes in the feature space. If we ignore the substitution between overlapping classes, as for handwritten letter recognition, the overlapping classes can be merged into a metaclass and then the ordinary dis-criminative classifiers can be applied to this reduced class set problem. Koerich [ 15 ] designed several neural network classifiers for recognizing 52 letters, 26 upper-case letters, 26 lower-case letters and 26 metaclasses, respectively, and sho-wed that the metaclass classifier outperforms the 52-class classifier (evaluated at metaclass level) and the combination of upper-case and lower-case classifiers.

In another work, Blumenstein et al. [ 16 ] merged 52 letters into 36 metaclasses: all upper-case letters except  X  X BDEGHNQRT X  are merged with their lower-case letters, and use a neural network for 36-class classification. Camastra et al. [ 8 ] use one-versus-all SVM classifiers for classifying handwritten letters in 52 classes, 26 classes and adaptively merged classes according to the overlap degree between upper and lower cases. Using classifiers of 52 classes, 38 classes and 26 classes, they obtained test accu-racies (evaluated at 26-metaclass level) of 89.20, 90.05 and 89.61%, respectively.

When ignoring the substitution between overlapping classes, the patterns of these overlapping classes can be vie-wed as multi-labeled. Multi-labeled classification is gene-rally transformed to multiple binary classification tasks, and different methods differ in the way of attaching binary labels to training samples [ 6 ]. An effective method, cross-training [ 5 ], uses each multi-labeled sample as the positive sample of each class it belongs to and not as negative sample for any of the labeled classes. For example, if a sample belongs to classes  X  X  X  and  X  X  X , it is used as positive sample when training the binary classifiers for  X  X  X  and  X  X  X , and as negative samples for the binary classifiers of the other classes. 3 Partial discriminative training The partial discriminative training (PDT) scheme is first pro-posed for neural networks [ 17 ], and then extended to other classifiers. 3.1 Training objectives of neural networks Assume to classify a pattern (represented by a feature vector x  X  R d ) to one of M classes {  X  training samples ( x n , c n ) ( c n is the class label of sample x n = 1 ,..., N , for training a multi-class classifier. On an input pattern x , the classifier outputs (sigmoidal) confidence values y k ( x , W ) ( W denotes the set of parameters) for classes k = 1 ,..., M . Often, neural network weights are estimated by minimizing the squared error (SE): min where t n k denotes the target output: t =  X ( c n , k ) = The SE in Eq. ( 1 ) can be re-written as SE = of a binary classifier for class  X  k versus the others. Thus, the training of the multi-class neural network is equivalent to the training of multiple binary one-versus-all classifiers. Accordingly, the class output y k ( x , W ) functions as the dis-criminant for separating class  X  k from the others.
The cross-entropy (CE) objective for neural networks can be similarly decomposed into multiple binary tasks: CE = X 
For multi-labeled classification, each sample x n is labeled to belong to a subset of classes C n . For training neural net-works in such case, the objective is the same as Eqs. ( 1 ), ( 3 ) or ( 4 ) except that the target output is changed to t = 1 Due to the class modularity of objective functions SE and CE, for either single-labeled or multi-labeled classification, we can either train a multi-class classifier or train multiple binary one-versus-all classifiers.

To apply multi-labeled learning algorithms to overlapping classification, we can enhance the label of each sample with the allied classes (those overlapping with the labeled class) to convert the problem to be multi-labeled. 3.2 Partial discriminative training (PDT) By ordinary discriminative training, which maximizes the separation between classes, the boundary between overlap-ping classes will be complicated. This may deteriorate the generalized classification performance and affect the boun-dary between metaclasses. On the other hand, simply mer-ging the overlapping classes into a metaclass will complicate the distribution of the metaclass.

If ignoring the substitution between overlapping classes, the training objective of neural networks, either squared error (SE) or cross-entropy (CE), can be modified to ignore the error of the allied classes of each training sample. Denote the allied classes of  X  k as a set ( k ) (e.g., in alphanumeric recognition, the allied classes of  X  X  X  are  X  X 0 X ), the squared errorofEq.( 3 ) is modified as SE = This implies, the training pattern x n is not used as negative sample for the allied classes of c n . Note that the relation of alliance is symmetric, i.e., k  X  ( c )  X  c  X  ( k ) .
Excluding a training pattern from the negative samples of the allied classes of the label prevents the classifier from over-fitting the boundary between the labeled class and its allied classes (which are overlapping with the labeled class). Still, the number of classes remains unchanged (the structure of the multi-class classifier does not change), unlike in meta-class merging, the number of classes is reduced. Keeping the number of classes has the benefit that the classifier still out-puts confidence scores to each of the overlapping classes. If two allied classes are partially overlapped, a sample from the un-overlapped region can be classified to its class unambi-guously. By class merging, however, the boundary between allied classes is totally ignored. 3.2.1 Extension to other classifiers The PDT scheme can be applied to all types of binary clas-sifiers, with multiple binary classifiers combined to perform multi-class classification. For multi-class classification using one-versus-all SVMs, when training an SVM for a class  X  k if  X  this sample is excluded from the negative samples of  X  k . For example, in training the binary classifier for  X  X  X  in let-ter recognition, the samples of  X  X  X  are not used as negative samples.

Many classifiers can be trained by the minimum classifica-tion error (MCE) method [ 18 ], such as the nearest prototype classifier [ 10 ] and the discriminative learning quadratic dis-criminant function (DLQDF) [ 11 ]. The MCE method is to estimate the classifier parameters by minimizing the empiri-cal classification error on a training dataset: L where I (  X  ) is an indicator function. The classification error (loss) l c ( x ) on a training pattern x with genuine class approximated by the sigmoidal function: l ( x ) =  X ( d c ) = where  X  is a constant to control the smoothness of loss func-tion. The misclassification measure d c ( x ) is usually approxi-mated by the difference of discriminant function between the genuine class (positive class) and the closest rival class (negative class): d ( x ) = X  f ( x , X  where the discriminant function f ( x , X  i ) involves the classi-fier parameters, which are adjusted in minimizing the empi-rical loss by stochastic gradient descent.

To apply the PDT scheme to classifiers trained by MCE, the misclassification measure d c ( x ) is modified as d ( x ) = X  f ( x , X  This is to say, the training pattern x is used as a positive sample of the labeled class  X  c and as a negative sample of the other classes except the allied classes of the label. 4 Specific classifiers The proposed PDT scheme has been applied to five types of neural networks, SVMs with two types of kernel functions, a nearest prototype classifier by learning vector quantiza-tion (LVQ) [ 10 ], and a DLQDF classifier [ 11 ]. The neural classifiers are single-layer neural network (SLNN), multi-layer perceptron (MLP), radial basis function (RBF) network [ 13 ],polynomialnetworkclassifier(PNC)[ 19 , 20 ],andclass-specific feature polynomial classifier (CFPC) [ 21 ]. Two one-versus-all SVM classifiers use a polynomial kernel and an RBF kernel, respectively. 4.1 Neural networks The neural classifiers have a common nature that each class output is the sigmoidal (logistic) function of the weighted sum of values of the previous layer. In SLNN, the input fea-ture values are directly linked to the output layer: y ( x , W ) =  X  where w ki denotes the connecting weights.

The MLP used in our experiments has one hidden layer, with class output computed by y ( x , W ) =  X  where N h denotes the number of hidden units, w kj and v ji denote the connecting weights of the output layer and the hidden layer, respectively.

The RBF network has one hidden layer of Gaussian kernel units: h ( x ) = exp  X  x and the kernel values are linked to the sigmoidal outputs: y ( x , W ) =  X  The Gaussian centers and variance values are initialized by clustering and are optimized together with the weights by error minimization.

The PNC is a single-layer network with the polynomials of feature values as inputs. To avoid high complexity, I use a PNC with the binomial terms of the principal compo-nents [ 20 ]. Denoting the principal components in the m -dimensional ( m &lt; d ) subspace by z , the class output is y ( x , W ) =  X  of the subspace.

The CFPC uses class-specific subspaces as well as the residuals of subspace projection: y ( x , W ) =  X  where z kj ( x ) is the projection of x onto the j -th principal axis of the subspace of  X  k , k ( x ) is the projection residual.
For saving the computation of projection onto class-specific subspaces, the CFPC is trained class by class [ 21 ], i.e., the binary one-versus-all classifiers are trained separa-tely. The other four neural networks are trained for all classes simultaneously. The weights of the neural networks are trai-ned by minimizing the squared error criterion by stochastic gradient descent. 4.2 SVM classifiers The one-versus-all SVM classifier has multiple binary SVMs each separating one class from the others. The discriminant function of a binary classifier is f ( x ) = where is the number of learning patterns, y i is the tar-get value of learning pattern x i (+1/  X  1 for positive/negative class), b is a bias, and k ( x , x i ) is a kernel function which implicitly defines an expanded feature space: k ( x , x i ) = ( x )  X  ( x i ). (18) Two types of kernels, polynomial and Gaussian (RBF), are frequently used. They are computed by k ( x , x i , p ) = ( 1 + x  X  x i ) p (19) and k ( x , x i , X  2 ) = exp  X  respectively. The SVM classifiers using polynomial kernel and RBF kernel are called SVM-poly and SVM-rbf, respec-tively.

In our implementation of SVMs, the pattern vectors are appropriately scaled for the polynomial kernel, with the sca-ling factor estimated from the lengths of the sample vectors. For the Gaussian kernel, the kernel width  X  2 is estimated from the variance of the sample vectors. In partial discrimi-native training (PDT) of a binary SVM for class  X  k , the only change is to remove from negative samples the ones of the allied classes of  X  k . 4.3 LVQ and DLQDF The PDT scheme is also applied to an LVQ classifier with prototypes trained by the MCE method [ 10 ] and a discri-minative learning quadratic discriminant function (DLQDF) classifier with parameters trained by MCE [ 11 ]. During MCE training, the misclassification measure is modified as ( 10 )for ignoring the boundary between overlapping classes.
The LVQ classifier uses equal number of prototypes per class, initialized by k -means clustering on the training data of one class. The prototypes of all classes are then optimized by minimizing the empirical classification error on training data. The discriminant function of a class  X  i is the negative of minimum distance (square Euclidean) to the prototypes of this class: f ( x , X  i ) = X  min where m ij denotes the prototype vectors of class  X  i .
The DLQDF has the same form with the modified qua-dratic discriminant function (MQDF) of Kimura et al. [ 22 ]. The MQDF of a class represents a quadratic distance, whose negative is regarded as the discriminant function:  X  f ( x , X  i ) = d Q ( x , X  i ) = where  X  i denotes the mean vector of class  X  i ,  X  ij and ( j = 1 ,..., d ) are the eigenvectors and eigenvalues of the covariance matrix sorted in descending order of eigenvalues. Retaining k principal eigenvectors, the minor eigenvalues are replaced by a constant  X  i (which is further made class-independent in our implementation). The initial parameters of DLQDF are inherited from the MQDF, and are then opti-mized by minimizing the empirical classification error on training data.

To improve the generalization performance of MCE trai-ning and make the classifier to be resistant to non-character patterns [ 23 ], a regularization term is added to the empirical loss: L where  X  c denotes the genuine class of ( x n ), d (  X  ) stands for either the square Euclidean distance (LVQ classifier) or the quadratic distance (DLQDF). Empirically, the regularization coefficient  X  is set as  X  = 0 . 05 / var for LVQ (var is the average within-class cluster variance) and  X  = 0 . 1 / D Q DLQDF ( D Q is the average within-class quadratic distance estimated with the initial parameters) [ 23 ]. 5 Experimental results The proposed partial discriminative training (PDT) scheme has been evaluated with various classifiers on a public hand-written letter database C-Cube [ 7 , 8 ] 2 and on online hand-written symbols from the HANDS databases of TUAT [ 9 ]. 5.1 Results on C-Cube letter database The C-Cube database contains 57,293 samples of 52 English letters, partitioned into 38,160 training samples and 19,133 test samples. The samples, segmented from handwritten words, are very cursive and the number of samples per class is seriously imbalanced. In addition to the confusion between upper-case and lower-case letters, the confusion between dif-ferent case-insensitive letters is also considerable, such as the letters in Fig. 2 . By k-NN classification based on vector quan-tization, the authors ordered the upper/lower case overlap degree of each letter for merging the cases of selected letters. The database provides binary images as well as extracted feature values (34D) of the samples [ 24 ]. I experimented on the given 34D features as well as 200D gradient direc-tion histogram features (extracted by moment normalization, 8-direction decomposition and 5  X  5 sampling [ 25 , 26 ]).
Three numbers of classes are considered as in [ 7 , 8 ]: 52 case-sensitive letters, 38 classes by merging the upper/lower cases of 14 letters ( X  X XOWYZMKJUNFVA X ), and 26 case-insensitive letters. In the cases of 38 classes and 26 letters, each merged upper-case letter is allied with its lower-case and vice versa. The configuration parameters of some classi-fiers (number of hidden units N h for MLP and RBF, principal subspace dimensionality m for PNC and CFPC, number of prototypes n per class for LVQ, number of principal eigen-vectors k for DLQDF) were empirically set as in Table 1 . PCA was not performed for PNC on the original 34D fea-tures ( m = 34). These configurations are not guaranteed to be optimal but perform reasonably well. The SVM-poly uses a fourth order polynomial kernel on uniformly re-scaled pattern vectors, and the SVM-rbf uses an RBF kernel with kernel width fixed at 0.5 times the average within-class variance [ 25 ].

First,fourmulti-classneuralnetworks(SLNN,MLP,RBF, and PNC) were trained with three training schemes optimi-zing the squared error: ordinary discriminative training, PDT, and cross-training (enhancing the label of each sample with its allied classes). The error rates on test samples are shown in Table 2 , where each row gives the accuracies evaluated at a number of metaclasses (52, 38 or 26, within-metaclass sub-stitution is ignored). By ordinary discriminative training, the number of classes can be reduced by class merging, whereas by PDT and cross-training, the number of classes remains unchanged but the samples are attached allied classes or multi-labeled. Each classifier can be evaluated at a redu-ced number of classes by ignoring within-metaclass substitu-tion. At each row (evaluated at a number of metaclasses), the lowest error rate is highlighted in bold face, and the error rates of merged metaclass training and PDT are boxed for compa-rison. The error rates of metaclass classification of all-class discriminative training are underlined for comparison with merged metaclass training and PDT.

The difference of error rates is measured by the confi-dence of significance z [ 27 ]. For two error rates p 1 and p evaluated on n test samples, z = ( p 2  X  p 1 )/ X  , where  X  =  X  2 p ( 1  X  p )/ n ( p is the average error rate). If | z | &gt; two error rates are judged to be significantly different with confidence higher than 0.95. The z values for PDT compa-red to all-class discriminative training (DT) and PDT com-pared to class merging (CM) are shown in Table 2 . When | z | &gt; 1 . 96, a  X (+) X  or  X (  X  ) X  is attached to indicate signifi-cant superiority or inferiority, otherwise the error rates are comparable.

InTable 2 ,itisevidentthatall-classdiscriminativetraining (third column) yields the lowest error rate for 52-class classi-fication. This is reasonable because all the classes are aimed to be separated in this case, while PDT ignores the separa-tion between allied classes. When evaluated at reduced num-ber of classes, however, merged metaclass training (fourth and fifth columns) and PDT (sixth and seventh columns) may give lower error rates than all-class training. In seven of eight cases (two class numbers 38 and 26 combined with four classifiers), PDT gives lower error rates than both all-class training and merged metaclass training. The inferior perfor-mance of cross-training can be explained that the framework of multi-labeled classification does not match the problem of overlapping classification.

Ordinary discriminative training and PDT were then applied to three one-versus-all classifiers (CFPC, SVM-poly and SVM-rbf) and LVQ and DLQDF classifiers. The error rates are shown in Table 3 . Again, all-class discriminative training gives the lowest error rates for 52-class classifica-tion. When evaluated at metaclass level, both merged meta-class training and PDT gives lower error rates than all-class training. For the CFPC, LVQ and DLQDF classifiers, PDT also outperforms merged metaclass training. For the SVM classifiers, merged metaclass training gives the lowest error rates of metaclass classification, but the error rates of PDT are closely competitive.

The error rates of discriminative training and PDT using 200D gradient features are shown in Table 4 . We can see that the error rates are lower than the ones of same classifiers in Tables 2 and 3 . This is because the gradient direction his-togram feature is more discriminative than the original 34D features given by [ 24 ]. Again, the error rate of PDT is compa-red with all-class discriminative training and class merging at metaclass level. In all the 18 cases of metaclass classifica-tion (two metaclass numbers combined with nine classifiers), PDT gives lower error rates than all-class training ( z &gt; 15 of 18 cases, PDT also outperforms the merged metaclass training, and in four cases, PDT outperforms significantly.
Table 5 givesthestatisticsofcomparingPDTwithall-class discriminative training and class merging. In Tables 2 , 3 and error rates). Overall, when evaluated at metaclass level, PDT outperforms all-class discriminative training significantly in 12 of 36 cases, and performs comparably in the remaining cases. Comparing PDT with class merging, PDT outperforms significantly in 10 of 36 cases, and performs comparably in the remaining cases.
 On both two types of features and all classifier structures, PDT yields lower metaclass-level error rates than all-class discriminative training. This confirms that discri-minative training attempting to separate overlapping classes is detrimental to the between-metaclass separation. Compa-ring PDT to merged metaclass training, the contrast of perfor-mance turns out to be depending on feature type and classifier structure, yet the metaclass-level error rate of PDT is lower in most cases. PDT outperforms merged metaclass training significantly on both feature types only with three classifiers: SLNN, MLP and LVQ. This is because the merged metaclass has more complicated distribution than a single class, while some classifiers cannot sufficiently model classes of complex distribution.

Table 6 shows the confusion matrices of a selected classi-fier (DLQDF on 200D gradient feature) trained by all-class discriminative training and PDT. Since it is space-consuming to list confusion numbers between all 26 letters, here I only give the confusion numbers between 13 letters that are fre-quently confused. We can see that by PDT, though the correct rates of two letters ( X  X  X  and  X  X  X ) are considerably lowered, the correct rates of five letters ( X  X mnou X ) are apparently raised. Particularly, the confusions between  X  X  X  X  X  X  X ,  X  X  X  X  X  X  X ,  X  X  X  X  X  X  X ,  X  X  X  X  X  X  X , and  X  X  X  X  X  X  X  are apparently reduced. This is because PDT improves the separation between different letters while sacrificing the separation between upper/lower cases of each letter. 5.2 Results on TUAT-HANDS symbols The HANDS databases of TUAT (Tokyo University of Agri-cultureandTechnology)[ 9 ]includetwodatabases(Kuchibue and Nakayosi) of online handwritten Japanese characters and symbols. The number of symbol classes (including alphanu-meric, hiragana, Katakana, punctuation marks, mathematic symbols, and others) is 380. The symbols have many confu-sing classes that have identical or similar shapes, and so, they cause appreciable percentage of misrecognition [ 21 ]. Based on the fact that many symbols are used in different contexts and are not necessarily separated in isolation, partial discri-minative training (PDT) is suitable for this case to design classifiers focusing on the separation between metaclasses. Conforming to previous works, the symbol samples of the Nakayosi database (163 writers) are used for training classi-fiers, and the samples of the Kuchibue database (120 writers) for testing. Because each writer wrote texts of natural lan-guage and the characters missing in the texts were written only once, the sample numbers of different characters are remarkably variable. My experiments used at most 300 trai-ning/test samples for each symbol. If a symbol has more than 300 samples, 300 of them were selected randomly. The resul-ting training set contains 163 or 300 samples per symbol, and the test set contains 120, 240 or 300 samples per symbol. The total numbers of training samples and test samples are 80,298 and 79,380, respectively.
According to the similarity between symbol shapes, I mer-ged the 380 symbols into 315 metaclasses. The merged classes are shown in Fig. 3 , where the symbols in each group are merged into a metaclass, or allied with each other in par-tial discriminative training.

For symbol recognition, each symbol is represented by a 288D feature vector extracted by moment normalization, trajectory direction decomposition and 6  X  6sampling[ 28 ]. Because of the large number of classes (380 or 315), not all the classifiers described in Sect. 4 are suitable. The MLP and RBF networks were not used in this case because to determine the appropriate number of hidden units is non-trial and the training process is time-consuming and prone to local optima. SVM classifiers with polynomial and RBF kernels were not used since for large number of classes and large sample set, the training process is very time-consuming and the resulting classifier has very high operational com-plexity due to the large number of support vectors. Instead, I tested an SVM classifier with linear kernel (SVM-linear), which has low complexity in both training and operation. The SVM-linear is a linear classifier like the SLNN, but they are trained under different criteria. For the other classifiers, the PNC uses 80 subspace features, the CFPC uses 40D princi-pal subspace per class, the LVQ classifier has 5 prototypes per class, and the DLQDF uses 40 principal eigenvectors per class.
First, I tested two parametric statistical classifiers (gene-rative classifiers): linear discriminant function (LDF) and MQDF.TheLDFassumesGaussiandensitiesandequalcova-riance matrices for all classes. Table 7 shows the error rates evaluated at 380 classes and at 315 metaclasses. We can see that when evaluated at metaclass level, merging classes in training does not benefit the classification accuracy because the class distribution becomes complicated. In contrast, the all-class generative classifier gives higher accuracy of meta-class classification.

The error rates of six discriminative classifiers on the online symbols of TUAT-HANDS database are shown in Table 8 . It is interesting to see that the DLQDF, as the dis-criminative version of the MQDF, gives lower error rates of metaclass classification than the MQDF when it is trained on merged classes or by PDT. The DLQDF trained by all-class discriminative training (DT), however, does not outper-form the generative MQDF. For all the six classifiers, merged metaclass training and PDT give lower error rates of 315-metaclass classification than all-class DT. PDT outperforms all-class DT significantly in all the six cases. Comparing PDT and merged metaclass training, PDT performs comparably in five cases and loses in one case.

Unlike in handwritten letter recognition where the upper/lower cases of most letters are partially overlapped, the overlap between allied classes of online symbols is so heavy that the shapes are almost identical, especially the pairs of hiragana/Katakana characters. This is why ordinary dis-criminative training with class merging performs very well. Anyway, PDT performs comparably and the trained classifier provides confidence scores to all classes, unlike the merged metaclass classifier that totally ignores the separation bet-ween allied classes. 6 Conclusion This paper proposed a partial discriminative training (PDT) scheme for classification of overlapping classes. It is appli-cable to all types of binary one-versus-all classifiers, inclu-ding neural networks, SVM classifiers, and the classifiers trained by the MCE method. The rationale of PDT is to ignore the difference between overlapping classes in training so as to improve the separation between metaclasses. Experiments in offline handwritten letter recognition and online handwritten symbol recognition show that when evaluated at metaclass level, the PDT scheme mostly outperforms ordinary all-class discriminative training. Compared to merged metaclass trai-ning, the PDT gives higher or comparable accuracies at meta-class level and provides more informative confidence scores.
We plan to apply PDT to character string recognition and multiple classifier systems. In character string recognition, since linguistic context and geometric context are available for disambiguating confusing characters, to make the classi-fier focus on discrimination between metaclasses while sacri-ficing the separation between confusing classes will be mea-ningful. In multiple classifier systems, combining a classi-fier focusing on between-metaclass discrimination and some others focusing on within-metaclass discrimination will be beneficial.
 References
