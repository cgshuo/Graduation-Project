 The problem of identifying high-dimensional activation patterns embedded in noise is important for applications such as contamination monitoring by a sensor network, determining the set of differen-tially expressed genes, and anomaly detection in networks. Formally, we consider the problem of identifying a pattern corrupted by noise that is observed at the p nodes of a network: bution with mean zero and variance  X  2 . This problem is particularly challenging when the network is large-scale, and hence x is a high-dimensional pattern embedded in heavy noise. Classical ap-proaches to this problem in the signal processing and statistics literature involve either thresholding the measurements at every node, or in the discrete case, matching the observed noisy measurements with all possible patterns (also known as the scan statistic). The first approach does not work well when the noise level is too high, rendering the per node activity statistically insignificant. In this case, multiple hypothesis testing effects imply that the noise variance needs to decrease as the num-ber of nodes p increase [10, 1] to enable consistent mean square error (MSE) recovery. The second approach based on the scan statistic is computationally infeasible in high-dimensional settings as the number of discrete patterns scale exponentially (  X  2 p ) in the number of dimensions p . In practice, network activation patterns tend to be structured due to statistical dependencies in the network activation process. Thus, it is possible to recover activation patterns in a computationally and statistically efficient manner in noisy high-dimensional settings by leveraging the structure of Figure 1: Threshold of noise variance below which consistent MSE recovery of network activation patterns is possible. If the activation is independent at each node, noise variance needs to decrease as network size p increases (in blue). If dependencies in the activation process are harnessed, noise variance can increase as p  X  where 0 &lt;  X  &lt; 1 depends on network interactions (in red). the dependencies between node measurements. In this paper, we study the limits of MSE recovery of high-dimensional, graph-structured noisy patterns. Specifically, we assume that the patterns x are generated from a probabilistic model, either Gaussian graphical model (GGM) or Ising (binary), based on a general graph structure G ( V,E ) , where V denotes the p vertices and E denotes the edges. In the Ising model, L = D  X  W denotes the graph Laplacian, where W is the weighted adjacency model, L =  X   X  1 denotes the inverse covariance matrix whose zero entries indicate the absence of an edge between the corresponding nodes in the graph. The graphical model implies that all patterns are not equally likely to occur in the network. Patterns in which the values of nodes that are connected by an edge agree are more likely, the likelihood being determined by the weights W ij of the edges. Thus, the graph structure dictates the statistical dependencies in network measurements. We assume that this graph structure is known, either because it corresponds to the physical topology of the network or it can be learnt using network measurements [18, 25].
 In this paper, we are concerned with the following problem: What is the largest amount of noise that can be tolerated, as a function of the graph and parameters of the model, while allowing for con-sistent reconstruction of graph-structured network activation patterns? If the activations at network nodes are independent of each other, the noise variance (  X  2 ) must decrease with network size p to ensure consistent MSE recovery [10, 1]. We show that by exploiting the network dependencies, it is possible to consistently recover high-dimensional patterns when the noise variance is much larger (can grow with the network size p ). See Figure 1.
 network. To this end, we propose using an estimator based on thresholding the projection of the network measurements onto the graph Laplacian eigenvectors. This is motivated by the fact that in the Ising model, unlike the GGM, the Bayes rule and it X  X  risk have no known closed form. Our results indicate that the noise threshold is determined by the eigenspectrum of the Laplacian. For the GGM this procedure reduces to PCA and the noise threshold depends on the eigenvalues of the covariance matrix, as expected. We show that for simple graph structures, such as hierarchical or lattice graphs, as well as the random Erd  X  os-R  X  enyi graph, the noise threshold can possibly grow in the network size p . Thus, leveraging the structure of network interactions can enable extraction of high-dimensional patterns embedded in heavy noise.
 This paper is organized as follows. We discuss related work in Section 2. Limits of MSE recov-ery for graph-structured patterns are investigated in Section 3 for the binary Ising model, and in Section 4 for the Gaussian graphical model. In Section 5, we analyze the noise threshold for some simple deterministic and random graph structures. Simulation results are presented in Section 6, and concluding discussion in Section 7. Proof sketches are included in the Appendix. Given a prior, the Bayes optimal estimators are known to be the posterior mean under MSE, the Maximum A Posterior (MAP) rule under 0/1 loss, and the posterior centroid under Hamming loss [8]. However, these estimators and their corresponding risks (expected loss) have no closed form for the Ising graphical model and are intractable to analyze. The estimator we propose based on the graph Laplacian eigenbasis is both easy to compute and analyze. Eigenbasis of the graph Laplacian has been successfully used for problems, such as clustering [20, 24], dimensionality reduction [5], and semi-supervised learning [4, 3]. The work on graph and manifold regularization [4, 3, 23, 2] is closely related and assumes that the function of interest is smooth with respect to the graph, which is essentially equivalent to assuming a graphical model prior of the form in Eq. (2). However, the use of graph Laplacian is theoretically justified mainly in the embedded setting [6, 21], where the data points are sampled from or near a low-dimensional manifold, and the graph weights are the distances between two points as measured by some kernel. To the best of our knowledge, no previous work studies the noise threshold for consistent MSE recovery of arbitrary graph-structured patterns. There have been several attempts at constructing multi-scale basis for graphs that can efficiently rep-resent localized activation patterns, notably diffusion wavelets [9] and treelets [17], however their approximation capabilities are not well understood. More recently, [22] and [14] independently pro-posed unbalanced Haar wavelets and characterized their approximation properties for tree-structured binary patterns. We argue in Section 5.1 that the unbalanced Haar wavelets are a special instance of graph Laplacian eigenbasis when the underlying graph is hierarchical. On the other hand, a lattice graph structure yields activations that are globally supported and smooth, and in this case the Lapla-cian eigenbasis corresponds to the Fourier transform (see Section 5.2). Thus, the graph Laplacian eigenbasis provides an efficient representation for patterns whose structure is governed by the graph. The binary Ising model is essentially a discrete version of the GGM, however, the Bayes rule and risk for the Ising model have no known closed form. For binary graph-structured patterns drawn from an Ising prior, we suggest a different estimator based on projections onto the graph Laplacian eigenbasis. Let the graph Laplacian L have spectral decomposition, L = U  X  U T , and denote the first k eigenvectors (corresponding to the smallest eigenvalues) of L by U [ k ] . Define the estimator which is a hard thresholding of the projection of network measurements y = [ y 1 ,...,y p ] onto the graph Laplacian eigenbasis. The following theorem bounds the MSE of this estimator.
 Theorem 1. The Bayes MSE of the estimator in Eq. (3) for the observation model in Eq. (1), when the binary activation patterns are drawn from the Ising prior of Eq. (2) is bounded as where 0 &lt;  X  &lt; 2 is a constant and  X  k +1 is the ( k + 1) th smallest eigenvalue of L . Through this bias-variance decomposition, we see the eigenspectrum of the graph Laplacian deter-mines a bound on the MSE for binary graph-structured activations. In practice, k can be chosen using FDR[1] in the eigendomain or cross-validation.
 Remark: Consider the binarized estimator rem 1 also provide an upper bound on the expected Hamming distance of this new estimator since E [ d H ( If the network activation patterns are generated by a Gaussian graphical model, it is easy to see that the eigenvalues of the Laplacian (inverse covariance) determine the MSE decay. Consider the GGM prior as in Eq. (2), then the posterior distribution is where I is the identity matrix. The posterior mean is the Bayes optimal estimator with Bayes MSE,
P a result similar to Theorem 1 for the sake of bounding the performance of the Bayes rule. Figure 2: Weight matrices corresponding to hierarchical dependencies between node variables. Theorem 2. The Bayes MSE of the estimator in Eq. (3) for the observation model in Eq. (1), when the activation patterns are drawn from the Gaussian graphical model prior of Eq. (2) is bounded as Hence, the Bayes MSE for the estimator of Eq. (3) under the GGM or Ising prior is bounded above by 2 / X  k +  X  2 k/p + e  X  p which is the form used to prove Corollaries 1, 2, 3 in the next section. In this section, we discuss the eigenspectrum of some simple graphs and use the MSE bounds derived in the previous section to analyze the amount of noise that can be tolerated while ensuring consistent MSE recovery of high-dimensional patterns. In all these examples, we find that the tolerable noise level scales as  X  2 = o ( p  X  ) , where  X   X  (0 , 1) characterizes the strength of network interactions. 5.1 Hierarchical structure Consider that, under an appropriate permutation of rows and columns, the weight matrix W has the hierarchical block form shown in Figure 2. This corresponds to hierarchical graph structured dependencies between node variables, where ` &gt; ` +1 denote the strength of interactions between nodes that are in the same block at level ` = 0 , 1 ,...,L . It is easy to see that in this case the eigenvectors u of the graph Laplacian correspond to unbalanced Haar wavelet basis (proposed in [22, 14]), i.e. u  X  1 | c same level that are merged together at the next level (see [19] for the case of a full dyadic hierarchy). Lemma 1. For a dyadic hierarchy with L levels, the eigenvectors of the graph Laplacian are the standard Haar wavelet basis and there are L + 1 unique eigenvalues with the smallest eigenvalue  X  0 = 0 , and the ` th smallest unique eigenvalue ( `  X  [ L ] ) is 2 `  X  1 -fold degenerate and given as Using the bound on MSE as given in Theorems 1 and 2, we can now derive the noise threshold that allows for consistent MSE recovery of high-dimensional patterns as the network size p  X  X  X  . Corollary 1. Consider a graph-structured pattern drawn from an Ising model or the GGM with weight matrix W of the hierarchical block form as depicted in Figure 2. If ` = 2  X  ` (1  X   X  )  X  `  X   X  log 2 p +1 , for constants  X , X   X  (0 , 1) , and ` = 0 otherwise, then the noise threshold for consistent MSE recovery ( R B = o (1) ) is Thus, if we take advantage of the network interaction structure, it is possible to tolerate noise with variance that scales with the network size p , whereas without exploiting structure the noise vari-ance needs to decrease with p , as discussed in the introduction. Larger  X  implies stronger network interactions, and hence larger the noise threshold. 5.2 Regular Lattice structure Now consider the lattice graph which is constructed by placing vertices in a regular grid on a d dimensional torus and adding edges of weight 1 to adjacent points. Let p = n d . For d = 1 [ n ] d . Then the weight matrix of the lattice in d dimensions is where  X  is the Kronecker delta function. This form for W and since all nodes have same degree gives us a closed form for the eigenvalues of the Laplacian, along with a concentration inequality. Lemma 2. Let  X  L  X  be an eigenvalue of the Laplacian, L , of the lattice graph in d dimensions with p = n d vertices, chosen uniformly at random. Then Hence, we can choose k such that  X  L k  X  d and k = d pe  X  d/ 8 e . So, the risk bound becomes O (2 /d +  X  e  X  d/ 8 + e  X  p ) , and as we increase dimensions of the lattice the MSE decays linearly. Corollary 2. Consider a graph-structured pattern drawn from an Ising model or GGM based on a lattice graph in d dimensions with p = n d vertices. If n is a constant and d = 8  X  ln p , for some constant  X   X  (0 , 1) , then the noise threshold for consistent MSE recovery ( R B = o (1) ) is given as: Again, the noise variance can increase with the network size p , and larger  X  implies stronger network interactions as each variables interacts with more number of neighbors ( d is larger). 5.3 Erd  X  os-R  X  enyi random graph structure Erd  X  os-R  X  enyi (ER) random graphs are generated by adding edges with weight 1 between any two vertices within the vertex set V (of size p ) with probability q p . It is known that the probability of edge inclusion ( q p ) determines large geometric properties of the graph [11]. Real world networks of edge inclusion and stronger network interaction structure. Using the degree distribution [7], and a result from perturbation theory, we bound the quantiles of the eigenspectrum of L . Lemma 3. Let  X   X  denote an eigenvalue of L chosen uniformly at random. Let P G be the probability measure induced by the ER random graph and P  X  be the uniform distribution over eigenvalues conditional on the graph. Then, for any  X  p increasing in p , Hence, we are able to set the sequence of quantiles for the eigenvalue distribution k p = d  X  p p 1  X   X  e such that P G {  X  k p  X  p  X  / 2  X  p  X   X  1 } = O (1 / X  p ) . So, we obtain a bound for the expected Bayes MSE (with respect to the graph) E G [ R B ]  X O ( p  X   X  ) +  X  2 O (  X  p p  X   X  ) + O (1 / X  p ) . Corollary 3. Consider a graph G drawn from an Erd  X  os-R  X  enyi random graph model with p vertices and probability of edge inclusion q p = p  X  (1  X   X  ) for some constant  X   X  (0 , 1) . If the latent graph-structured pattern is drawn from an Ising model or a GGM with the Laplacian of G , then the noise variance that can be tolerated while ensuring consistent MSE recovery ( R B = o P G (1) ) is given as: We simulate patterns from the Ising model defined on hierarchical, lattice and ER graphs. Since the Ising distribution admits a closed form for the distribution of one node conditional on the rest of the nodes, a Gibbs sampler can be employed. Histograms of the eigenspectrum for the hierarchical tree graph with a large depth, the lattice graph in high dimensions, and a draw from the ER graph with many nodes is shown in figures 3(a), 4(a), 5(a) respectively. The eigenspectrum of the lattice and ER graphs illustrate the concentration of the eigenvalues about the expected degree of each node. We use iterative eigenvalue solvers to form our estimator and choose the quantile k by minimizing the bound in Theorem 1. We compute the Bayes MSE (by taking multiple draws) of our estimator estimator is a substantial improvement over Naive (the Bayes estimator that ignores the structure). Figure 3: The eigenvalue histogram for the binary tree, L = 11 ,  X  = . 1 (left) and the performance of various estimators (right) with  X  = 0 . 05 and  X  2 = 4 , both with  X  = 1 . Figure 4: The eigenvalue histogram for the lattice with d = 10 and p = 5 10 (left) and estimator performances (right) with p = 3 d and  X  2 = 1 . Notice that the eigenvalues concentrate around 2 d . Figure 5: The eigenvalue histogram for a draw from the ER graph with p = 2500 and q p = p  X  . 5 (left) and the estimator performances (right) with q p = p  X  . 75 and  X  2 = 4 . Notice that the eigenval-ues are concentrated around p  X  where q p = p  X  (1  X   X  ) . Figure 6: The eigenvalue histogram for a draw from the Watts-Strogatz graph with d = 5 and p = 4 5 with 0 . 25 probability of rewiring (left) and estimator performances (right) with 4 d vertices and  X  2 = 4 . Notice that the eigenvalues are concentrated around 2 d . See Figures 3(b), 4(b), 5(b). For the hierarchical model, we also sample from the posterior using a Gibbs sampler and estimate the posterior mean (Bayes rule under MSE). We find that the posterior mean is only a slight improvement over the eigenmap estimator (Figure 3(b)), despite it X  X  difficulty to compute. Also, a binarized version of these estimators does not substantially change the MSE. We also simulate graphs from the Watts-Strogatz  X  X mall world X  model [26], which is known to be an appropriate model for self-organizing systems such as biological systems and human networks. The  X  X mall world X  graph is generated by forming the lattice graph described in Section 5.2, then rewiring each edge with some constant probability to another vertex uniformly at random such that loops are never created. We observe that the eigenvalues concentrate (more tightly than the lattice graph) around the expected degree 2 d (Figure 6(a)) and note that, like the ER model, the eigenspectrum converges to a nearly semi-circular distribution [12]. Similarly, the MSE decays in a fashion similar to the ER model (Figure 6(b)). In this paper, we have characterized the improvement in noise threshold, below which consistent MSE recovery of high-dimensional network activation patterns embedded in heavy noise is possi-ble, as a function of the network size and parameters governing the statistical dependencies in the activation process. Our results indicate that by leveraging the network interaction structure, it is possible to tolerate noise with variance that increases with the size of the network whereas with-out exploiting dependencies in the node measurements, the noise variance needs to decrease as the network size grows to accommodate for multiple hypothesis testing effects.
 While we have only considered MSE recovery, it is often possible to detect the presence of patterns Establishing the noise threshold for detection, deriving upper bounds on the noise threshold, and extensions to graphical models with higher-order interaction terms are some of the directions for future work. In addition, the thresholding estimator based on the graph Laplacian eigenbasis can also be used in high-dimensional linear regression or compressed sensing framework to incorporate structure, in addition to sparsity, of the relevant variables.
 Proof sketch of Theorem 1: First, we argue that whp, x T Lx  X   X p , where 0 &lt;  X  &lt; 2 is a constant. Let  X  = { x : x T Lx  X   X p } and  X   X  denotes its complement. By Markov X  X  inequality, for t &gt; 0 , Let  X  denote the uniform distribution over { 0 , 1 } p and N ( L ) = R  X  ( dx ) e  X  x T Lx . Then, N ( L ) ,N ((1  X  t ) L )  X  2 p ,  X  t  X  (0 , 1) . This gives us the Chernoff-type bound, by setting C =  X p and  X  = 1+log 2 t . If we choose t &lt; 1+log 2 2 then  X  &lt; 2 .
 Let u i denote the i th eigenvector of the graph Laplacian L , then under this orthonormal basis,
E [ k b x k  X  x k 2 ]  X  E [ i th eigenvalue of L . Consider the primal problem,  X p/ X  k +1 . Also, ||  X  x || 2  X || x || 2  X  p , so we obtain the desired bound.
 Proof sketch of Theorem 2: Under the same notation as the previous proof, notice that u T i x  X  is increasing in p . Therefore, we can pick k = 2 `  X  and since 2 `  X  /p = p  X   X  , the result follows. Proof sketch of Lemma 2: If v 1 ,...,v d are a subset of the eigenvectors of w with eigenvalues  X  distributed uniformly over [ n ] d . Then E [  X  w i So, using t = d we get that P { P d j =1 (2  X   X  w i Proof of Lemma 3: We introduce a random variable  X  that is uniform over [ p ] . Note that, con-ditioned on this random variable, d  X   X  Binomial ( p  X  1 ,q p ) and Var ( d  X  )  X  pq p . We decompose the Laplacian, L = D  X  W = (  X  d I  X  W ) + ( D  X   X  d I ) , into the expected degree of each vertex (  X  d = ( p  X  1) q p ), W and the deviations from the expected degree and use the following lemma. Lemma 4 (Wielandt-Hoffman Theorem) . [15, 27] Suppose A = B + C are symmetric p  X  p matrices and denote the ordered eigenvalues by {  X  A i , X  B i } p i =1 . If || . || F denotes the Frobenius norm, is known that for  X   X  (0 , 1) the eigenvalues converge to a semicircular distribution[12] such that P for large enough p (ii) . Using triangle inequality, for any  X  p which is an increasing positive function in p . We now analyze the right hand side. Hence, we are able to complete the lemma, such that for p large enough, using Eqs. (10) and (9) Proof sketch of Corollary 3: By lemma 3 and appropriately specifying the quantiles, Note that we have the freedom to choose  X  p = p p  X  / X  2 making  X  2 O (  X  p p  X   X  ) = O ( p  X  2 /p  X  ) = o (1) and O (1 / X  p ) = o (1) if  X  2 = o ( p  X  ) .
