 ORIGINAL PAPER Moftah Elzobi  X  Ayoub Al-Hamadi  X  Zaher Al Aghbari  X  Laslo Dings Abstract Even though a lot of researches have been con-ducted in order to solve the problem of unconstrained hand-writing recognition, an effective solution is still a serious challenge. In this article, we address two Arabic handwriting recognition-related issues. Firstly, we present IESK-arDB, a new multi-propose off-line Arabic handwritten database. It is publicly available and contains more than 4,000 word images, each equipped with binary version, thinned version as well as a ground truth information stored in separate XML file. Additionally, it contains around 6,000 character images segmented from the database. A letter frequency analysis showed that the database exhibits letter frequencies simi-lar to that of large corpora of digital text, which proof the database usefulness. Secondly, we proposed a multi-phase segmentation approach that starts by detecting and resolv-ing sub-word overlaps, then hypothesizing a large number of segmentation points that are later reduced by a set of heuris-tic rules. The proposed approach has been successfully tested on IESK-arDB. The results were very promising, indicating the efficiency of the suggested approach.
 Keywords Arabic OCR  X  Off-line handwriting recognition  X  Handwritten Arabic database  X  Handwriting segmentation  X  Baseline estimation 1 Introduction Since more than four decades, research in the field of off-line, unconstrained, handwriting recognition is going on with insignificant results [ 1 , 2 ]. Obstacles that slow down progress in the field are tightly correlated with the infinite variability nature of the handwriting recognition problem [ 2  X  4 ]. On the other hand, the complete dominance of the Internet as the main source of knowledge enforces a proper conversion of knowledge into an editable and searchable format, so that such priceless knowledge can be not only preserved, but also mined for information [ 5 , 6 ].

This article contributes, firstly, by introducing an off-line handwritten Arabic database. The database is developed in the Institute for Electronics, Signal Processing and Commu-nication (IESK) at Otto-von-Guericke University Magde-burg, Germany. It is named IESK-ArDB, where  X  X ESK X  is the German abbreviation of the Institute name and  X  X rDB X  stands for  X  X rabic Database X .

The demand for a database that is carefully designed, well articulated, covering different parts of speech, and mean-while useable for segmentation-based as well as segmen-tation-free recognition approaches, was the main motive behind developing IESK-arDB [ 7 , 8 ]. Unlike other databases in the field, IESK-arDB is the first database that comes equipped with manually annotated information regarding let-ter boundaries in the word images, in addition to other ground truth information that will be detailed in Sect. 3 . We believe such information is of crucial importance for the training and validation purposes in any segmentation-based recognition approach. The database is freely available in its first version at ( www.iesk-ardb.ovgu.de ). Nowadays, optical character recognition (OCR) systems built upon segmentation-free approaches are put successfully into service in a number of application areas such as automatic reading of postal addresses and bank checks, processing documents such as forms. [ 9 , 10 ]. The reason why such systems cannot cover the wide spectrum of applications successfully and achieve com-parable results is that such systems are completely avoiding segmentation, which is an intuitively pre-requisite operation to reduce the infinite domain of possible words into a lim-ited number of classes (graphemes or characters) that can be accommodated and processed further [ 11 , 12 ].

Given the importance and the difficulty of the segmenta-tion problem, solving such problem would be a great achieve-ment in the field of OCR applications [ 13 , 14 ]. Hence, the article X  X  second contribution is methodology for segmenta-tion of handwritten Arabic words, based on topological fea-tures. The methodology starts by pre-processing steps such as small holes filling and smoothing the outcropping pixels, so limitation of extraction stage can be compensated. Then to reduce the extreme variability of handwriting, normaliza-tion issues such as slant and skew correction are considered. The segmentation then conducted through dual-phase proce-dure. In the first phase, a connected component analysis is performed, in order to resolve sub-words overlapping. Then, topological feature X  X ased segmentation is carried out to seg-ment the word into identifiable units representing their con-stituent characters.

The rest of the paper is organized as follows. In Sect. 2 , we give a brief review of related literature. The development process of IESK-arDB database and a letter frequency analy-sis of its content are described in Sect. 3 . Section 4 presents the proposed segmentation methodology, which starts by text normalization issues, then resolving the sub-words overlap-ping problem, and ends with segmentation of character rep-resentatives. Section 5 illustrates and discusses the results. Finally, Sect. 6 concludes the paper and suggests future works. 2 Previous works In the literature, multiple research works reported the use of several off-line Arabic text databases. Table 1 summarizes most of the published databases that support off-line OCR X  X  research.

The most common used database in the field of off-line handwritten Arabic text recognition is IFN/ENIT database, which contains exclusively Tunisian town/village names. The database was created by the Institute of Communi-cation Technology (IFN) at Technical University Braun-schweig in Germany and the Ecole Nationale d X  X ng X nieurs de Tunis (ENIT) in Tunisia. The database is presented in [ 15 ]. It is reported that 411 writers have participated in gen-eration of 26,459 handwritten Tunisian twon/village names. The database is freely available at ( www.ifnenit.com ). The word images come with automatically generated ground truth information, like baseline coordinates, number of sub-words, number of characters. It contains no information about character borders or sub-word borders that can be helpful in facilitating the training and validation processes of any segmentation-based recognition approach.
 In the Center of Excellence for Document Analysis and Recognition (CEDAR), 10 people participated in creation of an Arabic database by writing 10 different page of text each. Every page compromises approximately 150 X 200 word. In total, the database contains 20,000 word images [ 16 ]. Cur-rently, the database is not available online.
In [ 17 ], a database for off-line Arabic handwriting (AHDB) is presented. Samples are collected from 100 writ-ers. The database contains words used in writing bank legal amounts, the most popular Arabic words, and also a freely available handwritten pages of text. Even though it has been reported that the database is freely available, it cannot be found on the Internet.

Another database of handwritten checks (CENPARMI) is introduced in [ 18 ]. It consists basically of 3,000 check images. From which 29,498 sub-word images, 15,175 digit images, and 2,499 legal a mount images are extracted and labeled. The database is designed to facilitate automatic check reading research for the banking and finance sectors. The database is freely available.

The applied media analysis (AMA) developed an Arabic handwritten full page dataset. It contains a set of 5,000 pages of handwritten text, transcribed by 49 different writers from six different countries. The collection contains various docu-ment types including, forms, memos, poems, diagrams, and number lists in both Arabic and Indic digits. The dataset is available and can be downloaded at charge [ 19 ].

Compared to off-line handwriting, databases for type-written and printed text are larger in size and more com-prehensive, since the process of collecting or producing such text can be easily automated.

A large-scale database called APTI is synthetically gen-erated for Arabic printed words. It is extracted from lexicons and contains 45,313,600 single word. Images are totaling to more than 250 million characters, 10 fonts, 10 font sizes, and 4 font style [ 20 ]. The database is available through the web site of DIVA group at the University of Fribourg, Switzer-land.

A database for machine printed Arabic text consists of 750 pages is created by the Environmental Research Institute of Michigan (ERIM). Images with different quality degrees are extracted from books and magazines typed in different font types and sizes [ 21 ]. The database is currently inaccessible.
Farrahi and Chariet in [ 22 ] illustrate the process of creat-ing IBN SINA database. The database contains 1,000 Arabic sub-words extracted from 50 folios of a historical Arabic handwritten manuscript. It is freely available for download, with sub-word images and their corresponding ground truth information stored as  X .MAT X  files.
 The alphabet and the writing styles of both Arabic and Farsi scripts are almost the same; 2 hence, OCR techniques or databases developed for one script, can be used straight-forwardly for the other. A database for Farsi handwriting called IfN/Farsi is presented in [ 23 ]. The database is devel-oped the same way as of the aforementioned IFN/ENIT data-base. It contains 7.271 binary images of handwritten samples for Iranian province/city names, where data collected from 600 writers. By contacting the authors, the database can be obtained free of charge.

Another database for Farsi handwritten text is illustrated in [ 24 ]. The database called FHT contains 1,000 forms filled out with passages of handwritten text; 250 writers from different ages and education levels are participated. The ground truth is a digital text correspond to the handwritten samples. To obtain this database, authors should be contacted.

As for the problem of handwritten text segmentation; even though most of research in the literature has been focusing more on issues related to Latin and Chinese scripts, most of the proposed techniques (especially for cursive Latin) can be modified and adapted to benefit in case of Arabic script [ 25 , 26 ]. A comprehensive literature reviews of the most sig-nificant research works on segmentation of Latin and Chinese scripts can be found in [ 14 , 27 ], respectively.

While the bulk of literature on segmentation of handwrit-ten Arabic did not report the results of segmentation method-ologies separated from the recognition performance, a few number of research works dedicated only to the segmentation of Arabic handwriting [ 4 , 12 , 26 ].

Hereafter, we will briefly discuss the most important related literature, starting by representatives of works, in which the segmentation is strongly tightened to recognition and the ultimate overall recognition rate is the only indicator of system performance. The last three works are samples of methods that focusing on the problem of Arabic handwriting segmentation.

One of the earliest segmentation-based approach, sug-gested for the recognition of Arabic handwritten text, is the one proposed by Almullim and Yamaguchi [ 3 ]. In this approach, words are over-segmented into their basic strokes, where a stroke is the curve between any two structure points (end points or branch points). Each stroke is then classified to one of five groups according to its shape. Two of these groups contain what is called secondary strokes, and the other three contain primary strokes. Furthermore, a set of heuristics pro-posed to assign secondaries to their primary strokes, in a try to construct the corresponding character. No segmentation results are reported, while recognition rate reported to be 81.25 %.

In [ 28 ], Bushofa and Spann propose a segmentation-based recognition methodology for off-line printed Arabic text. The segmentation algorithm stars by locating the text baseline. Having the baseline discovered, a fixed size window is used along the baseline to search for specific types of angles that are expected to be formed when letters joined together. Mul-tiple heuristics rules are used to confirm the segmentation results. Finally, features are extracted and a decision tree X  based classifier is used for recognition; a recognition rate of 94.17 % is reported. In an attempt to avoid over-segmentation, Atici and Yarman-Vural [ 29 ] proposed an analytical segmentation approach for type-written Arabic text, that is trying to extract the whole stroke that represents the character by means of the so-called Character Key Feature Set (KF), which is the set of end points, branch points, loop points, and dot points from the word thinned image. First, the minima and max-ima of the thinned image are calculated; then, the so-called Key Features Segments (KFSg) are determined. Secondly, a set of heuristic rules that employ KF set are applied on the set of the minima in order to elect cut candidates among them. A set of chain code features are extracted, and a hid-den Markov model X  X ased classifier is used for recognition. Authors reported a recognition rate of 96 %.

Abuhaiba et al. [ 30 ] presented a recognition system for off-line Arabic handwritten text. The system is segmentation-based one; thinned and smoothed images of the strokes (sub-word) are processed and converted into 1D representations called direct straight-line approximation. The representation is processed further to produce a loop-less graph called the reduced graph where loops replaced by vertices. Ultimately, the reduced graph representative is segmented into tokens that are fed to a fuzzy sequential machine for recognition. As a sub-word (no segmentation) recognizer, the proposed system achieved 55.4 % recognition rate. When segmentation involved, the recognition rate degraded to 51.1 %.

Lorigo and Govindaraju [ 12 ] propose an algorithm for segmentation of handwritten Arabic text. The algorithm inte-grated a gradient-based and down-up-based techniques to detect all possible break points. Then, by exploiting a prior knowledge of letter shapes, all inaccurate candidates are fil-tered out. By testing the algorithm on a set of 200 images, authors reported 92.3 % success rate.

In Xiu et al. [ 26 ], proposed probabilistic segmentation model, in which a tentative, contour-based over-segmentation is first performed on the text image; as a result, a set of what they called graphemes is produced. The approach differenti-ates among three types of graphemes. The confidence of each character is calculated according to the probabilistic model, respecting other factors, for example, recognition output, geometric confidence, and logical constraint. The authors experimented the proposed methodology on five different test sets, achieving 59.2 % success rate.

In a previous work [ 4 ], we proposed a structural feature X  based segmentation methodology for handwritten Arabic text. To segment letters X  representatives, a heuristic-based algorithm is formulated on a top of a set of structural fea-ture points, and then tested over a 50 samples of handwritten sentences, scoring a rate of 64 % correct segmentation of let-ters constituting all sentences. Although similar features and heuristics are used in the segmentation approach presented in this paper, we enhanced it further by introducing PAWs (Parts of Word)? overlap-resolving step, which simultane-ously improves the performance and reduces the number of heuristics needed. 3 IESK-arDB database The IESK-arDB is an off-line handwritten database. It con-tains more than 4,000 word images and 6,000 segmented character images. The database vocabulary covers most of Arabic part of speech (noun, verb, etc.), country/city names, security terms, and words used for writing bank amounts.
For the purpose of collecting data, we used an entry form consisting of eight pages, each page should be filled with eight handwritten words presented to the writers. Samples are collected from 22 writers from different Arabic countries and also from countries where the Arabic script is the writ-ing medium. Figure 1 shows a page from the form used for collecting handwritten samples. Writers have been asked to write according to Naskh style as much as they can. This has two reasons. First, Naskh is the most common used writ-ing style. Second, compared to other writing styles, Naskh emphasizes most of letters structural peculiarities [ 31 ]. 3.1 Data acquisition The extraction process started by scanning the filled forms with a flatbed Epson Perfection V300 Photo scanner. Usu-ally scanning with 300 dpi is enough for subsequent OCR processing steps. In Arabic, besides the alphabet there are what so-called Diacritic Symbols, which are used to indi-cate vowels, and written in a very small size (compared to letters size) above or below a letter (e.g., ).
 To increase the ability to capture details necessary to distin-guish among such symbols, we choose to scan forms at a little higher resolution (350 dpi) that increases quality and in the same time not costing a lot of resources.

Each word is extracted from the form automatically and given a label of 11 digits, according to the following coding  X  X 9999-99999 X , where  X  X  X  and  X 9 X  stand for letters and numbers, respectively, the two first digits reserved for the form X  X  id (e.g., A0 ... A9, B0 ... B9), the next three used for user X  X  id, and the last five assigned for a serial number for different words. We adapted such encoding, so that any future growth of the database can be encompassed and also to enable searching and sorting the database content efficiently. Additionally, to distinguish between different formats, binary and thinned image files are preceded with  X  X _ X  and  X  X _ X , respectively.

We chose to save images in PNG graphic file format rather than other format, for example, TIFF, BMP, JPEG, or GIF, since PNG files are relatively smaller in size with no loss in quality. Extracted images have been enhanced by improving contrast and reducing noise, then archived in three different formats. The first format is an enhanced gray scale version; the second is a globally binarized white on black version; and the third is a thinned version, where the thinning approach adapted is based on the Zhang-Suen X  X  thinning algorithm [ 32 ]. Figure 2 shows a three gray samples from the data-base (A, B, and C), associated with their binary and thinned counterparts. 3.2 Database annotation Ground truth information is always an essential part of every OCR database, since it is a prerequisite for recognition exper-iments [ 15 ]. In our database, each word is fully described by a ground truth XML file that contains several important entries. Figure 3 shows an example of one of such files. All ground truth files are named after their corresponding gray word image file name. The ground truth information includes the following: 1. The  X  X d X  tag contains the binary word image name with-2. The  X  X etterLabel X  tag contains Labels of the word X  X  let-3. The  X  X niCode X  tag contains the UNICOD equivalence 4. The  X  X ubWord X  contains continues two type of tags. The 5. The  X  X aseLine X  tag contains coordinates of two points 6. The  X  X xtIfo X  tag contains additional information like the
We belief that ground truth information for segmentation is of crucial importance for recognition of off-line cursive scripts (e.g Arabic) [ 7 ]; hence, we included entries that indi-cate the sub-words bounding boxes coordinates and charac-ters X  borders within each sub-word.

Figure 4 shows a word image that contains a three over-lapping sub-words, which indicated with the tag  X  X ubword-1, -2,-3 X  in Fig. 3 and bounded with boxes in Fig. 4 . The first sub-word from the right consist of two characters with their border point coordinates indicated with the tag  X  X etter X  in Fig. 3 and with pink line in Fig. 4 . The second sub-word con-tains only one character; hence, it is a sub-word as well as a letter in the same time. Similarly, the third sub-word contains three characters and two border points between characters.
To the best of our knowledge, our database is the first database of its kind that is equipped with such information. 3.3 Letters frequency analysis Language statistics are the foundation upon which differ-ent language models can be built [ 33 ]. Word-based language models and/or character-based language models are statis-tical models that provide prior knowledge about words or letters occurrences, respectively. Hence, we conducted letter frequency analysis on our database (IESK-arDB).

Figure 5 shows the frequency distribution of basic let-ters in IESK-arDB with their respective occurrence shapes (Begin, End, Isolated, Middle); from the distribution, read-ers can easily observe that letter Alf  X   X  exhibits the highest frequency rate while Tha  X   X  is the less frequent one.
Figure 6 illustrates the letters distribution in IESK-arDB compared to the letters distribution in huge corpora of text that contains 1,297,259 words or 5,122,132 letters [ 34 ]. A normalized chi-square test shows that letter frequency in both sources is nearly following the same distribution with a goodness fit value of  X  2 = 0 . 980. These results prove that the IESK-arDB is unbiased and follows the common distribution of language model. 4 Segmentation methodology The proposed methodology consists mainly of two phases. In the first phase, the issue of pre-processing is considered, in which fundamental pre-processing steps, for example, fil-tering, binarization, as well as pre-processing issues specific to handwriting are conducted.

The second phase is the segmentation, which starts by resolving the sub-words overlapping that resulted whenever disconnected sub-words vertically overlapped; such as the first two sub-words in Fig. 4 . Then, a heuristic-based voting procedure is followed to elect the most probable segmenta-tion point. 4.1 Pre-processing To suppress noisy pixels while preserving edges, a median filter is applied on the gray scale images. As a result of the extraction and binarization processes, issues like smooth-ing out outcropping pixels and small holes filling should be dealt with. Morphological-based operations such as Closing and combination of Opening and Reconstr ucti on are employed, respectively, to solve for those issues.

To reduce the amount of information to be processed, to the minimum necessary for conducting our segmentation, and also to ease the process of extraction of features points, thinning operation is applied on the enhanced binary images according to [ 32 ]. In the rest of this subsection, we will illus-trate enhancements on two of off-line handwriting-specific pre-processing issues, namely skew and slant correction. 4.1.1 Skew correction and baseline estimation Skew correction and baseline detection is proven to be of critical importance for segmentation of handwritten Arabic text. Various techniques have been reported in the literature; each with its pros and cons. Hough transform ( HT )isone of such method that is relatively insensitive to noise and tol-erates gabs within Arabic words [ 35 ]. As for the baseline detection, HT is insensitive to line direction. Consequently, it performs badly when the longest stroke is not parallel to the word baseline.

Another method is based on the linear regression of local minima of the word image skeleton ( LMR )[ 36 ]. Benefiting from the fact that most of the local minima ( LM ) points are usually occurring on or near of the baseline; the problem of finding the baseline can be reduced to a linear fitting problem of local minima points. Even though LMR is not as accurate as HT , its main advantage is the relatively insensitivity to stroke direction that is not parallel to the baseline. Experimentally, we noticed that reducing the domain of HT  X  X   X  parameter according to a priori direction estimation, firstly, increases accuracy, and secondly, reduces the compu-tational cost. Thus, we propose a HT -based technique com-bined with a LMR , for baseline estimation. The LMR is used for a priori estimation of the HT  X  X   X  parameter.
The first step in the proposed technique starts by calculat-ing the fitting line of LM points according to Eqs. ( 1 ), ( 2 ), and ( 3 ). The slope angle  X  of the fitted line is then calculated according to Eq. ( 4 ) y = a + bx (1) where the coefficients a , b were calculated as follows b = a =  X  y  X  b  X  x (3) where  X  x and  X  y are the statistical means of x and y coordinates, respectively.  X  = arctan ( b ) (4) The second step is to calculate the baseline using the HT , given the  X  restricted domain. We first discretize the  X  and  X  parameters, and then for each point ( x i , y i ) in the image space, we calculate  X   X  as stated in Eq. ( 5 ):  X   X  = x Where is a constant used to offset the random error that may be produced in the first step. Experimentally, we found that = 10  X  gives the most accurate results. Next, each point in the image space will vote for bins that could have generated it in the hough accumulator A , and votes will be accumulated in A according to Eq. ( 6 ) A (  X   X ,  X   X ) = A (  X   X ,  X   X ) + 1(6)
Finally,  X   X  and  X   X  with the maximum number (global max-ima) of votes will be considered as the parameters of the word baseline as showed in Eq. ( 7 ). arg max
Figure 7 shows an example of the results, where Fig. 7 a is the original image and Fig. 7 b is the skew corrected image and the estimated baseline according to LMR only. Figure 7 c shows the result of the skew correction and the estimated baseline of the word using the proposed technique. The reader can clearly see the improvement. 4.1.2 Slant correction Slant angle is the angle made by the vertical strokes with the absolute vertical direction. In order to reduce variability within handwritten character classes, it is necessary to nor-malize slant variations [ 5 ]. As for the segmentation process, slant correction improves accuracy, thanks to the fact that spaces between vertical strokes will be increased [ 10 , 37 ]. Further, it is being observed that vertical projection his-tograms of the slant free images have higher and clear peaks compared to the slanted ones [ 38 ]. For the detection and cor-rection of the slant in a word image, we adapted the approach detailed in [ 39 ], which basically proposed for skew correc-tion of Latin alphabet X  X ased text. Unlike [ 39 ], we used this approach to estimate the slant instead of the skew, and also we calculated it upon the horizontal gradient image at various shearing angles in the range [ X  45 ] .

The reasons behind choosing the horizontal gradient image for calculation are the following: first, vertical strokes will be emphasized at the expense of horizontal ones. Sec-ond, computation cost will be reduced, since relatively fewer pixels need to be processed.

Given ( x , y ) the coordinates of pixel in an image i , their sheared counterparts (  X  x ,  X  y ) in the sheared image lated according to Eq. ( 8 )  X  x = x  X  y  X  tan ( X ),  X  y = y (8) where  X   X  X  X  45 ] is the shearing angle. For each sheared image, we calculate vertical histogram H as stated in Eq. ( 9 ).
 H (  X  x
Then, we perform a variation analysis for each histogram profile according to Eq. ( 10 ), where the sheared angle is the angle associated with maximum variation according to Eq. ( 11 ).
 V ( X ) =  X   X  = arg max Figure 8 a shows a slanted word image and its corresponding vertical histogram, and Fig. 8 b shows its slanted free version and its vertical histogram. 4.2 Segmentation As mentioned above, our segmentation approach conducted in two steps. In the first step, a careful analysis of the x -axis coordinates of the connected components is performed. The result is words? images with resolved sub-word overlapping. The second step is to perform topological feature X  X ased seg-mentation for characters X  representatives.

Before detailing our approach, it is helpful to start by recalling some definitions that are thought to be necessary for the clarity of subsequent definitions and notations.
Firstly, let P refers to any foreground pixel in the thinned word image g ( x , y ) , and let N 8 ( P ) denotes the 8-neighborhood set of P . Secondly, by examine each P  X  g ( x , y ) , a set of feature points are identified, which we call critical feature point ( CFP ) . CFP set contains further four subset that are listed below: (i) The first subset is end points ( EP ) , which contains all (ii) The second subset is branch points ( BP ) , which con-(iii) The third subset is dot points ( DP ) , which is the union (iv) The fourth and last subset is the loop points ( LP ) Given the aforementioned four subsets, the CFP set can be defined as the union of all the four subsets. Figure 9 shows a thinned text image with all possible CFP s that will be utilized later to guide the characters segmentation process. CFP = X  X  LP , EP , BP , DP } (16)
The rest of the sub-section will first details a solution to the sub-words overlapping problem and then presents our segmentation approach. 4.2.1 Resolving sub-words overlapping For resolving the sub-words overlapping, we first find the word baseline as stated above. Then upon finding the base-line, we differentiate between two types of connected com-ponents ( CC ) . The first is what we call main ( CC s), which are all ( CC s) that intersecting with the baseline  X  y  X  coordi-nate. The second are what we call auxiliary ( CC s), which are all CC s that are not intersecting the baseline  X  y  X  coordinate. Figure 10 a shows an example of word image, where the main CC s are (1,2,4, and 6), and the auxiliaries are (3,5,7, and 8) and the horizontal blue line representing the baseline.
After identifying the main CC s, we conduct a distance analysis on their bounding boxes along the x -axis, in order to identify the baseline overlapped main CC s and their cor-responding overlapping distances. In Fig. 10 a, for example, main CC s that are overlapping are (1,2), (2,4), and (4,6). Another distance analysis is performed against the auxiliary ( CC s), so each can be assigned to its corresponding main CC according to the following rules: (i) If an auxiliary CC is overlapping only a given main CC (ii) If an auxiliary that is above the baseline is completely (iii) If two main CCs are overlapping an auxiliary under (iv) In case auxiliary is above the baseline and overlapping
Even though the aforementioned rules resolve almost all the cases, there are some extreme cases where auxiliaries are not overlapping any main CC . In these cases, auxiliary is assigned to the direct next main CC on the left. 3 After assigning the auxiliaries to their corresponding main CC s, we computed the sub-words borders along the x -axis against all its elements (auxiliary CC s and main CC s). The left bor-der of the sub-word bounding box is computed to be the farthest left border among all sub-word elements. Likewise, the right border is selected to be the farthest border to the right.

Eventually, a final distance analysis is preformed against the new sub-word borders and the overlapping solved by shifting away the overlapped sub-words. Figure 10 bshows an examples of the overlapping free version of Fig. 10 a. As a result of this pre-segmentation step, sub-words are separated by empty columns that make their segmentation a straight-forward process. 4.2.2 Segmentation of character representative After resolving the sub-words overlapping, the proposed approach starts the segmentation of sub-words into their con-stituent characters X  representatives. Given that Arabic char-acters have their boundaries in columns with the minimum number of pixels (only one pixel in the thinned version), our segmentation approach starts by generating a set C of columns indices as candidates for segmentation, where the elements of C are all column indices within the thinned image g ( x , y ) , containing only one foreground pixel. In our approach, we adopted the segmentation algorithm presented in [ 29 ] on the basis of our proposed segmentation algorithm. The algorithm is involving a broader set of segmentation candidates instead of using the set of contour local minima only. Using local minima only as candidates for segmentation are observed to risk an accurate segmentation because local minima often occur inside many of Arabic characters X  main strokes. Hence, we decided to generate a large set of segmen-tation candidates, and we did not restrict our candidates to be only local minima.

Figure 11 a is depicting the segmentation columns candi-dates for a text image. The reader can notice that each col-umn containing only one pixel is elected as a candidate. The next step is to exclude from the candidates set all columns that are intersecting with any CFP , this is to say that LP , EP , BP , and DP columns cannot be in the same time a cut point otherwise we lose important character features. Fig-ure 11 b shows the text image after excluding columns can-didates that are intersecting CFP columns. The reader can notice in Fig. 11 b that very few candidates are excluded as comparing with Fig. 11 a. This is because it is quit seldom that an LP or a BP column contains only one pixel, so it will not be chosen as a candidate in the first place.
The next step is to scan the list of candidates, starting from the most right one to the left, electing the left neighbor from each two adjacent segmentation candidates. Figure 11 c shows the result, in which we can easily notice the signif-icant reduction in number of candidates for segmentation. The candidates set obtained so far is an important improve-ment over the previous candidate sets. However, in order to resolve issues like the over-segmentation in Fig. 11 c( the let-ter ( TAA ), the first letter from the right is over-segmented into three parts ), we formulate four conditions to increase segmentation accuracy.

To ease notation of conditions, we will write m a subscript to CFP and/or CFPs elements, to refer to the respective column index. Also, we will use c i , c j to refer to any two column indices in the thinned image g ( x , y ) that are chosen to be candidates. The final election process is performed by applying the following condition on the candidates: (i) First condition: if there are two consecutive cut candi-(ii) Second condition: if there is a branch point column BP (iii) Third condition: if the direct neighboring on the left is (iv) If the next column contains an end point EP 1 , which is at
Moreover, to mark the start and/or the end of all letters, we insert segmentation candidates direct before and after each main CC . Finally, each set of pixels between every two segmentation candidates in the binary image are assumed to represent a letter in the word.

To extract the most possible accurate letter image and also to eliminate isolated pixels (or group of pixels) belonging to neighboring letters that may appear as a result of the crop process, we perform a reconstruction module according to [ 40 ], in which we use those sets of pixels as masks, and their counterparts in the thinned image as a marker. Then, we constructed the letter image and save it, as the result of our segmentation methodology.
 Figure 11 d illustrates the final segmentation results, and Fig. 13 shows zoomed in segmentation X  X  result of an Arabic word. 5 Experimental results We have experimented our proposed segmentation method-ology on 600 word images, taken from IESK-arDB data-base. Images are manually classified into 5 sets, according to the similarity in their writing style. For the purpose of com-parison, two segmentation modules have been implemented, one according to our methodology and the other according to the segmentation approach detailed in [ 26 ]. Figures 14 , 15 , and 16 illustrate some of the results. The source binary image is contained in the first column. The thinned images with character borders, and the segmentation results, are con-tained in the second and the third column respectively.
Complete success is reported in 67 %, where a com-plete success means that the system accurately discovers the character borders as may a human do. Figure 14 illustrates examples of 100 % success segmentation of character repre-sentatives.

Partial success is reported in 33 % of all cases. We have empirically noticed that 14.4 % of such cases are gener-ated when CFP s occur inside the character instead of on its borders, leading to what we call an over-segmentation. That is because part of a stroke is regarded as character rep-resentative, which in fact is not. This problem is specific to characters and (SIEN and SHIEN). Figure. 15 illus-trates some examples for such cases.
 The other 18.6 % of the partial success cases happen when CFP s cease to exist between two consecutive characters, leading them to being considered as a representative of one character. Figure 16 shows such problem. This problem is called under-segmentation and it is specific for cases, when the second character to left is connected (ALF) or con-nected (LAM) with sheared distortion angle to the left. Also, it appears occasionally incase that (KAF) occurs in the middle of two a connected characters, having upper part vertically overlapping the previous character on the right. The last row in Fig. 16 illustrate such case. We think that those problems can be solved, either by expanding the CFP set to contain more features points like local minima points, for example, and then accordingly modify and add heuristic rules, or they can be solved in subsequent recognition phases like in the post-processing phase, for example, where the recognition results can be corrected against lexicons using different text retrieval techniques.

Figure 17 and Table 2 illustrate a comparative perfor-mance analysis between our methodology and the one of [ 26 ]. The reasons behind choosing this approach are as follows: firstly, because it is addressing exactly the same problem, and secondly, it is the most detailed approach, we come cross in the published literature. Our segmentation module outper-formed a segmentation module implemented according to [ 26 ] in terms of the segmentation results. 6 Conclusion and future works In the first part of this article, we presented a database specific for off-line handwritten Arabic text. The database is equipped with very important ground truth information that no other database in the field has been equipped with before. With the help of character frequency analysis, we showed that our database has almost the same character distribution as in case of Arabic large lexica.
In the second part, we proposed dual-phase segmentation approach that starts first by sub-words borders identification and resolving overlapping among them, and then a topolog-ical feature X  X ased segmentation is taking place according to a set of heuristic rules. The dual-phase segmentation is pre-ceded by an extensive handwriting-specific pre-processing phase in which issues like morphological enhancement, skew correction, and slant correction are dealt with. Even though results were quite satisfactory, future works may investigate issues like, expanding CFP set by adding more topolog-ical features and correspondingly more heuristics. Also, a cyclic segmentation-recognition-based approach is expected to improve results further. In such approach, the segmen-tation decision is confirmed by a successful recognition. To avoid the high error tendency of segmentation based recognition approaches while still working in comparatively less restricted recognition environments, we believe, that a sub-words based holistic recognition approach can be an alternative to solve the problem of recognition of off-line handwritten Arabic text [ 9 , 12 ].
 References
