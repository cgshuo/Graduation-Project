 When developing data transformations X  X  task omnipresent in ap-plications like data integration, data migration, data cleaning, or scienti fi c data processing X  X evelopers quickly face the need to ver-ify the semantic correctness of the transformation. Declarative speci fi cations of data transformations, e.g., SQL or ETL tools, increase developer productivity but usually provide limited or no means for inspection or debugging. In this situation, developers to-day have no choice but to manually analyze the transformation and, in case of an error, to (repeatedly) fi x and test the transformation.
The goal of the Nautilus project is to semi-automatically sup-port this analysis-fi x-test cycle. This demonstration focuses on one main component of Nau tilus, namely the Nau tilus Analyzer that helps developers in understanding and debugging their data trans-formations. The demonstration will show the capabilities of this component for data transformations speci fi ed in SQL on scenarios from different domains that are based on real-world data.
We provide an overview the Nautilus Analyzer, discuss compo-nents and implementation techniques, and outline our demonstra-tion plan. The Nautilus website ( http://nautilus-system.org ) features a video, screenshots, and further details.
 H.4 [ Information Systems Applications ]: Miscellaneous Algorithms data provenance, query analysis
The Nautilus system [4] aims at supporting developers dur-ing data transformation development by providing semi-automatic tools for the otherwise manual analyze, fi x, and test tasks that they usually perform. Due to the lack of space, we refer readers to [4] for details, as in this section, we focus on illustrating the bene fi ts, the work fl ow, and the components of the Nautilus Analyzer, the subject of the proposed demonstration. As the name suggests, the Nautilus Analyzer manages the analysis phase of the process.
Consider the source tables R and T as well as queries Q 1 Q 2 in Fig. 1. Assume that the developer speci fi es that both queries should return the same number of tuples (a realistic requirement for instance in a scenario where eve ry department in the result of Q needs to be associated with a manager in the result of Q 2 that constraint be violated, the developer suspects the error to be in Q 1 . He thus declares the result of Q 2 over source instance D ,de-noted as Q 2 ( D ) , as  X  X rusted X , meaning that the extension of Q should not change during the analysis-fi x-test process. For analysis, the above speci fi cation translates to the questions  X  X hy are there too many tuples in the result of Q 1 ? X  (Why-question) or  X  X hy are tuples missing from the result of Q 1 ? X  (Why-Not question).
Essentially, when using Nautilus, a developer speci fi es what he wants during analysis in the form of a debugging scenario , as illus-trated above. The result of analysis is a set of explanations that pro-vide guidance for the fi x and test phases. These explanations take the form of provenance information, i.e., the Nautilus Analyzer re-lies on Why-provenance [2] and Why-Not provenance [1, 5, 7, 9] to generate explanations answering the respective types of ques-tions. Without Nautilus, the traditional manual process requires the developer to identify  X  X uspicious X  spots of the transformation, to explicitly spell out changes, and to verify that the manual changes do not violate the constraints in the debugging scenario.
Continuing our example of Fig. 1, we observe that the require-ment that both queries return the same number of tuples is not met. Therefore, the developer asks a Why-Not question on the result of Q . The following answers are possible: (i) R needs to contain one more tuple that joins with a tuple in T in order to return three tuples as well (instance-based explanation [5, 7]), (ii) the join between R and T is the operator fi ltering tuples from the result (query-based explanation [1]), or (iii) the query would produce the desired result if the join was replaced by a left-outer join (modi fi cation-based ex-planation [9]). Let us denote these explanations as X IB , X X MB , respectively.

Explanations cover many different possibilities of why the de-sired result was (not) computed. Should, based on this informa-tion, the developer deem the query to be correct, he is done. Oth-erwise, if the returned explanations suggest that one of the queries is faulty, a subsequent fi x phase becomes necessary. During the fi x
Processing the scenario without trusting any table would result in a larger set of Why and Why-Not questions to be answered. Figure 1: Sample data and queries (note: | Q 1 | = 2 = 3 = | phase, Nautilus suggests changes to the queries in order to meet the requirements speci fi ed in the debugging scenario. To guide the fi x algorithms, the developer, during analysis, may attach a Boolean annotation to some explanations , identifying them as be-ing (ir)relevant. For instance, he may deem the query-based ex-planation described above as rele vant, but may disagree with the modi fi cation-based explanation. This translates his belief that the join operator is indeed the faulty operator, but that a left-outer join is not the solution to the problem. The Nautilus Analyzer exploits these annotations in subsequent steps in the same or future analysis-fi x-test iterations.

As the number of explanations can become quite large, Nautilus also provides means on ranking these. Our ranking model takes into account both heuristics and explanation annotations, e.g., if the user marks the instance-based explanation above as interesting, similar and related explanations will be ranked higher.

The Nautilus Analyzer consists of two main components to im-plement the above functionality: a graphical user interface (GUI) to graphically and intuitively specify a debugging scenario and to interact with produced explanations and the explanation manager . Due to the lack of space, we focu s the technical discussion on the explanation manager and refer interested readers to the project website for screenshots and a video demonstrating the GUI.
The input of the explanation manager consists of a debugging scenario and optional developer-provided Boolean annotations. The explanation graph is the central data model that represents all explanations. Nautilus initializes and re fi nes the graph based on in-teractions with the four main components of the explanation man-ager: (i) the explanation generator , (ii) the explanation annotator , (iii) the annotation analyzer , and (iv) the explanation ranker . Explanation generator. Given a debugging scenario, the explana-tion generator returns a set of explanations. In general, a debugging scenario S = { Q , D , Q ( D ) , E , C } includes a set of SQL queries Q to be analyzed, the source instance D , the set of query results Q the set of tuples (existing or missing) to be explained, denoted as E , and a set of constraints C . In our current implementation, C is limited to trusting or minimally affecting tables in D or query results in Q ( D ) .Thisde fi nition of a debugging scenario naturally extends its de fi nition in [5]. In our example , the debugging scenario is de fi ned by Q = { Q 1 , Q 2 } , D = { R , T } , Q ( D E = { missing tuple from Q 1 ( D ) } ,and C = { trust on Q 2
The debugging scenario translates to a (set of) Why or Why-Not provenance questions. Each question is then processed using one or more of the implemented algorith ms. These include Why-Not [1], ConQuer [9], Missing-Answers [7], Artemis [5], and lineage [3] The result of each algorithm is generalized as a set of explanations, which form the input to the explan ation graph initialization. In our example, we return the Why-Not explanations mentioned in the introduction.
 Explanation graph. To initialize the explanation graph, we con-ceptually proceed as follows. Pleas e note that our implementation does not strictly follow this descriptions for ef fi ciency reasons.
Conceptually, we fi rst consider a logical operator tree representa-tion, the query tree , of the queries to be analyzed, e.g., of Q computed explanation results in an explanation node that connects to nodes in the query tree via edges directed from the explanation
The authors of [1, 9] kindly provided us with their code, which signi fi cantly facilitated the re-impl ementation and integration into our Nautilus Eclipse plugin. As for lineage, we reuse the lineage capabilities of the Trio system [10]. (a) conceptual intermediate graph (b) fi nal explanation graph node to query tree nodes. The edge between a query-based explana-tion (e.g., X QB ) simply connects the explanation node to the query tree node that represents the identi fi ed culprit operator, whereas a modi fi cation-based explanation (e.g., X MB ) connects to the query tree nodes affected by the modi fi cation. As for instance-based ex-planations, we create equivalence classes based on what we call  X  X oin patterns X . To illustrate these, let us assume that no trust lies on Q 2 ( D ) and hence, T is subject to modi fi cation. Then, three pos-sible patterns for instance-based explanations exist: (i) insert into R and T s.t. the join is satis fi ed, (ii) insert to R s.t. the inserted tuple joins with an existing tuple in T , and (iii) insert to T s.t. the inserted tuple joins with an existing tuple in R . This results in three equiva-lence classes which we denote C 1 , C 2 ,and C 3 . Explanations within an equivalence class only differ w.r.t. the existing tuples involved and convey the same information. Thus, we regard these as equiv-alent. For each equivalence class, we create a class node in the graph. The class node connects to each query tree node that either represents a source table to which an insertion applies or that rep-resents an unful fi lled join. Individual instance-based explanations appear in the explanation graph as nodes that simply connect to their respective class node. Considering all three types of Why-Not explanations of our example, at this point of conceptual process-ing, we obtain the graph shown in Fig. 2(a) (omitting the individual instance-based explanation nodes for conciseness).

In a second step, we abstract from the query nodes and directly connect explanations with weighted edges. The weight corresponds to explanation similarity. To determine the similarity of two class nodes, we compute the Jaccard coef fi cient of query tree nodes reached from the pair of compared explanations. For all remaining pairwise combinations of expl anation types, the Jaccard coef fi cient is solely based on the inner query nodes (i.e., source tables are no longer considered). Fig. 2(b) show s the resulting explanation graph (individual instance-based explanations, again omitted, are all con-sidered equivalent, so their similarity within an equivalence class equals 1, and their similarity across equivalence classes is equal to the similarity of the respective equivalence classes). Please ignore the dashed edges for now.
 Explanation annotator. Annotations are either user de fi ned or in-put by other Nautilus components 3 . The goal of the explanation annotator is to link annotations provided by a developer through the GUI to explanations in the explanation graph. We support two user-annotations, i.e.,  X  X elevant explanation X  and  X  X rrelevant expla-nation X . A developer provides these for an explanation if he decides that the inspected explanation provides valuable information in the context of the debugging scenario or not, respectively. Note that not all explanations need to be annotated by a developer. For those not explicitly annotated, we assu me an annotation of  X  X nknown X  or, as discussed next, derive an annotation using the explanation annotation analyzer.
At the current development stage, the Nautilus Analyzer only sup-ports user-de fi ned annotations and annotations derived by the anno-tation analyzer. In the future, the fi xing and testing components of Nautilus may contribute further annotations.
Internally, the Nautilus Analyzer translates annotations to prob-abilities that indicate the probability that an explanation pinpoints the problem to be detected by the developer. A user-annotation of  X  X elevant explanation X  translates into a probability of 1 whereas  X  X rrelevant explanation X  corresponds to a value of 0. Whenever no annotation (user-or system-provided) is available, a default proba-bility (currently 0.5) is assigned to an explanation.
 Annotation analyzer. Based on annotations collected by the expla-nation annotator, the explanation annotation analyzer derives fur-ther probab ilities for e xplanations similar t o the annotated ones. To this end, it spans a Bayesian network [8] over the explanation graph as follows: user-annotated explanations serve as source nodes that provide the prior pr obabilities. Starting fro m these nodes, we then traverse the explanation graph in breadth-fi rst, only following edges whose attenuated similarity is above a given threshold  X  .Theat-tenuated similarity of an edge e on a path of length k starting at a user-annotated explanation is calculated as  X  1  X  i  X  k where 0  X   X   X  1 denotes an attenuation factor. We compute condi-tional probabilities for all explanations reached during this traversal procedure. The details on the conditional probability functions are out of the scope of this paper, essentially, we have de fi ned a func-tion for each combination of sour ce and target explanation type. The functions either return a Boolean or a real-value.
 Fig. 2(b) shows an example of the network we span assuming X
QB (blue dashed edges) and an explanation in C 1 (purple dashed edges) are annotated as irrelevant and relevant, respectively,  X  and  X  = 0 . 8. Intuitively, the fact that the query-based explanation is invalidated also invalidates the modi fi cation-based explanation on the same query operator, e.g., we propagate a probability of 0 to X
MB .However, C 1 being relevant indicates that the join operator is after all relevant and hence a probability of, e.g., 1 X MB whose probability is then re-evaluated.
 Explanation ranker. From a user perspective, it is crucial that he does not drown in an overwhelmingly large number of explanations and that the analysis of few explanations is rewarding, i.e., rele-vant explanations are soon detected. To this end, Nautilus incorpo-rates an explanation ranker. Initial ranking is based on pre-de fi ned heuristics but as the developer provides annotations, the ranking adapts to these to favor explanations with higher probabilities.
Ranking heuristics for Why-provenance include ordering expla-nations by (i) number of tuples involved, (ii) similarity, and (iii) di-versity. Whereas the fi rst criterion is self-explanatory, let us brie fl y explain the rationale behind the other two heuristics. Similarity will sort similar explanations ne xt to each other (calculated based on shared tuples and query operators involved) whereas diversity aims at presenting different explanat ions consecutively. Intuitively, similarity is a good choice for eager annotators whereas diversity represents a better choice for lazy annotators. For Why-Not expla-nations, we additionally consider ranking based on side-effects an instance-based or modi fi cation based explanation may have (i.e., changes in query results if tuple insertions or query modi fi cations were actually performed, which they are of course not during anal-ysis). In our example, an initial ranking of instance-based explana-tions based on diversity for instance returns as top-3 an explanation from C 2 , an explanation from C 3 , and an explanation from C
In summary, the explanation ranker module is still in a prelim-inary state compared to the remaining components, as the current ranking schemes only represent a fi rst step towards a uni fi ed rank-ing of explanations (without the user specifying the function). The research on explanation ranking is an ongoing effort and we plan on incorporating more sophisticated ranking algorithms in the future.
The Nautilus system, and hence its Analyzer component, is im-plemented as an Eclipse plugin in Java 1.5. Using the Eclipse plat-form has two signi fi cant advantages: fi rst, it allows us to seam-lessly integrate the Nautilus com ponents in a platform with which developers are already familiar with; second, it comes with a rich set of plugins that we can leverage. For instance, the demonstra-tion we propose makes use of and extends the Data Tools Platform ( http://www.eclipse.org/datatools/ ) and the Zest Graph Visualization Toolkit ( http://www.eclipse.org/gef/zest/ ). Note that the Nautilus Analyzer signi fi cantly extends the Eclipse plugin we demonstrated for Artemis [6], an algorithm we devel-oped to generate instance-based explanations for Why-Not prove-nance [5]. Screenshots and a video are available on the project website.

At the conference, we will demonstrate the Nautilus Analyzer using several debugging scenarios and at least three use cases. The fi rst use-case reuses the sample CRIME scenario provided by the Trio system( http://infolab.stanford.edu/trio/ ). The second scenario comes from the govern-ment domain using publicly available data about US Congressmen( http://bioguide.congress.gov ), Spendings( http://usaspending.gov ), Earmarks ( http://earmarks.omb.gov ), etc.. The third scenario relies on IMDB movie data ( http://www.imdb.com ). Note that the last two scenarios are based on real-world data. Whereas the CRIME scenario will be used as introductory scenario. the queries demonstrated on the real-world datasets will be inspired by our previous data integration and data cleaning experiences on these data sets. The query complexity will go beyond the toy queries shown in this paper and will include aggregation and negation when applicable. Through these scenarios, we will demonstrate the functionality of all components described in this paper and show how the Nautilus Analyzer successfully allows developers to understand and debug complex SQL queries.

