 Text mining in clinical domain is usually more difficult than general domains (e.g. newswire reports and scientific liter-ature) because of the high level of noise in both the cor-pus and training data for machine learning (ML). A large number of unknown word, non-word and poor grammati-cal sentences made up the noise in the clinical corpus. Un-known words are usually complex medical vocabularies, mis-spellings, acronyms and abbreviations where unknown non-words are generally the clinical patterns including scores and measures. This noise produces obstacles in the initial lexical processing step as well as subsequent semantic analysis. Fur-thermore, the labelled data used to build ML models is very costly to obtain because it requires intensive clinical knowl-edge from the annotators. And even created by experts, the training examples usually contain errors and inconsistencies due to the variations in human annotators X  attentiveness. Clinical domain also suffers from the nature of the imbal-anced data distribution problem. These kinds of noise are very popular and potentially affect the overall information extraction performance but they were not carefully investi-gated in most presented health informatics systems.
This paper introduces a general clinical data mining archi-tecture which is potential of addressing all of these challenges using: automatic proof-reading process, trainable finite state pattern recogniser, iterative model development and active learning. The reportability classifier based on this architec-ture achieved 98.25% sensitivity and 96.14% specificity on an Australian cancer registry X  X  held-out test set and up to 92% of training data provided for supervised ML was saved by active learning.
 Clinical, active learning, text classification, named-entity recognition, natural languages processing
Processing clinical texts is quite challenging. Firstly, clin-ical records comprise idiosyncratic spellings, abbreviations, acronyms, poor grammatical structure, and up to 30 percent of non-word tokens. Besides resolving misspellings, know-ing the correct expansions of abbreviations and acronyms is critical to understanding the document for both automatic natural language processing as well as human comprehen-sion and interpretation [27]. Secondly, an important part of narrative reports that needs to be captured is clinical scores and measures as doctors infer a patient X  X  status by analyz-ing these complex patterns. For example, BP 140/65 (84) is an example of Blood Pressure; HR 72 is pattern of Heart Rate. These lexical obstacles in the clinical corpus should be addressed at an early step of processing to avoid inherited chain of errors.

Our research focuses on supervised ML approaches be-cause designing rules and patterns is time consuming and complicated process for human experts to complete. The basic advantage of ML is that the concept characteristics and classification rules can be automatically learnt through training examples. Therefore, the high quality of a seman-tically classified and annotated corpus is mandatory to con-tribute to the success of supervised learning algorithms. How-ever, the training examples are usually not free and obtain-ing these labels is a time consuming and high labour cost process. Narrative reports usually take longer time to anno-tate as they require expertise knowledge in the field. Even if tagging is provided by human experts, there is a high level of inconsistency in the labelled data because some instances are implicitly difficult for annotators and also they become dis-tracted or fatigued over time, introducing variability in the quality of their annotations [29]. Besides the noise caused during the annotation process, the train set of clinical ML also suffers from the problem of imbalanced data distribu-tion: one class (usually negative class) may have many more instances that dominate all other classes (e.g. non-cancer cases vs cancer cases in radiology reports). This can cause a bias in the training process and may require a special active learning method for the sample data to improve the overall ML performance.

To the best of our knowledge, none of the published health information system is capable of addressing all of the above challenges. In the present paper, we propose the novel data mining system that employed natural language processing techniques, supervised ML and active learning approaches to overcome the difficulties when working with clinical text. Our system focuses on clinical text classification and named-entity recognition and includes three main components:
The rest of the paper is structured as follows. Section 2 gives an overview of the previous clinical and radiological information extraction systems. Our system architecture is presented in Section 3. The proposed system demon-strated the high performance in classifying real-world and large scale radiology reports provided by Australian Cancer Registries. Experiments and results related to the Cancer Council X  X  project are reported in Section 4. We conclude in Section 5 with directions for future work.
A variety of methods and systems have been implemented in the clinical domain to extract information from free text. The popular learning tasks include document classification, named-entity recognition (NER) and classification of rela-tionship between the entities (medical concepts).

Friedman et al. (1994) [9] developed the medical language extraction and encoding (MedLEE) system, which used a domain-specific vocabulary and semantic grammar to pro-cess radiology reports. It was initially used to participate in an automated decision-support system, and to allow natural language queries. MedLEE was then adapted to automat-ically identify the concepts in clinical documents, map the concepts to semantic categories and semantic structures [10]. The final semantic representation of each concept contained information on status, location and certainty of each con-cept instance. Haug et al. (1995) [12] introduced symbolic text processor (SymText), a natural language understand-ing system for chest x-ray reports. SymText processes each sentence in a document independently with syntactic and probabilistic semantic analysis. Bayesian networks are used in SymText to determine the probability that a disease is present in the patient.

An early combined classifier approach in biomedical NER proposed a two-state model in which boundary recognition and term classification are separated into two phases [17]. In each classification phase, different feature sets were selected independently, which is more efficient for each task. A com-parative study between two classical ML methods, Condi-tional Random Fields (CRFs) and Support Vector Machines (SVMs) for clinical NER shows that the CRFs outperformed SVMs in clinical NER [19].

When extracting information from narrative text docu-ments, the context or assertion of the concepts extracted play a critical role. The NegEx algorithm of [3] implements dictionaries of pre-UMLS and post-UMLS phrases that are indicative of negation to identify positive and negative as-sertions. NegEx uses a rule-based method and heuristics to limit the scope of indicative phrases. The i2b2 challenge X  X  assertion classification is an extension of a previous system designed by [35], in the new specification an uncertainty as-sertion is divided into values of hypothetical, conditional and possible. A combination of ML and rule-based approaches is utilised in the system of [35]. One of these approaches extends the rule-based NegEx algorithm to capture alter-association in addition to positive, negative and uncertain assertions; the other employs an SVM to present a ML so-lution to assertion classification.

For the relationship classification task, there are many definitions of relationships between concepts in which each system classifies different relationship types. In general, rel-evant features are extracted from the text and are usually selected on the basis of the experimental results and intu-ition, or by statistical techniques [11]. First, by experience and intuition, we designed feature sets that were expected to have a strong correlation with the target classification. Forward selection was applied by sequentially adding each feature set to the model and evaluating its performance. The feature set is retained if a better result is achieved otherwise it is discarded before the next cycle is repeated.

ML systems have demonstrated high accuracy in infor-mation extraction and classification from radiology reports. It has also been used for automatic structuring of impor-tant medical information from radiology reports [15, 30]. Thomas et al. used Boolean logic built from 512 consecu-tive ankle radiography reports to create a text search algo-rithm and then applied to a different set of 750 radiology reports with a sensitivity of 87.8% and specificity of 91.3% [31]. The LEXIMER automated engine classified 1059 un-structured reports of radiography examinations based on the presence of important findings and suggestions for further actions with 94.9% and 97.7% sensitivity and specificity re-spectively [6]. In other research specialised on lung cancer reports, McCowan el at. used SVM learning techniques to investigate the classification of cancer stages [21, 20]. This system achieved an accuracy of 74% for tumour (T) stag-ing and 87% for node (N) staging on the complete 179-case trial data set. In recent work published by Cheng et al., they first accessed whether the text contains sufficient infor-mation for a classification process then the tumour status and progression was determined by utilizing the SVMs mod-els that reached 80.6% sensitivity and 91.6% specificity [4]. However, the sizes of the corpora used in previous research were relatively small compared to the number of reports processed by a registry each year.
Figure 1 presents the system architecture which comprises of three phases: pre-processing, iterative model development and active learning. The electronic medical records (EMRs) are retrieved from the database then passed into the pre-processing phase for proof-reading, lexical verification and medical concepts identification. At the end of this step, the records are cleaned and annotated with the medical concepts hence they are ready to be used to develop training data for ML models. At each development cycle, the model X  X  per-formance is evaluated using n-fold cross-validation method then more data will be added to the train set with the sup-port of active learning algorithms to query the most infor-mative instances. This process is repeated until the desired performance is met.
Proof-reading is a process whereby a clinical text is val-idated to identify unknown tokens/words and their valid forms. There are two principal tasks to be achieved, these are normalisation and standardisation. The normalisation process changes the texts in a way so that a human reader would consider it as normal, such as correcting spelling, ex-panding abbreviations and acronyms. The standardisation process converts the text into certain formats that an ex-pert community has defined as standard; a good example is converting scores and measures into a standard layout.
The ring-fencing tokeniser is capable of capturing pop-ular basic and complex patterns in clinical text. It is a cascaded Finite State Recognizer (FSR) which uses training examples to recognize token patterns constituting a score or measurement that requires standardisation [26]. There are a large number of different scores and measurements in clin-ical notes. Some other types of measurements and scores in the training patterns are illustrated in Table 1.

When using regular expressions (REs) to describe pat-terns as more rules are developed to capture missed items, the rules became so complicated that it makes them difficult to update as any change has the risk of losing previously rec-ognized patterns or introducing new false positives. Another problem is that the rule updating task requires an exhaus-tive knowledge of REs and a considerable amount of time modifying the rules. Consequently, the automated learning process to capture patterns using REs is particularly dif-ficult. On the other hand, a trainable FSR can be built Table 1: Examples of clinical measures and scores can be recognized by FSR. directly from training examples of data with high accuracy and efficient computational time.

To capture composite patterns in clinical text, the cas-caded FSR is generated by several levels of generalisation. The basic patterns is recognised first by simple FSR and then passed to complex FSR to capture the final compli-cated patterns. The example of applying a cascaded FSR to capture complex patterns in the text is showed in Figure 2.
After standardisation, each token is passed through the lexical verification process and then inserted into the Lexi-con Management System (LMS) which supports automated and manual resolution of unknown tokens. The LMS is a system developed to store the accumulated lexical knowl-edge and contains categorizations of spelling errors, abbre-viations, acronyms and a variety of non-word tokens. It also has a web interface that supports rapid manual correc-tion of unknown words with a high accuracy clinical spelling suggestor plus the addition of grammatical information and the categorization of such words into gazetteers [25]. The method of the clinical spelling suggestor is based on combin-ing heuristic-based suggestion generation and ranking algo-reports classification case study.
 rithms based on word frequencies and trigram probabilities. The lexical verification process contains an additional step to resolve misplaced whitespace (e.g.  X  X ooka fter X  should be  X  X ook after X ) and punctuation (e.g.  X  X atio;n X  should be  X  X a-tion X  or  X  X ation.The X  should be  X  X ation. The X ).
In EMRs, the same clinical concepts are usually expressed in different ways causing the models interpreted them as dif-ferent features. Hence, there is a need for normalization of the medical concept using universal terminologies such as UMLS r , SNOMED  X  CT r or their subsets. Besides gen-eral medical terminologies, the in-house design tag set was utilised in many studies because it is better controlled and more relevant to the predictive tasks, or to identify specific clinical entities in the text. Figure 3 illustrates the example annotation schema which was specially designed for classifi-cation of cancer radiology reports.

A detailed and well-designed tagging system can contribute significantly to the classification and extraction results. For example, the sentence  X  X here is no convincing metastatic bone lesion X  in the conclusion will be tagged as:
There is no
The occurrence of popular cancer terms (e.g. metastatic, lesion) in a sentence in the conclusion section is not enough to conclude that the cancer is reportable. The complete investigation has to consider whether the cancer term is negated or modalised on the basis of linguistic tags such as Lexical Polarity Negative (LPN) and Modality in the clas-sification process.
In the annotation process, free-text reports are annotated for examples of the information to be extracted and then algorithms are developed that use the examples to compute a more general model of the desired content. The small initial data set is selected to train the first model. The model is evaluated and the algorithm is revised in a feedback process to produce a more accurate result. This is continued over a series of experiments until an optimal model is identified.
In our model of iterative development, the annotators use a Visual Annotator (VA) tool as shown in Figure 3 which contains computational models of the tag set so as to sup-port the manual annotation and classification processing. Hence, they no longer need to annotate each report de novo but employ knowledge from previously annotated instances and knowledge from all annotators as learnt by the com-putational model. This not only reduces the workload and annotation time per report but also reduces the error rate and inconsistencies of human annotation which were gener-ated by different levels of expertise.

After each cross-validation cycle, the new model is deliv-ered to the VA to perform manual correction of the current gold-standard with the support of an annotation validation tool. This model is further supported by active learning al-gorithms to query the most informative instances to enrich the train set. The active learning selection process can query instances as a group in batch-mode which are suitable for parallel processing environment.

SVMs combined with CRFs are the main ML methods proposed for text classification and named-entity recognition for clinical textual data [14, 16]. For a large-scale classifica-tion problem with millions of instances and features such as in the radiology reports classification task, a linear kernel is usually a promising learning technique. Experiments were therefore performed with optimized linear kernel as the base classifier rather than SVMs with non-linear kernels [8]. The recommended sets for feature selection experiments include but not limited to:
Active learning (AL) is a subfield of ML where the learner is allowed to query the most informative instances to retrain the model instead of making a random selection. Based on this approach, with the same number of sample selections, the performance of active learners dominates random learn-ers in most cases [29]. This approach requires significantly fewer sample reports while maintaining comparable perfor-mance to traditional supervised learning with all training data or even bettering it.

In the present work, the main focus is on pool-based sam-pling which was introduced by Lewis and Catlett [18]. In this scenario, the learner has access to a pool of unlabelled in-stances and can request the labels for some number of them. Among many AL algorithms have been introduced in the literature, the four algorithms investigated and suggested for clinical text classification in this paper are Simple, Self-Confident (Self-Conf), Kernel Farthest-First (KFF), and Bal-anced Exploration and Exploitation (Balance-EE). These al-gorithms appear to be among the best performers based on empirical studies. Furthermore, they are reasonably well motivated and achieved high performance on real-world data sets [1, 24].
The Simple algorithm is based on the kernel machines and was independently proposed by three different research groups [28, 32, 2]. The name Simple (simple margin) used uncertainty estimation as its selection strategy [32]. In SVMs kernel space, the highest uncertain instance, which is defined as the most informative instance, is the one that lies closest to the decision hyperplane. For each unlabelled instance x , the shortest distance between the feature vector  X ( x ) and the hyperplane w i in the feature space is easily computed by | w i  X   X ( x ) | . Hence, the querying function of Simple uses the current classifier to choose an unlabeled instance which is closest to the decision boundary.
The Self-Conf algorithm chooses the next example to be labeled so that, when it is added to the training data, the fu-ture generalization error probability is minimized [1]. Since true future error rates are unknown, the learner attempts to estimate them using a  X  X elf-confidence X  heuristic, which uses its current classifier for probability measurements. The fu-ture error rate is estimated by a log-loss function, which uses the entropy of the posterior class distribution on a sample of the unlabeled instances. Each instance from the unseen pool is examined by adding it to the training set with a sam-ple of its possible labels and estimating the resulting future error rate as described in equation 1; the instance with the smallest expected log is then chosen.

Let P ( y | x ) be an unknown conditional distribution over inputs x , and output y  X  { y 1 ,y 2 ,...,y n } . At each query, a trained probabilistic (soft) classifier given by  X  already built from current training data. For each instance x from the unseen pool U , the algorithm trains a new classifier P 0 over L 0 ( x,y ) = L S { ( x,y ) } and the expected log-loss is defined as:
E (  X  P 0 L 0 ( x,y )) = 1 | U | X
The original SELF-CONF employed Naive Bayes proba-bility estimates. In the experiments described in this chap-ter, [1] implemented SELF-CONF using soft (confidence rated) SVMs. Probabilistic estimates are obtained in a standard way using the logistic regression transform. However, in each selection round, the expected log-loss is re-calculated for all instances in the unseen pool based on their possible labels and then the model is re-trained.
The KFF algorithm uses a simple AL heuristic based on the  X  X arthest-first X  traversal sequence in kernel space [13]. In this algorithm, the most informative instance is the far-thest instance in the unseen pool from the current training set, where the distance from a point to a set is defined as the Euclidean distance to the closest point in the set. The assumption behind the KFF heuristic is that the farthest in-stance is considered to be the most dissimilar to the current training data and needs to be learned first.

Given a set L of labelled examples, KFF chooses the next farthest example x from L in the feature space induced by the SVM X  X  kernel K to be labelled: where ||  X  K ( x )  X   X  K ( y ) || = p K ( x,x ) + K ( y,y )  X  2 K ( x,y ) (3) is the Euclidean distance from  X  K ( x ) to  X  K ( y ), which are the projections of x and y in the feature space induced by the kernel K.

The advantage of KFF over Simple and Self-Conf algo-rithms is that it does not use the model to evaluate the unseen pool during the querying process. Hence, there is no need to retrain the model after each AL trial, and it can be applied to any learning algorithms.
The Simple active learner is good at  X  X xploitation X  by se-lecting the examples near the boundary, but it does not carry out  X  X xploration X  by searching for large regions in the in-stance space that it might incorrectly predict. The Balance-EE method, which is based on a combination of Simple and KFF, to address the problem of balancing between exploita-tion of labeling instances that are near the current deci-sion boundary (Simple) and exploration by searching for in-stances that are far from the already labeled points (KFF) [24]. At each trial, Balance-EE randomly decides whether exploration (Simple) or exploitation (KFF) will be used. If the choice is exploration (Simple), the algorithm evaluates the efficiency of the exploration to adjust its probability of exploring again.

Let h and h 0 be the hypothesis before and after the new example from Simple is added. The change induced from h to h 0 d ( h,h 0 )  X  [  X  1 , +1] is evaluated. If d ( h,h the exploration was efficient and p will be kept high and vice versa.

Let S = x 1 ,x 2 ,...,x n = L  X  U be the set of labelled and unlabelled instances. For each of the hypotheses h (  X  ) ,h vectors of the predictions of h and h 0 on S are defined as H = ( h ( x 1 ) ,h ( x 2 ) ,...,h ( x n )) and H 0 = ( h 0 ( x Then d ( h,h 0 ) is defined as:
The probability p for exploration will be updated as: where defines the upper and lower-bounds for the value of p , and  X  is a learning rate for updating p . Figure 4: Data within the margin is less imbalanced than the entire data.

Ertekin et al. demonstrated that Simple method has the capability of overcoming the imbalanced data problem by providing the learner with much better balanced classes. Figure 4 presents an example of the imbalanced data dis-tribution (image source [7]). Uncertainty AL tends to select the instances that are close to the decision boundary (within the margin), this selection strategy more likely ends up with a better balanced class distribution than that of the entire data set. This problem can also be improved by not querying the redundant samples from the dominated class or query-ing the positive and negative examples repeatedly from the labelled pool.
Patrick and Nguyen (2011) conducted the evaluation for the role of pre-processing phase in improving the ML perfor-mance [25]. System performance was evaluated before and after an automatic proof reading process by comparing the computed SNOMED-CT codes to the coding created origi-nally by the clinical staff. The automatic coding of the texts increased the coded content by 15% after the automatic cor-rection process and the number of unique codes increased by approximately 5%.
 The early design of our system without the integration of AL was initially ranked among the top three teams in the i2b2 2010 international challenge on clinical information ex-traction [25, 34]. Our follow-up challenge experiments with AL methods recorded the equivalent performance to the win-ning team with 15% less training data used [22, 5].
In this section, we focus on the evaluation of the general system which was fully developed for a project with Vic-torian Cancer Council in Australia 1 . This project has the http://www.cancervic.org.au/research/registry-statistics/capture-stage-recurrence Tag TPs FPs FNs P R F De 230615 17980 16682 0.928 0.933 0.930 En 71340 6214 7640 0.920 0.903 0.912 Ra 1417 114 182 0.926 0.886 0.905 Li 141257 8464 8948 0.944 0.940 0.942 St 20370 204 391 0.990 0.981 0.986 All 464999 32976 33843 0.934 0.932 0.933 Table 2: Fivefold cross-validation tagging perfor-mance. special requirement of separating the cancer and non-cancer reports by using a deliberate bias to ensure virtually all can-cer reports are recognised. It has the consequence of pro-ducing as high an accuracy for sensitivity as possible with the challenge of maintaining reasonable specificity for the classifier. Ideally, the cancer registry does not want to miss any cancer cases, however they have accepted a sensitivity greater than 98% and specificity greater than 96%.
The cancer cases covered in this study included all reports provided in a year X  X  data collection by the imaging services in Australia. The pilot sites were Lake Imaging, Ballarat, Pe-ter MacCallum Cancer Centre, Melbourne, and Westmead Hospital, Sydney in conjunction with the Cancer Institute of NSW. The process of creating the classifiers relied on using a manually trained corpus drawn from each site. Initially, a sample of 16 472 reports was drawn from Lake Imaging and assigned to cancer (4784 reports) or non-cancer (11 688 re-ports) classes by the cancer registry and then incrementally delivered to our system.
To support the classification process, the training reports were tagged to identify structure and cancer-related infor-mation based on an in house design annotation schema. More than 3000 cancer reports were annotated with approxi-mately 500 000 tag instances. The overall F-score for fivefold cross-validation of the Named-Entity Recognizer (NER) is 93%. The CRF++ tool was used in our NER experiments 2 .
Our designed tag sets for cancer information extraction are well controlled and do not contain superfluous informa-tion, which can mislead the classification process. Table 2 presents the 5-fold cross validation performance of the com-putational tagger. In this table, the tags are divided into five subsets: http://crfpp.googlecode.com/svn/trunk/doc/index.html
Figure 5 shows the accuracy of the four AL algorithms and random sampling in classifying the radiology reports. The AL experiment was executed in batch mode with 10 reports/round for 100 rounds. Random sampling gave a consistent performance of 72.13% throughout the learning process; this accuracy is equal to the rate of non-reportable instances in the test set (1188/1647). Except for the last few cycles which can only capture one or two positive instances, the random classifier predicted every instance in the test set as non-reportable. This is because of imbalanced data distribution problem, e.g. the number of non-reportables in the training set is 2.6 times greater than the number of reportables, so they exceeded the reportable instances in the early cycles of random selection. The worst performance can be seen with the KFF algorithm, with 27.87% over time; this accuracy is equal to the rate of reportables in the test set (459/1647). Different from the random sampling, the KFF algorithm mostly selected the positive class (766 positives out of 1002 selected examples). Hence, the KFF classifier categorized all instances in the test set as reportable.
The three other AL algorithms show comparable results, with over 94.5% accuracy at the peak points. Balance-EE is a combination of Simple and KFF with a choice of algo-rithm in each trial. In this experiment, KFF was selected by Balance-EE for the first six trials for 60 examples, and then Simple was applied for the subsequent instances. As a result, except for several  X  X rop points X , Balance-EE had a similar learning curve to Simple because most examples were selected using the  X  X xploitation X  (Simple) strategy. The Self-Conf algorithm showed consistently higher accuracy than Simple and Balance-EE for the initial AL queries, and it quickly reached the top performance with only 60% queries used.

From these analyses, the Simple algorithm was chosen as the AL strategy for generating the priority list for manual reportability classification of radiology reports. As seen in Figure 5, the Simple method had comparable results to Self-Conf and Balanced-EE, but its implementation was simpler and more efficient. For 100 trials, Balanced-EE was slightly slower than Simple, while Self-Conf was five times slower than Simple.

The full learning curves for Simple active sampling and random sampling with a batch size of 100 reports per round for 145 rounds are presented in Figure 6. The batch size was increased from 10 to 100 to speed up the process in or-der to generate an overview of the comprehensive learning curves. However, the performance of the active learner with the same training size was reduced as compared to batch of 10 results because the model was updated 10 times less fre-quently. Figure 6 shows that the performance of the random sampler increased only when it had 3000 reports. At that point, the Simple active learner had already reached its top performance, which was over 23% higher than the random learner. There was not much difference in the performances of the two methods since 10 000 reports had been selected.
A known problem with many AL algorithms, especially in the early steps of learning, is that they are prone to generate a biased training set rather than be representative of the true underlying data distribution. As can be seen in Figure 6, there are a few points where the performance of the active learner dropped dramatically -for example, the performance fell below 80% when 1600 samples were used. This is due to limitations in the initial model that will perform AL. Many algorithms just randomly select a few instances to train the first model, which is usually not a good starting point for the real data distribution [22].
The evaluation of the reportability classifier presented here was executed independently at the Cancer Registry. They used sensitivity and specificity as evaluation metrics, while precision, recall, and F1-score were calculated in our experi-ments. For the binary classification problem,  X  X ensitivity X  is equal to  X  X ecall X  of the positive class (reportable), and  X  X peci-ficity X  is the  X  X ecall X  of the negative class (non-reportable). The Registry maintains the held-out test set to evaluate the system independently until the required sensitivity and specificity are achieved. None of the held-out test set was used for any part of the system development -for example, they were not used to build the gazetteers or the ML mod-els. This held-out test set comprised 400 reportables and 2100 non-reportables, which is a similar distribution to the released data.

The approved version of the classifier achieved the sensi-tivity of 98.25% and specificity of 96.14% (Table 3). The final version is implemented based on two ML algorithms, they are CRFs and SVMs. In addition, the special cancer gazetteers collected by the linguists who have experience at interpreting the reports are used to support the ML models. The addition of gazetteer features slightly increased the sen-sitivity of the model (1.5%) while decreased the specificity level (2.2%). The significant improvement of nearly 5.3% in the targeted sensitivity score was experienced when the classifier was supported with the tagging features (BOT) generated by the computational annotation models. Fur-thermore, the specificity was still maintained at over 96% which was fulfilled the requirement of the project. Table 3: Reportability classifier X  X  performance on evaluation (held-out) set for Lake Imaging
This paper presents a general system for text mining in clinical domain with a focus on dealing with multiple fre-quent kinds of noise. This system is then become part of an industrial-strength processing pipeline built to extract content from radiology reports for use in the Victorian Can-cer Registry. The most important practical application of the reportability classifier is that it can dramatically reduce human effort in identifying relevant reports from the large imaging pool for further investigation of cancer. The clas-sifier is built on a large real-world dataset and can achieve high performance in filtering relevant reports.

In future work, we will investigate the specialised parser to deal with the problem of poor grammatical sentences in the clinical corpus. In the patient notes, very few sentences could be successfully explored by full constituent parse tree due to the frequent ungrammatical notes written by the doc-tors. However, the partial trees are usually utilised when the complete parse could not be generated. Exploring the sub-trees between two target concepts can help to better classify the relationship between them.

In the present system, the models stopped learning when it reached the pre-defined performance. However, it is more ideal if the model only finishes the querying process when the best performance is archived. We plan to improve the cur-rent system by introducing the stopping criteria during the AL process. Several stopping criteria have been introduced and are based on measures of stability or self-confidence within the learner [36, 23].
This work was supported by the Victorian Cancer Reg-istry, Cancer Australia (2012-2013) and Data61 -CSIRO (2015). The authors would like to thank Helen Farrugia and Georgina Marr of the Cancer Council of Victoria, who pro-vided the funding for this project and the registry expertise, and Dr Alex Pitman of Lake Imaging for contributing his radiological expertise. [1] Y. Baram, R. El-Yaniv, and K. Luz. Online choice of [2] C. Campbell, N. Cristianini, A. Smola, et al. Query [3] W. Chapman, W. Bridewell, P. Hanbury, G. Cooper, [4] L. Cheng, J. Zheng, G. Savova, and B. Erickson. [5] B. de Bruijn, C. Cherry, S. Kiritchenko, J. Martin, [6] K. Dreyer, M. Kalra, M. Maher, A. Hurier, B. a. [7] S. Ertekin, J. Huang, L. Bottou, and L. Giles. [8] R. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin. [9] C. Friedman, P. Alderson, J. Austin, J. Cimino, and [10] C. Friedman, L. Shagina, Y. Lussier, and G. Hripcsak. [11] B. Haddow. Using automated feature optimisation to [12] P. Haug, S. Koehler, L. Lau, P. Wang, R. Rocha, and [13] D. Hochbaum and D. Shmoys. A best possible [14] T. Joachims. Text categorization with support vector [15] D. Johnson, R. Taira, A. Cardenas, and D. Aberle. [16] J. Lafferty, A. McCallum, and F. Pereira. Conditional [17] K. Lee, Y. Hwang, and H. Rim. Two-phase biomedical [18] D. Lewis and J. Catlett. Heterogeneous uncertainty [19] D. Li, K. Kipper-Schuler, and G. Savova. Conditional [20] I. McCowan and D. Moore. Collection of Cancer Stage [21] I. McCowan, D. Moore, and M. Fry. Classification of [22] D. Nguyen and J. Patrick. Reverse active learning for [23] F. Olsson and K. Tomanek. An intrinsic stopping [24] T. Osugi, D. Kun, and S. Scott. Balancing exploration [25] J. Patrick and D. Nguyen. Automated proof reading of [26] J. Patrick and M. Sabbagh. An active learning process [27] J. Patrick, M. Sabbagh, S. Jain, and H. Zheng. [28] G. Schohn and D. Cohn. Less is more: Active learning [29] B. Settles. Active learning literature survey. Computer [30] R. Taira. Automatic Structuring of Radiology [31] B. Thomas, H. Ouellette, E. Halpern, and [32] S. Tong and D. Koller. Support vector machine active [33] Y. Tsuruoka, Y. Tateishi, J. Kim, T. Ohta, [34] O. Uzuner, B. South, S. Shen, and S. DuVall. 2010 [35] O. Uzuner, X. Zhang, and T. Sibanda. Machine [36] A. Vlachos. A stopping criterion for active learning.
