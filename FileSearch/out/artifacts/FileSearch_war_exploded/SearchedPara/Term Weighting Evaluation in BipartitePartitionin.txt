 The high dimension in text clustering is a major difficulty for most probabilistic methods such as Naive Bayes [1] and AutoClass [2]. To circumvent this problem, graph-theoretic techniques have been co nsidered for clustering [3]. They model the document similarity by a graph whos e vertices correspond to documents and weighted edges give the similarity betw een vertices. Graphs can also model terms as vertices and similarity between terms i s based on documents in which they co-occur. Partitioning the graph yields a clustering of terms, which is assumed to be associated with similar concepts [4]. The duality between document and term clustering can also be naturally modeled using a bipartite, where documents and terms are modeled as vertices on tw o sides respectively and only edges linking different types of vertices are allowed in the graph [5]. Finding an optimal partitioning in such a bipartite gives a co-clustering of documents and terms, with the expectation that documents and t erms in the same cluster are related to the same topic. In addition, bipartite graphs have also been used to model other relationships, such as (documents, concepts)[6] and (authors, publications)[7].
Since the general partitioning goal is to minimize the edge cut, term weighting schemes, which assign weights to edges linking term and document vertices, is a vital step to the final clustering perfo rmance. Many researchers have studied text clustering based on different term w eighting schemes or different criterion functions using conventional probabilistic methods [8,9,10,11]. For instance, the authors of [9] pointed out that it is the text representation schemes that dominate the clustering performance rather th an the kernel functions of support vector machines (SVM). In other words, choosing an appropriate term weighting scheme is more important than choosing and tuning kernel functions of SVM for text categorization. However, to the best of our knowledge, there is little work on comparing weighting schemes for graph based text clustering, not to mention bipartite partitioning. For this purpose, we concentrate on the comparison of various term weighting schemes in biparti te based text clustering. Specifically, we provide a comprehensive experimental evaluation with real world document datasets from various sources, and with various external validation measures. Overview. The rest of this paper is organized as follows. Section 2 describes the necessary background. Section 3 provides the results of extensive experiments. Finally, we draw conclusions in Section 4. In this section, we first describe the overview of bipartite based text clustering. Then we introduce the term weighting schemes that will be compared in this paper. 2.1 Bipartite Generation To apply clustering algorithms, a document data set is usually represented by a matrix. First we extract from documents unique content-bearing words as features, which involves removing stopwords and those with extreme document frequencies. More sophisticated techniques use support or entropy to filter words further. Then each document is repres ented as a vector in this feature space. With rows for documents and columns for terms, the matrix A  X  X  non-zero entry A ij indicates the presence of term an absence.

Agraph G =( V, E ) is composed of a vertex set V = { 1 , 2 , ..., | V |} andanedge set { ( i, j ) } each with edge weight E ij . The graph can be stored in an adjacency matrix M ,withentry M ij = E ij ifthereisanedge( i, j ), M ij =0otherwise. Given the n  X  m document-term matrix A , the bipartite graph G =( V, E )is constructed as follows. First we order the vertices such that the first m vertices index the terms while the last n index the documents, so V = V W  X  V D ,where V
W contains document. Edge set E only contains edges linking different kinds of vertices, so the adjacency matrix M may be written as 2.2 Bipartite Partitioning Given a weighted graph G = { V, E } with adjacency matrix M , clustering the graph into K parts means partitioning V into K disjoint clusters of vertices V ,V goal is to minimize the sum of the weights of those cut edges. Formally, the cut between two vertex groups V 1 and V 2 is defined as cut ( V 1 ,V 2 )= i  X  V Thus the goal can be expressed as min { V avoid trivial partitions, often the constraint is imposed that each part should be roughly balanced in terms of part weight wgt ( V k ), which is often defined as sum of its vertex weight. That is, wgt ( V k )= i  X  V the same cut value, the above objective function value is smaller for the more balanced partitioning.

In practice, different optimization cr iteria have been defined with different vertex weights. The ratio cut criterion [12], used for circuit partitioning, defines wgt ( i ) = 1 for all vertices i and favors equal sized clus ters. The normalized cut criterion [13], used for image segmentation, defines wgt ( i )= j M ij .Itfavors clusters with equal sums of vertex degr ees, where vertex degree refers to the sum of weights of edges incident on it.

Finding a globally optimal solution to such a graph partitioning problem is in general NP-complete [14], though different approaches have been developed for good solutions in practice [15,16]. Here we employ Graclus [16], a fast kernel based multilevel algorithm, which involves coarsening, initial partitioning and refinement phases. As for the graph partitioning criterion used in Graclus, we tried both the normalized cut criterion and the ratio cut criterion. We found the former always produces better results, possibly for the following reasons. First, our datasets are highly imbalanced, which makes unreasonable the constraint of equal sized clusters by the ratio cut c riterion. Second we find that sometimes it yields clusters of pure word vertices, w hich makes it impossible to determine the number of document clusters (cluste rs containing the document vertices) beforehand. Those terms with low frequen cies are likely to be isolated together, since few edges linking outside are cut. As for the normalized cut criterion that tries to balance sums of vertex degrees in ea ch cluster, the resultant clusters tend to contain both document and term vert ices. So in this paper we only report results from the normalized cut criterion. 2.3 Term Weighting Term weighting schemes determine the value of non-zero entry A ij in the document-term matrix when term w j appears in document d i . Two frequen-cies are commonly used. Term frequency tf ij denotes the raw frequency of term w j in document discriminating power of w j ,where n j is the number of documents that contain w . In this paper, we compared six term weighting schemes listed in Table 1. Most of these term weighting schemes have been widely used in information retrieval and text categorization. The first four term weighting schemes are dif-ferent variants of tf factor. The last two incorporate idf . According to a recent study of text classification with SVM [11], although the first four schemes relate with term frequency alone, all of them sh ow competitive performance with other sophisticated schemes except binary .The idf factor, taking the collection dis-tribution into consideration, does not improve the terms discriminating power with SVM. The tf idf factor, combining both term and document frequencies, usually yields best results in query based document retrieval [17]. In this section, we present an extensive ex perimental evaluation of various term weighting schemes. First we introduce the experimental datasets and cluster validation criteria, then we report comparative results. 3.1 Experimental Datasets For evaluation, we selected 10 real data sets from different domains used in [10]. The RE0 and RE1 datasets are from the Reuters-21578 text categorization test collection Distribution 1.0. The datasets K1a, K1b and WAP are from the WebACE project; each document corresponds to a web page listed in the subject hierarchy of Yahoo. In particular, K1a and K1b contain the same data but K1a X  X  class labels are at a finer level. The da tasets TR31 and TR41 were derived from the TREC collection. The LA1 and LA2 datasets were obtained from articles of the Los Angeles Times that was used in TREC-5. The FBIS dataset is from the Foreign Broadcast Information Service data of TREC-5. For all data sets, we used a stoplist to remove common words, stemmed the remaining words using Porter X  X  suffix-stripping algorithm, and discard those with very low document frequencies. Some characteristics of them are shown in Table 2. 3.2 Validation Measures Because the true class labels of documents are known, we can measure the quality of the clustering solutions using externa l criteria that measure the discrepancy between the structure defined by a clustering and what is defined by the class labels. First we compute the confusion matrix C with entry C ij as the number of documents from true class j that are assigned to cluster i .Thenwecalculate the following four measures: normalized mutual information( NMI ), conditional entropy( CE ), error rate( ERR ) and F-measure.

NMI and CE are entropy based measures. The cluster label can be regarded as a random variable with the probability interpreted as the fraction of data in that cluster. Let T and C denote the random variables corresponding to the true class and the cluster label, respectiv ely. The two entropy-based measures are H ( X ) denotes the entropy of X and log 2 is used here in computing entropy. NMI measures the shared information between T and C and it reaches the maximal value of 1 when they are the same. CE tells the information remained in T after knowing C and it reaches the minimal value o f 0 when they are identical. Error rate ERR ( T | C ) computes the fraction of misclassified data when all data in each cluster is classified as the majority class in that cluster. It can be regarded as a simplified version of H ( T | C ).

F-measure combines the precision and r ecall concepts from information re-trieval [17]. We treat each cluster as if it were the result of a query and each class as if it were the desired set of documents for a query. We then calculate the P size / i -th cluster size. Note that C + j could be larger than the true size of class j if some documents from it appear in more than one cluster. F-measure of cluster i is a weighted average for each class, F = 1 n j C + j max i { F ij } ,where n is the total sum of all elements of matrix C . F-measure reaches its maximal value of 1 when the clustering is the same as the true classification. 3.3 Clustering Results Let c in cK denote the number of partitions we set. The six term weighting schemes are evaluated in terms of the four validation measures at 5 K, 10 K, 15 K and 20 K . The detailed results are shown in Table 3. NMI and F are preferred large while ERR and CE are preferred small. For each setting, the best results are highlighted in bold numbers. One can see in most cases tf idf gives the best results and binary performs worst. Although tf , logtf and itf perform best in certain settings, their gap between tf idf is not significant. The last six rows of Table 3 give the number of wins over all measures at four levels of clustering granularity, respectively. The superiority of tf idf is obvious at three levels.
To give a summary of performance, Figure 1 illustrates the average results over all datasets for each measure. Apparently, tf idf gives the best results on all the measures except CE . Recall that CE is an un-normalized measure, which makes averaging questionable. This indicates that the incorporation of the dis-criminating factor, idf , really makes a difference. It is confirmed again if we compare the performance by idf and tf alone. One can see at 5 K, 10 K, 15 K , idf beats tf . It shows that at relatively moderate clustering granularity, the term relevance for the whole document set is more important than that for each indi-vidual document. However, as the numbe r of clusters gets larger (e.g., 20 K )and each cluster gets smaller, the term fre quency within each document matters. Since the goal of bipartite partitioning is to minimize the edge cut, term weight-ing schemes are essential for the final clu stering performance. In this paper, we provided an extensive comparison of s ix commonly used schemes. Our experi-mental results show that tf idf generally yields better performance than other term weighting schemes in terms of various external validation measures. Besides, at moderate clustering granularity, idf is more important than tf . Because the graph partitioning is always subject to the balance constraint, the vertex weight-ing also plays an important role. For the future work, we plan to investigate the impact of vertex weighting schemes. To c apture the full semantics that cannot be represented with single words, another direction is to augment the raw vocab-ulary with word-sets based on frequent itemsets [18,19] or hypercliques [20,21]. Acknowledgments. This work was partially supported by the Scientific Re-search Foundation for the Returned Overseas Chinese Scholars, State Education Ministry of China, and the Dongguan Foundation of Scientific and Technological Plans.

