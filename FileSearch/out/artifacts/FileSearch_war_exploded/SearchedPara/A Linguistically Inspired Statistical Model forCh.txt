 YUQING GUO and HAIFENG WANG Toshiba (China) Research and Development Center and JOSEF VAN GENABITH Dublin City University 6: 2  X  Guo et al.
 1. INTRODUCTION To date, the generation of punctuation 1 has received comparatively little at-tention in natural language processing, despite general acknowledgment that punctuation is an essential structural element in written texts. In the mono-graph The Linguistics of Punctuation , Nunberg [1990] provides a systematic linguistic account of punctuation in written text. Since then there has been a growing interest in research on the incorporation of punctuation into NLP sys-tems. On the analysis side, a number of studies have confirmed that making use of syntactic cues provided by punctuation considerably reduces ambigui-ties in syntactic analyses and increases both parsing coverage and accuracy [Briscoe 1994; Doran 2000; Li et al. 2005]. On the synthesis side, punctuation is even more important for effective and accurate comprehension of the gener-ated texts. Surface realization takes an abstract semantic or syntactic repre-sentation, usually in the form of a predicate-argument structure, dependency representation or logical form, as input and produces a corresponding gram-matical string. Predicate-argument structures (and similar generation input) do not usually (and arguably should not) represent punctuation information, therefore some additional effort for producing punctuation in generation out-put is usually required for a sentence realizer. However, often this is not car-ried out in a principled fashion, but instead by using ad-hoc post-processing heuristics to add missing and to remove excess punctuation in generation output [Cahill and van Genabith 2006], or by partially encoding punctuation in the input representation [Langkilde 2002]. White [1995] makes the first attempt to provide a proper syntactic treatment of punctuation from a gener-ation perspective. He elaborates and adapts Nunberg X  X  approach to syntactic realization of punctuation and proposes a stratificational architecture that in-stantiates the linguistic analysis within the Meaning-Text framework of the CoGenTex core realizer. In a wide-coverage Combinatory Categorial Grammar setting for sentence realization, White and Rajkumar [2008] capture punctu-ation using semantic discourse features in the generation input, generating sentences using punctuation-enhanced grammars, again using supplementary post-processing heuristics to filter out inadequate punctuation realizations.
A common characteristic of previous work on realizing punctuation in writ-ten texts is generating punctuation by the application of linguistic principles. In practice, these approaches rely on the particulars of the generation input and the underlying grammar formalism, and involve carefully establishing syntactic or semantic constraints on punctuation. As a consequence, they are often difficult to adapt to other representations, languages, and domains. At the same time, an alternative data-driven paradigm has emerged in research on recovering punctuation marks for the output of automatic speech recogni-tion (ASR) systems. These approaches are based on statistical models mak-ing use of lexical, prosodic, and acoustic information [Beeferman et al. 1998; Christensen et al. 2001; Batista et al. 2008]. Compared with the grammar en-gineering approach, the statistical models are less knowledge-intensive, more robust, and reusable. However, written language and spoken language are substantially different in their use of expressions, constructions, sentence length, and the number of punctuation marks. Punctuation in transcriptions of speech is primarily used to indicate intonation and pauses, using a limited set of comma, full stop, question, and exclamation mark. By contrast, com-plex written language tends to make extensive use of a significantly larger set of punctuation marks. Furthermore, the knowledge sources and technologies used for written-language applications and speech applications are often quite different. On one hand, acoustic features, such as pitch, phone duration, and speaker information recorded in ASR transcriptions are usually not available for written texts. On the other hand, while analyses of syntactic and semantic structures are important aspects for processing written texts, they do not tend to feature to the same extent in speech processing.

The aim of this article is to produce the full range of punctuation marks for the output of a general-purpose sentence realizer for written text. To this end, we study the syntactic properties of punctuation marks used in written texts from a linguistic point of view. We capture the linguistic properties of punc-tuation marks based on statistical techniques using information from both raw surface strings and more abstract dependency representations available to the sentence realizer. Specifically, we design a three-tier architecture for generating three functionally different types of Chinese punctuation marks in sequence by three maximum entropy models. Moreover, we use mutual infor-mation to automatically extract cue words as a specialized feature to model sentence-final punctuation marks. In contrast to previous work, the linguis-tically inspired statistical approach significantly reduces the manual effort involved in formulating punctuation rules, and supports a modular integra-tion of the punctuation component with a general-purpose sentence realizer. This article concentrates on Chinese punctuation marks; however, the gen-eration methodology is generic and to a large extent domain-and language-independent.

The remainder of the article is organized as follows: Section 2 provides a lin-guistic analysis of the types of Chinese punctuation marks. Section 3 presents a maximum entropy model incorporating linguistically-motivated information for punctuation generation, and describes a general method to automatically acquire desired features from a corpus. We give details of the experiments and analyse results in Section 4. Finally, we conclude and provide directions for future work.
 6: 4  X  Guo et al.
 2. LINGUISTIC OBSERVATIONS ON CHINESE PUNCTUATION 2.1 Types of Chinese Punctuation Marks The Chinese punctuation system is derived from European languages, and most of the commonly used punctuation marks in modern Chinese have a counterpart in European languages. Nevertheless, there are differences be-tween European and Chinese punctuation marks in both shape and usage. Chinese punctuation marks are called full width, inasmuch as they double the width of their European counterparts. For example, the Chinese punctuation mark for ellipsis consists of six dots, whereas the English ellipsis is a row of three dots. Moreover, there are a few punctuation marks specific to Chinese (and East Asian languages). According to the Chinese National Standard Use of Punctuation Marks , 2 there are a total of 16 Chinese punctuation marks in standard usage. They fall into two major types in terms of function as defined by the national standard:
Delimiters indicate pauses in text and separate semantic units into words, phrases, clauses, and sentences. There are seven delimiting punctuation marks in Chinese, which are categorized into two subtypes according to their locations in a sentence.  X  Sentence-final delimiters consist of full stop, question mark, and exclama-tion mark. They are used at the end of a sentence, indicating the sentence boundary as well as expressing the mood of the sentence.  X  Intra-sentence delimiters include comma, enumeration comma, semicolon, and colon, which are used within a sentence to separate a variety of elements.

Indicators include nine different Chinese punctuation marks which are used to mark off elements of distinct functions and properties.

Table I lists the 16 punctuation marks with their counts and percentages as extracted from the text of the Penn Chinese Treebank version 6.0 (CTB6, LDC2007T36). The CTB is a segmented, part-of-speech tagged, and fully bracketed corpus, consisting of 18,906 sentences from Xinhua newswire, Hong Kong News and the Taiwan Sinorama news magazine. 4
As the counts exhibit, the frequencies of the 16 punctuation marks vary greatly. Emphasis marks and proper name marks are not found in the CTB6 data. They are usually used for typographical purposes in teaching materials or archaic books, to emphasize and indicate proper names, respectively, as in Examples (1) and (2). (1)  X   X   X  .  X  X  X  X  ,  X  X   X  .  X  X  X  X  D X  (2)  X   X  X  X  X  X  ,  X   X  X  D X 
Middle dots and en dashes are also rare and appear in limited contexts. A middle dot is often used in a foreign name to separate between surname, first name, and middle name, such as in Example (3). (3)  X  X   X  g X   X   X  g  X   X  X  X   X  X 
An en dash usually connects two words to signify a range as in Example (4), or relation as in Example (5). (4)  X  X  X  X  X  X  X   X  X  X  X  X  (5)  X   X   X   X  X   X  X 
Since these four marks are only occasionally used in modern Chinese, and moreover, since they have a fairly limited impact on text comprehension, this article focuses on the remaining commonly used punctuation marks.

Comma, full stop, quotation marks, and enumeration comma are the most frequently occurring Chinese punctuation marks, which in total constitute nearly 90% of the CTB6 punctuation data. The remaining marks are relatively infrequent, yet widely used. There are two marks specific to Chinese. One is the enumeration comma. The enumeration comma, also known as pause mark , is used particularly for separating words or short phrases in coordination, as in 6: 6  X  Guo et al. sentence (6). By contrast, long-length phrases or clauses in coordinated struc-tures, similarly to English, are generally separated by comma or semicolon. (6)  X   X   X   X  X  X   X   X  X   X   X  X  X  X  X 
The other special mark is the pair of book title marks. As the name implies, they are used for titles of books, newspapers, movies, songs, etc., as in Example (7). (7)  X   X  X  X  X   X  lj X  X  X  X   X   X  X  X  NJ
By and large, most common Chinese punctuation marks have similar func-tions as their English counterparts, with some remaining variation in usage. Some examples are given below. In Example (8-a), a colon is required to in-troduce direct speech (in contrast to a comma introducing indirect speech) in Chinese, whereas in the corresponding English sentence Example (8-b), a comma is used. (8) a.  X  X  X  X  X  X  X  :  X   X  ,  X  X  X  X  X  X  ! X 
Example (9) shows that the conventional use of comma for dates in English is unnecessary in Chinese. (9) a. 2009  X  10  X  1  X 
Though italics are commonly used in English for emphasis, the titles of works, or a word representing itself, among others, italics are seldom used as punctuation in Chinese, instead, the corresponding effect is achieved by em-phasis marks, book title marks and quotation marks, as in sentences (1), (7), and (10). (10) a.  X  X he X   X D X  X  X  2.2 Interaction Between Punctuation Marks Interactions between different types of punctuation marks in English have been extensively discussed by Nunberg [1990]. One of the important rules for punctuation presentation is the point absorption rule, which defines commas, dashes, semicolons, colons, and periods as points and stipulates that no more than one point can be presented in sequence. When two points happen to coin-cide, one is absorbed by the other according to their precedence (period &gt; colon &gt; semicolon &gt; dash &gt; comma). We observe that there is a similar situation in Chinese: delimiting punctuation marks approximate to points and conform to the point absorption rule, viz. two delimiters are never adjacent to each other in theory. This can also be interpreted from a functional perspective: each de-limiting punctuation mark serves to separate particular types of elements in a certain context. Indicators, however, have different functions, hence there is a good possibility that an indicator appears together with a delimiter or another indicator in the same context.

This article does not aim to provide a comprehensive theoretical analysis of interactions between Chinese punctuation marks, rather, in a more practi-cal way we include all possible adjacencies as observed in the data. From the CTB6 corpus, we extract a total of 73 types of adjacent punctuation marks, among them 23 types occur just once and over two thirds are rare types. Table II lists the most frequent punctuation sequences with a count higher than 50. From the punctuation sequences we extracted, we derive the follow-ing observations.  X  Most adjacencies happen between an indicator and a delimiter.  X  Sequential indicators are very rare. For example, the sequence of &lt; )  X   X  &gt; (an em dash following a close bracket) only occurs once in the whole CTB6 data. Though there are few cases involving the quotation mark and another indicator mark, none of them occurs more than 20 times ( &lt; 0.02%).  X  We find only one case of sequential delimiters &lt; ? ! &gt; that occurs three times in the CTB6 data. Therefore it can be viewed as a real exception, and in-dicates that the data accords well with the theoretical description that two delimiter should not be adjacent.

In our approach we do not impose any hard constraints on adjacent punctu-ation marks; nevertheless, inadequate punctuation sequences are effectively prevented by the statistics obtained from the data and valid sequences are guaranteed. For example, the sequence &lt; , ) &gt; (a comma followed by a closing parenthesis) is never found in the corpus and thus it is not a possible yield of our generator. This is also consistent with the bracket absorption rule as defined in Nunberg [1990].
 6: 8  X  Guo et al.
 3. A STATISTICAL APPROACH TO PUNCTUATION GENERATION Instead of representing the linguistic properties of punctuation by carefully devising grammar rules as in previous approaches to punctuation generation [White 1995; White and Rajkumar 2008] or formalizing the discourse functions of punctuation [Say and Akman 1996; Reed and Long 1997], the aim of this ar-ticle is to (1) minimize the amount of manual specification as much as possible by using statistical techniques; and (2) implement a punctuation generator for unpunctuated strings as output by a sentence realizer in a modular pipeline NLG architecture. 3.1 The Sentence Realizer We instantiate the syntactic treatment of punctuation for a sentence real-izer based on Lexical-Functional Grammar (LFG) [Kaplan and Bresnan 1982]. LFG is a constraint-based grammar formalism that postulates (minimally) two levels of representation: c(onstituent)-structure and f(unctional)-structure. A c-structure is a conventional phrase structure tree like a Penn treebank tree that represents surface grammatical constraints on word order and phrase configurations. An f-structure encodes more abstract grammatical functions (GFs), such as SUBJ (ect), OBJ (ect), ADJ (unct) etc. As illustrated in Figure 1, f-structures are hierarchical attribute-value matrices approximating to basic predicate-argument-adjunct structures or deep bilexical dependency relations. In contrast to c-structures that capture language-specific configurations and produce unique representations for different languages, f-structures encode a more abstract (and somewhat more universal) level of analysis, supporting cross-language parallelism at this level of abstraction. Similar to other genera-tion tasks using abstract syntactic or semantic input representations, we adopt f-structures as input to the sentence realizer [Guo et al. 2008], which linearizes GFs by a dependency-based n -gram model to produce words in the correct or-der. F-structures only encode notional words in the PRED (icate) values, and language-specific elements, such as punctuation marks, are not conventionally expressed in f-structures. As a consequence, punctuation marks are not easily generated by the sentence realizer. (11)  X  X  :  X   X  X  X   X  X  X   X  X  ! X  3.2 A Maximum Entropy Model In order to introduce punctuation into the unpunctuated string produced by the sentence realizer and make the generation components as independent as possible, we design a separate module for generating punctuation marks, ob-viating the necessity of incorporating complicated punctuation rules into the generation grammar. The problem of punctuation generation can be reformu-lated as a multiclass classification task that predicts which punctuation mark is likely to be present (or absent) at a position within a particular context. For the example of sentence (11), a colon and an open quotation mark should be inserted after the second word  X  /say , an exclamation mark and a clos-ing quotation mark should be inserted at the sentence-end position, and at all remaining positions (between two words) no punctuation marks should be presented. Other NLP problems, such as part-of-speech tagging [Ratnaparkhi 1996], sentence boundary detection [Reynar and Ratnaparkhi 1997], semantic role labeling [Xue and Palmer 2005], among others, are typical classification applications, which have been successfully solved by statistical techniques.
One of the classification approaches is the Maximum Entropy (MaxEnt) model, which offers a flexible way to combine information from many hetero-geneous sources for estimating the probability of a particular linguistic class occurring in a particular linguistic context. This article demonstrates the use of MaxEnt models for predicting punctuation marks. The MaxEnt model de-fines the probability P ( p | c ) for a punctuation mark p given a representation of the context c : where f i ( p , c ) is a feature function linking properties of the given context c and a particular punctuation mark p , and  X  i is the weight of f i . The goal of the Max-Ent model is to find the probability P with maximum entropy among all the distributions consistent with the constraints imposed by the feature functions, which is equivalent to choosing the set of parameters {  X  i } that maximizes the likelihood of the training data [Ratnaparkhi 1997].
 6: 10  X  Guo et al.
 3.3 The Generation Algorithm To better model the three types of Chinese punctuation marks as described in Section 2.1, we design a three-tier architecture for generating different punc-tuation types step by step. Figure 2 depicts the generation architecture. The input consists of surface strings and the corresponding f-structures as given to the sentence realizer. Three MaxEnt models are trained separately for the three types of punctuation marks. First, a sentence-final delimiter is attached to the end of each sentence, then intra-sentence delimiters are inserted to di-vide the string into units, finally indicators are generated for distinguishing special elements. The three-tier architecture has the following advantages.  X  Different features and parameters can be used by individual MaxEnt models to better account for varied properties of different punctuation types.  X  Generating delimiting and indicating punctuation marks in separate steps licenses punctuation adjacencies. For example, a colon generated in the sec-ond step will be treated equally to a normal word in the third step, which allows an open quotation mark to be inserted right after it, producing the sequence of &lt; :  X  &gt; . 3.4 Common Feature Functions Unlike transcripts of spoken language that provide a relatively small feature space, the syntactic representation provided by f-structures contains a wide range of features that can be adopted to construct the MaxEnt models. In our case, the feature functions describe the context of a particular punctuation mark, more specifically, the context of each site between two words within a sentence at which a certain (or no) punctuation mark presents. In this section, we briefly discuss some common features and explain at an intuitive level why they are useful for generating punctuation marks. The exact set of features appropriate for each of the three MaxEnt models (see Figure 2) is determined by experiments on the development data as reported in Section 4.2.1.
Word. If the word is a common word (whose frequency is higher than a cer-tain threshold), the surface form of the word is used, otherwise a set of generic lexical features (i.e., number, person, type, and the like) is used to represent the word. The word feature is extracted from the current word (after which a punctuation mark is inserted), the two previous words and the two following words. To indicate the beginning and end position of a sentence, two special the linguistic intuition that some punctuation marks have a tendency to oc-cupy particular positions in a sentence or co-occur along with specific words. For example, delimiting punctuation marks never occur at the beginning of a tend to precede a colon in Chinese.

GF. Each predicate in a local f-structure is realized as a word in a surface string. The GF feature stands for the grammatical function of the current predicate, for example, in Figure 1, the word  X  /come is realized from the predicate of f-structure f 4 , whose GF is COMP .

Ancestors. GFs of the upper-level f-structures dominating the current local f-structure. For example, f 3 is the parent f-structure dominating f 4 whose predicate is  X  /come , hence the parent GF of the word  X  /come is COMP . The ancestor feature denotes either the direct dominating parent f-structure or a larger context in the f-structure, for instance the whole sequence of the GFs along the path from the parent f-structure to the outermost f-structure. In Figure 1, though the parent GFs of f 4 and f 5 are both COMP , they can be disambiguated by their ancestor GF sequences, viz. COMP : TOP 5 representing the ancestors of f 4 and COMP : COMP : TOP representing the ancestors of f 5 .
Children. The set of GFs of the subordinate/children f-structures, for exam-ple, the children GF set of f 4 is { SUBJ , OBJ } .

The three GF features together determine the location of a word in an f-structure. These GF features are extracted for the current word and the left and right neighboring two words. Grammatical functions in many cases im-pose constraints on punctuation marks, for example, a comma usually appears after a subject, a matrix verb or an adverbial modifier to mark a pause; com-mas alternate with semicolons to separate conjuncts in coordinated structures for multiple levels of embeddings. In addition to GFs, we extract three further features containing structural information for the target position.
Head. The predicate word of the parent f-structure, for example, the head word of  X  / /come is  X  X  X  / /welcome . 6: 12  X  Guo et al.

Yield. The yield length of a local f-structure, for example, the f-structure f 3 yields a phrase  X  X  X  / /welcome  X  X  / /you  X  / /come  X  X  / /Beijing that consists of four words.

Border. A binary valued feature to indicate whether the predicate is a bor-der word, that is to say, if it is the first or last word in the yield covered by the f-structure. For example,  X  X  / /you is the only word realized by the f-structure f 5 and thus is a boundary word of f 5 , whereas it is the second word in the yield  X   X  / /welcome  X  X  / /you u  X  /come  X  X  /Beijing and thus not a boundary word of f 3 .

For the example of sentence (11) and the corresponding f-structure repre-sented in Figure 1, Table III contains the features generated for the insertion site of the word  X  /say , where a colon is presented. 3.5 Sequence Classification As described in the previous sections, given a sequence of unpunctuated words, we use MaxEnt models to determine which punctuation mark should be present or absent after each word in the sequence from left to right. In this regard, the task is a sequence classification, which can rely on features from the current site, features from surrounding context, as well as informa-tion from previous predictions of the classifier. Our MaxEnt models adopt two additional features to record the punctuation history.

History. The identity of punctuation marks for the previous two positions, which informs the relationship between punctuation marks to some extent.
Length. The length of the span between the current position and the loca-tion where the preceding punctuation mark is present. This feature reflects scope differences between punctuation marks. For instance, enumeration com-mas separate words or phrases that are generally shorter than clauses sepa-rated by commas or semicolons.

So far, the MaxEnt models have incorporated features from the current word, surrounding words as well as prediction for previous words, however, they are unable to use information from later punctuation to inform their de-cisions early on. For example, a closing bracket is usually a good hint that an open bracket should have been generated previously. To overcome this short-coming and to find an optimal solution for the whole sequence, we combine the local MaxEnt model with a beam search strategy similar to that used by Ratnaparkhi [1996]. Given a sequence { w 1 ,...,w n } where each word w i is surrounded with its context c i , the probability of the punctuation sequence { p to and including word w i . The beam search algorithm (with a beam width of N ) runs as follows.
 3.6 Automatic Extraction of Cue Words As mentioned in Section 2, the three sentence-final punctuation marks do not only serve as a sentence delimiter but also express the mood of the sentence. In general, a full stop indicates a statement, an exclamation mark indicates an exclamation or a command, and a question mark indicates a question. Ex-cept for declarative sentences, there are usually some cues to indicate mood, for example, interrogative words in questions and interjections in emphatic sentences. Cue words closely related to a punctuation mark can be captured by a separate feature of the MaxEnt model to facilitate the punctuation clas-sification. Linguistic observations show that certain cue words tend to occur at particular positions in some constructions, for instance, a wh-word appears at the beginning of an interrogative clause in English, and a modal particle expressing emotion occurs at the end of a sentence in Chinese. However, this is not a universal across constructions and languages. For example, Chinese is a wh-in-situ language that does not exhibit wh-movement, as in sentence (12), where the interrogative adverb  X  X  X  /why remains in the canonical word order position. (12)  X   X  X  X   X  X  X   X  ? 6: 14  X  Guo et al.

Adhering to our overall objective to automate the generation process to the maximum extent and learn as much information as possible from the data, we propose a general method to automatically extract cue words from a corpus to capture information which would otherwise be expressed by grammar patterns designed manually for specific constructions and languages. The basic idea is that if a word tends to coincide with a particular punctuation mark, the word can be used as a cue for the mark. To quantify the correlation between a word and a punctuation mark, we use the measure of mutual information , a concept derived from information theory. Fano [1961, pp. 27 X 28] originally defined pointwise mutual information between two particular events x and y as follows: the respective marginal probability distribution functions.

Pointwise mutual information has been used as a measure of association between two elements in many natural language applications, such as for dis-covering collocations [Church and Hanks 1990; Lin 1998], text categorization [Sebastiani 2002] and so on. In this specific task, we use it to select cue words closely related with a particular sentence-final delimiter. In this case, the two variables in Equation (3) are a word w and a sentence-final punctuation mark p . Given a large corpus, the marginal probabilities P ( w ) and P ( p ) are esti-mated by counting the numbers of sentences where w and p occur indepen-dently, viz. s ( w ) and s ( p ), and normalizing by the total number of sentences in the corpus S . The joint probability P ( w, p ) is estimated by counting the num-ber of sentences where w co-occurs with p , viz. s ( w, p ), and normalizing by S . Accordingly, Equation (3) is reformulated as:
Intuitively, the stronger the correlation between w and p , the greater the mutual information is, and vice versa. When w and p are independent, P ( w, p ) = P ( w )  X  P ( p ), then I ( w, p ) = 0. A well-known problem with mutual in-formation is that it overestimates low-frequency events [Manning and Schutze 1999, Section 5.4]. To avoid noise caused by infrequent words, words occurring less than a frequency threshold in the corpus, as well as proper nouns, tempo-ral nouns and numbers, are not considered as cue words. Another difficulty in this methodology is the full stop mark. As full stops occur in an overwhelm-ing number of sentences, this leads to P ( w, p )  X  P ( w ) for most words, hence mutual information is not a reliable indicator to estimate correlation between a word and a full stop. This also accords with the linguistic intuition that no very typical words imply a declarative sentence. As a consequence, we extract a list of cue words for question marks and exclamation marks according to the rankings of mutual information scores. Then the cue word feature is combined with the sentence-end token in the MaxEnt model for discriminating among the sentence-final delimiters.
 4. EXPERIMENTS AND RESULTS 4.1 Experimental Setup We carry out experiments on the data provided by the Penn Chinese Tree-bank 6.0 (excluding the portion of ACE broadcast news). Following the rec-ommended splits (in the list-of-files of CTB6), we divide the data into 756 files (83% of the total) as training set, 84 files (10%) as test set and 50 files (7%) as development set. Table IV provides details of the data sets.

The CTB consists of traditional syntactic phrase structure trees. To gen-erate LFG f-structures (or labelled dependencies) from the phrase structure trees, we have devised an automatic constituent-to-dependency conversion al-gorithm [Guo et al. 2007]. Based on the conversion algorithm, two types of f-structures are produced: (1) proper f-structures derived from gold handcrafted CTB trees; (2) noisy f-structures derived from raw text by the aid of an au-tomatic part-of-speech tagger and statistical parser. The two types of auto-matically acquired f-structures along with the raw sentences stripped of all punctuation marks are used as input for the punctuation generation experi-ments. In the experiments, we implement the MaxEnt classification models using the OpenNLP Maximum Entropy Package. 6
For evaluation purposes, each punctuation mark in a sentence is repre-ceding the punctuation mark in question and end is the following word. For Example (11), repeated below: (13)  X  X  :  X   X  X  X   X  X  X   X  X  ! X 
The triple: &lt;  X  ,  X   X  &gt; indicates a colon is inserted between the words  X  /say and  X  X  X  /welcome . Similarly, the triple  X  &lt; : ,  X  X  X  &gt; indicates an open quotation mark generated after the colon. 7 We evaluate the punctuation gen-eration algorithm in two respects: (1) if any punctuation mark is inserted in a correct position ( insertion ), in this evaluation, different punctuation forms in the triples are regarded the same; and (2) if a correct punctuation mark is restored in the position ( restoration ). The evaluation algorithm compares the 6: 16  X  Guo et al.
 automatically-punctuated sentences with the original reference sentences, and calculates precision, recall and f -score, as in the following equations: 4.2 Experiments on Gold Data 4.2.1 Performance of the MaxEnt Models. Most previous methods of punc-tuation realization for written text are rule-based. Unfortunately, such sys-tems are not easily available and are therefore not compared with our MaxEnt-based generation model. For the purposes of comparison, we adopt a baseline system that has been more commonly used for spoken language, for exam-ple, for restoring punctuation information in the evaluation campaign for spo-ken language translation (IWSLT 2006-2008). The baseline system is a simple word-based hidden 5-gram language model that is retrained on the CTB train-ing data (with interpolated Kneser-Ney smoothing) using the SRI Language Modeling Toolkit. 8 The first line (HNgram) of Table V gives the results of the hidden n -gram model for the CTB test set. The baseline model achieves an f -score of 54.99% for punctuation insertion and 52.01% for restoration. The results show that even though the word-based n -gram model has been widely used in speech research, it does not produce acceptable results on written text. The reason is mainly because the structure of written texts is more complicated than that of spoken language, and the set of punctuation marks in spoken lan-guage transcriptions is much smaller and simpler than for written language. The IWSLT 2008 evaluation campaign used a test set with average sentence length less than 10 words (compared to 28 words of the CTB test data), and only comma, full stop, question, and exclamation mark are taken into consid-eration in the IWSLT evaluation.
For the MaxEnt model, we take the surface word forms as a basic feature, then add the features described in Section 3.4 incrementally to the MaxEnt models. Table V gives the overall results for all produced punctuation marks. The basic word feature achieves an f -score of 57.22% for punctuation insertion and 52.61% for punctuation restoration, which is better yet still comparable to the baseline hidden-n -gram model. However, compared with the n -gram model, the MaxEnt model has the ability to incorporate a wide variety of structural features from the input data, which further dramatically improve the genera-tion quality. The three GF -related features encoding f-structure context con-sistently improve all scores in all three MaxEnt models, boosting the overall insertion f -score to 77.69% and the restoration f -score to 72.56%. The im-pact of the remaining common features is somewhat contradictory. By testing on the development data, all features are adopted for modeling intra-sentence delimiters, the border and head features are selected for modeling indicators, whereas no more features are used for sentence-final punctuation marks. The MaxEnt models incorporating all optimal common features achieve an f -score of 79.34% for insertion and 74.04% for restoration, which substantially outper-form the simple hidden n -gram model.

As described in Section 3.5, punctuation generation can be viewed as se-quence classification. In addition to the static common features, the MaxEnt models can also possibly benefit from history information regarding previ-ous predictions and future decisions about the following punctuation marks. Table VI evaluates the performance of the MaxEnt models incorporating history features, combined with the beam search strategy. Compared with Table V, including history features and keeping N -best alternatives further im-prove the performance of the MaxEnt models. However, the increase is rather small. We conjecture that there are two main reasons for this.  X  Unlike other typical sequence classification applications, for example, part-of-speech tagging, the distance between two punctuation marks in a sen-tence is relatively long, and the relationship between punctuation marks in a sequence sometimes varies considerably. For instance, we expect a closing quotation mark to follow an open quotation mark, however, in many cases such as direct speech, other punctuation marks appear between the pair of quotation marks.  X  Including history information does improve the results for commas and enu-meration commas for which the MaxEnt model produces better statistics, however, and contrary to our expectations, keeping N -best lists does not help to improve the generation for paired punctuation marks, for example, quota-tion marks and book title marks. This is mainly because the statistics from the training data does not model these marks well due to their low frequency 6: 18  X  Guo et al.
 or particular linguistic properties. As a result, a bad decision on one of the paired marks often leads to another bad decision on the other. 9 4.2.2 Cue words. Using the method described in Section 3.6, we automat-ically extract 100 cue words from the training set correlated with question marks and exclamation marks, respectively. 10 Table VII gives the results for the MaxEnt model incorporating the cue word feature. As question marks and exclamation marks account for a very small portion (around 1.2% in the CTB6) of the whole range of punctuation marks, the cue word feature produces only an insignificant improvement in the overall restoration scores. However, a closer look at the results shows that the cue word feature performs well for question marks, boosting the f -score from 16.44% to 31.71%. Furthermore, to some extent cue words help improve the prediction of exclamation marks. Though no special features are extracted for full stops, the improved results for question marks and exclamation marks also results in a slight increase in the precision for full stops.

As shown by the results, the exclamation mark is one of the most difficult punctuation marks to predict (even with the cue word feature). We conjecture that there are at least threes reasons accounting for this difficulty and the relatively limited effect of cue words on exclamation marks.  X  The use of exclamation marks is fairly varied. In a number of cases, a sen-tence ending in an exclamation mark does not feature any cue words, such as a command or an exclamation that expresses strong emotion by tones or high volume rather than any particular words in the string. In some other cases, cue words also commonly appear in other sentence types, for example, interrogatives feature both rhetorical questions (which may end in an ex-clamation mark) and normal questions, or degree adverbs are both common in exclamatory and declarative sentences. Examining the development data, we find that only four (out of 21) exclamation marks co-occur with a cue word extracted and used in our MaxEnt model. Among the rest, 12 sentences do not include any words overtly indicating exclamation, four sentences involve degree adverbs which are not extracted as cue words by our approach, and the last one is a rhetorical question.  X  Modal particles used to express emotion usually appear at the very end of a sentence in Chinese. These particles have been already captured by the feature function of surrounding words.  X  Exclamation marks are infrequent. Table I shows that they occur 295 times (compared to full stops 14,468 times and question marks 722 times) in the CTB6, which is possibly not enough to yield reliable statistics.

The cue word feature improves the precision for question marks to 65.00%, however, recall (though increased) still remains rather low. We find that in the test reference set 23 (37%) question marks are placed mid-sentence (even though they are defined as sentence-final marks in theory). This happens in two situations: (1) quoted speech consisting of several sentences; (2) alterna-tive interrogatives, as in sentence (14): (14)  X   X  X  X  ?  X  X   X   X  ?
The first situation requires a more precise analysis of sentence boundaries, and the second is a common variant, where, strictly speaking, a comma should have been used. 4.2.3 Results for Different Punctuation Marks. Table VIII lists a break-down by punctuation forms of the generation results. As it shows, the most commonly used punctuation marks in Chinese, namely, full stop, (open and closing) brackets, comma and enumeration comma take the five top spots in the list, giving reasonable f -scores in the range of 95.17% to 73.39%. One common punctuation mark that has relatively low scores is the quotation mark. This reflects the difficulties in modeling quotation marks due to their varied usage. Other than quotations and speech, quotation marks in many cases are used for 6: 20  X  Guo et al.
 rhetorical purposes, such as irony, emphasis, unusual usage of a concept etc., which are difficult to capture by syntactic constraints only.
 suspect the reason is that those punctuation marks are too rare to be properly modelled by the statistics. For example, ellipsis occurs only seven times in the test data. We expect that including more training data would result in more re-liable statistics. Also, other mechanisms, such as heuristics or patterns, would be a useful complement to the present statistical generation model. 4.3 Experiments on Noisy Data 4.3.1 Experiments with Automatically Parsed CTB Data. So far, our ex-periments have been carried out on gold data, viz. LFG f-structures induced from human-annotated treebank trees. However, in real applications such as machine translation, punctuation marks are produced for automatically gen-erated data that are imperfect and include errors. To capture this, we carry out experiments on test data fully automatically generated. We first extract seg-mented sentences from the CTB test set, 12 and assign parts of speech tags us-ing the Stanford log-linear POS tagger [Toutanova and Manning 2000]. Then the automatically tagged sentences are fed into Bikel X  X  parser [Bikel 2004] trained on the CTB6 training sections to produce syntactic trees. Finally f-structures are converted from the automatically parsed phrase structure trees and used as input to the punctuation generator. Table IX lists the scores for the test set using imperfect f-structures as input. First, we use the same Max-Ent models that are trained on features derived from the gold f-structures converted from the original CTB trees as in the above experiments. Compared with the gold test data, the f -score for the noisy test data only slightly drops from 79.83% to 78.99% for the insertion evaluation and from 74.61% to 72.87% for restoration, as shown in the left column in Table IX. These results demon-strate that the MaxEnt models do not only work in theory, but generalize well to practical data.

Ideally, methods based on machine learning techniques should train models on data closely resembling the final test instances. However, in our experiment the MaxEnt models are trained on features from gold f-structures generated from the original CTB trees, whereas at run time the models operate on f-structures automatically generated from noisy trees output by a parser. To reduce the difference between training and test instances, it is interesting to investigate whether training the model on noisy data can improve the gener-ation performance. We carry out 10-fold cross-validation to parse the training portion of the treebank, dividing the training data into 10 parts and parsing each part in turn with the parser trained on the remaining nine parts. Then f-structures are automatically converted from the reparsed trees of the train-ing set, and features are extracted from the imperfect f-structures to train the MaxEnt models. The right column of Table IX lists the scores of the new mod-els trained on reparsed data, with consistent increases in both punctuation insertion and restoration over the results for the previous models trained on gold data. 4.3.2 Experiments with More Data. From our observation of the results ob-tained for different punctuation marks in Section 4.2.3, we have found that the current MaxEnt models do not perform well on infrequent punctuation marks due to data sparseness. The experiment in Section 4.3.1, on the other hand, shows that the MaxEnt models can be effectively trained on data automatically obtained from raw texts. Because of this, it is possible to automatically extend the system to make use of more training data without manually-developed syn-tactic structures as the CTB provides. In this experiment, we use an additional corpus, the Chinese Gigaword Second Edition (LDC2005T14), which is a com-prehensive archive of newswire text data. From a small portion of the corpus (Xinhua news 2004), we extract about 16,000 sentences, almost the same size as the CTB training data. Each of the gigaword sentences we select includes at least one less common punctuation mark, that is, one of the punctuation are automatically segmented by the Stanford Chinese word segmenter [Tseng et al. 2005]. Then f-structures are generated from these sentences using the same method as in the previous experiment via automatic tagging, parsing and conversion. F-structures obtained from the gigaword corpus and the CTB corpus are used together to train a new MaxEnt model. Table X provides a breakdown of the results.

Even though we do not observe a significant improvement in the over-all scores, there is a prominent increase in the scores for infrequent punc-tuation marks. The additional training data especially contribute to colon, semicolon, and book title marks, boosting f -scores by 8.31-11.86 absolute per-centage points. The quotation marks with more versatile properties show an absolute increase in f -scores of 2.64 and 3.41 percentage points for the open and closing quotation marks, respectively. The least common three punctua-tion marks, however, still seem to suffer from data sparseness, which suggests that other mechanisms such as heuristic patterns or grammar rules should be adopted to capture them. The increased training set does not contribute to other commonly-used punctuation, which may indicate that the CTB corpus has provided reliable statistics for modeling the highly frequent marks, and, 6: 22  X  Guo et al.
 at the same time, that the noise contained in the additional automatically seg-mented training data possibly has an adverse side effect on those punctuation marks. 4.4 Human Analysis of the Results Our results also show that there is a considerable gap between the precision and recall scores for all punctuation except the full stop. This resonates well with the notion that the use of punctuation marks is subject to text genres, writing styles and, to some extent, personal preferences. For example, a pause after a sentence-initial modifier or a subject or both, often heavily depends on the author X  X  choice. To estimate the extent to which punctuation varies be-tween human annotators, two Chinese native speakers are asked to punctuate the raw strings of the test data as used in our experiments. Table XI gives the results for the human annotations compared against the reference sentences from CTB. The outcome is surprising: there is a considerable disagreement on the punctuation choices between the human annotations and the CTB refer-ences, and also between the two human annotators. Sentences punctuated by the first annotator (one of the authors), who knows the CTB data better, get an f -score of 81.62% for punctuation restoration. The second annotator (a col-lege student from a linguistic department), who had never seen the CTB data before, avails of a wider variety of punctuation choices, producing an f -score of 71.20%, which is even lower than the score (74.61%) of the automatically punctuated strings.

To verify the human annotations, the manually punctuated results were swapped between the two annotators and checked by each other given the orig-inal CTB sentences as reference. The proofreading task confirms that the dis-crepancy in the punctuation use is mainly a reflection of personal preferences. We also find 56 manually punctuated sentences have different readings from the CTB references due to different insertion or use of punctuation marks. Ex-amples (15) and (16) are two such examples, where Examples (15-a) and (16-a) are the original sentences in the CTB, Examples (15-b) and (16-b) are sen-tences punctuated by the human annotators. All sentences are grammatical but with different readings. Chinese is a language that has hardly any mor-phological inflection and tends towards pronoun-dropping. In Example (15-a) the word  X   X  is interpreted as a noun application , whereas in Example (15-b) the same word form is used as a verb apply because of the different location of the enumeration comma. In Example (16-a), the two enumeration commas separate three coordinated reasons, but the interpretation of Example (16-b) is that there is only one reason (death of parents) set off by the comma, and a pronoun dropped after the comma (who sold the house and left the farmland desolated). These examples demonstrate that punctuation marks are vital for Chinese to prevent misinterpretation. (15) a.  X  X  X   X  X  X   X   X  X  X  X   X   X  X   X  X   X  X 
Though in the experiments, our MaxEnt model achieves higher scores than the student annotator on punctuation restoration, the results are perhaps not as good as the scores indicate. To find out which proportion of the mismatched punctuation marks in the automatically punctuated realizations are real er-rors and which are merely alternative usages, we conducted a human evalua-tion by examining 200 sentences from the development set punctuated by the MaxEnt model. Errors classified by types and punctuation marks are illus-trated by examples in Table XII.

Roughly speaking, the MaxEnt model performs reasonably well on the punc-tuation task. Among a total of 258 mismatched punctuation marks, 101 (39.1%) are acceptable choices and 157 (60.9%) are real mistakes. The major type (41.5%) of mistakes is missing punctuation marks, which accounts for the 6: 24  X  Guo et al.
 low recall of the results. 12.8% of the mistakes are unnecessary (or redundant) punctuation marks and 6.6% are insertion errors or mark errors. Concentrat-ing on punctuation marks, 62 mistakes involve the misuse of a comma. This is consistent with the frequency distribution of Chinese punctuation marks, where commas account for nearly half. Two further common mistakes are the enumeration comma (30 times) and quotation marks (22 times). This is not only due to their high frequencies of occurrence, but also because of their var-ied nature: enumeration commas are not easy to distinguish from commas in some coordinated structures; quotation marks can be used in a wide variety of contexts, a number of which allow personal choice, and possibly downright arbitrariness. By comparison, the full stop is normally easy to predict, hence the 28 mistakes of excessive uses of full stops are an exception. Actually, all the 28 sentences in question are headlines without any sentence-final mark in the CTB news articles, which otherwise should end with a full stop when oc-curring in the main body of the text. In this sense, this error is beyond the syn-tactic level. Likewise, the incorrect use of comma and colon in sentences with reported speech are because direct and indirect speech is not distinguished by our f-structure representations (and the derived MaxEnt features). These types of errors, we assume, can be reduced by including dialog structure and stylistic features in the MaxEnt model when they are available from the input representations, or perhaps by adding some heuristic constraints. 5. CONCLUSION We have shown that combining sophisticated statistical techniques with lin-guistic analyses facilitates the generation of punctuation. We have devel-oped a modular punctuation generator for a general-purpose NLG system in a pipeline architecture. Our punctuation generator involves three separate Max-Ent models dedicated to three different classes of Chinese punctuation marks. To improve the generation of certain punctuation types using specialized fea-tures, we propose an automatic feature extraction method to reduce manual effort on feature selection. Testing on the CTB6 data, our MaxEnt models comprehensively outperform a 5-gram word-based baseline as commonly used in spoken language translation competitions. More importantly, the MaxEnt models work reasonably well on both gold data and automatically generated imperfect data. Experiments with human annotators show that the model comes close to human performance on the same task. Finally, a manual error analysis of the classifier output shows that many of the mismatched punc-tuation marks do in fact result in acceptable choices, a fact obscured in the automatic string-matching based evaluation scores.

The method presented in this article is based on statistical techniques. How-ever, as Table VIII shows, some of the low-frequency or versatile punctuation marks, such as exclamation marks, quotation marks (and a few others), yield rather unsatisfying results under the current approach. It would be promising to look into a hybrid strategy complementing the statistical model with limited grammar engineering by either explicit rules or enriching the feature set of the input. Also, in this article we have been focused on how to select the features that are most appropriate for the punctuation generation task. As regards fea-ture integration, other classification models, such as support vector machines (SVMs) and conditional random fields (CRFs), may perform similarly to Max-Ent models, and we plan to compare the performance of these models on the punctuation generation task in the future. Our results show that the Max-Ent models work reasonably well for generating Chinese punctuation marks in newswire text. It is worth testing the methodology on texts from a wide variety of domains and on typologically different languages, such as English. We are grateful to the anonymous reviewers for carefully reviewing this article and providing very insightful comments.
 6: 26  X  Guo et al.

