 A useful tool in natural language processing tasks such as translation, speech recognition, parsing, etc., is the rank ed list of results. Modern systems typ-ically produce competing partial results internally and return only the top-scoring complete result to the user . The y are, ho we v e r , also capable of pro-ducing lists of runners-up, and such lists ha v e man y practical uses:
Figure 1 sho ws the best 10 English translation parse trees obtained from a syntax-based translation system based on (Galle y , et. al., 2004). Notice that the same tree occurs multiple times in this list. This repetition is quite characteristic of the output of rank ed lists. It occurs because man y systems, such as the ones proposed by (Bod, 1992), (Galle y , et. al., 2004), and (Langkilde and Knight, 1998) represent their result space in terms of weighted partial results of v arious sizes that may be assembled in multiple w ays. There is in general more than one w a y t o as-semble the partial results to deri v e the same com-plete result. Thus, the -best list of results is really an -best list of derivations .

When list-based tasks, such as the ones mentioned abo v e , tak e a s input the top results for some con-stant , the ef fect of repetition on these tasks is dele-terious. A list with man y repetitions suf fers from a lack of useful information, hampering diagnostics. Repeated results pre v ent alternati v e s that w ould be highly rank ed in a secondary reranking system from e v en being considered. And a list of fe wer unique trees than e xpected can cause o v e r fi tting when this list is used to tune. Furthermore, the actual weight of obtaining an y particular tree is split among its repeti-tions, distorting the actual relati v e weights between trees. (Mohri, 1997) encountered this problem in speech recognition, and presented a solution to the prob-lem of repetition in -best lists of strings that are deri v e d from fi nite-state automata. That w ork de-scribed a w ay to use a p o werset construction along trees. with an inno v ati v e bookk eeping system to deter -minize the automaton, resulting in an automaton that preserv es the language b u t pro vides a single, prop-erly weighted deri v ation for each string in it. Put an-other w a y , if the input automaton has the ability to generate the same string with dif ferent weights, the output automaton generates that string with weight equal to the sum of all of the generations of that string in the input automaton. In (Mohri and Rile y , 2002) this technique w a s combined with a procedure for ef fi ciently obtaining -best rank ed lists, yielding a list of string results with no repetition.
In this paper we e xtend that w ork to deal with grammars that produce trees. Re gular tr ee gr am-mar s (Brainerd, 1969), which subsume the tr ee sub-stitution gr ammar s de v eloped in the NLP commu-nity (Schabes, 1990), are of particular interest to those wishing to w ork with additional le v els of structure that string grammars cannot pro vide. The application to parsing is natural, and in machine translation tree grammars can be used to model syntactic transfer , control of function w ords, re-ordering, and tar get-language well-formedness. In the w orld of automata these grammars ha v e as a nat-ural dual the fi nite tr ee r eco gnizer (Doner , 1970). Lik e tree grammars and pack ed forests, the y are compact w ays of representing v ery lar ge sets of trees. W e will present an algorithm for determiniz-ing weighted fi nite tree recognizers, and use a v ari-ant of the procedure found in (Huang and Chiang, 2005) to obtain -best lists of trees that are weighted correctly and contain no repetition.

Section 2 describes related w ork. In Section 3, we introduce the formalisms of tree automata, speci fi -cally the tree-to-weight transducer . I n Section 4, we present the algorithm. Finally , i n Section 5 w e sho w the results of applying weighted determinization to recognizers obtained from the pack ed forest output of tw o natural language tasks. The formalisms of tree automata are summarized well in (Gecse g and Steinby , 1984). Bottom-up tree recognizers are due to (Thatcher and Wright, 1968), (Doner , 1970), and (Magidor and Moran, 1969). T op-do wn tree recognizers are due to (Rabin, 1969) and (Magidor and Moran, 1969). (Comon, et. al., 1997) sho w the determinization of unweighted fi nite-state tree automata, and pro v e its correctness. (Borchardt and V ogler , 2003) present determiniza-tion of weighted fi nite-state tree automata with a dif-ferent method than the one we present here. While our method is applicable to fi nite tree sets, the pre vi-ous method claims the ability to determinize some classes of in fi nite tree sets. Ho we v e r , for the fi -nite case the pre vious method produces an automa-ton with size on the order of the number of deri v a -tions, so the technique is limited when applied to real w orld data. As described in (Gecse g and Steinby , 1984), tree au-tomata may be brok en into tw o classes, recognizers and transducers. Recognizers read tree input and de-cide whether the input is in the language represented by the recognizer . F ormally , a bottom-up tree recog-nizer is de fi ned by : 1 Figure 2: V isualization of a bottom-up tree recog-nizer
Consider the follo wing tree recognizer:
As with string automata, it is helpful to ha v e a vi-sualization to understand what the recognizer is rec-ognizing. Figure 2 pro vides a visualization of the recognizer abo v e . Notice that some members of are dra wn as arcs with multiple (and ordered) tails. This is the k e y dif ference in visualization between string and tree automata  X  t o capture the arity of the symbol being read we must visualize the automata as an ordered hyper graph.

The function of the members of in the hyper -graph visualization leads us to refer to the v ector of states as an input vector of states, and the single state as an output state . W e will refer to as the tr ansition set of the recognizer .

In string automata, a path through a recognizer consists of a sequence of edges that can be follo wed from a start to an end state. The concatenation of la-bels of the edges of a path, typically in a left-to-right order , forms a string in the recognizer X  s language. In tree automata, ho we v e r , a hyperpath through a recognizer consists of a sequence of hyperedges that can be follo wed, sometimes in parallel, from a start
Figure 3: Bottom-up tree-to-weight transducer to an end state. W e arrange the labels of the hy-peredges to form a tree in the recognizer X  s language b u t must no w consider proper order in tw o dimen-sions. The proper v ertical order is speci fi ed by the order of application of transitions, i.e., the labels of transitions follo wed earlier are placed lo wer in the tree than the labels of transitions follo wed later . The proper horizontal order within one le v e l o f the tree is speci fi ed by the order of states in a transition X  s input v ector . I n the e xample recognizer , the trees and are v alid. Notice that may be recognized in tw o dif ferent hyperpaths.

Lik e tree recognizers, tree transducers read tree input and decide whether the input is in the lan-guage, b u t the y simultaneously produce some out-put as well. Since we wish to associate a weight with e v ery acceptable tree in a language, we will consider transducers that produce weights as their output. Note that in transitioning from recognizers to transducers we are follo wing the con v ention es-tablished in (Mohri, 1997) where a transducer with weight outputs is used to represent a weighted rec-ognizer . One may consider the determinization of tree-to-weight transducers as equi v alent to the de-terminization of weighted tree recognizers.

F ormally , a bottom-up tree-to-weight transducer is de fi ned by where , , , and are de fi ned as for recognizers, and:
W e must also specify a con v ention for propagat-ing the weight calculated in e v ery transition. This can be e xplicitly de fi ned for each transition b u t w e will simplify matters by de fi ning the propagation of the weight to a destination state as the multiplication of the weight at each source state with the weight of the production.

W e modify the pre vious e xample by adding weights as follo ws: As an e xample, consider the fol-lo wing tree-to-weight transducer ( , , , and are as before):
Figure 3 sho ws the addition of weights onto the automata, forming the abo v e transducer . Notice the tree yields the weight 0.036 ( the hyperpath follo wed.

This transducer is an e xample of a nonsubsequen-tial transducer . A tree transducer is subsequential if for each v ector v of states and each there is at most one transition in with input v ector v and label . These restrictions ensure a subsequential transducer yields a single output for each possible input, that is, it is deterministic in its output.
Because we will reason about the destination state of a transducer transition and the weight of a trans-ducer transition separately , w e mak e the follo wing de fi nition. F o r a gi v e n v where v is a v ector of states, , , and lent shorthand forms are and . The determinization algorithm is presented as Algo-rithm 1. It tak es as input a bottom-up tree-to-weight transducer and returns as output a subsequential bottom-up tree-to-weight transducer such that the tree language recognized by is equi v alent to that of and the output weight gi v e n input tree on is equal to the sum of all possible output weights gi v e n on . Lik e the algorithm of (Mohri, 1997), this Figure 4: a) Portion of a transducer before deter -minization; b) The same portion after determiniza-tion algorithm will terminate for automata that recognize fi nite tree languages. It may terminate on some au-tomata that recognize in fi nite tree languages, b u t w e do not consider an y o f these cases in this w ork.
Determinizing a tree-to-weight transducer can be thought of as a t w o-stage process. First, the structure of the automata must be determined such that a sin-gle hyperpath e xists for each recognized input tree. This is achie v e d b y a classic po werset construction, i.e., a state must be constructed in the output trans-ducer that represents all the possible reachable desti-nation states gi v e n a n input and a label. Because we are w orking with tree automata, our input is a v ector of states, not a single state. A comparable po wer -set construction on unweighted tree automata and a proof of correctness can be found in (Comon, et. al., 1997).

The second consideration to weighted deter -minization is proper propagation of weights. F o r this we will use (Mohri, 1997) X  s concept of the r esidual weight . W e represent in the construction of states in the output transducer not only a subset of states of the input transducer , b ut also a number associated with each of these states, called the residual. Since we w ant  X  s hyperpath of a particular input tree to ha v e as its associated weight the sum of the weights of the all of  X  s hyperpaths of the input tree, we re-place a set of hyperedges in that ha v e the same input state v ector and label with a single hyperedge in bearing the label and the sum of  X  s hyper -edge weights. The destination state of the hyper -edge represents the states reachable by  X  s applica-ble hyperedges and for each state, the proportion of the weight from the rele v ant transition.

Figure 4 sho ws the determinization of a portion of the e xample transducer . Note that the hyperedge Figure 5: Determinized bottom-up tree-to-weight transducer leading to state in the input transducer contrib utes of the weight on the output transducer hyperedge and the hyperedge leading to state in the input transducer contrib utes the remaining . This is re-fl ected in the state construction in the output trans-ducer . The complete determinization of the e xample transducer is sho wn in Figure 5.

T o encapsulate the representation of states from the input transducer and associated residual weights, we de fi ne a state in the output transducer as a set of the algorithm b uilds ne w states progressi v ely , w e will need to represent a v ector of states from the output transducer , typically depicted as v . W e may construct the v ector pair q w from v , where q is a v ector of states of the input transducer and w is a v ector of residual weights, by choosing a (state, weight) pair from each output state in v . F or e x -ample, let . Then tw o possible out-put transducer states could be and v alid v ector pair q w is q , w .

The sets v , v , and v are de fi ned as follo ws: . structed from v where each q is an input v ector in a transition with label . v is the set of unique transitions paired with the appropriate pair for each q w in v . v is the set of states reachable from the transitions in v .

The consideration of vector s of states on the in-cident edge of transitions ef fects tw o noticeable changes on the algorithm as it is presented in (Mohri, 1997). The fi rst, relati v ely tri vial, change is the inclusion of the residual of multiple states in the calculation of weights and residuals on lines 16 and 17. The second change is the production of v ectors for consideration. Whereas the string-based algorithm considered ne wly-created states in turn, we must consider ne wly-a v ailable v ectors. F o r each ne wly created state, ne wly a v ailable v ectors can be formed by using that state with the other states of the output transducer . This operation is performed on lines 7 and 22 of the algorithm. W e no w turn to some empirical studies. W e e xamine the practical impact of the presented w ork by sho w-ing:
W e also compare our results to a commonly used technique for estimation of -best lists, i.e., sum-ming o v e r the top deri v ations to get weight estimates of the top unique elements. 5.1 Machine translation W e obtain pack ed-forest English outputs from 116 short Chinese sentences computed by a string-to-tree machine translation system based on (Galle y , et. al., 2004). The system is trained on all Chinese-English parallel data a v ailable from the Linguistic Data Consortium. The decoder for this system is a CKY algorithm that ne gotiates the space described in (DeNeefe, et. al., 2005). No language model w a s used in this e xperiment.

The forests contain a median of En-glish parse trees each. W e remo v e c ycles from each Algorithm 1 : W eighted Determinization of T ree Automata forest, 3 apply our determinization algorithm, and e x -tract the -best trees using a v ariant of (Huang and Chiang, 2005). The ef fects of weighted determiniza-tion on an -best list are ob vious to casual inspec-tion. Figure 7 sho ws the impro v ement in quality of the top 10 trees from our e xample translation after the application of the determinization algorithm.
The impro v ement observ ed circumstantially holds up to quantitati v e analysis as well. The forests obtained by the determinized grammars ha v e between 1.39% and 50% of the number of trees of their undeterminized counterparts. On a v erage, the determinized forests contain 13.7% of the original number of trees. Since a determinized forest con-tains no repeated trees b u t contains e xactly the same unique trees as its undeterminized counterpart, this indicates that an a v erage of 86.3% of the trees in an undeterminized MT output forest are duplicates.
W eighted determinization also causes a surpris-ingly lar ge amount of -best reordering. In 77.6% of the translations, the tree re garded as  X  X est X  is dif ferent after determinization. This means that in a lar ge majority of cases, the tree with the high-est weight is not recognized as such in the undeter -minized list because its weight is di vided among its multiple deri v ations. Determinization allo ws these instances and their associated weights to combine and puts the highest weighted tree, not the highest weighted deri v ation, at the top of the list. Figure 6: Bleu results from string-to-tree machine translation of 116 short Chinese sentences with no language model. The use of best deri v ation (unde-terminized), estimate of best tree (top-500), and true best tree (determinized) for selection of translation is sho wn.

W e can compare our method with the more com-monly used methods of  X  X runching X  -best lists, where . The duplicate sentences in the trees are combined, hopefully resulting in at least unique members with an estimation of the true tree weight for each unique tree. Our results indi-cate this is a rather crude estimation. When the top 500 deri v ations of the translations of our test cor -pus are summed, only 50.6% of them yield an esti-mated highest-weighted tree that is the same as the true highest-weighted tree.

As a measure of the ef fect weighted determiniza-tion and its consequential re-ordering has on an ac-tual end-to-end e v aluation, we obtain Bleu scores for our 1-best translations from determinization, and compare them with the 1-best translations from the undeterminized forest and the 1-best translations from the top-500  X  X runching X  method. The results are tab ulated in Figure 6. Note that in 26.7% of cases determinization did not terminate in a reason-able amount of time. F o r these sentences we used the best parse from top-500 estimation instead. It is not surprising that determinization may occasionally tak e a long time; e v en for a language of monadic trees (i.e. strings) the determinization algorithm is NP-complete, as implied by (Casacuberta and de la Higuera, 2000) and, e.g. (Dijkstra, 1959). 5.2 Data-Oriented P arsing W eighted determinization of tree automata is also useful for parsing. Data-Oriented P arsing (DOP) X  s methodology is to calculate weighted deri v ations, b u t a s noted in (Bod, 2003), it is the highest ranking parse, not deri v ation, that is desired. Since (Sima X  X n, 1996) sho wed that fi nding the highest ranking parse is an NP-complete problem, it has been common to estimate the highest ranking parse by the pre viously Figure 8: Recall, precision, and F-measure results on DOP-style parsing of section 23 of the Penn T ree-bank. The use of best deri v ation (undeterminized), estimate of best tree (top-500), and true best tree (de-terminized) for selection of parse output is sho wn. described  X  X runching X  method.

W e create a DOP-lik e parsing model 4 by e xtract-ing and weighting a subset of subtrees from sec-tions 2-21 of the Penn T reebank and use a DOP-style parser to generate pack ed forest representa-tions of parses of the 2416 sentences of section 23. The forests contain a median of parse trees. W e then remo v e c ycles and apply weighted determinization to the forests. The number of trees in each determinized parse forest is reduced by a f actor of between 2.1 and . O n a v e r -age, the number of trees is reduced by a f actor of 900,000, demonstrating a much lar ger number of du-plicate parses prior to determinization than in the machine translation e xperiment. The top-scoring parse after determinization is dif ferent from the top-scoring parse before determinization for 49.1% of the forests, and when the determinization method is  X  X pproximated X  by crunching the top-500 parses from the undeterminized list only 55.9% of the top-scoring parses are the same, indicating the crunch-ing method is not a v ery good approximation of determinization. W e use the standard F-measure combination of recall and precision to score the top-scoring parse in each method against reference parses. The results are tab ulated in Figure 8. Note that in 16.9% of cases determinization did not ter -minate. F o r those sentences we used the best parse from top-500 estimation instead. W e ha v e sho wn that weighted determinization is useful for reco v ering -best unique trees from a weighted forest. As summarized in Figure 9, the Figure 9: Median trees per sentence forest in ma-chine translation and parsing e xperiments before and after determinization is applied to the forests, re-mo ving duplicate trees. number of repeated trees prior to determinization w a s typically v ery lar ge, and thus determinization is critical to reco v ering true tree weight. W e ha v e im-pro v e d e v aluation scores by incorporating the pre-sented algorithm into our MT w ork and we belie v e that other NLP researchers w orking with trees can similarly bene fi t from this algorithm.

Further adv ances in determinization will pro vide additional bene fi t t o the community . The transla-tion system detailed here is a string-to-tree system, and the determinization algorithm returns the -best unique trees from a pack ed forest. Users of MT sys-tems are generally interested in the string yield of those trees, and not the trees per se. Thus, an algo-rithm that can return the -best unique strings from a pack ed forest w ould be a useful e xtension.
W e plan for our weighted determinization algo-rithm to be one component in a generally a v ailable tree automata package for intersection, composition, training, recognition, and generation of weighted and unweighted tree automata for research tasks such as the ones described abo v e .
 W e thank Liang Huang for fruitful discussions which aided in this w ork and Da vid Chiang, Daniel Marcu, and Ste v e DeNeefe for reading an early draft and pro viding useful comments. This w ork w a s sup-ported by NSF grant IIS-0428020.

