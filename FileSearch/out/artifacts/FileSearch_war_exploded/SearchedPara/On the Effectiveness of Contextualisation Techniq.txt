 In passage and XML retrieval, contextualisation techniques seek to improve the rank of a relevant element by consider-ing information from its surrounding elements and its con-tainer document. Recent research has demonstrated that some of these techniques are also particularly effective in spoken content retrieval tasks (SCR). However, no previous research has directly compared contextualisation techniques in an SCR setting, nor has it studied their potential to pro-vide robustness to speech recognition errors. In this paper, we evaluate different contextualisation techniques, including a recently proposed technique based on positional language models (PLM) on the task of retrieving relevant spoken pas-sages in response to a spoken query. We study the benefits of these techniques when queries and documents are tran-scribed with increasingly higher error rates. Experimental results over the Japanese NTCIR SpokenQuery&amp;Doc collec-tion show that combining global and local context is bene-ficial for SCR and that models usually benefit from using larger amounts of context in highly noisy conditions.
Recent advances in mobile and network technologies have led to an exponential increase in the amount of spoken ma-terial that is produced and stored. Effective spoken content retrieval [9] (SCR) techniques are needed to enable infor-mation access from these type of collections. SCR has been traditionally framed as an ad-hoc retrieval task: given some information need represented by a text or spoken query, one must produce a ranked-list of spoken documents or passages in order of relevance to the query to be presented to the user for further assessment. A standard approach for SCR uses an automatic speech recognition (ASR) system to obtain a 1-best recognition transcription of the collection and stan-dard text-based IR techniques to index the transcripts and to perform document or passage retrieval. Passage retrieval is normally preferred over full-document retrieval in cases where documents are long and multi-topical and in applica-tions that seek to minimise audio playback time [1]. This typically requires a mechanism to identify topically coherent units of information in the speech recordings that could be treated as retrieval elements and returned in response to a query. When the spoken content is known to be delivered in a structured fashion and when structural cues are avail-able to the SCR system, retrieval elements can be defined accordingly. Otherwise, suitable elements may be obtained by using a topic segmentation algorithm.

Although the quality of ASR systems has improved sig-nificantly over the past few years, ASR errors still pose a challenge to traditional text retrieval techniques. This oc-curs in domains where speech is informal, conversational, or spontaneous [9] or when the elements to be retrieved are short in length or lack sufficient contextual information and verbosity to be retrieved effectively [2]. Context and ver-bosity are desirable properties of an element because they can increase its chances of matching query terms, even when many of its terms are misrecognised by ASR. In general, the more repetitions of important terms used to convey the topic and the more exhaustively this topic is covered by the terms in the element to be retrieved, the more robust will be its matching process against ASR errors.

The context of an element, that is, the information that is present in its document, has been shown to be valuable for improving element-retrieval effectiveness [3]. The process of taking context into account when computing the relevance score of an element is known as contextualisation [3]. Vari-ous contextualisation techniques have been proven effective in XML retrieval [3], passage retrieval [6, 8], and SCR [12, 15] tasks. In these techniques, elements are scored depend-ing not only on the query terms occurring within the element itself but also on those occurring in other positions within the document. Two simple and widely adopted approaches are interpolating the scores of an element with those of its document to consider global context [12], or with those from a fixed number of surrounding elements to consider local context [15]. In contrast, techniques based on positional models (PM) [6, 8] allow consideration of longer-spans of context ignoring element boundaries.

In this paper, we study the impact of incorporating con-text into the task of retrieving short spoken passages given a spoken query from a collection of long spoken documents. Our hypothesis is that context becomes increasingly valuable for improving retrieval effectiveness as ASR errors increase in the transcripts of the spoken queries and documents. We Table 1: List of manual and ASR transcript types.
 validate this empirically by comparing the performance of various contextualisation techniques in a SCR task.
The remainder of the paper is organised as follows. Sec-tion 2 describes the spoken test collection. Section 3 presents our baseline retrieval approach which we extend with the contextualisation techniques presented in Section 4. Sec-tion 5 describes experiments and results obtained. Finally, Section 6 concludes and discusses future work.
The SDPWS dataset has been used at recent editions of the NTCIR SpokenQuery&amp;Doc (SQD) tasks [1]. This cor-pus contains speech recordings of 98 oral presentations in Japanese totalling approximately 27 hours. The task or-ganisers provided automatic transcripts of the recordings of varying quality. These were generated using an ASR system with different combinations of acoustic and language mod-els. Furthermore, human transcripts are provided which are manually annotated with the times when slide transitions were made by the presenters, as well as groups of consecu-tive slides used to present a single topic or idea. These slide groups define a list of topical segments within a presenta-tion, used in the task as pre-defined passages to be retrieved in response to a query. In our experiments, we similarly use these segments as retrieval passages. Table 1 lists the differ-ent transcript types. The first column shows the ID of each type as mentioned in the task overview [1], while the second shows short IDs used throughout this paper.
 Since no characters are placed between words in written Japanese, we process text transcripts with the morphologi-cal analyser MeCab 1 to obtain tokens that can be used as indexing features. For this purpose, we configure MeCab to output the base form of the identified words along with their part-of-speech tags. As lemmas of nouns and verbs are effective indexing features for Japanese SCR [12], we remove all tokens not tagged as verbs or nouns. Further, we remove function words that are misclassified as nouns or verbs with a stop word list containing 44 of the most frequent preposi-tions and determiners. After processing the text, we use the Terrier platform v4.0 2 to generate an index with positional information for each transcription type. Table 2 shows term statistics of post-processed documents and passages. In to-tal, the collection comprises 98 documents which are split into 2330 passages. For each transcript, we report word er-ror rates (WER) averaged across utterances and term error rates (TER) [7] averaged across passages.

The spoken queries used in the SQD-1 and SQD-2 tasks are available for the SDPWS collection. These sets contain 35 and 80 queries respectively. Also, organisers generated human and ASR transcripts for the queries by using the same ASR models used to transcribe the presentations. Ta-Table 2: Collection statistics of documents and passages. Table 3: Error rates and statistics of query transcripts. ble 3 summarises term statistics and ASR error rates for the spoken queries. Relevance judgements for these query sets were created by the task organisers from a pool of results submitted for the SQD tasks. Three relevance levels were annotated: full, partial, and no relevance. In our experi-ments, we treat partially relevant passages as fully relevant.
In our simplest retrieval set-up, we treat passages as re-trieval elements and use standard document retrieval to rank them in order of their relevance to the query. Our ranking function is based on the Okapi BM25 function of proba-bilistic retrieval [14]. Besides the well known k 1 , b , and k parameters, we include a fourth parameter, namely d  X  1, as the exponent of the IDF weight [16]. This parameter can be adjusted to increase the relative difference between weights assigned to frequent and rare terms. In our experi-ments, setting d &gt; 1 results in improved effectiveness as this may provide better estimates of IDF weights for small collec-tions. Similar effects can be obtained with the modification proposed in [11], although this results in lower retrieval ef-fectiveness in our task. The resulting function calculates the weight of a term t occurring in element e and query Q as: w e ( t ) = ( k 1 + 1) tf ( t ) where tf ( t ) and qf ( t ) denote the number of occurrences of t in e and Q respectively, docl is the length of e , avel is the average length of all the elements of the same type in the collection, n t is the number of elements containing term t , N is the number of elements in the collection, w log( N  X  n t + 0 . 5)  X  log( n t + 0 . 5) and b , k 1 , k tuning parameters. Given this term scoring function, we rank elements according to their relevance score with respect
In this section, we present the contextualisation techniques that are investigated in our study.
A simple and popular contextualisation approach consists of combining a passage relevance score with the score of its document. Firstly, passages and documents are scored inde-pendently to form two separate ranked-lists of results. Sec-ondly, initially retrieved passages are re-ranked according to the combination of their document scores. For score com-bination, we adopt a simple weighted linear sum of scores or CombSUM [5]. The relevance score of passage p within document D is given by:
S DSI ( Q,p ) =  X S BM 25 ( Q,D ) + (1  X   X  ) S BM 25 ( Q,p ) (2) where the interpolation parameter  X  adjusts the influence of the document score over the combined score.
Positional models (PM) seek to improve IR effectiveness by exploiting information about the positions where query terms occur in a document. A representative example of these models are positional language models (PLM) [10] which were introduced in IR as a mechanism to integrate evidence from term proximity features and passages into the language modelling framework. A PLM estimates the prob-ability P ( t | i,D ) that term t is generated at position i in doc-ument D . In this estimation, the so-called pseudo-frequency of the term is used which is calculated by means of a ker-nel decay function that propagates occurrences of the term to distant positions in the document. This probability can possibly be influenced by all occurrences of t in D depend-ing on their distance to i . The kernel function is commonly parametrised by a propagation parameter  X  which adjusts the influence that a term occurrence has over distant posi-tions. Among these, the Gaussian kernel exp(  X  ( j  X  i ) has been shown effective in previous studies [10, 6, 8].
In recent work, PMs were proposed as a contextualisation technique for passage retrieval [6]. In this work, a standard TFIDF approach was used to compute the relevance score of a passage p within document D where the frequency of term t in p is given by its pseudo-frequency estimate: where pos ( t,D ) is the set of positions where t occurs in D , p ,...,p n are the spanning positions of p in D , and kernel is the symmetric Gaussian kernel. To avoid longer passages from receiving unmerited pseudo-frequency counts, the sum-mation over the kernel function is applied across a fixed number of positions independent of p  X  X  length.

In our experiments with PMs, we incorporate the pseudo-frequencies into the TF factor within the Okapi BM25 model. This is performed by replacing the raw term frequency tf ( t ) by tf P M ( t ) in Equation 1 to obtain a modified weight w The resulting passage scoring function is then defined as: To reduce the execution time of our scoring algorithm, we evaluate the kernel only at the position within the passage that gives its maximum value. That is, we evaluate the kernel at j = p 1 or j = p n if i &lt; p 1 or i &gt; p n respectively, or at j = i otherwise. Finally, we also experiment with a model that combines passage scores obtained with S P M and document scores obtained with S BM 25 , this is:
S DSI -P M ( Q,p ) =  X S BM 25 ( Q,D ) + (1  X   X  ) S P M ( Q,p ) (5)
To study the value of context for improving passage rank-ing in noisy conditions, we evaluate the effectiveness of the models presented in Sections 3 and 4 on various combina-tions of query and document transcripts. In all experiments, retrieval effectiveness is measured in terms of mean average precision (MAP) at depth 1000. Each combination of tran-scripts imposes a different evaluation condition with varying level of noise, each of which may require adjusting model pa-rameters differently in order to achieve optimal performance. Further, in order to validate our hypothesis, we would like to find values for the contextualisation parameters  X  and  X  that provide the best performance in each case. Conse-quently, we optimise parameters for each model by seeking to maximise retrieval effectiveness in each evaluation condi-tion. The optimisation method we use is a greedy iterative approach called  X  X romising Directions X  [13] which performs multiple line searches over decreasing trusted regions in the parameter search space. Since MAP is not a smooth nor a convex function [13] this method is not guaranteed to find a global maximum. Nevertheless, it can still find configu-rations that perform significantly better in our task than if using default BM25 parameters 3 .

To obtain a fair estimate of the relative performance of the models, we optimise parameters on the SQD-1 queries and evaluate on the SQD-2 queries. Table 4 reports MAP scores obtained by the BM25, DSI, PM, and DSI-PM models on the SQD-2 queries for different combinations of transcript types. The percentages next to the MAP scores show rela-tive differences with respect to BM25. In each row, bold val-ues and markers *,  X  , and indicate statistically significant differences with respect to BM25, DIS, PM, and DIS-PM re-spectively based on a MaxT permutation test ( B = 100 , 000,  X  = 0 . 05) that corrects for multiple hypothesis testing [4]. Also, we report query-averaged TERs for each transcript combination calculated by restricting terms to only those occurring in the queries. Overall, results indicate that using global (DSI) and local (PM) context either in isolation or in combination (DSI-PM) provide gains in retrieval effective-ness across all evaluation conditions. Furthermore, the rela-tive gains of using context in highly noisy conditions (TER &gt; 55%) are greater on average than in less noisy conditions.
Finally, we study the effects of varying the contextuali-sation parameter  X  in the PM scoring function. Figure 1 shows how MAP scores vary as we increase the values of  X  in six representative evaluation conditions. Data points were generated considering optimal parameter values for the SQD-2 queries. For perfect or quasi-perfect transcripts (M-M and A0-A0), the model achieves maximum performance at  X  = 76 and  X  = 111 respectively, while for moderately noisy transcripts (A1-A1 and M-A3) it does so at  X  = 296 and  X  = 341 respectively. Finally, in extremely noisy condi-tions (A2-A2 and A2-A3), the maximum points are located at  X  = 530 and  X  = 682 respectively. These observations provide supporting evidence for our claim that longer spans of context become increasingly useful for retrieval as ASR errors increase in the transcripts.
This paper has investigated the benefits of using context for improving the ranking of spoken passages given a spoken query. Three contextualisation techniques have been evalu-ated and compared against a well-tuned state-of-the-art re-trieval model: a document score interpolation, a positional model, and their combination. Results of retrieval experi-ments with transcripts of varying quality validate previous findings that highlight the importance of using context in element-retrieval and SCR tasks and indicate that a com-bination of local and global context performs best for SCR. Further analysis reveals that considering greater extents of local context can improve IR effectiveness as ASR errors in-crease in the transcripts. In future work, we will evaluate models on an unsegmented spoken collection where we could fully exploit the  X  X oft X  characterisation of passage bound-aries provided by PMs. Since recognition quality may vary greatly across utterances, we also plan to develop new tech-niques that could re-adjust context-incidence parameters ac-cording to ASR confidence estimates.
This work is supported by Science Foundation Ireland through the CNGL Programme (Grant No: 12/CE/I2267) in the ADAPT Centre at Dublin City University. [1] T. Akiba, H. Nishizaki, H. Nanjo, and G. J. F. Jones. [2] J. Allan. Perspectives on information retrieval and [3] P. Arvola, J. Kek  X  al  X  ainen, and M. Junkkari. [4] L. Boytsov, A. Belova, and P. Westfall. Deciding on an
Figure 1: MAP scores versus  X  for the PM model. [5] J. P. Callan. Passage-level evidence in document [6] D. Carmel, A. Shtok, and O. Kurland. Position-based [7] S. E. Johnson, P. Jourlin, K. S. Jones, and P. C. [8] M. Keikha, J. H. Park, W. B. Croft, and [9] M. Larson and G. J. F. Jones. Spoken content [10] Y. Lv and C. Zhai. Positional language models for [11] M. Murata, H. Nagano, R. Mukai, K. Kashino, and [12] H. Nanjo, T. Yoshimi, S. Maeda, and T. Nishio. [13] S. Robertson and H. Zaragoza. The probabilistic [14] S. E. Robertson, S. Walker, S. Jones, M. M.
 [15] S.-R. Shiang, P.-W. Chou, and L.-C. Yu. Spoken term [16] C. Zhai. Notes on the Lemur TFIDF model. Technical
