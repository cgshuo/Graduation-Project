 With applications in biology, the world-wide web, and sev-eral other areas, mining of graph-structured objects has re-ceived significant interest recently. One of the major re-search directions in this field is concerned with predictive data mining in graph databases where each instance is repre-sented by a graph. Some of the proposed approaches for this task rely on the excellent classification performance of sup-port vector machines. To control the computational cost of these approaches, the underlying kernel functions are based on frequent patterns. In contrast to these approaches, we propose a kernel function based on a natural set of cyclic and tree patterns independent of their frequency, and discuss its computational aspects. To practically demonstrate the ef-fectiveness of our approach, we use the popular NCI-HIV molecule dataset. Our experimental results show that cyclic pattern kernels can be computed quickly and offer predic-tive performance superior to recent graph kernels based on frequent patterns.
 H.2.8 [ Database Management ]: Database Applications -Data Mining; I.2.6 [ Artificial Intelligence ]: Learning; I.5.2 [ Pattern Recognition ]: Design Methodology -clas-sifier design and evaluation Algorithms, Experimentation graph mining, kernel methods, computational chemistry  X 
This work was supported in part by the DFG project (WR 40/2-1) Hybride Methoden und Systemarchitekturen f  X  ur het-erogene Informationsr  X  aume .

In recent years, data mining has moved far beyond the original commercial applications into areas such as bioinfor-matics or web mining. While in most of the web mining applications instances are vertices of the single massive web graph [21]; in other application domains each instance can bea graph . This is perhapsmost obvious in applications that deal with molecules, since each molecule consists of atoms (the vertices of the graph) that are connected by bonds (the edges of the graph). In such chemical domains there is usu-ally also a label assigned to each vertex and edge, modelling for example atom and bond types.

Such graph structured instances have no natural represen-tation as a single row of a single fixed-width table. There-fore, there has recently been an increased interest in meth-ods that can accept graph-structured instances as input. In predictive graph mining dealing with graph instances, the learning algorithm is not only given a set of disjoint graphs of arbitrary size but also for each graph the value of some property is known. The task of the algorithm is then to pro-duce a model which approximates well the unknown depen-dence between graphs and the value of this target property.
In this paper, we concentrate on predictive graph min-ing dealing with graph instances. In particular, we focus on a class of supervised learning algorithms, namely support vector machines [31] and other kernel methods [28]. Ker-nel methods have proven superior to other approaches in a large number of application areas and for several types of data including tabular and text data. For general graphs, however, it has proven challenging to design kernel functions that are both powerful enough to handle arbitrarily struc-tured graphs while being efficient enough to be applied to large graph databases.

Many researchers have consequently resorted to represent each graph by its frequent subgraphs (see, e.g., [4, 9, 20]) and then to apply a kernel to the pattern sets correspond-ing to these frequent subgraphs. The first step of such ap-proaches usually involves a levelwise algorithm similar to APriori [1] for association rules, that is able to find all sub-graphs whose frequency in the graph database is beyond a user defined threshold. The efficiency of such an approach depends on the number of frequent patterns and thus on the frequency threshold that is chosen. A too low threshold hinders feasible computation of the frequent subgraphs; a too high threshold always risks losing interesting patterns that would have been necessary for optimal classification performance.
In this paper, we show that with a natural set of pat-terns, cyclic and tree patterns , it is possible to eliminate the restriction to frequent patterns. We map the graphs to these pattern sets, independent of the frequency of the patterns in the graph database. Similar to the frequent subgraph based approaches, we then apply a kernel to these pattern sets. The resulting kernel on graphs is called the cyclic pattern kernel . We draw on results from graph theory to arrive at an algorithm that is in practice capable of quickly identi-fying the set of cyclic and tree patterns even in large sets of example graphs. We present experiments on the popular NCI-HIV dataset containing 42687 molecules, to show that our kernel, compared to previous approaches, offers signifi-cant gains in accuracy, as measured by the area under the ROC curve [26]. We give a theoretical complexity analy-sis of the cyclic pattern kernel, indicating that its efficiency in practice results from a well-behavedness effect similar to the one observed in frequent set discovery for association rule mining.

The paper is organized as follows. In Section 2, we define all necessary notions of graphs and kernel methods. Be-fore proceeding to the development of our own kernel, in Section 3 we first briefly review previous results on graph kernels. In Section 4, we define cyclic pattern kernels and discuss their computational aspects. Section 5 contains the empirical evaluation on the NCI-HIV dataset, and Section 6 concludes with pointers to future work.
In this section we define some necessary notions and no-tations related to graphs and kernel methods.
We first recall some basic definitions from graph theory (see, e.g., Diestel X  X  textbook [10] for more details). For aset S ,[ S ] k denotes the family of k -subsets of S , i.e., [
S ] k = { S  X  S : | S | = k } .A labeled undirected graph is a quadruple G =( V,E,  X  , X  ), where V is a finite set of dered set of labels ,and  X  : V  X  E  X   X  is a function assigning a label to each element of V  X  E . The cardinalities of V E are denoted by n and m , respectively. Unless otherwise stated, in this paper by graphs we always mean labeled undi-rected graphs. A graph database is a set of disjoint graphs. For a graph database G , |G| denotes the number of graphs in
G . Note that graphs can be viewed as relational struc-tures [11] and hence, graph databases can be considered as relational databases.

Walks, Paths, and Cycles A walk is a sequence of edges of a graph. The walk w is a simple path if the v  X  X  are all distinct. If v 0 = v k and v i = v j for every (1  X  i&lt;j  X  k )then w forms a simple cycle .Wedenote by S ( G ) the set of simple cycles of a graph G .Twosimple cycles C and C in G are considered to be the same if and only if C or its reverse is a cyclic permutation of C .We note that the number of simple cycles is exponential in the number n of vertices in worst case 1 .
In fact, the number of simple cycles in a graph can grow faster with n than 2 n , and remains exponential even for Biconnected Components Let G =( V,E,  X  , X  )and G =( V ,E ,  X  , X  ) be graphs. G is a subgraph of G ,if V  X  V , E  X  E ,and  X  ( x )=  X  ( x ) for every x  X  V  X  E . Agraphis connected if there is a (simple) path between any pair of its vertices. A connected component of a graph G is a maximal subgraph of G that is connected. A vertex v of a graph G is an articulation (also called cut ) vertex, if its re-moval disconnects G (i.e., the subgraph obtained from G by removing v and all edges containing v has more connected components than G ). A graph is biconnected if it contains no articulation vertex. A biconnected component (or block )ofa graph is a maximal subgraph that is biconnected. It holds that biconnected components of a graph G are pairwise edge disjoint and form thus a partition on the set of G  X  X  edges. This partition, in turn, corresponds to the following equiv-alence relation on the set of edges: two edges are equivalent if and only if they belong to a common simple cycle. This property of biconnected components implies that an edge of a graph belongs to a simple cycle if and only if its bicon-nected component contains more than one edge. Edges not belonging to simple cycles are called bridges . The subgraph of a graph G formed by the bridges of G is denoted by B ( Clearly, each bridge of a graph is a (singleton) biconnected component, and B ( G )isaforest.

Isomorphism We will also use the notion of isomorphism between graphs. Let G 1 and G 2 be the graphs ( V 1 ,E 1 , if there is a bijection  X  : V 1  X  V 2 such that hold for every u,v  X  V 1 .

Although by the definition of graphs we consider only sim-ple graphs (i.e., graphs without loops and parallel edges), we finally note that the approach presented in this paper can be adapted to non-simple graphs as well.
Kernel methods [28] are a recent development within the machine learning and data mining communities. Being on one hand theoretically well founded in statistical learning theory, they have on the other hand shown good empirical results in many applications. One particular aspect of kernel methods such as the support vector machine is the forma-tion of hypotheses by linear combination of positive definite kernel functions  X  X entred X  at individual training examples. By the restriction to positive definite kernel functions, the underlying optimisation problem becomes convex and every locally optimal solution is globally optimal.

Kernel Functions Kernel methods can be applied to different kinds of (structured) data by using any positive definite kernel function defined on the data. Here then is positive integers):
Let X be a set. A symmetric function k : X X X X  R is a positive definite kernel on X if, for all n  X  Z + , x 1 ,...,x many restricted graph classes in worst case. For instance, in [2], Alt, Fuchs, and Kriegel investigate simple cycles of planar graphs, and show that there are planar graphs with lower bound 2 . 28 n on the number of simple cycles. X ,and c 1 ,...,c n  X  R , it holds that
Mercer X  X  theorem guarantees that for every positive defi-nite kernel function k ,thereisamap  X  into an inner product space, such that for every x,x  X  X  it holds that k ( x,x )=  X  ( x ) , X  ( x ) where  X  ,  X  denotes the inner product in that space. Although this inner product space may have infinite dimension, it is often possible to compute k in polynomial time. For a simple example, consider the map  X   X  that maps every positive integer x to a sequence of zeros and ones such that the x -th element of the sequence  X   X  ( x )isoneandall other elements are equal to zero. Clearly, the inner prod-uct space in which the images reside ( l 2 )doesnothavea finite base. Still, for all x,x  X  Z + ,  X   X  ( x ) , X   X  ( x computed in polynomial time.

Kernel Machines The usual supervised learning model [31] considers a set X of individuals and a set Y of labels, such that the relation between individuals and labels is a fixed but unknown probability measure on the set X X Y . The common theme in many different kernel methods such as support vector machines, Gaussian processes, or regu-larised least squares regression is to find a hypothesis func-tion that minimises not just the empirical risk (training er-ror) but the regularised risk . This gives rise to the optimi-sation problem known label (the training set), C trades off between regular-isations and empirical loss, H is a set of functions forming a Hilbert space (the hypothesis space), and V is a function that takes on small values whenever f ( x i ) is a good guess for y i and large values whenever it is a bad guess (the loss function). The representer theorem shows that under rather general conditions on V , solutions of the above optimisation problem have the form where k is the reproducing kernel of H . Different kernel methods arise from using different loss functions.
Support Vector Machines Support vector machines [5, 28] are a kernel method that can be applied to binary su-pervised classification problems. They are derived from the above optimisation problem by choosing the so-called hinge support vector machines often taken in literature is that the solution can be interpreted as a hyperplane that separates both classes and is maximally distant from the convex hulls of both classes. A different motivation is the computational attractiveness of sparse solutions of the function (1) used for classification.

Intersection Kernels An integral part of many kernels for structured data is the decomposition of an object into a set of its parts and the intersection of two sets of parts. The kernel on two objects is then defined as a measure of the intersection of the two corresponding sets of parts.
The general case of interest for set kernels is when the instances X i are elements of a semiring of sets S and there is a measure  X  with S as its domain of definition. Positive definiteness of the intersection kernel holds under these general conditions.

As, however, the discrete case is the most common case and simpler than the general case, here we restrict our at-tention to this case. Let X be the set of possible  X  X arts X  and let the objects be decomposed into sets of parts X i  X  X  . For every X i the characteristic function  X  i : X X  X  0 , 1 defined by  X  i ( x )=1  X  x  X  X i and  X  i ( x )=0otherwise. For any measure  X  on X and sets X i with  X  ( X i ) &lt;  X  intersection kernel is a positive definite kernel on 2 X as
The above kernel function can easily be extended to the case of multisets, where the characteristic function  X  i : multiset. This extension is of interest in the case that objects are not just decomposed into a set of its parts but into a multiset.

Note that in the discrete case considered here, with the set cardinality as the measure, the intersection kernel coincides with the inner product of the bitvector representations of the sets (or multiplicity vector representations of multisets). In the case that the sets X i are finite or countable sets of vectors it is often beneficial to use set kernels other than the intersection kernel. For example the crossproduct kernel In the case that the right hand side kernel is the matching the crossproduct kernel coincides with the intersection ker-nel.
The above described idea of decomposition and intersec-tion kernels is reflected in most work on kernels for struc-tured data, from the early and influential technical reports [17, 33] through other work on string kernels [23, 24] and tree kernels [6], to more recent work on graph kernels [15, 19].
 We now briefly review previous results on graph kernels. While in thispaper we areconcerned with undirectedgraphs, prior work mostly considered directed graphs. However, we can regard an undirected graph as a directed graph with edges in either direction. Conceptually, the graph kernels presented in [13, 15, 19] are based on a measure of the walks in two graphs that have some or all labels in com-mon. In [13] walks with equal initial and terminal label are counted, in [19] the probability of random walks with equal label sequences is computed, and in [15] walks with equal la-bel sequences, possibly containing gaps, are counted. Note that even very simple graphs contain an infinite number of walks and thus even with a small number of different la-bels, the feature space corresponding to the kernel has in-finite dimension. In [15] computation of these kernels is made possible in polynomial time by using the direct prod-uct graph and computing the limit of matrix power series involving its adjacency matrix. The work on rational graph kernels [7] generalises these graph kernels by using a general transducer between weighted automata instead of the direct graph product. However, only walks up to a given length are considered in the kernel computation.

Describing each vertex in a graph by the set of walks starting at this vertex can be seen as a colouring of the corresponding vertex. Such colourings are also used in iso-morphism tests. There one would like two vertices to be coloured differently iff they do not lie on the same orbit of the automorphism group [12]. As no efficient algorithm for the ideal case is known, one often resorts to colourings such that two differently coloured vertices cannot lie on the same orbit. Using the walks as colours for each vertex satisfies the latter condition.

Indeed, it has been shown [15] that a graph kernel for which the kernels centred at two graphs are equivalent if and only if the two graphs are isomorphic, is at least as hard as deciding graph isomorphism (these kernels are called complete graph kernels ). So far, the above mentioned graph kernels are the only known efficient alternatives to these complete graph kernels.

Now, consider the following kernel on graphs: Let G be the set of all graphs and  X  : G X  2 G be a function mapping each graph G to the set of connected subgraphs of G .Using the intersection kernel (given in Equation (2)) on the images of each graph under  X  with the set cardinality as measure, the subgraph kernel 2 is defined as It has been shown in [15] that this kernel cannot be com-puted in polynomial time. In the next section we will con-sider the complexity of computing a related kernel function.
In literature, different approaches have been tried to over-come this problem. [16] restricts the decomposition to paths up to a given size, and [8] only considers the set of con-nected graphs that occur frequently as subgraphs in the graph database. The approach taken there to compute the decomposition of each graph is an iterative one [22]. The algorithm starts with a frequent set of subgraphs with one or two edges only. Then, in each step, from the set of fre-quent subgraphs of size l , a set of candidate graphs of size l +1 is generated by joining those graphs of size l that have a subgraph of size l  X  1 in common. Of the candidate graphs only those satisfying a frequency threshold are retained for the next step. The iteration stops when the set of frequent subgraphs of size l is empty.
In this section, we first define cyclic pattern kernels (CPK) for graphs and then discuss their computational aspects. Our definition is based on the intersection kernel described in Section 2.2. To apply intersection kernels to graphs, we assign to each graph G the set of cyclic and tree patterns of
G . These patterns, in turn, are induced by the sets of simple cycles and bridges of the graph, respectively.
The intersection now means intersection with respect to isomorphism.
We start by defining the set of cyclic patterns induced by the set of simple cycles of a graph. Let G =( V,E,  X  , X  )be agraphand be a sequence of edges that forms a simple cycle in G .The canonical representation of C is the lexicographically small-est string  X  ( C )  X   X   X  among the strings obtained by concate-nating the labels along the vertices and edges of the cyclic permutations of C and its reverse. More precisely, denoting by  X  ( s ) the set of cyclic permutations of a sequence s its reverse, we define  X  ( C )by where for w = w 0 w 1 ...w k  X  1 ,  X  ( w )=  X  ( w 0 )  X  ( { w 0 ,w 1 } )  X  ( w 1 ) ... X  ( w k  X  1 Clearly,  X  is unique up to isomorphism, and hence, it indeed provides a canonical string representation of simple cycles. The set of cyclic patterns of a graph G , denoted by C ( G then defined by (We recall that S ( G ) denotes the set of simple cycles of
To assign a set of cyclic patterns to a graph G ,abovewe have used its set of simple cycles. To add more informa-tion to the kernel, we also consider the graph obtained by removing the edges of all simple cycles, or equivalently, by deleting every edge that belongs to some of G  X  X  biconnected components containing at least two edges. As discussed in Section 2.1, the resulting graph is a forest consisting of the set of bridges of the graph. To assign a set of tree patterns to G , we use this forest formed by the set B ( G )ofbridgesof G . Similarly to simple cycles, we associate each tree T with a pattern  X  ( T )  X   X   X  that is unique up to isomorphism 3 define the set of tree patterns T ( G ) assigned to G by
T ( G )= {  X  ( T ): T is a connected component of B ( G )
We are now ready to define cyclic pattern kernels for graphs. In the definition below, we assume without loss of generality that C ( G )and T ( G ) are disjoint for every in the database. Our kernel is an intersection kernel (2) on the sets defined by for every G . More precisely, we define cyclic pattern kernels by for every G i ,G j in a graph database G , where the measure  X  used in the intersection kernel is the cardinality.
We first transform the unordered free tree T into a rooted ordered tree T unique up to isomorphism, and define then  X  (
T ) by the string representing T . We omit the technical description and refer to e.g. [3, 34] for further details on canonical string representations of trees. Algorithm 1 Basic Algorithm Require: graph G with n vertices and m edges Ensure:  X  CP ( G ) 1: let S = B =  X  2: compute the biconnected components of G 3: for all biconnected component G of G do 4: if G contains more then one edge then 5: S = S  X  X  ( G ) 6: else 7: add the edge of G to B 8: S = S  X  X   X  ( t ): t is a connected component of B } 9: return S
As discussed in Section 2.2, even infinite dimension of the feature space associated with a kernel still does not imply its computational intractability. Another issue is, whether in general, the intractability of computing the value of a single feature implies the hardness of computing the kernel. We do not know the answer to this general problem which includes also the case of cyclic pattern kernels, as  X  CP may contain patterns corresponding to Hamiltonian cycles as well. Using a similar argument as [15], in Proposition 1 below we show that computing cyclic pattern kernels is intractable.
Proposition 1. The problem of computing cyclic pattern kernels is NP-hard.
 Proof. We shall use a reduction from the NP-complete Hamiltonian cycle problem. Let G and C n be a graph and a simple cycle, respectively, such that both G and C n consist of n vertices and are defined over the same singleton label set. Applying (4) to G and C n , it holds that k CP ( G,C if and only if G has a Hamiltonian cycle.

Although the cardinality of the set  X  CP ( G ) of cyclic and tree patterns of a graph G can be exponential in the number of vertices of G , we still turn to this problem restricting the approach to those well-behaved cases where  X  CP ( G )can be computed in practically reasonable time. In Section 5 we will present a large real-world molecular graph database satisfying this property. Before focusing in later sections on well-behaved domains, below we first discuss some issues of computing  X  CP ( G ) in the general case.
 Thebasic algorithm computing C ( G )and T ( G )foragraph G is sketched in Algorithm 1. From the arguments of Sec-tion 2.1 it follows that the algorithm computes the set of bridges in B , and returns finally  X  CP ( G )in S . Regarding the complexity of Algorithm 1, we first note that Step 2 can be solved in linear time; in [29], Tarjan gives a depth-first search algorithm computing all biconnected components of agraphintime O ( n + m ). Since the number of bridges of G is at most m , and the number of trees is bounded by the cardinality of the set of bridges of G ,inStep8wecom-pute the set T ( G ) of tree patterns in time polynomial in In Step 5, we compute the set of cyclic patterns of a bicon-nected component of G . From the point of view of efficiency, this step of the algorithm is critical, as even the number of cyclic patterns of a biconnected graph can be exponential in the number of vertices. In Example 2 below, we give a graph and show that 2 O ( | V | ) is a lower bound on the number of its cyclic patterns, where V is the set of vertices of the graph. Example 2. Let G =( V,E,  X  , X  ) be a graph such that V = { s,t,v 1 ,...,v 2 n } ,
 X = { 0 , 1 , 2 } , and  X  ( x )= for every x  X  V  X  E . Note that by the above definition,  X  assigns 2 to every edge in E , as well as to the vertices t . Omitting the labels of s , t , and the edges, the figure of is given in Fig. 1. Let where w  X  1 and | w | denote the reverse and the length of respectively. (The strings in  X   X  are compared by the lexi-cographic order induced by the linear order on  X  .) It holds that for every w  X  L , there is a simple cycle C in G such that we can obtain w by deleting all 2  X  X  from the canonical representation  X  ( C ) of C . Hence, for the cardinality of the set C ( G ) of cyclic patterns we have
Since the cardinality of the set C ( G ) of cyclic patterns can also be exponential in the number n of G  X  X  vertices, we next consider the problem of computing C ( G )in polynomial output complexity . That is, we ask whether there exists an algorithm that enumerates C ( G ) in time polynomial in n |C ( G ) | . Proposition 3 below states that even this problem is NP-hard.
 Proposition 3. Let G be a graph with n vertices, and N  X |C ( G ) | be an arbitrary non-negative integer. Then the problem of enumerating N elements from C ( G ) with param-eters n and N is NP-hard.

Proof. We show that the NP-complete Hamiltonian cy-cle problem is polynomial-time reducible to the above enu-meration problem. Let G be an ordinary undirected graph (i.e., a graph without labels) with n vertices. We assign a (labeled undirected) graph G to G such that G has the same sets of vertices and edges as G , and each vertex and edge of G is labeled by the same symbol, say 0. Since sim-ple cycles of the same length in G are mapped to the same pattern (i.e., simple cycles of length &amp; are associated with the pattern 0 2 ), |C ( G ) | &lt;n . Applying the enumeration algorithm with N = n  X  1, we obtain a set S containing at most n  X  1elementsof C ( G ). Clearly, 0 2 n is in S if and only if G has a Hamiltonian cycle.
 In order to overcome the negative complexity result implied by Proposition 3 above, we consider a restriction that yields an effective practical problem class.
In contrast to the case of cyclic patterns, the set S ( G of simple cycles of a graph G can be listed in polynomial output complexity. A depth-first search algorithm com-puting N  X |S ( G ) | simple cycles of a graph G in time O (( N +1) n + m ) is given by Read and Tarjan in [27]. From their result it also follows that, for a given graph G and k  X  0, one can decide efficiently, whether or not the number of simple cycles in G is bounded by k .Usingtheseposi-tive results, in this section we consider the case when the number of simple cycles is bounded by a constant for (al-most) every graph in the database. In certain real-world graph databases (e.g., drug molecules), the assumption of such a bound seems reasonable. As an example, in the NCI dataset 4 there is a very natural and clear such bound; this molecular graph database consists of 250251 graphs contain-ing altogether 2161831 simple cycles 5 .Notethateffectively, we are assuming a certain kind of well-behavedness of our graph-structured input data which results in only a very small number of objects with extremely large numbers of simple cycles. We note that a similar assumption is made for example in association rule mining where one assumes that transaction data are well-behaved so as to not induce many frequent sets of overly large sizes.
 Algorithm 2, a variant of Algorithm 1, computes the set  X 
CP of patterns (3) only for those graphs that have at most k simple cycles. In Step 6 of the algorithm, we call Read and Tarjan X  X  algorithm [27] (subroutine RT) with parameters be-ing the current biconnected component G and k  X  K +1, where K is a variable counting the simple cycles that have been found so far. If G contains more than k  X  K simple cy-cles, the algorithm halts and returns the empty set (Step 7), as in this case G has more than k simple cycles. The effi-ciency of the algorithm follows from that of the subroutine RT.

Finding an Appropriate Bound We now turn to the problem of deciding whether there exists an upper bound on the number of simple cycles such that all but a small subset of the graphs in the database can be processed within a user defined time limit. Depending on the value of a threshold given by the user, a small subset of the database requiring potentially too much computation time can be disregarded. For instance, in the NCI-HIV graph database used in our experiments it makes sense to consider only those graphs that have less than 100 simple cycles, as even in this case, 99 . 76% of the whole dataset is still covered (see also Table 1 in Section 5.1).

More precisely, given an upperbound T on the time needed to compute the cyclic and tree patterns for the database G and a threshold  X   X  [0 , 1], our goal is to find the smallest http://cactus.nci.nih.gov/
The NCI-HIV dataset used in our experiments is an anno-tated subset of the NCI domain.
 Algorithm 2 Bounded Cyclicity Require: graph G with n vertices and m edges, and k  X  N Ensure:  X  CP ( G )if |S ( G ) | X  k and  X  otherwise 1: let K =0 2: let S = B =  X  3: compute the biconnected components of G 4: for all biconnected component G of G do 5: if G contains more then one edge then 6: let X =RT( G ,k  X  K +1) 7: if | X | = k  X  K +1 then return  X  8: else 9: S = S  X  X  p ( C ): C  X  X } 10: K = K + | X | 11: else 12: add the edge of G to B 13: S = S  X  X   X  ( t ): t is a connected component of B } 14: return S such that (with high probability) (i) there is a subset G  X  X  with cardinality at least  X  |G| (ii) applying Algorithm 2 with parameter k to the graphs if such a k exists, and to print  X  X O X  otherwise. We first note that applying Algorithm 2 with parameter k to the graphs in G , the running time is bounded by where n max (resp. m max ) is themaximum number of vertices (resp. edges) of any graph in G . Thus, for a given time limit T , one can compute the maximum value of the upper bound K on the number of simple cycles for which the algorithm still runs in time T .

Given K , we would like to find the smallest k  X  K such that at least  X  |G| graphs in G have at most k simple cycles. We note that this problem cannot be solved efficiently by counting thenumberof simple cycles in a graph. This follows from the negative result of Valiant [30] which states that counting the number of simple paths between two vertices in a graph is #P-complete. We therefore propose a technique based on sampling 6 and enumerating at most K +1simple cycles of a graph. Using some sampling method, we first select a random sample S of the underlying graph database G . Then, applying Read and Tarjan X  X  algorithm, we assign the number  X  ( G ) defined by to every G  X  S . We then define k by where S ( k )= { G  X  S :  X  ( G )  X  k } , and print k if k  X  K and  X  X O X  otherwise (i.e., if k = K +1).
Note that our problem is to estimate the unknown param-eter of a Bernoulli distribution.
To evaluate empirically the predictive performance of cyclic pattern kernels, in this section we use the HIV dataset of chemical compounds. This database is maintained by the National Cancer Institute (NCI) 7 and describes information of the compounds X  capability to inhibit the HIV virus. It has been used frequently in the empirical evaluation of graph mining approaches (see, e.g., [4, 8, 9, 20]). So far, the only approaches to predictive graph mining on this dataset are describedin[8,9].
In the NCI-HIV database, each compound is described by its chemical structure 8 and classified into three categories: confirmed inactive (CI), moderately active (CM), or active (CA). A compound is inactive if a test showed less than 50% protection of human CEM cells. All other compounds were retested. Compounds showing less than 50% protection (in the second test) are also classified inactive. The other com-pounds are classified active, if they provided 50% protection in both tests, and moderately active, otherwise.

The NCI-HIV dataset we used contains 42689 molecules, 423 of which are active, 1081 are moderately active, and 41185 are inactive. The total number of vertices and edges in this dataset is 1951154 and 2036712, respectively. Ta-ble 1 shows how the number of simple cycles is distributed over molecules. Clearly, the well-behavedness assumption made above holds for this dataset. Figure 2 illustrates these frequencies on a log-log scale.. On a PC with a Pentium III/850 MHz Processor, the set of cyclic and tree patterns for every graph in the database has been computed in less than 10 minutes.
 Table 1: Distribution of simple cycles over com-pounds
Frequently, the predictive performance of algorithms is compared by means of a statistical test on the accuracies achieved by the learned model on given test sets. However, measuring accuracy is only meaningful if the class distribu-tion and misclassification costs in the target environment are known and do not change over time [25]. Clearly, the cost of not finding an active compound should be higher than the http://cactus.nci.nih.gov/
This database also describes other (propositional) features that we do not make use of, as we want to compare our results with those reported in [8]. Making use of the geo-metrical information will be considered in future work. Figure 2: Log-log plot of the number of molecules (y) versus the number of simple cycles (x) cost of a false alarm. However, we do not know the cost of false alarms in this domain.

Recently, an alternative way of comparing the predic-tive performance on binary classification problems is be-coming more and more popular in the machine learning and data mining communities. Receiver Operator Charac-teristic (ROC) is a two-dimensional tool (rather than one-dimensional error rates) that is able to overcome the above mentioned shortcomings of merely accuracy based compar-isons. In the two-dimensional ROC space each classifier is represented by its true-positive rate (the fraction of positive examples correctly classified) and its false-positive rate (the fraction of negative examples incorrectly classified). The ideal classifier (ROC heaven) has true-positive rate 1 and false-positive rate 0. All classifiers with equal true-positive and false-positive rate (the diagonal in ROC space) corre-spond to randomly choosing a class for each query instance.
Any classifier with a continuous output, such as the sup-port vector machine (compare Equation (1)), gives rise to a set of classifiers, each corresponding to a different threshold on the output. If the output for a query instance is above this threshold, it is considered positive (with respect to this threshold) and negative (with respect to this threshold) oth-erwise. The resulting ROC curve illustrates the possible tradeoffs between correctly classified positive examples and incorrectly classified negative examples.

Whenever a single number is preferred to compare differ-ent classifiers, ROC analysis offers the possibility of using the area under the ROC curve. To overcome the depen-dency of ROC analysis on a single test-set, it is desirable to combine ROC analysis with crossvalidation techniques. How to best average ROC curves, however, is still a matter of scientific dispute. In this paper we report the mean and variance of the area under the curve over different folds.
To empirically evaluate the benefits of using all patterns rather than frequent patterns only, we compared our ap-at a 1% level 0 . 017) 0 . 7420 1 . 0 0 . 9187 (  X  0 . 011) 0 . 8683 0 . 016) 0 . 7504 1 . 5 0 . 9264 (  X  0 . 006) 0 . 8676 0 . 018) 0 . 7864 15 . 0 0 . 9340 (  X  0 . 011) 0 . 9023 0 . 017) 0 . 7783 35 . 0 0 . 9321 (  X  0 . 010) 0 . 9097 0 . 017) 0 . 7731 50 . 0 0 . 9318 (  X  0 . 009) 0 . 9122 0 . 018) 0 . 7486 100 . 00 . 9285(  X  0 . 010) 0 . 9138 denote a significant win at a 1% level 0 . 014) 0 . 7420 1 . 0 0 . 9257 (  X  0 . 005) 0 . 8683 0 . 015) 0 . 7504 1 . 5 0 . 9311 (  X  0 . 007) 0 . 8676 0 . 012) 0 . 7864 15 . 0 0 . 9466 (  X  0 . 008) 0 . 9023 0 . 013) 0 . 7783 35 . 0 0 . 9441 (  X  0 . 008) 0 . 9097 0 . 013) 0 . 7731 50 . 0 0 . 9430 (  X  0 . 008) 0 . 9122 0 . 014) 0 . 7486 100 . 0 0 . 9426 (  X  0 . 007) 0 . 9138 proach to the results presented in [8] and [9]. The clas-sification problems considered there were: (1) distinguish between CA from CM, (2) distinguish CA and CM from CI, and (3) distinguish CA from CI. For each problem, the area under the ROC curve, averaged over a 5-fold crossvalidation, is given for different misclassification cost settings.
Note that, while the walk-based graph kernels consid-ered for example in [15] (see Section 3) can be computed in polynomial time, the exponent of the polynomial appears to be too large for this application. For this reason, we have not included experiments with the walk-based graph kernel. We note, however, that successful applications us-ing such graph kernels have been reported in [19] and [14] on smaller datasets. Computing the direct product graph takes time quadratic in the number of vertices of the graphs in
G . Inverting the adjacency matrix of the product graph or computing the eigen-decomposition of this matrix are both roughly of cubic time complexity in the number vertices of the product graph. For example, for a molecule of 214 atoms, we obtain a product graph with 34645 vertices (if we do not take the vertex labels into account we would have 214 2 = 45796 vertices). Techniques for speeding up walk based graph kernels will be considered in future work. 9 In our experiments we used a modified version of the SVM-light [18] support vector machine with the same set of misclassification cost parameters as used by [8]. The reg-ularisation parameter was chosen by SVM-light. We used two different kernel functions. The first is the cyclic pattern kernel (CPK) given in Equation (4), the other is a Gaussian version of that kernel (  X  CPK), that is:
Forexample,onecouldmakeuseofthenicepropertiesof eigen-decompositions under tensor product which is strongly related to the direct product graph. Alternatively, one could employ vertex colouring algorithms to increase the number of vertices with different colour in the original graphs  X  this would at the same time decrease the number of vertices in the product graph.

Tables 2 and 3 show our experimental results with cyclic pattern kernels (CPK) and those achieved in [8] (FSG) with a frequent subgraph based approach. As we did not know thevarianceof thearea undertheROCcurvefor FSG, weas-sumed the same variance as for CPK 10 . Thus, to test the hy-pothesis that CPK significantly outperforms FSG, we used a pooled sample variance equal to the variance exhibited by CPK. As FSG and CPK were applied in a 5-fold crossvalida-tion, the estimated standard error of the average difference is the pooled sample variance times tic is then the average difference divided by its estimated standard error. This statistic follows a t distribution. The null hypothesis  X  CPK performs no better than FSG  X  can be rejected at the significance level  X  if the test statistic is greater than t 8 (  X  ), the corresponding percentile of the distribution.

Table 2 reports the results achieved using the cyclic pat-tern kernel directly. In all experiments the mean result achieved by CPK is better than the result achieved by FSG. On a 1% significance level, CPK outperforms FSG in 13 out of 18 experimental settings performed in [8].

Table 3 reports the results achieved using the Gaussian version of the cyclic pattern kernel. For the parameter  X  problem of distinguishing CA from CM with misclassifica-tion costs set to 1 . 0. These experiments showed that using the parameter 0 . 05 we are able to obtain slightly better areas under the ROC curve than with the other two parameters. For that, we kept the parameter  X  =0 . 05 throughout all experiments. In all experiments the mean result achieved by CPK is better than the results reported for FSG. Indeed, on a 1% significance level,  X  CPK outperforms FSG in all classification problems and cost settings that were reported in [8].

In [9] the authors of [8] describe improved results (FSG  X 
We could alternatively assume that the mean area of CPK is the  X  X rue mean X  we are comparing with. This would simply result in higher significance levels. at a 5% level 0 . 017) 0 . 765 1 . 0 0 . 9187 (  X  0 . 011) 0 . 839 0 . 017) 0 . 794 100 . 0 0 . 9285 (  X  0 . 010) 0 . 908 denote a significant win at a 5% level 0 . 014) 0 . 765 1 . 0 0 . 9257 (  X  0 . 005) 0 . 839 0 . 013) 0 . 794 100 . 0 0 . 9426 (  X  0 . 007) 0 . 908 There the authors report results obtained by an optimised threshold on the frequency of patterns and by including ad-ditional, geometric features. Tables 4 and 5 compare our results to those reported in [9] for the optimised thresh-old. We do not compare our results to those obtained using the geometric features. We are considering to also include geometric features in our future work and expect similar improvements. Both CPK and  X  CPK perform better than FSG  X  in all learning problems and all misclassification cost settings reported in [9]. However, the improvement there is less significant. On a significance level 11 of 5% CPK per-forms significantly better than FSG  X  in3outof6cases;  X 
CPK performs significantly better than FSG  X  in 5 out of 6cases.
As an alternative to graph kernels based on frequent pat-terns, in this paper we have proposed a graph kernel based on cyclic and tree patterns independent of their frequency. To compute cyclic pattern kernels , we first extract all cyclic and tree patterns from each graph and then apply an inter-section kernel to these pattern sets. Empirical results on the NCI-HIV domain indicate that this graph kernel is superior to frequent pattern based graph kernels.

Since computing cyclic pattern kernels is intractable in general, the approach presented in this paper is limited to well-behaved graph databases. That is, graph databases in which there exists a natural small upper bound on the num-ber of simple cycles for almost every graph. As such a bound cannot be found by counting the number of simple cycles of each graph, we have proposed an algorithm based on sam-pling, that estimates whether the database meets this re-quirement. A small bound clearly exists for instance in the large NCI database including the NCI-HIV dataset that has frequently been used to evaluate graph mining approaches.
Despite the encouraging empirical results, there is still room for further work on graph kernels. It seems attrac-tive to try to combine the ideas presented here with kernels based on walks in graphs [15, 19]. To compute these graph kernels the direct product graph has to be computed and an eigen-decomposition or inversion of its adjacency matrix has to be performed. Due to the large number of vertices of the product graphs in this domain, these approaches cannot
A 5% significance level is the usual level. Tables 2 and 3 showed stronger results than that, however, a significant win on a 5% significance level is usually considered sufficient. directly be applied. Techniques for speeding up walk based graph kernels will be considered in future work.

Besides walk kernels, we are going to investigate graph kernels induced by shortest paths between vertices. In par-ticular, we consider path (resp. walk) kernels based on the set of k shortest simple (resp. non-simple) paths between each pair of vertices.

In contrast to simple cycles, the number of relevant cycles of a graph can be computed in polynomial time without listing them [32]. In future work, we are going to investi-gate the predictive power of graph kernels based on rele-vant cycles. To overcome the other complexity limitation, that cyclic patterns cannot be enumerated with polynomial output complexity, we plan to investigate graph classes for which this problem can be solved polynomially.

In addition to the graph structure of molecules, most com-pound databases also contain information about the 3D co-ordinates of each atom in one of the molecule X  X  low energy conformations. These coordinates can either be measured in experiments or computed with one of several software tools. The advantage of using software tools is that not only the coordinates of each atom in the molecule X  X  lowest energy conformation can be computed, but the coordinates can be obtained for a set of low energy conformations. From a chemical point of view, these coordinates are important when deciding whether a molecule binds well to a target. Thus, from a machine learning perspective this information is likely to improve the predictive power of our classifier. We are working on advanced kernel functions for molecules that also take the 3D information into account. [1] R. Agrawal, H. Mannila, R. Srikant, H. Toivonen, and [2] H. Alt, U. Fuchs, and K. Kriegel. On the number of [3] Asai, Arimura, Uno, and Nakano. Discovering frequent [4] C. Borgelt and M. R. Berthold. Mining molecular [5] B. E. Boser, I. M. Guyon, and V. N. Vapnik. A [6] M. Collins and N. Duffy. Convolution kernels for [7] C. Cortes, P. Haffner, and M. Mohri. Positive definite [8] M. Deshpande, M. Kuramochi, and G. Karypis.
 [9] M. Deshpande, M. Kuramochi, and G. Karypis.
 [10] R. Diestel. Graph theory . 2nd edition, Springer Verlag, [11] H.-D. Ebbinghaus and J. Flum. Finite Model Theory . [12] M. F  X  urer. Graph isomorphism testing without [13] T. G  X  artner. Exponential and geometric kernels for [14] T. G  X  artner,K.Driessens,andJ.Ramon.Graph [15] T. G  X  artner,P.A.Flach,andS.Wrobel.Ongraph [16] T. Graepel. PAC-Bayesian Pattern Classification with [17] D. Haussler. Convolution kernels on discrete [18] T. Joachims. Making large X  X cale SVM learning [19] H. Kashima, K. Tsuda, and A. Inokuchi. Marginalized [20] S. Kramer, L. D. Raedt, and C. Helma. Molecular [21] S. R. Kumar, P. Raghavan, S. Rajagopalan, [22] M. Kuramochi and G. Karypis. Frequent subgraph [23] C. Leslie and R. Kuang. Fast kernels for inexact string [24] H. Lodhi, J. Shawe-Taylor, N. Christianini, and [25] F. Provost, T. Fawcett, and R. Kohavi. The case [26] F. J. Provost and T. Fawcett. Robust classification for [27] R. C. Read and R. E. Tarjan. Bounds on backtrack [28] B. Sch  X  olkopf and A. J. Smola. Learning with Kernels . [29] R. Tarjan. Depth-first search and linear graph [30] L. G. Valiant. The complexity of enumeration and [31] V. Vapnik. The Nature of Statistical Learning Theory . [32] P. Vismara. Union of all the minimum cycle bases of a [33] C. Watkins. Kernels from matching operations. [34] M. Zaki. Efficiently mining frequent trees in a forest.
