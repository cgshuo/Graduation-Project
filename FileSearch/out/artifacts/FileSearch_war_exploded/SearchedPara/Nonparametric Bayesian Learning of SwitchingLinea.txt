 nonparametric Bayesian approach for learning switching LDS (SLDS) models. We also consider a special case of the SLDS X  X he switching vector autoregress ive (VAR) process X  X n which direct make them a practical choice in applications.
 One can view switching dynamical processes as an extension o f hidden Markov models (HMMs) learning SLDSs and switching VAR processes rely on either fix ing the number of HMM modes, exhibited dynamical behaviors.
 Hierarchical Dirichlet processes (HDP) can be used as a prio r on the parameters of HMMs with HMM X  X he sticky HDP-HMM of [5] X  X hat provides improved control over the number of mod es inferred by the HDP-HMM; such control is crucial for the prob lems we examine. Although the HDP-HMM and its sticky extension are very flexible time serie s models, they do make a strong Markovian assumption that observations are conditionally independent given the HMM mode. This sticky HDP-HMM formulation to learn an unknown number of per sistent, smooth dynamical modes and thereby capture a wider range of temporal dependencies. A state space (SS) model provides a general framework for ana lyzing many dynamical phenomena. The model consists of an underlying state, x linear time-invariant SS model, in which the dynamics do not depend on time, is given by where e An order r VAR process, denoted by VAR( r ), with observations y can be described in SS form by, for example, the following tra nsformation: r arbitrary dynamic matrix A subsumes the class of VAR( r ) processes.
 The dynamical phenomena we examine in this paper exhibit beh aviors better modeled as switches model is nonlinear. We define a switching linear dynamical system (SLDS) by The first-order Markov process z Gaussian noise e A Dirichlet process (DP), denoted by DP(  X , H ) , is a distribution on discrete measures on a parameter space  X  . The weights are generated via a stick-breaking construction [11]: Figure 1: For all graphs,  X   X  GEM (  X  ) and  X  choose  X   X  z  X   X  indicating which component generates y i  X  F (  X  z HDP draws G G j  X  DP(  X , G 0 ) encoding the frequency of each shared, global parameter: Because G probabilities, allowing an observation y via an indicator random variable z An alternative, non X  X onstructive characterization of sam ples G process states that for every finite partition { A in distribution to the HDP as L  X  X  X  [7, 12]: This weak limit approximation is used by the sampler of Sec. 4.2.
 this HDP-HMM, each HDP group-specific distribution,  X  and, due to the infinite mode space, there are infinitely many g roups. Let z Markov chain at time t . For discrete Markov processes z to which y observation y By sampling  X  butions ( E [  X  tween modes. When modeling dynamical processes with mode pe rsistence, the flexible nature of the HDP-HMM prior allows for mode sequences with unrealisti cally fast dynamics to have large considering a sticky HDP-HMM where  X  Here, (  X  X  +  X  X  of When  X  = 0 the original HDP-HMM is recovered. We place a vague prior on  X  and learn the self-transition bias from the data. our switching dynamical processes. Specifically, we develo p extensions of the sticky HDP-HMM for both the SLDS and switching VAR models. For the SLDS, we co nsider conditionally-dependent  X  Our choice of the number of columns of zeros is, in essence, a c hoice of model order. maneuvering target as a fixed LDS driven by a switching exogen ous input. Since the number of maneuver modes was assumed unknown, the exogenous input was taken to be the emissions of a HDP-HMM. This work can be viewed as an extension of the work by Caron et. al. [3] in which the exogenous input was an independent noise process genera ted from a DP mixture model. The HDP-SLDS is a major departure from these works since the dyna mic parameters themselves change with the mode and are learned from the data, providing a much m ore expressive model. The switching VAR( r ) process can similarly be posed as an HDP-HMM in which the obs ervations are modeled as conditionally VAR( r ). This model is referred to as the HDP-AR-HMM and is de-picted in Fig. 1(c). The generative processes for these two m odels are summarized as follows: Here,  X  4.1 Posterior Inference of Dynamic Parameters conditioned on a fixed mode assignment z the dynamic parameters given a fixed, known state sequence x of modes and resampling the sequences x gression problems, where K = |{ z Y ( k ) with N k columns consisting of the observations y associated with the k th mode decomposes as follows: has a matrix-normal distribution MN ( A ; M , V , K ) if respectively. A vectorization of the matrix A results in where  X  denotes the Kronecker product. The resulting posterior is d erived as We place an inverse-Wishart prior IW ( S Wishart prior reduces to the normal inverse-Wishart prior w ith scale parameter K . For the HDP-SLDS, we additionally place an IW ( R R , which is shared between modes. The posterior distribution is given by with S 4.2 Gibbs Sampler and both the dynamic and sticky HDP-HMM parameters. The samp ler for the SLDS is identical to sampler is described below and further elaborated upon in su pplemental Appendix II. Sampling Dynamic Parameters Conditioned on a sample of the mode sequence, z servations, y Sampling z approximation, and jointly sampling the mode sequence usin g a variant of the forward-backward algorithm. Specifically, we compute backward messages m and then recursively sample each z where p ( y of fewer than L components while allowing the generation of new components , upper bounded by L , as new data are observed.
 Sampling x tem. We can then block sample x The messages are given in information form by m the information parameters are recursively defined as See supplemental Appendix II for a more numerically stable v ersion of this recursion. Synthetic Data In Fig. 2, we compare the performance of the HDP-VAR( 1 )-HMM, HDP-VAR( 2 )-HMM, HDP-SLDS, and a baseline sticky HDP-HMM on three sets of test data (see Fig. 2(a)). The provide comparable performance since both the HDP-VAR( 2 )-HMM and HDP-SLDS with C = I contain the class of HDP-VAR( 1 )-HMMs. Note that the HDP-SLDS sampler is slower to mix since performance than the HDP-AR( 1 )-HMM while the performance of the HDP-SLDS with C = [1 0] HDP-AR( 2 )-HMMs. The data in the third scenario were generated from a 3 -mode SLDS model with C = I . Here, we clearly see that neither the HDP-VAR( 1 )-HMM nor HDP-VAR( 2 )-HMM is input representation, which is equivalent to an HDP-VAR( 1 )-HMM with random walk dynamics (
A much less effective than richer models which switch among le arned LDS. IBOVESPA Stock Index We test the HDP-SLDS model on the IBOVESPA stock index (Sao HDP-SLDS is compared to that of the HDP-AR( 1 )-HMM, HDP-AR( 2 )-HMM, and HDP-SLDS see the advantage of using a SLDS model combined with the stic ky HDP-HMM prior on the mode architecture.) The data consist of measurements y denotes the 2D coordinates of the bee X  X  body and  X  used in the MCMC procedure depend on the ground truth labels. (The authors also considered a  X  X arameterized segmental SLDS (PS-SLDS), X  which makes use of domain knowledge specific to in Table 1 we report the performance of these methods as well a s the median performance (over very good performance on sequences 4 to 6 in terms of the learn ed segmentation and number of 1 to 3 X  X hich are much less regular than sequences 4 to 6 X  X he pe rformance of the unsupervised reasonably good segmentations without having to manually i nput domain-specific knowledge. an unknown number of modes for describing complex dynamical phenomena. We presented a non-were based on a supervised MCMC procedure (DD-MCMC) [8] . HDP-SLDS and HDP-AR-HMM on real applications. Using the sam e parameter settings, in one
