 Graph matching plays an essential role in many real applications. In this paper, we study how to match two large graphs by maximiz-ing the number of matched edges, which is known as maximum common subgraph matching and is NP-hard. To find exact match-ing, it cannot handle a graph with more than 30 nodes. To find an approximate matching, the quality can be very poor. We propose a novel two-step approach which can efficiently match two large graphs over thousands of nodes with high matching quality. In the first step, we propose an anchor-selection/expansion approach to compute a good initial matching. In the second step, we propose a new approach to refine the initial matching. We give the optimality of our refinement and discuss how to randomly refine the matching with different combinations. We conducted extensive testing using real and synthetic datasets, and will report our findings. H.2.8 [ Database management ]: Database applications X  Data min-ing ; G.2.2 [ Discrete mathematics ]: Graph theory X  Graph algo-rithms Algorithm, Theory, Performance Graph Matching, Maximum Common Subgraph, Vertex Cover
Graph data proliferates in a wide variety of applications, includ-ing social networks in psycho-sociology, attributed graphs in image processing, food chains in ecology, electrical circuits in electricity, road networks in transport, protein interaction networks in biology, topological networks on the Web, etc. Graph data processing has attracted great attention from both research and industrial commu-nities.
 Graph matching plays an essential role in many applications. In biochemistry, the genome of an organism is represented as a graph with genes as nodes and the metabolic pathway is repre-sented as another graph with enzymes as nodes. These two graphs are then matched to identify FRECs (functionally related enzyme clusters) that reveal important biological features of the organisms [14]. In medicine, the electroencephalogram (EEG) signal can be transformed into a graph with the extracted energy bursts as nodes. Graph matching is applied to the comparison of two EEG signals to analyze different brain activities in terms of latency, frequency, en-ergy, and activated areas [5]. In video indexing, a region adjacency graph (RAG) is constructed to represent an object, where the nodes are segmented regions in video frames. Graph matching between two RAGs can be used to retrieve similar objects in video-shot col-lections [8].

In the literature, a number of algorithms have been proposed for graph matching including exact matching [20, 13, 11] and approx-imate matching [21, 7, 3, 9, 12, 15, 23, 17]. The exact approaches are able to find the optimal matching at the cost of exponential run-ning time, while the approximate approaches are much more effi-cient but can get poor matching results. More importantly, most of them can only handle small graphs with tens to hundreds of nodes. As an indication, exactly matching two undirected graphs with 30 nodes may take 100,000 seconds. It is important to note that real-world networks nowadays can be very large. The existing approaches cannot efficiently match graphs even with thousands of nodes with high quality.

In this paper, we study the problem of matching two large graphs, which is formulated as follows. Given two graphs G 1 and G find a one-to-one matching between the nodes in G 1 and G that the number of the matched edges is maximized. The optimal solution to the problem corresponds to the maximum common sub-graph ( MCS ) between G 1 and G 2 , which is an NP-hard problem, and has been studied in decades. It is known to be very difficult to find a high quality approximate matching efficiently even for small graphs. In order to meet the needs of handling large graphs for graph matching and analyzing, we propose a novel approximate solution with polynomial time complexity while still attaining high matching quality.

The main contributions of this paper are summarized below. We propose a novel two-step approach, namely, matching construction and matching refinement. In the matching construction, we propose a new anchor-selection/expansion approach to compute an initial matching. We give heuristics to select a small number of impor-tant anchors using a new similarity score, which measures how two nodes in two different graphs are similar to be matched by taking both global and local information of nodes into consideration. We compute a good initial matching by expanding from the anchors selected. The expansion is based on structural similarity among the neighbors of nodes in two graphs. In the matching refinement, we propose a new approach to refine the initial matching. The novelty of our refinement is as follows. First, we refine a matching M to a better one which is most likely to exist and can be identified. Sec-ond, we consider the efficiency, and focus on a subset of nodes to refine while giving every node in the graphs a chance to be refined. We show the optimality of our refinement. We also show how to randomly refine matchings with different combinations. Our refine-ment can improve the matching quality with small overhead. We conducted extensive testing using real and synthetic datasets, and confirmed the quality and efficiency of our approach. The average ratio of our approximate matching to the exact matching is above 90%, while the computational cost is less than 1% compared to the state of the art exact algorithms. This is a big step compared to all the approximate algorithms to match large graphs in the literature.
The rest of the paper is organized as follows. Section 2 discusses some related work. Section 3 gives the problem statement. Section 4 and Section 5 discuss the matching construction and matching refinement. Section 6 shows the performance results. Section 7 concludes this paper.
Graph matching can be divided into two categories, namely, ex-act graph matching and approximate graph matching.

For exact graph matching, in the literature, most of the algo-rithms use backtracking (refer to Ullmann X  X  algorithm for subgraph and graph isomorphism [20]). Existing solutions on finding maxi-mum common subgraph mainly focus on maximum common node induced subgraph, and most techniques can hardly be used for max-imum common edge induced subgraph. Among them, Mcgregor [13] proposes a backtracking search method for finding maximum common subgraph. An improved backtracking algorithm is given in [11] with time complexity O ( m n +1  X  n ) . Abu-Khzam et al. in [1] propose an algorithm that combines backtracking and vertex cover enumeration to solve the maximum common node induced subgraph problem. There are also some other works to calculate the maximum common node induced subgraph by finding the max-imum clique in the association graph [16, 10, 19]. The complexity of the maximum clique approach is no better than backtracking.
For approximate graph matching, there are three categories: prop-agation based method, spectral based method, and optimization based method. The propagation based method is introduced in [6]. It is based on the intuition that two nodes are similar if their respec-tive neighborhoods are similar. IsoRank [18] extends the propaga-tion based method by adding the weight of propagation. Spectral based method aims to represent and distinguish structural proper-ties of graphs using eigenvalues and eigenvectors of graph adja-cency matrices. It is based on the observation that if two graphs are isomorphic, their adjacency matrices will have the same eigen-values and eigenvectors. Since the computation of eigenvalues can be solved in polynomial time, it is used by a lot of works in graph matching [21, 7, 3, 9, 12, 15, 23]. Among these works, Umeyama [21] uses the eigendecomposition of adjacency matrices of the graphs to derive a simple expression of the orthogonal ma-trix that optimizes the objective function. However, such method is only suitable for graphs with the same number of nodes. [7] extends Umeyama X  X  work to match two graphs of different sizes by choos-ing the largest k -eigenvalues as the projection space. Knossow [9] improves the matching result by performing eigendecomposition on the Laplacian matrix since it is positive and semidefinite. Heat-kernel [23] is used to embed the nodes of the graph into vector-space based on the graph-spectral method, and the correspondence matrix between the embedded points of two graphs is computed by a variant of the Scott and Longuet-Higgins algorithm. The opti-mization based method aims to model graph matching as an opti-mization problem and solve it. The representative algorithms in-clude PATH [25] and GA [24]. In PATH [25], the graph matching problem is formulated as a convex-concave programming problem and approximately solved by starting from the convex relaxation and then iteratively solves the convex-concave programming prob-lem by gradually increasing the weight of the concave relaxation and following the path of solutions thus created. GA [24] is a gra-dient method based approach, which starts from an initial solution and iteratively chooses a matching in the direction of a gradient objective function. A survey can be found in [17].
In this paper, we deal with undirected and unlabeled graphs, which are difficult to match, because they do not have any label information to assist graph matching. For a graph G ( V; E ) , we use V ( G ) to denote the set of nodes and E ( G ) to denote the set of edges.
 Definition 3.1: Graph/Subgraph Isomorphism . Graph G 1 is iso-morphic to graph G 2 , if and only if there exists a bijective function f : V ( G 1 )  X  V ( G 2 ) such that for any two nodes u 1 and u 2  X  V ( G 1 ) , ( u 1 ; u 2 )  X  E ( G 1 ) if and only if ( f ( u  X 
E ( G 2 ) . G 1 is subgraph isomorphic to G 2 , if and only if there exists a subgraph G  X  of G 2 such that G 1 is isomorphic to G Definition 3.2: Maximum Common Subgraph . A graph G is the maximum common subgraph ( MCS ) of two graphs G 1 and G denoted as mcs ( G 1 ; G 2 ) , if G is a common subgraph of G G , and there is no other common subgraph G  X  , such that G larger than G . 2
The MCS of two graphs can be disconnected, and there are two kinds of MCS s, namely, maximum common node induced sub-graph ( MCSv ) and maximum common edge induced subgraph ( MCSe ). The former requires MCS to be the node induced subgraph of both G 1 and G 2 , and G latter requires MCS to be the edge induced subgraph of both G and G 2 , and G  X  is larger than G iff | E ( G  X  ) | &gt; shows the difference between MCSv and MCSe . Given two graphs G 1 and G 2 . Fig. 1(a) shows the MCSv of G 1 and G 2 , whereas Fig. 1(b) shows the MCSe of G 1 and G 2 . As can be seen from this example, MCSe can possibly get more common substructure for two given graphs. In this paper, we adopt MCSe since it can possibly get more common substructure for two given graphs, and we use MCS ( mcs ) to denote MCSe . Finding MCS is NP-hard. Definition 3.3: Graph Matching . Given two graphs G 1 and G matching M between G 1 and G 2 is a set of pairs M = { ( u; v ) V ( G 1 ) ; v  X  V ( G 2 ) } , such that for any two pairs ( u M of two graphs is the one with the largest number of matched edges. Finding the optimal matching M is the same as MCS . Pr oblem Statement : We aim to compute the optimal matching M for two given graphs G 1 and G 2 . For a given matching M , we evaluate its quality by computing score ( M ) as follows. score ( M ) = where e u;v = 1 if there is an edge between u and v , and e Algorithm 1 match ( G 1 ; G 2 ) 4: return M ; otherwise. Obviously, finding the optimal matching M is actually to find a matching with the maximum score ( M ) , and the maxi-mum score ( M ) is | E ( mcs ( G 1 ; G 2 )) | .

MCS problem is NP-hard, and it is known to be very difficult to obtain a tight and even useful bound in the past decades. For the quality of the result, [2] give a bound O ( n 2 ) based on the number of mis-matched edges, where n is the size of the larger graph. It means that it may mis-match all the edges. [16] provides an upper bound for the size of the MCS , which is computed by sorting the degree sequences of two graphs separately followed by summarizing the corresponding smaller degrees. The bound is almost the smaller graph, without considering any structural information of the two graphs, which does not provide much information. In [2], the time complexity is O ( n 6 L ) , where L is the size of LP model formulated for graph matching (at least n ). It cannot even handle graphs with more than 100 nodes.

In this paper, we propose a novel approach to solve the graph matching problem, which includes two steps: matching construc-tion and matching refinement. In the matching construction step, we construct the initial matching M by identifying anchors of two graphs G 1 and G 2 followed by expanding from the anchors. We do so based on a new similarity between nodes in the two different graphs which combines both global and local information of nodes. In the matching refinement step, we refine the initial matching. We prove that our refinement is efficient and can generate a matching with good quality. The framework of the algorithm is shown in Al-gorithm 1 and is self-explained. We discuss the first step in Section 4, and the second step in Section 5.
In this section, we discuss how to select anchors and how to ex-pand from the selected anchors to obtain the initial matching M for two graphs G 1 and G 2 , using a new node similarity matrix S . The node similarity between u  X  G 1 and v  X  G 2 is very important be-cause it indicates how likely the two nodes will be matched when computing the matching M . Let G 1 and G 2 be two graphs. The new node similarity matrix S we propose takes both global and local node similarities into consideration when matching nodes in two graphs.
 Here, S is a | V ( G 1 ) | X | V ( G 2 ) | matrix, in which the element S [ u; v ]  X  [0 ; 1] represents the similarity of two nodes, u in G and v in G 2 . S is based on S g and S l , where S g measures global similarity between u and v in the entire graphs G 1 and G measures local similarity between u and v in their neighborhoods. We will introduce an existing global similarity below followed by the discussion on our new local similarity in this section. Global node similarity: In the literature, the global similarity for nodes in two graphs can be the spectral based similarity. The rep-resentative study is Umeyama X  X  work [21] which is improved by [9]. Suppose G 1 and G 2 are two undirected graphs with the same number of nodes n . The Laplacian matrix L n  X  n of graph G with n nodes is defined as L = D  X  A , where A is the adjacency matrix and D is the diagonal degree matrix. A [ u 1 ; u 2 ] = 1 if ( u E ( G ) , and 0 otherwise. D [ u 1 ; u 1 ] = We denote the Laplacian matrices of G 1 and G 2 as L 1 and L spectively. Suppose the eigenvalues of L 1 and L 2 are 1  X   X  X  X  X  n and 1  X  2  X  X  X  X  X  X  n respectively. Since L 1 and L 2 are symmetric and positive-semidefinite, we have L 1 = U 1 1 and L 2 = U 2 2 U T 2 , where U 1 and U 2 are orthogonal matrices, and 1 = diag ( i ) and 2 = diag ( i ) . If G 1 and G 2 are isomorphic, there exists a permutation matrix P such that P U 1 1 U T U d i  X  X  +1;  X  1 } accounts for the sign ambiguity in the eigende-composition. When G 1 and G 2 are isomorphic, the optimum per-mutation matrix is P which maximizes tr ( P T U 2 U T 1 ) , where U and U 2 are matrices which have the absolute value of each element of U 1 and U 2 , respectively. When the numbers of nodes in G G 2 are not the same, we only choose the largest c eigenvalues [9]. Let c = min {| V ( G 1 ) | ; | V ( G 2 ) |} , and U  X  1 and U columns of U 1 and U 2 respectively, the global similarity matrix can be computed with Eq. (3).
 Here, S g [ u; v ]  X  [0 ; 1] is the global node similarity between the node u in V ( G 1 ) and the node v in V ( G 2 ) .

Example 4.1 shows an example of matching the two graphs using the global node similarity.
 Example 4.1: Consider the two graphs in Fig. 2. We first compute their global node similarity matrix S g . We construct a bipartite graph G b with | V ( G 1 ) | + | V ( G 2 ) | nodes, and for any u and v  X  V ( G 2 ) , we add an edge ( u; v )  X  E ( G b ) with weight S [ u; v ] . We compute the maximum weight bipartite matching of G b and get the matching as M = { ( u 1 ; v 1 ) , ( u 2 ; v ( u 4 ; v 4 ) , ( u 5 ; v 5 ) , ( u 6 ; v 12 ) , ( u 7 ; v 13 v 10 ) , ( u 11 ; v 3 ) , ( u 12 ; v 6 ) , ( u 13 ; v 14 ) , ( u v ) } . In this way, the number of matched edges is 10 , which is far away from the optimal solution mcs ( G 1 ; G 2 ) , 21 (bold edges in Fig. 2). Comparing to the optimal solution, u 3 is mismatched to v because they have a high global similarity, but obviously, the local structure near u 3 and the local structure near v 7 differ much.
The global node similarity gives a node similarity measure from the global point of view. However, when G 1 and G 2 are not suf-ficiently similar to each other, using global node similarity only is not sufficient to get a good matching because the global node similarity does not consider the local information for nodes in two graphs. We need a local node similarity.
 Local node similarity: For any node v in graph G and k  X  0 , we define the k -neighborhood of v , N k ( v ) , as the set of nodes in distance from v to u is no more than k . The shortest distance is defined as the number of edges in the shortest path from v to u . The k -neighborhood subgraph of v in G , denoted as G k v as the induced subgraph over N k ( v )  X  X  v } in G . For two nodes u  X  V ( G 1 ) and v  X  V ( G 2 ) , we measure their local node similar-ity by comparing the k -neighborhood subgraphs of them. Suppose d ( u ) and d ( v ) are the degrees of node u and v in G 1 set N k ( u ) in G k u sorted in non-increasing order, and d is the degree sequence of node set N k ( v ) in G k v sorted in non-increasing order. Let n min = min {| N k ( u ) | ; | N k ( v ) a |
V ( G 1 ) | X | V ( G 2 ) | local node similarity matrix S l
S l [ u; v ] = ( n min + 1 + D ( u; v )) Here, D ( u; v ) consists of two parts. The first part min is the ideal contribution of edges when matching u with v , and the second part edges when matching nodes in N k ( u ) with nodes in N k ( v ) . We show that S l has the following properties. (1) 0  X  S l [ u; v ] G v are isomorphic, and u is matched to v in the optimal matching of G k u and G k v , we have S l [ u; v ] = 1 . (4) If G k morphic to G k v , and u matches v in the optimal matching of G Example 4.2 shows an example of matching two graphs using S . Example 4.2: Reconsider the graphs in Fig. 2. Let k = 2 . We construct a bipartite graph G b with | V ( G 1 ) | + | V ( G for any u  X  V ( G 1 ) and v  X  V ( G 2 ) , we add an edge ( u; v ) maximum weight bipartite matching of G b and get the matching M = { ( u 1 ; v 1 ) , ( u 2 ; v 2 ) , ( u 3 ; v 3 ) , ( u ( u 7 ; v 13 ) , ( u 8 ; v 8 ) , ( u 9 ; v 17 ) , ( u 10 ; v 10 ( u 13 ; v 11 ) , ( u 14 ; v 15 ) , ( u 15 ; v 9 ) , ( u 16 ; v matched is 13 which is better than 10 when only using the global similarity. But it is still much less than the optimal solution, 21 . A problematic approach to compute M using S : Umeyama [21] computes a matching M by applying the Hungarian algorithm to the node similarity matrix, which can be with S we newly pro-posed or S g given in [21]. Using all the similar node pairs com-puted, a matching M can be found. In order to compute a matching, Umeyama constructs a bipartite graph G b that includes | V ( G | V ( G 2 ) | nodes. For any node u  X  V ( G 1 ) and node v  X  an edge ( u; v ) is added to G b with weight S [ u; v ] (or S The maximum weight bipartite matching of G b leads to a matching M of graphs G 1 and G 2 . Such an approach has two drawbacks. (1) Similarity optimality does not mean matched edge optimality, while our aim is to maximize the number of matched edges in two graphs. It is possible that two nodes are very similar in terms of S (or S g ) but the two nodes do not have many incident edges that help to increase the number of matched edges. (2) This approach only considers the matching of individual nodes in two graphs, and does not consider whether the nodes around them can be well matched when it matches two nodes. In other words, matching u  X  V ( G with v  X  V ( G 2 ) does not consider whether the nodes around u and v can be matched using the maximum weight bipartite matching.
In our approach, we solve the two drawbacks as follows. Instead of matching all the nodes, we first match some important nodes Algorithm 2 anchor -expansion ( G 1 ; G 2 ; A ) 2: for all ( u; v )  X  X  do 4: while Q X  =  X  do 8: return M ; as anchors. Every two anchors matched have high similarity and large degrees, and can contribute a large number of matched edges. Then, we expand from the anchors to match the other nodes using the local similarity S l as the measure. Thus, our solution consists of two steps, namely, anchor selection and anchor expansion.
The anchors selected play two important roles in matching con-struction. (1) The matching anchors contribute a large number of edges to the matching M . (2) The anchors are the references to start with when matching the other nodes. The algorithm for an-chor selection (line 1 in Algorithm 1) is illustrated below. First, we compute the similarity matrix S (Eq. (2)). Second, we sort the pairs ( u; v ) for all u  X  V ( G 1 ) and v  X  V ( G 2 ) in the decreasing as matched anchors, if they satisfy the following two conditions: (1) min { d ( u ) ; d ( v ) } X  , where is the larger average degree S [ u; v ]  X  , where is a threshold and generally &gt; 0 : 5 . Example 4.3: Consider the two graphs in Fig. 2. Suppose = 0 : 94 , we get the anchor pairs to be A = { ( u 1 ; v 1 ) ; ( u pair ( u 9 ; v 17 ) is not selected because it destroys the degree con-straint, even though it satisfies the similarity constraint . Expanding from the pair ( u 9 ; v 17 ) to match other pairs is a bad choice.
We illustrate the anchor expansion algorithm (Algorithm 2) to obtain a matching M . Let A be the anchor pairs ( u; v ) selected al-ready. Initially, M = A . Let N ( u ) and N ( v ) denote the immedi-ate neighbors of u and v in graphs G 1 and G 2 respectively. For ev-ery matched pair ( u; v ) in the initial M , we put all ( N ( u ) pairs in a queue Q , where Q is the set of candidate matching pairs sorted in decreasing order of their local similarity. In an iterative manner, we remove the pair ( u; v ) with the largest local similarity S [ u; v ] (Eq. (4)) from Q . If both u and v have not been matched before, we add ( u; v ) to M and put their all ( N ( u )  X  mediate neighbor pairs into Q for further consideration. We repeat it until Q =  X  .
 The example of anchor expansion is given below.
 Example 4.4: Given the two graphs in Fig. 2. After we get the set of anchor pairs A = { ( u 1 ; v 1 ) ; ( u 8 ; v 8 ) } we can construct our matching M = { ( u 1 ; v 1 ) ; ( u 2 ( u 4 ; v 4 ) ; ( u 5 ; v 5 ) ; ( u 6 ; v 6 ) ; ( u 7 ; v 2 ) ; ( u ( u The number of matched edges is 18. 2
The initial matching M is computed using the heuristics that matches the anchors first followed by matching the nodes around the anchors in a top-down fashion. The heuristics used cannot guar-antee that all the anchors are correctly matched. In this section, we propose a new approach to refine the initial matching M . It is important to note that our strategy is to refine the initial matching and is not to find a completely new matching. By refinement, we mean the following two things. First, we are not to explore all pos-sibilities without a goal when we refine a matching. In other words, we refine a matching M to a better one which is most likely to exist and can be identified. Second, we consider the efficiency when re-fining a matching. In our approach, each time we focus on a subset of nodes to refine by excluding a subset of nodes and including a subset of nodes. The set of nodes to be excluded from refinement at one time is neither large nor small. Also we give every node in the graphs a chance to be refined. We use a vertex cover C to refine a matching M . A vertex cover C of a graph G is a subset of nodes in V ( G ) , i.e., C  X  such that for every edge ( u; v )  X  E ( G ) , we have u  X  A minimum vertex cover of graph G is a vertex cover with the minimum number of nodes. A vertex cover C of G is a minimal vertex cover, if there does not exist a vertex cover C  X  of G such that C  X   X  C . A set of nodes C is a vertex cover of graph G if and only if its complement I = V ( G )  X  C is an independent set of G . Here, an independent set I of G is a subset of nodes in V ( G ) , i.e., I  X  V ( G ) , such that for any u  X  I and v  X  I , ( u; v ) = Below, we introduce some notations we use to refine a matching M based on vertex cover. Suppose we match two graphs G 1 and G , and M is a matching found. Let P 1 and P 2 be the matched nodes in G 1 and G 2 respectively using the matching M . For any ( u; v )  X  M , we have u  X  P 1 and v  X  P 2 . Given a cover C of G , we use F 1 to denote C  X  P 1 . For any subset of nodes S we use M [ S ] to denote the corresponding matched part of S in P 2 using matching M . For any subset of nodes S  X  P 2 , we use M  X  1 [ S ] to denote the matched part of S in P Let F 2 = M [ F 1 ] . The relationships among G 1 , G 2 , P F 2 and C are illustrated in Fig. 3.
 Vertex cover plays an important role when we match two graphs G 1 and G 2 . It allows us to focus on one graph G 1 , with the assis-tance of its vertex cover. The intuition is as follows. By definition, a vertex cover of G 1 is the set of nodes that covers all possible edges in G 1 . This implies that a node in a vertex cover can possibly have many edges to cover (or possibly have many matched edges with another graph G 2 ).

A vertex cover C of G 1 divides V ( G 1 ) into three parts, F C  X  P 1 , C  X  F 1 and V ( G 1 )  X  C . The implications are given below. The nodes in F 1 are most likely to lead to good matches, based on the definition of vertex cover. We exclude nodes in F 1 to refine. We include nodes in V ( G 1 )  X  C to refine, because the complement of the vertex cover V ( G 1 )  X  C is an independent set such that there are no edges between any two nodes in V ( G 1 )  X  C . Such a property makes it possible to apply some efficient polynomial algorithms for optimizing the matching. For C  X  F 1 , we will first discuss how to refine by excluding nodes in C  X  F 1 , and then discuss how to include nodes C  X  F 1 to refine.
 Remark 5.1: The size of a vertex cover for a graph is usually nei-ther too large nor too small. It can not be too large, because a graph contains a considerable number of nodes that are indepen-dent with each other, and the complement of such nodes forms a vertex cover. For example, if G is a star graph, it has a vertex cover of size 1; if G is a tree, it has a vertex cover of at most if G is a bipartite graph, it has a vertex cover of at most It can not be too small, because it must guarantee that all edges in the graph will be covered by at least one node in the vertex cover. In our experiments, the average size of the vertex covers is 53.1% to the number of nodes in the graph. 2 Given two graphs G 1 and G 2 , a matching M , and a vertex cover C of G 1 , we give a refinement M + ( C ) of M , and show its opti-mality below.

First, we show how to obtain a refinement M + ( C ) of M . We build a complete weighted bipartite graph G b . On one side G includes the nodes in V ( G 1 )  X  C , and on the other side G the nodes in V ( G 2 )  X  F 2 . For any node u  X  V ( G 1 ) v  X  V ( G 2 )  X  F 2 , we add an edge ( u; v ) in E ( G b ) , and the weight of the edge ( u; v ) is defined as follows. where N ( u ) and N ( v ) are the sets of immediate neighbors of u and v in graphs G 1 and G 2 respectively. Intuitively, w ( u; v ) is the contribution of the matched edges if we match u in graph G with v in graph G 2 . Next, we find the maximum weighted bipartite matching M b of G b using the Hungarian algorithm, such that the total weight of edges in M b is maximized. We obtain our new matching M + ( C ) as follows.
 where F 1  X  F 2 is the cartesian product of F 1 and F 2 . It includes all pairs ( u; v ) such that u  X  F 1 and v  X  F 2 .
 Example 5.1: Continue with Example 4.4. To make it more simple, let X  X  only consider part of the matching in the initial matching. Sup-pose the first graph in Fig. 4(a) is the partial graph induced by nodes { u 6 ; u 8 ; u 11 ; u 12 ; u 13 } in G 1 , and the second graph in Fig. 4(b) is the partial graph induced by nodes { v 6 ; v 8 ; v 11 ; v In the initial matching M generated in Example 4.4, only three edges are matched, which is showed as the bold edges in Fig. 4(a) and Fig. 4(b). We have P 1 = { u 8 ; u 11 ; u 12 ; u 13 } { v 8 ; v 11 ; v 12 ; v 13 } . Suppose C = { u 6 ; u 8 ; u 12 C  X  P 1 = { u 8 ; u 12 } and F 2 = M [ F 1 ] = { v 8 ; v 12 graph G b , the left part consists of the nodes in V ( G 1 { u 11 ; u 13 } , and the right part consists of the nodes in V ( G which is { v 6 ; v 11 ; v 13 } . The graph G b is shown in Fig. 4(c). For the edge ( u 11 , v 11 ), its weight is 2 because if we match node u with node v 11 , 2 edges will be matched in the original graphs, i.e., edge ( u 11 ; u 8 ) is matched to edge ( v 11 ; v 8 ) , and edge ( u is matched to edge ( v 11 ; v 12 ) . The maximum weighted bipartite matching of G b is M b = { ( u 11 ; v 11 ) ; ( u 13 ; v 13 improve the number of matched edges from 18 to 20 . Similarly, we can refine matching pairs { ( u 2 ; v 7 ) ; ( u 7 ; v 2 ( u 7 ; v 7 ) } such that it improves the number of matched edges to 21, which is the optimal value in this example. 2
Second, we give the optimality of M + ( C ) over a matching space Algorithm 3 select -random -cover ( G ) 2: for all u  X  L do 4: for all u  X  C do 6: return C ; M . The space M is a set of matchings between nodes in G 1 and G , such that for any matching M  X  , M  X   X  X  if and only if M ( F 1  X  F 2 ) = M  X  ( F 1  X  F 2 ) and M For the matching M , a matching M  X   X  X  , if and only if the matching for nodes in F 1 is not changed and the matching for nodes in C  X  F 1 is  X  . The second condition can also be expressed as M  X  [ C  X  F Theorem 5.1: Suppose min = min {| V ( G 1 ) | X  X  C | ; | V ( G  X  X  F 2 |} and max = max {| V ( G 1 ) | X  X  C | ; | V ( G 2 ) | X  X  we have (1) |M| =  X  |M| X  ( max + 1) min , (2) M  X  X  , (3) M + ( C )  X  X  and (4) M + ( C ) is optimal in M . 2 The proof sketch is given in the Appendix.
 Theorem 5.1 shows that the size of M is exponentially large. Both M and M + ( C ) are elements in M , and M + ( C ) is the opti-mal matching for all matchings in M . It implies that M + best among a large number of matchings in M and score ( M  X  score ( M ) . For two graphs with 2000 nodes each, the number of nodes in a vertex cover can be assumed as 1,000 (50%) reasonably. According to Remark 5.1, M + ( C ) is the best among a factorial of 1,000 (1,000 ! ) possible matchings.
If M itself is an optimal matching in M , or the selected vertex cover C includes most nodes in G 1 that are not well matched, it is possible that M + ( C ) cannot improve M . As an example, suppose C = { u 11 ; u 12 ; u 13 } , in Example 5.1, then the new bipartite graph G b is the one shown in Fig. 4(d). In other words, using the max-imum weighted bipartite matching of G b , the matching M + might be the same with M . The reason is that the mismatched nodes are excluded by the vertex cover C to refine. We give an ap-proach based on two strategies to solve such a problem. (1) Making C smaller, such that more mismatched nodes can be included and thus can be used to refine. (2) Iteratively refining the current match-ing using different vertex covers, such that every mismatched node will have a chance to be included to refine. The first strategy is based on the following Lemma.
 Lemma 5.1: For any two vertex covers C 1 and C 2 of G 1 , if C C , then score ( M + ( C 1 ))  X  score ( M + ( C 2 )) . 2 The proof sketch is given in the Appendix.

In order to make C small, a straightforward way is to find the minimum vertex cover of G 1 . This method is not practical for two reasons. (1) Finding a minimum vertex cover of a graph is NP-hard. (2) In a minimum vertex cover, the mismatched nodes do not have a chance to be included to refine. To avoid these, we use a minimal vertex cover instead, because (1) a minimal vertex cover is easy to be found, and (2) the number of different minimal vertex covers for a graph is much larger than the number of different minimum vertex covers. Thus, a minimal vertex cover gives the mismatched nodes in a minimum vertex cover more chances to be refined. The approach to randomly select a minimal vertex cover of graph Algorithm 4 re ne ( G 1 ; G 2 ; M ) 1: while M is updated or it is the first iteration do 2: for i = 1 to X do 7: return M ; Algorithm 5 re ne ( G 1 ; G 2 ; M ) 1: while M is updated or it is the first iteration do 2: for i = 1 to X do 5: compute M  X  ( F ) ; 7: return M ; G is shown in Algorithm 3. First, in line 1, we shuffle all nodes in the graph and put them into a list L , such that any permutation of V ( G ) has the same probability in L . In line 2-3, we find a vertex cover of G by adding node in L one by one. For any node to be added, we add it into the vertex cover if and only if it contributes at least one edge to the currently covered edges (line 3). Line 4-5 makes the current vertex cover minimal by removing those useless nodes, such that the removal of such nodes does not influence any edge currently covered. The following lemma shows that, for any minimal cover C of a graph G , there are considerable number of ways for Algorithm 3 to generate C .
 Lemma 5.2: For any minimal vertex cover C of graph G , there Algorithm 3 generates C . 2 The proof sketch is given in the Appendix.

The main refine approach is an iterative algorithm shown in Al-gorithm 4. We iteratively update the current matching until the matching is not improved in a certain iteration. In each iteration (line 2-6), we try X times to find a new random minimal vertex cover C (line 4), generate the matching M + ( C ) using the method introduced above (line 5), and update the current matching if M is a better matching (line 6). Here, X is a constant (  X  1 ) in order to avoid selecting a bad cover to terminate the whole process. In our experiments, when X = 5 and X = 10 over 92% and 99% of the nodes have a chance to be included to refine. We use X = 5 . Note that in line 3, we choose C to be a vertex cover of either G or G 2 with the same probability to increase the randomness. Theorem 5.2: The time complexity of Algorithm 4 is O ( m  X  m = min {| E ( G 1 ) | ; | E ( G 2 ) |} and n = max {| V ( G The proof sketch is given in the Appendix.
 Theorem 5.2 shows an upper bound of the time complexity for Algorithm 4. In practice, the processing time for the algorithm is much smaller than the upper bound because the initial matching M has already matched a lot of edges. In this section, we show that M + ( C ) can be further improved. Recall that in our previous approach to compute M + ( C ) , the nodes in C  X  F 1 of G 1 are excluded to refine. In order to refine the nodes in C  X  F 1 , we build a new weighted bipartite graph G follows. On one side G  X  b includes all nodes in V ( G 1 ) on the other side G  X  b includes all nodes in V ( G 2 )  X  node v  X  V ( G 1 )  X  F 1 and node u  X  V ( G 2 )  X  F 2 , there is an edge ( u; v )  X  E ( G  X  b ) with weight defined in Eq. (6). Suppose the maximum weighted bipartite matching of G  X  b is M  X  b matching M  X  ( F 1 ) is defined as follows.
 We define a matching space M  X  as follows. For any matching M between graphs G 1 and G 2 , M  X   X  X   X  if and only if M  X   X  F ) = M  X  ( F 1  X  F 2 ) and F 1  X  ( V ( G 1 )  X  P 1  X  M  X  X  X  1 is a vertex cover of G 1 . We have the following theorem. Theorem 5.3: M X  X   X  and suppose M  X  M is the optimal so-lution among all matchings in M  X  , we have score ( M  X  ( F score ( M  X  M )  X  score ( M + ( C )) . 2 The proof sketch is given in the Appendix.

Theorem 5.3 implies that the new space M  X  is larger than the space M in refinement excluding C  X  F 1 , and the new matching M  X  ( F plies that score ( M  X  ( F 1 ))  X  score ( M + ( C )) , where M the optimal matching in M . It is worth noticing that the cover C of G 1 does not participate in the construction of M  X  ( F The matching M  X  ( F 1 ) can be computed as long as F 1 is generated, and F 1 can be computed easily by the following lemma. Lemma 5.3: Suppose G 1 [ P 1 ] is the subgraph of G 1 induced by P . If C is a vertex cover of G 1 , then C  X  P 1 is a vertex cover of G vertex cover C of G 1 such that C P 1  X  C . 2 The proof sketch is given in the Appendix.
 Based on Lemma 5.3, we can derive that the vertex cover of rithm is shown in Algorithm 5 which is the re ne used in Algorithm 1. We use X = 5 . Comparing to Algorithm 4, there are two major modifications. The first is about the cover computing in line 3-4, instead of computing the cover of G 1 (or G 2 if we select G first graph in line 3), we only compute the vertex cover of G (or G 2 [ P 2 ] ). For the second modification, instead of computing M + ( C ) , we compute our new matching M  X  ( F ) .
We compare our algorithms, cons (matching construction only) and consR (matching construction plus matching refinement), with five state of the art graph matching algorithms: ume , heat , iso , path , and GA . Here, ume is the improved Umeyama algorithm in [21, 9], and heat , iso , path , and GA are the algorithms proposed in [23], [18], [25], and [24], respectively. We implement ume , heat (c) Vary Optimal Solution (NCI) and our algorithm using Visual C++ 2005 and Matlab R2009a. The C++ part calls Matlab to compute the eigenvalues and eigenvectors of the matrix, and executes the rest of the algorithm in C++. For iso , path , and GA , we download the source code of the graph matching package GraphM 1 . All tests were conducted on a 2.66GHz CPU and 3.43GB memory PC running Windows XP.
 We evaluated the algorithms using both real and synthetic datasets. The real datasets include the Power Network and the NCI dataset. The synthetic datasets are generated using two models, namely, Scale-Free/Power Law Model and Erdos Renyi Model. We use software Pajek 2 to generate graphs under these two models. Power Network (PN) is the electrical power grid of the western US selected from the University of Florida Sparse Matrix Collection The nodes are generators, transformers, and substations, and the edges are the high-voltage transmission lines between them. The graphs are proved to be power law networks [22, 4]. The dataset contains graphs with number of nodes varying from 39 to 5,300. The information of graphs used in our testing is shown in Table 1. NCI dataset (NCI) contains the compound structures from the Na-tional Cancer Institute Open Database 4 . The NCI dataset contains 233,281 connected graphs. The average node number is 21.17 and the average node degree is 2.2. According to the number of nodes in a graph, we selected 5 groups, containing graphs with node num-bers 10  X  4 , 15  X  4 , 20  X  4 , 25  X  4 , and 30  X  4 , respectively. Scale-Free/Power Law Model (SF) is a network model whose node degrees follow the power law distribution or at least asymp-totically. We generate graphs with node numbers 100, 500, 1,000, 2,500, and 5,000, respectively, with default value 1,000. The aver-age node degree is 4.
 Erdos Renyi Model (ER) is a classical random graph model. It defines a random graph as N nodes connected by M edges that are chosen randomly from the N ( N  X  1) = 2 possible edges. We generate graphs with node numbers 100, 500, 1,000, 2,500, and 5,000, respectively, with default value 1,000. The average node degree is 4.

For NCI, since each graph is small, we can compute the optimal matching for each pair of graphs using backtracking method. For graphs in other datasets, it is impossible to compute the optimal matching. Therefore, for any graph G 1 , we generate G 2 by ran-domly inserting/deleting a certain percent of nodes/edges followed by shuffling all nodes. The average percentage is 0.2. We set the parameters used in our algorithms as follows. For threshold used in anchor selection, we traversal interval [0.5, 1] with step size 0.02 and choose the one that generate the best result in matching con-struction. For k in the k -neighborhood definition, we vary it from 0 to 4 and find k = 2 achieves the best result in most cases. Thus we set k to 2 in the experiment.

We use matching ratio as our measure for the matching quality, Vary Graph Size : We vary the number of nodes in the graphs from 100 to 5,000 and test the matching ratio of each algorithm on PN, SF, and ER. The results for PN and SF are shown in Fig. 5(a) and Fig. 5(b). The results for ER dataset share the same patterns. For all cases, the matching ratios for ume , heat and iso algorithms are no larger than 0.2. It is because all the three algorithms get the match-ing by maximizing the total weight of the similarity matrix and consider little about their neighborhood information of two nodes. When the numbers of nodes are no large than 120, the performance of path is similar to consR , and reaches a matching ratio above 0.9. When the sizes of the graphs increase, the matching ratio of path decrease. GA performs better than path for large graphs, but still much worse than consR in all cases. When the sizes of the graphs are above 2,500, iso , path , and GA can not generate a result for some cases under our current computing environment. path even can not generate a result for graphs with more than 2,000 nodes. V ary Initial Matching : We compare ume , heat , iso , path , and GA with our matching construction algorithm cons . Fig. 6(a) shows the results for PN. In most cases, cons performs best among all algorithms. The only exception is the case when the numbers of nodes in graphs are smaller than 120, where path performs better than cons . Fig. 6(b) shows the results by applying our matching refinement algorithm to these five algorithms. Accordingly, they are denoted as umeR , heatR , isoR , pathR , and GAR . For ume , heat , and iso , the matching ratios increase 0.5 after refinement in all cases. Our consR algorithm performs best in all cases after re-finement even for the case when the numbers of nodes in the graphs are smaller than 120. Fig. 6(c) and Fig. 6(d) show the testing re-sults for SF. Our algorithms perform best both before and after re-finement in all cases. The results for ER are shown in Fig. 6(e) and Fig. 6(f). The performances for ER are similar to those in SF. Efficiency Testing : We test the efficiency of the algorithms with PN. The processing time is shown in Table 2. We divide the whole processing time of our algorithm into three phases, namely, selection , expansion and re nement , denoting the processing time for anchor selection, anchor expansion, and matching refinement respectively. The processing time for consR is the sum of the time for all the three phases. For the three phases, in all cases, the most costly part is anchor selection, because it involves calculating the eigenvalues for matrices. In all test cases, our consR algorithm is faster than iso , heat , path , and GA , and is similar to ume . For the largest graph g5, the total processing time for consR is smaller than 25 minutes, while heat needs more than one hour and iso , path , and GA even can not generate a result under our current computing environment.
With NCI, we randomly select 10 pairs of graphs from each group of graphs with different node numbers. For each pair, we compute the optimal matching by backtracking using the best up-per bound introduced in [16]. For each group, we record the aver-age processing time for backtracking and our algorithm as well as the average matching ratio in the two steps of our algorithm. The results are shown in Fig. 7(a) and Fig. 7(b). When node number increases from 10 to 30, the processing time of backtracking in-creases from 1 to 100,000 seconds and consR algorithm consumes  X  0.1 second in all cases. The average accuracy (matching ratio) for cons is 0.8 and the average accuracy for consR is 0.95.
We randomly select 1,000 pairs from the group of graphs with 20  X  4 nodes. For each pair, we compute the ratio of the optimal matched edges to the size of the smaller graph, and vary the ratio from 0.75 to 0.95. For each ratio, we compute the average process-ing time and average accuracy of our algorithm. The results are shown in Fig. 7(c) and Fig. 7(d). When the ratio increases from 0.75 to 0.95, the processing time of the backtracking algorithm de-creases from 10,000 seconds to 1 second. This is because when the ratio is large, the upper bound used to cut branches in the back-tracking is tight [16], thus the algorithm stops early. Our consR algorithm consumes no more than 0.1 second in all cases. The av-erage accuracy (matching ratio) for our cons algorithm is 0.8 and the average accuracy for our consR algorithm is 0.95. In the refinement step, each vertex cover is randomly selected. In order to test the sensitivity of such randomness, for each test case, we run our program for 100 times, and calculate their average match ratio (MR) as well as the standard deviation of the 100 match ratios. The average match ratio (MR) and the standard deviation for the PN dataset are shown in Table 3. For each test case, the standard deviation is very small. It means that no matter how the vertex covers are randomly selected, the program will generate almost the same result. There are two reasons. First, the nodes that largely influence the stability of results are those with large degrees, and with high probability such nodes will be selected in a vertex cover. Second, our refinement algorithm is an iterative algorithm and it stops when converged. In each iteration, every vertex cover has chances to be refined. The results for SF and ER datasets are similar with those in the PN dataset.
In this paper, we study how to find matching of two large graphs or how to score how similar two large graphs can be in terms of the possibly max number of matched edges. This is known to be NP-hard. We give a new two-step approach which ensures high efficiency and high quality. We conducted extensive testing using real and synthetic datasets, and confirmed the quality and efficiency of our approach.
 Proof sketch of Theorem 5.1: We prove it step by step. (1) To make things simple and without loss of generality, we as-sume | V ( G 1 ) | X  X  C | X | V ( G 2 ) | X  X  F 2 | , then min = and max = | V ( G 2 ) | X  X  F 2 | . Since V ( G 1 )  X  C and V ( G are the included parts of G 1 and G 2 respectively, we only con-sider the number of different matchings between V ( G 1 ) V ( G 2 )  X  F 2 . Suppose in V ( G 1 )  X  C , there are i nodes that participate in the matching in M , there are C i min different selec-tions of the i nodes, and for each selection, there are P ferent matchings between the i nodes and nodes in V ( G 2 There are totally C i min  X  P i max different matchings for a certain i . Since i  X  [0 ; min ] , the total number of different matchings is |M| = we remove the constraint that different nodes in V ( G 1 ) match different nodes in V ( G 2 )  X  F 2 , each node in V ( G will have max +1 choices include max nodes in V ( G 2 )  X  an empty match. The number of different relaxed matchings is then changed to ( max + 1) min which is an upper bound of |M| . (2) We only need to prove that M satisfies the two conditions of M . For the first condition, obviously, M  X  ( F 1  X  F 2 ) = M ( F 1  X  F 2 ) . For the second condition, the part C  X  F 1 is the nodes in C that are not matched in M , so M [ C  X  F 1 ] =  X  . As a result, M  X  (( C  X  F 1 )  X  V ( G 2 )) =  X  . (3) We need show that M + ( C ) satisfies the two conditions of  X  For the first condition, we have M + ( C )  X  ( F 1  X  F 2  X  For the second condition, M + ( C )  X  (( C  X  F 1 )  X  V ( G (4) For any matching M  X   X  X  we define a matching M  X  b as M b = M to denote the total weight for the bipartite matching M b partite graph G b . We claim (a) M  X  b is a bipartite matching of G (b) score ( M  X  ) = score b ( M  X  b ) + score ( M  X  ( F 1 score ( M + ( C )) = score b ( M b ) + score ( M  X  ( F 1  X   X  For (a), it is obvious because of two reasons. (1) M  X   X  For (b), we have score ( M  X  ) = score ( M  X   X  (( C  X  ( V ( G  X  For (c), it can be easily derived from (b) because M + ( C ) The theorem holds. 2 Proof sketch of Lemma 5.1: Suppose M ( C 1 ) and M ( C 2 ) are the matching spaces generated by C 1 and C 2 respectively. We use F 1 ( C 1 ) , F 1 ( C 2 ) , F 2 ( C 1 ) , and F 2 ( C ated by C 1 , F 1 generated by C 2 , F 2 generated by C 1 generated by C 2 , respectively. Since we have F 1 ( C 1 ) and F 2 ( C 1 )  X  F 2 ( C 2 ) , we need show M ( C 2 )  X  X  any M  X   X  X  ( C 2 ) , we have M  X   X  ( F 1 ( C 2 )  X  F 2 ( C ( F 1 ( C 2 )  X  F 2 ( C 2 )) and M Since F 1 ( C 1 )  X  F 2 ( C 1 )  X  F 1 ( C 2 )  X  F 2 ( C 2 M  X   X  ( F also have C 1  X  F 1 ( C 1 )  X  C 2  X  F 1 ( C 2 ) , thus M  X   X  F ( C ))  X  V ( G 2 )) =  X  , and accordingly M  X   X  X  ( C 1 ) . As a result, M ( C 2 )  X  X  ( C 1 ) . Since M + ( C 1 ) and M + the optimal solutions in M ( C 1 ) and M ( C 2 ) respectively, we have score ( M + ( C 1 ))  X  score ( M + ( C 2 )) . 2 Proof sketch of Lemma 5.2: We construct the | C | !  X | V ( G ) C | ! permutations as follows. For each permutation, we put C in the front in any order followed by V ( G )  X  C in any order. The number of such permutations is | C | !  X | V ( G )  X  C | ! . Now we prove for any such permutation, Algorithm 3 can generate C . Since C is minimal, in the first | C | loops of line 2-3 of Algorithm 3, the conditions in line 3 are all satisfied, and in the last | loops of line 2-3, the conditions in line 3 are all unsatisfied because C is already a vertex cover of G . So after the loop in line 2-3, C is generated. Since C is already minimal, the loop in line 4-5 will eliminate no node. Thus, Algorithm 3 can generate C . 2 Proof sketch of Theorem 5.2: Algorithm 4 is the main refinement. The while loop in line 1 will repeat for at most m times because the optimal solution can match at most m edges and in each loop, the number of edges for the latest solution will be increased for at least 1 . In each loop, the dominate part is finding the maximum weight bipartite matching using the Hungarian algorithm which can be done in O ( n 3 ) . Since X is a constant, the total time complexity for Algorithm 4 is O ( m  X  n 3 ) . 2 Proof sketch of Theorem 5.3: For  X  M  X   X  X  , we have M  X   X  ( F 1  X  F 2 ) = M  X  ( F 1  X  F 2 ) and M condition is the same as the first condition of M  X  . Since M F ] =  X  , we have ( C  X  F 1 )  X  M  X  X  X  1 [ V ( G 2 )  X  P 2 ] = have ( C  X  F 1 )  X  M  X  X  X  1 [ V ( G 2 )  X  P 2 ]  X  V ( G 1 ) C  X  F 1  X  V ( G 1 )  X  P 1  X  M  X  X  X  1 [ V ( G 2 )  X  P 2 ] , and thus C F 1  X  ( V ( G 1 )  X  P 1  X  M cover of G 1 , F 1  X  ( V ( G 1 )  X  P 1  X  M  X  X  X  1 [ V ( G 2 cover of G 1 , hence we have M  X   X  X   X  . Thus M X  X   X  holds. We now prove that score ( M  X  ( F 1 ))  X  score ( M  X  M ) . Suppose C  X  = F is a vertex cover of G 1 . Since M  X  M [ V ( G 1 )  X  P 1 ] we have M  X  M [ C  X   X  F 1 ] = M  X  M [ V ( G 1 )  X  P 1  X  M P ]] =  X  , so M  X  M  X  X  using vertex cover C  X  . This implies score ( M  X  M ) = score ( M  X  ( F 1  X  F 2 )) + score b ( M proof of Theorem 5.1), where M b is the maximum weight bipartite matching of G b generated by C  X  . We also have score ( M score ( M  X  ( F 1  X  F 2 )) + score b ( M  X  b ) . Since G b score ( M  X  ( F 1 ))  X  score ( M  X  ( F 1  X  F 2 )) + score score ( M  X  ( F 1  X  F 2 )) + score b ( M b ) = score ( M  X 
We last prove score ( M  X  M )  X  score ( M + ( C )) . This can be derived directly from M X  X   X  since M  X  M is optimal in M  X  M + ( C ) is optimal in M . 2 Proof sketch of Lemma 5.3: We first prove that if C is a vertex cover of G 1 , then C  X  P 1 is a vertex cover of G 1 [ P 1 C  X  P 1 is not a vertex cover of G 1 [ P 1 ] , then there exists an edge ( u; v )  X  E ( G 1 [ P 1 ]) such that u =  X  C  X  P 1 and v = Note that C is a vertex cover of G 1 , we have u  X  C or v Without loss of generality, we suppose u  X  C , since u =  X  we have u  X  C  X  ( C  X  P 1 ) , this contradicts with u  X  V ( G Accordingly, we prove C  X  P 1 is a vertex cover of G 1 [ P
We then prove that if C P 1 is a vertex cover of G 1 [ P 1 exists a vertex cover C of G 1 such that C P 1  X  C . We only need to prove that C = C P 1  X  ( V ( G 1 )  X  P 1 ) is a vertex cover of G because C P 1 is a vertex cover of G 1 [ P 1 ] . Otherwise, without loss of generality, we suppose u =  X  P 1 , then u  X  V ( G 1 )  X  ( u; v ) is also covered by C . As a result, all edged in E ( G covered by C , thus C is a vertex cover of G 1 . 2
