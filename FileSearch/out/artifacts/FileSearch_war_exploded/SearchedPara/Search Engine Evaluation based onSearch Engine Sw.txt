 In this paper we present a novel application of the search engine switching prediction model for online evaluation. We propose a new metric pSwitch for A/B-testing, which al-lows us to evaluate the quality of search engines in different aspects such as the quality of the user interface and the qual-ity of the ranking function. pSwitch is a search session-level metric, which relies on the predicted probability that the ses-sion contains a switch to another search engine and reflects the degree of the failure of the session. We demonstrate the effectiveness and validity of pSwitch using A/B-testing experiments with real users of search engine Yandex. We compare our metric with recently proposed SpU (sessions per user) metric and other widely used query-level A/B met-rics, such as Abandonment Rate and Time to First Click, which we used as our baseline metrics. We observed that pSwitch metric is more sensitive in comparison with those baseline metrics and also that pSwitch and SpU are more consistent with ground truth, than Abandonment Rate and Time to First Click.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Storage &amp; Retrieval]: Information Search &amp; Retrieval Keywords: online evaluation; search engine switching
Due to the intense competitiveness between commercial search engines, the correct evaluation of a search engine X  X  quality is critical for its ability to constantly improve its quality and hence maintain its search market share. Since the impact of the tested improvements becomes relatively smaller with the time, sensitivity of evaluation measures be-comes increasingly important. Two major approaches are used to evaluate search engine quality: Cranfield-style of-fline evaluation based on experts judgements [2] and online evaluation such as A/B-testing [6].

Despite the convenience of offline evaluation with docu-ment or search session labels provided by a group of ex-perts, this approach has several considerable disadvantages, namely: expert judgements may not reflect the actual rele-vance, which is often user-specific; the process requires sub-stantial costs and is also time consuming; some standard metrics do not necessarily correlate with user-centric per-formance measures. In contrast, the online evaluation ap-proach, such as A/B testing, alleviates these disadvantages as it uses the implicit feedback of real users directly. In this case, the major challenge of online evaluation methods is establishing the relationship between the observed search behaviour and the relative quality of the search engines ob-serving that behaviour. The idea of A/B-testing is the fol-lowing: users are randomly assigned to two groups. During some period of time, the first (control) bucket of users is presented with the original version of the search engine (A), while the second (treatment) bucket of users is presented with the experimental version of the search engine (B). Suc-cessful search engine development requires making effective decisions towards improving all of its components, such as the ranking algorithm, the snippet generation algorithm, the ads ranking algorithm or the user interface. A/B testing, by design, allows to measure the effect of innovations in any of these components, but requires highly sensitive and easily interpretable metrics that would detect even subtle differ-ences in the behavior of the users exposed to both versions of the system.

Many studies focused on the problem of predicting frustra-tion or satisfaction of a searcher during her search session [3], but, to the best of our knowledge, there were no attempts to study the utility of such predictive models for the purposes of A/B testing. In this paper, we present a new metric for evaluating the quality of a search engine that can be used in A/B-testing and which is based on the prediction of the pro-portion of unsuccessful user sessions among all user sessions. We argue that this is possible by predicting if the user had a reason to switch to another search engine for each search session in each bucket. Indeed, in 57% of switches from one search engine to another the cause of the switch is a poor quality of search results [5]. However, low quality search results are not the only reason of switching, there are other reasons, including the need to verify the acquired knowl-edge or to find additional information (26%) [5]. Actually, any search engine aims to maximize its sufficiency and hence prevent switches to its competitors in all and, especially, in these 83% of cases, what means to improve relevance, au-thority and trustworthiness of its search results, as well as to increase the size of its document index. Although, search engine switches are good indicators of user dissatisfaction, search engines are able to observe them only for a small Table 1: Characters assigned to actions. Charac-ters with * were not presented in the final metric classifier share of users that are used to switch and whose web surf-ing activities can be tracked. To expand this to the entire audience, one needs to predict the potential switches: the switches that indeed occurred but were not detected or the switches that could occur if the users in the corresponding search sessions had the habit to use more than one search engine (there is always a considerable share of users that never uses more than one search engine [10])(see Section 2).
An online metric has to be applicable for comparing rel-ative quality of modifications of search engines when they differ in different aspects such as ranking or user interface. We propose to use the probability that a session contains a switch to another search engine as such a metric (see Sec-tion 4.1).

In order to demonstrate that the proposed online met-ric is considerably more sensitive than the state-of-the-art online metrics [7, 9], we performed a series of A/B-testing experiments with the users of one of the most popular search engines (see Section 4).
In this section we consider the problem of search engine switching prediction in a search session. We define a switch as an event of changing one search engine to another in or-der to continue the current search session. Actually, the fact of switching can be unambiguously detected only in a small part of the search sessions performed by users who installed the browser or the special browser toolbar plug-in developed by a search engine [10]. We used the Yandex browser/toolbar interaction logs collected in October 2014. For each session recorded in these logs, we could exactly de-tect the fact of switching from the search engine to other search engines. After that, we detected users, who had at least one switch from the search engine to other search en-gines in a given period. Then, we extracted a random sam-ple of the search sessions of those  X  X witching-tolerant X  users from the period under study. Our final data set consisted of 224k search sessions, corresponding to 88k users. Further, we splited users into learn and test sets (1:1). In order to build a prediction model, we used a variation of stochastic gradient boosted decision trees [4]. The quality of the dif-feature group
Baseline sequence based fea-
Baseline +  X  X ctions dwell time X 
Baseline +  X  X uery text X 
Baseline +  X  X ser session number X 
Baseline +  X  X ertical re-sults and ads appearance X 
Baseline +  X  X ession-level time X 
Baseline + query text + session number ferent versions of predictors that we describe in the paper was measured using Area Under Curve (AUC) measure.
We started from encoding each search session into an or-dered set of characters representing a sequence of search actions. The alphabet for encoding is presented in Table 1. For example, using interaction logs, we encode the user ses-sion as follows: the user typed the query  X  X oyota X  and did not click anywhere. Seven seconds later, she reformulated her query  X  X oyota celica X  and, five seconds later, she clicked on the first result: image vertical search result. The next activity of that user happened after 2 minutes and it was a long click on the organic search result the third position. Using the full alphabet from Table 1, such a session would be encoded as (OOWSSL) and using the reduced version (with-out differentiation of clicked result types) as (OOLSSL). Our algorithm automatically extracts frequent subsequences of the characters that the sessions are represented by. Fur-ther, it uses these subsequences as binary features of the sessions. The classifier trained only with such sequence fea-tures (AUC = 0.59) was our baseline that we tried to im-prove with adding more features. Thus, we considered a number of session-level features, such as the duration of the session, the average duration of clicks, the level of advertis-ing shown, etc (56 features). Every feature was included in the dataset with two normalizations, as in [8]. AUC of the classifier trained with the baseline features and all these ad-ditional features was 0.73. Feature selection is worth doing not only for evaluating feature performance, but also for se-lecting the optimal feature set and reducing the amount of the data to be stored during the production process. To se-lect an optimal feature set, we grouped all features into the following groups:  X  X ctions dwell time X  features,  X  X uery text X  features,  X  X ser session number X  features,  X  X ertical results and ads appearance X  features and  X  X ession-level time X  features. The descriptions of each feature group are presented in Ta-ble 2. We evaluated the utility of each of the feature groups in two ways: we trained a classifier only with the considered additional feature group and without this additional feature group (with baseline features always present). The results are presented in Table 2. Table 3 contains the top-10 most useful features across all the groups in terms of AUC. The most important features for switch prediction are the fea-tures from  X  X uery text X  and  X  X ser session number X  groups: combining them into one group gives us AUC = 0.722, so we select features only from these two groups to include into the final set of features (26 additional features).
Since search engine switching can be interpreted as an in-dication of the failure of the user X  X  session, it can serve as a foundation for a search quality measure. We propose a new online metric for measuring and comparing the qual-ity of the search engines via A/B-testing. Our metric is based on the prediction of the proportion of unsuccessful user sessions among all user sessions. Using the classsifier built in Section 3 we can predict the event of a switch for any user session and estimate the proportion of unsuccessful user sessions by the probability of switching averaged over all sessions in the experiment. We call our metric pSwitch : Consider an A/B-testing experiment and suppose that the mean predicted switch probability in the experimental group is statistically significantly higher than the mean predicted switch probability in the control group: that indicates that the users in the experimental group are less satisfied. In order to measure the significance of the mean difference we used the bootstrap test [1] with sampling by users. We con-sidered results to be significant in the case of p-value &lt; 0.05.
To analyze pSwitch, we conducted 8 online experiments testing different new features of search engine Yandex. All the experiments were manually inspected and labeled by a group of experts as degrading or improving (it means that we expected each feature to either degrade or improve the user experience). We regarded these labels as the ground truth that we expected our metrics to agree with. We also ran our experiments on 3 A/A-testing experiments [6] to control the validity of pSwitch: reliable online evaluation metrics must not signal about any differences in such experiments, as otherwise these metrics are not valid. All the A/A and A/B experiments were also evaluated by SpU (Sessions per User) [9], Abandonment Rate and Time to First Click, which we used as our baseline metrics. The users from the treatment buckets of the A/B-testing experiments were exposed to the following configurations of the search engine.

Experiments with vertical results (2 experiments) group of experiments consisted of one improvement and one degradation of vertical results.

Swap (1 experiment) In this case, we made the follow-ing changes of the original ranking: two random documents from positions 2-9 were swaped. We believe that the search engine generates rankings which are better than a random ordering on average, so we could assume that such swapping will lead to a degradation of the search quality.

Old ranking algorithm(1 experiment) In this case, we used a 1-year-old ranking algorithm instead of the current production algorithm. We treat it as a degradation of the ranking algorithm.

Ranking improvements (4 experiments) This group of experiments consisted of 4 improvements of the ranking algorithm.

In each experiment, the changes were substantial and had to be detected by an online evaluation metric that is sup-posed to be sensitive to the changes in the target aspects of search engine quality.
We evaluate the performance of the two pSwitch met-rics: one based on the classifier with the full aplhabet and another based on the classifier with the reduced alphabet (without click type differentiation). The performances of the two types of pSwitch metric, SpU, Abandonment Rate and Time to First Click on the A/B dataset and A/A con-trol dataset are presented in Table 4. P-values for SpU met-ric and pSwitch metrics were measured using the bootstrap test [1]. P-values for Time to First Click and Abandonment Rate are measured by Wilcoxon signed rank test. None of the metrics detected a significant difference in A/A experi-ments, so all metrics proved their validity for online evalua-tion. As we can see from Table 4, pSwitch metric with the reduced alphabet is much more sensitive than SpU metric and is more consistent with the ground truth then Time to First Click and Abandonment Rate. However, the outcome of the pSwitch metric based on the classifier with the full alphabet produced the result opposite to the ground truth for vertical results improvement experiment. The differences for other experimental groups are consistent with the ground truth. The reason for this contradiction is the inclusion into 0,001
Experiment name SpU pvalue pSwitch improvement 1 0.2662 0.0302 * 0,0228 * 0.04 ( ! *) 0.43 improvement 2 0.0834 0.25 0,313 0.02 * 0.82 improvement 3 0.16 0.33 0,0604 0.35 0.01 ( ! *) improvement 4 0.0388 * 0.006 ** 0,0016 ** 0.12 0.78 vertical result improvement 0.0298 * 0.048 * 0,0144 ( ! *) 0 *** 0 *** vertical result degradation 0.6918 0.65 0 *** 0.73 0 ( ! ***) swap 0.2788 0.0016 ** 0.0002 *** 0.001 ** 0.18 old formula 0.4498 0.007 ** 0.0168 * 0.001 ** 0.91 A/A control 1 0.67 0.22 0.236 0.54 0.23 A/A control 2 0.48 0.63 0.62 0.14 0.23
A/A control 3 0.89 0.68 0.67 0.85 0.58 correct decisions ratio 0.25 0.625 0.625 0.5 0.125 incorrect decisions ratio 0 0 0.125 0.125 0.25 the alphabet those actions whose appearance in the sequence depends not only on the users preferences, but is also directly affected by the experiment X  X  design. Clicks on vertical re-sults (W) and ads (D) are good examples of such actions. For example, if there are no vertical results on the SERP by design, a user cannot click on them and therefore there will be no (W) action in the user action sequence, that could lead to wrong results. Our baseline metrics Time to First Click and Abandonment Rate also had experiments with results, which are opposite to the ground truth (two experiments for Time to First Click and one -Abandonment Rate ). SpU and pSwitch (classifier with reduced alphabet) metrics did not contradict the ground truth in any of the experiments.
In this paper, we proposed a new metric called pSwitch for evaluating the quality of search engines based on search engine switching prediction. To predict the probability of switching in a particular session, we presented the classifica-tion model that used a rich set of aggregated session features and sequences of search actions as features as well. We se-lected an optimal feature set and evaluated the performance of groups of features. We also presented a new direction of using models predicting of user frustration and a success during search sessions in A/B-testing. To analyze our met-ric, we conducted A/B-testing experiments with real users of search engine Yandex. We draw the attention to the im-portance of the action coding selection and illustrated this on real experiments. Our findings demonstrated that the new metric can be applied to evaluation of different aspects of a search engine. pSwitch is more sensitive in comparison with the recently proposed SpU metric and is also more con-sistent with the ground truth than query-level metrics, such as Abandonment Rate and Time to First Click. Since our work is the first to study an online evaluation metric based on search engine switching prediction, a variety of its exten-sions can be considered in the future. For instance, since our proposed metric can be used for evaluating different types of search engine X  X  changes, not only the ones used in our ex-periments, we plan to demonstrate such applicability in the future experiments. Another direction of future work is the improvement of the classification model, which serves as the basis of our metric and might benefit from using a broader set of features and alternative classification methods.
We would like to thank Irina Orlova for her great contri-bution to this work.
