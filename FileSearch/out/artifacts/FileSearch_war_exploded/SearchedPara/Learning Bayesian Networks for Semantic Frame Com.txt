 Recent developments in Spoken Dialog Systems (SDSs) have renewed the interest for the extrac-tion of rich and high-level semantics from users X  utterances. Shifting every SDS component from hand-crafted to stochastic is foreseen as a good op-tion to improve their overall performance by an in-creased robustness to speech variabilities. For in-stance stochastic methods are now efficient alter-natives to rule-based techniques for Spoken Lan-guage Understanding (SLU) (He and Young, 2005; Lef ` evre, 2007).

The SLU module links up the automatic speech recognition (ASR) module and the dialog manager. From the user X  X  utterance analysis, it derives a repre-sentation of its semantic content upon which the di-alog manager can decide the next best action to per-form, taking into account the current dialog context. In this work, the overall objective is to increase the relevancy of the semantic information used by the system. Generally the internal meaning representa-tion is based on flat concept sets obtained by either keyword spotting or conceptual decoding. In some cases a dialog act can be added on top of the concept set. Here we intend to consider an additional se-mantic composition step which will capture the ab-stract semantic structures conveyed by the basic con-cept representation. A frame formalism is applied to specify these nested structures. As such structures do not rely on sequential constraints, pure left-right branching semantic parser (such as (He and Young, 2005)) will not apply in this case.

To derive automatically such frame meaning rep-resentations we propose a system based on a two decoding step process using dynamic Bayesian net-works (DBNs) (Bilmes and Zweig, 2002): first ba-sic concepts are derived from the user X  X  utterance transcriptions, then inferences are made on sequen-tial semantic frame structures, considering all the available previous annotation levels (words and con-cepts). The inference process extracts all possible sub-trees (branches) according to lower level infor-mation ( generation ) and composes the hypothesized branches into a single utterance-span tree ( composi-tion ). A hand-craft rule-based approach is used to derive the seed annotated training data. So both ap-proaches are not competing and the stochastic ap-proach is justified as only the DBN system is able to provide n-best lists of tree hypotheses with confi-dence scores to a stochastic dialog manager (such as the very promising POMDP-based approaches).

The paper is organized as follows. The next sec-tion presents the semantic frame annotation on the M
EDIA corpus. Then Section 3 introduces the DBN-based models for semantic composition and finally Section 4 reports on the experiments. M
EDIA is a French corpus of negotiation di-alogs among users and a tourist information phone server (Bonneau-Maynard et al., 2005). The corpus contains 1,257 dialogs recorded using a Wizard of Oz system. The semantic corpus is annotated with concept-value pairs corresponding to word segments with the addition of specifier tags representing some relations between concepts. The annotation utilizes 83 basic concepts and 19 specifiers.

Amongst the available semantic representations, the semantic frames (Lowe et al., 1997) are probably the most suited to the task, mostly because of their ability to represent negotiation dialogs. Semantic frames are computational models describing com-mon or abstract situations involving roles, the frame elements (FEs). The FrameNet project (Fillmore et al., 2003) provides a large frame database for En-glish. As no such resource exists for French, we elaborated a frame ontology to describe the semantic knowledge of the M EDIA domain. The M EDIA on-tology is composed of 21 frames and 86 FEs. All are described by a set of manually defined patterns made of lexical units and conceptual units (frame and FE evoking words and concepts). Figure 1 gives the an-notation of word sequence  X  X taying in a hotel near the Festival de Cannes X  . The training data are auto-matically annotated by a rule-based process. Pattern matching triggers the instantiation of frames and FEs which are composed using a set of logical rules. Composition may involve creation, modification or deletion of frame and FE instances. About 70 rules are currently used. This process is task-oriented and is progressively enriched with new rules to improve its accuracy. A reference frame annotation for the training corpus is established in this way and used for learning the parameters of the stochastic models introduced in the next section. The generative DBN models used in the system are depicted on two time slices (two words) in figures 2 and 3. In practice, a regular pattern is repeated suffi-ciently to fit the entire word sequence. Shaded nodes are observed variables whereas empty nodes are hid-den. Plain lines represent conditional dependencies between variables and dashed lines indicate switch-ing parents (variables modifying the conditional re-lationship between others). An example of a switch-ing parent is given by the trans nodes which in-fluence the frame and FE nodes: when trans node is null the frame or FE stays the same from slice to slice, when trans is 1 a new frame or FE value is predicted based on the values of its parent nodes in the word sequence using frame (or FE) n-grams.
In the left DBN model of Figure 2 frames and FEs are merged in a single compound variable. They are factorized in the right model using two variables jointly decoded. Figure 3 shows the 2-level model where frames are first decoded then used as observed values in the FE decoding step. Merging frames and FEs into a variable reduces the decoding complex-ity but leads to deterministic links between frames and FEs. With their factorization, on the contrary, it is possible to deal with the ambiguities in the frame and FE links. During the decoding step, every com-bination is tested, even not encountered in the train-ing data, by means of a back-off technique. Due to the increase in model complexity, a sub-optimal beam search is applied for decoding. In this way, the 2-level approach reduces the complexity of the factored approach while preserving model general-ization.

Because all variables are observed at training time, the edge X  X  conditional probability tables are directly derived from observation counts. To im-prove their estimates, factored language models (FLMs) are used along with generalized parallel backoff (Bilmes and Kirchhoff, 2003). Several FLM implementations of the joint distributions are used in the DBN models, corresponding to the arrows in Figures 2 and 3. In the FLMs given below, n is the history length ( n = 1 for bigrams), the uppercase and lowercase letters F F E , F , F E , C and W re-spectively stand for frame/FE (one variable), frame, FE, concept and word variables:  X  Frame/FE compound variable:  X  Frame and FE variables, joint decoding:  X 
Frame and FE variables, 2-level decoding: Variables with hat have observed values.

Due to the frame hierarchical representation, some overlapping situations can occurred when de-termining the frame and FE associated to a concept. To address this difficulty, a tree-projection algorithm is performed on the utterance tree-structured frame annotation and allows to derive sub-branches associ-ated to a concept (possibly more than one). Starting from a leaf of the tree, a compound frame/FE class is obtained by aggregating the father vertices (either frames or FEs) as long as they are associated to the same concept (or none). The edges are defined both by the frame  X  FE attachments and the FE  X  frame sub-frame relations.

Thereafter, either the branches are considered di-rectly as compound classes or the frame and FE in-terleaved components are separated to produce two class sets. These compound classes are considered in the decoding process then projected back after-wards to recover the two types of frame  X  FE con-nections. However, some links are lost because de-coding is sequential. A set of manually defined rules is used to retrieve the missing connections from the set of hypothesized branches. Theses rules are sim-ilar to those used in the semi-automatic annotation of the training data but differ mostly because the available information is different. For instance, the frames cannot anymore be associated to a particular word inside a concept but rather to the whole seg-ment. The training corpus provides the set of frame and FE class sequences on which the DBN parame-ters are estimated. The DBN-based composition systems were evalu-ated on a test set of 225 speakers X  turns manually annotated in terms of frames and FEs. The rule-based system was used to perform a frame annota-tion of the M EDIA data. On the test set, an aver-age F-measure of 0 . 95 for frame identification con-firms the good reliability of the process. The DBN model parameters were trained on the training data using jointly the manual transcriptions, the manual concept annotations and the rule-based frame anno-tations.

Experiments were carried out on the test set under three conditions varying the input noise level:  X 
REF (reference): speaker turns manually tran-scribed and annotated;  X 
SLU: concepts decoded from manual transcrip-tions using a DBN-based SLU model comparable to (Lef ` evre, 2007) (10.6% concept error rate);  X 
ASR+SLU: 1-best hypotheses of transcriptions generated by an ASR system and concepts decoded using them (14.8% word error rate, 24.3% concept error rate).
 All the experiments reported in the paper were per-formed using GMTK (Bilmes and Zweig, 2002), a general purpose graphical model toolkit and SRILM (Stolcke, 2002), a language modeling toolkit.

Table 1 is populated with the results on the test set for the DBN-based frame composition systems in terms of precision, recall and F-measure. For the FE figures, only the reference FEs corresponding to correctly identified frames are considered. Only the frame and FE names are considered, neither their constituents nor their order matter. Finally, results are given for the sub-frame links between frames and FEs. Table 1 shows that the performances of the 3 DBN-based systems are quite comparable. Any-how the 2-level system can be considered the best as besides its good F-measure results, it is also the most efficient model in terms of decoding complex-ity. The good results obtained for the sub-frame links confirm that the DBN models combined with a small rule set can be used to generate consistent hi-erarchical structures. Moreover, as they can provide hypotheses with confidence scores they can be used in a multiple input/output context (lattices and n-best lists) or in a validation process (evaluating and rank-ing hypotheses from other systems). This work investigates a stochastic process for gen-erating and composing semantic frames using dy-namic Bayesian networks. The proposed approach offers a convenient way to automatically derive se-mantic annotations of speech utterances based on a complete frame and frame element hierarchical structure. Experimental results, obtained on the M E -DIA dialog corpus, show that the performance of the DBN-based models are definitely good enough to be used in a dialog system in order to supply the dialog manager with a rich and thorough representation of the user X  X  request semantics. Though this can also be obtained using a rule-based approach, the DBN models alone are able to derive n-best lists of se-mantic tree hypotheses with confidence scores. The incidence of such outputs on the dialog manager de-cision accuracy needs to be asserted.
 This work is supported by the 6th Framework Re-search Program of the European Union (EU), LUNA Project, IST contract no 33549,www.ist-luna.eu
