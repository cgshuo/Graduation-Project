 In many Web applications, such as blog classification and news-group classification, labeled data ar e in short supply. It often hap-pens that obtaining labeled data in a new domain is expensive and time consuming, while there may be plenty of labeled data in a related but different domain. Tr aditional text classification ap-proaches are not able to cope well with learning across different domains. In this paper, we propose a novel cross-domain text classification algorithm which extends the traditional probabilis-tic latent semantic analysis (PLSA) algorithm to integrate labeled and unlabeled data, which come from different but related do-mains, into a unified probabilistic model. We call this new model Topic-bridged PLSA , or TPLSA. By exploiting the common top-ics between two domains, we tran sfer knowledge across different domains through a topic-bridge to help the text classification in the target domain. A unique advantage of our method is its ability to maximally mine knowledge that can be transferred between domains, resulting in superior performance when compared to other state-of-the-art text classi fication approaches. Experimental evaluation on different kinds of da tasets shows that our proposed algorithm can improve the performan ce of cross-domain text clas-sification significantly. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval: Classification and Clustering. Algorithms, Performance, Experimentation. Topic-bridged PLSA, Cross-Do main, Text Classification In the traditional supervised learning framework, a classification task is to first train a classification model on a labeled training data. Then, the learned model is used to classify a test data set. Generally speaking, under such a framework, the learning algo-rithm relies on the availability of a large amount of labeled data. In practice, high-quality labeled data are often hard to come by, especially for learning tasks in a new domain. Labeling data in a new domain involves much human la bor and is time-consuming. Fortunately, there are often plenty of labeled data from a related but different domain. This may be the case when the labeled data are out-of-date, but the new data are obtained from fast evolving information sources such as the Web blogs. The situation is more serious in current dynamically -changing Web environment. Un-fortunately, traditional learning approaches cannot cope well with such a situation. For example, as described in [16], a text classifi-cation model trained on a Yahoo! directory performs poorly on a Weblog classification problem, si nce the distribution of terms may differ significantly. Another example is newsgroup classifi-cation, where the news items for financial news may have a dif-ferent distribution from the sporting events. It is often true that the performance of such classificati on decreases dramatically along the time dimension. This poses a new learning problem, because we often have to spend much effo rt in labeling the old documents, but it will cost us much to label the new ones. It would be a waste to throw away the old labeled docum ents entirely. In such a situa-tion, how to accurately classify the new test data using as much of the old data as possible becomes a critical problem. In this paper, we focus on the problem of cross-domain text clas-sification. We are given two data sets D L and D U , which are from two related but different domains. Here D L is a labeled data set from old domain, while D U is from a new domain and needs to be classified. We assume that, the class labels in D L and the labels to be predicted in D U are drawn from the same class-label set C . Under such circumstances, our objective is to accurately classify the documents in D U by fully utilizing the old domain data D their labels. We propose a novel approach called Topic-bridged PLSA (or TPLSA for short) for the cross-dom ain text classification problem. Our intuition is derived from the observation that the data in two domains may share some common topics, since the two domains are assumed to be relevant. Our key idea is to extend PLSA [8] to build a topic-bridge and then transfer the common topics between two domains. Using our TPLSA model, the common knowledge between two domains can be extracted as a prior knowledge in the model, and then can be transferred to the test domain through the bridge with respect to common latent topics. Overall, we combine this knowledge and the new knowledge learned from the unla-beled data for text classification in the test domain, even when the test domain has a different dist ribution from the training domain. In particular, we first extend the probabilistic latent semantic analysis (PLSA) [8] to incor porate both labeled and unlabeled data. Our extension allows us to use the hidden variables in PLSA as topics (or classes in classifi cation setting) to bridge the docu-ments in training and test domains, and learn under a joint prob-abilistic model. Such a new model is based on a simultaneous decomposition of the contingency tables associated with term occurrence knowledge in documents from both training and test domains, which identifies the princi pal topics of the training data as well as documents in the test data that support those topics. These topics are then taken as a bridge between the training and test domains. In addition, more prior knowledge from the training data is encoded in a set of constraints imposed between docu-ments, including the must-link c onstraints for documents to be-long to the same cluster and cannot -link constraints for documents to belong to the different clusters . This prior knowledge is learned and applied for learning a classifier on the test data from different domains. According to the above algorithm, an object function is given involving both the likelihood on all the data and the pair-wise constraints on the labeled tr aining data. The EM algorithm is applied to iteratively maximizing such object function to acquire the final categories for the test data. A major advantage of our work is that by extending the PLSA model for data from both training a nd test domains, we are able to delineate nicely parts of the knowledge through TPLSA that is constant between different domains and parts that are specific to each data set. This allows the transferring of the learned knowl-edge to be naturally done even when the domains are different between training and test data . We conduct experiments to test the performance of TPLSA on 11 different datasets and compare our PLSA-based cross-domain text classification algorithm with other state-of-the-art algorithms. Experimental results prove that topic-bridged PLSA can achieve higher performance than other algorithms. The paper is organized as follows. In Section 2, we give a brief review of related work. In Secti on 3, topic-bridged PLSA is pro-posed for cross-domain text classi fication. The evaluation results are shown in Section 4. Section 6 concludes with a summary and suggestion for future work. The traditional classification learning assumes that the class labels are given for training data under th e same distribution as the test data. Two schemes are generally considered, including supervised learning and semi-supervised lear ning. Supervised learning fo-cuses on the case where the labeled data are sufficient. Naive Bayes classifiers [13] and support vector machines [11] are known as two of the most effectiv e methods for supervised text classification. Semi-supervised learning [25] addresses the prob-lem that the labeled data are too few to build a good classifier. It makes use of a large amount of unl abeled data, together with a small amount of the labeled data to enhance the classification. Examples for semi-supervised learning include EM-based meth-ods [17], transductive learning [ 12] etc. Both supervised and semi-supervised classification re quires the labeled and unlabeled data should be under the same dist ribution. However, in our prob-lem, the labeled and unlabeled da ta come from different domains, and their underlying distributions are often different from each other. This violates the basic assumption of traditional classifica-tion learning. Correcting sample selection bias [ 24] is another branch of work related to cross-domain classifi cation. If we assume the domain discrepancy is only caused by sample selection bias , while other facts are ignored, we can simply apply the theory of correcting sample selection bias to solve the cross-domain classification problem. Sample selection bias [7], the Nobel-prize winning work in Economics in 2000, assumes that the distribution difference is resulted from using non-randomly se lected samples from the uni-verse. Sample selection bias wa s firstly introduced in the econo-metrics [7], and then came into the field of machine learning [24]. Zadrozny [24] proposed a two-step approach for correcting sam-ple selection. The probability dens ity is estimated to model the selection of training instances, and then the sample selection bias is corrected based on the estimated density. Several researches addressed the selection probability density estimation, e.g. kernel density estimation [20] and kernel mean matching [9]. Blitzer et al. [2] also analyzed the learning bounds for cross-domain learn-ing based on instance weighting. However, those algorithms did not investigate the rich structure of the test data. Learning from auxiliary data [21] sources addresses classification using small amount of high quality base training data and large amount of low quality auxiliary training data. For cross-domain learning, we can consider the ba se and auxiliary training data come from different domains. In this area, Wu et al. [21] proposed an SVM-based algorithm using bot h base and auxiliary training data. They demonstrated some improvement by using the auxil-iary data. After that, several othe r researches emerged to improve this learning task, e.g. active learning [14], and boosting [5]. Dif-ferent from these works, in our problem, there are not any base labeled data. The probabilistic latent semantic analysis [8] is a popular method for document clustering and rela ted tasks. PLSA was derived from the well-known latent semantic analysis (LSA) for text anal-ysis, and provides solid statistical foundation. In this model, each document is considered as the convex combination of several topics, where these topics or latent semantic variables are ob-tained using the maximum-likeli hood principle. An expansion of PLSA is proposed in [4], which integrates document content and hypertext connectivity for documen t clustering. Similar to other clustering methods, PLSA does not need labeled information, and thus does not consider the available prior knowledge of the do-main. We adapt this model to include the labeled information on topics, thus arriving at a new m odel we call topic-bridged PLSA (TPLSA) that uses the training documents as source to extract the prior knowledge. By making use of this prior knowledge, our algorithm is able to find documents that relevant for each topic even when these documents are from different domains. In this paper, we will incorporate the must-link and cannot-link constraints into our TPLSA model, which borrowed the idea from semi-supervised clustering. Semi -supervised clustering [1] builds clusters under some additional c onstraints provided by a few la-beled data, in the form of must-links (two examples must in the same cluster) and cannot-links (two examples cannot in the same cluster). It finds a balance betw een satisfying these constraints and optimizing the original clus tering objective function. Several semi-supervised clustering algor ithms have been proposed, in-cluding [1][3][10]. Our algorithm is essentially a classification algorithm in which the constraints given by the training data pro-vide a class structure. It will be shown theoretically and empiri-cally that our algorithm works well for cross-domain classifica-tion. In addition, we are interest ed in using the class-label knowl-edge gained from source domain tr aining data to help classify documents in the target domain, which is not solvable by tradi-tional semi-supervised clustering algorithms alone. In our problem, each training instance d is a text document and is assigned to a unique output label from a topic set C = { c A vocabulary of words W = { w 1 , ..., w v } is given, allowing each input document to be represented as a  X  X ag-of-words X  vector via term frequency. These labeled documents are D L = { d where each d l  X  D L is assigned with label c i are D U = { d 1 , ..., d n }, which are the unlabeled documents for prediction. In this paper, we assume the training dataset D from the related but different domain with test dataset D objective is to assign the labels c i  X  C to d u  X  D as possible using the training data D L in another domain. Since the documents in D L and D U are generally composed of terms, we can decompose two parts of the TPLSA on D L and D separately. According to the observation of D L and W , we can perform a PLSA on D L  X  W as: where d l  X  D L is the document in training set. For the test data D U , according to the observation of D we can perform the PLSA on D U  X  W : where d u  X  D U is the document in test set. Note that, D come from different domains. Analogous to [4], our key observati on is that even if the domains are different between the training and test datasets, they are re-lated and still share similar topics from the terms. Rather than applying two separate PLSAs, it is reasonable to integrate the two models into a joint probabilistic model, and use the hidden vari-able z to bridge the training and test domains. This is the essential part of TPLSA as shown in Figure 1. Figure 1. TPLSA Model for Bridging Training and Test Data By Equations (1) and (2), both decompositions share the same term-specific mixing part Pr( z | w ). They relate the conditional probabilities for training documents and test documents: each topic has some probability Pr( d l | z ) of generating a training doc-ument d l with one distribution as well as some probability Pr( d of generating a test document d u with other distribution. As a result, we can merge training docum ents and test documents into an integrated model. Since the mixing topics are shared, the learned decompositions must be consistent with training and test data. We define such mixing topic z as a bridge, and the knowl-edge transferred between traini ng and test domains should be consistent between two domains as we ll. In the knowledge trans-ferring application, such a bridge allows the model to take evi-dences about training data structure into account when predicting the categories of test data. Meanwhile, the structure of test data is also be exploited simultaneously under the coupling model. Thus, the model can capture the inherent distribution of test data. After the decomposition is learned on both training and test data jointly, the class information of the test data can be acquired through the categories of the training data. We integrate the two models using a relative weight parameter  X   X  (0, 1), which is introduced to represent the trad e-off of the weighting between training data and test data. Clearly, if  X  is near 0, we trust the test data more and training data less. If  X  approaches to 1, we rely heavily on the training data. After the integration, we can maxi-mize the following log-likelihood function with the relative weight  X  . To fully utilize the knowledge in the training domain, we apply the idea of must-link constraints and cannot-link constraints used in semi-supervised clustering [3] to our model: for all d same topic, there is a must-link constraint for all pairs ( d constraint. We encode these two ki nds of constraints as follows: z To enforce the must-link constraint for ( d l i , d z In the opposite way, the cannot-link constraint for two doc-Considering the penalty terms of f s ( d l i , d l j ) and f with the log-likelihood function L , the objective function can be expressed as follows. Recall that L is the log-likelihood of the observed data. The sec-ond term of Equation (6) denotes that all documents known to be on the same topic should be grouped into same cluster, while the third term denotes that all documen ts in different topic should be grouped into different clusters. Here  X  1 and  X  2 are the parameters to set the weights for the must -link and cannot-link constraints during estimation. According to our experiments, the performance is not very sensitive to the value  X  1 and  X  2 . Since the objective function L c is non-convex, EM algorithm [6] is used to find a local optimal solution of L c in Equation (6). For the E-step, we use the following formulae for the posterior probabilities of the latent variables associated with each observa-tion: As the optimization on constraints f s ( d l i , d l j ) and f Pr( z | d , w ) in the traditional PLSA. For all pairs d of a topic z on the condition that two documents belong to same C two different topics z i and z j . The terms C s the must-link and cannot-link cons traints, respectively. Specifi-cally, C ( d l , w , z ), C s ( d l i , d l j , z ) and C In the M-step, the probabilities with respect to unlabeled data d can be estimated in the similar ways to traditional PLSA, so that For labeled data d l , after C ( d l , w , z ), C s ( d z ) are determined, the new values of parameters Pr( d l | z ) can be calculated by where  X  1 and  X  2 are the weight parameters for these constraints. Hence, the estimation of Pr( d l | z ) is for the objective function L While considering w is co-occurrence with documents in training dataset D L and with the documents in test dataset D U estimated with the mixing properties: The principal of EM algorithm ensures that the value of the objec-tive function L c as defined in Equation (6) increases monotoni-cally and converges to a local optimization [6]. When the EM algorithm converges, the values of parameters Pr( d to assign each document to a category. We now put the entire model together as topic-bridged PLSA (TPLSA). Our proposed algorithm is an integration on the unified model and the constraints to maximize L c , Pr( d u | z ), Pr( z | w ), and Pr( d l | z ), respectively. As notations, we use c as the function to represent the category of the document or a hidden factor, c ( d ) as the category of document d , and c ( z ) as the category represented by a hidden factor. Our TPLSA algorithm is given in Algorithm 1. Here L c strained likelihood function at i th iteration. As described in [8], the time complexity of the standard PLSA is O( k X N ), where k is the total number of categories and N is the number of total term-document co-occurrence. In the worst case, N is equal to v X  ( m + n ) where v is the total number of terms while m and n is the total number of docum ents in training dataset and test dataset. According to Equations (12), (13), and (14), the total time cost of calculating the constrai nts for Equations (9) to (11) is O ( k X m 2 ). As a result, the total time cost for topic-bridged PLSA is O ( k X m 2 + k X v X  ( m + n )). In this section we empirically evaluate our algorithm for TPLSA for cross-domain text classification and compare it with other state-of-the-art algorithms. In order to evaluate the properties of our framework, we con-ducted experiments with three di fferent text corpora: UseNet news articles (20 Newsgroups 1 ), SRAA 2 and Newswire articles (Reuters-21578 3 ). Since these three data sets are not originally designed for evaluating cross-domai n classification, we split the original data in a way to make the domains of the training and test data different, as follows. First, we observe that all three data sets have hierarchical struc-tures. For example, 20 Newsgr oups corpus contain seven top categories. Under the top categories, there are 20 sub-categories. We define the tasks as top-cat egory classification problems, where our goal is to classify the documents according to the top-one level categories. When we sp lit the data to generate training and test datasets, the data are split based on sub-categories instead of based on random splitting. As shown in Figure 2, for example, A and B are two top categories, each having two subcategories. Consider a classification task to distinguish the test instances http://people.csail.mit. edu/jrennie/20Newsgroups/ http://www.cs.umass.edu/m ccallum/data/sraa.tar.gz http://www.daviddlewis.com/resources/testcollections/ Algorithm 1 Topic-bridged PLSA 
Input : Document-term matrices D L  X  W and D U
Output : Topic c for each d  X  D U . 1: Initialize Pr( d l | z ), Pr( d u | z ), , and Pr( z | w ) randomly. 2: while L c has not converged to a pre-specified value do 3: Estimate the values of Pr( z | d l , w ), Pr( z | d 4: Maximize the values of Pr( d u | z ), Pr( d l 5: end while 6: for each z do 7: | } ) ( | { | max arg ) ( z d c D d z c L c =  X  = . 8: end for 9: for each d  X  D U do 10: )) | Pr( max (arg ) ( d z c d c z = . 11: end for between A and B . Under A , there are two sub-categories A A , while B 1 and B 2 are two sub-categories under B . We split the and A 2 and B 2 are used as the test data. Then, the training and test sets contain data in different s ub-categories. Their domains also differ as a result. The 20 Newsgroups is a text co llection of approximately 20,000 newsgroup documents, partitioned across 20 different newsgroups nearly evenly. Six different da tasets are generated from 20 News-groups for evaluating cross-domain classification algorithms. Each data set contains two top categorie s: one as positive and the other as negative classes. Then, we split the data based on sub-categories, as category comp is treated as positive class and sci is as negative. Four sub-categories comp.graphics , comp.os.ms-windows.misc , sci.crypt and sci.electronics are chosen to be the training data, while five sub-categories comp.sys.ibm.pc.hardware , comp.sys.mac. hardware , comp.windows.x , sci.med , and sci.space to be the test data. The other five data sets are organized in the same way. SRAA is a Simulated/Real/Aviation/Auto UseNet data set for document classification. 73,218 UseN et articles are collected from four discussion groups about simulated autos ( sim-auto ), simulated aviation ( sim-aviation ), real autos ( real-auto ) and real aviation ( real-aviation ). Consider a task that aims to pr edict labels of instances between real and simulated . We use the documents in real-auto and sim-auto as training data, while real-aviation and sim-aviation as test data. Since the training and test sets are different from each other. The auto vs aviation data set is generated in the similar way. Reuters-21578 is one of the most used test collections for evaluating automatic text-categorization techni ques. It contains five top catego-ones. For the category places , we removed all the documents about the USA to make the three categories nearly even. Reuters-21578 corpus also has hierarch ical structure. The data sets are generated for cross-domain classification in the similar ways as what we have done on the 20 Newsgroups and SRAA corpora. places are generated for cross-domain classification. Since there are too many sub-categories, we do not list the details description here. Some preprocessing has been applied to the raw text data. First, we perform the Porter stemmer [18] on terms. Then, stop words were removed. A simple feature selec tion method, Document Frequency (DF) Thresholding [23], is used to cut down the number of features, in order to speed up the classifica tion. Based on [23], DF threshold-ing is suggested, as the method, which has comparable performance with Information Gain or CHI, is simplest with lowest cost in com-putation. In our experime nts, we set the DF threshold to 3. Finally, TFIDF [19] is used for feature weighting. After that, all the docu-ment vectors are converted to a unit vector dividing by its length. The first three columns of Table 3 show the statistical properties of the data sets. The first two data sets are from the SRAA corpus. The next six are generated using 20 News groups data set. The last three are from Reuters-21578 test collection. Kullback-Leibler divergence values [15] between the training and test sets in each test are pre-sented in the second column in the table, sorted in decreasing order from top down. The next column titled  X  X ocuments X  show the size of the data sets used. 
Table 3. Datasets for Cross-Domain Classification, including the Performance by SVM and NBC on the Measurement of KL divergence [15] is calculated as: where Pr L ( w ) is the estimation of feature w on D L estimation of feature w on D U . It can be seen that the Kullback-Leibler divergence (KL) values for all the data sets are much larger than the case when we simply split the same data set into test and training data, which has a KL value of nearly zero. An example of co-occurrence for document and word distribution on real vs. simulated dataset is shown in Figure 3. The first 8000 documents are training documents and next 8000 documents are test documents. Among 8000 doc uments, the first 4000 documents are labeled with real and the next 4000 documents are labeled with simulated . As shown in the figure, trai ning data has different distri-bution with the test data. Furthermore, there are still large common-ness among them. In this work, we propose to enlarge such com-monness to improve the classifica tion over the domains with differ-ent distributions. We further show the performance on these datasets with the other supervised classification algorithms that are typically used for text classification, to illustrate the eff ect of different domains. As shown in Figure 3, the column D U  X  D U shows the performance when we use the labeled training data D L to train the model for classification and test the new data D U while column D U  X  X V shows the best case of performing 10-fold cross-validation on D U . Note that in obtaining the best case for each classifier, the training data are obtained from the labeled data from D U and the test part is also from D for classification with the sa me domain, which gives the best possi-bly result for that classifier. As shown in the table, we can find that using a learned model from the differe nt domain data to classify the test data will significantly decreas e the performance. There exists a big gap between the worst and best cases for each row. 
Figure 3. Co-occurrence of Documents and Words for Dataset Three different types of algorithms are employed for comparison with proposed topic-bridged PLSA algorithm. The first two are supervised learning algorithms including NBC, SVM, semi-supervised learning algorithms TS VM [12] and Kernel Density Estimation (KDE) algorithm [24]. Na X ve Bayesian classifier (NBC) is a well-known supervised learn-ing algorithm, and SVM has been successfully applied in many applications like text classification [11]. For algorithms that use SVM light , we used a linear kernel function as done in [11]. We further compared topic-bri dged PLSA with semi-supervised learning. For semi-supervised lear ning, we implemented the Trans-ductive Support Vector Machines (TSVM) [12]. For TSVM, we used a linear kernel function as in [12] for the semi-supervised transductive learning. We also compare to the algorithm which corrects sample selection bias using Kernel Density Estimati on [24]. The basic classifier for KDE is based on NB and SVM with linear. We call them as KDE-NB and KDE-SVM, respectively. There exist many evaluation metrics for measuring the performance of classification. In this paper, we employ the metric accuracy for comparing different algorithms by considering that it is binary clas-sification. Assume that T is function which maps from document d According to the definition in [22], the accuracy is defined as: Ac-curacy = |{ d | d  X  D U  X  T ( d ) = L ( d )}| / | D U In this section, two compared experiments are conducted for four algorithms. For the proposed topic-bridged PLSA algorithm, we set iteration times is set to 100. Thes e parameters will be studied in parameter tuning section. The first experiment is conducted on 11 datasets listed in Table 3. The experimental results are listed in Table 4 with the measure ac-curacy. As shown in the table, TPLSA can achieve better perform-ance than other algorithms. Unlike the aforementioned algorithms, our algorithm incorporates the training data and test data into a uni-fied model, which can better exploit the knowledge of the training dataset and the inherent structure of test dataset. Since supervised approaches including SVM and NBC do not con-sider the domain difference on traini ng and test datasets, there algo-rithms get worse performance. Semi-supervised approach TSVM can achieve higher performance th an supervised methods, which prove that inspecting the testing data will achieve higher perform-ance. However, these methods assume that the training data and test data are from the same domain. These methods do not fully utilize the structure information contained in test data with different do-main. As a result, the semi-supervised algorithms can not be good enough. Correcting sample selecti on bias [24], the columns under  X  X ernel Density Estimation X  (KDE ), performs only slightly better than NB and SVM classification algorithms, respectively. We be-lieve it is because distribution diffe rence cannot cover all the issues in cross-domain text classification. As mentioned, the performance of supervised learning algorithms including SVM and NBC will be affected according to the distribu-tion of training data and testing da ta since they are from different domains. After performing topic-bridged PLSA, we can exploit training data and test data si multaneously. The improvement over the supervised methods is shown in Figure 4. The X-axis represents different datasets while the KL di vergence is decreased from 1 to 11. The Y-axis represents the impr ovement for our proposed algorithm over SVM and NBC. Generally, we can find that the lower KL divergence is, the less improvement will be achieved. It can be ex-plained the general supervised algorithms can also perform good enough on the datasets with lower divergence. Figure 5. Performance Comparison for Different Data Size For most of learning algorithms, the density of training data will affect the performance. In this experiment, we c onduct experiments on the  X  org vs. people  X  dataset to empirically analyze how classifi-cation accuracy evolves when the size of training data is changed from 10%, 20%, ..., and 100%. We simu late the different sizes of training data by randomly extracting from the training data. The experimental results are shown in Figure 5. We can find that the accuracy curve of TPLSA is above over the curves of other three algorithms. TPLSA can achieve hi gher performance over other three algorithms on different size of tr aining data, specifically on few training data. Furthermore, as s hown in the figure, our proposed algorithm is not very sensitive to th e data size. This also confirms that using the structure information of unlabeled data can achieve better performance. According to our calculating, the KL divergence between different training data and test data is decreased according to the increasing of training data size. As shown in Figure 5, we can also find that the higher KL divergence is, the more improvement will be achieved. In this section, the experiments ar e conducted to tune the parameters and to show that our proposed TPLSA algorithm is not sensitive to these parameters. Each parameter is tuned by fixing other parame-ters. The main two parameters of TPLSA are: the coefficients for the penalty terms. The coefficient  X  1 represents the degree of enforcement that the documents with same category in labeled data should be in same cluster, while the coefficient degree of enforcement that the documents with different categories of  X  1 and  X  2 on the performance, we fix the value of one of them and vary the other to show the ch ange in performance. The experi-ment for tuning is conducted on orgs vs. people dataset. Figure 6 and Figure 7 show the impact of  X  1 and  X  2 on the perform-ance of TPLSA. One character of the curves in Figure 6 is that when  X  is increased from 0 to 150 with interval 10, the performance will be increased firstly and then decreased. This indicates that setting properly will give a better performance. When the value of decreased to close to 0, the impact of the penalty terms for the must-links is removed. As a result, the performance will be decreased, which indicates that the must-link constraints have a strong impact on the performance. Furthermore, as shown in the Figure 6, the value  X  1 is relatively stable in the interval [30, 120]. We believe the TPLSA algorithm is not sensitive to the value  X  1 since the interval is large enough. As shown in Figure 7, changing  X  2 have little impact on the performance. The performance of TPLSA is not sensitive to the value  X  2 . In our experiments, 50 1 =  X  and 15 2 As shown in Equation (3), the parameter  X  is to tune the weight between labeled training data and unl abeled test data. Here we show the different performance affected by the parameter  X  to inspect the sensitiveness of TPLSA.  X  is tuned on 11 different datasets in table 3. The experimental results are s hown in Figure 8. In the figure, X-axis shows the change of parameter  X  which is tuned from 0.1 to 1. As shown in the figure, the perfo rmance will first increase and then decrease when  X  is increased from 0.1 to 1. It is shown that setting with a proper value will achieve higher performance for TPLSA. Furthermore, as shown in th e Figure, if the value of  X  is changed in the interval from 0.4 to 0.8, the performance of TPLSA will be less impacted across different datasets . It shows that our proposed algo-rithm is not sensitive to the parameter  X  . In this work, we set 0.5 for other experiments. The method that we used to find the optimal objective function L defined in Equation (6) is based on the EM algorithm, which is an iterative process that will converge to a local optimum. Figure 9 shows the change of performance with respect to the number of iterations. We observe that the pe rformance grows faster during the first 60 iterations. The performance is nearly constant when more than 100 iterations are performed. This proves that our algorithm will be converged in about 100 iterations. In this paper, we proposed a novel algorithm called Topic-bridged PLSA to handle the cross-domain text classification problem by allowing knowledge learned from doc uments in one domain to be effectively transferred to another. The algorithm extends the tradi-tional Probabilistic Latent Semantic Analysis (PLSA) to integrate the labeled training data and unlabeled test data under a joint prob-abilistic model with the common topic as bridge. We conducted experimental evaluation on 11 datasets and the results show that the proposed algorithm achieves better performance than other state-of-the-art classification algorithms. As a future work, we will consid er other learning methods to ac-quire the parameters used in the TPLSA model, and consider other related classification tasks such as multi-class classification. More-over, we plan to do further investig ation to inspect the inherent rela-tions among text datasets with different but related domains. [1] Basu, S., Banerjee, A., and M ooney, R. J. Semi-Supervised [2] Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., and Wort-[3] Cohn, D., Caruana, R., and Mc Callum, A. Semi-Supervised [4] Cohn, D., and Hofmann, T. The Missing Link -a Probabilistic [5] Dai, W., Yang, Q., Xue, G.-R., and Yu, Y, Boosting for Trans-[6] Dempster, A., Laird, N., and Rubin, D. Maximum Likelihood [7] Heckman, J. J. Sample Selection Bias as a Specification Error. [8] Hofmann, T. Probabilistic Latent Semantic Analysis. In SIGIR , [9] Huang, J., Smola, A., Gretton, A., Borgwardt, K. M., and [10] Ji, X., Xu, W., and Zhu, S. Document Clustering with Prior [11] Joachims, T. Text Categoriza tion with Support Vector Ma-[12] Joachims, T. Transductive Inference for Text Classification [13] Lewis, D. D. Representation a nd Learning in Information Re-[14] Liao, X., Xue, Y., and Carin, L. Logistic Regression with an [15] Kullback, S. and Leibler, R. A. On Information and Suffi-[16] Ni, X., Xue, G.-R., Ling, X., Yu, Y., Yang, Q. Exploring in the [17] Nigam, K., McCallum, A. K., Thrun, S., and Mitchell, T. Text [18] Porter, M. F. An Algorithm fo r Suffix Stripping. Program 14, [19] Robertson, S. E., Walker, S., Beaulieu, M. M., Gatford, M., [20] Shimodaira, H. Improving Predictive Inference under Covari-[21] Wu, P., and Dietterich, T. G. Improving SVM Accuracy by [22] Yang, Y. An Evaluation of St atistical Approaches to Text [23] Yang, Y. and Pedersen, J.P. A Comparative Study on Feature [24] Zadrozny, B. Learning and Ev aluating Classi fiers under Sam-[25] Zhu, X. Semi-Supervised Learni ng Literature Survey. CS TR 
