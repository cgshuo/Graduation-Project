 Question retrieval aims to increase the accessibility of the community Question Answer (cQA) archives and has at-tracted increasing research interests recently. In this paper, we present a novel method for improving the question re-trieval performance by investigating the question term se-lection and weighting as well as reranking results. Different from previous work, we propose a hierarchical question clas-sification method with a sparse regularization to mimc user X  X  question labeling in cQAs. Based on the hierarchical classifi-cation, we explore the local context of the question for term selection and reranking results and then integrating them in-to our proposed general question retrieval framework. The experimental results on a Yahoo! Answers dataset show the effectiveness of our method as compared to existing general question retrieval models and some state-of-the-art methods of utilizing category information for question retrieval. H.3.3 [ Information Systems ]: Information Storage and Retrieval Question Retrieval; Query Term Selection and Weighting; Question Classification; Question Reranking Community Question Answering services (cQAs), such as Yahoo! Answers ( answers.yahoo.com ) and Baidu Zhidao ( zhidao.baidu.com ), have become popular alternatives for users seeking information when their queries failed in tradi-tional search engines. In cQAs, people can ask questions in natural language expressions instead of segmented keywords, and other community participants would give answers to the posed questions online or offline. Since the cQA repository has accumulated a tremendous amount of valuable human generated question / answer pairs, one can also search the archived questions to find semantically equivalent or relevant questions and reuse the corresponding answers properly. To this end, question retrieval is an important issue which has attracted a great deal of interests from the research and in-dustry communities [15, 31, 27, 7, 29, 16].

The current approaches for question retrieval usually con-sider the query terms X  global statistics from the question archives. Table 1 presents some of the top retrieval results, shown as d 1 ;:::;d 4 , of the user query  X  best asian buffets in chicago? What are some of the best asian buffets restaurants in the ChicagoLand area?  X  by using the translation based language model [31]. However, for a particular question to search, there are some essential aspects of query meanings that are hidden in its local context. For instance, the query in Table 1 means to find the asian  X  X uffets X  in the place named  X  X hicago X . Hence, the terms  X  X uffets X  and  X  X hicago X  which reflect the  X  X ssence X  [21] of the local query should be more informative than the other peripheral terms. By fo-cusing on these terms, it can effectively raise the ranking of the truly relevant questions ( d 2 and d 4 ) and lower the rank-ing of those not very relevant ones ( d 1 and d 3 ). Besides, for the same query, two closely related questions which both capture the query meaning ( d 2 and d 4 ) should get similar and smoothing retrieval scores, no matter how different they look like in text.

We notice that when people ask questions on cQAs and leave them to be answered by other participants, they are often required to select a category label from the hierar-chy for the posed question (see the rightmost column of the Table 1). For instance, most of the archived questions un-der  X  X ining Out.United States.Chicago X  are about dining out choices in the specific area of Chicago. The hierarchical category label path reveals some essential semantic of the question. Hence, it inspires us to utilize both the archived questions and the concept hierarchy to build the hierarchical classifiers for question classification to mimic user X  X  concept labeling in cQAs. Different from previous work of explor-ing question X  X  category information for improving question retrieval [7-9], our motivation has two aspects: First, in the local context of a query, some terms (words) are highly in-volved with the category labels which reflect the  X  X ssence X  of the query meaning. Therefore we explore the hierarchi-cal question classification process to select the informative words and estimate their contributions to the classification. For instance, consider the query in Table 1, according to the category label, it can be deduced that the term  X  buffet-s  X  is more helpful for distinguishing the top level category  X  Dining Out  X , while the term  X  chicago  X  is very useful for fig-based language model [31].
 uring out the place requirement  X  Chicago  X  category at the leaf level. Second, if one question is found to be relevant to the query, the other questions which are closely related to it, both with very similar text content and in the same cate-gory hierarchy, should also tend to be relevant to the query. Therefore, we propose a reranking method for post process-ing the retrieval scores, ensuring that all semantic related questions will receive smoother scores, which can further improve the retrieval performance.

The overview of our proposed method is illustrated in Fig-ure 1. For an input question, we learn a set of sparse classi-fiers for the hierarchical category labels based on the feature (keyword) space which comprises the set of keywords ex-tracted from the archived questions. By selecting the query terms occurring in the sparse classifiers, the corresponding maximal coefficients are rendered as the local weights of the informative words. Then we integrate the local weights of the selected terms and the global weights of all terms in a general framework for computing the initial ranking scores. Lastly, we rerank the initial results by restricting the new ranking scores to be consistent with the initial scores as well as the inter-question similarity.

Our contributions are three-fold: First, we investigate the techniques of term selection and weighting for question re-trieval. Our method is based on a novel question classifica-tion approach with a hybrid sparse regularization to explore the local context information; Second, we propose a gener-al cQA question retrieval model to integrate both of global statistic and local context information to improve the per-formance of question retrieval; Third, based on the question classification, we propose a reanking method for question retrieval, to our best knowledge, we are the first to intro-duce the reranking approach into question retrieval. In our reranking method, both of the text content and the catego-ry hierarchy of cQAs are exploited to smooth the ranking scores. Experimental results on a Yahoo! Answers dataset for question retrieval show the effectiveness of our method as compared to the strong baselines and some state-of-the-art methods that utilize the category information.

The rest of the paper is organized as follows: Section 2 in-troduces some related work for question retrieval. Section 3 describes the hierarchical classification based term selection and weighting process, and Section 4 details our proposed question retrieval framework and the reranking approach. In Section 5 we present the experiment setups and results. Finally, we conclude the paper in Section 6.
Jeon et al. [15] studied to retrieve relevant Q/A pairs for general questions in a great number of question types in cQAs. They compared several general retrieval models, such as Okapi BM25 model [25], language model [32] and translation model [4] for question retrieval and found that the translation model which considers the transferring prob-ability from document word to query word could achieve the best performance because it alleviates the lexical chas-m (gap) problem. Xue et al. [31] verified that by combining the translation probability and the collection smoothing, the resultant translation based language model could further im-prove the retrieval performance significantly.

Besides the general models mentioned above, researchers have proposed some ways for improving the retrieval per-formance. Zhou et al. [35] found that phrase based trans-lation model outperforms the general unigram based trans-lation model. Bilingual translation technology was also ex-ploited for reducing the semantic ambiguity [36] of query words. The work of [28] redefined some structural simi-larities on syntactic trees for question matching in cQAs. Th eir another work [29] was proposed to solve the complex multi-sentence question retrieval by question segmentation technology. Zhang et al. [33] explored the dependency re-lationship of query term pairs based on the hypothesis of similar weights. Bendersky et al. [1, 2] proposed parame-terized concept weighting method through pseudo-relevance feedback for verbose queries.

Several work utilized the hierarchical category informa-tion for question or web document retrieval. Ming et al. [23] used three kinds of domain(category)-level evidences, name-ly, general collection, sub-domain Jensen-Shannon divergence and term entropy for re-weighting the query terms in the relevance scoring process by exploring the category infor-mation of archived questions. Cao et al. [7, 8, 9] studied several ways to utilize the category information from both sides of the query and archived questions, such as leaf cat-egory smoothing and query classification enhancement. Cai et al. [6] utilized the category information for estimating the latent topics of questions and integrated the Latent Dirichlet Allocation (LDA) with translation based language model for question retrieval. Different from the previous work, we uti-lize the query classification process for query term selection and weighting. It can be considered as the local probability of the term for revealing the essence of the question. In the supervised framework, Jin et al. [18] learned to rank query terms based on the correlation between word frequency and category information of hierarchically organized documents such as ImageCLEF dataset, while Wang et al. [30] utilized a limited number of question types ( such as Yes/No , Rea-son and Opinions ) which reflect users X  intentions to rank the retrieved questions based on their types.

There exist some work [10, 11, 22, 20] propose to smooth the scores of retrieved documents which are topically related. The clustering hypothesis states that closely related docu-ments tend to be relevant to the same request [14]. Based on this, Diaz proposed a score regularization method in ad hoc retrieval for smoothing the retrieved scores of closely related documents [10, 11]. The experimental results in their paper-s show after the reranking the performance improvement is consistent and significant. To our knowledge, we are the first to propose the reranking technology for score smoothing in question retrieval, and use a combination of both documen-t similarity and category based affinity, which can capture more accurate metric for the inter-document consistency.
In this section we will give the details of our hierarchical classification method with sparse regularization terms, and the process of the term selection and weighting respectively.
Hierarchical models have been found to be more efficient and sometimes more effective than their flat counterparts in text classification when the class labels form a hierar-chy [19]. In this section, we will present an effective hi-erarchical method for question classification. First of all, some notations are given: let A ( i ) and S ( i ) be the ances-tors and siblings of a tree node i in the hierarchy respec-tively, and A + ( i ) = A ( i ) of observations (with d -dimensional feature space) and let Y = { 1 ; 2 ;:::;m } be the set of class labels of the hierarchy (except the root label). For community question classifica-tion, we make use of the archived question titles, question contents and best answers, as well as the metadata from hierarchical context for training the models. The training process is defined as follows: given a set of training ques-tions D = { ( x 1 ;y 1 ) ;:::; ( x N ;y N ) } ;x k  X  X ;y { 1 ;:::;N } , we build m classifiers { w i } m i =1  X  R d , each for a class node i in the hierarchy, by solving the following op-timization problem, Here, R ( w ) is a regularization term preventing the model from over-fitting, { k } are slack variables. For the correct leaf node label and all its ancestors from bottom to up along the hierarchy, it will penalize the cases when the margin of the correct output i and other offensive labels j in the current level is less than 1. C 1 controls the balance of the two parts.

In this paper, we adopt a hybrid regularization method, which can both increase the discriminative power of hier-archical classification and obtain a sparse set of important features(terms) for exploring the local context information of query in question retrieval. The regularization terms are as follows: Here, the first term is L 1 regularization to sparse the classifi-cation models and C 2 controls the degree of sparseness, while the second and third terms are orthogonal constraints for hierarchical regularization. The matrix K = { K ij } ;i;j = 1 ;:::;m is the parameter matrix of orthogonal transfer[34]. The orthogonal constraint has been shown to result in better performance as compared to other regularization types since it can enforce category node ancestors on different hierarchy level and siblings on current level to use distinct features. However, in the case of multi-class hierarchical classifica-tion, the support vectors are dense, while after introducing the parameter matrix K in orthogonal regularization, pa-rameters are even denser. Hence from the perspective of sparse learning and to reduce the space for storage and clas-sification time, we prefer to learn a parsimonious model in which every classifier is based on a sparse linear combination of features. Therefore we introduce L 1 sparse regulariza-tion for parameters penalization in our hierarchical learning framework. More importantly, through the sparse represen-tation we can obtain the good and discriminative terms for question classification, which can lead to automatic term selection for question retrieval.

We use the maximum likelihood estimation for parameters learning. The second part of Equ. 1 is differentiable, while the regularization in Equ.2 is a little more complicated since it contains the non-differentiable L 1 part. We propose a two-step algorithm to deal with the sparseness problem. That is , for each iteration t we first put aside the L 1 term and use the regularized dual averaging (RDA) method [34] for updating on the orthogonal regularization terms and get the temporary result w t + 1 2 . Then we solve the L 1 regularization by using the FOBOS updating [13], i.e., the new parameters of iteration t + 1 is obtained by, where e is a small positive threshold controlling the spare-ness and [ z ] + denotes max { 0, z } .

With trained models, query q is classified by the top-down manner as follows: initialize the question label with the root label i = 0, and then recursively search its child nodes to find the one with the maximal dot product, i.e., i := argmax j 2 Child( i ) w T j q , until it reaches a leaf node. The final leaf category label is assigned to query q .
The category label assigned by the asker often reveals his/her real information needs to some extent. Therefore our hierarchical classification is a kind of prediction of the semantic of asker X  X  search, and the query terms which are helpful for figuring out the correct category label should be highlighted in the procedure of question retrieval.
Suppose there is a total of d unique keywords V = { v 1 ;:::;v (except for stopwords) extracted from archived questions for training, then the classifiers described in Section 3.1 admit the representor theorem [26] of the form, That is, each classifier w i of the hierarchical classification is some linear combination of the training instances x k by parameter ik , for k=1,...,N. Thus it is with the same di-mension to the input feature space V and has a number of d parameter coefficients &lt; w i 1 ;:::;w id &gt; . If one feature v is more helpful for classifying a category i , its correspond-ing coefficient w ij would receive a larger value. Hence, our query term weighting process is as follows:
Given a query q , we use the trained models to automatical-ly classify it to a leaf category label. For instance, the query  X  good thin crust pizza dillivery in chicago?  X  is classified to category label path  X  Dining Out.United States.Chicago  X . Then for query term t , we search all the classifiers appeared in the label path of the hierarchy to find its maximal coef-ficient t , which indicates the most contribution of term t to the question classification. It can be described with the following equation: where index t denotes the index of query term t in the feature space V . For instance, in label path  X  Dining Out.United S-tates.Chicago  X , the term X  pizza  X  X ets the maximal coefficient in the category node  X  Dining Out  X , and the term  X  chicago  X  receives the maximal coefficient in the category node X  Chica-go  X . Finally these maximal values are regarded as the raw value for term weighting. Since we train a serial of sparse classifiers, for a specific query there might be very few word-s appearing in the feature space of classifiers. Thus it can lead to the automatic selection of informative terms for the query. We summarize the above process into the following Algorithm 1.
 Al gorithm 1 Hierarchical Classification based Term Weighting feature space V = { v 1 ;:::;v d } , the query question q Output: the term weights 1: label leaf = HieClassify(q, w ) 2: for each t  X  q do 3: if t is in feature space V then 6: else 7: t = 0 8: end if 9: end for 10: return
S ince the value of obtained in Equ.5 is relatively small (with an order of magnitude 10 4 in our experiments) and not normalized, we use a Gibbs-like function to transform to be a probability distribution for the local weight of informative query terms.
 where is an empirical parameter controlling the weight significance of the category information. For the peripheral query terms which do not appear in the feature space of clas-sifiers, we still assign a small nonzero constant 1 = to them by the exponential transformation to prevent it from collapsing to 0.
After obtaining the local weight of the query term, we present a systematic framework to seamlessly integrate it to the global term statistic which is usually a linear com-bination of the term generating probability P ( t | d ) of the question d and the smoothing factor P ( t | C ) of the whole question collection C ,
Here, S 0 q;d denotes the ranking score of archived question d for query q = t 1 ;:::;t j q j . For each term t in query q , we combine Equ.6 with the term frequency tf t;q and get the n ew ranking score S q;d as follows:
In Equ.8, we integrate selected query term weight into the general framework. This is crucial for some short queries where each term appears only once, where we can not de-termine the term weight according to the term frequency.
In this subsection we instantiate our general framework with some existing question retrieval models, which are im-plemented in our experiment for comparison. 1. Okapi BM25 Model
The Okapi BM25 model is proposed by Robertson et al. [25] and first used for question retrieval by Jeon et al. [15]. Al-though the Okapi BM25 model is not in the probabilis-tic form, Equ.8 still holds by setting P ( t | d ) = exp( w P ( t ) = w q;t and = 0 (with no collection smoothing), where Here, k 1 , k 3 and b are empirical parameters, N is the num-ber of questions in the whole collection, f t is the number of questions containing word t , tf t;q and tf t;d are the frequen-cy of term t in query question q and archived question d , respectively. W d is the question length of d and W A is the average length of all questions. 2. Language Model
The language model (LM) is used for question retrieval in previous work [15, 12]. The basic idea of language mod-el is multiplying all terms X  generating probabilities in query q from the document d . The generative probability is of-ten estimated from a multinomial distribution with Jelinek-Mercer smoothing [32]. In our framework, we can easily integrate with LM by simply letting P ( t | d ) = P ml ( t P ( t | C ) = P ml ( t | C ), where He re, P ml ( t | d ) and P ml ( t | C ) are the maximum likelihood es-timate of word t in question d and collection C , respectively. 3. Translation Model
It is reported [4, 17, 24, 31] that translation model (TR) yields superior retrieval performance than other methods because it can explores the word translation probabilities to alleviate the lexical gap between the query and archived questions. The scoring in TR is also easily integrated into
Fi gure 2: The reranking of initial retrieval results. our framework by letting We follow the work of [15] to obtain the word translation probability T ( t | w ) from word w in archived questions to word t in the query. 4. Translation based Language Model
In [31], they find that combining LM and TR could achieve better performance than using either of them only, which results in the translation based language model (TRLM). It is also easy to seamlessly match our framework by setting where the parameter controls the balance of the LM and TR components.

For all the above general models, we will compare the effectiveness of the baseline and our corresponding model by integrating the local term weight.
The initial retrieval scores obtained by Equ.8 still suffer from some problems. For instance, we train a specific classi-fier for every category in the hierarchy independently, thus the resulted weights of the query words from the hierarchi-cal classification process might be inconsistent and exhibit different scale of values. In other aspects, we compute the retrieval scores of two questions independently. These two questions might vary in length and text but can both capture the query meaning more or less. The correlation of question scores is largely been ignored and the obtained scores might be not smooth. For community question retrieval, as illus-trated in Figure 2, we believe that if one question (e.g., d is found to be relevant to the query, the other questions (e.g., d 3 ) which is closely related ( both in text content and in category distribution ) to it should also tend to be rele-vant to the query with a large probability. It is clear these two similar questions should receive smoother scores for the same query. Thus we adopt the score smoothing method[10] for reranking the initial retrieval scores. In this paper, we measure the question relatedness by both the document text similarity and category based affinity, and this metric is re-garded as the inter-question consistency in score smoothing technology.
Let y  X  R n be the initial score vector of the top n returned questions, and f  X  X  n be the regularized score vector. Then for score smoothing [10] we minimize the following equation, Here,  X ( f ) is the cost function associated with the inter-question consistency of the scores, whose value will be high if closely related questions have very inconsistent scores. The other cost function E ( f ) is introduced for measuring the con-sistency with the original scores, whose value will be high if questions have scores very inconsistent with their original scores.

For measuring the inter-question consistency of the re-turned questions, current methods usually use the text based similarity , such as the cosine similarity and language mod-el based similarity [11]. However, it is not necessary that two text similar questions are talking about the same thing. For example, the question  X  best asian buffets in chicago?  X  and  X  best asian buffets in London?  X  share most of the word-s but with different meanings. In this paper, we use two kinds of similarities for measuring the inter-question consis-tency of the returned question i and j : the cosine similarity VecSim( i;j ) between the TF-IDF (term frequency -inverse document frequency) vector of the documents and the cat-egory based affinity CatSim( i;j ), where the latter one is inferred from the hierarchical classification process. Specif-ically, for each question i , it has a probability p i k belonging to the category k , for k = 1 ;:::;m , according to the result of the hierarchical classification. Thus, the category based affinity CatSim( i;j ) between document i and j is computed as: We linearly combine the two similarities to get the inter-question consistency of question i and j , and the optimal will be tuned for best retrieval perfor-mance. In the experimental section, we will show that using the combination of these two similarities can achieve better retrieval performance than using either of them alone.
For the top n returned questions, we compute the n  X  n affinity matrix W , where W ii = 0. Like [11], we will consider two kinds of graphs of the affinity matrix W . One is a dense graph with edges connecting any nodes i and j . The other is a sparse graph with only edges connecting the nearest-n neighboring nodes. The latter graph may be more proper since we are more confident about the smoothing of closely related questions other than the remote ones.

Based on the graph W , the inter-question consistency cost function  X ( f ) is defined as follows: where D is a diagonal normalizing matrix with the diagonal elements D ii = document i is to all the other documents.
 For measuring the consistency with the initial scores, the E ( f ) is defined as: where y i denotes the initial retrieval score of the question i .
To obtain the optimal regularized scores, we would like to minimize the objective: and the closed form solution [10] is, where = 1 1+ i s a smoothing parameter.
We have crawled more than 1 million questions from nine top categories from Yahoo! Answers. These contain one of the most popular category Sports and the least popular one Environment , and other categories such as Dining Out and Travel . We randomly choose 120 questions as queries, using 20 of them as validation data to tune the parameters and the rest 100 questions as testing data to evaluate the retrieval performance. For the query question, we use on-ly the question title field. While for the archived questions in the community QA archives, we concatenate the three fields (question title, question content and best answer) to-gether as they are all available at the searching time. Using them together can reduce the lexical gap, which achieves better retrieval performance as reported in [23]. To obtain the ground truth, we ask human annotators to judge the relevance. For fairness we pool the top 20 results returned from all models and remove the duplicates, thus the human annotators can not figure out which model generates the current result. Each retrieved question is annotated by two persons. If there is a conflict, we refer to the third person to make the final decision. Finally, we obtain a total of 4147 archived questions that are labeled as  X  relevant  X  or  X  irrele-vant  X . For the hierarchical classification, we randomly split the whole question collection into ten subsets, and use eight of them for training, one of them for tuning the empirical parameters, and the last one for testing.
There are some parameters to be tuned in our experi-ments. First for the hierarchical classification model we set K ii = 1, K ij = 0 : 05 ;i  X  = j , C 1 = C 2 = 1 and they get the best classification accuracy in the testing data. For the parameters in the retrieval model, we set them on the validation data for the best retrieval performance. Specif-ically, for Okapi BM25 model, we set b = 0 : 75, k 1 = 1 : 2 and k 3 =  X  following the original work of [25] and find that it works well. For language model, translation model and translation based language model, we find that by setting = 0 : 2 and = 0 : 8 they obtain the best performance. The effect of other parameters such as for transferring the raw feature weight to the local term importance, the number of neighbors used for reranking, and for controlling the T able 2: Results for some general question mod-els and ours by integrating the term selection and weighting as well as reranking from hierarchical clas-si cation. Here, `chg' denotes the boosted perfor-mance of our full model to the corresponding base-line. We have veri ed that all the improvements are in statistical signi cance at 0.95 con dence interval using the t -test.
 wBM 25+rerank 0. 2496 0. 5520 0.4 320 0. 3380 wT RLM+rerank 0. 2511 0. 5560 0 .4190 0. 3315 b alance of text and category similarities, will be carefully studied respectively in Section 5.4.

For evaluation metrics, we use Mean Average Precision (MAP) and Precision@N (P@N) for comparing the retrieval performance of different models. MAP favors the methods which return relevant questions early. P@N computes the fraction of the top-N returned questions which are relevant, and we use N=5,10,20 in our experiments.
To evaluate the effect of different components of our method, for each baseline (BM25, LM, TR and TRLM introduced in Section 4.2) we compare it to two instances of our model-s: one is the model with term selection and weighting by local context but without reranking (e.g., wLM), the oth-er is the full model (by adding the question reranking, e.g., wLM+rerank). The results are presented in Table 2. From the results we have the following observations: 1) By exploring the term selection and weighting as well as reranking from the automatic hierarchical classification, the best performance of each metric ( MAP (0.2511) and P@5 (0.556) in weighted TRLM , P@10(0.432) and P@20 (0.338) in weighted BM25) is achieved with our models, and our model outperforms the corresponding baseline model in all metrics. The improvements are all statistically significant, which demonstrates the effectiveness of our strategy of uti-lizing category information for improving question retrieval. 2) In each baseline, we find that by merely selecting and weighting the informative terms through exploring local con-text, the performance gains significant improvement for most metrics. By further reranking the initial results, the im-provement becomes even bigger and more significant. It demonstrates that by smoothing the retrieval scores of close-ly related questions, the term selection and weighting strat-Table 3: Results for our method and some state-of-the-art methods which also utilize category infor-mation by integrating with TRLM. Here,  X  ,  X  and  X  represent the improvement of our method has s-tatistical signi cance at 0.95 con dence interval to LS, DE and QC, respectively.
 M odels MA P P@5 P@1 0 P@ 20 DE[2 3] 0. 2397 0. 5260 0 .4040 0.3 140 egy from the hierarchical classification becomes more con-sistent and useful for finding relevant questions to the users X  queries. 3) Similar to the conclusion of [31], we note that the best performance of the baselines is achieved in translation based language models, which combine both the word translation probabilities and the smoothing effect from the whole ques-tion collections. By integrating our term selection and re-anking, we still can achieve significant improvement over the baselines for all metrics. We also observe that the empirical Okapi BM25 model performs the worst, perhaps it has more parameters to be tuned and gets no collection smoothing. However, by exploring the hierarchical classification the im-provement is more prominent (as much as 15% improvement for MAP) in our method and even gets the best performance in P@10 and P@20 as compared to other models.

Since there are some former researches utilizing the cat-egory information for improving the question retrieval per-formance, we also compare our method to theirs. We imple-ment three state-of-the-art methods as follows: LS [7]: using leaf category label of archived questions for query term smoothing; DE [23]: term weighting in the ranking score computing pro-cess by exploring three domain evidences of archived ques-tions; QC [9]: classifying the query to some leaf categories and multiplying the probabilities of these categories to the ini-tial score in historical questions.
 We instantiate these category enhanced methods by combin-ing them with the TRLM module which achieves the state-of-the-art retrieval performance. The experimental results are given in Table 3. The experimental results show that compared with the existing methods of exploiting category information, our method still obtains better performance in all the metrics significantly. Especially for the top five re-trieved questions, our method gets much higher results than all others. This verifies that for top results our strategy of exploring the automatic classification process is more effec-tive than other methods, such as enhancing by multiplying the classification probability (QC) or smoothing by catego-rization information of historical questions (LS, DE).
To evaluate the effect of sparse regularization we compare two kinds of regularization terms in the automatic classifi-cation process, the default L 2 and the hybrid regularization presented in Section 3.1. The MAP results for all our mod-els by using the two regularization terms are presented in T able 4: MAP for the L 2 and hybrid regularization in the proposed classi cation framework.
 Fi gure 3: Retrieval performance by varying the pa-rameter in weighted TRLM model.
 Table 4. We find that there are 3.4 terms selected in aver-age in the sparse model while more than 10 words are used in the denser model by L 2 regularization. By restricting the term selection and weighting to a sparse feature set in clas-sifiers through the hybrid regularization, the performances even get improved. It shows that other query words which contribute little in the hierarchical classification have limit-ed or negative impacts on the term weighting for question retrieval. By using the L 1 regularization term we get a s-parse representation of classifier parameters, which just use a small subset of indeed helpful features. Thus we can fo-cus on highlighting the really informative query terms for clarifying the  X  X ssence X  of the search.
The parameter for transferring the raw feature weight to the local term weight in our model is , and its impact to re-trieval performance in weighted TRLM model is illustrated in Figure 3. We find it achieves the best performance con-sistently when we set = 2000 for all metrics. Too smaller values put little impact on informative words selection and too bigger ones might overweight some terms and neglect other auxiliary words which are also useful for clarifying the specific information needs of users.
In the score smoothing process of the reranking, how many neighbors should we keep for a node (question) is a nontriv-ial problem, since we usually concern more about the close-ly category related questions rather than those not related ones. Thus in this subsection we evaluate the effect of the neighborhood graph sparsity for the retrieval performance. By varying the number of neighbors of each node, we can get different neighborhood graphs with various sparseness. For each query, we get the top 100 questions returned by the corresponding model. We compute the inter-question con-sistency for all pairs of questions, resulting in a full graph. Then we choose the nearest neighborhood subgraphs with Fi gure 4: MAP performance as a function of the number of neighbors. Fi gure 5: MAP performance as a function of the value of . different number of neighbors. The experimental results are illustrated in Figure 4. We note that by increasing the num-ber of neighbors for score smoothing, the MAP rises up at first, and after some point it drops sharply. It shows that keeping a suitable degree of graph sparsity benefits the score smoothing most.
In the measures of inter-question consistency, we use two kinds of similarities: document vector similarity and catego-ry based affinity. The weights of the two similarities can be changed by varying the value of parameter . Figure 5 shows the tendency by using different . We find that the category based affinity is more important in reranking process since the performance drops fast by decreasing the proportion of the corresponding metric. We also observe that the MAP in the combined similarity is significantly better than using either of them alone (by setting to 0 or 1), which verifies the optimized choice of the similarity measurement.
To intuitively study the reason why our method gets bet-ter retrieval performance, in this subsection we present the denotes the result is judged as ` relevant ' to the query question. sea rch results of two specific queries. For the first query  X  I need a free version of reliable video converter?  X , the top three questions retrieved by three question retrieval mod-els are listed in the upper half of Table 5. The first re-turned question in TRLM model and the second returned question in QC [9] model are irrelevant results since they are about  X  X ideo editor X  rather than  X  X ideo converter X . In TRLM and QC model, all query words have the same local weights, while apparently the term X  X onverter X  X s more useful for classifying the query to the right category  X  X omputers &amp; Internet.Software X . Thus, the local query term weight-ing can help our model focusing on the  X  X ideo converter X  in that specific category. Thus our method achieves better performance than other two methods.

The second query  X  best asian buffets in chicago?  X  is also presented in Table 1, and the returned questions are list-ed below in Table 5. We use this case to illustrate the merit of our method compared to previous question cate-gorization approach, i.e., QC method. The table shows that the manual label of the question is  X  X ining Out.United S-tates.Chicago X . However, since there are many questions in the  X  Travel  X  category asking dining choices, when we classify the query automatically, it is misclassified to the category  X  X ravel.United States.Chicago X . In this circumstance, the QC method gets irrelevant results at the top rank, due to retrieving questions in incorrect category  X  X ravel.United S-tates.Chicago X . On the contrary, instead of totally relying on the classification result for question retrieval, our model only using the term weights estimated from the classifica-tion process. Although the automatic classification is not consistent with manual label, the query terms  X  X hicago X  and  X  X uffets X  are still the most informative terms in the classifi-cation process, therefore they are selected and still get larger coefficients than other query terms. As a result, our model get better results compared with QC. It shows the flexibility of our method, which helps us achieve more improvement on question retrieval performance.
In this paper, we presented a novel question retrieval method for improving the retrieval performance by exploring the cat-egory hierarchy in community question answering service. We proposed a sparse hierarchical classification method to mimic the manual labeling process for clarifying the local context of the query. We selected the informative query terms and obtain the corresponding local weights from the automatic classification process by finding the maximal co-efficient of classifiers. We proposed a reranking method for smoothing the related questions of the initial results. The experiments on the real world Yahoo! Answers dataset verify the effectiveness of our method as compared to the baselines and some other state-of-the-art methods of utilizing catego-ry information for question retrieval. This work was partially supported by the National Basic Research Program of China (973) under Grant 2014CB347600, the Program for New Century Excellent Talents in Universi-ty under Grant NCET-12-0632, the NSFC Grant 61370157 and Shanghai Science and Technology Development Fund-s(13dz2260200, 13511504300). [1] M. Bendersky and W. B. Croft. Discovering key [2 ] M. Bendersky, D. Metzler, and W. B. Croft.
 [3] M. Bendersky and W. B. Croft. Modeling higher-order [4] A. Berger, R. Caruana, D. Cohn, D. Freitag, and V. [5] J. Bian, Y. Liu, E. Agichtein, and H. Zha. Finding the [6] L. Cai, G. Zhou, K. Liu, and J. Zhao Learning the [7] X. Cao, G. Cong, B. Cui, C. S. Jensen, and C. Zhang. [8] X. Cao, G. Cong, B. Cui, and C. S. Jensen. A [9] X. Cao, G. Cong, B. Cui, C. S. Jensen, and Q. Yuan. [10] F. Diaz. Regularizing ad hoc retrieval scores. In [11] F. Diaz. Regularizing query-based retrieval scores. In [12] H. Duan, Y. Cao, C.-Y. Lin, and Y. Yu. Searching [13] J. Duchi and Y. Singer. Efficient online and batch [14] N. Jardine and C. J. V. Rijsbergen. The use of [15] J. Jeon, W. B. Croft, and J. H. Lee. Finding similar [16] Z. Ji, F. Xu, B. Wang, and B. He Question-Answer [17] V. Jijkoun and M. de Rijke. Retrieving answers from [18] R. Jin, YC. Joyce and S. Luo Learn to weight terms in [19] D. Koller and M. Sahami. Hierarchically classifying [20] O. Kurland. Re-ranking search results using language [21] K. T. Maxwell and W. B. Croft. Compact Query [22] Q. Mei, D. Zhang, and C. Zhai. A General [23] Z. Ming, T.-S. Chua, and G. Cong. Exploring [24] S. Riezler, A. Vasserman, I. Tsochantaridis, V. O. [25] S. Robertson, S. Walker, S. Jones, M.
 [26] B. Scholkopf, and A. J. Smola. Learning with Kernels: [27] M. Surdeanu, M. Ciaramita, and H. Zaragoza [28] K. Wang, Z. Ming, and T.-S. Chua. A syntactic tree [29] K. Wang, Z. Ming, X. Hu, and T.-S. Chua.
 [30] W. Wang, B. Li, and I. King. Improving Question [31] X. Xue, J. Jeon, and W. B. Croft. Retrieval models [32] C. Zhai and J. Lafferty. A study of smoothing [33] W. Zhang, Z. Ming, Y. Zhang, L. Nie, T. Liu, and [34] D. Zhou, L. Xiao, and M. Wu. Hierarchical [35] G. Zhou, L. Cai, J. Zhao, and K. Liu. Phrase-based [36] G. Zhou, K. Liu, and J. Zhao Exploiting Bilingual
