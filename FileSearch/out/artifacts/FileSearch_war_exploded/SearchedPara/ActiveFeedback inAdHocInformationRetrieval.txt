 Information retrie val is, in general, an iterati ve search process, in which the user often has several interactions with a retrie val system for an information need. The retrie val system can actively probe a user with questions to clarify the information need instead of just passi vely responding to user queries. A basic question is thus how a retrie val system should propose questions to the user so that it can obtain maximum benefits from the feedback on these questions. In this paper , we study how a retrie val system can perform active feedback, i.e., how to choose documents for rele vance feedback so that the system can learn most from the feedback information. We present a general frame work for such an acti ve feedback problem, and deri ve several practical algorithms as special cases. Empiri-cal evaluation of these algorithms sho ws that the performance of traditional rele vance feedback (presenting the top K documents) is consistently worse than that of presenting documents with more diversity . With a diversity-based selection algorithm, we obtain fewer rele vant documents, howe ver, these fewer documents have more learning benefits.
 H.3.3 [ Inf ormation Sear ch and Retrie val ]: Rele vance Feedback, Search Process, Clustering Algorithms Acti ve Feedback, Ad Hoc Information Retrie val
In ad hoc information retrie val, a user often needs to interact with the retrie val system several times to obtain satisf actory re-sults for one information need, which pro vides opportunities for the retrie val system to acti vely participate in this iterati ve retrie val process. Most traditional retrie val systems just passi vely respond Cop yright 2005 ACM 1-59593-034-5/05/0008 ... $ 5.00. to user queries and put the responsibility of refining/impro ving the search solely on the user . But there has been evidence sho wing that a retrie val system can play an acti ve role in this process, e.g., ob-taining user feedback explicitly or implicitly when the user bro wses these documents, and exploiting such information to impro ve the performance in the next round of search [6, 8]. Ideally , a retrie val system should collaborate with the user in the whole interacti ve search period to impro ve the accurac y and reduce the number of interactions.

When explicit feedback is possible, a natural way for the retrie val system to acti vely participate in the retrie val process is to clarify the user X  s information need by probing the user with well-designed questions. A question could be whether a document or passage is rele vant, or whether a term describes the user X  s information need.
In this scenario, a basic question is how the retrie val system should intelligently propose the questions so that it can learn most from the user X  s answers to these questions. In this paper , we study how a retrie val system can perform active feedbac k , i.e., how to choose documents for rele vance feedback so that the system can learn most from the feedback information.

Rele vance feedback is kno wn to be effecti ve for impro ving re-trie val performance [16, 18, 4]. Pre vious work on rele vance feed-back focuses on query updating techniques such as query term reweighting and query expansion. The issue of choosing docu-ments for rele vance feedback has not been well addressed. Tra-ditionally , rele vance feedback methods just choose the top rank ed documents for feedback, which is not necessarily the best strate gy from the learning perspecti ve. For example, if the top two doc-uments have identical contents, the learning benefits of these two documents will be nearly equal to that of any one of them. Thus a very interesting research question is how to select appropriate doc-uments for user judgment to maximize the learning benefits, which is the focus of the study in this paper .

Acti ve feedback is essentially an application of acti ve learning in ad hoc information retrie val. Acti ve learning has been exten-sively studied in machine learning [17, 22, 3]. It has been applied to text cate gorization in several pre vious studies [12, 14, 23], and recently to adapti ve information filtering [29]. But there has been little work on applying it to ad hoc retrie val, partly because there are two special challenges in applying acti ve learning to ad hoc retrie val. First, in ad hoc retrie val, we do not have any training examples available to guide the retrie val system for acti vely select-ing the documents for feedback; the query is the only information that can be exploited. Second, it is unclear how we can define an objecti ve function that optimizes ranking performance rather than classification accurac y. An interesting recent work on applying ac-tive learning to ad hoc retrie val is [5], where a user is assumed to iterati vely choose clusters, and the acti ve learning task for the sys-tem is to design good clusters, a dif ferent task from acti ve feedback. The TREC Hard Track [1] has stimulated some recent work along the line of acti ve feedback including [15, 20].

In this paper , we frame the problem of acti ve rele vance feedback as a statistical decision problem, and examine several special cases in refining the frame work. We deri ve several practical algorithms for acti ve feedback, including the Top K, Gapped Top K and K Cluster Centroid algorithm. We empirically evaluate these three al-gorithms using the TREC2003 HARD data , AP88-89 and AP90. The results sho w that the performance of the Top K algorithm (i.e., the traditional way of rele vance feedback) is consistently worse than that of Gapped Top K algorithm and K Cluster Centroid al-gorithm which present documents with more diversity . In general, with a diversity-based selection algorithm, we obtain fewer rele vant documents, but these fewer documents have more learning benefits.
The remaining sections are organized as follo ws. In Section 2, we present the acti ve feedback frame work and deri ve several prac-tical algorithms. In Section 3, we describe our evaluation methods and three algorithms we tested. We discuss the experiment results in Section 4 and conclude our work in Section 5.
The problem of acti ve feedback is essentially a decision prob-lem in which we choose the best subset of documents for rele vance judgment by the user . To formalize this problem, we follo w the risk minimization frame work for retrie val [9] and treat it as the follo w-ing optimization problem: where D = f d 1 ; :::; d k g is a subset of the document collection is a query , U is a user variable, is the set of parameters of the query language model and document language models. p ( jU ; q; C ) posterior probability distrib ution of all the parameters, and is a loss function reflecting how much we can expect to learn by re-questing rele vance judgments on D from user U . In general, the loss function may also depend on other factors such as any rele-vance judgments available from pre vious iterations of retrie val, but here we ignore those factors for the con venience of presentation.
Without refining the language models p ( jU ; q; C ) , which is not the focus of this paper , we study how to define the loss function for acti ve feedback. Clearly , the actual value of a set of documents D for learning depends on not only D but also the judgments the user would mak e. Let J = f 0 ; :::; m g be the set of all possible rele vance levels that a user may assign to each presented document ( 0 for  X  X ompletely non-rele vant X ). For example, for binary judg-ments, J = f 0 ; 1 g . The loss function can now be written as where ~ j = ( j 1 ; :::; j k ) and j i is a possible judgment for document d in D ; p ( ~ j j D; ; U ) is the probability that the user judgments ~ j to all the documents in D ; and l ( D; ~ j; ) function that indicates how much we can learn from the judgments ~ j on D . In other words, l ( : ) tells us how good ( D; ~ j ) labeled examples for learning.

No w assuming that the user would judge each document inde-pendently , we have Note that this assumption is reasonable if a user explicitly judges a document, but it is unlik ely to hold when we infer a user X  s judg-ments based on, say , clickthrough data [6], as obviously a user would not open a redundant (but rele vant) document.

Thus our general frame work for acti ve feedback is the follo wing decision rule:
In the remaining part of the section, we discuss some interesting special cases. We will assume that the rele vance judgments are all binary , though most deri vations can be easily generalized to multi-level judgments.
Let us first simplify the loss function by assuming that the value of each judged example for learning is independent of each other . The total value of a set of examples ( D; ~ j ) can thus be written as the sum of the value of each indi vidual example, i.e., where l ( d i ; j i ; ) is the loss for a single judged document After some algebraic manipulation, we have And the acti ve feedback decision rule is
The optimal set D can thus be obtained by ranking all the doc-uments according to the follo wing risk function and taking the k documents with the least risk: which can be interpreted as the expected value of d i for learning over all possible judgments.

We now examine the assumptions underlying two simple meth-ods for defining r ( d i )  X   X  X op K X  and  X  X ncertainty Sampling X .
Let us assume that the loss of any rele vant example (document) and that of any non-rele vant example (document) are both con-stants. We further assume that the former is smaller than the latter , which is to say that a rele vant example is more useful for learning than a non-rele vant one. Formally , 8 d i 2 C , we have l ( d C , l ( d i ; 0 ; ) = C 0 , and C 1 &lt; C 0 .

The risk function now becomes Since C 1 C 0 &lt; 0 , clearly the optimal set D is precisely the documents with the highest probabilities of being judged as rele-vant (i.e., with the highest expected values of p ( j i = 1 j d That is, we should simply rank all the documents in C according to the estimated rele vance status of each document and select the top k documents that are most lik ely rele vant for feedback.

We have thus obtained the  X  X op K X  method as a special case un-der three assumptions 1 : (1) independent loss function; (2) constant loss for any rele vant (non-rele vant) document; and (3) a rele vant document has a smaller loss than a non-rele vant one. The results are not really surprising because assumption 2 basically says that all rele vant (non-rele vant) examples are equally good for learning. Ho we ver this analysis suggests that we may expect other methods to perform better than Top K if the underlying feedback algorithm does not satisfy all these three assumptions, e.g., independent loss function.
In [12, 11], a similar document selection problem is studied, though a set of documents are selected for labeling to train a text classifier instead of a ranking function. Authors propose to select the most uncertain documents for labeling. In [28], a similar idea, i.e., selecting most uncertain objects, is used to guide the hidden an-notation for content-based image information retrie val. Using our general acti ve feedback frame work, we can deri ve the uncertainty sampling method by assuming the follo wing loss function: where R 2 f 0 ; 1 g is a binary rele vance variable with 1  X  X ele vant X . This loss function essentially says that a rele vant exam-ple is more useful if the predicted probability of rele vance is smaller according to our current model, and similarly , a non-rele vant exam-ple is more useful if the predicted probability of rele vance is lar ger . In other words, an example is more useful if our prediction has less confidence.

With such a loss function, and assuming p ( R j d i ; ) is an approx-imation of p ( j i j d i ; ; U ) , the risk function becomes where H is the entrop y function. This means that, in order to obtain D , we should rank documents in the descending order of the expected entrop y of the corresponding rele vance variable is, we would pick documents with the highest uncertainty .
We have thus obtained the  X  X ncertainty Sampling X  method as a special case under two assumptions: (1) independent loss function; (2) an example is more useful for learning if our prediction of rele-vance is more uncertain. This method relies on explicitly predicting the probability of rele vance, which is often not feasible in ad hoc retrie val.
Our assumption about an independent loss on each example is not realistic. For instance, if two examples are completely identi-cal, their total value is clearly less than the sum of their indi vidual values, and is probably close to the value of one of the examples. Thus we need to model the interactions between documents with a dependent loss function. Unfortunately , the exact form of such a loss function highly depends on the specific feedback algorithms. Ne vertheless, intuiti vely , given a fix ed size of D , increasing the representati veness of documents in D appears to be always desir -able. At the same time, we can also reasonably assume that rele-vant examples are more useful than non-rele vant examples. Thus one possible way to refine a dependent loss function is to associate 1 Top K as an acti ve feedback method was first discussed in [10]. the value of D for learning with the rele vance status and diversity of D . That is, we write our loss function as where ( D; ) is a function that measures the diversity of docu-ments in D and is a parameter indicating the tradeof f between the rele vance and diversity .

According to this loss function, the acti ve feedback decision rule is D = arg min
That is, we need to select D to simultaneously optimize both rel-evance (the first term) and diversity (the second term). A possible greedy algorithm is to first optimize the rele vance term by selecting top N ( N &gt; K ) documents according to rele vance-based ranking, and then to further select K most diverse documents from the documents. We now discuss several simple methods along this line.
Suppose we let N = ( G + 1) K , where G + 1 is a small posi-tive inte ger . To capture the diversity , we partition the into K clusters based solely on the rele vance scores so that our first cluster would have the first G + 1 documents and the second one have the next G + 1 documents, and so on so forth. From each cluster , we then select a document with the highest rele vance score to form our feedback document set D . We refer to this method as  X  X apped Top K X  since it corresponds to selecting the top K ments with a gap of G documents in between any two documents. An interesting property of this method is that when G = 0 essentially the regular Top K method.
Maximal Mar ginal Rele vance (MMR) ranking is a greedy algo-rithm for ranking documents based on rele vance and at the same time avoiding redundanc y [2, 26]. Specifically , we iterati vely se-lect a document which optimizes the follo wing MMR function: where s ( d ) is a rele vance scoring function, sim ( d; d ity function, and is a parameter for trading off between rele vance and redundanc y.

This method can also be regarded as performing an implicit clus-tering and then selecting a document from a cluster with the highest rele vance value. The first document selected will be the top rank ed one based on rele vance. Since the next document to be selected must be far from this selected first document, we can interpret the first document as implicitly defining a cluster with the first docu-ment being the centroid, and none of the other documents in the cluster will be selected since the y are all too close to the selected first document. As we select the second document, we again have another cluster which further excludes some documents from be-ing selected. Ho we ver, it is unclear what the clustering boundary is exactly as it is affected by not just the similarity function, but also the rele vance scores of documents and the parameter . The MMR method can also cover the Top K method as a special case when = 1 .
A more direct method to maximize diversity is to perform ex-plicit clustering. Specifically , we can first select the top ments according to the rele vance scores. Then we partition these N documents into K clusters and construct D by selecting one representati ve document from each cluster .
 To optimize the rele vance term in equation 1, we restrict a relati vely small number . In this way, we ensure that each of the N documents has a reasonably high probability of rele vance. The diversity is ensured through clustering and choosing only one document from each cluster . There are several dif ferent ways to choose a representati ve document from each cluster . One is to choose the centroid document, which maximizes the average sim-ilarity between the chosen document and other documents in the cluster . Another choice is to choose the document with the highest rele vance score. We use two data sets for experiments. One is the Associated Press (AP) news data on TREC disks 1, 2, and 3. The other is the TREC2003 HARD (High Accurac y Retrie val from Documents) track data set [1]. TREC2003 HARD track puts search into conte xt, which allo ws a retrie val system to acti vely infer a user X  s informa-tion need and impro ve retrie val performance [20]. Our experiment process simulates two runs of the HARD track experiment setup [1]. For the HARD track data set, we use 48 topics that have rele-vance judgments. For the AP data set, we use 92 topics from topics 1-50 and 101-150, which have rele vance judgments on both the AP88-89 and AP90 data set. We use the title of each topic as the query . We use the Lemur toolkit as our retrie val system [24] and the KL-Di vergence language retrie val model as our retrie val model [9, 25]. K is fix ed to 6 in most experiments, and all parameters are set to def ault values [24] unless otherwise stated. Our baseline run is regular retrie val without any feedback. It allo ws us to test whether we can impro ve performance by performing feedback. From the baseline retrie val results, we use dif ferent acti ve rele vance feedback algorithms to select a set of documents for rele vance feedback. Us-ing the kno wn rele vance judgments available from these TREC data to simulate a user X  s judgments, we obtain rele vance judgments on the selected documents. These judgments are then used to perform feedback using the mixture model approach implemented in Lemur [27]. This method only uses rele vant documents for query model updating, which can be a limitation of our study . The retrie val re-sults in the second run using dif ferent acti ve feedback algorithms are compared for evaluation. This experiment procedure is illus-trated in Figure 1.
As a first step of studying acti ve feedback, we evaluate three rep-resentati ve acti ve feedback algorithms discussed in Section 2. The first one is Top K, which chooses top K documents from the base-line run retrie val, and is also what existing retrie val systems would normally do. The second one is Gapped Top K , which is to choose gapped top K documents from the baseline run results. For exam-ple, if we set the gap to 3 and K to 6, we will end up choosing the 1st, 5th, 9th, ..., 21st documents from the retrie val results. The third one is K cluster centroid, which represents the most direct way of modeling diversity . We use the K-Medoid clustering algo-rithm [7] to cluster the top N documents. And we use J-Di vergence [13] of two documents as the distance function. J-Di vergence is a divergence metric similar to KL-Di vergence. But unlik e the non-symmetry of KL-Di vergence, J-Di vergence is symmetric. The for -mula of J-Di vergence is as follo ws.

Ev aluation of these methods allo ws us to examine whether pre-senting a diverse set of documents for feedback leads to more effec-tive feedback than presenting the top k documents with the highest rele vance values.
To measure the performance of a ranking method, we use two standard ad hoc retrie val measures: (1) Mean Average Precision (MAP): This is the commonly used non-interpolated average preci-sion and serv es as a good measure of the overall ranking accurac y since it is sensiti ve to the rank of every rele vant document. (2) Pre-cision at 10 documents (pr@10): This measure does not average well and only gives us the precision at one single cutof f point. But it reflects the utility percei ved by a user who may only read up to top 10 documents. In all cases, the reported figure is the average over all the topics.

Since the task of acti ve feedback involv es identifying a certain number of rele vant documents by the user , an interesting ques-tion is whether we should include such rele vant documents when computing the retrie val precision of an acti ve feedback algorithm. While this is also a problem for rele vance feedback evaluation, it is especially a challenge for evaluating acti ve feedback algorithms be-cause the set of rele vant documents used for feedback can usually be controlled in regular rele vance feedback evaluation, but must vary in evaluating acti ve feedback algorithms.

In our evaluation, we decided to include all the judged docu-ments, including both rele vant and non-rele vant documents, be-cause if we exclude them, we would have a potentially dif fer ent test set for each method. In particular , it would be unf air for a method that tends to present more  X  X asy X  rele vant documents for feedback; indeed, the retrie val task would become artificially harder for such a method due to the fact that more  X  X asy to retrie ve X  rele vant docu-ments would be excluded.

Ho we ver, including such judged documents also has a problem  X  it does not accurately reflect the actual utility of a method as per -cei ved by a user . Indeed, a user would presumably not really care about where the judged feedback documents are rank ed because the user has already seen them. Thus any impro vement in the ranking of a seen rele vant document does not really bring any real benefit to the user .
In order to see more clearly how much a method can impro ve the ranking of unseen documents, we can run the acti ve feedback algorithms on one document database (i.e., the training database) to obtain rele vance judgments and then use another similar document database (i.e., the testing database) to test the retrie val performance [21]. Thus, in addition to the regular evaluation on the HARD track data set and AP88-89 with all the judged documents included, we also use AP88-89 for training and AP90 for testing to compare dif-ferent methods, assuming that the contents in these two databases are suf ficiently similar .
As we mentioned in Section 2.2.1 , Top K can be considered as a special case of Gapped Top K (i.e. when the gap equals to 0). We do experiments varying the gap to test whether a non-zero gap can perform better than Top K. The results on the HARD data set and AP88-89 data set are sho wn in Table 1, where we sho w the MAP , the precision at 10 documents, and the number of judged rele vant documents per query .

From the results, we can see Top K ( gap = 0 ) is clearly not the best strate gy. Actually , when we choose small gaps ( the performance is consistently better than Top K, which strongly suggests that top K is really a poor choice for acti ve rele vance feed-back. We may also note that, as we increase the gap, we obtain fewer rele vant documents than we could obtain with Top K. But using these fewer rele vant documents for feedback can achie ve bet-ter retrie val performance, which means these fewer rele vant docu-ments have more learning benefits. The same phenomenon is also observ ed when acti ve learning is applied in the classification prob-lem [19]. One explanation of this phenomenon is that when we increase the gap, we obtain more diverse documents, thus the judg-ments become more informati ve.
Here we use the clustering algorithm to select more diverse doc-uments for acti ve rele vance feedback. We cluster the top ments into K clusters and choose the K cluster centroid for rele-vance feedback. When N = K , we again have Top K as a special case. We vary N for fix ed K ( = 6 ) to test if presenting documents with higher diversity is beneficial. The results are sho wn in Table 2.
The variation of N causes a dif ferent tradeof f point for rele vance and diversity . If we choose a bigger N , we pay more attention to diversity , while if we choose a smaller N , we pay more attention to rele vance. We see that the optimal values are dif ferent for the two databases. Comparing Top K ( N = K ) with other results in the Table again sho ws that Top K is mostly the worst among all the results, suggesting that the rele vance judgments obtained with clustering are more effecti ve for feedback than those obtained using Top K. Moreo ver, with a lar ge N , we actually obtain fewer judged rele vant documents, but these fewer rele vant documents are better examples for learning.
Since the effecti veness of the underlying feedback mechanism ( the mixture model method in our case) is an important factor that may affect our evaluation, we compare several dif ferent feedback algorithms with the non-feedback baseline in Table 3. The perfor -mance for the Gapped Top K and the K Cluster Centroid is the best performance from Table 1 and Table 2, respecti vely .

From these results, we can see that the performance of both ac-tive feedback and pseudo feedback are better than that of baseline retrie val. We also see that the Top K rele vance feedback performs better than using the top K documents for pseudo feedback. All these results sho w that the underlying feedback mechanism is ef-fecti ve.

Among acti ve feedback algorithms, K cluster centroid outper -forms Gapped Top K algorithm, which in turn outperforms Top K algorithm, although the impro vement appears to be quite small. A very interesting observ ation is that the K cluster centroid algorithm obtains the fewest number of rele vant documents from user feed-back, yet its performance is the best. This suggests that selecting diverse documents leads to more effecti ve learning.

As mentioned in Section 3.4, when comparing dif ferent acti ve feedback algorithms, it is more reasonable to use one document database for acti ve feedback (training), and the other document database for measuring retrie val performance (testing). Thus we further compare these methods using AP88-89 as the training set and AP90 as the testing set. Specifically , we perform baseline re-trie val on AP88-89 database, select a document subset for rele vance the level of 0.05 and 0.1, respecti vely .
 of 0.05. feedback using dif ferent acti ve rele vance feedback algorithms, up-date the query model, all on AP88-89, and then retrie ve documents from AP90. The experiment results are sho wn in Table 4.
The results again sho w that the results of the Top K algorithm is the worst among three acti ve rele vance feedback algorithms. Al-though the performance dif ference is mostly insignificant accord-ing to the Wilcoxin signed rank test except in the case of pr@10 for Gapped Top K, there are more topics for which Gapped Top K and K Cluster Centroid are better than Top K than the other way in all cases. In the case of MAP , it is 42 topics vs. 31 topics (with 19 cases tied) for both Gapped Top K and K Cluster Centroid. In the case of pr@10, it is 12 topics vs. 3 topics (with 77 cases tied) and 9 topics vs. 5 topics (with 78 cases tied) for Gapped Top K and K Cluster Centroid, respecti vely . The lar ge number of tied cases indicates that our query expansion feedback mechanism is conser -vative. Indeed, as we sho w later in Table 6, when we change the query expansion parameter to perform more aggressi ve query ex-pansion, the performance impro vement is generally amplified. The performance of all acti ve feedback algorithms is also better than that of pseudo feedback and baseline retrie val.
The results sho wn so far are all obtained by fixing K = 6 now examine how choosing a dif ferent K may affect our conclu-sions. We compare Top K, Gapped Top K (gap=3), and K clus-ter centroid ( N = 100 ) for several dif ferent values of ble 5. The results sho w that our conclusion, i.e., the performances of Gapped Top K and K Cluster Centroid are better than that of Top K, is relati vely insensiti ve to the choice of K . Indeed, the Top K results are almost always the worst among the three methods. Also, on the HARD data, the K cluster centroid method consistently out-performs the other two methods with fewer judged rele vant docu-ments.
In the results sho wn so far, the impro vement of Gapped Top K and K Cluster Centroid over Top K is not so significant. We find that the feedback algorithm parameter is an important factor . In [27], the new query model ^
Here, controls how much weight we give to feedback doc-uments. In all the pre vious results, we set to 0 : 5 . But since the feedback documents are judged to be rele vant by users, we can give more weight to these feedback documents. So we did another set of experiments by varying and keeping all other parameters fix ed. The results are sho wn in Table 6. From these results, we can see clearly that the can amplify the effect of feedback. And when is increased, the impro vement of Gapped Top K and K Cluster Centroid over Top K is also amplified.
This paper presents the first serious study of the problem of ac-tive rele vance feedback, in which a retrie val system acti vely chooses the best documents for rele vance feedback. Ad hoc information re-trie val is lar gely an interacti ve process. Acti ve rele vance feedback allo ws a retrie val system to acti vely probe a user and clarify the user X  s information need, thus can impro ve retrie val performance.
We formulate the problem of acti ve feedback as a statistical de-cision problem and study several special cases. We analyze the assumptions made in each case. We deri ve three specific algo-rithms for acti ve rele vance feedback, i.e., Top K, Gapped Top K, and K Cluster Centroid algorithm. We evaluate these algorithms using the TREC2003 HARD data set, AP88-89 and AP90 data set. Experiment results sho w that the Top K algorithm, which is what an existing retrie val system normally uses for rele vance feedback, is not optimal for acti ve rele vance feedback, and is actually often worse than both the Gapped Top K algorithm and the K Cluster Centroid algorithm. Compared with the Top K algorithm, Gapped Top K algorithm and K Cluster Centroid algorithm emphasize re-turning more diversified documents. The results sho w that with fewer judged rele vant documents, both Gapped Top K and K Clus-ter Centroid outperform the Top K algorithm, suggesting that the diversity in the presented documents is a desirable property . Al-though the dif ference is generally small, the overall consistenc y strongly supports our conclusions.
Our work represents only a very preliminary exploration of this important topic. There are several interesting directions to explore. (1) It would be interesting to study how to learn from non-rele vant documents judged by the user so as to mak e full use of user efforts and feedback. (2) Another interesting question is how to optimize performance over the entire search session, rather than just one it-eration. (3) We may explore other approaches for selecting docu-ments. For example, MMR strate gy is also a promising strate gy. (4) We can try to combine pseudo feedback and acti ve feedback. Since those highly rank ed documents are very lik ely rele vant, we do not really need to present them for judgments; instead, we can propose documents rank ed belo w a few top documents for feedback (es-sentially the uncertainty sampling strate gy). In this way, feedback can be based on those top-rank ed documents, which we assume to be rele vant, and the obtained rele vance judgments through acti ve feedback.
We thank Karen Sp  X  arck Jones, Stephen Robertson and the anon y-mous revie wers for their useful comments. This material is based in part upon work supported by the National Science Foundation un-der award numbers CAREER-IIS-0347933 and ITR-IIS-0428472. [1] J. Allan. HARD track overvie w in TREC2003. In [2] J. Carbonell and J. Goldstein. The use of MMR, [3] D. A. Cohn, Z. Ghahramani, and M. I. Jordan. Acti ve [4] D. Harman. Rele vance feedback revisited. In Proceedings of [5] T. Jaakk ola and H. Sie gelmann. Acti ve information retrie val. [6] T. Joachims. Optimizing search engines using clickthrough [7] L. Kaufman and P. J. Rousseeuw . Finding Gr oups in Data: [8] D. Kelly and J. Teevan. Implicit feedback for inferring user [9] J. Laf ferty and C. Zhai. Document language models, query [10] D. D. Le wis. Acti ve by accident: Rele vance feedback in [11] D. D. Le wis and J. Catlett. Heterogeneous uncertainty [12] D. D. Le wis and W. A. Gale. A sequential algorithm for [13] J. Lin. Divergence measures based on the shannon entrop y. [14] A. K. McCallum and K. Nigam. Emplo ying EM in [15] S. E. Robertson, H. Zaragoza, and M. Taylor . Microsoft [16] J. J. Rocchio. Rele vance feedback in information retrie val. [17] N. Ro y and A. McCallum. Toward optimal acti ve learning [18] G. Salton and C. Buckle y. Impro ving retrie val performance [19] G. Schohn and D. Cohn. Less is more: Acti ve learning with [20] X. Shen and C. Zhai. Acti ve feedback X  X IUC TREC2003 [21] K. Sparck Jones. Search term rele vance weighting given little [22] S. Tong. Active Learning: Theory and Applications . PhD [23] S. Tong and D. Koller . Support vector machine acti ve [24] Lemur Toolkit. http://www .cs.cmu.edu/ lemur . [25] C. Zhai. Risk Minimization and Langua ge Modeling in Text [26] C. Zhai, W. W. Cohen, and J. Laf ferty . Be yond independent [27] C. Zhai and J. Laf ferty . Model-based feedback in the [28] C. Zhang and T. Chen. An acti ve learning frame work for [29] Y. Zhang, W. Xu, and J. P. Callan. Exploration and
