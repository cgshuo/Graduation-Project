 We designed and implemented Tagme , a system that is able to efficiently and judiciously augment a plain-text with pertinent hyperlinks to Wikipedia pages. The specialty of Tagme with respect to known systems [5, 8] is that it may annotate texts which are short and poorly composed, such as snippets of search-engine results, tweets, news, etc.. T his annotation is extremely informative, so any task that is cur -rently addressed using the bag-of-words paradigm could ben -efit from using this annotation to draw upon (the millions of) Wikipedia pages and their inter-relations.
 I.2.7 [ Artificial Intelligence ]: Natural Language Process-ing X  text analysis ; H.3.1 [ Information Storage and Re-trieval ]: Content Analysis and Indexing Algorithms, Experimentation, Performance.
The typical IR-approach to indexing, clustering, classifi-cation and retrieval, just to name a few, is that based on the bag-of-words paradigm. In recent years, a good deal of work attempted to go beyond this paradigm with the goal of improving the search experience on (un-)structured or semi-structured textual data. The key idea is to identify a sequence of terms (also called spots ) in the input text and to annotate them with un-ambiguous entities drawn from a catalog. The choice of the catalog is obviously crucial for the success of the approach; currently several systems adop t Wikipedia pages or derived concepts as entities because of their ever-expanding number (more than 3 million English pages, and more than 500K pages in each major European  X 
Part of this work has been supported by MIUR-FIRB Lin-guistica 2006 and MIUR-PRIN MadWeb.
 language) and the fact that Wikipedia offers the best trade-off between a catalog with a rigorous structure but with low coverage (like the one offered by the high-quality entity cat a-logs s.t. WordNet , CYC , TAP ), and a large text collection with wide coverage but unstructured and noised content (like the whole Web). This annotation has implications that go be-yond the enrichment of texts with explanatory links because it provides structured knowledge about any unstructured fragment of text. So any task that is currently addressed with bags of words-indexing could use these techniques to draw on a vast network of concepts and their inter-relations .
To our knowledge the first work that addressed the prob-lem of linking spots to Wikipedia pages was Wikify [6], followed by [2]. Recently Milne and Witten [8] proposed an approach that yielded considerable improvements by hing-ing on three main ingredients: (i) the identification in the input text of a set C of so-called context pages , namely pages linked by spots that are not ambiguous (because they link to just one page/sense); (ii) a measure rel ( p 1 , p 2 ) of related-ness between two pages p 1 , p 2 based on the overlap between their in-linking pages in Wikipedia; and finally (iii) a noti on of coherence of a page p with the other context pages in C . These ingredients allowed [8] to achieve an F-Measure of 74.8% over long and highly-linked Wikipedia full-articl es. Last year, Chakrabarti et al [5] further improved [8] by in-troducing two main novelties. The first one was to score an annotation with two terms: one local to the occurrence of the spot and the other global to the text fragment. The second novelty was to model the annotation process as a search for the mapping that maximizes the sum of the (lo-cal+global) scores of the selected annotations, via the sol u-tion of a quadratic assignment problem. Experiments over a manually annotated dataset showed that Chakrabarti X  X  approach yields a precision comparable to Milne&amp;Witten X  X  system but with a higher recall.

In this paper we add to this flow of work the specialty that the input texts to be annotated are very short , namely com-posed of few tens of terms. The context of use we have in mind is the annotation of either the snippets of search-engi ne results, or the tweets of a Twitter channel, or the items of a news feed, etc.. These poorly composed texts pose new challenges in terms of efficiency and effectiveness: (1) the annotation should occur on-the-fly, because in those con-texts data may be retrieved at query time and thus cannot be pre-processed; (2) the annotation needs new algorithms because the input texts are so short that it is difficult to mine significant statistics that are rather available when texts are long. The systems of [5, 8] are designed to deal with rea-sonably long texts, so they seem unsuitable in this context because either C is empty [8] or the annotation is slow [5]. Given these limitations we have designed and implemented Tagme a software system that, on-the-fly and with high pre-cision/recall, annotates short fragments of text with pert i-nent hyperlinks to Wikipedia articles. Tagme uses Wikipe-dia anchor texts as spots and the pages linked to them in Wikipedia as their possible senses. Tagme solves ambiguity and polysemy in the potentially many available anchor-page mappings by finding the collective agreement among them via new scoring functions which are fast to be computed and effective in the produced annotation. Tagme obtains this by taking explicitly into account the sparseness of the anchors in the short input text via the proper combination of the relatedness function among concepts proposed in [7] and some other statistics drawn from Wikipedia. Prelim-inary experiments show that Tagme outperforms the best known systems [5, 8] when they are adapted to work on short texts, and surprisingly, it results competitive on lo ng texts too. In particular Tagme yields an F-measure of about 78%, with the possibility to balance precision (up to 90%) vs recall (up to 80%), thus being always better than [5, 8]. The time complexity of Tagme  X  X  annotation is linear in the number of processed anchors (cfr. [5] X  X  quadratic time com-plexity), and indeed it can annotate texts in less than 2ms per anchor, so more than one order of magnitudes faster than [5]. Tagme is available at http://tagme.di.unipi.it and further details are reported in [4]. Notation and terminology. A (text) anchor (referred as spot ) for a Wikipedia page p is the text used in another Wikipedia page to point to p . Because of polysemy and variant names, we denote by Pg ( a ) the set of all Wikipedia pages linked by a , and set freq ( a ) as the number of times the text a occurs in Wikipedia (as an anchor or not); whereas we use link ( a ) to denote the number of times the text a occurs as an anchor in Wikipedia (so link ( a )  X  freq ( a )). Also we use lp ( a ) = link ( a ) /freq ( a ) to denote the link -probability that an occurrence of a has been set as an anchor; and use Pr( p | a ) to denote the prior -probability (a.k.a. commonness ) that an occurrence of an anchor a points to p  X  Pg ( a ).
The annotation of an anchor a with some page p  X  Pg ( a ) is denoted by a 7 X  p . Often a has more senses, thus | Pg ( a ) | &gt; 1, so we call disambiguation the process of selecting one of the possible senses of a from Pg ( a ). It goes without saying that not all occurrences of the spot a should be considered as text anchors, so we follow [5] and introduce a fake page na that is used in the annotation a 7 X  na in order to denote that an occurrence of anchor a has been pruned .
 Preprocessing and indexes. Tagme indexes some dis-tilled, but useful, information drawn from the Wikipedia snapshot of November 6, 2009.

Anchors were drawn from Wikipedia pages, plus we added the titles of redirect pages and some variants of the page-titles as suggested in [2]. From this set, we eventually re-moved the anchors composed by one character or just num-bers, and discarded all anchors whose absolute or relative frequency is small enough to argue that they are unsuitable for annotation and probably misleading for disambiguation . The final anchor dictionary contains about 3M anchors, and it is indexed by Lucene 1 .

Senses were built by taking all Wikipedia pages, and then discarding disambiguation pages, list pages, and redirect pages. At the end remained about 2.7M pages (indexed by Lucene), with a link-graph of about 147M edges (indexed in internal-memory by Webgraph 2 ).
 Anchor Disambiguation. Let A T be the set of all anchors occurring in an input text T , Tagme tries to disambiguate each anchor a  X  X  T by computing a score for each possi-ble sense p a of a (hence p a  X  Pg ( a )) via a new notion of  X  X ollective agreement X  between p a and the possible senses of all other anchors detected in T . To do this, Tagme deploys a new voting scheme that computes for each other anchor b  X  X  T \{ a } its vote to the annotation a 7 X  p a . Given that b may have many senses (i.e. | Pg ( b ) | &gt; 1) we compute this vote as the average relatedness between each sense p b of the anchor b and the sense p a we wish to associate to the an-chor a . Moreover, since not all possible senses of b have the same (statistical) significance, we weight the contributio n of p by means of its commonness Pr( p b | b ). Hence the formula b is un-ambiguous, it is Pr( p b | b ) = 1 and | Pg ( b ) | = 1, so we have vote b ( p a ) = rel ( p b , p a ) and hence we fully deploy the unique senses of the un-ambiguous anchors (as in [8]). But if b is polysemous, only the senses p b related to p a mainly affect vote b ( p a ) because of the use of the relatedness of the annotation a 7 X  p a is computed as the sum of the votes given to it by all other anchors b detected in the input text. This is the key difference with the scoring proposed by Milne&amp;Witten, which was based on un-ambiguous anchors only (here possibly missing). As for [5], we do not use term-vectors for all involved senses, but we use (implicitly) few short vectors: one per anchor and with one-dimension per sense. This allows Tagme to be fast.

To disambiguate a , thus select its best annotation a 7 X  p we designed and tested two ranking algorithms: Disam-biguation by Classifier ( dc ) and Disambiguation by Thresh-old ( dt ). dc is based on a classifier that uses as features the score rel a ( p a ) and the commonness Pr( p a | a ) to com-pute a  X  X robability of correct-disambiguation X  for all sen ses p a  X  Pg ( a ). Among all p a  X  Pg ( a ), dc selects the one re-porting the highest classification score. On the other hand, dt recognizes a roughness in the value of rel a ( p a ) among all p  X  Pg ( a ), so it computes the top- X  best senses p  X  in Pg ( a ) according to their rel a ( p  X  ), and then annotates a with the sense that obtains the highest commonness among them.
Since speed is a main concern, dc and dt discard from the above computation all senses p  X  Pg ( a ) whose prior-probability P r ( p | a ) &lt;  X  , where  X  is properly set (see next). Anchor Pruning. The set M ( A T ) of candidate assign-ments produced by the Disambiguation Phase has to be pruned in order to discard the possible un-meaningful an-chors a . These  X  X ad anchors X  are detected via a novel and efficiently-computable scoring function that takes into ac-count only two features: the link probability lp ( a ) of the an-chor a and the coherence of its candidate annotation a 7 X  p with respect to the candidate annotations of the other an-http://lucene.apache.org http://webgraph.dsi.unimi.it chors in M ( A T ). The link probability is a simple and ef-fective feature for detecting significant anchors [8]. The c o-herence with the un-ambiguous anchors was also shown to be effective in [8], Tagme extends this notion to all anchors present in the input text T and computes it as the average relatedness between the candidate sense p a for a and the can-didate senses p b for all other anchors b in T . The principle is to keep all anchors whose link probability is high or whose assigned sense (page) is coherent with the senses (pages) as -signed to the other anchors in M ( A T ). We implemented and tested this idea in two ways (called AVG and LR), each of them produces a score  X  ( a 7 X  p ) for a candidate annota-tion a 7 X  p which is then compared with a threshold  X  na so that if it is lower then that annotation, then it is pruned by setting a 7 X  na . The parameter  X  na allows to balance recall vs precision. The two approaches combine lp and coherence as follows:  X  AVG averages the two scores; whereas  X  LR uses a linear combination trained via linear regression.
In our experiments we considered three datasets. The first one is derived from the manually annotated dataset intro-duced in [5], called iitb dataset. It consists of about 100 documents and 19K anchors (40% of them are annotated with na ). For the experiments we split each iitb document into fragments of 30 words each, so to obtain a short in-put text (mimicking the snippet of a search-engine result). The second and third datasets were derived from Wikipe-dia, as done in [8], and have been used for evaluating the disambiguation phase ( Wiki-Disamb30 ) and the overall sys-tem ( Wiki-Annot30 ). The former dataset consists of 1.4M short texts randomly selected from Wikipedia pages, and consisting of about 30 words. To avoid any advantage to Tagme , we selected fragments that surely contain at least one ambiguous anchor (i.e. | Pg ( a ) | &gt; 1). The latter dataset consists of 150K fragments whose set of anchors is expanded by detecting all anchors that occur not annotated (say U ) and annotate them with one of the senses pointed out by (possibly other) anchors that occur in the same page from which that fragment was drawn (say O ). We did this be-cause Wikipedia-contributors usually link only the first oc -currence of an anchor-text in a page, so if the short fragment contains a subsequent occurrence of that anchor, this occur -rence could be not annotated in the ground truth. Therefore we expand Wiki-Annot30 by annotating an anchor a  X  X  with the page p that has the largest commonness Pr( p | a ) among the ones in Pg ( a )  X  X  (if 6 =  X  ). These are the senses associated to a in the same page from which that fragment was drawn (typically one only). After this expansion, Wiki-Annot30 contains about 1.5M annotated anchors, 63% of them annotated by na (i.e. not annotated).

To evaluate the performance of the Disambiguation Phase, we use standard precision and recall scores, whereas for the overall annotation process we follow [5] and thus focus on th e precision P ann and recall R ann measures that are computed only on the set of anchors which are annotated in the ground truth (i.e. the corpora above). These last measures are much demanding because they ask for a perfect match between the annotation in the ground truth and the one obtained by the tested system. If the goal is to identify topics in the text fragment, then it doesn X  X  matter which anchors got annotated but which senses got linked. So, let G ( T ) be the senses (pages) associated to the anchors of T in the ground Table 1: Performance of three disambiguators over Wiki-Disamb30 .
 Table 2: Performance of annotators over Wiki-Annot30 , using either annotation or topics metrics. truth, and let S ( T ) be the senses identified by the tested system over T . As in [8], we define a topic-based notion of precision ( P topics ) and recall ( R topics ) for the system to be evaluated over a set of fragments F , as follows: P topics Disambiguation Phase. We split Wiki-Disamb30 into two datasets: one contains 400K anchors, the other con-tains the remaining 1M anchors. We used these datasets to train/test dc and dt , and compare them against Milne and Witten X  X  disambiguator. Performances are shown in Table 1.
The recall of dc and dt are significantly better than the one of Milne&amp;Witten X  X  system, precision is slightly worse with a difference lower than 0 . 6%, so the overall F-measure of Tagme improves Milne&amp;Witten X  X  one by about 3%. The key difference between Tagme and Milne&amp;Witten X  X  system is that we are deploying relatedness not only with the senses of un-ambiguous anchors, but also with all the other candi-date senses in the fragment via our novel voting scheme. This is crucial in our scenario because the input texts are short and thus often do not contain un-ambiguous anchors. We plan to dig into the disambiguation X  X  algorithm in order to further improve these figures.

As for the comparison between dc and dt , although pre-cision and recall are very close, we decided to choose dt as disambiguator in Tagme because it has a better F-Measure, it is faster (with  X  = 2%), and it is also more flexible be-cause of the threshold  X  : we can increase  X  if the input text is ambiguous (and thus choose more often the most-common sense) or decrease  X  if the input text is longer and more fo-cused (and thus choose more often the most-related sense). At the end  X  was chosen to be 30%.
 The overall system: Disambiguation+Pruning. We recall that the pruning step hinges onto two features, lp and coherence . We trained LR over 50K short-texts extracted from Wiki-Annot30 , and then evaluated it over the remain-ing 100K fragments. For both AVG and LR we tried various values for the parameter  X  na which controls the sensibility of our annotation process. Other experiments are reported in [4]. Figure 1: Performance of two annotators by varying the value of  X  na .

As expected, annotation measures are more severe than topics measures, although dependencies do exist. We de-cided to implement in Tagme the simplest method based on  X  AVG , because it achieves a slightly better F-measure with no-need of a training step. The system by Milne&amp;Witten performed poorly, because many features used by their prun-ing method are not effective when dealing with short texts. In fact they consider features like location and frequency o f anchors (which may be  X  X ndefined X  or even misleading on short texts), as well as they consider only the un-ambiguous anchors to compute a coherence-score (and these are often absent in short texts).
 Finally Figure 1 reports the comparison over the dataset Wiki-Annot30 between the system of Milne&amp;Witten and Tagme , in which we used  X  AVG for pruning and set  X  = 2% and  X  = 30%. Chakrabarti X  X  system is not included in this comparison because it is un-available 3 . Looking at Figure 1 we notice that the performance of the two annotators has a uniform trend as  X  na varies in [0 , 1], and Tagme overcomes Milne&amp;Witten X  X  approach significantly over the entire rang e. Parameter  X  na can be set to balance precision vs recall, and indeed the on-line version of Tagme offers this feature to the user. In the future we plan to investigate other disam-biguators/pruners in order to better investigate the impac t of the various features and algorithmic choices in Tagme , and possibly draw some performance figures about Chakra-barti X  X  approach.

The most time consuming step in Tagme is the calcula-tion of the relatedness score, because anchor detection and other scores require time linear in the length of the input text T . If n is the number of anchors detected in T , s is the average number of senses potentially associated with each anchor, and d in is the average in-degree of a Wikipe-dia page, then the time complexity of the overall annotation process is O ( d in  X  ( n  X  s ) 2 ). In practice these numbers are very small for short texts, so our current implementation of Tagme takes less than 2ms per anchor on a commodity PC. This is more than one order of magnitudes faster than the time performance reported by [5] (which is &gt; 2 secs for 15 anchors). S. Chakrabarti X  X  personal communication.

For completeness we have adapted Tagme to work on long texts by shifting a text window of about 10 anchors. This way its time complexity grows linearly with the num-ber of detected anchors, which compares favorably against [5] X  X  system whose time complexity scales  X  X ildly quadrati -cally X . Of course, this is an unfavorable setting for Tagme because Chakrabarti X  X  and Milne&amp;Witten X  X  systems deploy the full input text (and thus probably more than 10 an-chors). We have preliminary evidence that Tagme is com-petitive with Chakrabarti X  X  annotator, whereas it improve s Milne&amp;Witten X  X  system by achieving an F-Measure of about 78% which surpasses the 74.8% reported in [8] for long and highly linked Wikipedia full-documents. So we can safely conclude that Tagme on long texts is either significantly faster than [5] or more effective than [8].

We are currently investigating the impact of Tagme  X  X  an-notation onto the performance of our past system SnakeT [3] for the on-the-fly labeled clustering of search-engine resu lts (a l  X a Clusty.com ). In fact SnakeT , as most of its competi-tors (see e.g. [1]), is based only on syntactic and statistic al features and thus, we believe that, it could benefit from Tagme  X  X  annotation to improve the effectiveness of the la-beling and the clustering phases. Another promising contex t of application for Tagme could be Web Advertising. The ex-planatory links and the structured knowledge produced by Tagme could allow the efficient and effective resolution of ambiguity and polysemy issues which often occur when ad-vertiser X  X  keywords are matched against the content of Web pages offering display-ads.
 We thank Soumen Chakrabarti for insightful discussions abo ut this work. [1] C. Carpineto, S. Osi  X nski, G. Romano, and D. Weiss. A [2] S. Cucerzan. Large-scale named entity disambiguation [3] P. Ferragina and A. Gulli. A personalized search [4] P. Ferragina and U. Scaiella. TAGME: On-the-fly [5] S. Kulkarni, A. Singh, G. Ramakrishnan, and [6] R. Mihalcea and A. Csomai. Wikify!: linking [7] D. Milne and I. H. Witten. An effective, low-cost [8] D. Milne and I. H. Witten. Learning to link with
