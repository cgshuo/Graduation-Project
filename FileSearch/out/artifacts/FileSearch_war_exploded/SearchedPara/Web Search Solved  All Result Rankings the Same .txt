 The objective of this work is to derive quantitative state-ments about what fraction of web search queries issued to the state-of-the-art commercial search engines lead to excel-lent results or, on the contrary, poor results. To be able to make such statements in an automated way, we propose a new measure that is based on lower and upper bound analy-sis over the standard relevance measures. Moreover, we ex-tend this measure to carry out comparisons between compet-ing search engines by introducing the concept of disruptive sets, which we use to estimate the degree to which a search engine solves queries that are not solved by its competitors. We report empirical results on a large editorial evaluation of the three largest search engines in the US market. H.3.3 [ Information Storage Systems ]: Information Re-trieval Systems Human Factors, Measurement Web search evaluation, editorial judgments, relevance Is web search solved? Is navigational search a commodity? Are all major web search engines roughly the same? Are tail queries the new battleground? In the past few years, there have been many qualitative statements about these and sim-ilar questions 1 . Despite lots of speculation, there are very
As an example, Marissa Mayer (Google VP of search products and user experience) said in an interview in 2008:  X  X earch is an unsolved problem. We have a good 90% to 95% of the solution, but there is a lot to go in the remaining 10%. X  ( http://latimesblogs.latimes.com/ technology/2008/09/marissa-mayer-t.html) few published research results on the topic [38]. We be-lieve that this is mainly due to the ill-defined nature of such questions and the difficulties in the investigation process. Nevertheless, despite all the difficulties involved, we claim that it is still possible to conduct an analysis.

In this paper, to address the questions raised before, we propose a novel performance measure based on the idea of bounding standard IR performance measures. This measure enables making quantitative statements of the form  X  X t least x % of queries lead to excellent results on search engine A X .
Specifically, the contributions of the paper are as follows:
The proposed techniques are applied to a large, editorially judged query sample, obtained from the Yahoo! web search engine. The following are the findings of our work.
We define the disruptive set of a search engine as the set of queries solved by that engine but not by its competitors.
These findings lead us to the conclusion that the only significant difference in search engines today is in the non-navigational tail, and in that different search engines are solving different regions of this tail. Many people had these intuitions before, but to the best of our knowledge, we pro-vide the first empirical confirmation and quantification.
The rest of the paper is organized as follows. In Section 2, we provide the details of the query set used in the work. Section 3 discusses several caveats related to our data and the methodology followed. In Section 4, we propose a tech-nique for frequency-based aggregation of judgments over a sample query set. The proposed performance measures are described in Section 5. Experimental results are presented in Section 6. In Section 7, we discuss some further issues related to our work. We provide a detailed survey of related literature in Section 8. The paper is concluded in Section 9.
We sampled 1,000 queries 3 from the web search query traffic of Yahoo! in 2008. Sampling is done with replace-ment from the query traffic distribution. Therefore, popular queries had a much higher probability of being sampled, i.e., the same query might be sampled more than once, leading to several overlapping data points. The resulting frequency distribution in our sample set (without any query normal-ization) is as follows: 931 queries appear with frequency 1, 15 with frequency 2, 5 with frequency 3, 1 with frequencies 4, 5, and 15. An almost identical distribution is obtained if we normalize the queries by removing punctuation, lower-casing, and sorting query terms in alphabetical order.
Sample queries were issued in late 2008 to the three popu-lar web search engines in the US market (Google, Microsoft Live Search, and Yahoo! Search), and the top 5 URLs re-turned by each search engine are recorded. A team of profes-sional editors judged every recorded page against its query, assigning a five-graded relevance score 4 to each result: unre-lated, related, relevant, very relevant, or perfect (denoted by u , r , R , V , and P , respectively). Along with this evaluation, the editors classified the queries as being navigational [10] or not. In evaluations, only algorithmic search results are con-sidered, i.e., advertising, editorial shortcuts, web site group-ing, and all other forms of shortcuts are ignored.
In this section, we point at the difficulties faced in search quality evaluations and also note some caveats in our work.
Query sample size. Most research works use very small query sets (see Section 8). Moreover, sample queries are of-ten selected by the authors themselves in an arbitrary fash-ion. Therefore, the sample queries typically represent only the popular or head queries. Since such queries are relatively easier to be answered by search engines, the quality estima-tions made by these works tend to be highly optimistic.
There are also some large-scale evaluations and average performance estimates made within the major search com-panies. Unfortunately, these cannot be used to answer the questions raised in this paper due to the following reasons. First, standard aggregate performance measures (e.g., the average NDCG measure [27]) cannot estimate in absolute
Note that this number is quite large as editorial judgments are performed for all major search engines.
Relevance is decided based on the web page content. terms how good or bad web search is. Second, since evalu-ations are conducted on a particular system and under spe-cific conditions, they cannot be compared to the evaluations on other search engines. Third, evaluations tend to be bi-ased since sample query frequencies are used for aggregation, without the correction proposed in this paper (see Section 4).
In our work, we use 1000 truly randomly sampled queries whose frequencies are corrected using a sample query log of 50 million queries. Hence, we expect to obtain a quite repre-sentative view of search engine behavior. Nevertheless, the question remains: can we represent the richness and com-plexity of today X  X  web search queries by only 1000 queries
User sessions. A number of works pointed out that users tend to rewrite their queries several times before they reach their goal. This is especially true for difficult informational queries. For such queries, perhaps, the entire user sessions should be sampled and judged instead of individual queries. Unfortunately, detecting user sessions [21, 25] with sufficient accuracy is still an open research problem, and our query log does not have any information on sessions or query rewrites.
Human judges. Perhaps, editorial judgment is one of the main issues in relevance evaluation [3]. Since judges are not originators of queries, the intent or information need of users must be identified by the judges themselves. For cer-tain queries, it is extremely difficult to identify the intent. Moreover, since the judges are not necessarily experts on many intents, it is difficult to accurately evaluate the qual-ity of search results. To mitigate these effects, commercial search engine companies use highly trained full-time editors, who can achieve quite high agreement rates, as in our case.
Shallowness of evaluations. In this work, we consider only the top five results of each search engine, but it is possi-ble that there are large differences between the engines below the fifth rank. It is known that, in web search, a user who does not see any relevant results in the top results is very likely to reformulate the query. Nevertheless, for very diffi-cult queries, users are willing to check the results at lower ranks. In these cases, our estimations of user satisfaction are pessimistic since such queries are treated as unsolved.
Misspelled queries. All three search engines studied correct common misspellings automatically. Therefore, for misspelled queries, the returned results are related to the correct spelling (or to some assumed correct spelling) of the query. The editors in our study take this into account. If they believe that a query is misspelled, they judge the results with respect to the correct spelling of the query. Hence, it is possible that many queries that we treat as solved are, in fact, misspelled queries that, strictly speaking, would return poor or no results. Moreover, we should note that search engines are sometimes unable to correct the spelling, and they return poor or no results. In our sample, 2%  X  0 . 5% of the queries return no results, and about two-third of them are clearly misspellings. Herein, we have treated all such queries as hard, but this is a debatable decision.

Relevance measure. Herein, because of two reasons, we use DCG [26] instead of NDCG [27] as our quality measure. First, DCG is shown to correlate better with user satisfac-tion than NDCG [1]. Second, we have very few judgments per query. Hence, for some queries, we may miss good re-sults, obtaining only poor ones. NDCG can have fluctuation in such queries, rendering it not suitable to our purpose.
According to the finding in [28], a query log with 650 queries is sufficient to reliably estimate significance.
In evaluating a search engine, one is typically interested in the expected performance, i.e., the average performance ex-pected over a large set of queries. Unfortunately, estimating the expected performance based on the performance of indi-vidual queries is not trivial because query frequencies follow a power law distribution [44]. Taking this into account dur-ing query sampling is crucial and can have a dramatic impact on performance estimations. In this section, we discuss this problem and propose a performance aggregation technique, which yields a much better estimate of the expected perfor-mance. We also show that, without this technique, it is hard to make quantitative statements about the quality.

Query samples are weighted sets, in the sense that the same query can be sampled multiple times. Let S be a sam-ple query set. For every q  X  X  , we have a sample frequency f ( q ), i.e., the frequency of q in S . Moreover, let us have as the relative frequency of q in S . If S is an i.i.d. sample of a distribution of queries, b P S ( q ) is the empirical plug-in estimator of the prior probability P ( q ) of q in the entire log. Also, let us assume that there is a discrete probability distribution Q  X  of queries issued to the search engine. If we denote the relevance measurement 6 on q by M ( q ), the expected performance of the search engine becomes where P Q  X  ( q ) is the true relative frequency of q . Unfor-tunately, (2) cannot be computed directly since it requires evaluating every query q and also knowing P Q  X  ( q ), neither of which is feasible. Instead, it is standard to i.i.d. sam-ple Q  X  and obtain a relatively small sample query set Q E (and sample frequencies). This set is then used to obtain a (maximum likelihood) empirical estimate of H . If the true relative query frequency distribution P Q  X  was known, we could approximate the true expected performance as In other words, we would sample as many queries as possi-ble, and then use their true probability P Q  X  ( q ) in aggregat-ing. Unfortunately, we cannot evaluate (3) because P Q  X  is unknown. Therefore, we must estimate it from samples. To our knowledge, all previously published work use one of the following two approaches for this estimation.
 A. Q  X  is assumed to be uniform, and we compute This can be any standard retrieval performance measure. B. The plug-in estimator b P Q E ( q ) is used to compute
Although the above-mentioned approaches have their value, they are both problematic and considerably distort the true expected performance. The reason for this is that the frequency distribution is very far from uniform (as A as-sumes). In fact, it is well known that the query distribution follows a power law [44] (at least in web search). For this reason, plug-in estimators on small or medium size samples (as used in B ) are also very inaccurate. Indeed, in power law distributions, the frequency plug-in estimator is not accurate unless the sample is extremely large, or the  X  coefficient is close to zero (making the distribution close to uniform).
In practice, the financial cost of evaluation prohibits search engines from having sufficiently large evaluated sets. This has the effect of making the plug-in estimator a very poor estimator, grossly underestimating the effect of head (frequent or popular) queries. We note, however, that fre-quency is independent of performance, and therefore we are not tied to the Q E set to evaluate frequencies. Indeed, we can use any available large set (even if it is not evaluated) to obtain a better estimate of the relative frequencies. Since evaluated queries are obtained by sampling query logs, it is possible to obtain a second, much larger sample to estimate the query frequency. This leads to the option we propose:
C. We obtain a larger (non-evaluated) sample Q U such
To demonstrate the problem empirically, we use a query log of 50M queries and construct samples (i.i.d. with re-placement) of sizes 1K, 100K, 10M, and 50M. In Fig. 1-a, for each sample, we plot query frequencies against ranks of queries. We observe that as the sample becomes smaller, its frequency range becomes flatter and flatter, approaching the uniform distribution. This demonstrates that the prob-abilities computed using small samples are far from real.
To show the impact of the proposed frequency correction approach, we perform the following experiment. We pick the queries in the 1K sample, and compute their frequencies sep-arately in all four samples. This simulates the effect that, in reality, the query set is fixed in advance. Fig. 1-b shows the obtained frequencies, where the queries are sorted by their rank in the 50M query sample. We observe that increasing the size of the sample on which the frequencies are computed helps to recover the original power law distribution. logarithmically. b) The effect of frequency correction.
In order to evaluate the quality of a search engine, most studies use averages over a relevance metric such as DCG. Typically, such retrieval performance measures are devel-oped to evaluate the relative quality of a ranking function and are excellent devices for model comparison (i.e., choos-ing which ranking function is better in a pool) and model selection (i.e., tuning the parameters of a particular rank-ing function). However, these performance measures are not easy to interpret in absolute terms and are hard to relate to the kind of quantitative statements that we are after. For example, if a web search engine has an average DCG of 18.7, does this mean that the web search is solved by this engine?
Herein, we propose a measure that allows making lower and upper bound statements about the quality of a search engine that is evaluated by the standard retrieval perfor-mance measures. The proposed measure first constructs several query sets of interest (e.g., solved, hard, disruptive), using standard relevance metrics. The relative sizes of these interest sets are then used to make quantitative statements about search performance. Although this measure appears to be simple and intuitive, it is more interpretable, robust, and general than the standard relevance measures.
 We now describe the proposed measure in more detail. Assume that we have evaluated a set of queries q  X  X  by some standard performance measure M ( q ). Also, assume that users are completely satisfied by the results when this measure is very high. More specifically, if the measure is higher than a fixed threshold  X  solved , the query is placed in the solved query set. Similarly, when the measure is lower than a fixed threshold  X  hard , users are completely unsatis-fied and the query is placed in the hard query set. Given these solved and hard sets, the measure we propose obtains bounds on the number of solved and hard queries by simply computing the sizes of the respective sets. These bounds let us make quantitative statements of the kind  X  X t least x % of queries are solved/hard with respect to this search engine X  (see Section 6.2). The first two rows in Table 1 define the solved and hard sets. The idea is also illustrated in Fig. 2-a.
Next, we extend our measure to enable comparisons be-tween a pair of search engines. The regions of interest used by the extended measure are defined in the last five rows of Table 1. We illustrate the process in Fig. 2-b, which shows a sample scatter plot of queries evaluated on engines I and II by some measure M . In the figure, by intersecting the two search engine X  X  respective solved and hard sets, we obtain two corner regions. We call the region at the top right cor-ner the two-engine solved set and the bottom left the two-engine hard set. These regions can be used to give bounds on the number of queries for which both search engines agree that they are solved (or hard). The top left region contains queries which are hard for engine II, but solved by engine I. We refer to this region as the disruptive-I set since, on queries in this region, engine I performs noticeably better than engine II. Hence, users who issue to search engine II a query that falls in this region may be compelled to switch to engine I. The symmetric region is similarly called the disruptive-II set. If we make no other assumptions about M , we can restrict the disruptive sets to these corners. How-ever, we make the disruptive sets slightly larger by making Figure 2: Generated query sets: a) for single-engine evaluation, b) for two-engine comparison. a third assumption about M . Although we do not trust the relative values of M in the (  X  solved ,  X  hard ) interval, we as-sume that if M is much higher for engine I than for engine II (i.e., M I ( q )  X  M II ( q )  X   X  tied ), then the user prefers engine I to II. This leads to the tied set, in which neither of the two search engines perform clearly better than the other.
An important issue in our measures 7 is to accurately iden-tify the three thresholds  X  solved ,  X  hard , and  X  tied . Note that these three thresholds must be set to be able to specify the interest regions used by the measures. In our work, to iden-tify the thresholds, we followed an ad-hoc approach. We an-alyzed the judgments of many queries with different DCG values, looking for reasonable thresholds. For the  X  threshold, we picked  X  solved = 9 as a reasonable cut-off point: above this value all the queries we examined were clearly ex-cellent, and the differences between search results above this value did not appear to be significant to us. We repeated this analysis looking for a DCG threshold under which a user would be upset with the results. We picked  X  hard = 2 as a reasonable threshold. We note that these values are very conservative, but they are reasonable since the identified thresholds are used as lower bounds. Furthermore, the DCG
In our experiments, we use DCG@5 as the standard mea-sure. In computation of this measure, some works use linear gain functions with few levels (e.g., `  X  { 0 , 1 , 2 } ,g ( ` ) = ` in [1]). Others give higher relative weight to excellent re-sults (e.g., `  X  X  0 ,..., 4 } ,g ( ` ) = 2 `  X  1 [49]). In our work, we use g ( ` )  X  X  0 ,. 5 , 3 , 7 , 10 } for `  X  X  0 ,..., 4 } and the standard logarithmic discount function d ( r ) = 1 / log 2 (1+ r ). distribution is linear around these values (see the discus-sion in Section 7). Hence, small changes in these thresholds may cause only small changes in the statistics obtained us-ing them. Finally, to identify  X  tied , we determined at which DCG value the quality difference between the engine pairs becomes distinguishable. This threshold is much harder to set and is probably not constant 8 , but its effect is small as it simply removes queries from consideration. Furthermore, as we will see in Section 7, the relative disruptive set sizes are stable with respect to the choice of this threshold. In our case, we set  X  tied = 1, which has the effect of removing queries having DCGs within 2 points of each other.
In this section, we analyze the aggregated performance of three search engines (Google, Microsoft Live Search, and Yahoo! Search) 9 by using the proposed quality measure (see Section 5) with uniform , sample , and corrected aggrega-tion methods (see Section 4). The solved and hard set sizes, obtained by averaging the three engines X  set sizes, are dis-played in Table 2. Herein, we discuss only the unique and corrected aggregation methods since the sample aggrega-tion method is always very close to unique and it is not informative anyway as it is too biased. In Table 2, when we look at unique queries ( unique ), we observe that a large fraction of queries remain unsolved for most search engines (23%), whereas less than half (47%) lead to excellent results on any engine. Moreover, we note that there is a high level of agreement between search engines on these numbers: all three evaluated search engines are within 3% of each other.
If we take into account the frequency of queries ( cor-rected ), however, this picture changes radically. In this case, we observe that there are almost no unsolved queries for any search engine ( . 3%  X  . 1%), and almost all queries lead to excellent results for all search engines (93%  X  1%). This is not very surprising as frequent head queries tend to be easier than infrequent tail queries. This is for several reasons, which are discussed in the next section.

In our opinion, the results obtained by both unique and corrected are interesting as they give different views of search engine quality. For a single user asking difficult queries, unique is a more interesting indicator of quality. Furthermore, it is more useful for comparing search engines as most search engines perform well in answering frequent queries. However, corrected depicts a better picture of the expected quality of a search engine in our daily lives. The results we obtained explain well why we depend on search engines so much: they work most of the time!
A comparison between the performance results obtained by unique and corrected shows that frequent queries are much better solved than infrequent queries. One explana-tion is the relative easiness of navigational queries [10], which are typically very frequent. Since our editors labeled nav-igational queries, we also apply our measure to only navi-gational and non-navigational query classes. According to Table 2, we observe that 30% of unique queries are naviga-tional and they constitute 87% of the volume. Indeed, look-ing at the sizes of the solved set, we confirm that search
It may be a function of the absolute DCG of the engines.
In order to avoid a debate over which search engine is bet-ter, throughout this paper, we report the obtained results without identifying individual search engines. In all plots, we use letters A, B, and C (arbitrarily mapped to engines). Query class Figure 3: The log-log scatter plot showing the fre-quencies of queries against their DCG values. engines excel in solving navigational queries. We observe that 81% and 98% of such queries lead to excellent results in all search engines for unique and corrected , respectively. This implies that search engines succeed, to a large degree, in solving navigational queries, not only the popular ones. This is also confirmed by the fact that the hard set sizes are 9% and . 2% for unique and corrected , respectively. Moreover, we note that the small spread (.4% and 0%, re-spectively) indicates that all three search engines have close performance. In conclusion, we can state that navigational search is mostly solved. It remains to investigate what is not solved in the remaining 9% ( unique ) of navigational queries.
Our findings are somewhat contrary for non-navigational queries. We first note that there are many more unique non-navigational queries (70%) than navigational queries. How-ever, they constitute only 13% of the query volume ( cor-rected ). The performance on non-navigational queries is much worse than that on navigational queries. With unique , only 33% of non-navigational queries lead to excellent re-sults, and more than 29% lead to poor results. We note that there is higher search engine variance here: the spread of these measurements is 3% and 4%, respectively, mean-ing that the performance difference between search engines is larger for non-navigational queries. With corrected , we observe that the solved set size is much higher (65%), but is still far from the performance on navigational queries. More interestingly, the hard set size with corrected is 29 times smaller than with unique , indicating that frequent non-navigational queries are much better solved by search engines than less frequent ones. This challenges the previ-ous explanation that search engines work because they have solved navigational queries: they work because they have solved all frequent queries, whether they are navigational or informational! This is further demonstrated in Fig. 3, where we show the DCG scatter plot for non-navigational queries. According to the figure, only low frequency queries lead to poor results. Again, the spread between search en-gines is high (4%), indicating that some engines can cope with non-navigational queries significantly better than the others. This is clearly where the web search engine innova-tion battles are being played out in the relevance field.
In this section, we compare the performance of a pair of search engines on a query per query basis. This allows us to investigate the question of whether search engines achieve similar result qualities for the same queries, or on the con-trary, some engines are better than the others for certain queries. To be able to perform this analysis, we employ the extended measure proposed in Section 5.

This analysis is interesting for a number of reasons. First, we are interested in knowing whether the hard set of engines is fundamentally hard, i.e., all search engines perform poorly on the queries in this set, or instead, there are queries that are hard for one search engine but not for another. Second, we are interested in verifying whether the solved set is not only equal in size for all search engines (as shown in the previous section), but is also composed of the same queries for all search engines. Finally, we want to see if there are important differences between search engines.

Note that we are not interested here in trying to de-cide which of today X  X  commercial search engines is better. Instead, we want to derive quantitative statements about the similarity or dissimilarity of search engines with respect to result quality. As in the previous section, in order to avoid a controversy generated by any direct comparison of two commercial search engines, we report only the averages computed over three possible search engine pairs and the spreads. The two-engine solved , two-engine hard , and tied sets are symmetric. However, the disruptive-I and disruptive-II sets are not symmetric. Hence, in pair-wise comparisons, we order the search engines such that the size of disruptive-I is always larger than that of disruptive-II , i.e., the first one is always the better search engine.
Table 3 summarizes aggregate results over all queries. We observe that the size of the two-engine solved set is 40% with unique (91% with corrected ) and has a small spread. This is not far from the 47% ( unique ) and 93% ( corrected ) values observed in single-engine evaluation (see Table 2 and the discussion in Section 6.2). This shows that queries solved by one engine tend to be solved also by the other two, i.e., all three engines perform perfectly on similar queries (at least 40% with unique and 91% with corrected ). Similarly, there is a high overlap in the hard set. At least 17% ( unique ) of queries lead to poor results in all three engines. Since the single-engine hard set size was 23%, this constitutes an overlap of poor results in search engines of at least 74%. However, these queries have very low frequencies. Hence, the two-engine hard set size is only .2% with corrected . Given Query class that these are only lower bounds, we can only conclude that there is an important body of queries (at least 17% with unique ) that are not solved by any of the three engines.
Herein, we also try to identify any other significant dif-ferences between search engines. According to Table 3, the size of disruptive-I is quite large (21% with unique and 4% with corrected ), larger than the two-engine hard set. We can therefore claim that there is a large number of unique queries that are significantly better solved by the superior search engine. Interestingly, the size of disruptive-II is also large (13% with unique and 2% with corrected ). If there was a search engine that solved all the queries that the other engines could not solve, then we would expect at least one disruptive set size to be close to zero, and hence the spread should be larger than average. However, this is not the case. This leads to the conclusion that there is always a significant number of queries that are better answered by one search engine than by the other, for all three search engines considered. Furthermore, these queries have non-negligible frequencies. Taking this further, we can conclude that there are significant differences in the three search engines in terms of their ranking functions or their crawls. These differ-ences are specially notable for low-frequency queries and non-navigational queries, but we can see in Table 3 that also minor differences exist for navigational queries.
Inadequacy of standard measures. We already men-tioned that correlating standard measures with absolute search quality is very difficult. It is even more difficult to interpret relative differences between search engines when they obtain similar results on these measures. We explain the problem by some examples. A ranking of the form Puuuu (perfect followed by unrelated) has a DCG@5 of 10, whereas uPuuu only 6.3. The following rankings have DCG@5 close to 8.5: RVRuu , uVRRR , uPrur . But how good (or bad) are these results in absolute terms? And why do we as users perceive some engines as being better than the others?
DCG values depend on a gain constant and a discount function, accumulated over all ranks. This makes it very hard to make absolute quantitative statements about search quality by using DCG results. Fig. 4 shows the cumulative probability distribution obtained in our evaluation 10 respect to DCG@5. In Fig. 4, we observe that 50% of queries have a DCG@5 close to 9 or higher, which are excellent results. But, we note that the values are not concentrated around their mean at all. The DCG@5 increases almost linearly from . 1 to 16, indicating that almost all values are equally common. We can draw two conclusions based on this observation. First, because of the shape of the DCG
Average DCG@5 range ( unique ) of all engines is 8 . 5  X  . 3. Probability(DCG(q) &gt; threshold) Figure 4: Cumulative probability distribution of DCG(q), reversed to emphasize which % of queries have at least some DCG value. distribution, the mean is not prototypical, i.e., queries have low likelihood of being near it. Second, if we consider that the mean is close to our notion of an excellent result (or a solved query), we see that DCG does not have the right resolution, i.e., it over emphasizes relative differences in very good results (going from 9 to 27) while underemphasizing differences in poor results. This is an indication that we should be more interested about the logarithmic DCG, or in other words, we should pay more attention to the geometric mean rather than the arithmetic mean 11 .

Stability of set sizes. An important issue in our analysis is the sensitivity of the obtained set sizes to selection of thresholds. In particular, we are most worried about the stability of results with respect to |S tied | since the tied set corresponds to a dense region of the scatter plot and small variations in  X  tied may have a strong impact on the results. However, as we will now illustrate, this is not the case in practice. In Fig. 5, we display the ratio of the disruptive of  X  tied . As usual, we show both the average (for the three pair-wise comparisons that are possible) and the  X  region in which the three averages lie. We observe that the computed
When we computed the geometric mean, we indeed saw slightly larger differences between the three search engines, but they were still ranked in the same order. ratio is quite stable, increasing very slowly between 1.5 and 2, regardless of the  X  tied threshold chosen. Similar curves can be drawn for  X  hard and  X  solved with similar results. These are less interesting since these two thresholds are more robust and cover less dense areas of the scatter plot.

Generality of thresholds. We believe that it is possible to find reasonable thresholds for most measures commonly used in retrieval evaluation. In order to verify this, we tried a measure different from DCG and observed what the re-sulting solved and hard set sizes are. We need to note that it is quite hard to find an alternative to DCG since the data is multi-valued and only the top 5 documents are eval-uated. Furthermore, when binarizing relevance, measures like precision and MRR on the top 5 results are very coarse, having only 5 possible values. Nevertheless, we found that, using reasonable threshold values for these measures, we can obtain solved and hard set sizes similar to those obtained with DCG. As an example, we replaced the previously used DCG@5 metric with P@5 by mapping perfect, very rele-vant and relevant labels to relevant, and the rest to irrel-evant. With these binary labels, when we set  X  solved = 0 . 5 and  X  hard = 0, the solved and hard set sizes we obtained are 48%  X  2% and 21%  X  3% ( unique ), respectively. Both values are within 2% of the values obtained using DCG.

Other factors effecting results. Besides query fre-quency, there are other factors that have an impact on our results. One factor we investigated, although we do not re-port any results, is the query length. We found that shorter queries are indeed much better solved than longer ones. Sim-ilarly, performance of the engines varied depending on query classes (e.g., person, organization, place queries). We should note that human factors, which are outside the scope of this study, may also have a strong effect on the results. The first factor that comes to mind is human adaptability [46]: as users learn to use search engines, the intuition is that they stop asking queries that they do not think the engine will be able to solve. This promotes a rich gets richer behavior, where frequent queries by definition obtain better results.
Prioritizing investment. Commercial web search en-gines make significant investments 12 to improve the qual-ity of their search results. Based on the proposed qual-ity measure, an investment can be made on improving the queries in one or more of the five sets identified (i.e.,
In this context, investment may refer to financial costs, time spent, or the amount of human resources allocated. two-engine solved , two-engine hard , tied , disruptive-I , disruptive-II ) 13 . It is important to prioritize these sets for investment so that the highest search quality increase can be achieved with the least amount of investment. One option is to invest on the queries in the two-engine solved set as they are relatively easy queries. However, these queries are already well-solved, and hence it may be difficult to improve them further. Even if they can be improved, users may not be able to notice any quality difference. Another option is to improve the queries in the disruptive-I set as some of them may have room for improvement. However, it is per-haps better not to prioritize them for investment since they do not form an immediate threat, as the rival search engine performs relatively poor on these queries. A third option is to try to optimize the queries in the two-engine hard set. If these queries are improved, the benefit can be significant. However, these queries are really hard as indicated by the fact that the rival search engine so far could not solve them either. Therefore, investment on such queries is risky and may not be cost-effective. In our opinion, the primary target for investment should be the disruptive-II set (followed by tied ). The queries in this set have room for improvement as the rival search engine is known to have them solved. More-over, solving these queries prevents user switches to the rival search engine. In summary, the following prioritization of sets may be the most feasible: disruptive-II &gt; tied &gt; two-engine hard &gt; disruptive-I &gt; two-engine solved . Ideally, the variation in set sizes needs to be followed in time, and necessary actions should be taken accordingly. For example, the hard set should be targeted only after the disruptive-I and tied set sizes are sufficiently small.
Evaluation measures. There are two lines of research in search evaluation measures. The first line of research in-vestigates the measures for relevance of search results. De-spite the wide range of proposals (e.g., P-R [26], ESL [17], RHL [9], RR [9], ASL [37], SR [42]), only a few measures are commonly used in traditional IR evaluations (e.g., pre-cision [52], recall [52], and MAP [11]). In case of availability of graded relevance judgments, which is mainly the case in internal search engine evaluations, CG [26], DCG [26], and NDCG [27] are the preferred measures. The latter two mea-sures take into account the ranks of documents. More recent measures such as RBP [39] and ERR [14] also consider the information gained by the user after viewing each rank.
The second line of research aims to develop measures to compute the distance between two given rankings. Two widely used measures are Spearman X  X  footrule [47] and Kendall-Tau [29] measures, which compute the distance be-tween two full rankings. These measures are later extended to distance computation between partial rankings [19] and top-k lists [20]. More recent works adapt these measures to handle graded judgments [31] and incorporate a DCG-like decay with increasing rank [13, 31, 54].

User studies. There have been numerous user studies for evaluating and comparing result qualities of search engines. Most of these works concentrate on evaluating the relevance of search engine results that are judged by humans, accord-ing to an average performance measure [5, 16, 18, 22, 23,
During the discussion, we assume that disruptive-II is the disruptive set of a rival search engine company. 32, 34, 43, 51]. Unfortunately, most of these studies are very small-scale (the largest has 100 queries [34]) and are obsolete due to the fast-changing nature of search engines. Therefore, it is hard to draw useful conclusions from them.
There are a few user studies in which search engines are evaluated explicitly for user satisfaction [1, 7, 36]. In [36], a user study (1000 queries) is carried out with students. It is found that Google is significantly better than Yahoo! and MSN Live at navigational queries and slightly better at informational queries. It is noted, however, that the gap started closing in 2007. The results we report are worse than those reported in [36] (perhaps, due to the fact that we are looking at lower bounds), but we confirm that the gap has indeed closed. In [1], a methodology is proposed to measure user satisfaction by observing user X  X  actions after the results are presented. In that study, user actions (e.g., copy-pasting, printing, saving, emailing) are used to estimate the level of importance of a result for the user. A user study is con-ducted in [1] to investigate the relationship between user satisfaction and retrieval effectiveness measures. This study showed that user satisfaction correlates better with the CG and DCG measures [26] than the NDCG measure [27].

Automated evaluation. Several studies investigated how search engines can be evaluated in the absence of rel-evance judgments [12, 15, 35, 41, 45]. A thread of re-search papers [35, 41, 45] suggested downloading the content pointed by the search results and assigning relevance scores via standard query-document similarity computations. Ob-tained similarity values are then used in estimating relevance of documents and evaluating search engines. In another thread [15, 41], some pseudo-relevant documents are auto-matically identified and used in evaluation. In [15], pseudo-relevant documents are identified by matching queries with the documents in the open directory project. Search engines are then evaluated by their ability to return these documents in high ranks. In [41], multiple search engine rankings are merged via rank aggregation and a certain fraction of top ranking documents are assumed to be relevant.

Other evaluation criteria. Besides relevance, there are a number of works that compare search engines across other criteria. Some of these works evaluate content-related issues, such as web coverage and speed in indexing newly published content [30], index freshness [33], index size and overlap [8], consistency in hit count estimates [50], and bias in web cov-erage [53]. Other works evaluate issues related to presented results, such as result page structure [24], change of results in time [6], uniqueness of results [48], result similarity [4], bias in results [40], and the coverage of domain names [50]. These studies are interesting as they highlight the fact that the search engine quality does not depend only on the rank-ing of results, but rather on many interrelated factors. In this work, we concentrated only on the result quality.
Search difficulty. We are aware of two works that ask questions similar to ours [2, 38]. In [2], a user study is con-ducted to understand the relationship between effectiveness of users in achieving a task and the accuracy of the retrieval system. The experiments indicate that there is no significant difference in utility unless the search accuracy is improved by a large amount. In [38], difficulty of search is estimated by measuring the entropy of the query logs. This study con-cludes that users often find the documents they are looking for in the top 10 results of search engines. However, the study does not provide a quantitative analysis as we do.
Throughout the paper, we pointed out many caveats and open issues in search quality evaluations. We demonstrated the important problem of query frequency aggregation and proposed a technique to correct it. In our opinion, this can have an important impact in many future evaluation studies. We developed novel measures to make quantitative state-ments on lower and upper quality bounds of web search en-gine rankings. We tested the proposed measures on a large, real-life, professionally evaluated web search query sample. We provided bounds on what fraction of queries are solved or hard. Moreover, we showed that all three major search engines solve navigational and frequent non-navigational queries, but there are differences in how they treat infre-quent non-navigational queries. Interestingly, each engine has a non-negligible disruptive set, on which it performs sig-nificantly better than the other two engines. We hope to extend this work to different query classes and connect it to related research areas (e.g., user session and click analyses).
