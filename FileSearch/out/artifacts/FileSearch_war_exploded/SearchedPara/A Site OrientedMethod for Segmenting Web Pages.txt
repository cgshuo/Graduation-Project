 Information about how to segment a Web page can be used nowadays by applications such as segment aware Web search, classi cation and link analysis. In this research, we propose a fully automatic method for page segmentation and evalu-ate its application through experiments with four separate Web sites. While the method may be used in other appli-cations, our main focus in this article is to use it as input to segment aware Web search systems. Our results indicate that the proposed method produces better segmentation re-sults when compared to the best segmentation method we found in literature. Further, when applied as input to a seg-ment aware Web search method, it produces results close to those produced when using a manual page segmentation method.
 H.3.3 [ Information Storage and Retrieval: Informa-tion Search and Retrieval ]: Search process Algorithms, Experimentation Page Segmentation, Segment Class Recent work has shown that Web pages can be sub-divided into distinct segments or blocks [5, 10, 21] (such as main content, service channels, decoration skins, navigation bars, copyright and privacy announcements), and that informa-tion about these segments can be used to improve informa-tion retrieval tasks such as ranking [1, 6, 13, 14, 28], Web link analysis [4], and Web mining [8, 19, 22].

Most of previous segmentation methods we found in liter-ature segment each Web page based on its content only, not considering the content of other pages present in the same Web site. However, since many pages belonging to a same Web site share a common structure and look and feel, we hypothesize that we can achieve a more accurate segmen-tation by taking all pages of a same Web site into account when dividing them into segments. Based on this idea, we propose and evaluate in this article a segmentation method which segments pages according to properties of the whole Web site, instead of just information obtained from a single page.

Our segmentation method adopts a DOM tree alignment strategy previously proposed in literature to solve the prob-lem of template detection [31, 33]. However, we nd no application of such strategy in literature to address the prob-lem of segmenting Web pages. While segmentation of pages and template detection may appear as similar tasks at a rst glance, there are important di erences between these two tasks. Segmentation consists in dividing a Web page into its structural blocks, each structural block may contain templates or not and may also be part of a template. Fur-ther, in a segmentation process an area that does not contain templates may be divided into several blocks.

Our segmentation method is particularly useful to seg-ment the so called data-intensive Web sites, such as digital libraries, Web forums, news Web sites, electronic catalogs, or institutional sites, whose main focus is providing access to a large quantity of data and services [9]. These sites usually contain a large set of web pages which can be orga-nized in a few tens of groups according to the regularity of their structure. Our segmentation method takes advantage of such regularity to automatically segment data intensive web sites.

Besides segmenting the Web pages, our algorithm is able to cluster the segments into segment classes , which are set of segments that play the same role in a group of pages of a site. Knowledge about the segment classes can be used to estimate a weight for each segment in a Web collection, indi-cating its importance for ranking purposes [14, 13]. In [13], the authors show how to compute these weights automati-cally, and then how to estimate term weights according to the weights of the blocks where they occur. The ranking metho d they proposed is particularly useful for data inten-sive web sites, where search is an important way of accessing data, given the large amount of information usually provided to their users. This type of Web site is also the main focus of our work here.

Even though this article focus on the use of the segments and their classes as input to segment aware Web search sys-tems, the segmentation method here proposed may be ap-plied to many other applications. For instance, segmenta-tion algorithms can be used to improve the quality of results in many Web mining applications [23]. As another example, segmentation techniques can be also useful for the contex-tual advertisement problem, in which advertisers often wish to place (or avoid placing) distinct ads on speci c regions of a same Web page [35].

To evaluate the performance of our segmentation algo-rithm, we performed experiments with four distinct Web collections to compare its results with those produced by the Rule-based Block Fusion algorithm | a recently proposed automatic segmentation algorithm that achieves good per-formance [21]. The results indicate that our algorithm out-performs the baseline for all four Web sites tested and that the results are far superior for two of the sites tested.
We also studied the impact of using our automatic seg-mentation method with a segment aware ranking search sys-tem. For this, we ran experiments to compare such ranking strategy with ( i ) a traditional ranking function that does not use segment information, ( ii ) a traditional ranking function applied to pages after template removal, and ( iii ) a segment aware ranking search system applied to manually segmented Web pages. The experiments indicate that our segmentation method, when combined with the segment aware ranking system, leads to improved results in comparison to the two rst baselines. Further, the loss in ranking quality in com-parison to the third ranking baseline is not signi cant in most cases and is small even in the worse results.
The remaining of this paper is organized as follows. Sec-tion 2 covers related work. Section 3 presents the model we adopt here to represent Web sites. Section 4 presents the proposed automatic Web page segmentation method. Section 5 reports results we obtained from experiments we run to evaluate the e ectiveness of our method in generating suitable segments and the impact of the segmentation it gen-erates in segment aware ranking systems. Finally, Section 6 concludes this article. The process of analyzing a Web site to detect structural di-visions of its pages has been addressed in several previous research articles [3, 5, 10, 16, 21]. One of the best solutions found in the literature, called Vision-based Page Segmenta-tion algorithm (VIPS) [5], segments a page by simulating the visual perceptions of the users about that page. The VIPS algorithm requires a considerable human e ort, since it is necessary to manually set a number parameters to each page to be segmented.

Chakrabarti et al [10] also consider the problem of seg-menting Web pages into visually and semantically cohesive pieces. This work uses approximation algorithms for clus-tering problems based on weighted graphs. In the case of the segmentation problem proposed by [10], the weights of edges of the graph capture the likelihood that two regions of the Web page are placed together or apart in the segmen-tation, and the clusters are the segments occurring in the pages.

Kohlsch  X  utter et al [21] examined the problem of Web page segmentation from a textual perspective. The key assump-tion in their work is that the number of tokens in a text fragment (or more precisely, its token density) is a valuable feature for segmentation decisions. Based on this assump-tion, the authors presented a Segment Fusion algorithm for Web page segmentation using text density metrics. The au-thors show that their method outperforms the method pro-posed by Chakrabarti et al . Given its quite strong results, we decided to use it as the baseline in our experiments. No-tice that this method uses less information than ours when segmenting pages, since it segments each page individually and we segment a page based on web site statistics.
Many authors have argued that information present on page templates is noisy and should be disregarded to avoid deteriorating search precision [20, 31]. Such an idea has motivated the proposal of methods for identifying noisy con-tents or templates based on the structure of the Web pages [3, 15, 20, 24, 31, 33]. However, template extraction is a task di erent from segmentation. Segmentation consists in divid-ing a Web page into its structural blocks and each structural block may contain templates or not. Further, in a segmen-tation process an area that does not contain templates may be divided into several blocks.

Our method aligns the DOM trees of Web pages of a site in order to uncover their implicit structure. Aligning the DOM trees to extract structural information from of Web sites and Web pages is not a new strategy. This idea has been used previously for Web site template identi cation [31, 33] and web mining [27, 34]. However, we nd no application of such strategy in literature to address the problem of segmenting Web pages. The adaptation and heuristics described by us to adopt the DOM tree alignment to the problem of segmenting Web pages constitute the major contribution of our work when compared to the previous application of such strategy. As we show through experiments, the DOM tree alignment strategy is an excellent alternative to solve the problem of segmenting Web pages.

Bar-Yossef and Rajagopalan have also proposed a method that uses the DOM tree of pages in a site to guide the seg-mentation process [3]. While their proposal can be consid-ered as a segmentation method, their main goal was to detect noisy information in Web pages. As the main heuristic to identify segments, they count the number of links found in each HTML element. If an element contains more than K links it is considered as a segment, otherwise it is counted as part of the element containing it. We have considered the possibility of including their method in our experiments as a segmentation method for sake of completeness, but the method would produce a quite low quality segmentation and we have not reported experiments with it.

In Cai et al . [6], the authors proposed a method for taking advantage of the segments generated for pages in a search task. They present a ranking strategy where the segments are processed as semantic passages and then apply previ-ously proposed passage-level ranking strategies [7] to evalu-ate the possible improvements obtained in ranking quality with their method. Results are superior to other previously studied types of passages, such as paragraphs or xed size windows.

The method proposed by Song et al . in [28] also adopted the vis ion based segmentation proposed by [5]. The authors used such a segmentation to compute an importance rank of segments found in a page through a learning approach. Spatial features, such as position and size of segments, and content features, such as the number of images and links on each segment, are extracted from each segment. Two learning algorithms, neural network and SVM, are used to rank the segments according to their importance.

In de Moura et al . [13], the authors propose a method for automatically computing segment importance factors to improve Web search. Such a method uses statistical infor-mation available in the Web page collection to assign an im-portance value to all segments of each page found in the col-lection. Their method produces a signi cant improvement in the quality of search results when compared to several base-lines. However, it requires a complex segmentation mecha-nism and the authors proposed no fully automatic strategy to generate it.

The segmentation methods found in the literature aim to identify the segment structure of a single page given as input. The method presented in [13] requires the identi cation of a common structure shared by a large quantity of pages. The purpose of such an identi cation is to cluster the segments into segment classes . A segment class is a set of segments that share a common role on a group of pages of a same site. Although the authors proposed the use of VIPS [5] to guide the segmentation process, the clustering of segments into segment classes requires a manual process even when using VIPS, making the application of the method unfeasible in large Web page collections.
 We here propose a novel method for automatically divide Web pages from a collection into segments. This method can be considered as a signi cant improvement in the proposal of de Moura et al ., since our experiments indicate that their segment aware Web search method produces results almost similar in terms of quality either using our new segmentation approach or using the manual approach they originally use. Unlike plain text documents, Web pages usually encompass multiple regions such as navigation bars, decoration stu s, copyrights, advertisements and contact information. Each of these regions play a particular role within the pages and are generically referred to as segments . In this Section we present some basic de nitions which are useful for better characterizing the problem of dividing the Web pages into segments. Our discussion is based on [13].

Definition 1. A segment s is a self-contained logical re-gion within a Web page that (i) is not nested within any other segment and (ii) is represented by a pair ( l; c ) , where l is the label of the segment, represented as the root-to-segment path in the Web page DOM tree, and c is the portion of text of the segment.
 It follows that a Web page n might be represented as a nite set of (non-overlapping) logical segments n = f s 1 ; :::; s with k varying according to the page structure. The de ni-tion of segments we adopt here allows several types of par-titions for a given Web page, subject to the interpretation of a human analyst or to the behavior of the segmentation method adopted. Since our segment de nition admits that any arbitrary subtree of a DOM structure be assigned as a segment, the decision of how to divide a page into segments should be done by the segmentation algorithm. In this work, we adopt the criterion that the division of a Web page into a set of non-overlapping segments must be performed accord-ing to the perception of users about the best logical division of the page.

A Web site S is represented here as a set of Web pages S = f 1 ; :::; m g , each of them composed of a set of segments. Given this representation, we can now de ne the concept of segment class . Segments of distinct pages can be clustered into classes as follows:
Definition 2. A segment class C is a set of segments that belong to distinct pages of a given Web site and that share a same label, i.e., where n C is the total number of segments in class C , l C the common label, s i is the i th segment in class C , and c its corresponding portion of text.
 To illustrate, consider the two news pages depicted on Fig-ure 1. Notice that each page contains a news title , which we refer to as title segments . Since these title segments are in the same position in both pages, they have a common root-to-segment path and thus, a same label. As a consequence, they are part of a same segment class. We also notice that segments with the same label tend to have the same role within their respective pages. That is, a segment class is usually composed of segments that belong to distinct pages of a Web site and that play the same role within the site. Various examples of segments that belong to the same class occur in the news pages depicted in Figure 1, such as seg-ments that represent the menu, the news body, the summary of the news in the page, etc.
 Figure 1: News pages 1 and 2 , extracted from CNN Web site. Algorithm 1 describes the automatic segmentation process, which tries to segment the pages close to the perception of users about how each page should be divided into segments, and how these segments should be clustered into segment classes. It takes the set of pages S found in the Web site as inpu t and produces as output a set of segment classes. Our segmentation algorithm is divided into three phases, that will be discussed in the following sections. The rst phase of our algorithm takes as input the pages of the site. In this phase, the DOM trees of the pages are pre-processed to obtain a structural representation that is closer to our de nition of segments and segment classes. For instance, we use this phase to identify and remove nodes of the DOM tree that may produce nested segments (that are not allowed by our model). It is also used to aggregate regions of regular structures, such as menus or lists of items, which would be seen by a user as a single segment, but that would be broken into several small segments by the heuristics we adopt in the second phase.
 Algo rithm 1 Segment ( S )
Furt her, the pre-processing also is used to aggregate fur-ther information about each node and adjusts the DOM trees to be used in the second phase of the algorithm. Fig-ure 2 depicts a segment of HTML code and the pre-processed version of its DOM tree. To simplify, only the leftmost node of the DOM tree depicts the additional information intro-duced by the pre-processing phase. This example is used in the following sections to show how each individual page is pre-processed (see Lines 1{4). The function P reP rocess of our algorithm consists of four main operations which are described on the following sections.
 Figure 2: An example of HTML code and the pre-processed version of its DOM tree.
 Given a page , its DOM tree is a structure that represents the hierarchical relationship between the tags found in the page. Each tag is represented by a node N that contains information about the name of the tag ( N:name ) and a list of its attributes ( N:attr ), where each attribute is composed by a pair of name ( N:attr:name ) and value ( N:attr:value ). We use this information to recursively de ne the label of each node N ( N:label ) as the concatenation of its N:name , N:attr:name eld values and the label value of its parent in the DOM tree (considering it as empty when the node is the root of the tree), separating each token in the concatenation by a slash. Whenever two sibling nodes get equal values assigned by this strategy, we distinguish them by adding a sequential number to their label values, so that each node has a unique label .

For instance, in the leftmost node of Figure 2, the value of name is\div"and the only value of attribute name is\class", thus attr.name= f \class" g . Its label is\1/div/class/body/html". The value \1" is added because there is a second node that would get the same label (see the rightmost div tag). This node is the identi ed by\2/div/class/body/hmtl"in our rep-resentation. For the node labelled as \img" in the Figure, the label is \img/body/html".

Although information about textual content is not in-cluded in DOM tree, it is useful for our segmentation al-gorithm. Thus, we add to each node N a boolean value ( N:f lag ), which indicates whether it has textual content as-signed to it ( N:f lag = true ) or not ( N:f lag = f alse ). Our segmentation algorithm assumes that only the leaf nodes of a DOM tree may have textual content assigned to them. This assumption is important in order to avoid the genera-tion of nested segments during the future phases of our algo-rithm. Thus, whenever we nd internal nodes with textual content, we do not represent their children in our DOM tree representation, and thus it becomes a leaf node. The textual content associated to the removed tags is then associated to this new leaf node.

For instance, let us consider the second tag &lt;div&gt; in DOM tree of Figure 3. Notice that such a tag contains a text directly attached to it (Last year in Orlando...). Besides, inside this text there is a tag &lt;a&gt; , that also has a textual content (anchor text). In this case, we say that the content of &lt;a&gt; is nested to the content of &lt;div&gt; . As nested con-tents can generate nested segments in the future phases of our algorithm, the tag &lt;a&gt; is removed from our DOM tree representation, and its textual content is associated to the content of &lt;div&gt; . Figure 3: Example of tag with nested content.
 Another change we perform in the DOM trees is to remove sequences of tags disposed in a recurrent (or regular) way. In the DOM tree of a typical Web page, it is very common to nd regions with a quite regular structure. Let us con-sider the Web page depicted in Figure 4. Notice that the menu signalized by the Arrow 2 is formed by a set of links disposed in a vertical bar. As the set of tags that separate two adjacent links is always the same, we can say these tags are disposed in a regular or recurrent way. Figure 4: Recurrent structures: the regions pointed by the arrows contain sets of items (links or books) disposed in a regular or recurrent way.

Regions with recurrent structure commonly contain a list of items highly related to each other such as list of links, products, or paragraphs (notice that the regions signalized by the arrows 1 and 3 are also examples of regions with recurrent structures). Thus, each list has a single purpose within their respective pages and then would probably be seen by an user as a single segment. In this way, the DOM subtree of these regions is represented by a single node in our DOM tree representation, in order that all textual content of the subtree with recurrent structure become directly linked to this node. This process is depicted in Figure 5.
Identifying recurrent structures in DOM trees is a pro-cedure commonly executed in methods for extracting data from HTML pages [25, 34]. The motivation is that the infor-mation to be extracted is often placed in a particular order, and repetitive patterns can be found in these Web pages when multiple records are aligned together.
 Figure 5: Recurrent substructures of the DOM trees are not represented in our DOM tree representation. The second phase of the Algorithm 1 creates an auxiliary hierarchical structure named SOM tree (SOM is an acronym to Site Object Model ) that summarizes the DOM trees of all pages found in a Web site (see Lines 5{18).

Each node N s of the SOM tree contains the same informa-tion found in a node N of our DOM tree representation, but has two extra elds: N s :counter , with the number of pages where it occurs in the site (i.e., the number of pages that contain a node N such that N :label = N s :label ); and N :pageList , with the list of pages where it occurs associ-ated to textual content in the site. Notice that the size of N :pageList may be smaller than the value of N s :counter , since only tag occurrences associated with textual content are inserted in N s :pageList . A single tag may occur in a page associated to content and appear in a second page with-out any content.

As an example of a SOM tree construction, consider a very simple Web site S formed by only two pages, 1 and 2 , depicted in Figure 6. A SOM tree for this site can be con-structed by merging 1 and 2 in a single structure. Phase 2 of our algorithm starts by reading page 1 , through a pre-order traversal (Lines 8{17). For each node N of 1 , the algorithm checks if the SOM tree already have a node N s such that N s :label = N :label (Line 9). However, since the SOM tree is created empty (Line 6), all nodes N found in this rst page are merged into SOM tree through the func-tion insertN ewN ode , that is called on Line 15, and detailed in Algorithm 2. The insertion point is guided by the value of N :label . Function insertN ewN ode assigns the value of N :label to N s :label , and assigns 1 to N s :counter . If N has textual content ( N :f lag = true ), then function insertN ewN ode also adds the id of page 1 (value 1) to the pageList of the node N s .

After that, the algorithm reads the second page of S , 2 , to merge it into SOM tree . In Figure 6, we can see that this page starts with the tag &lt;html&gt; . In the Line 9, it is veri ed that the SOM tree has already a node N s with the label html , and thus the algorithm only updates the information of N s (Lines 10{12): its counter value is incremented.

In Figure 6, we see that the representations of 1 and 2 diverge below the nodes div on right. Below this node, page has the nodes h1 and pre , and page 2 has the nodes h1 and p . As the nodes h1 have the same path in the DOM tree of both pages, then they result in a single node in the SOM tree . On the other hand, the nodes pre (in 1 ) and p (in 2 ) occur only once in their respective pages. Thus, each of them results in a di erent node in the SOM tree . The counter of these two nodes is 1 in both cases. The third phase of our algorithm has the goal of removing nodes of the SOM tree that would be considered by a human Figure 6: An example of a SOM tree construction.
 The pageList values are depicted below each node of this structure.
 Algo rithm 2 insertNewNode( SOM tree , N , i ) as in ternal parts of one or more segments of the site. By removing these nodes, that we refer to as noisy nodes, the SOM tree becomes a structure formed only by nodes that belong to the label (root-to-segment path) of one or more segments, and each each leaf node of this new structure will refer to a distinct segment class of the site.

To better illustrate what is a noisy node, let us consider that there is a table inside the body of a news page. This table would probably be considered by a human as part of a single segment containing the body of the news. However, when including this page in the SOM tree , the content of the table would create new nodes in the structure, and thus would let the segmentation system to consider the table as a separate segment.

When analyzing the results produced by Phase 2 of our segmentation algorithm for several samples, we realized that in general, these noisy segments occur when a given set of pages has a region with pure textual content, while another set of pages has textual content mixed with other type of information in the same region, such as a table containing text, or a di erent textual style. When humans see such variations, they can easily realize that the region has the same function in both sets of pages. However, the small structural di erences introduced a ect the representation of the site in the SOM tree . The main goal of the third phase of our algorithm is to remove the noisy nodes, what is done by applying two di erent heuristics.

The rst heuristic performs a join of nested nodes that have textual content and that occur close in the SOM tree The distance adopted is the di erence in the depth between the two nested nodes analyzed. We noticed that as this distance increases, also increases the chance that nodes rep-resent di erent segment classes of the site. On the other hand, when this distance is small, it is mostly due to small variations in pages, and thus the nodes probably represent the same segment class.

The ideal value for this distance, represented by the thresh-old in Algorithm 1, can be empirically set through experi-ments. An alternative to avoid the necessity of such thresh-old is to use a clustering algorithm to group the pages of the site according to their templates [11, 12, 30]. After cluster-ing the pages, we can run the algorithm 1 for each cluster. This procedure would avoid the presence of pages with large di erences in their structure as input to the segmentation algorithm, and then would allow all nested content to be joined, without a risk of joining segments of pages with dis-tinct structure. On the other hand, the use of clustering would make the segmentation process more expensive and the results we obtained by using our heuristic were good enough for avoiding such extra computational cost.
As an example, consider the SOM tree segment depicted in Figure 7(a). Notice that this segment contains two nodes, div and p , and that the p node is descendant of the div node. As one can see, there is a small variation in the structure in this region of the pages, since a set of pages contains textual content connected to the rst node ( div ), and another set of pages contain textual content associated to the second node p .
 Figure 7: (a) A SOM tree segment example and (b) its pruned version, considering a threshold equal to 6. Notice that, instead of presenting the list of docu-ments in each leaf element node, this gure presents only the number of documents in each list.

Line 23 of the algorithm 1 veri es whether the distance between these two nodes is smaller than the threshold or not. If it is smaller, then the pageList of the descendant node is appended to the ancestor, and the descendant node is deleted from the SOM tree (Lines 24{25). On the other side, if this distance is greater than the threshold , then we assume that these nodes refer to distinct segment classes. In our experiments, we empirically set the value 6 for this threshold based on training experiments. Considering this value for the threshold , the node p depicted in Figure 7(a) must be pruned during the re ning process, and its pageList merged to the pageList of the node div . The result of this process is depicted in Figure 7(b).

Notice that, although the join can fail in some cases, it does not cause a signi cant change in the nal result of the method, since if we set threshold too high, the only change is that a few segments with di erent classes may be joined in a single class. On the other hand, if we set it too low, some small di erences in the structure of pages may cause the split of segments considered by humans as single segments. While these changes may contribute to result in a set of segment classes di erent from the ones produced by humans, the imp act of these changes for ranking purposes is expected to be small for Web sites with regular structure, since only a few pages would have a few changes in our proposed term weighting scheme.

The second heuristic joins all the children of a node N to it whenever all of them have counter value smaller than a threshold (Lines 30{38). This parameter is related to the minimum quantity of elements in a segment class in order to allow safe use of the segment aware ranking method we adopt in the experiments. This parameter is proposed and studied by Moura et al [13], where authors suggest to set it as 8.

Notice that the children of a node N s can be joined to their parent only when all of them have counter value smaller than the threshold . The purpose of this rule is to avoid the generation, during this stage of the re ning process, of new nested nodes with textual content. Thus, even after this second phase of the re ning process, some nodes might remain with counter value smaller than the threshold .
The result of the algorithm after the re ning is a set of segment classes that are described by the SOM tree . Each leaf node N of the nal SOM tree represents a segment class, with label N:label and occurring in pages N:pageList . From the structure of the SOM tree it is possible to de ne how each page is partitioned into segments by performing a matching between the SOM tree and the DOM tree of the page. To validate our automatic segmentation method, we per-formed experiments on 4 real Web collections namely IG, CNN, CNET, and BLOGs. The results for these experi-ments are discussed in this section. The IG collection, which contains 34,460 pages crawled from one of the largest Brazilian Web portals (see www.ig.com.br), is composed of a recipe site, a forum site, and a news Web site, as detailed in Table 1. The set of test queries used in ranking experiments for this collection is composed of 50 popular queries extracted from a log of queries submitted to the IG Web search service. Relevance assessments for these queries were made by 85 volunteers from 5 di erent Brazilian universities. A pooling method [17] was used to collect rel-evance judgments. For each of the 50 queries, we composed a query pool formed by the union of the top 20 documents retrieved by each ranking method considered in our experi-ments, and then presented those documents in random order to the users. To avoid noise and erroneous judgements, each document retrieved by a given query was evaluated by three distinct volunteers. We considered a document as relevant to a query when at least two of the volunteers considered it as relevant to that query.

The second collection is a crawling of the CNN Web site composed of 16,257 Web pages. The queries used for ex-perimentation with this collection were proposed by 50 vol-unteers, such that each volunteer wrote a single query. We then inspect each one of the proposed queries to ensure that all of them are representative. As in the IG collection, for each query submitted we computed a pool formed by the union of the top 20 results of all the methods we considered.
The third collection was obtained by crawling four Web sites aliated to the CNET Web portal (see www.cnet.com): CNET News, that provides news, blogs, and special reports about technology; CNET Download, composed by a large set of pages containing free downloads; CNET Shopper, which is a virtual shop of tech products; and CNET Reviews, com-posed by a set of pages containing reviews of products. This collection contains a total of 352,770 Web pages. The 50 test queries were obtained in similar way as described for the CNN collection.

The fourth collection was obtained by crawling 9 popu-lar blogs from the top popular list presented in Technorati Blog 1 . Table 1 presents the list of the crawled blogs. This collection contains a total of 54,055 blog posts and 50 test queries, which were generated as in the CNN collection. To evaluate the results achieved by our segmentation method, we adopted the Adjusted RAND index (AdjRAND) met-ric [18, 21, 32]. The Adjusted RAND index between two partitions measures the fraction of pairs of terms that are either grouped together (in a same segment) or placed in separate segments in both partitions. Hence, the higher the Adjusted RAND index between the segmentation out-put (obtained by the algorithm we want evaluate) and a manually labeled segmentation, the better the segmentation quality of the algorithm.

We also have performed experiments with a second mea-sure, named as Average Mutual Information [29]. Mutual in-formation is a measure of the quantity of information shared between two partitions, and provides an indication of the similarity between the segmentation output of an automatic method and the manual segmentation of a same page. How-ever the conclusions obtained when using this second mea-sure were similar to the ones obtained with AdjRAND and we decided to not include them due to space restrictions.
To conduce this evaluation, we manually segmented the pages of the experimented collections. First, we clustered the pages of each collection according to their internal struc-ture. We asked volunteers to mark one example for each distinct structure of pages they found in the web site, and then clustered the pages based on their structural distance from each structure pointed out by the users.

In the second step, we used the VIPS [6] algorithm to seg-ment the pages of each cluster through a manual selection of parameters. Notice that a single set of parameters can be used to segment all pages of a same cluster, given that these pages have the same internal strucuture. While this fact allows us to perform a human segmentation to produce the reference sets, still the number of structures to be found is high. Further, to assert the quality of the segmentation, we selected three volunteers to analyze the quality of seg-mentation performed by VIPS. For each distinct structure found in the web sites segmented, we selected one page to be analyzed by a volunteer. Whenever the volunteer did not agree with the segmentation found in a page p , he was able to adapt the segmentation to its perception about how the page p could be segmented. In these cases, we then changed the segmentation of all pages with the same structure of p . Given the necessary manual intervention in the process, we named this approach as manual segmentation .

We performed experiments to compare the quality of the segments found by our automatic segmentation algorithm with those found by the algorithm proposed by Kohlsch  X  ut-ter and Nejdl [21], referred to as Rule-based Blockfusion in http ://technorati.com/blogs/top100 the exp eriments. The algorithm identi es the segments of Web pages from a textual perspective. It uses the number of tokens in a text fragment (or more precisely, its token density) to segment Web pages (instead of DOM-structural information, as in our method). The segmentation algo-rithm is based on a merge strategy called Block-Fusion , where adjacent text fragments of similar text density (in-terpreted as \similar class") are iteratively fused until the blocks' densities (and therefore the text classes) are distinc-tive enough. Using various settings, including a rule-based approach, authors show that the resulting block structure closely resembles a manual segmentation, achieving a seg-mentation performance better than those achieved by [10], which is another recent work that approached the Web page segmentation problem.

We also studied the impact of using our automatic seg-mentation method as input to a segment aware ranking sys-tem proposed by de Moura et al . [13]. This method changes the weight of terms in BM25 ranking model [26] according to the segments where they occur with the goal of improving the search quality. We compared the ranking results ob-tained when segmenting the Web pages with our method to the results obtained when manually segmenting the pages. We also report search results obtained when using the BM25 ranking applied to full pages (using no segment infor-mation), and the BM25 ranking applied to pages after tem-plates removal. Templates were manually removed to assure a high quality template removal, thus avoiding doubts which could arise when using an automatic method. This last base-line was included to show that using segment information is better than removing templates in search tasks.

To compare the ranking strategies, we adopted two evalua-tion metrics. The rst metric is the traditional mean average precision (MAP) and the second is precision at 10 (P@10), which measures the amount of relevant documents in the top 10 answers provided on each system [2]. Table 1 presents the total number of segments and segment classes found by the automatic segmentation approach. The Table also shows the number of segments found in classes with less than elements. We can see that, despite the number of pages in CNN collection be smaller than the other collections, the number of segment classes found in this col-lection is relatively high, if compared with those found in other collections. This happens because segments with the same role in the CNN collection may have two or more ver-sions of labels (root-to-segment path), i.e., it is possible to exist two or more segment classes formed by segments with the same function in such collection. For instance, news pages found in the CNN collection usually contain tables, graphics, photos and other illustrative components in dif-ferent parts of their text. Such characteristic of the CNN collection does not signi cantly interfere in the results ob-tained by a segment aware Web search results, since each segment class continues to be formed by segments with the same function.

On the other hand, notice that the number of segment classes found in BLOGS collection is quite small, when com-pared with those found in other collections. This happens because such collection is formed only by posts of their re-spective blogs, and the posts of a same blog tend to have the same structure, i.e., they tend to have the same set of segments. Thus, since there is not a big variety of segments from post to post, the number of segment classes found in each blog of this collection tend to be quite small.
An important point to observe in Table 1 is that in all collections just a few segments were grouped into classes with less than (i.e., 8) elements. For instance, from the total of 1 ; 404 ; 512 segments found in IG, only 1 ; 109 were grouped into classes with less than elements, which is less than 0 : 08% of the total number of segments found. In CNN the number is less than 0 : 03%, in CNET less than 0 : 3%, and in BLOGS it is less than 0 : 05% of the segments found. Figure 8: Adjusted RAND Index graphs found for IG, CNN, BLOGs and CNET collections. Due to space constraints, the values in x-axis were divided by 1000.

The results for Adjusted RAND Index is depicted on Fig-ure 8. To compose each curve of the Figure (for instance, the curve SOM Tree in IG collection graph), we computed the Adjusted RAND index achieved by the method of the curve for each page of the collection, and then plotted these values in increasing order. For instance, considering the graphs for IG collection, the k th page of the curve SOM Tree refers to a page segmented by our automatic segmentation method that achieved the k th smaller Adjusted RAND index.
As one can see, these graphs show that our method achieved a segmentation performance better than those achieved by Rule-based Blockfusion [21] in all collections. The graphics show that our method outperformed the baseline at all in-termediate points, being closer to 1 in all levels. Observing the graphs, we notice that the CNET collection presented the smaller agreement between the manual and the auto-matic approaches. When checking the results, we realized that this is due to irregularities in the page structures found in CNET which a ects the automatic identi cation of re-current structures (see Section 4.1). Such irregularities do not a ect the manual method since a human can easily deal with them. However, even for this collection, our automatic approach presented a segmentation performance better than Rule-based Blockfusion .

To better illustrate the results, in IG collection we have just a few pages with AdjRAND below 0 : 6, while almost found in classes with less than elements (S. Small Classes). 10,000 are below this threshold when using Rule-based Block-fusion . In CNN our method produces less than 4,000 pages with AdjRAND below 0.8, while the baseline produces more than 8,000 pages below this threshold. When taking the av-erage AdjRAND values obtained by each method we can see that our method obtained an average value of 0.83 when processing pages of IG, while the Rule-based Blockfusion achieved an average value of 0.72. In CNN the average AdjRAND value obtained by our method was 0.85, while Rule-based Blockfusion achieved 0.58.

We also performed some experiments to check whether these results are uniform for Web sites with a small number of pages. For this, we randomly divided the IG collection into subsets containing 1%, 5%, and 10% of the original col-lection, and then used our method to segment the pages of each subset. Using the AdjRAND metric, we compared the segmentation of each page of the subsets to the segmen-tation obtained for these pages when processing the whole collection. When checking the results, we realized that the segmentation output obtained for each subset was similar to that obtained for the whole collection (for instance, the average AdjRAND value achieved by the subset containing 1% of the pages of IG was 0.98), which lead us to conclude that our method is able to obtain good segmentation quality even for small Web sites.

Table 2 presents the results obtained when using the seg-ment aware model proposed by de Moura et al . [13] with our automatic segmentation method, combination referred to as AutS , and with the manual segmentation approach, combi-nation referred to as ManS . Notice that we do not perform ranking experiments with Rule-based Blockfusion because the search model adopted requires information about seg-ment classes that is not provided by the Rule-based Block-fusion method. The Table also shows the ranking quality obtained when using BM25 ranking strategy [26] with no block information, and when using BM25 ranking applied to pages after templates removal. This last baseline method is referred to as BM25NT, which means BM25 without tem-plates.

The rst important observation is that the segment aware ranking method yields improved results relative to the BM25 and BM25NT baselines, for both segmentation methods. Further, the results obtained when using our segmentation algorithm are quite close to the ones obtained when using the manual segmentation. When applying the Wilcoxon sta-tistical test with a 95% con dence level, we notice that the di erences between our method and ManS are not signif-icant in most cases, except for all metrics in CNET and MAP in IG. The di erences in the scores obtained by the two methods are not high, being smaller than 1.3% in most cases. Further, the fully automatic method has the advan-tage of not requiring manual intervention at any point of the segmentation process.
 Tabl e 2: Ranking quality obtained in four collections when using the BM25 formula, the BM25 without no templates (BM25NT), and the segment aware model applied to manual segmentation (ManS) and to our automatic segmentation (AutS). In this paper we proposed and evaluated a site oriented segmentation algorithm for Web pages. The method com-bines the DOM trees of all pages in a site into a single data structure which is called SOM tree. By inserting the indi-vidual pages into the SOM Tree, the algorithm performs a clustering-like procedure. By investigating regularities among the pages associated with the leaves of the SOM tree, the algorithm naturally breaks them into segments.
The experiments performed with our segmentation ap-proach indicate it produces quite competitive segmentation results, which opens an opportunity to the use of the method in several Web site segmentation applications.

We show that the method is particularly useful to pro-duce input to a previously proposed segment aware ranking method, since it not only segments the Web pages, but also is able to cluster them into classes. Further, the results ob-tained by our method in segment aware Web search are close to the ones obtained when using a segmentation approach based on manual intervention.

As future work we plan to study the impact of using our method in large scale Web search systems, where there are usually just a few sample pages of each Web site in the collection. Also, we intend to investigate the application of our segmentation method to mobile applications, including its use in alternative ways of presenting Web search results in mobile devices.
