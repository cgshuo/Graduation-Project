 In this article, we report our efforts in mining the information encoded as clickthrough data in the server log s to evaluate and monitor the relevance ranking quality of a commercial web search engine. We describe a metric called pSkip that aims to quantify the ranking quality by estimating the probability of users encou n-tering non relevant results that cost them the efforts to read and skip . A search engine with a lower pSk ip is regarded as having a better ranking quality. A key design goal of pSkip is to integrate the findings from two sets of user studies that utilize eye -tracking devices to track users X  browsing patterns on the search result pa g-es, and that use specially instrumented browsers to actively solicit users X  explicit judgments on their search activities . We present the derivation of the maximum likelihood estimation of pSkip and demonstrate its efficacy in describing the user study data. The mathematical properties of pSkip are further analyzed and co m-pare d with several objective me trics as well as the cumu lated gain method that use s subjective judgments. Experimental data show that pSkip can measure aspects of the search quality that these existing metric s are not designed or fail to address , such as ident i-fying the real search intents expressed in the ambiguous queries . Although effective and superior in many ways , we also report a series of experiments that show pSkip may be influenced by sys-tem issues t hat are not directly related to relevance ranking , su g-gesting that measurements complementary to pSkip are still needed in order to form a holistic and accurate characterization of the ranking quality .
 H.3. 4 [ Information Systems ]: Systems and Software  X  perfo r-mance evaluation (efficiency and effectiveness).
 Measurement, Experimentation . Web search, relevance metrics , NDCG, implicit feedback, pSkip . In this decade we have witnessed the evolution of Web search from an altruistic service into a highly lucrative business that changes not only how people find but also put information on the Web . Continuously improving the search quality is therefore a paramount goal for the service providers, and a key aspect of the search quality is the relevance of the search results. This paper describes our efforts for the past few years in evaluating and mo n-itoring the relevance ranking quality of a commercial Web search ser vice. Our approach consist s of two methods : one that utilize s explicit judg ments from trained human assessors and the other, the clickthrough data as the implicit feedback from the users . For the first method we follow the Cranfield Methodology used in the NIST Text Retrieval Conference ( TREC ) to evaluate inform a-tion retrieval (IR) systems on the W eb documents . A s reviewed recently in [30] , the methodology has three essential components: a collection of documents that forms the retrieval base , a set of statements, called topics, that represent the information needs of the users , and finally the relevance judgments carried out by h u-man assessors that specify which documents in the collection should be retrieved for which topics . Despite its wide adoption , aspects of the methodology, especially the implied assumptions made to facilitate the evalua tions, have been the subject of con-troversies [27] , many of which are made more pronounced as IR is being applied to the Web whose large scale has amplified issues that could otherwise be comfortably ignored. For instance , the evaluation framework for the web must be able to cope with a very large document collection whose size is unknown and con-tents constantly changing. It has been pointed out that the conve n-tional relevance judgments and the accompanied pooling tec h-niques are not scalable with the size of the document collection [30] , and effective methods to address the consequent incomple te and imperfect judgments remain an unsolved research problem [2] [6] [11] [21] [26] [32] . Second ly, thanks to the global reach and the diverse user base of the Web, a common practice of associating each qu ery with a single relevance set of documents deserves a closer examination . Users with different background may have drastically different assessments on the same document with respect to its relevance to a query [31] . This is exacerbated by the reality that, for W eb search, the queries do not often make explicit the true search i n-tents of the users and the topics of the information needs that are the bedrock of the Cranfield Methodology . For example, doc u-ments describing a car model may be relevant to the query  X  X  a-trix X  for interested car buyers, but it is hard to argue their rele v-ance to the users for which the car model is not even available in their markets. It stands to reason that one document meeting the information needs of a group of users might not be so to another. As data presented later in thi s article show , disagreements in r e-levance judgments are often a reflection of diverse intents being realized with the same query . If the information needs behind the queries remain elusive, how to properly execute relevance jud g-ments becomes an issue. Fin ally, since the goal of IR is to address the user X  X  information need s, it ap pears straightforward to eva luate the effectiveness of a retrieval method and avoid misinterpret ing the user X  X  intent is to directly ask the users if the search results are to their likings . The idea of relevance feedback or interactive relevance has been pop u-lar since the dawn of IR [4] [15] and has been applied to Web search . One of the notable efforts is the  X  X urious Browser  X  exp e-riments [9] [13] that utilize specially instrumented browsers to actively solicit user X  X  explicit feedback on their Web search a c-tiv ities. While the experimental data offer highly valuable obse r-vations, in our deployment attempts we have found that , in order to have meaningful interpretation, a large amount of data have to be collected due to the high variability in the user behaviors [31] . Finding enough users to participate in the Curious Browser exp e-riment is not only expensive but also involves many social and privacy concerns that cannot be easily overcome. In addition , the data collected from the instrumented experiments appear statist i-cally different from the control led environments , suggesting the users might have altered their behaviors when participating in the experiments . How to collect sizeable and unbiased data r emains a challenging question.
 In contrast to Cranfield or Curious Browser, web server log data are inexpensive to collect and easy to scale. The idea of using the server log data for Web search has been of great interests in the community. A straightforwa rd view is to regard the clickthrough data as users voting with their mouse clicks: the more clicks a l ink receives, the more likely the linked document is rele vant to the query [1] [3] [7] . However, since the server data do not capture user X  X  explicit feedback, such a straightforward view has been found to be not suitable for all applications [19] . In fact, it has been convincingly shown that the clicks are clearly biased [18] , and proper way s to adjust for the click biases are critical in cor-rectly use and interpret the search log data . In this paper, we describe pSkip, a measure ment designed to ev a-lua te the relevance ranking of a commercial Web search engine based on the log data . Short for  X  probability of skipping,  X  pSkip is motivated by an observation that the click biases seem to be r e-lated to and hence can be adjusted by considering the patterns of users read ing the search result pages. Since in the ideal case the more relevant documents should appear at the more prominent posi tions, user s will perceive the search engine as having made a ranking error when they see a more relevant result being pla ced after a non relevant one that they have to skip . We can thus est i-mate how often users have to read and skip the search results in order to f ind something relevant to them and use it as an indicator of relevance ranking quality . In other words, a lower pSkip co r-responds to a better perceived ranking quality . The design of pSkip embraces two essential notions that non -clicked result s are not necessarily all  X  X ad X  , and the clicked r e-sults are not necessarily equally  X  X ood X . The first point is largely based on the eye -tracking experiments that demonstrate users do not consume the search results in a uniform manner [18] . Specif i-cally, results that are placed in a less prominent position (e.g., the bottom of the search result page ) are viewed less frequently . A non -clicked result can therefore be attributed to it not being seen rather than not relevant , and consequently should not always be viewed negative ly. The second point is motivated by the user studies on the implicit feedback and our analysis of the data o b-tained through the  X  X urious Browser X  experiment s [13] . In this paper, we zoom in on one specific aspect that explores the correl a-tion s between the relevance of the document and the  X  X well time, X  i.e., the time d uration a user will spend in reading the document with respect to the query topic . The experimental data indicate that a short dwell time is a very effective predictor of a non rel e-vant document. This finding can be intuitively understood as users will return to the search result page more quickly when they click on a result and find the landing document not rele vant . The rest of the paper is organized as follows. In Sec. 2, we review the user studies that underlie the design of pSkip and th e relat ed work inspired by these experimental results. To our best kno w-ledge, this work is the first effort that attempts to combine into a single model the findings of the two user experiments that are conducted with very different apparatus and design goals. In Sec. 3, we describe the derivation of the maximum likelihood estim a-tion of pSkip in d etails . We show mathematically how pSkip is related to widely known metrics such as the mean reciprocal rank (MRR), the clickthrough rate (CTR), and the expected search length (ESL). We also elaborate how pSkip can be extend ed to general search sessions that receive multiple clicks, and explain why pSkip is more focused on measuring the ranking quality and less affected by the length of the search session , a confounding factor that we take into account to avoid unfair penalties on the informational type of queries. In Sec. 4, a Bayesian click quality predictor , use d in pSkip to probabilistically discount the clicks with short dwell time , is presented and its effectiveness v alidated by showing its ability in predicting the human labels in the C u-rious Browser data. Sec. 5 compare s pSkip to the alternative , TREC -style evaluation approach that involves trained human assessors in making explicit relevance judgments. We show the two approaches do not always agree, and present data on the source s of disagreements. The anal yse s show that , while pSkip can leverage the information from the server log effectively, the re are factors other than the relevance ranking that can also affect pSkip, and therefore complementary measures are still needed to ensure correct interpretation of the clickthrough data. Using implicit feedback to understand user behaviors has been an actively research ed topic dated back many decades. One pe rtinent research area is the relevance feedback in information retrieval , for which Kelly et al . [20] provide a comprehensive review of the literature. By cross referencing the passively observed behaviors with the follow -up survey results, researchers have identified many implicit feedbacks that are highly correlated to the explicit feedback, among them, the revisit frequency, the dwell time , and whether the user h as taken any actions in retaining the document (e.g. printing the document) [22] . These aspects have been further explored in the  X  X urious Brow s-er X  experiments by Claypool et al. [9] in which the explicit fee d-back solicitation s are automated using a specially instrumented web browser that, upon detecting th e viewing of a document r e-trieved by the search engine, the user s are prompted to label the relevance of the document. The experimental results confirm the significance of the dwell time and the scrolling activities. A large scale of Curious Browser deploy ment is repeated and results r e-ported by Fox et al [13] in which Bayesian inference techniques are employed to study not only the page level but also the search session level implica tions. The page level findings are largely in agreement with Claypool X  X , while the session level observations indicate that the users seem  X  X enient X , namely, a search session is likely to be perceived as satisfactory as long as at least one rel e-vant document is found in the ses sion. It is noted, however, that there exists con siderable amount of variability across us ers, and the Curious Browser studies do not capture user X  X  browsing pat-tern of a search result page nor do they address how t he click bias es might arise . In comparison, the series of eye -tracking experiments conducted by Joachims et al [18] convincingly demonstrate the existence of the click bias es. A key finding of the experiments is that the pro b-ability of a search result being viewed decreases dramatically with respect to its position on the search result page (Figure 1 ). Fu r-thermore, this decreas e in viewing probability is not exclusive to pages that rank the outcomes in accordance with their relevance. In one experimental setup where the search result pages are man i-pulated t o display the results in the reverse d order, the viewing bias remains patently observable , although the decrease with r e-spect to the position is in a slower pace. The clickthrough patterns in the reverse d condition , in the mean time , exhibit a higher d e-gre e of randomness.
 These experimental observations have inspired many behavioral models attempting to adjust for the click bias es. In their efforts to use clickthrough data to improve relevance ranking, Joachims [17] and Radlinski et al [24] [25] propose a model directly derived from the eye -tracking experiments in which the search results are assumed to be viewed in a sequential top -down order . It is demo n-strated that the clickthrough rate (CTR) is a useful feature in trai n-ing a search engine for the relevance ranking purpose . Craswell et al [8] further add on to the model the aspect of exploration, a s-suming that the users will continu e to read search results until they find something relevant. The model is improved and e x-tended by Guo et al [14] for search sessions receiving multiple cli cks . A key assump tion in this work is that the clicks in a session are statistically dependent in the sense that a later click might be an indication of an earlier clicked result not satisfying users enough, leading the users to continue exploring . Though intuiti ve, the hypothesis does not seem consistent with the Curious Browser data [13] , especially for the sessions involving informational qu e-ries where sequences of conse cutive clicks are all labeled as sati s-fied . As the intricacy of click dependencies requires deeper inve s-tigations , we in this work do not introduce elements explicitly modeling the exploration aspects of a search session . Dupert et al [12] propose a generative model that separates the relevance of the abstract displayed in the search result page from the contents of the docu ment, and Sarma et al [28] utilize the skipped results as negative feedback to reduce the query abandonment rate. All these efforts follow the eye -tracking observations and hence inh e-rit a limitation that , as pointed out by Joachims et al in [18] , tem-poral information is not utilized to weed out  X  X ad X  results tha t attract clicks (e.g. with misleadi ng abstracts) but have relatively short dwell time. The foundation of pSkip design in mitigating the click bias es is derived from the eye -tracking data [18] , which are replicated in Figure 1 and clearly show search results at lower rank positions are read and clicked more often . We note that, except for the first position, the conditional skip rate , i.e., the percentage of times a result being skipped given it is viewed by the user is less suscept i-ble to the positional bias. We surmise that users X  general perce p-tion of the ranking quality is correlated to the overall average of this skip rate (called pSkip in this paper) , and that a lower rate corresponds to a more satisfied experience. In the first occurrence model, we quantify the user experience of traversing sequentially along a list to find the first result that is not outright irrelevant . Intuitively, a shorter lengt h corresponds to a better search ranking. For a search engine having a pSkip =  X , we can describe the pro b-ability mass function of the random variable L modeling the pos i-tion of the first relevant result as: Namely, the user has to skip L -1 results before reaching the first promising outcome. The probability distribution in (1) is know n as the geometric distribution with which pSkip can be obtained by performing the maximum likelihood (ML) estimation from the observed clickthrough data. Suppose we observe a collection of que ry sessions {  X   X  } of which the first desired results are preceded by  X   X  skips, respectively . Denoting the prior probability of each query session as  X   X   X  , we have the expected log like lihood of the observations a s The ML estimation for the parameter  X  can be obtained by solving  X  X  X  [log  X  ]  X  X  X  = 0, which leads to The theoretical optimum, pSkip=0, occurs when the search engine always places the desired result at the first position, i.e., r all queries . As also implied in the above equation , pSkip is stron g-ly sensitive to the position of the intended result : misplacing the ideal results from position 1 to position 2 will incur a pSkip growth from 0 to 50%, and this ranking sensitivity tapers off for higher positions. This uneven growth of pSkip implies a built -in mechanism to  X  X iscount X  the higher ranked resu lts. The idea of discounting higher positions in pSkip is consistent with the insights underlying the design of the discounted cum u-lated gain (DCG) method proposed in [16] and commonly used in evaluating the TREC data . However, determining a discount fun c-tion in DCG remains a heuristic choice . Furthermore, because the dynamic range of DCG can vary dramatically, normalized DCG (NDCG) is found to be a more suitable measure to compare search engines. In order to compute NDCG, however, an  X  X deal X  ranking of the documents for each query has to be determined . The process of defining the ideal sets for normalization is expe n-sive for large scale application s because of the amounts of human efforts involved. Both issues are avoided in pSkip . 
Figure 1 : Data from eye -tracking experiments (bars) show search results are viewed and clicked unevenly with respect to their rank positions [17] and how pSkip attempts to lessen the positional bias (dotted line). Since the geometric distribution in (1) is a member of the exp o-nential family of distributions , the first occurrence model bears resemblance to the ranked half life (RHL) model proposed in [4] where the natu rally occurring radioactive decaying function is applied to approximate the user X  X  perception of an interactive IR quality . Despite the similarity , the mathematical model of (1) is not limited to interactive IR, and can be further extended to more general cases (Sec. 3.2) . Other metrics related to the first occurrence model are the average search length (ASL) described in [21] that computes the position of the first relevant document in the retrieval list, or equivalently, its reciprocal commonly known as the mean reciprocal rank (MRR). Using (1), we can derive that the average search length as  X  X  X  X  =  X   X  =  X  (1  X  X  X  )  X   X  X  X  1 Similarly, In other words , pSkip of the first occur rence model can be used to obtain ASL an d MRR . Unlike pSkip, however, both ASL and MRR remain undefined for queries that receive multiple clicks . Although the random variable L in (1) can theoretically take on a value all the way up to the infinity , we often have to consider the relevant result as not found if it is beyond the first K positions . Consid ering cut -off is realistic as few users persist beyond a couple of search result pages. The geometric distribution underl y-ing the first occurrence model offers a straightforward way to model the truncation because one can relocate all the proba bility mass beyond the cut -off to the boundary point as Without loss of generality, we can re-arrange the observation data so that the first M items are in the cut -off . Following the same ML estimation steps, it can be shown that with cut -off, The pSkip@K metric can thus provide the same attractive prope r-ty as NDCG that describe s the ranking performance of a retrieval method at any position [16] . Again, pSkip arrives at this desirable property without having to first obtain the ideal retrieval set for each query , a step required for computing NDCG . The first occurrence model considers the case where user X  X  info r-mation need s can be met with only one search result , e.g. the na-vigational queries . In general, however, users may continue to explore the search re sults regardless of the relevance of the fir st document they read . This is common ly observed in the Curious Browser data when , for example, users are making a purchase decision where a com pare -and -contrast from multiple and equally relevant sources is a good strategy. These types of search activ i-tie s are characterized by multiple clicks in a search session . Without making hypotheses on whether a continuing exploration indicates the previously examined results are relevant or not , we extend the model by treating the process of reach ing each promi s-ing document as a n individual first occurrence model. More sp e-cifically , if the user examines  X   X  search results before finding the j promising document, the total number of results L that the user has to inspect before finding N promising documents is Since L is a counting process of N geometric distributed random variables , it can be shown that the condition al probability distrib u-tion of L is given by Namely, it has the form of the negative binomial, or more preci se-ly, the Pascal distribution. One can show that t he ML estimation for the parameter  X , the pSkip, is given by where  X   X  denotes the number of results the user considered as promising for the query  X   X  , and  X   X  the number of documents skipped in between . The above equation shows that pSkip is the ratio of the number of the skipped and the number of the viewed results , weig hted by the query frequencies. By comparin g this estimation formula with the first occurrence model , it can be clearly seen that the first occurrence model is a special case of the genera l model in which all queries have  X   X  = 1 . Similarly, it is very strai ghtforward to model an abandon ed query in the general model by including an additional cut -off , namely where  X   X   X  has the cut -off exponential distribution described earlier (Sec. 3.2 ). In either case, the generalized random variable L is fully specified by pSkip and the distribution of N , the random variable modeling how far down the list the search result s are explored by the users . The general pSkip is related to the expected search length (ESL) proposed as a single metric to measure the effectiveness of a r e-trieval system [10] . The central idea underlying ESL is to ask that, on the a verage, how far down the search result list the users have to traverse in order to obtain enough documents that meet their information need s. Based on our mod el, we can derive ESL as In other words, ESL is intimately related to pSkip : the higher the pSkip is, the harder user s find promising results and therefore the larger the ESL will be . However , we note that ESL by itself does not only assess the rel e-vant ranking quality of a search engine . This is because ESL is affected by both pSkip and N , the number of results clicked by the users . While navigational queries can often be addressed by one single document, informational queries abound for which the number of documents to satisfy the information needs is indef i-nite . As a result, a large ESL can be an indication of poor ranking quality for the navigational queries, but it can also be attributed to a large amoun t of informational queries received by the engine , and therefore is not necessarily an indication of a poor search outcome . Ideally, one would like to normalize ESL with respect to the ideal number of documents for each query. For Web search, finding the p roper method for the normalization is a nontrivial task. The design of pSkip avoids the issue by counting the relative percentage rather than the absolute number of the  X  X asted X  effort s for the user to read and skip results , a principle applicable to both type s of queries. CTR is a commonly used metric in Web search an d ads. With its popularity come many nuanced definitions, all serving targeted purposes. Here, we compare pSkip to query CTR, which is de-fined as the percentage of queries that receive clicks on their search re sults. We have found this definition very useful in stud y-ing the effectiveness of query processing, such as alterations, expansions and suggestions. Based on this definition, CTR is the compleme nt to the percentage of queries that are abandoned or refor mulated before any clicks are received , namely, the perce n-tage of queries with all search results skipped : of the random variable L , the number of results a user reads before abandoning or reformulating the query. Like ESL, CTR is ther e-fore a combination of two factors: pSkip representing the ranking quality, and the trave rsed length of the search results L related to user persistence or the nature of the query . A key consideration in the design of pSkip is to recognize that not all clicked results satisfy users. It is not uncommon for users to find the abstract of a search result appealing and click on it, only to find the retrieved document is not relevant and quickly return to the search result page. Such cases often result in a short dwell time on the document, and this intuition is suppo rted by studies reviewed in Sec. 2 that relate the dwell time to the document r e-levance. In this work, we further estimate the likelihood that a clicked result should be treated as skipped in the pSkip comput a-tion . Let  X   X   X  denote the probability of a search result that is spec i-fied by the feature set F and should be treated as a skipped result . With Bayes X  rule, we have Here, we use ~ S to represent the comple mentary event space of S that  X   X  +  X  ~  X  = 1 and  X   X  to denote the probability of a search result having the feature set F . Since  X  (  X  ) can be counted from the search log, we can estimate  X   X   X  as long as we can decompose  X   X  into a two component mixture distribu -tion  X  (  X  |  X  ) and  X  (  X  |~  X  ) with  X   X  and  X  ~  X  as the mixture coefficients.  X   X   X  can be regarded as a  X  X oft X  skip count in the sense that we can interpret  X   X   X  = 0.25 as if the user were to repeat the search several times, a quarter of the t imes the result would have been skipped rather than clicked. Accordingly, for the j result having the feature  X   X  , we can compute the pSkip, following the notation in Sec. 3.3, as pSkip = The  X  X urious Browser X  data published in [13] suggests many features derived from user X  X  implicit feedback are highly corr e-lated to the u ser X  X  explicit judgments on whether they are satisfied (SAT) or dissatisfied (DSAT) with the documents . While all these features can be incorporated into the click quality predictor fo r-mulated above, a close examination of the data shows that using the dwe ll time in a logarithmic scale alone can already yield a mixture distribution that has a sufficient predicting power that agree s with the user labels , and that  X   X  , which in this case is simply the overall dwell time distribution, can be effectively mo d-eled as a Gaussian mixture . These surprising observation s are demonstrated in Figure 2 that depicts the experimental outcomes and the results from the Bayesian click quality predictor using three iterations of the EM algorithm to estimat e the mixture com-po nent s and weights (Sec. 4.1) . The property of Bayesian click quality predictor is further illu-strated in Figure 3 that shows the main utility of the click quality predictor is to smoothly classify the clicked results as skipped or not. We have observed th at the click quality predictor often has the shape of a sigmoid function as shown in Figure 3.

Figure 3: P ( S | F ) with respect to dwell time (solid line) given Figure 2 : Normalized histograms showing dwell time and e x-plicit relevance judgments. The dotted and solid lines are from Curious Browser and click quality predictor, respectively How well can pSkip describe the relevance ranking quality of a web search engine? We study this issue by computing pSkip using (3) on controlled configurations of the search engines for which we systematically change the properties of the ranking algorithm s and have ongoing explicit judgments following the TREC style Cranfield methodology [30] . On a regular basis, a query set con-sisting of 10,000 distinctive queries is created by randomly sa m-pling the server log over a one month pe riod. With the query set, a document collection is formed using the pooling techniques, i.e., by harvesting the URLs of the search results returned by a few publicly available search engines with respect to the query set. Trained human assessors are employed with monetary compens a-tion to assign each query -document pair a relevance score . Fina l-ly, the logarithmic discounting function described in [16] is used to compute DCG and NDCG. Among the tools we use to study the quality of the web search serv ice is the trend plot, for which pSkip and other quality me a-surements are computed at a regular interval from the search log . Figure 4 shows a pSkip trend plot for a period of three months in year 2007 computed for a search engine in the EN -US market (English version in the United States) . During this period, three major revisions of the ranking algorithms were deployed around the dates marked as T1, T2 and T3, respectively. The offline eva l-uations all showed the three algorithms had significant NDCG improvements over their predecessors.
 In the first two T1 and T2 deployments, the pSkip metric tracked the improvements of NDCG proportionally, with the effects of the revision on T1 particularly visible. However , the trend plot told a conflicting story for the T3 revision. There, pSkip indicate s the search engine has a slightly degraded ranking quality, whereas the NDCG shows the opposite.
 This discrepancy is further studied by cross examining the two measurements on a query by query basis using the da ta from a month after T3 . Roughly 6,000 queries in the query set do not have enough occurrences in the log to yield reliable pSkip estim a-tion and are excluded in the cross examination . Intuitively, pSkip and NDCG should have an inverse correlation, namely, pSkip should approach 0 as NDCG approaches 1, and vice versa. Ho w-ever, as illustrated in Figure 5 (a) and (b) where the scatter plots of NDCG and DCG versus pSkip are shown, respective ly, the correlations between pSkip and N/DCG are poor in this case . Wit hout the nonlinear normalization in NDCG, DCG appears to be somewhat more consistent with pSkip in that the queries with low DCG tend to have high pSkip . However, the variance is still too large and the correlation is weak . Several factors contributing to the poor correlation emerge when the analysis is narrowed down to the cases in which the manually assessed scores and pSkip are mostly disagreea ble. A discrepancy set, using the thresholds pSkip &gt; 0.66 and pSkip &lt; 0.33 for queries with high and low NDCG , respectively , is extracted and the ana l-ysis on this discrepancy set is described as follows . Even in the EN -US market, there is still substantial amount of traffic for which English contents do not best serve the users de-spite they have set their profile to say so. As an example, the log shows that, for the query  X  X outube X  and its equivalent, roughly a quarter of the users in the US actually pre fer the Spanish version of the site , namely, these users are better served by the ES -US rather tha n the EN -US contents . Accordingly, they typically skipped over the top ranked results that contain English contents, Figure 5: Correlations between pSkip and (a) NDCG and (b) even though these search results are highly relevant. On the other hand, the relevance scores reflect the assessors  X  judgments that foreign contents are not relevant to English users . Similarly, the market mismatch can originate not only from the users but also from the content providers . From the log, it can be observed that many international web sites targeting English speaking audience (e.g., shopping or dating services in Australia or United Kingdom) do not properly designate their locales . When these sites are mi r-rored or hosted in the US, they can inadvertently receive high relevance scores from our assessors but are often skipped by the users because the contents and the services offered are not acces s-ible to US residents . These two types of market mismatch encompass navigational, informational, and transactional query types (as defined in [5] ) and account for a wide range of high frequency queries. In the discrepancy set, 83.7% of the instances (weighted by query fr e-quencies) can be attributed to this kind of mismatch . Excluding the market mismatch, the rest 16.3% of the discrepancy set can be categorized into the factors shown in Figure 6 . Intent mismatch , labeled as  X  X ntent X  in Figure 6 , refers to the situ-ation s where the documents frequently sought for by the users are deemed not relevant by the asses sor . In labeling the intent mi s-match, a different question is asked. Here, t he human assessors are given a query -URL pair and asked whether the document r e-ferred by the URL can be a  X  reasonable  X  destination for the query. The data show that intent mismatch often arises when queries have multiple mean ings or when the queries are ill specified. For example, the c lickthrough data sug gest that , for the query  X  X oogle image X , many users intend to visit Google Earth rather than use Google X  X  image search. F or another high frequency query  X  X  a-hoo X  users may actually want to navigate to Yahoo X  X  email , me s-senger, news and o ther services, all with similar probabili ties as Yahoo X  X  main portal page , which is ranked first by the search engine but sometimes skipped. It appears many orthographically identical term s have different meanings to different users and not a sin gle ideal retrieval outcome can be properly defined for the normalization step in the NDCG computation.
 The opposite problem , where a single destination is as the best result for many seemingly unrelated queries , can also be observed from the data . For example, many stores contract their rebate or other product promotion processing to third party agents whose main web page may not even include the names or the logos of the stores they serve. As a result, the trained assessors frequently labeled these pages as not rel evant, contrary to the ideal outcome for the users . These two types of intent mismatch contribute to 85.6% of the discrepancies in the data. Typically, intent mismatch issues can be easily corrected when the assessors are asked to review the query -document pairs with hints from the online usage data . Malicious contents, labeled as  X  X alicious X  in Figure 6, refer to the discrepancy in determining whether the retrieved contents are harmful to the users. Typical examples include identifying spam and pornographic contents . The data show that false negatives and false positives are equally prominent in the relevance judgment process . From the data, we observe that many popular discussion forums or graphic intense sites are sometimes misrecognize d as spam and assigned low relevant scores. On the other hand , many malicious sites serve innocuous contents to the web crawler and consequently obtain high relevant scores from the explicit jud g-ments , only to serve different contents to regular users that are not relevant and result in a high pSkip due to short dwell time. In total, this category accounts for 7.23% of the discrepancy. Duplicated contents a re prevalent on the web and naturally have similar relevance with respect to a query. To optimize recall in the traditional IR sense , these similar contents should be retrieved . The idea is carried over to the design of N/DCG in that the r e-trieval system can maximize the metrics by cascading as many documents that are equal or close in relevan ce scores . This ran k-ing strategy effectively encourages duplication and is hardly ideal for web search , especially when the query has mul tiple meanings and the similar results being placed on top do not correspond to the particular sense sought after by the user and have to be skipped entirely . As an example, the top 5 results for the query  X  X atrix X  after T3 revision are all about the HD DVD release and reviews of the namesake movie that have to be skipped when users intend to search for matrix as a linear algebra concept, a hair style, or even a car model. This is an area where pSkip and N/DCG are measuring different aspects of the search outcome s and having a clear dis agreement in the metric design philosophy . Labeled as  X  X uplication X  in Figure 6 , the data show 5.63% of discrepancies are caused by this issue. Web is incredibly dynamic that queries often acquire new mea n-ings or document relevance can change dramatically when new contents become available. New domain names are creat ed every day , and existing domains can cease to exist. When the search index does not catch up with the changes and still serves the ou t-dated results at prominent positions, users skip these results more often, causing pSkip to rise. As shown in Figure 6, approximately 1% of the discrepancies are caused by the fact that pSkip is r e-flecting changes more rapidly than the search engine can update the index and, hence, the relevance judgments.
 In analyzing the data set, we have encountered other factors that ma y also contribute to the pSkip and N/DCG discrepancy. Al-though the portion of these factors is not large enough to pursue in greater details at the time of study , these factors might become more visible after aforementioned issues are resolved . One such factor is the abstract quality of the search results , a subject that is explicitly highlighted in the modeling work of [12] . We have seen some discrepancy cases where the abstracts fail to capture the essence of the document, and it appears that users are leery of clicking on URLs that are too long or not descript ive. In this sense, pSkip is really a measurement for the quality of the search result s including the abstracts , rather than just the relevance of the landing page as in the design of NDCG . While the comparison s with NDCG show that pSkip can provide valuable and complementary insights in assessing the ranking quality of a search engine, we have also observed that pSkip , like other metrics derived from the clickthrough data, can be sensitive to factors un related to what they are desi gned to measure which, in the case for pSkip, is the relevance ranking quality . This obse r-vation is highlighted in the  X  X erver delay X  experiments that are designed to study the effects of the search engine response time on the user X  X  search behaviors.
 In these experiments, queries are randomly diverted to one of the experimental engines , all of which share the same backend and produce ex actly the same search result pages . The only difference is artificial delay s are inserted into the experimental engines in delivering the search result pages. Surprisingly, pSkip drops mo-notonically with the delay , as shown in Figure 7 , even though the ran king algorithms are identical. To better describe the observed behavioral changes , Figure 7 also shows the CTR (Sec.3.3.2) drops as well . The data indicate that a sluggish search engine will have a lower clickthrough rate , but when users do click they tend to skip less often . Because the engines produce identical results, the pSkip drops in these experiments cannot be attributed to the improvements in the relev ance ranking, suggesting that pSkip metric can be affected by system issues and is better fused in to a broader context in order to draw correct interpretations of the relevance ranking quality from the clickthrough data . What characterizes a search engine as having a good relevance ranking quality is perhaps a n inherently subjective question . In this work, we surmise that a higher quality engine is the one that costs users less wasted effort s to find what they want . The pSkip metric is thus designed to estimate how often users have to read and skip the search results presented to them by the search engine . As an objective and analytically tractable measure, pSkip is shown to possess desirable properties and advantages as many advocated in the prior arts. When compared to the popular Cra n-field Methodology that employs subjective measure ments, pSkip is empirically shown to be able to better cope with the application settings for the Web in which the search intents of the users are often not well specified and the size of the documents is large . In addition, pSkip provides an end -to-end quality measurement, taking into account the duplication and search result abstract qua l-ity issues that are not considered in the design of NDCG.
 On the other hand, we note that pSkip is a derivative from the clickthrough data that are known to be noisy and biased. We tackle the noise issue with statistical techniques and adjust the biases based on the findings from the experimental data from multiple user studies . Nevertheless , pSkip inherits the caveats in these experiments.
 Perhaps the most critical quest ion to ask about the validity of pSkip is its underlying linkage of the perceived ranking quality to the observed behavior of skipping search results. Although the premise sounds intuitive and seems supported by the user study data, the server d elay experi ments clearly remind us of the co m-plicated nature in drawing conclusions from the observable human behavioral changes. Consequently, it is vital to supplement pSkip with complementary measurements in order to form a holistic view to accurately infer the se arch ranking quality. The authors would like to thank Susan Dumais for making avail a-ble the Curious Browser data and numerous suggestions, Eric Shurman for conducting the server delay experiments, Alex Ac e-ro, Peter Bailey, James Clark, Nick Craswell, Jianfeng Gao, Na n-cy Jacob, Ronny Kohavi, Yu -ting Kuo, Hang Li, Chin -yew Lin, Chao Liu, Wei -ying Ma, Ramez Naam, Mukund Narasimhan, Jianyun Nie, Kemp Peterson, Nils Pohlmann, Victor Poznanski, Filip Radlinski, Paul Viola, Rohit Wad, Yi -min Wa ng, Ryen White , and Dong Yu for their feedback and discussion, and Fritz Behr, Yu Chen, Jinliang Fan, Bin Jia, Li-wei He, Dmitriy Meye r-zon, Xiaolong Li, Ping Lin, Petr Slavik, Bryan Sera, and Robert Wang for their assistance in processing and analyzing the search log s. [1] E. Agichtein, E. Brill, S. Dumais, R. Ragno. Learning user [2] J. A. Aslam, V. Pavlu and E. Yilmaz. A statistical method for [3] D. Beeferman and A. Berger. Agglomerat ive clustering of a [4] P. Borlund and P. Ingwersen. Measures of relative relevance [5] A. Broder. A taxonomy of web search. ACM SIGIR Forum , [6] C. Buckley and E. M. Voorhees. Retrieval evaluation with 
Figure 7: Effects of server delays on pSkip and CTR. Results [7] N. C raswell and M. Szummer. Random walks on the click [8] N. C raswell, O. Zoeter, M. Taylor and W. Ramsey. An exp e-[9] M. Claypool, P. Le, M. Wased, and D. Brown. Implicit inte r-[10] W. S. Cooper. Expected search length: a single measure of [11] G. V. Cormack, C. R. Palmer and C. L. A. Clark. Efficient [12] G. Dupret and B. Piwowarski. A user browsing model to [13] S. Fox, K. Karnawat, M. Mydland, S. Dumais, and T. White. [14] F. Guo, C. Liu and Y. -M. Wang. Efficient multiple -click [15] P. Ingwersen and K. Jarvelin. Infomormation Retrieval in [16] K. Jarvelin and J. Kekalainen. Cumulated gain -based evalu a-[17] T. Joachims. Optimizing search engine using clickthrough [18] T. Joachims, L. Granka, B. Pan, H. Hembrooke , F. Radlinski, [19] M. T . Keane, M. O X  X rien and B. Smyth. Are people biased [20] D. Kelly and J. Teevan. Implicit feedback for inferring user [21] R. M. Losee. Text retrieval and filtering: analytical models of [22] M. Morita and Y. Shinoda. Information filtering based on [23] V. Poor. An introduction to signal detection and estimation. [24] F. Radlinski and T. Joachims. Query chains: learning to rank [25] F. Radlinski, M. Kurup and T. Joachims. How does clic k-[26] I. Saboroff, C. Nicholas and P. Cahan. Ranking retrieval [27] T. Saracevic. Evaluation of evaluation in information retrie v-[28] A. Sarma, S. Gollapudi and S. Ieong. Bypass rates: reducing [29] C. J. van Rijsbergen. A theoretical basis for the use co -[30] E. M. Voorhees. TREC: Continuing information retrieval X  X  [31] R. W. White and S. M. Drucker. Investigating behavioral [32] J. Zobel. How reliable are the results of large -scale inform a-
