 University of Chicago
Given a constraint set with k constraints in the framework of Optimality Theory (OT), what is standard complexity measure for concept classes in computational learnability theory .In this work, I use the three-valued logic of Elementary Ranking Conditions to show that the VCD of 1. Introduction
Given a set C ON of k constraints in the framework of Optimality Theory (OT; Prince an dSmolensky 1993), what is the capacity of C ON as a classification scheme for samples of language data? In OT, constraints are functions that map candidates to natural num-bers, where each candidate is a member of the (possibly infinite) set of possible deriva-tions of an input form i supplied by the candidate generating function G EN ( i ). The number that a constraint C i assigns to a candidate indicates how many times that candidate violates C i . A grammar is a ranking of the constraints that imposes a total ordering on C ON , R C ON (or simply R when C ON is clear from the context), an dthe language that is generate dby grammar R is the set of candidates that are optimal according to R as in Definition 1.

Definition 1 a. Candidate a is more harmonic than candidate b according to b. Candidate a is optimal according to ranking R iff no other candidate generated is the upper boun dof k ! languages in what Prince an dSmolensky (1993, page 27) dub the factorial typology of the constraint set. Another complexity metric that is useful in analyses of learnability (especially for non-finite concept classes) is the cardinality of the largest data set of which each subset corresponds to a different ranking hypothesis.
The idea of measuring the complexity of a concept class (in the case at hand, a set of grammars) in this way comes from the work of Vapnik an dChervonenkis (1971) an dis known as the Vapnik-Chervonenkis dimension (VCD). In OT, the VCD of a constraint set C ON (i.e., the concept class consisting of languages generate dby rankings of C ON ) is the size of the largest sample (set of candidates) that is shatterable as in Definition 2. Definition 2
Asample S is shatterable by a constraint set C ON iff, for every partitioning of S into two disjoint sets T and F (including the null/whole partition), there is at least one ranking R for samples consisting of OT candidates. For instance, each candidate in a shatterable sample S must be an input  X  output mapping for a unique input form because two candidates a and b with the same input woul deither tie with identical sets of violations or show harmonic inequality. In the case of a tie, no ranking coul drealize a partitioning that separates a and b and, in the case of harmonic inequality, no ranking could realize a partitioning in which a and b are simultaneously optimal. More generally, the VCD places an upper boun don the number of distinct grammar hypotheses that can be real-ized over any sample of linguistic data consisting of OT candidates, and thus provides a ready measure of the complexity of the hypothesis space in Optimality Theory. et al. (1989) point out, for any finite concept class C , the VCD is bounded at log because it takes at least 2 d hypotheses to associate a unique hypothesis with every sub-set of a sample of size d . Thus, because the number of grammars (hypotheses) over k constraints is finite X  X ne grammar for each of the k ! rankings X  X he VCD of OT is bounded at log 2 k !. Or, put more simply, because log 2 x ! k log 2 k as an upper boun don the VCD of OT. In this article, I will show how the structure of the hypothesis space in Optimality Theory provides a tighter bound on the VCD of OT than the boun destablishe dby the finitu de of the hypothesis space. I will improve upon the inherent boun dof k log 2 k by showing that the VCD of OT with k constraints is actually bounded at k  X  1 an dthus grows linearly with the size of learning in Optimality Theory. For instance, the VCD of a concept class places an abso-lute lower boun don the number of mistakes that any error-driven learning algorithm can be guarantee dof achieving (Littlestone 1988). This fact tells us that it may yet be possible to improve upon the quadratic mistake bound of ( k
Constraint Demotion (Tesar an dSmolensky 1993, 2000; Tesar 1995, 1997, 1998), the reigning mistake boun dfor any OT learning algorithm. The VCD of a concept class also provides a very general bound on the number of data samples that are required for learning in probabilistic models of learning that will be discussed in Section 5. 2. Elementary Ranking Conditions
The main result for the VC dimension of OT will be given in Section 4. First, some supporting results will be establishe dshowing that there is an upper boun dof k 48 shatterable sets of statements about constraint rankings that are expresse dwith Prince X  X  (2002) Elementary Ranking Conditions.
 in terms of the set of constraint rankings under which x is optimal. Prince (2002) provides a scheme for encoding this kind of ranking information called an Elementary
Ranking Condition (ERC). In this section, I will review some formal properties of ERCs that are relevant for establishing the VC dimension of OT. Prince demonstrates many formal properties of ERCs beyon dthose covere dhere an dshows that ERCS are equiv-alent to the implication-negation fragment of the three-value drelevance logic RM3 (cf.
Anderson and Belnap 1975). This section will review properties of ERCs that are most relevant for the results at hand. For formal proofs and a complete exposition of the logic of ERCs, see Prince (2002).
 use the symbols L , e ,and W to encode logical statements about rankings. Each con-straint is assigne dan arbitrary numeric in dex, an din each ERC  X  ,the i th coordinate  X  refers to the constraint with i th index C i . The meaning of an ERC is that at least one constraint whose corresponding coordinate contains a W outranks all of the constraints whose coordinates contain L  X  X . Thus, W , e , L , L means that
C , while L , L , W , W means that either C 3 or C 4 outranks both be constructed by comparing candidates as in Definition 3. Note that number of times candidate a violates the constraint with index i .
 Definition 3
Given a constraint set C ON with k constraints indexed { 1 ... k that share the same input, the function erc C ON ( a , b ) returns an ERC  X  =  X  describes the rankings under which a b . 1 The symbol W in  X  i of erc ( a , b ) =  X  is a mnemonic for the fact that (the winner ), whereas an L in coordinate i is a mnemonic for the fact that (the loser ). An e in  X  i indicates that the candidates are equivalent according to
Example 1 cand. a * ** * erc ( b , a ) = L , W , W = b a if C 2 or C cand. b ** * erc ( a , b ) = W , L , L = a b if C 1 outranks
Note the symmetry between erc ( a , b ) = W , L , L , which says that candidate a is more harmonic than b under any ranking where C 1 outranks both which says that b is more harmonic than a under any ranking where either ranking conditions. The opposition between these ERCs follows straightforwardly from the fact that only one of the two candidates can be optimal under any given ranking.
ERCs by making pairwise comparisons between the violations for one designated (or observed) winner and the violations for each other candidate.

Example 2 cand. a * ** winner cand. b ** * erc ( a , b ) = W , L , e = a b if C 1 outranks cand. c * ** erc ( a , c ) = e , L , W = a c if C 3 outranks cand. d * * * erc ( a , d ) = e , L , W = a d if C 3 outranks cand. e ** ** erc ( a , e ) = W , e , e = a e under every ranking cand. f *** * erc ( a , f ) = L , W , W = a f if C 2 or C
The comparison of candidate a with candidate e in Example 2, erc ( a , e ) = W , e , e , yields an odd ranking condition that does not actually express a particular ranking (no constraint has an L ), but instea din dicates that C 1 favors candidate a an dno constraint favors candidate e . In this case, candidate e is sai dto be harmonically bounded by candidate a because there can be no ranking under which e is more harmonic than a . Conversely, if candidate e were designated the winner, then erc ( e , a ) =
ERC also does not encode a specific ranking, but rather indicates that the mere existence of candidate a as an alternative means that no ranking can make candidate e optimal. fare with respect to one another according to a particular set of constraints. To know which rankings (if any) make candidate a globally optimal, it woul dbe necessary to define the candidate generating function G EN in order to obtain a representation of the entire set of ERCs { erc ( a , x ) | x  X  G EN ( input ) } might appear because, even though | G EN ( input ) | may be infinite, the fact that the num-ber of k -length ERCs is finite guarantees that each of the candidates in G EN ( input ) will map to one of a finite number of ERC sets. Furthermore, as Riggle (2004) demonstrates, the standard OT assumption of the universality of faithfulness constraints that pe-nalize changes to the input guarantees that all but finitely many of the members of
G EN ( input ) will be harmonically bounded. Riggle also presents an algorithm for com-puting this finite set of contenders (i.e., candidates that are not harmonically bounded) that can be use din cases where G EN is restricte dso that it is a rational function.
Regardless of how optimization is computed, what is relevant for the assessment of the VCD of OT is the definition of optimality. Following Definition 1, a ranking 50 not). The entire ERC set for a candidate ERCS ( a ) describes exactly the rankings under which candidate a is a globally optimal candidate.
 to reason about candidates. Most of the time, the ERCs of interest are those that contain at least one L an done W  X  X hat Prince calls nontrivial ERCs. ERCs that contain W  X  X  but no L  X  X  are generated when a candidate is compared with another candidate that it harmonically bounds, such as erc ( a , e ) = W , e , e in Example 2. This ERC reveals that candidate e cannot be optimal but yields no information about what rankings make candidate a optimal. Similarly, no ranking information can be gleane dfrom the all-e ERC that results from comparing  X  X ied X  candidates that have the same violations. fact that candidate e cannot be optimal under any ranking.
 relation among nontrivial ERCs is given in Definition 4 (Prince 2002, page 6, Proposi-tion 1.1).
 Definition 4
For nontrivial ERCs  X  and  X  ,  X   X   X  iff each  X  i  X   X  entails  X  Because nontrivial ERCs encode disjunctions of conjunctions (i.e., [ [
C 1 and ...
 disjunction introduction (whenever  X  has W where  X  has an L or an e ) an dconjunction elimination (whenever  X  has an e where  X  has an L ).
 Example 3 of ERCs makes it possible to derive new ranking conditions that are entailed by the combination of other ERCs. Prince (2002, page 8) provides a logical operation called fusion that derives entailments from sets of ERCs.
 Definition 5
The fusion of ERC set  X  is a single ERC  X  that is entaile dby  X  where:  X  i = L if any ERC in  X  has an L in its i th coordinate,  X  i = e if every ERC in  X  has an e in its i th coordinate,  X  i = W otherwise.
 page 14). Thus, the operation of fusion can reveal nonobvious entailments among ERCs.
Consider  X = { W , W , e , L , L , W , W , e , W , e , L , W  X 
C of  X  is L , W , L , L , which encodes the inference from  X  that bounded candidates, this ERC shows that no constraint ranking is consistent with the statements in  X  (in fact, they are circular). Prince refers to the class of ERCs with
L  X  X  but no W  X  X  as L + . He shows that these ERCs arise from fusion if an donly if the fuse dset contains incompatible ranking con ditions.
 Definition 6 An ERC set is consistent iff it has no subset that fuses to an ERC in page 11).
 all of its ERCs are true statements (Prince 2002, page 21). The ERCs in an inconsistent set, on the other hand, can never all be true of a single ranking. Inconsistency can arise Inconsistency can also arise across multiple candidate comparisons (e.g., ERCS ( d )in consistency, where several of the ERCs associated with a candidate fuse to from what Samek-Lodovici and Prince (1999) call collective harmonic bounding .
Finally, it is possible for inconsistencies to arise when ERCs for several candidates with distinct inputs are combined. For example, if ERCS ( x ) = { for distinct inputs (i.e., come from different tableaux), the union of their ERCs fuses to L , L , L  X  L + an dthereby reveals that there is no ranking un der which all three candidates are simultaneously optimal.
 whenever the original ERC is false an dvice versa. This opposition can be exploite din describing the range of consistent ERC sets.
 Definition 7
The negation of  X  is  X  where:  X  i = W if  X  i = L ,  X  i = L if  X 
Provided that  X  is not all e  X  X , every ranking is described by either  X  or  X  but not both (Prince 2002, page 42). In this way, ERC negation is just the standard notion of negation in three-value dlogics. The opposition between  X  and  X  makes a binary partition on the space of rankings. This is intuitively obvious for simple statements like W , L , e and L , W , e . The opposition is a bit less intuitive for more complex conditions like clear (i.e., if a and b are not tied, then every ranking must prefer one or the other). The antithetical relationship between an ERC an dits negation is reflecte din the operation of fusion by the fact that fusing antithetical ERCs will always yiel dan ERC in 3. The VCD of Elementary Ranking Conditions
Before turning to the question of the VC dimension of the sample space in OT, it will be helpful to define shatterability purely in terms of ERCs and thereby to establish a bound on the VCD of sets of ERCs. We will say that an ERC  X  is true of a given ranking the condition imposed by  X  is consistent with the linear ordering of the constraints defined by R .
 Definition 8
An ERC set  X  over constraints C ON is shatterable iff for every subset  X  ranking R C ON of which all ERCs in  X   X   X  are true while all the ERCs in  X  are false. 52
From this definition of shatterability for sets of ERCs, it is immediately clear that only nontrivial ERCs can occur in shatterable sets.
 Lemma 1 Every ERC in a shatterable set must contain at least one L an done W .

Proof :TheERCsof L + cannot occur in a shatterable set because there is no ranking of which they are true. Conversely, ERCs with no L  X  X  cannot occur in shatterable sets because there is no ranking of which they are false.
 it will be possible to reduce shatterability for ERC sets to consistency under negation. First, a definition of negation for sets of ERCs.
 Definition 9
A partial negation of ERC set  X  is obtaine dby negating every ERC in a subset  X 
For example:  X = { W , L , L , e , W , L } has four partial negations: one per subset. Theorem 1 An ERC set  X  is shatterable iff every partial negation of  X  is consistent.
Proof : Suppose every partial negation of  X  is consistent. Thus, for any partial negation in which  X  is the negate dsubset of  X  and  X  is the rest of  X  , it is the case that there is a ranking R of which all the ERCs of  X + X  are true. Because a nontrivial ERC and its negation are never both true of the same ranking an dtrivial ERCs cannot occur in shatterable sets, the ERCs in  X   X   X  are false of ranking R
Because  X  was arbitrary, it is the case that for every subset of  X  , there is a ranking of which the ERCs in that subset are false while the rest are true, an dthus consistency under partial negation is sufficient for shatterability. If, on the other hand, there is a partial negation that is not consistent, then there is a subset of  X  such that if the ERCs in that subset are negated, the resulting  X + X  is not consistent. However, because there is no ranking of which the members of an inconsistent ERC set are all true,  X  is not shatterable because there is no ranking of which the ERCs in  X  are true while the ERCs in
 X   X   X  are false. Thus, consistency under partial negation is both necessary and sufficient for shatterability.
 Corollary 1 Every subset of a shatterable ERC set is itself shatterable.

Proof : Because each partial negation of a shatterable ERC set must, by definition, be consistent an dbecause every subset of a consistent set must also be consistent, it is the case that every subset of a shatterable set is consistent under every partial negation and is thus shatterable.
 observation that no set containing  X  and  X  where  X   X   X  is shatterable because there can be no ranking of which the former is true while the latter is false. This is neatly captured by the fact that if  X   X   X  , no superset of {  X  ,  X  } can be shattere dbecause fusing is guarantee dto yiel dan ERC in L + . The requirement of consistency under partial co-occur in shatterable sets even though neither entails the other. In this case, fusing the negation of both ERCs yields L , L , L  X  L + . This follows transparently from the fact that either the statement  X  C 1 or C 2 outranks C 3  X  or the statement  X  true of any ranking of three constraints.
 negation makes it easy to demonstrate that for | C ON | = k , there are shatterable ERC sets of size k  X  1. Diagonal ERC sets provide a particularly simple example of a class of shatterable ERC sets of this size.
 Definition 10
ERC set  X  is diagonal if its members can be given and e  X  X  everywhere else.
 Lemma 2 Diagonal ERC sets are shatterable.

Proof : Assume that  X  is a diagonal ERC set and  X  is an arbitrary subset of an arbitrary partial negation of  X  .If n is the number of ERCs in  X  then, by the definition of diagonal ERC sets, there must be at least n + 1 coordinates (columns) in  X  that are fille dwith
L or W forsomeERCin  X  (i.e., are not all-e columns). Because each of the n ERCs has only one L ,atmost n columns contain L  X  X , thus the fusion of  X  contains at least one W .
Because  X  was an arbitrary subset, no subset fuses to L + was arbitrary, every partial negation is consistent an dthus  X  is shatterable. obtain a lower boun dof k  X  1 on the VCD of ERC sets. Having establishe dthat there are shatterable sets of k -length ERCs with k  X  1 members, what remains to be shown is that no set larger than k  X  1 is shatterable.
 Definition 11
Coordinate C i is W -unique in ERC set  X  if  X  has a partial negation  X  such that in the fusion of  X  ,  X  =  X  1 , ... ,  X  k , the only coordinate that contains a W is  X  Definition 12
The minor  X   X  , j of an ERC set  X  is a new set  X  in which ERC  X  has been remove dan d
For example, if  X = standard notion of the minor of a matrix. It is straightforward to show that every shatterable ERC set contains shatterable minors that can be obtaine dby removing one constraint X  X  coordinate (column) and one ERC (row).
 Lemma 3
Reduction Lemma . X  X f  X  is a shatterable ERC set, then it has a shatterable minor  X  54
Proof : By Corollary 1, for any  X   X   X  ,  X   X  X   X  } is shatterable. In  X  be at least one coordinate C j that is not W -unique. If this were not the case and every coordinate in  X   X  X   X  } was W -unique, then one of the L  X  X  in  X  woul docclu de the only
W in a partial negation of  X  , making it inconsistent contra the assumption that  X  is shatterable (  X  must have at least one L by Lemma 1). Because every partial negation of every subset of  X   X  X   X  } , there is a coordinate other than that fuses to W . This being the case, shatterability is preserve dif the minor  X   X  , j is shatterable as required.
 Theorem 2
For k &gt; 1, the largest shatterable set of k -length ERCs has k
Proof :If x is the size of the largest shatterable set of k -length ERCs and y is the size of the largest shatterable set of ( k + 1)-length ERCs, then y is not greater than x + 1. This must be so because if y  X  x + 2 then x coul dnot be the size of the largest shatterable set of k -length ERCs because a set of ( k + 1)-length ERCs woul dhave a shatterable minor larger than x . Because W , L and L , W are the only nontrivial ERCs for k = 2 an dbecause they are antithetical an dthus cannot co-occur in a shatterable set, the largest shatterable ERC set at k = 2 consists of a single ERC. This base case establishes an upper boun dof k  X  1 on the size of shatterable ERC sets an dthe diagonal ERC sets provide a lower bound of k  X  1. Together these bounds place the cardinality of the largest shatterable set at exactly k  X  1.
 members, but no shatterable sets with more than k  X  1 members. What remains now is to connect this result for ERC sets back to the realm of candidates. 4. The VCD of Optimality Theory
The question pose dat the outset of this article was: for a constraint set C ON with k constraints that map candidates to natural numbers, what is the cardinality of the largest set of candidates S such that, for each subset T  X  S , there is at least one ranking under which every t in T is optimal, but no s in S  X  T is optimal? Clearly, the answer to this question depends greatly on details of the constraints in C ON . However, if we reduce candidates to the ERC sets associated with them, it is possible to place an upper boun don the size of S without knowing anything about C ON other than its size k . in ERCS ( c ) is consistent with R . Conversely, c is mappe dto Fa l s e by in ERCS ( c ) is not consistent with R . This notion can be extended to sets of candidates as follows. If S is a set of candidates, then ERCS ( S )istheunionof ERCS ( s ) for all s Asample S is accepted by ranking R just in case every  X  in ERCS ( S ) is consistent with Conversely, S is rejected by R if any  X  in ERCS ( S ) is not consistent with if ERCS ( S ) is consistent, then there must be at least one ranking that accepts S .Inthis case, we will refer to S as a consistent sample . The concepts of partial negation and Definition 13
For a consistent sample S ,a partial exclusion is a partial negation of ERCS ( S )that rejects some F  X  S by rendering ercs ( f ) inconsistent for each f consistency of ercs ( s ) for every s  X  ( S  X  F ). Definition 14
C is w -unique in S if there is a partitioning of S into T and F under which coordinate that fuses to W in ERCS ( T ) for every partial exclusion that rejects F .
The property of W -uniqueness in samples crucially contrasts with what one might call being semi-unique X  X he case where for at least one, but not all, of the partial exclusions that reject F , C i is the only column that fuses to W in ERCS ( T ).
 Definition 15
Given a constraint set C ON an da sample S , the minor S x , j candidate x from S an dremoving constraint C j from C ON .

By extending partial negation, W -uniqueness, an dthe concept of minors to the realm of samples, it is straightforwar dto show that shatterable samples have shatterable minors. Lemma 4 Shatterable samples have shatterable minors.

Proof : Assume that S is a shatterable sample. Because removing candidate x from S has no effect on whether the remainder of S can be shattered, S
Sample S  X  X  x } must have at least one coordinate that is not W -unique. If this were not the case, then, because ERCS ( x ) must contain at least one coordinate with an L (else there woul dbe no way to reject { x } ), the presence of x in S woul dplace an L in a
W -unique coordinate in S  X  X  x } . However, this woul dmake it impossible to associate a ranking with at least one partitioning of S into accepte dan drejecte dsubsets contra the assumption that S is shatterable. If C j is a coordinate in S then, for every partial exclusion that rejects F , under each partitioning of S T and F , there is at least one other coordinate C i that fuses to W .Thus, remove dfrom C ON while preserving the shatterability of S shatterable minor as required.
 the illustration of a one-to-one relationship between k an dthe boun don shatterable sets by showing that removing an ERC from a shatterable set makes it possible to remove a coordinate from the remaining ERCs while preserving shatterability. If shatterable
ERC sets coul dbe larger than k  X  1, then it woul dhave been necessary to remove sev-eral ERCs before it was possible to safely remove a coordinate from the remaining
ERCs. Because ERC sets and candidate samples both have shatterable minors, a similar strategy will show that shatterable samples must also grow at a one-to-one rate with k . Theorem 3
If |
C ON | = k , then the size of shatterable sample sets is bounded at k
Proof :If k = 2, a sample consisting of a single candidate can be shattered if ERCS ( s )is { 
W , L } or { L , W } , but no larger sample can be shattered. If there were such a sample, it would contain at least two candidates a and b an dthere woul dbe a ranking un der which both candidates were optimal, a ranking under which neither candidate was optimal, a ranking that made a but not b optimal, an danother ranking that ma de b but not a optimal. This state of affairs requires at least four distinct rankings, which is impossible with only two constraints. Thus, it is establishe dthat, at k = 2, the largest shatterable sample set has at most one candidate. 56 the largest shatterable sample is | S | + 1. If this were not the case, there woul dbe a shatterable sample X such that | X | X | S | + 2for k + 1 constraints. However, because shatterable samples have shatterable minors (Lemma 4), this woul dmean that there was a shatterable sample of size | S | + 1for k constraints, contrary to the assumption that S was largest. Given the base case that | S | = 1 when k = 2, the cardinality of shatterable samples is thus bounded at k  X  1asrequired.
 didates in the sample space for any ERC set. In actual practice, the specific details of the constraints in C ON an dthe range of ways that they interact will determine which elements of the powerset of the set of k -length ERCs are associated with candidates in can be much lower than | C ON | X  1. Nonetheless, the result that the VCD of OT can be at most | C ON | X  1 is propitious for the learnability of Optimality Theoretic grammars. 5. Conclusions
Bounding the VC dimension of OT according to the number of constraints in C ON es-tablishes a general property of the sets of ranking hypotheses that can be associated with sets of candidates. This bound is independent of any assumptions about how the
ERC sets for candidates are computed, independent of any assumptions about how optimizations are computed, and independent of any assumptions about the formal properties of constraints other than that they map candidates to learnability result for OT. Blumer et al. (1989), building on the learning model of Valiant (1984), define a concept class C as uniformly learnable if there is a learning algorithm
A such that, for any error threshold an dconfi dence level  X  ,if samples randomly drawn according to a probability distribution  X  over the sample space, then A has at least probability  X  of generating a hypothesis whose likelihood of misclassifying any point in the sample space drawn randomly according to  X  is less than . Blumer et al. link the VC dimension to learnability by showing that concept classes are uniformly learnable if an donly if they have a finite VCD. Moreover, they show that upper bounds on m can be establishe dfor learning that depen donly on the
VC dimension of the concept class to be learned. The bound on m according to d = VCD from Blumer et al. is given in Equation (1).
This is a worst-case bound that holds for the most adversarial probability distributions over the sample space an dthe worst consistent learning algorithms (i.e., algorithms that are consistent in that they correctly classify all data in the training set, but worst-case in that they err maximally on all unobserve d data). Specific OT learning algorithms that have tighter bounds and non-worst-case probability distributions over samples will certainly present a different picture.
 (1993) basic CV syllable theory in which candidates are mappings from all ways of modifying i through deletion and insertion of . ,C,andV.
C insertion, (iv) a constraint against syllables with codas, and (v) a constraint against syllables without onsets, then the range of possible rankings of these five constraints allows for 120 different grammars which in turn define twelve different languages (i.e., twelve subsets of the sample space X = { C, V } *  X { mappings, then the probability distribution over the sample space can be characterized in terms of the probability distribution over the input strings in candidate a = ( i  X  o ) provides information about the teacher X  X  ranking in the form of
ERCS ( a ) = { erc ( a , b ) | b = ( i  X  x )  X  G EN ( i ) be derived via an algorithm called C ONTENDERS .Inthissystem, ERCS ( a ) can contain from zero to twelve ERCs. The zero-ERC cases arise for input strings that share the same optimal output under all rankings (i.e., /CV/  X  [ . CV . ]). The sets top out at twelve because there are never more than twelve contenders (i.e., non-harmonically-bounded candidates) for any given input string. The twelve ERC bound is a consequence of the fact the 120 rankings only realize twelve distinct languages. sets. Because of this, the boun don shatterable samples establishe din Section 4 carries transparently over to the more general case where learners are traine dwith optimal ( i , o ) mappings an dthen teste dwith novel inputs. Because the set of conten ders is determined solely by G EN an dC ON (which the learner is presume dto have access to) if the learner can compute C ONTENDERS ( i ), then testing on novel inputs reduces to having the learner select one optimal candidate from the set of contenders, which in turn reduces to binary questions of harmonic inequality between pairs a and b in consistent with the ERCs gleane dfrom previous observations.
 so that the VC dimension can predict its success. There are undoubtedly other possible formulations. Furthermore, as noted, real-world cases will often contain details that are more relevant than the VC dimension in predicting learnability. For instance, in syl-algorithm generates one candidate per language in the factorial typology. In such a case, the ERC set for a single optimal candidate can serve as a  X  X lobal trigger X  that is sufficient to uniquely identify the teacher X  X  language. Further analysis with specific constraints an dOT learning algorithms like Recursive Constraint Demotion (Tesar 1995, 1997, 1998; Tesar an dSmolensky 1993, 2000), the Gra dual Learning Algorithm (Boersma 1997, 1998; Boersma an dHayes 2001), an dthe ERC-Union learner (Riggle 2004) will surely yiel dfurther insights an da less abstract picture of learning in Optimality Theory. frameworks (Haussler, Kearns, an dSchapire 1992) an dis applicable without any as-sumptions other than that the learner is consistent. Any learner that bases its hypotheses on the union of the ERCs associate dwith the data on which it is traine dis guarantee dto be consistent, an dthus an extremely simple ERC-union learner can learn OT grammars 58 from random training texts whose size m is linear in k . This linear boun don the re-lationship between k an dsample complexity is a nice tightening of the k log that follows from the finitude of k ! an dcontrasts starkly with pessimistic assessments of learnability suggeste dby the factorial relationship between k an dthe number of possible grammars.
 References
