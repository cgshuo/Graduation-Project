 We present a recommender systems approach to conference paper assignment, i.e., the task of assigning paper submis-sions to reviewers. We address both the modeling of reviewer-paper preferences (which can be cast as a learning prob-lem) and the optimization of reviewing assignments to sat-isfy global conference criteria (which can be viewed as con-straint satisfaction). Due to the paucity of preference data per reviewer or per paper (relative to other recommender systems applications) we show how we can integrate mul-tiple sources of information to learn reviewer-paper prefer-ence models. Our models are evaluated not just in terms of prediction accuracy but in terms of end-assignment quality. Using a linear programming-based assignment optimization, we show how our approach better explores the space of un-supplied assignments to maximize the overall affinities of papers assigned to reviewers. We demonstrate our results on real reviewer bidding data from the IEEE ICDM 2007 conference.
 H.4.2 [ Information Systems Applications ]: Types of Systems X  Decision support ; J.4 [ Computer Applications ]: Social and Behavioral Sciences Algorithms, Human Factors Recommender systems, collaborative filtering, conference pa-per assignment, linear programming
Modern conferences are beset with excessively high num-bers of paper submissions. Assigning these papers to ap-propriate reviewers in the program committee (which can constitute a few hundred members) is a daunting task and hence motivates the use of recommender systems.

The primary input to the conference paper assignment problem (CPAP) is a papers  X  reviewers matrix of  X  X ids X , expressing interest or disinterest of reviewers to review spe-cific papers. The goal is to construct a set of reviewing as-signments taking into account reviewer capacity constraints , adequate numbers of reviews for papers, expertise modeling, conflicts of interest, and other global conference criteria.
There are three key differences between traditional recom-mender applications and the CPAP problem. (i) In a tradi-tional recommender, recommendations that meet the needs of one user do not affect the satisfaction of other users. In CPAP, on the other hand, multiple users (reviewers) are bid-ding to review the same papers and hence there is the pos-sibility of one user X  X  recommendations (assignments) affect-ing the satisfaction levels (negatively) of other users. Henc e the design of reviewer preference models must be posed and studied in an overall optimization framework. (ii) In a con-ventional recommender, the goal is often to recommend new entities that are likely to be of interest, whereas in CPAP, the goal is to ensure that reviewers are predominantly assigned their (most) preferred papers. Nevertheless, preference mod-eling is still crucial because it gives the assignment algorit hm some degree of latitude in aiming to satisfy multiple users. Finally, (iii) recommender systems are used to working with sparse data but the amount of  X  X ignal X  available to model preferences in the CPAP domain is exceedingly small; hence we must integrate multiple sources of information to build strong preference models.

We organize our framework into two stages:  X  X rowing X  the given bids by adapting recommendation techniques to pre-dict unknown reviewer-paper preferences, and identifying a good assignment by optimizing conference criteria. Other approaches to CPAP (e.g., [1]) are surveyed elsewhere [2]. We apply our framework on bids and auxiliary information (see Fig. 1) gathered from the 7th IEEE Intl. Conf on Data Mining (ICDM X 07) for which the third author was a pro-gram chair. Similar scope datasets from other conferences are not publicly available (also acknowledged in [5]) and we hope our research will spur greater availability. (The Cyber-chair system used by the ICDM series has expressed inter-est in implementing our approach and we plan to approach Easychair and other CMSs as well.) We emphasize that all datasets were anonymized before the modeling and analysis steps conducted here.
We are given ratings (henceforth, interchangeable with preferences ) between m reviewers and n papers. (Recall that these ratings are really bids/signs of interest to review pa-pers, not the actual ratings reviewers assign to papers after reading and evaluating them.) A rating r ui indicates the preference by reviewer u of paper i , where high values mean stronger preferences. Usually the vast majority of ratings are unknown, e.g., the ICDM data involves 529 papers, 203 reviewers, and only 6267 bids. In ICDM X 07, the given bids are between 1 and 4, indicating preferences as follows: 4=  X  X igh X , 3= X  X K X , 2= X  X ow X  and 1= X  X o X  and we aim to make predictions in the same space.

We distinguish predicted ratings from known ones, by us-ing the notation  X  r ui for the predicted value of r ui . To eval-uate the models we assess RMSE over 100 random 90-10 training-test splits. We hasten to add that we do not ad-vocate the myopic view of RMSE [4] as the primary crite-rion for recommender systems evaluation. We use it in this section primarily due to its convenience for constructing di-rect optimizers. In the next section we will evaluate perfor-mance according to criteria more natural to CPAP. We also note that small improvements in overall RMSE will typi-cally translate into substantial improvements in bottom-lin e performance for predicting reviewer-paper preferences.
The model we learn is of the form: and we proceed to explain each of the terms below.
Much of the variability in the data is explained by global effects, which can be reviewer-or paper-specific. It is im-portant to capture this variability by a separate component, thus letting the more involved models deal only with genuine reviewer-paper interactions. We model these global effects through the first three terms of Eq. 1, i.e., + b u + b i constant indicates a global bias in the data, which is taken to be the overall mean rating. The parameter b u captures reviewer-specific bias, accounting for the fact that different reviewers use different rating scales. Finally, the paper bias, b , accounts for the fact that certain papers tend to attract higher (or, lower) bids than others. We learn optimal values the associated squared error function with just these three terms (along with some regularization to avoid overfitting). The resulting average test RMSE is 0.6286 .

A separate analysis of each of the two biases shows re-viewer effect ( + b u , with RMSE 0.6336 ) to be much more significant than paper bias ( + b i , RMSE 1.2943 ) in re-ducing the error. This indicates a tendency of reviewers to concentrate all ratings near their mean ratings, which is supported by examination of the data.

While the baseline model could explain much of the data variability, as evident by its relatively low associated RMSE , it is useless for making actual assignments. After all, it give s all reviewers exactly the same order of paper preferences. Thus, we are really after the remaining unexplained vari-ability, where reviewer-specific preferences are getting ex-pressed. Uncovering these preferences is the subject of the next subsections.
Latent factor models (e.g., [3]) comprise a common ap-proach to collaborative filtering with the goal to uncover latent features that explain observed ratings. The premise of such models is that both reviewers and papers can be characterized as vectors in a common f -D space. The in-teraction between reviewers and papers is modeled by inner products in that space, the fourth term of Eq. 1. Here, p u  X  R f and q i  X  R f are the factor vectors of reviewer u and paper i , respectively. The resulting average test RMSE is slowly decreasing when increasing the dimensionality of the latent factor space. E.g., for f = 50 it is 0.6240 , and for f = 100 it is 0.6234 . Henceforth, we use f = 100.
While latent factor models automatically infer suitable categories, much can be learned by known categories at-tributed to both papers and reviewers. ICDM X 07 submis-sions specify a number of predefined categories as primary and secondary topics for a given paper. We model the en-tered matching between paper i and category c by: The value assignment (1 for  X  X rimary X , 0.5 for  X  X econdary X ) is derived by cross validation and is quite intuitive. Simi-larly, we use the following for matching reviewers with their desired categories: Notice that in ICDM X 07, reviewers could specify lack of in-terest (or inability to) review papers from certain categories (this is different from conflicts of interest, discussed later).
In the fifth term of Eq. 1, the weights w c indicate the sig-nificance of each category in linking a reviewer to a paper, and are learnt automatically by minimizing the squared er-ror on the training set. It is plausible that, e.g., a mutual interest in some category A, will strongly link a reviewer to a paper, while a mutual interest in another category B is less influential on papers choice. Table 1 depicts results of this analysis, showing differences in orders of magnitude in the ability of different categories to correctly predict association s of reviewers to papers. Note in particular that there is no obvious monotonic relationship between the weight imputed to categories and the number of papers/reviewers associated with the category. When adding subject categories to the baseline and factor models, the resulting RMSE is 0.6197 .
We inject paper-paper similarities into our models in a way reminiscent of item-item recommenders [6]. The build-ing blocks here are similarity values s ij , which measure the similarity of paper i and paper j . The similarities could be derived from the ratings data, but those are already covered by the latent factor model. Rather, we derive the similarity of two papers by computing the cosine of their abstracts. Usually we work with the square of the cosine, which better contrasts the higher similarities against the lower ones.
In the sixth term of Eq. 1, the set R( u ) contains all papers on which u bid. The constant  X  is for regularization: it is penalizing cases where the weighted average has very low support, i.e. P j  X  R( u ) s ij is very small. In our dataset it was determined by cross validation to be 0.001. The parameter  X  sets the overall weight of the paper-paper component. It is learnt as part of the optimization process (cross-validation could have been used as well). Its final value is close to 0.7. When this term is combined with the overall scheme, the RMSE drops down further to 0.6038 .
We craft reviewer-reviewer similarities s uv analogously to paper-paper similarities, measured as the number of com-monly co-authored papers as reported in DBLP. We point out that DBLP data might be incomplete, and co-authorship does not imply similarity of research interests. Nevertheless, our main contribution here is to show how to incorporate reviewer-reviewer similarities in Eq. 1 and more sophisti-cated ways to define s uv can be readily plugged in. By in-tegrating this factor, the RMSE is 0.6015 .
A final source of data is conflicts of interest for certain (pa-per, reviewer) combinations, e.g., the reviewer might be the former advisor of the author. Many conferences define what it means to have a CoI and solicit this information explicitly during the bidding phase. We do not aim to model/predict new CoIs but show in the next section how they are incor-porated to avoid making erroneous assignments.
Our predicted preference matrix can now be supplied as input to any of the assignment algorithms discussed in [2]. We chose the Taylor algorithm [7] as a representative exam-ple because it was used during ICDM X 07 and thus enables a baseline comparison with an approach that does not perform any preference modeling. It can incorporate global confer-ence constraints such as the desired number of reviewers for each paper ( k p ), and a desired maximum number of papers for each reviewer ( k r ). (For ICDM X 07, these values are 3 and 9, respectively.) Denoting the predicted ratings matrix as R , the goal is to optimize the assignments matrix A [7]: Here, the objective criterion X  X race ` R T A  X   X  X aptures the global affinity of all reviewers across all their assigned pa-pers. CoIs can be modeled by hardwiring the desired entries of A (to zero) and taking them  X  X ut of play X  in Eq. 2.
This integer programming problem is reformulated into an easier-to-manage linear programming problem by a series of steps, using the node-edge adjacency matrix, where every row corresponds to a node in A , and every column repre-sents an edge [7]. This reformulation is a bit more com-plicated, but essentially renders the problem solvable via methods such as Simplex or interior point programming. In particular, as Taylor shows in [7], because the reformulated constraint matrix is totally unimodular , there exists at least one globally optimal assignment with integral (and due to the constraints, Boolean) coefficients.
We have already shown the ability of our modeling to better capture reviewer-paper preferences. But do the im-proved models translate into better assignments? Note the key distinction between recommendations and assignments . To evaluate assignment quality, we extend the train-test methodology from above. In other words, both the predic-tion algorithm and the assignment algorithm cannot see the originally given preferences within the test set. We use the training set to learn model (1), predict all ratings using this model, and feed these predictions as input to (2). While the resulting assignment will be spread across the training and test sets, we will specifically evaluate those made from the test set and determine whether the reviewer had rated them as  X  X o, X   X  X ow, X   X  X K, X  or  X  X igh. X  This methodology mimics the real life scenario where the given reviewer ratings (cor-responding to the training set) are limiting the possibilities of the assignment algorithm, but by revealing more ratings through our prediction phase, we aim to gain the flexibility to provide better assignments. As the proportion of the test set increases, we take away more available preferences, which simulates an increasingly harsher assignment environment.
However, before using Taylor X  X  model (2), it is important to balance the rating scale of various reviewers. For exam-ple, some reviewers are very enthusiastic and tend to give mostly high ratings, while others are more cautious and give low ratings. While our preference modeling captures such variance, it is unnecessary for the assignment phase since Taylor X  X  model would concentrate only on reviewers with high ratings, which is undesirable. Thus, we suggest two alternative per-reviewer normalization strategies: 1. Subtract the per-reviewer mean from each predicted 2. Calculate normalized ratings for each reviewer, so Regardless of the chosen normalization scheme, we add the normalized predicted rating to the original preferences (if it is part of the training data) or to the mean rating value (2.5) (for test data; recall that this is between the  X  X k X  and  X  X ow X  ratings). This forms our final input matrix R , which we feed into Taylor X  X  optimization algorithm.

We evaluate many train-test splits, averaging 100 random trials for each split. The baseline is Taylor X  X  original al-gorithm, where all missing ratings, including those in the test set, are treated as  X  X nknowns. X  We compare this base-line against the two aforementioned alternatives, Resid and Norm, with an identical handling for missing ratings. Specif-ically, we look at the proportion of assignments from the test set that fall in the  X  X o, X   X  X ow, X   X  X K, X  and  X  X igh X  categories. Table 2: Evaluating assignments: observe the dra-matic improvement from the baseline (Taylor) to our methods (Norm and Resid).

The results presented in Table 2 were fairly consistent across different test set proportions. As illustrated here, the predominant number (around 60-65%) of test assignments made using the original preference matrix (Taylor) fall in the unpreferred ( X  X o X ) category, mirroring experiences dur-ing ICDM X 07 organization 1 . On the other hand, when im-
The assignments were manually re-wired afterward. puting the missing ratings using either Resid or Norm, the balance completely changes in favor of higher quality pref-erences. Resid makes about 60% of test assignments out of the highest quality ratings ( X  X igh X ), and only about 12% of test assignments are bad ( X  X o X ). Norm is close, but not quite as good as Resid, a difference that should be further investigated over additional datasets. Overall we find that the results strongly support our contention that assignment quality can be increased by providing more flexibility with additional ratings from which to choose.
Why does our approach work? Especially with harsh train-test splits? If we view a reviewer X  X  preferences as a partial order over papers, we can think of our approach as  X  X traightening X  out the partial order into a total order that is consistent with multiple sources of data. We in-tend to provide theoretical justification for our empirical re-sults using this viewpoint. The second new aspect to our work is the integration of recommendation and optimiza-tion/constraint satisfaction. In the future we seek to study how recommenders help aid optimization routines by pro-viding additional  X  X ues X  or flexibilities in constraint sati sfac-tion/search. Besides CPAP, this has applications to com-bined recommendation-optimization scenarios such as tar-geted marketing and advertising under resource constraints. Finally, to gain qualitative user feedback, we intend to field the recommendation/assignment capabilities presented here in a real conference management system and gain further insights into the issues involved.
We thank Prof. Xindong Wu, Steering Committee chair of IEEE ICDM for permission to use bidding and auxiliary data from ICDM X 07. [1] C. Basu, H. Hirsh, W. Cohen, and C. Nevill-Manning. [2] D. Conry, Y. Koren, and N. Ramakrishnan.
 [3] T. Hofmann. Latent semantic models for collaborative [4] S. McNee, J. Riedl, and J. Konstan. Being accurate is [5] D. Mimno and A. McCallum. Expertise modeling for [6] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl. [7] C. J. Taylor. On the optimal assignment of conference
