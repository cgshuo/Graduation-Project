  X  Manifold-ranking has proved to be an effective method for topic-focused multi-document summarization. As basic manifold-ranking based summarization method constructs the relationships between sentences simply by the bag-of-words cosine similarity, we believe a better similarity metric will further improve the effec-tiveness of manifold-ranking. In this paper, we propose a joint op-timization framework, which integrates the manifold-ranking pro-cess with a similarity metric learning process. The joint framework aims at learning better sentence similarity scores and better sen-tence ranking scores simultaneously. Experiments on DUC datasets show the proposed joint method achieves better performance than the manifold-ranking baselines and several popular methods. H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing X  abstracting methods Multi-document summarization; matrix factorization; manifold-ranking
Multi-document summarization (MDS) aims at producing a summary of the major information from a set of documents. Topic-focused multi-document summarization is a task which requires the generated summary to be biased to a given topic. As topic-focused MDS can provide a personalized profile, it is useful in news services, QA systems and search engines. Manifold-ranking based summarization method [13] has proved useful to deal with the topic-focused MDS task. It makes uniform use of the sentence-to-sentence relationships and sentence-to-topic relationships in a manifold-ranking process, and does not need a training procedure or subtle parameter tuning, which makes it more appropriate for practical application.  X  X iaojun Wan is the corresponding author.
 c
However, the basic manifold-ranking summarization method constructs the relationships of sentences simply by the bag-of-words cosine similarity, which cannot faithfully capture the seman-tic similarity among sentences. We believe a better similarity met-ric will further improve the performance of the manifold-ranking based summarization method. In this paper, we propose a Joint Matrix Factorization and Manifold-Ranking (JMFMR) framework for topic-focused multi-document summarization, which aims at learning better sentence similarity scores and better sentence rank-ing scores simultaneously.

In the following parts of this paper, we first summarize related works, and then introduce the manifold-ranking framework and the matrix factorization framework. Then we propose our approach and the experiments. At last is the conclusion of this paper.
There have been quite a lot works on multi-document summa-rization and in this section we mainly focus on the unsupervised topic-focused extractive-based summarization task. Some meth-ods treat summarization as a ranking task and apply graph-based ranking algorithms. [13] propose to use manifold-ranking to make uniform use of sentence-to-sentence and sentence-to-topic relation-ships. [12] extend a multi-modality manifold-ranking by consid-ering the within-document sentence relationships and the cross-document sentence relationships as two modalities. [1] further im-prove the manifold-ranking based relevance propagation via mutual reinforcement between sentences and theme clusters.

Matrix factorization technique has been applied to summariza-tion tasks in several works. For example, [8] propose using nonneg-ative matrix factorization to extract the query relevant sentences. [9] use nonnegative matrix factorization and K-means clustering for sentence clustering and extraction. [14] propose using sentence-level semantic analysis and symmetric nonnegative matrix factor-ization to factorize the sentence-sentence similarity matrix.
In this section we first introduce the framework of manifold-ranking based multi-document summarization in [13]. Then we will introduce the matrix factorization technique for latent seman-tic similarity learning in our approach. Given a set of data points as  X  = { s 1 , s 2 , . . . , s ing sentences to be ranked including the pseudo-sentence as s  X  :  X   X  R denotes a ranking function which assigns each point s (1  X  i  X  N ) with a ranking value f i .  X  can be viewed as a vec-tor F = [ f 1 , . . . , f N ] T . Define a prior vector Y = [ y in which y 1 = 1 because s 1 is a topic description pseudo-sentence and y i = 0 (2  X  i  X  N ) for all the remaining sentences to be ranked. The adjacency matrix of the graph can be denoted as W = { W ij } (1  X  i, j  X  N ) , with each element W ij correspond-ing to the similarity between s i and s j ( W ii is forced to 0 to avoid self-reinforcement). Then the manifold-ranking algorithm can be formulized as a regularization framework of minimizing: where D is a diagonal matrix with its ( i, i ) -element equals to the sum of the i -th row of W . Let F  X  denotes the solution of minimiz-ing  X  ( F ) , and F  X  can be solved by iterating (2) until convergence:
We introduce a Weighted Textual Matrix Factorization (WTMF) model [3] for assessing latent semantic text similarity. Given a M  X  N co-occurrence matrix X , where the row number M corresponds to M words in the corpus, and the column number N corresponds to N sentences. Each cell X ij contains the TF-IDF value of word w i in the sentence s j . The WTMF framework aims at factorizing matrix X into two matrices such that X  X  P T Q , where P is a K  X  M matrix, and Q is a K  X  N matrix. K is the dimension of latent vectors. The main difference between WTMF and LSA [7] is that WTMF allows for direct weight control on each matrix cell X ij . The model variables to be solved (vectors in P , Q ) are optimized by minimizing: where  X  is a free regularization factor to control the regularization term, and the weight matrix W defines a weight for each cell in X . P ,i is a K -dimensional latent semantic vector profile for word w and Q  X  ,j is the K -dimensional vector profile that represents sen-tence s j . Then the similarity of two sentences s j and s calculated by the cosine similarity of Q  X  ,j and Q matrix W is defined as:  X  m is a very small weight for penalty. The intuition of a small  X  when X ij = 0 in WTMF is to diminish the influence where word w i is missed in the sentence s j , and the weighted matrix factoriza-tion is argued to yield better latent vectors than traditional matrix factorization [3].
We propose a joint approach of matrix factorization based manifold-ranking, to learn the similarity metric and sentence ranking scores in a framework simultaneously. The underly-ing idea is that a better similarity metric will help improve the manifold-ranking process; meanwhile information from the manifold-ranking process can also help learn a better similarity metric. In this section we first introduce the objective function of our proposed joint approach, and then we discuss how to solve the minimization problem and the inner weighted nonnegative matrix factorization problem.
In the joint framework, we adopt a weighted nonnegative matrix factorization technique similar to WTMF, to learn sentence similar-ity. Specifically, we integrate similarity learning and sentence score learning into one optimization objective, as minimizing  X  : gle between latent semantic vectors Q  X  ,i and Q  X  ,j , intuitively rep-resenting the cosine similarity of sentence s i and s j after matrix factorization. D is a diagonal matrix with its ( i, i ) -element equals to the sum of the i -th row of W  X  . Y is same as in Section 3.1. X is same as in WTMF, representing the word-sentence co-occurrence matrix. W  X  X  here is defined as:
In the objective function (5), P , Q and F are to be optimized, and constrained to be nonnegative.  X  and  X  are free parameters to control the regularization terms. By optimizing the objective function, similarity measurements of the sentences will be got from scores of sentences will be got from F . We will explain the idea un-derlying the objective function as follows. We define the following two sub-objective functions:
Sub-objective function (7) corresponds to the objective function of manifold-ranking. The only difference is that W in the objective function of manifold-ranking is now replaced by W  X  , a function of an optimization variable Q . Sub-objective function (8) corresponds to the objective function of matrix factorization, which is similar to the WTMF framework, and we call it Weighted Textual Non-negative Matrix Factorization (WTNMF). Besides the nonnegative constraints, the main difference between WTNMF and WTMF is that when X ij 6 = 0 , W  X  X  ij is changed from 1 to e f j . The change aims to strengthen the influence of the terms in the important sentences which have high ranking scores obtained by manifold-ranking. This aim is achieved by forcing P  X  ,i  X  Q  X  ,j closer to X if sentence s j is more important. In the joint objective function, manifold-ranking is affected by similarity measurements from ma-trix factorization, and matrix factorization is affected by sentence importance scores from manifold-ranking. This co-effect will lead to a different convergence state from separate matrix factorization and manifold-ranking objective functions, which we believe will help improve the results.
In this section, we discuss how to minimize objective function (5). Gradient based optimization algorithms can be used, but they are too time-consuming for this task. To solve this problem, we e xplore an alternating strategy to find an approximate solution to the optimization problem. The approximate algorithm optimizes P , Q and F alternately. When optimizing P and Q , F is treated as fixed, and in this step P and Q are optimized to convergence, by minimizing  X  2 to convergence. After the optimization of P and Q , an adjacency matrix for manifold-ranking can be calculated from Q . Then it is turn to fix P , Q and optimize F to convergence, by minimizing  X  1 with (2) to convergence. Then a new F is achieved and the alternating procedure can be restarted to optimize P and Q again. This alternating optimization is repeated several times, and an approximate solution will be achieved. The entire proce-dure is shown in Algorithm 1. The underlying idea of the alter-nating strategy is that in the matrix factorization procedure, a new similarity metric is learned, and a better similarity metric will help the manifold-ranking procedure learn better sentence importance scores; at next step, better sentence importance scores will further help improve matrix factorization results. After several repeats, an approximate stable state will be reached and better similarity mea-surements and ranking scores of sentences will be achieved. Algorithm 1 A pproximate optimization algorithm In this section we discuss how to optimize P , Q in (8). In WTMF [3] without nonnegative constraints, P , Q are computed iteratively by the following equations: the i -th row of weight matrix W  X  X  and  X  W ( j ) = diag ( W M  X  M diagonal matrix containing the j -th column of W  X  X  derivation of (9) can be found in [10].

Inspired by the alternating nonnegative least squares algorithm used in [6], we apply the nonnegative least squares algorithm to solve (9) by solving the reformed alternating least squares problem (10). We used a C++ implement of nonnegative least squares tool called tsnnls 1 in the experiment to solve (10). h ttp://www.jasoncantarella.com/wordpress/software/tsnnls/
We conducted experiments on the widely used Document Under-standing Conference (DUC) datasets, with DUC 2005 as the devel-opment set to determine the parameters and DUC 2006 as the test set. We used the popular ROUGE toolkit 2 for evaluation.
We conducted the experiments similarly to [13]. For every topic and its related documents, we first constructed the word-sentence co-occurrence matrix X , and initialize all elements of P , Q and F with 1 . Y = [1 , 0 , . . . , 0] T , and parameters  X  ,  X  , K and  X  set before the optimization procedure starts.

After the optimization procedure in Algorithm 1 is finished, the ranking scores and similarity measurements of sentences can be got from F  X  and Q  X  , respectively. Then the greedy algorithm in [13] is applied to remove redundancy between sentences. A final summary is formed with the highest overall ranking score sentences.
To prove the effectiveness of the proposed method (JMFMR), we implemented two baselines of the basic manifold-ranking method (MR Baseline) [13] and a strong baseline (WTNMF-MR). WTNMF-MR replaces the bag-of-words similarity in the basic manifold-ranking method with latent semantic similarity learned with WTNMF. The main difference between WTNMF-MR and JMFMR is that WTNMF-MR does not consider the mutual effect of the manifold-ranking process and matrix factorization process. The comparison intends to test the effectiveness of the joint model. Pa-rameters of WTNMF-MR are the same with JMFMR. The ROUGE scores of recall are shown in Table 1. Two-tailed t-tests showed that the improvements of JMFMR to the two baselines are all statisti-cally significant ( p  X  0 . 001 ).

We also compared our method (JMFMR) with several rep-resentative unsupervised MDS methods, including 1) manifold-ranking based methods: Multi-modality manifold-ranking (MM-MR) [12], Mutually reinforced manifold-ranking (RDRP-AP) [1]; 2) matrix factorization based methods: Latent Semantic Analysis (LSA) and Symmetric Nonnegative Matrix Factorization (SNMF) [14]; 3) other typical methods: query Latent Dirichlet Alloca-tion (qLDA) and Topic Modeling with Regularization (TMR) [11], Multi-objective Optimization model (MO-LEX) [5], Document Summarization based on Data Reconstruction (DSDR-non) [4]. Results of these methods for comparison are directly borrowed from respective papers as shown in Table 1 and Table 2. As we cannot get the summary-level scores for each document set, we are unable to perform t-test between our results with the results directly copied from published papers. However, we can see the large per-formance gap between our results and these results.

Tables 1 and 2 show the results over ROUGE recall and F-measure scores on the DUC 2006 dataset [2]. The parameters of our model are tuned on DUC 2005 dataset and set as follows:  X  = 0 . 1 ,  X  = 0 . 09 and 6 iterations. We also set K = 100 ,  X  m = 0 . 0005 which are the same with WTMF [3]. As we can see from Tables 1 and 2, the proposed joint optimization method achieves significant improvement over the MR Baseline and WTNMF based manifold-ranking method. Compared with other unsupervised methods, the proposed method also achieves better results over most metrics, h ttp://haydn.isi.edu/ ROUGE/ The entire ROUGE command used is  X  X OUGE-1.5.5.pl -n 2 -m -x -2 4 -u -c 95 -r 1000 -f A -p 0.5 -t 0 -a -d -l 250 X . Method ROUGE-1 ROUGE-2 ROUGE-SU4 JMFMR 0.41482 0.08921 0.14671 R DRP-AP 0.39615 0.08975 0.13905 MO-LEX 0.4030 0.0913 0.1449
TMR 0.41176 0.08759 0.14213 qLDA 0.40211 0.08687 0.14419 WTNMF-MR 0.40908 0.08744 0.14404 MR Baseline 0.40552 0.08374 0.14169 NIST Baseline 0.30217 0.04947 0.09788 Method ROUGE-1 ROUGE-2 ROUGE-SU4 JMFMR 0.41244 0.0887 0.14585 M M-MR 0.40306 0.08508 0.13997 SNMF 0.39551 0.08549 0.13981 LSA 0.33087 0.05022 0.10226 DSDR-non 0.33168 0.06047  X  X  X  NIST Baseline 0.32082 0.05267 0.10408 w hich demonstrates the effectiveness of the proposed joint matrix factorization and manifold-ranking method.
In this section we explore how the proposed optimization model is influenced by different parameter values on F-measure scores of ROUGE-1. When tuning one parameter, we fixed other parameters at the initial values in the experiments introduced in Section 5.3.
It can be seen from Figure 1(a) that the model gets best results around  X  = 0 . 1 , and the overall results are good when  X   X  0 . 1 , and fall down rapidly when  X  is too large. The reason can be got from the iteration equation (2), that when  X  is too large,  X  = 1 / (1 +  X  ) becomes too small, and F is mainly determined by the initial score vector Y with most elements being 0. In Figure 1(b) the results are relatively good around  X  = 0 . 05 to  X  = 1 , showing that the model is not fiercely influenced by the matrix factorization regularization parameter  X  . In Figure 1(c) the results achieve a relatively stable state after 4 iterations, so after 6 times of iteration the optimization procedure can be viewed as convergent.
In this paper, we propose a joint optimization framework to inte-grate the manifold-ranking process and the similarity metric learn-ing process simultaneously. In the future work, we will explore more syntactic and semantic information of the documents to fur-ther improve the similarity metric learning. This work was supported by National Hi-Tech Research and Development Program (863 Program) of China (2015AA015403, 2014AA015102) and National Natural Science Foundation of China (61170166, 61331011). [1] Xiaoyan Cai and Wenjie Li. Mutually reinforced [2] Hoa Trang Dang. Overview of duc 2006. In Document [ 3] Weiwei Guo and Mona Diab. Modeling sentences in the [4] Zhanying He, Chun Chen, Jiajun Bu, Can Wang, Lijun [5] Lei Huang, Yanxiang He, Furu Wei, and Wenjie Li.
 [6] Yong-Deok Kim and Seungjin Choi. Weighted nonnegative [7] Thomas K Landauer, Peter W Foltz, and Darrell Laham. An [8] Sun Park, Ju-Hong Lee, Chan-Min Ahn, Jun Sik Hong, and [9] Sun Park, Ju-Hong Lee, Deok-Hwan Kim, and Chan-Min [10] Nathan Srebro, Tommi Jaakkola, et al. Weighted low-rank [11] Jie Tang, Limin Yao, and Dewei Chen. Multi-topic based [12] Xiaojun Wan and Jianguo Xiao. Graph-based multi-modality [13] Xiaojun Wan, Jianwu Yang, and Jianguo Xiao.
 [14] Dingding Wang, Tao Li, Shenghuo Zhu, and Chris Ding.
