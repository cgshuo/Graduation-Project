 Mehmet G  X onen gonen@boun.edu.tr Ethem Alpayd X n alpaydin@boun.edu.tr Kernel-based methods such as the support vector ma-chine (SVM) gained much popularity due to their suc-cess. For classification tasks, the basic idea is to map the training instances from the input space to a feature space (generally a higher dimensional space than the input space) where they are linearly separable. The SVM discriminant function obtained after training is: where w is the weight coefficients, b is the threshold, and  X ( x ) is the mapping function to the corresponding feature space. We do not need to define the mapping function explicitly and if we plug w vector from dual formulation into (1), we obtain the discriminant: where n is the number of training instances, x i , and Each  X ( x ) function has its own characteristics and cor-responds to a different kernel function and leads to a different discriminant function in the original space. Selecting the kernel function (i.e., selecting the map-ping function) is an important step in SVM training and is generally performed using cross-validation. In recent studies (Lanckriet et al., 2004a; Sonnenburg et al., 2006), it is reported that using multiple different kernels instead of a single kernel improves the classifi-cation performance. The simplest way is to use an un-weighted sum of kernel functions (Pavlidis et al., 2001; Moguerza et al., 2004). Using an unweighted sum gives equal preference to all kernels and this may not be ideal. A better strategy is to learn a weighted sum (e.g., convex combination); this also allows extract-ing information from the weights assigned to kernels. Lanckriet et al. (2004b) formulate this as a semidef-inite programming problem which allows finding the combination weights and support vector coefficients together. Bach et al. (2004) reformulate the prob-lem and propose an efficient algorithm using sequen-tial minimal optimization (SMO). Their discriminant function can be seen as an unweighted summation of discriminant values (but a weighted summation of ker-nel functions) in different feature spaces: where m indexes kernels, w m is the weight coefficients,  X  m ( x ) is the mapping function for feature space m , and p is the number of kernels. By plugging w m de-rived from duality conditions into (2), we obtain: where the kernel weights satisfy  X  m  X  0 and P m =1  X  m = 1. The kernels we combine can be the same kernel with different hyperparameters (e.g., de-gree in polynomial kernel) or different kernels (e.g., lin-ear, polynomial, and Gaussian kernels). We can also combine kernels over different data representations or different feature subsets.
 Using a fixed combination rule (unweighted or weighted) assigns the same weight to a kernel over the whole input space. Assigning different weights to a kernel in different regions of the input space may pro-duce a better classifier. If data has underlying locali-ties, we should give higher weights to appropriate ker-nel functions (i.e., kernels which match the complexity of data distribution) for each local region. Lewis et al. (2006) propose to use a nonstationary combination method derived with a large-margin latent variable generative method. They use a log-ratio of Gaussian mixtures as the classifier. Lee et al. (2007) combine Gaussian kernels with different width parameters to capture the underlying local distributions, by forming a compositional kernel matrix from Gaussian kernels and using it to train a single classifier.
 In this paper, we introduce a localized formulation of the multiple kernel learning (MKL) problem. In Section 2, we modify the discriminant function of the MKL framework proposed by Bach et al. (2004) with a localized one and describe how to optimize the parame-ters with a two-step optimization procedure. Section 3 explains the key properties of the proposed algorithm. We then demonstrate the performance of our local-ized multiple kernel learning (LMKL) method on toy, benchmark, and bioinformatics data sets in Section 4. We conclude in Section 5. We describe the LMKL framework for binary classi-fication SVM but the derivations in this section can easily be extended to other kernel-based learning algo-rithms. We propose to rewrite the discriminant func-tion (2) of Bach et al. (2004) as follows, in order to allow local combinations of kernels: where  X  m ( x ) is the gating function which chooses fea-ture space m as a function of input x .  X  m ( x ) is de-fined up to a set of parameters which are also learned from data, as we will discuss below. By modifying the original SVM formulation with this new discriminant function, we get the following optimization problem: min w.r.t. w m , b,  X  ,  X  m ( x ) s.t. y i where C is the regularization parameter and  X  is the slack variables as usual. Note that the optimization problem in (5) is not convex due to the nonlinearity introduced in the separation constraints.
 Instead of trying to solve (5) directly, we can use a two-step alternate optimization algorithm inspired from Rakotomamonjy et al. (2007), to find the parameters of  X  m ( x ) and the discriminant function. The first step is to solve (5) with respect to w m , b , and  X  while fixing  X  m ( x ) and the second step is to update the parame-ters of  X  m ( x ) using a gradient-descent step calculated from the objective function in (5). The objective value obtained for a fixed  X  m ( x ) is an upper bound for (5) and the parameters of  X  m ( x ) are updated according to the current solution. The objective value obtained at the next iteration can not be greater than the cur-rent one due to the use of gradient-descent procedure and as iterations progress with a proper step size se-lection procedure (see Section 3.1), the objective value of (5) never increases. Note that this does not guaran-tee convergence to the global optimum and the initial parameters of  X  m ( x ) may affect the solution quality. For a fixed  X  m ( x ), we obtain the Lagrangian of the primal problem in (5) as follows: and taking the derivatives of L D with respect to the primal variables gives: From (5) and (6), the dual formulation is obtained as: where the locally combined kernel matrix is defined as:
K  X  ( x i , x j ) = This formulation corresponds to solving a canonical SVM dual problem with the kernel matrix K  X  ( x i , x j ), which should be positive semidefinite. We know that multiplying a kernel function with outputs of a non-negative function for both input instances, known as quasi-conformal transformation, gives a positive semidefinite kernel matrix (Amari &amp; Wu, 1998). So, the locally combined kernel matrix can be viewed as applying a quasi-conformal transformation to each ker-nel function and summing them to construct a com-bined kernel matrix. The only restriction is to have nonnegative  X  m ( x ) to get a positive semidefinite ker-nel matrix.
 Choosing among possible kernels can be considered as a classification problem and we assume that the re-gions of use of kernels are linearly separable. In this case, the gating model can be expressed as: where v m , v m 0 are the parameters of this gating model and the softmax guarantees nonnegativity. One can use more complex gating models for  X  m ( x ) or equiva-lently implement the gating not in the original input space but in a space defined by a basis function, which can be one or some combination of the  X  m ( x ) in which the SVM works (thereby also allowing the use of non-vectorial data). If we use a gating model which is constant (not a function of x ), our algorithm finds a fixed combination over the whole input space, similar to the original MKL formulation.
 The proposed method differs from taking subsets of the training set and training a classifier in each subset then combining them. For example, Collobert et al. (2001) define such a procedure which learns an inde-pendent SVM for each subset and reassigns instances to subsets by training a gating model with a cost func-tion. Our approach is different in that LMKL couples subset selection and combination of local classifiers in a joint optimization problem. LMKL is similar to but also different from the mixture of experts framework (Jacobs et al., 1991) in the sense that the gating model combines kernel-based experts and is learned together with experts; the difference is that in the mixture of experts, experts individually are classifiers whereas in our formulation, there is no discriminant per kernel. For a given  X  m ( x ), we can say that the objective value of (7) is equal to the objective value of (5) due to strong duality. We can safely use the objective func-tion of (7) as J (  X  ) function to calculate the gradients of the primal objective with respect to the parame-ters of  X  m ( x ). To train the gating model, we take derivatives of J (  X  ) with respect to v m , v m 0 and use gradient-descent:  X  X  (  X  )  X  X  (  X  )  X  v m where  X  k m is 1 if m = k and 0 otherwise. After updating the parameters of  X  m ( x ), we are required to solve a single kernel SVM with K  X  ( x i , x j ) at each step. The complete algorithm of LMKL with the linear gat-ing model is summarized in Algorithm 1. Convergence of the algorithm can be determined by observing the change in  X  or the parameters of  X  m ( x ).
 Algorithm 1 LMKL with the linear gating model 1: Initialize v m and v m 0 to small random numbers 2: repeat 3: Calculate K  X  ( x i , x j ) with gating model 4: Solve canonical SVM with K  X  ( x i , x j ) 7: until convergence After determining the final  X  m ( x ) and SVM solution, the resulting discriminant function is: f ( x ) = We explain the key properties and possible extensions of the proposed algorithm in this section. 3.1. Computational Complexity In each iteration, we are required to solve a canoni-cal SVM problem with the combined kernel obtained with the current gating model and to calculate the gra-dients of J (  X  ). The gradient calculation step has ig-norable time complexity compared to the SVM solver. mined with a line search method which requires addi-tional SVM optimizations for better convergence. The computational complexity of our algorithm mainly de-pends on the complexity of the canonical SVM solver used in the main loop, which can be reduced by using hot-start (i.e., giving previous  X  as input). The num-ber of iterations before convergence clearly depends on the training data and the step size selection proce-dure. The time complexity for testing is also reduced as a result of localizing. K m ( x , x i ) in (8) needs to be evaluated only if both  X  m ( x ) and  X  m ( x i ) are nonzero. 3.2. Extensions to Other Kernel-Based LMKL can also be applied to kernel-based algorithms other than binary classification SVM, such as regres-sion and one-class SVMs. We need to make two basic changes: (a) optimization problem and (b) gradient calculations from the objective value found. Other-wise, the same algorithm applies. 3.3. Knowledge Extraction The MKL framework is used to extract knowledge about the relative contributions of kernel functions used in combination. If kernel functions are evaluated over different feature subsets or data representations, the important ones have higher combination weights. With our LMKL framework, we can deduce similar in-formation based on different regions of the input space. Our proposed method also allows combining multiple copies of the same kernel to obtain localized discrim-inants, thanks to the nonlinearity introduced by the gating model. For example, we can combine linear kernels with the gating model to obtain nearly piece-wise linear boundaries. We implement the main body of our algorithm in C++ and solve the optimization problems with MOSEK op-timization software (Mosek, 2008). Our experimental methodology is as follows: Given a data set, a random one-third is reserved as the test set and the remaining two-thirds is resampled using 5  X  2 cross-validation to generate ten training and validation sets, with strat-ification. The validation sets of all folds are used to optimize C by trying values 0.01, 0.1, 1, 10, and 100. The best configuration (the one that has the highest average accuracy on the validation folds) is used to train the final SVMs on the training folds and their performance is measured over the test set. So, for each data set, we have ten test set results. We perform simulations with three commonly used kernels: linear kernel ( K L ), polynomial kernel ( K P ), and Gaussian kernel ( K G ): We use the second degree ( q = 2) polynomial ker-nel and estimate s in the Gaussian kernel as the av-erage nearest neighbor distance between instances of the training set. All kernel matrices are calculated and normalized to unit trace before training. The step size forming line search and a total of 50 iterations are performed. 4.1. Toy Data Set In order to illustrate our proposed algorithm, we create a toy data set, named Gauss4 , which consists of 1200 data instances generated from four Gaussian compo-nents (two for each class) with the following prior prob-abilities, mean vectors and covariance matrices: p 11 = 0 . 25  X  11 = p 12 = 0 . 25  X  12 = p 21 = 0 . 25  X  21 = p 22 = 0 . 25  X  22 = where data instances from the first two components are of class 1 (labeled as positive) and others are of experiments on Gauss4 data set: ( K L -K P ) and ( K L -K
L -K L ). First, we train both MKL and LMKL for ( K L -K P ) combination. Figure 1(a) shows the classification boundaries calculated and the support vectors stored by MKL which assigns combination weights 0.30 and 0.70 to K L and K P , respectively. Using the kernel matrix obtained combining K L and K P with these weights, we do not achieve a good approximation to the optimal Bayes X  boundary. As we see in Figure 1(b), LMKL divides the input space into two regions and uses the polynomial kernel to separate one component from two others quadratically and the linear kernel for the other component. We see that the locally com-bined kernel matrix obtained from K L and K P with the linear gating model learns a classification bound-ary very similar to the optimal Bayes X  boundary. Note that the softmax function in the gating model achieves a smooth transition between kernels.
 The effect of combining multiple copies of the same kernel can be seen in Figure 1(c) which shows the classification and gating model boundaries of LMKL with ( K L -K L -K L ) combination. Using linear kernels in three different regions enables us to approximate the optimal Bayes X  boundary in a piecewise linear man-ner. Instead of using complex kernels such as the Gaussian kernel, local combination of simple kernels (e.g., linear and polynomial kernels) can produce ac-curate classifiers and avoid overfitting. For example, the Gaussian kernel achieves 89.67 per cent average testing accuracy by storing all training instances as support vectors. However, LMKL with three linear kernels achieves 92.00 per cent average testing accu-racy by storing 23.18 per cent of training instances as support vectors on the average.
 Initially, we assign small random numbers to the gat-ing model parameters and this gives nearly equal com-bination weights for each kernel. This is equivalent to taking an unweighted summation of the original kernel matrices. The gating model starts to give crisp outputs as iterations progress and the locally combined kernel matrix becomes more sparse (see Figure 2). The ker-nel function values between data instances from differ-ent regions become 0 due to the multiplication of the gating model outputs. This localizing characteristics is also effective for the test instances. If the gating model gives crisp outputs for a test instance, the dis-criminant function in (8) is calculated over only the support vectors having nonzero gating model outputs for the selected kernels. Hence, discriminant function value for a data instance is mainly determined by the neighboring training instances and the active kernel function in its region. 4.2. Benchmark Data Sets We perform experiments on ten two-class benchmark data sets from the UCI machine learning repository and Statlog collection. In the result tables, we report the average testing accuracies and support vector per-centages. The average accuracies and support vector percentages are made bold if the difference between the two compared classifiers is significant using the 5  X  2 cross-validation paired F test (Alpayd X n, 1999). Figure 3(a)-(b) illustrate the difference between MKL and LMKL on Banana data set with ( K L -K P ) com-bination. We can see that MKL can not capture the localities exist in the data by combining linear and polynomial kernels with fixed combination weights (it assigns 1.00 to K P ignoring the linear kernel). How-ever, LMKL finds a more reasonable decision bound-ary using much fewer support vectors by dividing the input space into two regions using the linear gating model. The average testing accuracy increases from 70.52 to 84.46 per cent and the support vector count is halved (decreases from 82.36 to 41.28 per cent). The classification and gating model boundaries found by LMKL with ( K L -K L -K L ) combination on Banana data set can be seen in Figure 3(c). The gating model divides the input space into three regions and in each region a local and (nearly) linear decision boundary is induced. Combination of these local boundaries with softmax gating gives us a more complex boundary. The results by MKL and LMKL for ( K P -K G ) and canonical SVMs with K L , K P , K G are given in Ta-ble 1. LMKL achieves statistically similar accuracies compared with MKL on all data sets. LMKL stores significantly fewer support vectors on Heart , Pima , and Wdbc data sets. With direct comparison of av-erage values, the localized variant performs better on seven and eight out of ten data sets in terms of testing accuracy and support vector percentage, respectively. Other kernel combinations behave similarly. We also combine p = 2 , . . . , 5 linear kernels on bench-mark data sets with LMKL. Table 1 (to the right) compares the results of canonical SVM with the lin-ear kernel and LMKL with three linear kernels. LMKL uses statistically fewer support vectors on six out of ten data sets and on three of these ( Banana , Pima , and Spambase ), accuracy is significantly improved. With direct comparison of average values, LMKL performs better than canonical SVM on seven and eight out of ten data sets in terms of accuracy and support vec-tor percentages, respectively. Using localized linear kernels also improves testing time due to evaluating linear kernels over only neighboring support vectors, instead of evaluating it over all support vectors. Using Wilcoxon X  X  signed rank test on ten data sets (see Table 1), when different kernels are combined, LMKL stores significantly fewer support vectors than MKL; when multiple copies of the same (linear) kernel are combined, LMKL achieves significantly higher accu-racy than canonical SVM using a single kernel. 4.3. Bioinformatics Data Sets We perform experiments on two bioinformatics data sets in order to see the applicability of LMKL to real-life problems. These translation initiation site data sets are constructed by using the same procedure de-scribed by Pedersen and Nielsen (1997). Each data in-stance is represented by a window of 200 nucleotides. Each nucleotide is encoded by five bits and the posi-tion of the set bit indicates whether the nucleotide is A, T, G, C, or N (for unknown).
 As in benchmark data sets when combining different kernels, LMKL achieves statistically similar accuracy results compared with MKL by storing fewer support vectors for all combinations (see Table 2). For exam-ple, using ( K P -K G ), LMKL needs on the average 24.55 and 22.32 per cent fewer support vectors on Ara-bidopsis and Vertebrates data sets, respectively. We combine p = 2 , . . . , 5 linear kernels on bioinformat-ics data sets using LMKL. Table 2 shows that LMKL with three linear kernels improves the average accu-racy statistically significantly. LMKL also uses signif-icantly fewer support vectors (the decrease is almost one-third) on these data sets. This work introduces a localized multiple kernel learn-ing framework for kernel-based algorithms. The pro-posed algorithm consists of: (a) a gating model which assigns weights to kernels for a data instance, (b) a kernel-based learning algorithm with the locally com-bined kernel matrix. The training of these two com-ponents are coupled and the parameters of both com-ponents are optimized together by using a two-step alternate optimization procedure in a joint manner. For binary classification tasks, the algorithm of the proposed framework with linear gating is derived and tested on ten benchmark and two bioinformatics data sets. LMKL achieves statistically similar accuracy re-sults compared with MKL by storing fewer support vectors. Because kernels are evaluated locally (i.e., zero weighted kernels for a test instance are not calcu-lated), the whole testing process is also much faster. This framework allows using multiple copies of the same kernel in different regions of the input space, ob-taining more complex boundaries than what the un-derlying kernel is capable of. In order to illustrate this advantage, we combine different number of lin-ear kernels on all data sets and learn piecewise linear boundaries. LMKL with three linear kernels gives sig-nificantly better accuracy results than canonical SVM with linear kernel on bioinformatics data sets. This work was supported by the Turkish Academy of Sciences in the framework of the Young Scien-tist Award Program under EA-T  X  UBA-GEB  X  IP/2001-1-1, Bo  X gazi  X ci University Scientific Research Project 07HA101 and the Turkish Scientific Technical Re-search Council (T  X  UB  X  ITAK) under Grant EEEAG 107E222. The work of M. G  X onen was supported by the PhD scholarship (2211) from T  X  UB  X  ITAK. Alpayd X n, E. (1999). Combined 5  X  2 cv F test for com-paring supervised classification learning algorithms. Neural Computation , 11 , 1885 X 1892.
 Amari, S., &amp; Wu, S. (1998). Improving support vector machine classifiers by modifying kernel functions. Neural Networks , 12 , 783 X 789.
 Bach, F. R., Lanckriet, G. R. G., &amp; Jordan, M. I. (2004). Multiple kernel learning, conic duality, and the SMO algorithm. Proceedings of the 21st Interna-tional Conference on Machine Learning (pp. 41 X 48). Collobert, R., Bengio, S., &amp; Bengio, Y. (2001). A par-allel mixture of SVMs for very large scale problems.
Advances in Neural Information Processing Systems (NIPS) (pp. 633 X 640).
 Jacobs, R. A., Jordan, M. I., Nowlan, S. J., &amp; Hinton, G. E. (1991). Adaptive mixtures of local experts. Neural Computation , 3 , 79 X 87.
 Lanckriet, G. R. G., Bie, T. D., Cristianini, N., Jor-dan, M. I., &amp; Noble, W. S. (2004a). A statistical framework for genomic data fusion. Bioinformatics , 20 , 2626 X 2635.
 Lanckriet, G. R. G., Cristianini, N., Bartlett, P.,
Ghaoui, L. E., &amp; Jordan, M. I. (2004b). Learning the kernel matrix with semidefinite programming. Journal of Machine Learning Research , 5 , 27 X 72. Lee, W., Verzakov, S., &amp; Duin, R. P. W. (2007). Kernel combination versus classifier combination. Proceed-ings of the 7th International Workshop on Multiple Classifier Systems (pp. 22 X 31).
 Lewis, D. P., Jebara, T., &amp; Noble, W. S. (2006). Non-stationary kernel combination. Proceedings of the 23rd International Conference on Machine Learning (pp. 553 X 560).
 Moguerza, J. M., Mu  X noz, A., &amp; de Diego, I. M. (2004).
Improving support vector classification via the com-bination of multiple sources of information. Proceed-ings of Structural, Syntactic, and Statistical Pattern
Recognition, Joint IAPR International Workshops (pp. 592 X 600).
 Mosek (2008). The MOSEK optimization tools manual version 5.0 (revision 79) . MOSEK ApS, Denmark. Pavlidis, P., Weston, J., Cai, J., &amp; Grundy, W. N. (2001). Gene functional classification from hetero-geneous data. Proceedings of the 5th Annual In-ternational Conference on Computational Molecular Biology (pp. 242 X 248).
 Pedersen, A. G., &amp; Nielsen, H. (1997). Neural net-work prediction of translation initiation sites in eu-karyotes: Perspectives for EST and genome analysis. Proceedings of the 5th International Conference on
Intelligent Systems for Molecular Biology (pp. 226 X  233).
 Rakotomamonjy, A., Bach, F., Canu, S., &amp; Grand-valet, Y. (2007). More efficiency in multiple kernel learning. Proceedings of the 24th International Con-ference on Machine Learning (pp. 775 X 782).
 Sonnenburg, S., R  X atsch, G., Sch  X afer, C., &amp; Sch  X olkopf, B. (2006). Large scale multiple kernel learning.
Journal of Machine Learning Research , 7 , 1531 X 
