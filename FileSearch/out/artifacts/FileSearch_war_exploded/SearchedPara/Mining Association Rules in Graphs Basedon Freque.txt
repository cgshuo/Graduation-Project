 Discovering interesting patterns in graphs is a popular data mining task, with wide applications in social network analysis, bioinformatics, etc. In traditional approaches, the dataset consists of a (very large) single graph and the task is to find frequent subgraphs, i.e., reoccuring stuctures consisting of labelled nodes frequently interconnected in exactly the same way. However, the concept of fre-quent subgraphs is not flexible enough to capture all patterns. Another problem with the subgraph mining approaches is that they are also typically compu-tationally complex since they are forced to deal with the graph isomorphism problem. For small graphs, isomorphism checking is still manageable, but for large graphs such checks become computationally very expensive.
 ( FCI ) approach proposed by Hendrickx et al. [ 6 ]. A frequent cohesive itemset is defined as a set of node labels that are all, as individual items, frequent, and are, on average, tightly connected to each other, but not necessarily always connected in exactly the same way. In this paper, we introduce the concept of association rules into this setting. More formally, the aim is to generate rules of the form if X occurs, Y occurs nearby , where X  X  Y =  X  and X  X  Y is a frequent cohesive itemset. Such rules provide the end user with much more information about correlations and interactions between node labels than itemsets alone. Consider, for example, the graph given in Figure 1 , and suppose ab is a frequent cohesive itemset (for simplicity, we denote an itemset { a, b } as ab in our examples). Note that this pattern will not necessarily be discovered by subgraph mining, as nodes labelled a and b are not always connected by an edge. Based on this itemset, we can generate rule if a node labelled  X  a  X  X ccurs,anodelabelled X  b  X  occurs nearby , which is, as we can see, true for every node labelled a . However, we can also generate rule ifanodelabelled X  b  X  X ccurs,anodelabelled X  a  X  occurs nearby , which does not necessarily hold, since some nodes labelled b are relatively far from the nearest node labelled a . If we only mine frequent cohesive itemsets, this type of information would remain undiscovered.
 Given an association rule X  X  Y , we define its confidence as the average distance from an occurrence of X to the nearest occurrence of Y . Inspired by a similar approach to mining association rules in sequential data by Cule and Goethals [ 4 ], we develop an efficient algorithm to mine association rules based on frequent cohesive itemsets. The main two features of the algorithm are that it allows us to mine itemsets and association rules in parallel (rather than first find-ing all itemsets, and then all association rules, as is the case in most traditional approaches), and that we only need to compute the confidence of a simple class of rules, which can then be used to quickly evaluate all other rules. Experiments show that our algorithm finds interesting rules within acceptable runtime. Mining knowledge from graph structured data is an active research topic in data mining. A survey of the early graph-based data mining methods is given by Washio and Motada [ 16 ]. The traditional way of finding patterns in graphs is limited to searching for frequent subgraphs, i.e., reoccuring patterns within which nodes with certain labels are frequently interconnected in exactly the same way. The first attempts to find frequent subgraphs in a single graph were made by Cook and Holder [ 3 ]. Meanwhile, Motoda and Indurkhya [ 18 ] developed an approach for a setting where the dataset consists of many (typically smaller) graphs, rather than a single large one. Further attempts at subgraph mining have been made by Yan and Han [ 17 ], Nijssen and Kok [ 14 ], and Dehaspe and Toivonen [ 5 ]. Although algorithms for discovering frequent subgraphs find pat-terns that are useful in many applications, they suffer from two main drawbacks  X  costly isomorphism testing and an enormous number of generated candidates since both edges and nodes must be added to the pattern.
 algorithm for mining frequent cohesive itemsets in graphs. An itemset is defined as a set of node labels which often occur in the graph and are, on average, tightly connected to each other. By searching for itemsets rather than subgraphs, the costly isomorphism tests are avoided, and the number of candidate patterns is reduced. Khan et al. [ 12 ] proposed another itemset mining approach for graphs, where node labels are, according to given probabilities, propagated to neigh-bouring nodes. Labels are thus aggregated, and can be mined as itemsets in the resulting graph.
 et al. [ 2 ]. An association rule, denoted X  X  Y , expresses an observation that an occurrence of pattern X implies an occurrence of pattern Y . In the setting of transaction databases, the strength of a rule was measured in terms of its confi-dence which determines how frequently itemset Y appears in transactions that contain X . Since then, association rules have been applied in a variety of settings, with various definitions of the confidence measure. Cule and Goethals [ 4 ] suc-cessfully applied association rule mining on sequential data, where they defined the confidence of a rule in terms of how far on average from an occurrence of itemset X one has to look in order to find itemset Y .
 data. In this setting, Inokuchi et al. [ 9 ] proposed an Apriori-based algorithm, called AGM , for mining frequenty appearing induced subgraphs in a given graph dataset and association rules among such subgraphs. In contrast with our work, they mine rules of the form G b  X  G h where G b and G h are subgraphs, rather than itemsets. In another work, Inokuchi et al. [ 10 ] proposed an approach to mine frequent graph structures and the association rules among them embedded in massive graph structured transaction data. Here, the graph data is transformed in such a way that traditional association rule mining techniques can be applied, but the resulting rules still contain subgraph patterns. In this section, we define the concept of association rules in graph data based on frequent cohesive itemsets. For simplicity of presentation, we assume that the graph consists of labelled nodes and unlabelled egdes, and we focus on connected graphs with at most one label per node. However, we can also handle input graphs that contain nodes with multiple labels, by transforming each such node into multiple nodes each carrying one label and connecting all of them to a central dummy node. As our work is based on the Frequent Cohesive Itemset mining approach of Hendrickx et al. [ 6 ], we start with a reproduction of some of the necessary definitions and notations. We define a graph G as a set of nodes V ( G ) and a set of edges E ( G ). Each node v  X  V ( G ) carries a label l ( v ) of all labels. For a label i  X  S , we denote the set of nodes in the graph carrying this label as L ( i )= { v  X  V ( G ) | l ( v )= i } . We define the frequency of a label i  X 
S as the probability of encountering that label in G ,or freq ( i )= From now on, we will refer to labels as items, and sets of labels as itemsets. For an itemset X , the set of all nodes labelled by an item in X is denoted by N ( X )= { v  X  V ( G ) | l ( v )  X  X } . In order to compute the cohesion of an itemset X we must look, for each occurence of an item in X , for the nearest occurence of all other items in X .Foranode v , we define the sum of all these smallest of the shortest path from node v to node w . The average of such sums for all occurences of items making up itemset X is expressed as The cohesion of an itemset X , where | X | X  2, is defined as the ratio of the itemset size and the average sum of the smallest distances defined above: Note that the itemset size is reduced by one, since each considered node already carries one item in X .If | X | &lt; 2, we define C ( X ) to be equal to 1. Cohesion measures how close to each other the items making up itemset X are on average. If the items are always directly interconnected by an edge, the sum of these distances for each occurence of an item in X will be equal to as will the average of such sums, and the cohesion of X will be equal to 1. Finally, an itemset X is considered frequent cohesive if, given user defined thresholds for frequency ( min freq ) and cohesion ( min coh ), it holds that X : freq ( x )  X  min freq and C ( X )  X  min coh . Two optional parameters, minsize and maxsize , can be used to limit the size of the discovered itemsets. In this setting, our goal is to generate rules of the form if X occurs, Y occurs nearby , where X  X  Y =  X  and X  X  Y is a frequent cohesive itemset. In the rest of this paper, we will denote a rule in the traditional way as X the body of the rule and Y the head of the rule. It is clear that the closer the items of Y occur to the items of X in the graph, the higher the confidence value of the rule should be. More formally, in order to compute the confidence of the rule, we must compute the average sum of minimal distances for X from the point of view of items making up itemset X , i.e., The confidence of a rule can then be defined as Given a confidence threshold min conf , we consider rule X c ( X  X  Y )  X  min conf .
 respectively. Looking at itemset ab we first note that freq ( a )= freq ( b )= 7 21  X  0 . 33, which shows us that both a and b are frequent. Now let us have a look at the cohesion of itemset ab . According to our defintions, 10. To compute the cohesion, we now search the neighbourhood of each node in N ( ab ) which gives us W ( v 2 ,ab )= W ( v 4 ,ab )= W ( v W ( v 15 ,ab )= W ( v 18 ,ab )=1, W ( v 12 ,ab )= W ( v 21 and W ( v 13 ,ab ) = 4. Subsequently, computing the average of the above minimal distances, we get W ( ab )= 17 10 =1 . 7. We can now compute the cohesion of ab as C ( ab )= 2  X  1 1 . 7  X  0 . 58, and conclude that itemset ab is sufficiently cohesive. node labelled a has a node labelled b nearby, but not vice versa. Therefore, the confidence of rule a  X  b should be higher than that of rule b our definitions, we see that for each node v labelled a , W ( v, ab ) = 1. Therefore, W ( a, b )= 3 3 =1,and c ( a  X  b ) = 1 which means that rule a expected, a confidence of 100%. Meanwhile, the sum of distances from each b to the nearest a is v  X  N ( { b } ) W ( v, ab ) = 14. It follows that W ( b, a )= c ( b  X  a )=0 . 5. We see that a  X  b is a confident association rule, while b not, and we conclude that while an occurence of a b does not imply finding an a nearby, when we find an a , however, we can expect to find a b nearby. In this section we present a detailed description of our algorithm for discover-ing confident association rules based on frequent cohesive itemsets. Unlike the traditional approaches, which need all frequent itemsets to be found before the generation process of the association rules can begin, we will generate the rules in parallel with the frequent cohesive itemsets.
 as a basis for our algorithm. The FCI algorithm, given in Algorithm 1 ,discov-ers frequent cohesive itemsets. Candidate itemsets are generated by applying depth-first search, using recursive enumeration. During this process, a candidate consists of two elements  X  items that make up the candidate, and all frequent items which still have to be enumerated, denoted as X and Y , respectively. The algorithm uses the UBC pruning function, which computes an upper bound on the cohesion of all candidates yet to be generated in a given branch of the search tree, and decides whether to proceed deeper into the search tree, or to prune the complete branch. When a frequent cohesive itemset is found in line 3, rather Algorithm 1. FCI ( X, Y ) finds frequent cohesive itemsets than outputting it, we proceed to mine association rules that can be generated from this itemset, as discussed below.
 The most computationally costly step of the FCI algorithm is the computa-tion of v  X  N ( X ) W ( v, X ), needed to evaluate the cohesion of itemset X . Com-puting this at each node in the search tree would be unfeasible, but it has been shown that such a sum for an itemset X can be expressed as a sum of separate sums of such distances for each pair of items individually [ 6 ].
 sums between individual items in a matrix which is generated in the beginning of the algorithm. Since we are only interested in itemsets consisting of frequent items, the matrix will only contain the minimal distances between each pair of frequent items. Therefore, the matrix that will be generated will only contain |
F | X | F | sums of smallest distances, where F are the frequent items.
 On top of being used in the generation of the frequent cohesive itemsets, this matrix can help us easily compute the confidence of association rules X More formally, for each x  X  X , we can find the sum of smallest distances to each y  X 
Y seperately. This would allow us to efficiently compute W ( X, Y ), and thus c ( X  X  Y ). However, we can optimise this process further, taking advantage of the fact that it is sufficient to limit our computations to rules of the form x  X 
X \{ x } (i.e., rules where the body consists of a single item), with x which can be generated from an itemset X . To compute the confidence of all other rules, we first note that The last part of the above expression can be rewritten as which allows us to reformulate the formula for the average sum of minimal distances as This, in turn, implies that For the case where X contains just one element, i.e., X = and it follows that As a result, once we have computed the confidence of all the rules of the form x  X  X \{ x } , with x  X  X , we can evaluate all other rules Y Y  X  X , without looking at either the dataset or the distance matrix.
 the confidence of rule ab  X  c , where abc is a frequent cohesive itemset. First we will compute the confidence of rules a  X  bc and b  X  ac . We find that compute c ( ab  X  c )as 10 3 for computing the confidence of the same rule, we get c ( ab which shows us that Equations 1 and 2 are equivalent. For reasons explained above, we will use Equation 2 in our algorithm.
 Algorithm 2 . Having found a frequent cohesive itemset X , we first generate rules of the form x  X  X \{ x } , store their confidence values in memory and send all confident rules to the output (lines 1 -4). We then generate all other rules, compute their confidence based on the stored confidences computed during the first stage, and output all confident rules (lines 5 -7).
 Algorithm 2. GENERATE RULES ( X ) generates rules based on itemset X In this section, we present the experimental results of our association rule mining algorithm. For our experiments, we used two different biological graph datasets. The first dataset concerns the human interactome. In this graph, each node is a protein and each edge is a direct interaction between two proteins. This network was obtained by fusing the BioGRID [ 15 ] database with the IntAct [ 11 ]and MINT [ 13 ] networks. Next, the protein nodes were labelled with the annotations contained in InterPro [ 7 ], which concerns common domains, protein families and active sites that are present in the corresponding protein. Note that each protein can contain multiple domains and can thus have more than one annotation label. The second dataset concerns the transcriptional regulatory network of Saccharomyces cerevisiae, commonly known as baker X  X  yeast, which serves as an important model organism in life sciences. The yeast regulatory network was obtained from the YEASTRACT [ 1 ] database. Within this graph, each node is a yeast gene, and each edge corresponds to a regulatory interaction between a transcription factor gene and a target gene. Each node was labelled with gene ontology annotation information with regards to the biological process, molecular function and cellular location as obtained from UniProtKB [ 8 ]. As a result, here, too, each node can have multiple labels.
 Since both datasets consist of nodes that can carry multiple labels, we first had to transform the given graphs. More formally, we expanded the given graph structure by replacing each node with a so-called dummy node carrying a unique label, and then connecting this node to a set of new nodes, each carrying one of the original labels. Note that due to the nature of this reconstruction, any two labels in the resulting graph will be at a distance of at least 2 from each other, as there will always be a dummy node in between. As a result, the maximal cohesion for any itemset in such a graph will be 0 . 5, and the same holds for the maximal confidence for any possible association rule. For the human interactome graph dataset, the transformed graph consisted of 64 090 nodes and 141 828 edges. The yeast regulatory network was transformed into a graph consisting of 21 314 nodes and 62 991 edges.
 For all the experiments discussed below, we chose a frequency threshold that was low enough to guarantee a large number of frequent items, since we were only interested in the performance of our association rule miner. For this reason, we do not include the time needed to set up the distance matrix in our analysis, since this only needs to be done once, after which the matrix can be reused for various cohesion and confidence thresholds. The frequency threshold, minsize and maxsize were set to 0 . 002, 2 and 7, respectively, for the human interactome network, and to 0 . 001, 2 and 5, respectively, for the yeast network. In our first set of experiments, we also fixed the cohesion threshold in order to evaluate the effect of varying the confidence threshold. For the human interactome network we used min coh =0 . 2, and for the yeast network min coh =0 . 3. Figure 2 shows, for both datasets, the number of discovered rules and the runtimes needed to generate these rules for varying min conf values. As expected, the number of discovered rules grows as we lower the confidence threshold, but the runtimes stay stable, since most time is spent mining itemsets and generating candidate rules, regardless of how many rules are actually found to be confident. we set the confidence threshold to 0 . 4 for both datasets. The results are shown in Figure 3 and confirm that varying the cohesion threshold does have an effect on runtimes. Naturally, the number of rules decreases as the cohesion threshold increases, since the number of frequent cohesive itemsets on which we base the rules also decreases.
 of the confidences of all association rules that can be generated from that item-set. In our third set of experiments, we varied both the cohesion and confidence thresholds, by setting min conf = min coh , thus finding those rules that had an above average confidence within the set of rules originating from the same item-set. The results are shown in Figure 4 and are similar to those given in Figure 3 . Naturally, the number of rules now decreases faster, as the confidence threshold is raised together with the cohesion threshold. Finally, we note that in all exper-iments we managed to find a large number of confident association rules quickly and efficiently.
 the output to domain experts, it appeared that, for the human protein-protein interaction network, the majority of the discovered rules concern protein kinases. 99.4% of the rules included an annotation with the term  X  X inase X . Kinase pro-teins are enzymes that can modify other proteins by phosphorylating specific residues on the target protein. Phosphorylation results in the addition of a phos-phate group to the target protein. This is a common mechanism for many signal transduction pathways in all living cells, and thus a critical component of the protein-protein interaction network subjected to analysis here. For example, with min coh and min conf both set to 0 . 3 one example rule, existing of items which occur often in data but which form a non-trivial set, is Tyrosine-protein kinase, catalytic domain  X  Protein kinase, ATP binding site, Serine-threonine/tyrosine-protein kinase catalytic domain, SH2 domain with a confidence of 0 . 42. Proteins that contain a catalytic domain for tyrosine-protein kinase activity will phos-phorylate a tyrosine residue on their target. The phosphorylation reaction typi-cally requires ATP, and thus kinases will often co-occur with ATP-binding sites within the network. The annotation of  X  X erine-threonine/tyrosine-protein kinase catalytic domain X  is a higher level one that contains all instances of  X  X yrosine-protein kinase, catalytic domain X  and thus is redundant with this annotation. The presence of the  X  X H2 domain X  in the head of the association rule is much more interesting. While the SH2 domain is not directly related to protein kinase activity itself, it allows a protein to dock phosphorylated tyrosine residues. In the yeast dataset, with min coh and min conf set to 0 . 3, we found rule het-erocycle catabolic process  X  cellular nitrogen compound catabolic process , with a confidence of 0 . 5. Genes annotated with  X  X eterocycle catabolic process X  code for proteins that catalyse reactions for breaking down heterocyclic compounds. Heterocyclic molecules contain ring structures that consist of at least two types of atoms. The high confidence of this rule suggests that all genes involved in the degradation of heterocycles are also associated with  X  X ellular nitrogen com-pound catabolic process X  in our network. This is to be expected as almost all heterocycles in living organisms consist of carbon and nitrogen atoms. Thus any gene involved in the breakdown of heterocycles will also be associated in the breakdown of compounds that include carbon and nitrogen. Another logical rule is sulfur compound biosynthetic process  X  cellular nitrogen compound biosyn-thetic , with a confidence of 0 . 48. This association rule suggests that there is a direct link between genes involved in the synthesis of compounds containing sulfur and genes involved in the synthesis of nitrogen compounds. This can be explained by the fact that the primary carriers of sulfur in yeast are the amino acids methionine and cysteine, both of which also contain nitrogen. We conclude that our algorithm discovers both expected and interesting rules, but produces no spurious output. In this work, we presented a novel method for mining association rules among node labels in a graph dataset. We relax the structural constraint used in sub-graph mining, and discover rules that consist of sets of labels on both sides. A discovered rule X  X  Y tells us that if we encounter all the labels in X in a tightly connected form somewhere in the input graph, there is a high probabil-ity that all the labels in Y will be encountered nearby. We evaluate the rules by looking at how far from itemset X , on average, do we need to look in order to find itemset Y . This approach provides more insight into the data than merely mining itemsets or subgraphs. We developed an algorithm for mining association rules, and experimentally confirmed its efficiency and usefulness. In future work, we intend to look at the possibility of extending this work to a setting where the dataset consists of multiple graphs, rather than a single graph.

