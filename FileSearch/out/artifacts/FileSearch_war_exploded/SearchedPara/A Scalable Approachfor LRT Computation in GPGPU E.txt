
Linsey Xiaolin Pang 1 , 2 ,SanjayChawla 1 , Bernhard Scholz 1 , and Georgina Wilcox 1 With the widespread availability of GPS-equipped smartphones and mobile sensors, there has been an urgent need to perform larg e scale spatial data analysis. For example, by carrying out a geographic projection of Twitter feeds, researchers are able to narrow down  X  X otspot X  regions where a particular type of activity is attracting a disproportion-ate amount of attention. In neuroscience, high resolution MRIs facilitate the precise detection and localization of regions of the brain which may indicate mental disorder. The statistical method of choice for identifying hotspots or anomalous regions is the Likelihood Ratio Test (LRT) statistic. Informally, the LRT of a spatial region compares the likelihood of the given spatial region with its complement, and hence can be used to identify hotspot regions. In [10], it was known that the LRT value always follows a  X  2 distribution, independent of the distribution of the underlying data.

For a n  X  n spatial grid, the worst case execution time for identifying the most anomalous region is O ( cn 4 ) ,where c is the execution time of computing the LRT over a single region. As noted by Wu et al. [8], a naive implementation of LRT for a moderate 64  X  64 spatial grid may take nearly six hundred days. 1 Wu et al. [8] proposed a method which reduces the computation time to eleven days. However as noted previously in [9], this approach will not scale for larger data sets and the biggest spatial grid reported in [8] was 64  X  64 .

The nature of LRT permits the computatio n of regions independently of each other, which facilitates parallelization to some degree. However, new algorithmic techniques and parallelization strategies are required in order to fully harvest the computational power of GPGPUs. We have identified the following challenges that need to be ad-dressed for achieving a speed up of several orders of magnitude with GPGPUs:  X  Computing the LRT of a given region R requires the computation of the likelihood  X  The LRT computation on sequential computers is intractable for large spatial grids. Our strategy to overcome the challenges noted above is shown in Figure 1. First, in our framework, we distinguish between data distributions following the one-parameter exponential family (1EXP) and those that are outside of 1EXP. When the data belongs to a 1EXP distribution, we can estimate the LRT of a given region R by omitting the computation of  X  R (c.f. [3]). When the data is assumed to be generated by an arbitrary statistical distribution, we provide a solution to eliminate the bottleneck associated with the computation of  X  R : we use an  X  X pper-bounding technique X  to replace the computa-tion of  X  R with that of its constituent regions [8 ]. Second, we provide three GPGPU algorithms named as: Brute-Force GPGPU (BF-GPGPU), De Morgan GPGPU (DM-GPGPU) and Approximate GPGPU (AX-GPGPU). The BF-GPGPU and DM-GPGPU implementations provide exact solutions for 1EXP. The AX-GPGPU algorithm provides approximate solution for any underlying data distribution and it uses the BF-GPGPU or DM-GPGPU as a sub-routine. We designed our  X  X locking X  scheme for partitioning the work by dividing the spatial grid into overlapping sub-grids and mapping these re-gions onto blocks of threads [4]. The majority of the computation is performed on the GPGPUs and we utilize shared memory for each block by pre-loading the data that will be used by multiple threads.

The rest of the paper is structured as follows. In Section 2, we provide background materials on LRT computation and upper-bounding technique. Related work is given in Section 3. In Section 4, we explain how we use De Morgan X  X  law and dynamic pro-gramming to speed up the enumeration and processing of regions measurements. The description and details of three designed algorithms (i.e. BF-GPGPU, DM-GPGPU and AX-GPGPU) are presented in Section 5. In Section 6, we evaluate the algorithms on both synthetic and real data sets consistin g of Magnetic Resonance Imaging (MRI) scans from patients suffering from dementia. We give our conclusions in Section 7. 2.1 The Likelihood Ratio Test (LRT) We provide a brief but self-contained introduction for using LRT to find anomalous regions in a spatial setting. The regions are mapped onto a spatial grid. Given a data set X , an assumed model distribution f ( X,  X  ) , a null hypothesis H 0 :  X   X   X  0 and an alternate hypothesis H 1 :  X   X   X   X   X  0 , LRT is the ratio where L () is the likelihood function and  X  is a set of parameters for the distribution [8]. In a spatial setting, the null hypothesis is that the data in a region R (that is currently being tested) and its complement (denoted as  X  R ) are governed by the same parameters. Thus if a region R is anomalous then the alternate hypothesis will most likely be a better fit and the denominator of  X  will have a higher value for the maximum likelihood estimator of  X  . A remarkable fact about  X  is that under mild regularity conditions, the asymptotic distribution of  X   X  X  X  2log  X  follows a  X  2 k distribution with k degrees of freedom, where k is the number of free parameters 2 . Thus regions whose  X  value falls in the tail of the  X  2 distribution are likely to be anomalous [8].
For example, if we assume the counts m ( R ) in a region R follow a Poisson distri-bution with baseline b and intensity  X  , then a random variable x  X  P oisson (  X  X  ) is a member of 1EXP with T ( x )= x/ X  ,  X  =1 / X  , a (  X  )=  X  ,  X  = log (  X  ) , B e  X  = b c ( m R log ( m R b for Poisson distribution generalizes to the 1EXP family of distributions [3]. 2.2 The Upper-Bounding Technique The upper-bounding technique for LRT was introduced by Wu et al. [8] to reuse the likelihood computation, i.e., the likelihood of a region could be upper-bounded in terms of its sub-regions. The basic observation is that the likelihood value of any given region R under the complete parameter space is not greater than the product of the likelihood value of all its non-overlapping sub-regions under the null parameter model. For in-stance, if a region R is composed of two non-overlapping sub-regions R 1 and R 2 ,then hood estimators (MLEs). The  X  R is computed under the complete parameter space, and  X  R 1 and  X  R are computed under the null parameter space.

The upper-bounding technique can be used to prune non-outliers as follows: if we replace the likelihood of a region R by the product of the likelihoods of its sub-regions and the new LRT is below the anomalous threshold (i.e. confidence level  X  ), then R cannot be anomalous. As noted earlier, in this paper we use the upper-bounding tech-nique to speed up the computation of the likelihood of the complement of a region  X  R when the data is assumed to follow a general statistical distribution. Previous attempts to parallelize LRT computation have only achieved limited success. For example, the Spatial Scan Statistic (SSS), which is a special case of LRT for Poisson data, is available as a program under the name SatScan [2]. It has been parallelized for multi-core CPU environments and its extension for a GPGPU hardware [7] has achieved improved speed up of two over the multi-core baseline. The GPGPU implementation in [7] has proposed loading parts of the data into shared memory but has achieved only a modest speed up. The other attempt of [14] applied their own implementation of a spatial scan statistic program on the GPU to the epidemic disease dataset. The number of blocks was decided by both the number of the exploring villages and the number of diseases to take full advantage of stream multiprocessors on GPGPUs. This solution is only applicable to its special disease scenario . In each of these cases, we believe there is further room for optimising the algorithms for the GPGPU by carefully exploiting the thread and block architecture and further utilising shared memory.

Furthermore, all the existing parallel solutions perform simplified Poisson LRT tests solution is different and provides a fully paralleled template for a general LRT com-putation in a grid, based on the work of [8]. For the 1EXP family, we can estimate the LRT of a region R by just computing the statistic of the region. We do not need to separately compute the statistic of  X  R .Fora more general statistical family, we can upper-bound the likelihood of  X  R by estimating the likelihood of the four sub-regions that are anchored at the corners of the grid.
We now present a novel and unique approach, based on De Morgan X  X  law and dy-namic programming to accumulate the statistic of a region R . We will build a table in time O ( n 2 ) and then we will be able to compute the statistic of any region R in constant ( O (1) ) time.

Consider Figure 2(a) that shows a rectangular area R embedded in a grid G . Instead of counting the number of elements in R directly, we express set R as set intersections of two sets A and B asshowninFigure2(b).Set A is a rectangular region that starts in the upper left corner of the grid and ends at the lower right corner of R .Set B is a rectangular region that starts at the upper left corner of R and ends at the lower right corner of the grid. Hence, R = A  X  B . We denote the region from the lower left corner of G to the lower left corner of R by X and the region from the upper right corner of R to the upper right corner of G by Y . By applying De Morgan X  X  law and inclusion/exclusion principle we obtain following relationship: To obtain a query time of O (1) , we need to pre-compute sets A , B , X ,and Y for all possible regions in G . Since one of the corner is fixed we can pre-compute the cardinalities of these sets in tables of size O ( n 2 ) . Hence, the query of counting the and ( x 2 ,y 2 ) is the lower right corner is expressed by: To obtain the tables for A , B , X ,and Y , we employ dynamic programming. For ex-ample the table for A can be computed using the following recurrence relationship: first element and the first column and row n eed to be populated (initialized) so that all cardinalities of A can be computed. We present three GPGPU-based algorithms and provide a complete parallel solution for LRT computation. The Brute-Force GPGPU (BF-GPGPU) and De Morgan GPGPU (DM-GPGPU) algorithms provide an exact solution of LRT when data is assumed to be generated from 1EXP. For general data distributions the Approximate GPGPU (AX-GPGPU) uses the BF-GPGPU and DM-GPGPU as a subroutine to upper bound the likelihood of the complement region  X  R .

In the GPGPU memory hierarchy architect ure, global memory is large and shared memory is small. Access to shared memory is nearly two orders of magnitude faster compared to global memory. A common strategy to exploit this mismatch is to partition the data into subsets called tiles which can fit into shared memory [13,12]. We now describe our block and thread schemes which underpins the three algorithms. 5.1 Exact Solution for 1EXP 5.1.1 BF-GPGPU Smart Block and Thread Scheme: We assume that spatial grid 3 G ( n  X  n )issmall enough so that it can be loaded into GPGPU global memory but too big to be loaded into shared memory directly. The entire spatial grid G is partitioned into tiles which can be separately read into shared memory. Assume that the size of cell data is f bytes and shared memory accommodates s bytes. There are s/f cells or equivalently w  X  h sub-grids that can be stored into shared memory, where w  X  h = s/f . Each such rectangular region will be assigned to a block of threads. If the number of threads in a block is tw  X  th , which is less than the maximum number of threads allowed per block, then each thread can process ( w/tw )  X  ( h/th ) cells. We use an example with the Poisson data model to illustrate how the computation is done, but our algorithm also works with more general distributions.
In a Poisson model, each cell has a populatio n count and a success count, which takes 8 bytes. If the size of shared memory is 16KB, a maximum of 2048 cells can be loaded into the shared memory. This resembles a rectangular region of 45 x 45 cells. To use threads efficiently, the rectangular region is set to 44  X  44 cells and associated with a block. A block with coordinates (x,y) in the C UDA grid is associated to the region with ( x, y )  X  ( x +43 ,y +43) in the spatial grid. In our implementation, the maximum number of threads per block is limited by the hardware to 512 . Therefore, 22  X  22 threads can be assigned to process 44  X  44 cells for maximizing the usage of shared memory. So and ( x 2 ,y 2 ) is the lower right cell. See Figure 3a and algorithm 1.

A two-pass scheme is used to obtain the likelihood ratio of each region: (a) In the first pass, all rectangular regions of fixed sizes up to 44  X  44 are enumerated (b) The second pass enumerates all rectangular regions that do not fit into a single block 5.1.2 DM-GPGPU We now illustrate the GPGPU pre-computation step based on De Morgan X  X  Law. Rest of the computation is identical as BF-GPGPU. In our De Morgan model, four datasets are pre-computed for all possible rectangular regions. From above, we already know that the pre-computed datasets come fro m four groups of regions, each group of which shares one corner with the grid G. The computation of a n  X  n grid requires the pre-computation of 4  X  n  X  n regions. Algorithm 1 . Exact GPGPUs solution (all regions with size up to (w, h) )
To parallelize the pre-computation, we split the whole spatial grid G into equal dis-joint parts. For a grid of size n  X  n , if the shared memory can load region with maximum size of w  X  h , we create ( n/w )  X  ( n/h ) non-overlapping parts. Each such disjoint part is associated to a block. If the thread number in a block is tw  X  th , each thread can be assigned to process ( w/tw )  X  ( h/th ) regions. Each thread is only responsible for the regions that share one of the four corners of its correspondent disjoint sub-grid. The approach for pre-computing  X  R in the next section will be along similar lines. See Figure 3b.

An additional second pass is carried out to merge the results from each sub-grid to get the full pre-computed set. In our implementation, each block holds 32  X  16 cells to fully utilize the threads capability. 5.2 AX-GPGPU 5.2.1 Block and Thread Scheme When we cannot assume that data follows 1EXP, the likelihood of  X  R cannot be esti-mated from the statistic of R .Since  X  R has an irregular shape, a naive implementation will not be able to leverage the strengths of a GPGPU environment. Experiments in a CPU environment [7] have identified that the LRT computational bottleneck comes from the enumeration of the complement region for each given region.

However we can leverage the strategy proposed by Wu et al. [8] for pruning non-bounded by the sum of the likelihood of four regions. These four regions share one of the corner of the grid separately and they constitute  X  R . We use an approach similar as DM-GPGPU for  X  R computation and BF-GPGPU for R computation.

Each point (x,y) in the spatial grid is the coordinate of a block. To satisfy the tight-bound criteria and fully utilize the thread capability, our block size is defined as 32  X  16 . Therefore, 32  X  16 regions are stored into shared memory for processing R .Thereare 8568 sub-regions to be enumerated in each block. So each thread is responsible for 2 sub-regions. Figure 3b shows the block scheme in pre-computing  X  R . It is different from Figure 3a. The pre-computation of  X  R only involves the group of regions that share one of the four corners [8]. Each point ( x, y ) does not need to be associated to a block, the entire sub grid is divided into several equa l disjoint parts and each part corresponds to a block. We choose 32  X  32 regions to be block size for pre-computation of  X  R and there are ( n/ 32)  X  ( n/ 32) blocks. 5.2.2 Concurrent Execution of Different Kernels For the general distribution model we have to first compute an bounds for R ,  X  R and then use them to get an upper bound for the LRT. We can break the code up into three distinct kernels: pre compute r , pre compute  X  r and search . Since the two kernels pre compute r , pre compute  X  r are independent of each other, we can use streaming operations to run them concurrently.

After each kernel finishes execution, th e results are transferred back to the CPU where the merging is carried out for bigger regions. Instead of loading the cell values, kernel search loads the scores of pre-computed sets from the previous two kernels into shared memory. In our implementation, we can only load a sub-grid of maximum size 32  X  8 into shared memory. After the final s core of each region is obtained, one more kernel for parallel sorting is executed and the top outlier is obtained. 5.3 Processing Larger Data Grid All our algorithms assume that the entire spatial grid can be loaded into GPGPU global memory. This may not be a realistic scenario. For example, in our environment, the maximum size of the spatial grid that can be processed by a single GPGPU is 128  X  128 . We provide several solutions to overcome this constraint.
 Split the Grid into Small Sub-grids. We can split the spatial grid into several sub-grids and load each of them in a sequentia l fashion onto a GPGPU. The block and thread scheme is applied to each sub-grid and the final computation is merged on the CPU. This may involve multiple iterations of the same kernel function.
 Multiple GPGPUs Approach. We combine multiple GPGPU cards into a computa-tional node. For example, in our case, a single GPGPU implementation can handle a maximum grid size of 128  X  128 grid due to the 4GB global memory limitation. For a 256  X  256 grid, we can split it into 4 equal 128  X  128 subgridsandsendthegrids to the two GPGPUs , separately. Merging is carried out on the cpu after the results are obtained from the two GPGPUs.
 Multi-tasked Streaming. C UDA offers APIs for asynchronous memory transfer and streaming. With these capabilities it is possible to design algorithm that allows the com-putation to proceed on both CPU and GPGPUs, while memory transfer is in progress. In our implementation, we use streaming operation to support asynchronous memory transfer while the kernel is being executed. We have designed and implemented a set of experiments to answer the following ques-tions.  X  What are the performance gains of the brute-force versus the De Morgan technique  X  What are the performance gains of sequential versus parallel outlier detection?  X  What are the performance gains of sequential versus parallel pruning using our  X  How does our technique perform on real-world data sets? The experiments were conducted on an 8-core E5520 Intel server that is equipped with two GPGPU TeslaC 1060 cards supporting CUDA 4.0. Each GPGPU card has 4 GB global memory, 16 KB shared memory, 240 cores and 30 multiprocessors. The experi-ments are performed on a Poisson distribution model and a randomly generated anoma-lous region was planted for verification in synthetic data sets. To ensure correctness of the parallel approaches, the results were verified by implementing a sequential version of all the algorithms which ran on a single CPU machine. For the real data, we used MRI images of people suffering from dementia and used MRIs of normal subjects as the baseline.
 6.1 Performance on Synthetic Data 6.1.1 De Morgan vs. Brute-Force Processing on 1EXP Family Figure 4a illustrates the performance of the brute-force method versus the De Morgan method executed on a single CPU core. The x-axis represents the size of the input instances and the y-axis th e runtime in milliseconds. It is clear that the De Morgan approach consistently out performs the brute-force and for the largest grid size, the difference is almost 90 times more.

We compare the overheads of De Morgan pre-computation in a CPU vs. a GPGPU environment. The results are shown in Figure 4b. For smaller grid sizes, the CPU imple-mentation has a smaller overhead compared to the GPGPU implementation. However, as the size of the spatial grid increases, the GPGPU implementation is three times faster than its CPU counterpart. 6.1.2 Sequential vs. Parallel Outlier Detection To compare the performance of the bru te-force approach on a CPU and a GPGPU environment for outlier detection, we implemented the BF-CPU and BF-GPGPU al-gorithms. Experiments were conducted on a on single-core CPU, one GPGPU, and two GPGPUs. We have applied parallel sorting provided by the C UDA Thrust API. Figure 5a illustrates the results. The BF-CPU performs well on small data sets but is clearly outperformed by parallel GPGPU searching for larger dataset. BF-GPGPU is six times faster than BF-CPU on the grid of size 64  X  64 and around forty times faster than on a grid of size 128  X  128 . With the current memory limitation of 4 GB ,sin-gle GPGPU implementation cannot process very large data sets. For larger data sets, multiple GPGPUs become a viable option. Figure 5b depicts the running time of exact outlier detection algorithm on a single GPGPU versus two GPGPUs without perform-ing parallel sorting. The gains obtained from using two GPGPUs is around a factor of three compared to a single GPGPU. 6.1.3 Sequential Pruning vs. Parallel Pruning Figure 6a and Figure 6b depict the performance gains obtained by using exact pruning versus approximate pruning on a GPGPU and CPU implementation respectively. As shown in the figures, the pruning approach accel erates the execution. Furthermore, the parallelized pruning approach shows a vast improvement with a running time of at least 20 times faster than AX-CPU. Moreover, it is almost 400 times faster than BF-CPU. The generalized GPGPU pruning approach provides us with a suitable template for fast computing of the likelihood ratio under any statistical model. 6.2 Outlier Detection on MRI Image To illustrate a real-world application, the GPGPU implementation was tested on a MRI data set (OASIS) made available by Washington University X  X  Alzheimer X  X  Disease Re-search Center [1]. It consists of a cross-section collection of subjects including indi-viduals with early-stage Alzheimer X  X  Dis ease (AD). MRIs of a subject suffering from dementia and two normal subjects were select ed as shown in Figure 7. The resolution of the MRI data is 176  X  176  X  208 . Figure 7 shows one slice of each subject X  X  data, which is 176  X  208 . A large number of voxels are black on the first and last 20 rows, and the first 40 and the last 40 columns. The area tested has dimensions of 128  X  128 .
The application of LRT using both AX-GPGPU and BF-GPGPU identifies regions where the subjects suffering from dementia have a high likelihood ratio value for the af-fected regions in the brain. The same areas of normal subjects have low likelihood ratio values. The AX-GPGPU found the anomalous region in 24 s compared to 1 min 6 s by BF-GPGPU. The BF-CPU on the other hand required 17 min 55 s to find the anomalous region. The Likelihood Ratio Test Statistic (LRT) is the state-of-the-art method for identifying hotspots or anomalous regions in large spatial settings. To speed up the LRT computa-tion, this paper proposed three novel contributions: (i) a fast dynamic program to enu-merate all the possible regions in a spatial grid. Compared to a brute force approach, the dynamic programming method is nearly a hundred times faster. (ii) a novel way to use an upper bounding techni que, initially designed for pruning non-outliers, to accelerate the likelihood computation of a compleme nt region. (iii) a systematic way of mapping the LRT computation onto the GPGPU programming model. In concert the three con-tributions yield a speed up of nearly four hundred times compared to their sequential counterpart. Until now, existing GPGPU implementations for specialized versions of LRT such as Spatial Scan Statistic had given only modest gains. Moving the computa-tion of the LRT statistics to the GPGPU enables the use of this sophisticated method of outlier detection for larger spatial grids than ever before.
 Acknowledgment. We thank Dr. Jinman Kim at the University of Sydney for providing us with the MRI image resources and helping us analyze the data.

