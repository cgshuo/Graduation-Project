 When data are too big to reside in RAM, machine learning has two options. The first is learn from a sample, thereby potentially losing information implicit in the data as a whole. The second is to process the data out-of-core. In the latter case, data access is very expensive, and single -pass learning becomes very desirable. The Averaged n -Dependence Estimators (AnDE) family of Bayesian learning algorithms provide efficient single pass learning with accuracy competitive with the state-of-the-art in-core learning [1]. In addition, AnDE classifiers  X  have time complexity linear with respect to the number of training examples,  X  directly handle multiple class problems,  X  directly handle missing values, and  X  do not require parameter tuning.
 These features make them strong contenders for application with big data.
Previous research has shown that as n is increased, the bias of the AnDE algorithms decreases, at the cost of an increase in variance [1]. Variance tends to decrease as data quantity increases, so for big data low bias algorithms tend to have an advantage [2]. Hence, for large data, larger n is desirable. Unfortu-nately, however, large n has high time and space complexity, especially as the dimensionality of the data increases. In practice, A2DE has proven effective for moderate dimensional data.
 A number of techniques have demonstrated a capacity to lower the bias of A1DE with negligible computational cost. Subsumption Resolution (SR) [3] achieves this with a form of lazy (classification time) feature elimination. Weight-ily Averaged One-Dependence Estimators (WAODE) [4] achieves it by weighting the sub-models. While previous studies have demonstrated the independent ef-fectiveness of each of these algorithms, their interoperability has not previously been investigated. In this paper we investigate whether they are compatible and the extent to which applying both together reduces bias relative to applying each alone. Further, neither of these techniques has been studied in the context of AnDE with n greater than 1. We herein inves tigate their effectiveness when applied to A2DE, both severally and jointly. We reveal that they are indeed effective at further reducing A2DE X  X  bias with minimal additional computation.
The rest of this paper is organized as follows. We discuss related work and our proposed improvements to A2DE in section 2. We will discuss experimental results in section 3. We conclude in section 4. We seek to estimate P( y | x ), where y is a class label and x is a vector of attribute values x = x 1 ,...x m . For notational convenience we define
AnDE aims to estimate P( y | x )usingP( y | x )  X  P( y, x ) and hence normaliz-is estimated using  X  P where A n indicates the set of all size-n subsets of { 1 ,...a } and  X  ( x  X  )isa function that is 1 if the training data contains an object with the value x  X  , otherwise 0.

Note that P( x i | y,x s )=1when i  X  s . Whereas other probability estimates should be smoothed or regularized, smoothed estimates should not be used in this case, and in practice these values are not included in the calculation.
Subsumption resolution [3] is an effectiv e technique for rectifying a specific class of extreme violations of the attribute independence assumption, those hence all inaccuracies introduced into  X  P( y | x ) by this violation of the attribute independence assumption can be avoided by dropping x i from (1). For example, when the attribute values include female and pregnant only the latter should be used, when they include male and not-pregnant only the former should be used, and when they include female and not-pregnant both should be used. This requires, however, that one infer whether P( x i | y,x s )=1foreachpair of attribute values. In the current research we infer that P( x i | x j )=1 . 0if #( x j )=#( x i ,x j ) &gt; 100, where #( x j ) is the count of the number of times attribute value x j occurs in the data and #( x i ,x j ) is the count of the number of times both x i and x j occur together in the data. To prevent both attribute values being deleted if they cover exactly the same data, we delete the one with the higher index if #( x i )=#( x j ).  X  Subsumption resolution has been shown to be effective at reducing the bias of A1DE [5,3].

Another approach to reducing bias in AnDE that has been shown to be effec-tive for A1DE [6,4,7] is to weight the sub-models, modifying (1) to WAODE [4] weights A1DE, where s is a single attribute value. It sets w s to the mutual information of the attribute with the class. WAODE is effective at reducing the bias of A1DE with minimal computational overhead. We here generalize that strategy to MI-weighted AnDE, using w s =MI( S,Y ), where Y is the set of class labels and X s is the cross product of values for attributes with indices in s .

While subsumption resolution and weighting have each been shown to reduce the bias of AnDE in isolation, they have not previously been used in conjunction. To assess the effect of doing so we also evaluate MI-weighted AnDESR,  X  P 2.1 Computational overheads AnDE has training time complexity of O( t m n +1 ) and classification time com-plexity of O( km m n ) for classifying a single example, where t is the number of training examples.

Subsumption resolution requires no additional training time and at classifica-tion time requires m 2 comparisons to identify any subsumed attribute values, and hence does not increase the classification time complexity so long as n&gt; 0. In practice subsumption resolution can substantially reduce classification time by reducing the number combinations of attribute values that must be processed. MI weighted AnDE requires the calculation of the weights at training time, O( k m n ). In practice this is dominated by the training time complexity of regular AnDE and hence does not increase the effective complexity and the additional training time impost is modest. The classification time impact is negligible. The experiments are conducted in the Weka work-bench (version 3 . 5 . 7) on data sets described in table 1. Each algorithm is tested on each data set using 20 rounds of 2-fold cross validation. Probability estimates were smoothed using m-estimation [8] with m =1.

The bias-variance decomposition provides valuable insights into the compo-nents of the error of learned classifiers. Bias denotes the systematic component of error, which describes how closely th e learner is able to describe the decision surfaces for a domain. Variance describes the component of error that stems from sampling, which reflects the sensitivity of the learner to variations in the training sample [9,10]. There are a number of different bias-variance decomposition defi-nitions. In this research, we use the bias and variance definitions of [9], together with the repeated cross-validation bias-variance estimation method [10]. When two algorithms are compared, we count the number of data sets for which one al-gorithm performs better, equally well or worse than the other on a given measure. A standard binomial sign test, assuming that wins and losses are equiprobable, is applied to these records. We assess a d ifference as significant if the outcome of a two-tailed binomial sign test is less than 0.05. The base probabilities of each algorithm are estimated using m -estimation, since in our initial experiments it leads to more accurate probabilities than Laplace estimation for naive Bayes, A1DE and A2DE. The data sets are divided into four categories. First, consist-Third, medium data sets with number of instances &gt; 1000 and &lt; 10 , 000. Fourth, small data sets with number of instances &lt; 1000. The following techniques are compared:  X  NB, Standard naive Bayes with m-estimates of probabilities.  X  A1DE,  X  P AnDE ( y, x ) with n =1.  X  A1DE-S,  X  P AnDE SR ( y, x ) with n =1.  X  A1DE-W,  X  P WAnDE ( y, x ) with n =1.  X  A1DE-SW,  X  P WAnDE SR ( y, x ) with n =1.  X  A2DE,  X  P AnDE ( y, x ) with n =2.  X  A2DE-S,  X  P AnDE SR ( y, x ) with n =2.  X  A2DE-W,  X  P WAnDE ( y, x ) with n =2.  X  A2DE-SW,  X  P WAnDE SR ( y, x ) with n =2.  X  RF10, Random Forest with 10 decision trees.
 Numeric attributes are discr etized using MDL discretization [11] for all compared techniques except Random Forest. Bias , variance, 0-1 Loss and RMSE results are reported in the following sections. 3.1 Comparison of Bias and Variance The WDL bias and variance results are shown in Tables 2 and 3 respectively with significant (  X  =0 . 05) results shown in bold. We summarize the results as:  X  Both weighting and subsumption resolution reduce the bias of both A1DE  X  Jointly applying both weighting and subsumption resolution to either A1DE  X  Both weighting and subsumption resolution increase the variance of both  X  Jointly applying both weighting and subsumption resolution to either A1DE  X  Random Forest has lower bias and higher variance significantly more often The average bias and variance results are shown in figure 1. One can see that RF10 has better bias than any member of the AnDE family but worse variance. 3.2 Comparison of the Accuracy -0-1 Loss and RMSE The above results show that subsumption resolution and weighting both reduce bias at the cost of an increase in variance. These two techniques have synergistic effect. Used together they further reduce bias at cost of increased variance. If we accept that as data quantity increases, t he bias term will incr easingly dominate error, we should expect these strategie s to be most effective at decreasing error for larger data sets.
 The WDL 0-1 Loss and RMSE results are shown in Table 4 and 5 respectively. The significant (  X  =0 . 05) results are shown in bold. We summarize the results as:  X  Subsumption resolution decreases error more often than not relative to both  X  Subsumption resolution with weighting can decrease error for both measures  X  Subsumption resolution in tandem with weighting can project AnDE to be To give an indication of the magnitude of the differences in performance, the average 0-1 Loss results and RMSE results are shown in figures 2 and 3 re-spectively. It is apparent that A2DE-SW achieves lower average 0-1 loss and RMSE on all of small, medium and large datasets, although this advantage does diminish to being very slight for the largest datasets. 3.3 Analysis of Classification and Learning Time The average results of classification and learning time for all the compared tech-niques are shown in figure 4. One can see that subsumption resolution can greatly reduce A2DE X  X  classification time. While A2DE-S and A2DE-SW require only slightly less training time on average than RF10, the training time complexity of AnDE and its variants is linear with respect to data quantity while RF10 X  X  is super-linear, as shown by the difference between training times for all data and for large data. The training time advantage would substantially increase if RF10 were applied to data that were too large to maintain in RAM. A2DE and its variants require substantially more classification time than RF10, even with the decreases introduced by subsumption r esolution. However, it can be seen that the classification time of RF10 is also super-linear with respect to training set size, whereas AnDE X  X  is not. This is due to the size of the trees increasing as the data quantity increases.
 3.4 Code The code of the methods proposed in this work can be obtained from the website, https://sourceforge.net/projects/averagedndepend/ . AnDE is a strong contender for learning from big data due to its capacity to learn in a single pass through the training data, and consequent training time com-plexity that is linear with respect to the number of training examples. Weighting using mutual information and subsumption resolution have both previously been demonstrated to be computationally efficient approaches to further reducing the bias of A1DE. As low bias is desirable when learning from large data, it is im-portant to assess the extent to which each of these approaches can reduce the bias of A1DE X  X  lower bias sibling, A2DE. Further, it is important to assess the extent to which these two approaches can augment one another.

The experimental evidence is conclusive. We confirm previous findings that each technique reduces A1DE X  X  bias. We demonstrate that each technique is just as effective at reducing A2DE X  X  bias as it is at reducing A1DE X  X . We find further that there is strong synergy between the two techniques and that they operate in tandem to reduce the bias of both A1DE and A2DE more effectively than does either in isolation. As is inevitable, these gains in bias come at a cost in increased variance. This bias/varianc e trade-off can be expected to play out in different error outcomes for different types of data. In particular, for big data, where variance can be expected to be lo w, low bias can be expected to result in low error [2]. Our experiments demonst rate that this expectation is born out in practice, with both weighting and subsumption resolution reducing error on the largest datasets significantly more often than not relative to standard A2DE and with the two in tandem significantly often further reducing the error relative to MI-weighting alone, and often, but not significantly so, further reducing the error of subsumption resolution alone.
We compared A2DE with MI-weighting and subsumption resolution against the state-of-the-art in-core learning algorithm Random Forest. Random Forest is a lower bias algorithm. However, that bias advantage comes with a considerable variance disadvantage. Even for datasets with 10,000+ training examples Random Forest achieved lower error slightly less often than higher relative to A2DE-SW.
Using only single-pass learning, A2DE with MI-weighting and subsumption res-olution achieves accuracy that is very comp etitive with the state-of-the-art in in-core learning, making it a desirable algorithm for learning from very large data. Acknowledgment. This research has been supported by the Australian Re-search Council under gra nt DP110101427 and Asian Offi ce of Aerospace Re-search and Development, Air Force Office of Scientific Research under contract FA23861214030.

