 We introduce interactive intent modeling, where the user directs ex-ploratory search by providing feedback for estimates of search in-tents. The estimated intents are visualized for interaction on an In-tent Radar , a novel visual interface that organizes intents onto a ra-dial layout where relevant intents are close to the center of the visu-alization and similar intents have similar angles. The user can give feedback on the visualized intents, from which the system learns and visualizes improved intent estimates. We systematically evalu-ated the effect of the interactive intent modeling in a mixed-method task-based information seeking setting with 30 users, where we compared two interface variants for interactive intent modeling, namely intent radar and a simpler list-based interface, to a con-ventional search system. The results show that interactive intent modeling significantly improves users X  task performance and the quality of retrieved information.
 H.3.3. [ Information Search and Retrieval ]: Miscellaneous Exploratory Search; Intent Modeling; Search User Interfaces tional and the corresponding search behavior is fragmented to in-dividual queries corresponding to evolving information needs [5]. One of the main problems in exploratory search is that it can be hard, if not impossible, for users to formulate queries precisely, since information needs evolve throughout the search session as users gain more information [11]. In a commonly observed ex-ploratory search strategy, the information seeker issues a quick, imprecise query, hoping to get into approximately the right part *Equal contributions of the information space, and then directs the search to obtain the information of interest around the initial entry-point in the infor-mation space [2, 10]. Despite existing evidence on such behavior of the users [5], current methods to support users to explore are ei-ther based on typed queries, suggesting terms or rephrased queries [8], facets [13], result visualization and navigation through clusters [6], or they rely on relevance feedback mechanisms proven to be tedious to use [7]; or emphasize narrowing down the search within the initial query scope [13].

Existing techniques are effective for tasks where the user X  X  goal is well defined and success is measured based on system response to well formed queries [6, 13]. In exploratory search the user X  X  information needs evolve throughout the course of the search and her ability to direct the search to solve her task is critical [4, 9].
We introduce interactive intent modeling that lets users direct exploration via rapid relevance feedback in an interactive model-based loop where the user X  X  search intents are estimated and vi-sualized for interaction. The user iteratively adjusts the model by relevance feedback on keywords representing the current search in-tent. In the interface, keywords representing estimated search in-tents are arranged onto an Intent Radar , as a radial layout where relevant intents are close and similar intents have similar angles.
To evaluate the effect of interactive intent modeling on ex-ploratory search we conducted a mixed X  X ethod task X  X ased user experiment with 30 users performing a scientific information seek-ing task. Two interface variants, Intent Radar and a simpler list-based interface, were compared to a conventional typed-query sys-tem that did not support interactive intent modeling. The results show that interactive intent modeling improves the quality of re-trieved information, the ability of users to target interactions to di-rect exploratory search, and the task performance of the users. We illustrate interactive intent modeling and the novel Intent Radar visualization by a walk-through example of an information seeking task (left side of Figure 1). Imagine the user issues a query  X  X achine vision X ; the system responds with the predicted user in-tent and projected potential future intents along with a list of docu-ments.

User interface. Besides a typical query box and article list, the interface uses a novel Intent Radar visualization, which represents search intents as relevant keywords corresponding to the predicted intents. The center of the Intent Radar represents the user. The inner gray circle represents the current search intent. The outer grey area represents future intent projections: potential directions the user may like to follow given the current search intent estimate. The radius of keywords represents their relevance: the closer a key-word is to the center the more relevant it is for the current estimated search intent. Angles of keywords represent their similarity: similar angles indicate similar intents. The interface colors keywords based on a clustering to distinguish topically different search intents from each other. Keywords with highest relevance in each cluster are shown with labels to characterize the cluster, other keywords are shown as dots that can be enlarged with a fisheye lens.

We use a polar coordinate system and radial layout. This lets the visualization focus on the relation between the intents which is more important than their exact weights. It also allows users to select directions through a non-intrusive relevance feedback mech-anism, where the user pulls keywords closer to the center of the radar. The radial layout has a good tradeoff between the amount of shown information and comprehensibility: a simple list of key-words only uses one degree of freedom and does not show keyword relationships, whereas higher than two-dimensional visualizations could make interaction with the visualization more difficult [3].
Interaction and feedback. The user can provide relevance feed-back for the intents by dragging a keyword on the Intent Radar (closer to center means higher relevance) or by clicking a keyword under a document (assigns full relevance). Negative relevance feed-back is possible by dragging a keyword outside the radar.
In the first iteration no user feedback is available, and docu-ments and keywords are selected based on pseudo-feedback ac-quired from the top-ranked documents and visualized for the user. The user browses the visualization, in our example notices key-words "infrared" and "cameras", drags them towards the center of the radar, and clicks the center to retrieve new estimates of intent and documents. Then the system computes and visualizes new es-timates for the user X  X  current and potential future intents.
We use the language modeling approach of information retrieval to estimate the relevance ranking of documents d j given the esti-mate of the user X  X  search intent. The intent model yields a keyword weight vector  X  v having a weight  X  v i for each keyword k back is not available on the first iteration, we start with the typed query with weight 1 as the intent model. Documents are ranked by their probability given the intent model. We use a probabilistic multinomial unigram language model. The  X  v is treated as a (small) sample of a desired document, and documents d j are ranked by the probability that  X  v would be observed as a random sample from the language model M d j for the document; with maximum likelihood estimation we get  X  P (  X  v | M d j ) = Q |  X v | i =1  X  v avoid zero probabilities and improve the estimation we then com-pute a smoothed estimate by Bayesian Dirichlet smoothing so that  X  P keyword k in document d j , p ( k i | C ) is the occurrence probability (proportion) of keyword k i in the whole document collection, and the parameter  X  is set to 2000 as suggested in the literature [14].
The documents d j are ranked by  X  j =  X  P (  X  v | M d j ) . We could just show the top ranked documents, but to expose the user to more novel documents, we sample a set of documents from the list and display them in ranked order. This favors documents whose key-words often received positive user feedback. We use Dirichlet Sam-pling, where a value f j  X  Gamma (  X  j , 1) = f  X  j  X  1 j is sampled for each document d j , and the documents with high-est f j are shown to the user. At each iteration, the weight  X  increased by 1 for documents d j where at least one keyword got positive user feedback, and the weights are then renormalized.
Our model uses two main representations: the current estimate of search intent , and the alternative future intents that could occur in response to future feedback of the user; they are visualized in the inner and outer circle in Figure 1.We represent the current esti-mated search intent as a relevance vector  X  r current over keywords, and the alternative future intents as a set of the same kind of rele-vance vectors  X  r f uture,l predicted into the future, called the future relevance vectors . Each vector  X  r future,l , l = 1 , . . . , L , is a projec-tion of the current search intent into the future in response to a set of L feedback operations the user could potentially use.
The user provides relevance feedback to search intents by giving relevance scores r i  X  [0 , 1] to a subset of J keywords k 1 , . . . , J . Here r i = 1 denotes keyword k i is highly relevant to the user and she would like to direct her search in that direction, and r = 0 denotes the keyword is of no interest to the user.

Estimating keyword relevances. Let each keyword k i be repre-sented as a binary n  X  1 vector k i telling which of the n documents the keyword appeared in. To boost significance of documents with rare keywords, we convert the k i into the tf-idf representation.
We assume the relevance score r i of a keyword k i is a random variable with expected value E [ r i ] = k  X  i w . The unknown weight vector w determines the relevance of keywords and it is estimated based on the relevance feedback given so far in the search session.
Estimating the weight vector. The algorithm maintains an es-timate  X  w of the vector w which maps keyword features to rele-vance scores. To estimate w for a given search iteration, we use the LinRel algorithm [1]. In each search iteration, LinRel yields an estimate  X  w . Let K be a matrix where each row k  X  i is a feature representation of one of the keywords k i shown so far, and let the column vector r feedback = [ r 1 , r 2 , . . . , r p ]  X  contain the p rele-vance scores received so far from the user. LinRel estimates  X  w by solving the linear regression r feedback = Kw , and calculates an estimated relevance score  X  r i = k  X  i  X  w for each keyword k
Selecting keywords for presentation to the user. At each it-eration the system might simply pick the keywords with highest estimated relevance scores, but if  X  w is based on a small set of feed-back, this exploitative choice could be suboptimal; or the system could exploratively pick keywords where feedback would improve accuracy of  X  w . To deal with the exploration-exploitation tradeoff we select keywords not with the highest relevance score, but with the largest upper confidence bound for the score. If  X  i is an up-per bound on standard deviation of the relevance estimate  X  r upper confidence bound of keyword k i is computed as  X  r i where  X  &gt; 0 is a constant used to adjust the confidence level of the bound. Let r feedback again denote the vector of all relevance scores received from the user. In each iteration, LinRel computes s = K ( K  X  K +  X  I )  X  1 k i where  X  is a regularization parameter, and the keywords k i that maximize s  X  i r feedback +  X  2 lected for presentation; they represent the estimated current search intent and are visualised in the inner grey circle of the Intent Radar visualization (Figure 1). We use LinRel since it allows, at the same time, to maximize relevance of intent estimates based on user in-teractions and reduce system uncertainty about the relevant intents that occurs because of limited and possibly suboptimal feedback.
Estimating alternative future intents. Our approach not only estimates user X  X  current intents, but also suggests potential search directions to the user. At each iteration, based on the current esti-mated search intent (relevance vector  X  r current over keywords), the system estimates a set of alternative future search intents (future estimates of the relevance vector). The future search intent is es-timated for each of L alternative feedbacks l = 1 , . . . , L ; in each feedback l , a pseudo-relevance feedback of 1 is given to the l th keyword in the search intent visualization, the feedback is added to the feedback from previous search iterations, and LinRel is used to estimate the future relevance vector  X  r future,l for keywords.
Each  X  r future,l provides the user a set of keywords she would most likely be shown, if she decided to give positive feedback to the l th currently shown keyword. Thus the user gets a view of L potential search directions which can be explored in more detail.
Denote the current estimated search intent as  X  r current [ X  r evance of the l th keyword. Future intents are estimated as the N keywords  X  L matrix  X  R future , where the element in row i , col-umn l , is  X  r future,l i  X  [0 , 1] , predicted relevance of the i th keyword in the next search iteration according to the l th future intent.
We optimize a data-driven layout for the search intent and alter-native future intents on the Intent Radar interface. We optimize lo-cations of keywords in the inner circle (representing current intent) and keywords in the outer circle (representing future intents) by probabilistic modeling-based nonlinear dimensionality reduction.
Representation of the outer keywords. We lay out the future potentially relevant keywords into the outer circle, based on their potential future relevances. Consider the matrix  X  R future dicted future keyword relevances across a set of future search in-tents as discussed in Section 2.2. Each keyword k i in the outer circle can be characterized by row i of  X  R future , that is, by the row the estimated relevance of k i in the l th future search intent.
The norm ||  X  r i || represents overall predicted relevance of key-word k i across future search intents; we use it as the radius of k on the radar. The vector  X  r i =  X  r i / ||  X  r i || then tells which future search intents make k i most relevant, that is, which direction of fu-ture intent k i is associated with. We use a radial layout in which keywords associated with similar future intents have similar angles.
Layout of keywords in the outer circle. Keywords k i and k in the outer circle can be called neighbors if their characterizations  X  r ,  X  r j are similar: the keywords most similar to k i can be described as a probabilistic neighbor distribution p i = { p ( j | i ) } where p ( j | i ) = exp(  X  X |  X  r i  X   X  r j || 2 / X  2 i ) ( X and the  X  i are set as in [12]. On the display k i and k j in the outer circle if they have close-by directions (angles) a a ; the keywords that appear most similar to k i in the outer circle can then be described by neighbor distribution q i = { q ( j | i ) } where q ( j | i ) = exp(  X  X  a i  X  a j | 2 / X  2 i ) X The task of the layout algorithm is to place keywords so that neigh-boring keywords on the display have neighboring characterizations. To do so, we measure the total Kullback-Leibler divergence D between the neighborhoods of display locations versus characteri-zations, as ( P s D KL ( p i , q i ) + P s D KL ( q i , p vergence is a function of the angles a i of the keywords in the outer circle; we optimize the a i by gradient descent to minimize the to-tal divergence. A similar approach was used to visualize fixed data sets in [12]. This layout approach can be shown to correspond to optimizing information retrieval of neighboring keywords from the display layout (minimizing misses and false positives of such re-trieval).

Highlighting of keywords in the outer circle. To highlight the structure in the outer circle layout, we apply a simple agglomera-tive clustering to angles a i of keywords in the outer circle. In detail, start a cluster from the keyword with the smallest angle, and itera-tively add the keyword with the next largest angle into the cluster as long as the angle difference is below a treshold and the size of the cluster is smaller than a specified percentage of all keywords in the outer circle; when either condition fails start the next cluster. We show clusters with different colors, and show for each cluster the label of the predicted most relevant keyword (having largest ||  X  r
Layout of the keywords in the inner circle. T he keywords in the inner circle represent the current search intent; for each such keyword k l , its radius naturally represents its current estimated rel-evance  X  r l  X  [0 , 1] . The angles a l of keywords in the inner circle must be placed consistently with the layout of the outer circle (the keywords of future search intents): since we estimate the alterna-tive future search intents in response to an interaction with an inner keyword k l , a l should represent which future keywords become most relevant in the l th future search intent. We thus set a highest weighted mode of angles a i of future keywords k i the angle of each future keyword is weighted by the predicted fu-ture relevance  X  r future,l i . The resulting angle a l of each keyword k in the inner circle indicates which keywords would become rele-vant by interacting with k l : thus the angles of keywords in the inner circle indicate directions of future search intent.
A task-based user experiment was designed to investigate the effects of interactive intent modeling on exploratory search. The advantage of a task-based setting is that it allows us to measure natural user interaction and task performance, but still retain the advantages of a controlled experiment. We setup the experiments to answer the following research questions: 1. User task performance: Does the interaction paradigm lead to better user responses in the given tasks? 2. Quality of displayed information: Does the paradigm help users reach high quality in-formation in response to interactions? 3. Interaction support for directing exploration: Does the paradigm elicit more interaction from the user? Is the elicited interaction targeted to relevant in-teraction options? Does the paradigm let the user explore novel information more than a conventional system where users might be constrained by limited interaction capabilities?
We chose a 2  X  3  X  5 between-subjects design with two search tasks, three system setups and five users for each task/system com-bination. We chose the design to avoid learning effects of users as each user only used one of the systems and performed a single task.
Three systems were created: two versions of our interactive in-tent modeling with different extents of intent prediction and visual-ization, denoted as  X  X ntent Radar X  and  X  X ntent List X , and a conven-tional typed-query based system  X  X yped Query X .
 The two systems with interactive intent modeling are as follows. Intent Radar implements the full versions of interactive intent mod-eling with future intent prediction and Intent Radar visualization as described in previous sections. The implemented system updated search results and the interface in response to interactions under three seconds. Intent List implements only intent estimation and has a simpler interface that visualizes the intent model for the user as a list. Figure 1 (bottom right) shows a screen shot of this inter-face. The users interact with the system by typing queries and pro-viding binary relevance feedback on keywords shown under each document, as well as on keywords in the list.

The Typed Query system is a query-based system, where neither intent modeling nor visualization are used. Users express their in-formation needs only by typing queries. Keywords are visualized underneath the articles; users can use them as cues for new typed queries, but cannot directly interact with them.
We chose a task type that is complex enough to ensure that some interaction is necessary for users to acquire the information to ac-complish the task; is complex enough to allow users to choose the kind of interaction that best supports solving the task; and is com-plex enough to reveal exploratory search behavior. The tasks were defined as scientific writing scenarios, i.e., participants were asked to prepare materials to write an essay on a given topic. The assign-ments were (1) to search for relevant articles that they would be likely to use as reference source in their essay and (2) to answer a set of predefined questions related to the task topic.

We recruited two post-doctoral researchers to define two infor-mation seeking tasks. The task fields chosen by the experts were  X  X emantic search X  and  X  X obotics X . The experts wrote task descrip-tions using this template:  X  X magine that you are writing a scientific essay on the topic. Search scientific documents that you find useful for this essay X . To provide clear goals for exploration, the experts provided questions about specific aspects of the topic. The ques-tions defined by the experts for the robotics tasks were:  X  X hat are the sub-fields, application areas and algorithms commonly used in the field of robotics X ; for the semantic search task the questions were:  X  X hat are the techniques used to acquire semantics, meth-ods used in practical implementation, organization of results, and the role of Semantic Web technologies in semantic search X .
We recruited 30 students from two universities to participate in the study. All the participants were graduate students with a back-ground in computer science or a related field. In a prior background survey we ensured that every participant had conducted literature search before and was neither an expert nor a novice in the topic of the assigned search task (self-assessment on a scale from 1 to 5; we selected people who rated themselves between 2 and 4).

The basic protocol for each experiment scenario was the follow-ing: demonstration of the system (10 min) and performing of the search task by the participant (30 min). The experiments were per-formed in an office-like environment using standard equipment The demonstration of the system was done by the instructor using a sep-arate computer. All user interactions were logged with timestamps: typed queries, the documents and keywords presented by the sys-tem in response to interactions, the keywords the user interacted with, and the articles the user bookmarked.
We used a dataset of over 50 million scientific documents from the Web of Science prepared by THOMSON REUTERS, Inc., and from the Digital Libraries of the Association of Computing Ma-chinery (ACM), the Institute of Electrical and Electronics Engi-neers (IEEE), and Springer. The dataset contains the following in-formation about each document: title, abstract, keywords, author names, publication year and publication forum.
Experts conducted two types of double-blind relevance assess-ments. For the quality of information displayed , all documents and keywords that were presented to the participants by any of the three systems were pooled resulting in a collection of 5612 documents and 4097 keywords. The experts assessed the articles on binary scale on three levels: (1) relevance X  X s this article relevant to the search topic; (2) obviousness X  X s this a well-known overview arti-cle in a given research area; and (3) novelty X  X s this article an un-common yet relevant to a given topic or specific subtopic in a given research area. These assessments constituted the ground truth for evaluating retrieval performance of the systems. The ground truth consisted of 3384 relevant documents (731 were obvious and 2653 were novel). Experts also assessed the keywords on three levels: (1) relevance X  X s this keyword relevant for the topic; (2) general X  does this keyword describe a relevant subfield, (3) specific X  X  oes this keyword describe a relevant specifier for the subfield? The Co-hen Kappa test indicated substantial agreement between experts, Kappa = 0 . 71 , p &lt; 0 . 001 . For the quality of responses of the users to the tasks , for each question answers of all participants were pooled and assessed by experts on a 5-point Likert scale.
User task performance was the main measure of success. It was measured using an averaged score of expert assessments of the participants X  written answers in response to the tasks. The given written answers were evaluated by the same experts who wrote the task descriptions and conducted the article assessments. The experts scored each answer between 0 (no answer) and 5 (perfect answer). In addition, we measured the number of bookmarked rel-evant, obvious, and novel documents the users were displayed in response to their interactions while completing the tasks.
Quality of displayed information was measured by precision, recall, and F-measure. The measures were computed both for the documents displayed for the user, and for the keywords the user interacted with. These characterize the quality of document users were able to reach and the quality of keywords users chose to ma-nipulate. The measures were computed with respect to the different assessment categories, so that for the documents we considered in turn either the relevant, or the obvious, or the novel documents as the ground truth; for the keywords we similarly took the relevant, general, and specific keywords in turn as the ground truth.
Interaction support for directing exploration was measured using two separate types of measures. First, we measured the num-ber and type of interactions (typed query or interaction with the intent model). Second, we measured the type of information (novel or obvious) received in response to different types of interactions. These measures characterize how well a particular type of interac-tion was able to support each user to direct the search to relevant information, and in particular characterize the differences of the interaction types in finding obvious and novel information.
The results are summarized in Figure 2 and discussed in detail in the following sections corresponding to the evaluation aspects. The main result of the experiments is that the users of the Intent Radar system achieve significantly better task performance than the users in the Intent List and the Typed Query systems. For Intent Radar users X  responses to the tasks are graded to be significantly better by experts than the responses of the users of the other systems as shown in Figure 2 (Task performance). The results are statisti-cally significant (Friedman test with post-hoc analysis, p &lt; 0 . 05 for Intent Radar vs. Typed Query, p &lt; 0 . 05 for Intent Radar vs. Intent List). Note that, all participants were able to accomplish the tasks and completed the task in the given timeframe (no significant time differences between the systems or tasks).
Figure 2 (Quality of displayed information) shows the quality of displayed articles and the quality of keywords users interacted with. The two versions of the interactive intent modeling achieve substantially better performance than the Typed Query compari-son system. The differences are statistically significant using the non-parametric McNemar X  X  test for categorical data with Bonfer-roni correction to correct for the multiple comparisons ( p &lt; 0 . 001 ).
The Intent List shows slightly better performance for obvious documents. A possible explanation is that the less advanced inter-action capabilities in the Intent List interface, and even more lim-ited in the Typed Query comparison system, make it more difficult to move away from the initial query context, thus failing to increase recall but preserving slightly better precision.

The quality of the keywords the users interacted with is signifi-cantly better (higher F-measure) for the Intent Radar interface than for the Intent List interface, for all relevant keywords and for both subcategories (general and specific keywords). This indicates that the Intent Radar interface has made it easier to target interactions to more relevant keywords. Moreover, the significantly higher quality of the displayed keywords themselves can add to the users X  under-standing of the information seeking task and is an explanation for the increased task performance for users of Intent Radar.
Figure 2 (Interaction support for exploration) shows that users adopt and make use of interactive intent modeling when offered to them. In particular, users interacted with the Intent Radar inter-face twice as much as with the Intent List and nearly four times more than the Typed Query. Typed queries were used equally in each interface, and the intent models were interacted with in cycles in which typed keywords were first issued and then intent models were used to direct the search. This indicates that users did not replace the typed queries with interaction with the intent models, but rather directed their search from the initially issued imprecise query. The users of the Typed Query system had trouble reach-ing novel information. A possible explanation is that coming up with queries was difficult for users of the Typed Query system as intent models were not available. This was the case even though they could see the keywords under each document returned by the system and could use them as cues for typed queries. As noted in Figure 2 (Quality of displayed information), the keywords users in-teracted with were highly relevant (high precision in the Relevant category), for both Intent List and Intent Radar; thus the elicited interaction with the intent models and the further increased interac-tion in Intent Radar were targeted to relevant interaction options.
Interestingly, the interactive intent modeling engages users to move more rapidly in the information space. Users in the Intent Radar and in the Intent List conditions chose to use typed queries as a shortcut to a previous view; this is seen in the fact that users repeat typed queries more with the Intent Radar interface (14% queries were repeats) and the Intent List interface (20%) than with Typed Query (4%). Users of the Intent Radar condition repeated fewer queries than the users of the simpler version, perhaps because the full interface already allows efficient movement through the visu-alized current and future search intents.

An important aspect of the interaction support is also whether the interaction with the predicted intents made it possible for the users to direct the search and to reach more novel information. The results in Figure 2 (Interaction support for exploration) show that users were successful in directing their search with interactive in-tent modeling. After directing the search via the predicted intents, users were displayed a significantly larger portion of novel docu-ments than after typing queries. Conversely, the users were dis-played a larger portion of obvious documents in response to typed queries. This suggests that the interaction with the intent model en-ables users to direct their search and find novel documents that are not found using the typed queries, but at the same time achieve more relevant information than conventional search systems. A similar effect is also present in the documents users bookmarked. Users bookmarked more novel documents from the results that they support for exploration, and Task performance. received in response to interactions with the intent models, while users bookmarked more obvious documents from the results they obtained using typed queries.

Overall the results suggest that interactive intent modeling, in particular the Intent Radar interface, which complements future intent prediction with appropriate visualization, allowed users to reach the novel documents that were harder to find with the Typed Query system.
In this paper we introduced interactive intent modeling for di-recting exploratory search and demonstrated its usefulness in task-based user experiments. Our results show that interactive intent modeling, in which visualization is used to allow uses to engage with directing their search from initial expressions of their infor-mation needs, can significantly improve users X  performance in ex-ploratory search tasks. The improvements can be attributed to im-proved quality of displayed information in response to user interac-tions, better targeted interaction between the user and the system, and improved support for directing search to achieve novel infor-mation. Interaction with intent visualization does not replace the query-typing interaction, but offers an additional complementary way to express more specific intents to direct search towards novel, but still relevant information. The improved quality of informa-tion, in particular when displayed on the Intent Radar interface, also transfers to improved task performance. Our findings suggest that interactive intent modeling can significantly improve the effec-tiveness of exploratory search.
This work has been partly supported by the Academy of Finland (Multivire and the COIN Center of Excellence) and TEKES (D2I). Certain data included herein are derived from the Web of Science prepared by THOMSON REUTERS, Inc., Philadelphia, Pennsyl-vania, USA: Copyright THOMSON REUTERS, 2011. All rights reserved. Data is also included from the Digital Libraries of the ACM, IEEE, and Springer.
