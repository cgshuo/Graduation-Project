 We present a new unsupervised topic discovery model for a collection of text documents. In contrast to the major-ity of the state-of-the-art topic models, our model does not break the document X  X  structure such as paragraphs and sen-tences. In addition, it preserves word order in the document. As a result, it can generate two levels of topics of differ-ent granularity, namely, segment-topics and word-topics. In addition, it can generate n-gram words in each topic. We also develop an approximate inference scheme using Gibbs sampling method. We conduct extensive experiments using publicly available data from different collections and show that our model improves the quality of several text mining tasks such as the ability to support fine grained topics with n-gram words in the correlation graph, the ability to seg-ment a document into topically coherent sections, document classification, and document likelihood estimation. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval -Clustering Topic Modeling, Topic Segmentation, N-gram words, Gibbs Sampling, Document Classification
Simplicity may not always lead to greatness! Topic mod-els such as Latent Dirichlet Allocation ( LDA ) [5] have been  X  a document into topically coherent sections, document clas-sification, and document modeling.
Topic models: Some previous works have considered the order of the words in a document during the topic discov-ery process. For example, in [35], the author described the Bigram Topic Model ( BTM ) which incorporates the hierarchi-cal Dirichlet language model into the unigram based topic model in order to capture the dependencies between the words in sequence. One drawback of this model is that it only generates bigram words in a topic. This limitation was addressed in the LDA Collocation model ( LDACOL ) [14] which introduces a new set of status variables in the model called the bigram status variable. This variable indicates whether two consecutive words form a bigram or not. One limita-tion of the LDACOL model is that it cannot decide whether to form a unigram or a bigram for the same two consec-utive words depending on their nearby context. Another issue with the model is that only the first term in a bigram has a topic assignment. One needs to make some assump-tions in order to give the topic assignment to every term in a bigram [38], and [37]. The limitations inherent in the LDACOL model have been addressed in the Topical N-gram model, TNG [39]. The TNG model allows for consecutive words in a topic model to depend on each other in that they can be selected either to come from a unigram term distribu-tion or from a bigram distribution. However, one limitation of the Topical N-gram model is that it cannot segment a document into topically coherent units, known as topic seg-mentation task. There are also other limitations and they have been addressed recently in [23] where the authors gave the same topic assignment to every term in a phrase and the words share the same probability mass in the phrase by in-troducing the hierarchical Pitman-Yor processes ( HPYP ) [34] in their model named PDLDA .The PDLDA model also cannot perform topic segmentation. Incorporating the HPYP model in our NTSeg model to capture phrasal terms would make our model NTSeg overly complex leading to inefficient processing of large text corpora.

Recently in [26] the authors proposed a topic model where n-gram words are viewed as random variables. The authors manually generated n-gram words from the dataset and con-sidered those n-grams as part of the vocabulary. This model focuses on handling discussions or debates. In [19], the au-thor proposed another topic n-gram model which mines sen-timents from text corpora. Wang et al. [36] proposed an n-gram topic model for academic retrieval. They apply an online inference algorithm and find unigrams and bigrams in a topic. In [42], the authors presented an n-gram based news thread extraction model that uses the TNG model with a background distribution. A method which has adopted a different approach to n-gram topic modeling is [20]. It com-bines the paradigms of frequent pattern mining and topic modeling. All the topic models proposed above do not em-ploy topic segmentation. In [41], the authors proposed an Auto Topic Number LDA ( ATNLDA ) model for topic segmen-tation and apply the model on stem cell research litera-ture. This model can automatically calculate the optimal topic number. A difference between ATNLDA and ours is that ATNLDA does not consider the word order.
 Topic Correlations: Shafiei et al. in [31] described a latent Dirichlet co-clustering method, known as LDCC , which cap-the aspect model in [17]. Recently, in [30], the authors pre-sented TopicTiling based on LDA . Their algorithm is very similar to the TextTiling algorithm, and segments docu-ments using the LDA topic model. Also, in [29], the authors presented methods in which topic models can help segmen-tation based methods by extending their own TopicTiling model. Similarly, in [32] the authors proposed a topic seg-mentation based topic model, known as LDSEG ,wherethey assumed that the word order is not important. The au-thors introduced the notion of topic hierarchy where sen-tences are assigned to the document-topics and unigrams are assigned to the word-topics. The LDSEG model repre-sents documents as a distribution over document-topics or super-topics in such a way that each segment is assigned a super-topic or document-topic, which is then used to choose the parameters of a document independent Dirichlet dis-tribution from which the word-topics for the segment is drawn. In order for consecutive segments to have similar word-topic distributions, an additional binary variable per segment encodes whether the document-topic is forced to be the same as that of the previous segment. In [9], the authors proposed a topic model based hierarchical segmen-tation approach where they assumed that the word order within the segment is not important and apply variational Bayesian Expectation-Maximization procedure for comput-ing the posterior inference. This model has been designed for segmenting the speech data. In [11], the authors proposed a collapsed Gibbs sampler for the topic segmentation problem are semantically linked to one another so that a reader could relate the storyline as one moves forward in the discourse [21]. Our model comprises of two levels of topic of differ-ent granularity. One is the segment-topic to which atomic segments in a document are assigned and the ordering of segments as they appear in the document defines the topic change-points in the document. The other is the word-topic to which n-gram words in the segment are assigned. Segment-topics come from a predefined number of segment-topics K . Each segment-topic comprises of a mixture of several word-topics where the mixture coefficients uniquely specify the segment-topic. Word-topics come from a prede-fined number of word-topics Z . In general, the number of segment-topics will be less than the number of word-topics. The reason is that the number of segments in a document is less than compared to the number of words [31].

In the graphical model shown in Figure 1, M denotes the number of documents in the collection and V denotes the number of words in the vocabulary. In each atomic seg-ment s , NTSeg finds n-gram words in a word-topic z .It can also find correlations between both kinds of topics i.e. word-topics z and segment-topics y . The segments of each document are assumed to follow a Markov structure on the topic distributions of each segment. We assume that there will be a high probability that the topic for the segment s in the document will be the same as that of the segment s  X  1. A segment binary switching variable c ( d ) s for the segment-topic in the document d indicates whether there is a change of topic between the segments. The states of the switching variable correspond to the segmentation of the document into coherent topical units. Apart from the segment switch-ing binary variable, NTSeg also incorporates another random variable known as the bigram status variable x which indi-cates the bigram status i.e. whether a word w at position i in the segment s in the document d , denoted as w ( d ) si ,forms a bigram with the previous word w ( d ) s,i  X  1 . The mechanism is they do not.

It can be observed that the existing topical n-gram model ( TNG ) [39], LDSEG , [32], and LDCC , [31] are special cases de-rived from our model. For example, consider only a seg-ment, for instance segment s , in Figure 1. Removing the segmentation scheme along with a set of arrows pointing One can observe that our model, NTSeg , has the capability of deciding whether to generate a unigram or a bigram in a topic and the topic assignment for the words in a bigram are the same. This aspect differentiates NTSeg from TNG . Similar to TNG , NTSeg assumes a first order Markov assump-tion i.e. it is mainly a bigram model, but the basic genera-tion process produces unigram or bigram words. However, NTSeg has the ability to produce higher order n-grams (i.e. n&gt; 2) by concatenating consecutive n-grams (unigram or bigram words) having the same topic and the bigram status variable between them is 1. In this way, the words in the n-gram share the same topic. This again contrasts NTSeg from TNG where TNG analyzes each n-gram post hoc as if the topic of the final word in the n-gram was the topic assign-ment of the entire n-gram. But it violates the principle of non-compositionality [23]. Removing the bag-of-words as-sumption in each segment of our proposed NTSeg model re-duces to the LDSEG model. Relaxing both bag-of-words and c s indicates whether there is a change in the segment-topic between the segments s  X  1and s in the document d .If c s = 1 then it means that y does not change between the segments in the document d . However, when c ( d ) s =0,then y ( d ) s is drawn from a Multino-mial distribution parameterized by  X  ( d ) . The computation from Multinomial (  X  ( d ) )when c ( d ) s =0.
 erly defined for the first segment of every document. There-fore, c ( d ) s = 1 is defined for the first segment which is drawn from Multinomial (  X  ( d ) ). Similarly we assume that x ( d ) s 1 is observed and only unigram is allowed at the beginning of every segment.
The inference problem is related to computing the poste-rior probability of the hidden variables when the input pa-rameters  X ,  X ,  X ,  X ,  X  and the observed variable w are given. Also, an estimate of the  X  hyperparameter has to be made. It can be shown that computing the exact inference in our model is intractable. Hence, we need to resort to approxi-mation techniques such as Gibbs sampling [7]. Adoption of Bayesian methods results in some hidden parameters being integrated out instead of being explicitly estimated. Assum-ing conjugate priors on the model parameters also eases the inference algorithm significantly. Algorithm 1 depicts the Gibbs sampling used in our approximate inference for NTSeg We need to compute the two conditional distributions:
Note that w ( d )  X  si defines all the words in the segment except is the word-topic assignment for all other words except the current word w ( d ) si . In Equations 3 and 4, n zw is the number of times the word w is assigned to the word-topic z as a unigram. m wvz isthenumberoftimestheword w appears as a second word of a bigram with a previous word v and both words in the bigram are assigned to the same word-topic z . p zwt denotes the number of times the status variable x = t (0 or 1) given the previous word w and the previous word X  X  word-topic z . h ( d ) sz isthenumberoftimesawordin segment s of document d is assigned to word-topic z .  X  ( d ) c is set to 0 and 1 in the document d , respectively.  X  is the corresponding Dirichlet parameter for the segment-topic y ( d ) s . b ( d ) k is the number of times a segment in the document d has been assigned to the segment-topic k . y ( d )  X  s is the segment-topic assignments for all the segments except the current segment s in the document d .

Beginning with the joint probability of a dataset, and us-ing the chain rule, we obtain the conditional probabilities conveniently. We obtain the following equations: sample from the conditional distribution of the word-topics in a document conditioned on the word-topic assignments for all other words except the current word (Line 23 in Al-gorithm 1). In addition, we also sample the bigram status variable (Line 24). We sample from the conditional distribu-tion of a segment-topic for a segment (Line 14) and also the corresponding switching variable given the segment-topic as-signments (Line 14).

At each iteration of the Gibbs sampling procedure, we only sample a subset of the variables which are directly re-lated to the conditional probability. We perform this step repeatedly until we arrive at some approximation. A vari-able is sampled from the conditional distribution given that the assignments for all other variables are known which is a standard procedure in a Gibbs sampler. As the list of words is being scanned along with the bigram status variables, the sampler keeps track of any new segment being encountered. For each new segment, the sampler decides about the topic assignment of the segment i.e., whether it should assign the current segment to the same topic as the previous segment or a new segment-topic. If the segment has to be assigned to a new segment-topic, the sampler estimates the probability of assigning the segment to the segment-topic. These proba-bilities are computed from the conditional distribution for a segment given all other topic assignments to every other seg-ment and all words in the segment as depicted in Algorithm 1.
Evaluation of topic models is a challenging task. Simply showing the highly probable n-gram words obtained from each topic may not be able to portray the underlying strengths or weaknesses of a topic model. Therefore, we evaluate our model on several text mining tasks including the ability to support fine grained topics with n-gram words in the corre-lation graph, the ability to segment a document into topi-cally coherent sections, document classification, and docu-ment likelihood estimation.

In each experiment, we chose several existing closely re-lated comparative methods for comparison purpose. We will describe those comparative methods in the subsections that follow. For our proposed framework, NTSeg , the segment granularity is basically a paragraph because topical changes typically occur at paragraph boundary and this strategy is also used in [31]. Note that NTSeg can also work at the granularity of a sentence which has also been used in one of our experiments (refer Section 5.2). In our experiments, the number of iterations for the Gibbs sampler is 1000 which is the value of the M axIteration used in Algorithm 1. We have chosen the following hyperparameter values  X  =0 . 01,  X  =0 . 1,  X  =0 . 1,  X  = 0 . 1, and  X  =0 . 1. Other topic models such as TNG , LDSEG etc, also assume fixed hyperparameter values. We did not perform any stemming, but removed stopwords 1 from the collection.
NTSeg produces two levels of topics, namely, segment-topics and word-topics. A word-topic is comprised of n-grams. We show the correlation graph for the purpose of depicting how our model finds correlations among various http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a11-smart-stop-list/english.stop segment-topics and word-topics. Details regarding construct-ing and interpreting such correlation graphs can be found in [22], and [31].

We have used the OHSUMED 2 collection to show the cor-relation graph. The collection is composed of 348,566 doc-uments with 154,711 words in the vocabulary without stop-words.

We have experimented by varying both number of the word-topics Z and the number of the segment-topics K . Z was varied from 50 to 200 in steps of 50 whereas K was varied from 50 to 150 in steps of 50. However, we did not observe significant difference in the quality of the results. The resulting correlation graph is shown in Figure 2 which is obtained by setting Z = 200 and K = 100. Due to space constraint, we only show the graph obtained from our NTSeg model. Note that other models such as PAM , LDCC , LDSEG , GD-LDA [6] and CTM , only form unigrams in a topic leading to ambiguous interpretation. For example, presenting the uni-gram  X  X onfidence X  will not be that insightful in a correlation graph. In contrast, presenting the term  X  X onfidence interval X  is more meaningful as shown in Figure 2.
The purpose of this experiment is to show how well NTSeg generates segmentation of documents corresponding to co-herent topical units. The segmentation information is ob-tained via the segmentation switch variable c ( d ) s which gives the segment topic change-points in the document. In our problem setting we know the segment boundaries in ad-vance such as paragraphs or sentences, but we do not know the word and segment topics. Our purpose is thus to learn the segment and word topics from the document collection. The prediction output of the segment status variable will define the segmentation of a document. To evaluate the performance, we make use of the annotated segmentation information. We use two standard metrics, namely, Pk and WinDiff which are widely used in the topic segmentation literature [32]. As described in [32], Pk is defined as the probability that two segments drawn randomly from a doc-ument are incorrectly identified as belonging to the same topic [2]. WinDiff [28] moves a sliding window across the text and counts the number of times the hypothesized and reference segment boundaries are different from within the http://ir.ohsu.edu/ohsumed/ohsumed.html window. The lower the values obtained for these two met-rics, the better is the segmentation result.

We use two publicly available datasets that contain seg-ment boundaries corresponding to the topic changes. The first dataset, called Lectures in our experiment, consists of spoken lecture transcripts from an undergraduate physics class and a graduate artificial intelligence class. The tran-scripts consist of a 90 minute lecture recording and have 500 to 700 sentences with about 9000 words. Note that here the segment granularity is a sentence. More details about this dataset can be obtained from [32]. Our second dataset, called Books in our experiment, is the books 3 dataset in which each document is a chapter extracted from a medical textbook.
 We chose a recently proposed topic segmentation method TopicTiling [30] which has outperformed many state-of-the-art text segmentation models proposed in the literature and chose the best performing variant of TopicTiling from [30]. Note that TopicTiling only has the notion of word-topics. For each of the segment and word-topics, we run the Gibbs sampler five times and take the average of the Pk and WinDiff values at the end of the fifth run.

We illustrate the segmentation results in Figure 3. From the results, we note that our model performs extremely well in both datasets compared to the state-of-the-art topic seg-mentation model. Using a two-tailed significance test, our results are statistically significant with p&lt; 0 . 05 against Top-icTiling . In the Books dataset, NTSeg performs reasonably better, but the improvement obtained is not very high con-sidering both Pk and WinDiff metrics. However, good im-provement is obtained in the Lectures dataset using both metrics.
We conduct document classification experiment using topic models. In the training phase, a topic model is learned for each class using the set of training documents in that class. In testing, to conduct document classification for a testing document, we compute the likelihood of the testing docu-ment against each trained topic model for each class. The testing document is classified to the model that produces http://groups.csail.mit.edu/rbg/code/bayesseg/ Politics datasets. PDLDA also proved to be a better model in comparison to the other comparative methods.
Another evaluation scheme to compare the relative per-formance of topic models is to study how the models gener-alize on an unseen data. The entire corpus in this method is first split into training and testing set. The training set generally contains more number of documents as compared to the testing set. A model is first learned on the training data, and the testing set is used to measure the generaliza-tion performance of the topic models. In the topic mod-eling literature, metrics such as perplexity computation or log-likelihood have often been used. For example, PAM uses empirical log-likelihood [10] as an evaluation metric and so does a recently proposed method GD-LDA [6]. Log-likelihood has also been widely used as one of the evaluation metrics, for example in [3]. We chose log-likelihood metric for com-paring the topic models. The comparative methods here are LDSEG , PAM , LDACOL , TNG ,and PDLDA .

We use the NIPS dataset 5 . The NIPS collection is widely used in the topic modeling literature. Note that the original raw NIPS dataset consists of 17 years of conference papers. But we supplemented this dataset by including some new raw NIPS documents 6 and it has 19 years of papers in total. Our NIPS collection consists of 2741 documents comprising of 453,606,9 non-unique words and 94961 words in the vo-cabulary. In addition to the NIPS collection we also use the OHSUMED collection. http://www.cs.nyu.edu/ e roweis/data.html http://ai.stanford.edu/ e gal/Data/NIPS/
We have developed a generative topic discovery model, known as NTSeg , which maintains the document X  X  structure such as paragraphs and sentences and also keeps the order of the words in the document intact. NTSeg incorporates the notion of word-topics and segment-topics. We have conducted extensive experiments and shown results using both qualitative analysis where we show the n-gram words in the correlation graph and quantitative performance. Ex-perimental results demonstrate that by relaxing the bag-of-words assumption in each segment improves the perfor-mance of the model.

Giving an arbitrary number of word-topics and segment-topics to the model is one issue that we would look into for future work. We would attempt to work towards a model which could automatically find out the desirable number of word-topics and segment-topics in the collection.
