 Probabilistic graphical models for sequence data enable us to effectively deal with inherent uncertainty in many real-wor ld domains. However, they operate on a mostly propositional level. Logic approaches, on the other hand, can compactly represent a wide variety of knowledge, especially first-ord er ones, but treat uncertainty only in limited ways. Therefore , combining probability and first-order logic is highly desir able for information extraction which requires uncertainty mod -eling as well as dependency and deeper knowledge represen-tation. In this paper, we model both segmentations in obser-vation sequence and relations of segments simultaneously i n our proposed integrated discriminative probabilistic fra me-work. We propose the Metropolis-Hastings, a Markov chain Monte Carlo (MCMC) algorithm for approximate Bayesian inference to find the maximum a posteriori assignment of all the variables of this model. This integrated model has sev-eral advantages over previous probabilistic graphical mod els, and it offers a great capability of extracting implicit relat ions and new relation discovery for relation extraction from enc y-clopedic documents, and capturing sub-structures in named entities for named entity recognition. We performed exten-sive experiments on the above two well-established informa -tion extraction tasks, illustrating the feasibility and pr omise of our approach.
 I.5.1 [ Pattern Recognition ]: Models X  Statistical ; I.2.7 [ Ar tificial Intelligence ]: NLP X  Text analysis  X 
The work described in this paper is substantially sup-ported by grants from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project No: CUHK4128/07) and the Direct Grant of the Fac-ulty of Engineering, CUHK (Project Codes: 2050391 and 2050442). This work is also affiliated with the Microsoft-CUHK Joint Laboratory for Human-centric Computing and Interface Technologies.
 Algorithms, Experimentation Text mining, information extraction, probabilistic graph ical models, first-order logic
Information Extraction (IE) aims at identifying specific pieces of information in a unstructured or semi-structured textual document and transforming unstructured informa-tion in a corpus of documents or web pages into a struc-tured database. There are several representative tasks in I E: named entity recognition, which aims at identifying phrase s that denote types of named entities, entity relation extrac -tion, which aims at discovering the events or relations rela ted to the entities, and the task of coreference resolution, aim s at determining whether two extracted mentions of entities refer to the same object.

Recently, probabilistic graphical models for sequence dat a have become the predominant formalism for IE, achieving state-of-the-art performance [18]. Typically, probabili stic graphical models can deal well with uncertainty, but they ar e less expressive and flexible than logical or symbolic system s. Usually, they involve propositional, rather than first-ord er representations. When required, more expressive, higher level representations are obtained by ad hoc manipulations of lower level, propositional systems.

More specifically, a major disadvantage of probabilistic graphical models, is the limited expressiveness of attribu te-value representation of features. Attribute-value vector s have the same expressiveness as propositional formalisms, that is, they only allow the representation of atomic propositions a nd constants. While attribute-value representation is suita ble for statistical machine learning approaches, they can hard ly handle IE problems involving complex knowledge which re-quires richer representational power facilitating logica l infer-ence or reasoning. Another limitation is that, a unique rep-resentation for all examples is needed, resulting in a quite sparse data representation, since for certain features of s ome examples, the values will not be available. The problem of data sparseness increases as more knowledge is exploited an d this can cause problems for large scale real-world tasks. Fi -nally, in this kind of representation, equivalent features may have to be restricted to distinct identifiers. Each feature w ill be treated as independent pieces of information.
First-order logic [6], on the other hand, is a powerful paradigm to represent a wide variety of knowledge. It is a more expressive formalism and allows the representation of variables and n -ary predicates, i.e., domain and relational knowledge. And it can capture complex and implicit prop-erties through rich expression of conditions. Therefore, d e-pendency and deeper relations between features can be more adequately described. First-order formalisms allow a gene ric predicate to be created for every possible example, relat-ing two or more elements [17, 2]. For example, the logic work_for(person, company) could have several instantia-tions such as work_for(John, Microsoft) and work_for(Bo b, IBM) , etc. Since each example is represented indepen-dently from the others, the data sparseness problem is min-imized. While highly expressive, this type of model lacks a sophisticated treatment of degrees of uncertainty and fuzz i-ness, which permeates real-world domains, especially the ones usually associated with intelligence.

Clearly, probabilistic graphical models and first-order lo gic offer complementary strengths and weaknesses for sequence data, and the integration of both is highly desirable. How-ever, incorporating logic into probabilistic models is gen er-ally quite challenging. This is because probabilistic grap h-ical models operate on the token level, and they are inca-pable of incorporating entity-level commonly found domain knowledge. There is a fundamental mismatch in representa-tion resulting in difficulty in the integration. We solve this problem by relaxing probabilistic graphical models with th e introduction of segments, and the labels of tokens inside a segment are assumed to be the same. Given the segments in observation sequences, various kinds of relational or do -main knowledge can be easily and concisely formulated into first-order logic, and logical learning can be conducted.
Inspired by this motivation, in this paper we combine the advantages of both probabilistic graphical models for sequence data and first-order logic in a principled way, re-sulting in an integrated discriminative probabilistic fra me-work which models both segmentations in sequence data and relations of different segments simultaneously for IE tasks. This integrated model offers a great flexibility to cap -ture uncertainty for sequence modeling, as well as a variety of first-order knowledge. We illustrate the benefits of this model for mining implicit relations and new relation dis-covery, and capturing sub-structures in named entities. We propose the Metropolis-Hastings [11], [7], an approximate Markov chain Monte Carlo (MCMC) algorithm to enable ef-ficient and tractable inference for this model. This algorit hm performs efficient sampling from segmentations via Markov chains, and it is guaranteed to converge. Joint parameter estimation in this model can be too expensive or even in-tractable. We perform parameter estimation somewhat sep-arately for this integrated model. Substantial gains are ob -tained from extensive experiments on two representative IE tasks  X  encyclopedia relation extraction and named entity recognition, which demonstrate that the integrated model significantly outperforms state-of-the-art purely probab ilis-tic graphical models and purely logic models. We also dis-cuss and analyze the benefits of the proposed Metropolis-Hastings inference algorithm, showing boosted performanc e compared to the greedy and N -best list algorithms. More importantly, these promising results will significantly fu rther the applicability of our proposed approach to other large-scale real world IE tasks.
High-level information extraction requires both probabil is-tic modeling and complex and deeper knowledge represen-tation. In this section, we show the shortcomings of sophis-ticated probabilistic approaches to sequence modeling, an d illustrate the advantages of the integrated model incorpo-rating probability with first-order logic for two real-worl d IE tasks.
 Implicit Relation Extraction and New Relation Dis-covery
A large number of engineered systems were developed for identifying relations of interest. However, reliably extr act-ing relations between entities in text is still a difficult and unsolved problem. Traditional probabilistic systems extr act relations assuming that entities are already known or ex-tracted from text. They rely on the assumption that entity extraction has been solved without errors. Unfortunately, such assumption is not valid in practice.

Another major limitation is that, implicit relations can hardly be discovered in these models. Implicit relations ar e those that do not have direct contextual evidence and gener-ally exist in different paragraphs, or even across documents . They require additional knowledge to be detected. Notably, they are ubiquitous and are the sorts of relations that are likely to have significant impact on performance. Unfortu-nately, extracting implicit relations is challenging even for current state-of-the-art relation extraction models.
In particular, consider the following 4 sentences: 1. Rosemary Forbes is the mother of John Kerry . 2. Rosemary Forbes has a sibling James Forbes . 3. James Forbes  X  X  son is Stuart Forbes . 4. John Kerry celebrated with Stuart Forbes on the grad-Figure 1: An example of implicit relation extraction. The real lines show explicit (general) relations and the dashed line shows an implicit relation.

State-of-the-art relation extraction systems may be able t o detect the son and sibling relations (as shown in Figure 1) from local contextual clues. However, the cousin relation is only implied by the text and requires additional knowledge to be extracted.
 We show that our approach can enable this technology. First-order formalism allows the representation of deep an d relational knowledge. By employing the logic parent(x,z)  X  sibling(z,w)  X  child(w,y)  X  cousin(x,y) , the implicit re-lation can be easily extracted and new relation can be discov -ered ultimately. Since these formulas will not always hold, we would like to handle them probabilistically by estimatin g the confidence of each formula. Our approach can incorpo-rate rich dependencies between entities. It can also exploi t relational autocorrelation , a widely observed characteristic of relational data in which the value of a variable for one instance is highly correlated with the value of the same vari -able on another instance.
 Modeling Sub-structures in Named Entities Structured data are widely prevalent in the real world. Observation sequences tend to have distinct internal sub-structure and indicate predictable relationships between in-dividual class labels. For example, in the named entity reco g-nition task, many named entities have particular character is-tics in their composition and human beings usually use prior knowledge to recognize them. A location name can option-ally end with a location salient word (such as City , Province , etc.), but cannot end with any organization salient word (such as Government , University , etc.). A complex, nested organization name may be composed of a person name, a location name, or even another organization name. All en-tities cannot comprise any stopword or punctuation. These complex and expressive structures can largely influence pre -dictions. For example, the organization name U.S. Govern-ment consists of a location name U.S. and an organization salient word Government that implies the organization class label. This characteristic is crucial, and without modelin g it may lead to an incorrect location label for U.S. Govern-ment . However, the efficiency of probabilistic sequence mod-els, such as the linear chain CRF-based [9] approach heavily depends on its first-order Markov property  X  given the ob-servation, the label of a token is assumed to depend only on the labels of its adjacent tokens. The CRF approach models the transitions between class labels to enjoy advantages of both generative and discriminative methods, thus capturin g external dynamics, but unfortunately it lacks the ability t o represent internal sub-structure.

Fortunately, these sub-structures can be modeled well by first-order logic in the integrated model. For example, the logic formulas endwith(r,p)  X  locsalient(p)  X  loc(r) and endwith(r,p)  X  orgsalient(p)  X  org(r) convey that if an entity candidate ends with a location salient word, then it i s a location name; if it ends with an organization salient word , then it is an organization name. containstop(p)  X  non_enti ty(p) and containpunc(p)  X  non_entity(p) illustrate stop-word and punctuation restrictions in named entities. These logic formulas can be designed easily and concisely to cap-ture and model the essential sub-structures.
Let X be an observation sequence of tokens and | X | be the length of the sequence (i.e., number of tokens). Let S = h S 1 ,S 2 ,...,S L i be a segmentation of the input se-quence X , and each entry is a segment which is a triple S = h t i , X  i ,y i i , with t i as a start position,  X  i as an end po-sition, and y i as the label of this segment. y i  X  Y where Y is the label set. Thus, a segment S i means that the label y is assigned to all the observations between the start positi on t and the end position  X  i in the observation sequence X . It is reasonable to assume that segments have positive lengths and adjacent segments touch, that is, 0  X  t i  X   X  i  X  | X | and t i +1 =  X  i + 1. S can essentially model entity candidates to be considered. Let R = h R 1 ,R 2 ,...,R M i be a first-order logic possible world of segment relations expressed as a set of ground predicates R i with truth value assigned. When only one segment candidate appears in the arguments of R , it represents a particular segment constraint (e.g., sub-structure in its composition). When more than one segment candidate appears in the arguments of R i , it represents re-lations of segments.

We now describe in detail our proposed model. We jointly consider segmentations S in observation sequence X and possible worlds of segment relations R . Therefore, an as-signment of all the variables in the integrated model is a pair h R,S i . A valid assignment h R,S i must satisfy the con-dition that both of the two assignments are optimized, that is, the assignments of the segments and the assignments of the relations of segments are maximized simultaneously. Le t  X  R and  X  S denote the most likely relation assignment and seg-mentation assignment, respectively. By applying chain rul e,  X  R and  X  S can be obtained as follows:
Clearly, our model consists of two sub-structures  X  seg-mentations S conditioned on observation sequence X , rep-resented by p ( S | X ), and relations R of segments given a segmentation S and observation sequence X , represented by p ( R | S, X ). Note that this model is quite general, and has potential to integrate a variety of probabilistic segmenta tion and logic models.

In particular, we investigate the use of undirected, discri -minatively-trained probabilistic graphical models, know n as Semi-Markov conditional random fields (Semi-CRFs) [16], to effectively model segmentations S in sequence data. Be-sides the modeling flexibility similar to conventional CRFs , Semi-CRFs are capable of measuring properties of segments, and transitions within a segment can be non-Markovian. For segment relations, we employ the idea of Markov logic net-works (MLNs) [15], a recently introduced framework for first -order logic, to model relations R of segments. An MLN is a set of first-order knowledge base (KB) with a real-valued weight assigned to each formula. Together with a finite set of constants representing objects in the domain, it defines a ground Markov network containing one feature for each possible grounding of a first-order formula in the KB, with the corresponding weight. The weights associated with the formulas in an MLN jointly determine the probabilities of those formulas (and vice versa) via a log-linear model. The KB is a set of general constraints C = { C 1 ,C 2 ,...,C expressed as first-order logic formulas. Each C i contains some predicates representing constraints on elements in th e domain. When only one segment variable appears in C i , it represents certain attribute or characteristic of that typ e of segment. When more than one segment variable appears in C , it represents certain relations among segments. Some C i may not contain segment variable modeling relations or characteristics of non-segment objects in the domain. For-mulas are constructed using constants, variables, functio ns, and predicates. Constant symbols represent objects in the domain of interest (e.g., people such as John and Bob ). Vari-able symbols range over the objects in the domain (e.g., x and y ). Function symbols represent mappings from tuples of objects to objects and predicate symbols represent relatio ns among objects (e.g., person ) in the domain or attributes of objects (e.g., endwith ). Variables and constants are typed , in which case variables range only over objects of the corre-sponding type, and constants can only represent objects of the corresponding type, to reduce the size of ground Markov network. For example, the variable x may range over people (e.g., John , Bob , etc.) and the constant Microsoft may rep-resent a company. Formulas are recursively constructed fro m atomic formulas (predicates applied to a tuple of terms) us-ing logical connectives and quantifiers. The formulas in a KB are implicitly conjoined. A ground atom is an atomic formula all of whose arguments are ground terms (terms containing no variables).

Markov logic is a highly expressive language to specify the connectivity and template of a Markov network. The nodes in the network structure of an MLN are atomic formulas, and the edges are the logical connectives used to construct the formula. Each formula is considered to be a clique, and the Markov blanket is the set of formulas in which a given atom appears. However, atomic formulas do not have a truth value until they are ground atoms with a Herbrand inter-pretation. Thus, an MLN becomes a Markov network only with respect to a specific grounding and interpretation, and the resulting Markov network is called the ground Markov network. Given different sets of constants, it will produce different networks. In the graphical structure of a ground Markov network, the nodes are ground atoms. There is an edge between two nodes iff the corresponding ground atoms appear together in at least one grounding of one formula in the KB. The atoms in each ground formula form a clique in the ground Markov network. Each state of the ground Markov network represents a possible world. Under several reasonable assumptions [15], the set of possible worlds is fi -nite, and the ground Markov network represents a unique, well-defined probability distribution over those worlds, i rre-spective of the interpretation and domain.

The segments from the Semi-CRF sub-structure are con-sidered to be entity candidates. Given these entity candi-dates and the first-order logic KB, the ground Markov net-work can be constructed to learn the relations between them. The MLN sub-structure attempts to provide logical inferenc e on the entity candidates. It is constructed via grounding the first-order logical formulas associated with entity can di-dates. It also consists of a graphical structure derived fro m the formulas instantiated with the data and the weights. Consequently, the entities can be identified and the entity relations can be learned from the integrated model.
Let B be the ground predicates generated from the input sequence X . In other words, B contains atomic formulas whose arguments are not variables. Given a particular seg-mentation S , Evidence predicates are a set of ground atoms with known truth values. Take the NER task for exam-ple, for the first-order KB listed in Table 5, some predicates such as per , loc , org , and non_entity are entity or query predicates. These predicates are used to predict whether an entity candidate is a PER, a LOC, an ORG, or a non-entity. The remaining predicates such as endwith(r,p) are Evidence predicates. p ( R | S, X ) in Equation 1 can be com-puted efficiently by calculating p ( R | Evidence, B ,S ). There-fore, Equation 1 can be rewritten as: Figure 2: Graphical representation of the integrated discriminative probabilistic model consisting of two types of repeated sub-structures: (1) a semi-Markov chain on the segmentations conditioned on the ob-servation sequences, the segments from this struc-ture are considered to be entity candidates; (2) given these entity candidates and the first-order logic KB, an undirected graph is constructed via grounding process for learning relations. The nodes in this graph are ground atoms with a possible world or Herbrand interpretation. where W R is a set of segments, a set of functions, and a set of relations of segments; together with an interpretation, th ey determine the truth value of each ground atom. Equation 3 can be inferred to Equation 4, due to the fact that for a constructed ground Markov network, p ( Evidence | B ,S ) is constant.

As described above, the model consists of two types of sub-structures: (1) a semi-Markov chain on the segmentations S conditioned on the sequences X ; (2) an undirected graph constructed via grounding the first-order KB associated wit h segments (entity candidates). As shown in Figure 2, the nodes in this graph are ground atoms with a possible world or Herbrand interpretation assigning a truth value to each node. For the node F 1 ( A,C ), A and C are segments from observation sequence X and F 1 is a predicate. Therefore, our model can be formally derived as follows: where g = h g 1 ,g 2 ,...,g K i is a vector of segment feature functions. Each g k depends on the current segment, the whole observation, and the label of previous segment, that i s, g X ,S )) is the potential function conditioned on features of X for segmentations. n i ( W R ) is the number of true groundings of a formula in the i -th first-order logic formula,  X  i ( W e potential function is associated to each formula, and takes the value of 1 when the formula is true, and 0 when it is false. W R { i } is the truth value of the grounded predi-cate appearing in the formulas.  X  = h  X  1 , X  2 ,..., X  K i and  X  = h  X  1 , X  2 ,..., X  L i are parameter vectors of the two sub-structures respectively. Z S and Z W are normalization fac-tors of the Semi-CRF and MLN sub-structures respectively. As can be seen, this model offers a sound theoretical foun-dation for uncertainty, and has the advantage of combining the expressiveness of first-order logic.
We discuss in detail the inference and training algorithms for our model in this section. We propose the Metropolis-Hastings algorithm, which consists of efficient sampling fro m segmentations, to enable approximate and tractable infer-ence for this model. Joint parameter estimation in this model is prohibitively intractable, and we perform parame-ter estimation separately for each of the two sub-structure s.
The objective of inference is to find the most likely assign-ments of variables in the integrated model, that is, the pair h  X 
R,  X  S i that has the maximum posterior probability. Unfortu-nately, exact inference by computing the posterior probabi l-ity of all possible segmentation assignments S and world of relation assignments R is generally intractable, as evaluating the normalization factors Z S and Z W for this distribution re-quires summing over all possible segmentations for the enti re dataset and evaluating all possible world of relations.
Consequently, approximate inference becomes an alterna-tive by relaxing the requirement (e.g., computing the distr i-bution explicitly) of exact inference. We propose the Metro polis-Hastings algorithm, a specific Markov chain Monte Carlo (MCMC) approach in which a Markov chain is used to sam-ple from the segmentations, to perform the maximum a pos-teriori (MAP) estimates for the inference of this model.
The Markov chain Monte Carlo (MCMC) methods are a class of algorithms for sampling from probability distribu -tions based on constructing a Markov chain that has the desired distribution as its stationary distribution. In pr ac-tice, it means that we can construct a Markov chain whose states are the objects we wish to sample. The state space S includes all possible segmentations S of the entire dataset in our case. And the transition probabilities are specified via a scheme guaranteed to converge to the desired distribution. We can walk the Markov chain, occasionally outputting sam-ples. These samples are likely to be in high probability area s, increasing our chances of finding the maximum. After the chain has run long enough for it to approach its stationary distribution, the expectation E  X  [ f ] of any function f ( S ) of the state S will be approximated by the average of that func-tion over the set of sample states produced by the MCMC algorithm. The challenge is how to sample segmentation se-quences efficiently from the conditional distribution define d by the Semi-CRF sub-structure. We propose the efficient Metropolis-Hastings, a specific MCMC algorithm for sampling directly from p ( S | X ), as shown in Figure 3. Let S ( t ) be the current state of seg-mentation sequence S and S ( t +1) be the next state of S . Figure 3: The transition probability of the Markov chain from state S ( t ) to state S ( t +1) is the conditional distribution of the attribute (boundary and label) at the segment S ( t ) i (1  X  i  X  L ) given the rest of the segmentation sequence S ( t )  X  i . S ( t ) i is divided into k sub-segments ( S i 1 S i 2 S ik ) ( t +1) in the state S The Markov chain is defined as follows: from each state se-quence we transfer to a state sequence obtained by changing the state at a particular segment S i . In other words, the transition probability of the Markov chain is the condition al distribution of the attribute (boundary and label) at the se g-ment S i given the rest of the segmentation sequence. Let L be an upper bound on segment length for the Semi-CRF sub-structure. If | S i | = 1, we only change the label of this segment. If 1 &lt; | S i |  X  L , we divide S i into k sub-segments S i 1 S i 2 S ik with different labels. In summary, the distri-bution over these possible transitions from state S ( t ) S ( t +1) is defined as follows and we could sample from: where S  X  i is all segments except S i , S  X  i = ( S 1 ,...,S S S .

Note that the length of all segments at state S ( t +1) should not exceed the upper bound L . From Equation 7 we know that the segment S i may be divided into several sub-segments, and this may change the boundary (length) of adjacent seg-ments S i  X  1 and S i +1 . The number of possible sub-segments can be large for long segment S i . Instead of exhaustively enumerating the list of all possible sub-segments, we restr ict our targets to limited ( k  X  3) sub-segments to enable effi-cient computation, based on the assumption that for a long segment, it is more likely that this segment be separated int o a small number of sub-segments than into a large number of sub-segments. Note that it is possible that the total number of segments in S ( t +1) is less than that of S ( t ) if S S ik is merged with the neighboring segments in S can walk the Markov chain to loop through segment S i from i = 1 to i = L , and the attribute (boundary and label) of every segment can be changed dynamically. And for each one, we re-sample the state at segment S i from distribution given in Equation 7 mentioned above.

This distribution is easy to compute in the Semi-Markov sequence model as follows, where y ij is the label of the sub-segment S ij (1  X  j  X  k ) and Y is the label set. After resam-pling all L segments, we can sample the whole segmentation sequences from the conditional distribution.
To evaluate the quality of the generated samples and to avoid low grade samples, we exploit the Metropolis-Hasting s (MH) algorithm [11], [7]. An MH step of the target distri-bution p ( S ( t ) ) and proposal distribution q ( S  X  | S sampling a candidate sequence S  X  given the current value S ( t ) according to q ( S  X  | S ( t ) ), and uses an acceptance/rejection scheme to define a transition kernel with p ( S ( t ) ). The Markov chain then moves towards S  X  with acceptance probability and with probability 1  X  A ( S ( t ) ,S  X  ) it is rejected and the next state remains at S ( t ) . In our case, p ( S  X  ) = p ( S p ( S ( t ) ) = p ( S ( t ) | X ). The proposal distribution q ( S defined as p ( S  X  | S ( t ) , X ) and can be computed by Equation 8. q ( S ( t ) | S  X  ) can also be easily computed as follows:
As described above, the (random) sampling is still inef-ficient, since the state space is extremely non-convex. To perform global optimization, a more principled strategy is to adopt simulated annealing [8], and we rewrite the accep-tance probability as follows: where c t is a decreasing cooling schedule with lim t  X  X  X  c 0. As c t  X  0 the distribution becomes sharper, and when c = 0 the distribution places all of its mass on the maximal outcome, having the effect that the Markov chain always climbs uphill. Thus if we gradually decrease c t from 1 to 0, the Markov chain increasingly tends to go uphill.

For each sample drawn from p ( S | X ), the most proba-ble relation assignment R that maximizes the probability p ( R | S, X ) can be efficiently computed by the MC-SAT algo-rithm [13]. Thus, this approximated procedure will max-imize the joint probability p ( h R,S i| X ) of the integrated model. After many samples are drawn, it will converge to h  X 
R,  X  S i , that is, the most likely segmentation assignment and corresponding most likely relation assignment  X  R can be obtained.
Given annotated data, training can be performed to esti-mate the parameters in the integrated model. Each training sample in D = { ( y i ,x i ) } N i =0 is a pair ( h R,S i log-likelihood function is where  X  and  X  are parameter vectors of the two sub-structures, respectively. Substituting the distribution in Equation 1 into the log-likelihood and we get
Ideally, we would perform parameter estimation by nu-merically climbing the gradient of the full, joint likeliho od. However, optimizing parameters  X  and  X  simultaneously is too expensive or even intractable for large-scale IE prob-lems. Previous research indicated that learning parameter s by maximizing a product of local marginals provides equal or superior accuracy to stochastic gradient ascent on an ap-proximation of the full joint likelihood [10], [19], [20].
Following this idea, we assume that  X  and  X  are indepen-dent. Therefore we train each sub-structure of the model separately, either as structured pseudo-likelihood, or si m-ply independently, and existing algorithms are sufficient fo r the training procedure. In all cases, estimation is iterati ve, consisting of dynamic programming algorithms to maximize the log-likelihood of the correct segmentation sequence [1 6] and limited-memory BFGS on optimizing the pseudo-log-likelihood of relations of segments [15]. To avoid over-fitt ing, the Gaussian prior with mean  X  = 0 and variance matrix  X  =  X  2 I is used to penalize the log-likelihood (or pseudo-likelihood) when training each part of the model.
In this section, we report empirical results by applying our proposed model to two representative IE tasks  X  rela-tion extraction from encyclopedic documents and named en-tity recognition (NER). The results show that the integrate d model achieves significant improvement over the state-of-the-art purely probabilistic graphical and purely logic mo d-els. We also study and analyze the benefits and superiority of the proposed inference algorithm over the greedy and N -best list algorithms. Table 1: Statistics of relation types and correspond-ing frequencies.

We investigated the task of discovering semantic relations between entity pairs from English encyclopedic articles. T he basic document is an article , which mainly defines and de-ciate competition, respectively.
 Table 2: Representative first-order formulas for extract-scribes an entity (known as principal entity ). This document mentions some other entities as secondary entities related to the principal entity . Our goal is to predict what relation, if any, each secondary entity has to the principal entity . Since all the secondary entities in the document are unknown (e.g., not tagged or hyper-linked), we should consider both sec-ondary entity identification (boundary and type) and the relation to the principal entity . By modeling both segmen-tations in sequence data for secondary entity identificatio n, and relations of segments for relation extraction simultan e-ously, our proposed model offers a natural way for this task. Since knowing the secondary entities is helpful for their re -lations to the principal entity and vice versa, modeling bot h simultaneously is highly desirable.

We used the same dataset as in [3] to conduct our ex-periments. This dataset consists of 1127 paragraphs from 441 pages from the online encyclopedia Wikipedia with 4701 relation instances and 53 relation types labeled. Table 1 shows the relation types and corresponding frequencies of this dataset. The 10 most frequent relation types are job title , visited , birth place , associate , birth year , member of , birth day , opus , death year , and death day . We deleted all the hyper-links which define secondary entities in text to make this dataset appropriate for our experiments. Note that this tas k involving jointly secondary entity identification and rela tion discovery becomes more challenging and cannot be solved as a sequence labeling problem described in [3].

We exploited multiple features, including contextual, par t-of-speech, morphological, syntactic, keyword, semantic a s well as Wikipedia characteristic features for the Semi-CRF sub-structure. We deal with implicit relation extraction a nd label consistency problem in the MLN sub-structure. Im-plicit relations are those that do not have direct contextua l evidence, and they require additional knowledge to be de-tected. By employing the first-order logic formalism, impli cit relations can be easily discovered from text. Some represen -tative first-order formulas for extracting implicit relati ons and handling label consistency are listed in Table 2. These formulas are generally simple, and allow the representatio n of deep and relational knowledge 1 .
Table 3 presents the performance of our system compared to the Semi-CRF and MLN models for different types of relations. The performance is measured by Precision (P), Recall (R), and F  X  =1 which is the harmonic mean of P and R ( F  X  =1 = 2 P R P + R ). 10-fold cross-validation was performed on this dataset, and average performance was shown in Table 3. The sample size was set to 3000 for the MH inference algo-rithm. The overall F-measures for Semi-CRFs, MLNs, and our integrated model are 63.5, 62.9, and 65.7, respectively . It shows that our model combining probabilistic graphical models with first-order logic significantly outperforms the purely graphical Semi-CRF model by 2.2% on the overall F-measure and logical MLN model by 2.8% on the overall F-measure, respectively. We conducted statistical significa nce estimates using McNemar X  X  paired tests and our integrated model was found to be statistically significantly better ( p -value &lt; 0.05 with a 95% confidence interval).

As shown in Table 3, the performance varies greatly from different relation types. All the three systems perform quit e well on 4 relations: death day , death year , birth year , and birth day . Since these relations can be easily identified using the distinct contextual evidence. However, some relations (e.g., role , owns , etc.) can hardly be extracted. 19 types of relations in this dataset cannot be extracted by all the thre e systems. One possible reason is the lack of training data (these relations occur rarely in the dataset, for example, t he aunt relation only occurs 4 times). For all the 34 relations listed in Table 3, the Semi-CRF model obtained the best F-measure on 9 relations and the MLN model obtained the best performance on 6 relations. Semi-CRFs perform relation extraction as sequence segmentation independently withou t
We designed no more than 30 formulas in total, and we only list some representative formulas due to space limit. capturing connections and dependencies between different entities, and implicit relations can hardly be investigate d. While MLNs model relations and rich dependencies between entities via first-order logic, they can hardly identify ent ity (boundary and type) in text. As we shown, the integrated model combines the advantages of both approaches, thus achieving the best F-measure on 17 relation types.
We study the effectiveness of the proposed inference al-gorithm by comparing it with the greedy and N -best list algorithms described as follows.

For the Semi-CRF sub-structure, we restrict our re-ranking targets to the N -best list L = { L 1 ,L 2 ,...,L N } , where { L ,...,L N } is ranked by the conditional probability p ( S | X ). For a sequence X , we maintain N -best segmentations over this sequence. For each segmentation S in this list, we can find a relation assignment R over S that maximizes the probability p ( R | S, X ). The N -best list of segmentation se-quences L and their corresponding probabilities p ( S | X )( S  X  L ) can be obtained using the Semi-Markov analog of the usual Viterbi algorithm [16]. Given a particular segmenta-tion assignment S , the most probable relation assignment R and its probability p ( R | S, X ) can be inferred by MC-SAT [13]. Having the N -best list of segmentation assignments and their corresponding relation assignments, the approximat ed solution that maximizes the joint probability p ( h R,S i| X ). And the most probable relation assignment along with this segmentation is our final output. The greedy algorithm is a special case of the N -best list algorithm when N = 1, that is, it greedily takes the best output of segmentations and corresponding relations.

The greedy algorithm is very simple, but it only makes use of 1-best list of segmentations and corresponding relation s, losing much useful information. A very limited improvement was obtained ( F 1 = 63 . 6, 0.1 higher than the Semi-CRF) for our model when using this algorithm. To investigate the size of N -best list and its impact on performance, we chose N values from 10 to 50, with an incremental step of 10. It was found that this algorithm gives useful improvement. The best performance F 1 = 64 . 1 was obtained when N = 30, which is still significantly lower than the proposed inferen ce algorithm. The Metropolis-Hastings algorithm outperform s N -best list when enough samples (sample size &gt; 1900) are drawn. We analyze the advantages of this sampling algo-rithm over N -best list as follows. The N -best list does not necessarily correspond to the best N list. Intuitively, sam-pling should give more diversity at each state, reducing the likelihood of not even considering the correct segmentatio ns.
Our results cannot be directly compared with existing work on this dataset because we deal with a more challenging problem involving both secondary entity recognition and re -lation extraction. Culotta et al . [3] proposed a probabilistic model based on CRFs to integrate extraction and data min-ing tasks performed on biographical Wikipedia articles. Re -lation extraction was treated as a sequence labeling proble m and relational patterns were discovered to boost the per-formance. However, this model extracts relations without considering dependencies between entities. As mentioned before, our task can hardly be modeled as a sequence label-ing problem as in [3]. Nevertheless, our result is close to Culotta et al . [3] (65.7 vs. 67.9 on the F-measure). Nguyen et al . [12] proposed a subtree mining approach to extracting relations from Wikipedia by incorporating information fro m the Wikipedia structure and by the analysis of Wikipedia text. In this approach, a syntactic tree that reflects the rel a-tion between a given entity pair was built, and a tree-mining algorithm was used to identify the basic elements of syntact ic structure of sentences for relations. This approach mainly relies on syntactic structures to extract relations. Synta c-tic structures are important for relation extraction, but i n-sufficient to extract relations accurately. The obtained F-measure 37.76 is much worse than ours, which shows that there is a large room for improving. We used one-month data from People X  X  Daily (January-Jun, 1998) corpus for Chinese NER experiments, which con-tains 44818 sentences, with tagged entities of 19879 person , 25661 location, and 11590 organization names, respectivel y.
For the Semi-CRF sub-structure, we used features that have been shown to be very effective for NER, namely the current character and its POS tag, several characters sur-rounding the current character and their POS tags, current word and several words surrounding the current word, and some clue word features which can capture non-local depen-dencies. In addition, we take the advantage of Semi-CRFs by using entity-level dictionary features. This gives us a r ich feature set using both local and non-local information.
For the MLN sub-structure, we extracted 165 location salient words and 843 organization salient words from Wikip -edia and the LDC 2 Xinhua News database. We also made a punctuation list which contains 18 items and some stop-words that named entities cannot contain. The stopwords are mainly conjunctions, auxiliary and functional words. W e introduced various types of domain knowledge which can capture essential characteristics of NER and can be well and concisely formulated in first-order logic. The consid-ered domain knowledge is listed as follows, and the corre-sponding first-order logic is shown in Table 5. The goal of the MLN sub-structure is to determine whether the candi-dates are entities and the types of entities by answering the query predicates (e.g., per , loc , org , and non_entity ) given the Evidence predicates (e.g., endwith(r,p) ) and other re-lations that can be deterministically derived.
The Linguistic Data Consortium, see http://www.ldc.upen n.edu/ Table 5: Domain knowledge and corresponding first-order formulas for NER.

Using Semi-CRF and MLN as baselines, we compared the performance of our model with a sample size of 3000 for ap-proximate inference. 10-fold cross-validation was perfor med on this dataset, and average performance is summarized in Table 4. As can be seen, our model yields substantially better results, leading to a relative error reduction of up t o 20.26% on the overall F-measure over the Semi-CRF model and a relative error reduction of up to 38.14% over the MLN model, respectively. The improvement is statistically sig nif-icant according to McNemar X  X  paired tests ( p -value &lt; 0.05 with a 95% confidence interval).

It is worth noticing that our proposed model boosted the performance for all 3 entity types. For example, when com-pared to the Semi-CRF model, the improvement on the F-measure is 0.73% for person, 1.25% for location, and 2.81% for organization, respectively. This can be explained by th e fact that there are much more sub-structures existing in or-ganization names than in person or location names. In that case, modeling the internal sub-structures is more helpful for organization names in the NER task. This phenomenon also demonstrates the advantage and capability of our model for effective sub-structure modeling in named entities.
It is not surprising that the Semi-CRF model performs better than the MLN model, since the NER task can be for-mulated as a sequence labeling problem, and can therefore be effectively modeled by probabilistic sequence segmenta-tion approaches such as Semi-CRFs. The MLN model, how-ever, can hardly capture the first-order Markov property in sequence data, leading to reduced performance on the NER task.
Similar to the task of encyclopedic relation extraction, we also compared the approximate MH sampling algorithm with the greedy and N -best list algorithms for NER. The greedy algorithm slightly improves the performance (by 0.12 on the overall F-measure) of the Semi-CRF model. For the N -best list algorithm, we also chose N values from 10 to 50, with an incremental step of 10. When N value was set to 20, the best F-measure of 91.61 was obtained. However, the performance of our model can hardly be further improved when we increased N to 30. To illustrate the effectiveness of the sampling algorithm, we set the sample size from 500 to 3000, with an incremental step of 500. And the obtained overall F-measures of the integrated model are 90.6, 91.08, 91.39, 91.64, 91.89, and 91.97, respectively. These number s showed that the performance is gradually improved with the increase of sample size. We observed that the sampling algo-rithm outperforms N -best list when the sample size is larger than 2000.
There is a number of literature dedicated to combining probability and logic, and we mention some closely related approaches. Our work here is related to Bunescu and Mooney [1], who employed relational Markov networks (RMNs) to represent influences and dependencies between different ex-tractions, and Finkel et al. [5], who incorporated long-distance dependencies and combined that with a standard sequence model. However, for the RMN model in [1], the dependencies must be defined in the model structure and it is necessary to make some initial assumptions about which nodes should be connected. So the authors use crude heuris-tic part-of-speech patterns, and then add dependencies be-tween these text spans using clique templates. This gen-erates a extremely large number of overlapping candidate entities. Another disadvantage of this approach is that it uses loopy belief propagation and a voted perceptron for approximate learning and inference -ill-founded and inher -ently unstable algorithms which are noted by the authors to have caused convergence problems. Finkel et al. [5] incor-porated a limited number of constraints (e.g., label consis -tency) into probabilistic sequence models. Using first-ord er logic, our model allows for a much broader class of rela-tions and dependencies. They employed Gibbs sampling for approximate inference, which is just a special case for the proposed Metropolis-Hastings sampler. In Gibbs sampling, the candidate sample is always accepted with the probabilit y of 1, lacking the capability of measuring quality of samples and eliminating low grade samples. Another disadvantage is that, when distributions are complex or even unknown, Gibbs sampling cannot be applied while Metropolis-Hasting s can still work efficiently.

Culotta et al . attempted to incorporate domain knowledge into CRFs [4]. They designed a restricted form of first-order logic, which is basically cluster-wise compatibility of th e ob-served tokens. Such logic is then incorporated into CRFs in the form of features which can only capture some raw facts observed from data and are unable to conduct logical inference. Consequently, both expressiveness and reason-ing power are very limited. A joint inference method based on MLNs was proposed in [14], where segmentation of all records and entity resolution are performed together in a single framework for citation matching. However, as shown in our experiments, the power of MLN alone for modeling sequence data is limited, since MLNs can hardly capture the Markovian property in observation sequences.

Wellner et al . [19] proposed the integration of information extraction and data mining in the CRF model for citation matching. They used N -best list of sequences for inference. Inference is performed by maximizing posterior probabilit y of entire sub-structures of the model, conditioned on all ot h-ers, and repeatedly iterating over all variables. However, the difference between our work and [19] is that we motivated by combining probabilistic sequence models with first-orde r logic. And, as we show in the experiments, the Metropolis-Hastings algorithm has several benefits and advantages, and outperforms N -best list. Zhu et al . [20] proposed an inte-grated probabilistic approach to both page structure and text content understanding for Web page understanding. Our model differs from [20] in that this model has the advan-tage of incorporating the expressiveness of first-order log ic and a variety of domain knowledge. Moreover, the proposed Metropolis-Hastings is theoretically well-founded and gu ar-anteed to converge, and generates samples efficiently from segmentations for inference.
The contributions of this paper are three-fold and we sum-marize as follows: 1. First, we presented an integrated discriminative proba-2. Second, we proposed the Metropolis-Hastings, a the-3. Finally, we illustrated the benefits of this model for
Directions of the future work include (1) feature engineer-ing to seek further gains of this model, and (2) further the applicability of our proposed approach to other large-scal e real world IE tasks.
