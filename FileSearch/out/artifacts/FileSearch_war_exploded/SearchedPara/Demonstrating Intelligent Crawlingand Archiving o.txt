 We demonstrate here a new approach to Web archival crawl-ing, based on an application-aware helper that drives crawls of Web applications according to their types (especially, ac-cording to their content management systems). By adapting the crawling strategy to the Web application type, one is able to crawl a given Web application (say, a given forum or blog) with fewer requests than traditional crawling tech-niques. Additionally, the application-aware helper is able to extract semantic content from the Web pages crawled, which results in a Web archive of richer value to an archive user. In our demonstration scenario, we invite a user to compare application-aware crawling to regular Web crawling on the Web site of their choice, both in terms of efficiency and of experience in browsing and searching the archive. H.3.5 [ Information Storage and Retrieval ]: Online In-formation Services X  Web-based services ; H.3.7 [ Information Storage and Retrieval ]: Digital Libraries X  Collection ; H.5.4 [ Information Interfaces and Presentation ]: Hy-pertext/Hypermedia content management system, crawling, Web application, Web archiving
The advent of the Web 2.0 in the past decade has had significant impact on the number of Web pages and the amount of user-generated content on the Web: Web users are now billions [9], Web search engine robots such as Google have discovered more than a trillion unique URLs [1], and one of the most popular content-management system for blogs, WordPress, is powering dozens of millions of Web sites [14].
 content of Web applications. We rely on an application-aware helper (AAH) that assists the crawling process by identifying crawled Web application types and providing appropriate crawling actions (links to follow, content to extract) accord-ingly. For supported Web sites, this approach directly targets useful content-rich areas, avoids archive redundancy, and en-riches the archive with semantic description of the content. The AAH makes use of a knowledge base of known Web application types, together with algorithms for flexible and adaptive matching of Web applications to these types.
Next, in Section 2, we present the main features and architecture of the application-aware helper. We then present in detail in Section 3 the demonstration scenario that will be proposed to conference attendees.

A companion video for this demonstration paper is avail-able at http://dbweb.enst.fr/aah . We refer to our related publication [5] for a detailed presentation of the AAH algo-rithms, system, and performance.
We now present the main components of the AAH for intelligent and adaptive crawling of known and adaptable Web applications. We refer to Figure 1 as an illustration of the architecture of the system, with items of the following enumeration referring to the numbers in the figure. 1. The AAH relies on a knowledge base of Web application 2. The system loads the Web application type detection
We now describe a specific use case where the AAH helps building richer archives with less resource wasted. This is a real-world case where the AAH has been used.

The German broadcasting company SWR is a partner each year of the Rock am Ring music festival. SWR archivists are interested in building archives of both official information and public perception of this music festival as displayed on the Web. Many Web sites are in the scope of this archival campaign, however, crawling, storage, analysis resources are limited, and timely archival matters, which requires to limit crawling requests to those that will effectively bring meaningful content to the archive. The AAH has been designed to meet these challenges and ensures that duplicates and templates are avoided, and all useful information has crawled.

In our demonstration scenario, conference attendees will have the possibility of either focusing on this specific Rock am Ring use case, or choosing a Web site of their own to crawl.

We now present in detail our demonstration scenario. See the accompanying video for a peek at the GUI. 1. The user enters in the crawling panel the URL of a Web site to crawl with the possibility to limit the number of HTTP requests. The user will be offered to choose any URL she wants; obviously, interesting results will be produced only for Web application types supported by the AAH (currently, various versions of WordPress, phpBB, and vBulletin, with a robustness to template change thanks to the adaptation module). A few Web sites will be pre-crawled locally to simulate their crawl without having to rely on network delays (especially an issue since the crawler respects per-server crawling delays as per crawling ethics). The user is also given the choice of the specific crawler to run the job: either the AAH crawler, or a baseline non-intelligent crawler, or both. Selecting both allows comparing their performance. 2. Once the crawl is completed (a few seconds when run from a local cache of the Web site, arbitrarily longer for a remote crawl), the system shows the number of HTTP requests made by both crawlers, as well as the number of broken links found. For instance, on one specific crawl of the blog http://www.rockamring-blog.de/ , the AAH has made only 1,705 HTTP requests to crawl the whole given Web application, compared to 5,000 for the baseline crawler with no certainty that all useful content has been retrieved. Comparatively, the AAH also encounters fewer broken links than the baseline crawler. We additionally provide the number of failed and adapted crawling actions. The crawl effectiveness panel (watch video) compares crawl performance on a plot of Web site coverage (measured as a number of distinct k -grams [5]) vs number of requests. 3. The system provides the data view of the crawled Web pages with the AAH. The AAH crawls the Web pages in an intelligent manner and extracts useful information. For instance, depending on the Web application, we may show a list of blog posts with crawled content and Web objects, identifying semantic components such as post body, publica-tion date, author, and post comments with their publication date and author. Similar views exist for Web forum content. 4. The data view of the crawled Web pages with the baseline crawler is rather very simple (direct HTML display), as Web pages are crawled blindly, without avoiding spider traps and noisy links.
