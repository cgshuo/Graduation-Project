
Discourse parsing determines the internal structure of a text, e.g., the discourse relationship between its text units. Recently it has become the research focus due to its critical importance on the downstream natural language processing (NLP) applications, such as coherence modeling [1, 11], text summarization [9], statistical machine translation [14] and information extraction [16].
 pus [19] adds a layer of discourse annotations on the top of the Penn TreeBank (PTB) corpus [13] and has attracted increasing attention in current discourse related work [5, 18, 10, 20, 7, 6, 12]. However, although much research work has been carried out on certain components since the release of the PDTB corpus, there is little work on constructing an end-to-end discourse parser.
 parser for explicit connectives. Particularly, we decompose the explicit discourse parser into two component, i.e., a connective labeler, which identifies connec-tives from a text and determines their senses in classifying discourse relation-ship, and an argument labeler, which identifies corresponding arguments for a given connective. Besides, a joint learning approach via structured perceptron is introduced to reduce error propagation and incroporate the interaction between the two components. Evaluation on the PDTB corpus shows the appropriateness of our framework and the effectiveness of our joint learning approach. corpus in Section 2, we briefly review the related work on explicit discourse parser on the PDTB corpus in Section 3. Section 4 describes our end-to-end explicit discourse parser in detail. Then our joint learning approach is proposed in Section 5. After reporting the experiment results, we give a discussion in Section 6. Finally, we conclude the paper in Section 7. The PDTB corpus follows the lexically grounded, predicate-argument approach in the D-LTAG framework [21]. It regards discourse connectives as discourse-level predicates that take exactly two text spans as their arguments. The span to which the connective is syntactically bound is labeled Arg2, while the other is labeled Arg1.
 explicit or implicit, among which explicit connectives occupy 53.49% (18459 tokens). Besides, each connective is assigned a sense from a three-level hierarchy, as shown in table 1 [19]. In this paper, we limit to the 16 Level-2 types. with the connective while underlined, the Arg1 span italicized , and the Arg2 span bold . The last line of this example shows the relation sense. It will be used as a running example throughout this paper. (1) Total advertising linage was modestly lower as classified-ad volume increased, Since the release of the PDTB corpus, much research work has been carried out on certain components of discourse parsing, with focus on explicit connec-tives. For example, Pitler and Nenkova[18] proposed various kinds of syntactic features to identify explicit connectives and determine the sense of an exlicit con-nective on marking the discourse relationship. On gold parse trees, they achieved 94.19% in F-measure and 94.15% in accuracy for the connective identification and the Level-1 sense disambiguation, respectively. Dinesh et al.[4] introduced a tree subtraction algorithm for labeling only subordinate discourse connectives 2 . Wellner and Pustejovsky [22] proposed several machine learning approaches to identify the head words of the two arguments for an explicit connective. Follow-ing this work, Elwell and Baldridge [5] combined several general and connective specific rankers to improve the performance of labeling the head words of the two arguments. Ghosh et al.[7] viewed argument labeling as a sequence label-ing problem, and used two conditional random fields (CRFs) to label Arg1 and Arg2, respectively. Besides, they exploited the sense of connective to improve the performance of argument labeling.
 ing, there is little work on constructing a complete end-to-end explicit discourse parser. The only exceptions are Lin et al. [10, 12], which designed an end-to-end explicit discourse parser into three components, i.e., a connective identifier, which identifies connectives from a text, a connective sense disambiguator, which determines the sense of a given connective, and an argument labeler, which iden-tifies corresponding arguments for a given connective.
 Different from Lin et al. [10, 12], we decompose it into two components, i.e., a connective labeler and an argument labeler, by combing the connective identifier and and the connective sense disambiguator into a single connective labeler. Particularly, a joint learning approach is proposed to reduce error propagation and capture the interaction between the two components. In this section, we will introduce our end-to-end PDTB-style explicit discourse parser, which consists of two components, i.e. a connective labeler, which identi-fies connectives from a text and determines their senses in classifying discourse relationship, and an argument labeler, which identifies corresponding arguments for a given connective. 4.1 Connective Labeling Since only true connectives can convey the sense of a discourse relationship, we view the non-discourse usage of a connective candidate as special sense  X  X il X . As this paper deals with the 16 Level-2 types in the connective sense hierachy, our connective labler becomes as a 17-category classfication task.
 connective labeler with the fourth column showing the feature instances corre-sponding to the gold parse tree of Example 1 as shown in Figure 1, taking [ IN while] as the given connective candidate in question.
 4.2 Argument Labeling As stated above, the PDTB views a connective as the predicate of a discourse relation. Similar to semantic role labeling (SRL), we propose a constituent-based approach to do this task.
 which are clearly not arguments to the connective in question, and all the re-maining constituents are kept as the argument candidates of the given connec-tive. Here, the pruning algorithm works recursively. Starting from the lowest node dominating the connective (called the connective target node), it collects all the siblings of the connective target node as argument candidates, moves on to the parent of the connective target node and collects its siblings as argument candidates. This process repeats until it reaches the root of the parse tree. Please note that, if the connective target node does not cover the connective exactly, the children of the connective target node are also collected as argument candidates. On the gold parse tree as shown in Figure 1 and given [ IN while] as connec-tive. We can get five argument candidates, i.e. [ S there was . . . linage], [ SBAR as classified-ad volume increased], [ ADJP modestly lower], [ V BD was], [ NP Total advertising linage].
 each argument candidate as whether  X  X rg1 X ,  X  X rg2 X , or  X  X il X . Prasad et al. [19] reported that Arg1 can be located within the same sentence as the connective (SS), in some previous sentence of the connective (PS), or in some sentence following the sentence containing connective (FS). Our statistics on the PDTB corpus shows that in the PS case, the distribution of immediate previous sentence as Arg1 accounts 76.9% (5549 of 7215). Thus we can resolve the PS case by viewing the root of the parse tree of the immediate previous sentence as a special argument candidate. Table 3 lists various kinds of lexical and syntactic features used in our argument labeler, on reflecting the properties of the connective, the argument candidate and the relationship between them, with the fourth of column showing the feature instances corresponding to Figure 1, given[ IN while] as the connective and [ S there was . . . linage] as the argument candidate in question.
 respectively. With the sequential pipeline architecture as described in Section 4, our two-components explicit discourse parser ignores the interaction between the con-nective labeler and the argument labeler. To reduce error propagation and cap-ture the interaction between the two components, we propose a joint learning approach via structured perceptron. tured prediction, which was proposed in [2]. Given an instance x  X  X , which in our case is a set of constituent nodes in a parse tree, the algorithm involves the following decoding problem which finds the best configuration d  X  Y according to the current model w : where f ( x, d 0 ) represents the feature vector of instance x under configuration d 0 . 5.1 Training be the set of training instances. In each iteration, the algorithm finds the best configuration d for instance x under current model w (Eq 1). If d is incorrect, the weights are updated as follows: algorithm. The key step of the training and testing is the decoding procedure, which aims to find the best configuration under current model parameters. As like many NLP tasks (e.g., syntactic parsing, part-of-speech tagging) have too many states, the exact inference is often intractable. Instead, we employ beam-search to perform inexact inference. Collins and Roark [3] proposed the early-update idea for inexact inference, and later Huang et al [8] proved that structured perceptron could converge even under exact inference, and they pointed out  X  X i-olation X  is just the need for the algorithm. Based on these suggestions and in order to reduce overfitting, we use a variant of stand perceptron called aver-aged perceptron [2] with beam-search for inexact inference to train our explicit discourse parser. 5.2 Decoding Our decoding algorithm will joint two components. Here we use R to denote the discourse sense label alphabet, where R consists of 16 Level-2 sense types and a  X  X il X  sense which indicates non-discourse usage. Similarly, A = { Arg 1 , Arg 2 , N il } denotes the argument label alphabet. We use K to denote the beam-size for the decoding procedure.
 given connective, and a 1 , . . . , a m are the pruning argument candidates. We use function g (  X  ) maps current constituent node to its output label. Consider Exam-to the annotation, the output is y = ( Contrast, Arg 2 , Arg 1 , Arg 1 , Arg 1 , Arg 1). discourse parsing. For each instance, there are two sub-steps:  X  connective labeling We enumerate all possible sense label alphabet, and  X  argument labeling After the relation step, we traverse all configurations because of the argument candidates selection. Likewise, the choice of later argu-ment candidates would be affected by previous argument assignments. 5.3 Features and Representation In this joint framework, we use the same features as described in Table 2 and Table 3. In general, each feature f is a function f : X  X  Y  X  R , which maps x and y to a feature value. In this framework, we use indicator function to represent each feature like the following format: We have systematically evaluated our end-to-end explicit discourse parser on the PDTB corpus, with focus on the effectiveness of joint learning via structured perceptron. 6.1 Experimental Setting We follow the PDTB recommendation [17] to use Section 02-21 in the PDTB for training, Section 22 for development, and Section 23 for testing. All staged clas-sifiers are trained with the OpenNLP maximum entropy package 3 with default parameters.
 spectively. We use the NIST text segmenter 4 to split sentences and the Charniak parser 5 to parse sentences.
 training we chose the first sense as training instance. During test if the classifier assigns either of the two senses, we consider it as correct. For fair comparison with the state-of-the-art system, we report 16 types sense performance excluding  X  X il X  senses which indicate non-discourse connective usage. We evaluate our argument labeling using exact match metric [15] without all punctuation at the boundaries. 6.2 Parameters Selection There are two important parameters in our joint learning framework: the maxi-mum iteration number T and the beam size K . The harmonic mean of discourse relation classification F 1 and two arguments both right F 1 using gold parse trees is employed to measure the overall performance of our system on the develop-ment set.
 As we can see that almost all curves converge around iteration 19. We also find that our system can achieve the best performance on the development set when beam-size K = 4. At this point, discourse relation classification achieves 83.67% in F 1 , argument labeling achieves 58.61% in two arguments both right F 1 , and the harmonic mean equals 68.93%. Therefore we set the maximum iteration number T = 19 and beam-size K = 4 for the remaining experiments. 6.3 Results and Analysis For comparison, We also construct an end-to-end explicit discourse parser in pipeline way (thereafter called as  X  X ur pipeline system X ). Table 4 lists the re-sults of three different systems on test set using gold and automatic parse trees, respectively. 6 From the results, we can find that:  X  On the necessary of independent connective identification:  X  On the impact of parse trees:  X  On the impact of joint learning: In this paper, we focus on building an end-to-end PDTB-style explicit discourse parser by decomposing it into two components, i.e., a connective labeler, which identifies connectives from a text and determines their senses in classifying dis-course relationship, and an argument labeler, which identifies corresponding ar-guments for a given connective. Particularly, to reduce error propagation and incorporate the interaction between the two components, a joint learning ap-proach via structured perceptron is proposed. Experimental results show that our end-to-end explicit discourse parser can perform well using both gold and automatic parse trees. This work is supported by grants from National Natural Science Foundation of China (No. 61333018, No. 6127320), and National 863 program of China (No.2012AA01112).

