 In this paper, the task of text segmentation is approached from a topic modeling perspective. We investigate the use of latent Dirichlet allocation (LDA) topic model to segment a text into semantically coherent segments. A major ben-efit of the proposed approach is that along with the seg-ment boundaries, it outputs the topic distribution associated with each segment. This information is of potential use in applications like segment retrieval and discourse analysis. The new approach outperforms a standard baseline method and yields significantly better performance than most of the available unsupervised methods on a benchmark dataset. Categories and Subject Descriptors: I.5.4 [Pattern Recognition] : Applications -text processing General Terms: Algorithms, Experimentation, Performance Keywords: text segmentation, unsupervised topic model-ing, latent Dirichlet allocation, dynamic programming
Text segmentation is the task of dividing a given text data into topically coherent segments [9, 13, 1, 6, 15]. Text seg-mentation is a fundamental requirement for many IR appli-cations, e.g., segmenting a news broadcast transcription into stories (if possible, with a topic tag) could be very useful for browsing/retrieval. If no text segmentation is performed and auserneedstoaccessaparticularstoryinanewsbroadcast, he may have to view the entire broadcast to get the story. In contrast, if the news is segmented (either manually or au-tomatically) into stories and labeled, the relevant story can be retrieved directly. Text segmentation can also improve a user X  X  retrieval experience by segmenting a document into topics and subtopics, and presenting only the relevant parts of the document during a search operation. Text segmenta-tion can be useful in tasks such as text summarization and discourse analysis [9].

Several approaches have been proposed in the past to per-form this task. Most of the unsupervised approaches exploit lexical chain information, the fact that related or similar words tend to be repeated in topically coherent segments and segment boundaries often correspond to a change in the vocabulary [9, 6, 15]. Such approaches do not require a training phase (data), and can be directly applied to any text from any domain, subject to the (only) constraint that word boundaries can be identified. A potential drawback of most of these approaches is that even when the segment boundaries are estimated correctly, the segments are not as-sociated (labeled) with any topic information.

A new approach for text segmentation is proposed in this paper which builds upon well established latent Dirichlet allocation (LDA) [3] model. LDA is a generative and unsu-pervised topic model; during training it learns the seman-tics information from the dataset and hence does not rely on mere word repetitions to segment the text. This is a depar-ture from the lexical chain approaches which are typically knowledge-free.

The proposed LDA based approach also differs from the lexical chain approaches in that it  X  X ointly X  performs seg-mentation and topic labeling (outputs the topic distribu-tion associated with each segment). An expected benefit of this approach is its ability to identify the topic of each seg-ment, thus allowing to track topics within a long document or within a collection.

In the proposed LDA based approach for text segmenta-tion, the model is first trained on a large amount of text data andisthenusedtosegment running texts it has not seen ear-lier (text not used for training). This is one of the differences with the approach reported in [14] where the data to be seg-mented itself is used to train the LDA model, thus making the approach unfit for segmenting running texts. Moreover, the data to be segmented is usually limited; therefore the LDA parameters may not be estimated reliably. This may be the main reason why the performance reported in [14] is not significantly better than that of the basic Texttiling method [9].

The rest of the paper is organized as follows: In Section 2, we recall the principles of dynamic programming (DP) for text segmentation, first reviewing the method proposed by Utiyama and Ishara [15] (one of the most cited baseline ap-proaches to date) and then explaining how to adapt these principles when fragments are scored under the LDA topic model. In Section 3.2, we compare and analyze the perfor-mance of the two methods on Choi X  X  benchmarks [6]. Con-clusions of this study and the future directions are discussed in Section 4.
Text segmentation can be efficiently implemented with DP techniques [15]. Assuming that text is represented as a linear Figure 1: Nodes and segments in dynamic program-ming. graph, a segment is defined by two nodes, the begin (B) and the end (E) nodes. For instance, in Fig. 1, segment Seg 5 (dotted line) is from begin node B 1 (excluding B 1 )toend node E 5 (including E 5 ). Node 0 is treated as null node for convenience.

In the standard DP approach, scores for all possible node pairs are computed. Therefore, if the graph contains N nodes, one has to consider N  X  ( N +1) / 2nodepairs.
As described in [15], text segmentation thus proceeds as follows: We denote d = w 1 w 2  X  X  X  w l d a document of length l ;and S = S 1 S 2  X  X  X  S m a particular segmentation S made up of m segments. The likelihood of S is thus In (1), P ( d | S ) is the probability of d under segmentation S and P ( S ) is a prior over segmentations, which corresponds to a penalty factor [15]. Assuming S i contains n i word to-kens, and that w j i denotes the j th word token in S i denote W i = w 1 i  X  X  X  w n i i . Therefore, d = W 1  X  X  X  W l = a one to one correspondence. Further, assuming that seg-ments are independent of each other, (1) can be rewritten as
P ( S | d )  X  The most likely segmentation is defined as  X  S =argmax and can be recovered using DP in a manner similar to the resolution of shortest path problems. During the forward-pass, for each pair of nodes ( B, E ), the score of Seg
For a given do cument d , P ( d ) is constant for all the seg-mentations and can be dropped from the equation. computed. The path that maximizes the cumulative score from the first to the last node is searched, and for each E node the value of the best start node B is stored. The infor-mation about the best start node is used during trace back to find the path that maximizes the score, and in turn, the segment boundaries.
The method proposed in [15] consists of modeling each segment using the conventional multinomial model, assum-ing segment specific parameters are estimated using the usual maximum likelihood estimates with Laplace smoothing. In literature, this approach has often been used as a standard baseline and shown to deliver competitive results on several datasets [15, 7]. The second term intervening in the prob-ability of a segmentation is the penalty factor. In [15], it was optimized to log P ( S )=  X  m log( l d ) to yield the best performance.
LDA is a generative unsupervised topic model [3, 8]. In [3], the authors showed that the model can capture semantic in-formation from a collection of documents. They also inves-tigated the use of LDA for the task of text modeling, text classification and collaborative filtering.

This paper explores the use of topic modeling properties of LDA for the task of text segmentation. Our approach is based on the premise that using a topic model may allow bet-ter detection of segment boundaries because segment change should be associated with a significant change in the topic distribution.

LDA adopts the traditional view that texts are repre-sented as word count vectors, and relies upon a two step generation process for these vectors. A key assumption is that each document is represented by a specific topic dis-tribution and each topic has an underlying word distribu-tion . Gibbs sampling is used in this paper to train the LDA model [8].

In LDA model, the probabilistic generative story of a doc-ument is as follows: assuming a fixed and known number of topics, T ,foreachtopic t , a distribution  X  t is drawn from a Dirichlet distribution of order V ,where V is the vocabulary size of the training corpus. The first step for generating a document is to draw a topic distribution , X = {  X  t ,t =1 ...T from a Dirichlet distribution over the T -dimensional sim-plex. Next, assuming that the document length is fixed, for each word occurrence in the document, a topic, z ,ischosen from  X  and a word is drawn from the word distribution as-sociated with the topic z . Given the topic distribution, each word is thus drawn independently from every other word using a document specific mixture model.

Given  X , the likelihood of a document, represented as a count vector C ,isgivenby where C v is the count of word v in the document.
Being a generative model, LDA can also be used to make predictions regarding novel documents ( assuming they use the same vocabulary as the training corpus -vocabulary mis-match issue in LDA model is explained later ). As the topic distribution of a test document gives its representation along the latent semantic dimensions, computing this distribution is important in many contexts, including the present task of text segmentation. This computation can be performed us-ing the iterative procedure suggested in [10, 12], which relies on the following update rule where l d is the document length, computed as the number of running words.

As discussed in [12], this update rule converges monoton-ically towards a local optimum of the likelihood, and con-vergence is typically reached in less than 10  X  15 iterations. Once the  X  has been obtained for a document, the likelihood of the document can be computed by (3). This recently pro-posed step for computing  X  for unseen documents is key to computing the likelihood of a document. In this paper, we extend this idea to compute likelihood of a segment and use the estimated likelihood of segments as scores for performing the text segmentation task.

The LDA based method proposed in this paper is based on the following premise: if a segment is made up of only one story it will have only a few active topics, whereas if a segment is made up of more than one story it will have a comparatively higher number of active topics. Extending this reasoning further, if a segment is coherent (the topic distribution for a segment has only a few active topics), the log-likelihood for that segment is typically high, as com-pared to the log-likelihood in the case when a segment is not coherent [12]. This observation is of critical importance in the success of the proposed LDA based approach for text segmentation task, and has been left unexplored except for its original use in detecting coherence of a document [12]. It is thus tempting to use the log-likelihood of each possi-ble segment as a score in the DP algorithm and to recover the segmentation from the path that yields the highest log-likelihood.

The proposed LDA based approach for text segmentation task works like this: 1. For each possible segment, S i , 2. Substitute the scores of the segments in (2), and use The penalty factor we used is defined as log P ( S )=  X  p log( l d ), where p = 3 was empirically found to yield the best performance on some heldout dataset and used throughout.
The dataset used in this study is the Choi X  X  dataset ( http: //www.freddychoi.co.uk/ ), which has been used repeat-edly in benchmarking text segmentation algorithms. Choi X  X  dataset is derived from Brown corpus which consists of run-ning text of edited English prose printed in US during the calendar year 1961. Choi X  X  dataset is divided into 4 subsets ( X 3-5 X ,  X 6-8 X ,  X 9-11 X  and  X 3-11 X ) depending upon the num-ber of sentences in a segment/story. For example, in subset  X  X -Y X , a segment is derived by (randomly) choosing a story from Brown corpus, followed by selecting first N (a random number between X and Y) sentences from that story. Ex-actly 10 such segments are concatenated to make a docu-ment. Further, in each subset there are 100 documents to be segmented. By design, the segments are not complete stories.

From Reuters Corpus Volume 1 (RCV1) [11] collection, we selected 27,672 news items for training the LDA model ( ReutersTrain ). In these experiments, the number of topics ( T ) and Dirichlet priors (  X  and  X  )aresettothefollowing values: T = 50,  X  =1and  X  =0 . 01. A standard practice in the task of text segmentation is to assume that sentence boundaries are known [6, 5, 15]. We also make use of this information, that is, each sentence beginning is a possible B node and each sentence end is a possible E node.
In this section, we compare the results of the following segmentation systems (Table 1): (i) The results reported
Method Porter Stemmer P k ,in%(time,insec) Baseline Choi X  X  13 6 6 11 Baseline None 14 7 7 11 LDA None 22.5 15.4 13.1 15.5 Table 1: The performance of text segmentation algo-rithms on Choi X  X  dataset. in [15] (using Choi X  X  implementation of Porter stemmer), (ii) Our own implementation of Utiyama X  X  method, and (iii) LDA based segmentation.

Theresultsarepresentedintermsof P k , the probabilistic error metrics introduced in [1]. P k is the probability that two randomly drawn sentences which are k sentences apart are classified incorrectly. As in [6], k is set to the average segment length in our experiments. A lower value of P k indicates a higher accuracy in text segmentation.
The results reported in Table 1 suggest that the perfor-mance of both the approaches improves with an increase in the segment size. This is an expected result: longer segments allow a better estimation of the multinomial parameters for the baseline method and of the topic distribution for LDA.
Compared to LDA based approach, the baseline method is more accurate. An inspection of outputs gives a possible explanation for this: There is a serious mismatch in vocabu-lary between ReutersTrain dataset (used for LDA training) and Choi X  X  dataset used for testing. Similar issues of seman-tic mismatch were highlighted, for example, in [2], where the authors used a generic latent semantic space while per-forming text segmentation. The baseline method utilizes the full available vocabulary and all the content words (ex-cept the stop words that were removed before segmenting the text) for computing the score of a segment. In contrast, the vocabulary of an LDA model is defined by its training data. During segmentation, the content words in the data (text to be segmented) that are not present in the train-ing vocabulary are not used for computing the score of a segment. Comparing the baseline and LDA methods, ap-proximate loss in vocabulary and content words by LDA is 11.6% and 10.5% respectively.

To overcome this problem of vocabulary mismatch, we divided the Choi X  X  dataset into two parts. The first 50 doc-uments from each subset were put in SET A (50 documents * 10 segments/document * 4 sub-sets = 2000 segments) and the last 50 documents from each subset were put in SET B. SET A was used along with ReutersTrain to train the LDA model and SET B was used for testing. The results for the baseline and adapted LDA models for SET B are given in Table 2. The results present in Table 2 show that Table 2: The text segmentation performance on the SET B of Choi X  X  dataset by the baseline, Unadapted LDA (ReutersTrain) and Adapted LDA (Reuter-sTrain+Choi SET A) methods. The performance by several other methods for complete Choi X  X  dataset is also mentioned from their respective papers. the vocabulary mismatch was the main reason for the poor performance of LDA based method. The performance of the adapted LDA model is significantly better than that of the unadapted LDA model, and it also performs better than the baseline method (improvement is statistically significant with respect to the baseline). In fact, the performance of the adapted LDA model is better than any other unsupervised model previously reported in the literature for the text seg-mentation task on this benchmark [6, 15, 5, 7]. Noticeably, all these results are for the case when the algorithms used the information about the number of segments for getting the best performance, and in certain cases some data was used for adaptation [15, 7]. In the table, [7] has better per-formance than adapted LDA model for subset  X 9-11 X . This particular method introduced some extra variables in the DP algorithm to capture the local as well as global dynam-ics. These variables were first adapted over a train dataset and then the best performance was reported. Lastly, though our baseline method is also from [15], it does not utilize the information about the number of segments. That is why the results of first and fifth rows are different in Table 2.
In this study, we proposed an LDA based method for the text segmentation task. The proposed method computes topic distributions jointly with segmentation, thus allowing to collect information about the thematic content of each segment. This information can be used to keep track of recurring topics.

We investigated and compared the performance of our method with a standard approach often used as a baseline for the text segmentation task [15]), and analyzed its poten-tial strengths and weaknesses. The LDA based method gave a better performance than that of the baseline in adapted conditions (a small amount of data from the test domain is used along with a large amount out-of-domain data to train the LDA model). In fact, the proposed LDA based method outperformed every other unsupervised approach on Choi X  X  dataset.
This research was supported by the European Commission under the contracts FP6-045032-SEMEDIA , FP6-033715-MIAUCE and FP6-027122-SALERO .
