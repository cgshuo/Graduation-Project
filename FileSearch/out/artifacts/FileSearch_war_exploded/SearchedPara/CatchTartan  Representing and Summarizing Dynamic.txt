 Representing and summarizing human behaviors with rich contexts facilitates behavioral sciences and user-oriented services. Tradi-tional behavioral modeling represents a behavior as a tuple in which each element is one contextual factor of one type, and the tensor-based summaries look for high-order dense blocks by clustering the values (including timestamps) in each dimension. However, the human behaviors are multicontextual and dynamic : (1) each behav-ior takes place within multiple contexts in a few dimensions, which requires the representation to enable non-value and set-values for each dimension; (2) many behavior collections, such as tweets or papers, evolve over time. In this paper, we represent the behavioral data as a two-level matrix (temporal-behaviors by dimensional-values) and propose a novel representation for behavioral summary called Tartan that includes a set of dimensions, the values in each dimension, a list of consecutive time slices and the behaviors in each slice. We further develop a propagation method C ATCH TAN to catch the dynamic multicontextual patterns from the tem-poral multidimensional data in a principled and scalable way: it determines the meaningfulness of updating every element in the Tartan by minimizing the encoding cost in a compression manner. C
ATCH T ARTAN outperforms the baselines on both the accuracy and speed. We apply C ATCH T ARTAN to four Twitter datasets up to 10 million tweets and the DBLP data, providing comprehensive summaries for the events, human life and scientific development. H.3.5 [ Information Systems ]: Information Storage and Retrieval -On-line Information Services; J.4 [ Computer Applications ]: So-cial and Behavioral Sciences Behavior Representation; Behavior Summarization; Minimum De-scription Length
Behavioral representation and summarization is a fundamental component of behavioral scientific discovery: it supports the sys-tematic analysis and investigation of human behaviors. It is also a (a) Representing a behavior with dimensional values including non-value and set-values, instead of one guaranteed value.
 (b) Tartans in a  X  X wo-level matrix X : dimensions and values on the columns, time slices and behaviors on the rows.
 Figure 1: The representation and summarization of dynamic multi-contextual behaviors: it takes every behavior while the tensor fails. fundamental problem in many user-oriented applications for a bet-ter understanding of the event from news, human life from tweets, and the scientific development from publications. However, it is rather challenging for the following two characteristics of the be-haviors [10, 4]. (Terms and their definitions are given in Table 1.)
First, human behaviors are multicontextual : a behavior consists of one or multiple types (i.e., dimensions) of contextual factors [12], and it has one or multiple values in each dimension. Take the  X  X uper Bowl X  tweet in Figure 1a as an example: it has several dimensions such as the user, phrase, hashtag and shorten-URL, and this behavior has one user, one hashtag, several phrases and no URL. The publishing-paper behavior also has multiple values in the author, keyword and cited-paper dimensions. The representa-tion should enable different combinations of the dimensions and a non-value/set-value setting of the dimensional values.

Second, human behaviors are dynamic . They naturally evolve with the changing of personality, physical environment and so-Term Definition
Dimension The type of a contextual factor (Dimensional) value The contextual factor in the dimension Time slice The period for consecutive behaviors
Behavior A set of dimensions, a set of values in each
Table 1: Terms used throughout the paper and their definitions. cial interaction [34]. For example, the crowds predicted the score before the Super Bowl, commented the singers and bands at the half-time show, and expressed their happiness or sadness after the match. Therefore, the representation should make the behaviors sortable by their time dimensional values (i.e., timestamps), while the other dimensions are not required to be compared.

Traditional behavior modeling used the  X  X ensor X  [15, 30, 11, 9] to represent the multidimensional behaviors and proposed a great line of block detection methods [5, 17, 24] to capture the dense blocks as interesting patterns.
 Why not Tensor? FEMA [11] and C ROSS S POT [9] represented the tweets as (user, phrase, hashtag, URL) tuples and used the 4-mode tensor to define the tweet data. However, when the tweet has neither a hashtag nor a URL, it either has to be moved out or individually creates a 2-mode dense block. Therefore, the tensor representation either loses a large amount of such information or overweights the meaningfulness of the tweet.
 Why not Block Detection? SVD and tensor decompositions have been widely used for multidimensional clustering, subgraph mining and community/block detection [17, 24, 9]. However, they mix all the values into one dimension even including the timestamp values. Their blocks cannot select the meaningful dimensions; the grouped timestamps cannot capture the dynamic patterns.

In this paper, we propose novel representations for the behav-iors and summaries (see Figure 1b): a  X  two-level matrix  X  for the behaviors in which the columns are the dimensional values of the contextual types, and the rows are the behaviors in the time slices; a  X  Tartan  X  for the behavioral summary that includes (1) a set of meaningful dimensions, the meaningful values in each dimension to define the multicontextual patterns; and (2) a list of consecu-tive time slices and the representative behaviors in each slice to define the dynamic patterns. To address the problem of catching the Tartans (i.e., summarizing the behavioral data), we propose a propagation method called C ATCH T ARTAN that defines the mean-ingfulness metric of including or excluding a value, a dimension, a behavior and a time slice by leveraging the Minimum Description Length (MDL) principle. The general philosophy is that saving more bits in compression indicates a more important element in the Tartan. Moreover, C ATCH T ARTAN is carefully developed with sev-eral desired properties: it requires no user-defined parameters, runs in parallel and adapts to the dynamic environment.

Figure 2 shows seven of the Tartans that C ATCH T ARTAN catches from tweets about the  X  X uper Bowl 2013 X  event. They summa-rize its five phases such as the score prediction, first half, half-time show, second half and sentiments after a win/loss. The Tartans consist of different numbers of dimensions from 1 ( X  X hrase X ) to 3 ( X  X ocation-Phrase-Hashtag X , X  X hrase-RT@User-URL X ) and differ-ent consecutive time slices from 5pm, 8pm to 10pm, indicating the advertising campaigns, local trends and topical discussions.
It is worthwhile to highlight our contributions as follows.
Principled scoring
Parameter-free
Multidimensional
Multicontextual
Timestamp value
Dynamics Dataset #Tweet #User #Loc #Phrase #Hashtag #URL # RT @User #@User Time Period Dataset #Paper #Author #Venue #Keyword #Cited paper Time Period
Traditional approaches model behaviors in three ways: graphs, tensors/cubes and multidimensional itemsets. However, none of the above can represent the dynamic multicontextual patterns in the human behavioral data. Table 2 gives a visual feature-based comparison of C ATCH T ARTAN with the existing methods. Graph data summarization. Graph is common to represent the binary relations inside human behaviors. G RAPH S COPE [28] uses graph search for hard-partitioning of temporal graphs to find dense temporal cliques and bipartite cores. V O G [18] and T IME [27] use MDL to label subgraphs in terms of stars, (near) cliques, (near) bipartite cores and chains: the former approach works on static graphs, while the latter focuses on dynamic graphs. S B
URN [16] is a recursive node-reordering approach to leverage run-length encoding for graph compression. Toivonen et al. [31] uses structural equivalence to collapse nodes/edges to simplify graph representation. These approaches work on flat representations, while the behavioral dataset itself is naturally multidimensional. Tensor decomposition and cube analysis. Tensor decompositions [29, 17, 11] conduct multidimensional analysis; C OM CP/PARAFAC tensor decomposition with MDL. However, the ten-sor has a big flaw: it has to drop the behaviors in which some di-mension is missing. On the cube side, T OPIC C UBE [15] proposes a topic-concept cube that supports online multidimensional mining of query log. G RAPH C UBE [35] defines analysis cubes and OLAP operations on cubes over graphs. E VENT C UBE [30] performs mul-tidimensional search and analysis of large collections of free text. Our C ATCH T ARTAN proposes a totally different representation for the behaviors and it has a principled scoring function to select the dimensions for the summaries.
 Frequent pattern mining and multidimensional clustering. We can adopt the concept of itemsets in both the frequent pattern min-ing [20, 8] and multidimensional data clustering [23, 14, 1, 19] to represent the behavioral contexts. F. Cordeiro et al. [7] pro-poses B O W method for clustering very large and multidimensional datasets with M AP R EDUCE . However, the mixture of the dimen-sional values (itemsets in the above methods) kills the selectivity of meaningful dimensions and thus fails to describe the multicon-textual patterns. The timestamp clustering cannot describe the dy-namic patterns either.
 MDL theory and applications. Rissanen [25] proposes optimal encoding for integers greater than or equal to 1, which minimizes the description length one obtains estimates of the integer-valued structure parameters. Cilibrasi et al. [3] proposes a hierarchical clustering method on compression using the non-computable no-tion of Kolmogorov complexity. Faloutsos et al. [6] demonstrates that compression and Kolmogorov complexity can measure struc-ture and order. The MDL principle aims to be a practical version of Kolmogorov Complexity [21]. Vreeken et al. [32] uses the MDL principle to catch large groups of patterns essentially describing the same set of transactions in the data.

To summarize, our C ATCH T ARTAN is unique for its (1) novel representations for behaviors and summaries to capture dynamic multicontextual patterns; (2) principled scoring function with no user-defined parameters; and (3) scalable propagation algorithm.
In this section, we first introduce several datasets of behaviors and preliminarily analyze the multicontextual characteristic. Then we propose our representations for the behaviors and summaries, following by the problem definition of behavioral summarization. Datasets. We use four large Twitter datasets as well as the DBLP data (see Table 3). The tweets were collected from different sources: (1) NYC14 and LA14 were crawled using Twitter Streaming API from August 1 st to November 30 th 2014. The NYC14 dataset con-sists of 10 million tweets in New York and the LA14 consists of 0.4 million in the Greater Los Angeles Area. (2) SPB13 (Super Bowl 2013) and GRM13 (the Grammy Awards 2013) were collected by Techtunk 2 , each of which has over 2 million tweets. We extract many hard-encoded dimensions such as location, hashtag, URL, https://dev.twitter.com/streaming/overview http://www.techtunk.com/
Percentage of behaviors
Figure 3: The distribution of #dimensions in human behaviors. @User, as well as the rich phrase dimension. For the DBLP data, we have the author, venue, keyword and cited-paper dimensions. Dimension distributions. Given the number of dimensions in a behavior, Figure 3 shows the percentage of the behaviors of that many dimensions in the datasets. The most frequent number in all the datasets is 3: (User, Phrase, Location/Hashtag/URL) in the Twitter data, and (Author, Venue, Keyword) in the DBLP. The be-haviors are allowed to have various dimensions and for each di-mension, they are allowed to have multiple values (a few phrases or a few keywords). A specific behavioral intention shares a set of specific contextual factors and creates a pattern of specific dimen-sions and values. For example, advertisers often generate tweets of similar phrases and the same URL; local events often share a group of hashtags and phrases. This is so called  X  X ulticontextual X .
Now we know that the behavioral data include temporal and con-textual information, because every behavior has its timestamp and a set of contexts, or called dimensional values. On the contextual side, suppose the data have D dimensions, and for each dimension d  X  [1 ,D ] , there are N d values. On the temporal side, suppose the data can be divided into T time slices, and for each time slice t  X  [1 ,T ] , there are E ( t ) behaviors. The symbols are their defini-tions are given in Table 4.

Figure 1b has illustrated our proposed  X  X wo-level matrix X  to rep-resent human behaviors. The formal definition is as follows. A two-level matrix X consists of P D d =1 N d columns (dimensional values) and P T t =1 E ( t ) rows (behaviors), in which X notes how many times the i -th value in the d -th dimension appears in the b -th behavior at the t -th time slice. The top level consists of D dimensions and T time slices.

Note that our definition can represent any dimensional setting and any type of values including non-value and set-values. A be-havioral summary is a subset of the data that creates a representa-tive pattern. Specifically, the definition is as follows. havioral summary A has five components: The size of the first four components are denoted by 1  X  D 1  X  n d  X  N d , 1  X  T A  X  T and 1  X  e ( t )  X  E ( t ) .
As shown in Figure 1b and 4, the Tartan is named after its par-ticular shape in the two-level matrix.
 Symbol Definition
In this paper, the ultimate goal is to summarize the behaviors, in other words, to find the behavioral summaries in the temporal mul-tidimensional data. With the above representations for the behavior and summary, we define the problem of behavioral summarization as follows, equally as catching Tartans in the two-level matrix. havioral data (a two-level matrix) X = { D,N d | D d =1 ,T,E find a list of behavioral summaries (Tartans)  X  A = { ..., A ,... } ordered by a principled metric function f ( A , X ) which defines how well the sets of meaningful dimensions, values, time slices and be-haviors are partitioned and how well the meaningful subset of data is summarized, where A = {D , V d | d  X  X  , T , B ( t ) | t  X  X 
Good summarization including good partitions will be determined in an information-theoretic manner. We would like to emphasize that we solve the problem in a parameter-free and scalable way.
Our C ATCH T ARTAN method is based on the Minimum Descrip-tion Length (MDL) principle and employs a lossless encoding scheme for the temporal multidimensional data. Our objective function es-timates the number of bits that encoding the Tartan can save from merging this meaningful knowledge into the data. In this section, we will address the proposed problem in Section 3 by answering three questions: (1) how to derive the cost of encoding the Tartan; (2) how to define the principled scoring function for optimization; (3) how to develop a scalable algorithm to catch the Tartans.
Figure 4 illustrates the MDL-based scheme for encoding the five components of the Tartan. The components are (1) first-level columns (dimensions), (2) second-level columns (dimensional values), (3) first-level rows (time slices), (4) second-level rows (behaviors), and (5) the behavior-value entries.
 Encoding the dimensions. Suppose D = 5 and the set of dimen-sions in the Tartan A is D = { 1 , 2 , 4 } ( D A = 3 ), the binary string
Figure 4: Encoding the 5 components when encoding a Tartan. to encode the set is 1 1 01 0. The length of this string is D and the number of 1 s is D A . To further save space, we can adopt Huff-man coding or arithmetic coding to encode the binary string, which formally can be viewed as a sequence of realizations of a binomial random variable X . We denote by H D ( X ) the entropy: Additionally , two integers need to be stored: D and D A . The cost for storing these integers is (log  X  D +log  X  D A ) bits, where log is the universal code length for an integer x [26]. Therefore, the description length is where g ( x, y ) = x log x  X  ( x  X  y ) log ( x  X  y )  X  y log y,y  X  x ; x is the total number of values and y is the number of selected values. Encoding the dimensional values. For each dimension d  X  D , the binary string is of the length N d and has n d 1 s. Therefore, the entropy is The total description length is Encoding the time slices. The set of consecutive time slices in the Tartan A is T = [ t start ,t end ]  X  [1 ,T ] , where t end T A  X  1 . Thus, the description length is Encoding the behaviors. For each time slice t  X  T , the binary string is of the length E ( t ) and has e ( t ) 1 s. The entropy is The total description length is Encoding the entries in the Tartan. The entries in the Tartan A are non-negative counts instead of binary values. The volume, i.e., the length of the non-negative integer string, is The sum of the non-negative counts is It is straightforward to add bits in order to store the integer string as a binary string. For example, if the string is 2 1 0 4 0, it can be stored as 110 10 0 11110 0, where 1  X  X  X  10 encodes a non-negative integer x with x 1 s. Therefore, the binary string is of the length v + c and has c 1 s. The entropy is (a) Update the set of behaviors. (b) Update the set of values. (c) Update the consecutive time slices. (d) Update the set of dimensions. Figure 5: Updating the four elements of the Tartan till convergence: time slices, behaviors, dimensions and dimensional values. The description length is The entir e encoding cost of the Tartan A is L ( A ) = L D ( A ) + L V ( A ) + L T ( A ) + L B ( A ) + L The goal is to find the Tartan with high  X  X eaningfulness score X . The scoring function is defined as the number of bits (description length) that encoding the Tartan A saves from encoding every indi-vidual entry in the first-level matrix X A : where X A \A is the individual entries in X A except the Tartan A . Encoding the individual entries in the first-level matrix. X includes every value from the dimension in the set D and every behavior from the time slice in the set T : X The volume of this first-level matrix is Its sum of the non-negative counts is Therefore, the description length of X A is Gi ven the A and X A , the #bits to encode the individual entries is
Our proposed scoring function encodes different partitions in-cluding the dimensions, dimensional values, as well as the time slices and behaviors in the time slice, in order to achieve a concise description of the data. The fundamental trade-off that decides the  X  X est X  summaries is between (1) the number of bits needed to de-scribe the Tartan, and (2) the number of bits needed to describe the individual entries in the data.
 Properties. We list several good properties that agree with intu-ition of the function f ( A , X ) , which directs us to a propagation algorithm that updates the Tartan for a high score. These properties are proved in the Appendix.
 Property 1. A Tartan of a higher sum c saves more bits, when other variables are fixed (which is assumed for all properties). Property 2. A Tartan of a smaller volume v saves more bits. Property 3. The first-level data of a smaller sum C saves more bits. Property 4. The data of a bigger volume V saves more bits.
We propose a greedy search algorithm for optimal partitions in the Tartans. However, finding the optimal solution is NP-hard So we present an iterative alternating optimization where we find the optimal set of dimensions, values, time slices and behaviors while holding other variables in the Tartan. We run this sequence of updates until convergence. The algorithm is scalable to run on multiple threads sharing the memory of the dataset.
 Algorithm 1 C ATCH T ARTAN : Catching the dynamic multicontex-tual Tartans for behavioral summaries Require: the behavioral data X = { D,N d | D d =1 ,T,E ( t ) 1:  X  A = {} 2: while the threads run do 3: generate a seed Tartan A = {D , V d | d  X  X  , T , B ( t ) 4: while not converged do 5: for each time slice t  X  X  = [ t start ,t end ] do 6: Update the set of behaviors B ( t ) (see Figure 5a) by 7: end for 8: for each dimension d  X  X  do 9: Update the set of values V d (see Figure 5b) 10: end for 11: Update the consecutive time slices: check if includes 12: for each dimension d /  X  X  do 13: Check if includes the dimension (see Figure 5d) 14: end for 15: end while 16:  X  A X   X  A X  X  sorted in descending order by f ( A , X ) 17: end while 18: return  X  A : the list of Tartans in X Seed selection. We recommend three ways of generating seed Tar-tans: (1) one or several random behaviors in a single time slice, (2) several popular dimensional values, and (3) high-order SVD on the partial data. Experimental results in Section 5 show that the first, simple setting performs well and runs fast.
 Complexity. The properties with guarantees ensure that the top behaviors in B ( t ) and top dimensional values in V d . Intuitively, a higher score looks for a better compression, i.e., few behaviors/values in the time slice/dimension that give large sums of counts. There-fore, the optimization can be solved via a quick sorting of the val-ues. The time complexity is O ( P d N d log N d + P t E ( t )
In this section, we evaluate C ATCH T ARTAN and seek to answer the following questions. Can it accurately catch the Tartans from
It is NP-hard since, even allowing only column re-ordering, a re-duction to the TSP problem can be found [13]. the temporal multidimensional data? Is it scalable? For real-world behaviors, are their patterns dynamic and multicontextual? If they are, what Tartan structures do we see and what do they mean?
It is impracticable to evaluate the behavioral summaries in real datasets. Thus, we generate the synthetic datasets and report the quantitative results.
 Synthetic datasets and experimental setup. We generate random  X  X ehavioral X  data and inject a Tartan into it. We set up extensive experiments with many parameters on (1) the Tartan distribution: 1. T A  X  [2 , 9] , the number of consecutive time slices in the 2. e ( t )  X  [100 , 2 , 000] , the number of behaviors in the time 3. D A  X  [2 , 9] , the number of dimensions in A , 3 as default; 4. n d  X  [50 , 200] , the number of values per dimension in A , 5.  X   X  [1 , 10] , the average number of values per dimension in and (2) the data distribution: 6. T  X  [5 , 30] , the total number of time slices in the dataset, 10 7. E ( t )  X  [1 , 000 , 10 , 000] , the number of behaviors per time 8. N d  X  [1 , 000 , 2 , 000] , the number of values per dimension
Our task is to catch the Tartan, which has two binary classifica-tion subtasks: (1) detecting the set of behaviors in the Tartan, (2) detecting the set of dimensional values (contexts) in the Tartan.
We adopt the following methods as the baselines: The experiments were conducted on a machine with 20 cores of Intel(R) Xeon(R) CPU E5-2680 v2 @ 2.80GHz. We set up 10 threads and each thread searches the Tartans with 2 seeds.
Note that none of the above methods selects the dimensions nor time slices. The tensor-based methods including decompositions [29, 11] and the C ROSS S POT [9] fail to represent the multicontex-tual data. They lose lots of information and gave poor performances in the Tartan detection, so we do not show their results in this paper.
We evaluate the performance of our C ATCH T ARTAN and the baselines from two perspectives, (1) accuracy: F1 score that is the harmonic mean of precision and recall, (2) efficiency: the cost of running time. A high F1 score indicates accurate performance and a small time cost indicates high efficiency.
 Accuracy and efficiency. Figure 6 presents the extensive experi-mental results. As every parameter varies in a big range, it shows the accuracy (on the 1st and 3rd columns in the figure) and the time cost (on the 2nd and 4th columns) of our C ATCH T ARTAN and the baseline methods. The solid lines are for the behavior detection, and the dashed lines are for the dimensional value detection. The symbols of our C ATCH T ARTAN is the red triangles. We have the following observations. Figure 7: Our C ATCH T ARTAN takes fewer than 5 iterations in over 90% runs; it requires 4 seeds to reach a higher-than-0.92 F1 score. The robustness to the number of iterations and seeds. Figure 7a shows that over 92% of the processes of catching the Tartans take fewer than 5 iterations. In Figure 7b, we spot that in the default setting, C ATCH T ARTAN requires only 4 seeds to reach an as-high-as-0.92 F1 score for both the detection tasks of behaviors and di-mensional values. C ATCH T ARTAN requires no user-defined param-eters, and it is robust to the number of iterations and seeds. In this section, we discuss qualitative results from applying C TAN to the tweet datasets and DBLP data mentioned in Table 3.  X  X uper Bowl 2013 X  event summaries. We have introduced Fig-ure 2 in which the seven of the Tartans summarize the behavioral patterns in the data. They present five phases of the event such as the score prediction, first half, half-time show, second half and sen-timents after a win/loss. The Tartans consist of different numbers of dimensions and different consecutive time slices, which indicates the advertising campaigns, local trends and topical discussions.  X  X rammys Award 2013 X  event summaries. Figure 9 presents ten Tartans caught by our method from the GRM13 data. The Tartans have meaningful dimensional settings: (User, Phrase, @User) in-Figure 9: Tweet summaries of the GRM13 (Grammys): Tartans of different consecutive half-hours and different dimensions. Figure 10: Tweet summaries of the LA14 (Los Angeles): Tartans of different consecutive days and different dimensions. dicating that a group of users communicated with each other about the same words, (User, Phrase, Hashtag) indicating that the users discussed similar topics with a group of phrases; besides these three-dimensional Tartans, there are two four-dimensional ones. All these Tartans include a list of consecutive time slices for their dynamic behavioral patterns.  X  X os Angeles 2014 X  tweet summaries. Figure 10 presents eight Tartans from the LA14 data. The Tartans have various dimensional settings: three are five-dimensional, (User, Location, Phrase, Hash-tag, URL) for well-designed advertising campaigns; two are four-dimensional, (User, Location, Phrase, Hashtag) for local topics and (User, Location, Phrase, URL) for advertisements; one is three-dimensional, (User, Location, @User); and two are two-dimensional, (User, Location). These Tartans show not only the multicontextual view but also the dynamic patterns for 3 days, 5 days or even one week. C ATCH T ARTAN provides comprehensive behavioral sum-maries about the information at the Greater Los Angeles Area.  X  X ew York City 2014 X  tweet summaries. Figure 8 takes more space to introduce four Tartans in the NYC14 data. The first Tartan has four dimensions. It has 18 users from 9 locations, talking about 42 phrases and 19 hashtags during 4 days in August, 2014. It has as many as 1,734 tweets. The tweets show that this Tartan encodes a campaign that promotes the band 5SOS (5 Seconds of Summer).
The second Tartan has 1,632 tweets with 22 users, 8 locations and 26 phrases. This 3-dimensional Tartan also takes 4 days but in October, 2014. The tweets encode the positive sentiments of the New York citizens.

The third Tartan has four dimensions, user, location, phrase and @user. The volume is quite small: only 2 users, 3 locations, 11 phrases and 3 users who were mentioned, but the sum of tweet counts is big. The 1,585 tweets were generated from November 14 to November 18, which promoted the EP by the band TKLband (The Killing Lights). The messages are too similar to be generated by the two  X  X egitimate X  users: they are high probably created by some scripts. We even doubt whether the message  X  X reorder our EP for only $3.99 X  is true or false or fraudulent.

The last example has five dimensions: 2 users, 1 location, 4 phrases, 16 hashtags and 10 URLs. The messages are a large group of interesting news about job hunting in the Manhattan, NY. The number of the messages is 80. It looks small but the messages are from a even smaller number of users and locations. Compressing these messages as a whole can save lots of bits from compressing the messages individually.  X  X BLP X  summaries. Figure 12 presents seven (but not the least) Tartans in the DBLP data. They are shown for their relatedness to the area of database, data mining, machine learning and so on.
The first Tartan has four dimensions, author, venue, keyword and cited paper. From 2003 to 2007, 76 authors published papers about  X  X nformation retrieval X ,  X  X ata integration X  and  X  X ext classification X  on the SIGIR, VLDB conferences and TKDE journal. The paper p56743 ( X  X  language modeling approach to information retrieval X ) was frequently cited by these papers. The authors have all read this paper and explored new techniques to address the problems.
The second Tartan has only two dimensions, the venue and key-word. There were 40 papers about  X  X einforcement learning X  on the ICML and NIPS conferences from 1997 to 2002. The author and cited-paper dimensions are missing as many scholars cited many related papers in their publications. The Tartan automatically ex-cluded both the dimensions because no leading authors nor cited papers could be summarized.

The third Tartan has three dimensions, author, venue and cited paper. A group of six famous researchers, e.g., Dr. Jiawei Han and Dr. Xifeng Yan, published 22 papers on the SIGMOD conference which cited the same paper p76095 ( X  X requent subgraph discov-ery X ) from 2004 to 2010. The 22 papers used different phrases be-cause the area of  X  X ubgraph mining X  or  X  X requent subgraph pattern mining X  was promising but not mature: many different methods, models and algorithms were proposed.

The forth Tartan has two dimensions, venue and keyword. 25 papers about  X  X nomaly detection X  were published in the ICDM, AAAI conferences and TKDE journal from 2005 to 2013. The papers were written by a large number of authors to address the detection problem in many applications with different methods.
The fifth Tartan is relatively large. It has 3 dimensions of 27 au-thors, 6 venues and 12 keywords. These 70 papers were published from 2006 to 2013. The group of researchers studied the  X  X arge graphs X ,  X  X ata streams X ,  X  X volving data X  and  X  X volving graphs X  on the KDD, ICDM, ICDE conferences and TKDE journal.

The sixth Tartan presents the behaviors by the  X  X eb search X  community. It has four dimensions. A group of 12 authors stud-ied 3 keywords  X  X eb search X ,  X  X lick-through data X  and  X  X ponsored search X  and published 32 papers on the SIGIR, WWW, WSDM, CIKM conferences from 2006 to 2013. These conferences are the major information retrieval conferences. There are 12 highly-cited papers. The most representative paper is p82630 ( X  X ptimiz-ing search engines using clickthrough data X ).

Finally, the small but meaningful Tartan represents the  X  X ransfer learning X  community. 8 authors, e.g., Dr. Qiang Yang and Dr. Dou Shen, published 17 papers about  X  X ransfer learning X  and  X  data min-ing X  on the KDD and AAAI conferences. This is a 3-dimensional Tartan during the year 2007-2010.
 Efficiency. Figure 11a shows the distributions of the number of iterations until convergence when we apply C ATCH T ARTAN ferent datasets: C ATCH T ARTAN takes fewer than 15 iterations, and the most frequent number is consistently smaller than 10. Fig-ure 11b shows that the time cost is linear in the number of behaviors in the real-data experiments, which demonstrates the scalability.
In this paper, we uncovered the dynamic and multicontextual pat-terns of human behaviors and focused on the problem of behavioral summarization. We proposed a novel representation called the Tar-tan that includes a set of dimensions, sets of dimensional values, consecutive time slices, and sets of behaviors in the slices. We proposed a parameter-free and scalable method C ATCH T ARTAN capture the Tartan summaries with a principled scoring function. We applied our C ATCH T ARTAN to the synthetic data, DBLP data and Twitter datasets. The experimental results including the com-prehensive event summaries have demonstrated the effectiveness and efficiency of our proposed C ATCH T ARTAN .
We thank the reviewers for their insightful comments. This work was sponsored in part by the U.S. Army Research Lab. under Co-operative Agreement No. W911NF-09-2-0053 (NSCTA), National Science Foundation CNS-1314632, IIS-1408924, IIS-1017362, IIS-1320617, IIS-1354329, HDTRA1-10-1-0120, and grant 1U54GM114838 awarded by NIGMS through funds provided by the trans-NIH Big Data to Knowledge (BD2K) initiative (www.bd2k.nih.gov), and MIAS, a DHS-IDS Center for Multimodal Information Access and Synthesis at UIUC. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Laboratory, the U.S. Government, or other funding parties. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on. [1] R. Agrawal, J. Gehrke, D. Gunopulos, and P. Raghavan. [2] M. Araujo, S. Papadimitriou, S. G X nnemann, C. Faloutsos, [3] R. Cilibrasi and P. Vitanyi. Clustering by compression. [4] P. Cui, H. Liu, C. Aggarwal, and F. Wang. Computational [5] L. De Lathauwer, B. De Moor, and J. Vandewalle. A [6] C. Faloutsos and V. Megalooikonomou. On data mining, [7] R. L. Ferreira Cordeiro, C. Traina Junior, A. J.
 [8] J. Han, H. Cheng, D. Xin, and X. Yan. Frequent pattern [9] M. Jiang, A. Beutel, P. Cui, B. Hooi, S. Yang, and [10] M. Jiang, P. Cui, and C. Faloutsos. Suspicious behavior [11] M. Jiang, P. Cui, F. Wang, X. Xu, W. Zhu, and S. Yang. [12] M. Jiang, P. Cui, F. Wang, W. Zhu, and S. Yang. Scalable [13] D. Johnson, S. Krishnan, J. Chhugani, S. Kumar, and [14] K. Kailing, H.-P. Kriegel, and P. Kr X ger. Density-connected [15] D. Kang, D. Jiang, J. Pei, Z. Liao, X. Sun, and H.-J. Choi. [16] U. Kang and C. Faloutsos. Beyond  X  X aveman communities X : [17] T. G. Kolda and B. W. Bader. Tensor decompositions and [18] D. Koutra, U. Kang, J. Vreeken, and C. Faloutsos. Vog: [19] H.-P. Kriegel, P. Kr X ger, M. Renz, and S. Wurst. A generic [20] M. Kuramochi and G. Karypis. An efficient algorithm for [21] M. Li and P. Vit X nyi. An introduction to Kolmogorov [22] S. Padmanabhan, B. Bhattacharjee, T. Malkemus, [23] L. Parsons, E. Haque, and H. Liu. Subspace clustering for [24] B. A. Prakash, A. Sridharan, M. Seshadri, S. Machiraju, and [25] J. Rissanen. Modeling by shortest data description. [26] J. Rissanen. A universal prior for integers and estimation by [27] N. Shah, D. Koutra, T. Zou, B. Gallagher, and C. Faloutsos. [28] J. Sun, C. Faloutsos, S. Papadimitriou, and P. S. Yu. [29] J. Sun, D. Tao, and C. Faloutsos. Beyond streams and [30] F. Tao, K. H. Lei, J. Han, C. Zhai, X. Cheng, M. Danilevsky, [31] H. Toivonen, F. Zhou, A. Hartikainen, and A. Hinkka. [32] J. Vreeken, M. Van Leeuwen, and A. Siebes. Krimp: mining [33] F. Wang, T. Li, X. Wang, S. Zhu, and C. Ding. Community [34] T. Zhang, P. Cui, C. Song, W. Zhu, and S. Yang. A multiscale [35] P. Zhao, X. Li, D. Xin, and J. Han. Graph cube: on Proofs of the Properties in Section 4. Suppose the partitions have much smaller encoding cost than the data entries, the scoring func-tion can be written as f ( A , X ) = g ( V + C,C )  X  g ( v + c,c )  X  g ( V + C  X  v  X  c,C  X  c ) . Since g 0 ( x ) = log x  X  log ( x  X  y ) and g 0 ( y ) = log ( x  X  y )  X  log y , we have the derivatives of the function as follows.  X  X  In a summary, the density of the Tartan is higher than the data: &gt; C V . Thus, we obtain that  X  X   X  X  &gt; 0 (Property 1), (Property 2),  X  X   X  X  &gt; 0 (Property 3), and  X  X   X  X  &lt; 0 (Property 4).
