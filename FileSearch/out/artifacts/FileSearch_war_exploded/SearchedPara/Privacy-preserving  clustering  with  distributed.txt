
Generally distributed knowledge discovery is viewed as an optimization X  X ata is distributed and the traditional method of building a centralized data warehouse is costly. Distributed data mining methods can offer savings in processing time through use of the inherent parallelis m in a distributed system, storage cost because the data doesn X  X  need to be copied and the human cost of integrating data into a warehouse.
Privacy and security concerns provide another motive for distributed knowledge discovery. Often, data is distributed because it has been collected or produced by different parties. Contractual or regulator y controls on privacy can prevent release of the data. Trade secrecy concerns may outwe igh the perceived benefit of global data mining. In these cases, building a centralized data warehouse is impossible, as no mining is the only alternative.

For example, a corporation may cluster its customers to identify different groups to target in marketing campaigns. Now imagine that a multinational corporation would like to develop a global advertising campaign. Building a data warehouse of worldwide customers would enable the desired clustering, but privacy laws may prevent transferring customer data across borders (Pri 2001; Blackmer and Wilmer,
Cutler, Pickering 1998). Clustering within each country doesn X  X  give the knowledge needed to develop a global campaign. Distributed clustering is the only solution, provided it can be done without violating the privacy laws restricting transborder flow of customer data.
 distributed data mining problem: 1. What is the data mining task? Perhaps the easiest way to answer this question 2. How is the data distributed? Simple examples include horizontal partitioning , 3. What are the security constraints? Privacy regulations may prevent disclosure of
This paper addresses (1) the task of clustering, where (2) the distributed data is horizontally partitioned, and (3) security constraints prevent sharing any information that can be traced to an individual site.
 tion (EM) mixture model from distributed sources. EM mixture clustering (Dempster, troids at each iteration. Over time, these converge to good cluster centers. We show ing which portion of the model came from which site. We assume only that their be able to learn individual data points or which portion of the model came from which site.
 ponding to partitions of the data. Each partition can be computed locally, based on the local data points and global information from the previous iteration. The global sum is then computed without revealing the individual values. This provides sufficient information to compute the global information needed for the next iteration. Once this process converges, the individual sites can use the resulting model to determine in which cluster their data values lie.
 how to distribute EM mixture modeling and use this to produce a secure clustering methodology. Section 3.2 addresses evaluating the stopping criterion for the EM algorithm in a distributed setting. Section 4 provides an analysis of what is disclosed by the method.
First, we give some background on secure mu ltiparty computation and discuss other approaches to privacy-preserving data mining. This is followed with a brief intro-duction to EM mixture modeling.
Yao X  X  millionaire X  X  problem (Yao 1986) succinctly captures the problem of secure multiparty computation. Suppose two m illionaires want to find out who is worth more, but neither want to reveal their worth? More generally, the goal is to compute a global function, without any party learning anything except their own input and since extended to the multiparty case by Gol dreich, Micali, and Wigderson (1987). One key issue in the above definition is what is meant by learning anything .
At first glance, this would seem to imply that no communication is allowed be-cause wouldn X  X  any communication tell the parties something? The formal definition is based on distributions of random data. Each party should be able to construct a data distribution from a random distribution, its own input and the global out-put that is computationally indistinguishable from the data distribution of messages exchanged in runs of the secure multiparty computation. This shows that what is learned in the actual run can be modeled with only local information and the global result, therefore nothing has been learned by the data exchanged in the computa-tion.

Another issue is what the parties may do to defeat the protocol. The two models used in secure multiparty computation are semihonest (also called honest but curious) and malicious. A semihonest party is required to follow the protocol but may try to deduce private information from what it sees during execution of the protocol.
This is insufficient for most practical purposes. The malicious model solves this, requiring that, regardless of the actions of the malicious party, it learns nothing but some input from the malicious party or knows that the other party is malicious. To elaborate, there is no way to detect if the malicious party just gives wrong input, but otherwise behaves honestly X  X his is equivalent to an honest party that really had that input. There is also no way to prevent a malicious party from stopping the protocol at any point (say, after learning the final result but before the honest party learns the result). However, in this case, the honest party knows the other is malicious. This is more than is needed to preser ve privacy. We instead define a noncolluding majority standard. Parties may arbitr arily cheat, but as long as at most a minority collaborate honest parties may obtain bad results and not be able to detect the malicious parties, enables more efficient solutions than the secure multiparty computation malicious model.

There are still two issues to address before applying secure multiparty computa-tion to a practical problem. 1. Is it sufficient? Secure multiparty computation assumes that every site learns the global result. However, we still must ensure that this global result does not reveal protected informati on. Just because the computation is secure doesn X  X  mean the result is. 2. Is it necessary? The general method (Yao 1986; Goldreich et al. 1987) is not practical for large inputs. However, relaxing the security constraints may enable efficient solutions that still meet practical privacy and security requirements.
Section 4 addresses these issues for the method presented in this paper. We show that the EM mixture model generated does not disclose individual data values or release information that can be traced to a specific site. The method does not meet the definition of secure multip arty computation, as each iteration reveals some in-formation. However, because this information does not reveal individual values or specific site information, it does meet the security constraints of our problem. curity constraints. Lindell and Pinkas showed how to construct decision trees under secure multiparty computation constraint s (Lindell and Pinkas 2000). More recently, association rule mining has been addressed in both horizontally partitioned (Kantar-data. Secure K -means clustering has b een addressed, but only where the data is verti-cally partitioned, i.e. each dimension is com pletely contained at one site (Vaidya and Clifton 2003). To our knowledge, this is the first work to address secure distributed
Clustering, where the data is horizontally partitioned (each site contains complete information about a set of entities.)
The expectation maximization (EM) algorithm is an iterative method based mainly on the maximum likelihood principle. Since Dempster, Laird, and Rubin X  X  celebrated paper on the EM algorithm (Dempster et al. 1977), it has become a very popular method in the AI and statistics community. More details on the EM algorithm and mixture models can be found in McLachlan and Basford (1988); McLachlan and Krishnan (1997); McLachlan and Peel (2000).
 drawn from a population with density function f ( y ;  X ) .
 parameters. The observed data log likelihood is
The maximum likelihood principle says that the estimators that maximize the data impossible to find analytical solutions. The EM algorithm is an iterative procedure to find the  X  that maximizes log L ( X ) by data augmentation. The observed data y are augmented by the missing value z that contains group information of the observed data. More specifically, z = ( Z 1 ,  X  X  X  , Z n ) ,where Z means data point j belongs to the i th component. For instance, Z means that the j th data point belongs to component 1. x = data with density function f c ( x ;  X ) . The complete-data log likelihood is
Typically, the complete-data likelihood has a simpler structure and its expected likeli-hood can be maximized analytically. Dempster et al. (1977) proved that by maximiz-for each iteration step, which guarantees co nvergence of the algorithm. The EM algo-rithm takes advantage of this and solves the maximum likelihood problem iteratively. The algorithm contains two steps:
E-Step: On the ( t + 1 ) st step, calculate the expected complete-data log likelihood
M-Step: Find  X  ( t + 1 ) to maximize G ( X  ;  X  ( t ) ) .
The algorithm stops when log L ( X  ( t + 1 ) )  X  log L ( X  ( old.

In this paper, we focus on an EM algorithm for finite normal mixtures, as is widely used in the data mining community. Assume a mixture of k components (clusters), with a d -dimensional data set y of size n . Assume further that the unknown parameters are  X  = ( X  1 ,... , X  k , X  1 ,... , X  k , X  1 ,... , X  model is where f i ( Y ;  X  i ) is the normal density:
Let component information Z i , j , representing that the j th data point belongs to com-ponent i , be missing data. The complete-data log likelihood is
It is clear that, for the ( t + 1 ) st iteration, G maximized in each iteration step to obtain a good estimator for
Based on the framework of the previous section, we present a privacy-preserving EM algorithm that utilizes the linearity of parame ter estimators in each iteration step dur-ing the EM algorithm. Section 3.1 gives the general algorithm. Section 3.2 discusses evaluation of stopping criteria and Sect. 3.3 explains the connection between the al-gorithm and privacy. For clarity, the conventions for notation of the paper are given in Table 1. Table 2 defines the meaning of indexes. Different versions of EM algorithms have been proposed during the past 25 years.
Examples include Meng X  X  SEM (Meng and Rubin 1991), a stochastic version of the EM algorithm by Celeux (Celeux, Chauveau and Diebolt 1996), and Mclust by
Banfield (Banfield and Raftery 1993; Fraley and Raftery 1998). In this section, we follow the classical EM steps and relate the results to privacy-preserving clustering. can be found by computing the zeros of  X  G ( X  ;  X  ( t ) )/ X  X  updates at the ( t + 1 ) th iteration:
Assume the data ( y j ) are partitioned across s sites (1 data items; the total number of items n = s l = 1 n  X  where will show that these items do not reveal individual data values or the respective grouping information. Furthermore, it is not necessary to share n from site to site as the parameter updates only require the global sums over different sites. Appendix A shows how to compute these summations securely, in the secure multiparty computation sense.

After the global parameters ( X  ( t + 1 ) i ,  X  ( t + 1 ) i wise, z ijl can be computed locally as where y jl is a data point at site l . Algorithm 1 summarizes the method. Algorithm 1 Secure EM algorithm
The number of values communicated at each step is 3
This is quite reasonable, particularly as it is constant in n and thus scales well with data size.

Usually convergence in an EM mixture algorithm is defined as where and is a predetermined threshold. The sum can be partitioned among the sites, where
Using the secure sum protocol in Appendix A, D l can be computed as the sum of the locally computed log L ( t ) l . The master site can then check the stopping criterion, i.e. to see whether the algorithm has converged. Once the stopping criterion is met at
Through our analysis, linearity of the G ( X  ;  X  ( t ) ) at each site, then combine them through a secure summation.

EM step. Thus, the general properties of a finite mixture model using the EM algo-rithm still hold, which guarantees the convergence of the algorithm. We have also empirically validated that the results generated by this method are comparable to a well-known EM mixture model, FastMix (Moore 1999).
 have assumed a priori knowledge of the number of clusters, k . Researchers have been using criteria such as Bayesian information criteria and minimum description length to select this value. The basic idea is to fit data into mixtures with different numbers of normal components (clusters), then choose the one with the largest convergent convergent log likelihood across the sites can be computed and compared under different model assumptions.
The goal of this method is to develop an EM mixture model without: 1. Disclosing individual data values beyond the site containing those items or 2. Revealing any information t hat can be traced to a specific site
Without applying the secure summation algorithms, values of n sample mean and covariance matrices at site l by
With these quantities, confidence intervals for the true mean and covariance matrix can by derived for each site. However, by u sing the secure summation protocol of
Appendix A, only the global values A i , B i , C i , n and L are revealed. This results in revealing only the global values  X  i and  X  i . From these values, it is not possible to deduce confidence intervals w.r.t. the mean and covariance matrix for component i at the local sites.

Furthermore, if  X  il and  X  il for every site l are shared across all the sites, the probability that a data point y belong to a specified interval can be calculated at each site l as where  X  is the usual cumulative function of normal distribution. By comparing the probabilities across every site l , it is possible to deduce to which site the data point y belongs. Revealing this would violate the privacy constraints. When only the global values of  X  i and  X  i are disclosed, these inductions are not possible.
We now address whether the revealed values themselves can be used to deduce any information on individual data items. tifiable information. Provided there are more than two sites, it reveals only an upper bound on the number of items at any given site X  X ost likely innocuous information.

A number for each component, independent of the number of data values or sites.
Thus, by itself, it does not reveal restricted information. Even if a component contains only a single data item (and thus the cluster center converges to that item), no site can know that this is the case.

B is constructed from data from a previous iteration, along with knowledge of the local number of items n l . Because it is a single value for each component, n not revealed.

C use individual data values, but again these are distilled into a single value for each component, as with A i .
 tied to an individual site or data item.
 Thus, a single iteration reveals no restricted information.
 that should not be disclosed? To address t his question, assume, without loss of gen-erality, that s new data points are assigned to component i . From the mean and variance of steps t and t + 1, we have
Clearly, when s &gt; 1, these two equations have infinite solutions for y
In other words, values from previous iterations will not reveal any information that is not already revealed by step t + 1 alone.
 summation prevents us from knowing which site is responsible for a change in values between iterations or even how many sites are responsible.
 while no individual values are disclosed, a dishonest party could learn how data clusters on the honest party. Simple input modification allows this X  X et n and participate in the protocol normally. The result is an EM mixture model based only on the honest party X  X  data, with  X  i ,  X  i and result in the same problem. However, the method can be extended to be secure with an honest majority as described in Appendix A.
We have presented a clustering method based on expectation maximization that limits the disclosure of data between sites in a distributed environment. Specifically, 1. The values of individual data items are not disclosed. 2. No information can be traced to a specific site.
 tering even when data sharing is constrained.
 method is to generate the values A ( t + 1 ) i , B ( t + late  X  ( t + 1 ) i ,  X  i ( t + 1 ) and  X  ( t + 1 ) i , the local log likelihood estimates L  X  ,  X  i and  X  i from the central site to the distributed sites. Thus, the communication cost for each iteration scales as O ( ks ) ,where k is the number of clusters and s is the of convergence, and thus the number of iterations, is independent of the distributed aspect of the problem.
 secure multiparty communication sense. This requires showing how the iteration can be performed without learning the results of any but the last iteration. This would en-able a more formal proof than the arguments given in Sect. 4. However, the practical benefit is questionable, as secure multiparty computation is neither necessary , as it prevents the disclosure of innocuous information that could enable a more efficient algorithm, or sufficient , as the result combined with a site X  X  own input may disclose restricted information.

This comes back to the problems described at the beginning of Sect. 4. Without a for-mal definition of what constitutes acceptabl e and unacceptable disclosure, a formal proof is meaningless. Security policy is today specified with informal descriptions, a practical formal security semantics is still an open problem.

One approach is to reduce the final result to a minimal model that does not dis-close any unneeded information. Assuming that the goal of clustering is that each mixture model result (which represents cluster centers) provides unneeded informa-tion. A minimal result would simply pro vide each party with a mapping from their own items to cluster numbers. While sufficient, such a solution is unlikely to be necessary and the benefit must be weighed against the likely extra communication, computation and complexity costs. This would address the two-party case: A solu-tion secure under such a definition would not reveal anything to a dishonest party simulating no input, although other attacks may be possible.

There are many open problems in privacy-preserving data mining. Section 2.1 discusses some of the problems that have been addressed, but there are many more that have not. In addition to distributed privacy issues, there has also been work on preserving privacy by distorting the data values, while still allowing data mining. This has as of yet only been applied to decision trees (Agrawal and Srikant 2000; Agrawal and Aggarwal 2001) and association rules (Rizvi and Haritsa 2002).

Distributed knowledge discovery has many benefits. Enabling data mining that would otherwise be prevented due to privacy and security constraints is a key benefit and is worthy of further exploration.
 This method frequently needs to calculate the sum of values from individual sites.
Assuming three or more parties and no collusion, the following method (from Be-naloh (1986)) securely computes such a sum.

Assume that the value v = s l = 1 v l to be computed is known to lie in the range [ 0 ... n ) .

One site is designated the master site, numbered 1. The remaining sites are num-bered 2 ... s . Site 1 generates a random number R , uniformly chosen from
Site 1 adds this to its local value v 1 and sends the sum cause the value R is chosen uniformly from [ 0 ... n ) , the number also distributed uniformly across this region, independent of the value of fore, site 2 learns nothing about the actual value of For the remaining sites, l = 2 ... s  X  1, the algorithm is as follows: Site l receives
Again, this value is uniformly distributed across [ of v j ,so l learns nothing about the values of and passes it to site l + 1.
 can subtract R to get the actual result. Note that site 1 can also determine by subtracting v 1 . However, given only the final sum, any site can determine the and one X  X  own input, it does not represent an information leak from the algorithm. Figure 1 depicts how this method operates.
 compare the values they send/recei ve to determine the exact value for can be extended to work for an honest majority. Each site divides sum for each share is computed individually . However, the path used is permuted for each share such that no site has the same neighbor twice. To compute neighbors of l from each iteration would have to collude. Varying the number of shares varies the number of dishonest (colluding) parties required to violate security.
