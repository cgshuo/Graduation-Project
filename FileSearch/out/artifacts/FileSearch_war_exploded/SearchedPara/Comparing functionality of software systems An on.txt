 1. Introduction Organizations can reduce the costs and enhance the quality of the software they need by adapting existing software systems.
Adaptation entails changing functionality of existing systems to accommodate users' requirements. Software adaptation decisions given set of requirements and how much effort developers need to expend to adapt the system to the requirements. However, to how these differing views can be reconciled.

A possible meeting ground for users and developers is the functionality provided by a software system. For users, this implemented. For developers, functionality has been the anchor for effort estimation methods [5,22] , because it is defined independent of design and implementation aspects that might not be stable over time. More generally, the literature has commonly applied functionality-based methods and techniques for software comparison. This includes, in particular, selection among alternative solutions in feasibility studies [3,39] and variability analysis of software system families (termed software product lines) [10,26] .

Usually developers compare software either in an ad-hoc fashion based on practical considerations or in terms of taxonomies of software features (e.g., [1,13,16,20,24] ). Often these comparison methods confound systems functionality with implementation-oriented considerations. Therefore, these methods may not be easily applicable in early stages of system development when users are involved. It follows that a set of terms that both reflect users' views and are usable to developers could be valuable in the context of adapting software systems.

In [28] we introduced a classification framework for software variability types based on the premise that what mainly matters changes in a given domain. We term such changes application behaviors . The framework uses an ontological model of behaviors based on concepts from Bunge's work [6,7]. Bunge's ontological concepts have been widely used to analyze conceptual modeling applying Bunge's ontology to conceptual modeling (e.g., [4,8,15] ).

In the current paper we present the use of the ontological framework to the comparison of the functionality of different usefulness of the approach, we conducted an exploratory experiment with software engineering students as subjects. In the experiment, the subjects were trained with the new concepts and then asked to rank descriptions of existing software systems based on the amount of changes needed to adapt the systems to given requirements. The results demonstrated that the statistically significant less time to apply the ontological approach than to apply the function point-based approach.
The rest of the paper is organized as follows. Section 2 discusses the relevant literature. We then present the ontological approach in Section 3 and extend it to comparing functionality of software systems in Section 4 . Section 5 describes the experiment and its results. Finally, Section 6 summarizes the work and suggests future research directions. 2. Literature review to variability ( Subsection 2.2). In particular we discuss the notion of external variability which is closely related to model, we briefly discuss the relationships between this model and fields related to software and system development (Subsection 2.3). 2.1. Software effort estimation
Software effort estimation [5] is related to our topic because an important consideration in comparing systems is the amount of effort needed to adapt the systems to the given requirements. The literature presents different sets of concepts that can be define the amount of functionality provided by a software system. Five types of function points are typically defined [17]: which generates data or control information sent outside the application boundary; (3) external inquiry which is defined by an data or control information maintained by the application.

Several alternatives to function point analysis exist, intended to support different development stages and different system system. The use of object points requires detailed information: are detailed enough to permit reasonable estimation of the input elements function points [25]. Feature points [19] extend the five elements of function points with a sixth element program's algorithms  X  relevant for computationally intensive systems, such as real time systems. Finally, in agile estimation, development teams evaluate story points by examining user stories in comparison to previously implemented stories [12].
However, we note that the literature expresses concerns about the use of story points. For example, Chemuturi states that is no universally accepted definition of what constitutes a story point. There is also no defined methodology to which software estimators can refer when estimating in story points  X  ([9], p. 45).

In particular, some of these methods employ implementation-oriented constructs, e.g., object points. They, thus, require a detailed specification or design of the system to be accurately used. Other methods may be not general enough as they depend on and feature points are specifically relevant for computationally intensive systems.

We propose the ontological approach as a potential way to compare software functionality in early development stages we use a function point-based approach as a comparison benchmark. We chose function points for three main reasons: (1) they are the state of practice, as evidenced by the decision to make them part of an ISO standard in 2002 [18], by support of the
International Function Point Users Group (IFPUG) [17], and by literature claims with functionality and behavior; and (3) they are the basis of many software cost estimation methods [5]. 2.2. Variability types
The field of software product line engineering has extensively studied variability of software systems [16,24,26] .Particularly standards that have to be fulfilled by the software products. In ternal variability reflects technical issues such as testing, development stages (e.g., statements of requirements and design models). However, external variability is mostly relevant in implementation, and testing.

Halmans and Pohl [16] distinguish between different types of external variability, which they term point of view, we refer in our work to these types of variability and formalize them abstractly by using ontological concepts. 2.3. Bunge's system ontology
We use concepts from Bunge's ontological model [6,7] and its adaptations to software and information systems [34,35,36] .We have chosen this ontology because it formalizes concepts that are important for representing functionality and behaviors.
Specifically, these concepts include things, states, events, and transformations. Furthermore, Bunge's ontological model has to data quality issues [33], and to business process modeling [31].

The application of Bunge's ontological concepts to the analysis of conceptual modeling techniques [34,35,37] is of special used at early stages of system development. Several studies tested the efficacy of ontologically-based rules for using modeling [4,8,14,15] ). Finally, we note that a specific ontology reflects a set of beliefs about some domains. Therefore, whether an ontological model is appropriate for a given purpose or not is an empirical question. The corroboration of predictions derived based on Bunge's ontology can be considered as an indication to its applicability in reflecting users' views.
In the next section we present the ontological approach, devoting a subsection for summarizing the basic concepts in Bunge's system ontology. 3. The ontological approach
Our approach for defining functionality of software systems is based on the concept of behavior which we formalize in
We then define behavior in terms of the ontological concepts ( Section 3.2). Finally, we discuss comparison of behaviors (Subsection 3.3). The use of these concepts for software comparison is explained in the next section. 3.1. Basic concepts in Bunge's system ontology
Bunge's ontology describes the world as made of things that possess properties . The main concepts relevant to our work are summarized in Table 1 , along with explanations and notations.

We demonstrate the concepts using a warehouse example. The warehouse is a composite thing which includes an inventory ; workers , who are responsible for taking and processing customer orders, updating inventory, and ordering from suppliers; and managers , who are in charge of approving orders and handling exceptions.

External to the warehouse are customers , who place orders and receive in return goods or items. Customers are considered things in the environment of the warehouse. Orders reflect external events which involve interactions between the warehouse the customer and the warehouse manager and worker. Mutual properties reflect the information managers and workers exchange with customers and among themselves while receiving and processing orders. The manager first decides on whether to approve orders or not. Workers process approved orders. Cancelation of orders can occur due to internal warehouse problems (e.g., insufficient inventory) or in response to customers' requests.
 A possible set of state variables describing the above scenario are listed in Table 2 .
 descriptions in terms of the state variables. 3.2. De fi ning behaviors
Recall, our purpose is to compare functionalities of different software systems in terms of software behavior as can be observable to users and usable to software developers. To define behavior we use ontological concepts adapted from Bunge's work. We show how the ontological terms can be used to compare behaviors of different systems. Our analysis depends on the nature of the compared systems. We make two general assumptions regarding the behavior of the systems we deal with. We will use these assumptions when we explain how behavior can be modeled using the ontological concepts. Furthermore, we will explain why these assumptions do not effectively limit our analysis in practical cases. In the following, we present the Later on, the things we will refer to will be the compared systems.
 The first assumption relates the behavior of composite things to the behavior of their components.

While the assumption of no interruption might not be generally true, it is acceptable if: (1) changes of state are considered instantaneous, or (2) a mechanism exists which prevents external events from having an effect until the thing (or one of its components) is in a stable state. For example, a computerized interrupt handling system may implement the second condition is also often the case when human actors complete the activity they are engaged in before beginning the next activity.
To illustrate the no interruption assumption, consider the case when the customer wants to cancel an order. Since orders are processed by the worker, once the worker has begun processing an order, the order cannot be canceled. However, when the manager is considering an order, the worker is available (idle), result, when the manager completes the consideration of the order request, the worker, based on state information reflecting a customer's cancelation event, can cancel the order.

The second assumption relates to the nature of the behavior of things. Specifically, we assume that every behaving thing should reach a stable state: Assumption 2 (stability). A thing in an unstable state will eventually reach a stable state.
 the environment continuously interrupts them. The meaning of this assumption is that the systems we deal with are stable, until interrupted by an external event. An external event may change the state of the system to unstable. The system will then begin external event (i.e., reach again a stable state).

Note that for composite things, stability is not obvious. Whether or not a system is considered stable depends on the choice of the customer's point of view), different maintenance operations may be carried out. These operations, however, might not be visible to the customers (are not in their view).

The above example demonstrates the need to define the states in which the system is ready to take inputs (respond to external events). We therefore define input sensitive states and behaviors:
De external event.
 every input sensitive state at least one component must be in a stable state (and hence can handle external events).
We consider the behavior of a thing as a change in the thing's state due to a sequence of external events. The behavior begins when the thing is in an input sensitive state and ends when the thing reaches a stable state.

De fi nition 2. Given an input sensitive state s 1 and a sequence of external events s  X  is the first stable state the thing reaches after it was in state s the initial state of the behavior and s  X   X  the final state of the behavior.

Note that for a given input sensitive state and a given sequence of external events, a different combinations of external events and interim states are possible (leading to different interim transitions). However, according to the stability assumption, we assume that all things we deal with in practice will eventually reach some kind of stability. In other words, we assume that a thing will always have a way to complete before starting processing the next sequence of external events.

Returning to our warehouse example, three possible behaviors are depicted in Fig. 1 . (c) differs from behaviors (a) and (b) due to an additional external event ( two events, the warehouse eventually reaches a stable state (as required by the stability assumption).
Note that the behaviors in Fig. 1 may seem similar or different depending on the point of view. Behaviors (a) and (b), for outcome (i.e., whether an order is supplied or canceled).

To summarize the definitions so far, Appendix A presents a meta-model of the main concepts underlying our approach and their relationships. 3.3. Comparing behaviors those of the first one.

The set of relevant state variables (X B ) reflects the chosen level of abstraction. When comparing two behaviors, B and we assume that in these cases a mapping between the different choices of state variables and their values exists. In order to explore ways to compare behaviors, we will assume that they have some relevant state variables in common:
De fi i.e., X B 1  X  X B 2  X   X  .

In the following, we will assume that B 1 and B 2 are state comparable. Comparison of behaviors will be done in terms of a that may affect the examined things, we consider external events similar if they are exactly the same. Otherwise, they will be considered different.
 a way to accommodate the aspects relevant to uses and developers, including a choice of the level of abstraction.
De fi nition 4. Let X be a set of state variables common to the two behaviors B of B
Formally expressed,  X  x  X  X s.x=t.x. X will be termed the view of interest .
As an example, consider the following two destination states in Fig. 1 :
Ready to receive additional orders  X  and  X  Request has been Canceled due to client request, Ready to receive additional orders
From an inventory point of view, these states are equivalent: both do not change the inventory level. However, from a requested item, while in the right case the customer gives up this opportunity (by an explicit cancelation request).
In the next section, we present our approach to comparing software systems functionality using the behavior concept. 4. Using the ontological approach in software comparison
We now describe how we apply the ontological approach to comparing software systems. Fig. 2 depicts the suggested approach. Note, the procedure assumes that users' requirements have been defined and based on them alternative software systems were identified.

We view the required system and the alternative software systems as things in the ontological sense. For each, the environment occurrences in the environment which cause changes of the state of the system. External events reflect the impact of the environment on the system and represent actions by users and by other systems (i.e., elements of the environment). Following an external event, the system may keep changing its state but will eventually stop changing when reaching a stable state (the stability assumption).
 completes changing in response to external events.

As an example, consider an order processing system in the warehouse domain. According to Fig. 2 , the analysis requires the statement of requirements and descriptions of alternative software systems. The required system needs to support receiving and cancelation of orders by phone or fax and to update inventory status. The warehouse worker enters incoming orders and cancelations into the system. The warehouse cannot process in parallel more than 10 orders. Inventory can be updated only if no orders are being processed.
 Based on these requirements we need to examine different relevant software systems and to list their external behaviors.
One such alternative, OrdSys, has the following capabilities. It handles orders received directly from the customer via the internet. Orders can be received at any time, even when the warehouse is closed. However, the orders will be processed for fulfillment only if a signed confirmation with the same order details arrives by fax within 3 days of the original order. If customer is allowed, but the warehouse worker can cancel orders, even if they have been already initially processed (and the customer will be reimbursed if payment has been already made). The system does not have the functionality to update the inventory status.
 order has been successfully processed or not), and the details of the open orders. However, the user is interested only in interest is: X={inventory level, order status (Processing, Not Processed, Canceled, Pending)}.

Taking this view of interest, we now formalize the requirements and system capabilities as behaviors. Each behavior is software engineering, we employ use case models where each behavior is a scenario. We use diagrams with a simplified template behavior completes. The pre-and post-conditions are defined in terms of the values of state variables. For convenience, state values are not affected by the behavior do not appear in the post-conditions.

Fig. 3 depicts the requirements model (R) in terms of the requested behaviors. Fig. 4 depicts the capabilities of the proposed system (OrdSys) in terms of available behaviors.

We now turn to step 3 ( Fig. 2 )  X  comparing the capabilities of proposed systems to the requirements. We use the view of interest and the notions of equivalence between states and equality between events (presented in Subsection 3.3). Table 4 summarizes the outcome of this comparison, where equivalent states and equal events are termed ordering or canceling.

Examining the functionality that OrdSys provides with respect to the requested behavior b requirements at hand. However, behavior b 1  X  seems to provide an option better than the other two behaviors, b adapt this system to the requirements than adapting the alternatives. Still, adapting behavior b necessitate adjustment of the interfaces to the different external events. Following the same line of reasoning, behavior b most similar to behavior b 2 . Regarding the cancelation activities, b except for the final states. Behavior b 5 have no counterpart in OrdSys. 5. Some empirical evidence
To evaluate the practicality of the proposed ontological approach, we conducted an exploratory experiment. The purpose of the experiment was to test whether or not the concepts of the approach were usable by software developers and were effective in estimating adaptation effort. To this end, we provided software engineering students with 10 requirements, and for each alternative systems that could be considered. We asked the subjects to rank the four alternatives for each requirement based on the amount of changes needed to adapt the alternatives to the requirement. To evaluate the answers provided by subjects we requirements engineering and software development, to independently rank the alternatives. We compared the students' results concepts we compared the results to those obtained using a function point-based method. As already explained, we chose function points because they are a commonly used method in industry [22].

The rest of this section provides a detailed description of the experiment. In particular, we discuss the goals and hypotheses procedure ( Section 5.4 ), the results ( Section 5.5), the conclusions ( Section 5.6), and threats to validity ( Section 5.7). 5.1. Experiment goals and hypotheses
As noted, the goal for the experiment was to analyze whether or not novice software developers can utilize the ontological approach for comparing alternative software systems, based on the estimated effort needed to adapt them to given requirements.
To this end, we compared in the experiment ranking done by software engineering students who used the ontological approach to ranking done by software engineering students who used a function point-based approach. We evaluated the answers agreement between the experts regarding the ranking of the alternative systems. For that reason, we decided to compare pairs of alternatives, as will be elaborated later. We distinguished between three categories: (1) five experts agreed on the ranking; (2)  X  broadly agreed  X  X  (3)  X  partially agreed  X  X  cases on which three out of the five experts (60%) agreed.

The independent variable was the approach used by the subjects (the software engineering students), i.e., ontological or explained in detail below.
 For each of the three types of cases we tested the following null hypothesis:
H . There will be no significant difference between the methods (ontological approach and function point-based approach) in terms of quality of outcomes obtained by novice software developers.
 The alternative hypothesis is:
H . There will be a significant difference between the methods (ontological approach and function point-based approach) in terms of quality of outcomes obtained by novice software developers.

We further phrased the following null hypothesis regarding the effort to perform the experimental task with the two approaches. The effort was measured in terms of the time it took the subjects to complete the whole task.
H . There will be no significant difference between the methods (ontological approach and function point-based approach) in terms of effort to perform the experimental task by novice software developers.
 The alternative hypothesis here is:
H . There will be a significant difference between the methods (ontological approach and function point-based approach) in terms of effort to perform the experimental task by novice software developers. 5.2. Function point-based analysis as a benchmark
Function point analysis uses concepts which are by and large oriented towards design and implementation. As we seek definitions which will capture aspects of the system that can be related to requirements and yet maintain the essence of the original function point concepts as closely as possible, we adapt the original function point definitions. We will name this adaptation  X  function point -based approach . The main concepts in the function point-based approach are:  X 
External inputs (EI) which represent the causes of the action to be done by the system and the data entered into the system by the user or transmitted to the system by other systems.  X 
External outputs (EO) which are the outcomes of the system action that are presented to users or transmitted to other systems.  X 
External interfaces (EIF) which are the operations applied by the system to the data which are transmitted to and from other systems .  X 
Internal logical components (ILC) which comprise the data maintained by the system and the operations applied by the system to these data.

Table 5 explains how the original definitions of function points were adapted to our purpose. In adapting the concepts, we a given software system to that implied by the requirements, assuming that similarity of processing indicates adaptation effort required. We further omitted the term  X  external inquiry  X  Table 6 ).

In Table 6 we demonstrate the description of system behaviors in terms of modified function point concepts for the ordering and canceling operations in the warehouse example ( Figs. 3 and 4 ).
 Table 7 indicates the degrees of similarity that exists between the requested behaviors and behaviors of the system OrdSys.
We indicate a required behavior and a behavior of a proposed system as similar when each function point element of the required system is contained in a function point element of the same type of the proposed system. Otherwise we consider the behaviors different. Note that as the warehouse example does not involve external systems, the EIF (external interface) entry is always empty.

Examining Table 7 , based on the function point-based analysis one can conclude that b and b 2 ;b 4  X  is the most similar behavior to b 3 (they are actually identical); and it is not clear whether b behavior b 4 .
 To emphasize the differences between the ontological and the function point-based approaches, we briefly compared them.
Our findings are summarized below. (1) External events in the ontological approach are mapped to the external inputs in the function point-based approach; (2) There is no external output in the ontological approach. However, state variables can be used to represent outputs. For 5.3. Subjects
The subjects in the experiment were third year software engineering students. They had background of computer science courses and preliminary background in information systems engineering, which included topics related to object-oriented analysis and design, process-oriented analysis, and database management. The experiment was conducted in class as part of a that the bonus points will be proportional to the performance in the experiment. Post analysis has shown that the subjects were very committed and provided detailed answers and explanations.
 The subjects were divided into two groups: Group OA, which was introduced to the ontological approach (23 subjects), and
Group FPA, which was introduced to the function point-based approach (19 subjects). groups, we ordered the subjects by their Grade Point Average (GPA). Then we assigned all odd-ordered subjects to one group and all even-ordered subjects to the other group. We further tested the similarity of the groups based on their GPA. Using Shapiro
Wilk test [30], we found that the GPAs in both groups were normally distributed. A t -test found no statistically significant differences between the GPAs in the two groups (t=0.1, p=0.992). Thus, we considered the groups as comparably capable. 5.4. Experimental design and procedure
The experiment was designed as one factor with two treatments: the ontological approach and the function point-based approach. We selected a domain which was relatively known to the subjects requirements on parts of such system that students are not expected to be familiar with, e.g., inter-library loan.
We defined 10 requirements for a system to support the selected (library) domain. We further defined four alternative systems for each requirement. Fig. 5 presents an example for a requirement with the relevant alternatives as it appeared in the questionnaire form. 6 The sentences before the dashed line provide general relevant information about the domain, but are not part of the requirement specification.

We then provided the questionnaire to the five experts, who were asked to rank the alternative available systems based on the effort they estimated would be needed to adapt each alternative to the requirements of the new system. The experts had to explain their rankings, but were not made aware of the two compared approaches. Each expert completed the whole questionnaire independently. No consolidation meeting for reaching a consensus has been scheduled, as we were interested in separately analyzing fully and partially agreed cases.

Each of the two experimental groups was assigned to a different classroom and was trained by a different teaching assistant, the whole class at the same time and avoid information exchange between the subjects. The presentation material which was used by the teaching assistants was prepared by the authors and was kept as similar in scope and contents as possible for both treatments. In particular the teaching examples were the same.
The experiment comprised two phases. In the first phase each group participated in a training session on one approach using the prepared material. The training session took about an hour in which the subjects studied and exercised the main concepts of exemplified and explained for the warehouses domain.
 In the second phase, the students received the task questionnaire, which was identical for the two groups, and answer pages.
The first three questions in the questionnaire referred to how well subjects understood the method, how well they understood the task, and how familiar they were with the domain (the library).
FPA group (out of 5). Yet, this difference was not statistically significant (applying Mann
The main part of the questionnaire comprised the 10 requirements to be analyzed and the alternative systems to be ranked (for example, see Fig. 5 ). The answer pages were designed to engage the subjects with the examined approaches. The expected answers to the task given in Fig. 5 are shown in Appendices B.1 (for the OA Group) and B.2 (for the FPA Group). We allocated 2 hours for the second phase and the subjects had to record their start and end times. 5.5. Results
To obtain our references for evaluating subjects' responses, we first analyzed the rankings provided by the experts. Not which there was no consensus among the experts. We therefore used the following approach. For each pair of alternatives, Si and
Sj (i, j=1 ... 4, i&gt;j), we defined the relation  X  Si is not better than Sj which all five experts (5 experts=100%) agreed full agreement , relations for which four experts (80%) agreed broad agreement , agreement, 14 relations of broad agreement and 5 relations of partial agreement. Due to our definition of the relation Si better than  X  ) there were no other cases.

With these findings as a base line, we analyzed the results for each subject, counting the number of relations that the subject identified when compared to the experts (or majority of experts) in each of the three categories ( agreement). These numbers served as scores indicating the overall as percentages of the total cases in each category.

Table 8 presents the average scores obtained by the two groups (OA and FPA). Fig. 6 presents these findings graphically. The scores for the three categories were: for the full agreement category (100%) category (80%)  X  70% and 67% for the ontological and function point-based methods respectively. For the partial agreement category (60%)  X  59% for the ontological method and 50% for the function point-based method.

To evaluate the statistical significance of the first null hypothesis for each group we used the non-parametric Mann difference between the groups. Thus, the first null hypothesis (H partial agreement category the percentage of students using the ontological approach who correctly indicated the relations (as agreed by 60% of experts) was close to the degree of agreement among the experts. This outcome offers the possibility that the cases which are not clear-cut.
We also tried to compare the efforts it took to use each of the two methods. For this purpose, we measured the time it took the students to perform the entire task, i.e., ranking the alternative systems for the 10 requirements. The average times for be normally distributed, we analyzed their differences using the non-parametric Mann differences were statistically significant (p=0.041). Thus, the second null hypothesis (H differences in effort between the two approaches is rejected. We interpret this outcome as indicating that performing the comparison task with the ontological approach takes less effort than performing the same task using the function point-based function point-based approach. 5.6. Discussion
The results indicate that the ontological approach is usable by novice software developers even after very modest training. Performance scores on ranking alternatives for cases where experts fully agreed were high (average same as those obtained with the function point-based approach. Moreover, performance scores obtained using the ontological concepts in cases where less agreement existed among experts, were on average higher than those obtained using function point-based concepts. As well, the ontological approach took less time to complete the task than the function point-based approach.

If we consider the level of agreement among experts as an indication to how difficult analyzing each case was, an interesting pattern emerges. For the easiest cases  X  indicated by complete agreement among experts, there was no difference between the group using the ontological approach and the group using the function point-based approach. For the broadly (but not completely) agreed cases  X  likely to be more difficult to analyze than the fully agreed ones clear (where even experts only agreed at 60%).

Two main conclusions can be derived from the results. First, we note the high scores obtained by the subjects for the cases of obtained for these two categories also for the function point-based approach, although the scores of the function point-based group were somewhat lower in the broad agreement cases (but not significantly so).

Second, the difference between the two groups was more remarkable when the degree of agreement among experts was minimal (60%). Moreover, the subjects using the ontological approach performed at about 60% score good way to analyze such cases. A possible explanation is that this approach can make it possible to formalize these cases in a higher level of abstraction (compared to the function point-based approach) using concepts such as states and events. The function point-based approach, on the other hand, employs software-related concepts (such as data and operations). Such concepts might be more difficult and time consuming to use for comparison of software systems functionality, especially at the requirements analysis stage. More specifically, while the the ontological approach may facilitate the task of comparison by requiring only separation between initial and final states of behaviors (under external occurrences), the function point-based approach differentiates between inputs and outputs provided by/to users and interfaces to other systems, and as well requires referring to the internal logical components. The latter ones are manifested in the ontological approach as transformations between initial and final states.

The results may also indicate that the ontological approach is easier to use when compared to the function point-based approach, as the time it took to perform the ranking task was shorter when using the ontological approach. This may be an outcome of the ontological approach employing fewer and more abstract concepts. 5.7. Threats to validity We now discuss threats to validity, following the suggestions provided by Wohlin et al. [38].

Construct validity threats concern the relationships between theory and observation. They are mainly due to the method used to assess the outcomes of tasks. We used a questionnaire to assess the ability of the subjects to compare the functionality of software systems and rank the systems according to their suitability for given requirements and the amount of required adaptation; subjects' answers were evaluated with respect to those of five experts. Furthermore, the questionnaire was designed of using either the ontological or the function point-based approach. The teaching assistants who conducted the experiment validated the start and end times as recorded by the subjects on the answer pages.

These effects were mitigated by the experimental design that we chose. We used one factor design with two treatments and by granting them additional marks based on their performance in the experiment. Furthermore, applying the t -test, we found no materials (in terms of examples, order of presentation, and format) that were prepared beforehand and had to be followed during the training sessions which took place at the same time. Yet, two different teaching assistants taught these materials and this understanding of the taught approach after taking the training session and before performing the experimental task. As reported above, no significant differences were found between the groups.
 the task  X  as the controlled experiment had to be completed within a standard 3-hour university lesson). In this regard, the that of the experts. The requirements chosen were simple but realistic. The requirements were focused and isolated from other requirements in order to avoid misinterpretation by the subjects. The task was chosen from a domain of library systems, which most of the subjects were expected to be familiar with. Since the level of domain familiarity could have influenced the results, we included aspects (behaviors) that subjects were not expected to know well, such as inter-library loans. Indeed, the average familiarity score reported by the subjects was about 3 out of 5 with no statistically significant differences between the groups.

Another concern relevant to external validity is that the students had only one training hour before the actual experiment and thus might have faced difficulties originating from insufficient familiarity with the nature of the task and with the use of the approaches. Yet, reviewing the subjects' answers, it seems that all of them understood the studied approaches and applied them in full detail. This was evidenced by detailing the concepts used for each approach for each requirement or software system.

Finally, conclusion validity concerns the relationship between the treatment (the used approach) and the outcome. The outcome was subjects' performance, which was evaluated by comparing subjects' rankings to those provided by the experts. This method of evaluation was directly related to the purpose of the experiment would enable novice software developers to perform well (i.e., as well as experts). This evaluation method also prevented any bias that might be introduced by the researchers.
 the first aspect, once experts' rankings were given (at three levels of agreement), they provided an objective way to score the answers of each subject. Hence, there should be no concerns related to the reliability of the measured subjects' scores. With such as the number of expected changes, the complexity of the changes, the type of change (i.e., removing, changing, or adding all cases.
 data normality could not be assumed, we performed a statistical analysis using mainly non-parametric tests (i.e., the Mann
Whitney U test for unpaired analysis). 6. Summary and future work
When considering software systems for adaptation, developers need to take into account two types of considerations views. The second is usually linked to software development. Moreover, adaptation requires comparison of alternative software systems. Such comparisons are often performed in an ad-hoc manner, apply software-oriented concepts, and are based on limited theoretical foundations.

We propose an ontological approach for comparing functionality of software systems. Based on outcomes of research on ontology and conceptual modeling we posit that ontological concepts can represent users' requirements better than software concepts. The approach enables formalizing and analyzing systems functionality and requirements from an external (users') point approach supports software comparison in an abstract manner independent of software-related concepts. However, a question exists as to the usability of the approach by software developers who usually apply software-related concepts when estimating adaptation effort.

We present the results of an exploratory experiment aimed at examining the usability and usefulness of the ontological approach by software developers who are mostly familiar with software-related concepts. Software engineering students were asked to rank alternative systems based on the amount of changes required to adapt them to given requirements. The rankings were compared to those provided by expert software developers and to rankings obtained using an approach based on function point analysis. The results showed that both the ontological and the function point-based approaches were usable by novice software developers after a relatively short training. Moreover, both approaches provided outcomes comparable to rankings done by experts but the ontological approach seemed to provide better results, especially in cases where ranking appeared more difficult. The results also showed that the ontological approach was faster to use than the function point-based approach. This comparable or better results.

In the future, we plan to perform additional experiments to examine the proposed approach in various domains for different sets of requirements and existing software systems including subjects who do not have software development background and hence better represent users. We plan to further develop considerations such as the times and order of external events, and to non-functional requirements.
 Acknowledgment This research was supported in part by a grant of the Natural Sciences and Engineering Research Council of Canada to Yair experiment.
 Appendix A. A metamodel of the main concepts
The meta-model in Fig. 7 presents the main concepts underlying our approach and their relationships. In the meta-model, each box represents a concept, and each connecting line  X  definitions or assumptions. The elements depicted in white are those we adopted from Bunge's ontology and the shaded elements are those we added in this work. In particular, we modified the concept purpose. It is thus shown as partially shaded.
 Appendix B. The experimental material Appendix B.1. An Example of an expected Answer for the task presented in Fig. 5 by the Ontological Approach (OA) Group Appendix B.2. An example of an expected answer for the task presented in Fig. 5 by the point-based approach (FPA) group
References
