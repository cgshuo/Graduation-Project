 In scientific domains, knowledge is often discovered from experiments by grouping or clustering them based on the similarity of their output. The causes of similarity are an-alyzed based on the input conditions characterizing a given type of output, i.e., a given cluster. This analysis helps in applications such as decision support in industry. Cluster representatives form at-a-glance depictions for such applica-tions. Randomly selecting a set of conditions in a cluster as its representative is not sufficient since distinct combi-nations of inputs could lead to the same cluster. In this paper, an approach called DesCond is proposed to design semantics-preserving cluster representatives for scientific in-put conditions. We define a notion of distance for conditions to capture semantics based on the types of their attributes and their relative importance. Using this distance, methods of building candidate cluster representatives with different levels of detail are proposed. Candidates are compared using the DesCond Encoding proposed in this paper that assesses their complexity and information loss, given user interests. The candidate with the lowest encoding for each cluster is re-turned as its designed representative. DesCond is evaluated with real data from Materials Science. Evaluation with do-main expert interviews and formal user surveys shows that designed representatives consistently outperform randomly selected ones and different candidates suit different users. D.2.8 [ Software Engineering ]: Metrics X  complexity mea-sures, performance measures Performance, Design, Human Factors Copyright 2006 ACM 1-59593-433-2/06/0011... $ 5.00. Domain Knowledge, Visual Displays, Post-processing, Dis-tance Metrics, Minimum Description Length, Decision Trees
Clustering is often used to group data that involves a mix-ture of different types of attributes such a numbers and plain text. The data typically has semantics associated with it in the context of a given domain. Examples of such data in-clude information on handheld PDAs (Personal Digital As-sistants) [7], documents on web pages [15] and association rules derived from databases [11].

In this paper, we deal with such data coming from sci-entific experiments [1]. More specifically, we deal with the input conditions of experiments, where each condition gives the name and value of a process parameter and a set of such conditions forms the experimental input setup. These ex-periments are typically clustered based on their output and sets of input conditions leading to each cluster are identified to aid comparison of the corresponding processes [1]. In-ferences drawn from clustering help in various applications such as parameter selection [14], computational estimation [18], simulation tools [12] and decision support systems [19].
Cluster representatives form concise depictions of each cluster in such applications. However, a randomly selected representative may not incorporate the necessary informa-tion in the cluster since distinct combinations of input con-ditions could lead to a single cluster. Moreover, different ap-plications may need different levels of detail in the cluster. For example, presenting all the information in the cluster causes inefficiency in certain applications such as simulation tools [12] (elaborated in Section 2). In other applications such as visual displays for parameter selection [14] avoiding clutter is important. Hence it is advisable to design clus-ter representatives that preserve domain semantics in the context of specific applications.

In this paper, we propose an approach called DesCond to design semantics-preserving cluster representatives for sci-entific input conditions. A significant issue in the design is the notion of distance for the input conditions. These con-ditions have a mixture of different types of attributes such as categorical, numeric and ordinal. Each of them conveys a certain concept in the domain whose meaning needs to be captured. Moreover, the relative importance of the condi-tions in the domain also needs to be taken into account. In this paper, we propose a distance function for input condi-tions that incorporates all these factors.

Using the given distance, candidate representatives are designed for each cluster showing increasing levels of detail. Three candidates are considered for each cluster, namely, a Single (set of) Conditions Representative closest to all other sets of conditions in the cluster, a Multiple (set of) Condi-tions Representative showing sub-clusters within the clus-ter, and an All (set of) Conditions Representative showing all information in the cluster abstracted suitably.
Candidate representatives are compared using an objec-tive measure called the DesCond Encoding proposed in this paper. This encoding takes into account the complexity of each representative and the information loss due to it based on the interests of targeted users. The winning candidate based on the encoding is returned as the designed represen-tative.
 DesCond is evaluated using real data from the domain of Heat Treating of Materials [6] that inspired this research. Evaluations are conducted with domain expert interviews and with formal user surveys. Domain experts provide in-puts to the DesCond Encoding reflecting interests of tar-geted users. The evaluation results show that the designed representatives are consistently better than randomly se-lected ones and that different candidates win in different applications. This is confirmed by the evaluations through user surveys where different categories of users prefer dif-ferent designed representatives in applications such as pa-rameter selection [14] decision support systems [19] and simulation tools [12]. Hence the output of DesCond is use-ful in designing the respective applications.

The rest of this paper is organized as follows. Section 2 gives a background of the Heat Treating domain with a mo-tivating example for the given problem. Section 3 introduces the DesCond approach for designing representatives of input conditions while sections 4, 5 and 6 give its details. Section 7 summarizes the user evaluation. Section 8 overviews related work. Section 9 states the conclusions.
We present a brief overview of the Heat Treating domain, since we will use examples from this domain throughout to explain the concepts in this paper. In Heat Treating, an important process is quenching, namely, the rapid cooling of a material in a liquid or gas medium to achieve desired mechanical and thermal properties [6]. Quenching is con-ducted with the following sets of input conditions. Given this background, we present a motivating example. Consider the sets of conditions S 1 through S 9 in Example 1 showing a given cluster of experiments.

Example 1
All these sets of conditions in Example 1 lead to a similar experimental output, hence they have been assigned to the same cluster.
 We now consider the application of simulation tools [12]. Users often run simulations of real experiments with a given set of input conditions. These simulations are typically as time-consuming as a real experiment (about 6 hours). They are preferred over a real experiment mainly because they save resources. Imagine that a randomly selected set of input conditions is displayed to the user as the output of estimation. If the user runs a simulation using this repre-sentative, then ranges of information in the cluster are not captured, thus reducing the sample space of simulations. On the other hand if the user runs a simulation using a rep-resentative that conveys all the information in the cluster, it would take very long to run. Since each simulation takes approximately 6 hours with one set of input conditions, run-ning it with 9 sets of conditions would take 54 hours, which is often not practical. Thus there is a need for a trade-off between the two extremes in such applications.

However, there are other applications where information loss is more critical while efficiency is not an issue, and vice versa. Thus there is a need to cater to various types of users. Hence it is necessary to design semantics-preserving cluster representatives in the context of targeted applications.
We propose an approach called DesCond to design a rep-resentative set of input conditions for each cluster. We first define the following terminology.
The input to DesCond is clusters of experiments with all the sets of conditions characterizing each cluster. In our work [18], decision trees [16] are used to identify the com-binations of conditions that characterize the clusters. All the decision tree paths leading to a given cluster of experi-ments are referred to as a cluste r of conditions. Given this, the process of design is as follows.

The semantics of the domain is captured by defining a suitable a distance function for the set of conditions. Us-ing this notion of distance, candidate representatives are designed for each cluster showing gradually increasing levels of detail. In the first level, the representative is selected from the original cluster as a single object (set of conditions) such that it forms the nearest neighbor for all other objects in the cluster. This candidate is called the Single Conditions Rep-resentative (SCR) . In the second level, a candidate known as the Multiple Conditions Representative (MCR) is con-structed by forming sub-clusters within each original cluster using domain knowledge and the given notion of distance. In the third level, the candidate is constructed by combining all information in the cluster and abstracting it in a suitable form. This candidate is called the All Conditions Represen-tative (ACR) .
 The candidates are compared using a measure called the DesCond Encoding analogous to the Minimum Description Length principle [17]. This encoding takes into account the complexity of each representative measured as the number of data points stored for it and the information loss due to it measured as its distance from other objects in the clus-ter. The interests of targeted users based on the relative importance attached to the complexity and information loss are also taken into account in the encoding. The candidate giving the lowest value in the encoding is the winner and is returned as the designed rep resentative. Note that there could be multiple winners based on the encoding, reflecting the corresponding user interests.

Thus, in our framework, the three main tasks in the design of domain-specific cluster representatives for conditions are as follows: 1. Defining a notion of distance for the set of conditions. 2. Obtaining candidate cluster representatives showing 3. Proposing an encoding to compare the candidates in
These tasks are discussed in the following three sections.
We consider three criteria in defining distance. The first one is the data type of each attribute as applicable to the domain. The second criterion is the distance between the in-dividual attribute values defined in a domain-specific man-ner. The third one is the weight of each attribute based on its relative importance in the domain. These are explained as follows.
The attributes describing the input conditions are of dif-ferent types such as numeric, categorical and ordinal [10]. Categorical attributes are of the character or string type and store descriptive information. Numeric attributes represent data that is of the integer or real number type. Ordinal attributes are those whose values are also of the string and character type but store information where the order mat-ters.

The types of attributes applicable to the Heat Treating datasets in our problem are listed below. For ordinal at-tributes, their possible values are also stated. Each attribute represents an individual input condition in Heat Treating.
We use the sets of conditions shown in Example 1 in or-der to explain the calculation of distance for each type of attribute.
For categorical attributes, the distance is defined as 0 if the attribute values are identical and 1 if they are not iden-tical. Hence the distance is calculated as: 1if v i &lt;&gt; v j where S i and S j are the respective sets of con-ditions, while v i and v j are the respective values of the given categorical attribute.

Thus, considering the categorical attribute Part Mate-rial and referring to Example 1, we calculate distance be-tween the Part Material values as D PM ( S 1 ,S 3 ) = 1, and D
PM ( S 1 ,S 2 ) = 0, since Part Material values are not equal in the sets of conditions S 1 and S 3 , while they are equal in S 1 and S 2 .
For numeric attributes, distance is calculated as the abso-lute difference of their attribute values. If the values are grouped into ranges as a data pre-processing step, then we consider the difference between the mean values of the respective ranges. Suitable scaling factors are applied if needed to maintain parity with other attributes. Thus, dis-tance for numeric attributes is calculated as: the respective sets of conditions, v i and v j are the values (or mean value of ranges) of the respective numeric attributes, and SF is a scaling factor based on domain knowledge. Thus in Example 1, for the numeric attribute Quenchant Temperature with scaling factor SF =1 / 10 (given in the domain) we get distances between Quenchant Temperature values as D QT ( S 1 ,S 2 )=1and D QT ( S 1 ,S 3 )=2.
For ordinal attributes, the distance is calculated as the absolute difference between their values after the values are mapped to numeric based on their order. For example, Agi-tation values of high , low and absent are mapped to 3, 2 and 1 respectively. The mapping is a data preprocessing step. Distance for ordinal attributes is then given as: spective sets of conditions, while v i and v j are numeric val-ues to which the respective ordinal values are mapped. In Example 1 therefore, for the ordinal attribute Agitation Level, distance is calculated as D AL ( S 1 ,S 2 )=3  X  3=0and D
AL ( S 3 ,S 4 )=3  X  2=1.
Given these distances for the attribute types, the distance function D cond for the set of conditions is then defined in terms of the distances between individual attribute values and the weights of the respective attributes as follows:
D cond = X  A i =1 W i  X  D i where each D i is a distance function for the individual attributes, each W i is a weight giving the relative importance of the corresponding attribute and A is the total number of attributes. The weights are obtained as explained in the next subsection.
As stated earlier, in our problem decision trees [16] are used to learn the relative importance of the conditions char-acterizing each cluster with respect to the domain. Hence the decision tree paths are used to derive the weights of the attributes depicting these conditions. The reasoning behind the method for deriving the weights is as follows. 1. An attribute is considered to have a higher weight than 2. The shorter the path in which an attribute appears, 3. The greater the number of experiments in the cluster
We draw an analogy with the decision tree induction algo-rithms such as ID3 and J4.8 [16] in this reasoning. It is not feasible to directly use the weights from these algorithms, because the weights are different in each epoch and we need one uniform set of weights for the attributes. Moreover, if we were to use their weights we would need to define a con-stant of proportionality which is not known apriori. Also, running the ID3/J4.8 epochs again on the same dataset is likely to be inefficient, given that the tree has already been constructed. Thus, we use the analogy behind the induction of decision trees.

Given these considerations and applying the reasoning above, a heuristic for the weights of the attributes in the decision tree is defined as below.
 Decision Tree Weight Heuristic where, W i = weight of each attribute, P = total number of paths in the decision tree, G j = number of graphs in the cluster of path j , H i,j = height of node for attribute i in path j and
H j = height of path j such that,  X  X eight X  H is defined number of nodes away from the leaf.

Thus, in a given path the leaf has a height of 0, the node immediately above the leaf has a height of 1 and so forth. The height of a path is basically the height of its root node.
The use of the decision tree heuristic in calculating weights is explained in Example 2 using the partial decision tree showninFigure1. Example 2
For the given partial decision tree, assume that Cluster B has 10 experiments and Cluster H has 5 experiments. Then we get the following weights.
 Quenchant Name: W QN = 1 3 ( 4 4  X  10+ 3 3  X  5+ 4 4  X  10) = 8 . 33 Part Material: W PM = 1 3 ( 3 4  X  10 + 2 3  X  5+ 3 4  X  10) = 5 . 44 Agitation Level: W AL = 1 3 ( 2 4  X  10 + 0 + 2 4  X  10) = 3 . 33 Oxide Layer: W OL = 1 3 (0 + 1 3  X  5+ 1 4  X  10) = 1 . 39 Quenchant Temperature: W QT = 1 3 ( 1 4  X  10+0+0) = 0 . 83 Probe Type: W QT = 1 3 (0+0+0)=0
We will use the weights derived from the corresponding complete decision tree in this example in order to illustrate the design of the candidate cluster representatives. The weights of the attributes inferred from the complete tree (whose partial snapshot is shown in Figure 1) are as follows.
Thus the distance function derived from the complete tree is: D cond =8 . 12  X  D QN +5 . 97  X  W PM +3 . 05  X  D AL +2 . 08 D
OL +0 . 81  X  D QT where the individual distances D QN , D PM and so forth are calculated based on the values and types of the individ-ual attributes. Given the manner in which it is derived, this distance function incorporates domain semantics and can be used for the design of candidate cluster representatives.
We consider the following levels of detail in designing the candidate representatives. The process of designing each of these is explained below. In order to illustrate the concepts, we consider Example 1 showing all the sets of conditions, i.e., decision tree paths leading to a given cluster. These paths are obtained from thecompletedecisiontreeoverthegivendataset. Using the distance function derived from the complete decision tree, candidate representatives are designed as follows.
The Single Conditions Representative, SCR, is one of the original set of conditions in the given cluster. Using the distance function for conditions that incorporates domain semantics, SCR is selected as the set of conditions closest to all others in the cluster. It other words SCR is such that the sum of its distances from all other sets of conditions in the cluster is the least. The SCR for the cluster in Example 1 is shown in Figure 2.

The Single Conditions Representative is designed in order to show the most important cluster information in a concise form. It is useful in applications where the user is interested in finding out the best possible set of input conditions that would give a desired nature of output.
This Multiple Conditions Representative, MCR, summa-rizes the information in the cluster and is constructed as follows. The set of conditions in each cluster are grouped into sub-clusters based on the similarity of the conditions. The notion of similarity for sub-clustering the conditions is the distance function D cond defined earlier.

The number of sub-clusters for each cluster is determined based on domain knowledge. For example, in Heat Treating we have the following information.
Based on this knowledge, the number of sub-clusters is set equal to the number of distinct values of Quenchant Name. In other words, the sets of conditions in each cluster are grouped into sub-clusters based on the similarity of their Quenchant Names.

Sub-clustering is then done using any suitable clustering algorithm using D cond as the notion of distance [10]. For each sub-cluster, a representative is selected as the set of conditions closest to all the others in the sub-cluster. Like-wise, representatives are obtained for each sub-cluster. The Multiple Conditions Representative is an aggregation of all sub-cluster representatives displayed in a tabular form. The MCR for Example 1 is shown in Figure 3.

The Multiple Conditions Representative is designed be-cause it depicts a trade-off between the amount of detail displayed to the user and the amount of information cap-tured within the cluster. It is useful in applications where the user wishes to find out, for example, distinct combi-nations of the most significant condition that would give a desired nature of output.
The All Conditions Representative, ACR, is designed to capture all the data in the cluster with no information loss. It is built by retaining all the original sets of conditions and displaying them sorted in ascending order of the most sig-nificant attribute, followed by the next significant one and so forth. The significance of the attributes is determined based on the distance function D cond . The values of each set of conditions are abstracted using domain knowledge wher-ever possible. For example, in Heat Treating, if three sets of conditions are identical except that the value of Agita-tion Level is absent for one, low for another and high for the third, then this is abstracted as Agitation = any ,where any refers to any possible value of agitation applicable to the domain. Likewise, if two sets of conditions are identi-cal except that Quenchant Temperature has two consecutive ranges (110  X  120) and (120  X  130), then these are abstracted into a single set of conditions with Quenchant Temperature = (110  X  130). This is in order to avoid visual clutter, while still displaying all information in the cluster.

The All Conditions Representative is an aggregation of all the sets of conditions sorted in ascending order from the most to the least significant. The ACR for Example 1 is showninFigure4.

The All Conditions Representative is designed so as to convey all the information in the cluster in an organized manner. It is useful in applications where the user is inter-ested in studying in detail all the possible inputs that would lead to a given nature of output.

Thus, three types of candidate representatives are de-signed for each cluster.
The candidate representatives are compared using an anal-ogy with the Minimum Description Length (MDL) principle. The MDL principle proposed by Rissanen [17] aims to mini-mize the sum of encoding the theory and the examples using the theory. In the literature, when MDL is used to encode cluster information, it is essential to be able to recover the original cluster from the encoding. However, in the context of our problem, we do not need to retrieve the cluster. In-stead, we need to compare the cluster representatives with each other in order to evaluate them. Hence we propose a measure for comparison that is analogous to the Minimum Description Length of the cluster.
 Our proposed measure is called the DesCond Encoding . In our context, the theory (with respect to MDL) refers to the cluster representatives while the examples refer to all the otherobjectsinthecluster. Wetakeintoaccountthecom-plexity of each representative and the information loss due to it. Complexity refers to the ease of interpretation which is measured as the amount of data stored for the represen-tative. Information loss refers to the capacity of the repre-sentative in capturing information within the cluster and is measured as the distance of the representative from all the objects in the cluster. The relative importance attached to the two terms of complexity and distance (information loss) is also taken into account in the encoding, based on the in-terests of targeted users. Given this, the DesCond Encoding is described below.
 The DesCond Encoding
En c = UBC  X  log 2 ( AV )+ UBD  X  log 2 1 where, En c = encoding for conditions, A = number of attributes in the representative,
V = number of values for each attribute in the represen-tative, R = cluster representative, S i = each set of conditions in cluster,
D ( R, S i ) = distance between representative and every set of conditions using the given distance function, s = total number of sets of conditions in cluster, UBC = percentage weight giving user bias for complexity, UBD = percentage weight giving user bias for distance.
The first term in this encoding log 2 ( AV ) denotes the com-plexity of the representative. This is calculated as the num-ber of attributes and values that need to be stored for that representative. The second term, i.e., the distance term log 2 1 s  X  s i =1 D ( R, S i ) denotes the information loss due to the representative. It is calculated as the average distance of the representative from all the other sets of conditions in the cluster. The terms UBC and UBD are the percentage weights assigned to the complexity and distance terms re-spectively in order to give the user bias for those two terms. Unless otherwise specified, equal weights are assigned to complexity and distance, i.e., 50% each.
 Candidate cluster representatives are evaluated using the DesCond Encoding. The representative with the lowest value of the encoding for the given cluster is considered the best and is returned as its designed representative.
DesCond is implemented in Java and evaluated using real data from the Heat Treating domain [6]. Evaluation is conducted with domain expert interviews and with formal user surveys. Each of these is described below.
In this evaluation, domain experts provide different user bias weights in the DesCond Encoding based on their no-tions of targeted user interests. Using these weights candi-date representatives are evaluated. Different datasets con-sisting of Heat Treating experiments placed into clusters are sent as input to DesCond. Parameters altered in DesCond besides the user bias weights are dataset size and number of clusters. Any suitable algorithm such as k-means [13] is used to generate the clusters over the datasets. In addi-tion to altering the values of k , i.e., number of clusters, the clustering seeds are also alte red to provide ra ndomization. Given these clusters as input, the output of DesCond is the winning candidate for each cluster.

For comparison, a random representative is considered per cluster in the evaluation process. Scores are then assigned to each representative as the number of clusters in the given dataset in which it is the winner. For example, in a dataset of 25 experiments placed in 5 clusters with (50 / 50) weights, if the winner is SCR for one cluster and ACR for four, then the scores are, SCR:1, MCR:0, ACR:4 and Random:0. The results are report ed accordingly.
 We show the evaluation results with a small dataset of 25 Heat Treating experiments placed in 5 clusters, a medium dataset of 150 experiments in 10 clusters and a large dataset of 400 experiments in 20 clusters. We consider 7 different user bias weights in the DesCond Encoding spreading over various possible applications as identified by experts. Re-sults are reported as scores for r epresentatives in Figures 5, 6and7respectively.
DesCond is developed in the context of our larger project, the AutoDomainMine system [18] that performs computa-tional estimation. In this system the designed representa-tives are used to estimate the results of experiments given their input conditions and to estimate the input conditions that would obtain a given result
Hence the effectiveness of the designed representatives in estimation is assessed through formal surveys conducted by the prospective users of this system. Users execute tests comparing the estimation of AutoDomainMine with real lab-oratory data not used for training. For every test, if the estimation provided by AutoDomainMine matches the real data, then the users report the test as accurate, else inac-curate. Accuracy of the system is then computed as the percentage of accurate tests over all the tests conducted.
In each test executed by users, the designed representa-tives are compared with each other in terms of how effec-tive they are in displaying information in the applications Figure 8: Winners in Computational Estimation of AutoDomainMine. The applications are parameter selec-tion [14], simulation tools [12], intelligent tutoring systems [4] and decision support systems [19]. In order to perform this evaluation, the estimated output of AutoDomainMine is displayed to the users in three different levels of detail, as the Single Conditions Representative, the Multiple Con-ditions Representative and the All Conditions Representa-tive respectively. Different categories of users are asked to choose which display (designed representative) best meets their needs with respect to the given application. We sum-marize the results of the surveys with respect to different applications. The survey results in this category are for the AutoDomain-Mine system as a whole indicating the effectiveness of the designed representatives in computational estimation [18]. The users conducted 100 tests in this category. Figure 8 shows a pie chart giving the distribution of winners among the candidate representatives. In this pie chart the region corresponding to None Wins shows the inaccurate estima-tions. The estimation accuracy is observed to be 94%.
From Figure 8, it is seen that for computational estima-tion, All Conditions Representatives and Multiple Condi-tions Representatives are winners in most tests executed by users, with Single Conditions Representatives trailing closely behind. Since computational estimation has a broad range of users, different types of representatives are found to win.
In these applications, the output of DesCond is used to select process parameters in industry [14]. The users con-ducted 53 tests in this category. The winners in these ap-plications are shown in the pie chart in Figure 9. The None Wins region in this chart (and charts in the following ap-plications) indicates the tests where none of the candidate representatives were found suitable by the users.
As observed in Figure 9, Single Conditions Representa-tives are the winners for most tests. The reason for this likely would be that in process parameter selection, typi-cally most users want one right answer.

In simulation tools, the users need the cluster representa-tives to run computer simulati ons of a real laboratory exper-iment [12]. The simulation users conducted 62 tests with DesCond. Figure 10 shows the winning candidates in these applications.

From Figure 10, it is seen that Multiple Conditions Rep-resentatives are the winners in most tests. This is probably because simulation tool users generally want to use ranges of information in order to increase the sample space of the simulations, but they also care about complexity since simu-lations are time-consuming. Hence, we find that they prefer the MCRs.
Intelligent tutoring systems are used to study in detail the behavior of processes analogous to classroom study on the given topic [4]. Totally 37 tests were conducted by users in this category. Figure 11 shows what type of representatives suited the users of these applications.

From Figure 11, it is clear that in most cases All Condi-tions Representatives are the winners. This is most likely due to the fact that in most intelligent tutoring applications, users are interested in learning more details about the sys-tem and do not care much about complexity. Hence, more detail is appreciated.
Decision support systems [19] are used for various pur-poses. In high level business decision support, at-a-glance retrieval of information is important without much empha-sis on detail. Some decision support users however, focus on process optimization and need to scrutinize information in more detail. We had 44 tests conducted by decision sup-port system users. The distribution of winning candidates in decision support systems in shown in Figure 12.
From Figure 12, it is found that there is a fairly good mix of winners in these applications. This is because differ-ent decision support users are interested in different levels of detail. Hence it would be desirable to retain all the rep-resentatives in designing such applications, and to display information in increasing levels of detail.
Figure 12: Winners in Decision Support Systems
The following conclusions can be drawn from the results of the user surveys.
In recent years there has been much interest in abstracting information that involves plain text, semi-structured text and so forth. Personal Digital Assistants (PDAs) often have displays in levels of detail. In [7] an approach is described for building representatives consisting of Semantic Textual Units (STUs) with paragraphs, sentences etc. Each STU is revealed gradually in terms of keywords, a single sentence, the first paragraph and the whole STU. In [15] An approach for text summarization over the web is proposed based on constructing representatives by exploiting diversity concepts in text. They take into account probability of occurrence of words, the type of grammatical constructs (nouns, verbs etc.) and the number of documents. However, neither [7] nor [15] propose objective evaluation measures to compare representatives based on user interests.

There is also work on similarity measures over character data. In [8] they consider similarity between categorical at-tributes not only based on the values of the given attributes but also based on the values of other attributes that are inter-dependent. In [9] they present the Iterated Contex-tual Distances algorithm which learns distances between at-tributes taking into account such inter-dependencies. How-ever, the kind of inter-dependencies that they define do not exist in our datasets. Learnable similarity measures for strings are presented in [5] based on support vector ma-chines and expectation maximization and applied for dupli-cate detection. However, they deal with natural language text strings and the involved semantics, while our data is different. We work with domain-specific input conditions that involve a mixture of attributes such as numeric, cat-egorical and ordinal. We do not deal with strings of text whose meaning has to be interpreted in a broader natural language context. Moreover, in our context domain knowl-edge has already been derived from decision trees and can directly be applied to define a distance function for the con-ditions without further learning.

The Minimum Description Length (MDL) principle has been used in the context of clustering. In our earlier work [20] we propose an MDL-based effectiveness measure for cluster representatives of graphs. However, those graphs are images storing two-dimensional plots of numbers while in this paper we focus on input conditions involving different types of attributes. Hence the semantic issues are different. In the literature, clustering of association rules has been proposed in [11] and an MDL encoding is proposed to eval-uate the clusters. In [3] they propose an MDL encoding as an objective evaluation criterion for clustering. However, these encodings are not used to evaluate different types of cluster representatives. Also, in their work they need to re-trieve the original cluster from the encoding which is not a requirement in our context.

In [2] they discover knowledge from simulators. They identify regions in the input space that lead to a certain type of output behavior. Since the cost of simulations is high, they develop automated methods for for efficient knowledge discovery. They focus on which simulations to run next by using Support Vector Machines. However, their data is numeric while ours has different types of attributes such as categorical, numeric and ordinal. Also, they do not build and evaluate different types of representatives. Their focus is on knoweledge discovery while we focus on the display of information as well. Hence we develop methods to suit our problem.
In this paper an approach called DesCond is proposed to design semantics-preserving cluster representatives over input conditions of scientific experiments. Using a domain-specific notion of distance for conditions, candidate repre-sentatives are designed for each cluster showing increasing levels of detail. Candidates are compared using the DesCond Encoding analogous to the Minimum Description Length principle. The winning candidate for each cluster is returned as its designed representative. DesCond is evaluated with real data from Heat Treating. In evaluations conducted with domain expert interviews using the DesCond Encoding, de-signed representatives are observed to be consistently bet-ter than random ones and different designed representatives win in different applications. In the formal user evaluation surveys, it is found that different categories of users like dif-ferent designed representatives. The output of DesCond is useful in developing the corresponding applications. User surveys also indicate that DesCond improves the estimation accuracy of the AutoDomainMine computational estimation system that motivated its development.
This work is supported by the Center for Heat Treat-ing Excellence (CHTE) and its member companies and by the Department of Energy -Industrial Technology Program (DOE-ITP) Award Number DE-FC-07-01ID14197.

The authors thank all the Hea t Treating users who spent their precious time in completing the user evaluation sur-veys. We also thank the organizers of the MPI (Metal Pro-cessing Institute) Spring 2006 Seminar that gave us the op-portunity for system demonstration to set the stage for the user surveys.
 The feedback of the Quenching Research Group in the Department of Materials Science and of the Database Sys-tems Research Group (DSRG), the Artificial Intelligence Re-search Group (AIRG) and the Knowledge Discovery and Data Mining Research Group (KDDRG) in the Department of Computer Science at WPI is gratefully acknowledged. [1] D. Askeland and P. Phule. Essentials of Materials [2] M. Burl, D. DeCoste, B. Enke, D. Mazzoni, W.
 [3] A. Banerjee and J. Langford. An Objective Evaluation [4] D. Bierman and P. Kamsteeg. Elicitation of [5] M. Bilenko and R. Mooney. Adaptive Duplicate [6] H.BoyerandP.Cary. Quenching and Control of [7] O. Buyukkokten, H. Garcia-Molina and A. Paepcke. [8] G. Das, H. Mannila and P. Ronkainen. Similarity of [9] G. Das and H. Mannila. Context-Based Similarity [10] J. Han and M. Kamber. Data Mining Concepts and [11] B. Lent, A. Swami and J. Widom. Clustering [12] Q. Lu., R. Vader, J. Kang and Y. Rong. Development [13] J. B. MacQueen. Some Methods for Classification and [14] M. Maniruzzaman, J. Chaves, C. McGee, S. Ma and [15] T. Nomoto and Y. Matsumoto. A New Approach to [16] J. R. Quinlan. Induction of Decision Trees. Machine [17] J. Rissanen. Stochastic Complexity and the MDL [18] A. S. Varde, E. A. Rundensteiner, C. Ruiz, D. C. [19] A. S. Varde, M. Takahshi, E. A. Rundensteiner, M. [20] A. S. Varde. Graphical Data Mining for Computational
