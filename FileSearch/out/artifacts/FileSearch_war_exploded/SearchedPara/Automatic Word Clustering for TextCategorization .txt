 Chen Wenliang, Chang Xingzhi, Wang Huizhen, Zhu Jingbo, and Yao Tianshun The goal of text categorization is to classify documents into a certain number of predefined categories. A variety of techniques for supervised learning algorithms have demonstrated reasonable performance for text categorization[5][11][12]. A common and overwhelming characteristic of text data is its extremely high di-mensionality. Typically the document vectors are formed using bag-of-words model. It is well known, however, that such count matrices tend to be highly sparse and noisy, especially when the training data is relatively small. So when the text categorization systems are applied, there are two problems to be counted:  X  High-dimensional feature space: Documents are usually represented in a  X  Short of training documents: Many applications can X  X  provide so many train-such as Document Frequency,  X  2 statistic, Information Gain, Term Strength, and Mutual Information[13]. But feature selection is better at removing detrimental, noisy features. The second procedure is cluster-based text categorization[1][2][3] [10]. Word clustering methods can reduce feature spaces by joining similar words into clusters. First they grouped words into the clusters according to their dis-tributions. Then they used these clusters as features for text categorization. Based on class distributions of words, Baker[1] proposes a clustering model. In clustering processing, we will select two most similar clusters by comparing the similarities directly. But Baker X  X  model only considers two related clusters, when computing the similarity between the clusters without taking into account the information of other clusters. In order to provide better performance, we should take into account the information of all the clusters when computing the similarities between the clusters. This paper proposes a clustering model which considers the global information over all the clusters. The model can be understood as the balance of all the clusters according to the number of words in them.
 We present experimental results on a Chinese text corpus. We compare our text classifier with the other three classifiers. The results show that the proposed clustering model provides better performance than Baker X  X  model. The results also show that it can perform better than the feature selection based classifiers. It can maintain high performance when the number of features is small and the size of training corpus is small.
 a global Clustering Model (globalCM). Section 4 describes a globalCM-based text categorization system. Section 5 shows the experimental results. Finally, we draw our conclusions at section 6. Distributional Clustering has been used to address the problem of sparse data in building statistical language models for natural language processing[7][10]. There are many works[1][2] related with using distributional clustering for text categorization.
 on word-clusters. First, find word-clusters that preserve the information about the categories as much as possible. Then use these learned clusters to represent the documents in a new feature space. Final, use a supervised classification al-gorithm to predict the categories of new documents. Specifically, it was shown there that word-clustering can be used to significantly reduce the feature dimen-sionality with only a small change in classification performance.
 In this section, we simply introduce the class distribution of words[1]. Then we propose the Global Clustering Model, here we name it as globalCM. In our clustering model, we define a similarity measure between the clusters, and add the candidate word into the most similar cluster that no longer distinguishes among the words different. 3.1 Class Distribution of Words Firstly, we define the distribution P ( C | w t ) as the random variable over classes C, and its distribution given a particular word w t .Whenwehavetwowords w t and w s , they will be put into the same cluster f . The distribution of the cluster f is defined new cluster f new . The distribution of f new is defined 3.2 Similarity Measures Secondly, we turn to the question of how to measure the difference between two probability distributions. Kullback-Leibler divergence is used to do this. The KL divergence between the class distributions induced by w t and w s is written D ( P ( C | w t ) || P ( C | w s )), and is defined infinite when p ( w s ) is zero. In order to resolve these problems, Baker[1] proposes a measure named  X  X L divergence to the mean X  to measure the similarity of two distributions(Here we name it as S mean ). It is defined But it only considers the two related clusters without thinking about other clusters. Our experimental results show that the numbers of words in learned clusters, which are generated by Baker X  X  clustering model, are very different. Several clusters include so many words while most clusters include only one or two words.
 clustering algorithm, it can X  X  work well if the numbers of words in the clusters are very different at iterations.
 tering model, a new candidate word will be put into an empty cluster). We will compute the similarities between f and the other two clusters( f i and f j )using Equation 4. Let f i has many words(ie. 1000 words) and f j has one or two words. We define: According to Equation 2, if a word is added to a cluster, the word will affect tiny to the cluster which includes many words and affect remarkable to the cluster which includes few words. So the distribution of f  X  f i is very similar to f i because f i has many words and f has only one word. And then D i 2 is near zero.  X  i is near 1 and (1  X   X  i ) is near zero because the number of f i is very large than f .Weknow: So when we compute the similarities between f and the other clusters using Equation 4, f will be more similar to the cluster which includes more words. the clusters when computing the similarity between the two clusters. If we only take into account the two related clusters, the system will can X  X  work well. In order to resolve the problems, we propose a new similarity measure that considers the global information over the clusters. The similarity between a cluster f i and a cluster f j is defined Where N ( f k ) denotes the number of words in the cluster f k , M is the list of clusters. Equation 8 can be understood as the balance of all the clusters according to the numbers of words in them. In our experimental results show that it can work well even if the numbers of words in the clusters are very different. 3.3 Global Clustering Model(globalCM) Now we introduce a clustering model which use Equation 8. The model is sim-ilar to Baker X  X  clustering model[1]. In this paper, we name Baker X  X  model as BakerCM, and our model as globalCM.
 vocabulary by  X  2 statistic with the class variable. Then the clusters are initialized with the Top M words from the sorted list. Then we will group the rest words into the clusters. We compute the similarities between all the clusters(Equation 8) and then merge the two clusters which are most similar. Now we have M-1 clusters. An empty cluster is created and the next word is added. So the number of clusters is back to M. Table 1 shows the clustering algorithm. This section introduces our globalCM-based Chinese text categorization Sys-tem. The system includes Preprocessing, Extracting the candidate words, Word Clustering, Cluster-based Text Classifier. Word Clustering has been described at Section 3. 4.1 Preprocessing First, the html tags and special characters in the collected documents are re-moved. Then we should use a word segmentation tool to segment the documents because there are no word boundaries in Chinese documents. 4.2 Extracting the Candidate Words We extract candidate words from the documents: First we use a stoplist to eliminate no-informative words, and then we remove the words whose frequencies are less than F min . Final, we generate the class distributions of words which is described at Section 3. 4.3 The Cluster-Based Classifier Using the learned clusters as features, we develop a cluster-based text classifier. The document vectors are formed using bag-of-clusters model. If the words are included in the same cluster, they will be presented as the single cluster symbol. After representation, we develop a classifier based on these features. na  X   X ve Bayes briefly since full details have been presented in the paper [9]. The basic idea in na  X   X ve Bayes approach is to use the joint probabilities of features and categories to estimate the probabilities of categories when a document is given. Given a document d for classification, we compute the probabilities of each category c as follows: is the frequency of the feature f t (Notes that the features are the cluster symbols in this paper.) in document d i , F is the vocabulary and | F | is the size of F, f t that a randomly drawn feature from a randomly drawn document in category c j will be the feature f t . The probability is estimated by the following formulae: In this section, we provide empirical evidence to prove that the globalCM-based text categorization system is a high-accuracy system. 5.1 Performance Measures In this paper, a document is assigned to only one category. We use the conven-tional recall, precision and F1 to measure the performance of the system. For evaluating performance average across categories, we use the micro-averaging method. F1 measures is defined by the following formula[6]: Where r represents recall and p represents precision. It balances recall and pre-cision in a way that gives them equal weight. 5.2 Experimental Setting The NEU TC data set contains Chinese web pages collected from web sites. The pages are divided into 37 categories according to  X  X hina Library Categorization X  [4] 1 . It consists of 14,459 documents. We do not use tag information of pages. We use the toolkit CipSegSDK[14] for word segmentation. We removed all words that have less than two occurrences( F min = 2). The resulting vocabulary has 75480 words.
 formly split each category into 5 folds and we take four folds for training and one fold for testing. In the cross-validated experiments we report on the average performance. 5.3 Experimental Results We compare our globalCM-based classifier with the other three clustering and feature selection algorithms: BakerCM-based classifier,  X  2 statistic based clas-sifier, and document frequency based classifier. These two feature selection meth-ods are the best of feature selection methods according to Yang X  X  experiments[13]. Experiment 1: globalCM VS BakerCM. In this experiment, we provide empirical evidence to prove that the globalCM based text classifier provides bet-ter performance than that based on BakerCM. Figure 1 shows the experimental results.
 BakerCM in most different features size cases. With 100 features, globalCM provides 10.6 % higher than BakerCM. Only when the number of features is less than 7, BakerCM can provide the similar performance to globalCM.
 Experiment 2: globalCM-based classifier VS Feature-Selection-based classifiers. In this experiment, we use three different size of training corpus: 10 % ,50 % , 100 % of the total training corpus(Here we name them as T10, T50 and T100). And we select two feature selection methods: document frequency and  X  2 statistic for text categorization.
 ent amounts of training dataset, where globalCM denotes our clustering model, fs x2 denotes  X  2 statistic feature selection method and fs df denotes document frequency feature selection method. For 3 different quantities of documents for training, we keep the number of features constant, and vary the number of doc-uments in the horizontal axis.
 mance is. The best result of globalCM with T100 training corpus is 75.57 % ,1.73 % higher than the best result with T50 and 11.42 % higher than the best result with T10. The best result of fs x2 with T100 training corpus is 74.37 % , higher than the result of the other two training corpus.
 100 features globalCM achieves 70.01 % ,only5.6 % lower than with 5000 features. In comparison, fs x2 provides only 56.15 % and fs df provides only 52.12 % .When with 1000 features, fs x2 can yields the similar result as globalCM with 100 features. Even with only 50 features, globalCM provides 67.10 % . The best of globalCM is 1.20 % higher than the best of fs x2 and 1.37 % higher than fs df. The performance indicates that globalCM is providing more accuracy. And it can maintain near 70 % with only 100 or less features while feature selection based classifiers have fallen into the 50s.
 globalCM can maintain good performance with small training corpus. With T10 training corpus and 50 features, globalCM achieves 60.33 % .Itisnear20 % higher than fs x2 and fs df. To our surprise, it is similar to the results of the feature selection based classifier using T100 training corpus and 200 features. better than the feature selection based classifier. In feature selection method, the system discards some words that are infrequent. But in our clustering algorithm merges them into the clusters instead of discards them. So it can preserves information during merging.
 In this paper, we present a cluster-based text categorization system which uses globalCM. While considering the global information over all the clusters, glob-alCM can group the words into the clusters more effectively. So it can yield better performance than the model which doesn X  X  think about global information. results indicate that our globalCM-based text categorization system can provide better performance than feature selection based systems. And it can maintain high performance when the size of training corpus is small and the number of features is small.
 ing algorithm because words forming phrases are a more precise description of content than words as a sequence of keywords[8]. For example,  X  X orse X  and  X  X ace X  may be related, but  X  X orse race X  and  X  X ace horse X  carry more circumscribed mean-ing than the words in isolation. We also plan to look at techniques for learning words from unlabeled documents to overcome the need for labeled documents. This research was supported in part by the National Natural Science Foundation of China &amp; Microsoft Asia Research (No. 60203019) and the Key Project of Chinese Ministry of Education (No. 104065).

