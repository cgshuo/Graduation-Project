 We present a novel graph-based framework for timeline summa-rization, the task of creating different summaries for different times-tamps but for the same topic. Our work extends timeline sum-marization to a multimodal setting and creates timelines that are both textual and visual. Our approach exploits the fact that news documents are often accompanied by pictures and the two share some common content. Our model optimizes local summary cre-ation and global timeline generation jointly following an iterative approach based on mutual reinforcement and co-ranking. In our al-gorithm, individual summaries are generated by taking into account the mutual dependencies between sentences and images, and are it-eratively refined by considering how they contribute to the global timeline and its coherence. Experiments on real-world datasets show that the timelines produced by our model outperform several competitive baselines both in terms of ROUGE and when assessed by human evaluators.
 H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing X  Abstracting methods ; I.4.9 [ Image Processing and Computer Vision ]: Application Algorithms, Experimentation, Performance Evolutionary summarization, iterative reinforcement, visual time-line, text-to-image translation
With the rapidly growing amounts of information now being available over the Internet, it is becoming increasingly important to develop improved computer-based methods for document access, filtering, and content extraction. Document summarization is one of the oldest applications of information gathering. A summary can be loosely defined as a text that is derived from one or more orig-inal texts, conveys the most important information in them, and is substantially shorter in length.

Of the many types of summaries that have been identified over the years (see [20] and [16] for comprehensive overviews), perhaps the most important distinctions concern whether the summary is being created over a single or multiple documents, whether it is an extract (portions of the original text appear in the summary verba-tim) or an abstract (the extracted content is reformulated in novel terms), and whether it is indicative (it merely provides an indica-tion of the subject matter of the input text) or informative (it is a shortened version of the content of the original one).

In this paper we address a relatively new summarization task, namely timeline summarization . The idea behind timeline summa-rization is to capture how a news topic evolves over time. Timelines consist of several component summaries that are linked to differ-ent timestamps. Each component summary provides a local view on the news story for a specific point in time, whereas the time-line provides a global view on the development of the story across time. Timeline summarization is related to but different from up-date summarization [25]. Update summaries focus on what is new relative to a previous body of information, whereas timelines in-volve not a single but several steps of updates. Both tasks produce multi-document, typically extractive, summaries for a set of articles on the same topic.

Our work extends timeline summarization to a multimodal set-ting. Most online news articles contain images whose role is to complement or emphasize what is described in the text. An exam-ple is shown in Table 1; the article describes a devastating earth-quake in Japan and the picture accentuates the extent of damage it caused. We argue that incorporating visual information into time-lines is desirable for several reasons. Firstly, images will supple-ment the textual summaries by providing additional information. Secondly, visual timelines can be seen as an extreme form of sum-marizing the content of very large document collections. Users need not read the textual summaries; instead, they can only look at the images to get an impression of how a story developed over time. Thirdly, by exploiting the natural dependence between arti-cles and their images during modeling, it is possible to improve the output of the textual summaries as the images provide cues about important content.

Our approach exploits the fact that news documents are often accompanied by pictures and the two share some common con-tent. Our summarization model formalizes the following intuitions. Component summaries must capture local importance in relation to specific timestamps (e.g., corresponding to specific dates). They consist of two heterogeneous streams, i.e., images and text which are correlated and thematically matched. Component summaries must combine to create a globally coherent timeline across times-tamps. Our model optimizes local summary creation and global timeline generation jointly following an iterative approach based on mutual reinforcement and co-ranking (see Figure 1 for an illus-tration). Our algorithm operates over sentences and images whose dependencies are captured in a heterogeneous network consisting of three types of graphs. One graph represents how individual sen-tences (and analogously images) relate to each other, a bipartite graph represents how sentences relate to images, and a third graph captures global dependencies between local sentences (and images) and the timeline created at each time step. The main idea behind co-ranking is that there is a mutually reinforcing relationship between sentences and images which influences their ranking and whether they should be included in the timeline or not.

An important component of the framework sketched above is to be able to express and quantify the meaning shared between im-ages and the documents that contain them. With the help of image annotation techniques [6, 7, 8], we create a translation model that bridges the gap between textual and visual information. We pre-process images so that they resemble word-like units and define a probabilistic model that translates a visual word into a textual word and vice versa.

Experiments on real-world datasets show that the timelines gen-erated by our model outperform several competitive baselines both in terms of ROUGE and when assessed by humans. Interestingly, our results also indicate that the model selects images and sentences that are thematically related and that the visual information helps create better textual summaries in addition to improving the time-lines altogether. The remainder of this paper is structured as fol-lows. In Section 2 we present an overview of previous work and then move on to formalize our summarization model (Section 3). We describe our experimental setup in Section 4 and present our results in Section 5. Section 6 concludes the paper.
Most work to date focuses on extractive summarization. The idea is to create a summary automatically simply by identifying and subsequently concatenating the most important sentences in a document. The approach is robust (it can be easily ported to differ-ent languages and domains) and produces grammatical output. One of the most popular extractive methods that have been proposed for multi-document summarization is centroid-based. It operates on a document collection with a common subject. A cluster centroid is built representing the most important words from the whole col-lection. The centroid is then used to determine which sentences from the individual documents are most representative of the col-lection. MEAD [19] is a publicly available implementation of this approach. NeATS [11] adds topic signatures and term clustering to sentence selection. Both MEAD and NeATS use MMR [9] to re-
A massive earthquake has hit off north-east of Japan today, trig-gering a tsunami that has caused extensive damage. Japan X  X  TV showed cars, ships and even buildings being swept away in the Fukushima prefecture, after the 8.9 magnitude earthquake.
Officials said a wave as high as 6 m (20 ft) could strike the coast. The quake struck about 250 miles (400km) from Tokyo at a depth of 20 miles, shaking buildings in the capital for several minutes.
 T able 1: An excerpt of a news report from the BBC website and its accompanying image. move redundancy. Information based on themes in documents has been also used for sentence selection [22, 26].

More recently, graph-based methods have been proposed to rank sentences based on  X  X otes X  or  X  X ecommendations X  between them. TextRank [17] and LexPageRank [5] use algorithms similar to Page-Rank and HITS to compute sentence importance. These meth-ods first construct a graph representing the relationships between sentences and then evaluate their importance or salience based on the topology of the graph. Wan et al. [23, 22] present an im-proved graph-ranking algorithm which differentiates intra -and in-ter -document linkage between sentences [24] and incorporates topic cluster information in manifold-ranking.

A few methods have been developed specifically for timeline summarization For example, Swan et al. [21] construct timelines by extracting clusters of noun phrases and named entities. Later they build a system to provide timelines which consist of one sen-tence per date, based on their usefulness and novelty. Chieu et al. [3] present a system that extracts events (i.e., sentences) relevant to a query from a collection of documents and places such events along a timeline. Events are considered important if they are widely cited in many documents for a period of time. Yan et al. [27, 28] improve timeline summarization by modeling the evolutionary na-ture of news articles. Specifically, they model correlations among component summaries using inter-date and intra-date sentence de-pendencies.

To the best of our knowledge, the use of visual information in timeline summarization is unexplored in previous work. We pro-pose a novel framework for this task which is based on iterative reinforcement: text-to-image correlations and local-to-global de-pendencies are taken into account simultaneously in order to create component summaries which are locally informative and globally coherent. Figur e 1: Iterative reinforcement for VTS. Shaded areas denote texts and images published on the same date; arrows between them indicate mutual reinforcement.
The input to our visual timeline summarizer consists of a doc-ument collection C relating to a specific news subject (e.g., an earthquake or a disease). C is partitioned into sentences S and images M , i.e., C = { S, M } . Each sentence s  X  S and each im-age m  X  M is associated with a timestamp, i.e., the published date of the source document where the sentence and image appeared. The sentence collection S is further partitioned into S = S the image collection M is partitioned into M = { M 1 , M 2 Let C i = { S i , M i } denote the collection consisting of sentence set S and image set M i published on the same date t i . 1 The output of our system is a series of individual but correlated component summaries I = { I 1 , I 2 , . . . , I | T | } . Each component I ( I
We conceptualize VTS in terms of three components, namely sentence selection, image selection, and sentence-to-image match-ing. Rather than optimizing each component in isolation, we in-troduce a global framework that performs the optimization task jointly, and thus exploits inter-and intra-component dependencies. For example, ranking individual sentences depends on image selec-tion, and the selection of images should also relate to the sentence ranking.

We propose an iterative reinforcement framework for co-ranking sentences and images. The framework is illustrated in Figure 1 and formalized in Section 3.1. As can be seen, sentences and images (and their ranking) are coupled together and dependent on each other.
The mutual reinforcement chain shown in Figure 1 captures the following intuitions behind the VTS problem. A local sentence is important if (1) it associates to other important local sentences; (2) it associates to important local images ; and (3) associates to other important sentences selected for neighboring timestamps (we refer to these as global neighbors of sentences). Analogously, a local image is important if (1) it associates to other important local images; (2) it associates to important local sentences ; and (3) as-sociates to important images selected for neighboring timestamps (called global neighbors of images). W e use day-based timestamps [1, 3, 28, 27].

For each timestamp t  X  T we aim to find the most important or salient local sentences I s t and the most important local images I so that they semantically related. Selected sentences should explain the images and selected images ought to depict some of the sen-tences X  content. We derive the ranking of local sentences and local images iteratively from the mutual reinforcement chain across dif-ferent timestamps. To simplify notation, we remove the subscript t from all local component choices when there is no ambiguity. We use two vectors s = [  X  ( s i )] 1  X | s | and m = [  X  ( m saliency scores  X  ( . ) of local sentences and local images from times-tamp t . We use vectors u = [  X  ( s i )] 1  X | I s | and v = [  X  ( m denote the candidate sentences and images in each iteration; the discrimination function  X  ( . ) records the saliency scores of can-didate sentences (and images) that are selected for timelines (see Equation (4)).

We use an adjacency matrix [  X  U ] | s | X | s | to represent the homoge-neous affinity between local sentences, and matrix [  X  U ] describe the affinity between local images. We use adjacency matrix [  X  W ] | s | X | m | to capture the local hetero-geneous affinity between sentences and images; analogously ma-ages and sentences. We use two matrices to represent neighboring sentences and images sentences and [  X  N ] | m | X | I m | global neighboring images. in vectors u , v ; this is the case for sentences or images that are selected as summary candidates:
We are now ready to formulate our iterative procedure for text-to-image co-ranking. The following two steps are used to select sentences and their corresponding images until convergence. We initialize the algorithm by setting the the saliency scores for all sen-tences and images to 1: Step 1: compute the saliency scores of local sentences, and then normalize using  X  -1 norm. Step 2: compute the saliency scores of local images, and then normalize in  X  -1 norm.
Parameters  X  ,  X  and  X  specify the relative contributions to the fi-nal saliency scores from 1) the affinity between homogeneous local sentences (and analogously images), 2) the affinity between hetero-geneous local sentences and images, and 3) the affinity from homo-geneous global salient nodes. For simplicity, we let  X  +  X  +  X  =1. In Algorithm 1 Local Text-Image Reinforced Co-Ranking 1: pr ocedure L-C O R ANK ( u , v ) 2: calculate  X  U ,  X  U ,  X  W ,  X  W ,  X  N ,  X  N 3: n  X  0, initialize s , m 4: repeat 5: s  X   X  s , m  X   X  m 6: s  X   X   X  U T s  X  +  X   X  W T m  X  +  X   X  N T u 7: m  X   X   X  U T m  X  +  X   X  W T s  X  +  X   X  N T v 8: s  X  s / || s || , m  X  m / || m || 9: 10: n  X  n+1 11: until  X  L &lt;  X  12: choose top ranked sentences/images by s and m 13: // update the saliency scores of local sentences/images 14: u  X   X  UPDATE( u ) 15: v  X   X  UPDATE( v ) 16: return u  X  , v  X  17: end procedure 18: 19: function U PDATE ( x ) 20: for all x i  X  x do 21: if x i  X  I then 22: // highly-ranked ones are chosen as candidates 23: x i  X  x i 24: else 25: x i  X  0 26: end if 27: end for 28: end function order to guarantee the convergence of the iterative form, we must force the transition matrix to be stochastic and irreducible. To this end, we must make the s and m column stochastic [10]. We there-fore normalize s and m after each iteration in Equations (5) and (6).
When all component summaries are generated, u and v are up-dated and the algorithm returns to Step 1 and the same procedure is repeated until convergence. Note that n and k are different itera-tion indicators. n controls the iteration towards convergence among local sentences/images (see Algorithm 1). k controls the iteration towards global convergence at the timeline level, shown in Algo-rithm 2. Empirically, the algorithm converges when the difference between the scores computed at two successive iterations for any sentences/images falls below a small threshold  X  (set to 0.001 in this study).

The framework just described critically explores the relationship between elements within the same modality and across modalities. In order to capture which images correspond to which summaries and vice versa we need some means of translating between vi-sual and textual information. In the following, we explain how we model text-to-image correspondences and then move on to describe the estimation of our affinity matrices.
Texts and images represent distinct modalities. Images live in a continuous feature space, whereas words are discrete. We fol-low previous work [6, 2, 4] in converting the visual features from a continuous onto a discrete space, thereby rendering image fea-tures more like word units. In order to do this, we use the Scale Invariant Feature Transform (SIFT) algorithm [13]. The general Algorithm 2 Visual Timeline Summarization 1: k  X  0, initialize u , v 2: repeat 3: u  X   X  u , v  X   X  v 4: for t  X  1 to | T | do 5: (u, v)  X  L-CoRank( u  X  , v  X  ) 6: end for 7: u  X  u / || u || , v  X  v / || v || 8: 9: k  X  k+1 10: until  X  G &lt;  X  11: OUTPUT( u , v ) // output non-zero elements as summaries 12: 13: function OUTPUT( x ) 14: for all x i  X  x do 15: if x i &gt; 0 then 16: select x i into timeline 17: end if 18: end for 19: end function idea behind the algorithm is to identify local image regions using a difference-of-Gaussians point detector. Importantly, this detector is, to some extent, invariant to to small shifts in position, changes in illumination, noise, and viewpoint and can be used to perform reli-able matching between different views of an object or scene. Each detected region is represented with a SIFT descriptor which is a his-togram of edge directions at different locations. We further quan-tize the SIFT descriptors using the K-means clustering algorithm to obtain a discrete set of visual words which form our visual vo-cabulary. Each entry in this vocabulary stands for a group of image regions which are similar in content or appearance and assumed to originate from similar objects. Formally, each image is expressed in a bag-of-visual-words format vector, [ w v 1 , w v 2 , ..., w
Both visual and textual modalities are represented as a bag-of-units, i.e., a vector of textual or visual terms. Given this repre-sentation, we can then define a translation model between the two modalities under the assumption that they express related concepts. Our model defines the probability of translating a textual word into a visual word and vice versa. We learn image-to-text correspon-dences from training data consisting of documents and the images embedded in them. In this work, we make use of news articles which are often accompanied with images illustrating events, ob-jects or people mentioned in the text.

Let images m denote a set of visual words m = { w v } , and texts s denote words w . Let  X  = { m, s } denote a training pair consisting of a document and its corresponding image. The translation model can be estimated by maximizing the likelihood of images given their surrounding texts:
Although we could use EM to estimate Equation (7), we approx-imate it heuristically with a simpler form which is considerably more efficient to compute [14]:
T able 2: Kernel functions for temporal biased projection. where #( w v , w ) is the number of times w v and w co-occur in  X  , and # w is the total number of times w occurs in the training set.
The model in Equation (8) allows us to translate a visual word into a textual word, which we denote as w v = f ( w ) . Analogously, we use w = f  X  1 ( w v ) to denote the translation of a textual word into a visual word. We can also translate an image m containing a set of visual words into a set of textual words ( f  X  1 ( m ) ) as well as a sentence into visual words ( f ( s ) ).
In this section we explain how our affinity matrices are calcu-lated. Recall from Section 3.1 that our co-ranking framework makes use of a local homogeneous affinity matrix (  X  U,  X  U ), a local hetero-geneous affinity matrix (  X  W ,  X  W ), and a global homogeneous affin-ity matrix (  X  N,  X  N ).
The local sentence collection can be modeled as a weighted undi-rected graph. Nodes in the graph represent sentences, edges repre-sent intra-sentential relatedness, and their weights are computed via cosine similarity. The adjacency matrix U describes such a graph with each entry corresponding to the weight of an edge. Analogously, we calculate the cosine similarity between images represented by visual words. The adjacency matrix for sentences U follows: where s and m represent vectors of textual and visual words, re-spectively. Vector components are set to their tf.idf values [18]. tf is the term frequency (visual or textual) and idf is the inverse
Figur e 2: Proximity-based kernel functions, where  X  =10. sentence frequency in the case of textual words and inverse image frequency in the case of visual words.

Both matrices are normalized to make the sum of each row equal to 1, i.e., U s is normalized into  X  U and U m into  X  U :
We capture the dependencies between sentences S and and im-ages M = { m  X  M } , in a bipartite graph. Graph edges are weighted according to semantic relatedness: T o ensure that images and texts contribute equally to the definition of the matrix, we take the average similarity of the bidirectional translation; again we calculate similarity using the cosine measure. Vector components representing sentences and images are set to their tf.idf values as mentioned above. W is normalized to make the sum of each row equal to 1. In addition, we normalize the transpose of W , i.e., W T , to  X  W to make the sum of each row in W
T equal to 1.
We represent the relationship between local sentences (or im-ages) and the summary candidates  X  X  (or  X  X  ) as a weighted graph: Function t ( . ) denotes the timestamp of the sentence or image. The adjacency matrix N s describes the affinity between local sentences and candidate timeline sentences. Each entry of the matrix corre-sponds to the linkage weight and is normalized to  X  N so that the sum of each row equals 1. Similarly, the image matrix N m malized to  X  N .

Function  X ( X  t ) is a temporal decay function denoting the time distance between two component summaries. Intuitively, compo-nent summaries with a temporal gap will have less influence on each other [28, 27]. Following [15], we experiment with five rep-resentative kernel functions expressing temporal bias: Gaussian, Triangle, Cosine, Circle, and Window. The functions are defined in Table 2 and illustrated in Figure 2. Different kernels lead to differ-ent projections. For instance, the Window kernel treats neighbor-ing component summaries equally within a certain scope while the Gaussian kernel models a gradient decaying relationship.
All kernels are parameterized by  X  which controls the spread of the kernel curve, i.e., it restricts the projection scope of each sentence and image. In general, the optimal setting for  X  may vary across datasets. For example, some news subjects may evolve quickly and require a small value of  X  , whereas others may evolve slowly and require a higher  X  value.
Our experiments used the data provided in [28], a collection of articles gathered from British (e.g., BBC), American (e.g., CNN, ABC, Reuters), and Chinese (e.g., Xinhua) news sources. We cre-ated four datasets, each covering a different news topic (i.e., sci-ence, disasters, accidents, and politics). Table 3 provides basic statistics for each dataset. Aside from documents and images, the datasets are accompanied with timeline summaries (text-based, or image-based, or both) written by professional editors. We treat these as gold standard and use them for the system evaluation. Ne ws Subjects #Doc #Sent #Img #T #R T Influenza A 2557 115026 952 331 11 BP Oil Spill 1468 63021 337 135 9 Haiti Quake 247 12073 175 83 5 Obama Presidency 2160 79761 813 349 16 T able 3: Datasets used in our experiments (#Doc: number of documents, #Sent: number of sentences, #Img: number of im-ages, #T: number of timestamps, #RT: number of reference timelines).
In order to train our model and estimate the affinity matrices from Section 3.3, we decomposed news articles into sentences. We also removed stop-words and performed stemming. We trained a translation model based on pairs of images and their surrounding text found in our datasets. Each pair comes from the same web document. We generate a component summary for each timestamp according to a user specified compression rate . All component summaries constitute the global timeline.

Source documents bear temporal tags, i.e., the time of their pub-lication. Sentences inherit temporal tags from their documents, as well as images. The sentence and image collections S and M are further partitioned according to their timestamps (e.g., S = S S 2  X  X  X  X  X  S | T | ). As mentioned previously, I s i is generated from sub-collection S i , and I m i from M i . The sizes of component sum-maries are not necessarily equal. Users specify the overall com-pression rate  X  , and we extract more content (either texts or im-ages) for important dates and less for other dates. The importance of dates is measured by their burstiness with probable significant occurrences [3]. The compression rate on t i is set as  X  s for sentences and  X  m i = | M i | | M |  X  for images.
We compared our approach against a wide range of well-known summarization methods. All comparison systems were subject to the same preprocessing procedures as our own algorithm, includ-ing the image-to-text translation model. As these methods have not been developed with images in mind, we selected images by translating them into pseudo-sentences which we then subsequently ranked. However, note that none of the comparison systems take the mutual dependence of images and sentences into account. In-stead, they select sentences and images in parallel.

Our first baseline selects sentences or images randomly for each document collection (Random). Our second method uses MEAD algorithm [19] to extract sentences and images according to cen-troid value and positional value (Centroid). The third baseline ap-plies the graph-based summarization model proposed by Wan et al. [22]. It first constructs a sentence (or image) connectivity graph based on cosine similarity and then selects important sentences or images based on eigenvector centrality (GBS). We also compared our method against the evolutionary timeline summarization algo-rithm proposed by Yan et al. [28] which is the state of the art but ignores the image stream (ETS).
We evaluated the summaries produced by our system and the baselines automatically using the ROUGE evaluation metric [12]. ROUGE counts the number of overlapping units such as N-grams, word sequences, and word pairs between a candidate summary and reference summaries. There are several variants of ROUGE, all aiming at measuring similarity between system and reference sum-maries. We formally describe ROUGE-N below, one of the most widely used variants. We first define ROUGE-N-R and ROUGE-N-P, two N-gram metrics based on recall and precision, respectively. Here N is the length of the N-gram and N-gram  X  GT denotes the N-grams in the reference timeline GT, while N-gram  X  CT denotes the N-grams in the system timeline CT. Count match (N-gram) is the maximum number of N-grams in the candidate summary and in the set of reference summaries. Count (N-gram) is the number of N-grams in the reference summaries or system summary. Rouge-N is the harmonic mean of ROUGE-N-R and ROUGE-N-P:
We evaluated our textual summaries using all variants provided by the ROUGE package (version 1.55) and obtained similar re-sults across the board. For the sake of brevity we only report ROUGE-1, ROUGE-2, and the weighted longest common subse-quence ROUGE-W (with W set to 1.2). We evaluated our visual summaries in a similar fashion. Recall that we represent images as a bag of visual words. We can therefore use ROUGE to measure the visual word overlap between the images selected by our system and those found in the reference summaries. Since visual words do not have sequential dependencies, we only report ROUGE-1.
Finally, as the timeline consists of a series of individual sum-maries I which are not equally significant, we compute ROUGE-N scores for timelines as the weighted average ROUGE-N of all sum-maries: System R s -1 R s -2 R s -W R m -1 A(s,m) H(s,m) Random 0.317 0.039 0.081 0.126 0.079 0.784 Centroid 0.331 0.050 0.114 0.267 0.223 1.25 GBS 0.364 0.062 0.130 0.283 0.259 1.512 ETS 0.396 0.085 0.139 0.297 0.285 2.016 VTS 0.402 0.087 0.138 0.320 0.376 3.380 System R s -1 R s -2 R s -W R m -1 A(s,m) H(s,m) Random 0.262 0.041 0.096 0.317 0.157 0.512 Centroid 0.369 0.062 0.128 0.317 0.196 1.482 GBS 0.389 0.084 0.139 0.317 0.217 1.782 ETS 0.483 0.119 0.163 0.369 0.298 2.248 VTS 0.486 0.121 0.168 0.394 0.402 3.630 System R s -1 R s -2 R s -W R m -1 A(s,m) H(s,m) Random 0.266 0.043 0.093 0.114 0.139 0.806 Centroid 0.362 0.060 0.129 0.266 0.175 1.651 GBS 0.380 0.106 0.137 0.237 0.198 1.871 ETS 0.481 0.123 0.160 0.316 0.259 2.509 VTS 0.488 0.128 0.163 0.339 0.387 3.595 System R s -1 R s -2 R s -W R m -1 A(s,m) H(s,m) Random 0.254 0.039 0.084 0.085 0.039 0.206 Centroid 0.325 0.053 0.111 0.117 0.099 1.089 GBS 0.359 0.061 0.129 0.128 0.124 0.091 ETS 0.388 0.083 0.134 0.233 0.179 1.780 VTS 0.393 0.103 0.138 0.228 0.339 3.085 We set the weight  X  i to the compression rate for a sentence com-ponent summary or an image component summary.

We also assessed the semantic fit between the selected images and sentences directly. We used Equation (11) to measure the cor-relation between images and texts, under the assumption that higher values indicate higher similarity between sentences and images, and should thus correspond to better timelines.

We also obtained human judgements for the same task. We asked 15 participants to read the output summaries based on texts and im-ages and rate how well the two correlate. Participants used a 5-point rating scale and were advised to use high numbers in cases where texts and images were a good match and low numbers otherwise (0-terrible, 1-bad, 2-normal, 3-good, 4-excellent).
Our results are summarized in Table 4. We report results on each dataset using the following cross-validation scheme: we train parameters on one news set and examine the performance on the others. After 4 training-testing iterations, we take the average per-formance on all sets. We report results on textual summaries us-ing ROUGE-1 (denoted as R s -1 in the table), ROUGE-2 (denoted as R s -2) and ROUGE-W (denoted as R s -W). We use ROUGE-1 to evaluate the similarity between reference visual summaries and system summaries (denoted as R m -1). We also show results using an automatic measure of the semantic fit between textual and visual summaries (denoted as A(s,m)) and the human judgments (denoted as H(s,m)).

As can be seen from Table 4 the VTS approach outperforms the comparison systems on almost all datasets, and evaluation mea-sures. The random baseline (Random) performs worst across the board. This is not entirely surprising as it does not take into account the importance of texts or images. The centroid-based system (Cen-troid) performs better than Random as it tries to identify important sentences and images by taking into account positional informa-tion as well as content overlap. The GBS system outperforms the Centroid method in terms of ROUGE and the correlation evaluation introduced in Section 4.4. This is due to the fact that the PageRank-based framework ranks sentences (and images) using eigenvector centrality which implicitly accounts for information subsumption among all sentences or images. ETS produces better timelines than more traditional methods since it has explicit mechanisms for cap-T able 5: Performance of individual components and compo-nent combinations for the VTS system. turing how information evolves over time. A major difference be-tween ETS and VTS is that the former does not take the correlation between images and text into account. Visual and textual timelines are generated independently and as result the fit between the two is not perfect. Taking into account the dependency between the two modalities, improves textual and visual summarization across the board and also produces thematically coherent timelines with the textual and visual components being properly matched. Notice that VTS outperforms all comparison methods on A(s,m) and H(s,m) scores by a large margin on all datasets.

The results in Table 4 have been produced with optimal parame-ter values. We explore the influence of different parameters in the following sections. Examples of system output are given in Ta-bles 6 and 7 using a compression rate of 5%. Our implementation allows users to have access to the source documents by clicking on the extracted sentences that make up the output summary.
We next examine the relative contribution of the individual com-ponents of our algorithm. Specifically, we assess the algorithm X  X  performance using only one of the three affinity matrices: Lo-cal Homogeneous Affinity (LOA), Local Heterogeneous Affinity (LEA) and Global Homogeneous Affinity (GHA). We also perform ablation studies where one component is removed at a time. Table 5 shows the performance of the individual components (upper half) and the results of our ablation studies (lower half; we use the sym-bol  X   X   X  to indicate which component has been removed). For com-parison, we also show the performance of our VTS system. Scores in the table are averages across all four datasets. We use boldface to indicate which components incur remarkable performance changes. As can be seen, LOA has the highest effect on system performance when the latter is measured in terms of ROUGE. LEA on the other hand is important for achieving a good semantic fit between visual and textual summaries. This component, on its own, achieves the highest A(s,m) score. Its removal from the VTS system results in the lowest A(s,m) score.
Recall that our co-ranking framework is parameterized with re-spect to the relative influence of three different affinities. Parame-ters  X  ,  X  , and  X  (see equations (5) and (6)) control the contribution of the local homogeneous, local heterogeneous, and global homo-geneous affinities. We performed a series of experiments to deter-mine the optimal parameter values for our VTS framework. We varied one parameter at a time, keeping all other parameters fixed as illustrated in Figure 3. Our results indicate that the local ho-mogeneous affinity (parameter  X  ) has the greatest effect on system performance. High values of the  X  parameter (global homogeneous affinity) have a negative influence on performance as do very low or very high values of the  X  parameter (local heterogeneous affinity). Optimal parameter values were set to  X  =0.6,  X  =0.3, and  X  =0.1. We also experimented with the effect of the compression rate  X  . Note that typically  X  is regulated by users. For example, users who want to read more content, might favor a larger  X  . We var-ied  X  from 0.1 to 1 with a step of 0.1. Generally, the ROUGE lines are down-sloping (see Figure 3d) as our ground truth timelines are small compared to the large source collection our system has ac-cess to. Interestingly, the correlation line (see A(s,m) in Figure 3d) is more stable, indicating the intrinsic dependence between texts and images within the news documents.

Finally, an important parameter in timeline summarization is  X  which controls the influence of the temporal projection for sen-tences/images from different dates and thus influences the weights of the neighboring affinity. Changes in  X  values do not incur large differences in performance (see Figure 3f). This is partly due to the small value of  X  =0.1. We experimented with  X   X  [20, 80] and em-pirically set the parameter to  X  =35. Given this value, we next ex-amined the effect of different projections. Generally, the Gaussian kernel is the best performing projection and the window kernel the worst performing one. We attribute this to the fact that the Gaus-sian kernel provides the best smoothing effect without imposing any arbitrary cutoffs.
In this paper we introduced visual timeline summarization, a novel summarization task that creates visual and textual timeline summaries for news topics. We proposed a framework that ranks images and sentences jointly whilst taking into account how a sen-tence (or an image) associates to other sentences (or images), how sentences and images related to each other, and how the two re-late to the overall textual and visual summaries being created. Our model explores the relationship between elements within the same modality and across modalities. We proposed an algorithm that se-lects images and sentences through mutual reinforcement: it uses the global timeline summary to iteratively refine the local compo-nent summaries.

Experimental results on four datasets show that our system out-performs previously proposed competitive baselines. Analysis of the components of our system and its parameters indicates that the semantic fit between images and sentences is important for VTS (  X  =0.3) as well as taking auxiliary global information into account (  X  =0.1). The local homogeneous affinity would perform best in isolation and its absence would lead to the most significant perfor-mance drop.

Currently our model treats sentences and images equally with-out using any prior information with respect to how important they are. In the future, we plan to use prior knowledge to select seed sentences and images. Such knowledge can be based on the posi-tional information of sentences in the document, their timestamps and so on. This way we can ensure better local optima and faster convergence. We thank the anonymous reviewers for their valuable feedback. This work was partially supported by NSFC Grant No.60933004 and HGJ Grant No. 2011ZX01042-001-001; Xiaojun Wan was supported by NSFC Grant No. 61170166, Beijing Nova Program (2008B03) and the National High Technology Research and De-velopment Program of China (2012AA011101). Rui Yan was sup-ported by the MediaTek Fellowship. [1] J. Allan, R. Gupta, and V. Khandelwal. Temporal summaries [2] K. Barnard, P. Duygulu, D. Forsyth, N. de Freitas, D. M. [3] H. L. Chieu and Y. K. Lee. Query based event extraction [4] P. Duygulu, K. Barnard, J. De Freitas, and D. Forsyth. Object [5] G. Erkan and D. Radev. Lexpagerank: Prestige in [6] Y. Feng and M. Lapata. Automatic image annotation using [7] Y. Feng and M. Lapata. How many words is a picture worth? [8] Y. Feng and M. Lapata. Topic models for image annotation [9] J. Goldstein, M. Kantrowitz, V. Mittal, and J. Carbonell. [10] A. Langville and C. Meyer. Deeper inside pagerank. Internet [11] C.-Y. Lin and E. Hovy. From single to multi-document [12] C.-Y. Lin and E. Hovy. Automatic evaluation of summaries [13] D. Lowe. Object recognition from local scale-invariant [14] Y. Lu, J. He, D. Shan, and H. Yan. Recommending citations [15] Y. Lv and C. Zhai. Positional language models for [16] I. Mani. Automatic Summarization . John Benjamins Pub Co, [17] R. Mihalcea and P. Tarau. A language independent algorithm [18] J. Neto, A. Santos, C. Kaestner, D. Santos, et al. Document [19] D. Radev, H. Jing, M. Stys, and D. Tam. Centroid-based [20] K. Sparck Jones. Automatic summarizing: Factors and [21] R. Swan and J. Allan. Automatic generation of overview [22] X. Wan and J. Yang. Multi-document summarization using [23] X. Wan, J. Yang, and J. Xiao. Manifold-ranking based [24] X. Wan, J. Yang, and J. Xiao. Single document [25] D. Wang and T. Li. Document update summarization using [26] D. Wang, T. Li, S. Zhu, and C. Ding. Multi-document [27] R. Yan, L. Kong, C. Huang, X. Wan, X. Li, and Y. Zhang. [28] R. Yan, X. Wan, J. Otterbacher, L. Kong, X. Li, and s : The World Health
