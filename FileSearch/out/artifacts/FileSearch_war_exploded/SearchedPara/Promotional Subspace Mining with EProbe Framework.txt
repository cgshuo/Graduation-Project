 In multidimensional data, Promotional Subspace Mining (PSM) aims to find out outstanding subspaces for a given object, and to discover meaningful rules from them. In PSM, one major research issue is to produce top subspaces efficiently given a predefined sub-space ranking measure. A common approach is to achieve an exact solution, which searches through the entire subspace search space and evaluate the target object X  X  rank in every subspace, assisted with possible pruning strategies. In this paper, we propose EProbe, an E fficient Subspace Prob ing framework. This novel framework strives to initialize the idea of  X  X arly stop X  of the top subspace search process. The essential goal is to provide a scalable, cost-effective, and flexible solution where its accuracy can be traded with the efficiency using adjustable parameters. This framework is especially useful when the computation resources are insufficient and only a limited number of candidate subspaces can be evalu-ated. As a first attempt to seek solutions under EProbe framework, we propose two novel algorithms SRatio and SlidingCluster. In our experiments, we illustrate that th ese two algorithms could produce a more effective subspace traversal order. Being effective, the top-k subspaces included in the final results are shown to be evaluated in the early stage of the subspace traversal process.
 Categories and Subject Descriptors General Terms: Algorithms Keywords: promotional subspace mining, top subspaces, early stop
In many real-world applications, objects are given ranks based on a predefined score measurement. Whether for a single person or a merchandize product, they can be ranked because there exist comparable objects in the same category. For examples, an athlete has other athletes as competitors in terms of the performance score; and a product has other similar products as competitors in terms of the sales revenue. It is worth noting that, for multi-dimensional data, a target object X  X  rank may vary in different subspaces. To find out the potential top subspaces, in many areas, can be very useful. One motivating application area is to promote the target object from the subspaces where it ranks high. For example, when promoting a notebook computer, the information  X  X anked the 3 rd in terms of battery life X  can be more informative than the information  X  X anked the 15 th in an overall feature evaluation X  for a potential buyer. The problem then becomes, how to pinpoint these top subspaces? In [1], the authors proposed Promotion Query , a database query function with the functionality of returning top ranked subspaces with a multidimensional data set as the input. The authors propose two categories of methods: (1) evaluating through the entire sub-space search space, assisted with some pruning techniques; and (2) constructing data cubes to make some calculations off-line, so as to achieve fast querying operations in OLAP applications. How-ever, the major challenges that pose on these methods include the scalability issue, and the complexity of the data cubes X  design &amp; maintenance issue.

By considering these important research issues, we suggest a completely different approach for the problem of producing top subspaces in multi-dimensiona l data. We propose EProbe  X  an E fficient Subspace Prob ing framework. As opposed to commonly adopted strategies that produce an exact solution, this novel frame-work aims at generating a flexible solution where its accuracy can be traded with efficiency using adjustable parameters. The ideal situation is, when the computation resources suffice for the task, an exact solution will be produced; otherwise, the accuracy of the result can be sacrificed with the return of the computational speed-up.

As a first effort to implement this EProbe framework, we pro-pose two heuristics, which lead to two novel algorithms SRatio and SlidingCluster. The former applies score ratio ( SR ) based subspace sorting to obtain a sorted subspace set. The latter further includes the design of subspace sampling from sliding subspace clusters. Both algorithms strive to achieve an  X  X arly stop X  of the subspace search, when a certain number of t op subspaces have been probed and evaluated. By using two evaluation metrics: AVG TraceIn-dex and Coverage , we compare SRatio and SlidingCluster with a baseline algorithm DFP ( D epth F irst P arent Subspace Pruning). We show an remarkable superiority of algorithm SRatio (Sliding-Cluster with w =1 ) over DFP; and a consistent and significant improvement of algorithm SlidingCluster over SRatio, when only a limited number of candidate subspaces can be evaluated.
The remaining of this article is organized as follows. In Section 2, we define relevant concepts and introduce necessary notations. In Section 3, we present the EProbe framework and the heuristics that generate algorithms SRatio and SlidingCluster. In Section 4, we describe our data and present the experimental results. We fi-nally conclude this work and discuss our future work in Section 5.
We first introduce the preliminaries and definitions in Section 2.1, then we formulate our problem in Section 2.2.
Assuming we are given a data set D , which consists of a set of n instances. Also assuming that D has d categorical attributes A = { A 1 ,A 2 ,...,A d } , an object attribute A obj denoting object IDs, and a score attribute A score denoting the non-negative score of the corresponding object in a specific subspace. We denote the domain in which a variable X takes value from as dom ( Specifically, we denote the complete set of competitive objects by O = dom ( A obj ) , in which the target object ID t a  X  O is given by the user. We also have dom ( A score )= R + . A subspace is represented as S = { A 1 = a 11 ,A 2 = a 21 ,  X  X  X  ,A d = a where a i 1  X  dom ( A i ) or a i 1 =  X  . The value  X * X  refers to  X  X ny X , which indicates any value in the corresponding attribute is included. S induces a projection of the data set D S (  X  D subspace of objects O S (  X  O ) . We represent the full attribute space S  X  = { A 1 =  X  ,A 2 =  X  ,  X  X  X  ,A d =  X  X  , a short form of which is S  X  = { X  X  . We denote N 0 as the number of candidate spaces in the entire subspace search space.

Due to the space limit, please refer to [1] for the definitions of concepts  X  X arent-Child Subspace X ,  X  X een-Unseen Subspace X ,  X  Rank of an object X ,  X  X ignificance Sig  X , and  X  X romotiveness P  X .
Definition 1: ( SR : Score Ratio of an Object) If the score of tar-get object A in subspace S is denoted as SUM S ( t a ) ,andthescore of all the objects in S is denoted as t  X  O S SUM S ( t ) ratio SR of t a in S is defined as the ratio of these two measures, as shown in Eq. (1).

Definition 2: ( TraceIndex of a Subspace) The TraceIndex of subspace S , TraceIndex ( S ) is defined as the ordinal position, at which S is being evaluated. TraceIndex ( S )  X  [1 ,N 0 ] .
Definition 3: ( Distance : Distance between Two Subspaces) Given subspace S 1 = { A 1 = a 11 ,A 2 = a 21 ,  X  X  X  ,A d = a d 1 space S 2 = { A 1 = a 12 ,A 2 = a 22 ,  X  X  X  ,A d = a d 2 } between S 1 and S 2 is defined as the Hamming Distance between S 1 and S 2 , i.e. the number of positions at which the corresponding attribute values are different [2]. Distance ( S 1 ,S 2 ) lated as shown in Eq. (2).
 In Eq. (2), I denotes an indicator function, which returns 1 when the values on the corresponding attribute does not equal and 0 oth-erwise. If S 1 is a parent subspace of S 2 , Distance ( S Distance ( S 2 ,S 1 ) =1.
Based on the notations and definitions specified in Section 2.1, we present the target problem as follows. Assuming we are given multi-dimensional data set D , target object A , and subspace rank-ing measure P . Assuming I k is the subspace set that contains the subspaces with k largest P ,and r ( r  X  N 0 ) is the number of sub-spaces that can be evaluated.

For a given r , the proposed problem is to find a subspace set I ( |
I r | = r ), in which k r of these r subspaces belong to I k k ( k r  X  k ) is maximized.
EProbe is an efficient subspace probing framework that aims at providing a scalable, cost-effective, and flexible solution for pro-ducing top-k subspaces in multidimensional data. The essential concept of EProbe is the development of an accuracy-efficiency tradable solution along with the limited computation resources. Fol-lowing this framework, we propose the idea of  X  X arly stop X  of the subspace search, when a certain number of top subspaces have been probed and evaluated. To be specific, we intend to find a special subspace traversal order, such that the subspaces being evaluated earlier have higher probability to be included in the final top-k subspace result set. The ideal situation is, when the computation resources suffice for the task, an exact solution will be produced; otherwise, the accuracy of the result can be sacrificed with the re-turn of the computational speed-up.

The question is, how feasible it is to design an  X  X arly stop X  cri-terion? Obviously, it is impossible to define an exact measure that positively correlates P while maintains a time complexity at the same level as or a lower level than P , because the target object X  X  Rank is non-monotonic over any parent-child subspaces, and the subspace ranking measure P is a monotonic function of Rank .
However, when considering the attributes of the real-world data, it is still feasible to produce a more efficient subspace traversal or-der. For a candidate subspace and a given target object, the ranking measure P value is closely related to two factors: internal and ex-ternal factors, respectively. First, it is positively correlated with the score of the target object itself. Second, it is negatively cor-related with the combined score effects of other competitive ob-jects. In other words, both a higher core of the target object, and a lower score of competitive objects will contribute to a higher P . Motivated from the observation that the score distribution of all the competitive objects in the data partly reflects their comparative po-sition, we develop following heuristic.

Heuristic 1: Given target object A , a subspace X  score ratio SR tends to be positively correlated with the measure P .

Based on this heuristic, we propose algorithm SRatio, in which the subspace traversal order is formed based on the score ratio SR . In other words, the SR of every candidate subspace is calculated first, and their P values will be evaluated with a descending order of their corresponding SR .

Although the score ratio SR is a good indicator of the compar-ative position of the target object among its peers, it still cannot serve as a precise metric to correlate the P value. This is because the impact coming from the external factor  X  the score fluctuation
Figure 1 shows the three improving algorithm designs, of which de-sign (a) shows a random or non-metric based subspace traversal order; design (b) shows a metric based subspace traversal order; and design (c) shows a metric based and learning/adjustment enhanced traversal order. The algorithms DFP, SRatio, and SlidingCluster are represen-tatives of these three designs, respectively. We compare these three methods in Section 4. of other competitive objects also play an important role. As shown in Figure 1 (c), it will be more effective if a learning and/or adjust-ment module that considers this external factor can be appropriately designed. Therefore, we come up with following heuristic, based on which the algorithm SlidingCluster is proposed.

Heuristic 2: Subspaces with close SR and close genealogy rela-tionship tend to share similar value of measure P .

The genealogy relationship between subspaces S 1 and S 2 can be measured by the Distance measure, as shown in Eq. (2). When Distance ( S 1 ,S 2 )=1 , S 1 and S 2 are regarded having the clos-est genealogy relationship. This heuristic is formed based on the following observations. An object X  X  score in a subspace S is accu-mulated by a set of child subspaces of S . Assuming the object A shows high score in a parent subspace compared to other objects, it is very likely, if not guaranteed, that object A excels in one or more child subspaces as well. Therefore, there exist many situa-tions in which parent-child subspaces may share very close value of measure P . In Figure 2, we illustrate the idea of algorithm SlidingCluster. At first, the same as algorithm SRatio, the candidate subspaces are ordered based on the score ratio SR. Then the ordered sequence of candidate subspaces will be split into neighboring clusters based on Heuristic 2 . By specifying a window size w , one candidate sub-space will be sampled from each of the top w clusters. After cal-culating the measure P of these sampled subspaces, the subspace with the largest P is selected, and the corresponding cluster will be selected as well. The remaining subspaces in this cluster then will be evaluated. Following that, the next subspace cluster is in-cluded into the current cluster window. And again, a subspace will be sampled, evaluated, and compared from this cluster. This proce-dure continues until all the subspace clusters are evaluated. In Section 3, we have proposed two new design components: SR based subspace sorting, and sliding cluster subspace sampling, which are distinguished from any random or non-metric based sub-space exploration methods. We develop algorithm SRatio by con-sidering the first component, and algorithm SlidingCluster by con-sidering both components. The essential purpose of our experi-ments is to explore whether these two algorithms can produce a more effective subspace traversal order. Being effective, the top-k subspaces to be included in the final results are to be evaluated in the early stage of the subspace traversal process. For comparison purpose, we use a baseline method DFP, which applies a depth first subspace evaluation order from the root subspace ( S  X  = { X  X  leaf subspaces (dimension attribute with non-star values). In the remaining of this section, we first introduce the experiment settings and evaluation measures in Section 4.1, then we report and analyze the experimental results in Section 4.2.

Objects can be ranked by their aggregated score. We use 6 dif-ferentobjectsA,B,C,D,E,F,asthetargetobjectstoper-form the experiments. These objects are ranked 50, 28, 27, 23, 18, and 22 respectively, among 89 objects, in the full attribute spaces. N ( S i ) denotes the number of candidate subspaces in the corresponding subspace search space. The  X  X luster Size X  column shows the number of subspace clusters being produced based on Heuristic 2 .
We perform our experiments on a machine with an Intel Core i5 2.27GHz processor, and 3.5GB of memory. We implement al-gorithms DFP, SRatio, and SlidingCluster using JDK 1.6 on Linux kernel 2.6.34 OS. When we evaluate the comparative experimental results on these three algorithms, we develop the AVG TraceIndex and the Coverage as the major evaluation measures, where these two measures can be calculated based on Eq. (3) and Eq. (4) re-spectively. We report the results with two varying variables: the window size w , and different target objects with varying number of candidate subspaces N ( S i ) . We set the distance d =1 threshold minsup = 100.
 As defined in Definition 2 ,the TraceIndex of a subspace denotes the ordinal position on which this subspace is being evaluated. For the top-k subspaces that are eventually produced, the objective is to rather evaluate them on earlier ordinal positions. Therefore, the average TraceIndex of the top-k subspaces, as shown in Eq. (3), qualifies a good indicator of the performance.
 In this study, the proposed problem is to obtain top subspaces when only a limited number of subspaces can be evaluated. It indicates that when the subspace evaluation is terminated in an intermediate spot without finishing traversing the search space N 0 , it is hoped that a significant portion of the top-k subspaces have been evalu-ated. The measure Coverage , as defined in Eq. (4), indicates the percentage of the subspaces that are in the final top-k result when the first r subspaces are evaluated. I denotes an indicator function, which returns 1 when the condition is true and 0 otherwise. forces that only those subspaces with TraceIndex not greater than r will count.
In this experiment, we use Product_Sales data. Product_Sales data contains 66,801 record tuples, and each record has six cate-gorical subspace attributes, the cardinalities of whi ch are 6, 305, 8, 25, 3, 30 and 4, respectively; and one non-negative numerical at-tribute Score . The theme of this data set is about the sales revenue of different product brands. In Table 1, we show the characteristics of the target objects we experiment with. We choose these six spe-cific target objects mainly because we intend to investigate how it affects the performance of algorithms when the number of candi-date subspaces increase. We do not disclose the detailed theme of the data for privacy reasons.
Varying w . w represents the window size of subspace clusters applied in the subspace sampling procedure in algorithm Sliding-Cluster. Figure 3 and 4 respectively show the comparison result on measures AVG TraceIndex and Coverage of the three algo-rithms: DFP, SRatio, and SlidingCluster, along with the varying window size w ,where w  X  X  1 , 2 ,  X  X  X  , 19 , 20 } ,and k = The experiment trial with low AVG TraceIndex and high Cover-age indicates a promising result. When the window size w =1, algorithm SlidingCluster degrades to SRatio, we therefore use a big sized  X  X  X  to represent the corresponding data point. For algo-rithm DFP, only Figure 3 (a) shows the corresponding data point. It is because the data points are all positioned out of the plot area for other experiment trials, when comparing the results with the same scale. As a remedy, we record AVG TraceIndex of DFP algorithm on the corresponding legend. Because the subspace traversal or-der of algorithm DFP is not manipulated, its average TraceIndex of the top-k subspaces is significantly higher than those produced by SRatio and SlidingCluster. Meanwhile, when the window size w enlarges, the average TraceIndex further decreases consistently. These evidences clearly indicate that, the average ordinal order of the top subspaces being evaluated has been brought forward effec-tively when using our proposed methods. As a result, SRatio and SlidingCluster have higher chance than DFP to produce the most important subspaces when the com putation resources are limited and only the first r of N 0 candidate subspaces can be evaluated.
We are able to draw the same conclusion by examining Figure 4 as well. This figure shows the percentage of the top subspaces that are actually catched when the first r subspaces have been eval-uated. Generally speaking, our experiments show an remarkable advance of algorithm SRatio over algorithm DFP; and a significant improvement of algorithm SlidingCluster over algorithm SRatio. When the window size w enlarges, the Coverage of SlidingClus-ter consistently improves. Taking Figure 4 (b) as an example, we suppose the computation resources are only sufficient to finish eval-uating the first r = 100 subspaces. For algorithm DFP, only 9% subspaces in the final top k = 100 subspaces will have been eval-uated by the time of algorithm terminating at r = 100 .However, 80% and 87% top subspaces will have been evaluated for SRatio and SlidingCluster with w =20 . Figure 4: Subspace Coverage Comparison with Varying w
In multi-dimensional data, the problem of promotional subspace mining (PSM) aims to find out outstanding subspaces for promot-ing a given object among a group of competitive objects. One major research problem is how to produce the top subspaces efficiently given a predefined subspace ranking measure. In this paper, we proposed a novel EProbe framework to address the scalability is-sue of this problem under the assumption that limited computation resources are enforced. We initialized the  X  X arly stop X  idea by de-signing the algorithms SRatio and SlidingCluster, and showed very promising results in our experiments. We plan on our future work in two directions. Firstly, we will perform extensive experiments on data sets with different concepts and distribution, on different target objects within the same data set, and on relatively small or large k top subspaces. We will design and develop statistical mod-els that consider the window size w , the number of subspaces r that should be evaluated, and the expected Coverage to achieve. Sec-ondly, we have plan to refine the learning/adjustment module of the EProbe framework. In algorithm SlidingCluster, we had assumed that the values of measure P within the same subspace cluster were close. However, this assumption may be violated if the ranks of the competitive objects change violently. We will design a self-adaptive machine learning unit, which updates the window size w and the distance d according to the values of measure P of already evaluated subspaces. We will also investigate more sophisticated partition clustering methods [3, 4]. [1] Tianyi Wu, Tong Xin, Qiaozhu Mei, and Jiawei Han.
 [2] R. W. Hamming. Error detecting and error correcting codes. [3] Ian Davidson, Kiri L. Wagstaff, and Sugato Basu. Measuring [4] Omar Alonso, Michael Gertz, and Ricardo Baeza-Yates.
