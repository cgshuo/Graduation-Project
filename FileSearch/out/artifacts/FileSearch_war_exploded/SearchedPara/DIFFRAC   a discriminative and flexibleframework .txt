 Many clustering frameworks have already been proposed, wit h numerous applications in machine similarity structure over the space of configurations.
 of
M , and essential to the success of our approach. we show how the convex relaxed problem can be solved efficient ly through a sequence of lower our new set of algorithms to other competing approaches. In this section, we first assume that we are given n points x matrices y  X  { 0 , 1 } n  X  k such that y 1 ones, of dimensions k and n . We let denote I 2.1 Discriminative clustering cost where the Frobenius norm is defined for any vector or rectangu lar matrix as k A k 2 the data; namely, the global optimum is attained at w  X  = ( X  X   X  value is then equal to where the n  X  n matrix A ( X,  X  ) is defined as: singular vector of A ( X,  X  ) , i.e., A ( X,  X  )1 relaxations may be obtained, as presented in the next sectio n. 2.2 Indicator and equivalence matrices indicator matrix y with M = yy  X  .
 There are many outer convex approximations of the discrete s ets E matrices in E if (denoted as M &gt; 0 ) , and (c) the diagonal of M is equal to 1 Moreover, if M corresponds to at most k clusters, we have M &lt; 1 convex outer approximation: constraints. 2.3 Minimum cluster sizes Given the discriminative nature of our cost function (and in particular that A ( X,  X  )1 minimum value 0 is always obtained with M = 1 the size of those clusters. Following [1], we impose a minimu m size  X  row sums and eigenvalues: Row sums If M  X  E smaller than ( n  X  ( k  X  1)  X  Eigenvalues When M  X  E Thus, for a matrix in E k , where  X  P and convexity [8]. The previous constraint can be seen as  X ( M ) &gt; k , with  X  (  X  ) = 1 concave upper envelope of this function, namely  X  additional constraint.
 such that  X  P i =1 min {  X  i ( M ) / X  0 , 1 } our simulations we use  X  2.4 Comparison with K-means ized case (  X  = 0 ), we aim to minimize second-order polynomials or, equivalently, a polynomial k ernel. 2.5 Kernels The matrix A ( X,  X  ) in Eq. (3) can be expressed only in terms of the Gram matrix K = XX  X  . Indeed, using the matrix inversion lemma, we get: where e K =  X  framework with any positive definite kernel [5]. 2.6 Additional relaxations (1) relaxing the constraints M &lt; 1 D which is always between 0 and 1. shows that this relaxation leads to an eigenvalue problem: l et A = P n decomposition of A , where a relaxed convex optimization problem is attained at M = P j optimization algorithm in Section 3.
 In the kernel formulation, since the smallest eigenvectors of A = 1 keep in order to achieve the best embedding and clustering is the constraint diag( M ) = 1 Since  X  is reduce the computational load. 3.1 Optimization by partial dualization decompositions for large scale algorithms.
 tained my maximizing F (  X  ) = min and  X  The variables  X  The function J ( B ) = min computed in closed form through an eigenvalue decompositio n. Moreover, a subgradient may be than n 2 . Indeed, if we subsample the pointwise positivity constrai nt N &gt; 0 (so that  X  size in n (instead of the primal variable M being quadratic in n ). of a smoothed function is generally faster than subgradient iterations) [13]. 3.2 Computational complexity simulations we limit the number of those iterations to 200.
 in n . 3.3 Rounding After the convex optimization, we obtain a low-rank matrix M  X  C E M and can thus easily be included in our convex formulation.
 We assume throughout this section that we have a set of  X  X ust-link X  pairs P not-link X  pairs P forming a pair in P can be obtained by transitive closure. ber of irrelevant dimensions, with 20%  X  n and 40%  X  n random matching pairs used for semi-supervision.
 Positive constraints Given our closure assumption on P p points that must occur together in the final partition. We let C one for rows in C form M = P M M which is equivalent when M  X  E In Figure 2, we compare constrained K-means and the D IFFRAC framework under the same setting as in Figure 1, with different numbers of randomly selected p ositive constraints. rounding procedure also has to be constrained, e.g., using t he procedure of [17]. k + k  X   X  2 partitions have the same number of clusters). 5.1 Clustering classification datasets we compare the performances of K-means, RCA [18] and D IFFRAC , for linear and Gaussian ker-sion. nels, D IFFRAC outperforms both K-means and RCA. Note that all algorithms w ork on the same frameworks. 5.2 Semi-supervised classification is straightforward.
 as hints on clusters are worth for investigation and further research. clusters using variation rates of our discriminative costs are very promising.
