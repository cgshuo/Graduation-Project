 Large scale knowledge bases such as WordNet [1] and FreeBase [2] are important resources for natural language processing (NLP) applications like web search-ing [3], automatic question answering systems [4], and even medical informat-ics [5]. Formally, a knowledge base is a dataset containing triples of two entities and their relation. A triplet ( h, r, t ), for example, indicates that the head entity h and the tail entity t have a relation r . Despite massive triplets a knowledge base contains, evidence in the literature suggests that existing knowledge bases are far from complete [6, 7].
 matically construct or populate knowledge bases from plain texts [6, 8], semi-structured data on the Web [9, 10], etc. Recently, studies show that embedding the entities and relations of a knowledge base into a continuous vector space is an e  X  ective way to integrate the global information in the existing knowledge base and to predict missing triplets without using external resources (i.e., additional text or tables) [11, 7, 12 X 14].
 e mbeddings by that of a relation, to model knowledge bases. That is to say, the relation between two entities can be represented as a vector o  X  set, similar to word analogy tasks for word embeddings [15] and sentence relation classification by sentence embeddings [16]. For one-to-many, many-to-one, and many-to-many relations, however, such straightforward vector o  X  set does not make much sense. Considering a head entity China and the country-city relation, we can think of multiple plausible tail entities like Beijing , Tianjin , and Shanghai .These entities cannot be captured in the same time by translating the head entity and relation embeddings. Therefore, researchers propose to map entities to a new space where embedding translation is computed, resulting in TransH [7], TransR [12], and other variants. Among the above approaches, TransR achieves the highest performance on established benchmarks.
 one-hop relation) between two entities is considered. In a knowledge base, some entities and relations only appear a few times; they su  X  er from the problem of data sparsity during training. Fortunately, the problem can be alleviated by using multi-hop information in a knowledge base. Guu et al. [13] present a ran-dom walk approach to sample entities with composited relations. Likewise, Lin et al. [14] propose a path-augmenting approach that uses multi-hop relations between two entities to regularize the direct relation between the same entity pair. Their experiments show the p ath-augmented TransE model (denoted as PTransE) outperforms the one-hop TransE model.
 the path-augmenting technique is also useful to a better one-hop  X  X ase X  model. Therefore, we propose to leverage TransR [12] as our cornerstone, but enhance it with p ath information as in [14], resulting a new variant, PTransR. We evaluate our model on the FreeBase dataset. Experimental results show that modeling relation paths is beneficial to the base model TransR, and that PTransR also outperforms PTransE in entity prediction. In this way, we achieve the state-of-the-art link prediction performance in the category that uses only the knowledge base itself (i.e., without additional textual information).
 base model TransR and then discuss the path-augmented variant PTransR. In Section 3, we compare our PTransR model with other baselines in an entity prediction experiment; we also have in-depth analysis regarding di  X  erent groups of relations, namely 1-to-1, 1-to-n , n -to-1, and n -to-n relations. In Section 4, we briefly review previous work in information extraction. Finally, we conclude our paper in Section 5. In this section, we present our PTransR model in detail. In Subsection 2.1, we introduce the TransE model and explain how TransR overcomes the weakness of TransE. Then, we augment TransR model with path information in Subsec-tion 2.2.
 2.1 Base Model: TransR As said in Section 1, embedding entities and their relation into vector spaces can e  X  ectively exploit internal structures that a knowledge base contains, and thus is helpful in predicting missing triplets without using additional texts. and their relation in a same low-dimensional vector space; the two entities X  em-beddings are translated by a relation embedding, which can be viewed as an bold letters refer to the embeddings of head/tail entities and the relation.) The plausibility of a triplet ( h, r, t ) is then evaluated by a scoring function ( h, r, t ) is a positve triplet.
 lations into four groups, namely 1-to-1, 1-to-n , n -to-1, and n -to-n , according to the mapping properties of a relataion. For example, country-city is a 1-to-n relation, because a country may have multiple cities, but a city belongs to only one country. The weakness of TransE is that entity embeddings on the many side tend to be close to each other, which is the result of expecting f r ( h , t )to be small for all positive triplets. Therefore, it is hard for TransE to distinguish among the entities which are on the many side.
 two separate spaces: the entity space and the relation space . It uses relation-specific matrices M r to map an entity from its own space to the relation space, given by h r = M r h and t r = M r t , so that translation can be accomplished by regarding relation embedding as an o  X  set vector, i.e., h r + r  X  t r . To achieve this goal, TransR defines the scoring function as loss. The overall cost function of the TransR model is where negative samples are constructed as established benchmarks, indicating TransR is the best one-hop model at present. However, TransR fails to utilize the rich path information, which will be dealt with in the following subsection. 2.2 Path-Augmented TransR: PTransR Using path information to regularize one-hop models can be beneficial [14, 13]. Here, we adopt the path modeling method in PTransE, and extend the TransR model to path-augmented TransR (denoted as PTransR).
 satisfying h r 1 ! e 1 r 2 !  X  X  X  r n ! t .If n = 1, then p = r 1 is a direct (1-hop) relation. To enhance TransR model with multi-hop information, we follow the treatment in PTransE [14] and represent a relation path as an embedding vector by additive compositional methods. Then such multi-hop information is used to regularize one-hop direct relation between the same entity pair. A reliability score is computed to address the strength of regularization by a particular path. Details are described as follows.
 relations r 1 ,r 2 ,  X  X  X  ,r n ,i.e., p = r 1 r 2  X  X  X  r n (where denotes the composition operation), we add the embeddings of these primitive relations, given by where bold letters denote the vector of a relation or a path.
 the vector representation of path p should be close to that of direct relation r if it is likely to infer r from p . For example, the representation of path father ! mother ! is expected to be close to that of direct relation grandmother ! .
 two entities, not every path is equally useful for inferring direct relations. For ex-ample, the relation path John friend ! Tim gender ! male gives little contribution to inferring the gender of John .
 algorithm (PCRA) [14], which is also applied in our approach. This algorithm first assigns a certain amount of resource (i.e., a value of 1) to the head entity h ; then each node distributes resource evenly to its direct child nodes (Figure 2). The value of p along an entity pair h, t is denoted as v ( p | h, t ). path p and a direct relation r . To address this problem, a relatedness measure is training triplet ( h, r, t )with p as a relation path from h to t . P r ( r, p )isthesum as a relation path. The overall reliability of a path p on a triplet ( h, r, t ) is given by E ( h, r, t ) is the same as the scoring function of TransR (Equation 2) which evaluates the plausibility of ( h, r, t ) without considering relation paths from h to t . Following PTransE, E ( P | h, r, t ) is defined as where Z is a normalizing factor for R ( p | h, t ) and P is the set of all paths from relation paths from h to t . The overall loss function of PTransR is PTransR learns entity and relation embeddings by minimizing L PTransR . 2.3 Training Details We train PTransR by mainly following PTransE [14].
 Initial vectors and matrices. Following TransR, initial vectors and matrices for PTransR are obtained from TransE. The configuration of TransE is: margin = 1, learning rate  X  =0 . 01, method = unif, and epoch = 1000.
 Negative samples. We sample negative triplets by randomly replacing head en-tity h or tail entity t or relation r . For example, ( h, r, t )-derived negative triplets ( h, r, t 0 ) / 2 S .
 Vector representation constraints. Following TransR, to regularize the rep-resentations, we impose the following constraints on the entity and relation em-beddings. Path selection. PTranE restricts the length of path to less than 3. Its results show that 3-hop paths do not make significant improvement, compared to 2-hop paths. For e ciency, we only consider 2-hop relation paths.
 Inverse relation. As inverse relations sometimes contain useful information, for each training triplet ( h, r, t ), ( t, r 1 ,h ) is added to the training set. In this section, we present results of our experiment. We first briefly introduce the dataset and the task of entity prediction. Then we show the experimental results and analyze the performance. 3.1 Dataset FB15k is a commonly used dataset in knowledge base completion. Table 1 shows statistics of FB15k. FB15k dataset contains factual information in our world, e.g., location/country/language spoken . As FB15k has various kinds of re-lation, it is suitable for the evaluation of PTransR. Therefore, we choose FB15k as our experimental dataset. 3.2 Experimental Settings We evaluate PtransR on the task of entity prediction. Entity prediction aims at predicting the missing entity in an incomplete triplet, i.e., predicting h given r and t , or predicting t given h and r . Following the settings in TransE, for a triplet ( h, r, t ), we replace the head entity h with every entity e and compute the score of ( e, r, t ). Entity candidates are ranked according to their scores. We repeat the same process to predict the tail entity t .Thenweusethetwometrices in TransE to evaluate the performance: MeanRank (average rank of the expected entity) and Hits@10 (proportion of triplets whose head/tail entity is among top-10 in the ranking). However, there could be several entities that are plausible for the same incomplete triplet. The plausible entities which are ranked before h or t may cause underestimation of performance. One solution is to remove other plausible entities in the ranking, which is referred to as a filter . In comparison, the results without removing other plausible entities are referred to as raw .A good model should achieve low MeanRank and high Hits@10.
 i.e., We first rank all candidates according to their scores which are computed by the scoring function of TransR, which means that path information is not considered in the first ranking. Then we rerank the top-500 candidates according to the scores computed by the scoring function mentioned above, namely score ( h, r, t ). for SGD among { 0.01,0.001,0.0001 } , dimension of enitity space R k and rela-tion space R d between { 20,50 } , 1 and 2 among { 1,2,4 } , batch size B among { 480,960,4800 } . The optimal configuration on valid set is  X  =0 . 001, k = d = 50, 1 = 2 = 1, and B = 4800. The training process is limited to less than 500 epochs. 3.3 Overall Performance Table 2 shows the experimental results. By comparing the results of PTransR with the results of previous models, we have the following main observations: (1) PTransR outperforms TransR on every metric to a large margin, which shows that path-augmented model can achieve better results than one-hop base model. (2) PTransR outperforms PTransE in MeanRank and is comparable to PTransE in Hits@10, which shows that path-agumented model with a better one-hop base model can achieve better performance.
 to-n , n -to-1, and n -to-n , with Hits@10( filter ) as the metric. From Table 3, we find that, compared to TransR, PTransR shows consistent imporvement on all four relation categories. Also, compared to PTransE, PTransR performs better on 1-to-1, 1-to-n and n -to-1 relations, especially on 1-to-n and n -to-1. 3.4 In-Depth Analysis and Discussion As pointed out in Section 1, despite the massive train set of FB15k, some re-lations cannot be properly captured due to the problem of data sparsity. We separate relations into five groups according to their frequency in the train set, as shown in Table 4. MeanRank( raw ) of TransR and PTransR is compared in Table 4 and the improvement from TransR to PTransR is presented. First of all, we see PTransR outperforms TransR in all five groups of relations. Second, as relation frequency decreases, the improvement goes up, which means that path information is useful for dealing with the problem of data sparsity. Relation extraction is an important research topic in NLP. It can be roughly divided into two categories based on the source of information.
 For example, Hearst [17] uses  X  X s a | an X  pattern to extract hyponymy relations. Banko et al. [6] proposes to extract open-domain relations from the Web. Fully supervised relation extraction, which classify two marked entities into several predefined relations, have become a hot research arena in the past several years [18, 19, 8].
 ditional text. Socher et al. [20] propose a tensor model to predict missing relations in an existing knowledge base, showing neural networks X  ability of entity-relation inference. Then, translating embeddings approaches are proposed for knowledge base completion [11, 7, 12, 14]. Recently, Wang et al. [21] use additional informa-tion to improve knowledge base completion by using textual context.
 additional resources. We combine the state-of-the-art one-hop TransR model [12] and path augmentation method [14], resulting in the new PTransR variant. In this paper, we augment one-hop TransR model with path modeling method, resulting in PTransR model. We evaluate PTransR on the task of entity pre-diction and compare the performance of PTransR with that of previous models. Experimental results show that path information is useful in sovling the problem of data sparsity, and that PTransR outperforms previous models, which makes PTransR the state-of-the-art model in the field that populates knowledge base without using additional text.

