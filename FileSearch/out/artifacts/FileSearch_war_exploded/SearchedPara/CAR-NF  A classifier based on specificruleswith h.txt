 Department of Computer Science, National Institu te of Astrophysics, Optics and Electronics, Mar  X  ia Tonantzintla, Puebla, Mexico Advanced Technologies Application Center, Siboney, Playa, Havana, Cuba 1. Introduction integrates Classi fi cation Rule Mining (CRM) [7,25] and Association Rule Mining (ARM) [1,19]. This
Associative classi fi cation aims to mine a set of CARs from a class-transaction dataset; where a CAR CAR list l , and a mechanism for classifying unseen transactions using l .
 mammalian mesenchymal stem cell differentiation [34] and prediction of protein-protein interaction types [23], among others.

Currently, all classi fi ers based on CARs use the Support and Con fi dence measures for computing and ordering the set of CARs [15,21,22,30,31,38]. In CARM, similar to ARM, it is assumed that a transaction t  X  D consists of an itemset and a class. The Support of an itemset X  X  I (denoted as form X  X  c where X  X  I and c  X  C . A CAR with k items (including the class i.e. | X | = k  X  1 ) will implies the rule consequent c .ACAR X  X  c satis fi es or covers a transaction t if X  X  t . where D X is the set of transactions in D containing X .
 Many studies [1,19,40] have pointed out the combinatorial number of association rules that could be obtained when a small Support threshold is used. To address this problem, recent works [15,35 X 37] prune the CAR search space stopping the growth of the rule when a CAR satis fi es the Support and to obtain general (small) rules. This strategy has some drawbacks, for example:  X  Many branches of the CAR search space could be explored in vain because the CAR search space  X  If a CAR X  X  c is obtained then a CAR X  X  c with X  X  X can not be obtained, it does not
In order to overcome these drawbacks, in this paper we introduce the use of Netconf measure instead of support and con fi dence for computing the rules with a new pruning strategy.

Although previous works [22,36] have shown that associative classi fi cation seems to achieve better methods, associative classi fi cation still has some weaknesses that must be addressed, for example:  X  The Con fi dence measure detects neither statistical independence nor negative dependence among  X  Threshold values used to compute the set of CARs are not supported.
 mance than the best classi fi ers based on CARs reported in the literature CBA, CMAR, CPAR, TFPC and HARMONY classi fi ers, all of them following the Support-Con fi dence framework. CAR-NF introduces Additionally, CAR-NF introduces a new way for ordering the set of CARs, based on the size of the CARs and their Netconf value. Moreover, we determine an appropriate Netconf threshold value, supported by a proposition, that avoids ambiguity at the classi fi cation stage.

This paper is organized as follows: The next section describes the related work. The third section and future works are given in Section 5.
 2. Related work tasks: reducing telecommunication order failures and detecting redundant medical tests. Later, several according to the strategy used for computing the set of CARs: Once a subset of CARs has been generated, regardless of the used strategy, the CARs are ordered. There are fi ve main ordering schemes reported in the literature: a) CSA (Con fi dence  X  Support  X  Antecedent size): The CSA ordering scheme combines Con fi dence, b) ACS (Antecedent size  X  Con fi dence  X  Support): The ACS ordering scheme is a variation of CSA. c) WRA (Weighted Relative Accuracy): The WRA ordering scheme, proposed in [20], assigns to each d) LAP (Laplace Expected Error Estimate): The LAP ordering scheme was introduced by Clark and e)  X  2 (Chi-Square): The  X  2 ordering scheme is a well known technique in statistics, which can be
All these ordering schemes take into account the Con fi dence measure. But, as we have mentioned in the introduction, this measure has some drawbacks. We will come back to this point in the next subsection.
 isfaction (or covering) mechanisms for classifying unseen data. use it in our work. 2.1. Drawbacks of the con fi dence measure
As we mentioned above, all the classi fi ers developed for CARM use the Con fi dence measure for mining the set of CARs. However, several authors have pointed out some drawbacks of this measure items with high Support can lead us to obtain misleading rules (see Example 1) because higher-Support items appear in many transactions and they could be predicted by any itemset [5].
 Example 1. Let X  X  assume that Sup ( X )= 0.5, Sup ( Y )= 0.7, Sup ( X  X  Y )= 0.3 and the Con fi dence threshold is set to 0.5. By Eq. (3), Conf ( X  X  Y )= 0.6. We are tempted to choose X  X  Y as an of Con fi dence it does worse than just randomly guessing. In this case, X  X  Y is a misleading rule. where | I | = 2166 and there are many items with Support above 95%).

In [24], the authors de fi ned a good accuracy measure (ACC), as a measure that separates strong rules from weak rules, assigning them high and low values respectively. Additionally, the authors suggested several desirable properties that a good ACC should satisfy. These properties are the following: Property 1. If Sup ( A  X  B )= Sup ( A ) Sup ( B ) then ACC ( A  X  B )= 0 This property claims that any good accuracy measure must test the independence [5]. Property 2. ACC ( A  X  B ) monotonically increases with Sup ( A  X  B ) when all other parameters remain the same.
 The property 2 can be interpreted as follows: Suppose a dataset D and two rules A  X  B and A  X  B such that Sup ( A )= Sup ( A ) and Sup ( B )= Sup ( B ) . If the fraction of transactions in D that contains A  X  B ( Sup ( A  X  B ) ) is greater than the fraction of transactions in D that contains A  X  B ( Sup ( A  X  B ) )then ACC ( A  X  B ) &gt;ACC ( A  X  B ) (which means that A  X  B is stronger than A  X  B ).
 Property 3. ACC ( A  X  B ) monotonically decreases when Sup ( A ) ( or Sup ( B ) ) increases and all other parameters remain the same.
 An ACC satisfying property 3 avoids to obtain misleading rules because its value does not increase by only increasing the consequent (or antecedent) Support.
 An ACC satisfying properties 2 and 3 has local maxima when Sup ( A  X  B )= Sup ( A ) or Sup ( A  X  B )= Sup ( B ) and it has a global maximum when Sup ( A  X  B )= Sup ( A )= Sup ( B ) . Now we will show that Conf ( A  X  B ) (see Eq. (4)), which has been used in all the algorithms for CARM, does not satisfy simultaneously all these properties: Proposition 1. Conf ( A  X  B ) does not satisfy the property 1.
 Proof. Here is a counterexample: Consider the transactional dataset shown in Table 1(a), where rows { i 0 . 25 / 0 . 5=0 . 5 = 0 Proposition 2. Conf ( A  X  B ) satis fi es the property 2 Proof. Trivial according to Eq. (4).
 Proposition 3. Conf ( A  X  B ) satis fi es the property 3 for Sup ( A ) Proof. Trivial according to Eq. (4). Proposition 4. Conf ( A  X  B ) does not satisfy the property 3 for Sup ( B ) .
 strong rules from weak rules.

In [5] the authors analyzed several measures (Conviction, Interest or Lift,  X  2 and Certainty factor), of these measures overcome the drawbacks of the Con fi dence measure but only Interest and Certainty limitations.

The Interest measure has a not bounded range [5], therefore differences among its values are not measure is symmetric Eq. (5) but this almost never happens in practice.
 On the other hand, Certainty factor is de fi ned by Eq. 6.
 Negative values of Certainty factor mean negative dependence, while positive values mean positive dependence and 0 means independence. However, the value that Certainty factor takes depends on the the following example taken from [2]: Example 2. Suppose that Sup ( A )= 0.5 and Sup ( B )= 0.9. If Sup ( A  X  B )= 0.45 then A and B are independent according Certainty Factor, since: If
Sup ( A  X  B )= 0.43, the Certainty factor of A  X  B is  X  0.044 by Eq. (6). This means that there is a slightly negative relationship between A and B .Butif Sup ( A  X  B )= 0.47, the Certainty factor of A  X  B is 0.4 by Eq. (6). This shows that A and B are positively dependent. The difference between 0.43 and 0.45 is equal to the difference between 0.45 and 0.47. However, the Certainty factor obtains very different results.
 by the Con fi dence. As a simple example, suppose that Sup ( X )= 0.4, Sup ( Y )= 0.8 and Sup ( X  X  Y )= 0.3, therefore Sup (  X  X )=1  X  Sup ( X )= 0.6 and Sup (  X  X  X  Y )= Sup ( Y )  X  Sup ( X  X  Y )= 0.5 (see Table 2). If we compute Conf ( X  X  Y ) we obtain 0.75 (a high Con fi dence value) but Y occurs in 80% of the transactions, therefore the rule X  X  Y does worse than just randomly guessing, clearly, X  X  Y is a misleading rule [5]. For this example, Netconf ( X  X  Y )=  X  0.083 showing a negative dependence between the antecedent and the consequent. If we analyze the rule  X  X  X  Y then guessing. However, the Netconf value for rule  X  X  X  Y is 0.083 showing a positive dependencebetween the antecedent and the consequent.
 The authors in [2] showed that Netconf measure overcomes the drawbacks of all above mentioned 1 X 3, we will come back to this point further.

Since Netconf overcomes the drawbacks of all above mentioned measures but it was not used before 3. CAR-NF classi fi er prove that the Netconf measure completely ful fi lls the properties 1 X 3 above mentioned. Also, we show that Netconf does not have the drawbacks of other measures. Additionally, we propose and prove a proposition that supports the use of a Netconf thres hold value equal to 0.5 for mining the CARs. An computing the set of CARs employing the Netconf measure. Finally, in section 3.3, we propose a new way for ordering the set of CARs that together with the  X  X est K rules X  mechanism de fi nes the CAR-NF classi fi er. 3.1. Netconf measure As mentioned in subsection 2.1, Netconf overcomes the drawbacks of Con fi dence, Conviction, Interest useful properties, for example:  X  Netconf tests independence, therefore Netconf ( A  X  B )=0  X  Sup ( A  X  B )= Sup ( A )  X  Netconf ( A  X  B ) = Netconf ( B  X  A ) for Sup ( A ) = Sup ( B ) , it means that Netconf is not  X  Netconf ( A  X  B ) takesvaluesin[  X  1,1].  X  Positive values of the Netconf measure represent positive dependencies, negative values of Netconf However, the authors did not prove that Netconf satis fi es the properties suggested in [24]. that Netconf satis fi es the properties 1 and 2, and also satis fi es the property 3 for Sup ( B ) . In order to show that Netconf completely satis fi es property 3 we will pr ove the next proposition. Proposition 5. Netconf satis fi es the property 3 for Sup ( A ) .
 Proof. In Eq. (7), let Sup ( A  X  B )= S ab , Sup ( B )= S b , Sup ( A )= x be the Support of A  X  B , B rewrite the right member of Eq. (7) in terms of S ab , S b ,and x , as follows: 5istrue.Computingthe fi rst derivative and reducing terms we have: Due to 0 &lt;S ab S b &lt; 1 then: and x 2 (1  X  x ) 2 &gt; 0, therefore, f ( x ) &lt; 0.
 rules. Thus if we return to the example 1 of subsection 2.1 and we evaluate the Netconf measure for Sup ( X )= 0.5, Sup ( Y )= 0.7 and Sup ( X  X  Y )= 0.3, we obtain a Netconf value equal to  X  0.2, meaning that there is a negative dependence between X and Y and consequently this is a bad rule. In our CAR-NF classi fi er, we bet for CARs with high Netconf values (positive dependence between X and Y ), therefore, the misleading rules are avoided. In this paper we propose to use Netconf for computing and ordering the set of CARs.

Previous works use different Support and Con fi dence thresholds for mining the set of CARs. The threshold values used in those works must be carefully de fi ned because a huge volume of CARs could be generated. However, those threshold values are not supported. In our case, we choose a Netconf threshold that allows to obtain CARs with different antecedent, avoiding ambiguity at classi fi cation 6, which states that for any itemsex X , only one CAR with X as antecedent can have a Netconf value greater than 0.5.
 obtain at most one rule X  X  c k ( c k  X  C ) with Netconf value greater than 0 . 5 . Proof. Let us assume that there are two CARs X  X  c k then, adding these inequalities we obtain the following statement, From Eqs (1) and (2) de fi ned in Section 1, we have that Sup ( c k following inequalities are ful fi lled, Since the three inequalities of Eq. (9) have the same direction, we can add them obtaining 1+ Sup and moving some terms to the right side we have 1 Sup Now, working with the fi rst and second terms of the right side of Eq. (10) we obtain which contradicts Eq. (8).

Based on proposition 6, if we select a Netconf threshold  X  0.5, for each itemset X we can obtain at most one CAR X  X  c, c  X  C such that Netconf ( X  X  c ) &gt; X  , and in this way, we can select is important to notice that a Netconf value greater than 0.5 can be considered as a high Netconf value because the Netconf takes values in [  X  1,1], being the dependence between antecedent and consequent when the Netconf threshold decreases and the smallest Netconf threshold value that avoids ambiguity in the classi fi cation stage is 0 . 5 , then in our classi fi er we use this value as Netconf threshold. 3.2. CAR-CA algorithm of the frequent itemset mining algorithm CA [18], which according to the experiments shown in [18], (usedinCMAR),Eclat(usedinMCAR)andTFP(usedinTFPC).
 CAR-CA uses a new equivalence relation to group theCARs andbit-to-bitoperations forfastcomputing Supports and in this way to speedup CAR computing employing the Netconf measure.

In [40], for mining ARs t he authors propose partiti oning the itemset space into equivalence classes grouping k -itemsets will be denoted as EC k . In CAR-CA, unlike the algorithm proposed in [40] we consider each prede fi ned class c  X  C as another item and we propose to divide the CAR space into to the same equivalence class X . In Fig. 1 we show graphically this equivalence relation.
In order to take advantage of bit-to-bit operations we represent the dataset as an m x n binary matrix being m the number of transactions and n the number of items including the class item. The binary values 1 and 0 denote the presence or absence of an item in a transaction, respectively. Each column, associated to an item j , can be compressed and represented as an integer array I j , as follows: where each integer of the array represents 32 transactions (in a 32 bit architecture).
Previous algorithms developedfor CAR mining need extra operations or extra dataset scans to compute the Support of rule antecedents. Our proposal avoids these extra operations; for that, it iteratively generates a list L EC next format: antecedent pre fi x, and IA AntP ref IA is as follows: Let i and j be two items, then: now let X be an itemset and j be an item, then: the Eqs (13), (14) and (15).

To illustrate the overall CAR m ining process, suppose that we have the equivalence class E = c etc. For simplicity, we assume a 4-bit architecture and we show, in Fig. 2, the arrays I c for only 16 transactions (four blocks of four transactions each one). Add itionally, we show the pairs ( value, id ) resulting from the intersection of the arrays I i 1 and I i 2 (see IA i 1 i 2 in Fig. 2). In the fi rst step, see Fig. 3(a), the IA i I see Fig. 3(b), the IA i the equivalence class c 1 ,i 1 i 2 i 3 ,IA i Using integer arrays, CAR-CA avoids extra operations or extra dataset scans to compute rule antecedent CAR-CA decreases rapidly because they are built using AN D operations, which generate a lot of null integers, and null integers are not stored by our algorithm.
 the ef fi cient use of bit-to-bit operations for computing the Netconf of a CAR.
 Recent algorithms for mining the set of CARs [15,35 X 37] prune the CAR search space each time a Netconf threshold, we propose the following pruning strategy: If a candidate CAR X  X  c does not satisfy the Netconf threshold we do not extended the CAR anymore avoiding to explore this part of the CAR search space in vain, i.e., we prune the CAR search space avoiding to generate candidate CARs from CARs that do not satisfy the Netconf threshold. Otherwise, if the candidate CAR X  X  c satis fi es the Netconf threshold we follow extending it while Netconf ( X  X  X  i } X  c ) is greater than or equal to Netconf ( X  X  c ) , thus we allow to obtain more speci fi c rules (large rules) with high Netconf. The pseudo code of CAR-CA is shown in Algorithm 1.

In line 3 of Algorithm 1, the 1-itemsets are calculated. In line 5, the equivalence classes of size 2 function.

The ECGen function takes, as an argument, an equivalence class of size k  X  1 and generates a set of equivalence classes of size k (see Algorithm 2). The equivalence classes generated by ECGen only contain CARs with Netconf greater than 0.5. 3.3. Ordering and classifying
Once the set of CARs has been generated, using the CAR-CA algorithm, the CAR list is sorted. As highest values fi rst). size, rules with high Netconf valued should be preferred before rules with low Netconf value. Remember that a rule with high Netconf value has a high positive dependence between its antecedent and its with three CARs, which are sorted by the criterion of the most general fi rst. Given the transaction { i { i rule { i 1 } X  c 1 only considers the item i 1 . In Table 3(b), we show the same three CARs but sorted { i
For classifying unseen transactions, we decided to follow the  X  X est K rules X  satisfaction mechanism, because, as it was explained above, the  X  X est rule X  mechanism could suffer biased classi fi cation or the pseudo code of the training phase and classi fi cation phase respectively:
In the training phase (Algorithm 3), the CAR  X  CA function computes the set of CARs from the training dataset. After, the Ordering CARs function sorts the set of CARs in a descending order transactions. 4. Experimental results
In this section, we report some experimental results where the CAR-NF classi fi er is compared against the main classi fi ers based on CARs reported in the literature: CBA [22], CMAR [21], CPAR [38], TFPC [15] and HARMONY [32]. Other good classi fi ers like RCBT [16] and DDPMine [11] were not algorithms, which can not be implemented based on the details provided in [11,16]. Besides, the fi rst one was evaluated using only four gene expression datasets, which were not provided by the authors; and the second one was evaluated using only 8 unusual datasets from the UCI repository.
The codes of CBA, CMAR, CPAR and TFPC were downloaded from the Frans Coenen X  X  homepage (http://www.csc.liv.ac.uk/  X  frans) and for HARMONY, we used the accuracy values reported in [32]. In is computed as: presented to the classi fi er.
 Our tests were performed on a PC with an Intel C ore 2 Duo at 1.86 GHz CPU with 1 GB DDR2 RAM, running Windows XP SP2.

In the same way as in other works [15,21,22,38], experiments were conducted using several datasets, 20 in our case. The chosen datasets (see characteristic in Table 4) were originally taken from the UCI Machine Learning Repository [4], and their numerical attributes were discretized by the author of [13] using the LUCS-KDD discretized/normalized ARM and CARM Data Library. The discretization technique used in LUCS-KDD is different from those used in [15,21,22,38]; thus, our results reported in Tables 6, 7 and 8 are different from previous studies, even for the same classi fi er and the same CAR-NF, against the best reported accuracies of all the other evaluated classi fi ers.
For CBA, CMAR, CPAR and TFPC classi fi ers we used the Con fi dence threshold set to 50% and in [15]. In [32], the authors of HARMONY obtained the best results using a support threshold of 50%. In CAR-NF we used the Netconf threshold set to 0.5 (equivalent to 75% if we map Netconf from [  X  1,1] to [0,1]) based on proposition 6 and in our previous analysis in section 3.1.

For classifying a new transaction we used the  X  X est K rules X  satisfaction mechanism (Section 2), class that will be assigned to the new transaction.
In Table 6, the results show that CAR-NF yields an average accuracy higher than the other evaluated i.e. the best average in the fi rst place, the second best average in the second place and so on.
Analyzing these tables, we can see that CBA had the worst performance in average accuracy and because, although CBA had low accuracy for some datasets (e.g. letRecog, ionosphere and mushroom), it reached the fi rst or second place in 10 of the 20 datasets, as opposed to CMAR, which had a good average accuracy but a poor average ranking.
 was HARMONY. HARMONY was the second best in average accuracy and average difference w.r.t. the best classi fi er; and it shared with CBA the second place in average ranking position.
Although the original implementations of CBA, CMAR and CPAR use different discretization/nor-malization techniques, we consider interesting to show, in Table 9, a comparison of the accuracies of HARMONY, the authors did not report which technique was used for discretization/normalization. classi fi ers, in the other datasets.

Despite the discretization/normalization technique is not the same, CAR-NF obtains the best average accuracy being 2.1% better than the second best. 5. Conclusions introduces a new strategy for computing CARs, using the Netconf as measure of interest, which allows pruning the CAR search space for obtaining speci fi c rules with high Netconf (greater than 0.5 which corresponds to greater than 0.75 if we map Netconf from [  X  1,1] to [0,1]). We also prove that the for ordering the set of CARs using the CAR size and the Netconf value.

The experimental results show that CAR-NF has better performance than CBA, CMAR, CPAR, TFPC and HARMONY classi fi ers. In general, CAR-NF has the best classi fi cation accuracy.
As future work, we are going to study the problem of producing rules with multiple labels, it means rules with multiple classes in the consequent. This kind of rules could be useful for problems where some transactions can belong to more than one class.
 Acknowledgment
This work was partly supported by the National Council of Science and Technology of Mexico under the project CB-2008-01-106443 and grant 228056.
 References
