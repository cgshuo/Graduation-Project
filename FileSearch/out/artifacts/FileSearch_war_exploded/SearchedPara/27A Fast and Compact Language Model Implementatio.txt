 JUN-YA NORIMATSU, MAKOTO YASUHARA, TORU TANAKA, and MIKIO YAMAMOTO , Language models based on n -grams [Jelinek 1990] are widely used as probabilistic models for sentences in natural language processing. Wide use of the Internet has re-sulted in a dramatic increase in the amount of text data available for training language models. We can harness this text data to obtain a significant improvement in model quality. In particular, Brants et al. [2007] have shown that the performance of sta-tistical machine translation (SMT) systems is improved by using a larger amount of training data. Although we are able to train large n -gram language models by Kneser X  X ey smoothing [Heafield et al. 2013], it is still not trivial to index such large models efficiently in terms of speed and space for use in actual applications. Models resulting from considering larger amounts of text can be large. In recent years, many methods to improve the efficiency of language models by tackling this problem have been proposed [Talbot and Brants 2008; Guthrie and Hepple 2010; Pauls and Klein 2011; Heafield 2011]. These methods not only reduce the amount of memory needed to handle the same amount of data but also improve query speed.

In this article, we propose a data structure for an n -gram language model based on a double-array data structure, namely the double-array language model (DALM). A double-array structure [Aoe 1989] is an implementation of a trie that allows fast querying in a compact space. This type of structure is widely used in text processing, such as processing of Japanese.

To inherit the characteristics of the double-array structure, such as fast querying and compact data size, we exploit otherwise unused space in the double-array structure as storage for model parameters. Moreover, we integrate optimization methods [Yoshinaga and Kitsuregawa 2014; Liu et al. 2011] to improve efficiency in terms of model size and query speed.

In our experiments, we use language models whose underlying dataset is the dataset from the NTCIR10 patent translation task [Goto et al. 2013]. The largest model size considered here is about 85GB when stored on disk in the ARPA file format. In our evaluations, we focus on query speed and model size. State-of-the-art systems allow choosing between smaller space (model size) and faster speed (query speed). Our results indicate that DALM achieves a size almost as small as the smallest state-of-the-art systems and is only slightly slower than the fastest existing implementations.
For additional validation of the proposed implementation, we conduct experiments on machine translation by applying a hierarchical phrase-based model [Chiang 2007]. On the translation task as well, DALM achieves almost the same speed as the fastest state-of-the-art systems and almost the same model size as the smallest of the existing implementations.

Although part of our work is described in Yasuhara et al. [2013], we have improved on those earlier results in four specific ways. First, we propose a reverse trie version of DALM in addition to one using backward suffix trees. Second, we eliminate the VALUE array, which was used to store backoff weights. Third, we incorporate a state optimiza-tion technique [Li and Khudanpur 2008], which is used here to improve recombination in the decoder of the SMT. Fourth and finally, we conduct experiments on machine translation tasks in addition to perplexity tasks and report the results.

This article is organized as follows. In Section 2, we describe n -gram language models and their data structures, and then we introduce the double-array structures that are used in our proposed methods. In Section 3, we describe research in related areas and highlight the key points of our research. In Section 4, we describe our proposed methods, which use the double-array structures. In Section 5, we provide results of experiments using the proposed methods. These are compared against results from state-of-the-art methods. Finally, we conclude and describe some implications of our method in Section 6. As originally proposed, n -gram models [Jelinek 1990] are probabilistic models of sen-tences. Presently, they are widely used in natural language processing, automatic speech recognition, and optical character recognition. In this section, we describe n -gram models and some open problems.
 Given a proposed sentence, an n -gram model assigns a probability to that sentence. In this article, we denote the i th word in the given sentence by w i . We denote the sentence as w 1 ,w 2 ,...,w l , which we abbreviate as w l 1 . Then, for a model with order n , the probability of a sentence w l 1 is typical requirements for language model implementations:
The probabilities associated with each m -gram (where m  X  n ) are stored in memory as a table, and the system looks them up when it is queried. However, the table of probabilities is often too large to hold entirely in memory because the size of the table is n m = 1 | V | m entries, where | V | is the number of unique types (i.e., unique words) in the model X  X  vocabulary. No matter the size, it is difficult to estimate the probabilities of m -grams that do not appear in the training corpus (these are referred to as unseen m -grams in the following).

A backoff model [Katz 1987] gives the probabilities of unseen m -grams. It also solves the size problem by limiting the table to include only seen m -grams rather than storing the n m = 1 | V | m possible patterns. When we query an m -gram, the querying system first searches for the target m -gram in the table. 1 If it finds the m -gram, then it returns the probability; otherwise, it backs off to find the lower-order ( m  X  1)-gram ( v m 2 )andmul-tiplies the probability by a correction term, known as the backoff weight . The backoff model has parameters of two types: probabilities and backoff weights. To distinguish a value stored in memory from a calculated value, we denote the probability of an m -gram the backoff model is
Although adopting a backoff model reduces the table size, recent language models are increasing in size because they include more data. This is done because the use of more training text improves the system X  X  accuracy (e.g., as measured by the BLEU scores on machine translation tasks [Brants et al. 2007]). If we can store a language model compactly, then we can use more data to improve the quality of the system output. Thus, it is desirable to store the model in memory as compactly as possible. 2.2.1. Backward Suffix Trees. Tries [Fredkin 1960] are one of the most widely used tree structures for n -gram language models because they require less memory, which they accomplish by allowing sharing of common prefixes.
 Among representations of tries, backward suffix trees [Bell et al. 1990; Stolcke 2002; Germann et al. 2009] are one of the most efficient data structures for language models in terms of query speed. They organize a trie tree in reverse order to store history words. Each backward suffix tree comprises a trie and word lists. The trie indicates the history words of the m -grams ( v m  X  1 1 ) in reverse order. Each node of the trie contains a word list of the target words for those m -grams ( v m ) that have the history words v m  X  1 1 in common. The pseudocode in Algorithm 1 shows how to look up a given m -gram.
To look up an m -gram in the backward suffix trees, we need to traverse the nodes from v  X  1  X   X  X  X   X  v 1 and then find v m in the slot of the word list connected with the node.
Figure 1 shows an example of a backward suffix tree representation. The history words of the phrases  X  X  eat, X   X  X ou eat, X  and  X  X o you eat X  are stored in reverse order of appearance. Querying this trie about an m -gram is simple: just trace the history words in reverse order and then find the target word in a list. For example, we consider querying about the 3-gram  X  X  eat fish. X  First, we simply trace the history in the trie in reverse order ( X  X at X   X   X  X  X ); then, we find  X  X ish X  in the list &lt; 1 &gt; . Similarly, querying a backward suffix tree about unknown m -grams is also efficient because the backward suffix tree representation is well suited to backoff calculation. For example, in querying about the 4-gram  X  X o you eat soup, X  we first trace  X  X at X   X   X  X ou X   X   X  X o X  in a manner similar to that presented earlier. However, the search for the word  X  X oup X  in the list &lt; to the previous node (the node we passed while traversing,  X  X ou X ) and search the list &lt; 2 &gt; , where we find  X  X oup. X  This means that the 3-gram  X  X ou eat soup X  is contained in the tree, but the 4-gram  X  X o you eat soup X  is not. This technique can be used for efficient backoff calculation. 2.2.2. Reverse Tries. The reverse trie [Bell et al. 1990] is a well-known data structure for looking up dictionary entries and another option for the language model implemen-tation. The representation is simple: just place the m -gram words in reverse order. The pseudocode in Algorithm 2 shows how to look up the given m -gram.

Figure 2 shows an example of a reverse trie. For example, in querying about the 3-gram  X  X ou eat soup, X  we first place the target word,  X  X oup, X  then set the history in reverse order:  X  X at X   X   X  X ou. X 
The reverse trie has one drawback; we must trace the trie again during backoff calculation. When we cannot find the node of v m 1 , we need the node of v m  X  1 1 to get the backoff weight  X  ( v m  X  1 1 ). To get the node, we have to traverse the node from the root node. Since this repetition degrades query speed, the reverse trie structure was not recommended for language model implementation until Heafield [2011] solved the problem by extending the language model state. 2.3.1. Double-Array Structure. In this section, we describe a double-array structure [Aoe 1989]. Double-array structures represent nodes in a trie as slots having the same index in two arrays. Before showing the details, we first show a fast but naive implementation of a trie and illustrate the concept of the double array.
 The most naive implementation of a trie uses a two-dimensional array NEXT . For such an implementation, let t be the node index, and let WORDID( v )bea function that returns a word ID as a number corresponding to the word v ; then, NEXT [ t ][WORDID( v )] (which represents the WORDID( v )th slot of the t th row in the NEXT array) stores the node index that can be transitioned to from the node index, t , by the word, v . This allows traversing the trie efficiently and easily to serialize the array in memory. Figure 3 shows an example of the implementation. In the figure, the structure on the left is an example trie, and the table on the right is its representation, NEXT . The rows of each table indicate nodes, and the columns indicate words. Cell values are present when the node indicated by the row can transit to a child node via the word indicated by the column.

The idea is simple, but it wastes memory because many slots are unused. Double-array structures solve this problem by taking advantage of the sparseness of the NEXT array. The two-dimensional array NEXT is restructured into a one-dimensional array BASE by shifting the entries of each row of the NEXT array and combining the set of resulting arrays (Figure 4). Additionally, a CHECK array is introduced to check whether the transition is valid or not, because we cannot see the node links from the original trie after restructuring. By using a CHECK array, we can safely move to the correct child node without errors.

The double-array structure uses the index of the slot to indicate the node. As a definition, a node link from a node s (represented in the double array by an index t s ) via word v to the node next (similarly, represented by the index t next )is The pseudocode in Algorithm 3 shows how to transit from a node s via a word v .Figure5 shows an example of a transition from a parent node s via a word v .
 The double-array structure consumes 8 bytes per node because the BASE and CHECK arrays consist of 4-byte array variables in our implementations. Therefore, the structure can store nodes compactly as long as the double array has only a small number of unused slots. Moreover, queries are very fast because they require only one addition operation and one comparison operation per transition. We use a double-array structure in DALM. We maximize the potential of the double-array structure.
The computational and space costs are straightforward to describe. Traversing from one node to an adjacent node requires an addition operation and a comparison opera-tion; therefore, looking up an m -gram has time complexity O ( m ). For space, the order is O ( N + U ), where N is the number of nodes in the trie and U is the number of unused slots. If we assume that the number of unused slots is linearly or sublinearly proportional to N , then the space needed is O ( N + U ) = O ( N ). 2.3.2. Greedy Double-Array Construction. Building a compact double-array structure is difficult. Greedy algorithms are widely used for constructing static double-array struc-tures. 2 Currently, this is the only method that we can apply. The construction steps are as follows. (1) Define the root node of a trie to correspond to index 0 of the double-array structure. (2) Greedily obtain a BASE value and insert its child nodes (i.e., checking values in (3) Recursively repeat (2) for each node.
 In practice, once the BASE value of a node is fixed, the positions of its children are fixed at the same time, and we can find the BASE values for each child recursively.
Figure 6 shows an example of this kind of construction. In this example, three nodes (each of which will be reached via one of the words  X  X , X   X  X ou, X  and  X  X hey X ) are inserted at the same time. This is because the three node positions are determined by the BASE value of the node  X  X at. X  To insert the nodes  X  X , X   X  X ou, X  and  X  X hey, X  the following three slots must be empty (i.e., not used by other nodes): BASE [ t ] + WORDID( X  X  X ), BASE [ t ] + WORDID( X  X ou X ), BASE [ t ] + WORDID( X  X hey X ), where t is the index of the node  X  X at. X  During the construction step, we need to find a value of BASE [ t ]that satisfies these conditions.

Because achieving a reasonable construction time for a double-array structure poses a significant challenge, we use a more efficient method proposed by Nakamura and Mochizuki [2006] instead of the naive method for constructing the double-array struc-ture. We refer to this method as the  X  X mpty doubly linked list X  method.

In the naive method, finding a suitable BASE value for each node requires a lot of time. In particular, we must repeatedly increment the BASE value by one until all of the child nodes are placed into the double array without conflict. To overcome this, Nakamura and Mochizuki [2006] proposed a method in which the empty slots are used to find suitable BASE values efficiently. Their algorithm is one of the most efficient construction methods devised to date.

Nakamura and Mochizuki observed that we can use the empty slots of double arrays (slots that do not indicate any nodes of the trie) to increase the efficiency of finding BASE values. The basic idea is that if we can look up the empty slots efficiently, then we can narrow the search space, which will result in faster building of the double-array structure. Instead of increasing the BASE values by one, we can set the values by checking for which the emptiness criterion is satisfied for at least one node.
Algorithm 4 shows pseudocode for the method. Using their method, we can store the index of the nearest empty slot for each empty slot. Empty slots are then linked with each other, which forms a doubly linked list. At the first stage of construction, all slots of the double arrays are empty; therefore, we initially connect adjacent slots. When we store a node of the trie in one of the empty slots, we update the list of empty slots by removing that slot. This maintains the list.

We can efficiently find the BASE value of each node by using the CHECK array to store the next empty slot. Figure 7 shows an example of an empty doubly linked list. In this example, when searching for a suitable BASE value for a node, the first child node walking the list of empty slots instead of searching all BASE values 0 , 1 , 2 , 3 , 4 , 5 ,... .
As analyzed by Nakamura and Mochizuki, the computational cost of a node insertion is less than by the naive method. The original naive method requires O ( N | V | )timefor node insertion, whereas the algorithm that uses an empty double-linked list requires O ( U | V | ). 2.3.3. Efficient Storage Method. Because tries are often used as a key X  X alue store, double arrays are also required to store corresponding values. A naive implementation is to prepare a VALUE array of the same length as BASE and CHECK . Then we can store the corresponding values into the slots of that array. The VALUE array consumes memory in addition to that required for the main double-array structure, and more efficient methods have been proposed.

The most well-known method of storing values with a double array is Kudo X  X  method, which is implemented in the Darts library. This method stores the value into the BASE slot of the leaf node because the BASE slot of the leaf node is always empty. Because the method uses the empty slots in the double array, it does not require extra space. Guthrie and Hepple [2010] have proposed a language model implementation, ShefLM, that uses minimal perfect hash functions [Belazzougui et al. 2009], which can allow storing entries without requiring empty spaces. Furthermore, values are compressed by simple dense coding [Fredriksson and Nikitin 2007]. ShefLM uses fingerprints to confirm that the storage indicated by the hash function actually contains the queried words. Since the bit length of each fingerprint in their experiments is short, the query sometimes fails. This means that it is a  X  X ossy X  language model.

Federico et al. [2008] have proposed a language model implementation, IRSTLM, that is well suited to large-scale language models. In this implementation, sorted array structures are used to construct trie structures; this method was first used by the CMU Toolkit [Clarkson and Rosenfeld 1997]. Pauls and Klein [2011] have pro-posed BerkeleyLM, which also uses sorted array structures. In BerkeleyLM, variable-length coding and block compression are used when small model size is more important than query speed. Watanabe et al. [2009] proposed a language model implementation based on level-order unary degree sequence (LOUDS) [Jacobson 1989] representations. Because a LOUDS is a succinct representation of a trie, the resulting model size is very small; however, the query speed was not investigated. Sorensen and Allauzen [2011] proposed another LOUDS-based language model structure, which decomposes the lan-guage models into finite-state acceptors and stores those as LOUDS.

Heafield [2011] has proposed an efficient language model toolkit called KenLM .It has been widely used in recent machine translation systems, for which large language models are often needed. KenLM has two variants, which are distinguished according to main structure type: trie and probing . In KenLM, the trie variant employs a trie as its data structure. Although it uses sorted arrays similar to those used in IRSTLM, it improves the query speed by using an interpolative search for traversing nodes, which costs O (log log M ) time, where M is the number of child nodes. Moreover, it uses a lossless compression method proposed by Raj and Whittacker [2003] for trie structures using sorted arrays. In contrast, the probing variation employs hash tables with a linear probing search. Since a hash table must save identifying information for entries to resolve collisions, KenLM stores a 64-bit fingerprint for each entry. The trie structure is compact but relatively slow to query, whereas the probing structure is relatively large but faster to query.

In this article, we propose methods to store the language model in double-array structures. The greatest difficulty in storing model information arises from the size of the parameters. Here, each entry in the model has two values, consuming 63 bits in total, and so it is difficult to include more information without increasing the model size. Implementations for efficiently storing small values for each entry in double arrays have been studied before. The double-array trie system DARTS is one such implementation system, 3 and research by Yoshinaga and Kitsuregawa [2014] describes another. Methods in those systems use otherwise empty space in the data structure, but the space can be used to store at most 32 bits for each key. It is therefore difficult to store language model parameters, and implementations often need additional space, which consumes more memory.

We solve this problem by focusing on the characteristics of the language model and the double-array structure. Because double-array structures have large amounts of unused or unnecessary memory allocated, we can store the values into those areas. To avoid breaking the restrictions on the double array, we use the sign bits of the model parameters to store information. Our method successfully harmonizes between the data structure and the model parameters, achieving a fast and compact language model implementation. In this section, we describe application of the double-array structure to a language model. We examine the two different ways to implement tries in language model sys-tems: backward suffix trees and reverse tries. Next, we show how DALM optimizes the state information in SMT. Then, we introduce the tuning method referred to as leaf compression , which decreases the model size. Last, we describe a technique for building a large DALM by dividing tries. First, we describe the backward suffix trees version of DALM, 4 which we refer to as
To represent backward suffix trees with a double-array structure, we must represent the tables that contain target words in the trie. In a prior work, Aoe [1989] used endmarker symbols to indicate whether an m -gram is in the trie. In general cases outside language models, each node of the trie can be just a fragment of another entry. We can add nodes to indicate the endmarker symbol after the last node of each entry in those situations. For the language model, we use these symbols to indicate nodes that are end-of-history words and place target words after those. For example, to store a trigram v 1 ,v 2 ,v 3 , we place the history words in reverse order, append the endmarker &lt; can trace the trie as v 2  X  v 1  X  &lt; # &gt;  X  v 3 .

To efficiently use double-array structures for language models, we should efficiently store the probabilities and backoff weights. We embed probabilities and backoff weights in the unused spaces in the double arrays. We represent these values as 32-bit floating-point numbers 5 ; the size of the probabilities and backoff weights is compatible to the slot sizes in the BASE and CHECK arrays, which store 32-bit integers.

Using this observation, we modify the original double-array structures to store model parameters, which are log probabilities and log backoff weights. Although it is difficult to use basic double-arrays to embed 63-bit values for each key, we can successfully embed them into the model structure by taking advantage of the characteristics of the language models. The most important characteristic of the language models is that all entries indicated by nodes are in the model. We do not need to distinguish whether the trie node is included in the language model; we can treat all nodes in the trie as contained in the model. Thus, we can use the CHECK slots to store the parameters of the language model.

We use the BASE array slots of target word nodes for storing log probability values and the CHECK array slots of endmarker symbols for storing the log backoff weights. Figure 8 illustrates an example of the language models. To avoid transition errors, we store only negative values into the CHECK slot of the endmarker . To store positive log backoff weights, which would violate the constraint to do directly, we subtract a constant C that is larger than the largest log backoff weight.

Moreover, to increase the space efficiency, we tune the word ID ordering by using code mapping methods [Liu et al. 2011]. We assign word IDs in order of 1-gram (unigram) probability. When word IDs are assigned in order of unigram probability, sibling nodes tend to become concentrated and packed in clusters. This makes insertion easier than in the untuned case during the building stage. We can expect this method to allow higher filling rates, which implies smaller double-array structures. This is done during a preprocessing stage, before the DALMs are built.
 Finally, we demonstrate how to look up an m -gram stored in the DALM bst structures. The pseudocode in Algorithm 5 shows how to look up m -grams in DALM bst .Whenwe look up an m -gram v m 1 , we first traverse the double-array in reverse order for history words as v m  X  1  X  v m  X  2  X   X  X  X   X  v 1 . Then, we move to the endmarker node &lt; # &gt; and get  X  (  X  ( Next, we describe the reverse tries version of DALM. We refer to the method as
In DALM re v , we use the slots of endmarkers to store both log probabilities and log backoff weights for each m -gram. The technique for this is based on the same concept that we described in Section 4.2; we do not need to check the transition to endmarkers because all nodes have the endmarker symbol.

Figure 9 shows an example of the DALM re v structure. During queries, we can dis-tinguish endmarker slots from other node slots because almost all current computers represent the sign of a value (whether an integer or a floating-point type) in the most significant bit. Since the log probabilities are always negative, we can detect, by check-ing a single bit, whether the CHECK value is positive or not at query time and thereby avoid incorrect transitions.
 We have now successfully stored the n -gram model in a single double-array structure. Algorithm 6 shows the procedure for looking up the log probability and the log backoff weight from DALM re v . When we try to look up an m -gram v m 1 , we first traverse the double array in reverse order as v m  X  v m  X  1  X   X  X  X   X  v 1  X  &lt; # &gt; , and then we read the values from the slot of the node &lt; # &gt; ( endmarker node) as floating-point numbers. We can use the obtained numbers as the log probability and the log backoff weight. States [Li and Khudanpur 2008] are used in SMT for storing information on history words and for calculating the conditional probability that words will appear in text still to be decoded. Li and Khudanpur [2008] show that optimizing states can im-prove recombination and thereby reduce search errors in the SMT decoder. KenLM implements further optimizations and achieves more efficient decoding [Heafield 2011; Heafield et al. 2011].

Conceptually, the idea used for state optimization is simple: we can use previous queries to speed up future calculations. For example, if a language model has a 3-gram 1 but it does not have a 4-gram we say that  X  v 3 1 cannot extend to the right. X  Therefore, we must check whether v 3 1 can extend to the right and truncate the left-most word v 1 .

Similar optimization can be applied when we apply the language model to hierar-chical machine translation [Chiang 2007]. We check whether v 3 1 can be extended to the left because the hypothesis will extend to both left and right. When the decoder creates a new hypothesis, it wraps the previous one in a phrase. For example, if the previous hypothesis is  X  X ou eat soup X  and the phrase is  X  X o X? X  then the new hypothesis is  X  X o you eat soup? X  Next, the decoder attempts to update the probabilities, such as P (you) to P (you | Do), P (eat | you) to P (eat | Doyou), and so on. However, if the 2-gram  X  X ou eat X  cannot extend to the left, updating the probability for the word  X  X at X  is not necessary. Therefore, when we calculate P (eat | you), we check whether the 2-gram can extend to the left.

We implement a way of checking whether an m -gram can extend to the left, to the right, or both, as described earlier. For DALM bst ,weuse BASE slots, which correspond to history words, to indicate the ability to extend to the right (Figure 10). Since the double-array structure requires BASE values to be positive values, we can use the sign bit of these values to store a flag. We store a positive value when the m -gram can extend to the right and a negative value otherwise. To indicate the ability to extend to the left, we use the BASE slots of the probability nodes (in which the log probabilities are already stored), and specifically, we use the sign bit of that slot. Using the sign bit of the log probability for left-extension information is the same technique used in the KenLM probing structure. For DALM re v , we use the BASE slot of the endmarker nodes to indicate both left-and right-extension information (Table I). The sign bit of the slot is used to indicate the left-extension information. As we described earlier, since log backoff weights can be positive, we subtract a constant C larger than the largest log backoff weight from each in advance. Additionally, we use a value of infinity to indicate that the m -gram does not extend to the right, which is acceptable because an m -gram that cannot extend to the right will have a log backoff weight of exactly 0 without the adjustment. In DALM re v , we enhance the space efficiency of the trie by a method we call leaf compression . The method allows us to drop endmarker nodes without loss of information when certain conditions are satisfied.

As an example, the highest-order entries ( n -grams) do not have backoff weights, even though storing the values consumes space. Additionally, the nodes indicating the highest-order n -grams in the reverse trie do not have any children. In these cases, the backoff weights are irrelevant, and we should have only probabilities. In other words, we need to store only one value in the double array for each of these nodes.
We use the reduced double-array trie technique [Yoshinaga and Kitsuregawa 2014] for this. To eliminate these spaces, we delete endmarkers of the m -grams and embed the log probability of each such m -gram into the BASE slot of its parent node. Because the log probabilities are always negative and the BASE value is always positive, we can detect the compressed position without trying to transition.

More importantly, we can delete the endmarker nodes of an m -gram v m 1 as long as it satisfies all of the following conditions:  X  X he m -gram does not have a backoff weight.  X  X he m -gram does not extend to the left X  X hat is, the model does not contain any ( m + 1)-grams, such as vv 1 v 2  X  X  X  v m , for any word v .  X  X he m -gram does not extend to the right X  X hat is, the model does not contain any ( m + 1)-grams, such as v 1 v 2  X  X  X  v m v , for any word v .
 The second and third conditions are necessary for m -grams where m &lt; n (i.e., where the length is smaller than the order of the model). We note that leaf compression applies not only to the highest-order entries but also to lower-order entries that satisfy the preceding conditions.

The third condition applies to m -grams with m smaller than n . We must distinguish those m -grams that do not extend to the right because the language model must return not only the probability but also the state information. If an m -gram does not extend to the right, then we need to exclude the left-most word from the state. Since the actual language model includes entries that extend to the right, even though they do not have log backoff weights [Heafield 2011], we can apply the compression method only when the third condition is satisfied.
 As discussed in Section 4.3, leaf compression can detect whether the value of a BASE slot is a probability or a BASE value because we constrain BASE values to being positive values and the log probabilities are always negative. For example, when we look up an m -gram v m 1 that satisfies the conditions, we first traverse the double array in reverse order, v m  X  v m  X  1  X   X  X  X   X  v 1 , and then we read the value from the BASE slot of node v 1 . We can interpret the value as a log probability when it is negative. Under this condition, we know that the backoff weight of the given m -gram does not exist. We treat the value as a probability of the m -gram with a corresponding backoff weight of 0.
Since the leaf compression simply decreases the number of nodes, we can expect that it will result in a smaller model size. The largest language model in our experiments, the specifications for which are shown in Table II, has 2,049,778,360 entries. If each entry has an endmarker , the number of nodes is doubled to 4,099,556,720. The number of nodes remaining after leaf compression is 2,889,010,118. We thus reduced the number of nodes by 30%. Building a double-array structure requires a long time, which can sometimes be im-practical. In fact, waiting on construction of the double-array structure of the large model that we use in the experiments is not feasible. We therefore need a way to build the structure in a reasonable amount of time.

Although there are several research results showing how to build double-array struc-tures faster [Nakamura and Mochizuki 2006; Yata et al. 2009; Shigekoshi et al. 2009], we cannot apply most of them. The primary reason a new method is needed is that prior methods are intended for byte-based tries; in contrast, our methods are intended for word-based tries. In byte-based tries, node transitions are done according to char-acter code, 6 which means that the function WORDID does not yield values higher than 255. Since the double array for word-based tries is not yet well studied, the choices are limited.

To shorten the build time, we divide the original trie into several parts, which enables us to build the double-array structures in parallel. We refer to the number of parts in which the trie is divided into as its  X  X ivision number. X  To choose the trie for each query, we use the word ID of the first traversed word. Specifically, we use the remainder after dividing the ID by the division number. As a special case, for DALM bst unigrams, the word is used to choose the trie, yet the first traversed word is the endmarker . Although the method shortens the build time, the query results for the original and divided tries are equivalent because the divided tries hold all of the model statistics of the original trie. This technique is similar to that used in randomized language models [Talbot and Brants 2008].

Building parts of the original trie reduces its number of nodes. We can see that dividing tries not only enables us to build the structure in parallel but also reduces the computational cost of building the double-array structure. As described in Section 2.3.2, the efficient algorithm requires time O ( U | V | ) to insert one node, and the insertion is iterated N (the total number of insertions) times. If we assume that the number of unused slots at the i th insertion, U i , is proportional to i ,orthat U i = c  X  i , where c is a proportionality constant, then we can calculate the order of the build time as that the number of nodes in each trie is equal), the division makes the build twice as fast (( 1 2 ) 2  X  2). Therefore, reducing N will make the build time shorter.
Since each BASE slot consists of 4 bytes and we use the most significant bit to store signs, the maximum number of nodes is limited to at most 2 31  X  1. Therefore, if the number of m -grams that end with a high-frequency word (e.g., m -grams that end with  X  X he X ) is larger than the limit, another method of division is required. The method that we propose in this section does not fully lift this constraint. More research is needed. We report the results of experiments to confirm the effect of our methods. Our experi-ments consist of three parts:  X  Building (Section 5.3): We measure DALM building times with respect to data size and division numbers. We also report the filling rates of double arrays.  X  Perplexity Calculation (Section 5.4): We report query speeds and memory usage for the perplexity calculation. The experiment shows the pure performance of the lan-guage model.  X  Translation (Section 5.5): We report the translation speeds and memory usage for the translation, which shows how the language model works in real situations. Although the second and third experiments are similar, the perplexity calculation measures performance on valid texts that are written by humans and the translation measures performance on mainly invalid texts that are generated by an SMT decoder. In our experiments, we use a corpus from the document text of the NTCIR10 PatentMT [Goto et al. 2013]. We use Japanese texts to learn language models.
We prepare four language models X  X iny, Small, Mid, and Large X  X nd use them to confirm the scalability of our methods. We trained those language models by using KenLM [Heafield et al. 2013]. We use a subset of the document text for the year 2005 as the test set for the perplexity calculation. The test set contains 4,765,362 sentences, 261,341,268 tokens, and 720,800 types. Table II shows the specifications of the corpora and models.

For the translation, we also use the NTCIR10 parallel corpus for training and testing the performance. The task is English-to-Japanese translation. Table III shows the specifications. In the training step, we do not clean the training set, and we use all of the data. We use KenLM to tune the model parameters and use the results for the test. Therefore, we prepare four feature weight sets in total. The Moses decoder has a feature to filter the phrase table, which eliminates phrase pairs that do not appear in the test set, but we do not use this feature to better see the results for a real situation. In these experiments, we use the hierarchical phrase-based model [Chiang 2007].
We compare our method with KenLM [Heafield 2011]. We use both probing and trie structure types in KenLM. For KenLM probing , there is a tuning option p used to control hash table redundancy. We set the parameter p to each of 1 . 2, 1 . 5, and 2 . 0. 3.20GHz processor with eight cores and about 141GB of memory. For the building time experiments, we reboot the server and measure the performance of the first runs, and for the other experiments, we run the program twice, measuring the performances of the second runs. DALM is implemented in C++ and uses a dictionary to convert word surfaces to word IDs. We use the darts-clone library 7 as the word dictionary. We conduct experiments to measure the performance of building DALMs. The experi-ments are intended to  X  X ompare the building speed to that of KenLM,  X  X ompare the building time for each implementation,  X  X ompare the difference between the division numbers, and  X  X how the filling rate of the double array, which is used to confirm how the dividing method inflates the model size.

In the building time experiments, we measure the CPU time and real time. Although the building stages of DALMs are computed in parallel, the CPU time is cumulative but does not include disk I/O time. In contrast, the real time is the actual waiting time until the model structure is built.

For building the KenLM model, we use a program included in the Moses de-coder [Koehn et al. 2007], release 3.0. Since KenLM has two build options, mmap and after , we measure both. The mmap option causes KenLM to first allocate space for the model file on disk and then use the mmap feature. With the after option, KenLM builds the model in memory and writes to a file after finishing the build. For building the DALM model, we build the DALMs in eight threads at most because the server has only eight cores, and words are converted to their corresponding IDs during the preprocessing stage.
 First, we compare the building time of DALMs and KenLMs for different model sizes. Figure 11 shows the building time of each model. Since after is always faster than mmap for KenLM on our test equipment, we show results for only the after method to simplify the graph. The division number of DALMs is fixed to 16. We can see that the required time for building DALM is not exponential but does increase with size. Although the required time increases with model size, the real time is shorter than the CPU time. Dividing the trie thus works well by allowing parallel computation. The corresponding times for KenLM were much faster, which confirms that finding BASE values incurs high computational costs.

Next, we investigate how the division number affects the building time and filling rate. In Section 4.6, we assume that the number of nodes between the divided tries is the same for each and that the number of unused slots is proportional to the number of insertions. In actual situations, the number of nodes is unlikely to be equal and the division number may affect the number of unused slots. We try to see the actual effect in a real situation in the experiment. Table IV shows the time for building DALMs. Comparing CPU time with real time, the parallel computation works well. For DALM bst , the number of nodes directly affects the building time, but the building time for DALM re v is not affected much. Therefore, with DALM bst , we can increase the division number when we need faster building. The reason why DALM re v does not affect CPU time is the division overhead. In the implementation of DALM, each thread of the program reads the entire language model file, which increases time cost. The building cost (which is reduced by division) and the division overhead (which is increased by division) are almost the same here. We can see that the real time for DALM re v is also little affected. Although we divide the original trie into several parts and build them in parallel, the slowest build determines the reported real time. To confirm that parallel computing works even with DALM re v , we try to build DALM re v without the division, using Tiny. It takes 5,916s in real time and 5,734s in CPU time. Therefore, we can achieve faster building by using the division method for DALM re v .

Figure 12 shows the filling rate of each model. Increasing the division number de-creases the filling rate. Especially for DALM bst , the difference is small in comparison with the benefit of increased building speed. As for DALM re v , the rate of decrease is slower than for DALM bst because the number of nodes in DALM re v is smaller than in DALM bst , and the building time is much faster than for DALM bst . Faster building means that each BASE value is found in earlier stages. Therefore, each BASE value is expected to be small, resulting in shorter array sizes.

Finally, Figure 13 shows the relations between the number of nodes inserted and the filling rates of the double array. Because DALM has several double arrays resulting from the dividing method, we calculate the filling rates by using the total numbers from multiple double arrays.

Throughout the experiments, DALM re v has better performance than DALM bst in terms of both the building speed and filling rate but is still much slower than KenLM. More research is required regarding how to shorten the construction time of the DALMs. Next, we evaluate performance on perplexity calculation. Reported performance for both DALM and KenLM does not include conversion time from the word surfaces to IDs because we want to see the pure query speed, and in practice the conversion is often conducted at startup time or upon model loading. We exclude the startup time from the reported times by executing the calculation program, which just loads models, converts word surfaces to IDs, and exits. We then subtract the time from that of the corresponding query program, which calculates the perplexity. For comparison, we use the version of KenLM included in the Moses decoder [Koehn et al. 2007], release 3.0.
First, we conduct an experiment to see how the division number affects the query speed. Figure 14 shows the result of each model in terms of the division number and query speed. We can see that increasing the division number tends to decrease the query speed a little. This is because increasing the number makes the filling rate drop, and this decreases the cache hit rate.

Next, we compare the query speed of DALM to that of KenLM. Figure 15 shows the result of each model in terms of memory usage and query speed using the models Tiny and Large. The division number of the double arrays in these experiments is fixed to 16.

For KenLM, we experiment with both patterns:  X  X map with populations X  and  X  X ead X  (which means that KenLM does not use the any mmap feature) modes. KenLM uses mmap with populations in Linux environments when configured with the default pa-rameters for query speed. Because the performance reported by Yasuhara et al. [2013] was obtained with the default parameters and we implement DALM without any mmap features, the conditions are not the same. We show the results for only  X  X ead X  because  X  X ead X  shows better results than  X  X map. X  8 Moreover, DALM does not implement any mmap features, so we conduct experiments with  X  X ead X  set for KenLM in the following.
There is a trade-off between speed and size for KenLM trie and probing . The model size of DALM bst is larger than that of DALM re v and is almost the same as that of KenLM probing (with p = 1 . 5). This is because DALM bst needs more space for storing the highest-order n -grams. Specifically, DALM re v needs n nodes and DALM bst needs n + 1. Comparing DALM and KenLM, the speed of DALM tending toward a bit worse than KenLM probing , and has a slightly larger model size than does KenLM trie ; DALM bst is worse than KenLM.

Finally, we conduct an experiment to see the scalability of each implementation. In this experiment, we can see how the query speed drops when the model usage increases. Figure 16 shows the effect on query speed when increasing the model size from Tiny to Large. We can see that the scalability of DALM is almost the same as that of KenLM, whereas the query speed is at the midpoint between KenLM trie and probing . Here, we can see that DALM re v is much more compact than KenLM probing and near to KenLM trie ; we can store the Large model in the same space as the Mid model of KenLM probing .

Overall, DALM re v is faster than DALM bst . This is likely because DALM re v needs fewer transitions than DALM bst . Since the leaf compression reduces the nodes, the number of transitions is also reduced. In addition, the model size of DALM re v is smaller than that of DALM bst , which works to increase the cache hit ratio. Although Yasuhara et al. [2013] report that DALM bst outperforms KenLM probing , our experiments differ. There are several possible reasons:  X  X e have introduced a state-handling method, which increases overhead.  X  X asuhara et al. [2013] perform experiments using mmap mode in KenLM.  X  X asuhara et al. [2013] perform experiments with only p = 1 . 5 (the default value) in
KenLM.  X  X e have corrected for the common overhead of the experimental environments.
To introduce a state-handling method into DALM bst , we need to traverse the trie twice in a single query: first, we need to get the log probabilities, and second, to access the right extension information to build the state, which are stored separately. The backward suffix trees are efficient at single queries but not at sequential queries. The extra traversal takes more time, although we try to minimize the overhead.
Although Devlin et al. [2015] show that the query speed of their neuralnet-based language model is on par with that of DALM (about 1.4M queries/s), our results in this work show faster querying (about 2 to 5 M queries/s). This difference is accounted for by the fourth item of the preceding list. The overhead of the perplexity calculation program used to measure the speed of each method in Yasuhara et al. [2013] means that the reported speed includes time spent on tasks that are not part of the lan-guage model. Here, the overhead was removed from the program, and so the query speeds in this article more clearly express more purely the speeds of the language models.

We see that the scalability of DALM is almost the same as that KenLM, and, espe-cially for DALM re v , the memory size is near to that of KenLM trie .
 Next, we evaluate the performance of DALM on the translation. We use the Moses decoder [Koehn et al. 2007], release 3.0. This release has a DALM wrapper class for DALM bst , but we have fixed some defects and use our modified version. Reported performance includes a part of word lookup time. Although the language models convert the word IDs of the decoder to the language model, the required lookup time is very small. The decoder and language models have their own independent word IDs, and the decoder has a conversion table (as a one-dimensional array) of word IDs from the decoder to the language models. We exclude the startup time from the reported times by executing the decoder. Specifically, we run the decoder, load the models, and exit. We then subtract the startup time from the corresponding total time. The division number of the double arrays is fixed to 16.

First, we conduct experiments to evaluate the difference between memory usage and translation speed for each language model. Figure 17 shows the results of the machine translation. In terms of balance between translation time and total memory size, DALM re v is smaller than KenLM probing at almost the same speed, and it is faster than KenLM trie . The results for DALM bst are almost the same as those for KenLM probing ( p = 1 . 5). We thus see that the proposed methods exploit the double array X  X  ability, resulting in a small size and fast queries in the translation.
We next conduct experiments to assess the trade-off between speed and translation quality. If a faster language model can accelerate the decoder, then we can increase the search space for better translation quality. Moses decoder uses cube pruning [Chiang 2007], which is one of the most efficient methods of decoding hypotheses graphs. We can tune the degree of decoding error by setting the cube-pruning pop limit. If we increase the pop limit, then the decoder can find better hypotheses at the cost of slower decoding. In other words, the pop limit allows choice in the trade-off between quality and speed.

Table V shows the results in terms of translation speed and memory usage with increasing pop limits. We use pop limits of 2, 20, 200, 400, and 600. Because the BLUE score and model score between methods are the same, we make comparisons on only time and memory usage. The experimental results show that the differences between KenLM trie and DALMs increase as the pop limit is increased. For each pop limit, DALM re v is faster than or almost the same as KenLM probing , and DALM bst is almost the same speed as KenLM probing . We have proposed language model implementations based on the double-array struc-ture. Because the double-array structure is a fast, compact representation of trie trees, we applied the structure to a language model.

In applying the double-array structures to the language model, we proposed DALMs for two types of representations of language model tries: DALM bst and DALM re v ,which correspond to backward suffix trees and reverse tries, respectively. Moreover, we em-bedded the model parameters in the double array itself. In particular, in DALM re v ,we also dropped some of the endmarker nodes to reduce the size of the trie without loss of information.

The experimental results for model-building tasks show that the construction of the double array is affected by the number of the nodes in the trie. The build time is much longer than that for the state-of-the-art systems, but dividing the trie can reduce the time.

The results on perplexity calculation tasks show that the double-array structures work well for the language model queries and that DALM achieves fast query speeds relative to the model size. In comparison with the state-of-the-art system, DALM bst is slower than KenLM; however, DALM re v achieves better results: almost the same size as with KenLM trie with a slightly slower speed than with KenLM probing .

Additionally, we conducted experiments to test the performance under the transla-tion task for the hierarchical phrase-based model. The results show that DALM is also effective for translation in terms of the speed and size. The trends of the experimental results are almost the same for the perplexity calculation tasks: DALM bst performs similarlytoKenLM probing , and DALM re v is almost as fast as KenLM probing and gives a slightly larger size than KenLM trie .

Although DALM bst is slower than KenLM probing , the differences in translation speed between DALM re v and a state-of-the-art system increase in terms of the balance between translation speed and decoder search errors. We conclude that the reverse trie is more suitable than the backward suffix trees for the decoder because the reverse trie can use the states X  true values.
 Experimental results show that DALM re v achieves better results than DALM bst . On the basis of those results, we recommend DALM re v for general use. Although we can show that DALM is fast and compact, building requires a lot of time. For some production environments, such as certain types of Web-based services, faster querying may be more important than fast building, and we recommend DALM for such cases. Last, we describe three targets for future work. The first is to improve building speed. Experimental results show that building the DALM structure needs more time than the analogous task in KenLM. Although dividing the trie can mitigate this, it is difficult to assert that this is scalable. We need a faster method. The second is to remove the limit on number of nodes in the trie. At present, if the trie exceeds the limit, then we cannot construct the double-array structures. The last is quantization. DALM does not have quantization features, although KenLM trie does. It is difficult to introduce quantization into DALM because DALM stores values in the double-array structures, and all slots consumed in the double arrays must have equal size. Therefore, we need to develop a new method for introducing quantization features.

We distribute implementations of DALM at https://github.com/nowlab/DALM under the GNU Lesser General Public License.

