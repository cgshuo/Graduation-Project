 A large class of queries can be viewed as linear combina-tions of smaller subqueries. Additionally, many situations arise when part or all of one subquery has been preprocessed or has cached information, while another subquery requires full processing. This type of query is common, for exam-ple, in relevance feedback settings where the original query has been run to produce a set of expansion terms, but the expansion terms still need to be processed. We investigate mechanisms to reduce the time needed to process queries of this nature.

We use RM3, a variant of the Relevance Model scoring algorithm, as our instantiation of this arrangement. We ex-amine the different scenarios that can arise when we have access to the internal structure of each subquery. Given this additional information, we investigate methods to uti-lize this information, reducing processing costs substantially. Depending on the amount of accessibility we have into the subqueries, we can reduce processing costs over 80% without affecting the score of the final results.
 Categories and Subject Descriptors: H.3.3 Information Search and Retrieval: Relevance Feedback, Retrieval Models General Terms: Algorithms, Performance Keywords: query optimization, pseudo-relevance feedback, query modeling
Recent research in information retrieval has recognized the utility of structure in queries [14, 12, 7, 8, 3, 9], further justifying complex query languages in some search engines (i.e. InQuery, Indri). In addition, various evaluation confer-ences have led efforts towards researching the use of struc-ture in queries, such as INEX 1 and TREC X  X  Entity Track 2 Assuming that queries have some structure has become the expectation in retrieval; unfortunately large complex queries http://www.inex.otago.ac.nz/ http://ilps.science.uva.nl/trec-entity/ are likely to be slow to process, indicating a clear need for optimization if such queries have any hope of being called interactievly. Towards this end, we look to improve query processing on a recurring pattern in information retrieval models: linear combinations.

Linear combinations of scores lie at the heart of many well-known models in information retrieval. The class of proba-bilistic models contains, or easily translates to, linear combi-nations of unigrams or n-grams. The Language Model [13] is technically a ranking based on joint probability of the query terms q 1 q 2 ...q k = Q occurring in document D ; however in order to avoid underflow the scores produced are the sum of the log-probabilities: Score ( Q,D ) = P q  X  Q log P ( q | D ). A term-order aware extension to the Language Model, the Sequential Dependence Model, uses the weighted geomet-ric mean to combine the unigram, ordered, and unordered components of a given query [12]. When we take the log-arithm of these quantities, the entire equation again re-duces to a series of sums over log-probabilities. The Rel-evance Model [10], after generating the estimates for the likelihood of a term w coming from relevance model R , de-noted P ( w | R ), is a linear combination of log-probabilities as well: P w  X  V P ( w | R ) log P ( w | D ). Likewise, the binary inde-pendence models, most famously the BM25 model [15], are also linear combinations of term weights.

If we look to vector space models, we can observe a similar phenomenon. The standard cosine similarity scoring func-tion takes the inner product of D and Q  X  which in turn is a sum of products between the two vectors [17]. The Rocchio algorithm for relevance feedback [16] creates an interpola-tion between the components of each query term: These new weights are then used to provide a  X  X ore com-plete X  vector representation of the information need. In im-plementation, the score consists of calculating the dot prod-uct, as before. There are numerous other examples of estab-lished models that operate as linear combinations of various components, and linear combination is often one of the first methods tried when adding new information into existing models.

The major contribution of this work is to provide op-timization mechanisms to interpolating subquery compo-nents, depending on what we can assume about the sub-queries. Given the increasing use of structure in retrieval, optimization algorithms should operate on queries as struc-tured objects. We focus our efforts on linear combinations as they are a basic structure used throughout the field.
Query processing optimization has been a constant activ-ity in information retrieval since the field X  X  inception. Many strategies have been developed for various models, provid-ing different guarantees. While we cannot cover the entire history of optimization in IR, some prior work has more di-rectly influenced the work here.

The use of an inference network as a retrieval model was introduced by Turtle and Croft in the early 1990s [22, 23]. Their analyses showed that a significant number of exist-ing retrieval models at the time could be efficiently repre-sented in the inference network framework, a form of di-rected graphical model, making it capable of representing a wide range of retrieval models. This framework is the inspi-ration for the graphic representation of queries used here, which affords us an efficient way to manipulate the query structure.

Turtle and Flood developed the Maxscore optimization for early-termination in both document-at-a-time (daat) and score-at-a-time (saat) query evaluation strategies. Follow-ing on this work, Strohman et al. combined the  X  X opdocs X  caching method of Brown [6] with Turtle and Flood X  X  Maxs-core algorithm, producing a hybrid algorithm that outper-formed both prior techniques [21]. We make use of this version of the algorithm several times here.

In work focusing on saat index organization, Anh and Mof-fat extended the work of pruned query evaluation to impact-sorted indexes [2]. The document scores are first binned and truncated to integers, allowing for better compression and faster query evaluation. They then use an algorithm to limit the size of the accumulator table during scoring. They main-tain a top candidate list in addition to the accumulator ta-ble, and use score bounds to determine when they can stop adding candidates, as well as stop tallying scores. Their paper also provides an excellent overview of index models and optimization strategies to date. Our approach here is not directly comparable to their work, as we study a dif-ferent index organization; however several of our algorithms would work in conjunction with optimizations designed for saat-organized indexes.
 Research in the area of efficient query expansion includes Billerbeck and Zobel, who conduct several rounds of experi-ments examining the use of auxiliary data structures for im-proving the efficiency of query expansion [5, 4]. They discov-ered that using short summaries of documents significantly reduced the time needed to analyze the documents used for feedback, which they considered to be the largest bottleneck during automatic expansion. In contrast, our concern is not with generating the expansion query -the techniques shown here are applied after such generation. The methods de-scribed by Billerbeck and Zobel should smoothly integrate with the merging techniques shown here, although experi-ments to prove this hypothesis are beyond the scope of this work.

Cartright et al. first look into the problem of specifically optimizing the Relevance Model [11]. They considered a document-document similarity matrix to serve as the of-fline store for supplemental data. They then tried to reduce the size of this matrix in order to improve its scalability to larger scale collections. Their results indicate that both Table 1: Statistics on the collections used in exper-iments. static (i.e., index time) and dynamic (i.e., query time) prun-ing techniques have potential to dramatically reduce the size of the matrix with minor negative impact on the effective-ness of the original Relevance Model. Their approach at-tempted to directly optimize retrieval scores via document-document similarity rather than manipulating the structure of the query components, as we do here.
In this section we describe the different algorithms we test in our experiments. We begin with data, software, and eval-uation in order to provide context necessary for the remain-ing discussion in this section.
We conduct our experiments on three collections, as shown in Table 1. The AQUAINT collection refers to the dataset and 50 queries from the TREC 2005 Robust Track. GOV2 uses the dataset and 150 automatic run queries from the Adhoc task of the TREC 2006 Terabyte Track. Clue-B refers to the ClueWeb Category B sub-collection and 50 queries used in the Adhoc task of the TREC 2009 Web Track.

We conduct all retrieval experiments using the Galago 3.0 search engine 3 . We generate all on-disk data structures, such as the indexes and the topdocs lists [21] described in later sections, using the TupleFlow distributed processing framework [19]. Our primary concern is improving query processing time. We perform two separate kinds of evaluation to measure the effect on processing time. First, we record the number of times we call Score on a term scoring node, which is the only construct in our model that directly iterates over data stored on the disk. Since the optimizations described here focus on reducing this factor, we get an upper bound of how effective these algorithms can be in reducing processing load. Second, we record the actual elapsed time of each query to determine the improvement in processing time in a live system. We use the first evaluation as the focus of our initial analysis of the algorithms, and we discuss the second type of evaluation in Section 4.4.2.

Some techniques presented here guarantee that if a doc-ument appears in R , the final ranked list, the score is the same as the unoptimized version. We call an algorithm with this property score-accurate to rank k . Prior work has re-ferred to a similar property, rank-safe up to k , meaning the ranks of documents up to rank k are unchanged from the original ranking function [24]. As defined, these two prop-erties are unrelated: an algorithm may exhibit the score-accurate property, but not the rank-safe property, and vice-versa. This occurs, for example, when an algorithm only http://galagosearch.org fully scores a subset of the entire candidate list. A document in the original R may not be scored by the algorithm, there-fore is not in the new R , however all documents appearing in the new R have been scored correctly. The Rewind al-gorithm, described in Section 3.4.2, behaves in this fashion. For the remainder of this work, we assume r is the number of documents requested, so we abbreviate score-accurate to rank r to score-accurate , and rank-safe up to r to rank-safe .
In the cases where an optimization is score-accurate and rank-safe, we do not report the effect on retrieval effective-ness. Otherwise, we report Mean Average Precision (MAP) as implemented by the trec_eval program 4 . In calculating the statistical significance between score count samples, we use a paired randomization test, as described by Smucker et al. [18]. When determining statistical significance of the real time measurements, we use a standard randomization test to maintain tractability. In both cases we set our sample size to 10 6 .
We choose to focus on the RM3 scoring function [1] as our exemplar scoring model for this work, for several reasons: 1) previous research has shown it to be effective in practice [1]; 2) it interpolates only two subquery components, making it a canonical case for our study; and 3) the internals of both subqueries also have simple structure, affording easy manipulation.

RM3 is a variant of the Relevance Model that interpolates the Language Model [13] and Relevance Model [10] scores for a given query Q and document D : Score RM 3 ( Q,D ) =  X Score LM ( Q,D )+(1  X   X  ) Score RM ( Q,D ) Typical implementations of RM3, such as the implemen-tation in Indri [20], construct an interpolated query after an initial round of retrieval and submit the entire revised query for scoring. That is, the LM scores are used to build the query that generate the RM scores. The algorithms described here are applied after the expansion terms have been selected. Clearly if  X   X  { 0 , 1 } , then we can simply drop the zero-weighted subquery, and only evaluate the re-maining subquery. However in cases where 0 &lt;  X  &lt; 1, both subqueries matter, and we must evaluate both. This is the situation we address here. Instead of re-submitting the re-vised query, including the initial query as a subquery, we intervene to leverage structure in this expanded two-part query in order to improve performance.

Given the point of intervention, we slightly recast the for-mula above to specify the components involved. The original query we label P , which is the processed subquery at this point. The expansion terms we label U , the unprocessed part of the query. Therefore we reformulate the RM3 scor-ing function to reflect this: Score RM 3 ( Q,D ) =  X Score LM ( P,D )+(1  X   X  ) Score RM ( U,D ) Note that in order to create U , the system must have gen-erated an initial ranked list based solely on P ; we call that list R P . We allow the system to access this initial ranked list during processing.

In order to organize the remaining discussion, we rep-resent the query structure as a network of interconnected nodes, similar to an Inference Network [22]. As an example, http://trec.nist.gov suppose P is the query hydrogen energy , and we construct U using 3 expansion terms, science , nuclear , and research . Assume that each term comes with an associated weight. We label the associated weights as follows: the i th term in subquery P has associated weight p i , likewise the j th term in subquery U has associated weight u j . Let  X  ( t,D ) and  X  ( t,D ) represent the scoring functions of a single term given a document D and term t for subqueries P and U , respec-tively. Mathematically, the score of a document D for this query is:
Score ( Q,D ) =  X Score LM ( P,D ) + (1  X   X  ) Score RM ( U,D ) In choosing RM3 as our scoring function,  X  and  X  are in fact the same term scoring function: the log of the smoothed term likelihood of t in D .
 The graphical representation of this query is shown in Figure 1. The construction provides us with a clean way of expressing the query visually. The nodes at the bottom of the tree are term scoring nodes -they represent the scoring functions  X  and  X  in Equation 1. The nodes up the tree are all combining nodes -they combine the incoming scores according to the weights along the incoming edges. In our case, all combining nodes represent a weighted sum over the contributed scores. Assuming N is a combining node, let  X  denote the subtree rooted at node N , excluding N itself. In implementation, the term scoring nodes act as iterators over the posting list of the given term, and the combining nodes act as meta-iterators that ensure the scoring over documents proceeds correctly.

The processing model uses document-at-a-time process-ing, meaning each document is scored in its entirety before another document is scored. Under this model, we construct the network once, and iterate over all documents, having each term scoring node score each document (a term scor-ing must produce a score for any document), combine the scores, then move to the next document. We assume all iterators are capable of the following functions: 1) score a given document, 2) tell the document id that the iterator is currently on in the posting list, 3) reset to the beginning of its posting list, and 4) move to/past a particular docu-ment id in its posting list 5 . Combining nodes simply pass the command forward to all nodes under them. For exam-ple, the statement  X  X e reset P . . .  X  means that the term scoring iterators under P are all reset to the beginning of their respective posting lists.

During description and analysis of the algorithms, we will also consider the set of documents that appear in the posting list of a given term. We denote this as  X ( N ), where N is a node in the network. If N is a combining node, then  X ( N ) = S M  X   X  N  X ( M ).
Given the graphical representation of a query, we now assume that we have varying amounts of information about
All posting lists are sorted in increasing document id order, so any id can be found in log time using binary search over a sample of positions from the list. Figure 1: Graphical representation of an example query. each combining node that we can leverage. We define an additional property over the combining nodes, known as its visibility , which has two values: accessible and inaccessible . We say a combining node N is accessible if we have the capability to examine and modify the weights and nodes in the subtree of N . We denote this as A ( N ). If N is inaccessible, denoted  X  A ( N ), we cannot examine or modify the internals of N . Note that even if a node is inaccessible, we may still perform the previously mentioned operations on it; therefore an inaccessible node may still be scored, as is the case in federated search or when using a third-party system.

We examine 4 distinct cases, shown in Figure 2. In the first case, we have  X  A ( P ) and  X  A ( U ), meaning we cannot di-rectly observe or modify the structure of either subquery. Treating a node as inaccessible guarantees that the seman-tics of the subquery are not inadvertently changed, which is desirable in some cases. This situation may occur in feder-ated search or distributed search, where a query comes from an external source, and the ability of the system to restruc-ture the query is restricted. This may also occur when both P and U represent scoring nodes that the system cannot modify, for example if both subqueries are phrases. In the second case, we have A ( P ) and  X  A ( U ) and in the third case  X  A ( P ) and A ( U ). Since we assume that P has undergone some amount of processing whereas U has not, we do not assume the optimizations available between cases 2 and 3 will be the same. These cases occur whenever we encounter a subquery the system may understand (e.g., a subquery that is also a linear combination, such as a Language Model query), but the other subquery cannot be understood or is implemented externally, for example if the node represents a function that calculates a score for D based on geolocation data. We can still use the node to score documents, but we have no way of reorganizing the implementation of the node. Finally, we have A ( P ) and A ( U ) which is the fourth case. This represents the fortunate case where the system can manipulate both subqueries. As shown earlier, this case occurs frequently across various text-based retrieval models. We do not consider the case when  X  A ( Q ), as that leaves us with no information about the query structure, therefore the methods described here do not apply.
In addition to the standard linear combining nodes in our network, we also implement the Maxscore algorithm de-scribed by Turtle and Flood [24] as a combining node. Fun-Figure 2: 4 cases under investigation, with varying amounts of accessibility. Shaded nodes are inacces-sible. damentally a Maxscore node is a combining node that per-forms extra bookkeeping to determine if it can stop combin-ing scores early. If it stops early, then the candidate docu-ment would not have passed the threshold score (the lowest score in the list of top r documents so far) and it returns the incomplete score early. Otherwise the node completes scoring and returns the correct score. We denote a Maxs-core combining node N as N M . We define a function, Make-Maxscore , which accepts a regular combining node and produces the Maxscore -enabled version of that node. Mathematically, the two node types will return the exact same documents and scores for the number of documents requested ( r ). While we may replace any combining node in the network with its Maxscore equivalent, we consider the direct interaction of multiple Maxscore nodes outside the scope of this work.

We also augment Maxscore similar to Strohman et al. by including topdocs lists for the term scoring nodes [21]. The topdocs implementation in Galago operates similar to the implementation in Indri. Every posting list over length l has its best b scoring documents stored in a short list, known as its topdocs list. When Maxscore accesses these lists, it partially scores the union of all available topdocs lists, allowing the algorithm to immediately lower its threshold to filter candidates, as well as lowering the potential on each term scoring node with a topdocs list. Both l and b are set to 1000 for our experiments.

In Galago, the topdocs are kept as a separate posting list file in the index, and are only accessed when requested by the retrieval model. While this organization may not be as real-time efficient as Indri, we believe this creates a better in-terface for future experimentation with such data structures where supplemental information can be made available on demand. We implemented the Maxscore node to check for topdocs lists. Therefore if a term scoring node has a topdocs list available, the Maxscore node will utilize that extra information.

We now describe the experimental algorithms we test in each scenario. For the following discussion, we assume R is the final ranked list returned by the algorithm. 3.4  X  A ( P ) and  X  A ( U )
We begin with the first case, where both P and U are inaccessible.
Fusion is the first, and simplest, of the algorithms we present. The intuition behind this algorithm is simple: RM3 scores using both P and U . Instead, Fusion only makes use of whatever R P provides, and ignores P . The pseudocode for Fusion is shown below. We assume R P and R U act as maps from the documents returned to their scores, for notational convenience. The algorithm begins by evaluating U , producing ranked list R U (lines 1-4). To guarantee that every document receives some kind of score, we establish lower bounds for each list (lines 5 and 6). These bounds acts as the default score for any document not explicitly in the corresponding list. We then take the linear interpolation of R P and R U using parameter  X  (lines 7-14). Finally, we return the highest-scoring r items from this merge (lines 15-19).
 Fusion ( R P ,U, X ,r ) 1 R U  X  New-Map 2 foreach d  X   X ( U ) 3 Insert ( R U ,  X  d, Score ( U,d )  X  ) 4 Trim ( R U ,r ) 7 A  X  New-Map () 8 foreach d  X  R P 9 A [ d ]  X   X R P [ d ] + (1  X   X  ) min U 10 foreach d  X  R U 11 if d  X  A 12 A [ d ]  X  A [ d ] + (1  X   X  )( R U [ d ]  X  min U ) 13 else 14 A [ d ]  X   X min P + (1  X   X  ) R U [ d ] 15 R  X  New-Queue () 16 foreach d  X  A 17 Insert ( R,  X  d,A [ d ]  X  ) 18 Trim ( R,r ) 19 return R The Fusion algorithm is simple, requires no further disk-accesses after evaluation, and two linear passes through R and R U , which are typically much smaller than the number of documents evaluated to produce the corresponding lists. Note that the only time we call the Score function is at the beginning, when we populate R U (line 3). We do not score any document against P , whereas the standard RM3 algorithm would. This is where we receive the gain in effi-ciency over RM3. This method also has the advantage that it can be applied across virtually all scoring and processing strategies, as long as the final scores can be combined inde-pendently of each other (i.e. the final score of a document does not rely on any other score in the same list).
Fusion is neither score-accurate nor rank-safe: a docu-ment that does not appear in R P will not receive the cor-rect score -instead it will receive the lowest available score in R P . This in turn can cause a change in the ordering of documents in R . Consequently all documents not in R are overestimated if they appear in R U . The converse is true as well. Also, the ability of this method to reduce the number of accesses is limited  X  as the number of expansion terms grows, the percent of processing that Fusion will save relative to processing the whole query will decrease.
In an effort to improve correctness among the final scores, we present another algorithm, which caches R P and makes use of the scores in it when possible; otherwise we use P to score the document. We call this algorithm Rewind , since we rewind P , and use it to score documents as requested by U . The pseudocode for Rewind is shown below.
 Rewind ( P,R P ,U, X ,r ) 1 Reset (P) 2 R  X  NewQueue () 3 foreach d  X   X ( U ) 4 if d  X  R P 5 score  X   X R P [ d ] + (1  X   X  ) Score ( U,d ) 6 Insert ( R,  X  d,score  X  ) 7 else 8 MoveTo (P, d) 9 score  X   X  Score ( P,d ) + (1  X   X  ) Score ( U,d ) 10 Insert (R,  X  d,score  X  ) 11 Trim ( R,r ) 12 return R The Rewind algorithm is not as efficient as Fusion ; in ad-dition to calling Score with node U , it also will frequently call Score using node P (line 9). However iteration is di-rected by U (line 3), so Rewind still makes less Score calls than the original RM3 algorithm.

Rewind is a score-accurate algorithm but it is not rank-safe. Consider the set  X ( P )  X   X ( U ). These are documents in the posting list(s) of P that are not in U . They will never be considered as candidates, therefore they will never receive a score, and have no chance of making it into R , the final ranked list. Therefore, all of the scores returned in R are correct, however some documents that should be in R may be omitted. 3.5 A ( P ) and  X  A ( U )
We now assume that P is accessible, therefore we have access to its internal structure. The first modification we make is to replace P with P M , its Maxscore equivalent, to improve evaluation over P  X  X  subtree. However we can go even further, and incorporate U directly into P M  X  X  subtree. We do not need access to U to do this; we only need to take care to properly adjust the weight of U when we add it. We call this algorithm Max-Plus .
Our plan is to add U to the subtree of P M . However we must take care to properly adjust the weights when adding P . Fortunately, we have access to the weights associated with  X  P M as well as  X  , our interpolation weight between the two components. Using these we can derive the appropri-ate weight for U . Let W U be the desired weighting for U , W  X  + (1  X   X  ) = 1. We know the ratios of  X  : 1 and W P M : W are equivalent: Substituting into the ratio for W U : meaning we can represent the required weight for U in terms of the weights of  X  P M and  X  . The conversion of the query example is shown graphically in Figure 3.
 Figure 3: Converting the standard maxscore expan-sion to maxscore-plus. 3.6  X  A ( P ) and A ( U )
We now assume that U is accessible but P is not. As be-fore, we replace U with U M , to prune as much as we can over U  X  X  subtree. We can use the Max-Plus modification to incorporate P directly into the subtree of U M . The cal-culation for W P proceeds in the same fashion as for W earlier, therefore W P =  X  (1  X   X  )  X  1 W U M . To differentiate this version of the algorithm from Max-Plus , we use the label Max-Plus-U in the results section.
Unlike the case of A ( P ) and  X  A ( U ), we can now make use of R P to modify the operation of U . We would like to avoid adding P into the subtree of U M , to save the cost of iterating over the posting lists in P again. Instead, we only use R populate the internal candidate list of U M ; in other words we  X  X arm up X  the cache in U M . From this point forward, we use the Fusion algorithm to produce final results. We define the SetBounds function, which populates the candidate list of U M with R P . The pseudocode for Warmup is shown below. Warmup ( R P ,U, X ,r ) 1 U M  X  Make-Maxscore ( U ) 2 SetBounds ( U M ,R P , X ,r ) 3 return Fusion ( R P ,U M , X ,r )
We come to the situation where we can observe the inter-nal structure of both P and U .
We continue in the vein of Max-Plus and rewrite the query in order to flatten it into a single layer. Figure 4 shows the flattening of our query example from earlier. As before, we replace Q with Q M to use pruning where possible. We remove nodes P and U entirely, and report scores directly to Q M . Note that P and U themselves do not contribute to the scoring tally for our measurements. Therefore their removal does not affect the number of nodes being tallied; any change in the count will come from pruning.

This assumption, that our components P and U may be expressed as linear combinations themselves, holds true for many of the popular retrieval models studied today. There-fore while it is a strong assumption, it also applies to a large portion of models studied to date. We vary three parameters when performing experiments: Interpolation Weight The weight  X  determines the im-Number of Expansion Terms The ratio of terms between Number of Items Requested We vary r , the number of algorithm in question.

We present the main efficiency results in Table 2. Each row provides the results for a particular parameter setting in a given collection, and a column is the performance of a method over the different parameter settings. For each parameter setting, we provide the average number of times the Score function was called, and the percent change from the average for the RM3 method. In all cases, the default setting indicates the following parameter values: r = 100,  X  = 0 . 5, and  X  = 10. We include an RM3 run that makes use of Maxscore (RM3+), for comparison. In the graphical representation, we accomplish this by replacing node Q with Q
M , therefore we use the Maxscore algorithm directly on nodes P and U .

The results from the three datasets look largely equiva-lent, suggesting that the methods described here will retain their ability to improve over the baseline as the collection size increases. Both RM3+ and MaxPlus seem to generally perform better on the Clueweb-B dataset; analysis of this phenomenon reveals that the proportion of queries actively optimized (i.e. pruned) is much higher in Clueweb-B ( &gt; 45% in both cases) than in the other two collections ( &lt; 23%). This in turn produces a larger impact on the average score count.
 Table 3: MAP results over the AQUAINT collec-tion. Only methods that are non-safe are shown. The baseline method is the query-likelihood model, shown for comparison. 4.1  X  A ( P ) and  X  A ( U )
The behavior between the Rewind and Fusion algorithms is consistent between the collections; Fusion provides a sig-nificant decrease in the number of scores requested, while Rewind only reduces the evaluation cost by less than 10%. However we must also consider MAP scores for these al-gorithms. Table 3 shows the MAP scores produced by the non-safe 6 algorithms across the different parameter settings. Generally, the Fusion algorithm only provides a relatively small increase in performance over the baseline method (Lan-guage Model), and only in the case of r = 1000 does the performance of Fusion approach that of RM3.

Conversely, Rewind seems to stay true to the original scoring model, except in two cases. When r = 1000, the score is slightly lower than RM3. Analysis of the mean pre-cision at different ranks shows identical precision scores until past rank 200. When  X  = 0 . 8, the score is slightly higher than RM3, most likely due to the increased weight on U . Hence, more relevant documents would come from U , which are all correctly scored using Rewind .
Given the results above, we wonder if increasing the num-ber of results from just U would improve results. We have no control over R P , however we can instruct U to return a larger list of documents that we could use in merging. We run a few experiments to observe this behavior (Table 4). We do not show the average number of scores requested, as they are the same across all runs. We express the increase as a percentage of the original list, i.e. a value of 10 indicates that we request 1 . 1 r (10% more) documents to be scored, where r is the original list size.

The results suggest that increasing the size of the list re-turned from U can provide some minor benefits, but only
Non-safe indicates an algorithm lacks either the rank-safe or the score-accurate property. % More 0 10 25 50 100 MAP 0.0978 0.0977 0.0984 0.0986 0.0983 Table 4: Changes in MAP when varying the number of results requested from U . up to a point. The decrease when we double the size of the list indicates that around that point we stop adding useful documents to the merging procedure, and that the overes-timated score from P for those documents introduces noise in R .
While we have no control over the internals of either sub-query in this configuration, we examine how Fusion and Rewind may interact with Maxscore . To test this effect, we replace P and U with their Maxscore version where appropriate. In Fusion , we apply it to U , and in Rewind we apply it over the cached P and U . The results over the default parameter set are shown in Table 5. We can immediately see a large improvement in efficiency over the baseline methods. This indicates that Fusion and Rewind can combine with Maxscore to improve other using either technique alone. We leave further analysis as future work. Table 5: Results over the AQUAINT collection, now using the Maxscore optimization. The rightmost column indicates the change over the same method without the optimization. 4.2 A ( P ) and  X  A ( U )
The Max-Plus algorithm is the only algorithm tested for this scenario. Under the default and r = 1000 parameter set-tings, Max-Plus provides a consistent minor improvement over the original algorithm. The differences are not statisti-cally significant in the AQUAINT collection, however they are for GOV2. If the number of feedback terms is increased, the effectiveness of the algorithm decreases, which we expect  X  we have no control over U , therefore if its organization changes, it will have an effect on Max-Plus .
 The most interesting result comes when  X  = 0 . 8. Both RM3+ and Max-Plus perform exceptionally well. Further inspection reveals that in both cases, the bias towards P allowed for massive pruning during query processing. The only documents that have a chance to enter the candidate list are documents that contain components from P ; specif-ically, the entire subquery. Therefore the only documents that are even considered are in  X ( P ). The main differ-ence between these algorithms is that Max-Plus directly integrates U into its early-termination algorithm, whereas RM3-Maxscore can only interact with the nodes opaquely. We believe this allows the Max-Plus algorithm to converge to a stable threshold more rapidly, resulting in the increase in performance over RM3-Maxscore. 4.3  X  A ( P ) and A ( U ) We now look to the performance of Warmup and Max-Plus-U . Both algorithms provide substantial improvements over the original RM3 algorithm. As anticipated, Warmup consistently outperforms the Fusion algorithm in efficiency, where we do not pre-set the candidate list for the U node. However if we consider effectiveness (Table 3), we see that in fact Warmup performs worse than Fusion in all cases, and in some cases performs worse than the baseline method.
In contrast, the Max-Plus-U method appears to consis-tently provide substantial gains in efficiency, while main-taining score correctness. The only noticeable drop in per-formance for the Max-Plus-U algorithm comes from in-creasing the number of feedback terms (  X  = 100), which is consistent with the behavior of the other algorithms. How-ever unlike several of the other algorithms, Max-Plus-U appears to be more robust to the parameter change.
Clearly, having access to both sub-nodes allows the most effective optimization of the entire set. The Max-Flat pro-cedure consistently shaves off over 80% of the scoring re-quests, except when we increase the number of feedback terms by a factor of 10. Even under such conditions, Max-Flat still cuts over half of the scoring requests. Deeper analysis of this model shows that like Max-Plus and Max-Plus-U , having all of the term nodes managed by one Maxs-core node allows for the fastest stabilization of the thresh-old scores. The only case where this is not true is when  X  = 0 . 8, where P has so much weight that it largely over-whelms the contributions of U , allowing Max-Plus and RM3+ to perform on par with Max-Flat .
In the original Maxscore algorithm, the term scoring nodes are ordered according to the estimated lengths of their posting lists, the intuition being that if pruning occurs by the nodes processed first during a scoring pass, then moving the shortest posting lists to the front should minimize the number of documents considered. However our model has scoring nodes scaled by weights. Not only must we consider weights now, but as we rescale the weights of the nodes man-aged by the algorithm, as we do in the Max-* algorithms, we often have the situation that some subset of the nodes have weights orders of magnitude larger than the others.
Considering these new factors, we explore the possibility of ordering by weight to increase pruning further. We reason as follows: If the pruning decision is contingent on the heav-ily weighted nodes, then if we order by length of the posting lists, this weight imbalance is ignored. Hence we tend to prune wherever we hit these heavily weighted nodes, no mat-ter where they fall in the scoring order. Ordering by weight pushes these nodes to the front of the scoring list, removing the wasted effort of scoring nodes that do not trigger prun-ing. We test this hypothesis on the Max-Flat algorithm, and call it Max-Flat-W . Results are shown in Table 6 for different values of  X  , and for  X  = 100.

Surprisingly, ordering by weight is only effective when  X  = 0 . 8 or when  X  = 100. This indicates that the weight imbalance between the subquery components must be quite high for it to be effective. Worse performance of Max-Flat-W when  X  = 0 . 2 supports this hypothesis. This weight brings the individual component weights closer to unifor-Table 6: Comparing list length and weight ordering for the Max-Flat algorithm. mity, reducing the importance of ordering the scoring list by weight.
We now examine changes in performance based on system time measurements. We perform 5 runs of the GOV2 col-lection, with the queries permuted randomly in each run to lessen any dependence on query order. We record the time to execute each query for each method, making a total of 750 samples for each algorithm. The system used is a non-dedicated cluster of 32 nodes, each node having 2 Xeon 3.2 GHz cores with 4Gb of RAM and having access to a shared disk-server, where the index resides. Therefore a single run executes on a single core in this cluster. To compute statisti-cal significance between algorithms, we use a randomization test of 1 million samples with the sample mean as the suf-ficient statistic. We also determine what percentage of the queries shows some  X  X ositive effect X  relative to unmodified RM3. Let A be the algorithm in question. If the value for A is less than 90% of the value for RM3 for a particular measure (either score count or real time), we consider that to be a positive effect. For example, a value of 10 in the table indicates that 10% of the queries (75 of 750) had that measure reduced by at least 10% compared to RM3. Ta-ble 7 shows the mean runtimes of the samples, the percent change from RM3, and percentage of queries affected when measuring via score count and real time.

Based on the results from Table 7, the algorithms seem to fall into 3 distinct groups. The strongest of these is Max-Flat , and to a slightly lesser degree, Max-Plus-U , which seem to consistently reduce both the score count and the system time by a significant amount. Their average impact is high, and they cover a high percentage of the queries. The next group includes the Warmup and Fusion algorithms. They also have a consistent impact, but their average impact is less pronounced. The final group, consisting of RM3+, Rewind , and Max-Plus , have average runtimes are greater than the unmodified RM3, but they still register at least a 10% measurement improvement for some number of the queries tested. Further analysis of this last group shows that when the algorithms work, they have a significant impact on both measurements, often resulting in reductions over 40%. However, as the table shows, these algorithms do not  X  X rigger X  very often, and the added logic used to continually check for a pruning opportunity results in a notable increase in execution time.
We have shown that it is possible to greatly improve the efficiency of queries represented by a linear combination of subqueries, a common mechanism for integrating partial scores in information retrieval. Our results show that when the two subqueries are inaccessible, our options for safe opti-Table 7: Statistics over 750 queries run over GOV2. Mean times are in seconds. The F indicates statis-tical significance at p  X  0 . 02. The Score and Time columns report the percentage of queries that expe-rienced at least a 10% drop in the given measure-ment. mization are limited, but there are several courses of action we can take that typically have little negative effect on the accuracy of results. When we have access to the internal structures, we can manipulate the query structure further, providing greater efficiency gains while making stronger cor-rectness guarantees. Under the correct conditions, we can reduce the scoring cost by over 50%, and in many cases by over 80%, while not affecting the ranking results at all.
Several phenomena here require further study to fully ex-plain. All of the algorithms presented here appear to be sensitive to  X  , therefore, determining the underlying cause of this sensitivity will give us even further insight into the interaction between the subquery components. Given the emergent behaviors shown by the algorithms, understand-ing what properties of the algorithms cause such behavior would help to predict the interactions between optimiza-tions, queries, and collections.

Although several described algorithms provide substantial gains in efficiency, our experiments also raise new interesting questions to pursue. The results suggest that for different values of  X  , different algorithms afford the greatest reduc-tion in cost. Query analysis may enable us to choose which algorithm to use based on the value of  X  , thereby leveraging the best possible optimization available. Also, the real-time analysis here shows that if we can cheaply predict whether a certain optimization will activate on a given query, we may be able to avoid unnecessary overhead by not trying to optimize the query in the first place.

The above results show how we can reduce the process-ing cost of the RM3 algorithm, implying a similar impact on other retrieval models as well. RM3 serves as an archetype of interpolated subqueries: many other retrieval models share structure similar RM3, therefore the algorithms developed here should easily transfer to those instances. Furthermore, we can look to apply these algorithms to even more diverse query types. Consider, for example, an n-gram index, where these mechanisms could significantly impact the processing time of scanning the term-level posting lists by making use of the stored n-grams. Recent research has made use of n-gram posting lists [8, 7], therefore we can consider the pre-calculated n-gram posting list as the subquery contain-ing information, and the rest of the query (the unigrams) as not. We may encounter subqueries that must be evaluated against different indexes, such as in desktop search as de-scribed by Kim and Croft [9] or graph-based data, such as citation analysis information. One subquery may apply to the graph data, whereas the other applies to textual infor-mation. Other applications include temporal information, precalculated statistics for document or collection-level data, and geolocation information.

In the context of web search, the optimizations discussed here would most likely not impact the head queries that are heavily cached and optimized for performance. However tail queries could benefit greatly, as the use of psuedo-relevance feedback can still improve retrieval performance for such queries. The results here indicate that we can apply pseudo-relevance feedback, among other expansion techniques, to improve ranked results without suffering from high over-head.

In the past it was sufficient to investigate optimizations for queries that were simply bags of words -therefore one could assume that each component in the query represented a single iterator over a posting list. However with the ad-vent of structured queries that are now being applied over increasingly larger and larger sets of data, the need to re-spect this additional structure while designing optimizations for query processing has become evident. We believe this re-search moves towards addressing this need, and we intend to continue this line of research to provide even greater im-provements for structured query processing.
This work was supported in part by the Center for In-telligent Information Retrieval and in part by NSF grant #IIS-0910884. Any opinions, findings and conclusions or recommendations expressed in this material are the authors and do not necessarily reflect those of the sponsors. We would like to thank all reviewers for their helpful comments during the development of this work.
