 Tepper School of Business Motivated by [KS02, KK99] among others, Li, Littman and Wals h [LLW08] introduced the KWIK framework for online learning, standing for knows what it knows . Roughly stated, in the KWIK model, the learning algorithm is required to make only accur ate predictions, although it can opt examples on which it answers  X  . The goal of the algorithm is to minimize the number of exampl es on which it answers  X  . Several aspects of the model are discussed in [LLW08], and t here are many other papers, including [WSDL, DLL09, SL08], using the fram ework. It is worth mentioning that in earlier work such as [RS88], and referred to as reliable learning .
 Generally, it is highly desirable to have an algorithm that l earns a concept in the KWIK framework using a few, or even polynomial, number of  X  s. But unfortunately, for many concepts, no such the Mistake-bound model [Lit88], e.g. the class of singleto ns or disjunctions, the KWIK algorithm needs to say  X  exponentially many times. The purpose of our paper is to rela x the assumption of not making any mistakes , by allowing a few mistakes, to get much better bounds on the n umber of  X  s. Or, in the other direction, our aim is to produce algorithm s that can make substantially fewer mistakes than in the standard Mistake-Bound model, by tradi ng off some of those for (presumably less costly) don X  X -know predictions.
 In [LLW08], the authors show, through a non-polynomial time enumeration algorithm , that a finite class H of functions can be learned in the KWIK framework with at most | H | X  1 number of  X  s. we show that the problem is equivalent to the famous egg-drop ping puzzle, defined formally in time depends on | H | ; however, we propose polynomial versions of our algorithm f or two important classes: monotone disjunctions and linear separators.
 Allowing the algorithm to make mistakes in the KWIK model is e quivalent to allowing the algorithm Mistake-bound model by allowing the algorithm to say  X  . The rest of the paper is structured as follows. First we define the model and describe the limits of K WIK model. Then in section 2, we describe how would the bounds on the number of  X  s change if we allow a few mistakes in KWIK model. Finally, we give two polynomial algorithms for impor tant classes, Monotone Disjunctions and Linear Separators with a margin, in Section 3. 1.1 Model We want to learn a concept class H consisting of functions f : X  X  X  + ,  X  X  . In each stage, the assume h  X   X  H . The algorithm might answer, + ,  X  or  X  representing  X  X  don X  X  know X . After the want to design an algorithm such that for any sequence of exam ples, the number of times M that it makes a mistake is not more than k , and the number of times I that it answers  X  is minimized. the majority vote algorithm can learn the class with no  X  responses, i.e. I = 0 . Since we want to derive worst-case bounds, we assume that the sequence of the examples, as well as 1.2 The KWIK Model Although the idea of the KWIK framework is quite useful, ther e are very few problems that can be solved effectively in this framework. The following exampl e demonstrates how an easy problem in the Mistake-bound model can turn into a hard problem in the KW IK model.
 Example 1 Suppose that H is the class of singletons. In other words, for h otherwise. Class H can be learned in Mistake-bound model with mistake bound of o nly 1 . The algorithm simply predicts  X  on all examples until it makes a mistake. As soon as the algori thm makes a mistake, it can easily figure out what the target funct ion is.
 However, class H needs exponentially many  X   X  X  in the KWIK framework to be learned. Since the  X  that the answer is  X  on all the first 2 n  X  1 examples that it sees.
 The situation in Example 1 happens for many other classes of f unctions, e.g. conjunctions or dis-junctions, as well.
 Next, we review an algorithm (called the enumeration algorithm in [LLW08]) for solving problems in the KWIK framework. This algorithm is the main ingredient of most of the algorithms proposed in [LLW08].
 Algorithm 1 Enumeration the true label of x , the algorithm then removes from H those functions h that answered incorrectly on x and continues to the next example. Note that at least one func tion gets removed from H each number of  X   X  X . Example 1 shows how hard it can be to learn in the KWIK model. To address this, we give the following relaxation of the framework that allows concepts to be learned much more effectively and at the same time preserves the original motivation of the KWI K model X  X t X  X  better saying  X  X  don X  X  know X  rather than making a mistake.
 Specifically, we allow the algorithm to make at most k mistakes. Even for very small values of k , this can allow us to get much better bounds on the number of tim es that the algorithm answers  X  . to of mistakes needed to learn in the pure Mistake Bound model.
 We saw, in Algorithm 1, how to learn a concept class H with no mistakes and with O ( | H | ) number of However, if we allow the algorithm to make one mistake, we sho w that the number of  X   X  X  can be classic  X  X gg game X  puzzle (See [GF]). First suppose that k = 1 . We make a pool of all functions in
H , initially consisting of | H | candidates. Whenever an example arrives, we see how many of t he is  X  p | H | , we say  X  . Those functions that predict incorrectly on an example are removed from the that before making any mistake, we remove at least p | H | of the functions from the pool each time we answer  X  . Therefore, the total number of  X   X  X  cannot exceed 2 p | H | . This technique can be generalized for k mistakes, but first we mention a connection between this prob lem and the classic  X  X gg game X  puzzle.
 Example 2 Egg Game Puzzle or very fragile or anywhere in between: they may break if drop ped from the first floor or may not can be dropped without breaking. The question is how many dro ps you need to make. Note that you can break only two eggs in the process.
 when there are k eggs available, instead of just two eggs, is given in [GF]. Th e  X  minimization problem when k mistakes are allowed is clearly related to the egg game puzzl e when the building adjusts the threshold p | H | recursively each time an example arrives, we can decrease th e number of  X  s from 2 p | H | to p 2 | H | .
 Algorithm 2 Learning in the KWIK Model with at most k Mistakes each example that arrives, examine how many functions in P label it + and how many label it  X  . If  X  X  don X  X  know X  X .
 Therefore, the total number of  X   X  X  will not exceed 2 problem, which is often the case, the running time of Algorit hm 2 becomes exponential. In the next section, we give polynomial-time algorithms for two import ant concept classes. We can look at the problem from another perspective: instead of adding mistakes to the KWIK framework, we can add  X  X  don X  X  know X  to the Mistake Bound mode l. In many cases, we prefer our improve over optimal mistake bounds by allowing the algorit hm to use a modest number of  X   X  X , and an algorithm can always replace its  X   X  X  with random +  X  X  and  X   X  X , therefore, we must expect that decreasing the number of mistakes by one requires increasin g the number of  X   X  X  by at least one. 3.1 Monotone Disjunctions We start with the concept class of Monotone Disjunctions . A monotone disjunction is a disjunction in which no literal appears negated, that is, a function of th e form Each example is a boolean vector of length n , and an example is labeled + if and only if at least class can be learned with at most n mistakes in Mistake-bound Model [Lit88] where n is the total k -DNF formulas. We are interested in decreasing the number of mistakes at the cost of having (hopefully few)  X   X  X .
 First, let X  X  not worry about the running time and see how well Algorithm 2 performs here. We have each mistake for four  X  X  don X  X  know X  X . But unfortunately, Al gorithm 2 cannot do this in polynomial time. Our next goal is to design an algorithm which runs in pol ynomial time and guarantees the same good bounds on the number of  X   X  X .
 Algorithm 3 Learning Monotone Disjunctions with at most n/ 2 Mistakes During the process of learning, the variables will be moved f rom P to P  X  or P + . The pool P + that P gets empty.
 In each step, an example x arrives. Let S  X  X  x | S  X  P | = 1 , we answer  X  .
 If we make a mistake, we move S  X  P to P  X  . Every time we answer  X  , we move S  X  P to P + or P Proposition 2 Algorithm 3 learns the class of Monotone Disjunctions with a t most M  X  n/ 2 mis-takes and n  X  2 M number of  X  s.
 answered positive; for this to happen, we must have | S  X  P | X  2 . So, after a mistake, we can move S  X  P to P  X  . The size of P , therefore, decreases by at least 2 .
 P + or P  X  . Additionally, the size of P decreases by at least one on each  X  prediction. 2 saying  X  is cheaper than making a mistake, Algorithm 3 strictly domin ates the best algorithm in Mistake-bound model. Note that the sum of its  X  s and its mistakes is never more than n . More this algorithm is strictly smaller than n .
 Next we present an algorithm for decreasing the number of mis takes to n/ 3 .
 Algorithm 4 Learning Monotone Disjunctions with at most n/ 3 Mistakes one of the sets P , P + , or P  X  .
 Whenever an example x arrives we do the following. Let S  X  X  x of Also, if S contains both members of a pair in P  X  , we can say that the label is + . 3  X  . Description of how the algorithm moves variables between s ets upon receipt of the correct label is given in the proof below.
 Proposition 3 Algorithm 4 learns the class of Monotone Disjunction with at most M  X  n/ 3 mistake and 3 n/ 2  X  3 M number of  X   X  X .
 Since P  X   X  P +  X  n , we will not make more than n/ 3 mistakes.
 after knowing the correct label, S  X  P will be moved to P + or P  X  , so the same bound applies. If of  X   X  X  cannot exceed n/ 2 + n  X  3 M . 2 3.2 Learning Linear Separator Functions In this section, we analyze how we can use  X  predictions to decrease the number of mistakes for versus those that predict  X  and to make prediction decisions based on the result. Setting: We assume a sequence S of n d -dimensional examples arrive one by one, and that these and only if x is a positive example. Define  X  to be min that all examples have unit length.
 Below, we show how to formulate the problem with a Linear Prog ram to bound the number of mistakes using some  X  X  don X  X  know X  answers.
 Assume that n examples x we have to answer when a point arrives. The objective is to mak e a small number of mistakes and some  X  X  don X  X  know X  answers to find a separation vector w such that w x x is a + point. We can formulate the following linear program using t his instance (this sequence of points). Note that there are d variables which are the coordinates of vector w , and there are n linear con-straints one per input point. Clearly we do not know which poi nts are the + points, so we can not the coordinates of the vector w are always in range [  X  1  X   X / we are choosing the bounds to be  X  (1 +  X / Now assume that we are at the beginning and no point has arrive d. So we do not have any of the n constraints related to points. The core of the linear progra m is the set of vectors w in [  X  1  X   X /  X  first. For now assume that we can not use the  X  X  don X  X  know X  answ ers. We show how to use them program with a core of lesser volume. So we obtain one LP for ea ch of these two possibilities, and of our current linear program. We will show how to compute the se volumes, but for now assume we answer + . If our answer is true, we are fine, and we have passed the query with no mistake. Otherwise we have made a mistake, but the volume of the core of our linear program is halved. We do the same for the  X  case as well, i.e. we answer  X  when the larger volume is for  X  case. Now there are two main issues we have to deal with. First of all , we have to find a way to compute the volume of the core of a linear program. Secondly, we have t o find a way to bound the number of mistakes. In fact computing the volume of a linear program is # P -hard [DF88]. There exists a randomized in n, d, and 1 / X  . We can use this algorithm to get estimates of the volumes of t he linear programs we need. But note that we really do not need to know the volumes of these linear programs. We just need to know whether the volume of the linear program of t he + case is larger or the  X  case is larger or if they are approximately equal. Lovasz and Vemp ala present a faster polynomial time algorithm for sampling a uniformly random point in the core o f a linear program in [LV06]. One way to estimate the relative volumes of both sides is to sampl e a uniformly random point from the volume of the linear program for + (  X  ) case is at least a 1 with high probability. So we can answer based on the majority of these sampled points, and if we make a mistake, we know that the volume of the core of the linea r program is multiplied by at most 1  X  ( 1 2  X   X  ) = 1 2 +  X  .
 Suppose we have already processed the first l examples and now the l + 1 st example arrives. We program, and based on the majority of them we answer + or  X  for this new example. Using the following Theorem, we can bound the number of mistakes.
 Lemma 4 With high probability ( 1  X  1 of the core of the linear program decreases by a factor of ( 1 Proof: Without loss of generality, assume that we answered + , but the correct answer was  X  . So we number of positive sampled points would be less than ( 1 Therefore, by Chernoff bounds, the chance of the sample havi ng a majority of positive-predicting (at least 1  X  1 +  X  . 2 Now we show that the core of the linear program after adding al l n constraints (the constraints of the variables) should have a decent volume in terms of  X  .
 Lemma 5 If there is a unit-length separator vector w  X  with min complete linear program after adding all n constraints of the points has volume at least (  X / are in range (  X   X / Without loss of generality assume that it is a + point. So w  X  x | w  X  x i | is at least  X  X  w  X  || x i | . Since all its d coordinates are in range (  X   X / that | w  X  | is less than  X  . So ( w  X  + w  X  ) x that the coordinates of w  X  + w  X  are in range (  X  1  X   X / all its coordinates are between  X  1 and 1 ), and the coordinates of w  X  are in range (  X   X / least (2  X / Lemmas 4 and 5 give us the following folklore theorem.
 Proof: The proof easily follows from Lemmas 4 and 5. 2 Now we make use of the  X  X  don X  X  know X  answers to reduce the numb er of mistakes. Assume that we do not want to make more than k mistakes. Define Y core at the beginning before adding any of the constraints of the points. Define Y R We want to use  X  X  don X  X  know X  answers to reduce this number of m istakes. Define C to be R 1 /k . Let V, V constraint that the new point is a  X  point respectively. If V new point is a  X  point. In this case, even if we make a mistake the volume of the core is divided by at least C , and by definition of C , this can not happen more than log if than 1 /C , we answer  X  X  don X  X  know X , and we know that the volume of the co re is multiplied by at most 1  X  1 /C .
 Since we just need to estimate the ratios V probability for these two specific tests (to see if V sample 16 C log n points, and there are at most 8 log n + points among them, we can say that V is at most 1 /C with probability at least 1  X  e  X  64 log 2 n/ 32 log n = 1  X  1 make more than k mistakes. We also know that for each  X  X  don X  X  know X  answer the volume of the core is multiplied by at most 1  X  1 completes the proof of the following theorem.
 Theorem 7 For any k &gt; 0 , we can learn a linear separator of margin  X  in  X  d using the above algorithm with k mistakes and O ( R 1 /k  X  log R )  X  X  don X  X  know X  answers, where R is equal to We have discussed a learning framework that combines the ele ments of the KWIK and mistake-bound models. From one perspective, we are allowing the algo rithm to make mistakes in the KWIK model. We showed, using a version-space algorithm and throu gh a reduction to the egg-game puzzle, that allowing a few mistakes in the KWIK model can significant ly decrease the number of don X  X -know predictions.
 From another point of view, we are letting the algorithm say  X  I don X  X  know X  in the mistake-bound model. This can be particularly useful if don X  X -know predic tions are cheaper than mistakes and we can trade off some number of mistakes for a not-too-much-l arger number of  X  X  don X  X  know X  X . We gave polynomial-time algorithms that effectively reduc e the number of mistakes in the mistake-bound model using don X  X -know predictions for two concept cl asses: monotone disjunctions and linear separators with a margin.
 Acknowledgement The authors are very grateful to Adam Kalai, Sham Kakade and N ina Balcan as well as anonymous reviewers for helpful discussions and comments. [DF88] Martin E. Dyer and Alan M. Frieze. On the complexity of computing the volume of a [DFK91] Martin E. Dyer, Alan M. Frieze, and Ravi Kannan. A ran dom polynomial time algorithm [DLL09] C. Diuk, L. Li, and B.R. Leffler. The adaptive k-meteo rologists problem and its appli-[GF] Gasarch and Fletcher. The Egg Game. www.cs.umd.edu/~gasarch/BLOGPAPERS/egg.pdf . [KK99] M. Kearns and D. Koller. Efficient reinforcement lear ning in factored MDPs. In Inter-[KS02] M. Kearns and S. Singh. Near-optimal reinforcement l earning in polynomial time. Ma-[LLW08] L. Li, M.L. Littman, and T.J. Walsh. Knows what it kno ws: a framework for self-aware [RS88] R.L. Rivest and R. Sloan. Learning complicated conce pts reliably and usefully. In Pro-[SL08] A.L. Strehl and M.L. Littman. Online linear regressi on and its application to model-based
