 Measuring similarity between objects is a fundamental issue for numerous ap-plications in data-mining and machine learning domains such as in clustering or in classification tasks. In that context , numerous recent approaches that tackle the latter tasks are based on kernels (see for example [1]). Kernels are special dot products considered as similarity measures. They are popular because they implicitly map objects initially represented in an input space, to a higher di-mensional space, called the feature space. The so-called kernel trick relies on the fact that they represent dot products of mapped vectors without having to explicitly represent the latter in the feature space. From a practical standpoint, kernel methods allow one to deal with data that are not easy to linearly separate in the input space. In such cases, any clustering or classification method that makes use of dot products in the input space is limited. By mapping the data to a higher dimensional space, those m ethods can thus perform much better. Consequently, many other kinds of comp lex objects can be efficiently treated by using kernel methods. We have mentioned previously that kernels are generally introduced as similarity measures but as underlined in [2], dot products in gen-eral do not necessarily fit one X  X  intuition of a similarity index. Indeed, one could find in the literature several axioms that clarify the definition of a similarity index and in that context, any kernel doe s not necessarily satisfy all of them. As an example, one of these conditions that a dot product, and thus a kernel, does not always respect, is the maximal s elf-similarity axiom which states that the object to which any object should be the most similar, is itself.

In this paper, we are interested in des igning kernels which respect the basic axioms of a geometrical based similarity index. In that context, kernels nor-malization methods are useful. Basically, the most common way to normalize a kernel so as to have a similarity index, is to apply the cosine normalization. In that manner, maximal self-similarity for instance, is respected unlike for unnor-malized kernels. In this work we propose a new family of kernel normalization methods that generalizes the cosine normalization. Typically, the cosine normal-ization leads to similarity measures be tween vectors that are based upon their angular measure. Our proposal goes beyond the cosine measure by refining the latter score by using another geometrica l based measure which relies on the vec-tors X  norm ratio in the feature space. We give the following example in order to motivate such normalized kernels. Let u s take two vectors which are positively colinear in the feature spa ce. In that case, their cosine measure is 1. However, if their norms are not the same ones therefore, we cannot conclude that these two mapped vectors are identical. Accord ingly, their similarity measure should be lower than 1. Unlike the cosine normalization, the normalization approaches that we introduce in this paper aim at taking into consideration this point.
The rest of this paper is organized as follows. In section 2, we formally intro-duce new normalization methods for kernels. Then, in section 3, we give several properties of the resulting normalized kernels in the co ntext of similarity indices. We show that using normalization methods allows one to make any kernel satisfy the basic axioms of a similarity index. Particularly, we prove that these kernel normalizations define metrics. In other words, we show that normalized kernels are kernels. In section 4, we illustrate the benefits of our proposal in the context of clustering tasks. The method we use in that regard, relies on kernel PCA based k -means clustering which can be understood as a combination between kernel PCA [3] and k -means via PCA [4]. This two step approach is a spectral clustering like algorithm. Using several datasets from the UCI ML repository [5], we show that different normalizations can better capture the proximity rela-tionships of objects and improve the clustering results of the cosine measure and of another widely used normalized kernel, the Gaussian Radial Basis Function (RBF) kernel. We finally conclude and sketch some future works in section 5. We first recall some basic definitions about kernel functions and their cosine normalization. We then introduce our new kernel normalization methods. 2.1 Kernel Definition and the Cosine Normalization Let denote X the set of objects, represented in an input space, that we want to analyze.
 Definition 1 ((Positive semi-definite) Kernel). Afunction K : X X X  X  R is a positive semi-definite kernel if it is symmetric, that is, K ( x , y )= K ( y , x ) for any two objects x , y in X and positive semi-definite, that is: for any n&gt; 0 , any choice of n objects x 1 ,..., x n in X and any choice of any numbers c 1 ,...,c n in R .
 In the sequel, we will simply use the term kernel instead of positive semi-definite kernel. We have the following well-known property.
 Theorem 1. For any kernel K on an input space X , there exists a Hilbert space F , called the feature space, and a mapping  X  : X X  X  such that for any two objects x , y in X : where ., . is the Euclidean dot product.

When the objects of X are vectors represented in an input space which is an Euclidean space then, we can mention the following two well-known types of kernel functions:  X  Polynomial kernels: K p ( x , y )=( x , y + c ) d ,with d  X  N and c  X  0.  X  Gaussian RBF kernels: K g ( x , y )=exp After having recalled basics about kernels, we recall the definition of the cosine normalization of a kernel K , denoted K 0 .
 Since we have K ( x , x )=  X  ( x ) 2 , it is easy to see that: Moreover, we have K 0 ( x , x ) = 1 for all objects in X . This means that the objects in the feature space are projected on an unit hypersphere. In addition, we have the following geometrical i nterpretation from the feature space representation viewpoint: with  X  (  X  ( x ) , X  ( y )) being the angular measure be tween the vectors in the feature space. In order to simplify the notations, we will denote  X  for  X  ( x , y )andcos  X  for cos(  X  (  X  ( x ) , X  ( y ))), thereafter.
 2.2 Kernel Normalization of Order t The main purpose of this paper is to introduce a new family of kernel nor-malization approaches that generalizes the cosine normalization. Our approach amounts to integrating another geometrical based measure that allows us to re-fine the cosine measure K 0 . This additional feature is related to the difference between the norms of the two vectors in the feature space.

These normalization procedures invo lve generalized mean (also known as power mean) operators which generalize t he classical arithmetic mean. Given a sequence of p values { a i } p i =1 = { a 1 ,a 2 ,...,a p } , the generalized mean with exponent t is given by: Famous particular cases of (5) are given by t =  X  1, t  X  0and t =1whichare respectively the harmonic, geo metric and arithmetic means.
 Definition 2 (Kernels normalization of order t&gt; 0 ). Given a kernel func-tion K , the normalized kernel of order t&gt; 0 for any two objects x and y of X , is denoted K t ( x , y ) andisdefinedasfollows: Similarly to the cosine normalization, K t ( x , x ) = 1 for all x  X  X  .Asaresult, those normalization methods also amount to projecting the objects from the feature space to an unit hypersphere. However, this family of normalized ker-nels goes beyond the cosine measure since it extends the latter which actually corresponds to the limit case t  X  0.
 In order to better interpret such measures, let us equivalently formulate K t ( x , y ) with respect to the follo wing norm ratio measures,  X  ( x ) We can easily show that: This formulation expresses K t ( x , y ) according to geometrical based measures. However, let us introduce the following notation as well:  X  (  X  ( x ) , X  ( y )) lies within [1 , +  X  [ and is related to the difference between the norm measures of  X  ( x )and  X  ( y ).  X  (  X  ( x ) , X  ( y )) = 1 means that  X  ( x ) =  X  ( y ) and the greater the difference between the norms X  value, the higher  X  (  X  ( x ) , X  ( y )). Similarly to the angular measure, we will denote  X  for  X  (  X  ( x ) , X  ( y )) in order to simplify the notations. Using  X  , we have the different formulations below: The latter relation expresses K t ( x , y ) as a multiplication between two factors. On the one hand, we have the cosine index cos  X  and on the other hand, we have the following term which is only dependent on  X  ,
Following (9), we observe that  X   X  :cos  X   X  [  X  1 , 1] and  X  t&gt; 0 ,  X   X   X  1: * [  X  1 , 1].

In what follows, we detail the roles the geometrical parameters  X  and  X  play with respect to the introduced normalized kernels of order t . First, since  X  (  X  ( x ) , X  ( y ))  X  X  2 :  X   X  1; K t (  X ,  X  ) is clearly a monotonically increasing func-tion with respect to cos  X  1 . Then, using (9), we can better underline the effect of the norm ratio  X  on K t (  X ,  X  ). Indeed, by computing the first derivative with respect to this parameter, we obtain: With the following conditions,  X   X  1, t&gt; 0, one can verify that in the numer-ator of the second term of (10), the first factor is positive but the second one, * the same as  X  cos  X  . Therefore, for t&gt; 0, K t (  X ,  X  ) is monotonically decreasing with respect to  X  providing that cos  X &gt; 0whereas, K t (  X ,  X  ) is monotonically increasing with respect to  X  as long as cos  X &lt; 0(seeFig.1).

Intuitively, when t&gt; 0, the norm ratio measure aims at refining the cosine measure considering that the latter is less and less  X  X eliable X  for measuring prox-imity, as the difference between the vect ors X  norms becomes larger and larger. Thereby, regardless the sign of the cosine index, the greater  X  , the closer to 0 K t (  X ,  X  ). More formally, we have:  X  t&gt; 0 ,  X   X  , lim  X   X  +  X  K t (  X ,  X  )=0.
Notice that for t&lt; 0, we observe the opposite effect since the sign of the derivative  X  X  t  X  X  is the same as cos  X  . Thus, in that case, when cos  X &gt; 0for example, K t (  X ,  X  ) is monotonically increasing with respect to  X  .Thisisnot a suitable behavior for a similarity measure, that X  X  the reason why we define K t (  X ,  X  )for t&gt; 0only.

In more general terms, the sign and the value of t respectively express the nature and the degree of the influence of  X  on K t (  X ,  X  ). First, when t is negative, it defines a coefficient which is not appropriate for measuring similarity. On the contrary, when t is positive it allows one to refine the cosine measure in an appropriate way. Second, assuming that t&gt; 0, when the latter increases, it makes  X  have a more and more important impact on K t (  X ,  X  ). In that context, it is worthwhile to mention the two following limit cases: when t  X  0weobtain the cosine index which is independent of  X  ,whereaswhen t  X  +  X  we have the following index 2 : To illustrate these points, we plotted in Fig. 1, the graphs corresponding to K t (  X ,  X  ) for different values of t namely the limit when t  X  0, t =1, t = 10, t = 100 and the limit when t  X  +  X  . In the left-hand side graph, we fixed cos  X  =  X  1 and in the right-hand side graph, cos  X  is set to 1. While the cosine measure is fixed,  X  varies from 1 to 2 (the horizontal axis). The goal of these graphs is to represent the effect s of the norm ratio parameter  X  on the normalized kernel value (the vertical axis) for different values of t and depending on the sign of the cosine measure. F or example, when cos  X  =1and  X  = 2, we can observe that, in comparison with the case t  X  0 for which the normalized kernel gives 1, the value drops to 0 . 85 when t = 1 and it drops further to 0 . 5when t  X  +  X  . In this section, we want to better charact erize the family of normalized kernels that we have introduced. To this end, we give some relevant properties that they respect. First, we show that K t ( x , y )with t&gt; 0 satisfies the basic axioms of geometrical based similari ty indices. Second, we show that the normalized kernels of order t&gt; 0 are kernels. This result is the main theoretical contribution of this paper. 3.1 Basic Properties of K t We start by giving some basic properties of K t ( x , y ) with respect to the general definition of similarity indices defined in a metric space (see for example [6]).  X  ( x , y )  X  X  2 ,wehave: 1  X  t&gt; 0: | K t ( x , y ) | X  1 2  X  t&gt; 0: K t ( x , x )=1 3  X  t&gt; 0: K t ( x , y )= K t ( y , x ) 4  X  t&gt; 0: x = y  X  K t ( x , y )=1 According to property 1, K t ( x , y ) is bounded by  X  1 and 1 which is a common axiom required for a similarity index. Notice that unnormalized kernels do not respect this axiom in general.

Properties 2 and 3 respectively state that the normalized kernel of order t respects the maximal self-similarity axiom 3 and the symmetric axiom.

According to property 4, the situation K t ( x , y ) = 1 corresponds to the case where the vectors are strict ly identical in the feature space. Indeed, considering (9), we can see that the normalized kernel value is 1 if and only if cos  X  =1and  X  =1whichisthesameas  X  ( x )=  X  ( y ). Note that property 4 is not true for the limit case t  X  0forwhichweonlyhave  X  ( x )=  X  ( y )  X  K 0 ( x , y ) = 1. Since for this case the norm ratio plays no role, it is only sufficient for the vectors to be positively colinear to obtain the maximal similarity value 1. Once again, this shows that the normalized kernels of order t&gt; 0 are similarity measures that are more discriminative than the cosine measure. It is also worth mentioning in that context, the two other particular cases : both vectors are completely opposite to each other when we observe K t ( x , y )=  X  1(cos  X  =  X  1and  X  =1)andtheyare geometrically orthogonal when K t ( x , y )=0(cos  X  =0).

In what follows, we focus on the differen t relationships between normalized kernels of two distinct orders t and t . 5  X  t  X  t &gt; 0:sign( K t ( x , y )) = sign( K t ( x , y )) 6  X  t  X  t &gt; 0: 7  X  t  X  t &gt; 0: | K t ( x , y ) | X | K t ( x , y ) | Property 5 states that the sign of the similarity measure is independent of t . More precisely, the sign is only dependent on the angular measure. This is a consequence of (9) since M t (  X ,  X   X  1 ) is strictly positive.

Properties 6 and 7 must be put into relation with the comments we made in section 2 and Fig. 1. Accordingly, these properties formally claim that as t grows, K t ( x , y ) makes the cosine measure less an d less  X  X eliable X . We previously mentioned that, all other things being equal, the greater the difference between the vectors X  norms in the feature space, the closer to 0 the normalized kernel value. These properties express the fact t hat this convergence is faster and faster as t grows.

These aforementioned properties are di rect consequences of the following re-lations between generalized means,  X  0 &lt;t  X  t&lt;  X  : To complete the analysis of the basic properties that K t ( x , y ) respects with regards to the general axioms required fo r a similarity index, we need to better characterize the metric properties of the latter. We address this issue in the following subsection. 3.2 Metric Properties of K t In this paragraph we denote K t the similarity matrix of objects in X .
 Theorem 2. The similarity matrix K t with t&gt; 0 and general term given by (6) is positive semi-definite. In other words, any normalized kernel K t with t&gt; 0 is a positive semi-definite kernel.
 Proof (Proof of Theorem 2).
 The proof of this result is based on Gershgorin circle theorem. Let A be a ( n  X  n ) complex matrix with general term A ii .Foreachrow i =1 ,...,n , its associated Gershgorin disk denoted D i is defined in the complex plane as follows: Given this definition, Gershgorin circle th eorem (see for example [7]) states that all eigenvalues of A lies within elements of A are small enough then the eigenvalues are not  X  X ar X  from the diagonal elements. In other words, the lower the R i quantities, the closer to the diagonal elements the eigenvalues.

Now, let us consider normali zed kernel matrices of order t&gt; 0 of objects of { x i ; i =1 ,...,n t  X  0. We know that K 0 is the cosine similarity matrix. As a consequence, K 0 is positive semi-definite and its eigenvalues are all non negative. Next, let us denote R we have:  X   X  t&gt; 0and  X  i =1 ,...,n : K t ii =1,  X   X  t&gt;t &gt; 0and  X  i =1 ,...,n : R t i  X  R t i .
 Therefore, when t grows, (6) defines a continuous and differentiable operator for which the absolute value of off-diagonal elements of K t , and consequently the quantities R t i ; i =1 ,...,n , are lower and lower while the diagonal entries of K t remain equal to 1. Thus, applying Gers hgorin theorem, we can see that when t increases, the spectrum of K t is closer and closer to the vector of ones with dimension n . As a consequence, since the eigenvalues of K 0 are non negative then so are the eigenvalues of K t with t&gt;t &gt; 0 as the latter are closer to 1 than the former. Finally, for t&gt; 0, K t is symmetric and has non negative eigenvalues. These properties are equivalent to the conditions mentioned in Definition 1 thus, for t&gt; 0, we can conclude that K t are positive semi-definite kernels. * + Finally, a corollary of Theorem 2 [8], is that the related distance D t ( x , y )= 2(1  X  K t ( x , y )) respects the triangle inequality axiom,  X  ( x , y , z )  X  X  3 : 8  X  t&gt; 0: D t ( x , y )  X  D t ( x , z )+ D t ( z , y ) In order to illustrate the potential benefits of our proposal, we tested different normalized kernels of order t&gt; 0 in the context of clustering tasks. The clustering algorithm we used is based on kernel Principal Component Analysis (kernel PCA) and the k -means algorithm. Our experiments concern 5 real-world datasets from the UCI ML repository [5]. Our purpose is to show that the normalized kernels that we have introduced, can better capture the proximity relationships between objects compared with other st ate-of-the-art no rmalized kernels. 4.1 Kernel PCA Based k -means Clustering Our clustering approach is a spectral clustering like algorithm (see for example [9]). First, from a kernel matrix K t , we proceed to its eigen-decomposition in order to extract from the implicit high dimensional feature space F , an explicit and proper low dimensional representation of the data. Then, we apply a k -means algorithm in that reduced space in order to find a partition of the objects.
Principal Component Analysis (PCA) is a powerful and widely used statistical technique for dimension reduction and features extraction. This technique was extended to kernels in [3]. Formally, let denote  X  1  X   X  2  X  ...  X   X  k  X  1 the leading k  X  1 eigenvalues of K t and v low dimensional space extracted from K t that we used as data representation for the clustering step is spanned according to: (  X 
When applying dimension reduction techniques prior to a clustering algo-rithm, an important issue is the number of dimensions that one has to retain. In this paper, since we aim at using the k -means algorithm as the clustering method, we follow the work presented in [ 4] that concerns the relationship be-tween k -means and PCA. Accordingly, if k is the number of clusters to find then we retain the k  X  1 leading eigenvectors.
 With regards to related works, we can cite the following papers that use Kernel PCA based clustering in the contexts of i mage and text analysis respectively, [10,11].
 4.2 Experiments Settings The datasets that we used in our experiments are the following ones [5]:  X  Iris (150 objects, 4 features in the input space, 3 clusters)  X  Ecoli (336 objects, 7 features in the input space, 8 clusters)  X  Pima Indian Diabetes (768 objects, 8 f eatures in the input space, 2 clusters)  X  Yeast (1484 objects, 8 features in the input space, 8 clusters)  X  Image Segmentation (2310 objects, 18 features in the input space, 7 clusters) For each data set, we first normalized the data in the input space by centering and standardizing the features. Next, we applied different kernels K namely, the linear kernel K l ( x , y )= x , y which is simply the dot product in the input space, and the polynomial kernel K p ( x , y )=( x , y +1) 2 . For each type of kernels, we computed diffe rent normalized kernels K t by varying the value of t : t  X  0, t =1, t = 10, and the limit t  X  +  X  . To each of those similarity matrices, we applied the kernel PCA clustering met hod described prev iously. Since the k -means algorithm is sensitive with respect to the initialization, for all cases we launched the algorithm 5 times with different random seeds and took the mean average value of the assessment measures.

Since we deal with normalized kernels t hat amount to projecting the objects on an unit hypersphere, we also tested the kernel PCA based k -means clustering with the Gaussian RBF kernel which presents the same property. This case is our first baseline. However, when using such a kernel, one has to tune a parameter  X  . In this paper, to get rid of this problem, we applied the approach proposed in [12] which suggests to use the following affinity measure: where  X  x is set to the value of the distance between x and its 7th nearest neighbor.

Besides, as our purpose is to show that taking into account the mapped vec-tors X  norm ratio in addition to the cosine measure can be beneficial, we took the case t  X  0, which simply corresponds to the cosine index, as a baseline as well. The assessment measure of the clustering outputs we used is the Normalized Mutual Information (NMI) introduced in [13]. Let denote U the partition found by the kernel PCA clustering and V the ground-truth partition. This evaluation measure is denoted NMI( U, V )andisgivenby: where N is the contingency table between U and V and n the total number of objects. NMI( U, V ) lies in [0 , 1] and the higher the measure, the better the clustering output U . 4.3 Experiments Results In Fig. 2, we report the results we obtained for each dataset. On the left-hand side we give the results related to the linear kernel whereas on the right-hand side, the NMI values correspond to the polynomial kernel. In both graphs, the results provided by the Gaussian RBF kernel are shown (1st bar). Compared to this baseline we can see that the cosine measure and the normalized kernels all perform better on the datasets used in these experiments.

In comparison with the other baseline K 0 , we can observe that in most cases there are normalized kernels of order t&gt; 0 which can lead to better NMI val-ues. When using the linear kernel, this is true for all K t except for the Image Segmentation dataset. Furthermore, for the Iris, Ecoli and Yeast datasets, as t grows the performances are consistently better. This shows that taking into account the vectors X  norm ratio in the feature space in order to refine the cosine measure, is beneficial.

In the case of polynomial kernel, not all normalization techniques are inter-esting since many normalized kernels d o not outperform the cosine measure. However, when using this kernel, it seems that taking K 1 p as a normalization method is a good choice. Particularly, K 1 p leads to the best performances for the Iris and the Pima Indian Diabetes da tasets. Besides, concerning the Image Segmentation dataset, we can see that unlike the linear kernel, the normalized polynomial kernels can outperform the c osine index since the best result is ob-tained with K 10 p . We have introduced a new family of normalization methods for kernels which extend the cosine normalization. We have detailed the different properties of such methods and the resulting proximity measures with respect to the basic axioms of geometrical based similarity indices. Accordingly, we have shown that normalized kernels are  X  X roper X  similarity indices that amount to projecting the data on an unit hypersphere. We have, in addition, proved that these normalized kernels are also kernels. From a practical standpoint, we have also validated the utility of normalized kernels in the context of clustering tasks using several real-world datasets. However, one remaining issue is the choice of the order t when normalizing a kernel. We have shown from a theoretical and a practical point of view that the norm ratio measure can ma ke the normalized kernel more efficient but still, the weight one should give to this parameter in comparison with the angular measure is not straightforward to set. In our future work we intend to further investigate this problem.
 Support Vector Machines (SVMs), and Ke rnel Methods in general, became popular due to their very good predictive performance [1]. As most of the real-world data can not be easily represented in an attribute-value format many kernels for various kinds of structured data have been proposed [2]. In particular, graphs are a widely used tool for modelling structured data in data mining / machine learning and many kernels over these complex structures have been proposed so far. These kernels have been mainly applied for predicting the activity of chemical molecules represented by sparse, undi-rected, labelled and two-dimensional graphs.

However, due to the rich expressiveness of graphs it has been proved that kernels over arbitrarily structured graphs, taking their full structure into account, can be neither com-puted [3] nor even approximated efficiently [4]. The most popular approach to tackle the above problem is based on decompositions of graphs into sub-graphs of specific types which are compared via sub-kernels. The sub-graph types mainly considered are walks [3, 5 X 8]. However, other researchers have experimented with shortest paths [9], sub-trees [4, 10], cyclic and tree patterns [11] and more general sub-graphs [12, 13]. A common feature in all the above graph kernels is that the Cross Product (CP) Kernel is used between the corresponding multi-se ts of decompositions. More precisely, for particular decompositions G t 1 and G t 2 of the graphs G 1 and G 2 into sub-structures of type t the above kernels can be written as K ( G 1 ,G 2 )= ( g where k is a kernel over a specific type t of graphs, and the summation over the elements of the multisets takes into account their multiplicity.

One problem with the above kernel is that all the possible sub-graphs of a given type are matched by means of a sub-kernel. This m ight adversely affect the generalization of a large margin classifier since, due to the combinatorial growth of the number of dis-tinct sub-graphs, most of the features in the feature space will be poorly correlated with the target variable [12, 14]. Possible solutions to this problem include down-weighting the contribution of larger sub-graphs [3, 15], using prior knowledge to guide the se-lection of relevant parts [16], or considering contextual information for limited-size sub-graphs [12]. Yet another solution was proposed in [10] in which only specific el-ements of the corresponding multi-sets are matched in such a manner that the sum of similarities of the matched elements is max imum. The underlying idea in this kernel is that the actual matching will fo cus on the most important structural elements, neglect-ing the sub-structures which are likely to introduce noise to the representation. The idea of using specific pairs of points in a set kernel is promising, however, it is easy to show that this kernel is not positive semi-definite (PSD) in general [17, 18].
More importantly, to the best of our knowledge, all existing graph kernels are cur-rently limited to a single type of decomposition which is then, most often, used in the context of the CP kernel. However, in general it is difficult to specify in advance the appropriate type of sub-structures for a given problem. Although, it is in principle pos-sible to simultaneously exploit kernels defined over different representations, this is usually not done because there is a trade-off bet ween the expressivity gained by enlarg-ing the kernel-induced feature space and the increased noise to signal ratio (introduced by irrelevant features). A common intuition is that by decomposing into more complex sub-graphs the expressiveness, and consequently the performance, of resulting kernels increases. This is, however, in contrast with some experimental evidence [12] which suggest that decompositions into rather simp le sub-structures perform remarkably well with respect to more complex decompositions on a number of different datasets. A simplified solution to the problem of repres entation selection is t o select the decompo-sition by cross-validation. However, this approach is problematic since it requires the use of extra data and only one representation can be selected which limits the expres-siveness of the resulting method. We can also directly learn to combine graph kernels using multi-kernel learning methods, [19]. Ne vertheless, the problem with learning ker-nel combinations is that the combined elements should, obviously, be valid kernels. However, as we saw previously this type of kernels are based on the CP kernel that re-quires the complete matching of the components, raising the problems that we described above.

To tackle the above problems we propose a class of adaptive graph kernels built on proximity spaces [20]. The proximity spaces are induced by learned weighted combi-nations of a given set distance measure on a number of decompositions of different sub-graph types, and constructed on the basis of a representation set, typically the full 1 training set. The weighted combinations are learned from the data, exploiting the NCA method [21] developed for learning Mahalanobis metrics for vectors. By learning the weights of the different decompositions we hope to obtain a combination of graph rep-resentations that is expressive enough for the task at hand and does not overfit. Using the set distance measures on the graph decompositions allows for flexible ways of mapping the elements of these decompositions, namely it is not necessary to use all the elements (sub-graphs) in the mapping. We originally explored the use of proximity spaces to de-fine a graph kernel in [22]; however, there we were limited to a fixed decomposition and did not consider learning the combinati ons of the different types of substructures.
In this paper we will experiment with, and learn combinations of, two types of de-compositions: walks and unordered trees of various lengths and heights, respectively. The former were shown to be very effective in chemical domains and achieved state-off-the-art results [3, 5 X 8]. Decomposition kernels based on trees were first proposed in [4], however, experimental evaluation was not performed. Kernels based on trees were examined in [10]. Walks and trees can be easily compared by a graded similarity (e.g. standard Euclidean metric for walks). Nevertheless, most of the kernels on graphs use the Kronecker Delta Kernel (i.e. k  X  ( x, y )=1 if x = y, k  X  ( x, y )=0 otherswise) on these kernels is reduced [7]. A graded similarity on walks was considered in the graph kernel presented in [7], however, it suffers from high computational complexity since it requires taking powers of the adjacency matrix of the direct product graph, leading to huge runtime and memory requirements [9] . Graded similarities on trees were also used in [10]. Finally, we note that our framework is not limited only to walks and trees and can be applied on decompositions of any type.

This paper is organized as follows. In S ect. 2 we define all the necessary notions of graphs. In Sect. 3 we define the adaptive kernels on graphs in the proximity space in-duced by the set distance measures. Experiment al results are reported in Sect. 4. Finally, Sect. 5 concludes the work. and a finite set of edges E ,where E = {{ v i ,v j } : v i ,v j  X  X } ;for directed graphs we set E = { ( v i ,v j ): v i ,v j  X  X } . We shall also use the following notation for edges: e tex labels L V and edge labels L E together with functions l V : V X  X  V and l E : E X  X  E that assign labels to vertices and edges, respectively. We will use lab ( x ) to denote, in be clear from the type of the x argument, i.e. vertex or edge. In this work we assume that the labels are vectors, i.e. L V  X  IR p and L E  X  IR n for some values of m and n .By dim ( lab ( x )) we will denote the dimensionality of the label vector lab ( x ) which obviously will be the dimensionality of either L V or L E , depending again on x . A walk , W ,inagraph G is a sequence of vertices and edges, W =[ v 1 ,e 12 ,v 2 ,...,e s,s +1 ,v s +1 ] , such that v j  X  X  for 1  X  j  X  s +1 and e ij  X  X  for 1  X  i  X  s .Walks is the number of vertices and edges in W . We denote the set of all walks of length l in a graph G by W ( G ) l .A tree T with vertices V and edges E of a graph G is a sub-graph of G which is connected and acyclic. The root node of a tree is denoted as root ( T ) .In this work we will only consider directed trees where the order is given from the root node to the leafs. The height h of a tree T is the length of the longest walk from the root node to any of the leaf nodes (similar to walks we allow that the trees have edges as leafs). We denote the set of all directed trees of height h in a graph G by T ( G ) h .We define the neighborhood of a node, v , in a tree as  X  ( v )= { e : e =( v, u )  X  X } ,and the neighborhood of an edge, e ,as  X  ( e )= { u : e =( v, u )  X  X } , note that in fact the neighborhood of an edge is a one element set, containing a single node. We denote the decomposition of a graph, G , into the multi-set of sub-graphs of type t by G t . We focus on decompositions into walks of various lengths l ,i.e. G t = W ( G ) l , and (directed) trees of various heights h ,i.e. G t = T ( G ) h . More generally, a graph G can be represented by a tuple of m different decompositions as G =( G t 1 ,..., G t m ) T .
In this section we will define a class of adaptive kernels for labelled graphs. The construction of these kernels is based on set distance measures which will be used to compute the distances on the components of G . Since it is difficult to select a priori the appropriate types of sub-structures, i.e. components of G , for a given problem, we pro-pose a method which learns a weighted (quadratic) combination of a fixed set distance on the different components of G . Finally, we use the learned weighted combination of the set distance to induce a proximity space on which we define the final kernel. Distances on Sets. The central idea in the set distan ce measures that we consider is the definition of a mapping of the elements of one set to the elements of the other such that the final distance is determined on the basis of specific pairs of elements from the two sets. More precisely, consider two nonempty and finite sets A = { a } X  X  and B = { b } X  X  .Let d (  X  ,  X  ) be a distance measure defined on X . The set distance mea-of pairwise distances over specific pairs which are defined by F  X  A  X  B ; different F correspond to different set distance measures. Within this framework we can define Average Linkage ( d AL ) , Single Linkage ( d SL ) , Complete Linkage ( d CL ) , Sum of Min-( d tailed description of these distance measures can be found in [23].
 Distances Between Sets of Decompositions. We will now show how to use the above set distance measures to compute distances between sets of decompositions of graphs to sub-graphs of specific types. As already mentioned, we will focus on decompositions into walks and (directed) trees of various lengths and heights. In the latter, we consider unordered trees where the order among the siblings at a given height is not important. Both types of decompositions are obtained from a depth first exploration emanating from each node in a graph. We note that the walks and trees we consider in this work are non-standard in the sense that a walk can end in an edge and a tree can have edges as leafs.
In our decompositions we do not allow repetitions of a node within a walk or a tree, i.e. we do not allow cycles, in order to avoid the problem of tottering [6]. We exclude from the graph decomposition walks of the form W =[ v 1 ,e 12 ,v 2 ,..., e s,s +1 , v s +1 ] with v i = v i +2 for some i since these are likely to introduce redundancy. In the case of trees we do not allow such walks in the trees X  descriptions. Even though tottering-free graph representations do not seem to bring an improvement to the predictive perfor-mance [6, 10], they have the advantage of inducing decompositions smaller in size. This has a direct influence on the computational complexity of the set distance mea-sures applied over this representation.

The main difficulty in applying the set distance measures from the previous section is to define the distance, d , between the elements of the sets, i.e. walks or trees. To define a distance measure over walks we exploit the (normalized) Euclidean metric. More precisely, the distance of two walks W i and W j of equal length l is defined as the Euclidean metric between the lab els of the elements of the walks d 2 W ( W i ,W j )= is the Euclidean metric between the labels of corresponding walks. Obviously, W i [ k ] and W j [ k ] are of the same type and they will be either edges or vertices. The N in the denominator is a normalization factor and corresponds to the sum of the dimen-
Lets now define a distance, d T ( T i ,T j ) , between two trees, T i ,T j ; we should note here that unlike walks, trees do not have to be of the same height. Let x, y ,betwo elements of the two trees found at the same height h . These elements will be either two vertices, u, v , or two edges e i ,e j . Then the (squared) distance between x and y , d ( x, y ) , is recursively defined as: labels of the x, y ;  X  ( x ) is the neighbourhood function, defined in Sect. 2, that returns either: the set of edges to which a vertex connects to as a starting vertex, if x is of type vertex, or the vertex to which an edge arrives if x is of type edge; finally d M is the matching set distance measure between the sets of elements that are found in the neighborhoods of x and y .The N in the denominator is also a normalization factor that corresponds to the dimensionality of the label vectors of x and y plus one to account for the set distance dimension, i.e. N = dim ( lab ( x )) + 1 = dim ( lab ( y )) + 1 .The letting outside, and penalizing for, sub-stru ctures that cannot be matched. In a discrete case scenario it is equivalent to a frequency based distance, i.e. for each of the discrete tance will be the sum of differences of these frequencies. Note here that d t ( x, y ) is a recursive distance requiring the computation of set distances between the elements of the trees that are associated to x, y ,atthe h +1 height. The final distance between two trees, T i , T j ,isgivenby d T ( T i ,T j )= d t ( root ( T i ) , root ( T j )) . Combining Different Decompositions. Here we show how to combine different de-compositions of labelled graphs in order to produce a final weighted graph distance. Recall that two graphs G 1 and G 2 can be represented by m different decompositions tance measure d set and the given m decompositions we define the vector d ( G 1 ,G 2 )= ( d ent decompositions. Then the weighted combination of these decompositions is defined as where A is a m  X  m matrix. It should be noted that for any matrix A the matrix A T A is PSD, and hence d A is a pseudo-metric. Here we propose a method for learning the ma-trix A of equation (1) directly from the data by casting the problem as an optimization task.

To learn the matrix A we use the Neighborhood Component Analysis (NCA) method This method attempts to directly optimize a continuous version of the leave-one-out bution is introduced which for each example G i selects another example G j as its selects. The probability p A ( j | i ) isbasedonthesoftmaxofthe d 2 A distance given by p
A ( j | i )= e to be maximized, as described in [21], is F A = i log( j  X  C
It is clear that for full matrices A the number of parameters to estimate is m 2 .This could be problematic in cases where m is large with respect to the number of instances in the training database. One possible solution to overcome this problem is to add a soft constraint to the above objective function which results in the following regularized objective function F reg A = i log( j  X  C Frobenious norm of matrix A and  X &gt; 0 is a regularization parameter. The other solu-tion is to restrict matrix A to be diagonal resulting in a we ighted combination of dis-tances for different decompositions (we denote NCA where optimization is performed over a diagonal matrix by NCA diag ). The main advantage of the NCA algorithm is that it is non-parametric, making no assumptions about the shape of the class conditional distributions [21]. Its main problem is that the objective function is not convex, hence some care should be taken to avoid local optima.
 Adaptive Matching Based Kernels. Here we exploit the adaptive combinations of decompositions presented in Sect. 3, and th e notion of proximity spaces, to define a class of adaptive matching based kernels over labelled graphs. The proximity space is defined by an instantiation of d A from equation (1) and a representation set (i.e. set of prototypes) of learning instances. More precisely, consider a graph G which is decom-posed in m different ways as G =( G t 1 ,..., G t m ) T . For a given representation set S = {
G 1 ,...,G p } (we assume that each graph is uniquely identified by its index) and a dis-tance measure d A we define a mapping d A ( G, S )=( d A ( G, G 1 ) ,...,d A ( G, G p )) T . Since distance measures d A are non-negative, all the data examples are projected as nel in the proximity space between graphs G 1 and G 2 is defined as k d A ( G 1 ,G 2 )= space; k can be any standard kernel on vectorial data.

We mention that the complexity of computing the adaptive matching kernel between two graphs (if the weights of different decompositions are known) can be shown to be be upper bounded by a small constant (usually 4), and m = l + h is the number of where n is the number of graphs in the training set. We will experiment with two graph classification problems: Mutagenesis and Carcino-genicity. The application task in the Mutagenesis dataset is the prediction of mutagenic-ity of a set of 188 aromatic and hetero aro matic nitro-compounds which constitute the  X  X egression friendly X  version of this dataset. The other classification problem comes from the Predictive Toxicology Challenge and is defined over carcinogenicity prop-erties of chemical compounds. This dataset lists the bioassays of 417 chemical com-pounds for four type of rodents: male rats (MR), male mice (MM), female rats (FR) and female mice (FM) which give rise to four independent classification problems. We transformed the original dataset (with 8 classes) into a binary problem by ignoring EE (equivocal evidence), E (equivocal) and IS (inadequate study) classes, grouping SE (some evidence), CE (clear evidence) and P (positive) in the positive class and N (nega-tive) and NE (no evidence) in the negative one. After this transformation the MR, MM, FR and FM datasets had 344, 336, 351 and 349 instances, respectively. References to these datasets can be found in [22].

In the experiments we want to examine a number of issues related to our graph ker-nels. More precisely, first, for different set distance measures we will explore the effect of the length of walks (and the height of trees) on the performance of k d set ,i.e.wefixthe h, h =2 ,..., 5 ) and we use only the corresponding decomposition to construct the fi-nal kernel; we do not combine decompositions. We will compare the performance of the above simplified kernels with k d A where we combine all the different m =11+4 decompositions. As the kernel k on the proximity space we will be always using the linear kernel in order to make a fair comparison between the algorithms and to avoid the situation where an implicit mapping given by a nonlinear kernel will influence the results. Second, we will examine how the performance of the kernel k d A compares with the following two set kernels based on averag ing: (i) the direct sum kernel [1] based of the cross product kernels applied on the different m =11+4 decompositions with the linear kernel (in case of walks) and the tree kernel 3 (for trees) as kernels on the also over the combination of the different m =11+4 decompositions, where d AL set distance measure is applied over the decompositions. Finally, we will compare our graph kernels with other graph kernels form the literature.

We use the SVM algorithm where the parameter C is optimized in an inner 10-fold stated otherwise, we use the regularized version of the NCA algorithm over full matri-ces A , where the regularization parameter  X  is internally 10-fold cross-validated over  X  = { 0 , 0 . 1 , 1 , 10 } . In some experiments we also use the version of NCA, denoted as NCA diag ,where A is limited to a diagonal matrix. Note that in the experiments we have 11 different instantiations of the k d A kernel, each one corresponding to one of the 11 set distance measures. In all the experiments accuracy is estimated us ing stratified 10-fold cross-validation and controlled for the statistical significance of observed differences using McNemar X  X  test, with significance level of 0.05. 4.1 Results and Analysis Performance of Individual vs. Combined Decompositions. We first examine the in-fluence of the length of the walks and heights of the trees to the predictive performance of SVM. The results for the Mutagenesis dataset are presented in Fig. 1. From the re-sults it is clear that in Mutagenesis the optimal decomposition depends on the actual set distance measure. For example, for d SMD the highest predictive accuracy is obtained for walks of length 6, whereas for d S the best decomposition is into walks of length 10. Additionally, with the exception of d SL , decompositions in walks in general outperform decompositions into trees. For the other examined datasets the optimal decompositions are different. Thus we have no way to know a-priori which type of decomposition is appropriate. By combining them using NCA (results also given in Fig. 1) we get a clas-sification performance that is in most cases as good as that of the single decompositions. Indeed, in Mutagenesis the p erformance of NCA was significantly better in 68 cases, in 92 cases the differences were not significan tly meaningful, and in 5 cases it was sig-nificantly worse. 4 In FM the corresponding values are 7, 158 and 0; in FR: 5, 147 and 22; in MM: 12, 153 and 0; in MR: 81, 83 and 1. We should stress here that it is not fair to compare NCA with the results of the best decomposition, since this estimate is optimistically biased since we need to have the results of the cross-validation to deter-mine which is the best decomposition. In a fair comparison NCA should be compared to a model selection strategy in which the best decomposition is selected using an in-ner cross-validation. However an inner cross-validation-based model selection strategy does not make full use of the data, thus its estimates might not be reliable, and it is computationally expensive.

The results of the optimization process that learns the optimal combination of de-compositions can be graphically presented providing insight to the relative importance of different decompositions. In Fig. 2 we give an example of such a visualization for the FM dataset and the d SMD set distance measure. In the left graph of that figure the optimization was performed for diagonal matrices using NCA diag . The different ele-ments in x-axis are the elements of the diagonal of the matrix A which correspond to a decomposition into walks and trees whereas the y-axis represents the weights, normal-ized by a Frobenious norm of A , returned by the optimization method. What we see from the graph is that in FM the highest weights are assigned to walks of lengths longer than 6 and all tress, independent of their heights. The right graph of Fig. 2 provides the visualization of the optimization process for NCA, where now the matrix A is a full matrix (  X  =0 ). One surprising observation is that for NCA the decompositions into trees and long walks are assigned low weights. At the same time combinations of trees with long walks and combinations of shorter walks are of high importance. Performance of Matchings Strategies. The next dimension of comparison is the rel-ative performance of the instantiations of the k d A kernel for different set distances and the following two set kernels based on averag ing: (i) the direct sum kernel [1] based on the cross product kernels applied on the different 15 decompositions ( k  X  , CP ), and (ii) the linear kernel in the proximity space induced by d A , also over the combination of the different 15 decompositions, with the d AL set distance measure.

The results are presented in Table 1. The relative performance of kernels based on specific pairs of elements and kernels based on averaging depends on the actual appli-cation. For Mutagenesis and MR there is an advantage of the kernels based on specific pairs of elements, while for the remaining datasets the performances are similar. Over-all, the choice of the appropriate way of matching the elements of two sets depends on the application and ideally should be guided by domain knowledge, if such exists. Nev-ertheless, the relative perfo rmance of the different kernels provides valuable informa-tion about the type of problem we are facing. For example, by examining Mutagenesis and MR we see that averaging performs poorly, indicating that the local structure of the molecules is important, while in the remaining datasets both global and local structures seem to be equally informative.
 Comparison with Other Graph Kernels. In Table 2 we provide the best results re-ported in the literature on the same benchmark datasets. 5 Additionally, we report the performance of kernels corresponding to d SMD and d T , both achieving stable good results across the different datasets. The values in the  X  X ur best kernel X  row correspond to the best kernel for each dataset, selected over all the examined mappings. It is obvi-experimentation with various set distance measures. However, the same could be ar-multiple results where available and we reported on the best. The corresponding results show that if the mapping is carefully sel ected for a problem at hand, then the perfor-mance of the corresponding kernel is better than the performance of the existing graph kernels. At the same time kernels corresponding to d SMD and d T achieve stable results which are close to the state-of-the-art for the considered datasets. It has been argued in [7] that a  X  X ood X  kernel for graphs should fulfil at least the follow-ing requirements: (i) should be a good similarity measure for graphs, (ii) its computa-tional time should be possible i n polynomial time, (iii) should be positive semi-definite and (iv) should be applicable for various graphs. In this paper we proposed a class of adaptive kernels for graphs which are base d on walks and trees, that are computable in polynomial time, that are PSD, and are applicable to a wide range of graphs. A distinctive feature of our kernels is that they allow for (i) combinations of different types of decompositions and (ii) specific types of mappings between sub-parts. The effectiveness of the approach was demonstr ated on problems of activity prediction of chemical molecules. Finally, the ideas pr esented in this work are not limited to graphs and can be directly exploited to define kernels over not only special kinds of graphs like sequences and trees but also over instances described using first-(or higher-)order logic [24].

In the future work we would like to examine decompositions of graphs into sub-structures other than walks and trees (e.g. the cyclic patterns from [11]). Moreover, we will apply the methods developed in this work on larger datasets, e.g. the HIV dataset described in [12].
 Acknowledgments. This work was partially funded by the European Commission through EU projects DebugIT (FP7-217139) and e-LICO (FP7-231519). The support of the Swiss NSF (Grant 200021-122283/1) is also gratefully acknowledged.
