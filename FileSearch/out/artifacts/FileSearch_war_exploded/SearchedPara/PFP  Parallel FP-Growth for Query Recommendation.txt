 Frequent itemset mining (FIM) is a useful tool for discov-ering frequently co-occurrent items. Since its inception, a number of significant FIM algorithms have been developed to speed up mining performance. Unfortunately, when the dataset size is huge, both the memory use and computa-tional cost can still be prohibitively expensive. In this wo rk, we propose to parallelize the FP-Growth algorithm (we call our parallel algorithm PFP) on distributed machines. PFP partitions computation in such a way that each machine executes an independent group of mining tasks. Such parti-tioning eliminates computational dependencies between ma -chines, and thereby communication between them. Through empirical study on a large dataset of 802 , 939 Web pages and 1 , 021 , 107 tags, we demonstrate that PFP can achieve virtu-ally linear speedup. Besides scalability, the empirical st udy demonstrates that PFP to be promising for supporting query recommendation for search engines.
 H.3 [ Information Storage and Retrieval ]; H.4 [ Information Systems Applications ] Algorithms, Experimentation, Human Factors, Performance Parallel FP-Growth, Data Mining, Frequent Itemset Mining
In this paper, we attack two problems. First, we par-allelize frequent itemset mining (FIM) so as to deal with large-scale data-mining problems. Second, we apply our de-veloped parallel algorithm on Web data to support query recommendation (or related search ).

FIM is a useful tool for discovering frequently co-occurren t items. Existing FIM algorithms such as Apriori [9] and FP-Growth [6] can be resource intensive when a mined dataset is huge. Parallel algorithms were developed for reducing mem-ory use and computational cost on each machine. Early efforts (related work is presented in greater detail in Sec-tion 1.1) focused on speeding up the Apriori algorithm. Sinc e the FP-Growth algorithm has been shown to run much faster than the Apriori, it is logical to parallelize the FP-Growth algorithm to enjoy even faster speedup. Recent work in parallelizing FP-Growth [10, 8] suffers from high communi-cation cost, and hence constrains the percentage of compu-tation that can be parallelized. In this paper, we propose a MapReduce approach [4] of parallelizing FP-Growth al-gorithm (we call our proposed algorithm PFP), which in-telligently shards a large-scale mining task into indepen-dent computational tasks and maps them onto MapReduce jobs. PFP can achieve near-linear speedup with capability of restarting from computer failures.

The resource problem of large-scale FIM could be worked around in a classic market-basket setting by pruning out items of low support. This is because low-support itemsets are usually of little practical value, e.g., a merchandise w ith low support (of low consumer interest) cannot help drive up revenue. However, in the Web search setting, the huge number of low-support queries, or long-tail queries [2], ea ch must be maintained with high search quality. The impor-tance of low-support frequent itemsets in search applicati ons requires FIM to confront its resource bottlenecks head-on. In particular, this paper shows that a post-search recom-mendation tool called related search can benefit a great deal from our scalable FIM solution. Related search provides related queries to the user after an initial search has been completed. For instance, a query of  X  X pple X  may suggest  X  X range X ,  X  X Pod X  and  X  X Phone X  as alternate queries. Relate d search can also suggest related sites of a given site (see ex-ample in Section 3.2).
Some previous efforts [10] [7] parallelized the FP-Growth algorithm across multiple threads but with shared memory. However, to our problem of processing huge databases, these approaches do not address the bottleneck of huge memory requirement. key="": value
To distribute both data and computation across multiple computers, Pramudiono et al [8] designed a distributed vari -ant of the FP-growth algorithm, which runs over a cluster of computers. Some very recent work [5] [1] [3] proposed solutions to more detailed issues, including communicatio n cost, cache consciousness, memory &amp; I/O utilization, and data placement strategies. These approaches achieve good scalability on dozens to hundreds of computers using the MPI programming model.

However, to further improve the scalability to thousands or even more computers, we have to further reduce com-munication overheads between computers and support au-tomatic fault recovery. In particular, fault recovery beco mes a critical problem in a massive computing environment, be-cause the probability that none of the thousands of comput-ers crashes during execution of a task is close to zero. The demands of sustainable speedup and fault tolerance require highly constrained and efficient communication protocols. In this paper, we show that our proposed solution is able to address the issues of memory use, fault tolerance, in additi on to more effectively parallelizing computation.
In summary, the contributions of this paper are as follows: 1. We propose PFP, which shards a large-scale mining 2. With the scalability of our algorithm, we are able to
To make this paper self-contained, we first restate the problem of FIM. We then define parameters used in PF-Growth, and depict the algorithm. Starting in Section 2.2, we present our parallel FP-Growth algorithm, or PFP.
Let I = { a 1 , a 2 , . . . , a m } be a set of items , and a trans-action database DB is a set of subsets of I , denoted by said a transaction . The support of a pattern A  X  I , denoted by supp( A ), is the number of transactions containing A in DB . A is a frequent pattern if and only supp( A )  X   X  , where  X  is a predefined minimum support threshold. Given DB and  X  , the problem of finding the complete set of frequent patterns is called the frequent itemset mining problem.
FP-Growth works in a divide and conquer way. It requires two scans on the database. FP-Growth first computes a list of frequent items sorted by frequency in descending order (F-List) during its first database scan. In its second scan, the database is compressed into a FP-tree. Then FP-Growth starts to mine the FP-tree for each item whose support is larger than  X  by recursively building its conditional FP-tree. The algorithm performs mining recursively on FP-tree. The problem of finding frequent itemsets is converted to con-structing and searching trees recursively.

Figure 1 shows a simple example. The example DB has five transactions composed of lower-case alphabets. The firs t step that FP-Growth performs is to sort items in transac-tions with infrequent items removed. In this example, we set  X  = 3 and hence keep alphabets f, c, a, b, m, p . After this step, for example, T 1 (the first row in the figure) is pruned compresses these  X  X runed X  transactions into a prefix tree, which root is the most frequent item f . Each path on the tree represents a set of transactions that share the same prefix; each node corresponds to one item. Each level of the tree corresponds to one item, and an item list is formed to link all transactions that possess that item. The FP-tree is a Procedure : FPGrowth( DB ,  X  )
Define and clear F-List : F []; foreach T ransaction T i in DB do end Sort F [];
Define and clear the root of FP-tree : r ; foreach T ransaction T i in DB do end foreach item a i in I do end compressed representation of the transactions, and it also al-lows quick access to all transactions that share a given item . Once the tree has been constructed, the subsequent pattern mining can be performed. However, a compact representa-tion does not reduce the potential combinatorial number of candidate patterns, which is the bottleneck of FP-Growth. Algorithm 1 presents the pseudo code of FP-Growth [6]. We can estimate the time complexity of computing F-List to be O (DBSize) using a hashing scheme. However, the computational cost of procedure Growth () (the detail is shown in Algorithm 2) is at least polynomial. The procedure F P Growth () calls the recursive procedure Growth (), where multiple conditional FP-trees are maintained in memory and hence the bottleneck of the FP-Growth algorithm.

FP-Growth faces the following resource challenges: 1. Storage . For huge DB  X  X , the corresponding FP-tree 2. Computation distribution . All steps of FP-Growth can 3. Costly communication . Previous parallel FP-Growth 4. Support value . The support threshold value  X  plays an
Procedure : Growth( r , a ,  X  ) if r contains a single path Z then else end Figure 2: The overall PFP framework, showing five stages of computation.
Given a transaction database DB , PFP uses three MapRe-duce [4] phases to parallelize PF-Growth. Figure 2 depicts the five steps of PFP.
 Step 1: Sharding : Dividing DB into successive parts and Step 2: Parallel Counting (Section 2.3): Doing a MapRe-Step 3: Grouping Items : Dividing all the | I | items on F-Step 4: Parallel FP-Growth (Section 2.4): The key step of Step 5: Aggregating (Section 2.5): Aggregating the results
Counting is a classical application of MapReduce. Be-cause the mapper is fed with shards of DB , its input key-value pair would be like h key, value = T i i , where T i is a transaction. For each item, say a j  X  T i , the mapper outputs a key-value pair h key  X  = a j , value  X  = 1 i . generated by the mappers, the MapReduce infrastructure collects the set of corresponding values (here it is a set of
MapReduce provides convenient software tools for shard-ing.

Procedure : Mapper(key, value= T i ) foreach item a i in T i do end Procedure : Reducer(key= a i , value= S ( a i ))
C  X  0; end Call Output ( h null, a i + C i ); 1 X  X ), say S ( key  X  ), and feed the reducers with key-value pairs h key  X  , S ( key  X  ) i . The reducer thus simply outputs It is not difficult to see that key  X  X  is an item and value  X  X  is supp( key  X  X  ). Algorithm 3 presents the pseudo code of the first two steps: sharding and parallel counting. The space complexity of this algorithm is O ( DBSize/P ) and the time complexity is O ( DBSize/P ).
This step is the key in our PFP algorithm. Our solution is to convert transactions in DB into some new databases of group-dependent transactions so that local FP-trees built from different group-dependent transactions are independent during the recursive conditional FP-tree constructing pro -cess. We divide this step into Mapper part and Reducer part in details.

Algorithm 4 presents the pseudo code of step 4, Paral-lel FP-Growth. The space complexity of this algorithm is O ( Max ( NewDBSize )) for each machine.
When each mapper instance starts, it loads the G-list gen-erated in Step 3. Note that G-list is usually small and can be held in memory. In particular, the mapper reads and organizes G-list as a hash map, which maps each item onto its corresponding group-id.

Because in this step, a mapper instance is also fed with a shard of DB , the input pair should be in the form of h key, value = T i i . For each T i , the mapper performs the following two steps: 1. For each item a j  X  T i , substitute a j by corresponding 2. For each group-id, say gid , if it appears in T i , locate its
After all mapper instances have completed, for each dis-tinct value of key  X  , the MapReduce infrastructure collects corresponding group-dependent transactions as value value  X  , and feed reducers by key-value pair h key  X  = key  X  , value  X  i . Here value  X  is a group of group-dependent transactions cor-responding to the same group-id, and is said a group-depende nt shard.

Notably , this algorithm makes use of a concept introduced in [6], pattern ending at... , to ensure that if a group, for example { a, c } or { b, e } , is a pattern, this support of this Procedure : Mapper(key, value= T i ) Load G-List;
Generate Hash Table H from G-List; a[]  X  Split( T i ); for j = | T i |  X  1 to 0 do end Procedure : Reducer(key= gid ,value= DB gid )
Load G-List; nowGroup  X  G-List g id ;
LocalF P tree  X  clear; foreach T i in DB ( gid ) do end foreach a i in nowGroup do end
Algorithm 4 : The Parallel FP-Growth Algorithm pattern can be counted only within the group-dependent shard with key  X  = gid , but does not rely on any other shards.
In this step, each reducer instance reads and processes pairs in the form of h key  X  = gid, value  X  = DB ( gid ) i one by one, where each DB ( gid ) is a group-dependent shard.
For each DB ( gid ), the reducer constructs the local FP-tree and recursively builds its conditional sub-trees simi lar to the traditional FP-Growth algorithm. During this recursiv e process, it outputs found patterns. The only difference from traditional FP-Growth algorithm is that, the patterns are not output directly, but into a max-heap indexed by the support value of the found pattern. So, for each DB ( gid ), the reducer maintains K mostly supported patterns, where K is the size of the max-heap HP . After the local recursive FP-Growth process, the reducer outputs every pattern, v , in the max-heap as pairs in the form of The aggregating step reads from the output from Step 4. For each item, it outputs corresponding top-K mostly sup-ported patterns. In particular, the mapper is fed with pairs in the form of h key = null, value = v + supp( v ) i . For each a  X  v , it outputs a pair h key  X  = a j , value  X  = v + supp( v ) i .
Because of the automatic collection function of the MapRe-duce infrastructure, the reducer is fed with pairs in the for m of transitions including item a j . The reducer just selects from S ( a j ) the top-K mostly supported patterns and out-puts them.

Procedure : Mapper(key, value= v + supp( v )) foreach item a i in v do end Procedure : Reducer(key= a i , value= S ( v + supp( v )))
Define and clear a size K max heap : HP ; foreach pattern v in v + supp( v ) do end Call Output ( h null, a i + C i ); Table 1: Properties of the TTD (tag-tag) and WWD (webpage-webpage) transaction databases.

Algorithm 5 presents the pseudo code of Step 5, Aggregat-ing. The space complexity of this algorithm is O ( K ) and the time complexity is O ( | I | X  Max ( ItemRelatedP attersNum )  X  log ( K ) /P ).

To wrap up PFP, we revisit our example in Figure 1. The parallel algorithm projects DB onto conditional DBs, and distributes them on P machines. After independent tree building and itemset mining, the frequent patterns are foun d and presented on the right-hand side of the figure.
Our empirical study was designed to evaluate the speedup of PFP and its effectiveness in supporting query recommenda-toin or related research . Our data were collected from del. icio.us , which is a well-known bookmark sharing applica-tion. With del.icio.us , every user can save their book-marks of Webpages, and tag each bookmarked Webpage with tags. Our crawl of del.icio.us comes from the Google search engine index and consists of a bipartite graph coveri ng 802 , 739 Webpages and 1 , 021 , 107 tags. From the crawled data, we generated a tag transaction database and name it TTD, and a URL transaction database WWD. Statistics of these two databases are shown in Table 1.

Because it is often that some tags are labelled many times by many users to a Webpage and some Webpages being as-sociated with a tag many times, some tag/Webpage trans-actions are very long and result in very deep and inefficient FP-trees. So we divide each long transaction into many short ones. For example, a long transaction containing 100 a  X  X , 100 b  X  X  and 99 c  X  X  is divided into 99 short transactions { a, b, c } and a transaction of { a, b } . This method keeps the total number of items as well as the co-occurrences of a , b and c . Figure 3: The long-tail distribution of the del.icio.us tags.
We conducted performance evaluation on Google X  X  MapRe-duce infrastructure. As shown in Section 2.2, our algorithm consists of five steps. When we distributed the processing of the TTD dataset (described in Table 1) on 2 , 500 computers, Step 1 and Step 2 takes 0 . 5 seconds, Step 5 takes 1 . 5 sec-onds, Step 3 uses only one computer and takes 1 . 1 seconds. Therefore, the overall speedup depends heavily upon Step 4. The overall speedup is virtually identical to the speedup of Step 4.

The evaluation shown in Figure 4 was conducted at Google X  X  distributed data centers. Some empirical parameter values were: the number of groups, Q , is 50 , 000 and K is 50. We used various numbers of computers ranging from 100 up to 2 , 500. It is notable that the TTD data set is so large that we had to distribute the data and computation on at least 100 computers. To quantify speedup, we took 100 ma-chines as the baseline and made the assumption that the speedup when using 100 machines is 100, compared to using one machine. This assumption is reasonable for our experi-ments, since our algorithm does enjoy linear speedup when the number of machines is up to 500. From Figure 4, we can see that up to 1500 machines, the speedup is very close to the ideal speedup of 1:1. As shown in the table attached with Figure 4, the accurate speedup can be computed as 1920 / 2500 = 76 . 8%. This level of scalability, to the best of our knowledge, is far better than previous attempts [10, 8]. (We did not use the same data set as that used in [10, 8] to perform a side-by-side comparison. Nevertheless, the substantial overhead of these algorithms hinder them from achieving a near-linear speedup.) Notice that the speedup cannot be always linear due to Amdahl X  X  law. When the number of machines reaches a level that the computational time on each machine is very low, continue adding machines receives diminishing return . Nevertheless, when the dataset size increases, we can add more machines to achieve higher speedup. The good news is that the larger a mined dataset, the later Amdalh X  X  law would take effect. Therefore, PFP is scalable for large-scal e FIM tasks.
The bipartite graph of our del.ico.us data embeds two kinds of relations, Webpage-tags and tag-Webpages. From the TTD and WWD transaction databases, we mined two kinds of relationships, tag-tag and Webpage-Webpage, re-spectively.

Figure 5 shows some randomly selected patterns from the mining result. The support values of these patterns vary significantly, ranging from 6 to 60 , 726, which could show the characteristic of long tail Web data.
To the left of the figure, each row in the table shows a patten consisting of tags. The tags are in various languages , including English, Chinese, Japanese and Russian. So we have to write a short description for each pattern to explain the meaning of tags in their language.

The first three patterns contain only English tags and as-sociate technologies with their inventors. Some rows inclu de tags in different languages and can act as translators. Row 7, 10 and 12 are between Chinese and English; Row 8 is between Japanese and English; Row 9 is between Japanese and Chinese; and row 11 is between Russian and English.
One interesting pattern conveys more complex semantics is on Row 13, where  X  X horf  X  and  X  X homsky X  are two ex-perts in areas of  X  X nthropology X  and  X  X inguistics X  ; and they did research on a tribe called  X  X iraha X  . One other pattern on Row 2 associates  X  X rowser X  with  X  X irebox X  . These tag-tag relationship can be effectively utilized in suggesting rela ted queries.
To the right of Figure 5, each row of the table shows pat-tern consisting of URLs. By browsing the URLs, we find out and describe their goals and subjects. According to the descriptions, we can see that URLs in every pattern are in-trinsically associated. For example, all URLs in Row 1 point to cell phone software download site. All pages in Row 10 are popular search engines used in Japan. Please refer to Fig-ure 6 for snapshots of Web pages of these four search engines. Note that although Google is world-wide search engine and Baidu is run by a Chinese company, what are included in this pattern are their .jp mirrors. These Webpage-Webpage association can be used to suggested related pages of a re-turned page.
The frequent patterns can serve many applications. In addition to the previously mentioned dictionary and query suggestion, we think an interesting and practical one is vi-sualizing the highly correlated tags as an atlas, which allo ws users browsing the massive Web data while keeping in their interests easily.

The output formats our PFP algorithm fits this applica-tion well  X  for each item, a pattern of items are associated. When the items are text like tags or URLs, many well de-veloped methods can be used to build an efficient index on them. Therefore, given an tag (or URL), the system can in-stantly returns a group of tightly associated tags (or URLs) . Considering a tag as a geological place, the tightly associ-ated tags are very likely interesting places nearby. Figure 7 shows a screen shot of the visualization method implemented as a Java program. This shot shows the tag under current focus of the user in the center of the screen, with neighbors scattered around. To the left of the screen is a long list of top 100 tags, which are shown with fisheye technique and serve as a global index of the atlas.
In this paper we presented a massively parallel FP-Growth algorithm. This algorithm is based on a novel data and computation distribution scheme, which virtually elimina tes communication among computers and makes it possible for us to express the algorithm with the MapReduce model. Ex-periments on a massive dataset demonstrated outstanding scalability of this algorithm. To make the algorithm suit-able for mining Web data, which are usually of long tail distribution, we designed this algorithm to mine top-k pat-terns related to each item, rather than relying on a user specified value for global minimal support threshold. We demonstrated that PFP is effective in mining tag-tag as-sociations and WebPage-WebPage associations to support query recommendation or related search . Our future work will apply PFP on query logs to support related search for Google search engine. [1] Lamine M. Aouad, Nhien-An Le-Khac, and Tahar M. [2] A.-L. Barab  X asi and R. Albert. Emergence of scaling in [3] Gregory Buehrer, Srinivasan Parthasarathy, Shirish [4] Jeffrey Dean and Sanjay Ghemawat. Mapreduce: Figure 6: Examples of mining webpage-webpages re-lationship: all the three webpages ( www.google.co.jp , www.livedoor.com , www.baidu.jp , and www.namaan.net ) are related to Web search engines used in Japan.
 [5] Mohammad El-Hajj and Osmar R. Za iane. Parallel [6] Jiawei Han, Jian Pei, and Yiwen Yin. Mining frequent [7] Li Liu, Eric Li, Yimin Zhang, and Zhizhong Tang. [8] Iko Pramudiono and Masaru Kitsuregawa. Parallel [9] Agrawal Rakesh and Ramakrishnan Srikant. Fast [10] Osmar R. Za  X   X ane, Mohammad El-Hajj, and Paul Lu.
