 } Co-occurrence data is quite common in many real applications. La-tent Semantic Analysis (LSA) has been successfully used to iden-tify semantic relations in such data. However, LSA can only handle a single co-occurrence relationship between two types of objects. In practical applications, there are many cases where multiple types of objects exist and any pair of these objects could have a pairwise co-occurrence relation. All these co-occurrence relations can be exploited to alleviate data sparseness or to represent objects more meaningfully. In this paper, we propose a novel algorithm, M-LSA , which conducts latent semantic analysis by incorporating all pair-wise co-occurrences among multiple types of objects. Based on the mutual reinforcement principle, M-LSA identifies the most salient concepts among the co-occurrence data and represents all the ob-jects in a unified semantic space. M-LSA is general and we show that several variants of LSA are special cases of our algorithm. Ex-periment results show that M-LSA outperforms LSA on multiple applications, including collaborative filtering, text clustering, and text categorization.
 Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval:] Indexing methods General Terms: Algorithms Keywords: M-LSA, LSA, mutual reinforcement principle, multiple-type
Co-occurrence data arises naturally and frequently in a variety of applications such as information retrieval and text mining. In most existing work on analysis of co-occurrence data, only a sin-gle pairwise co-occurrence relationship between two types of ob-jects is considered. For example, in information retrieval, the co-occurrence information between documents and words is used to rank documents with respect to queries [2]. In collaborative filter-Copyright 2006 ACM 1-59593-369-7/06/0008 ... $ 5.00.
 Figure 1: Example of multiple-type interrelated data objects. Each edge denotes a single co-occurrence relationship. ing, items are recommended to an active user based on historical co-occurrence data between users and items [14].

However, in most applications, there exist multiple types of data objects and each pair of them could have a pairwise co-occurrence relationship. For example, in the Web domain as shown in Fig-ure 1, users co-occur with Web pages by viewing , queries co-occur with Web pages by referencing , Web pages co-occur with words by containing , and so on. With each kind of objects containing thou-sands of instances, each single co-occurrence relationship could be quite sparse. In the example above, using a single relationship, say, &lt; user, Web page &gt; , to represent users may not be meaningful since there are millions of Web pages and each user may only view a tiny portion of them. Latent Semantic Analysis (LSA) [10] was proposed to alleviate the data sparseness problem by representing objects in a low-dimensional semantic space. In this space, seman-tically related objects are expected to be near to each other. Such a dimension reduction technique has been shown to improve per-formance in a variety of applications (e.g., [10, 13, 4]). However, the application of LSA is rather limited since it can only consider the co-occurrence relationship between two types of objects. With multiple co-occurrence relationships available, it is beneficial to ex-ploit all of them to identify the semantic relations and alleviate the data sparseness problem. In the example above, we can also exploit better represent users, since users with similar interests tend to is-sue similar queries, and similar queries could refer to similar Web pages. All these co-occurrence relations could be complementary, thus it is desirable to incorporate all of them so as to represent each type of objects more meaningfully.

Though promising, expl oiting the co-occurre nce relations among multiple types of objects is challenging: 1) It is not clear how to effectively utilize all the co-occurrence relations among multiple types of objects to overcome data sparseness. 2) There may exist hidden relations between any two types of objects and these rela-tions are complex since the information can also propagate through any co-occurrence path. Take Figure 1 as an example. There is no direct co-occurrence between users and words. But similar words can induce similar queries and Web pages, thus in turn, similar users. The similarity between words can be propagated to users through the path  X  X ords  X  queries  X  users X  or  X  X ords  X  Web pages  X  users X . Even more complex propagations are also possi-ble.

To effectively utilize all the information among heterogeneous objects, we propose a novel and unified latent semantic analysis algorithm, M-LSA, to model all the objects in a unified frame-work and identify the latent semantic relations underneath all the co-occurrence data. By exploiting all the pairwise co-occurrence data simultaneously, M-LSA identifies the most salient or impor-tant concepts among them. These concepts span a unified low-dimensional semantic space, where each object is represented by a vector which reflects the strengths of its association with these concepts.

Specifically, to identify important concepts, a natural belief is that important concepts are related to important objects. Based on this assumption, we utilize the mutual reinforcement principle , which is underlying the traditiona l LSA, to identify the important objects in each type leveraging all the co-occurrence relations quan-titatively. This principle leads to an eigenvector problem and the obtained eigenvectors are regarded as the latent concepts. We show that the M-LSA algorithm is a natural generalization of the tradi-tional LSA from two to multiple types of objects.

The rest of this paper is organized as follows. Section 2 is to discuss previous work. We define our problem in Section 3. To solve this problem, we identify the mutual reinforcement principle underlying LSA and extend it to multiple types of objects in Sec-tion 4. This principle naturally leads to a solution to our problem: M-LSA. Section 5 is to present our experimental results for differ-ent applications. Finally, we conclude our paper in Section 6.
LSA was first introduced to address the synonym and polysemy problems in information retrieval [10]. Since then, LSA has at-tracted much attention and several researches analyzed it theoreti-cally [3, 11, 21, 4]. For example, [11] and [21] used probabilistic model to study the effectiveness of LSA. More recently, [4] argued that spectral algorithms such as LSA can expand the documents implicitly to improve retrieval accuracy.

Some variants of LSA have also been proposed recently. Proba-bilistic LSA (PLSA) [15] applies a probabilistic aspect model to the co-occurrence data. Iterative Res idual Rescaling (IRR) [1] is pro-posed to counteract LSA X  X  tendency to ignore the minor-class docu-ments. Unlike LSA, Nonnegative Matrix Factorization (NMF) [19, 25] decomposes the &lt; document, word &gt; matrix into two matrices with no negative values.

Most work above only considers co-occurrence relations between two types of objects. High-order co-occurrence data or high-order tensor is studied in multilinear algebra [18]. In [18], the High-Order Singular Value Decomposition (HOSVD) is proposed to fac-tor high-order tensors; in contrast, we consider the pairwise co-occurrence relations between different types of objects in this pa-per, which is less computationally expensive than HOSVD.
Several recent studies utilize pairwise co-occurrence data for dif-ferent specific purposes such as object clustering [5] and similarity measuring [24, 16]. In [9], a unified approach is proposed to ana-lyze both link and text information. Compared with these studies, our approach is more general and fundamental in that we provide a general principled method for analyzing any multiple types of objects.

Our work is related to the HITS algorithm [17] and a key-phrase extraction algorithm [27] in the sense of sharing the mutual rein-forcement principle . HITS uses this principle to find good pages from a Web subgraph and [27] uses it to identify salient key-phrases from a document. In this paper, we use this principle for multiple-type latent semantic analysis.
The problem we study is to analyze the co-occurrence relation-ship among multiple types of objects. Suppose we have N types of objects { X 1 ,X 2 , ..., X N } and each pair of them could have a pair-wise co-occurrence relation. Formally, we construct an undirected graph G ( V, E ) . V consists of N vertices with each corresponding to a type of objects. If there is a pairwise co-occurrence relation be-tween two types of objects, we have an edge in E which connects the corresponding vertices. We name graph G as  X  X ultiple-type graph X . In G , each type corresponds to a set of objects and we use | X i | to denote the number of objects of this type. For each edge e ij  X  E ,wehavea | X i | X | X j | co-occurrence matrix M Each edge could have a weight  X  ij to measure the importance of the co-occurrence relation between X i and X j . Note that G is not necessary to be a complete graph. An edge e ij is absent if the cor-responding co-occurrence data is unavailable or not meaningful for an application.

For example, in Figure 1, the corresponding graph G contains 4 types of objects: users, queries, Web pages, and words. We have 5 co-occurrence relations in G and each of them is denoted by an edge in Figure 1, thus we have 5 co-occurrence matrices.
Intuitively, based on graph G , objects of any type (e.g., users) can be represented by objects of the other types to which it is directly connected (e.g., Web pages and queries). However, this method is not effective to exploit all the information on a multiple-type graph. A more general method is to represent objects of any type by all the types of objects which have paths to them (e.g., representing users by words). However, due to the complex relations among data ob-jects, there may be many paths between two types of objects (e.g., users and words), thus this method is difficult to be implemented directly.
 To effectively utilize the information on a multiple-type graph G , our goal is to find the latent semantic representations for each type of objects. Specifically, based on the co-occurrence data of G , we first identify the most salient concepts basedonthe mutual reinforcement principle . These concepts span a semantic space. We then represent each object in this unified low-dimensional space.
In this section, we first describe the mutual reinforcement princi-ple based on the analysis of the traditional LSA. We then extend it to multiple types of objects and present the M-LSA algorithm. Fi-nally, we show that two variants of the traditional LSA are special cases of M-LSA. In the following, we denote matrices by upper-case letters (e.g., A , B ), scalars by lower-case letters (e.g., a , b ), and vectors by bold lower-case letters (e.g., a , b ).
LSA is based on a mathematical operation, Singular Value De-composition (SVD), which is akin to factor analysis. Consider the analysis of document-word co-occurrence data, if there are a total of n documents and m words in a document collection, the process starts with the creation of the co-occurrence matrix between the documents and words A =[ a ij ] , with each entry a ij representing the co-occurrence frequency of the i -thwordinthe j -th document. For the m  X  n matrix A , where without loss of generality m and rank ( A )= r , the SVD is defined as [12]: where U =[ u 1 , u 2 , ..., u r ] is an m  X  r column-orthonormal matrix whose columns are called left singular vectors;  X = diag [ ...,  X  r ] is an r  X  r diagonal matrix whose diagonal elements are pos-itive singular values sorted in descending order. V =[ v 1 is an n  X  r column-orthonormal matrix whose columns are called right singular vectors.

Givenaninteger k ( k r ) ,LSAusesthefirst k singular vectors to represent the documents and words in a k -dimensional space [10]. Each singular vector is regarded as a latent concept which captures a salient recurrent word combination pattern in the document collection; a document has a large index value for a con-cept if it contains the corresponding word pattern [6]. More pre-cisely, LSA represents each document by a row of [  X  1 v 1 and each word by a row of [  X  1 u 1 , ...,  X  k u k ] .
In LSA, the first k singular vectors of A represent the most im-portant k concepts in the document collection. Alternatively, we can assume that important concepts are related to both important documents and important words. To identify the most important concept from A , we associate importance property for documents and words respectively and utilize the mutual reinforcement prin-ciple 1 , Numerically, if we associate an importance value v i with the i -th document and an importance value u j with the j -th word, the mutual reinforcement principle is expressed as: where j  X  i means that the j -th word co-occurs with the i -th doc-ument. If we denote the importance of all documents by a vector and the importance of all words by a vector u , we can express the mutual reinforcement principle as:
It is easy to see that v = A T A v and u = AA T u . Therefore, and v are the principal left and right singular vectors of A respec-tively (please refer to [8] for proof).

Based on the co-occurrence data, the mutual reinforcement prin-ciple provides a reasonable solution to factor the most important concept out. However, a collection generally contains multiple top-ics. The most important concept represents well the most salient topic, but not other topics. Fortunately, the mutual reinforcement principle can also be extended to the non-principal singular vectors of A easily [17]. Specifically we can get the first k singular vec-tors. Each vector is then considered as a latent concept, and the magnitude of the corresponding singular value represents the im-portance of the concept. LSA projects the documents and words to a low-dimensional semantic space spanned by these concepts.
Similar principles are used in HITS [17] and [27].
In LSA, the latent concepts are represented by v in document space and u in word space. Conceptually, however, they are asso-ciated with the same concept. Thus, our key idea for generalizing LSA to handle multiple co-occurrence matrices is to represent each concept as a single vector. We now describe the unified importance and concept vectors.

Recall in Equation (1), we have v and u . If we concatenate them as a unified importance vector [ u , v ] T , Equation (1) can be rewritten as: where B is defined as:
It is easy to see that [ u , v ] T is the eigenvector of B . Mathemati-cally, [8] shows that the eigenvectors of B are closely related to the singular vectors of A : u and v in Equation (2) are the left and right singular vectors of A respectively. Thus, computationally the SVD of A is equal to finding the eigenvectors of B , while B gives us a (more preferred) unified view of importance.
 With the notion of a unified concept vector, we can now explain LSA in a unified view. Let c =[ u , v ] T be the unified concept vector. We denote the first k eigenvectors of B as { c 1 , ..., Each of them is a unified concept vector and has the corresponding importance precisely represented by the eigenvalue of B ,  X  is the same as the singular value of A [8]. Therefore, with this unified view, LSA represents each object by a row of the matrix: The upper part of the matrix is for words and the lower part of the matrix is for documents.
To factor the latent semantic concepts out from multiple co-occurrence relations represented by a multiple-type graph G ,wefirstextend the mutual reinforcement principle of LSA to multiple-type graph.
The mutual reinforcement principle on a multiple-type graph is a natural generalization of that of two types of objects. This prin-ciple also provides a reasonable solution to finding the important latent concepts among the multiple co-occurrence data as we will describe in the following.

Formally, recall that we have N types of objects on graph G : {
X 1 ,X 2 , ..., X N } . For any two types of objects: X i have the co-occurrence matrix M ij ( M ij =0 if the edge e absent on G ). It is easy to see that M ij = M T ji . (For brevity, we only consider the co-occurrence data between different types of objects. The co-occurrence relationship within a single type of objects can be incorporated similarly.) Let us associate an impor-tance value with each object. For the i -th type of objects in X have one weight vector w i to denote their importance. The mutual reinforcement principle can be expressed as:
Taking unified view of latent concepts, we use w =[ w 1 , ..., as the concatenated importance vector and define as the unified co-occurrence matrix. We can rewrite Equation (4) in a matrix format: It is easy to show that w will converge to the eigenvector of the co-occurrence matrix R .

Similar to LSA, since important objects will have high weights in w ,weregard w as the most important latent concept vector across all the co-occurrence relations. Each entry in w corresponds to an object and its value can be regarded as the association weight between the object and this latent concept. Similarly, the first k eigenvectors of R represent the top k important concepts, which span a k -dimensional semantic space to represent all the objects. Specifically, let be the top k eigenvalues of R and the corresponding vectors are respectively The symmetry of matrix R guarantees that all eigenvalues are real numbers and  X  l gives precisely the importance of the correspond-ing concept vector c l (1  X  l  X  k ) . Therefore, the i -th object can be represented by where c li is the i -th entry in c l , i.e., the association weight between the i -th object and the l -th concept. All the objects are represented in the matrix: with each row representing an object in the k -dimensional space.
The above analysis treats all the co-occurrence relationships equally, i.e., G is unweighted. It is not the best choice in many cases. For example, different co-occurrences might not be equally reli-able on the same scale; some may contain more noises than oth-ers. On the other hand, the entries in different co-occurrence matri-ces may have different scales, thus need to be normalized. There-fore, in general, we want to give different weights to different co-occurrence matrices, i.e., G is weighted. To incorporate these weights, we can directly replace M ij by  X  ij  X  M ij in matrix R in Equa-tion (5) and then conduct semantic analysis on this new matrix. These weights can be used as normalization factors or to reflect the importance of different matrices for a specific application. Since  X  ij  X  X  only represent the relative importance of the matrices and their scales do not change the eigenvectors of matrix R , without loss of generality, we could add a constraint i&lt;j  X  ij a specific  X  ij =1 and adjust others.

We use the name M-LSA to denote our algorithm since its anal-ysis is based on multiple-type graphs.
It is trivial to show that the traditional LSA is a special case in our framework. In particular, Since LSA only deals with two types of objects, say, X 1 and X 2 , there is only one co-occurrence matrix  X  12  X  M 12 . In the M-LSA framework, we thus have It is easy to see that the eigenvectors of R is the same as the eigen-vectors of B in Equation (3) when M 12 = A .Furthermore,by setting  X  12 =1 , the result of M-LSA for R is the same as the re-sult of LSA for A . Therefore, the traditional LSA is a special case in our M-LSA algorithm.

Previous work has also applied LSA to the objects with two types of features. For example, in [20], LSA was applied to images incor-porating both keyword features and low-level image features (e.g., colors of images). By concatenating two kinds of features as longer feature vectors, [20] conducted LSA on a larger matrix. In our M-LSA framework, if we use X 1 as images, X 2 as keywords, and X as low-level features, we have,
R = where M =[  X  12 M 12 , X  13 M 13 ] , which concatenates the keyword and low-level features of images together. Thus, this variant of LSA is also a special case in our M-LSA algorithm.
In this section, we evaluate M-LSA on different data sets and for different tasks, including collaborative filtering, text clustering, and text categorization. In these applications, we design the co-occurrence matrix R from the available data and study the effec-tiveness of our algorithm. In our experiments, we use two bench-mark data sets, MovieLens 2 and 20-Newsgroup 3 .
Collaborative Filtering (CF) [14] is to recommend items to an active user based on the historical data of like-minded users. Based on the current ratings of the active user, memory-based algorithms find its nearest neighbors and recommend items based on the rat-ings of the neighbors. In this paper, we use the Pearson method, a popular memory-based method, as one of our baselines. Pear-son method uses Pearson correlation coefficient to find the nearest neighbors for an active user v : where m is the number of items, r v,i ( r u,i )istheratingofuser v is the similarity score between the two users. We use N v selected set of the nearest neighbors of v , then the prediction of v  X  X  rating for an unseen item i is calculated as
For the memory-based CF algorithm, the prediction accuracy de-pends on the accuracy of the nearest neighbors. In our experiments, we show that M-LSA can improve the prediction accuracy by rep-resenting the users more meaningfully, thus finding more accurate nearest neighbors. http://www.cs.umn.edu/research/GroupLens http://people.csa il.mit.edu/jrennie/20Newsgroups Figure 2: CF result comparison of different methods on Movie-Lens data
We use the benchmark data set MovieLens, which includes 100,000 ratings (1 X 5) from 943 users on 1682 movies. Each movie has the title keyword information. Therefore, we have three types of ob-jects: users ( X 1 ), items ( X 2 ) and keywords ( X 3 ). The co-occurrence matrices include user-item ( M 12 ), user-word ( M 13 ) and item-word ( M 23 ). M 12 is constructed with the ratings of users on movies; M 23 is constructed with the movie titles. We construct M 13 follows: if a keyword appears in a title of any movie rated by a user, this keyword and this user has a co-occurrence. Their co-occurrence frequency is calculated by the user X  X  rating on movies and the term-frequency of this keyword in the corresponding movie titles. We weigh M 13 and M 23 by the standard TF-IDF scheme which is commonly used in retrieval retrieval [2]. Finally, we can construct the relation matrix R based on the three matrices as: where the weight parameters  X ,  X ,  X   X  0 ,and  X  +  X  +  X  =1
We use two baseline methods. One is the Pearson method which is based on user-item matrix M 12 . The other is based on user-word matrix M 13 to calculate the nearest neighbors. We also apply the traditional LSA on these two matrices and then use the low-dimensional representation to calculate the nearest neighbors. Our M-LSA will be compared with all the four methods. For all the five methods, after calculating the nearest neighbors, the predictions are based on Equation (7) and the comparison is based on the half-time utility metric defined in [7]. For a user v , the expected utility of a ranked list of items is: where  X  is the half-time parameter (  X  =5 in our experiments) and r v,j is v  X  X  rating for the item which is at the j -th position in the current rank list. The final score over all users in the test set is: where R max v is the maximum possible utility obtained where all the test items are ranked at top according to user v  X  X  rating. In the following figures, we use  X  X ank gain X  to denote R score.
All the results are averaged over a randomly split of 5 folds on the MovieLens data. We fix the number of neighbors to 50 and Figure 3: Impact of number of neighbors on baseline methods
Figure 4: Impact of the dimensionality on LSA and M-LSA the number of dimensions for LSA and M-LSA to 20. We set  X  =0 . 3 ,  X  =0 . 5 ,and  X  =0 . 2 in matrix R . Figure 2 gives the comparison results of different methods. The figure shows that M-LSA achieves the best result. Compared with the other methods, M-LSA achieves a relative improvement of 15.5% over user-item, 8.0% over user-word, 14.4% over LSA(user-item), and 6.0% over LSA(user-word) with respect to the R score measure. Our results are consistent with that of [22], where the authors found that user-word matrix could get better result because it is less sparse than user-item matrix. LSA based methods could only achieve marginal improvement compared with baselines. Our method can improve the utility over LSA significantly. We also calculate the standard deviation of the five methods over the 5 folds. The results in Fig-ure 2 show that M-LSA is more stable. This confirms that M-LSA can effectively explore all the co-occurrence relations to represent objects more meaningfully.

We also study the parameters for different methods. In Figure 3, we plot the results along with the number of neighbors for the two baselines. We can see that the best results are obtained when the number is 50. Thus we fix the number of neighbors as 50 in all the experiments.
 In Figure 4, we study the impact of number of dimensions for LSA and M-LSA. For M-LSA, we set  X  =0 . 3 ,  X  =0 . 5 ,and  X  =0 . 2 in matrix R . It can be seen that M-LSA consistently outperforms LSA based methods and all attain their best results when the dimensionality is 20.
 Finally, we study the influence of  X  ,  X  ,and  X  in matrix R for M-LSA. Since  X  +  X  +  X  =1 , we only vary  X  and  X  and report our results in Table 1. We vary  X  (  X  respectively) from 0.1 to 0.9 by step 0.2 and we retain the top 20 eigenvectors for M-LSA. In this table, we get the best result when  X  =0 . 3 ,  X  =0 . 5 , thus  X  Furthermore, when  X   X  0 . 3 , most of the results (bold font) are better than the baselines and LSA based methods, thus M-LSA is effective for a wide range of parameters. Table 1: Impact of weight parameters of M-LSA on CF results
It is worth noting that when  X  =0 (i.e.,  X  +  X  =1 . 0 in Table 1), we only consider the user-item matrix M 12 and user-word matrix M 13 but not M 23 , which means we concatenate both features with appropriate weights to represent the users. However, we obtained the best result when  X  =0 . 3 ,  X  =0 . 5 ,and  X  =0 . 2 . This means all the co-occurrence information is useful and can be incorporated by M-LSA effectively.
Text clustering is one of the fundamental problems and has re-ceived much attention recently. When the vector space model (VSM) is used, each document is represented as a term vector and the similarity score between two documents is calculated as the co-sine value (or dot product) of corresponding term vectors. In this section, we show experimentally that our M-LSA method can im-prove the document representation and boost text clustering result significantly. We use k-means, one of the most popular clustering algorithms, to compare different document representation methods.
To obtain multiple co-occurrence data, we use the 20-Newsgroup data set. In this newsgroup data, different emails or posts may have the same subject. Thus, besides the email-word relation, we have email-subject and subject-word relations. Therefore, the objects we have are: emails ( X 1 ), subjects ( X 2 ), and words ( X pair of them has a co-occurrence matrix and the obtained matrix R is similar as in Equation (8). We select the five  X  X omp.* X  out of the 20 categories as our data set. Each of the five categories has 1000 emails, thus, we have 5000 in total. After preprocessing the subjects by removing  X  X e: X  and  X  X wd: X , we obtain 2933 subjects in total. We use the F measure defined in [23] as our evaluation metric. For each cluster, we calculate its Precision and Recall with respect to each given category. The Fmeasure is defined by com-bining the Precision and Recall together. Specifically, for cluster j and category i : where n ij is the number of members of category i in cluster j , n is the number of members in category i , n j is the number of members in cluster j and F ( i, j ) is the Fmeasure of cluster j and category i . The F measure of the whole clustering result is defined as a weighted sum over all categories as follows: where the max is taken over all clusters. Figure 5: Results on text clustering. We compare the result on different dimensions
In our experiments, we first represent each email (body + sub-ject) using the TF-IDF method. This gives us the baseline re-sult. LSA is applied to this TF-IDF representation and our M-LSA method is based on the matrix R defined above. K-means is then run on the low-dimensional representations for LSA and M-LSA. We randomly select the initial centroids 20 times and have 20 runs. The results for each method are averaged over these 20 runs.
We set  X  =0 . 3 ,  X  =0 . 5 ,and  X  =0 . 2 for M-LSA. Figure 5 is the primary results for clustering. In this figure, we vary the number of dimensions for both LSA and M-LSA. It is clear that M-LSA and LSA outperform the baseline method (0.449) substan-tially. By comparing M-LSA with LSA, we can see that M-LSA always achieves better result. For example, when we set the num-ber of dimensions to 20, the F measures of M-LSA and LSA are 0.574 and 0.532 respectively. Thus, M-LSA achieves 8.0% relative improvement over LSA. The t-test over the 20 runs indicates the improvement is statistically significant (p-value=0.0001).
For M-LSA, we also study the parameters  X  ,  X  ,and  X  in a sim-ilar way as in collaborative filtering. We set the number of dimen-sions to 20 and the results are reported in Table 3. The best result (0.5736) is obtained when  X  =0 . 3 ,  X  =0 . 5 ,and  X  =0 . difference here is that when  X  is set to a large value, the F measure is lowered a lot. This is because  X  is the weight for the (email, subject) co-occurrence matrix M 12 and each email has only one subject. On average, 1 . 70 emails share a subject. Thus, the co-occurrence between email and subject is extremely sparse and only two emails that have the same subject will have a nonzero simi-larity score if we set  X  =1 . Thus, the clustering result is biased when  X  is set too large. However, given an appropriate weight to this co-occurrence matrix, M-LSA can improve the clustering per-formance substantially. This again confirms the effectiveness of M-LSA to incorporate the meaningful co-occurrence information.
In Table 2, we show the first five eigenvectors of R when we set  X  =0 . 3 ,  X  =0 . 5 ,and  X  =0 . 2 . The most important words and their weights associated with each eigenvector are given in this table. It is clear that each eigenvector is a focused concept. For example, eigenvector 2 is about  X  X perating system X , eigenvector 3 is about  X  X ardware X , and eigenvector 4 is about  X  X onitor X . This shows the effectiveness of M-LSA to identify latent semantic con-cepts by the mutual reinforcement principle. Finally, we test our algorithm on text categorization problem. This experiment is to show that M-LSA is much flexible to model Table 3: Impact of weight parameters for M-LSA on clustering results a variety of co-occurrence relations. In particular, we show that M-LSA can incorporate category information.
The data set we used in this experiment is the same as above: 5 categories of  X  X omp.* X  newsgroup data. For each category, we randomly select 800 documents as training and use the remaining 200 as test data.

To incorporate the category information, in this experiment, we use a different co-occurrence relation matrix. The objects are: emails ( X 1 ), categories ( X 2 ), and words ( X 3 ). M 13 isbasedontheTF-IDF weighting of email-word matrix. Since each training exam-ple has a word vector in M 13 , for each category, we calculate the centroid vector of the corresponding training examples. The category-word matrix ( M 23 ) is composed of all these centroid vec-tors. We construct the binary email-category matrix M 12 as fol-lows: for each training example, it has a co-occurrence with its labeled category. The test data is not used in the construction of the co-occurrence matrix.

We apply the M-LSA algorithm to the above co-occurrence ma-trix and obtain the first k eigenvectors. We then project both the training and test examples along these k eigenvectors, thus rep-resent all the examples in a k -dimensional space. Classification experiments are conducted in this space.

Since M 13 and M 23 are standard TF-IDF matrices, we set their weights to 1.0. We thus only vary the weight  X  for matrix M study its impact. The co-occurrence matrix is: Figure 6: Results on text categorization. We compare the re-sults on different dimensions
For the classifier, we use the SV M light software 4 and all the re-sults reported below are based on the micro-averaging F1 (micro-F1) measure defined in [26]. F1 measure is a tradeoff between pre-cision and recall. Another commonly-used F1 measure is macro-averaging F1 (macro-F1). Macro-F1 is the arithmetic average of F1 measure over all the categories and micro-F1 is the weighted average that emphasizes on categories with more examples. Since all the categories in our data set have the same size, macro-F1 is similar to micro-F1 measure. Thus we only report micro-F1.
We compare our result with the standard SVM on the email-word matrix and LSA based method. We set  X  =0 . 3 and Figure 6 shows the results along with different dimensions. It is clear that M-LSA can outperform the baseline, while LSA can not. When we set the number of dimensions to 50, M-LSA achieves 0.803 on micro-F1, http://svmlight.joachims.org which is higher than the baseline 0 . 790 and the best result of LSA 0 . 789 . Since M-LSA incorporates the category supervised infor-mation into the co-occurrence matrix, it can achieve better result even when the dimension is set to a smaller value (e.g., 20), as shown in Figure 6.

Figure 7 shows the impact of the weight  X  for the matrix M We set the number of dimensions to 50 and vary  X  from 0 . 1 . 0 with step 0 . 1 .When  X  =0 . 0 , M-LSA does not consider the category information thus the result is the same as LSA. From this figure, we can see that M-LSA is insensitive to the change of  X  when  X   X  0 . 1 and its micro-F1 is always higher than the base-line. We obtain the best result when the weight is 0.3. The result is really encouraging since it indicates: although M-LSA is unsuper-vised in spirit, it can also incorporate the supervised information by appropriately introducing the co-occurrence relationship.
Finally, we show that M-LSA can associate meaningful words to categories. In Table 4, we show the most salient words in the centroid vector of each category (denoted by Centroid). For M-LSA, we do not use category-word matrix M 13 in Equation (9) and set the weight of email-category M 12 to  X  =0 . 3 .Weusethe first 50 eigenvectors for M-LSA. In M-LSA, all the categories and words are projected into a unified semantic space, we thus calculate the similarities between words and categories by their dot products in this space. For each category, the most similar words are reported in Table 4 (denoted by M-LSA). It is clear that there is a big overlap between Centroid and M-LSA based methods. This again confirms the effectiveness of M-LSA in identifying the latent concepts by utilizing all the co-occurrence information and representing each type of objects meaningfully in a unified semantic space.
It is important to exploit the co-occurrence relations among dif-ferent types of objects in many applications. In this paper, we modelled all the pairwise co-occurrence relations with a multiple-type graph and proposed a general algorithm, M-LSA, which con-ducts latent semantic analysis by incorporating all pairwise co-occurrences among multiple types of objects. Based on the mu-tual reinforcement principle as used in the traditional LSA, M-LSA identifies the most salient concepts among all the co-occurrence data and represents each object in a unified semantic space. M-LSA is general and covers several variants of LSA as special cases. We evaluated M-LSA on three applications and obtained very encour-aging results. All the experiments showed that M-LSA is effective in utilizing all the information on a multiple-type graph. M-LSA can be applied to any co-occurrence data involving multiple types of objects, thus has potentially many applications in multiple do-mains.
