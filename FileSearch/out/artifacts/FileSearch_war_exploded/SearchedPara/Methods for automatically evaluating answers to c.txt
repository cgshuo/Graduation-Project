 Jimmy Lin  X  Dina Demner-Fushman
Abstract Evaluation is a major driving force in advancing the state of the art in language tech-nologies. In particular, methods for automatically assessing the quality of machine output is the preferred method for measuring progress, provided that these metrics have been validated against human judgments. Following recent developments in the automatic evaluation of ma-chine translation and document summarization, we present a similar approach, implemented in a measure called P OURPRE , an automatic technique for evaluating answers to complex questions based on n -gram co-occurrences between machine output and a human-generated answer key. Until now, the only way to assess the correctness of answers to such questions involves manual determination of whether an information  X  X ugget X  appears in a system X  X  re-sponse. The lack of automatic methods for scoring system output is an impediment to progress in the field, which we address with this work. Experiments with the TREC 2003, TREC 2004, and TREC 2005 QA tracks indicate that rankings produced by our metric correlate highly with official rankings, and that P OURPRE outperforms direct application of existing metrics.
Keywords Question answering . Evaluation 1. Introduction
Question answering, which lies at the intersection between natural language processing  X  X nswers X  instead of  X  X its X . In the past few years, researchers have made significant strides in systems capable of answering fact-based natural language questions such as  X  X hat city is the home to the Rock and Roll Hall of Fame? X ,  X  X ho invented the paper clip? X , and  X  X ow far is it from the pitcher X  X  mound to home plate? X  These so-called  X  X actoid X  questions can be typically answered with named entities such as dates, locations, people, organizations, measures, etc.

Implicit in the factoid question answering task is the notion that there exists a single, short, correct answer. A straightforward extension that relaxes this criterion yields list questions such as  X  X hat countries export oil? X , where the desired response is an unordered set of named entities. List questions are challenging because it is difficult to predict a priori the total number of existing answer instances and to recognize answers that refer to the same entity (due to synonymy, name variations, etc.).

Progress in factoid question answering does not alter the fact that such questions comprise only a small fraction of all possible information needs. To address the limitations of current technology, researchers have begun to explore techniques for answering more complex ques-tions such as the following: Who is Aaron Copland?
How have South American drug cartels been using banks in Liechtenstein to launder money?
What was the Pentagon panel X  X  position with respect to the dispute over the US Navy training range on the island of Vieques?
The first is an example of a so-called  X  X efinition X  question, where the goal is to generate a profile of a person, entity, or event that integrates information from multiple sources within a given text collection. The second is an example of a  X  X elationship X  question, focused on the ties (economic, familial, etc.) between different entities. The last is an example of a so-called  X  X pinion X  question, which might involve sentiment detection and analysis of language use. This work presents a method for automatically evaluating answers to such complex questions.

Research in evaluation methodology is important because evaluation is arguably the single language technology is accomplished through the use of a test collection, which consists of a corpus, a series of well-defined tasks, and a set of judgments indicating the desired system output. Another requirement is the existence of a meaningful metric that quantifies machine performance in relation to a reference, usually human output.

Automatic methods for evaluating system output are highly desirable because humans represent a bottleneck in the experimental cycle. The ability to assess system performance without human intervention allows quick experimental turnaround and rapid exploration of the solution space, which often leads to dramatic advances in the state of the art. A case in point is machine translation, where the development of B related metrics has stimulated significant improvements in translation quality. Prior to the existence of these automatic metrics, humans were required to assess translation quality, a process that was both slow and error-prone.

This work presents P OURPRE , an automatic technique for evaluating answers to complex questions based on n -gram co-occurrences between machine output and a human-generated answer key. Experiments with data from the TREC question answering evaluations show that our metric correlates well with human judgments. P OURPRE evaluation method for complex questions: by serving as a surrogate for human assessors, it enables rapid experimentation in question answering, which will hopefully lead to accelerated advances in the state of the art.

This paper is organized as follows: Section 2 discusses the general methodology for eval-uating answers to complex questions, and how it is implemented in the context of the TREC question answering tracks. Section 3 relates our proposed metric with previous work for automatically evaluating language technologies: machine translation and document summa-rization. Section 4 provides an algorithmic description of P evaluation methodology for validating the metric using data from previous TREC question answering tracks. Section 6 presents results from our evaluation, comparing different variants of P OURPRE and direct application of existing metrics. Section 7 describes a series of more detailed experiments that attempts to provide a better understanding of how P and factors that affect its performance. Finally, Section 8 concludes this paper. 2. Evaluating answers to complex questions
Consider a  X  X efinition X  question such as  X  X ho is Vlad the Impaler? X  A  X  X ood X  answer might include information about the 16th century warrior prince X  X  life, accomplishments, interesting things about X . X , where X (the target) can be a person, an organization, a common noun, an event, etc. Given that named entities alone are insufficient to describe the target, answers to  X  X efinition X  questions might be comprised of natural language sentences and phrases that express relevant facts about the target entity. Taken together, these facts might qualify for a  X  X ood answer X . Consider the following  X  X uggets X  of information about  X  X lad the Impaler X : 16th century warrior prince Inspiration for Bram Stoker 1897 novel  X  X racula X  Buried in medieval monastery on islet in Lake Snagov Impaled opponents on stakes when they crossed him Lived in Transylvania (Romania) Fought Turks Called  X  X rince of Darkness X  Possibly related to British royalty
By viewing complex questions as requests for collections of facts that address a particular information need, it becomes possible to characterize the properties of a  X  X ood X  answer. From this basic idea, one can develop the desiderata for an evaluation metric:
Answers that contain more relevant facts are preferred over answers that contain fewer relevant facts.
 Shorter answers are preferred over longer answers containing the same number of facts. Verbosity should be punished.
 Some facts are more important than others, and this should be reflected accordingly.
Despite differences in form, a fact-based evaluation methodology can be applied to many types of complex questions, including the  X  X elationship X  and  X  X pinion X  questions described above. In fact, this methodology has been implemented in the TREC question answering tracks, organized by the National Institute of Standards and Technology, since 2003. The QA tracks, which first began in 1999, provide the infrastructure and support necessary to provides meaningful comparisons between different retrieval techniques. TREC evaluations have gained widespread acceptance as the de facto benchmark by which question answering performance is measured, and results from previous years provide data necessary to validate our automatic evaluation metric.

To date, NIST has conducted four separate evaluations of complex questions, all using a fact-based evaluation methodology:  X  X efinition/other X  questions in TREC 2003 X 2005 (e.g.,  X  X hat is feng shui? X );  X  X elationship X  questions in TREC 2005 (e.g.,  X  X o the military per-sonnel exchanges between Israel and India show an increase in cooperation? If so, what are the driving factors behind this increase? X ); a small-scale pilot of  X  X pinion X  questions in 2005 (e.g.,  X  X o the American people think that Elian Gonzalez should be returned to his fa-ther? X ). Since relatively few teams participated in the TREC 2005 evaluation of relationship questions, this work focuses only on the three years X  worth of definition/other questions.
In 2004 and 2005, factoid and list questions were organized in  X  X eries X  around top-ics (Voorhees, 2005), for example: the band Nirvana 1 factoid Who is the lead singer/musician in Nirvana? 2 list Who are the band members? 3 factoid When was the band formed? 4 factoid What is their biggest hit? 5 list What are their albums? 6 factoid What style of music do they play? 7 other Requests for definitions were implicit in the other questions associated with each topic.
These other questions differed from the TREC 2003 definition questions only in that duplicate questions are best paraphrased as  X  X ell me interesting things about X that I haven X  X  already explicitly asked about. X  For the purposes of this study, this fine distinction was ignored, as the decision does not have any significant consequences for our results.

Answers to definition questions are comprised of an unordered set of [document-id, answer string] pairs, where the strings are presumed to provide some relevant information about the of the answer string or the number of response pairs, the final F -score penalizes verbosity (described below).

To evaluate system responses, NIST pools answer strings from all systems, removes their association with the runs that produced them, and presents them to a human assessor. Using these responses and research performed during the original development of the question, the assessor creates an  X  X nswer key X  comprised of a list of  X  X uggets X  X  X ssentially facts about the target. According to TREC guidelines, a nugget is defined as a fact for which the assessor could make a binary decision as to whether a response contained that nugget (Voorhees, 2003). or  X  X kay X . Vital nuggets represent concepts that must be present in a  X  X ood X  definition; on the other hand, okay nuggets contribute worthwhile information about the target but are not essential. The vital/okay distinction has significant implications for scoring, demonstrated below. As an example, nuggets for the question  X  X hat is the Cassini space probe? X  are shown in Table 1.

Once the answer key of vital/okay nuggets is created, the assessor then goes back and manually scores each run. For each system response, he or she decides whether or not each nugget is present. Two examples of this matching process are shown in Fig. 1: nuggets 1 and 2 were found in the top passage, while nuggets 4, 5, and 6 were found in the bottom passage.
 the final score of a system run is simply the mean of scores across all questions. The per-question F -score is a harmonic mean between nugget precision and nugget recall, where recall is heavily favored (controlled by the  X  parameter, set to five in 2003 and three in 2004 and 2005). Nugget recall is calculated solely on vital nuggets (which means no credit is given for returning okay nuggets), while nugget precision is approximated by a length allowance based on the number of both vital and okay nuggets returned. Early on in a pilot set of nuggets contained in a system response (Voorhees, 2003), which corresponds to the precision.

Because the answer key to a question only needs to be created once, the process of manually assessing each system response is the bottleneck in TREC evaluations. It is thought that a assessors do not simply perform string matches in this decision process. Rather, matching occurs at the conceptual level, abstracting away from issues such as vocabulary differences, syntactic divergences, paraphrases, etc. Consider the following examples: Who is Al Sharpton? Nugget : Harlem civil rights leader System response : New York civil rights activist What is Freddie Mac? Nugget : Fannie Mae is sibling of Freddie Mac
System response : both Fannie Mae and Freddie Mac, the quasi-governmental agencies that together ...
 Who is Ari Fleischer? Nugget : Elizabeth Dole X  X  Press Secretary System response : Ari Fleischer, spokesman for ... Elizabeth Dole What is the medical condition shingles? Nugget : tropical [ sic ] capsaicin relieves pain of shingles System response : Epilepsy drug relieves pain from ... shingles In each of the above cases, is the nugget contained in the system response fragment?
Regardless of the judgment, these examples show the supposed necessity of having a human in the loop X  X o decide whether two strings contain the same semantic content. For this reason, there exists no automatic method for evaluating answers to complex questions. As a result, the experimental cycle for developing systems has been tortuously long; to accurately assess the performance of new techniques, one must essentially wait for the yearly TREC cycle (or conduct less-accurate in-house manual evaluations).

In attempts to remedy this situation, we reexamined the assumption that matching answer nuggets with system responses requires human involvement. We show that, despite inher-ent difficulties in matching meaning, automatically computing substring overlap is a viable alternative to human X  X n X  X he X  X oop evaluation. This insight, first demonstrated by Papineni et al. (2002) in the context of machine translation, has given rise to much subsequent work focused on the development of automatic evaluation metrics for various language technolo-gies. P OURPRE , our algorithm for evaluating answers to complex questions, follows very much in this line of research. It is shown through experiments on data from previous TRECs that P OURPRE scores correlate well with human judgments, thus validating the usefulness of our metric for guiding system development. 3. Related work
The evaluation of many language applications involves comparing system-generated out-lation, for example, testsets include a number of human-generated  X  X eference X  transla-tions; in document summarization, manually-generated summaries provide examples of  X  X ood summaries X . The idea of employing n -gram co-occurrence statistics (i.e., substring matches) to compare machine-generated output with desired output was first successfully implemented in the B LEU metric (Papineni et al., 2002) and the closely-related NIST met-ric (Doddington, 2002) for machine translation evaluation. Experiments have shown that these automatically-generated scores correlate well with human judgments across large test-sets (although not on a per-sentence basis) (Kulesza and Shieber, 2004). Since then, the basic method for scoring translation quality has been improved upon by others, e.g., (Babych and Hartley, 2004; Lin and Och, 2004). Although there have been recent attempts to in-troduce more linguistically-rich features in the evaluation algorithms, e.g., (Banerjee and
Lavie, 2005), the basic idea of matching n -grams remains at the core of machine translation evaluation.

The simple idea of matching substrings from system output against a reference standard has also been applied to document summarization evaluation. The R
Hovy, 2003) has been shown to correlate well with human judgments and has been widely adopted by researchers for system development. Although more sophisticated methods, e.g., the pyramid scheme (Nenkova and Passonneau, 2004), have been proposed, R an important benchmark due to its ease of use and accuracy.

Recently, Soricut and Brill (2004) employed n -gram co-occurrences to evaluate question answering in a FAQ domain; unfortunately, the task differs from that of answering  X  X efinition X  questions, making their results not directly comparable. Xu et al. (2004) applied R automatically evaluate answers to definition questions, viewing the task as a variation of document summarization. Because TREC answer nuggets are short snippets of text, the authors found it necessary to rephrase them X  X wo humans were asked to manually create  X  X eference answers X  based on the assessors X  nuggets and IR results. This proved to be a labor-intensive process. Furthermore, Xu et al. did not perform a large-scale assessment of the reliability of R OUGE for evaluating answers to  X  X efinition X  questions (which we describe developed for one task cannot usually be directly applied to another task without modification.
Our experiments show that R OUGE is less well-suited for evaluating answers to complex the vital/okay nugget distinction and precision length penalty.

In general, we have noted a recent convergence between multi-document summarization and question answering (Lin and Demner-Fushman, 2005). The move towards more complex information needs in question answering is complemented by the development of topic-focused summarization (Dang, 2005). Most notably, the DUC 2005 task requires systems to generate answers to natural language questions based on a collection of known relevant documents:  X  X he system task in 2005 was to synthesize from a set of 25 X 50 documents a brief, well-organized, fluent answer to a need for information that cannot be met by just stating a name, date, quantity, etc. X  (DUC 2005 guidelines 1 ). These guidelines were modeled after the information synthesis task suggested by Amig  X  o et al. (2004), which they characterize obtain a comprehensive, non-redundant report that satisfies the information need X . This trend represents a remarkable opportunity for cross-fertilization and intellectual dialog between two mostly-disjoint communities. 4. Algorithm description
As previously mentioned, it has been widely assumed that matching nuggets from the asses-sors X  answer key with systems X  responses must be performed manually because it involves semantics (Voorhees, 2003). In actuality, however, this assumption has not been experimen-tally verified, and we hypothesize that term co-occurrence statistics can serve as a surrogate for this semantic matching process. Experience with the R effectiveness of matching unigrams, an idea we employ in our P longer n -grams (bigrams, trigrams, etc.) has proven useful in machine translation because it measures the fluency of system-generated responses, an important aspect of translation qual-ity. However, since almost all current question answering systems employ extractive tech-niques (i.e., no natural language generation), fluency is not usually a concern. The presence or absence of relevant content, captured via unigram co-occurrence, is the more important factor in both document summarization and complex question answering.

The idea behind P OURPRE is relatively straightforward: calculate a match score for each nugget by summing the unigram co-occurrences between terms from the nugget and terms from the system response. We decided to start with the simplest possible approach: count the term overlap and divide by the total number of terms in the answer nugget. The only additional wrinkle is to ensure that all words appear within the same answer string. Since nuggets represent coherent concepts, they are unlikely to be spread across different answer strings (which are usually extracts of different source documents). As a simple example, let X  X  say we X  X e trying to determine if the nugget  X  X  B C D X  is contained in the following system response: 1. A 2. B C D 3. D 4. A D
The match score assigned to this nugget would be 3 / 4, from answer string 2; no other an-swer string would get credit for this nugget. This provision reduces the impact of coincidental term matches.
Once we determine the match score for every nugget, the final F -score is calculated in the usual way (see Fig. 2), except that the automatically-derived match scores are substituted where appropriate. For example, nugget recall now becomes the sum of the match scores for all vital nuggets divided by the total number of vital nuggets. In the official F -score calculation, the length allowance X  X or the purposes of computing nugget precision X  X as 100 non-whitespace characters for every okay and vital nugget returned. This remained exactly the same under P OURPRE .

A major drawback of this basic unigram overlap approach is that all terms are considered equally important X  X urely, matching  X  X ear X  in a system X  X  response should count for less than matching  X  X uygens X , in the example about the Cassini space probe. We decided to capture this intuition using inverse document frequency, a commonly-used measure in information tion, and c i is the number of documents that contains the term t term counts are simply replaced with idf sums in computing the match score, i.e., the match score of a particular nugget is the sum of the idf s of matching terms in the system response divided by the sum of all term idf s from the answer nugget. The effects of stemming, i.e., matching stemmed terms derived from the Porter stemmer, were also examined.

Finally, we attempted two different methods for aggregating results: microaveraging and macroaveraging. For microaveraging, final per-run scores were calculated by computing nugget match scores over all nuggets for all questions. For macroaveraging, scores for each question were first computed, and then averaged across all questions in the testset. With microaveraging, each nugget is given equal weight, while with macroaveraging, each question 5. Validation methodology
Submitted runs to the TREC 2003, 2004, and 2005 question answering tracks were used to validate P OURPRE (2003  X  X efinition X  questions, 2004 and 2005  X  X ther X  questions). There were 50 questions in 2003 testset, 65 in 2004, and 75 in 2005. Table 2 shows the median and standard deviation of okay and vital nuggets in each year X  X  testset. In total, there were 54 runs from TREC 2003, 63 from TREC 2004, and 72 from TREC 2005. For each experiment, system runs were automatically scored with a variant of our P were then correlated with the official scores generated by humans. As a baseline, we compared P
OURPRE to R OUGE ,B LEU , and N IST , using a simple concatenation of all answer nuggets as the reference output.
 an automatic evaluation metric. Direct correlation between official scores and automatically-generated scores, as measured by Pearson X  X  r , seems like an obvious metric for quantifying the validity of a scoring algorithm. Indeed, this measure has been employed in the evaluation of B LEU ,R OUGE , and other related metrics.

However, we believe that there are better measures of performance. In comparative eval-uations, we ultimately want to determine if one technique is  X  X etter X  than another. Thus, the system rankings produced by a particular scoring method are often more important than the actual scores themselves. Following the information retrieval literature, we employ Kendall X  X   X  to capture this insight; earlier uses of this metric for validation question answering eval-uations can be seen in, for example, (Voorhees and Tice, 2000). Kendall X  X   X  X istance X  between two rankings as the minimum number of pairwise adjacent swaps neces-sary to convert one ranking into the other. This value is normalized by the number of items being ranked such that two identical rankings produce a correlation of 1 between a ranking and its perfect inverse is  X  1 . 0; and the expected correlation of two rank-ings chosen at random is 0 . 0. Typically, a value of greater than 0 although 0 . 9 represents a threshold researchers generally aim for. In this study, we primarily focus on Kendall X  X   X  , but also report Pearson X  X  r where appropriate. 6. Validation results The Kendall X  X   X  correlations between rankings produced by P are shown in Table 3 for all testsets. The Pearson X  X  r correlations between the two sets of scores are shown in Table 4. These two tables contain results of four separate P variants along two different parameters: scoring by term counts only vs. scoring by term idf , and microaveraging vs. macroaveraging. We present results for six distinct sets of runs: In
TREC 2003, the value of  X  , which controlled the relative importance of precision and recall,  X  was readjusted to three in TREC 2004. In our experiments with TREC 2003, we report figures for both values. In the answer key to the TREC 2005  X  X ther X  questions, we noticed that many nuggets were written with shorthand or contained typos, e.g., Bob Dole rcvd $45000 weekly while endorsement ran.
 CBS bdcsts event to 100 + countries.
 Purchasing 28 wud cost NZ $346 million (US).

From the assessors X  point of view, answer nuggets serve as notes to assist in scoring; they were not aware of  X  X he nuggets X  subsequent use as an answer key. In addition to using the verbatim answer key for the 2005  X  X ther X  questions, we also manually corrected it (without altering nugget content) and reran experiments. Finally, we also report results of experiments using the 2005 data that excludes a manually-generated run; this is denoted  X  X uto only X  in Tables 3 and 4 (more on this below).

Interestingly, scoring based on macroaveraged term counts outperformed any of the other variants X  X t does not appear that idf term weighting helps at all. Also surprising is the fact that correcting errors in the answer key had almost no effect on performance. This suggests that P OURPRE is relatively insensitive to errors in the reference nuggets. Although the best variant of P OURPRE , macroaveraging with term counts, predicts human preferences well for
TREC 2003 and TREC 2004, the correlations are much lower for the 2005 testset (although the values are still high enough for P OURPRE to be useful in guiding the development of future systems). In Section 7.4, we will examine this issue in more detail.
 How does P OURPRE stack up against using R OUGE 2 ,B LEU , and N answers to complex questions? For all three measures, the reference output consisted simply of all answer nuggets concatenated together. Results are shown in Table 5 for Kendall X  X 
Table 6 for Pearson X  X  r . We tested three variants of R OUGE removal nor stemming (base), with stopword removal but no stemming (stop), and with both stopword removal and stemming (s + s). For B LEU and N corrected nuggets, the P OURPRE condition with macroaveraging and term counts outperforms all variants of the other three metrics (based on Kendall X  X 
F -scores against P OURPRE scores (macro, count) and R OUGE stemming) for TREC 2003 (  X  = 5), TREC 2004 (  X  = 3), and TREC 2005 ( in Fig. 3. Corresponding graphs for other variants appear similar, and are not shown here. the rightmost edge (marked by two arrows): the highest-scoring run according to human assessors performs poorly with P OURPRE . There is a straightforward explanation: answers contained in that run were actually supplied by a trained librarian, in an attempt to better understand human performance on this particular task (Lin et al., 2005). The human run did not primarily consist of sentence extracts, but truly summarized information nuggets within the documents X  X nd thus was not fairly assessed by P OURPRE R
OUGE ) for extractive responses. Experiments with this run removed are denoted as  X  X uto only X  in Tables 3 through 6. It can be seen that removing this manual run increases both Kendall X  X   X  and Pearson X  X  r correlations.
 The effect of stemming on Kendall X  X   X  and Pearson X  X  r between P and official scores is shown in Table 7. Results from the same stemming experiment on the other P OURPRE variants are similarly inconclusive.
 Why does neither stemming nor idf -weighting improve the correlation of P explanations for both seemingly counter-intuitive results.

Stemming is primarily a recall-focused technique that handles morphological divergences between the reference nuggets and system outputs X  X t allows terms to match despite differ-ences in surface morphology; cf. (Kraaij and Pohlmann, 1996). Reference nuggets can be broadly classified into two categories: those that are essentially extracts of documents from the corpus, and more  X  X bstractive X  ones that summarize content found in multiple documents.
For extractive nuggets, stemming doesn X  X  make a difference because most TREC systems are extractive also, taking passages directly from relevant documents. Hence, terms already match without additional morphological processing. For  X  X bstractive X  nuggets, since they are often paraphrases of system responses, stemming doesn X  X  help bridge the gap between the semantically equivalent, but superficially different, forms anyway. Thus, stemming has a relatively minor overall effect on the performance of P OURPRE
A term-weighting scheme based on idf doesn X  X  affect performance much for similar rea-sons. The rationale behind idf is that more emphasis would be placed on content words, so that matching frequently-occurring words would be penalized by comparison. However, the reference nuggets in general have a high proportion of content words (since they must be sufficiently descriptive to assist in the evaluation process), which lessens the impact of idf -weighting.
 P 7. Understanding performance effects
Our experimental results show a strong correlation between official human-generated TREC scores and automatically-computed P OURPRE scores, indicating that a simple term-matching approach can capture various aspects of the evaluation process. Starting from a set of questions and a list of relevant nuggets, P OURPRE can accurately assess the performance of a complex question answering system without any human intervention.
 To better understand the limitations of our work, it is worth mentioning that P any automatic evaluation metric based on substring overlap, for that matter, cannot actually replace human judgments or capture all the fine-grained semantic considerations that go into matching system responses with reference output. We have merely demonstrated that it is possible to approximate human judgments in a manner that facilitates system development. It is certainly possible that at some future point in time, P the creation of more sophisticated systems until then. Similar limitations exist for all auto-matic evaluation metrics based solely on surface string matching. The machine translation community is contending with exactly such an issue: it is probable in the not X  X o X  X istant fu-ture that system performance will be indistinguishable from human performance according to B
LEU . At that point, the MT community would need to develop metrics that are better able to capture various linguistic phenomena (e.g., paraphrases). Meanwhile, however, B remain useful in guiding system development.

Although P OURPRE addresses the problem of automatically assessing system output, it does not assist in the creation of test collections themselves, which is a time-and resource-consuming endeavour. In particular, our metric depends on the existence of reference nuggets, (and performing additional topic research). Similar limitations apply to all metrics of this sort X  X  LEU requires human-generated reference translations and R generated summaries. Naturally, since the notion of relevance features prominently in the human judgments, there are theoretical limitations on the degree to which evaluations can be automated. However, it may be possible to develop systems that assist in the process of nugget creation; for example, the work on Basic Elements (Hovy et al., 2005) represents such an attempt.
 The results reported in the previous section leads to a number of interesting questions: Why exactly does P OURPRE work better than R OUGE ? How reliable are P OURPRE scores and what is the margin of error associated with them? How sensitive is P OURPRE to different precision X  X ecall tradeoff points? How does the vital/okay distinction affect the validity of P
In the following subsections, we describe a series of detailed experiments that address the above issues. The goal is to gain more insight into the nugget evaluation methodology and the automatic evaluation process. 7.1. Algorithm analysis and hybridized matching We believe that P OURPRE better correlates with official scores than R the length penalty, etc. Other than a higher correlation, P R
OUGE in that it provides a better diagnostic than a coarse-grained score, i.e., it can reveal why an answer received a particular score. With our measure, it is possible to reconstruct which answer nuggets were present in a response and which nuggets were omitted. This allows researchers to conduct failure analyses to identify opportunities for improvement.
Comparisons with the baselines of R OUGE and B LEU /N IST cerning the nature of complex question answering. The correlation between R official human-generated scores is much higher than that of B the task of answering complex questions is closer to document summarization than it is to machine translation, in that both are recall-oriented and focus primarily on content. Since al-most all question answering systems employ extractive techniques, fluency (i.e., precision) is not usually an issue. However, as systems begin to employ abstractive techniques to generate answers, evaluation metrics will need start assessing answer fluency; however, such systems appear to be just beyond the short-term horizon.

To better understand why P OURPRE outperforms R OUGE , we conceptually decomposed the evaluation of complex questions into two distinct stages. The first deals with the matching of the nuggets themselves, while the second deals with the manner in which the nugget match scores are combined to generate a final F -score for a particular question. Since P attempts to capture both stages of the evaluation process, it outperforms R not explicitly match individual nuggets, nor does it capture the TREC scoring model. In order to confirm this hypothesis, we experimented with a P that matches individual nuggets using R OUGE , but combines the match scores in a manner consistent with the TREC scoring model, like P OURPRE . Nugget recall and length allowance, the two variables that determine F -score, can be computed in the following manner:
Nugget recall. Instead of lumping all nuggets in a single  X  X ummary X , we treated each individual vital nugget as a distinct reference summary and scored system output using R OUGE in the following manner: Where A is the set of answer passages returned by a system. Since R between zero and one, this produced the equivalent of nugget recall, i.e., r
Length allowance. Length allowance is a product of a constant (100 non-whitespace characters) and the number of vital and okay nuggets matched. With R computed in the following manner (using all vital and okay nuggets as separate reference summaries):
Once again, A represents the set of all answer passages returned by a system. This produces the equivalent of r + a in Fig. 2.
 How does this P OURPRE  X  X  OUGE hybrid (which we term P OURPRE P
OURPRE alone? Results of correlation experiments are shown in Table 8 (for Kendall X  X  and Table 9 (for Pearson X  X  r ). For the R OUGE component inside P with three variants of R OUGE -1 recall scores: the default with neither stopword removal nor stemming (base), with stopword removal but no stemming (stop), and with both stopword removal and stemming (s + s). Correlation values are juxtaposed with the best P R OUGE values from Tables 5 and 6.
 Our experiments show that P OURPRE -R is comparable to the simpler version of P and in some cases, slightly better. This confirms our hypothesis regarding why P achieves higher correlations. Of the two stages in the evaluation process, it does not appear maximal unigram recall method employed by P OURPRE or the use of multiple summaries with R OUGE inside P OURPRE -R does not appear to have a large impact on performance. The contribution to correlation appears to be coming from explicit recognition of the nugget-based evaluation methodology. 7.2. Rank swap analysis
What do rank correlations mean in the context of real-world evaluations? An analysis of a common technique used in information retrieval research. A rank swap is said to have occurred if the relative ranking of two runs is different under different scoring conditions, i.e., according to one condition, system A is better than system B, but the opposite conclusion is drawn based on the other condition. Rank swaps are significant because they may prevent researchers from confidently drawing conclusions about the relative effectiveness of different techniques or systems. For example, observed rank swaps between official and P scores indicate cases where the automatic scoring metric is not accurately capturing human judgments.
 We analyzed rank swaps between official human-generated scores and P on testsets from TREC 2003 (  X  = 5), TREC 2004 (  X  = 3), and TREC 2005 ( the 2003 testset, we observed 81 rank swaps out of a total of 1431 pairwise comparisons for 54 runs (5.7% of all comparisons). For the 2004 testset, we observed 157 rank swaps out of a total of 1953 pairwise comparisons for 63 runs (8.0% of all comparisons). For the 2005 testset, we observed 357 rank swaps out of a total of 2556 pairwise comparisons for 72 runs (14% of all comparisons). Removing the human-generated run reduces the number of rank swaps to 320.

Histograms of these rank swaps, binned by the difference in official score between the two runs, are shown in Fig. 4 for the four different testsets described above. As can be seen, on the TREC 2003 (  X  = 5) testset, 48 rank swaps (59.3%) occurred when the difference similarly.

What is the meaning of these rank swap analyses? Since measurement error is an in-escapable fact of evaluation, we need not be concerned with rank swaps for which differences in the original score is less than the bounds of error for the evaluation (since one could not distinguish the two runs with confidence anyway). For TREC 2003, Voorhees (2003) calcu-lated this value to be approximately 0.1; that is, in order to conclude with 95% confidence that one run is better than another, an absolute F -score difference greater than 0.1 must be observed. As can be seen, all the rank swaps observed can be attributed to error inherent in the evaluation process. The histogram for TREC 2004 shows two rank swaps in range of [0.09, 0.10), but none above .1; the 2005 testset shows greater variability, with 23 rank swaps above that threshold (although many of which can be attributed to the top-scoring run, which was generated by a human).

From these results, we can conclude that evaluation of  X  X efinition X  questions is relatively coarse-grained. In general, P OURPRE accurately predicts human judgments on the quality of answers to complex questions, to the extent where the margin of error is approximately equal to the measurement uncertainty associated with the evaluation itself.
 7.3. Balancing precision and recall
The final F -score of an answer is the harmonic mean of nugget recall and nugget precision, where relative weight between the two components is controlled by the ously, a single measure of effectiveness is desirable from an evaluation point of view, but it is important to note that the specific setting of  X  operationalizes one particular tradeoff between recall and precision. For different applications, the relative importance of these two components may vary dramatically. The model of complex question answering implemented at TREC places significantly more emphasis on recall, with the years thereafter.

Recognizing that different parameter settings may be appropriate for other types of ap-plications, we ran our experiments with  X  as the independent variable. More specifically, we computed the Kendall X  X   X  correlation between P OURPRE scores (macroaveraging and term counts) and official scores under various settings of  X  . These results are shown in Fig. 5. We observe that an emphasis on recall yields better correlation between P official scores. A  X  value of one (equal precision and recall) results in a lower Kendall X  X  value than a  X  value of two, but higher values of  X  don X  X  seem to make much of a difference. In general, these results confirm that P OURPRE is primarily a recall-oriented metric, much like R
OUGE . This makes sense because unigram overlap measures the amount of shared content between system and reference nuggets. Precision enters into the calculation only as a length penalty. 7.4. Implications of the vital/okay distinction
The vital/okay distinction on nugget labels attempts to capture the intuition that some facts are more important than others. Nevertheless, this binary distinction may be problematic from an automatic evaluation point of view. In this section, we explore the implications of this evaluation aspect in greater detail.
 From Tables 3 and 4, it can be seen that the accuracy with which P judgments varies from testset to testset. In particular, correlation is highest for the TREC 2003 testset, slightly lower for the TREC 2004 testset, and noticeably lower for the TREC 2005 testset.

We believe that this can be attributed to the distribution of vital and okay nuggets in the three testsets. Table 10 shows statistics about the number of questions that have only one or two vital nuggets. Compared to the size of the testset, these numbers are relatively large. As a concrete example,  X  X 16 X  is the target for question 71.7 from TREC 2005. The only vital nugget is  X  X irst F16s built in 1974 X . With only one vital nugget, a system X  X  score on that particular question would be zero unless it returned the sole vital nugget, since okay nuggets figured only in the precision calculation X  X n other words, a system had only one chance dictated by chance as much as by actual system performance. The same effect is also present for questions with only two vital nuggets, although to a lesser degree. This phenomenon is further amplified by the final F -score X  X  emphasis on recall over precision. As a result, P OURPRE is not able predict human scores on the TREC 2005 testset as well.

To test this hypothesis, we created a variant answer key in which all nuggets were consid-ered vital and reran our experiments. Because assessors recorded nugget matches independent of vital/okay status, it is possible to recalculate scores based on different conditions. These results (Kendall X  X   X  ) are shown in Table 11; for brevity, only the best P and R OUGE (stopword removal, no stemming) configurations are shown. Indeed, higher cor-relation is observed when all nuggets are considered vital, with P R
OUGE . Nevertheless, there is still a gap in performance between the three different testset, which can be attributed to the quality of the different reference nuggets, as discussed in Section 6.

The vital/okay distinction aims to capture differences in assessors X  notions of relevance. As with many other information retrieval tasks, legitimate differences in opinion about relevance are an inescapable fact of evaluating definition/other questions X  X ystems are designed to satisfy real-world information needs, and users inevitably disagree on which nuggets are important or relevant. These disagreements manifest as scoring variations in an evaluation setting. The important issue, however, is the degree to which variations in judgments affect conclude that one system is  X  X etter X  than another (across a broad sampling of users)? For the ad hoc document retrieval task, research has shown that system rankings are stable with respect to disagreements about document relevance (Voorhees, 2000). In the remainder of this section, we explore the effect of judgment variability on the stability and reliability of TREC nugget-based evaluation methodology.

The vital/okay distinction on nuggets is one major source of differences in opinion, as has been pointed out previously (Hildebrandt et al., 2004). In the Cassini space probe question, for example, we disagree with the assessors X  assignment in many cases. More importantly, however, there does not appear to be any operationalizable rules for classifying nuggets as either vital or okay. Consider some relevant nuggets for the question  X  X hat is Bausch &amp;
Lomb? X : world X  X  largest eye care company about 12000 employees in 50 countries approx. $1.8 billion annual revenue based in Rochester, New York means that the location of Bausch &amp; Lomb X  X  headquarters is considered less important than employee count and revenue. Such mysteries are common within answer nuggets across all examined testsets, and the vital/okay distinction does not seem to follow from any deducible principle. As a result, there is little hope for systems to learn and exploit this difference.
Without any guiding principles, how can we expect our systems to focus more on returning vital nuggets?
How do differences in opinion about vital/okay nuggets impact the stability of system rankings? To answer this question, we measured the Kendall X  X  separate variants were considered: all nuggets considered vital vital/okay flipped (all vital nuggets become okay, and all okay nuggets become vital) randomly assigned vital/okay labels
Results are shown in Table 12. Note that this experiment was conducted with the manually-evaluated system responses, not our P OURPRE metric. For the last condition, we conducted one thousand random trials, taking into consideration the original distribution of the vital and okay nuggets for each question using a simplified version of the Metropolis-Hastings algorithm (Chib and Greenberg, 1995); the 95% confidence intervals are reported.
These results suggest that system rankings are sensitive to assessors X  opinion about what constitutes a vital or okay nugget. In general, the Kendall X  X  are lower than values computed from corresponding experiments in ad hoc document retrieval (Voorhees, 2000).

It appears that differences between P OURPRE and the official scores are about the same as (or in some cases, smaller than) differences between the official scores and scores based on variant answer keys (with the exception of  X  X verything vital X ). This means that further refinement of the P OURPRE metric to increase correlation with human-generated scores may not be particularly meaningful; it might essentially amount to overtraining on the whims of a particular human assessor. We believe that sources of judgment variability and techniques for managing it represent important areas for future study. Recently, we have looked into techniques for combining judgments from multiple assessors in order to obtain a more refined estimate of nugget importance (Lin and Demner-Fushman, 2006). This was inspired by the pyramid scheme (Nenkova and Passonneau, 2004), related work from the multi-document summarization literature. 8. Conclusion
The nugget-based evaluation methodology described in this paper is broadly applicable to many types of complex questions X  X ot only the definition/other questions specifically ex-plored here, but also relationship and opinion questions that are beginning to benefit from large-scale formal evaluations at TREC. Our work shows that P automatically evaluate answers to complex questions, and that rankings generated by this method correlate well with human-generated rankings. We hope that P plish for complex question answering what B LEU has done for machine translation, and R
OUGE for document summarization: allow laboratory experiments to be conducted with rapid turnaround. A much shorter experimental cycle will allow researchers to explore dif-ferent techniques and receive immediate feedback on their effectiveness. Hopefully, this will translate into rapid progress in the state of the art. 5 References
