 REGULAR PAPER Jaideep Vaidya  X  Hwanjo Yu  X  Xiaoqian Jiang Abstract Traditional Data Mining and Knowledge Discovery algorithms assume free access to data, either at a centralized location or in federated form. Increas-ingly, privacy and security concerns restrict this access, thus derailing data mining projects. What is required is distributed knowledge discovery that is sensitive to this problem. The key is to obtain valid results, while providing guarantees on the nondisclosure of data. Support vector machine classification is one of the most widely used classification methodologies in data mining and machine learning. It is based on solid theoretical foundations and has wide practical application. This paper proposes a privacy-preserving solution for support vector machine (SVM) classification, PP-SVM for short. Our solution constructs the global SVM classifi-cation model from data distributed at multiple parties, without disclosing the data of each party to others. Solutions are sketched out for data that is vertically, hori-zontally, or even arbitrarily partitioned. We quantify the security and efficiency of the proposed method, and highlight future challenges.
 Keywords Support vector machine  X  Classification  X  Privacy  X  Security 1 Introduction Data Mining has many applications in the real world. One of the most important and widely found problem is that of classification. For example, fraud detection can be posed as a classification problem. Specifically, take the case of identify-ing fraudulent credit card transactions. Banks collect transactional information for credit card customers. Due to the growing threat of identity theft, credit card loss, etc., identifying fraudulent transactions can lead to annual savings of billions of dollars. Deciding whether a particular transactions is true or false is a classification problem. Many such problems abound in various diverse domains.
 cation (i.e., on a data warehouse). The accuracy of a classifier usually improves with more training data. Data collected from different sites is especially useful, since it provides a better estimation of the population than the data collected at a single site. However, privacy and security concerns restrict the free sharing of data. There are both legal and commercial reasons to not share data. For exam-ple, HIPAA laws [ 32] require that medical data not be released without appro-priate anonymization. Similar constraints arise in many applications; European Community legal restrictions apply to disclosure of any individual data [ 8]. In commercial terms, data is often a valuable business asset. For example, complete manufacturing processes are trade secrets (although individual techniques may be commonly known). Thus, it is increasingly important to enable privacy-preserving distributed mining of information. However, central accumulation of summaries or obfuscated models might be considered reasonable as long as the original data is not revealed.
 sification methodology in data mining and machine learning. It provides salient properties such as the margin maximization and nonlinear classification via ker-nel tricks and has proven to be effective in many real-world applications [ 6, 7, 41]. We propose a privacy-preserving SVM classification solution, PP-SVM for short, which constructs the global SVM classification model from the data distributed at multiple parties. The data may be partitioned horizontally, vertically, or in an arbitrary manner between the parties. The data of each party is kept private, while the final model is constructed at an independent site. This independent site then performs classification of new instances.
 clearing house for a consortium of banks. The different banks collect data for their customers. The features collected, such as age, gender, balance, average monthly deposit, etc., are the same for all banks. Thus, the data is horizontally distributed. The clearing house is an independent entity, unrelated to any of the banks. The classification model is constucted at the clearing house while preserving the pri-vacy of the individual data from each of the banks. When a bank has a new in-house to classify just this instance. The clearing house learns nothing. This would allow all of the banks to leverage the global data without compromising on privacy at all.
 sary information for the learning algorithm and fuses information about the data and the kernel. As such, the kernel matrix plays a role as the intermediate pro-file that does not disclose any information on local data but can generate the global model. To construct the global SVM model without disclosing the data, our method securely computes the kernel matrix from the distributed data; The global SVM model can then be generated from the kernel matrix. We assume that knowledge of the kernel matrix is necessary to efficiently perform classification. Later, we do discuss exactly what information is leaked by the kernel matrix itself, and ways to circumvent this restriction.
 nels can be computed from the gram matrix, both linear and nonlinear. The key challenge here is the issue of scalability: To build the gram matrix, we need to compute the dot products of every data pair . Thus, the communication cost is key. Our method generates the same SVM classification model as when the data is cen-tralized. We quantify the security and efficiency of our algorithm. We also assume that each party does not collude and does follow the proposed protocol correctly. While the assumption of noncollusion is quite strict, it enables an efficient algo-rithm. We later discuss this issue in more detail.
 Vector Machines. In Sect. 3, we develop our PP-SVM technique for vertically par-titioned data. In Sect. 4, we briefly sketch the solution for horizontally partitioned data. Finally, in Sect. 5, we develop the solution for arbitrarily partitioned data. Related work is discussed in Sect. 6 and Sect. 7 concludes the paper. 2 SVM overview vector machines.
 Notation All vectors are column vectors unless transposed to a row vector by a prime su-perscript . The scalar (inner) product of two vectors x and y in the n -dimensional real space R n is denoted by x y and the 2-norm of x is denoted by || x || .An m  X  n matrix A represents m data points in a n -dimensional input space. An m  X  m diag-onal matrix D contains the corresponding labels (i.e., + 1or  X  1) of the data points in
A . (A class label D A .) A column vector of ones of arbitrary dimension is denoted by e . The identity matrix of arbitrary dimension is denoted by I . 2.1 SVM overview First, consider a linear binary classification task, as depicted in Fig. 1. For this problem, SVM finds the separating hyperplane ( w  X  x =  X  ) that maximizes the margin , denoting the distance between the hyperplane and closest data points (i.e., support vectors). In practice, we use the  X  X oft X  margin to deal with noise, in which the distance from the boundary to each support vector could be different. The margin while minimizing the error, the standard SVM solution is formulated into the following primal program [ 12 , 41]: which minimizes the reciprocal of the margin (i.e., w w )andtheerror(i.e., e y ). By having the slack variable y in the constraint ( 2), SVM allows error or the soft margin. The slack or error is minimized in the objective function ( 1) and it will be larger than zero when the point is on the wrong side or within the margin area. The soft margin parameter  X  (a user parameter) is tuned to balance the margin size and the error. The weight vector w and the bias  X  will be computed by this optimization problem. Once w and  X  are computed, we can determine the class of a new data object x by f ( x ) = w x  X   X  , where the class is positive if f ( x )&gt; 0, or else negative .
 be able to apply the kernel trick, we transform the primal problem to the following dual problem by applying the Largrange multipliers: data vector x i . The coefficients  X  are to be computed from this dual problem. An m  X  m matrix Q is computed by the scalar product of every data pair, i.e., Q K ( x the data vectors { x i } such that the corresponding coefficients  X  i &gt; 0. The weight vector w =  X  i d i x i and thus the classification function f ( x ) =  X  i d i x i  X  x  X   X  for linear SVM. For nonlinear SVMs, f ( x ) =  X  i d i K ( x i , x )  X   X  , where we can further details on SVM. 3 Privacy-preserving SVM over vertically partitioned data When data is vertically partitioned, parties collect different information about the same set of entities. For instance, a bank, health insurance company and auto insurance company collect different information about the same people. A bank has customer information like average monthly deposit, account balance, etc. The health insurance company has access to medical information and other policy in-formation. The car insurance company has access to information such as car type, accident claims, etc. Together, they might evaluate if the person is a credit risk for life insurance.
 which are computed from the dual problem in Sect. 2.
 SVM model (i.e., computed only over local data), because the global SVM model G cannot be built only from local SVM models; The globally optimal coefficients (computed by the dual problem) will be different from the locally optimal coeffi-cients computed on local data. Since each party has the data of an attribute subset, the dual problem on the attribute subset will not generate the globally optimal coefficients.
 K ( x diagonal matrix D for d i is given as class labels, thus we only need to compute the the global matrix K can be directly computed from local matrices because K is a gram matrix and a gram matrix can be merged from gram matrices of vertically partitioned data, as Lemma 1 proves.
 Lemma 1 Suppose the m  X  n data matrix A is vertically partitioned into A 1 and A 2 as Fig. 2 illustrates. Let K 1 and K 2 be the m  X  m gram matrices of matrices A 1 and A 2 , respectively. That is, K 1 = A 1 A 1 and K 2 = A 2 A 2 . Then, K ,the gram matrix of A , can be computed as follows: Proof An ( i , j ) th element of K is x i  X  x j ,where x i and x j are i th and j th data vectors in A .Let x 1 i and x 2 i be vertically partitioned vectors of x i , which are the parts from A 1 and A 2 , respectively. Then, From Eq. ( 6), each element in K is equal to the sum of the elements in K 1 and K 2 . Thus K = K 1 + K 2 .
 gram matrix which is the kernel matrix K for linear kernel. Some popular non-linear kernel matrices can also be computed from the gram matrix: The poly-nomial kernel is represented by a dot product of data vectors (i.e., K ( x i , x j ) = ( x K ( x matrix from each party is sufficient to construct the global kernel matrix K for nonlinear kernels such as polynomial and RBF that can be represented by dot products.
 can run a quadratic programming solver to compute the global SVM model G , which will be the same for every party. If the global gram matrix is computed at an independent site, that site then computes the global model. 3.1 Secure merge of local models In order to securely merge the local m  X  m gram matrices, a secure addition mechanism for m  X  m matrices is required. For k  X  3 parties, we present such a method based on simple secure addition of scalars.
 from individual sites under the assumption that there are at least three parties and the parties do not collude. We then extend the method so as to seamlessly merge the local models with high efficiency and privacy. 3.1.1 Secure sum of integers Formally, we assume k  X  3 parties, P 0 ,..., P k  X  1 , with party P i holding value v i . Together, they want to compute the sum v = k  X  1 i = 0 v i . Assume that the sum v is known to lie in a field F .
 on a suitable order and no third party can be found, then a protocol developed by Sweeney and Shamos can be used [ 33] to fix upon a random ordering. The protocol developed by Sweeney and Shamos is quite efficient and requires only O ( k ) communication. For this paper, to simplify the presentation, without loss of generality, we assume that this order is the canonical order P 0 ,..., P k  X  1 .In general, any order can be decided on. The protocol proceeds as follows: this to its local value v 0 , and sends the sum R + v 0 mod | F | to site P 1 .Forthe remaining sites P i , i = 1 ,..., k  X  1, the algorithm proceeds as follows: P i receives P i then computes and passes it to site P i + 1 ( mod k ) . Finally, P 0 , subtracts R from the final message it gets (i.e., adds  X  R ( mod | F | ) ) to compute the actual result.
 evaluate the security of the protocol, it is necessary to have a definition of what is meant by security. The area of Secure Multiparty Computation (SMC) provides a theoretical framework for defining and evaluating secure computation. This pro-tocol can be proven to be completely secure under our assumptions in the SMC framework. A complete proof of security is presented in our technical report [ 47]. 3.1.2 Secure sum of matrices We can extend the secure addition of scalars to securely adding matrices. The key idea is as follows. Suppose a master party wants to merge (i.e., add) its local matrix with those in other slave parties. We assume that the parties have arranged themselves in some sequence and the master initiates the protocol. 1. The master party creates a random matrix of the same size as its local matrix. 2. The master party merges (adds) the random matrix with its local matrix, sends 3. Each slave party, receives the perturbed matrix, merges it with its local matrix 4. The master subtracts the random matrix from the received matrix, which re-complement. This secure addition mechanism is proven to be secure and efficient [47 , 49]. The extra computation required by the first party is the generation of the random matrix and the final subtraction. In terms of communication overhead, k rounds are required for every party to acquire the summed matrix, where k is the number of participating parties. One problem with the matrix summation method is that it is vulnerable to collusion. The parties preceding and following a party, can collude to recover its local matrix. However, the technique can easily be made collusion resistant to q parties by splitting up the local matrices into q random parts and carrying out the addition protocol q times. The sum of the final matrices from all q rounds gives the real global matrix. As long as the parties are ordered differently in each run, recovery of a local matrix is only possible if collusion occurs in all q rounds. Further details can be found in [ 47]. 3.2 Security Our method preserves  X  X ata privacy X , since only the original party gets to exactly see the data. The local model is directly computed from the local data. However, to ensure  X  X odel privacy, X  we need at least three participating parties. Each party gets the final global model, which is simply the sum of local models. Thus, with only two parties participating, the other party X  X  local matrix could be found simply by subtracting the local model from the global model. What is revealed, is the sum of the local models of the other parties. Since the SVM requires knowing the global matrix, this is always possible from the final result as well, and so is unavoidable.
 puted over the attributes of other parties. In general, the number and type of at-tributes of the other parties are still assumed to be unknown. As such, the summed matrix does not disclose any attribute values. If the exact number and types of attributes of the other parties are known, a number of quadratic equations will be revealed in the attribute values; Every cell of the gram matrix corresponds to a dot product X  X hus the quadratic equation. Since the matrix is symmetric, thereareatotalof m ( m + 1 )/ 2 distinct equations (where m is the number of data objects). If the number of total variables (i.e., the sum of all the attributes of other parties) is larger than m ( m + 1 )/ 2, it is impossible to recover the exact attribute values. Knowing that the matrix is symmetric and positive semidefinite does not disclose further information. While this does reveal more information than strictly necessary, this is a tradeoff in the favor of efficiency. If complete se-curity is required, the summed matrix could be kept randomly split between two of the parties, and an oblivious protocol run to compute the global model using the generic circuit evaluation technique developed for secure multiparty compu-tation [ 14, 45]. Otherwise, if a third party holds the final model and is asked to do new classfications, all leakage is avoided (since the third party has no extra information). 3.3 Testing Once the global SVM model is constructed at each party, to test a new data object using the model, we need to compute dot products between the support vectors and the new data object. The testing function is formulated as f ( x ) = After training, each party holds coefficients  X  , label d ,bias  X  , and partial attributes of the support vectors and the testing data object. Thus, from Eq. ( 6), to securely compute the dot products and thus the kernel functions between the support vec-tors and the testing data object, we need to run the secure addition of vectors, such that each element of the vector is a dot product between a partial support vector and the testing data object. For instance, when each party holds m partial support vectors, the vector needs to be securely merged (or added) will have m elements. Its security proof will be the same as that for the secure addition of matrices, as a column vector is a matrix of one column. Accordingly, to test a new data object, one round of communication is needed. 3.4 Experimental evaluation The goal of the experiments is simply to demonstrate the scalability of our PP-SVMV. The accuracy will be exactly the same as that of SVM when the data is centralized. We revised the sequential minimal optimization (SMO) source 1 to implement the PP-SVMV. We used the Tic-Tac-Toe data set included in the SMO package for our experiment. We sampled around 958 data objects ( m )and extracted around 27 features ( n ). PP-SVMV generates above 99% with an RBF kernel that is the same as that of the original SVM when the data is centralized. ipating parties, we vary the number of parties from three to 10. We divide the 27 features about equally between the participating parties. For instance, when 10 parties participate, three parties have two features, and the other seven parties have three features. Figure 3 shows results of our experiments: The total training time (including the parallel local computations) hardly changes; SVM is sensitive to the number of data objects more than the features, and the change in the number of features is not visibly influential to the total training time. The difference of the communication time is also not visible due to the dominant computation time. The results are averaged over 10 runs. 4 Privacy-preserving SVM over horizontally partitioned data With horizontal partitioning of data, each party collects the same features of in-formation for different data objects. Numerous practical problems fall under this data model. For instance, different banks collect data for their customers. The fea-tures collected, such as age, gender, balance, average monthly deposit, etc., are the same for all banks.
 partitioned and distributed over multiple parties; the corresponding class labels in D are also distributed likewise. ing the data of each party to the others, we securely compute the kernel ma-trix K where K ij = K ( x i , x j ) : First, to build an SVM model, we need to solve the dual problem in Eqs. ( 3)and( 4), which requires the m  X  m matrix Q = K ( x of each class in each party. However, it does not reveal any information on the data itself as long as the kernel matrix K = K ( x i , x j ) is securely computed. Thus, we assume that the class labels are shared, and we focus on the secure compu-tation of the kernel matrix. To securely compute the kernel matrix, we compute the gram matrix G securely where G ij = x i  X  x j . As stated before, the non-linear kernel matrices like polynomial or RBF can be computed from the gram matrix G .
 uct computation method. However, most existing secure dot product computation methods [ 9, 16, 29, 34 ] are inefficient or insecure to be applied for computing the gram matrix. The scalar product protocol of Goethals et al. [ 13] is completely se-cure and can indeed be used. One simply needs to run the protocol for each data pair to compute each scalar product. Indeed, a modified version of this is used to compute the gram matrix from arbitrarily partitioned data, as described in Sect. 5 later.
 communication-efficient alternative is possible. This is based on using a secure protocol [ 39] for computing the size of the intersection of local sets. tation of a binary feature vector into an ordered set such that the elements a 10-dimensional space. Then, they will be represented as ordered sets x 1 = { 1 , 3 , 4 , 8 , 10 } and x tors becomes equivalent to the size of the set intersection between the two sets. That is, x 1  X  x 2 =| x 1  X  x 2 |= 2. On this revised representation of data, we se-curely compute the set intersection cardinality using a commutative one-way hash function. The key idea here is that the commutative property guarantees that the order of hashing does not matter. Thus, each party hashes its set of values and passes them on to the next party to hash and send forward. Now, the same original entries will overall hash to the same values, while the one-way property guaran-tees that a hashed value cannot be used to infer the original. More details on this multiple parties (e.g., more than two parties) are involved: Unlike computing the intersection of all parties presented in [ 39], we need to perform the intersection between all pairs of the parties, because the gram matrix contains the dot product of every data pair . With k participating parties, the communication cost becomes O ( putation. Yu et al. [ 46] presents the complete details for the scalable and secure protocol that computes the kernel matrix in linear time, without disclosing any data. 5 Privacy-preserving SVM over arbitrarily partitioned data Both vertical partitioning and horizontal partitioning are special cases of arbitrary data partitioning. In arbitrary data partitioning, different parties may own arbitrary subsets of the data. For example, Fig. 4 depicts an example where nine parties jointly hold the complete data set (data set A i is held by party P i ). eral solution for this case would work for any data partitioning. Thus, such a gen-eral solution will be of great use. We now show how to generate the gram matrix when the data is arbitrarily partitioned. The key idea is to generate the gram ma-trix, one element at a time, using a modified version of the scalar product protocol of Goethals et al. [ 13], based on homomorphic encryption. Homomorphic Encryp-tion is a semantically-secure public-key encryption which, in addition to standard guarantees has the additional property that given any two encryptions E ( A ) and E ( B ) , there exists an encryption E ( A  X  B ) such that E ( A )  X  E ( B ) = E ( A  X  B ) , where  X  is either addition or multiplication (in some abelian group). The cryp-tosystems mentioned earlier are additively homomorphic (thus the operation  X  denotes addition). Using such a system, it is quite simple to create a scalar prod-times). Since each vector is arbitrarily partitioned, the party owning each x i en-crypts and sends it to the party owning the corresponding y i . This party can now use the additive homomorphic property to compute the product in encrypted form. Again, using the additive property the sum of all the products can be computed to compute the dot product. The specific details are given in Algorithm 1. There are several homorphic encryption systems such as the Goldwasser X  X icali cryptosys-tem [ 5], the Benaloh cryptosystem [ 4], the Naccache X  X tern cryptosystem[ 25], the Paillier cryptosystem [ 28], and the Okamoto X  X chiyama cryptosystem [ 26], any of which can be used in practice. 5.1 Security Theorem 1 In the absence of collusion, Algorithm 1 is secure in the semihonest model X  X nly Q learns the gram matrix while the other parties learn nothing. Algorithm 1 Computing the gram matrix from arbitrarily partitioned data Proof The first message sent by Q is simply the encryption key chosen X  X his can be simulated by each party simply by choosing a random key. After this, every message received by any party is encrypted by ek . Since the cryptosys-tem used is semantically secure, and the decryption key is known only to Q , each message can be simulated using a random number. Finally, Q gets to know the gram matrix as the final result, so it can easily simulate the messages sent to it.
 the other party X  X  input. Though the assumption of noncollusion is often implic-itly made in the real world, this is a serious problem. Even if it were possible to find trustworthy real-world relationships, technical solutions would be more satisfying. We now present a modification to the algorithm to make it collu-sion resistant. In lines 8 X 10, instead of using the public key sent by Q , P a creates its own keypair and uses it to encrypt. Now it is no longer possible ing back the encryption of ( A ik  X  A jk ) ,ashareofitissentback.Thisatthe end of line 10, after decryption, both P a and P b have shares of ( A ik  X  A jk ) . Now, all the parties can together run the collusion-resistant secure sum pro-tocol to finally compute the dot product. In this way, each party is protected against collusion to the extent of the collusion resistance of the secure sum protocol. 5.2 Computation and communication cost The computation and communication cost of this algorithm is quite reasonable. We next sketch out the worst case estimate of time required, when the data is completely fragmented between mn parties, each owning exactly one element of one point. Here, each scalar product requires n encryptions, n exponentiations, n multiplications, and 1 decryption. There are a total of m 2 scalar products. Thus, a total of m 2 n encryptions, m 2 n exponentiations, m 2 n multiplications, and m 2 decryptions are required to compute the gram matrix. The cost of the exponenti-ations dominates all of the other costs. One exponentiation requires 0 . 0015 s for 512 bit encryption and 0 . 0104 s for 1024 bit encryption on an Intel Pentium Dual 830, 3.00 GHZ with 2 GB RAM. Table 1 gives the estimated approximate cost for several different values of m and n . Again, note that these are the worst case the time taken will be orders of magnitude lower depending on how many scalar products or elements of it can be done locally. In the case of pure vertical par-titioning, we have already seen that a dataset of about 1000 points with about 30 different features, requires about 10 s. Similarly, with pure horizontal parti-tioning, the efficient protocol in Yu et al. [ 46] only requires about 80 s. While the arbitrary partitioning protocol will correspondingly take more time, it is still significantly less than the maximum times given later (for completely partitioned data). 6 Related work Recently, there has been significant interest in the area of Privacy-Preserving Data Mining. We briefly cover some of the relevant work. Work in PPDM has followed two major directions X  X he randomization/perturbation approach and the crypto-graphic approach.
 fore mining is done. For example, if we add a random number chosen from a Gaussian distribution to the real data value, the data miner no longer knows the exact value. However, important statistics on the collection (e.g., average) will be preserved. Special techniques are used to reconstruct the original distri-bution (not the actual data values). The mining algorithm is modified to work while taking this into consideration. The seminal paper by Agrawal and Srikant [3] introduced this notion to the data mining community. Several different algo-rithms are proposed to reconstruct distributions and learn a decision tree classi-fier from the perturbed data. A convergence result was proved in [ 1]forare-finement of this algorithm. There has been work using the same approach for association rule mining [ 11, 30, 50]. Zhu and Lei [ 51] study the problem of opti-mal randomization for privacy-preserving data mining and demonstrate the con-struction of optimal randomization schemes for density estimation. This model works in the  X  X ata warehouse X  model of data mining, but trades off privacy for decomposition method for data distortion to preserve privacy. However, several problems have been pointed out with the privacy inherent in such an approach [15 , 19, 20, 24].
 vacy was first utilized for the construction of decision trees [ 23]. There has since been significant work on many techniques in data mining. Specifically, secure methods have been proposed for association rule mining [ 18, 39], clus-regression [ 21, 31]. Our work follows the same approach. A good overview of prior work in this area can be found in [ 40, 42 ]. Recently, some alter-native techniques such as condensation [ 2] and transformation [ 27]havealso been proposed, though the security properties of these need to be carefully investigated.
 cure Multiparty Computation. Yao first postulated the two-party comparison problem (Yao X  X  Millionaire Protocol) and developed a provably secure solu-tion [ 45]. This was extended to multiparty computations by Goldreich et al. [14 ]. The key result in this field is that any function can be computed se-curely. Thus, the generic circuit evaluation technique can be used to solve our current problem. However, the key issue in privacy-preserving data mining is one of efficiency. The generic technique is simply not efficient enough for large quantities of data. This paper proposes an efficient technique to solve the problem.
 izontally partitioned data. Since their method is based on the trick of the proximal SVM [ 12 ], it is limited to linear classification. Yu et al. [ 46] proposes a technique for securely computing nonlinear kernels as well. Reference [ 48] tackles the prob-lem when the data is vertically partitioned. In this paper, we integrate the known approaches and propose secure techniques that work when data is arbitrarily par-titioned as well. 7Conclusion We propose a scalable solution for privacy-preserving SVM classification based on gram matrix computation. Assuming an independent untrusted third party, we show how to securely compute the global SVM model, without disclosing any extra information about the data of each party to others. Future work may address the idea of efficiently achieving complete security by keeping the global model split between parties as well. References Author Biographies
