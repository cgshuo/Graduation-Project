 Question answering aims at finding exact answers to a user X  X  natural language question from a large collection of documents. Most QA systems com-bine information retrieval with extraction techniques to identify a set of likely candidates and then uti-lize some selection strategy to generate the final answers (Prager et al., 2000; Clarke et al., 2001; Harabagiu et al., 2001). Since answer extractors may be based on imprecise empirical methods, the selection process can be very challenging, as it often entails identifying correct answer(s) amongst many incorrect ones.
 Figure 1: A traditional QA pipeline architecture
Figure 1 shows a traditional QA architecture with an example question. Given the question  X  Which city in China has the largest number of foreign fi-nancial companies?  X , the answer extraction com-ponent produces a ranked list of five answer can-didates. Due to imprecision in answer extraction, an incorrect answer ( X  X eijing X ) was ranked at the top position. The correct answer ( X  X hanghai X ) was extracted from two documents with different confi-dence scores and ranked at the third and the fifth po-sitions. In order to select  X  X hanghai X  as the final answer, we need to address two issues:  X  Answer Validation . How do we identify correct  X  Answer Similarity . How do we exploit evi-
To address the first issue, several answer selec-tion approaches have used semantic resources. One of the most common approaches relies on Word-Net, CYC and gazetteers for answer validation or answer reranking; answer candidates are pruned or discounted if they are not found within a re-source X  X  hierarchy corresponding to the expected an-swer type (Xu et al., 2003; Moldovan et al., 2003; Prager et al., 2004). In addition, the Web has been used for answer reranking by exploiting search en-gine results produced by queries containing the an-swer candidate and question keywords (Magnini et al., 2002), and Wikipedia X  X  structured information has been used for answer type checking (Buscaldi and Rosso, 2006).

To use more than one resource for answer type checking of location questions, Schlobach et al. (2004) combined WordNet with geographi-cal databases. However, in their experiments the combination actually hurt performance because of the increased semantic ambiguity that accompanies broader coverage of location names. This demon-strates that the method used to combine potential answers may matter as much as the choice of re-sources.

To address the second issue we must determine how to detect and exploit answer similarity. As an-swer candidates are extracted from different docu-ments, they may contain identical, similar or com-plementary text snippets. For example, the United States may be represented by the strings  X  X .S. X ,  X  X nited States X  or  X  X SA X  in different documents. It is important to detect this type of similarity and ex-ploit it to boost answer confidence, especially for list questions that require a set of unique answers. One approach is to incorporate answer clustering (Kwok et al., 2001; Nyberg et al., 2003; Jijkoun et al., 2006). For example, we might merge  X  X pril 1912 X  and  X 14 Apr 1912 X  into a cluster and then choose one answer as the cluster head. However, clustering raises new issues: how to choose the cluster head and how to calculate the scores of the clustered an-swers.

Although many QA systems individually address these issues in answer selection, there has been lit-tle research on generating a generalized probabilistic framework that allows any validation and similarity features to be easily incorporated.

In this paper we describe a probabilistic answer selection framework to address the two issues. The framework uses logistic regression to estimate the probability that an answer candidate is correct given multiple answer validation features and answer sim-ilarity features. Experimental results on TREC factoid questions (Voorhees, 2004) show that our framework significantly improved answer selection performance for four different extraction techniques, when compared to default selection using the indi-vidual candidate scores produced by each extractor.
This paper is organized as follows: Section 2 de-scribes our answer selection framework and Section 3 lists the features that generate similarity and va-lidity scores for factoid questions. In Section 4, we describe the experimental methodology and the re-sults. Section 5 describes how we intend to extend our framework to handle complex questions. Finally Section 6 concludes with suggestions for future re-search. Answer validation is based on an estimate of the probability P ( correct ( A question and A tion. Answer similarity is is based on an estimate of the probability P ( correct ( A is similar to A ence answer selection performance, it is important to combine them in a unified framework and es-timate the probability of an answer candidate as: P ( correct ( A i ) | Q, A 1 , ..., A n ) .

In this paper, we propose a proba-bilistic framework that directly estimates P ( correct ( A i ) | Q, A 1 , ..., A n ) using multiple answer validation features and answer similarity features. The framework was implemented with logistic regression, which is a statistical machine learning technique used to predict the probability of a binary variable from input variables. Logistic
K 1 ( A i ) , sim 1 ( A i ) , ..., sim K 2 ( A i )) P ) +
P ( A i , A j ) . ( A ) , ..., val K 1 ( A i ) , sim 1 ( A i ) , ..., sim K 2 ( A regression has been successfully employed in many applications including multilingual document merg-ing (Si and Callan, 2005). In our previous work (Ko et al., 2006), we showed that logistic regression performed well in merging three resources to vali-date answers to location and proper name questions. We extended this approach to combine multiple similarity features with multiple answer validation features. The extended framework estimates the probability that an answer candidate is correct given the degree of answer correctness and the amount of supporting evidence provided in a set of answer candidates (Equation 1).

In Equation 1, each val used to produce an answer validity score for an an-swer candidate A ity function used to calculate an answer similarity between A answer validation and answer similarity features, re-spectively. N is the number of answer candidates.
To incorporate multiple similarity features, each sim metric. For example, if Levenshtein distance is used as one similarity metric, sim summing N-1 Levenshtein distances between one answer candidate and all other candidates. As some string similarity metrics (e.g. Levenshtein distance) produce a number between 0 and 1 (where 1 means two strings are identical and 0 means they are differ-ent), similarity scores less than some threshold value are ignored.

The parameters  X ,  X ,  X  were estimated from train-ing data by maximizing the log likelihood as shown in Equation 2, where R is the number of training questions and N for each question Q used the Quasi-Newton algorithm (Minka, 2003).
To select correct answers, the initial answer candi-date set is reranked according to the estimated prob-ability of each candidate. For factoid questions, the top answer is selected as the final answer to the ques-tion. As logistic regression can be used for binary classification with a default threshold of 0.5, we can also use the framework to classify incorrect answers: if the probability of an answer candidate is lower than 0.5, it is considered to be a wrong answer and is filtered out of the answer list. This is useful in deciding whether or not a valid answer exists in the corpus, an important aspect of the TREC QA evalu-ation (Voorhees, 2004). This section details the features used to generate an-swer validity scores and answer similarity scores for our answer selection framework. 3.1 Answer Validation Features Each answer validation feature produces a validity score which predicts whether or not an answer can-didate is a correct answer for the question. This task can be done by exploiting external QA resources such as the Web, databases, and ontologies. For fac-toid questions, we used gazetteers and WordNet in a knowledge-based approach; we also used Wikipedia and Google in a data-driven approach. 3.1.1 Knowledge-based Features In order to generate answer validity scores using gazetteers and WordNet, we reused the algorithms described in our previous work (Ko et al., 2006). Gazetteers : Gazetteers provide geographic information, which allows us to identify strings as instances of countries, their cities, continents, capitals, etc. For answer selec-tion, we used three gazetteer resources: the Tipster Gazetteer, the CIA World Factbook (https://www.cia.gov/cia/publications/factbook/inde x.html) and information about the US states pro-vided by 50states.com (http://www.50states.com). These resources were used to assign an answer validity score between -1 and 1 to each candidate (Figure 2). A score of 0 means the gazetteers did not contribute to the answer selection process for that candidate. For some numeric questions, range checking was added to validate numeric questions similarly to Prager et al. (2004). For example, given the question  X  X ow many people live in Chile? X  , if an answer candidate is within  X  10% of the population stated in the CIA World Factbook, it receives a score of 1.0. If it is in the range of 20%, its score is 0.5. If it significantly differs by more than 20%, it receives a score of -1.0. The threshold may vary based on when the document was written WordNet : The WordNet lexical database includes English words organized in synonym sets, called synsets (Fellbaum, 1998). We used WordNet in or-der to produce an answer validity score between -1 and 1, following the algorithm in Figure 3. A score of 0 means that WordNet does not contribute to the answer selection process for a candidate. 3.1.2 Data-driven Features
Wikipedia and Google were used in a data-driven approach to generate answer validity scores. Wikipedia : Wikipedia (http://www.wikipedia.org) is a multilingual free on-line encyclopedia. Fig-ure 4 shows the algorithm used to generate an answer validity score from Wikipedia. If there is a Wikipedia document whose title matches an answer candidate, the document is analyzed to obtain the term frequency (tf) and the inverse term frequency (idf) of the candidate, from which a tf.idf score is calculated. When there is no matched document, each question keyword is also processed as a back-off strategy, and the answer validity score is calculated by summing the tf.idf scores. To calculate word frequency, the TREC Web Corpus (http://ir.dcs.gla.ac.uk/test collections/wt10g.html) was used as a large background corpus.
 Google : Following Magnini et al. (2002), we used Google to generate a numeric score. A query con-sisting of an answer candidate and question key-words was sent to the Google search engine. To calculate a score, the top 10 text snippets returned by Google were then analyzed using the algorithm in Figure 5. 3.2 Answer Similarity Features We calculate the similarity between two answer can-didates using multiple string distance metrics and a list of synonyms. 3.2.1 String Distance Metrics
There are several different string distance metrics to calculate the similarity of short strings. We used five popular string distance metrics: Levenshtein, Jaccard, Jaro, Jaro-Winkler, and Cosine similarity. 3.2.2 Synonyms
Synonyms can be used as another metric to calcu-late answer similarity. We defined a binary similar-ity score for synonyms. sim ( A i , A j ) =
To get a list of synonyms, we used three knowl-edge bases: WordNet, Wikipedia and the CIA World Factbook. WordNet includes synonyms for English words. Wikipedia redirection is used to obtain an-other set of synonyms. For example,  X  X alif. X  is redi-rected to  X  X alifornia X  in Wikipedia, and  X  X illiam Jefferson Clinton X  is redirected to  X  X ill Clinton X .
The CIA World Factbook includes five different names for a country: conventional long form, con-ventional short form, local long form, local short form and former name. For example, the conven-tional long form of Egypt is  X  X rab Republic of Egypt X , the conventional short form is  X  X gypt X , the local short form is  X  X isr X , the local long form is  X  X umhuriyat Misr al-Arabiyah X  and the former name is  X  X nited Arab Republic (with Syria) X . All are con-sidered to be synonyms of  X  X gypt X .

In addition, manually generated rules are used to obtain synonyms for different types of answer can-didates (Nyberg et al., 2003):  X  Dates are converted into the ISO 8601 date for- X  Temporal expressions are converted into the  X  Numeric expression are converted into sci- X  Representative entities are converted into the This section describes the experiments we used to evaluate our answer selection framework. The JAVELIN QA system (Nyberg et al., 2006) was used as a testbed for the evaluation. 4.1 Experimental Setup A total of 1760 factoid questions from the TREC8-12 QA evaluations served as a dataset, with 5-fold cross validation.

To better understand how the performance of our framework varies for different extraction techniques, we tested it with four JAVELIN answer extraction modules: FST, LIGHTv1, LIGHTv2 and SVM (Ny-berg et al., 2006). FST is an answer extractor based on finite state transducers that incorporate a set of extraction patterns (both manually-created and gen-eralized patterns). LIGHTv1 is an extractor that se-lects answer candidates using a non-linear distance heuristic between the keywords and an answer can-didate. LIGHTv2 is another extractor based on a different distance heuristic, originally developed as part of a multilingual QA system. SVM is an extrac-tor that uses Support Vector Machines to discrimi-nate between correct and incorrect answers.

Answer selection performance was measured by average accuracy: the number of correct top answers divided by the number of questions where at least one correct answer exists in the candidate list pro-vided by an extractor. The baseline was calculated with the answer candidate scores provided by each individual extractor; the answer with the best extrac-tor score was chosen, and no validation or similarity processing was performed. For Wikipedia, we used a version downloaded in Nov. 2005, which con-tained 1,811,554 articles. 4.2 Results and Analysis We first analyzed the average accuracy when us-ing individual validation features. Figure 6 shows the effect of the individual answer validation fea-tures on different extraction outputs. The combina-Figure 6: Average accuracy of individual answer validation features (GZ: gazetteers, WN: WordNet, WIKI: Wikipedia, GL: Google, ALL: combination of all features). tion of all features significantly improved the per-formance when compared to answer selection using a single feature. Comparing the data-driven features with the knowledge-based features, the data-driven features (such as Wikipedia and Google) increased performance more than the knowledge-based fea-tures (such as gazetteers and WordNet); our intuition is that the knowledge-based features covered fewer questions. The biggest improvement was found with candidates produced by the SVM extractor: a 242% improvement over the baseline. It was mostly be-cause SVM tended to produce several answer can-didates with the same or very similar confidence scores, but our framework could select the correct answer among many incorrect ones by exploiting answer validation features.

Table 1 shows the effect of individual similarity features on different extractors when using 0.3 and 0.5 as a similarity threshold, respectively. When comparing five different string similarity features (Levenshtein, Jaro, Jaro-Winkler, Jaccard and Co-sine similarity), Levenshtein and Jaccard tended to perform better than the others. When comparing synonym features with string similarity features, synonyms performed slightly better.

We also analyzed answer selection performance when combining all six similarity features ( X  X ll X  in Table 1). Combining all similarity features did not improve the performance except for the FST extrac-tor, because including five string similarity features synonyms,  X  X ll X : the combination of all similarity metrics) Table 2: Average accuracy of individual features (Sim: merging similarity features, Val: merging val-idation features, ALL: combination of all features). provided too much redundancy to the logistic regres-sion. We also compared the combination of Leven-shtein with synonyms and the combination of Jac-card with synonyms, and then chose Levenshtein and synonyms as the two best similarity features in our framework.

We also analyzed the degree to which the average accuracy was affected by answer similarity and val-idation features. Table 2 compares the average ac-curacy using the baseline, the answer similarity fea-tures, the answer validation features and all feature combinations. As can be seen, the similarity fea-tures significantly improved performance, so we can conclude that exploiting answer similarity improves answer selection performance. The validation fea-tures also significantly improved the performance.
When combining both sets of features together, the answer selection performance increased for all four extractors: an average of 102% over the base-line, 30% over the similarity features and 1.82% over the validation features. Adding the similarity features to the validation features generated small but consistent improvement in all configurations. We expect more performance gain from similar-ity features when merging similar answers returned from all four extractors. Although we conducted our experiments on fac-toid questions, our framework can be easily ex-tended to handle complex questions, which require longer answers representing facts or relations (e.g.,  X  X hat is the relationship between Alan Greenspan and Robert Rubin? X  ). As answer candidates are long text snippets, different features should be used for answer selection. Possible validation features include question keyword inclusion and predicate structure match (Nyberg et al., 2005). For exam-ple, given the question  X  X id Egypt sell Scud mis-siles to Syria? X  , the key predicate from the ques-tion is Sell(Egypt, Syria, Scud missile) . If there is a sentence which contains the predicate structure Buy(Syria, Scud missile, Egypt) , we can calculate the predicate structure distance and use it as a val-idation feature. For answer similarity, we intend to explore novelty detection approaches evaluated in Allan et al. (2003). In this paper, we described our answer selection framework for estimating the probability that an an-swer candidate is correct given multiple answer vali-dation and similarity features. We conducted a series of experiments to evaluate the performance of the framework and analyzed the effect of individual val-idation and similarity features. Empirical results on TREC questions show that our framework improved answer selection performance in the JAVELIN QA system by an average of 102% over the baseline, 30% over the similarity features alone and 1.82% over the validation features alone.

We plan to improve our framework by adding reg-ularization and selecting the final answers among candidates returned from all extractors. As our current framework is based on the assumption that each answer is independent, we are building another probabilistic framework which does not require any independence assumption, and uses an undirected graphical model to estimate the joint probability of all answer candidates. This work was supported in part by ARDA/DTO Advanced Question Answering for Intelli-gence (AQUAINT) program award number NBCHC040164.

