 Wikipedia has grown to be the world largest and busiest free encyclopedia, in which articles are collaboratively written and maintained by volunteers online. Despite its success as a means of knowledge sharing and collaboration, the public has never stopped criticizing the quality of Wikipedia arti-cles edited by non-experts and inexperienced contributors. In this paper, we investigate the problem of assessing the quality of articles in collaborative authoring of Wikipedia. We propose three article quality measurement models that make use of the interaction data between articles and their contributors derived from the article edit history. Our Basic model is designed based on the mutual dependency between article quality and their author authority. The PeerRe-view model introduces the review behavior into measuring article quality. Finally, our ProbReview models extend PeerReview with partial reviewership of contributors as they edit various portions of the articles. We conduct ex-periments on a set of well-labeled Wikipedia articles to eval-uate the effectiveness of our quality measurement models in resembling human judgement.
 H.3.5 [ Online Information Services ]: [Web-based ser-vices]; H.4 [ Information Systems Applications ]: [Mis-cellaneous] Algorithms, Experimentation Wikipedia, article quality, collaborative authoring, author-ity, peer review
Aiming at facilitating collaboration and information shar-ing, Wikipedia 1 , The Free Encyclopedia , has evolved into the most popular wiki site worldwide 2 . Since its first launch in January 2001, Wikipedia has grown to approximately 7 million articles in 251 languages, among which more than 1.7 million are from the English Wikipedia 3 .Thenumber of registered Wikipedians has grown to more than 4 million by early this year. The English Wikipedia alone has been doubling the number of its articles every year from 2002 to 2006 4 .

The success of Wikipedia has to be attributed to the qual-ity of its content [5, 8, 20, 23, 27]. A recent sampling, conducted by Nature revealed that the scientific entries in Wikipedia are of quality comparable to those in the more established Encyclop X dia Britannica [8].

However, like many other open and free websites, Wiki-pedia has its fair share of quality problems. Other than vandalism and spamming, Wikipedia articles are not of uni-formly good quality [21, 28]. One example is a hoax in-serted into the Wikipedia article for John Seigenthaler, Sr., a well known journalist, linking him to the Kennedy assas-sinations. Although the hoax was finally detected and re-moved, it raised great concerns among the Wikipedia users and underlined the importance of quality assurance in Wiki-pedia.
In this research, we aim to design models to measure qual-ity of Wikipedia articles, which is an essential step in qual-ity assurance. The ability to calibrate article quality brings about numerous benefits. http://www.wikipedia.org/
Traffic rank on http://www.alexa.com/ , as of April 2007. http://en.wikipedia.org/ http://en.wikipedia.org/wiki/Wikipedia Determining the quality of articles in Wikipedia is not an easy task to human users, though there have been some serious attempts [6, 28]. The difficulties can be attributed to: Our objective is to develop quantitative measurement mod-els to determine the quality of articles in Wikipedia with minimal human interpretation on the article content. We believe that by not having to check article content, more efficient quality checking can be conducted on Wikipedia, saving much effort of contributors in content creation. This work, however, is not about replacing the existing qual-ity checking mechanisms in Wikipedia. Quality checking and quality measurement actually complement each other. Without the former, contributors will not be able to collabo-ratively produce good quality articles. Furthermore, quality measurement may also require interaction data produced by quality checking to calibrate article quality.
In our approach, we observe the dependency between the quality of contribution to article content and the authority of contributors. We design quality measurement models that make use of the interaction data among articles and their contributors to produce quality ranking.

We summarize our research contributions as follows: While our research is mainly designed for Wikipedia, it can be adopted and extended to measure quality of articles in other wikis and other websites that support collaborative editing 5 . This is possible because our proposed quality mea-surement models are built using the interaction data main-tained by most wiki X  X . With the advent of Web 2.0, wikis and similar websites are expected to increase in number, hence making the quality measurement problem more critical, and quality measurement models like ours more relevant.
The subsequent discussion of this paper is organized as follows. We summarize previous academic studies on eval-uating Wikipedia in Section 2. We introduce our quality measurement models in Section 3, accompanied by our dis-cussion on computational issues. Section 4 presents our ex-perimental design and results. Finally, Section 5 concludes this paper.
Wikipedia has recently attracted growing interest in the research community [2, 5, 6, 16, 29]. Several research works closely related to ours were devoted to evaluating metadata to benchmark article status [16], assessing content trustwor-thiness [29] and user reputation [2, 5]. There were also other works on Wikipedia that focused on inter-article link anal-ysis [1, 23], semantic relatedness [18, 24] and collection evo-lution [4, 26].

Lih X  X  discussion [16] is among the very first on evaluat-ing Wikipedia article in a systematic manner. He proposed a method to judge article quality based solely on meta-data from the article edit history. Statistic features such as rigor (total number of edits) and diversity (total number of unique authors) were argued to be indicators of  X  X evel of good standing X . He experimentally estimated the median values of these two features, which were then used to bench-mark high quality articles. He also showed that citations from other established media has driven public attention di-rectly to certain articles of Wikipedia, improving their qual-ity subsequently.

Zeng et al. [29] modeled the trustworthiness of Wikipedia articles in a dynamic Bayesian network ( X  X BN X  for short). Based on revision history, they hypothesized that  X  X he trust-worthiness of the revised content of an article depends on the trustworthiness of: the previous revision, the authors of the previous revision, and the amount of text involved in the pre-vious revision X . To define their DBN, they approximated the trustworthiness of Wikipedia authors as Beta distributions corresponding to 4 general user groups, namely administra-tors, registered users, anonymous users, and banned users. Their experiments using articles under Geography category of the English Wikipedia showed marginal lead by the mean
A list of wiki sites can be found at WikiIndex, http:// wikiindex.org/ . trustworthiness of featured articles 6 over the mean trustwor-thiness of clean-up articles 7 .
 Anthony et al [5] described Wikipedia articles as a form of Internet collective goods. They contributed a set of hypothe-sis on the correlation among user registration status, partici-pation level, and the quality of their contribution. Adler and Alfaro assessed reputation gains for Wikipedia users based on content survival in revision history. They proposed a chronological method, in which each user gains their stimu-lated amount of reputation upo n every arrival of new revi-sions. The cumulative reputation of an author is determined by how long his/her edited content could survive in terms of time span ( text survival ) and number of revisions ( edit sur-vival ). The longer-lived edits would gain more reputation for their authors; those edits that only sustain in a short while in history would gain negative reputation for their contribu-tors. Their experiments on the French and Italian Wikipedia has shown that:  X  X hanges performed by low-reputation au-thors have a significantly larger-than-average probability of having poor quality and being undone X . They also reported low recall on  X  X ew comers X  in their chronological approach. Nevertheless, investigations in [5] and [2] focused on char-acterizing contributors rather than a horizontal comparison among the articles. Thus, the question of how to assess the uneven quality of the massive amount of articles was left unanswered.

We adopt a fixed-point data-driven approach in measur-ing article quality. In other words, we take a snapshot of Wikipedia, and model the associations between articles and their contributors based on the contribution from each user to the current revision of each article. By doing so, we in-tend to avoid the bias towards long-lived entities over newly created entities, which might be caused by the chronological approach [2]. In our previous research [17], we developed the Basic and PeerReview models based on the mutual dependency between article quality and contributor author-ity. This paper further extends this idea by introducing the review probability for each word as a contributor edits an article.
In this section, we introduce our article quality measure-ment models, namely Basic , PeerReview and ProbRe-view .
We first introduce the notations to be used throughout our discussion. Assuming there are N article entries in Wikipe-dia, we denote an article as a i for 1  X  i  X  N ; and, from the edit histories of all a i  X  X , we could identify M users ,we denote a user as u j for 1  X  j  X  M . The users X  contribution to articles can be categorized into two types: http://en.wikipedia.org/wiki/Wikipedia:Featured_ article http://en.wikipedia.org/wiki/Wikipedia:Clean_up Our basic unit of article content is a word .Awordinan article a i is denoted by w ik . The relationship between users andawordcanbedenotedby: The author and reviewer(s) of each word is identified by comparing the revisions of the article containing the word. Our quality measurement models therefore seek to compute:
Our Basic model is designed based on the principle that  X  X he higher authority are the authors, the better quality is the article. X  This principle measures the quality of an ar-ticle by the aggregation of authorities from all its authors. However, an author X  X  authority would then depend on the quality of articles the author has authored. Therefore, the two quantities, i.e., article quality and author authority, re-inforce each other. Our Basic model is defined by Equa-tions 1 and 2: where c ij denotes the amount of words u j authored in a i Formally, c ij = { w ik | w ik A  X  u j } . By weighing the author-ity (or quality) values by c ij when summing them up, Basic considers precisely the amount of contribution from the au-thors.

Similar to the link analysis in Web search [7, 11, 12, 14, 15, 22], the reinforcing quantities Q i and A j can be com-puted iteratively to derive their converged values. We dis-cuss the computational issues of Basic together with our other models and the convergence properties of an iterative implementation in Section 3.6. In Wikipedia, peer review on articles is ever on-going. An article may undergo a series of edits by contributors. When editing an article, the contributor would have first reviewed the prior content of the article, and then makes his/her own modifications. Content that survives through the new edit indicates the approval from the current con-tributor. If the content is approved by high authority re-viewers, it is expected to be of better quality, even though the original authors of the reviewed content might be of low authority. Observing this peer reviewing process, we would like to incorporate reviewer authority into the definition of article quality. An intuitive way is to aggregate all reviewer authorities into content quality. Equations 3 and 4 give our definition to PeerReview model: In such definition of quality measurement model, each arti-cle is considered as a  X  X ag of words X , formally a i = { w Hence, quality of the article sums up all word qualities, i.e., Q i = k q ik . It is worth noting that PeerReview model considers a user as the true reviewer of a word as long as: 1. The user has created a revision of the article that con-2. In this revision, the word is taken from the article X  X  Therefore, whenever the relationship w ik R  X  u j can be estab-lished between w ik and u j , the quality of word w ik would take full credit from the authority of its reviewer u j ,and vice versa. Also, we consider each reviewer as important as the word X  X  author, and there is no difference in weighing their contributions to derive word quality. Similarly, the re-viewed words are credited to reviewer X  X  authority in exactly the same way as the authored words. The intuition is based on the observation that  X  X ood contributors not only author but also review a considerable amount of good quality con-tent X .
PeerReview  X  X  assumption that each user, who edits the article content, would review the entire article prior to his/her edit is not always true. Consider these scenarios: 1. Certain statistics entries in an article were missing in 2. Some Wikipedia users volunteered themselves for for-In both the above cases and many more, we are not abso-lutely certain that a reviewer will go through all other parts of an article when he or she edit only one portion of the arti-cle. In ProbReview , we therefore associate the relationship w that, the essence of identifying the reviewer(s) of a word is to imply their approval so as to apply their authorities to the word. ProbReview therefore modifies the definition of PeerReview model to capture the partial reviewership of each word.

We define ProbReview model in Equations 5 and 6: where, The function Prob ( w ik R  X  u j ) is defined to return 0 in the following special cases: 1. Prob ( w ik R  X  u j ) = 0, when user u j has never updated 2. Prob ( w ik R  X  u j ) = 0, when the word w ik has never To elaborate these ideas more precisely, we introduce the notation of timestamp, t p . Wedenotethe p th revision of an article a i by a t p i where t p denotes the time when the revision is created. Formally, t p &lt;t q for p&lt;q . Our previous notation a i corresponds to the latest revision of article i , i.e., a i  X  a t max p i .Ifuser u j contributes the revision a we denote it by C ( a t p i )= u j .Hence,user u j has updated has existed in user u j  X  X  revision, if  X  a t p i , such that u
Our next task is to determine Prob ( w ik R  X  u j )incases where the value is non-zero. Intuitively, when a user authors some content in an article, other parts of the article that are closer to these authored content are more likely to be read. In other words, if there is a word w il such that is authored by u , formally w il A  X  u j ,then Prob ( w ik R  X  u j ) should be larger if w ik is close to w il ; it should decrease when w ik is further away from w il . Therefore, Prob ( w ik R  X  u j )canbemodeled as a monotonically decaying function of d kl , the distance between w ik and w il . When there are more than one such w il in article a i , Prob ( w ik probability derived from all possible w il . To summarize the above, we define Prob ( w ik R  X  u j ) in the table shown below: where, S ( d kl ) is the review probability decaying scheme, as a function of the word distance d kl from w ik to w il .
There are several candidate decaying schemes that can be used: Figure 1 depicts these three candidate schemes. S 1 ( d kl creases Prob ( w ik R  X  u j ) at the word level. Even if w w il are few words away, the chances that w ik is reviewed by the author of w il will drop to no more than half quickly.
S(d ) Figure 1: Review Probability Decaying Schemes Whereas S 2 ( d kl )and S 3 ( d kl ) emulate the sentence concept, by keeping Prob ( w ik R  X  u j ) constant within  X  number of words before and after w i l . Words that belong to the same sentence as of w il are most likely to be reviewed. While, for words that fall beyond the boundary of the sentence to which w il belongs, their chances of being reviewed would then decrease as a function of the distance to the boundary. Parameter  X  canberegardedastheestimatedaveragedis-tance from w il to the boundary of the sentence it belongs to. As shown in Figure 1, Prob ( w ik R  X  u j ) drops quickly as d kl increases beyond  X  . Therefore, S 3 ( d kl ) was intended to improve S 2 ( d kl ) by smoothing the decaying rate.
The estimated average sentence length, i.e.,  X  in Equa-tions 9 and 10, is the only parameter to tune in our refined ProbReview model.
In contrast with our authority -based quality measurement models, a na  X   X ve way of judging article is based on length alone. The longer is the article, the better quality it is expected to carry. Equation 11 defines the Na  X   X ve model, which will be used as the baseline for performance evalua-tion.

Basic , PeerReview and ProbReview are models that involve mutual dependency between quality and authority quantities in different forms. They can be implemented us-ing iterative computation over a set of equations. This iter-ative computation process includes the following steps: 1. initialize all quality and authority values uniformly 8 2. for each iteration:
Random non-zero initialization is another option. It has been proven [10] that the final converged quality and au-thority values would be independent of the values used in initialization. 3. repeat step 2 until the authority and quality values The convergence of such computation has been intensively addressed in [10, 15, 25]. If we represent quality values in vector Q 9 ,ofdimension D ; all authority values in vector A ,ofdimension M ; and all interaction data between qual-ity and authority in an adjacency matrix M d of dimension D  X  M . Given the condition that M d is diagonalizable and has a unique largest eigenvalue 10 [10], the resulting quality vector Q would converge to the eigenvector of M d corre-sponding to its largest eigenvalue, and authority vector A would converge to the corresponding eigenvector of M d T , almost independently of the initial values used.

Besides absolute convergence, other terminating condi-tions commonly adopted in practice [11, 12, 14, 25] are: Because of L 1 normalization, the resulting quality(authority) values are in the range of [0 , 1]. These values preserve the relative ratios within article quality and contributor author-ity. The normalized quality score values that preserves the relative ratio among them are good enough for us to rank the articles.
To summarize, our Basic model is designed based on the mutual dependency between article quality and contributor authority. Only authors of the article are considered in this model. Our PeerReview model is built on top of Basic by introducing the role of word reviewers. Reviewer authority is treated equally as that of authors. Finally, our ProbRe-view model refines PeerReview by emulating the partial reviewership of each contributor. Words that are farther away from those that originate from a contributor are as-sumed less likely to have been reviewed by the contributor. Our objective in conducting the experiments is two-fold. Firstly, we want to evaluate and compare the effectiveness of our proposed quality measurement models. Secondly, by varying some parameters and by incorporating length fea-tures of articles, we study the behavior of the proposed mod-els in more details. In the following, we describe the dataset used, our proposed performance metric, and our experimen-tal results.
Q entries may refer to article quality Q i  X  X  as in Basic they may refer to word quality q ik  X  X  in PeerReview and ProbReview .
For cases when M d consists of multiple diagonalizable sub-matrices, where each can be linearly separated from one an-other, our intuitive solution is to decompose M d and solve the equations for each sub-matrix separately. However, such case was not encountered in our datasets so far. Further in-vestigation on these cases is scheduled in our future work. # articles 14 20 11 155 30 0 230
We chose a set of 242 country articles in Wikipedia for our experiments. These article titles were obtained from the page  X  X ist of countries X  11 in Wikipedia. The main rea-son for choosing this set of articles was because the majority of them have been assigned class labels according to Wikipe-dia Editorial Team X  X  quality grading scheme 12 . These man-ual labels were regarded as the ground truth in our model evaluation.

The label  X  X A X  stands for Featured Articles , which repre-sent the best works in Wikipedia.  X  X  X -class articles provide a complete treatment to their subjects.  X  X A X  stands for Good Articles , which must meet the criteria of  X  X ell written, sta-ble, accurate, referenced, have a neutral point of view, and show relevant illustrations with an appropriate copyright X .  X  X  X -class is the next best.  X  X tart X -class articles are new ar-ticles being constructed, and  X  X tub X  13 -class has the lowest quality status. We present them in decreasing quality in Table 1, i.e., FA  X  A  X  GA  X  B  X  Start  X  Stub.

We crawled the latest revisions of the 242 articles dated 5th November, 2006, together with all past edit histories using MediaWiki X  X  query API 14 .Wealsoextractedtheclass labels of the articles from each article X  X  talk page, dated 5th November, 2006. Table 1 also summarizes the class label distribution statistics of this set of articles. Note that none of the articles in this collection was labeled Stub. There were 12 articles left unlabeled, which represented less than 5% of the article collection. And, the majority were of B-class, which occupied 64% of the article collection.
Article quality is measured on the latest revision of each article. The author and reviewer(s) for each word instance are extracted from the article edit history.

The edit history of articles consists of not only revisions submitted by human users, but also revisions created by robots (or  X  X ots X  for short), which are automatic processes that interact with Wikipedia articles. The functions of edit-ing bots are mainly: automatic importing, spell checking, wikifying, anti-vandalism and ban enforcement 15 .Sincebot-created revisions generally does not carry content contribu-tion, and revisions created by them do affect article quality measurement, we therefore decided to filter out revisions created by bots.

Besides bot revisions, we also removed a series of consec-utive revisions from the same user by keeping only the last revision of the series. It was observed users had the habit http://en.wikipedia.org/wiki/List_of_countries http://en.wikipedia.org/wiki/Wikipedia:Version_1. 0_Editorial_Team/Assessment
A Wikipedia stub is a very short article in need of expansion. See page http://en.wikipedia.org/wiki/ Wikipedia:Stub http://en.wikipedia.org/w/api.php http://en.wikipedia.org/wiki/Wikipedia:Types_of_ bots # authors per article 60 1058 227.6 # articles per author 1 194 1.7 #words #reviewers per article 90 2,087 406.1 # articles per reviewer 0 234 2.9 of saving intermediate revisions to avoid loss of work due to unexpected hardware, software or network errors. By removing these intermediate revisions, we reduced the com-putation time without losing necessary interaction data.
We first extracted the lexicon for each revision. Punctu-ation, stop words and Wikipedia X  X  markup syntax were re-moved. The relative order of word instances was retained for the purpose of revision comparison. We then performed Diff comparisons between the article X  X  latest revision and every older revisions in the reverse-chronological order. When a word instance in the latest revision was found to have ex-isted in an older revision, the contributor who edited the latter revision was added as a reviewer of the word instance; when a word instance was found to be missing in all older revisions, the last added reviewer of that word instance was assigned as the author.

As a result of data preprocessing, we identified 103,067 unique non-bot users from this article collection; 33,249 of them had authored at least one word in the latest revisions; and only 29.8% (i.e., 9,896) of these authors were registered users. Table 2 summarizes the statistics of our preprocessed dataset.
We adopt two evaluation metrics. The first metric is called Normalized Discounted Cumulative Gain at top k ( X  X DCG@k X  for short) to evaluate the accuracy of the article ranking produced by a given quality measure-ment model. NDCG was first defined as an IR evaluation metric by Jarvelin et al [13] to consider the degree of rele-vance in retrieved results. More relevant results retrieved at top positions in the rank would accumulate higher score to the top k gain. This metric was chosen because it is par-ticularly suited for ranked articles that have multiple levels of assessment, corresponding to the FA  X  A  X  GA  X  B  X  Start class labels.
 As shown in Equation 12, NDCG@k is computed by sum-ming up the gains from position p =1to p = k in the ranking results. Given rank position p , s ( p ) is an integer representing the amount of reward given to the article at po-sition p .Inourcase, s ( p ) = 4 when the p -th ranked article has a FA label, s ( p ) = 3 for A-labeled article, and so on and so forth. We summarize s ( p ) values and their corresponding article label at position p in the third row of Table 1. Note that Start-labeled or unlabeled article at position p does not contribute to the cumulative gain.

The term Z is a normalization factor derived from a per-fect ranking of top k articles so that it would yield a NDCG and S 1 , S 2 and S 3 corresponds to S 1 ( d kl ) , S 2 ( d ( d ) respectively) of 1 [3]. Intuitively, the perfect ranking should place all FA-class articles before all A-class articles, followed by all GA-class articles and so on; finally, all Start-class articles should be place at the bottom end of the ranking result.
Our second metric is the Spea rman X  X  rank correlation co-efficient. Spearman X  X  rank correlation coefficient is a well-known metric for comparing the agreement between two rankings on the same set of objects.

Different from NDCG that considers articles belonging to the same quality class equally, Spearman X  X  rank correlation coefficient preserves the one-to-one correspondence between articles in the two rankings. Another difference in these two metrics is in their value ranges. NDCG@k metric has value in the range of [0,1]. NDCG = 1 indicates perfect performance, and NDCG = 0 indicates worst performance. Whereas, Spearman X  X  rank correlation coefficient has value in the range of [-1,1]. When two rankings on the same set of objects give a coefficient of 1, this means the two rank-ings are perfectly matched on the ordering of these objects; when they give a coefficient of -1, the two rankings are in ex-act reverse order; the median value 0 indicate total random correlation on these two rankings.
In this set of results, we compare NDCG@k among Na  X   X ve Basic , PeerReview and ProbReview model with S 1 ( d kl ), S ( d kl )and S 3 ( d kl ) schemes defined in Equations 8, 9 and 10 respectively. We took six k values at 14, 34, 45, 200, 230 and 242 respectively. These k values were derived from our dataset statistics shown in Table 1. k was incremented each time by the total number of articles in the next best quality class.

For the ProbReview models, we used  X  =7. Thechoice of  X  = 7 was based on the past empirical studies that said: (i) there are 5 to 35 words in one sentence for text doc-uments [9]; and (ii) the span of immediate memory which imposed limitations on the amount of information that peo-ple were able to receive, process, and remember is  X  X he magic number seven X  [19]; and (iii) our brief calculation indicates an average of 6 . 04 words per sentence, with standard de-viation of 5 . 85, for this set of Wikipedia articles after stop words removal. These studies suggest that  X  =7isarea-sonable value for our experiments. The NDCG@k values for Na  X   X ve , Basic , PeerReview ,and ProbReview models are depicted in Figure 2.

It is clear from Figure 2 that our PeerReview and Pro-bReview models with S 2 ( d kl )and S 3 ( d kl )alwaysoutper-form the baseline model Na  X   X ve for all k values. Especially, when k is small, the performance gaps between these three models and the Na  X   X ve model are seen more significant. On the down side, the Basic and ProbReview with S 1 ( d kl ) models do not give superior performance than Na  X   X ve .
When k = 14, the PeerReview and ProbReview mod-els with S 2 ( d kl )and S 3 ( d kl ) returned 5, 6 and 6 FA-class articles respectively in the top 14 ranked articles produced by their quality rankings. The Na  X   X ve model, on the other hand, only returned 3 FA-class articles in the top 14 ranked articles. Note that there are altogether 14 FA-class articles in our collection.

As k take larger values, not only the cumulative gain gets larger, but also the normalization factor grows. When k = 242, which includes all articles in our dataset, the result-ing NDCG is an indicator of how well the overall ordering of all articles in our collection matched with the human man-ual assessment. From the right most block in Figure 2, it is clear that our authority-based PeerReview and ProbRe-view with appropriate schemes give promising performance in terms of overall article ranking.

The poor performance of S 1 ( d kl )in ProbReview model could be caused by the drastic drops in S ( d kl )withincreas-ing word distance d kl . We suspected this reviewing behavior was presumably rare in practice. As a result, S 1 ( d kl )donot perform as well as the other two schemes that incorporate a non-zero  X  .

Not surprisingly, the performance of Basic is comparable to that of ProbReview with S 1 ( d kl ). By the definition of S 1 ( d kl )in ProbReview model, if we discard all review probabilities for d kl =0, ProbReview with S 1 ( d kl )would degenerate into Basic , which considers only authority of word authors. However, because of the non-zero tail at d kl 0, S 1 ( d kl ) X  X  performance deviates a little from that of Table 3: Spearman X  X  Rank Correlation Coefficients, with  X  =7 Nv 1.000 0.293 0.870 0.022 0.279 0.289 Bc -1.000 0.377 0.046 0.201 0.206 Rv --1.000 0.032 0.367 0.382 Pb S 1 ---1.000 0.504 0.506 Pb S 2 ----1.000 0.998 Pb S 3 -----1.000
Table 4: Variance in NDCG@k vs Parameter  X  k =14 0.000 0.000 0.000 0.000 0.000 0.000 k =34 0.000 0.020 -0.008 0.000 0.019 0.024 k =45 0.000 0.019 -0.007 0.000 0.008 0.004 k = 200 0.001 0.003 -0.020 -0.001 0.006 -0.004 k = 230 0.001 0.004 -0.018 -0.001 0.007 -0.004 k = 242 0.001 0.004 -0.018 -0.001 0.006 -0.004
On the whole, this set of results suggests that by care-fully considering the authority of reviewers together with the authority of the actual authors (i.e., PeerReview and ProbReview with S 2 and S 3 ) in collaborative editing, we are able to achieve better quality ranking of articles than na  X   X vely judging articles by length.
Table 3 shows the Spearman X  X  rank correlation coefficients between pairs of article quality rankings produced by various models. Same as in Figure 2, parameter  X  wassetto7for S ( d kl )and S 3 ( d kl )in ProbReview .

The highest rank correlation is observed between S 2 ( d kl and S 3 ( d kl )of ProbReview . This observation agrees with the comparable performance between these two schemes in terms of NDCG. Recall our definition of these two review probability decaying schemes in Figure 1, S 3 ( d kl ) is intended to improve S 2 ( d kl ) by smoothing the decaying rate at d beyond the sentence boundary of w il . As shown in Table 3, the high correlation between these two schemes suggests that these two decaying schemes did not alter the final ranking significantly in this experimental setting.
 The lowest rank correlation is observed in the column of ProbReview with S 1 ( d kl ). Because of the assumption, that review probability decreases quickly with small d kl ,ispre-sumably rare, S 1 ( d kl ) did not produce article quality rank-ing compare with Na  X   X ve , Basic and PeerReview models. However, because of the inherent formulation of ProbRe-view models, S 1 ( d kl ) is correlated with S 2 ( d kl )and S to some extent.
Other than using ProbReview model with  X  =7,we also explored the options of  X  = 10,  X  =20and  X  = 30, to extend the boundary in which the review probability re-mains 1. In this case, we are interested in the way NDCG changes in the resulting article rankings with different  X  set-tings. We summarize the percentage of difference in NDCG as compared with that using  X  =7inTable4.

In general, when  X  grows, the variance in NDCG is ob-served to be small for all k . The same observation, that Table 5: Average Words per Article for each Class std dev 1682.7 2582.2 1431.3 1812.9 1327.6 larger  X  does not alter the ranking of top 14 articles, holds for both S 2 ( d kl )and S 3 ( d kl )of ProbReview . Both schemes seem to favor  X  = 20, as shown by the rise of positive vari-ance. Larger  X  with smoother review probability decaying scheme in ProbReview shows smaller reduction in NDCG in Table 4. Theoretically, when  X  =  X  , ProbReview with both S 2 ( d kl )and S 3 ( d kl ) will degenerate into PeerReview
Interestingly, while Na  X   X ve did not produce the best per-formance (see Figure 2), it performed better than Basic and ProbReview with S 1 ( d kl ). We therefore suspect a correla-tion between article length and article quality. Table 5 sum-marizes the average word count (after stop words removal) in articles for each quality class in our collection.
Table 5 shows that higher quality class corresponded to larger average article length. The three classes, FA, A and GA, were of comparable average article length, i.e., in the range of 5 , 330 to 5 , 990. There were, however, significant gaps in average article length between these three classes and class B articles, as well as between class B articles and class Start articles. It is noted that, in Wikipedia, FA-and GA-class articles require to be nominated as well as peer-reviewed before their status can be promoted. This empir-ical observation suggests that article length does play an important part in judging article quality in Wikipedia.
In this section, we therefore study how article length can possibly improve the performance of our proposed quality measurement models. The intuition is to bring good fea-tures together yet letting each play a part in final quality ranking. We essentially combine the Basic , PeerReview and ProbReview models linearly with the Na  X   X ve model as shown in Equation 13. This leads to the hybrid versions of the proposed models. In this equation,  X  g ( a i ) denotes the combined quality measure measures given by Na  X   X ve model and any one of the other quality measurement models respectively. We experimented combination by quality scores and quality rank positions, as shown in Figure X 3(a) and 3(b) respectively. For the hy-brid ProbReview , we chose to use  X  =7and S 2 ( d kl )only. S ( d kl ) is not reported in this section because it gave very similar results as S 2 ( d kl ).

Figures 3(a) and 3(b) depict the NDCG@k=242 16 for hy-brid models by combining their computed quality scores and quality ranks respectively. We varied the  X  values from 0 to 1 in 0.1 intervals. On the whole, with different  X  values, the performance of proposed models remain largely unchanged in their hybrid versions, except for the following cases.
Hybrid Basic using quality score combination improves over the original Basic significantly such that it even out-242 is the total number of articles in our country collec-tion. NDCG@k=242 indicates a measure of overall ranking of articles in the assessment scale. Category #users avg # words Score k NDCG@k authored reviewed Bc Rv Pb Registered 9,816 56.2 5,235.3 1.0 9,896 0.408 0.487 0.384 0.372 0.376 performs Na  X   X ve slightly when  X   X  0 . 1. This observation shows that, Basic model benefits from article length in-formation. Nevertheless, the improvement does not make Basic much better than Na  X   X ve .

Hybrid PeerReview shows slightly better NDCG@k=242 at  X  =0 . 2. This improvement (i.e., 0.007 over the origi-nal PeerReview and 0.05 better than Na  X   X ve ) is however not significant. When  X  becomes larger, this improvement vanishes gradually. Hybrid ProbReview does not improve over the original ProbReview model for all the  X  values. In fact, ProbReview model suffers from the article length information.

This set of results on the hybrid models shows that lin-ear combination does not help improving PeerReview and ProbReview with S 2 ( d kl ) further. However, by incorpo-rating length feature of articles, it improves Basic model over the baseline to some extent.
In this section, we examine the derived authority of users by dividing them into 3 groups, namely  X  X on-registered X ,  X  X egistered X  and  X  X ikiProject:Country participants X . The  X  X on-registered X  users are identified by their IP addresses. The  X  X egistered X  users are identified by their unique user names.  X  X ikiProject:Country participants X  are enthusiasts who volunteered themselves to constantly improving Wiki-pedia articles in the set of country subjects. Their user names were found on page  X  X ikipedia:WikiProject Coun-tries/Participants X  17 , as of 10 March 2007.

In Table 6, we show the overall statistics of these three user groups. Intuitively, we expected that, registered users should be more authoritative than non-registered users since the former X  X  user names are known and their efforts would http://en.wikipedia.org/wiki/Wikipedia: WikiProject_Countries/Participants affect their reputation. The  X  X ikiProject:Country partici-pants X  belong to an even more exclusive user group and are expected to be more authoritative. We computed NDCG@k for users rankings. Values of k were taken at 80, 9896 and 33249, corresponding to number of  X  X ikiProject:Country participants X ,  X  X ikiProject Country participants X  cum reg-istered users, and all users, respectively.

As Table 6 shows, PeerReview yields the best NDCG performance on user ranking for all three k . Interestingly, the best model for article ranking, i.e., ProbReview model with S 2 ( d kl )and S 3 ( d kl ), does not perform well in user ranking. We believe it is due to voluntary participation in WikiProjects. Contributors of the high quality contri-bution may not volunteer themselves to participate in the  X  X ikiProject: Countries X  project. On the other hand, un-registered user could also give very high quality contribu-tions [5]. Therefore, our previous assumption about differ-ent authority levels of contributors might be too general to represent the ground truth. In this paper, we study models for automatically deriving Wikipedia article quality rankings based on the interaction data between articles and their contributors.

Our PeerReview model, which was first proposed in [17], had already shown promising performance over the baseline model Na  X   X ve . We further extended it to emulate the proba-bility of article content being reviewed by each contributor. As shown in our experiments, the extended ProbReview models with review probability decaying schemes S 2 ( d and S 3 ( d kl ) were the best performers compared with all other models under the same setting. By observing that, user interaction data itself is not sufficient in judging arti-cle quality and article length appears to have some merits in identifying quality articles, we incorporated article length into article quality measurement. Our experimental results showed some performance improvement by Hybrid Basic and hybrid PeerReview models at  X  =0 . 1and  X  =0 . 2re-spectively. However, ProbReview models, did not benefit from article length.

Our evaluation and analysis in this paper have been focus-ing on article quality rankings for collaboratively authored content. However, assessing content quality and contribu-tor authority are complementary steps in our data-driven approach. In the future work, we plan to investigate cus-tomized review behavior in co-editing, and to derive con-tributor expertise is the first step. Besides, we also plan to apply our proposed models on a much larger Wikipedia arti-cle set. Managing model scalability and integrating quality measurements with quality checking procedures in collabo-rative authoring are other steps forward in this research.
This work was supported in part by A*STAR Public Sec-tor R&amp;D, Project Number 062 101 0031. We gratefully ac-knowledge the valuable comments and suggestions from our anonymous reviewers. [1] S. F. Adafre and M. de Rijke. Discovering missing [2] B. T. Adler and L. de Alfaro. A content-driven [3] E. Agichtein, E. Brill, and S. Dumais. Improving Web [4] R. B. Almeida, B. Mozafari, and J. Cho. On the [5] D. Anthony, S. Smith, and T. Williamson. Explaining [6] T. Cross. Puppy smoothies: Improving the reliability [7] C. Dwork, R. Kumar, and M. Naor. Rank aggregation [8] J. Giles. Internet encyclopaedias go head to head, [9] J. Goldstein, M. Kantrowitz, V. Mittal, and [10] G. H. Golub and C. F. V. Loan. Matrix Computations . [11] Z. Gy  X  ongyi, P. Berkhin, H. Garcia-Molina, and [12] Z. Gy  X  ongyi, H. Garcia-Molina, and J. Pedersen. [13] K. Jarvelin and J. Kekalainen. IR evaluation methods [14] G. Jeh and J. Widom. Scaling personalized Web [15] J. M. Kleinberg. Authoritative sources in a [16] A. Lih. Wikipedia as participatory journalism: [17] E.-P. Lim, B.-Q. Vuong, H. W. Lauw, and A. Sun. [18] Max V  X  olkel and Markus Kr  X  otzsch and Denny [19] G. A. Miller. The magical number seven, plus or [20] B. B. C. News. Wikipedia survives research test, 2005. [21] A. Orlowski. Wikipedia founder admits to serious [22] L. Page, S. Brin, R. Motwani, and T. Winograd. The [23] P. Sch  X  onhofen. Identifying document topics using the [24] M. Strube and S. P. Ponzetto. Wikirelate! computing [25] P. Tsaparas. Using non-linear dynamical systems for [26] J. Voss. Measuring Wikipedia. In Proc. of the 10th [27] J. Wales. Wikipedia sociographics, 2004. Retrieved [28] Wikipedia. Replies to common objections, 2007. [29] H. Zeng, M. A. Alhossaini, L. Ding, R. Fikes, and
