 Dept. of Computer Science &amp; Automation, yields accurate classification.
 vector of class labels and K  X  S n performance measure  X  ( K ) by solving defined positive constant. 1.1 Background and Related work optimization problem where ( X ) if cutting plane (ACCP) method to solve the outer maximization in  X  . alternate loss function k K  X  S k formulation. The choice of k K  X  S k 2 a kernel matrix, K  X  S n which requires O ( log n ces. Like the previous case it requires O ( m 2 log n proposed in [7].
 Given multiple similarity matrices { S where  X   X  0 is a trade-off parameter, L operating on K and S individual kernel matrix K (MKL) setup to obtain a kernel matrix K  X  K , P MKL problem is proposed as where the kernels K kernel learning formulation Note that  X ( K The restriction of  X ( K P where  X  function on  X  tions. For m = 1 with L ( X ) = k X k 2 loss function L 3.1 Entropic single kernel learning (ESKL) algorithm We denote the feasible set of kernels as K = { K  X  S n  X  : where (  X  descent algorithm proposed in [2].
 Algorithm 1 Entropic single kernel learning (ESKL) algorithm
Initialization: K (1)  X  int( K ) . Set t = 0 . repeat until Convergence objective value. If the ESKL algorithm is initialized with K (1) =  X  as  X  Lipschitz constant of f such that k f  X  ( K ( t ) ) k Proof. The strong convexity constant of  X  w.r.t. k k distance function [2] generated by  X  . Then we have B 3.2 Entropic multiple kernel learning (EMKL) algorithm where  X  above, the objective function F in (6) can be expressed as
F ( K 1 ,...,K m ) = max
F i ( K i ;  X  ,  X  ) = 1 m 1  X   X   X  1 2  X  Let V := N m that  X ( K objective function F in (6) is convex and Lipschitz continuous on K m . Lemma 3.1. Let (  X   X  ,  X   X  ) be a solution of (10) and L  X  of F is given by Proof. First, we observe that F  X  By optimality of (  X   X  ,  X   X  ) we have F ( K ( K  X  1 ,...,K  X  m )  X  K m . Applying (13) we arrive at Hence, F  X  ( K continuously differentiable function  X  on the product space K m as where (  X  kernel learning (EMKL), is given below.
 Algorithm 2 Entropic multiple kernel learning (EMKL) algorithm
Initialization: K (1) repeat until Convergence objective value. If the EMKL algorithm is initialized with K (1) chosen as  X  Lip( F ) is a Lipschitz constant of F such that k  X  K Proof. Let K  X  = ( K  X   X  where  X  &gt; 0 is the strong convexity constant of  X  and B ated by  X  . For the  X  function defined in Eqn. (14), we have  X  = 1 have B the desired result. 3.3 Restricted entropic kernel learning (REKL) algorithm Eqn. (10). We denote the feasible set for  X   X   X  X , is a convex function on the Cartesian product space X m := N m  X  is assumed to be a convex function of  X  gradient of  X   X   X  = (  X  convex and continuously differentiable function  X  Algorithm 3 Restricted entropic kernel learning (REKL) algorithm
Find eigen decomposition: S
Initialization:  X  (1) repeat until Convergence objective value. If the REKL algorithm is initialized with  X  (1) chosen as  X  is a Lipschitz constant of g such that | g  X  ( t ) Proof. The proof is similar to that of Theorem 3.2. 3.4 Discussion The ESKL formulation requires O log n similarity matrix S with eigen-decomposition S = P erate kernel matrix as K := P (b) Flip: lowing choices for the loss functions in ESKL / EMKL: [ L [
L 2 ] L ( K  X  S ) = k K  X  S k F choose  X  (  X  train / test splits. 4.1 Data sets similarity measures are indefinite in general. 4.2 Effect of various loss functions disorder data set L both L AuralSonar data set ESKL formulation works best with L set L keeping the choice of loss function almost open.

Liver disorder 57.6 54.5 55.5 59.7 61.0 62.8 60.7 4.3 Combining multiple sequence similarity matrices for Pr oteins family SVM Denoise [8] [12] [ L reasonably good performance.
