 This paper explores the use of statistical machine translation (SMT) methods in natural language gen-eration (NLG), specifically the task of mapping statements in a formal meaning representation lan-guage (MRL) into a natural language (NL), i.e. tacti-cal generation. Given a corpus of NL sentences each paired with a formal meaning representation (MR), it is easy to use SMT to construct a tactical gener-ator, i.e. a statistical model that translates MRL to NL. However, there has been little, if any, research on exploiting recent SMT methods for NLG.

In this paper we present results on using a re-cent phrase-based SMT system, P HARAOH (Koehn et al., 2003), for NLG. 1 Although moderately effec-tive, the inability of P HARAOH to exploit the for-mal structure and grammar of the MRL limits its ac-curacy. Unlike natural languages, MRLs typically have a simple, formal syntax to support effective au-tomated processing and inference. This MRL struc-ture can also be used to improve language genera-tion.

Tactical generation can also be seen as the inverse of semantic parsing , the task of mapping NL sen-tences to MRs. In this paper, we show how to  X  X n-vert X  a recent SMT-based semantic parser, W ASP (Wong and Mooney, 2006), in order to produce a more effective generation system. W ASP exploits the formal syntax of the MRL by learning a trans-lator (based on a statistical synchronous context-free grammar) that maps an NL sentence to a lin-earized parse-tree of its MR rather than to a flat MR string. In addition to exploiting the formal MRL grammar, our approach also allows the same learned grammar to be used for both parsing and genera-tion, an elegant property that has been widely ad-vocated (Kay, 1975; Jacobs, 1985; Shieber, 1988). We present experimental results in two domains pre-viously used to test W ASP  X  X  semantic parsing abil-ity: mapping NL queries to a formal database query language, and mapping NL soccer coaching instruc-tions to a formal robot command language. W ASP  X  1 is shown to produce a more accurate NL generator
We also show how the idea of generating from linearized parse-trees rather than flat MRs, used effectively in W ASP  X  1 , can also be exploited in P
HARAOH . A version of P HARAOH that exploits this approach is experimentally shown to produce more accurate generators that are more competi-tive with W ASP  X  1  X  X . Finally, we also show how
If our player 4 has the ball, then our player 6 should stay in the left side of our half.
What states does the Ohio run through? aspects of P HARAOH  X  X  phrase-based model can be used to improve W ASP  X  1 , resulting in a hybrid sys-tem whose overall performance is the best. In this work, we consider input MRs with a hi-erarchical structure similar to Moore (2002). The only restriction on the MRL is that it be defined by an available unambiguous context-free grammar (CFG), which is true for almost all computer lan-guages. We also assume that the order in which MR predicates appear is relevant, i.e. the order can affect the meaning of the MR. Note that the order in which predicates appear need not be the same as the word order of the target NL, and therefore, the content planner need not know about the target NL grammar (Shieber, 1993).

To ground our discussion, we consider two ap-plication domains which were originally used to demonstrate semantic parsing. The first domain is R
OBO C UP . In the R OBO C UP Coach Competition ( www.robocup.org ), teams of agents compete in a simulated soccer game and receive coach advice written in a formal language called CL ANG (Chen et al., 2003). The task is to build a system that trans-lates this formal advice into English. Figure 1(a) shows a piece of sample advice.

The second domain is G EOQUERY , where a func-tional, variable-free query language is used for querying a small database on U.S. geography (Kate et al., 2005). The task is to translate formal queries into NL. Figure 1(b) shows a sample query. In this section, we show how SMT methods can be used to construct a tactical generator. This is in con-trast to existing work that focuses on the use of NLG in interlingual MT (Whitelock, 1992), in which the roles of NLG and MT are switched. We first con-sider using a phrase-based SMT system, P HARAOH , for NLG. Then we show how to invert an SMT-based semantic parser, W ASP , to produce a more effective generation system. 3.1 Generation using P HARAOH P
HARAOH (Koehn et al., 2003) is an SMT system that uses phrases as basic translation units. Dur-ing decoding, the source sentence is segmented into a sequence of phrases. These phrases are then re-ordered and translated into phrases in the target lan-guage, which are joined together to form the output sentence. Compared to earlier word-based methods such as IBM Models (Brown et al., 1993), phrase-based methods such as P HARAOH are much more effective in producing idiomatic translations, and are currently the best performing methods in SMT (Koehn and Monz, 2006).

To use P HARAOH for NLG, we simply treat the source MRL as an NL, so that phrases in the MRL are sequences of MR tokens. Note that the grammat-icality of MRs is not an issue here, as they are given as input. 3.2 W ASP : The Semantic Parsing Algorithm Before showing how generation can be performed by inverting a semantic parser, we present a brief overview of W ASP (Wong and Mooney, 2006), the SMT-based semantic parser on which this work is based.

To describe W ASP , it is best to start with an ex-ample. Consider the task of translating the English sentence in Figure 1(a) into CL ANG . To do this, we may first generate a parse tree of the input sen-tence. The meaning of the sentence is then ob-tained by combining the meanings of the phrases. This process can be formalized using a synchronous context-free grammar (SCFG), originally developed as a grammar formalism that combines syntax anal-ysis and code generation in compilers (Aho and Ull-man, 1972). It has been used in syntax-based SMT to model the translation of one NL to another (Chi-ang, 2005). A derivation for a SCFG gives rise to multiple isomorphic parse trees. Figure 2 shows a partial parse of the sample sentence and its corre-sponding CL ANG parse from which an MR is con-structed. Note that the two parse trees are isomor-phic (ignoring terminals).

Each SCFG rule consists of a non-terminal, X , on the left-hand side (LHS), and a pair of strings, h  X ,  X  i , on the right-hand side (RHS). The non-terminals in  X  are a permutation of the non-terminals in  X  (indices are used to show their correspondence). In W ASP ,  X  denotes an NL phrase, and X  X   X  is a production of the MRL grammar. Below are the SCFG rules that generate the parses in Figure 2: i All derivations start with a pair of co-indexed start symbols of the MRL grammar, h S 1 , S 1 i , and each step involves the rewriting of a pair of co-indexed non-terminals (by  X  and  X  , respectively). Given an input sentence, e , the task of semantic parsing is to find a derivation that yields h e , f i , so that f is an MR translation of e .
 Parsing with W ASP requires a set of SCFG rules. These rules are learned using a word alignment model, which finds an optimal mapping from words to MR predicates given a set of training sentences and their correct MRs. Word alignment models have been widely used for lexical acquisition in SMT (Brown et al., 1993; Koehn et al., 2003). To use a word alignment model in the semantic parsing scenario, we can treat the MRL simply as an NL, and MR tokens as words, but this often leads to poor results. First, not all MR tokens carry spe-cific meanings. For example, in CL ANG , parenthe-ses and braces are delimiters that are semantically vacuous. Such tokens can easily confuse the word alignment model. Second, MR tokens may exhibit polysemy. For example, the CL ANG predicate pt has three meanings based on the types of arguments it is given (Chen et al., 2003). Judging from the pt token alone, the word alignment model would not be able to identify its exact meaning.

A simple, principled way to avoid these difficul-ties is to represent an MR using a list of productions used to generate it. This list is used in lieu of the MR in a word alignment. Figure 3 shows an exam-ple. Here the list of productions corresponds to the top-down, left-most derivation of an MR. For each MR there is a unique linearized parse-tree, since the MRL grammar is unambiguous. Note that the structure of the parse tree is preserved through lin-earization. This allows us to extract SCFG rules in a bottom-up manner, assuming the alignment is n -to-1 (each word is linked to at most one production). Ex-traction starts with productions whose RHS is all ter-minals, followed by those with non-terminals. (De-tails can be found in Wong and Mooney (2006).) The rules extracted from Figure 3 would be almost the same as those used in Figure 2, except the one for token (1) denotes a word gap of size 1, due to the un-aligned word the that comes between has and ball . It can be seen as a non-terminal that expands to at most one word, allowing for some flexibility in pat-tern matching.

In W ASP , G IZA ++ (Och and Ney, 2003) is used to obtain the best alignments from the training ex-amples. Then SCFG rules are extracted from these alignments. The resulting SCFG, however, can be ambiguous. Therefore, a maximum-entropy model that defines the conditional probability of deriva-tions ( d ) given an input sentence ( e ) is used for dis-ambiguation: The feature functions, f i , are the number of times each rule is used in a derivation. Z  X  ( e ) is the normalizing factor. The model parameters,  X  i , are trained using L-BFGS (Nocedal, 1980) to maxi-mize the conditional log-likelihood of the training examples (with a Gaussian prior). The decoding task is thus to find a derivation d  X  that maximizes Pr  X  ( d  X  | e ) , and the output MR translation, f  X  , is the yield of d  X  . This can be done in cubic time with re-spect to the length of e using an Earley chart parser. 3.3 Generation by Inverting W ASP Now we show how to invert W ASP to produce W
ASP  X  1 , and use it for NLG. We can use the same grammar for both parsing and generation, a partic-ularly appealing aspect of using W ASP . Since an SCFG is fully symmetric with respect to both gen-erated strings, the same chart used for parsing can be easily adapted for efficient generation (Shieber, 1988; Kay, 1996).

Given an input MR, f , W ASP  X  1 finds a sentence e that maximizes Pr( e | f ) . It is difficult to directly model Pr( e | f ) , however, because it has to assign low probabilities to output sentences that are not grammatical. There is no such requirement for pars-ing, because the use of the MRL grammar ensures the grammaticality of all output MRs. For genera-tion, we need an NL grammar to ensure grammati-cality, but this is not available a priori . This motivates the noisy-channel model for W
ASP  X  1 , where Pr( e | f ) is divided into two smaller components: Pr( e ) is the language model , and Pr( f | e ) is the parsing model . The generation task is to find a sen-tence e such that (1) e is a good sentence a priori, and (2) its meaning is the same as the input MR. For the language model, we use an n -gram model, which is remarkably useful in ranking candidate generated sentences (Knight and Hatzivassiloglou, 1995; Ban-galore et al., 2000; Langkilde-Geary, 2002). For the parsing model, we re-use the one from W ASP (Equa-tion 1). Hence computing (2) means maximizing the following: where D ( f ) is the set of derivations that are con-sistent with f , and e ( d ) is the output sentence that a derivation d yields. Compared to most exist-ing work on generation, W ASP  X  1 has the following characteristics: 1. It does not require any lexical information in 2. Each predicate is translated to a phrase. More-
For decoding, we use an Earley chart generator that scans the input MR from left to right. This im-plies that each chart item covers a certain substring of the input MR, not a subsequence in general. It requires the order in which MR predicates appear to be fixed, i.e. the order determines the meaning of the MR. Since the order need not be identical to the word order of the target NL, there is no need for the content planner to know the target NL grammar, which is learned from the training data.
 Overall, the noisy-channel model is a weighted SCFG, obtained by intersecting the NL side of the W
ASP SCFG with the n -gram language model. The chart generator is very similar to the chart parser, ex-cept for the following: 1. To facilitate the calculation of Pr( e ( d )) , chart items now include a list of ( n  X  1) -grams that encode the context in which output NL phrases appear. The size of the list is 2 N + 2 , where N is the number of non-terminals to be rewritten in the dotted rule. 2. Words are generated from word gaps through special rules ( g )  X  h  X ,  X  X  , where the word gap, ( g ) , is treated as a non-terminal, and  X  is the NL string that fills the gap ( |  X  |  X  g ). The empty set symbol indicates that the NL string does not carry any meaning. There are similar constructs in Car-roll et al. (1999) that generate function words. Fur-thermore, to improve efficiency, our generator only considers gap fillers that have been observed during training. 3. The normalizing factor in (3), Z  X  ( e ( d )) , is not a constant and varies across the output string, e ( d ) . (Note that Z  X  ( e ) is fixed for parsing.) This is un-fortunate because the calculation of Z  X  ( e ( d )) is ex-pensive, and it is not easy to incorporate it into the chart generation algorithm. Normalization is done as follows. First, compute the k -best candidate out-put strings based on the unnormalized version of (3), Pr( e ( d )) exp P i  X  i f i ( d ) . Then re-rank the list by normalizing the scores using Z  X  ( e ( d )) , which is ob-tained by running the inside-outside algorithm on each output string. This results in a decoding al-gorithm that is approximate X  X he best output string might not be in the k -best list X  X nd takes cubic time with respect to the length of each of the k candidate output strings ( k = 100 in our experiments).
Learning in W ASP  X  1 involves two steps. First, a back-off n -gram language model with Good-Turing discounting and no lexical classes 2 is built from all training sentences using the S RILM Toolkit (Stolcke, 2002). We use n = 2 since higher values seemed to cause overfitting in our domains. Next, the parsing model is trained as described in Section 3.2. The SMT-based generation algorithms, P HARAOH and W ASP  X  1 , while reasonably effective, can be substantially improved by borrowing ideas from each other. 4.1 Improving the P HARAOH -based Generator A major weakness of P HARAOH as an NLG sys-tem is its inability to exploit the formal structure of the MRL. Like W ASP  X  1 , the phrase extraction al-gorithm of P HARAOH is based on the output of a word alignment model such as G IZA ++ (Koehn et al., 2003), which performs poorly when applied di-rectly to MRLs (Section 3.2).

We can improve the P HARAOH -based generator by supplying linearized parse-trees as input rather than flat MRs. As a result, the basic translation units are sequences of MRL productions, rather than se-quences of MR tokens. This way P HARAOH can exploit the formal grammar of the MRL to produce high-quality phrase pairs. The same idea is used in W
ASP  X  1 to produce high-quality SCFG rules. We call the resulting hybrid NLG system P HARAOH ++. 4.2 Improving the W ASP -based Generator There are several aspects of P HARAOH that can be used to improve W ASP  X  1 . First, the probabilistic model of W ASP  X  1 is less than ideal as it requires an extra re-ranking step for normalization, which is expensive and prone to over-pruning. To remedy this situation, we can borrow the probabilistic model of P HARAOH , and define the parsing model as: which is the product of the weights of the rules used in a derivation d . The rule weight, w ( X  X  h  X ,  X  i ) , is in turn defined as: P (  X  |  X  )  X  1 P (  X  |  X  )  X  2 P w (  X  |  X  )  X  3 P w (  X  |  X  ) where P (  X  |  X  ) and P (  X  |  X  ) are the relative frequen-cies of  X  and  X  , and P w (  X  |  X  ) and P w (  X  |  X  ) are the lexical weights (Koehn et al., 2003). The word penalty, exp(  X  X   X  | ) , allows some control over the output sentence length. Together with the language model, the new formulation of Pr( e | f ) is a log-linear model with  X  i as parameters. The advantage of this model is that maximization requires no nor-malization and can be done exactly and efficiently. The model parameters are trained using minimum error-rate training (Och, 2003).
 Following the phrase extraction phase in P
HARAOH , we eliminate word gaps by incorpo-rating unaligned words as part of the extracted NL phrases (Koehn et al., 2003). The reason is that while word gaps are useful in dealing with unknown phrases during semantic parsing, for generation, using known phrases generally leads to better fluency. For the same reason, we also allow the extraction of longer phrases that correspond to multiple predicates (but no more than 5).
 We call the resulting hybrid system W ASP  X  1 ++. It is similar to the syntax-based SMT system of Chi-ang (2005), which uses both SCFG and P HARAOH  X  X  probabilistic model. The main difference is that we use the MRL grammar to constrain rule extraction, so that significantly fewer rules are extracted, mak-ing it possible to do exact inference. We evaluated all four SMT-based NLG systems in-troduced in this paper: P HARAOH , W ASP  X  1 , and the hybrid systems, P HARAOH ++ and W ASP  X  1 ++.
We used the R OBO C UP and G EOQUERY corpora in our experiments. The R OBO C UP corpus consists of 300 pieces of coach advice taken from the log files of the 2003 R OBO C UP Coach Competition. The ad-vice was written in CL ANG and manually translated to English (Kuhlmann et al., 2004). The average MR length is 29.47 tokens, or 12.82 nodes for lin-earized parse-trees. The average sentence length is 22.52. The G EOQUERY corpus consists of 880 En-glish questions gathered from various sources. The questions were manually translated to the functional G
EOQUERY language (Kate et al., 2005). The av-erage MR length is 17.55 tokens, or 5.55 nodes for linearized parse-trees. The average sentence length is 7.57.

Reference: If our player 2, 3, 7 or 5 has the ball and the ball is close to our goal line ...

P HARAOH ++: If player 3 has the ball is in 2 5 the ball is in the area near our goal line ...

W ASP  X  1 ++: If players 2, 3, 7 and 5 has the ball and the ball is near our goal line ...
 Figure 4: Sample partial system output in the R P HARAOH ++ 0.4336 5.9185 0.5354 6.3637 Table 1: Results of automatic evaluation; bold type indicates the best performing system (or systems) for a given domain-metric pair ( p &lt; 0 . 05 ) 5.1 Automatic Evaluation We performed 4 runs of 10-fold cross validation, and measured the performance of the learned generators using the B LEU score (Papineni et al., 2002) and the N
IST score (Doddington, 2002). Both MT metrics measure the precision of a translation in terms of the proportion of n -grams that it shares with the refer-ence translations, with the N IST score focusing more on n -grams that are less frequent and more informa-tive. Both metrics have recently been used to eval-uate generators (Langkilde-Geary, 2002; Nakanishi et al., 2005; Belz and Reiter, 2006).

All systems were able to generate sentences for more than 97% of the input. Figure 4 shows some sample output of the systems. Table 1 shows the automatic evaluation results. Paired t -tests were used to measure statistical significance. A few observations can be made. First, W ASP  X  1 pro-duced a more accurate generator than P HARAOH . Second, P HARAOH ++ significantly outperformed P
HARAOH , showing the importance of exploiting the formal structure of the MRL. Third, W ASP  X  1 ++ significantly outperformed W ASP  X  1 . Most of the gain came from P HARAOH  X  X  probabilistic model. Decoding was also 4 X 11 times faster, despite ex-act inference and a larger grammar due to extrac-tion of longer phrases. Lastly, W ASP  X  1 ++ signifi-cantly outperformed P HARAOH ++ in the R OBO C UP domain. This is because W ASP  X  1 ++ allows dis-contiguous NL phrases and P HARAOH ++ does not. Such phrases are commonly used in R OBO C UP for constructions like: players 2 , 3 , 7 and 5 ; 26.96% of the phrases generated during testing were discontiguous. When faced with such predicates, P
HARAOH ++ would consistently omit some of the words: e.g. players 2 3 7 5 , or not learn any phrases for those predicates at all. On the other hand, only 4.47% of the phrases generated during testing for G EOQUERY were discontiguous, so the advantage of W ASP  X  1 ++ over P HARAOH ++ was not as obvious.
Our B LEU scores are not as high as those re-ported in Langkilde-Geary (2002) and Nakanishi et al. (2005), which are around 0.7 X 0.9. However, their work involves the regeneration of automati-cally parsed text, and the MRs that they use, which are essentially dependency parses, contain extensive lexical information of the target NL. 5.2 Human Evaluation Automatic evaluation is only an imperfect substitute for human assessment. While it is found that B LEU and N IST correlate quite well with human judgments in evaluating NLG systems (Belz and Reiter, 2006), it is best to support these figures with human evalu-ation, which we did on a small scale. We recruited 4 native speakers of English with no previous experi-ence with the R OBO C UP and G EOQUERY domains. Each subject was given the same 20 sentences for each domain, randomly chosen from the test sets. For each sentence, the subjects were asked to judge the output of P HARAOH ++ and W ASP  X  1 ++ in terms of fluency and adequacy. They were presented with the following definition, adapted from Koehn and Monz (2006):
Score Fluency Adequacy Table 3: Results of automatic evaluation on the mul-tilingual G EOQUERY data set
Score Fluency Adequacy For each generated sentence, we computed the av-erage of the 4 human judges X  scores. No score normalization was performed. Then we compared the two systems using a paired t -test. Table 2 shows that W ASP  X  1 ++ produced better generators than P HARAOH ++ in the R OBO C UP domain, con-sistent with the results of automatic evaluation. 5.3 Multilingual Experiments Lastly, we describe our experiments on the mul-tilingual G EOQUERY data set. The 250-example data set is a subset of the larger G EOQUERY cor-pus. All English questions in this data set were manually translated into Spanish, Japanese and Turkish, while the corresponding MRs remain un-changed. Table 3 shows the results, which are sim-ilar to previous results on the larger G EOQUERY corpus. W ASP  X  1 ++ outperformed P HARAOH ++ for some language-metric pairs, but otherwise per-formed comparably. Numerous efforts have been made to unify the tasks of semantic parsing and tactical generation. One of the earliest espousals of the notion of grammar re-versability can be found in Kay (1975). Shieber (1988) further noted that not only a single gram-mar can be used for parsing and generation, but the same language-processing architecture can be used for both tasks. Kay (1996) identified parsing charts as such an architecture, which led to the develop-ment of various chart generation algorithms: Car-roll et al. (1999) for HPSG, Bangalore et al. (2000) for LTAG, Moore (2002) for unification grammars, White and Baldridge (2003) for CCG. More re-cently, statistical chart generators have emerged, in-cluding White (2004) for CCG, Carroll and Oepen (2005) and Nakanishi et al. (2005) for HPSG. Many of these systems, however, focus on the task of sur-face realization  X  X nflecting and ordering words X  which ignores the problem of lexical selection. In contrast, our SMT-based methods integrate lexical selection and realization in an elegant framework and automatically learn all of their linguistic knowl-edge from an annotated corpus. We have presented four tactical generation systems based on various SMT-based methods. In particular, the hybrid system produced by inverting the W ASP semantic parser shows the best overall results across different application domains.
 We would like to thank Kevin Knight, Jason Baldridge, Razvan Bunescu, and the anonymous re-viewers for their valuable comments. We also sin-cerely thank the four annotators who helped us eval-uate the SMT-based generators. This research was supported by DARPA under grant HR0011-04-1-0007 and a gift from Google Inc.

