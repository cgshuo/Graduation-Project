 Data mining (e.g., classification [1,15], pattern mining [8,9,19,23], social network mining [26,27,28]) aims to discover implicit, previously unknown and potentially useful knowledge from data. Numerous studies [7,17,22] have been proposed for the research problem of frequent itemse t mining from traditional large static databases (DBs). To improve efficiency, the FP-growth algorithm [13] uses an extended prefix-tree structure called Frequent Pattern tree ( FP-tree )tocapture the contents of transaction DBs in memory. Although there are some works [3,12] that use disk-based structures for mining, they mostly mine frequent itemsets from static DBs. As a preview, we mine frequent itemsets from dynamic data streams.

The automation of measurements and data collection, together with the in-creasing development and usage of a large number of sensors, has produced streams of valuable data in many real-life application areas. In order to be able to make sense of the streaming data (e.g., detect outliers from streams [6,11]), stream mining algorithms are needed [14,20,24,25]. When compared with the mining from traditional static DBs, mining from dynamic data streams is ob-served to be more challenging [4,18] because data streams are continuous and unbounded. Once the streams flow through, we lose them. Hence, we need some data structures to capture the important contents of the streams. Moreover, as data in the streams are not necessarily uniformly distributed, their distributions are usually changing with time. A currently infrequent itemset may become fre-quent in the future, and vice versa. Hence, we must be careful not to prune infrequent itemsets too early; other wise, we may not be able to get complete information such as frequencies of certain itemsets (as it is impossible to retract those pruned itemsets).

To efficiently mine frequent itemsets from data streams, approximate algo-rithms (e.g., FP-streaming [10]) may find some infrequent itemsets or miss fre-quency information of some frequent itemsets (i.e., some false positives and some false negatives). To avoid false positives or false negatives, an exact al-gorithm mines truly frequent itemsets by (i) constructing a Data Stream Tree ( DSTree ) [21] to capture contents of the streaming data in memory and then (ii) recursively building local FP-trees for projected DBs (containing subsets of streaming data) based on the information extracted from the DSTree. This al-gorithm runs well when the global DSTree and subsequent local FP-trees fit into main memory.

To handle situations where not all local FP-trees can fit into memory, one can construct a Data Stream Projected tree ( DSP-tree ) [16] for the projected DB of every frequent singleton { x } .Asallfrequent k -itemsets containing x can then be mined from that tree, it avoids the recu rsive construction of local FP-trees for all  X  -projected DB (where k -itemset  X   X  X  x } for all k  X  1).

Along this direction, to handle situations where the global DSTree cannot fit into memory, an on-disk data structure called Data Stream Table ( DSTable )[5] can be built. Mining with such a DSTable performs well for very sparse streaming data when not too much information needs to be captured in the DSTable.
As streams of high-volume data can be g enerated at high velocity, efficient data structures and algorithms for mining frequent itemsets with limited mem-ory are in demand. In this paper, we present a simple yet powerful on-disk data structure X  X alled Data Stream Matrix ( DSMatrix ) X  X or capturing and maintain-ing relevant data found in dense data streams. This structure is designed for stream mining of frequent itemsets with sliding window models. It captures the contents of relevant transactions in the streams. When the streams flow through, a fixed-size window (i.e., a window containing the interesting portion of the streams X  X sually, recent data) slides an d our structure is p roperly updated. Although we design the DSMatrix for dense data streams in limited memory environments, this on-disk data structure can also be used as an alternative to (i) the DSTree for stream mining in env ironments with sufficient memory and/or (ii) DSTable for mining very sparse data streams.
 The remainder of this paper is organized as follows. Section 2 introduces our DSMatrix for capturing important information from data streams. Sections 3 and 4 describe the use of the DSMatrix in mining frequent itemsets horizon-tally and vertically. Section 5 shows evaluation results. Finally, conclusions are presented in Section 6.
 Given a dense stream of uncertain data with a limited memory environment, we propose an on-disk Data Stream Matrix ( DSMatrix ) structure to capture important contents of transactions in all batches of the streaming data within the current sliding window. Specifically, the DSMatrix is a two-dimensional binary matrix, which represents the presence of an item x in transaction t i by a  X 1 X  in the matrix entry (Row x ,Column t i ) and the absence of an item y from transaction t j by a  X 0 X  in the matrix entry (Row y ,Column t j ). With this binary representation of items in each transaction, each column in the DSMatrix captures a transaction. Each column in the DSMatrix can be considered as a bit vector.

Our DSMatrix is so flexible that it is applicable to different stream processing models. For instance, (i) when using the landmark model , DSMatrix just captures the contents of all transactions after the  X  X andmark X . Alternatively, (ii) when us-ing the sliding window model , DSMatrix captures the boundary information .By keeping track of the boundary that marks the end of each batch of streaming data, when the window slides, transactions in the older batches can be easily removed and transactions in the newer batches can be easily added. Note that, as the same boundaries are applied for all rows representing all m domain items, the amount of boundary information that needs to be kept does not directly depend on the num-ber of transactions (or the number of columns in the DSMatrix). The amount of the kept boundary information is proportional to the number of batches, and each batch can be of equal or variable size. Similarly, (iii) when using the time fading model , DSMatrix captures the same boundary information as in the sliding window model. The only difference is that extra wor k is required to incorporate the fading factor in the computation of the support of itemsets when using the time fading model.
 Example 1. Consider two recent batches of tran sactions in a dense data stream as shown in Fig. 1(a). Fig. 1(b) shows how our DSMatrix captures (i) contents of these w =2 batches of transactions and (ii) their associated boundary infor-mation when using a sliding window or time fading model with a window size w =2 batches. For example, Column 1 shows a transaction containing a,c,d &amp; f . The boundary information reveals that (i) the first batch (starts from Column 1 and) ends in Column 3 and (ii) the second batch (starts from Column 3+1=4 and) ends in Column 6. / 0 When data streams flow through, our DSMatrix is constantly updated so that the mining can be  X  X elayed X  until it is needed. To find frequent itemsets from the updated DSMatrix, we propose a tree-based horizontal mining algorithm .It first extracts relevant transactions from the DSMatrix to form a local tree for the { x } -projected DB for every frequent singleton { x } .Eachnodeinsuchalocal tree contains (i) an item x and (ii) a counter. The value of this counter is initially set to the support of x on that tree path and is decremented during the mining process until it reaches 0 (cf. value remains unchanged in the FP-tree). From the tree for the { x } -projected DB, we get every frequent k -itemset ( { x } X   X  )by traversing the path from leaf node y to the root (where  X  is formed by some items along that path). If this k -itemset ( { x } X   X  ) has not been generated, its support is set to the value of the counter of node y ; otherwise, its support is incremented by the value of the counter of node y . After examining node y , counters along the path from y to the root are decremented by the value of the counter of y . Any node with a zero counter can be pruned. See Example 2. Example 2. Continue Example 1 with user-specified minsup threshold = 2. We start the tree-based horizontal mining process by computing the support of the m =6 domain items, and find that all except e are frequent. Then, we form the { f } -projected DB. For every column in Row f with a value  X 1 X  (i.e., Columns 1, 2, 4 &amp; 5), we extract its column upwards. Specifically, we extract { a,c,d,f } from Column 1. We also extract { a,d,e,f } , { a,c,f } &amp; { a,c,d,f } from Columns 2, 4 &amp; 5, respectively. All these form the { f } -projected DB, from which a local tree can be built as shown in Fig. 2(a). From this tree, we traverse the left path a :4, { a,d,f } &amp; { c,d,f } , and (iii) 4-itemset { a,c,d,f } . The support of these four itemsets are set to 2 (the value in the counter for d ), and the counters of every node in this tree path are decremented by the same value to become a :2, c :1, d :0 (where the zero-counter node d can then be pruned). Based on this path, we form (i) 2-itemsets { a,f } &amp; { c,f } and (ii) 3-itemset { a,c,f } .Astheyhavepreviously been formed, their support are incremented by 1 (the current value in the counter for c ) to become 2+1=3. Again, the counters of every node in this tree path are decremented by the same value to become a :1, c :0 (where the zero-counter node c can be pruned). Afterwards, we traverse the right path a :1, d :1 to form exist, their support values are incremented by 1. The counters of nodes a and d in this right path are then decremented by 1 to become a :0, d :0 (where both zero-counter nodes a &amp; d can be pruned). To summarize, we find from { f } -projected DB the following itemsets with their support: { a,f } :2+1+1=4, { c,f } :2+1=3, { d,f } :2+1=3, { a,c,f } :2+1=3, { a,d,f } :2+1=3, { c,d,f } :2 &amp; { a,c,d,f } :2. Simi-lar procedures can be applied to the remaining four frequent domain items a,b,c &amp; d to find frequent k -itemsets containing these items. / 0 Given that the data captured in our DSMatrix can be considered as a collection of bit vectors, the DSMatrix is also applicable for vertical mining . To mine frequent 1-itemsets, we examine each row (representing a domain item). The row sum (i.e., total number of 1s) gives the support of the item represented by that row. Once the frequent 1-itemsets are found, we intersect the bit vectors for two items. If the row sum of the resulting intersection  X  minsup , then we find a frequent 2-itemset. We repeat these st eps by intersecting two bit vectors of frequent patterns to find frequent itemsets of higher cardinality.
 Example 3. Revisit Example 2. We start the vertical mining by first computing the row sum for each row (i.e., for each domain item). As a result, we find that we intersect the bit vector of a (i.e., Row a ) with any one of the remaining four intersection of gives a bit vector 110010, and ( iii) the intersection of 110110. Next, we intersect (i) find frequent 3-itemsets { a,c,d } :2, { a,c,f } :3 and { a,d,f } :3. We also intersect with containing item a . Similar procedures can be applied to the remaining four frequent domain items b,c,d &amp; f to find frequent k -itemsets containing these items. / 0 Analytical Results. Similar to mining with the DSTree [21] or DSTable [5], mining with our DSMatrix also uses a  X  X elayed X  mode for mining. So, the actual mining of frequent itemsets is delayed until it is needed to find frequent itemsets. Hence, for S =1000 batches, we only need to build a single global DSMatrix on the disk and update it 995 (= S  X  w = 1000  X  5) times. Afterwards, we have an updated DSMatrix capturing the 996th to the 1000th batches. In terms of main memory space, the DST ree [21] is an in-memory structure. In contrast, as both the DSTable [5] and our DSMatrix are on-disk structures, they do not take up memory space.

In terms of disk space, the DSTable [5] requires 64 N bits (for 32-bit integer representation) for storing the item ID and its corresponding  X  X ext X  pointer for the N entries in the DSTable (i.e., a total of N occurrences of any domain items). In contrast, as our DS Matrix consists of m rows (one row for each domain item) and n columns (one column for each transaction), it requires only m  X  n bits. For dense data streams, N is approaching m  X  n . This implies that the DSTable requires close to 64 times more than th e amount of disk space required by our DSMatrix. For sparse data streams, the D STable still requires more disk space than our DSMatrix (unless the data density in streams is lower than 1.6%). Mining with the DSTree [21] requires the construction of the in-memory global DSTree and all subsequent in-memory local trees in memory. The number of nodes in the DSTree can be as large as the number of item occurrences in the data stream. Mining with the DSTable [5] requires the construction of the on-disk global DSTable, but all subsequent local trees need to be in memory. The number of nodes in the DSTable can be as large as the number of item occurrences in the data stream. In contrast, mining with our DSMatrix does not require any construction of global tree in memory. The DSMatrix is stored on disk. Moreover, as the DSMatrix is a binary matrix, the amount of required disk space is substantially lower than that for the DSTable. Horizontal mining with our DSMatrix builds at most m local trees (one for each projected DB containing frequent itemsets); vertical mining with our DSMatrix intersects at most 2 m pairs of bit vectors (though usually much fewer than 2 m pairs of bit vectors). Experimental Results. We used many different databases including IBM syn-thetic data, real-life DBs from the UC Irvine Machine Learning Depository (e.g., connect4 data) as well as those from the Frequent Itemset Mining Implemen-tation (FIMI) Dataset Repository. IBM synthetic data are generated by the program developed at IBM Almaden Research Centre [2]. The data contain many records with an average transaction length of 10 items, and a domain of 1000 items. We set each batch to be 0.1M transactions and the window size w =5 batches. All experiments were run in a time-sharing environment in a 1 GHz machine. The reported figures are based on the average of multiple runs. Run-time includes CPU and I/Os; it includes the time for both tree construction and frequent itemset mining steps. In th e experiments, we mainly evaluated the accuracy and efficiency of our DSMatrix.

In the first experiment, we measured th e accuracy of the three mining options: (i) global DSTree, local trees , (ii) global DSTable, local trees , and (iii) global DSMatrix, local trees options. Experimental results show that mining with any of these three options give the same mining results.

While these three mining options found the same frequent patterns, their per-formance varied. In the second and thi rd experiments, we measured the space and time efficiency of our proposed DSTable and DSMatrix. Results show that the DSTree option required the largest main memory space as it stores one global DSTree and multiple local FP-trees in main memory. The DSTable option re-quired less space as the global DSTable is a disk-based structure. The DSMatrix option required the smallest memory space because our proposed DSMatrix is a disk-based structure and it is a binary matrix. See Fig. 3(a).

Runtime performance of the three options also varied. When compared with the DSTree, both the DSTable and DSMatrix options just took slightly longer. This is because they both need to read from disk whereas the former two just read from main memory. See Fig. 3(b). It is important to note that reading from disk would be a logical choice in a limited main memory environment.
Moreover, we performed some addition al experiments. In the fourth experi-ment, we tested with the effect of minsup . As shown in Fig. 3(b), the runtime decreased when minsup increased. In the fifth experiment, we tested scalabil-ity with the number of transactions. The results show that mining with our DSMatrix was scalable.

We repeated the above experiments on o ther dense as well as sparse datasets. This paper provides users with (i) a simple yet powerful on-disk structure (DS-Matrix) for capturing important information of dense data streams when mem-ory space is limited, (ii) a tree-based horizontal mining algorithm that extracts information from our DSMatrix to find frequent itemsets, and (iii) a vertical mining algorithm that intersects the  X  X it vectors X  stored in our DSMatrix to find frequent itemsets. Analytical and experimental evaluation show the contri-butions of DSMatrix in frequent itemset mining from dense data streams. Acknowledgement. This research project is partially supported by (i) NSERC (Canada) and University of Manitoba, as well as (ii) DAPA (South Korea) under the contract UD110006MD.
