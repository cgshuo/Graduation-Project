 Eye tracking experiments have shown that titles of Web search re-sults play a crucial role in guiding a user X  X  search process. We present a machine-learned algorithm that trains a boosted tree to pick the most relevant title for a Web search result. We compare two modeling approaches: i) using absolute editorial judgments and ii) using pairwise preference judgments. We find that the pair-wise modeling approach gives better results in terms of three of-fline metrics. We present results of our models in four regions. We also describe a hybrid user satisfaction evaluation process  X  search success  X  that combines page relevance and user click behavior, and show that our machine-learned algorithm improves in search success.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Experimentation, Theory Web summarization, machine learning, user satisfaction
While the  X  X en blue links X  that search engines return in response to a query are important in the user X  X  search, the titles and sum-maries associated with the links can greatly influence a user X  X  per-ception of the link X  X  relevance and the efficiency of the search.  X  This was done while Kanungo was at Yahoo! Labs.
 Furthermore, badly formed abstracts can lead to  X  X lick inversions X  where documents ranked lower get more clicks [1].

Titles on the search result page convey the first impression of relevance of the pages to the user. Most major search engines use a variety of sources to pick the title for a search result. Common sources include: HTML title, anchor text, internal anchor text, open directory page title, various HTML headline titles on the page, Ya-hoo! directory page title, etc. Thus it is important to have a system-atic way of picking the best title from the candidate set.
Web page summaries can be query-independent [6] or query de-pendent [9, 10]. A query-independent summary conveys general information about the document, and can be computed offline and cached for fast access. The main problem with query-independent summaries is that they do not convey to the user why the Web page is relevant to the query. Query-dependent summarization attempts to address this by biasing the summaries towards the query. These summaries are typically constructed at query time.

In this paper, we present a machine-learned algorithm for select-ing titles. Given a query and a Web page, a query-dependent title is generated as follows. First, all candidate titles are identified, and their features are extracted. Next for each title, we get an edito-rial (human) judgment regarding their relevance to the query and document on an absolute scale. Then we learn a regression model using features as the independent variables and the judgment as the target. We also show how to induce pairwise preference judgments and train a regression model using the preference judgments.
Machine learning approaches have been recently proposed for query-dependent sentence selection. Wang et al. [10] showed that ranking support vector machines (SVMs) outperform SVM clas-sifiers and BM25 on a test collection that only consisted of 10 queries. Metzler and Kanungo [8] use TREC data for sentence selection and propose a machine learning framework. However, they do not apply it to the Web domain and do not provide any click-based evaluation. The above approaches are similar to those proposed in the document ranking literature [7, 11].

Numerous evaluation approaches [2, 4, 5] have been used in the past to to model and measure user satisfaction. Our summariza-tion work, in contrast is based on large samples of Web data and we use a hybrid evaluation approach based on clicks and editorial judgments to judge user satisfaction, in addition to the standard IR metrics like mean reciprocal rank and discounted cumulative gain.
The key contributions of our work are: i) we present a machine-learned algorithm for selecting titles of Web pages in Web search results, ii) we compare absolute regression models for title selec-tion with pairwise preference based models, iii) we propose a hy-brid user satisfaction evaluation method that uses clicks and edito-rial judgments to quantify user satisfaction, and iv) we show results on the US, Taiwan (TW), Korea (KR) and Japan (JP) regions.
In this section, we describe two machine learning algorithms for title selection. We use Gradient Boosted Decision Trees (GBDT) to learn models from absolute judgments and Gradient Boosted Rank-ing (GBRank) to learn from pairwise preference judgments.
GBDT is a technique that can be used for estimating a regres-sion model. We use the stochastic variant of GBDTs [3]. GBDTs compute a function approximation by performing a numerical op-timization in the function space instead of the parameter space. We provide an overview of the the GBDT algorithm.

A basic regression tree f ( x ) , x  X  X  K , partitions the space of ex-planatory variable values into disjoint regions R j , j = 1 , 2 , . . . , J associated with the terminal nodes of the tree. Each region is as-signed a value  X  j such that f ( x ) =  X  j if x  X  R j . Thus the com-plete tree is represented as: where  X  = { R j ,  X  j } J 1 , and I is the indicator function. Let ( x i = 1 , . . . , N be the given set of points. For a loss function L ( y i ,  X  j ) , parameters are estimated by minimizing the total loss:
A boosted tree is an aggregate of such trees, each of which is computed in a sequence of stages. That is, where at each stage m,  X  m is estimated to fit the residuals from the m  X  1 th stage: where  X  is a the learning rate . In the stochastic version of GBDT, instead of using the entire data set to compute the loss function, one sub-samples the data and finds the values  X  j that minimize the loss on the test set. The stochastic variant minimizes over-fitting.
In practice, one has to empirically set (by cross-validation) the parameters: the number of trees M , the number of nodes per tree P , learning rate  X , and sampling rate  X , (in the stochastic version).
During the process of collecting absolute judgments, an editor first sees a query and a set of possible titles and then assigns abso-lute grades to the titles. However, since the editor sees all the titles simultaneously, we suspect the grades are not independent. That is, the grades assigned can easily influence the grade of the next title. This facilitates the use of preference judgments for modeling. preference judgments such that x i  X  R K is the feature vector for and title t ( y i ) , and the editorial grades are represented by g ( x and g ( y i ) respectively.

The learning algorithm needs to learn a function h such that ber of disagreements with the editorial judgments. GBRank [11] tries to achieve this and a sketch of the algorithm is given below. 1. Guess an initial h k for k = 0 . 2. For k = 1 , . . . , M.
As in the case of GBDT, GBRank parameters M, P,  X ,  X , and  X  have to be experimentally set (by cross-validation). Both GBDT and GBRank also provide feature importance [3], which is com-puted by keeping track of the reduction in the loss function at each feature variable split and then computing the total reduction of loss function along each explanatory feature variable. The importance is useful for analyzing which features contribute most to the model.
In this section we describe a subset of the features we used in our experiments. Some features are query-dependent and others are query-independent.
Query-dependent features capture relevance at different levels of granularity and expressiveness. They include: UniqueQueryUnitHits: Unique query term hits DuplicateQueryUnitHits: Repeated query term hits QueryTermHitsFrac: Fraction of query terms hit FirstHitOffset: The position of first query term hit HitsCompactness: Number of hits over the hit offset range URLMatchQ: Number of title terms found in the URL that are
Query-independent features attempt to express prior knowledge about titles. They represent the degree to which the title captures the document nature and genre. In fact, they can be used, in part, to pick the best query-independent title. They include: ClickTextMatch: Fraction of terms that are present in the URL X  X  URLMatch: Fraction of terms in URL that also occur in the title URLMatch: Fraction of title terms that also occur in the URL ScriptLFrac: extent of foreign language characters or words TitleSourceX: Binary features indicating title source
Other query-independent features capture structural attributes of titles, thus addressing readability at more coarse granularity. They consider fraction of capitalized letters and words, title length in words and characters, word length, and punctuation, for example.
Our train and test data was generated as follows. We randomly sampled 425 queries from a two-week query stream in the US re-gion. Each query was issued to the search engine and top 10 URLs were collected. Then a random subset of 2,169 query-URL pairs was selected for editorial judgment. On average, a URL has three title candidates, and URLs for popular queries have up to seven ti-tles. The final train-test sample consists of 7,456 query-URL-title triples. Table 1 has an example of titles for a query-URL pair.
For each query-URL pair, editors assigned a grade (1-5, 5 being the best possible grade) for each title. In addition, they chose a single best title among candidate titles. Data set generation in JP, TW, and KR was similar. Characteristics of the data are in Table 2. Table 1: An example query-URL pair and associated titles. URL http://www.theknot.com/ QUERY the knot
Title Source Title Text Judgment anchortext the knot 4 headline1 featured content 2 Ydirectory The Knot 5
HTML Wedding Dresses | Wedding Cakes 3 Table 2: Data Characteristics:  X  X vg # t  X  is average number of title candidates for all query URL pairs.  X  X vg Grd X  represents average grade of all titles while  X  X vg B X  and  X  X vg NB X  are av-erage grade of best titles and of non-best titles, respectively.
Experiments consisted of multiple trials. In each trial we trained on a random 70% of data (without replacement) and tested on the remaining 30%. We report averages over trials and t -tests to deter-mine whether the optimal model is better than the basic model.
Query popularity is used to influence the tuple weight in training and testing. We used a scaled, discretized, and smoothed log query frequency as a weight.

We ran experiments varying GBDT/GBRank parameters (differ-ent values for M , P ,  X  , and  X  , for example). We chose the optimal parameter setting via cross-validation. 1
Several offline measures are used to estimate the quality of a model. Suppose the test data consists of query-URL pairs D = ( q , u 1 ) ... ( q n , u N ) . The metrics are as follows: For US pw.optimal : M = 750 , P = 20 ,  X  = 0 . 05 ,  X  = 0 . 3 . For US gb.optimal : M = 500 , P = 6 ,  X  = 0 . 05 and  X  = 0 . 7 . For JP/TW/KR gr.optimal : M = 1000 ,  X  = 0 . 15 and  X  = 0 . 7 . For JP/TW P = 10 and for KR P = 8 . where g ( t ) is the grade of title t and r f M is the ranking function according to f M , for the i th query-URL pair. b ( t j ) = 1 if t best title picked by the editor, and 0 otherwise.
We present results for the GBDT and GBRank title selection al-gorithms for US, JP, TW and KR, and compare the two modeling approaches. Performance of GBRank in the pairwise setting is sig-nificantly better than GBDT. We present the timing performance of these algorithms.There are four types of models:
On average over 30 random trials in US, pw.basic had a 0.56% improvement in ACC over gr.basic . However, adding features and optimizing to produce gr.optimal yielded a 0.6% improvement over gr.basic . The model pw.optimal trumps this improvement, resulting in a 2.76% improvement in performance over gr.basic (see Table 3). Model pw.optimal is better than pw.basic by 2.17% for US, 3.07% for JP, 4.09% for TW and 4.35% for KR.
 Table 3: Average performance over 30 trials for the three offline metrics. t -tests show that the optimal model is better than the basic model for the three regions. In each case we find that the difference is statistically significant at the 0.05 level.
Additionally we use metrics that evaluate the coverage , or frac-tion of titles that changed, over a set of 3000 random query-url pairs. Model pw.optimal had 11% coverage with respect to gr.basic .
Parameter  X  , described in section 2, allows judgment grades to influence the score (  X  =  X  ( g ( x i )  X  g ( y i )) ). Value  X  = 0 uses only pairwise preferences. Figure 1 suggests that graded judgments decrease performance when used in this way (for 0 &lt;  X  &lt; 0 . 1 the same pattern occurred). One possible explanation is that pairwise preferences are more reliable than graded judgments in a pairwise setting, and the grades do not add much to the data point.
Table 3 suggests that with the addition of a richer feature vocab-ulary, GBRank performance improves, while GBDT performance may saturate. GBDT modeling may be more sensitive to sparse training datasets than pairwise models, since it requires sufficient data for each grade. Additionally, while GBDT has N training ex-amples for each query-URL pair, GBRank has C ( N, 2) pairs of feature vectors (43% more datapoints than GBDT).

Both approaches select titles in real-time. Based on 96,545 ran-domly selected query-URL pairs, on average, gr.optimal spent 0.12 ms to select the best title, while pw.optimal spent 0.24 ms. The difference can be explained partly by the fact that pw.optimal uses 750 trees while pw.basic uses 500 trees. The performance numbers were collected on a 1.8GHz Intel Xenon machine running Linux.
According to feature importance, for GBDT, the top two features were structural. QueryTermHitsFrac, URLMatch, ClickTextMatch and TitleHTML were also important. GBRank was similar, but ranked TitleHTML, FirstHitOffset, ClickTextMatch and URLMatch relatively higher than GBDT. Figure 1:  X  vs performance for the average over all parameter configurations as well as for the top three performing configu-rations. pw.optimal uses M = 750 trees; other models use the same parameters but M = 500 or M = 250 .
We measured user satisfaction with the  X  X earch success X  metric (S), which is defined per query as follows:
We randomly sampled a few thousand queries from the control ( N control ) and test ( N test ) populations over the same week. For each clicked result, the query document relevance was measured by editorial staff, and whether the query was navigational or not. The search success S was measured for every query and the mean search success for test  X  S test and control  X  S control A Gaussian approximation to the binomial distribution was used to estimate the error:  X  = Overall search success rates for the control sample are shown in Table 4, along with click through rate (CTR) for the query results returned at ranks 1 ( CTR 1 ) and 2 ( CTR 2 ). We have divided the queries into navigational vs non-navigational, as well as the cases where the query result document relevance at rank 1 ( g 1 or worse than the query result document relevance at rank 2 ( g
The test sample showed a decrease in CTR 1 by 2%, an increase in CTR 2 by 1%, and a statistically significant increase in mean search success  X  S by 0.7% ( &gt; 0 . 1% at 95% confidence). We found that the queries which had g 1 &lt; g 2 contributed to the decrease query ranking % CTR@1 CTR@2  X  S non-nav g 1  X  g 2 59.5% 33.4% 11.6% 25.4% non-nav g 1 &lt; g 2 9.4% 27.7% 18.1% 18.1% in CTR 1 ; in particular, we observed a decrease of clicks on bad and fair documents with the improved summaries. The increase in CTR 2 (and lower) was due to more clicks on good or better documents. The combination of these effects resulted in an increase in  X  S , which we interpret to be an improvement in user satisfaction. We obtained similar results to those in Table 4 for TW and JP.
Titles and abstracts shown on a search result page influence user perception of a link X  X  relevance. In this paper we presented a fast query-dependent machine-learned algorithm for selecting titles for links to Web pages on a Web search result page. We presented a model trained using absolute human judgments, and pairwise pref-erence judgments. We find that the pairwise training algorithm per-forms better than the absolute judgment model. We also presented a hybrid metric, search success , that uses clicks and editorial judg-ments to quantify user satisfaction, and show that our algorithm performs better than the baseline. [1] C. Clarke, E. Agchtein, S. Dumais, and R. White. The [2] S. Dumais, T. Joachims, K. Bharat, and A. Weigend. SIGIR [3] J. H. Friedman. Stochastic gradient boosting. Computational [4] M. A. Hearst. Models of information seeking. In Search User [5] R. Khan, D. Mease, and R. Patel. The impact of result [6] J. Kupiec, J. Pedersen, and F. Chen. A trainable document [7] P. Li, C. J. Burges, and Q. Wu. Mcrank: Learning to rank [8] D. Metzler and T. Kanungo. Machine learned sentence [9] A. Tombros and M. Sanderson. Advantages of query biased [10] C. Wang, F. Jing, L. Zhang, and H.-J. Zhang. Learning [11] Z. Zheng, H. Zha, , K. Chen, and G. Sun. A regression
