 Information retrie val with statistical language mod-els (Laf ferty and Zhai, 2003) has recently attracted much more attention because of its solid theoreti-cal background as well as its good empirical per -formance. In this approach, queries and documents are assumed to be sampled from hidden generati ve models, and the similarity between a document and a query is then calculated through the similarity be-tween their underlying models.

Clearly , good retrie val performance relies on the accurate estimation of the query and document mod-els. Indeed, smoothing of document models has been pro ved to be very critical (Chen and Good-man, 1998; Kneser and Ne y, 1995; Zhai and Laf-ferty , 2001b). The need for smoothing originated from the zero count problem: when a term does not occur in a document, the maximum lik elihood esti-mator would give it a zero probability . This is un-reasonable because the zero count is often due to in-suf cient sampling, and a lar ger sample of the data would lik ely contain the term. Smoothing is pro-posed to address the problem.

While most smoothing methods utilize the global collection information with a simple interpolation (Ponte and Croft, 1998; Miller et al., 1999; Hiemstra and Kraaij, 1998; Zhai and Laf ferty , 2001b), sev-eral recent studies (Liu and Croft, 2004; Kurland and Lee, 2004) have sho wn that local corpus structures can be exploited to impro ve retrie val performance. In this paper , we further study the use of local cor -pus structures for document model estimation and propose to use document expansion to better exploit local corpus structures for estimating document lan-guage models.

According to statistical principles, the accurac y of a statistical estimator is lar gely determined by the sampling size of the observ ed data; a small data set generally would result in lar ge variances, thus can not be trusted completely . Unfortunately , in re-trie val, we often have to estimate a model based on a single document. Since a document is a small sam-ple, our estimate is unlik ely to be very accurate.
A natural impro vement is to enlar ge the data sam-ple, ideally in a document-specic way. Ideally , the enlar ged data sample should come from the same original generati ve model. In reality , howe ver, since the underlying model is unkno wn to us, we would not really be able to obtain such extra data. The essence of this paper is to use document expansion to obtain high quality extra data to enlar ge the sam-ple of a document so as to impro ve the accurac y of the estimated document language model. Docu-ment expansion was pre viously explored in (Sing-hal and Pereira, 1999) in the conte xt of the vec-tor space retrie val model, mainly involving selecting more terms from similar documents. Our work dif-fers from this pre vious work in that we study doc-ument expansion in the language modeling frame-work and implement the idea quite dif ferently .
Our main idea is to augment a document prob-abilistically with potentially all other documents in the collection that are similar to the document. The probability associated with each neighbor document reects how lik ely the neighbor document is from the underlying distrib ution of the original document, thus we have a  X probabilistic neighborhood X , which can serv e as  X extra data X  for the document for es-timating the underlying language model. From the vie wpoint of smoothing, our method extends the ex-isting work on using clusters for smoothing (Liu and Croft, 2004) to allo w each document to have its own cluster for smoothing.

We evaluated our method using six representati ve retrie val test sets. The experiment results sho w that document expansion smoothing consistently outper -forms the baseline smoothing methods in all the data sets. It also outperforms a state-of-the-art cluster -ing smoothing method. Analysis sho ws that the impro vement tends to be more signicant for short documents, indicating that the impro vement indeed comes from the impro ved estimation of the docu-ment language model, since a short document pre-sumably would benet more from the neighborhood smoothing. Moreo ver, since document expansion and pseudo feedback exploit dif ferent corpus struc-tures, the y can be combined to further impro ve per -formance. As document expansion can be done in the inde xing stage, it is scalable to lar ge collections. 2.1 The KL-di vergence retrie val model We rst briey revie w the KL-di vergence retrie val model, on which we will develop the document expansion technique. The KL-di vergence model is a representati ve state-of-the-art language model-ing approach for retrie val. It covers the basic lan-guage modeling approach (i.e., the query lik elihood method) as a special case and can support feedback more naturally .

In this approach, a query and a document are as-sumed to be generated from a unigram query lan-guage model  X  model  X  ment, we would rst compute an estimate of the cor -responding query model (  X   X  (  X 
 X  D ), and then score the document w.r.t. the query based on the KL-di vergence of the two models (Laf-ferty and Zhai, 2001):
D (  X   X  Q ||  X   X  d ) = X where V is the set of all the words in our vocab ulary . The documents can then be rank ed according to the ascending order of the KL-di vergence values.
Clearly , the two fundamental problems in such a model are to estimate the query model and the doc-ument model, and the accurac y of our estimation of these models would affect the retrie val performance signicantly . The estimation of the query model can often be impro ved by exploiting the local cor -pus structure in a way similar to pseudo-rele vance feedback (Laf ferty and Zhai, 2001; La vrenk o and Croft, 2001; Zhai and Laf ferty , 2001a). The esti-mation of the document model is most often done through smoothing with the global collection lan-guage model (Zhai and Laf ferty , 2001b), though re-cently there has been some work on using clusters for smoothing (Liu and Croft, 2004). Our work is mainly to extend the pre vious work on document smoothing and impro ve the accurac y of estimation by better exploiting the local corpus structure. We now discuss all these in detail. 2.2 Smoothing of document models Given a document d , the simplest way to estimate the document language model is to treat the docu-ment as a sample from the underlying multinomial word distrib ution and use the maximum lik elihood estimator: P ( w |  X   X  the count of word w in document d , and | d | is the length of d . Ho we ver, as discussed in virtually all the existing work on using language models for re-trie val, such an estimate is problematic and inaccu-rate; indeed, it would assign zero probability to any word not present in document d , causing problems in scoring a document with query lik elihood or KL-divergence (Zhai and Laf ferty , 2001b). Intuiti vely , such an estimate is inaccurate because the document is a small sample.

To solv e this problem, man y dif ferent smoothing techniques have been proposed and studied, usually involving some kind of interpolation of the maxi-mum lik elihood estimate and a global collection lan-guage model (Hiemstra and Kraaij, 1998; Miller et al., 1999; Zhai and Laf ferty , 2001b). For exam-ple, Jelinek-Mercer(JM) and Dirichlet are two com-monly used smoothing methods (Zhai and Laf ferty , 2001b). JM smoothing uses a x ed parameter  X  to control the interpolation: while the Dirichlet smoothing uses a document-dependent coef cient (parameterized with  X  ) to con-trol the interpolation: Here P ( w |  X  the collection language model  X  estimated using the whole collection of documents C , e.g., P ( w |  X  2.3 Cluster -based document model (CBDM) Recently , the cluster structure of the corpus has been exploited to impro ve language models for retrie val (K urland and Lee, 2004; Liu and Croft, 2004). In particular , the cluster -based language model pro-posed in (Liu and Croft, 2004) uses clustering infor -mation to further smooth a document model. It di-vides all documents into K dif ferent clusters ( K = 1000 in their experiments). Both cluster informa-tion and collection information are used to impro ve the estimate of the document model: P ( w |  X   X  d ) =  X  where  X  and  X  and  X  are smoothing parameters. In this clustering-based smoothing method, we rst smooth a cluster model with the collection model using Dirichlet smoothing, and then use smoothed cluster model as a new reference model to further smooth the document model using JM smoothing; empirical results sho w that the added cluster information in-deed enhances retrie val performance (Liu and Croft, 2004). 2.4 Document expansion From the vie wpoint of data augmentation, the clustering-based language model can be regarded as  X expanding X  a document with more data from the cluster that contains the document. This is intu-itively better than simply expanding every document with the same collection language model as in the case of JM or Dirichlet smoothing. Looking at it from this perspecti ve, we see that, as the  X extra data X  for smoothing a document model, the cluster con-taining the document is often not optimal. Indeed, the purpose of clustering is to group similar doc-uments together , hence a cluster model represents well the overall property of all the documents in the cluster . Ho we ver, such an average model is often not accurate for smoothing each indi vidual document. We illustrate this problem in Figure 1(a), where we sho w two documents d and a in cluster D . Clearly the generati ve model of cluster D is more suitable for smoothing document a than document d . In gen-eral, the cluster model is more suitable for smooth-ing documents close to the centroid, such as a , but is inaccurate for smoothing a document at the bound-ary , such as d .

To achie ve optimal smoothing, each document should ideally have its own cluster centered on the document, as sho wn in Figure 1(b). This is pre-cisely what we propose  X  expanding each document with a probabilistic neighborhood around the doc-ument and estimate the document model based on such a virtual, expanded document. We can then ap-ply any simple interpolation-based method (e.g., JM or Dirichlet) to such a  X virtual document X  and treat the word counts given by this  X virtual document X  as if the y were the original word counts.

The use of neighborhood information is worth more discussion. First of all, neighborhood is not a Figure 1: Clusters, neighborhood, and document ex-pansion clearly dened concept. In the narro w sense, only a few documents close to the original one should be included in the neighborhood, while in the wide sense, the whole collection can be potentially in-cluded. It is thus a challenge to dene the neighbor -hood concept reasonably . Secondly , the assumption that neighbor documents are sampled from the same generati ve model as the original document is not completely valid. We probably do not want to trust them so much as the original one. We solv e these two problems by associating a condence value with every document in the collection, which reects our belief that the document is sampled from the same underlying model as the original document. When a document is close to the original one, we have high condence, but when it is farther apart, our con-dence would fade away. In this way, we construct a probabilistic neighborhood which can potentially include all the documents with dif ferent condence values. We call a language model based on such a neighborhood document expansion langua ge model (DELM).

Technically , we are looking for a new enlar ged document d 0 for each document d in a text collec-tion, such that the new document d 0 can be used to estimate the hidden generati ve model of d more accurately . Since a good d 0 should presumably be based on both the original document d and its neigh-borhood N ( d ) , we dene a function  X  :
The precise denition of the neighborhood con-cept N ( d ) relies on the distance or similarity be-tween each pair of documents. Here, we simply choose the commonly used cosine similarity , though other choices may also be possible. Given any two document models X and Y , the cosine similarity is Figure 2: Normal distrib ution of condence values. dened as:
To model the uncertainty of neighborhood, we as-sign a condence value  X  the collection to indicate how strongly we belie ve b is sampled from d 's hidden model. In general,  X  can be set based on the similarity of b and d  X  the more similar b and d are, the lar ger  X  be. With these condence values, we construct a probabilistic neighborhood with every document in it, each with a dif ferent weight. The whole problem is thus reduced to how to dene  X 
Intuiti vely , an exponential decay curv e can help regularize the inuence from remote documents. We therefore want  X  centered around d . Figure 2 illustrates the shape of this distrib ution. The black dots are neighbor -hood documents centered around d . Their proba-bility values are determined by their distances to the center . We fortunately observ e that the cosine sim-ilarities, which we use to decide the neighborhood, are roughly of this decay shape. We thus use them directly without further transformation because that would introduce unnecessary parameters. We set  X  ( b ) by normalizing the cosine similarity scores :
Function  X  serv es to balance the condence be-tween d and its neighborhood N ( d ) in the model es-timation step. Intuiti vely , a shorter document is less suf cient, hence needs more help from its neighbor -hood. Con versely , a longer one can rely more on itself. We use a parameter  X  to control this balance. Thus nally , we obtain a pseudo document d 0 with the follo wing pseudo term count: We hypothesize that, in general,  X  more accurately from d 0 rather than d itself because d 0 contains more complete information about  X  This hypothesis can be tested by by comparing the retrie val results of applying any smoothing method to d with those of applying the same method to d 0 . In our experiments, we will test this hypothesis with both JM smoothing and Dirichlet smoothing.

Note that the proposed document expansion tech-nique is quite general. Indeed, since it transforms the original document to a potentially better  X ex-panded document X , it can presumably be used to-gether with any retrie val method, including the vec-tor space model. In this paper , we focus on evalu-ating this technique with the language modeling ap-proach.

Because of the decay shape of the neighborhood and for the sak e of efcienc y, we do not have to ac-tually use all documents in C  X  { d } . Instead, we can safely cut off the documents on the tail, and only use the top M closest neighbors for each document. We sho w in the experiment section that the performance is not sensiti ve to the choice of M when M is suf-ciently lar ge (for example 100 ). Also, since doc-ument expansion can be done completely ofine, it can scale up to lar ge collections. We evaluate the proposed method over six repre-sentati ve TREC data sets (Voorhees and Harman, 2001): AP (Associated Press news 1988-90), LA (LA Times), WSJ (W all Street Journal 1987-92), SJMN (San Jose Mercury Ne ws 1991), DOE (De-partment of Ener gy), and TREC8 (the ad hoc data used in TREC8). Table 1 sho ws the statistics of these data.

We choose the rst four TREC data sets for per -formance comparison with (Liu and Croft, 2004). To ensure that the comparison is meaningful, we use identical sources (after all preprocessing). In addi-tion, we use the lar ge data set TREC8 to sho w that our algorithm can scale up, and use DOE because its documents are usually short, and our pre vious expe-rience sho ws that it is a relati vely dif cult data set. 3.1 Neighborhood document expansion Our model boils down to a standard query lik elihood model when no neighborhood document is used. We therefore use two most commonly used smoothing methods, JM and Dirichlet , as our baselines. The re-sults are sho wn in Table 2, where we report both the mean average precision (MAP) and precision at 10 documents. JM and Dirichlet indicate the standard language models with JM smoothing and Dirichlet smoothing respecti vely , and the other two are the ones combined with our document expansion. For both baselines, we tune the parameters (  X  for JM, and  X  for Dirichlet) to be optimal. We then use the same values of  X  or  X  without further tuning for the document expansion runs, which means that the pa-rameters may not necessarily optimal for the docu-ment expansion runs. Despite this disadv antage, we see that the document expansion runs signicantly outperform their corresponding baselines, with more than 15% relati ve impro vement on AP . The parame-ters M and  X  were set to 100 and 0 . 5 , respecti vely .
To understand the impro vement in more detail, we sho w the precision values at dif ferent levels of recall for the AP data in Table 3. Here we see that our method signicantly outperforms the baseline at ev-ery precision point.

In our model, we introduce two additional param-eters: M and  X  . We rst examine M here, and then study  X  in Section 3.3. Figure 3 sho ws the perfor -mance trend with respect to the values of M . The x-axis is the values of M , and the y-axis is the non-interpolated precision averaging over all 50 queries. We dra w two conclusions from this plot: (1) Neigh-borhood information impro ves retrie val accurac y; adding more documents leads to better retrie val re-sults. (2) The performance becomes insensiti ve to Table 3: PR curv e on AP data. *,**,*** indicate that we accept the impro vement hypothesis by Wilcoxon test at signicant level 0.1, 0.05, 0.01 respecti vely . M when M is suf ciently lar ge, namely 100 . The reason is twofold: First, since the neighborhood is centered around the original document, when M is lar ge, the expansion may be evenly magnied on all term dimensions. Second, the exponentially decay-ing condence values reduce the inuence of remote documents. 3.2 Comparison with CBDM In this section, we compare the CBDM method us-ing the model performing the best in (Liu and Croft, 2004) 1 . Furthermore, we also set Dirichlet prior pa-rameter  X  = 1000 , as mentioned in (Liu and Croft, 2004), to rule out any potential inuence of Dirichlet smoothing.

Table 4 sho ws that our model outperforms CBDM in MAP values on four data sets; the impro vement Figure 3: Performance change with respect to M presumably comes from a more principled way of exploiting corpus structures. Given that clustering can at least capture the local structure to some ex-tent, it should not be very surprising that the im-pro vement of document expansion over CBDM is much less than that over the baselines.

Note that we cannot fulll Wilcoxon test because of the lack of the indi vidual query results of CBDM. 3.3 Impact on short documents Document expansion is to solv e the insuf cient sam-pling problem. Intuiti vely , a short document is less suf cient than a longer one, hence would need more  X help X  from its neighborhood. We design experi-ments to test this hypothesis.

Specically , we randomly shrink each document in AP88-89 to a certain percentage of its original length. For example, a shrinkage factor of 30% means each term has 30% chance to stay , or 70% chance to be ltered out. In this way, we reduce the original data set to a new one with the same number
Figure 4: Performance change with respect to  X  of documents but a shorter average document length.
Table 5 sho ws the experiment results over docu-ment sets with dif ferent average document lengths. The results indeed support our hypothesis that doc-ument expansion does help short documents more than longer ones. While we can manage to impro ve 41% on a 30% -length corpus, the same model only gets 16% impro vement on the full length corpus.
To understand how  X  affects the performance we plot the sensiti vity curv es in Figure 4. The curv es all look similar , but the optimal points slightly migrate when the average document length becomes shorter . A 100% corpus gets optimal at  X  = 0 . 4 , but 30% corpus has to use  X  = 0 . 2 to obtain its optimum. (All optimal  X  values are presented in the fourth row of Table 5.) 3.4 Further impr ovement with pseudo Query expansion has been pro ved to be an effec-tive way of utilizing corpus information to impro ve the query representation (Rocchio, 1971; Zhai and Laf ferty , 2001a). It is thus interesting to examine whether our model can be combined with query ex-pansion to further impro ve the retrie val accurac y. We use the model-based feedback proposed in (Zhai and Laf ferty , 2001a) and tak e top 5 returned docu-ments for feedback. There are two parameters in the model-based pseudo feedback process: the noisy pa-Table 6: Combination with pseudo feed-back.*,**,*** indicate that we accept the impro ve-ment hypothesis by Wilcoxon test at signicant level 0.1, 0.05, 0.01 respecti vely . Table 7: Performance of the interpolation algorithm combined with the pseudo feedback. rameter  X  and the interpolation parameter  X  2 . We x  X  = 0 . 9 and tune  X  to optimal, and use them directly in the feedback process combined with our models. (It again means that  X  is probably not optimal in our results.) The combination is conducted in the fol-lowing way: (1) Retrie ve documents by our DELM method; (2) Choose top 5 document to do the model-based feedback; (3) Use the expanded query model to retrie ve documents again with DELM method.
Table 6 sho ws the experiment results (MAP); in-deed, by combining DELM with pseudo feedback, we can obtain signicant further impro vement of performance.

As another baseline, we also tested the algorithm proposed in (K urland and Lee, 2004). Since the al-gorithm overlaps with pseudo feedback process, it is not easy to further combine them. We implement its best-performing algorithm,  X interpolation X  (labeled as inter . ), and sho w the results in Table 7. Here, we use the same three data sets as used in (K urland and Lee, 2004). We tune the feedback parameters to optimal in each experiment. The second last column in Table 7 sho ws the performance of combination of the  X interpolation X  model with the pseudo feedback and its impro vement percentage. The last column is the z-scores of Wilcoxon test. The negati ve z-scores indicate that none of the impro vement is signicant. In this paper , we proposed a novel document expan-sion method to enrich the document sample through exploiting the local corpus structure. Unlik e pre-vious cluster -based models, we smooth each doc-ument using a probabilistic neighborhood centered around the document itself.

Experiment results sho w that (1) The proposed document expansion method outperforms both the  X no expansion X  baselines and the cluster -based mod-els. (2) Our model is relati vely insensiti ve to the set-ting of parameter M as long as it is suf ciently lar ge, while the parameter  X  should be set according to the document length; short documents need a smaller  X  to obtain more help from its neighborhood. (3) Document expansion can be combined with pseudo feedback to further impro ve performance. Since any retrie val model can be presumably applied on top of the expanded documents, we belie ve that the pro-posed technique can be potentially useful for any re-trie val model. This work is in part supported by the National Sci-ence Foundation under award number IIS-0347933. We thank Xiao yong Liu for kindly pro viding us sev-eral processed data sets for our performance com-parison. We thank Jing Jiang and Azadeh Shak ery for helping impro ve the paper writing, and thank the anon ymous revie wers for their useful comments.
