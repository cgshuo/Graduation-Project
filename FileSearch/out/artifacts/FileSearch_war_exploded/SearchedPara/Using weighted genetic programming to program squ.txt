 Hsing-Chih Tsai n 1. Introduction
Soft computing approaches involve neural networks, fuzzy logic, support vector machines, genetic algorithms (GA), genetic programming (GP) and so forth. Each offers distinct merits when employed in particular application categories. Back-propagation neural network is the most familiar soft computing approach for inference tasks, from which many neural network derivatives have been developed and applied in various categories ( Tsai, 2010;
Mehrjoo et al., 2008; Behzad et al., 2009 ). However, NN has been argued primarily as a  X  X  X lack box X  X  model, due to the massive number of nodes and connections within its structure. Since first proposed by Koza (1992) , GP has earned significant attention in terms of its ability to model nonlinear relationships for input X  output mappings. Baykasoglu et al. (2008) attempted to compare a promising set of genetic programming techniques, including Multi
Expression Programming (MEP), Gene Expression Programming (GEP), and Linear Genetic Programming (LGP) ( Oltean et al., 2002;
Ferreira, 2001; Bhattacharya et al., 2001 ). Results revealed an LGP to be the most efficient algorithm among those three for the studied limestone strengths. Differences between the algorithms focus primarily on the methodology used to generate an individual.
Chromosome representation, tree topology, and a linear string are used, respectively, by MEP, GEP, and LGP. However, this paper models present specific advantages over each other. However, analytical models must use certain assumptions in formulas to work through problems, resulting in a level of accuracy often inferior to that offered by neural network models, assuming the latter use of sufficient historical data sets. A key drawback of neural network models, however, is that predictions are invari-ably supplied without accompanying formulas.

This paper utilizes genetic programming, a branch of soft-computing, to achieve both prediction results and programmed formulas. Genetic programming is improved by introducing the concept of fully weighted connections that create weighted genetic programming (WGP). Another contribution of this paper is its integration of analytical and WGP formulas together to obtain prediction results/formulas. Such confers the attributes of analytical models on WGP results, i.e., tuning analytical formulas with WGP.
 The remaining sections of this paper include Section 2:
Proposed WGP and GA optimization; Section 3: Programming shear strengths of squat walls to study WGP capacities and further tune analytical formulas; and Section 4: Conclusions. 2. Weighted genetic programming optimized using a genetic algorithm
The WGP adopted a layer number ( NL ) setting (see Fig. 1 ). Each node x 1 i in the first layer represented one of the input parameters (including unit parameter  X  X 1 X  X ) x  X  one  X  1 P 1 P 2 ... P j ... P NI  X  , j  X  0 NI  X  1  X  where x 1 i represents nodes in the first layer and i denotes a related node number; P j is the j th input parameter; and NI represents the number of inputs. Each i 1 node selects one attached P
Layers from the second to the  X  X ventual X  (i.e., the layer immediately following the NL th) use operator nodes to calculate values in the top X  X own order (see Fig. 2 ). For two adjacent layers, x represents front nodes, which are treated as layer inputs, and y denotes back nodes, which are treated as layer outputs. A y is calculated by operators of two front x values. Operators involve two weights ( w ) and one function ( F ). Elements in GP and WGP are different in terms of connection weight. Number of scenarios for a GP element depends on the number of function candidates
This paper adopted the first nine functions in Eq. (2) for every F selection. The f 1 was designed to inherit the left-hand side front nodes with w i scaling and is a one-handed operator (use  X  X 1 X  X  to represent). f 2 indicates a weighted  X  X + X  X  operator and is a two-handed operator. f 3 and f 4 are both two-handed operators related to  X  X   X  X  and  X  X /, X  X  respectively. f 5 is a two-handed power ( X  X  X  X  X ) operator, while f 6 , f 7 , f 8 , and f 9 are all one-handed functional function should be unique. However, exceptions are permitted based on user requirements. The last function in Eq. (2) is an example of a case, in which the user has confidence in the role of a lucky guess function. Although such a function may be repro-duced in combinations of the 4th, 5th, 1st, and 3rd equations, assigning such an appropriate function as a candidate could greatly improve convergence speed in users X  expectations. The answer O of a two-layered WGP can be represented as
O  X  y  X  F 1 f w 1 , w 2 , F 2  X  w 3 , w 4 , P 1 , P 2  X  , F
For instance, a WGP example output may be represented as follows (see Fig. 3 )
O  X  f no  X  0 : 3 P 1 0 : 4 P 2 0 : 5 P 4 0 : 6 P 3  X  4  X  In terms of GP, the output may be represented as (see Fig. 3 ) O  X 
P 1  X  P 2 1 : 5 P 3  X  5  X 
While optimum coefficients exceed the search domain, GP will identify other derivatives to cover this optimum coefficient.
Numerically, a large coefficient in WGP may be reproduced, using certain weight combinations. Therefore, WGP is naturally equipped to produce weight coefficients. Although a WGP should be devoted to optimize certain weight numbers, it is worthy to introduce weights for each nodal connection. Fig. 4 used two examples to demonstrate the possibility of their being equals of GP products capable of replacing the WGP unit, and vice versa. While it seems that scenarios for substituting the WGP unit with
GP product cause larger tree topology, the opposite is actually true, as the GP unit is one scenario of the WGP unit only. In WGP, each front input (operator) has a balanced weight, i.e., a weight always happens to each input (operator). However, GP always produces weights on tree edges (i.e., produces a weight cost of a tree branch). If NL is limited, replacing a WGP unit with GP products may not be possible. WGP X  X  advantages over GP are therefore realized.
 of the training root mean square error (RMSE), with a larger fitness value indicating a healthier individual. 3. Perform crossover  X  a positive scalar should be set for parts of the population prior to performing a crossover. A function must be selected in order to create crossover children. This study performed a scattered crossover (@crossoverscattered function in MATLAB), with a crossover rate of 0.8. 4. Perform mutation  X  a mutation produces spontaneous random changes in chromosomes. The MATLAB function @mutationu-niform with a mutation rate at 0.05 was used herein. 5. Select individuals  X  @selectionstochinif was used herein to select parents of crossover and mutation children, and two elitist individuals were guaranteed to survive to the next generation.

Prior to model execution, two other additional major para-meters to be set include population size and iteration number.
These two parameter settings are set on a case-by-case basis in accordance with the experienced input of experts. The population size chosen for this study was 200; the number of iterations adopted was 5000. 3. Weighted genetic programming for squat wall strengths
Accurately predicting shear strength of squat walls involves complex mechanics. Hwang et al. (2001) studied squat wall shear strengths, using analytical mechanics. Results demonstrated that the softened strut-and-tie model has good abilities in squat wall shear capacity prediction. Building on t heir findings, Tsai studied squat wall strengths using a high-order back-propagation network and high-order neural networks. Previously, Tsai (2009) achieved reference results of 112.08/184.37 and 69.936/120.51 kN for train-ing/testing RMSEs of a back-propagation and high-order back-propagation network, respectively. Moreover Tsai (2010) developed high-order neural network derivat ives optimized by center-unified particle swarm optimization, and studied squat wall strengths as well. The best training/testing RMSE result set was 86.39/95.16 kN.
Previous contributions related to this paper include: (1) providing reference results for WGP programming accomplished in this paper, (2) demonstration th at soft computing approaches (neural net-works) provide results at a good level of accuracy, and (3) finding that neural network prediction outputs are restricted to values only.
WGP can offer advantages in formu la programming. Besides, this paper further tuned referenced formulas derived by Hwang et al (2001) with WGP. 3.1. Descriptions of squat wall strengths
Cases used typical reinforced concrete squat walls horizontally loaded on the top and fixed at the bottom (see Fig. 5 ). Vertical shear V wv and horizontal shear V wh couples can be expressed by the following relationship between lever arms l and H without vertical forces acting on the wall
V
V The 11 parameters addressing shear capacity defined by
Hwang et al. (2001) were adopted as parameter inputs in this paper. The 11 inputs and experimental shear strength (treated as a output) are listed in Table 1 with lower and upper bounds. Of 62 data sets, 52 were designated for WGP training and 10 for testing in a format that followed those randomly selected in Tsai (2010) . Fig. 7 . The attached programmed formula of O 3 is
O  X  8 : 86 9 8 : 30cos  X  4 : 16 P 2  X  9 2 : 69sin  X  4 : 75 P  X  8 : 86 9 8 : 30cos  X  4 : 16 P 2  X  9 2 : 69sin  X  4 : 75 P
O  X  7 : 78  X  51 : 0 P 5  X  143 P 7  X  402 P 11 P 3 cos  X  0 : 87 P  X  466cos  X  6 : 50 P 4  X  X  7 : 51 9 0 : 53 P 3 9 0 : 57 P 4
O  X  523 P 5  X  232cos  X  18 : 5 P 5  X  73 : 6  X  49 : 6 9 8 : 81cos  X  3 : 81 P 2  X  X  8 : 04 P 10 P 11 9 8 : 5
O 4 and O 5 are obtained in a similar manner. While O 6 also provides an accurate prediction, the high complexity of the programmed formula makes it unsuitable for use in soft-computing programming. It is, therefore, ignored in this paper.
Comparing O 3 , O 4 and O 5 , O 4 was found to have struck the best balance between formulaic simplicity and predictive accuracy.
Parameter impacts are outlined in Table 3 . O 4 and O 5 results do not reflect the impact of certain parameters. A possible reason for such is that learning targets vary across a wide range, and seem to cluster into two opposing groups (see Fig. 8 ). It is, therefore, difficult to calculate formulas suited to both data clusters. Fig. 8 takes O 5 as an example to show that WGP provides good ability to deliver accurate predictions for both training and testing data sets. However, O 5 required significantly greater formulaic com-plexity than O 4 , and was thus rejected in favor of the latter to provide final WGP squat strength results.

Of all GP derivatives mentioned, GOT is the one able to provide optimized coefficients and, therefore, was used in comparison a coefficient parameter and, when a coefficient parameter is selected, it needs GA optimization. Therefore, GOT obtains final answers within a maximum layer limitation. Ten runs of GOT were executed with same GA settings, five layers at most, and six operators including  X  X + X  X ,  X  X   X  X ,  X  X   X  X ,  X  X / X  X ,  X  X  X  X  X , and  X  X  X n X  X . GOT associated RMSE results shown in Table 2 give an average of 155.11/192.70 kN, best of 136.99/151.83 kN, and worst of 178.38/ 244.97 kN. The referenced GOT formula was given as O GOT O GOT  X  10 : 6 0 : 0426  X  P 3  X  8 : 42 P 11 P 6  X  X  P 4  X  P 2
Although O GOT seems simpler than O 4 in terms of formulaic complexity, O 4 wins in terms of prediction accuracy. The first two constants of O GOT were calculated, using linear regression analysis to improve prediction accuracy. To ac hieve the same level of accuracy using WGP, the GOT structure may be much more complicated without the linear regression analysis. Eq. (10) was a output from
Fig. 9 . Only a 4-layered GOT structure was found and three operators were used. At this point, two partic ular issues should be concerned: (1) what is the output when every individual adopts a fully 5-layered
GOT structure during the evolution process? and (2) some scenarios are hard to approach by GOT without the use of weighted balances (e.g., 1.05 P 1 +0.9 P 2 1.1 ). 3.3. Using WGP to tune analytical formulas
Two types of analytical methods based on the softened strut-and-tie model (i.e., the general and simple methods) have been developed in Hwang et al. (2001) . An ACI method to obtain squat wall strength was also introduced in the literature. Attached
RMSE results were calculated at 404.19, 155.78, and 510.93 kN for the general, simple, and ACI methods, respectively. Previously, O provided training and testing RMSEs of 128.56 and 143.36 kN, respectively, which demonstrates the efficacy of WGP for both prediction accuracy and programming formulas. This paper further utilized WGP to tune referenced analytical models. For simple method (155.78 kN). Programmed formulas were
O simple , 3  X  simple  X  0 : 22sin  X  6 : 94log 9 0 : 78 P 11
O
O
O
O simple , 2  X  simple  X  11 : 5 P 5  X  2 : 75 P 6 7 : 03 P 4 0 : 852 P for tuning the simple method for squat wall strength. ACI is the third method in Hwang et al. (2001) . This paper posits that WGP can tune this method, using the same process.

In terms of parameter impacts, it is meaningless to argue that a specific parameter should be absolutely involved in formula tuning, as soft computing approaches compute results based on minimizing target errors rather than an analytical mechanism.
Additional historical data may cause alterations in programming scenarios. However, further WGP studies can still be expected to identify further significant relationships. After a final WGP or
WGP tuning formula is identified, one can pay more attention to parameters that appear in a final formula, owing to their contributions to better prediction accuracy.

Recalling f 5 in Eq. (2), a new function type may be assigned f  X  9 w i x i 9 w j  X  23  X 
Although it provides a subset of the original Eq. (2), f 5 capable for other uses. When f 5 new is adopted, parameters no longer take place in formula power terms. While such restricts the scope of final formulas, it may be treated as ensuring final formula types. Therefore, this paper states that WGP function types depend on user demands. 4. Conclusions
WGP incorporates weights and functions to program squat wall strength and tune associated formulas. The primary
