 Online social networks (OSNs) such as Twitter provide a good platform for event discussions. Recent research [26] [25] has shown that event discussions in OSNs are diverse and innovative and encourage public engagement in events. Al-though much research has been conducted in OSNs to track and detect events, there has been limited research on de-tecting or understanding the event context . Event context helps to better predict users X  participation in events, identify relations among events, and recommend friends who share similar event context.

In this work, we have developed AnchorMF , a matrix fac-torization based technique that aims to identify event con-text by leveraging a prevalent feature in OSNs, the anchor information. Our AnchorMF work makes three key contri-butions: (1) a formal definition of the event context identifi-cation problem; (2) anchor selection and incorporation into the matrix factorization process for effective event context identification; and (3) demonstration of applying event con-text for user-event participation prediction, relevant events retrieval, and friendship recommendation. Evaluation based on 1.1 million Twitter users over a one-month data collection period shows that AnchorMF achieves a 20.0% improvement in terms of user-event participation prediction.
 H.3.3 [ Information Search and Retrieval ]: Information Filtering Event context identification; Twitter; matrix factorization
With the rapid growth of online social networks (OSNs), more and more real-world events are being discussed on Web 2.0 platforms such as Facebook, Twitter, Tumblr, etc. Re-searchers have been using these platforms as social sensors to detect events, analyze event-related discussions, and predict event popularity. Despite much research on the aforemen-tioned topics, there has been limited research that aims to detect or understand the context of events. The context for an event is essentially represented by the group of users who show inherent interests or willingness to participate in the event, such as people supporting their home football team, residents affected by a local fire or flooding, or people inter-ested in Oscar nominations. The aggregated attributes of the group typically demonstrate commonalities in location, interests, age, gender, etc.

Event context identification is an important research prob-lem and has many real-world applications. Successful event context identification will help to better predict the users who are going to participate in an event, thus creating value for enterprises and organizations for better marketing and event management. Event context also helps to identify re-lations among events if they share the same or similar con-text. Interesting patterns may be discovered even if events are not semantically related but otherwise share similar con-text. For example, as we will show in the experiments, event Obama 2013 inauguration is related to event The Interna-tional Consumer Electronics Show (CES) 2013 according to identified context. Another application is friendship recom-mendation based on the event context for past event partic-ipation. As we will later show in the experiments described in Section 5, friendships are correlated with event context similarity among users.

Event context identification is a challenging problem for several reasons. First, it is difficult to define event con-text properly. Context is a subjective concept and the same group of users may be interpreted according to different common features. It is typically easier for a computer al-gorithm to discover a contextual pattern, rather than ex-plain the cause for this pattern. Second, although other techniques may be applied to solve the event context iden-tification problem, their performance is not good [24] [11]. Given historical event data, we can extract event contexts by characterizing events based on user participation, and at the same time characterizing users by their event par-ticipation. This process is very similar to the idea of ma-trix factorization [24]. Previous research mainly considered the original user-item rating matrix (i.e., the user-event ma-trix in our setting) and friendship information if available. However, as we show later in our experiments, friendship information does not show significant performance improve-ment. Finally, users interested in certain types of events tend to follow certain anchor accounts in OSNs. However, it is not clear how these anchor accounts can be selected (among massive following/follower relations), nor is it clear how to incorporate such anchor information into the overall event context identification process.

To address these challenges, we have developed AnchorMF , a unified solution for identifying event context by utilizing both user-event participation information and anchor infor-mation in OSNs. Given observations of user-event and user-follower matrices, a probabilistic model is built to consider users, events, and anchors as latent factors. An anchor se-lection algorithm is proposed to automatically identify in-formative anchors for the model. Finally, a Gibbs sampler and a maximum a posteriori (MAP) estimator are proposed to estimate the parameters of the model. AnchorMF is im-plemented and evaluated using a real-world Twitter data set which we have collected over one month and contains 1.1 mil-lion Twitter users. Evaluation results show that AnchorMF outperforms state-of-the-art techniques by 20.0% in terms of prediction accuracy. AnchorMF can identify relevant events using an information retrieval process. We also show that event contexts can be used for friendship recommendation.
To the best of our knowledge, this is the first work that aims to address the event context identification problem. This paper makes the following contributions: (1) a for-mal definition of the event context identification problem; (2) anchor selection and incorporation into the matrix fac-torization process for effective event context detection; and (3) application of event context to user-event participation prediction, relevant events retrieval, and friendship recom-mendation.
There has been much event-related research in the lit-erature. The field of event detection and tracking can be traced back to [32] [1] [31]. Kleinberg defined and extracted bursts of activity from emails using an infinite-state automa-ton [13]. Bursty events can also be detected from news texts by identifying bursty features with a binomial distri-bution model and threshold-based heuristics [4]. Ihler et al. focused on time-series data such as logs and proposed Markov-Poisson models to detect anomalous events [10]. A general probabilistic model was proposed to extract corre-lated bursty topic patterns in [28]. Chen et al. used user tag information to identify events that involve browsing and searching photos on Flickr [2]. Lappas et al. explored how bursty terms help enhance the search process [16]. These works show the importance and effectiveness of event anal-ysis using Web data.
 More research has been conducted on Twitter recently. Different crisis events have been analyzed to identify gener-ative and innovative properties of discussion on Twitter [15] [26] [25]. Sakaki et al. developed an earthquake alarm system by extracting real-time earthquake events on Twitter [22]. Petrovi  X c et al. presented a locality sensitive hashing ap-proach to efficiently detect events that have not been seen before based on tweets [21]. Weng et al. proposed wavelet-based signal clustering on Twitter text stream data to detect events [30]. Lin et al. leveraged interests of users and social relations to track the evolution of popular events [17]. Event popularity can be predicted by considering a variety of so-cial features [7]. Such event detection techniques support algorithmic discovery of events on OSNs, and help to build the foundation of event-related research. However, they do not solve the event context identification problem directly.
This work also builds upon existing matrix factorization techniques. Salakhutdinov et al. proposed a probabilistic matrix factorization model, which factorizes the explicit user-item matrix to a user latent trait matrix and an item la-tent trait matrix [24]. A full Bayesian version of the model was also proposed to provide generalized parameter tuning and avoid overfitting [23]. For implicit datasets, Hu et al. adopted more features from the original user-item matrix and proposed an improved gradient descent method to solve the problem more efficiently [8]. More recent research con-siders friendship information as a useful feature to incorpo-rate into the current framework. Trust based approaches consider friendship as trust to influence users X  latent fac-tors. In [18], friendship information was modeled as a linear combination of the basic model. Another approach was also proposed to model friends as a separate latent matrix and used friendship as observations [19]. SoicalMF was proposed to incorporate friendship into the same latent space as users and a user X  X  latent factor is represented as the average of all friends X  latent factors [11]. Gartrell et al. proposed to consider only close friends when combining friends X  latent factors and used a Markov random field to aggregate latent factors [5]. Influence based models consider users X  interests to be influenced by their friends. Huang et al. considered re-ceiver interests, item qualities, and interpersonal influences for final recommendation [9]. Jiang et al. incorporated inter-personal influences into the existing PMF model and showed significant performance improvement [12]. The assumption of influence-based models is that items must be coming from their friends, which is not always the case. Our work sep-arates the anchor latent factor space from the user latent factor space with a feature selection process, which shows better performance than existing solutions.
Each event e = { m 1 ,m 2 ,... } is represented by a set of messages obtained by searching for specific keywords W = { w 1 ,w 2 ,... } in an OSN (e.g., Twitter) and corresponds to a real-world event. Each message m i = &lt; u i ,t i &gt; , meaning that the message was posted by user u i at time t i considered a  X  X articipant X  of event e in the cyber world.
The context of a given event is defined as a group of users who participate in the event because of some inher-ent reasons, i.e., common attributes of the participants or latent event/user factors. For instance, both location and interest are important attributes to represent event context: the context of a local basketball game could be the group of local people who like their basketball team. Therefore, the context of event e j can be jointly characterized by the event latent factor E j and the set of user latent factors U e j the participants of e j .

Anchors are popular users or public pages in OSNs, and their followers tend to participate in certain types of events, e.g., the Twitter account of a local news venue or a user posting actively on a specific topic. Usually, anchors are not directly identified by OSNs, and any user who has followers can be an anchor candidate. Selecting anchors for effective event context identification is the key. Let U a be the set of followers of anchor candidate a , we select a as an anchor based on the following two factors: 1. | U a |  X  threshold , i.e., the anchor must have at least 2. The probability of a being an anchor depends on a  X  X 
The problem of event context identification is then defined as follows. Given M events E = { e 1 ,e 2 ,...,e participated by N users U = { u 1 ,u 2 ,...,u N } , the output of event context identification is C = { c 1 ,c 2 ,...,c M } , where each c i is the context of e i . The event contexts capture the event latent factors and user latent factors, which in turn can identify the subset of users who are likely to participate in each event. The success of event context identification can be evaluated by comparing the user-event participation predicted by the event contexts and the actual user partic-ipation in events. Detailed evaluation results are presented in Section 5.
Figure 1 illustrates the high-level process of AnchorMF for event context identification. Given a set of events, we first select anchors from the candidate users (Section 4.1), then incorporate the selected anchors into an extended proba-bilistic matrix factorization (PMF) model (Section 4.2), and finally through model inference (Section 4.3) we obtain the event contexts represented by event and user latent factors. Given a set of N users U = { u 1 ,...,u N } , a set of M events E = { e 1 ,...,e M } , and the binary matrix R = [ R ui representing users X  participation in events, the probabilis-tic matrix factorization (PMF) model factorizes R into two latent matrices U  X  R K  X  N and E  X  R K  X  M , representing k -dimensional latent trait vectors for users and events. The graphical model is shown in Figure 2. Figure 2: Probabilistic matrix factorization (PMF).
PMF defines the following distributions:
As discussed in Section 3, anchors are any user accounts which have at least a certain number of followers and whose followers show a good concentration on similar events. Using a real-world Twitter dataset we have collected (Section 5.1), we start with anchor candidates with at least 1 follower, and the set of candidate anchors shrinks as the selection process progresses. For simplicity, we refer to the anchor candidates in each round as anchors.
We first need to understand the relation between anchors and their followers. The problem can be decomposed into the distribution of followers given anchors and the distribu-tion of anchors given followers.
The red curve in Figure 3 shows the complementary cu-mulative distribution function (CCDF) of the number of fol-lowers given anchors. The x-axis is the number of followers and the y-axis is the percentage of anchors. This heavy-tailed distribution shows that many anchors are followed by few users and very few anchors are followed by many users. The black curve in Figure 3 shows the CCDF of the number of anchors that users follow. Here, the x-axis is the number of anchors and the y-axis is the percentage of users. This black curve is also a heavy-tailed distribution and shows that many users follow few anchors and very few users fol-low many anchors. We notice that the black curve has a flat beginning; this indicates that users tend to have a minimum numbers of anchors to follow, which is approximately 100 as we can see from the figure. We also notice that there is an anomaly near 2,000 followers for the black curve. This is likely due to the fact that Twitter X  X  policy [27] allows each user to follow at most 2,000 anchors unless he/she is very active on Twitter. As we can see from the figure, less than 5% of the users in our dataset follow more than 2,000 an-chors. The gap between the red and black curves is caused by the fact that when counting the number followers of an-chors, we only consider the users in our event dataset, and not all followers of the anchors at Twitter.

We use the goodness-of-fit based method proposed in [3] to fit the two CCDFs shown in Figure 3. The power law model gives us two parameters  X  and x min .  X  is the scal-ing parameter, which indicates how skewed the distribution is (the slope of the CCDF). As described in [3], a typical value of  X  is between 2 and 3. Our estimated  X  is 2.24 for the anchor distribution and 2.26 for the user distribution. Table 1: Number of user pairs and their average number of shared anchors and shared events. Table 2: Correlation between #anchors and #events per user.
 These results match what we see in Figure 3 and the model shown in [14], which indicate that our dataset is representa-tive. The second parameter, x min , indicates the minimum x-axis value that fits the power law. The x min of the anchor distribution is 269, and we use this number as the minimum frequency of an anchor candidate. Therefore, all the anchor candidates must have at least 269 followers. With this pa-rameter setting, the number of anchor candidates is reduced to less than 1% of the original anchor candidate set size, which significantly reduces the amount of computation in our modeling process.
Before considering the event concentration of anchors, we first study the relation between anchors and events, specif-ically, if users who follow the same anchors tend to partic-ipate in the similar events. We randomly sampled 10,000 users from our dataset, and consider for each pair of users the number of shared anchors and number of shared events. We separate all the user pairs into different buckets based on quantile and ensure that all user pairs with the same number of shared anchors fall into the same bucket. Table 1 shows the aggregate results for each bucket, including the number of user pairs, average number of shared anchors, and average number of shared events. As shown in the table, when users share more anchors, the number of shared events also in-creases. Therefore, identifying the appropriate anchors can serve as good indicators for event participation and event context identification.

We further analyze for each user if the number of anchors he/she follows is correlated with the number of events he/she participates in. We use both Pearson X  X  correlation to check linear correlation and Spearman X  X  correlation to check non-linear correlation. The formulas are shown in Eq. 2. r
As shown in Table 2, there is very little correlation be-tween a user X  X  number of anchors and number of events, showing that one is not a substitute for the other. These re-sults indicate that anchor information and event information are complementary signals, and adding anchor information on top of user-event information can potentially boost the performance of event context identification.
Based on the anchor-user distribution analysis, we prune anchor candidates with fewer than 269 followers. Next, we need to select candidates which show a good concentration of similar events. We solve this problem by first looking at the users who follow an anchor and the events that those users participate in. We compute an anchor-event matrix by multiplying the anchor-user and user-event matrices:
Each element N ki = M ae [ k,i ] is the number of anchor k  X  X  followers who participate in event i . We denote E the set of events participated by anchor k  X  X  followers, and each event i is duplicated N ki times in the set, i.e., E { e whether the events in E 0 are similar to each other. The event concentration for each anchor k is defined as:
The formula above aims to compute the average pair-wise event similarity for events in E 0 , which is used to represent the anchor X  X  concentration over events. If a pair contains two of the same events, the similarity is 1, otherwise, the similarity is defined by S ( i,j ):
The similarity function is the cumulative normal distri-bution of the inner product space given the i and j event pairs [5]. The intuition is that the similarity should be de-fined in the inner product space and lie between 0 and 1. Based on the similarity function and each anchor X  X  event concentration, we can then select anchors and proceed with incorporating the anchor information in the AnchorMF model.
We incorporate anchor information into the PMF frame-work by factorizing the user-anchor matrix and also con-sidering the importance of the anchors. As illustrated in Figure 4, the AnchorMF model considers a new observation F , which indicates what anchors each user follows. Corre-spondingly, we add a latent factor A to represent the an-chors X  latent influence on users. According to Equation 4, each anchor k has a weight, and weight is the same for every user u , denoted as W uk . Consistent with the PMF model, we also add priors and hyper-priors into the model. U and E are latent variables for users and events, respectively, and R is a binary observation matrix where each element indicates whether or not a user participates in an event. We derive Equation 6 directly from Figure 4.
To facilitate model inference, we also derive the log of the posterior probability as follows: ln p ( U,E,A | R,F, X  2 R , X  2 U , X  2 E , X  2 A , X  2 F ) as conjugate Gamma distributions with flexible hyperpriors similar to:
Given the Bayesian framework defined in the previous section, inference for this model can be performed through Gibbs sampling [6]. Gibbs sampling generates a number of samples from an aperiodic and irreducible Markov chain, and involves sampling from the conditional distribution for each latent variable to approximate the joint distribution given that sampling from the joint distribution of the model is difficult. In the above model, we denote the random vari-ables by  X  = { U,E,A, X  U , X  E , X  A , X  R , X  F } , and we derive the following conditional distributions for all the random variables based on Equation 7.  X  U is sampled from a Gamma distribution:  X  E and and  X  A are sampled from similar conditional dis-tributions.  X  R is also sampled from a Gamma distribution: 1, and a R 0 = b R 0 = a F 0 = b F 0 = 1 . 5. These parameter settings specify flat gamma prior distributions that allow the model to be flexible when learning.

U u is conditionally sampled from a multivariate Gaussian distribution:
U u | R,F, X  \ U u  X  X  ( U u ;  X  u ,  X  u )
E i is conditionally sampled from a multivariate Gaussian distribution:
A k is also conditionally sampled from a multivariate Gaus-sian distribution:
The Gibbs sampling approach described above computes an approximation of the posterior distribution, which allows us to infer users X  participation in events, but it does not find the maximum point of the posterior. Therefore, it is difficult to compute a point estimate of the latent matrices U and E which result in the maximum function value from the Gibbs sampling results. However, U and E describe the event context we need, and thus we need good point estimate for these variables to calculate the similarities between users and events. To this end, we also propose a maximum a posteriori (MAP) estimation for the model which estimates the maximum point (mode) of the posterior distribution, and therefore generates point estimates for U and E . The MAP estimator empirically converges faster than the Gibbs sampling approach.

MAP estimation works by maximizing the conditional dis-tributions of U and E iteratively, where:
Since all the conditional distributions are Gaussian distri-butions, the Gaussian X  X  mean will define the curvature and how far to step towards the maximum point for each iter-ation. This approach is similar to the inference algorithm described in [20].
 The complete inference algorithm is shown in Algorithm 1: Algorithm 1 AnchorMF for t = 1 to num of samples do end for
Although the sampling of U u , E i and A k must be con-ducted sequentially, the sampling of different U s, E s and A s can be conducted in parallel. This saves significant com-putation time in practice. We implemented a parallelized Gibbs sampler and MAP estimator using the thread pool mechanism in the Java standard library. Empirically, the Gibbs sampler converges after 200 iterations with 50 burn-in samples and finishes within 1 hour. The MAP estimator converges within 100 iterations and finishes within 0.5 hours. The results are based on our own dataset described in Sec-tion 5.
In this section, we evaluate AnchorMF, our proposed event context identification solution, using real-world events that we have collected. Our evaluation aims to answer the fol-lowing questions: Entertain 134 Table 3: Event cate-gories.
 We collect data using the Twitter API. We monitor daily Twitter trending topics and get a list of ranked popular key-words by considering both how long they stay on the trend-ing topics and their rank. Then a human review process is used to review the top 200 keywords and identify the ones that match real-world ongoing events. The selected key-words are then filtered on real-time Twitter streams to con-tinue collection of messages which contain the keywords. At the same time, we search for historical tweets which contain the keywords for up to 7 days. Since the selected keywords are mostly filtered on the day they became popular, we be-lieve a 7-day look-back window is enough to collect complete events based on keywords. The data collection process intro-duces some noise into the dataset, but we carefully choose representative keywords to ensure events are not too gen-eral. For example, the 2013 Obama inauguration event was collected based on the keywords Obama inauguration , rather than Obama , which tends to have a much broader scope. Af-ter we collect all the desired events, we only consider users who have participated in at least 5 of these events. This pro-cedure helps us to remove much of the noise in the dataset. We believe most of our data consists of complete and coher-ent events. After obtaining the users who participated in each event, we also collect friends of the users, users X  pro-files, and lists (a group of followed users with a group name). In total, over a one-month period of time from Jan 4th to Feb 3rd, 2013, we collected 461 events consisting of 20.79M tweets and 1.1M users. All the data are stored in MongoDB and the total volume of the data is 554 GB. Statistics for this dataset are shown in Table 4. From Table 3 we can see that events are divided mainly into five categories. Sports and Entertainment events dominate the event type distribu-tion. The CDF of the number of users per event is shown in Figure 5. We see that our event dataset consists of both large and small events and event size follows a heavy-tailed distribution.
All the experiments have been conducted on a 2.4 GHz 16-core machine with 48GB of memory. This machine runs Ubuntu 12.04.2 and JVM 1.6.0 27. All of the implementa-tion and experiments are written in Java.
In this experiment we want to examine the effectiveness of our event context identification in terms of predicting user participation in events. We run 10-fold cross validation and in each fold randomly select 10% of all the events in our dataset as test events and the remaining 90% as training events. For each test event, we sort all the users according to the time they participated in the event, and use the top 10%, 20%, 30%, 40%, or 50% as training users for the test event. Our goal is to evaluate the predictions of the other 90%, 80%, 70%, 60% and 50% of users for test events.
In Table 5, each row represents a method of ranking test users for a test event. These methods are: 1. Random ranking predicts testing users in a randomized 2. Popularity based ranking predicts test users who are 3. Baseline ranking makes prediction based on event con-4. SocialMF identifies event context based on the ma-5. AnchorMF is the model we propose to identify event
We use average rank percentile as our main evaluation metric, which is a recall-based metric from [8], since the implicit dataset we use does not include complete data for precision based measurements. Users who actually partici-pated in the events should be ranked higher in the prediction results. The average rank percentile is computed as follows: where rank ue is the average rank percentile for each user u in the event e . 0 represents the highest rank, while 1.0 represents the lowest rank so lower percentile means better performance.

As shown in Table 5, AnchorMF outperforms SocialMF by 20.0% when using 50% of the users as training users for the test events. As the percentage of training users decreases, we see a larger performance boost for AnchorMF compared to SocialMF, up to 32.2%. We also notice that for both So-cialMF and baseline PMF, in the case where we only use 10% of the training data, the performance is almost as bad as the non-contextual popularity-based method. 1 However, AnchorMF with 10% training data performs better than the best cases for both SocialMF and baseline PMF. This shows the effectiveness of the identified anchor information, and indicates that it is particularly helpful to identify event con-text and predict user participation in the early stage of an event. When we compare the SocialMF and the baseline PMF approaches, we do not see much performance differ-ence for our dataset. One possible explanation, as we will see in Section 5.5, is that on Twitter users tend to have friends who are very dissimilar in terms of the latent trait space. Therefore the use of aggregated friends X  interests, as performed in SocialMF, may not be beneficial. Additionally, since we removed users who have participated in fewer than 5 events from our dataset, and since SoicalMF has been proved to be most effective for cold start users in recom-mender systems, SocialMF is not effective in our scenario since there are no cold start users.
As shown above, our proposed event context identification algorithm is effective and outperforms other existing related approaches. We would now like to see how to interpret the identified context. The experiment described in this subsec-tion examines three different scenarios, where each scenario has a different event context. The results show that the identified event context is interpretable and meaningful.
We select 6 events from all predicted results we get from the experiment described in Section 5.2. Each event con-sists of the predicted users for that event; the information for each user includes Twitter profile and list data. We ag-gregate the user information for each event and use this data to populate a table, as shown in Tables 6 through 11. Each column represents a source or dimension of user information that we will examine, including location, the self-provided user profile description, and tags from users X  list information. We extract all keywords from this aggregated user informa-tion and list the top five keywords ranked by probability of occurrence ( P = frequency count/total frequency ). We study the context of events by looking at these keywords and manually verify whether they have coherent semantic meaning. First, we look at two cases where users discuss events on Twitter based on location. Table 6 shows results from an event about a local famous cafe that moved to a new lo-cation, which happened on Jan 17, 2013. As we can see from the results, the location dimension has a concentration of probability on the keywords new orleans , which matches
We only consider the popularity of users in the training events, so the performance of popularity based ranking is independent of the percentage of training users in testing events. new orleans 0.42 sports 0.06 sports 0.08
Louisiana 0.05 world 0.03 politics 0.03 pennsylvania 0.06 marketing 0.05 local 0.04 the actual location of this event. The rest of the keywords, such as LA and Louisiana in the location dimension also have coherent meaning. Although city and usa are general location terms which do refer to a specific location, they have much lower probability compared with higher ranked keywords. If we look at both the description and tag dimen-sions, the keywords all have fairly low probability without much concentration, and they also lack coherent semantic meaning.

Table 7 shows results from an event about a local social club meetup in Harrisburg, Pennsylvania that happened on Jan 21, 2013. The results are very similar to what we see from the #nola event. The location dimension has a concen-tration and coherent meaning, while the tag dimension does not. We do see that the description dimension has the social keyword with higher probability. The reason for this is the type of the event is essentially a social event and people par-ticipating in the event are self-identified with the keyword  X  X ocial X .
We now look at two examples that are based on users X  in-terests. We focus on the description dimension and the tag dimension to see if the keywords extracted give us meaning-ful information.

Table 8 shows results from an event about the Interna-tional Consumer Electronics Show from Jan 8 to Jan 11, 2013. As we can see from the results, the tag dimension has a concentration on news, media, and tech, which match the event X  X  semantic meaning. Also as expected, the loca-tion dimension shows a broad coverage of different locations and does not have a concentration as compared to location-specific events. However, we do not see significant concen-tration in the description dimension, although the top key-words have coherent semantic meaning. We will discuss this result further in Section 5.3.3.

Location P Description P Tags P new york 0.08 media 0.05 news 0.37 los angeles 0.06 tv 0.09 entertain 0.10
Location P Description P Tags P los angeles 0.10 life 0.08 nba 0.08 california 0.05 love 0.08 basketball 0.03 san antonio 0.03 music 0.03 lakers 0.02
Table 9 shows results from an event about the Oscar nom-inations which happened on Jan 10, 2013. The results show the same pattern as what we find in the International Con-sumer Electronics Show event.
Next, we look at two events that are both location and interest specific. Good examples of these types of events are local sports events. We will focus on all of the three dimensions to see if there are any interesting patterns.
Table 10 shows the results of NBA basketball game event that involved the Los Angeles Lakers vs. the San Antonio Spurs, which happened on Jan 9, 2013. The location dimen-sion shows an interesting concentration on both Los Angeles and San Antonio, which are the expected locations. The de-scription dimension shows a somewhat noisy results, but the sport keyword is apparent. The tag dimension shows good concentration and gives us confidence that this is indeed a local sports event.

Table 11 shows the results of an event involving two En-glish soccer teams in the Premier League, Manchester United vs. Liverpool, which happened on Jan 13 , 2013. Similar to the results of the previous example, the location and tag di-mensions show the expected results, while the description is relatively noisy.

From all these event case studies, we find that it is rela-tively easy for humans to understand the event context by looking at the location and tag dimensions. This results from the fact that the location field is specifically designed for users to provide their location on Twitter, and most peo-ple tend to follow this rule. Tags are provided by users X  fol-lowers and they serve identification purposes, and so tend to include location and interest information. The results also indicate the usefulness of the textual data in these dimen-sions and may lead us to incorporate this information into our model in the future. We also notice that the descrip-manchester 0.05 united 0.04 soccer 0.05 tion dimension is noisy for all three types of events, because self descriptions are very informal and users do not usually include their location and interest information in their self-provided profile description.
By looking at the common attributes of predicted users for some events, we can understand the meaning of event context. The next important issue to investigate is how we can use the identified context to better understand events. In this experiment, our goal is to demonstrate the feasibility of building an event-based search engine by leveraging event context information. The challenge here is that we do not consider the text of events and the relevance is only based on event context.

We have built a proof-of-concept system to evaluate the effectiveness of the relevant event retrieval process. We con-struct queries from all of the 461 events in our dataset with different number of events. We first randomly select 46 events as length-1 queries. Then 46 length-2 events are ran-domly selected by combining any two of the length-1 queries. Next, 46 length-3 events are randomly selected by combin-ing any three of the length-1 queries. After this process, we have 138 queries in total. For each query, all of the returned results are assessed as either relevant or irrelevant; there are 42,733 labeled judgment pairs in total. Standard informa-tion retrieval evaluation metrics [29], including precision@3, precision@5, precision@10, Mean Reciprocal Rank (mRR) and Mean Average Precision (mAP), are used to evaluate the results. The first three precision based metrics are con-sidered good metrics for results returned from mobile devices or Web searches. mRR measures the rank of the first rele-vant result, and mAP considers recall as well as precision in the measurement.
 For each query, we divide the query q into separate events. Each potential result i has a relevance score S ( i,j ) according to Equation 5 given event j . Therefore, the relevance score RS i is computed according to:
The first three rows of Table 12 show three sets of con-textual retrieval results based on length-1-to-3 queries. The last three rows show three sets of randomized retrieval re-sults based on length-1-to-3 queries.

As we can see from Table 12, using event context, the retrieved results have very good top-K accuracy and very high performance for first relevant result retrieval. By con-sidering recall, the mAP also shows high performance. We also see that queries with different length show very similar performance. This means contextual retrieval is consistent for queries of different lengths. Compared to randomized retrieval methods shown in the last three rows, contextual retrieval can return events that are much more relevant.
The results above show that the semantic relevance of events as labeled by a human. However, there are some cases where, although human may think two events are se-mantically unrelated, they share the same context. These cases show interesting results for relevant events that can not be captured by semantics. One good example that we found in our dataset is the 2013 Obama inauguration event, which happened from Jan 19 to Jan 21, 2013 and the The 2013 International Consumer Electronics Show (CES) event happened from Jan 8 to Jan 11, 2013. At a first glance by human, these two events are completely unrelated. How-ever, our model shows that these two events have a common context of people who like technology. The discovery of non-semantic but contextually relevant events is the unique ben-efit of event context identification.
To emphasize the importance of identified event context, we will show another possible application. In this experi-ment, we investigate the correlation of identified event con-text with user friendship. We will demonstrate that users who belong to similar event context are more likely to be friends. This observation allows us to build better friend-ship recommendation services. Figure 6: CDF of user-friend similarity based on event context.

We randomly sample 100,000 users and obtain all of their friends. For each user and friend pair, we calculate the user-friend similarity of their latent factor vectors based on the similarity defined in Equation 5 that are generated from the matrix factorization. Let Rank f be the rank of a friend based on similarity, the relative similarity is calculated for each user as: The reason for defining the relative similarity is that the rank is important, and we want to normalize the rank. A similarity value of 1 indicates the most dissimilar user-friend pair.
 We plot the CDF of all the user-friend similarity pairs in Figure 6. As we can see from the black line in Figure 6, users tend to have many similar friends. 50% of the friends are within 0.2 similarity. The red line shows the case where user-friend similarity is randomly calculated. At the end of the CDF curve, we do see there is a trend of increasing dissimilarity. This means users also have many dissimilar friends. This can affect performance when we consider the use of friends X  interests to help predict users X  interests; we will verify this further in the next experiment.
 Figure 7: Friend similarity distribution per user.
The above analysis shows only aggregated similarity be-tween users and their friends. In our second experiment, we want to see the friendship similarity distribution for each user. In Figure 7, the x-axis represents users, the y-axis indi-cates relative friendship similarity as defined above, and each dot represents a friend. As we can see from the figure, there is high density around similarity of 1 and similarity of 0, meaning that users have both similar and dissimilar friends. The dissimilar friends are likely to be loosely-connected so-cial friends and they may not share common interests with the user. This also explains why a direct average of friends X  interests, as performed in the SocialMF model, will not help infer users X  interests. In reviewing all of the results from the analysis in this subsection, we see that better friend-ship recommendation can be made based on event context information.
In this paper we have presented AnchorMF, a matrix fac-torization technique to solve the event context identification problem. AnchorMF selects anchors from users X  followers and incorporates anchor information into an extended PMF framework. We have also presented several applications of using identified event context to predict users X  participation in events, retrieve relevant events, and recommend friends based on event context. Our evaluation using real-world Twitter data shows that AnchorMF outperforms existing matrix factorization techniques by 20.0%. In our future work, we would like to explore other potential features, such as location information in users X  profiles and tag information from users X  followers, and consider how these features can be used in our model for better event context identification.
