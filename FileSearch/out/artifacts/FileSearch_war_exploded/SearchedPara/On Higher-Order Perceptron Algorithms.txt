 depends on the specific task at hand.
 normalized, i.e., || x guesses the binary label y b y  X  X  X  1 , 1 } . Then the true label y t is disclosed. In the case when has made a prediction mistake . We call an example a pair ( x any sequence S = ( x One such measure (e.g., [24]) is the cumulative hinge-loss (or soft-margin) D linear classifier u at a given margin value  X  &gt; 0 : D that D of the comparison class to mistaken trials only and, as a consequence, to refine D include only trials in M : D sequence  X  S = ((  X  x into  X  x examples up to trial t = i then the transformed sequence  X  S is likely to be separable with a larger margin. and maintains a product matrix B (plus one). Upon receiving the t -th normalized instance vector x its binary prediction value B while vector v The new matrix B performed (hence the algorithm is mistake driven). Observe that  X  behavior be substantially different from Perceptron X  X ) we need to ensure P  X  sequel, our standard choice will be  X  three of them, each one having its own merits. 1) Primal version . We store and update an n  X  n matrix A vector v taking O ( n 2 ) operations, while v v denote by X where a mistake occurred so far, and y labels. We store and update the k  X  k matrix D D D the other hand, H Observe that on trials when  X  This amounts to say that matrix A ucts needed to compute g  X  k &gt; 0 that for any vector z we can compute B B normalize instance vectors x eralize the matrix update as B contained in the matrix B the margin g ( B behavior of the best linear classifier over the transformed sequence being t ( k ) the trial where the k -th mistake occurs, and B Observe that each feature vector x case p = 2 . A more general statement holds when p  X  2 .
 ples S = ( x an arbitrary sequence S = ( x mistake occurred. We study the evolution of || B B where we set for brevity r above chain of equalities. To this end, we need to ensure r r k  X  0 r to be nonnegative, it suffices to pick  X  r k  X  c &gt; 0 (combined with r term follows by observing that || x eigenvalue) of A spectral norm one, we have x &gt; summing over k = 1 , . . . , m = |M| (or, equivalently, over t  X  M ) and using v upper bound apply the standard Cauchy-Schwartz inequality: || B a generic trial t = t ( k ) the update rule of our algorithm allows us to write where the last inequality follows from r the above over k = 1 , . . . , m and exploit c  X  r the claimed bound.
 algorithm operating on the transformed sequence  X  S which is suggestive of the improved margin properties of  X  S ( x small positive numbers. Assume, again for simplicity, that all  X  value  X  &gt; 0 . Then, up to first order, matrix B B k ' I  X   X  P y Now, y the Higher-order Perceptron vector w (recall that v &gt; y from a computational standpoint, since in those cases the matrix update B skipped (this is equivalent to setting  X  tigation to collecting experimental evidence. x . The labels so obtained are flipped with probability  X  . If | u  X  x a new vector x corresponding test set accuracy. Again, no kernel functions have been used. reported results refer to algorithms with no kernel functions. best parameter settings for each kernel.
 the other two tasks we set p = 2 . In any case, for each value of p , we set 7  X  0 the matrix B Higher-order algorithm with norm parameter p and  X  instance, FO = HO results. In all our experiments HO comparison HO clearly superior in Breast. On Lymphoma, HO out to be only slightly worse than SO . On the artificial datasets HO perform better than SO . On USPS, HO sparse or not) significantly outperforms FO .
 periments). Thus, for instance, HO the comparison HO the smallest figures achieved on each row of the table.
 Figure 2: Experiments on the two artificial datasets (Artificial Notice that the test set in Artificial The reader might have difficulty telling apart the two kinds of algorithms HO into a single inference procedure. Like other algorithms, HO smallest figures achieved for each of the 8 combinations of dataset (RCV1 phase (training or test).
 version of HO kernel type (Gaussian or Polynomial), and phase (training or test).
