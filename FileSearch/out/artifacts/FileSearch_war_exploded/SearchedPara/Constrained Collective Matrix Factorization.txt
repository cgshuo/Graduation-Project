 Transfer learning for collaborative filtering (TLCF) aims to solve the sparsity problem by transferring rating knowledge across multiple domains. Taking domain difference into ac-count, one of the issues in cross-domain collaborative filter-ing is to selectively transfer knowledge from source/auxiliary domains. In particular, this paper addresses the problem of inconstant users (users with changeable preferences across different domains) when transferring knowledge about users from another auxiliary domain. We first formulate the prob-lem of inconstant users caused by domain difference and then propose a new model that performs constrained col-lective matrix factorization (CCMF). Our experiments on simulated and real data show that CCMF has superior per-formance than other methods.
 H.3 [ INFORMATION STORAGE AND RETRIEVAL ]: Information Search and Retrieval Collaborative Filtering, Transfer Learning, Collective Ma-trix Factorization, Inconstant Users.
Collaborative filtering [9] in recommender systems aims to predict users X  ratings in the future on a set of items based recommender systems, the rating matrix may be extremely sparse. As reported in [11], the density of the available rat-ings in commercial recommender systems is often less than tering, one common approach is to pool together the rating  X 
Corresponding author. This work was supported by Na-tional Natural Science Foundation of China (61003140, 61033010).
 knowledge transfer and sharing. Methods like [2, 3] assume lated to the target domain, while in practice it is often much users or related items but not both. In this paper, we ad-dress the problem of user-sided transfer learning, although the method proposed in this paper can also be applied to item-sided transfer learning. One well-known approach to such one-side-related problem is Collective matrix factor-ization (CMF) [12]. CMF is proposed for jointly factor-each user/item. When the domain difference is small be-tween source domain and target domain, e.g. transferring knowledge from a movie recommender system to another, this approach can easily improve prediction performance by simply jointing data from two systems.

CMF assumes that all users X  features are constant. How-ever, in real life, some users may change their features / pref-erences across different domains. For example, users having a preference over light music may like horror movies instead of ones about love stories because horror movies entertain than horror music (sounds). Even though the target do-main and auxiliary domain are of same type (e.g. movie), ferent item sets. We refer  X  X nconstant users X  to those with changeable preferences across different domains. For these users, it would not hold that the source domain and target domain share a user latent feature matrix. To improve rec-ommendation for inconstant users, we need a new method that takes the change of user feature into account.
To model the change of user feature caused by domain difference, we make two assumptions:
To model the assumptions above, we extend CMF to a new matrix factorization model named Constrained CMF, denoted as CCMF in this paper. Unlike CMF, CCMF does source domain and target domain.

The remainder of this paper is organised as follows. In sec-tion 2, we first introduce the problem setting. In section 3, we formulate the model. Then we experimentally validate the effectiveness of the proposed models in section 4. Re-lated work is introduced in section 5. Finally we conclude this paper in section 6. In our problem setting, we are given one source domain D src , say music, and one target domain D tgt (movie in our experiments). u 1 ,u 2 ,...,u n denote n users having rating records in both D src and D tgt . In rating matrix R D src , users make ratings on m src items { v src 1 ,v src 2 in rating matrix R tgt of D tgt , users make ratings on m v j is denoted as R and another n  X  m tgt rating matrix R test is for testing and is unknown while training the model. All the elements from R src , R tgt and R test are at the same scale (e.g. 1-5 or 0-1 [5]). Our goal is to make use of R src and R tgt example of our problem. ,  X  , users make }, and the 
A well-known and effective approach to recommender sys-tems is to factorize the user-item rating matrix, and utilize make prediction for future ratings [1, 4, 8, 13, 14] .
In order to learn the latent characteristics of the users and items in source and target domain, we employ probabilistric two domains are defined as: where N ( x  X , X  2 )is the probability density function of the Gaussian distribution with mean  X  and variance  X  2 . I user rated item and equal to 0 otherwise.

The zero-mean spherical Gaussian priors are also placed on user and item feature vectors:
To transfer knowledge about users from auxiliary domain, factorizing rating matrices in source and target domain with a same user feature matrix ( U src = U tgt ), our model factor-izes R src with U src and V src and factorizes R tgt with U and V tgt . We transfer the user feature matrix U src learned in source domain to help factorize the rating matrix in target domain by conducting a constraint on U src and U tgt :
Here U src i. is the prior mean of U tgt i. and the second term is the user feature bias caused by domain difference. Y v k . Intuitively, the more similar v source domain, the closer Y k. is to zero. The Gaussian prior for Y is given as follows: Hence, through a Bayesian inference, we have the log of the posterior distribution over the user and item feature and Y : =  X  1  X  1
V  X  1 where C is a constant that does not depend on the param-eters and l denotes the feature number.

Maximizing the log-posterior over latent features with hy-perparameters (i.e., the observation noise variance and prior variances) kept fixed is equivalent to minimizing the follow-ing sum-of-squared-errors objective functions with quadratic regularization terms:
E = 1 + 1 + k X k 2 F ro denotes the Frobenius norm. The graphical model is shown in Figure 2.  X  X  X  X   X  X  X  X   X  X  X  X   X   X   X   X   X   X   X   X   X  .  X   X   X   X   X   X   X   X   X   X   X   X   X   X   X  .  X  X  X  X   X  X  X   X  X  X  X 
To learn CCMF, we first calculate the partial derivatives of the objective function with respect to each variable:  X  X   X  X   X  X   X  X  Then a local minimum of the objective function can be found by performing gradient descent in Y , U src , V src and V
To check whether CCMF can fit with different settings, we evaluate CCMF on two data sets.
 One is a simulated dataset sampled from the Netflix dataset. The Netflix data set used in the experiments is constructed as follows. We first randomly extracted a 10 , 000  X  16 , 000 dense rating matrix R from the Netflix data, and take the data, so that R tgt and R src share only common users.
The other is a real dataset crawled from Douban 1 , which is launched in 2005 and is a Chinese SNS website allowing reg-istered users to record ratings and reviews related to movies, books, and music. We crawled 290 , 633 rating records rated by 5 , 000 users on 3 , 000 musical items and 10 , 000 movies. auxiliary training data. Then we sampled randomly from the movie rating matrix to generate training data and test-ing data. The final datasets are summarized in Table 1. We adopt two evaluation metrics: RMSE (Root Mean Square Error) and MAE (Mean Absolute Error).

Table 1: Description of Douban data and Netflix data
We compare out CCMF methods with two non-transfer learning methods: the UserMean and PMF [10], as well as CMF [12].
We denote the CCMF methods that utilize different con-straints given in Eqs. (6) and (7) as CCMF1 and CCMF2, respectively. The best results of using different parameters as described in the previous section are reported in Table 2. We can make the following observations:
One challenge of the transfer learning for recommender systems is that it is difficult to recommend items to users who have very few ratings in the target domain. In order target domain, and then evaluate prediction accuracies of different user groups. The experimental results are shown http://www.douban.com Table 2: Performance Comparisons. Numbers in boldface second best results among all methods, respectively.
Figure 3: Performance Comparison on Different Users in Figure 3. Users with less than 160 ratings are grouped into 6 classes:  X 1-10 X ,  X 11-20 X ,  X 21-40 X ,  X 41-80 X ,  X 81-160 X , and  X 161-640 X , denoting how many ratings users have rated in target domain.

Figure 3a summarizes the distributions of testing data ac-cording to groups in the training data (sparsity 0 . 4%). For example, there are a total 19820 user-item pairs to be pre-dicted in the testing dataset in which the related users in the training dataset have rating numbers from 1 to 10. In Fig-ure 3b, we observe that our CCMF algorithm consistently performs better than other methods when recommending items for all user groups.
PMF Probabilistic matrix factorization (PMF) [10] is a method for missing value prediction in a single matrix.
CMF Collective matrix factorization (CMF) [12] is pro-posed for jointly factorizing two matrices with the constraints of sharing one-side (user or item) latent features. However, CMF didn X  X  address the problem that users X  may have dif-ferent interests in different domains.

CST Coordinate System Transfer (CST) was a method proposed by [7] CST addresses the data sparsity problem in a target domain by transferring knowledge about both users and items from auxiliary data sources. In our problem setting, only knowledge about one side (user) from auxiliary domain is available. Hence, CST are not applicable to the problem in this paper.
 TCF [6] proposed a framework of Transfer by Collective Factorization that use the binary preference data expressed in the form of like/dislike to help reduce the impact of data sparsity of more expressive numerical ratings.
In this paper, we formulated the problem of inconstant users caused by domain difference and presented a new model to address the problem in transfer learning for collaborative filtering. Our method iteratively factorizes the rating ma-trices in source/auxiliary domain and target domain with a constraint on the user feature matrices for target domain and auxiliary domain. Experimental results on both and simulated and read data show that CCMF performs better than CMF at various sparsity levels. The study in this pa-per clearly demonstrates (a) the necessity of taking domain difference and change of user features into account, and (b) the items rated by a user implies to whether and how much the user X  X  feature has changed.

In this paper, we assumed that the rating matrices in aux-iliary and target domain are one-sided aligned. But in real life, some users may not have rating records in both domains. Hence, in order to model the domain difference more realis-tically, for future work, we will extend CCMF so that it can fit partial-users-aligned setting. In the future, we will also extend CCMF in heterogeneous settings, e.g. for transfering like/dislike knowledge from books or music to target domain that involves rating. [1] Y. Koren. Factorization meets the neighborhood: a [2] B. Li, Q. Yang, and X. Xue. Can movies and books [3] B. Li, Q. Yang, and X. Xue. Transfer learning for [4] I. Murray and R. Salakhutdinov. Evaluating [5] R. Pan, Y. Zhou, B. Cao, N. N. Liu, R. M. Lukose, [6] W. Pan, N. N. Liu, E. W. Xiang, and Q. Yang.
 [7] W. Pan, E. W. Xiang, N. N. Liu, and Q. Yang.
 [8] J. D. M. Rennie and N. Srebro. Fast maximum margin [9] P. Resnick, N. Iacovou, M. Suchak, P. Bergstrom, and [10] R. Salakhutdinov and A. Mnih. Bayesian probabilistic [11] B. M. Sarwar, G. Karypis, J. A. Konstan, and [12] A. P. Singh and G. J. Gordon. Relational learning via [13] K. Yu, S. Zhu, J. D. Lafferty, and Y. Gong. Fast [14] Y. Zhou, D. M. Wilkinson, R. Schreiber, and R. Pan.
