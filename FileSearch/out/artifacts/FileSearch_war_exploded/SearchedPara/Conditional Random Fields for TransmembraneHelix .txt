 A number of high throughput projects have been positioned to assist in the interpretation of the human genome sequence data. Structural determination of integral membrane proteins can be problematic due to difficulties in obtaining sufficient amounts of sample. Protein sequence analysis methods extended by our knowledge of protein structure may be suited to contribute significantly to these aspects of protein structure and function.
 tial classification problem and use Conditional Random fields (CRFs) to solve it [2]. Given a set of membrane proteins sequences, each single record in the set contains pair of sequences: The observation sequence, represented by x and the label sequence, represented by y . The protein observation sequence is a se-quence of amino acids, represented by 20 different letters. The label sequence is a transmembrane helical/non-helical structure sequence represented by binary labels 0/1 respectively. This data, called the training data, is represented by dict the helical structure of a target set, which has observation sequences only. The sequential classification problem is well known in many different fields such as computational linguistics, part of speech tagging, computational biology and many more. Given set of observation sequences, goal here is to find correspond-ing label sequences to these observations. A very common approach is using generative models, such as Hidden Markov Models (HMMs), finding the joint probability distribution p ( X, Y ) where X and Y are random variables describ-ing the observation and the labelled sequences respectively. This approach suffers from a major drawback that in order to find the joint distribution, a generative model has to calculate all possible observation sequences, which may be not prac-tical [3]. In contrast, the conditional models specify the probability of a label given an observation sequence p ( Y | X ). Thus, no effort is spent on modelling all possible observation sequences, but only on selecting the labels which maximize the conditional probability [2]. Conditional Random Fields (CRFs) is a probabilistic framework for labelling se-quential data. CRFs is a form of undirected graphical state model that defines a log-linear distribution for each state over the label sequence based on the obser-vation sequence [3]. CRFs main advantage over other non-generative finite-state models based on directed graphical models, such as Maximum Entropy Markov Models (MEMMs), is by avoiding a weakness called the label bias problem. The Markovian assumptions in MEMMs and similar state-conditional models separate the decision making at one step from future dependent decisions of consecutive steps, and may be biased towards states with fewer outgoing transi-tions. In contrast, CRFs have a single exponential model for the joint probability of the entire sequence of labels given the observation sequence [2]. corresponding to each of the random variables representing a label sequence Y v from Y and e  X  E corresponding to the transition between a given label to the next one. Even though in theory the structure of graph G may be arbitrary, in our application the graph is a simple chain, where each node corresponds to a label [3]. 3.1 Definition Let G =( V, E ) be a graph that Y =( Y v ) v  X  V . If each random variable Y v in the graph G obeys the Markov property, then ( Y, X ) is a conditional random field F in which p ( Y v | X, Y w ,w = v )= p ( Y v | X, Y w ,w  X  v ), where w  X  v are neighbors in G . A clique c in the graph G is defined as a subset of vertices which are completely connected. In a chain graph the cliques are either from first order (single vertex) or second order neighbors (two neighbor vertices).
 ables f is said to be a Gibbs random field if and only if its configuration obey a Gibbs distribution of the form: where Z is a normalizing factor: Z = f  X  F e  X  1 T U ( f ) , T is a constant called the temperature which equals to 1 in the most simple case and U ( f ) is the energy function. By the The Hammersley-clifford theorem if f obeys the Markov prop-erty (and positivity) then the physical topology (chain) coincides with the logi-cal topology and the energy function can be expressed as a sum of the cliques X  X  neighbors order: [4]. Since conditional random fields also hold the conditions of Markov ran-dom field, then according to Hammersley-clifford theorem, they have a Gibbs distribution, leading us to the fundamental theorem of random fields: sequence and the labels at positions i and i  X  1, g k ( y i ,x,i ) is a state feature function of the entire observation sequence and the label at position i .  X  j and  X  k are estimated from the training data. We assume that the feature functions f k and g k are given and fixed [3]. 3.2 Feature Functions and Model Estimation Each potential function actually represents a constraint on subset of random variables on which it operates. Thus, by satisfying a constraint we actually at the transition function as a general case of the state function by writing tion or state function [3]. Therefore, the probability of a label sequence y given the observation sequence x is in the form where Z ( x )= y exp ( j  X  j F j ( y, x )). The parameters (  X  j ) are computed by maximizing the log-likelihood with the training data using either iterative scaling or conjugate gradient methods [5, 3]. The most likely label sequence  X  y for input sequence x is 3.3 Feature Integration with the Model The most important aspect of specifying the model is selecting the set of features that capture the important relationships among the observation and the label sequences, in our case the protein sequence and the helical structure respectively [6]. In our work we have selected a basic set of features capturing the model X  X  constraints and divided them into several groups: Start, End and Edge Features. By using these features we capture the prob-ability of starting/ending a sequence with assigning a given label or the transition probability for moving from one state to the consecutive state. For instance, the start unigram feature has the form: u start ( x, i )= 1 if the Amino Acid at position i is the first in the sequence The relationship between the observation and a potential helix membrane struc-ture is described in the feature: Similarly, we define another set of features for the relationship with a non-helix membrane structure.
 consecutive labels: Basic Amino Acid Feature. Amino acids have different tendencies to popu-late one membrane helical structure in preference to another. Since our language contains 20 possible amino acids, we have 20 different unigram features from this type. The unigram feature of amino acid n in position i is: u n ( x, i )= 1 if the Amino Acid in sequence x at position i is from type n Amino Acid Property Feature. Amino acids differ one from another in their chemical structure expressed by their side chains, providing them different prop-erties. The fact that amino acids from the same classification group tend to appear in similar locations, motivated us to create special property features. We have adopted the properties classification taken from Sternberg [7] classifying the amino acids into nine groups 1 , each group described by a unigram feature. Note that some amino acids may appear in more than one group simultaneously. We now report on our experiments to test the effectiveness of features proposed in Section 3.3, embedded in a CRF model, to predict the location of membrane helical regions in protein sequences. 4.1 Data Set The data set consists of a set of 148 transmembrane protein sequences with experimentally confirmed transmembrane regions, which are significantly non-similar, based on pairwise similarity clustering compiled by M  X  oller et al [8]. The data set can be accessed via ftp://ftp.ebi.ac.uk/databases/testsets/trans membrane. We randomly picked 24 sequences out of the 148 and grouped them as a test set, using the remaining 124 sequences as the training set. We repeated this procedure ten times, having a cross validation test of ten independent ex-periments and calculated the average values of these measurements. 4.2 Results and Analysis In our experiment we have evaluated the prediction accuracy of the test set with the experimentally confirmed results based on two two main approaches: per-residue accuracy and per-segment accuracy as described in Chen, Kernytsky and Rost (henceforth referred as CKR) [9]. In per-residue accuracy the predicted label and actual label are compared by residue. In per-segment accuracy we determine how accurately a method correctly predicts the location of a trans-membrane helix (referred as TMH) region. We have used two popular methods to score per-segment accuracy. The first method requires a minimal overlap of 3 residues between the two corresponding segments and does not allow the same helix to be counted twice, as used in the paper of Chen et al. [9]. This method we refer as 3 R . The second method requires minimal overlap of 9 residues but does allow counting the same helix twice, indicated by 9 R . For our comparison we will closely follow the CKR paper as it has collated results of several methods for transmembrane helix prediction on a common benchmark data set displayed in the following table: the work of Chen et al. [9] and methods contained within as a reference. In the  X  X er-Residue Accuracy X  results we have achieved high prediction accuracy for both transmembrane and non-transmembrane residues, lower accuracy of trans-membrane residues only, and higher accuracy of non-transmembrane residues. In the  X  X er-Segment Accuracy X  results we can see a considerable difference between prediction among those helices who were detected by the model. When com-paring our prediction results with the other methods, our model performed well with high percentage of accuracy on the per-residue test. The CRFs model achieved the highest score among all 28 other methods in the over-all percentage of residues predicted correctly in both transmembrane and non-transmembrane helices ( Q 2 ) with 83% of true prediction. On the per-segment test, our model achieved high precision but low prediction score compared to the other models. Notice that some methods may have involved use of proteins from the data set as training so their results may be overestimates. In this paper we introduced the Conditional Random Fields (CRFs) technique which has found good application in the solution of sequential mining problems. We used CRFs to segment and label sequence data to solve the membrane pro-tein helix prediction problem. Our results look promising compared to currently available methods, and as such will motivate the future use of CRFs to solve se-quential labelling data problems. For more information on this paper please check our website on http://www.it.usyd.edu.au/  X  chawla/publications/crf1.pdf.
