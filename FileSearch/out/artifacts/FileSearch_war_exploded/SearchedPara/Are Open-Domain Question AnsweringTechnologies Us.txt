 TSUNEAKI KATO The University of Tokyo JUN X  X CHI FUKUMOTO Ritsumeikan University FUMITO MASUI
Mie University and NORIKO KANDO National Institute of Informatics 1. INTRODUCTION
Open-domain question answering (QA) technologies allow users to ask a ques-tion in natural language and obtain the answer itself rather than a list of documents that contain the answer. These technologies make it possible to re-trieve information itself rather than merely documents, and will lead to new styles of information access [Voorhees et al. 2000]. Research in this field has been encouraged and guided by a series of TREC conferences [TREC 2003], and has concentrated on answering factoid questions one by one in isolation from each other, although there was a notable exception [Voorhees 2001].
Such systems that answer isolated factoid questions are the most basic level of QA technologies, and will lead to more sophisticated ones that can be used by professional reporters and information analysts. At some stage of that sophis-tication, a young reporter writing an article on a specific topic will be able to translate the main issue addressed by his report into a set of simpler questions and then pose those questions to the QA system [Burger et al. 2001]. In daily situations also, questions are rarely asked in isolation, but rather in a cohe-sive manner that involves a sequence of related questions to meet the person X  X  information needs.

In addition, there is a relation between multidocument summarization and question answering. In his lecture, Eduard Hovy mentioned that multidocu-ment summarization may be able to be reduced into a series of question an-swering [Hovy 2001]. In SUMMAC, an intrinsic evaluation was conducted that measures the extent to which a summary provides answers to a set of obligatory questions on a given topic [Mani et al. 1998]. Those studies suggested that QA systems that can answer a series of related questions would surely be a useful aid to summarization work by humans and by machines.

In this paper, we first examine whether open-domain QA technologies meet those expectations, in other words, we identify what kind of questions QA sys-tems need to be answered and what kind of abilities those systems should have in order to participate in dialogues for accessing information, which occur when gathering information for a report on a specific topic, or when browsing information of interest to the user. Second, we propose a challenge designed to objectively and quantitatively measure a range of abilities of QA systems to par-ticipate in information access dialogues, which have characteristics we found in our empirical study. In this paper, we call this challenge QACIAD (Ques-tion Answering Challenge for Information Access Dialogue), although it is also called QAC2 Subtask3. Third, based on an analysis of the QACIAD run, we show that existing open-domain QA technologies have the potential to address this challenge, that is, to be used effectively in information access dialogues, although those have ample room to develop and accomplish a better result. 2. FEATURES OF QUESTIONS IN INFORMATION ACCESS DIALOGUES We collected and analyzed questions in order to examine what kind of abilities
QA systems need in order to be able to participate in information access dia-logues. As mentioned in Liddy [2002], the nature of questions asked by users varies, depending on the environments in which QA systems are used. In this study, we focused on the situation in which users interactively collect informa-tion for writing a report on a given topic, despite a wide diversity of information access dialogues. 2.1 Collecting Questions
Questions were collected as follows. Subjects were presented various topics, which included persons, organizations, and events, and were requested to devise questions in Japanese to elicit information for a report on that topic. The report was supposed to describe facts on a given topic, rather than state opinions or prospects on the topic. The questions were restricted to wh -type questions and a natural series of questions that may contain anaphoric expressions and ellipses were constructed.

As we were interested in the relationship between the amount of knowledge on a given topic and questions asked, the topics were presented in three differ-ent ways: (1) with a short description of the topic, which corresponds to the title part of the TREC topic definition [TREC 2003]; (2) with a short article or the lead of a longer article, which is representative of that topic and corresponds to the narrative part of the TREC topic definition; and (3) with five articles con-cerning that topic. The subjects were instructed to construct questions without considering whether the answer was contained in the given articles. That is, the information given was used only to understand the topic. The subjects then devised questions to elicit the information required for their reports. The number of topics was 60, selected from 2 years of newspaper articles.
Thirty subjects participated in the experiment. Each subject was instructed to structure one series of questions for the topic that consisted of about ten questions. Ten topics were processed per presentation type making a series of questions totaling 30 different topics.

Selecting 40 topics and three series of questions for each topic and topic pre-sentation type, that is, nine series per topic, and excluding inadequate ques-tions, such as yes-no question, we obtained 3,014 questions for the analysis.
The topics consisted of 6 persons, 3 organizations, 19 events, 9 artifacts, and 3 animals and fishes, among which 5 topics concerned sets of organizations and events, such as the big three companies in the beer industry, simultaneous ter-rorist attacks, and annual festival events. As those topics were selected from newspaper articles, many topics, even if the topics themselves were artifacts or animals, related to some events featured in those articles. 2.2 Characteristics of the Questions and Answers Table I shows the classification of questions according to the subject asked.
When users asked questions to get information for a report, the number of why -questions was relatively small. Moreover, there were fewer questions re-questing an explanation or definition than expected, probably because defini-tion questions such as  X  X ho is Seiji Ozawa? X  were decomposed into relatively concrete questions such as those asking for his birthday and birth place.
Table II shows the result of categorization according to the type of expected answers, that is, in what kind of syntactic and semantic units a given question is likely to be answered. This categorization is closely related to the previous one, as a why-question, for example, usually needs clauses or sentences as its answer. However, not all questions that were categorized as 4W questions could be answered by names. Whereas questions asking where, such as  X  X here was Shakespeare born? X  could be answered by a place name, questions like  X  X here do lobsters like to live? X  need a description and not a proper name as the answer. In this categorization, proper names include the titles of novels and movies; common names are something like names of species and body parts.
This categorization was conducted by inspecting questions only and it was difficult to decisively determine whether some of the questions could be an-swered by names or by clauses; these were categorized as  X  X ames probably. X 
For example, the question  X  X here does the name  X  X IBO X  come from? X  could be answered by name if AIBO is an acronym, but there may be a long story as to its origin. Although such cases happened in other combinations of categories, those questions were just categorized into a more complex category, as only the border of names and descriptions is especially important.

As Table II shows, 58% X 75% of questions for writing reports could be an-swered by values or names. The amount of those questions was almost the same as the amount of 4W questions, since while some 4W questions could be answered by neither names nor values, some definition and explanation ques-tions could be answered by names or values.

Regarding the difference of distributions of question and answer types in terms of the topic presentation types, which is related to the amount of knowledge that the subjects had on a given topic, we found a slight tendency that those subjects who had greater knowledge asked more questions seeking an explanation, and accordingly more questions needed clauses and sentences to answer. This is likely to occur because definition questions are rarely decom-posed when asking peripheral objects and the more knowledge a subject has, the more such objects are considered. However, more sophisticated experiments are needed to investigate this issue. 2.3 Pragmatic Phenomena Observed
Japanese has four major types of anaphoric devices: pronouns, zero pro-nouns, definite noun phrases, and ellipses. Zero pronouns are very common in Japanese, in which pronouns are not apparent on the surface. As Japanese also has a completely different determiner system from English, the difference between definite and indefinite is not apparent on the surface. Definite noun phrases usually have the same form as generic noun phrases. Table III shows the summary of such pragmatic phenomena observed. 2 The total number is more than 3014, as some questions contain more than one anaphoric expres-sion.  X  X ho had been the leader of that country until then ? X  is an example of such a question with multiple anaphoric expressions.

Among 1135 questions with no reference expression, 329 questions (29%) are the first one of a question series. The others are divided into two cases. In the first case (39%), the current foci appear literally rather than in pronouns or definite noun phrases. Those cases are frequent when the topic is a person and his/her first name is used for reference and when the topic is a general concept not an individual (named entity), such as Japanese river otters and hybrid cars. The second case (32%) is the beginning of and the resume from a subdialogue. For example, in the series on an erroneous report regarding the high concentration of dioxin, after some questions about the news pro-gram, the chemical feature of dioxin was asked such as  X  X ow poisonous is dioxin? X 
Out of 1915 reference expressions excluding ellipses in Table III, 546 expres-sions (29%) refer to items other than the global topic. Moreover, 502 expressions of those (92%) are a sole reference expression in a question. The existence of questions without referring to the global topic means that the focus shifts in those question sequences, and the current focus differs from the global topic.
Our corpus has 496 questions (16% of the total questions) of this kind.
A wide range of reference expressions is observed in the questions in in-formation access dialogues, even when such dialogues are limited to those for report writing and forbidding interactions, such as clarification subdialogues.
Moreover, our study shows that those sequences of questions are sometimes very complicated and include subdialogues and focus shifts. 3. DESIGN OF QACIAD
In the rest of the paper, we propose a challenge, QACIAD, to objectively and quantitatively measure such abilities of QA systems that can address informa-tion access dialogues with the features observed in the empirical study reported above. QA systems need a wide range of abilities in order to participate in infor-mation access dialogues [Burger et al. 2001]. First, the systems must respond in real time to make interaction possible. They must also properly interpret a given question within the context of a specific dialogue, and also be coopera-tive by adding appropriate information not mentioned explicitly by the user.
Moreover, the systems should be able to pose a question for clarification to re-solve ambiguity concerning the user X  X  goal and intentions, and to participate in mixed initiative dialogue by making suggestions and leading the user toward solving the problem. Among these various capabilities, focusing on the most fundamental aspect of dialogue, that is, interpreting a given question within the context of a specific dialogue, QACIAD measures the context-processing abilities of systems, such as anaphora resolution and ellipsis handling.
In QACIAD, QA systems are requested to answer a series of related ques-tions. This series of questions and the answers to those questions comprise an information access dialogue. Although systems are supposed to participate in dialogue interactively, the interaction is only simulated; systems answer a se-ries of questions in a batch mode. Such a simulation may neglect the inherent dynamics of dialogue, as the evolution of dialogues is fixed beforehand and sys-tems have no opportunity to control it. It is, however, a practical compromise for objective evaluation. Since all participants have to answer the same set of questions in the same context, the results for the same test set are comparable with each other and the test sets of the challenge can be made reusable by pooling correct answers.

The origin of QACIAD comes from QAC1 (Question Answering Challenge), one of the tasks of the NTCIR workshop 3 [Fukumoto et al. 2003][NTCIR 2003]. The current design of QACIAD reported in this paper is its extensive elaboration. 3.1 Type of Individual Questions
QACIAD covers factoid questions in Japanese that have values and names as answers. Two years of articles from two newspapers are used for the underlying document set. Using those documents as the data source, the systems answer various open-domain questions. This setting looks realistic where users write reports through interacting with a QA system of this range, since, as the study above showed, 58 X 75% of questions for writing reports fall into this category. In addition, an examination of 737 questions of this category revealed that the answers to 84% of those could be found by humans from those articles. supports the reality of the setting where users write reports interacting with a QA system that has newspaper articles as its underlying document set.
QACIAD focuses on QA technologies that can be used as components of larger intelligent systems and technologies that can handle realistic problems. It re-quests exact answers rather than the text snippets that contain them, while avoiding addressing definition questions and why questions, because such an-swers are crucial inputs to other intelligent systems. Moreover, as such a situa-tion is considered to be more realistic, the systems must collect all the possible correct answers and detect the absence of an answer. Therefore QACIAD re-quests systems to return one list of answers that contains all and only correct answers. 3.2 Information Access Dialogue
Considering scenes in which those QA systems participate in a dialogue, we classified information access dialogues with them into the following two categories.

Gathering Type. The user has a concrete objective such as writing a report and summary on a specific topic, and asks a system a series of questions all concerning that topic. The dialogue has a common global topic, and, as a result, each consecutive question shares a local context.

Browsing Type. The user does not have any fixed topic of interest; the topic of interest varies as the dialogue progresses. No global topic covers a whole dialogue but each consecutive question shares a local context.

The challenge was designed to measure the abilities of QA systems useful in both types of dialogue. 3.3 Characteristics of Question Series
QACIAD requests participant systems to return answers to a series of ques-tions. This series of questions and the answers to those questions comprise an information access dialogue. Three examples of the series of questions are shown in Figure 1, which were picked from our test set discussed in Section 3.5.
Series 14 and 20 are of the gathering type, while series 22 is a typical browsing type.

Precisely speaking, the series in the test set can be characterized through the pragmatic phenomena that they contain. The gathering-type series consist of questions that have a common referent in a broad sense, which is a global topic mentioned in the first question of the series. The strictly gathering-type series can be distinguished as a special case of gathering-type series. In those series, all questions refer exactly to the same item mentioned in the first question and do not have any other anaphoric expression. In other words, questions about the common topic introduced in the first question comprise a whole sequence.
Series 14 is an example of the strictly gathering type and all questions can be interpreted by supplying Seiji Ozawa, who is introduced in the first question.
Other gathering-type series have two other types of questions. The first type of questions not only makes reference to the global topic but also refers to other items or has an ellipsis. The second type of question makes a reference to a complex item, such as an event that contains the global topic as its component. Series 20 is such a series. The third question refers not only to the global topic,
George Mallory, in this case, but also to his famous phrase. The sixth one refers to the event George Mallory was involved in.

On the other hand, the questions of a browsing-type series do not have such a global topic. In some cases, the referent is an item mentioned in previous questions and, in other cases, it refers to the answer of the preceding question.
The former is the case in the third and fourth questions in series 22. The lat-ter is the case in the fifth, seventh, and eighth. According to this definition, as some focus shifts were observed in our experiment, dialogues for gather-ing information contain the browsing-type dialogue as those subparts, while the gathering-type dialogue mainly occurs in such a situation. This is why we included browsing-type series in our challenge. Even when we focus on informa-tion access dialogue for writing reports, the systems must handle focus shifts appearing in browsing-type series.

In both series, all questions, except the first one of each series, have some anaphoric expressions, which may be zero pronouns. That is, anaphoric expressions are used eagerly when those are possible and no subdialogue ap-pears in any series. It may be unrealistic, as our study showed, but a practical compromise in view of the difficulties of the challenge.

In QACIAD, several series are given to the systems at once and the systems are requested to answer those series in a batch mode. The systems must identify the type to which a series belongs, as it is not given, although the systems need not identify the changes of series, as the boundary of series is given.
The systems must not look ahead to the questions following the one currently being handled. This restriction reflects the fact that QACIAD is a simulation of interactive use of QA systems in dialogues. This restriction, accompanied with the existence of two types of series, increases the complexity of the context processing that the systems must employ. Especially in Japanese, frequent use of zero anaphora and no evident distinction between the definite and indefinite make those problems more serious. 3.4 Evaluation Measure
The judgment as to whether a given answer is correct or not takes into account not only the answer itself but also the accompanying article from which the answer was extracted. If the article does not validly support the answer, that is, assessors cannot understand whether the answer is the correct one for a given question by reading that article, it is regarded as incorrect, even though the answer itself is correct. The correctness of an answer is determined according to the interpretation of a given question done by human assessors within the given context. The system X  X  answers to previous questions and its understanding of the context from which those answers were derived are irrelevant. For example, the correct answer to the second question of series 22, namely when the Yankee stadium was built, is 1923. If the system wrongly answers the Shea stadium to the first question and then  X  X orrectly X  answers the second question 1964, the year when the Shea stadium was built, that answer to the second question is not correct. On the other hand, if the system answers 1923 to the second question accompanying by an appropriate article supporting it, that answer is correct no matter how the system answered the first question. Although we know it is somewhat counterintuitive in its extreme case, it is another compromise to make the evaluation practicable to perform.

In QACIAD, as the systems are requested to return one list consisting of all and only correct answers and the number of correct answers differs for each question, a modified F measure is used for the formal evaluation, which takes account of both precision and recall. Two modifications were needed. The first is for the case where an answer list returned by a system contains the same answer more than once or answers in different expressions denoting the same item. In that case, since such a redundancy is to be penalized, only one answer is regarded as the correct one and other duplication as a wrong one. Thus, the pre-cision of such an answer list decreases. Cases regarded as different expressions denoting the same item include a person X  X  name with and without the position name, variations of foreign name notation, differences of monetary units used, differences of time zone referred to, and so on. The second modification is for questions with no answer. For those questions, the modified F measure is 1.0, if a system returns an empty list as the answer, and is 0.0 otherwise. Hereafter we call this modified F measure, MF1 . The formal evaluation measure of this challenge, which was announced to the participants in advance, is MMF1 : the mean of MF1 over all questions in a test set.

MF1 can be considered as a measure for evaluating, as a whole, both of the abilities to collect all and only the correct answers and to identify redundancy. In addition to MF1 , two auxiliary evaluation measures are proposed and adapted to the challenge in order to evaluate various aspects of system abilities. The first auxiliary measure, which we call MF2 , ignores the ability to identify duplication and evaluates other abilities. This measure is another modified F measure and uses precision calculated by removing duplicated answers from not only the correct answers but also from the whole answer list. For example, if a system returns to a question with four correct answers a list of five items among which three are correct and one is a duplicate, the precision in MF2 is (3 0 . 5 while the precision in MF1 is (3  X  1) / 5 = 0 . 4. Recall used in common in MF1 and MF2 is given by (3  X  1) / 4 = 0 . 5. MF1 results in 0 0 . 5, in this case.

The second measure considers the difficulty of collecting all and only the cor-rect answers and approximates this difficult task to an easier one in which only one correct answer needs to be found. This is a generalization of RR (Reciprocal Rank) for ranked lists to lists without rank.

RR can be interpreted as follows. In case of question answering, users have to verify the answers the system returned. In the case of answers with a rank, the number of verifications needed to obtain one correct answer is one when the correct one is in the first rank, two when in the second rank and so on, since the verification is done by the order of the given rank. That is, the number of verifications, which can be regarded as a cost, is equal to the highest rank of the correct answers in the returned list. RR is the reciprocal of that cost.
In the case of lists without rank, the verification cost, c ( n , m ), for obtaining one correct answer from a list of m items with n correct answers is defined as follows. First, select at random, one item from the list, and verify it. If confirming it is correct, the cost is equal to 1, otherwise repeat the same procedure on the remaining m  X  1 items, which incurs a further cost of c ( n , m of not obtaining the correct answer in the first verification is ( m we obtain the following recurrence formula: c ( n , m ) = this formula, c ( n , m ) = ( m + 1) / ( n + 1). The measure is the reciprocal of this cost, and is named RC (Reciprocal Cost). When n = 0, as nothing is obtained from this verification procedure, the numerator is zero and so RC are evaluated using MRC , the mean of RC over all questions in a test set. In this evaluation, as a natural conclusion from its characteristics, an answer is correct regardless of whether it is duplicated. Questions with no answer in a given test set are ignored.

QACIAD uses MMF1 as the formal measure announced to the participants in advance, and the other two, MMF2 and MRC , as subordinate measures for closer examinations, and tries to examine the various characteristics of participant systems. It is worth noting that these three measures are not specific to QACIAD, but are general for any challenge that addresses list-type questions. 3.5 Constructing a Test Set
We constructed a test set for QACIAD using questions collected in the previous study as material. Those questions were natural in both content and expression since, in the experiment, the subjects did not consider whether the answers to their questions would be found in the newspapers; some subjects did not read the articles at all. We selected 26 from 40 topics and chose appropriate questions and rearranged them for constructing gathering-type series. Some of the questions were edited in order to resolve semantic or pragmatic ambiguities, although we tried to use the questions without modification where possible.
We constructed each series to have about seven questions. The topics of the gathering series consisted of 5 persons, 2 organizations, 11 events, 5 artifacts, and 3 animals and fishes.

Browsing-type series were constructed by using some of the remaining ques-tions as seeds of a sequence and adding new questions to create a flow to/from those questions. 4 For example, series 22 shown in Figure 1 was composed by adding the last four newly created questions to the first four questions, which were collected for the Yankee stadium. We created 10 browsing series in this way.

Finally, the test set constructed this time contained 36 series and 251 ques-tions, with 26 series of the gathering-type among which 5 series are of the strictly gathering type and 10 series of the browsing-type. The average number of questions in one series was 6.92.

Table IV shows the summary of observed pragmatic phenomena in the same form as Table III. Similarly to questions collected, a wide range of pragmatic phenomena is observed in the test set. The numbers in parentheses show the number of cases in which the referenced item is an event. Since questions are often handled as a sequence of keywords in the current techniques of QA, it is anticipated that reference expressions referring to events that cannot be represented by one keyword are more difficult to handle than those referring to persons or organizations. This table shows our test set contains as much such difficult reference expressions as questions collected. The test set differs from questions observed in that only the first questions of series have no anaphoric expression. Anaphoric expressions are used eagerly when they are possible and no subdialogue appears in any series. As mentioned above, this is a result of a practical compromise in view of the difficulties of the challenge.

Sophisticated focus tracking is indispensable to get correct answers from this test set. Systems cannot even retrieve articles containing the answer just by accumulating keywords. This is clear for the browsing-type, as an article is unlikely to mention both the New York Yankees and Campbell soup. In the gathering-type, since the topics mentioned in relatively many articles were chosen, it is not easy to locate the answer to a question from those articles retrieved using that topic as the keyword. For example, there are 155 articles mentioning Seiji Ozawa in our document sets of which 22 mention his move to the Vienna Philharmonic Orchestra; only 2 also mention his birthday. 3.6 Reference Set
The ability that QACIAD measures is a combination of several kinds of abilities concerning question answering for handling information access dialogues. Al-though this may be desirable and an objective of QACIAD, occasionally we need an isolated evaluation of context processing. This isolation cannot be achieved by introducing any evaluation measure. In order to fulfill this need, we devised two types of accompanying test sets for reference.

The first reference test set consists of isolated questions, that is, not in se-ries, obtained from questions of the original test set by manually resolving all anaphoric expressions including zero anaphora. The second reference test set consists of isolated questions obtained from questions of the original test set by mechanically removing anaphoric expressions. Although most of the ques-tions in the second test set are semantically underspecified, such as asking a birthday without specifying whose birthday, all the questions are syntactically well formed (in the case of Japanese). The first reference test set measures the ceiling of the context processing, in a given original test set, while the sec-ond measures the floor. These are only for reference, since there are several ways of resolving anaphora and context processing, sometimes makes things worse. Nevertheless, the reference test sets should be useful for analyzing the characteristics of technologies used by the participant systems. 4. EXPERIENCE OF THE CHALLENGE AND THE CURRENT STATE OF TECHNOLOGIES
Seven teams and fourteen systems participated in the run in December 2003 using the test set mentioned in Section 3.5, which was conducted as a subtask of QAC2, which, in turn, is a task of the NTCIR workshop 4 [NTCIR 2003].
This experience allows us to examine the difficulty of QACIAD and how ex-isting technologies address the challenge. The behavior of the three measures proposed and the role of the reference sets are also examined, based on the result of the run. 4.1 Difficulties
Figure 2 shows the MMF1 of the top 10 participant systems, each of which is designated by its run ID such as CRL2 and TKB1 . It depicts the evaluations for three categories of questions: all of the test set, the first in each series, and the second and later. As anticipated, it is more difficult to correctly answer the questions other than the first question in each series. 5 questions falls to about one-half of those for the first questions. This difference, at least partially, comes from the insufficiency of context processing, that more sophisticated context processing is needed. The performances shown by MMF1 are not high even for the top systems, which are around 0.2. Those are inadequate for practical use. Considering those values are about one-half of the performance of handling list-type questions without context processing, however, the setting of QACIAD does not make problems excessively hard and this challenge is considered to be reasonable and challenging, although difficult, for existing QA technologies.

Figure 3 shows the difference of the performance according to the type of se-ries: MMF1 for the strictly gathering-type, other gathering-type, and browsing-type. For the majority, the questions in the browsing-type series are more dif-ficult to answer. 7 4.2 Evaluation Measures and Reference Sets
Figure 4 shows the behavior of three measures, MMF1 , MMF2 , and MRC , for the top 10 systems. The evaluation of each system differs a little, depend-ing on the measures employed, but the differences among systems are too small to change the rank of the systems. Concerning MMF1 and MMF2 , this means there is no big difference in the systems in terms of the ability to detect redundancy, and the difference is something that can be improved by enhanc-ing this process. As for MRC , its values range around those of the other two measures, although MRC is related solely to precision. 8 This suggests that the systems offer a reasonable balance of precision and recall.
 Actually, the differences in those evaluations were smaller than we expected.
The main reason must be that all systems were designed on the basis of the evaluation using MMF1 and tuned to it, without consideration of MMF2 and
MRC . For valuable evaluations using multiple measures, those planned to be used should have been announced in advance, which is a lesson we learned from our experience of the challenge.
 The run using the reference test sets provides very useful information.
Table V shows the averages of MMF1 ( AMMF1 ) of the top 10 participant sys-tems concerning the first questions of series of the test set and second and later questions, compared with the corresponding questions in the first reference set. In the test set, AMMF1 of the second and later questions falls considerably compared to the first questions. This is natural, as this table is a summary of the data shown in Figure 2. It is worth noting that a similar but weak ten-dency can be observed in the corresponding questions of the reference set. The reason could be that when constructing a series of questions on a given topic, prominent questions, which are easier than the others, tend to be put at the beginning of the series. Of course, on the second and later questions, the dif-ference of difficulty is significant between the test set and the corresponding questions of the reference set. 9 Therefore, it is still true that the insufficiency of context processing lowers the evaluation. It should be noted that the insuf-ficiency of context processing is not only the reason for the difference shown in Figure 2.

Similarly, Table VI shows the difference of AMMF1 of the top 10 participants according to the type of series, compared with the corresponding questions of the first reference set. The difference of performance is also observed in the reference set. It is interesting that questions from browsing series, which are the anaphoric expressions had been manually resolved. This may be because browsing series contain more prominent questions like the first question in the series than gathering series, since the former type asks fewer questions on each topic. Since this data means that organizing easier questions into browsing series makes them difficult, it strongly supports the assertion regarding the difficulty of browsing series made in the previous section.

The second reference set, accompanied with the first set, discloses whether a given question really needs context processing in order to answer it correctly.
For example, considering the question  X  X hat is the name of the hybrid car that the company released in 1997? X  following  X  X hat automobile company was
Toyoda Shoichiro chairman of? X  (Series 5), we found that the evaluation of its corresponding questions in the first and second reference set had almost the same value (0.58 and 0.56 in average MF1 ), which was much higher than the evaluation of the original question (0.02). This shows that context processing is not needed for this question; rather, it makes the situation worse. It seems strange that the systems can answer the question correctly without restriction of the company, but, in fact, as only one hybrid car was released in 1997, the answer is the same as the answer to the question  X  X hat is the name of the hybrid car released in 1997? X  The question  X  X t what altitude on Everest was he last seen? X  is another example, since no one except George Mallory was seen on Everest. It is difficult to estimate for each question in advance how much dispensable context processing exists in order to obtain the correct answer, since it relates to the content of the underlying data source. Our reference sets reveal such information clearly. 4.3 Techniques Employed
As far as is known from the participants X  reports submitted to the NTCIR workshop 4 meeting, techniques employed for context processing for the run are rather simple. These techniques provide a basis for further development, although those still leave room for enhancement and there are possibilities of employing other techniques that use the findings of research on discourse understanding.

In the most prevailing case, systems do not analyze referential expressions in a given question at all, but simply treat that question as a continuation of preceding questions [Akiba et al. 2004; Hidaka et al. 2004; Takaki 2004].
Systems that use keywords extracted at the question-analysis stage into the subsequent stages, take keywords in the preceding questions into consideration, in addition to those in the current one. In one system, which uses higher-order characteristics, namely, word bigrams, a concatenation of the preceding and current questions is regarded as a question to be processed. Such systems differ in which questions are considered as the preceding ones. Some use only the first of the series, while others use the whole series up to the current one. Another consideration is the balance of weights of keywords in those questions. One system adds the answers to the preceding question into the keywords of the current question.

Another system employs a more sophisticated way of handling context in its question-analysis stage [Fukumoto et al. 2004]. It determines referents us-ing a shallow syntactic X  X emantic analysis of questions. An antecedent question is analyzed and decomposed into the entity description, attribute description, and interrogative expression. For example, the entity description, attribute de-scription, and interrogative expression of the question  X  X ho is the president of the United States? X  are  X  X he United States, X   X  X he president, X  and  X  X ho, X  re-spectively. When the current question has no apparent reference expression, that is, when it has zero anaphora or is an elliptical fragment, the similarity of interrogative expressions of the antecedent and the current one is used as a clue to determine whether the entity or the attribute should be supplied as the referent. The difference between  X  X ho is  X  of France? X  and  X  X ow large is land area? X  following the above question is handled in this manner. When the question contains a reference expression, its semantic category is used for that decision. As one series consists of several questions, there is ambiguity as to which of them could be the antecedent, which is resolved using heuristics. QA systems could handle context in modules other than question analysis.
A system determines the documents from which the answers are extracted while processing the first question of a series and uses them exclusively while processing the whole of the series [Hidaka et al. 2004]. Although this technique seems rather crude, it is unique for question answering and its refinement may lead to a novel technique.

Each technique mentioned in this section has its advantages and disadvan-tages derived from its intrinsic characteristics. For example, document restric-tion by the first question cannot work properly for the browsing series. In our experience, however, it is not clear that such a relationship exists between tech-niques employed and evaluation results of the run. This is probably because system performances depend on several factors, including robustness against noise and existence of spurious keywords. 5. RELATED WORK
The attempt most closely related to our challenge is the context task conducted in TREC 2001 [Voorhees 2001]. The purpose is to evaluate the systems X  ability to track context in order to support interactive user sessions, which is basically the same as ours. The results were unexpected, as they revealed that the abil-ity to correctly answer questions later in a series was uncorrelated with the ability to correctly answer questions earlier in the series. This was considered to be because the first question in a series defined a sufficiently small subset of documents so the results were dominated by whether the system could answer the particular type of current question. This task was judged to be unsuitable for evaluating context-sensitive processing for the latest QA technology and was not followed by any similar task in TREC evaluations.
 We speculate that those results come from two characteristics of the task.
First, the number of questions in a series is relatively small, around 3 and 4. Second, series, such as the browsing-type mentioned in this paper, are not included in the test set. It was reported that answers to 85% of consecutive questions were found from the same paragraph [Harabagiu et al. 2001]. In contrast, in our challenge, the rate of answers to consecutive questions being found from the same article is 66% in the browsing-type series, and 83% for the gathering-type series. Only for three series are some answers to all of the questions found in the same article. That is, the situation is different between our challenge and the TREC context task. It is also important to note that the situation naturally came from the setting of information access dialogues for writing reports. The TREC context task, on the other hand, does not have such a realistic background.

The results, however, do not discount the importance of the TREC context task. For example, research on discourse structure for interactive QA has in-spired Chai and Jin [2004]. In their paper, they evaluate the TREC context task as an experience that motivates a more in-depth investigation of the role of discourse in interactive QA. The QACIAD proposed in this paper would play the same role, although perhaps more effectively.

Another research trend in interactive QA is the several projects which are part of the ARDA AQUAINT program [AQUAINT 2003; Small et al. 2003; Liddy 2003; Hickl et al. 2004]. These studies concern scenario-based QA, the aim of which is to handle nonfactoid, explanatory, analytical questions posed by users with huge background knowledge. The issues include clarification dialogues in order to disambiguate users X  intention and interests and question decomposi-tion to simpler and more tractable questions. It would be rash to consider that those researches have different interests and scope from ours. A wide range of abilities is necessary in order for QA systems to become a dialogue participant in a genuine sense and those studies merely focus on abilities that supplement ours, namely context processing. It is certain that scenario-based QA needs the abilities evaluated by our challenge. 6. FUTURE WORK
We need to consider the evaluation method in greater detail. There are two major problems. The first concerns the F measure, for which the number of correct answers needs to be calculated. That is, the value of the F measure changes when a new correct answer is found. In question answering, as the number of correct answers is usually relatively small, the recall rate sometimes falls to half if a minor alternative answer is found to a question that had been assumed to have only one correct answer. Even worse, some questions have more than one way of enumerating correct answers. For example, to a question asking for the sites of a ski jump competition, a system may answer six city names, and another system may answer three country names. Neither is wrong.
A system could even answer four city names and one country name. Although we address these case by case, we need a principle for handling such cases.
In TREC 2003 this problem was cleverly avoided by carefully checking the questions. 10
The second and more serious problem comes from handling dialogues. As mentioned above, whether an answer is correct or not is determined by human interpretation of a given question within the given context and is not affected by a system X  X  interpretation and the answers it returned to the previous ques-tions. Although it is a practical compromise, many feel that this evaluation cri-terion is somewhat peculiar in its extreme case. As mentioned in the example in
Section 3.4, in series 22, the answer to the second question, 1923, is considered correct even if the system wrongly answered Shea stadium to the first question.
This is not completely absurd because that system may manage the context in-tentionally rather than extentionally, in which case the system may interpret the second question as  X  X hen was the home to the New York Yankees built? X  It is doubtful, however, whether such a  X  X orrect X  answer has any value in practice.
This problem shows the importance of cooperative response. It may be effec-tive to change the style of answering from a current list of answers to answers with additional information. In this example, it would be better to answer  X  X he
Yankee stadium was built in 1923, X  and the correctness of answers should be judged by including this additional information. The difficult and remaining problem is to formalize this type of cooperative response to a sufficient level for use in objective evaluations like QACIAD.

Regarding the design of the challenge and construction of the test sets, the remaining problem is that the framework for constructing browsing-type series has not yet been established, although we can construct natural gathering-type series by the procedure of collection and selection mentioned in this paper.
The current browsing-type series are rather arbitrary and are not based on real situations. In addition, in order to make our challenge more realistic, we may have to address questions that ask for explanations and definitions, and question series with a more complicated discourse structure. It is problematic to include those into our challenge while maintaining the criteria of objectivity and reusability.

Although our study alluded to a relationship between the amount of knowl-edge and questions asked, its details and implications for QA technologies are still not clear. We must analyze this issue more closely and combine the result with conceptual studies on question answering such as the ones by Lehnert [1977] and Ram [1999]. Such studies may bridge the gap between our approach and scenario-based QA. 7. CONCLUSION
In this paper, we empirically examined what kind of abilities are needed for QA systems in information access dialogues and proposed a challenge, QACIAD, for objectively and quantitatively evaluating those abilities. From the empirical study, we found that questions that have values and names as answers account for a majority in realistic information-gathering situations and that those se-quences of questions contain a wide range of reference expressions, sometimes complicated by the inclusion of subdialogues and focus shifts. The challenge proposed is not only novel as an evaluation of the handling of information ac-cess dialogues, but also includes several valuable ideas, such as categorization and characterization of information access dialogues. Three measures are intro-duced to judge various aspects in addressing list-type questions and reference test sets for evaluating context-processing ability in isolation. Our experiments showed that existing technologies have the potential to address the challenge, although those have ample room to develop and accomplish a better result.
We wish to thank all participants in QAC2 for their valuable discussion and comments.

