 Lexical weighting features (Koehn et al., 2003) es-timate the probability of a phrase pair or translation rule word-by-word. In this paper, we introduce two simple improvements to these features: one which smooths the probability of translating word f to word e using English morphology, and one which conditions it on the kind of training data that f and e co-occurred in. These new variations lead to im-provements of up to + 0 . 8 BLEU, with an average im-provement of + 0 . 6 BLEU across two language pairs, two genres, and two translation systems. Since there are slight variations in how the lexi-cal weighting features are computed, we begin by defining the baseline lexical weighting features. If f = f pair, let a i ( 1 i n ) be the (possibly empty) set of positions in f that e i is aligned to.

First, compute a word translation table from the word-aligned parallel text: for each sentence pair and each i , let Then where f can be NULL .

Second, during phrase-pair extraction, store with each phrase pair the alignments between the words in the phrase pair. If it is observed with more than one word alignment pattern, store the most frequent pattern.

Third, for each phrase pair (  X  f ,  X  e , a ) , compute This generalizes to synchronous CFG rules in the ob-vious way.
 Similarly, compute the reverse probability t (  X  f j  X  e ) . Then add two new model features Consider the following example Chinese sentence: (5)  X  X  X  X  (6) Human: Wen Jiabao said that C X te d X  X voire is (7) MT (baseline): Wen Jiabao said that Cote (8) MT(better): Wen Jiabao said that Cote d X  X voire The baseline machine translation (7) incorrectly gen-erates plural nouns. Even though the language mod-els (LMs) prefer singular nouns, the lexical weight-ing features prefer plural nouns (Table 1). 1
The reason for this is that the Chinese words do not have any marking for number. Therefore the infor-mation needed to mark friend and partner for num-ber must come from the context. The LMs are able to capture this context: the 5-gram is China X  X  good friend is observed in our large LM, and the 4-gram China X  X  good friend in our small LM, but China X  X  good friends is not observed in either LM. Likewise, the 5-grams good friend and good partner and good friends and good partners are both observed in our LMs, but neither good friend and good partners nor good friends and good partner is.

By contrast, the lexical weighting tables (Table 2, columns 3 X 4), which ignore context, have a strong preference for plural translations, except in the case of t (  X  X  X  j friend ) . Therefore we hypothesize that, for Chinese-English translation, we should weaken the lexical weighting features X  morphological pref-erences so that more contextual features can do their work.

Running a morphological stemmer (Porter, 1980) on the English side of the parallel data gives a three-way parallel text: for each sentence, we have French f , English e , and stemmed English e  X  . We can then build two word translation tables, t ( e  X  j f ) and t ( e j e  X  ) , and form their product Similarly, we can compute t m ( f j e ) in the opposite direction. 2 (See Table 2, columns 5 X 6.) These tables can then be extended to phrase pairs or synchronous CFG rules as before and added as two new features of the model:
The feature t m (  X  e j  X  f ) does still prefer certain word-forms, as can be seen in Table 2. But because e is generated from e  X  and not from f , we are protected from the situation where a rare f leads to poor esti-mates for the e . When we applied an analogous approach to Arabic-English translation, stemming both Arabic and English, we generated very large lexicon tables, but saw no statistically significant change in BLEU. Perhaps this is not surprising, because in Arabic-English translation (unlike Chinese-English transla-tion), the source language is morphologically richer than the target language. So we may benefit from fea-tures that preserve this information, while smoothing over morphological differences blurs important dis-tinctions. Typical machine translation systems are trained on a fixed set of training data ranging over a variety of genres, and if the genre of an input sentence is known in advance, it is usually advantageous to use model parameters tuned for that genre.

Consider the following Arabic sentence, from a weblog (words written left-to-right): (10)  X  X  X  X  (11) Human: Perhaps this is one of the most impor-(12) MT (baseline): This may be one of the most (13) MT(better): Perhaps this is one of the most im-The Arabic word  X  X  X  X  can be translated as may or per-haps (among others), with the latter more common according to t ( e j f ) , as shown in Table 3. But some genres favor perhaps more or less strongly. Thus, both translations (12) and (13) are good, but the lat-ter uses a slightly more informal register appropriate to the genre.

Following Matsoukas et al. (2009), we assign each training sentence pair a set of binary features which we call s-features :
Matsoukas et al. (2009) use these s-features to compute weights for each training sentence pair, which are in turn used for computing various model features. They found that the sentence-level weights were most helpful for computing the lexical weight-ing features (p.c.). The mapping from s-features to sentence weights was chosen to optimize ex-pected TER on held-out data. A drawback of this method is that we must now learn the mapping from s-features to sentence-weights and then the model feature weights. Therefore, we tried an alternative that incorporates s-features into the model itself.
For each s-feature s , we compute new word trans-lation tables t s ( e j f ) and t s ( f j e ) estimated from only those sentence pairsf on which s fires, and ex-tend them to phrases/rules as before. The idea is to use these probabilities as new features in the model. However, two challenges arise: first, many word pairs are unseen for a given s , resulting in zero or undefined probabilities; second, this adds many new features for each rule, which requires a lot of space.
To address the problem of unseen word pairs, we use Witten-Bell smoothing (Witten and Bell, 1991): where c ( f , s ) is the number of times f has been ob-served in sentences with s-feature s , and d ( f , s ) is the number of e types observed aligned to f in sentences with s-feature s .

For each s-feature s , we add two model features In order to address the space problem, we use the following heuristic: for any given rule, if the absolute value of one of these features is less than log 2 , we discard it for that rule. Setup We tested these features on two ma-chine translation systems: a hierarchical phrase-based (string-to-string) system (Chiang, 2005) and a syntax-based (string-to-tree) system (Galley et al., 2004; Galley et al., 2006). For Arabic-English trans-lation, both systems were trained on 190+220 mil-lion words of parallel data; for Chinese-English, the string-to-string system was trained on 240+260 mil-lion words of parallel data, and the string-to-tree sys-tem, 58+65 million words. Both used two language models, one trained on the combined English sides of the Arabic-English and Chinese-English data, and one trained on 4 billion words of English data.
The baseline string-to-string system already incor-porates some simple provenance features: for each s-feature s , there is a feature P ( s j rule ) . Both base-line also include a variety of other features (Chiang et al., 2008; Chiang et al., 2009; Chiang, 2010).
Both systems were trained using MIRA (Cram-mer et al., 2006; Watanabe et al., 2007; Chiang et al., 2008) on a held-out set, then tested on two more sets (Dev and Test) disjoint from the data used for rule extraction and for MIRA training. These datasets have roughly 1000 X 3000 sentences (30,000 X 70,000 words) and are drawn from test sets from the NIST MT evaluation and development sets from the GALE program.
 Individual tests We first tested morphological smoothing using the string-to-string system on Chinese-English translation. The morphologically smoothed system generated the improved translation (8) above, and generally gave a small improvement:
We then tested the provenance-conditioned fea-tures on both Arabic-English and Chinese-English, again using the string-to-string system: The translations (12) and (13) come from the Arabic-English baseline and provenance systems. For Arabic-English, we also compared against lex-ical weighting features that use sentence weights kindly provided to us by Matsoukas et al. Our fea-tures performed better, although it should be noted that those sentence weights had been optimized for a different translation model.
 Combined tests Finally, we tested the features across a wider range of tasks. For Chinese-English translation, we combined the morphologically-smoothed and provenance-conditioned lexical weighting features; for Arabic-English, we con-tinued to use only the provenance-conditioned features. We tested using both systems, and on both newswire and web genres. The results are shown in Table 4. The features produce statistically significant improvements across all 16 conditions.
Figure 1 shows the feature weights obtained for the provenance-conditioned features t s ( f j e ) in the string-to-string Chinese-English system, trained on newswire and web data. On the diagonal are cor-pora that were equally useful in either genre. Surpris-ingly, the UN data received strong positive weights, indicating usefulness in both genres. Two lists of named entities received large weights: the LDC list (LDC2005T34) in the positive direction and the NewsExplorer list in the negative direction, sug-gesting that there are noisy entries in the latter. The corpus LDC2007E08, which contains parallel data mined from comparable corpora (Munteanu and Marcu, 2005), received strong negative weights.
Off the diagonal are corpora favored in only one genre or the other: above, we see that the wl (we-blog) and ng (newsgroup) genres are more help-ful for web translation, as expected (although web oddly seems less helpful), as well as LDC2006G05 (LDC/FBIS/NVTC Parallel Text V2.0). Below are corpora more helpful for newswire translation, like LDC2005T06 (Chinese News Translation Text Part 1). Many different approaches to morphology and provenance in machine translation are possible. We have chosen to implement our approach as exten-sions to lexical weighting (Koehn et al., 2003), which is nearly ubiquitous, because it is defined at the level of word alignments. For this reason, the features we have introduced should be easily ap-plicable to a wide range of phrase-based, hierarchi-cal phrase-based, and syntax-based systems. While the improvements obtained using them are not enor-mous, we have demonstrated that they help signif-icantly across many different conditions, and over very strong baselines. We therefore fully expect that these new features would yield similar improve-ments in other systems as well.
 We would like to thank Spyros Matsoukas and col-leagues at BBN for providing their sentence-level weights and important insights into their corpus-weighting work. This work was supported in part by DARPA contract HR0011-06-C-0022 under subcon-tract to BBN Technologies.
