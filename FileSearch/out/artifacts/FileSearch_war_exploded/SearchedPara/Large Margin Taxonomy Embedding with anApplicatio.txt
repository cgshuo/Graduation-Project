 Multi-class classification is a problem that arises in many applications of machine learning. In many cases the cost of misclassification varies strongly between classes. For example, in the context of object recognition it may be significantly worse to misclassify a male pedestrian as a traffic light than as a female pedestrian . Similarly, in the context of document categorization it seems more severe to misclassify a medical journal on heart attack as a publication on athlete X  X  foot than on Coronary artery disease . Although the scope of the proposed method is by no means limited to text data and topic hierarchies, for improved clarity we will restrict ourselves to terminology from document categorization throughout this paper.
 The most common approach to document categorization is to reduce the problem to a  X  X lat X  classi-fication problem [13]. However, it is often the case that the topics are not just discrete classes, but are nodes in a complex taxonomy with rich inter-topic relationships. For example, web pages can be categorized into the Yahoo! web taxonomy or medical journals can be categorized into the Medical Subject Headings (MeSH) taxonomy. Moving beyond flat classification to settings that utilize these hierarchical representations of topics has significantly pushed the state-of-the art [4, 15]. Additional information about inter-topic relationships can for example be leveraged through cost-sensitive de-cision boundaries or knowledge sharing between documents from closely related classes.
 In reality, however, the topic taxonomy is a crude approximation of topic relations, created by an editor with knowledge of the true underlying semantic space of topics. In this paper we propose a method that moves beyond hierarchical presentations and aims to re-discover the continuous latent semantic space underlying the topic taxonomy. Instead of regarding document categorization as classification, we will think of it as a regression problem where new documents are mapped into this latent semantic topic space. Very different from approaches like LSI or LDA [1, 7], our algorithm is entirely supervised and explicitly embeds the topic taxonomy and the documents into a single latent semantic space with  X  X emantically meaningful X  Euclidean distances. Figure 1: A schematic layout of our taxem method (for Taxonomy Embedding). The classes are embedded as prototypes inside the semantic space. The input documents are mapped into the same space, placed closest to their topic prototypes.
 In this paper we derive a method to embed the taxonomy of topics into a latent semantic space in form of topic prototypes. A new document can be classified by first mapping it into this space and then assigning the label of the closest prototype. A key contribution of our paper is the derivation of a convex problem that learns the regressor for the documents and the placement of the prototypes in a single optimization. In particular, it places the topic prototypes such that for each document the prototype of the correct topic is much closer than any other prototype by a large margin. We show that this optimization is a special instance of semi-definite programs [2], that can be solved efficiently [16] for large data sets.
 Our paper is structured as follows: In section 2 we introduce necessary notation and a first version of the algorithm based on a two-step approach of first embedding the hierarchical taxonomy into a semantic space and then regressing the input documents close to their respective topic prototypes. In section 3 we extend our model to a single optimization that learns both steps in one convex op-timization with large margin constraints. We evaluate our method in section 4 and demonstrate state-of-the-art results on eight different document categorization tasks from the OHSUMED med-ical journal data set. Finally, we relate our method to previous work in section 5 and conclude in section 6. We assume that our input consists of documents, represented as a set of high dimensional sparse vectors ~x 1 ,...,~x n  X  X of dimensionality d . Typically, these could be binary bag of words indicators or tfidf scores. In addition, the documents are accompanied by single topic labels y ,...,y n  X  X  1 ,...,c } that lie in some taxonomy T with c total topics. This taxonomy T gives rise to some cost matrix C  X  R c  X  c , where C  X  X   X  0 defines the cost of misclassifying an element of topic  X  as  X  and C  X  X  = 0 . Technically, we only require knowledge of the cost matrix C , which could also be obtained from side-information independent of a topic taxonomy. In this paper we will not focus on how C is obtained. However, we would like to point out that a common way to infer a cost matrix from a taxonomy is to set C  X  X  to the length of the shortest path between node  X  and  X  , but other approaches have also been studied [3].
 Throughout this paper we denote document indices as i,j  X  { 1 ,...,n } and topic indices as  X , X   X  X  1 ,...,c } . Matrices are written in bold (e.g. C ) and vectors have top arrows (e.g. ~x i ). Figure 1 illustrates our setup schematically. We would like to create a low dimensional semantic feature space F in which we represent each topic  X  as a topic prototype ~p  X   X  X  and each document ~x i  X  X as a low dimensional vector ~z i  X  F . Our goal is to discover a representation of the data where distances reflect true underlying dissimilarities and proximity to prototypes indicates topic membership. In other words, documents on the same or related topics should be close to the respective topic prototypes, documents on highly different topics should be well separated. Throughout this paper we will assume that F = R c , however our method can easily be adapted to even lower dimensional settings F = R r where r &lt; c . As an essential part of our method is to embed the classes that are typically found in a taxonomy, we refer to our algorithm as taxem (short for  X  X axonomy embedding X ).
 Embedding topic prototypes The first step of our algorithm is to embed the document taxonomy into a Euclidean vector space. More formally, we derive topic prototypes ~p 1 ,...,~p c  X  F based on the cost matrix C , where ~p  X  is the prototype that represents topic  X  . To simplify notation, we define the matrix P = [ ~p 1 ,...,~p c ]  X  X  c  X  c whose columns consist of the topic prototypes.
 There are many ways to derive the prototypes from the cost matrix C . By far the simplest method is to ignore the cost matrix C entirely and let P I = I , where I  X  X  c  X  c denotes the identity matrix. This results in a c dimensional feature space, where the class-prototypes are all in distance from each other at the corner of a (c-1)-dimensional simplex. We will refer to P I as the simplex prototypes.
 Better results can be expected when the prototypes of similar topics are closer than those of dissim-ilar topics. We use the cost matrix C as an estimate of dissimilarity and aim to place the prototypes such that the distance k ~p  X   X  ~p  X  k 2 2 reflects the cost specified in C  X  X  . More formally, we set If the cost matrix C defines squared Euclidean distances (e.g. when the cost is obtained through the shortest path between nodes and then squared), we can solve eq. (1) with metric multi-dimensional scaling [5]. Let us denote  X  C =  X  1 2 HCH , where the centering matrix H is defined as H = I  X  11 &gt; , and let its eigenvector decomposition be  X  C = V  X  V &gt; . We obtain the solution by setting P Both prototypes embeddings P I and P mds are still independent of the input data { ~x i } . Before we can derive a more sophisticated method to place the prototypes with large margin constraints on the document vectors, we will briefly describe the mapping W : X  X  F of the input documents into the low dimensional feature space F .
 Document regression Assume for now that we have found a suitable embedding P of the class-prototypes. We need to find an appropriate mapping W : X  X  X  , that maps each input ~x i with label y i as close as possible to its topic prototype ~p y i . We can find such a linear transformation ~z i = W ~x i by setting Here,  X  is the weight of the regularization of W , which is necessary to prevent potential overfitting due to the high number of parameters in W . The minimization in eq. (2) is an instance of linear ridge regression and has the closed form solution eq. (3) can be solved very accurately without the need to ever compute the d  X  d matrix inverse ( XX &gt; +  X  I )  X  1 explicitly, by solving with linear conjugate gradient for each row of W indepen-dently.
 Inference Given an input vector ~x t we first map it into F and estimate its label as the topic with the closest prototype ~p  X  Figure 2: The schematic layout of the large-margin embedding of the taxonomy and the documents. matrix P whose columns are the prototypes ~p  X  = P ~e  X  and which defines the final transformation is closer to ~z i than any other prototype ~p  X  by a large margin.
 For a given set of labeled documents ( ~x 1 ,y 1 ) ,..., ( ~x n ,y n ) we measure the quality of our semantic space with the averaged cost-sensitive misclassification loss, So far we have introduced a two step approach: First, we find the prototypes P based on the cost matrix C , then we learn the mapping ~x  X  W ~x that maps each input closest to the prototype of its class. However, learning the prototypes independent of the data { ~x i } is far from optimal in order to reduce the loss in (5). In this section we will create a joint optimization problem that places the prototypes P and learns the mapping W while minimizing an upper bound on (5).
 Combined learning In our attempt to learn both mappings jointly, we are faced with a  X  X hicken and egg X  problem. We want to map the input documents closest to their prototypes and at the same time place the prototypes where the documents of the respective topic are mapped to. Therefore our first task is to de-tangle this mutual dependency of W and P . Let us define A as the following matrix product: It follows immediately form eqs. (3) and (6) that W = PA . Note that eq. (6) is entirely independent of P and can be pre-computed before the prototypes have been positioned. With this relation we have reduced the problem of determining W and P to the single problem of determining P . Figure 2 illustrates the new schematic layout of the algorithm.
 position. We can then rewrite both, the topic prototypes ~p  X  and the low dimensional documents ~z i , as vectors within the range of P : R c  X  X  c : Optimization Ideally we would like to learn P to minimize (5) directly. However, this function is non-continuous and non-differentiable. For this reason we will derive a surrogate loss function that strictly bounds (5) from above. The loss for a specific document ~x i is zero if its corresponding vector ~z i is closer to the correct prototype ~p y i than to any other prototype ~p  X  . For better generalization it would be preferable if prototype ~p y i was in fact much closer by a large margin. We can go even further and demand that prototypes that would incur a larger misclassification loss should be further separated than those with a small cost. More explicitly, we will try to enforce a margin of C y i  X  . We can express this condition as a set of  X  X oft X  inequality constraints, in terms of squared-distances, where the slack-variable  X  i X   X  0 absorbs the amount of violation of prototype ~p  X  into the margin of ~x . Given this formulation, we create an upper bound on the loss function (5): Theorem 1 Given a prototype matrix P , the training error ( 5) is bounded above by 1 n P i X   X  i X  . Proof: First, note that we can rewrite the assignment of the closest prototype (4) as  X  y i = equality when  X  y i = y i ). We therefore obtain: The result follows immediately from (9) and that  X  i X   X  0 : Theorem 1, together with the constraints in eq. (8), allows us to create an optimization problem that minimizes an upper bound on the average loss in eq. (5) with maximum-margin constraints: Note that if we have a very large number of classes, it might be beneficial to choose P  X  X  r  X  c with r &lt; c . However, the convex formulation described in the next paragraph requires P to be square. Convex formulation The optimization in eq. (11) is not convex. The constraints of type (8) are quadratic with respect to P . Intuitively, any solution P gives rise to infinitely many solutions as any rotation of P results in the same objective value and also satisfies all constraints. We can make (11) invariant to rotation by defining Q = P &gt; P , and rewriting all distances in terms of Q , Note that the distance formulation in eq. (12) is linear with respect to Q . As long as the matrix Q is positive semi-definite, we can re-decompose it into Q = P &gt; P . Hence, we enforce positive semi-definiteness of Q by adding the constraint Q 0 . We can now solve (11) in terms of Q instead of P with the large-margin constraints Regularization If the size of the training data n is small compared to the number of parameters c 2 , we might run into problems of overfitting to the training data set. To counter those effects, we add a regularization term to the objective function.
 Even if the training data might differ from the test data, we know that the taxonomy does not change. satisfies all constraints (8) as equalities with zero slack. This gives us confidence that the optimal solution P for the test data should not deviate too much from P mds . We will therefore penalize Table 1: Statistics of the different OHSUMED problems. Note that not all nodes are populated and that we pruned all strictly un-populated subtrees. k Q  X   X  C k 2 F , where  X  C = P &gt; mds P mds (as defined in section 2). The final convex optimization of taxem with regularized objective becomes: The constant  X   X  [0 , 1] regulates the impact of the regularization term. The optimization in (14) is an instance of a semidefinite program (SDP) [2]. Although SDPs can often be expensive to solve, the optimization (14) falls into a special category 2 and can be solved very efficiently with special purpose sub-gradient solvers even with millions of constraints [16]. Once the optimal solution Q  X  is found, one can obtain the position of the prototypes with a simple svd or cholesky decomposition Q  X  = P &gt; P and consequently also obtains the mapping W from W = PA . We evaluated our algorithm taxem on several classification problems derived from categorizing pub-lications in the public OHSUMED medical journal data base into the Medical Subject Headings (MeSH) taxonomy.
 Setup and data set description We used the OHSUMED 87 corpus [9], which consists of abstracts and titles of medical publica-tions. Each of these entries has been assigned one or more categories in the MeSH taxonomy 3 . We used the 2001 version of these MeSH headings resulting in about 20k categories organized in a tax-onomy. To preprocess the data we proceeded as follows: First, we discarded all database entries with empty abstracts, which left us with 36890 documents. We tokenized (after stop word removal and stemming) each abstract, and represented the corresponding bag of words as its d = 60727 dimen-sional tfidf scores (normalized to unit length). We removed all topic categories that did not appear in the MeSH taxonomy (due to out-dated topic names). We further removed all subtrees of nodes that were populated with one or less documents. The top categories in the OHSUMED data base are  X  X rthogonal X   X  for instance the B top level category is about organism while C is about diseases. We thus created 8 independent classification problems out of the top categories A,B,C,D,E,F,G,H. For each problem, we kept only the abstracts that were assigned exactly one category in that tree, making each problem single-label. The statistics of the different problems are summarized in Ta-ble 1. For each problem, we created a 70% / 30% random split in training and test samples, ensuring however that each topic had at least one document in the training corpus.
 Document Categorization The classification results on the OHSUMED data set are summarized in Table 2. We set the regular-ization constants to be  X  = 1 for the regression and  X  = 0 . 1 for the SDP. Preliminary experiments on data set B showed that regularization was important but the exact settings of the  X  and  X  had no crucial impact. We derived the cost-matrix C from the tree hop-distance in all experiments. We Table 2: The cost-sensitive test error results on various ohsumed classification data sets. The algo-rithms are from left to right: one vs. all SVM, MCSVM [6], cost-sensitive MCSVM, Hierarchical SVM [4], simplex regression, mds regression, large-margin taxem. The best results (up to statistical significance) are highlighted in bold. The taxem algorithm obtains the lowest overall loss and the lowest individual loss on each data set except B. compared taxem against four commonly used algorithms for document categorization: 1. A lin-ear support vector machine (SVM) trained in one vs. all mode (SVM 1/all) [12], 2. the Crammer and Singer multi-class SVM formulation (MCSVM) [6], 3. the Cai and Hoffmann SVM classifier with cost-sensitive loss function (SVM cost) [4], 4. the Cai and Hoffmann SVM formulation with a cost sensitive hierarchical loss function (SVM tax) [4]. All SVM classifiers were trained with regularization constant C = 1 (which worked best on problem B; this value is also commonly used in text classification when the documents have unit length). Further, we also evaluated the differ-ence between our large margin formulation (taxem) and the results with the simplex ( P I -taxem) and mds ( P mds -taxem) prototypes. To check the significance of our results we applied a standard t-test with a 5% confidence interval. The best results up to statistical significance are highlighted in bold font. The final entry in Table 2 shows the average error over all test points in all data sets. Up to statistical significance, taxem obtains the lowest loss on all data sets and the lowest overall loss. Ignoring statistical significance, taxem has the lowest loss on all data sets except B. All algorithms had comparable speed during test-time. The computation time required for solving eq. (6) and the optimization (14) was on the order of several minutes with our MATLAB TM implementation on a standard Intel TM 1.8GHz core 2 duo processor (without parallelization efforts). In recent years, several algorithms for document categorization have been proposed. Several authors proposed adaptations of support vector machines that incorporate the topic taxonomy through cost-algorithm is based on a very different intuition. It differs from all these methods in that it learns a low dimensional semantic representation of the documents and classifies by finding the nearest prototype.
 Most related to our work is probably the work by Karypis and Han [10]. Although their algorithm also reduces the dimensionality with a linear projection, their low dimensional space is obtained through supervised clustering on the document data. In contrast, the semantic space obtained with taxem is obtained through a convex optimization with maximum margin constraints. Further, the low dimensional representation of our method is explicitly constructed to give rise to meaningful Euclidean distances.
 The optimization with large-margin constraints was partially inspired by recent work on large margin distance metric learning for nearest neighbor classification [16]. However our formulation is a much more light-weight optimization problem with O ( cn ) constraints instead of O ( n 2 ) as in [16]. The optimization problem in section 3 is also related to recent work on automated speech recognition through discriminative training of Gaussian mixture models [14]. In this paper, we have presented a novel framework for classification with inter-class relationships based on taxonomy embedding and supervised dimensionality reduction. We derived a single convex optimization problem that learns an embedding of the topic taxonomy as well as a linear mapping from the document space to the resulting low dimensional semantic space.
 As future work we are planning to extend our algorithm to the more general setting of document categorization with multiple topic memberships and multi-modal topic distributions. Further, we are keen to explore the implications of our proposed conversion of discrete topic taxonomies into continuous semantic spaces. This framework opens new interesting directions of research that go beyond mere classification. A natural step is to consider the document matching problem (e.g. of web pages and advertisements) in the semantic space: a fast nearest neighbor search can be performed in a joint low dimensional space without having to resort to classification all together. Although this paper is presented in the context of document categorization, it is important to empha-size that our method is by no means limited to text data or class hierarchies. In fact, the proposed algorithm can be applied in almost all multi-class settings with cost-sensitive loss functions (e.g. object recognition in computer vision).

