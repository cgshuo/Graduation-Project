
Due to the exponentially large number of subspaces, it is an open challenge to select the relevant subspaces for each outlier. Existing techniques from the field of subspace out-lier mining perform such a combinatorial search [3, 18, 22, 17]. However, they all rely on a fixed outlier model, which cannot be exchanged depending on the application domain. On the other hand, subspace search techniques [8, 12, 13, 23] are agnostic w.r.t. the outlier model, which only is applied as a post-processing step. They ignore the underlying out-lier definition and focus on global data characteristics such as entropy, density, and comparison of distributions. De-pending on the outlier model however, different objects in different subspaces have the highest deviation. Hence, sub-space search results should be tailored to the outlier char-acteristics of individual outliers. To solve this problem, the approach envisioned must be both flexible (the method al-lows to exchange the outlier model at all) and adaptive (the method performs the search tailored to the outlier model). A subspace search scheme with these properties is applicable to a broad range of application domains. Flexibility also en-sures that the approach directly benefits from any research progress on traditional outlier models. Adaptiveness allows to search for relevant subspaces individually for each outlier and, hence, enables to describe each outlier by its specific outlier properties.
 As the main contribution of this paper we propose Ref-Out , a flexible and adaptive subspace search framework for outlier mining. It finds relevant subspaces by a refine-ment process that adapts to the given outlier model. The key idea is based on the observation that traditional out-lier detection methods (applied to subspaces) do capture at least small deviations of an outlier even though some irrel-evant attributes are included. In the distance-based out-lier model for instance, o 2 is a clear outlier in subspace S 2 = { Voltage Magnitude , Transient Voltage } . In a high di-mensional database it is hard to detect this subspace di-rectly. But when considering random subspaces T with | T |  X  | S 2 | , some of these random spaces will contain S When applying the distance-based model to evaluate o 2 in such a space T  X  S 2 the model will report a relatively high outlier score. In contrast to this, we measure relatively low outlier scores in all other spaces T 6 X  S 2 in which o 2 shows no irregular behavior w.r.t. the distance-based model. Our main idea is to detect these score discrepancies of high out-lier scores in T  X  S 2 over low scores in T 6 X  S 2 for individual objects. We extract information hidden in outlier scores to make a conclusion which subspace induces a high outlier score for the given outlier model. We use this information to refine a pool of random subspaces according to the dis-crepancies in the outlier scores. This means that if we for instance perform RefOut with an angle-based outlier model in our example, it would ignore o 2 and S 2 and instead focus on angle-based outliers and their respective subspaces. With RefOut , we make the following contributions: To the best of our knowledge RefOut is the first subspace search technique that is both flexible and adaptive w.r.t. dif-ferent outlier models. In our experiments we show that this adaptivity leads to an enhanced quality for various outlier models.
A number of different outlier paradigms have been pro-posed in the literature. We review the main directions and highlight the difference to our approach.
 Traditional Outlier Mining: We use the term traditional outlier mining to refer to any outlier scoring technique that operates on a fixed attribute set. There are various full-space outlier models ranging from deviation-based methods [24], distance-based methods [14], local density-based meth-ods [7] right up to angle-based methods [19] or hashing-based scoring [25]. We abstract from these individual models and propose to use an abstract outlier score in our framework. Our research is orthogonal to the development of novel out-lier scores, i.e., RefOut benefits from any future improve-ments of traditional outlier mining w.r.t. quality, efficiency, or novel outlier definitions.
 Mining Descriptions (for given outliers): There are several approaches that identify subspaces as so-called out-lier descriptions [15, 5, 21]. These methods extract a sub-space for a given outlier, assuming that outlier detection has taken place in advance. Obviously this results in a chicken and egg dilemma : (1  X  2) Traditional outlier detectors re-quire a prior subspace selection to detect outliers hidden in subspaces. (2  X  1) Outlier descriptions would provide such a subspace selection, but they require the outliers to be de-tected in advance. With the proposed RefOut approach, we break this cyclic dependency by solving these two prob-lems simultaneously. Our search process applies to both outliers and the corresponding subspaces.
 Subspace Outlier Mining: Subspace outlier mining was first specified by [3], and recent approaches have extended this idea to subspace outlierness scores [18, 9, 22, 17]. They propose an interleaved detection of both outliers and sub-spaces. This means that each of these techniques relies on some specific outlier criterion that is tailored to its subspace processing. They are restricted to this outlier criterion, and thus, are not flexible w.r.t. instantiations with different out-lier models. In contrast to these methods, RefOut not only allows to exchange the outlier model, it also adapts its subspace search to the actual outlier score detected by the model.
 Subspace Search: Approaches from the field of subspace search [8, 12, 13, 23] in turn focus on the selection of sub-spaces. They can be used as a pre-processing step to any traditional outlier mining algorithm. Thus, subspace search allows to exchange the outlier model. While flexibility is ful-filled, adaptivity is not: The outlier model is ignored, and the subspace search does not take the specific characteristics of the given outlier model into account.
Let DB be a database consisting of N objects, each de-scribed by a D -dimensional real-valued data vector ~x = ( x 1 , . . . , x D ). The set A = { 1 , . . . , D } denotes the full data space of all given attributes. Any attribute subset S = { s 1 , . . . , s d }  X  A will be called a d -dimensional subspace projection. For calculations in specific subspaces we con-strain the vectors to the respective attributes, i.e., ~x ( x s 1 , . . . , x s d ). This allows to deploy notions such as dis-tance, density, and outlierness directly at the subspace level.
To define an adaptive outlier detection framework, we for-malize the notion of an outlier model:
Definition 1. An outlier model is a function that maps every object of the database to a real-valued outlier score w.r.t. a given subspace S :
Since our framework evaluates individual objects in dif-ferent subspaces, the only necessary requirement is that the outlier scores are comparable among different subspaces. Most outlier models do not immanently provide this com-parability among subspaces. However, comparability can always be enforced by applying a normalization scheme. We assume that the normalization ensures that the outlierness distribution of the majority of regular objects has (1) a mean of default out and (2) a variance of 1 independent of S . For examples of such normalization schemes for arbitrary outlier models we refer to unification techniques [16]. For the outlier models used in this work we obtain the required properties by applying the following transformation: Var ( score S ) = 1 In the remainder of this work, we apply this transformation to all outlier models utilized. For the sake of presentation, we also assume an increasing sort order of score ( ~x higher values correspond to stronger outlier characteristics. Finding alternative normalization schemes is orthogonal to our work. We focus on the selection of subspaces only and use this existing pre-processing scheme.
In the following we focus on one individual object ~x and formalize the outlier score properties evaluated over different subspaces by keeping one subspace S fixed for comparison.
Definition 2. The outlierness profile of an individ-ual object ~x w.r.t. subspace S is a function over random subspaces T with | T | = d 0 defined as profile ~x,S ( d 0 ) =
Based on this outlier profile, we are able to compare the outlier score of ~x in subspace S with all of its super-and sub-spaces T . Considering various spaces T with different dimensionality d 0 we derive the definition of a true subspace outlier as follows:
Definition 3. An object ~x is a true subspace outlier with respect to subspace S iff We call this maximum value the peak of ~x in subspace S and we further require: We will also refer to true subspace outliers as d -dimen-sional outlier with d = | S | , the dimensionality of the sub-space. Figure 2: Ideal profile of a true subspace outlier
Figure 2 illustrates these definitions. The plot shows an idealized outlierness profile of an individual object (blue line) that fulfills the true subspace outlier conditions. The red area shows the distribution of regular objects (unit vari-ance as a result of normalization). At d 0 = | S | , we can see the clear outlier score peak, deviating by several standard deviations.

When considering random superspaces of S ( T  X  S ), the expectation value of the outlier score decreases monotoni-cally. It is precisely this manifestation of the curse of dimen-sionality [6] that is commonly observed in reality: Adding ir-relevant attributes hampers the outlier detection. Thus, the measured outlier score decreases with increasing dimension-ality since all objects become more and more alike. Com-paring the blue curve of an individual outlier with the red distribution of regular objects shows that at some point the deviation of our true subspace outlier is comparable with the average deviation of regular objects. Thus, it is no longer possible to detect the true subspace outlier.

For lower dimensional subspaces d 0 &lt; | S | the object is projected in random subspaces of S . The defining property profile ~x,S ( d 0 ) peak for these spaces means that the true subspace outlier is projected into regions of regular densities in these subspace projections. This effect is also very com-mon in reality. Think of o 1 from our example in Figure 1. This object clearly has a peak  X  profile ~o 1 ,S 1 (2). Project-ing the two dimensional subspace S 1 = { Voltage Magnitude , Harmonic Content } to its one-dimensional subspaces will project o 1 into regions of high density. In none of these sub-spaces the object shows an exceptional outlier score, thus, profile ~o 1 ,S 1 (1) peak . By assuming that no other at-tribute contributes to the deviation of o 1 , all properties are fulfilled and o 1 is a true subspace outlier in S 1 .
Regarding higher dimensional true subspace outliers (i.e. large | S | ), the condition profile ~x,S ( d 0 ) peak  X  d implies that the object is not exceptional in all lower di-mensional projections. For instance, a true subspace outlier in a 4-dimensional subspace S appears to be regular in all 3-, 2-, and 1-dimensional projections of S . Only the joint consideration of all attributes makes the object exceptional, and no single attribute of S is responsible for the anomalous-ness alone. This property of true subspace outliers makes their detection exceptionally hard. Note that, if an object deviates in for instance two attributes s 1 and s 2 , this ob-ject is not a true subspace outlier in S = { s 1 , s 2 } since it suffices to clearly detect the outlier by considering the at-tributes separately. Thus, we would consider this object to be a true (1-dimensional) subspace outlier in both S 1 = { s and S 2 = { s 2 } .

Please note that our definition of true subspace outliers is not a binary definition. For our detection framework we output the size of the peak as final outlier score for each object. Thus, we provide an outlier ranking with the most prominent true subspaces outliers ranked first.
To corroborate our model of outlierness profiles and of true subspace outliers, Figure 3 shows examples of real out-lierness profiles. For the sake of illustrating outlierness pro-files we introduce profile instantiations: We draw a single line corresponding to one specific sequence of random sub-spaces T over the dimensionality range (each point corre-sponds to the outlier score in a random subset/superset of S ; T = S at the peak). This allows to visualize outlier score dis-tributions and expectation values by plotting a large number of these instantiations. The first figure shows a real world outlier from the Breast dataset, detected as 4-dimensional true subspace outlier in our evaluation. The outlierness pro-file was generated based on the local density outlier model. As a reference we show profiles of regular objects in gray. The second figure shows the outlierness profile of a differ-ent object with a 3-dimensional peak, this time evaluated with a distance-based model. Overall, the observed outlier-ness profiles are in good agreement with our model. In fact, such observations of true subspace outliers on real world data were the primary motivation for the development of RefOut . We also generate our synthetic data according to these observations (cf. Sec. 5) and include hidden outliers of different subspace dimensionalities. The third figure shows examples from our synthetic data; this time evaluated with an angle-based model. Note that the three examples are generated based on outlier models with vast differences in their raw outlier score distributions, but the general shape of outlier profiles is preserved after normalization.
All these examples illustrate the need for subspace selec-tion: Outliers can be clearly detected in the peaking sub-space. In addition, this subspace is a valuable description of the individual outlier characteristics. Our RefOut approach consists of two building blocks. The first one is the definition of a general framework for an adaptive subspace analysis based on traditional outlier scores. The underlying idea is based on the transformation of the subspace search problem into a score discrepancy anal-ysis problem (Sec. 4.1 and Sec. 4.2). The second building block of RefOut deals with the question of how to solve this novel score discrepancy problem. We will propose our solution in Sec. 4.3.
Identifying outlier in subspaces is computationally expen-sive. In principle, an exhaustive search for true subspace outliers requires scanning through all possible subspaces 2 for each object in the database. Due to the exponential number of subspaces, this would only be feasible for very low dimensional databases. To achieve a scalable subspace out-lier detection it is necessary to drastically reduce the search space. To this end, we follow the idea of random subspace sampling [20] as a basis for our adaptive subspace search.
In order to take a new perspective on the subspace search problem, we look at the effects of applying a given outlier model in subspaces selected randomly. In the following, we focus on a single object ~x that is a true subspace outlier in subspace S under the outlier model. To simplify the analy-sis, we further assume that the object ~x is not a true sub-space outlier in any other subspace. We denote the set of irrelevant attributes as I = A\ S .

Let T be a random variable of subspaces, i.e., T is drawn uniformly from 2 A . We refer to the sample over these ran-dom subspaces as subspace pool P = { T | T drawn iid from 2 A } . By applying the given outlier model to the ran-dom subspaces T , we obtain a sample of outlier scores: The subspace S of ~x plays an important role in the random sampling process: It partitions both the subspace pool P and the outlier scores O depending on whether the random subspace T is a superset of S or not. We denote the split of the subspace pool P as
P
S = { T | T  X  S  X  T  X  X } P and the partition of the outlier scores O as: We now examine the two outlier score populations O + S and O
S by considering our observations w.r.t. the outlierness profiles. We know that for the spaces in P + S , the outlier score is described by the outlierness profile (cf. Fig. 2), since they are supersets of the true subspace S . This means that for score o  X  X  + S we have E [ o ] &gt; default out , i.e., the expectation value of the score is increased over default out . Note that this observation only applies for the expectation value of the score; in reality one can obtain an o &lt; default chance.

For the spaces T  X  X   X  S the true subspace S is never com-pletely covered. We have to consider two cases when ana-lyzing the population O  X  S . The first case is that T partially covers S , i.e., T includes some but not all attributes of S . This means that we obtain a subspace which projects the true subspace outlier into a region of regular density. Re-garding the outlierness profile, this corresponds to the left side of the peak. Thus, in this case we have E [ o ]  X  default for o  X  X   X  S . The second case is that the random subspace T and true subspace S are completely disjunct. Thus T  X  I , i.e., T exclusively consists of attributes that are irrelevant for this true subspace outlier. In these attributes ~x is com-pletely regular, thus, E [ o ]  X  default out .

Combining these observations implies that we observe a discrepancy between the expectation values of the outlier score populations O + S and O  X  S , namely: The main idea behind our framework is to exploit this dis-crepancy.
 Effects of random sampling: Before we reformulate the problem statement, we analyze how the random sampling of T influences this discrepancy. The general goal is to keep the total number of analyzed subspaces |P| low to ensure a feasible processing, i.e., |P| 2 A . This means that in prac-tice we have to deal with the limited size of the populations O
S and O when comparing O + S and O  X  S as in Eq. 4. This statistical uncertainty is influenced by the dimensionality | T | of the subspaces T  X  P . We have to consider the effects of both high and low dimensional T : Low | T | : Considering the dimensionality dependence of the outlierness profile (cf. Fig. 2), it is obvious that the observed outlierness difference becomes statistically more significant when the subspace T is more similar to S , i.e., when the superset T contains only a small number of additional irrel-evant attributes. In Fig. 2, this corresponds to subspaces with a dimensionality close to the outlierness peak. This means that we can maximize the discrepancy in Eq. 4 by reducing the dimensionality of the subspaces in P to a di-mensionality that is only slightly larger than | S | . High | T | : On the other hand, we have to consider the under-lying combinatorial problem: What is the probability that a random subspace T is a superset of S ? Since the subspaces are drawn independently, we can use the hypergeometric dis-tribution to quantify the probability that a space T  X  P is a superset of subspace S . For a database consisting of D attributes, we obtain the coverage probability : Intuitively, the coverage probability increases if either | T | is large (large covering subspace) or | S | is small (small sub-space to cover). For instance, in a database with D = 100 attributes and | T | = 25 the coverage probability is 6.06% for a two-dimensional subspace and 0.07% for a five-dimensional one. Increasing the size of the sampled sub-spaces to | T | = 75 increases these probabilities to 56.1% and 22.9% respectively. As we can see, if the subspaces in P are low-dimensional, it becomes more and more likely that P does not contain any superspaces of S . For a lim-ited subspace pool sample P , the superset samples P + S and O
S become very small or even empty. This means that the comparison O + S and O  X  S is affected by a high statistical un-certainty. Thus, we require high dimensional subspaces T to ensure that the superset populations P + S and O + S are large enough to allow a statistical inference with a high signifi-cance level.
 Problem Statement: To finally transform the problem of searching for relevant subspaces into a new formulation of the problem statement, we reverse the interpretation of Eq. 4 in the following. So far, we have assumed a given true subspace S and analyzed its influence on P and O . We now turn to the question of searching for an S 0 given a subspace pool P and outlier scores O . We have found that for a true subspace outlier the corresponding true subspace S causes a partition of subspaces and outlier scores. For this partition we observe the discrepancy of E O + S and E O  X  S . The re-versal yields our problem statement: Given a subspace pool P and outlier scores O , which refinement S 0 causes a par-titioning that maximizes the discrepancy of the outlier score populations O + S 0 and O  X  S 0 ? For the given object, this S the best possible approximation of the underlying true sub-space S given the limited sample size of P and O . For the construction of our adaptive framework, we consider this to be a stand-alone problem and only require a subspace re-finement function of the form: This function takes a subspace pool P and outlierness scores O of the considered object as input. The third parameter d determines the dimensionality of the output candidate, i.e., | S | = d 0 . The output S 0 is the refined subspace candidate. Formally, this refined candidate is the subspace maximizing the discrepancy, i.e.: Intuitively, this S 0 is the best possible d 0 -dimensional sub-space that lets the given object appear anomalous. In other words, we can use Refine to get the best lower dimensional attribute explanation why the considered object is an outlier for the given outlier model. The Refine function is the key component of our adaptive framework and is used to refine the subspaces adaptively to the outlier score of an individ-ual object. We postpone the discussion of an instantiation of the Refine function to Section 4.3 and continue with the overview of our framework in the following.
At a glance, the RefOut framework consists of three steps: (1) perform outlier mining on the subspaces of an initial subspace pool P 1 consisting of random subspaces; (2) refine P 1 resulting in a refined subspace pool P 2 that contains subspaces tailored to the given outlier model; (3) perform outlier mining on P 2 to obtain the final output. The first step of the framework can be considered a modified version of the random feature bagging approach proposed in [20]. However, our approach goes beyond this random guessing by performing an adaptive refinement in the second step. Step 1: The objective of the first step is to collect as much information about objects and subspaces as possible. We randomly draw subspaces of dimensionality d 1 without re-placement and add them to P 1 until |P 1 | reaches a threshold psize . Note that this allows RefOut to perform an exhaus-tive search on dimensionality level d 1 for very low dimen-sional databases or large psize , but in general D d The dimensionality parameter d 1 controls the trade-off be-tween a good subspace coverage probability (large d a less severe curse of dimensionality (low d 1 ). The frame-work then applies the given traditional outlier model to all subspaces T  X  P 1 . To ensure the desired property of com-parable outlier scores amongst different subspaces, we apply the normalization (Eqs. 1-3) to the outlierness distribution in every subspace. The framework stores these normalized outlier scores for every object in every subspace.
 Step 2: The goal of the second step is to exploit the infor-mation collected in Step 1 by refining the subspaces adap-tively to the outlier scores resulting in the refined subspace pool P 2 . Note that the subspace refinement operates per ob-ject, i.e., every object has an individually refined subspace. In principle it would be possible to produce a refined sub-space for every object in the database, resulting in |P 2 However, if an object does not show anomalous behavior in any of the subspace projections of P 1 , it is very likely that this object simply is regular. Thus, to speed up the pro-cessing, the framework excludes these inliers for subspace refinement. Instead of processing all objects, the framework ranks all objects according to their maximum outlier score over all subspaces in P 1 . A parameter opct controls the number of objects (expressed as ratio of the database size) that are considered for subspace refinement, i.e., we con-sider the top b opct  X  N c objects from this ranking. Since each subspace refinement adds one subspace to the refined pool, this also determines the size |P 2 | . The target dimen-sionality of the subspace refinement is given by parameter d , i.e., | T | = d 2  X  T  X  X  2 .
 Step 3: The third step applies the outlier model again  X  this time to the refined pool P 2 . As in Step 1, we normalize the outlier scores of each subspace to ensure comparability. The final outlier score of an object is the maximal normalized outlier score observed over all subspaces in |P 2 | . Algorithm 1 summarizes the steps of the RefOut framework.
 Algorithm 1 Adaptive Subspace Search
To analyze the complexity of this algorithm, we look at the search space processed. A naive algorithm would check all 2
A subspaces, which clearly does not scale. In contrast, we only look at a limited set of subspaces. The search space is limited by the parameters psize and opct . Furthermore, the subspace candidate refinement requires only a small number of subspaces considered in the pool. The total number of subspaces processed is ( psize + b opct  X  N c ). Thus, the com-plexity of the framework itself is O ( N ). In terms of the un-derlying outlier model to check these subspaces, we depend on the complexity of the detection algorithm, which range from O ( D  X  N ) for efficient distance-based [11], O ( D  X  N for density-based methods [7], up to O ( D  X  N 3 ) for the basic version of angle-based methods [19].
The goal of the refinement function Refine is to obtain the d -dimensional subspace S 0 that maximizes the discrepancy of the populations O + S 0 and O  X  S 0 . The input of Refine is the set of subspaces P and the corresponding outlier scores O of an individual object ~x . To simplify the notation we treat both input sets P and O as sequences with an arbitrary but fixed order. Since there is an outlierness value for every subspace T  X  P , we define the order P  X  ( T 1 , T 2 , . . . , T and O  X  ( o 1 , o 2 , . . . , o M ) such that o i = score ( ~x use the notation ( T i , o i ) to refer to a pair of subspace and corresponding outlier score.

To illustrate the problem to solve, we introduce a running example in Figure 4. The table shows the measured out-lier scores of an outlier with true subspace S = { 1 , 2 , 3 , 4 } evaluated in random subspaces of dimensionality 9 within a database of dimensionality 12. A green box indicates that an attribute is included in the random subspace. To ease presentation, we have ordered the ( T i , o i ) tuples according to the outlier score of the object in the respective subspaces. If we partition the rows according to T  X  S vs T 6 X  S , we obtain the rows with the ranks 1, 2, 3, 4, and 7 as popula-tion P + S . Considering the corresponding outlier score popu-lations O + S and O  X  S clearly shows that O + S is stochastically greater than O  X  S . Ideally, for any d 0  X  4 the goal of the Refine function is to detect this discrepancy and return a refined subspace S 0  X  S .

In the following we point to the three major challenges of the refinement problem and explain how we deal with them in our proposed solution.

Uncertainty of populations: This challenge refers to the general problem of comparing populations. For instance, the example demonstrates that the two populations are not strictly separable in general due to statistical fluctuations: We observe that the subspace on rank 7, which is a superset of the true subspace, is ranked below two irrelevant sub-spaces that coincidentally show a high outlierness for the object. Hence, any solution of the refinement problem must handle uncertainty in outlier score distributions. Another issue is that for a high dimensional S 0 , the partition may yield a very small sample P + S 0 due to the low coverage prob-ability of high dimensional subspaces. In this case the size of the outlier score populations becomes unbalanced, i.e., O is much smaller than O  X  S 0 . For instance, if we consider an S that corresponds exactly to the top ranked subspace in the example, the statistical significance of comparing O + S O
S 0 is low since |O statistical tests that are designed for comparing populations and properly handle uncertainty. To quantify the separa-tion power for a given candidate C , our approach requires an instantiation of the following function: discrepancy O + C , O  X  C  X  p-value of a statistical test By using the p-value we leave the question of the statis-tical significance to the underlying test: In case of a very small population O + C , any reasonable test will report lower p-values, since it is not possible to reject the null-hypothesis of identical populations with a high certainty. There are many possibilities to instantiate the statistical test. For instance, we can use the one-sided versions of the Mann-Whitney-Wilcoxon test or the Student X  X  t-test. We eval-uated several instantiations in our experiments. Although we observed only minor differences, we obtained the overall best results with Welch X  X  t-test (a Student X  X  t-test without assuming equal variances of the samples). The reason could be that a t-test is more sensitive to outliers compared to the Mann-Whitney-Wilcoxon test, which only considers the ranks of the populations. While the t-test X  X  sensitivity to outliers is an issue in other domains, it actually is useful in our case: For a high dimensional true subspace S the coverage probability is low. Thus, we might only have a few matching subspaces in the subspace pool. Fortunately, the t-test captures this discrepancy well compared to a rank test. According to our experiments, this property seems to outweigh the fact that the Gaussian assumption of a t-test does not necessarily apply to the outlier score distributions.
Joint occurrence property: We know from the outlier-ness profiles that only the joint occurrence of the attributes S causes an increased outlier score of a true subspace outlier. In projections of S , the object falls in regions of regular den-sity. In the given example, we observe that the individual occurrences of attributes { 1 , 2 , 3 , 4 } below Rank 7 are com-pletely random and independent from each other since the complete set is never included in these subspaces. Detecting joint occurrences highlights the set-like property of the prob-lem and its exponential characteristic: An exhaustive search to find the exact d 0 -dimensional subspace S 0 that maximizes the discrepancy of O + S 0 and O  X  S 0 would require to evaluate the discrepancy of all possible D d 0 partitions. Thus, it is not feasible to search for an exact solution. Instead we propose a heuristic search for a subspace S 0 that approximately maxi-mizes the discrepancy. We define the quality of a candidate subspace C according to the discrepancy of the correspond-ing partition: Based on this quality function we perform a beam search of the candidates in a bottom-up processing. A parameter beamSize determines the number of candidates that we keep on each dimensionality level. We start with all possible one-dimensional candidates. In each iteration we calculate the quality quality ( C ) of all candidates C . We rank the candi-dates depending on their quality and discard all candidates that have low quality, i.e., we only keep the top-beamSize ones. These top candidates are used to construct higher dimensional candidates. This construction is similar to con-structing higher dimensional candidates in frequent itemset mining [4]: We form a ( d +1)-dimensional candidate in case our candidate set contains all its d -dimensional projections. If it is not possible to construct a higher dimensional candi-date, the processing stops.

To highlight the rationale of such a processing we discuss the question whether there is some kind of monotonicity in the candidate generation. In frequent itemset mining, monotonicity refers to the fact that when the quality crite-rion of a candidate C (in this case the itemset support) is above a certain threshold, so it is for all subsets of S . In our score discrepancy problem, we are faced with a quality cri-terion which is more complex than a simple count of items, and monotonicity does not hold. However, we observe that our problem has a property which we would call per-level-monotonicity . On a fixed dimensionality level d , we have where C true are d -dimensional subsets of S and C rand are random d -dimensional candidates which do not share at-tributes with S . We can see this by noting that O + C O
S . Thus, the population O scores of the true population O + S plus a random sample of O S . When taking expectation values, we still have: For random candidates C rand the expectation values of the samples O + C holds. This per-level-monotonicity ensures that by keeping the top candidates on each level in the beam search, we maximize the likelihood of finding the correct S in each step.
To finally obtain the refined d 0 -dimensional output sub-space, we proceed as follows: During the bottom-up beam search we keep a record of all candidate qualities ever evalu-ated. We rank all candidates according to their quality ( C ), i.e., their p-values expressing how well they separate the outlier score populations. To collect exactly d 0 attributes for the output candidate, we iterate over this list, starting with the top ranked candidates. We add the attributes of the candidates in the ranking to the output candidate S until | S 0 | = d 0 . In case of adding a candidate C completely would yield | S 0 | &gt; d 0 , we rank the attributes a  X  C accord-ing to their one-dimensional qualities quality ( { a } ) and only add the best attributes until | S 0 | = d 0 .

Limited size of subspace pool: Another challenge is in-troduced by the limited size of the subspace pool. If this number is low, combinatorial interferences are likely to oc-cur. For instance, the last attribute in Figure 4 is not part of the relevant subspace. But since it was never excluded from the top ranked subspaces, there is no way to detect that it is an irrelevant attribute for the given object. Due to the limited number of combinations, the attribute must be added to the set of relevant attributes as a false positive. In order to completely avoid false positives, it would be nec-essary to evaluate all D d possible d -dimensional subspaces on each level. Clearly this is not feasible. However, we can reduce the issue of false positives by relaxing the general goal of the subspace refinement. After all, any reduction of irrelevant attributes already improves outlier detection. Thus, detecting the true S precisely is unlikely unless we construct a huge subspace pool. Instead, the framework in-creases outlier detection quality by refining the subspace to a dimensionality level d 2 . This allows the refinement step to output an S 0  X  S which may include some false positive at-tributes. From the framework X  X  point of view, the main goal is achieved: It has been possible to remove ( d 1  X  d 2 ) irrele-vant attributes, adaptively on the underlying outlier model, allowing enhanced outlier detecting by scoring an object in its individually best subspace S 0 .

We conclude this section with a brief summary of our solu-tion: The proposed Refine function extracts a refined sub-space individually for each object based on the outlier scores according to the underlying outlier model. These proper-ties, per-object processing and adaptiveness, distinguish our approach from existing subspace search techniques [8, 12, 13, 23]. The refined subspace is obtained by maximizing the discrepancy in outlier score distributions. Our algo-rithm performs a beam search that exploits the per-level-monotonicity. Exploiting this special property of our prob-lem distinguishes our approach from approaches e.g. in sub-group detection [26], where such a property does not hold. Furthermore, we have proposed a construction of the out-put subspace which allows S 0  X  S , and thus, is tailored to the idea of refining subspaces within the enclosing RefOut framework.
Our experiments focus on the interplay of traditional out-lier models with subspace search approaches. From the field of outlier models we chose three representative techniques: (1) Local Outlier Factor (LOF) [7], (2) distance-based out-lier detection (DB) [14], and (3) angle-based outlier mining (ABOD) [19]. Our general evaluation scheme is to com-bine these three models with the following subspace selec-tion schemes: (1) random subspace selection (RS) and (2) the full attribute space (FS) as two baselines; (3) HiCS [13] as representative of subspace search techniques; (4) Out . For HiCS and RS we always use the maximum outlier score of all subspaces. To ensure repeatability, we provide details on our experiments online. 1
Our main focus is to analyze outlier detection quality on real world data. We use the area under the ROC curve (AUC) as quality criterion. To perform scalability experi-ments and to evaluate all RefOut parameters, we utilize synthetic data. Our synthetic data generator injects true subspace outliers in a database as follows: We partition the attributes of the database of dimensionality D in subspace components of dimensionality d randomly between 2 and 8 with equal probability. To create a structure of regular objects in each subspace component, we draw random val-ues satisfying x s 1 + . . . + x s d = 1. We inject a true subspace outlier by deviating one object slightly from this hyperplane, satisfying that all its lower dimensional projections are in a region of regular density. This special type of true subspace outlier can be detected clearly by all three outlier models in the subspace components. http://www.ipd.kit.edu/~muellere/RefOut/
As already illustrated in our toy example in the intro-duction, it is clear that a LOF outlier is not necessarily an ABOD outlier. Since the true subspace outliers are individ-ual to each model, it would be desirable to have a ground truth of true subspace outliers of each type. To this end, we introduce a novel evaluation approach for detection qual-ity of true subspace outliers in dependence on the outlier model. We propose to perform an exhaustive search to ob-tain a ground truth of true subspace outliers for each model. That is, we scan all subspaces of a dataset exhaustively with each model up to an upper dimensionality level. This is obvi-ously a very time-consuming operation. Therefore, we have to focus on datasets of moderate size and dimensionality to reach a reasonable upper dimensionality level. We chose the datasets Breast, Breast Diagnostic [10] and a larger Electric-ity Meter dataset from a collaboration partner. Note that we had to drop two discrete attributes from the Breast dataset to ensure a well defined local outlier factor. We further nor-malized all attributes to a unit interval. We scanned up to a dimensionality of 4 for Breast Diagnostics (31,930 subspaces for each model) and Electricity Meter (5,488 subspaces), and up to level 5 for Breast (206,367 subspaces). The overall scanning took several days, mainly spent on running ABOD (using the FastABOD version [19]).

Since in Sec. 3 we defined the target function to quantify true subspace outliers to be the height of the peak, we store the maximal peak for each object and the corresponding subspace during our exhaustive scan. A first insight is that the three models show very different distributions regarding the dimensionality in which each object showed its maximal subspace outlierness. These results are given in Table 1. For instance, we can see that for Breast and Breast Diag-nostic LOF tends to see more high dimensional peaks, while for Electricity Meter ABOD detects more high dimensional peaks. Note that for ABOD the outlierness rarely peaks in 1-dimensional subspaces, since the ABOD score degener-ates to a (still meaningful) variance over reciprocal distance products in one dimension.

For the following experiments we rank the peaks (for each model and dataset) and extract three different true sub-space outlier ground truths for each model corresponding to the top 2%, 5%, and 10% of the peaks. This allows us to investigate interesting cross evaluations and analyze questions like how well does LOF detect ABOD outliers, or which one of the true subspace models is the hardest to detect in the full space? To this end, we evaluate all 12 combinations of { FS (full-space), RS (random-subspaces), HiCS, RefOut } X { ABOD, DB, LOF } on all ground truths. The average AUC values of these experiments are shown in Figure 5: True subspace outlier detection quality (AUC) on real world data Fig. 5. Each row corresponds to a certain ground truth model ABOD/DB/LOF. We highlight the blocks where a subspace approach uses the same outlier model as the ground truth, and intuitively we expect the best results in this case. We can see that this for instance is strongly pronounced for Breast Diagnostic with the DB model. On the other hand, we were surprised to find that the ABOD ground truth is sometimes better detected using DB/LOF instead of ABOD itself as detection model.

Regarding the adaptiveness of the subspace search mod-els, we can see that the static selection scheme of HiCS does not perform well in general, especially in combination with ABOD. Using random subspaces shows better overall adap-tation simply by making no assumption for the selection at all. In most cases RS improves over a full-space detection, but not when combined with ABOD. Regarding RefOut , we can see that its adaptive design clearly improves the sub-space selection for all models. We observe the most pro-nounced improvement over the other subspace techniques in combinations with ABOD. The systematic quality improve-ment of RefOut comes along with a slightly increased run-time: The average runtimes over all models and datasets were: 41.6 sec for RS, 49.0 sec for HiCS, and 76.2 sec for RefOut , which is still several orders of magnitudes below the runtime for exhaustive searching and is worth to be in-vested when looking at the improved detection and descrip-tion of individual outliers.
To analyze the dependence of the detection quality with the database dimensionality we performed experiments on different dimensionality levels. We generated 5 random data-sets on each dimensionality level 25, 50, 75, and 100 with subspace outliers of a random dimensionality up to 8. For this experiment we focus on a single outlier model to keep the number of results manageable. We chose the LOF out-lier model due to its high popularity. We kept the LOF parameter M inP ts = 10 constant for all approaches. For the random subspace detection we chose the same dimen-sionality level as the dimensionality of the initial pool of RefOut (75% of D ) to highlight the improvement due to subspace refinement. We keep the total number of evaluated subspaces equal for RS, HiCS, and RefOut . Fig. 6 shows the results. Regarding quality, we can see that even the ran-dom subspace approach consistently outperforms a fullspace subspace detection. Regarding HiCS we can see that it can improve over random subspaces on average. But we also see the effect of its non-adaptiveness: Sometimes the subspaces detected by HiCS match quite well (on the 50 dimensional level); other times HiCS outputs subspaces that are of no use to the outlier model (on D = 75). For RefOut we observe a very good scalability with respect to the dimen-sionality: The subspace selection consistently outperforms the other subspace approaches. The price for the increased quality is a slightly increased runtime. However, we can see that the increase over the runtime baseline defined by RS is rather low: This means that the majority of the runtime is spent on applying the outlier model itself and not on the subspace refinement framework. Overall RefOut shows a linear scalability w.r.t. the number of dimensions, making it capable of handling high dimensional databases. F igure 6: Scalability w.r.t. increasing dimensionality on synthetic data (from left to right in each group: FS, RS, HiCS, RefOut) We performed a thorough analysis of all parameters in RefOut , again based on the LOF model. We evaluated each parameter configuration on the pool of 20 datasets for Sec. 5.2. This means that the dataset pool contains both difficult and more easier datasets. In our opinion this is im-portant to ensure that we do not analyze the influence of a parameter for a single database dimensionality. In order not to use absolute values for d 1 and d 2 , we set these parameters as percentage of D . Our default parameters were psize =100, opct =20%, d 1 =75%, d 2 =30%, and a beamSize =100. Start-ing from this configuration we performed a sensitivity anal-ysis by varying each parameter individually. The results are shown in Fig. 7. We can see that in general the parameters are robust and slight variations of a parameter do not harm the results significantly. Note that the main fluctuations in the results are caused by the broad spectrum in difficulty of the datasets. As expected, increasing the pool size has a positive influence on the results, although we did not ob-serve further improvements above a pool size of 125. The opct parameter that controls how many objects are consid-ered for subspace refinement is also straightforward to set up: Higher values produce better results since the detec-tion quality of the high dimensional subspace scan is less relevant. Our primary choice of 75% for d 1 was motivated by the idea that we wanted both good subspace coverage while keeping the number of irrelevant attributes low. The results show that this choice was still a bit too high: Check-ing subspaces of a dimensionality of 60% gave slightly better results. This indicates that RefOut works well with a low subspace coverage; the influence of irrelevant attributes is the bigger issue. We did not observe a significant influence of the beamSize in our bottom-up subspace refinement on the results, which shows that even low values in the beam search can find reasonably good refinement candidates.
In this work, we present a flexible and adaptive subspace search technique for outlier mining. It refines a pool of ran-dom subspaces by exploiting the score discrepancy in differ-ent subspaces. Based on the statistical comparison of outlier scores, we achieve an adaptive search tailored to the under-lying outlier model. This allows us to inherit the properties (quality, performance, etc) of various well-established out-lier definitions for the subspace search. This results in an improved outlier detection but also in individual outlier de-scriptions for each object.

Regarding future work, we aim at utilizing adaptive sub-space search for outlier ensembles. So far we detect sub-spaces for each outlier according to a single model. Com-bining multiple outlier models is a promising extension of RefOut in order to find outliers that deviate w.r.t. differ-ent outlier models in different subspaces. This has not been addressed so far, and hence, RefOut might impact future development of outlier ensembles [1].
 This work is supported by the German Research Foundation (DFG) within GRK 1194, by the Young Investigator Group program of KIT as part of the German Excellence Initiative, and by a Post-Doctoral Fellowship of the Research Founda-tion  X  Flanders (FWO). [1] C. C. Aggarwal. Outlier ensembles: Position paper. [2] C. C. Aggarwal. Outlier Analysis . Springer, 2013. [3] C. C. Aggarwal and P. S. Yu. Outlier detection for [4] R. Agrawal and R. Srikant. Fast algorithms for mining [5] F. Angiulli, F. Fassetti, and L. Palopoli. Detecting [6] K. Beyer, J. Goldstein, R. Ramakrishnan, and [7] M. Breunig, H.-P. Kriegel, R. Ng, and J. Sander. LOF: [8] C.-H. Cheng, A. W. Fu, and Y. Zhang. Entropy-based [9] P. Filzmoser, R. Maronna, and M. Werner. Outlier [10] A. Frank and A. Asuncion. UCI machine learning [11] A. Ghoting, S. Parthasarathy, and M. Otey. Fast [12] K. Kailing, H.-P. Kriegel, P. Kr  X  oger, and S. Wanka. [13] F. Keller, E. M  X  uller, and K. B  X  ohm. HiCS: High [14] E. Knorr and R. Ng. Algorithms for Mining [15] E. M. Knorr and R. T. Ng. Finding intensional [16] H.-P. Kriegel, P. Kr  X  oger, E. Schubert, and A. Zimek. [17] H.-P. Kriegel, P. Kr  X  oger, E. Schubert, and A. Zimek. [18] H.-P. Kriegel, E. Schubert, A. Zimek, and P. Kr  X  oger. [19] H.-P. Kriegel, M. Schubert, and A. Zimek.
 [20] A. Lazarevic and V. Kumar. Feature bagging for [21] E. Loekito and J. Bailey. Mining influential attributes [22] E. M  X  uller, M. Schiffer, and T. Seidl. Statistical [23] H. V. Nguyen, E. M  X  uller, J. Vreeken, F. Keller, and [24] P. Rousseeuw and A. Leroy. Robust Regression and [25] Y. Wang, S. Parthasarathy, and S. Tatikonda.
 [26] S. Wrobel. An algorithm for multi-relational discovery
