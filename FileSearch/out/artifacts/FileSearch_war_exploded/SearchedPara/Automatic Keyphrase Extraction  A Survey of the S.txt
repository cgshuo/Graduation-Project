 Automatic keyphrase extraction concerns  X  X he au-tomatic selection of important and topical phrases from the body of a document X  (Turney, 2000). In other words, its goal is to extract a set of phrases that are related to the main topics discussed in a given document (Tomokiyo and Hurst, 2003; Liu et al., 2009b; Ding et al., 2011; Zhao et al., 2011).
Document keyphrases have enabled fast and ac-curate searching for a given document from a large text collection, and have exhibited their potential in improving many natural language processing (NLP) and information retrieval (IR) tasks, such as text summarization (Zhang et al., 2004), text categorization (Hulth and Megyesi, 2006), opin-ion mining (Berend, 2011), and document index-ing (Gutwin et al., 1999).

Owing to its importance, automatic keyphrase extraction has received a lot of attention. However, the task is far from being solved: state-of-the-art performance on keyphrase extraction is still much lower than that on many core NLP tasks (Liu et al., 2010). Our goal in this paper is to survey the state of the art in keyphrase extraction, examining the major sources of errors made by existing systems and discussing the challenges ahead. Automatic keyphrase extraction systems have been evaluated on corpora from a variety of sources ranging from long scientific publications to short paper abstracts and email messages. Ta-ble 1 presents a listing of the corpora grouped by at least four corpus-related factors that affect the difficulty of keyphrase extraction.
 Length The difficulty of the task increases with the length of the input document as longer doc-uments yield more candidate keyphrases (i.e., phrases that are eligible to be keyphrases (see Sec-tion 3.1)). For instance, each Inspec abstract has on average 10 annotator-assigned keyphrases and 34 candidate keyphrases. In contrast, a scientific paper typically has at least 10 keyphrases and hun-dreds of candidate keyphrases, yielding a much bigger search space (Hasan and Ng, 2010). Conse-quently, it is harder to extract keyphrases from sci-entific papers, technical reports, and meeting tran-scripts than abstracts, emails, and news articles. Structural consistency In a structured doc-ument, there are certain locations where a keyphrase is most likely to appear. For instance, most of a scientific paper X  X  keyphrases should ap-pear in the abstract and the introduction. While structural information has been exploited to ex-tract keyphrases from scientific papers (e.g., title, section information) (Kim et al., 2013), web pages (e.g., metadata) (Yih et al., 2006), and chats (e.g., dialogue acts) (Kim and Baldwin, 2012), it is most useful when the documents from a source exhibit structural similarity. For this reason, structural in-formation is likely to facilitate keyphrase extrac-tion from scientific papers and technical reports because of their standard format (i.e., standard sections such as abstract, introduction, conclusion, etc.). In contrast, the lack of structural consistency in other types of structured documents (e.g., web pages, which can be blogs, forums, or reviews) may render structural information less useful. Topic change An observation commonly ex-ploited in keyphrase extraction from scientific ar-ticles and news articles is that keyphrases typically appear not only at the beginning (Witten et al., 1999) but also at the end (Medelyan et al., 2009) of a document. This observation does not neces-sarily hold for conversational text (e.g., meetings, chats), however. The reason is simple: in a conver-sation, the topics (i.e., its talking points) change as the interaction moves forward in time, and so do the keyphrases associated with a topic. One way to address this complication is to detect a topic change in conversational text (Kim and Baldwin, 2012). However, topic change detection is not al-ways easy: while the topics listed in the form of an agenda at the beginning of formal meeting tran-scripts can be exploited, such clues are absent in casual conversations (e.g., chats).
 Topic correlation Another observation com-monly exploited in keyphrase extraction from scientific articles and news articles is that the keyphrases in a document are typically related to each other (Turney, 2003; Mihalcea and Tarau, 2004). However, this observation does not nec-essarily hold for informal text (e.g., emails, chats, informal meetings, personal blogs), where people can talk about any number of potentially uncorre-lated topics. The presence of uncorrelated topics implies that it may no longer be possible to exploit relatedness and therefore increases the difficulty of keyphrase extraction. A keyphrase extraction system typically operates in two steps: (1) extracting a list of words/phrases that serve as candidate keyphrases using some heuristics (Section 3.1); and (2) determining which of these candidate keyphrases are correct keyphrases using supervised (Section 3.2) or un-supervised (Section 3.3) approaches. 3.1 Selecting Candidate Words and Phrases As noted before, a set of phrases and words is typically extracted as candidate keyphrases using heuristic rules. These rules are designed to avoid spurious instances and keep the number of candi-dates to a minimum. Typical heuristics include (1) using a stop word list to remove stop words (Liu et al., 2009b), (2) allowing words with certain part-of-speech tags (e.g., nouns, adjectives, verbs) to be candidate keywords (Mihalcea and Tarau, 2004; Wan and Xiao, 2008b; Liu et al., 2009a), (3) al-lowing n-grams that appear in Wikipedia article titles to be candidates (Grineva et al., 2009), and (4) extracting n-grams (Witten et al., 1999; Hulth, 2003; Medelyan et al., 2009) or noun phrases (Barker and Cornacchia, 2000; Wu et al., 2005) that satisfy pre-defined lexico-syntactic pattern(s) (Nguyen and Phan, 2009).

Many of these heuristics have proven effective with their high recall in extracting gold keyphrases from various sources. However, for a long docu-ment, the resulting list of candidates can be long. Consequently, different pruning heuristics have been designed to prune candidates that are un-likely to be keyphrases (Huang et al., 2006; Kumar and Srinathan, 2008; El-Beltagy and Rafea, 2009; You et al., 2009; Newman et al., 2012). 3.2 Supervised Approaches Research on supervised approaches to keyphrase extraction has focused on two issues: task refor-mulation and feature design . 3.2.1 Task Reformulation Early supervised approaches to keyphrase extrac-tion recast this task as a binary classification prob-lem (Frank et al., 1999; Turney, 1999; Witten et al., 1999; Turney, 2000). The goal is to train a classifier on documents annotated with keyphrases to determine whether a candidate phrase is a keyphrase. Keyphrases and non-keyphrases are used to generate positive and negative examples, respectively. Different learning algorithms have been used to train this classifier, including na  X   X ve Bayes (Frank et al., 1999; Witten et al., 1999), decision trees (Turney, 1999; Turney, 2000), bag-ging (Hulth, 2003), boosting (Hulth et al., 2001), maximum entropy (Yih et al., 2006; Kim and Kan, 2009), multi-layer perceptron (Lopez and Romary, 2010), and support vector machines (Jiang et al., 2009; Lopez and Romary, 2010).

Recasting keyphrase extraction as a classifica-tion problem has its weaknesses, however. Recall that the goal of keyphrase extraction is to identify the most representative phrases for a document. In other words, if a candidate phrase c 1 is more representative than another candidate phrase c 2 , c 1 should be preferred to c 2 . Note that a binary clas-sifier classifies each candidate keyphrase indepen-dently of the others, and consequently it does not allow us to determine which candidates are better than the others (Hulth, 2004; Wang and Li, 2011).
Motivated by this observation, Jiang et al. (2009) propose a ranking approach to keyphrase extraction, where the goal is to learn a ranker to rank two candidate keyphrases. This pairwise ranking approach therefore introduces competi-tion between candidate keyphrases, and has been shown to significantly outperform KEA (Witten et al., 1999; Frank et al., 1999), a popular su-pervised baseline that adopts the traditional super-vised classification approach (Song et al., 2003; Kelleher and Luz, 2005). 3.2.2 Features The features commonly used to represent an in-stance for supervised keyphrase extraction can be broadly divided into two categories. Within-collection features are computed based solely on the training documents. These features can be further divided into three types.

Statistical features are computed based on sta-tistical information gathered from the training documents. Three such features have been exten-sively used in supervised approaches. The first one, tf*idf (Salton and Buckley, 1988), is com-puted based on candidate frequency in the given text and inverse document frequency (i.e., number The second one, the distance of a phrase, is de-fined as the number of words preceding its first occurrence normalized by the number of words in the document. Its usefulness stems from the fact that keyphrases tend to appear early in a docu-ment. The third one, supervised keyphraseness , encodes the number of times a phrase appears as a keyphrase in the training set. This feature is de-signed based on the assumption that a phrase fre-quently tagged as a keyphrase is more likely to be a keyphrase in an unseen document. These three features form the feature set of KEA (Witten et al., 1999; Frank et al., 1999), and have been shown to perform consistently well on documents from var-ious sources (Yih et al., 2006; Kim et al., 2013). Other statistical features include phrase length and spread (i.e., the number of words between the first and last occurrences of a phrase in the document).
Structural features encode how different in-stances of a candidate keyphrase are located in different parts of a document. A phrase is more likely to be a keyphrase if it appears in the ab-stract or introduction of a paper or in the metadata section of a web page. In fact, features that en-code how frequently a candidate keyphrase occurs in various sections of a scientific paper (e.g., in-troduction, conclusion) (Nguyen and Kan, 2007) and those that encode the location of a candidate keyphrase in a web page (e.g., whether it appears in the title) (Chen et al., 2005; Yih et al., 2006) have been shown to be useful for the task.

Syntactic features encode the syntactic pat-terns of a candidate keyphrase. For example, a candidate keyphrase has been encoded as (1) a PoS tag sequence , which denotes the sequence of part-of-speech tag(s) assigned to its word(s); and (2) a suffix sequence , which is the sequence of morphological suffixes of its words (Yih et al., 2006; Nguyen and Kan, 2007; Kim and Kan, 2009). However, ablation studies conducted on web pages (Yih et al., 2006) and scientific articles (Kim and Kan, 2009) reveal that syntactic features are not useful for keyphrase extraction in the pres-ence of other feature types. External resource-based features are computed based on information gathered from resources other than the training documents, such as lex-ical knowledge bases (e.g., Wikipedia) or the Web, with the goal of improving keyphrase extrac-tion performance by exploiting external knowl-edge. Below we give an overview of the exter-nal resource-based features that have proven use-ful for keyphrase extraction.

Wikipedia-based keyphraseness is computed as a candidate X  X  document frequency multiplied by the ratio of the number of Wikipedia articles where the candidate appears as a link to the number of articles where it appears (Medelyan et al., 2009). This feature is motivated by the observation that a candidate is likely to be a keyphrase if it occurs frequently as a link in Wikipedia. Unlike super-vised keyphraseness, Wikipedia-based keyphrase-ness can be computed without using documents annotated with keyphrases and can work even if there is a mismatch between the training domain and the test domain.

Yih et al. (2006) employ a feature that en-codes whether a candidate keyphrase appears in the query log of a search engine, exploiting the ob-servation that a candidate is potentially important if it was used as a search query. Terminological databases have been similarly exploited to encode the salience of candidate keyphrases in scientific papers (Lopez and Romary, 2010).

While the aforementioned external resource-based features attempt to encode how salient a candidate keyphrase is, Turney (2003) proposes features that encode the semantic relatedness be-tween two candidate keyphrases. Noting that can-didate keyphrases that are not semantically re-lated to the predicted keyphrases are unlikely to be keyphrases in technical reports, Turney em-ploys coherence features to identify such can-didate keyphrases. Semantic relatedness is en-coded in the coherence features as two candidate keyphrases X  pointwise mutual information, which Turney computes by using the Web as a corpus. 3.3 Unsupervised Approaches Existing unsupervised approaches to keyphrase extraction can be categorized into four groups. 3.3.1 Graph-Based Ranking Intuitively, keyphrase extraction is about finding the important words and phrases from a docu-ment. Traditionally, the importance of a candi-date has often been defined in terms of how related it is to other candidates in the document. Infor-mally, a candidate is important if it is related to (1) a large number of candidates and (2) candidates that are important. Researchers have computed re-latedness between candidates using co-occurrence counts (Mihalcea and Tarau, 2004; Matsuo and Ishizuka, 2004) and semantic relatedness (Grineva et al., 2009), and represented the relatedness in-formation collected from a document as a graph (Mihalcea and Tarau, 2004; Wan and Xiao, 2008a; Wan and Xiao, 2008b; Bougouin et al., 2013).
The basic idea behind a graph-based approach is to build a graph from the input document and rank its nodes according to their importance us-ing a graph-based ranking method (e.g., Brin and Page (1998)). Each node of the graph corresponds to a candidate keyphrase from the document and an edge connects two related candidates. The edge weight is proportional to the syntactic and/or semantic relevance between the connected candi-dates. For each node, each of its edges is treated as a  X  X ote X  from the other node connected by the edge. A node X  X  score in the graph is defined recur-sively in terms of the edges it has and the scores of the neighboring nodes. The top-ranked candidates from the graph are then selected as keyphrases for the input document. TextRank (Mihalcea and Ta-rau, 2004) is one of the most well-known graph-based approaches to keyphrase extraction.

This instantiation of a graph-based approach overlooks an important aspect of keyphrase ex-traction, however. A set of keyphrases for a doc-ument should ideally cover the main topics dis-cussed in it, but this instantiation does not guaran-tee that all the main topics will be represented by the extracted keyphrases. Despite this weakness, a graph-based representation of text was adopted by many approaches that propose different ways of computing the similarity between two candidates. 3.3.2 Topic-Based Clustering Another unsupervised approach to keyphrase extraction involves grouping the candidate keyphrases in a document into topics , such that each topic is composed of all and only those candidate keyphrases that are related to that topic (Grineva et al., 2009; Liu et al., 2009b; Liu et al., 2010). There are several motivations behind this topic-based clustering approach. First, a keyphrase should ideally be relevant to one or more main topic(s) discussed in a document (Liu et al., 2010; Liu et al., 2012). Second, the extracted keyphrases should be comprehensive in the sense that they should cover all the main topics in a document (Liu et al., 2009b; Liu et al., 2010; Liu et al., 2012). Below we examine three representative systems that adopt this approach. KeyCluster Liu et al. (2009b) adopt a clustering-based approach (henceforth KeyClus-ter) that clusters semantically similar candidates using Wikipedia and co-occurrence-based statis-tics. The underlying hypothesis is that each of these clusters corresponds to a topic covered in the document, and selecting the candidates close to the centroid of each cluster as keyphrases ensures that the resulting set of keyphrases covers all the topics of the document.

While empirical results show that KeyCluster performs better than both TextRank and Hulth X  X  (2003) supervised system, KeyCluster has a poten-tial drawback: by extracting keyphrases from each topic cluster, it essentially gives each topic equal importance. In practice, however, there could be topics that are not important and these topics should not have keyphrase(s) representing them. Topical PageRank (TPR) Liu et al. (2010) pro-pose TPR, an approach that overcomes the afore-mentioned weakness of KeyCluster. It runs Tex-tRank multiple times for a document, once for each of its topics induced by a Latent Dirichlet Al-location (Blei et al., 2003). By running TextRank once for each topic, TPR ensures that the extracted keyphrases cover the main topics of the document. The final score of a candidate is computed as the sum of its scores for each of the topics, weighted by the probability of that topic in that document. Hence, unlike KeyCluster, candidates belonging to a less probable topic are given less importance.
TPR performs significantly better than both tf*idf and TextRank on the DUC-2001 and Inspec datasets. TPR X  X  superior performance strength-ens the hypothesis of using topic clustering for keyphrase extraction. However, though TPR is conceptually better than KeyCluster, Liu et al. did not compare TPR against KeyCluster.
 CommunityCluster Grineva et al. (2009) pro-pose CommunityCluster, a variant of the topic clustering approach to keyphrase extraction. Like TPR, CommunityCluster gives more weight to more important topics, but unlike TPR, it extracts all candidate keyphrases from an important topic, assuming that a candidate that receives little focus in the text should still be extracted as a keyphrase as long as it is related to an important topic. Com-munityCluster yields much better recall (without losing precision) than extractors such as tf*idf, TextRank, and the Yahoo! term extractor. 3.3.3 Simultaneous Learning Since keyphrases represent a dense summary of a document, researchers hypothesized that text sum-marization and keyphrase extraction can poten-tially benefit from each other if these tasks are per-formed simultaneously. Zha (2002) proposes the first graph-based approach for simultaneous sum-marization and keyphrase extraction, motivated by a key observation: a sentence is important if it con-tains important words, and important words ap-pear in important sentences. Wan et al. (2007) ex-tend Zha X  X  work by adding two assumptions: (1) an important sentence is connected to other im-portant sentences, and (2) an important word is linked to other important words, a TextRank-like assumption. Based on these assumptions, Wan et al. (2007) build three graphs to capture the asso-ciation between the sentences (S) and the words (W) in an input document, namely, a S X  X  graph, a bipartite S X  X  graph, and a W X  X  graph. The weight of an edge connecting two sentence nodes in a S X  X  graph corresponds to their content simi-larity. An edge weight in a S X  X  graph denotes the word X  X  importance in the sentence it appears. Fi-nally, an edge weight in a W X  X  graph denotes the co-occurrence or knowledge-based similarity be-tween the two connected words. Once the graphs are constructed for an input document, an itera-tive reinforcement algorithm is applied to assign scores to each sentence and word. The top-scored words are used to form keyphrases.

The main advantage of this approach is that it combines the strengths of both Zha X  X  approach (i.e., bipartite S X  X  graphs) and TextRank (i.e., W X  W graphs) and performs better than both of them. However, it has a weakness: like TextRank, it does not ensure that the extracted keyphrases will cover all the main topics. To address this problem, one can employ a topic clustering algorithm on the W X  W graph to produce the topic clusters, and then en-sure that keyphrases are chosen from every main topic cluster. 3.3.4 Language Modeling Many existing approaches have a separate, heuris-tic module for extracting candidate keyphrases prior to keyphrase ranking/extraction. In contrast, Tomokiyo and Hurst (2003) propose an approach (henceforth LMA) that combines these two steps.
LMA scores a candidate keyphrase based on two features, namely, phraseness (i.e., the ex-tent to which a word sequence can be treated as a phrase) and informativeness (i.e., the extent to which a word sequence captures the central idea of the document it appears in). Intuitively, a phrase that has high scores for phraseness and informa-tiveness is likely to be a keyphrase. These feature values are estimated using language models (LMs) trained on a foreground corpus and a background corpus. The foreground corpus is composed of the set of documents from which keyphrases are to be extracted. The background corpus is a large corpus that encodes general knowledge about the world (e.g., the Web). A unigram LM and an n-gram LM are constructed for each of these two corpora. Phraseness, defined using the foreground LM, is calculated as the loss of information in-curred as a result of assuming a unigram LM (i.e., conditional independence among the words of the phrase) instead of an n-gram LM (i.e., the phrase is drawn from an n-gram LM). Informativeness is computed as the loss that results because of the assumption that the candidate is sampled from the background LM rather than the foreground LM. The loss values are computed using Kullback-Leibler divergence. Candidates are ranked accord-ing to the sum of these two feature values.
In sum, LMA uses a language model rather than heuristics to identify phrases, and relies on the lan-guage model trained on the background corpus to determine how  X  X nique X  a candidate keyphrase is to the domain represented by the foreground cor-pus. The more unique it is to the foreground X  X  do-main, the more likely it is a keyphrase for that do-main. While the use of language models to iden-tify phrases cannot be considered a major strength of this approach (because heuristics can identify phrases fairly reliably), the use of a background corpus to identify candidates that are unique to the foreground X  X  domain is a unique aspect of this ap-proach. We believe that this idea deserves further investigation, as it would allow us to discover a keyphrase that is unique to the foreground X  X  do-main but may have a low tf*idf value. In this section, we describe metrics for evaluating keyphrase extraction systems as well as state-of-the-art results on commonly-used datasets. 4.1 Evaluation Metrics Designing evaluation metrics for keyphrase ex-traction is by no means an easy task. To score the output of a keyphrase extraction system, the typical approach, which is also adopted by the SemEval-2010 shared task on keyphrase extrac-tion, is (1) to create a mapping between the keyphrases in the gold standard and those in the system output using exact match , and then (2) score the output using evaluation metrics such as precision (P), recall (R), and F-score (F).
Conceivably, exact match is an overly strict con-dition, considering a predicted keyphrase incor-rect even if it is a variant of a gold keyphrase. For instance, given the gold keyphrase  X  X eural network X , exact match will consider a predicted phrase incorrect even if it is an expanded version of the gold keyphrase ( X  X rtificial neural network X ) or one of its morphological ( X  X eural networks X ) or lexical ( X  X eural net X ) variants. While morphologi-cal variations can be handled using a stemmer (El-Beltagy and Rafea, 2009), other variations may not be handled easily and reliably.

Human evaluation has been suggested as a pos-sibility (Matsuo and Ishizuka, 2004), but it is time-consuming and expensive. For this reason, re-searchers have experimented with two types of automatic evaluation metrics. The first type of metrics addresses the problem with exact match. These metrics reward a partial match between a predicted keyphrase and a gold keyphrase (i.e., overlapping n -grams) and are commonly used in machine translation (MT) and summarization evaluations. They include B LEU , M ETEOR , N IST , and R OUGE . Nevertheless, experiments show that these MT metrics only offer a partial solution to problem with exact match: they can only detect a subset of the near-misses (Kim et al., 2010a).
The second type of metrics focuses on how a system ranks its predictions. Given that two sys-tems A and B have the same number of correct predictions, binary preference measure (Bpref) and mean reciprocal rank (MRR) (Liu et al., 2010) will award more credit to A than to B if the ranks of the correct predictions in A  X  X  output are higher than those in B  X  X  output. R-precision ( R p ) is an IR metric that focuses on ranking: given a docu-ment with n gold keyphrases, it computes the pre-cision of a system over its n highest-ranked can-didates (Zesch and Gurevych, 2009). The motiva-tion behind the design of R p is simple: a system will achieve a perfect R p value if it ranks all the keyphrases above the non-keyphrases. 4.2 The State of the Art Table 2 lists the best scores on some popular evalu-ation datasets and the corresponding systems. For example, the best F-scores on the Inspec test set, the DUC-2001 dataset, and the SemEval-2010 test
Two points deserve mention. First, F-scores de-crease as document length increases. These re-sults are consistent with the observation we made in Section 2 that it is more difficult to extract keyphrases correctly from longer documents. Sec-ond, recent unsupervised approaches have rivaled their supervised counterparts in performance (Mi-halcea and Tarau, 2004; El-Beltagy and Rafea, 2009; Liu et al., 2009b). For example, KP-Miner (El-Beltagy and Rafea, 2010), an unsupervised system, ranked third in the SemEval-2010 shared task with an F-score of 25.2, which is comparable to the best supervised system scoring 27.5. With the goal of providing directions for future work, we identify the errors commonly made by state-of-the-art keyphrase extractors below. 5.1 Error Analysis Although a few researchers have presented a sam-ple of their systems X  output and the corresponding gold keyphrases to show the differences between them (Witten et al., 1999; Nguyen and Kan, 2007; Medelyan et al., 2009), a systematic analysis of the major types of errors made by state-of-the-art keyphrase extraction systems is missing.

To fill this gap, we ran four keyphrase extrac-tion systems on four commonly-used datasets of varying sources, including Inspec abstracts (Hulth, 2003), DUC-2001 news articles (Over, 2001), sci-entific papers (Kim et al., 2010b), and meeting transcripts (Liu et al., 2009a). Specifically, we ran-domly selected 25 documents from each of these Table 2: Best scores achieved on various datasets. four datasets and manually analyzed the output of the four systems, including tf*idf, the most fre-quently used baseline, as well as three state-of-the-art keyphrase extractors, of which two are unsu-pervised (Wan and Xiao, 2008b; Liu et al., 2009b) and one is supervised (Medelyan et al., 2009).
Our analysis reveals that the errors fall into four major types, each of which contributes signifi-cantly to the overall errors made by the four sys-tems, despite the fact that the contribution of each of these error types varies from system to system. Moreover, we do not observe any significant dif-ference between the types of errors made by the four systems other than the fact that the super-vised system has the expected tendency to predict keyphrases seen in the training data. Below we describe these four major types of errors.

Overgeneration errors are a major type of pre-cision error, contributing to 28 X 37% of the overall error. Overgeneration errors occur when a system correctly predicts a candidate as a keyphrase be-cause it contains a word that appears frequently in the associated document, but at the same time er-roneously outputs other candidates as keyphrases because they contain the same word. Recall that for many systems, it is not easy to reject a non-keyphrase containing a word with a high term fre-quency: many unsupervised systems score a can-didate by summing the score of each of its compo-nent words, and many supervised systems use un-igrams as features to represent a candidate. To be more concrete, consider the news article on athlete Ben Johnson in Figure 1, where the keyphrases are boldfaced. As we can see, the word Olympic(s) has a significant presence in the document. Con-sequently, many systems not only correctly predict Olympics as a keyphrase, but also erroneously pre-dict Olympic movement as a keyphrase, yielding overgeneration errors.

Infrequency errors are a major type of re-Figure 1: A news article on Ben Johnson from the DUC-2001 dataset. The keyphrases are boldfaced. call error contributing to 24 X 27% of the overall error. Infrequency errors occur when a system fails to identify a keyphrase owing to its infre-quent presence in the associated document (Liu et al., 2011). Handling infrequency errors is a challenge because state-of-the-art keyphrase ex-tractors rarely predict candidates that appear only once or twice in a document. In the Ben Johnson example, many keyphrase extractors fail to iden-tify 100-meter dash and gold medal as keyphrases, resulting in infrequency errors.

Redundancy errors are a type of precision er-ror contributing to 8 X 12% of the overall error. Re-dundancy errors occur when a system correctly identifies a candidate as a keyphrase, but at the same time outputs a semantically equivalent can-didate (e.g., its alias) as a keyphrase. This type of error can be attributed to a system X  X  failure to determine that two candidates are semantically equivalent. Nevertheless, some researchers may argue that a system should not be penalized for re-dundancy errors because the extracted candidates are in fact keyphrases. In our example, Olympics and Olympic games refer to the same concept, so a system that predicts both of them as keyphrases commits a redundancy error.

Evaluation errors are a type of recall error con-tributing to 7 X 10% of the overall error. An evalu-ation error occurs when a system outputs a can-didate that is semantically equivalent to a gold keyphrase, but is considered erroneous by a scor-ing program because of its failure to recognize that the predicted phrase and the corresponding gold keyphrase are semantically equivalent. In other words, an evaluation error is not an error made by a keyphrase extractor, but an error due to the naivety of a scoring program. In our exam-ple, while Olympics and Olympic games refer to the same concept, only the former is annotated as keyphrase. Hence, an evaluation error occurs if a system predicts Olympic games but not Olympics as a keyphrase and the scoring program fails to identify them as semantically equivalent. 5.2 Recommendations We recommend that background knowledge be extracted from external lexical databases (e.g., YAGO2 (Suchanek et al., 2007), Freebase (Bol-lacker et al., 2008), BabelNet (Navigli and Ponzetto, 2012)) to address the four types of er-rors discussed above.

First, we discuss how redundancy errors could be addressed by using the background knowledge extracted from external databases. Note that if we can identify semantically equivalent candidates, then we can reduce redundancy errors. The ques-tion, then, is: can background knowledge be used to help us identify semantically equivalent candi-dates? To answer this question, note that Freebase, for instance, has over 40 million topics (i.e., real-world entities such as people, places, and things) from over 70 domains (e.g., music, business, ed-ucation). Hence, before a system outputs a set of candidates as keyphrases, it can use Freebase to determine whether any of them is mapped to the same Freebase topic. Referring back to our run-ning example, both Olympics and Olympic games are mapped to a Freebase topic called Olympic games . Based on this information, a keyphrase ex-tractor can determine that the two candidates are aliases and should output only one of them, thus preventing a redundancy error.

Next, we discuss how infrequency errors could be addressed using background knowledge. A natural way to handle this problem would be to make an infrequent keyphrase frequent. To ac-complish this, we suggest exploiting an influen-tial idea in the keyphrase extraction literature: the importance of a candidate is defined in terms of how related it is to other candidates in the text (see Section 3.3.1). In other words, if we could relate an infrequent keyphrase to other candidates in the text, we could boost its importance.

We believe that this could be accomplished us-ing background knowledge. The idea is to boost the importance of infrequent keyphrases using their frequent counterparts. Consider again our running example. All four systems have managed to identify Ben Johnson as a keyphrase due to its significant presence. Hence, we can boost the im-portance of 100-meter dash and gold medal if we can relate them to Ben Johnson .

To do so, note that Freebase maps a candi-date to one or more pre-defined topics, each of which is associated with one or more types. Types are similar to entity classes. For instance, the candidate Ben Johnson is mapped to a Freebase topic with the same name, which is associated with Freebase types such as Person , Athlete , and Olympic athlete . Types are defined for a specific domain in Freebase. For instance, Person , Ath-lete , and Olympic athlete are defined in the People , Sports , and Olympics domains, respectively. Next, consider the two infrequent candidates, 100-meter dash and gold medal . 100-meter dash is mapped to the topic Sprint of type Sports in the Sports do-main, whereas gold medal is mapped to a topic with the same name of type Olympic medal in the Olympics domain. Consequently, we can relate 100-meter dash to Ben Johnson via the Sports do-main (i.e., they belong to different types under the same domain). Additionally, gold medal can be related to Ben Johnson via the Olympics domain.
As discussed before, the relationship between two candidates is traditionally established using co-occurrence information. However, using co-occurrence windows has its shortcomings. First, an ad-hoc window size cannot capture related can-didates that are not inside the window. So it is difficult to predict 100-meter dash and gold medal as keyphrases: they are more than 10 tokens away from frequent words such as Johnson and Olympics . Second, the candidates inside a window are all assumed to be related to each other, but it is apparently an overly simplistic assumption. There have been a few attempts to design Wikipedia-based relatedness measures, with promising ini-tial results (Grineva et al., 2009; Liu et al., 2009b;
Overgeneration errors could similarly be ad-dressed using background knowledge. Recall that Olympic movement is not a keyphrase in our ex-ample although it includes an important word (i.e., Olympic ). Freebase maps Olympic movement to a topic with the same name, which is associated with a type called Musical Recording in the Mu-sic domain. However, it does not map Olympic movement to any topic in the Olympics domain. The absence of such a mapping in the Olympics domain could be used by a keyphrase extractor as a supporting evidence against predicting Olympic movement as a keyphrase.

Finally, as mentioned before, evaluation errors should not be considered errors made by a sys-tem. Nevertheless, they reveal a problem with the way keyphrase extractors are currently evaluated. To address this problem, one possibility is to con-duct human evaluations. Cheaper alternatives in-clude having human annotators identify semanti-cally equivalent keyphrases during manual label-ing, and designing scoring programs that can au-tomatically identify such semantic equivalences. We have presented a survey of the state of the art in automatic keyphrase extraction. While unsu-pervised approaches have started to rival their su-pervised counterparts in performance, the task is far from being solved, as reflected by the fairly poor state-of-the-art results on various commonly-used evaluation datasets. Our analysis revealed that there are at least three major challenges ahead. 1. Incorporating background knowledge.
 While much recent work has focused on algo-rithmic development, keyphrase extractors need to have a deeper  X  X nderstanding X  of a document in order to reach the next level of performance. Such an understanding can be facilitated by the incorporation of background knowledge. 2. Handling long documents. While it may be possible to design better algorithms to handle the large number of candidates in long documents, we believe that employing sophisticated features, es-pecially those that encode background knowledge, will enable keyphrases and non-keyphrases to be distinguished more easily even in the presence of a large number of candidates. 3. Improving evaluation schemes. To more ac-curately measure the performance of keyphrase extractors, they should not be penalized for evalu-ation errors. We have suggested several possibili-ties as to how this problem can be addressed. We thank the anonymous reviewers for their de-tailed and insightful comments on earlier drafts of this paper. This work was supported in part by NSF Grants IIS-1147644 and IIS-1219142.
