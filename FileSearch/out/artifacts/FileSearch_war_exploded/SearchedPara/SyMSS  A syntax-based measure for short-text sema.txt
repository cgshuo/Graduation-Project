 1. Introduction extractive document summarization) to cluster similar sentences and then improve with sentence semantic similarity calculation.
 same. For example, at textual level, word co-occurrence can be an important source of information, but it is less signi semantic and syntactic information.

SyMSS obtains semantic information from a lexical resource such as WordNet [8] , WordNet is also widely spread and used in applications such as text summarization [9] or document clustering [10] . method after the way humans compute sentence semantic similarity. We particularly use, the results obtained by Wiemer-way humans do.

We use two benchmark data sets to evaluate the method. The with human ratings and with the methods proposed by Li et al. [12] and Islam and Inkpen [13] . method proposed in this paper outperform the best results obtained by the Li importance of syntactic structure in semantic similarity.
 The next section presents a brief review of the most common approaches used to compute sentence semantic similarity. to the method. Lastly, Section 6 sums up the work and poses some conclusions and future work. 2. Background that these methods need adequate information in order to perform well, and in most cases they cannot compute semantic similarity: corpus-based methods, word co-occurrence methods and hybrid methods.
Latent Semantic Analysis (LSA). LSA uses a word by passage matrix formed to re syntactic information, as will be shown in this paper. For example, the sentences semantic similarity of short texts or sentences [16] . Also, Islam and Inkpen [13] present a corpus-based method that between words and a common word order similarity measure.
 key feature is their use of corpora, which enables them to similar sentences if the sentences have no words in common; and, vice-versa, they can with many co-occurring words but not actually very similar, as in:
However, there have been some proposals for improving word co-occurrence methods, such as pattern matching methods word. Manual pattern set compilation is an arduous task, and there is no automated method for doing it.
There are also some hybrid methods that use both corpus-bas ed and knowledge-based techniques. The best-performing forming the word vector entirely on the basis of the words in the compared sentences. Then the method computes the semantic similarity by combining information drawn from a st ructured lexical database and from corpus statistics. Also measures and combine the results to show how these measures can be used to derive a short texts similarity measure. The main drawback of this method is that it computes the similarity of words using eight different methods, which is not computationally ef fi cient.
 reported in the literature in terms of correlation with human intuition, thus con in-depth syntactic information.
 an asymmetric task. For example,  X  I have two dogs  X  entails high semantic similarity score is not enough to assess textual entailment. For example, the sentences cat step in the textual entailment recognition task but not always suf and Mihalcea show that semantic similarity measures could represent a necessary to combine them with other techniques to achieve better results. One of these techniques could be the use of similarity measures and textual entailment recognition methods. Vanderwende and Dolan [22] showed that 390 pairs of sentences out of the 800 of the test set used on the fi rst PASCAL challenge can be classi spaces. Moreover, in the different PASCAL challenges can be found many systems that combine syntax and semantics. For et al. [27] ) using dependency trees and WordNet-b ased semantic similarity measures. 3. Sentence similarity method
The SyMSS method captures how the syntactic structure of the compared sentences in may denote two very different meanings. between concepts to be calculated. Syntactic information is obtained through a deep parsing process that the same syntactic function. 3.1. Semantic similarity between concepts performance of the proposed method. Thus, a comparative study of different measures of the semantic similarity between detailed explanation of these measures):  X  Path-based measures between concepts can be used to measure the similarity between concepts. Two different measures of this type were used: important to note that this particular implementation only takes into account
Hirst and St. Onge measure [HSO] [30] : this takes into account many other WordNet relations, beyond the (antonyms, synonyms ... ).  X  Information content measures
Information content measures the speci fi city of a concept, which is higher for more speci of this type were used: they share. So the measure is calculated as the information content of their lowest common subsumer in the hierarchy:  X  Gloss-based measures were used: associated with each concept and with their related concepts in WordNet.
 calculates the similarity by fi nding the cosine between these vectors.

Because each word may have many senses, in the comparison of two words, all the senses associated with each word capable of computing the similarity between adjectives and a dverbs. In WordNet, adjective structure has some kinds of relations (e.g.  X  similar-to  X  ,  X  antonyms  X  ,  X  see-also between adjectives and adverbs. 3.2. Semantic similarity between sentences the phrases that have the same syntactic function in both sentences, following the formula below:
Letusassumethatsentence s 1 is made of n phrases and their heads are h h , ... ,h 2n are their heads; and moreover the phrases of h shared by the other, a penalization factor (PF) is introduced to re the same syntactic role in both sentences. This way our method overcomes some of the limitations of WordNet such as the absence of proper nouns.

In the example presented above,  X  My brother has a dog with four legs which yielded the best results in the CoNLL-2008 shared task, concerning syntactic dependency parsing and semantic role statisticalapproachtoobtainalistofcandidatesyntac tic structures while the semantic submodel uses a classi set of syntactic  X  semantic structures using information from both syntactic and semantic sources.
Once the syntactic information has been extracted, the met hod computes the semantic similarity between the words that get a similarity of 1; and the heads of the direct objects does not have a noun complement of the direct object, the penalization factor is used to take into account this extra to formula (1) would be: this is exactly the similarity captured by our method. However, word-co-occurrence-based methods or word-order-based methods would give a higher similarity because of the presence of the words although actually these two words do not contribute to increase the semantic similarity.
Note also that, because of our method's use of word semantic similarity measures, our method considers the sentence brother has four cats  X  much more similar to the sentence has four legs  X  . This is due to the fact that  X  cat  X  is more similar to co-occurrence methods, which would fi nd a lower similarity between the two sentences of this second example. Our method thus outperforms some other methods that make use only of primary syntactic information, such as word order.
Taking into account only the word order information, the sentences detect this real similarity.
 clauses are considered equivalent. For example, the sentences for each of them. In our example, the term  X  book  X  of the is selected given that  X  book  X  book  X  is the most similar pair. In the same way, the term compared attending to its syntactic functions in the subordinate clause. For example, in the sentences: month  X  and  X  This is the computer he gave me  X  , SyMSS compares the terms the verbs and subjects of the two main clauses and it also compares clauses that have the same function in both sentences. 4. Experimental results 4.1. Data sets
Two data sets were used to evaluate the performance of the SyMSS method. As stated before, there is not much work about used the data set proposed by Li et al. [12] to enable comparison with the Li challenging task, we used the Microsoft Paraphrase Corpus [37] .
The fi rst data set contained 65 pairs of sentences created from the 65 noun pairs from [38] ,replacedbythenouns' de fi method was employed in using the data set to compare sentenc e semantic similarity measures). In order to enable com-sentence pairs you can see www2.docm.mmu.ac.uk/STAFF/J.Oshea/TRMMUCCA20081_5.pdf ). This data set contained the average similarity scores given by 32 human judges collected by Li et al. [12] .

The Microsoft Paraphrase Corpus consists of 4076 training and 1725 test sentence pairs collected from thousands of news which can be considered as an upper bound for the automated task. 4.2. Evaluation measures
Three different measures were used to evaluate the performance of the seven variations of SyMSS (see Table 2 for an 2) Spearman's rank correlation coef fi cient was also used to evaluate the different proposed variations. This coef 4.3. Evaluation of semantic measures between concepts 4.3.1. Experiments and results
The fi rst thing we wanted to do was compare the performance of each of the seven measures of word semantic similarity how different the ranks given by humans and by our method were.
 do not exploit syntactic information. The baseline method just compares each word of the method using each of the seven concept similarity measures used by SyMSS.

Table 2 shows the Pearson's and Spearman's correlation coef have l syntactic roles that are only present in one of the sentences, PF is subtracted l times from the ranking with human evaluators, are included. 4.3.2. Discussion
The very fi rst conclusion that can be extracted from the only slightly outperform its corresponding baseline does not weaken the conclusion. hierarchy. For example,  X  mouse  X  and  X  rodent  X  are separated by a path of length one, and in fl uential because most compared words in the assessed data set of this experiment are at similar levels of the hierarchy.
We can also see that other path-based measures and information content measures also yielded good results (all showing this paper make measures of this kind unsuitable. This could be due to factors like the following: of 703 to the pair of words  X  chicken  X  and  X  chicken  X  but gives a similarity of 2529 to the pair of words from the initial Pearson's and Spearman's correlations with human similarities of 0.51 and 0.46 to correlation coef words pertaining to the same synset is pointless. In fact, normalizing LESK-E using its maximum value produces no major improvement, so LESK-E does not seem to offer a good solution to the problem of computing sentence semantic similarity. support the observation that syntax is a very important source of information for computing semantic similarity between improving syntactic analysis would seem likely to lead to an improvement of our method's overall results.
Finally, it is important to note that the characteristics of this data set could have in and also the similarity values given by the human evaluators. Because each sentence was a de term in the sentence could have been a bias point in the evaluation process. 4.3.3. Error analysis difference between the similarity scores and rankings given by PATH
There are two main kinds of errors: overestimation and underestimation errors. The made of de fi nitions. Thus, the main verb of all the sentences is the verb would result in a score of (0+0+1)/3=0.33, which is surely much higher than expected. For example, SyMSS ranks sentence 30 (see Table 4 ) which are ranked 12 and 15 positions below its corresponding human ranking. de fi nitions.

In order to deal with underestimation errors, it wou ld be necessary to smooth both the number of phrases n and the the number of different phrases but also of the difference of length between the two sentences. 4.4. In fl uence of adjectives and adverbs 4.4.1. Experiments and results
Anotherissuestudiedinthisresearchwasthein fl uence of adjectives and adverbs on the computation of sentence semantic adjectives and adverbs do give sentences some shade of mean ing that should be taken into account in the computation of pair of sentences  X  Ihaveabigdog  X  X  X  Ihaveabigdog  X  was 1, and the score obtained for the pair of sentences dog the fi ve path-based and information content variations of SyMSS together with VECTOR. The only modi
SyMSS used. For example, HSO gave a score from 0 to 16, so the similarity found by VECTOR for a pair of adjectives was multiplied by 16 (note that VECTOR gives similarity scores from 0 to 1).
 computation. 4.4.2. Discussion
The results shown in Table 5 con fi rm the intuitive idea that adjectives and adverbs play a signi and so ought to be used in computing sentence semantic similarity. The of 6.59 points.
 which was very similar to the scoring scale used in path-based measures. HSO gave a mean score of 0.56 and a standard were: 0.48 X 0.22, 0.66 X 0.22 and 0.67 X 0.24, respectively. Because the similarity between adjectives and adverbs was calculated with VECTOR, and its typical scores are different from the typical scores given by information-content-based measures, some degree of correlation with human evaluatio ns was lost. However, the improvements made in terms of rank correlation showed that the nuances introduced by adjectiv es and adverbs are of great importance in sentence semantic similarity. 4.5. Improving word semantic similarity [39] and Pirr X  and Seco [40] (which will be referred to as the Liu metric and the Pirr X  improvements brought about by a better concept semantic similarity measure. The Li words would lead to an overall improvement of the system. The SyMSS variation that used the Pirr X  of the variations in terms of scores and rank correlation with human ratings. Moreover, both the Liu and the Pirr X  score correlation is in most cases not as useful as the relative information given by rank correlation. 4.6. Paraphrasing identi fi cation 4.6.1. Experiments and results two sentences in a pair were semantically equivalent paraphrases or not. The agreement between human evaluators was approximately 83%, which can be considered as an upper bound for the automated task. For this paraphrase identi results obtained with this set for each of the variations tested are shown in Table 7 . The evaluation metrics used to measure the performance of the different variations of SyMSS were the ones proposed by correctly predicted sentences compared to all sentences. F1 and f1 are the uniform harmonic mean of precision the same way as we did. 4.6.2. Discussion signi fi cant (p b 0.001) in all the experiments, using a parametric paired t-test once we con distributed by a Chi-Square test for the goodness of fi t(p different variants of SyMSS method was done. It was found that all the SyMSS variants were signi t-test, p b 0.01) than the corresponding variants proposed by Mihalcea et al. [18] poor results that these similarity measures yielded when evaluated on the Li between these results and the results reported with the Li
As acknowledged by Corley and Mihalcea [44] and Islam and Inkpen [13] , sentence semantic similarity measures are a necessary step in the paraphrase recognition task, but are not always suf contains the following sentence pair: demonstrate the different nature of the two tasks. Thus, the evaluation of semantic similarity measures with human tool for comparing different measures. 5. W-SyMSS: weighted SyMSS
Wiemer-Hastings [11] points that human judges tend to ignore similarities between segments that are playing different to our method by weighting the different syntactic roles the way humans do.

The results reported by Wiemer-Hastings [11] show that, when computing semantic similarity between sentences, humans tend to give the highest importance to verbs. The subject an ddirectobjectareofgreatimportancealso,butthereisno we assigned the following weights to each syntactic role: Table 8 .
 information, W-SyMSS computed sentence similarity as follows:
Let us assume that sentence s 1 and s 2 are made of a subject, a verb, an object, and an adverbial complement whose h 1n and h 21 , ... ,h 2n respectively for sentence s 1 and s similarity between both sentences is calculated as shown by the formula above.

We evaluated this new approach of the proposed system, with the Microsoft Paraphrase Corpus, obtaining the results fi gure out which results differ signi fi cantly from each other.

The results showed that adding psychological plausibilit y to the method by weighting the different syntactic roles
JCN is statistically signi fi cant at the 0.05 level. These results supported the hypothesis repeated in this paper, stating that human judgments are affected to different extents by different syntactic roles. 6. Conclusions and future work
This paper has presented SyMSS, a syntax-based method to measure the semantic similarity between sentences or very
WordNet, which models common human knowledge about words i n natural language and allows different types of measures fi nds the phrases that make up the sentence and their syntacti c functions. With this information, the proposed method measures the semantic similarity between concepts that share the same syntactic function. method of Mihalcea et al. [18] .
 and adverbs were taken into account.

The proposed method has two main critical points: the syntact ic analysis and the word semantic similarity measure. So,
SyMSS variations using these two measures outperformed all the previous variations tested. Moreover, both variations outperformed the methods proposed by Li et al. [12] and Islam and Inkpen [13] intermsofrankcorrelation.Thisisan important than their similarity value.
 psychological plausibility to our method led to a statistically signi be necessary to optimize the weights used by the method.
 formula to overcome underestimation errors by taking into ac count the generality when both sentences present identical and not only de fi nitions, since the presence of the de fi proposed method to different NLP tasks, such as text summarization and conversational agents, to check the method's performance in different application domains.

References
