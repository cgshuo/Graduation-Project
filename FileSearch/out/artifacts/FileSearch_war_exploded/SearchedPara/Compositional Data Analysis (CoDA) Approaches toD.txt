 Many techniques in information retrieval produce counts from a sample, and it is common to analyse these counts as proportions of the whole X  X erm frequencies are a familiar example. Proportions carry only relative information and are not free to vary independently of one another: for the proportion of one term to increase, one or more others must decrease. These constraints are hallmarks of compositional data. While there has long been discussion in other fields of how such data should be analysed, to our knowledge, Com-positional Data Analysis (CoDA) has not been considered in IR.

In this work we explore compositional data in IR through the lens of distance measures, and demonstrate that com-mon measures, na  X   X ve to compositions, have some undesirable properties which can be avoided with composition-aware measures. As a practical example, these measures are shown to improve clustering.
 H.3.3 [ Information Storage and Retrieval ]: Information search and retrieval X  clustering, retrieval models . Aitchison X  X  distance; compositions; distance; similarity, ratio
Compositional data analysis (CoDA) deals with data that carry only relative information, such as proportions, per-centages, and probabilities [1]. Information is carried only in the ratios of different components. Examples include the chemical composition of rocks, the fraction of a certain DNA sequence in a sample, nutritional content of food, or employment data by industry. In text processing, compo-sitional data can include genres, documents, or probability distributions.
 Figure 1: The set of vectors with D positive components that sum to the constant  X  form the D -part simplex, denoted by S D [3]. From left to right, and with  X  = 1 , we see S 2 , S and S 4 .

In a compositional analysis the absolute size X  X ass of a rock, amount of DNA in the sample, amount of food, length of document vector X  X s not of interest. Instead, the interest is in the relative information, the proportions of the whole.
We can illustrate these ideas with a simple example: the content of soils. Suppose a 50 g soil sample is found to contain 38 g of sand; 10 g of silt; and 2 g of clay. This three-component datum, (38 , 10 , 2) , is the basis vector . This vector is sum-constrained: if we have less sand, we must have more silt and/or clay to make up the 50 g. Now, since we are interested in the relative information but not the size of the sample, it is easy to constrain (or  X  X lose X ) the basis vector to the equivalent (0 . 76 , 0 . 20 , 0 . 04). This is a composition : a vector of D parts, ( x 1 , . . . , x D ) , that sum to a constant  X  which, in this case, we choose to be 1.

Geometrically, compositions are points on a D -part simplex (Figure 1). CoDA theory, as pioneered by Aitchison [1] and developed by Pawlowsky-Glahn, Egozcue and others [2] is founded on logratio transformations that map the simplex be applied. If necessary, the results from these methods can be back-transformed to the simplex, secure in the knowledge that the constant-sum constraint will be satisfied.
Compositional data is common in IR (Section 2), but compositional data analysis is not. This paper aims to raise awareness of CoDA approaches, focusing especially on distances and dissimilarities common in IR and their compo-sitional counterparts (Sections 3 and 4). Section 5 explores how these distances perform in a document clustering task.
Compositions are common in information retrieval for various reasons. IR often deals with fixed-size samples: the genres of pages on a website, for example, or the terms in Figure 2: S 3 and the positive orthant of the hypersphere. Cosine similarity is unchanged whether we project onto the hypersphere or the simplex. 1000 scientific abstracts, where the sum is constrained to the size of the sample. We may have proportions, when the denominator is not known but the sum is constrained to 100% (or 1). Further, even individual documents or queries can be considered compositions. This can happen directly, if we are using probabilistic language models; or as an alternative representation, if we are using vector-space (geometric) models.
 If we count the number of in-and out-links for a web page, the sum is constrained to that page X  X  degree. If we count the number of nouns, verbs, or specific terms, in a 10,000-word sample, the sum is constrained to 10,000.

While observational count data does not carry purely rel-ative information 1 it is often modelled and analysed using compositional methods. (Expected counts, on the other hand, do carry only relative information but, in general, these are not known to us and have to be estimated.) distributions play important roles in modelling discrete data. Clearly, the vector of event probabilities in a multinomial probability distribution, and the vector of concentration parameters in a Dirichlet distributions are compositional, summing to one over the set of categories they describe.
In IR, multinomial and Dirichlet distributions are often used as the basis of probabilistic document models, topic and topic/term distributions, and Markov transition probabilities.
We note that CoDA offers a more flexible alternative to the Dirichlet distribution: the logistic normal , which allows dependencies between components to be expressed and opens up opens up the full range of linear modelling available for the multivariate Normal distribution in R D [2].
 points in vector-space where each dimension corresponds to a term, and the coordinate value represents the count of that term in the document. In IR, interest commonly lies in the relative abundance of different terms, rather than the actual counts observed since the former relates more strongly to the meaning and content of a document. For this reason, count vectors are generally normalised to a constant magnitude so that documents of different lengths become meaningfully comparable.  X  X wo heads and one tail X  in a coin toss experiment tell a different story than  X  X wo thousand heads and one thousand tails X , even though the ratio of heads and tails is the same in both instances.

While this has many of the hallmarks of compositional data, IR often uses cosine dissimilarity as a means to com-pare documents, an approach based on transforming the data to the positive orthant of the D -dimensional unit hy-persphere, rather than working within the D -dimensional simplex. Figure 2 illustrates that the cosine dissimilarity is unaffected by the magnitude of vectors, only their direction. However, as we shall see, distances on the hypersphere are constrained X  X here are limits to how different two documents can be in terms of cosine dissimilarity X  X hereas the logratio transformation used in CoDA enforces no such limits.
Transforming compositional data to the hypersphere is a concept that has been discussed and debated within CoDA; it appeals because it handles zero values simply but lacks the robust theoretical foundation of the logratio approach [7].
Conceptualising documents as points on the simplex opens up similarity and distance measures that are new to IR. As well as the familiar cosine and Euclidean measures, Aitchi-son X  X  (compositional) distance can be applied, as can Eu-clidean distance between log-transformed data and Kullback-Leibler divergence [5, 6]. In this section we will quickly review these distances and dissimilarities, then look at their behaviours and implications for information retrieval.
Distances are measured between two objects, here two documents. We will treat these as basis vectors X and Y , where each element X i and Y i is a count of terms. We will use x and y to refer to the same two documents, as compositions: i.e. x is a vector such that P i x i = 1.

In addition, we require all x i to be strictly positive to avoid the issue of dealing with zeros which, for approaches using logarithms or ratios, is a research topic in its own right [7]. Various language modelling approaches, including Bayesian methods, can ensure that there are no zeros in IR data [9]. measure similarity between two documents by the cosine of the angle between them. Cosine dis similarity can be defined: the shortest line segment connecting our two points, and is straightforward (note that here we use squared distance):
On the simplex d E is bounded by [0 , be at most is at a different  X  X orner X . In the case of high-dimensional data such as text, it is also clear that differences in any one component will count for little as they are  X  X rowned out X  by the large number of other components.
 ing with count data, especially counts which vary over several orders of magnitude, it is common to use a logarithmic trans-formation. This leads to a distance of the form: how the different distances vary as Y 1000 changes from 10 . The right-hand plot gives a zoomed-in view.
 Note that, unlike plain Euclidean distance, there is no upper limit to the distance between log-transformed components. can express the dissimilarity between two probability dis-tributions [4]. It is not strictly a distance metric, but is commonly used in information retrieval to compare two sets of normalised counts X  X hat is, to compare two compositions [8]. In its symmetric form it is written: Kullback-Leibler divergence depends on both the ratio of compositional components and their absolute values. ratios of components and can be written: normalised to lie on the unit hypersphere (as is often done in IR applications) then d 2 E ( X , Y ) = 2 d C ( X , Y ). This does not hold for vectors on the unit simplex .

By Napier X  X  inequality, d 2 E ( x , y ) &lt; d KL ( x , y ) &lt; d and, as shown by Lovell et al. [5], d 2 A ( x , y )  X  d 2 that both Aitchison and Euclidean-log distances are invariant to re-scaling (i.e., weighting) of components, such as by the inverse document frequency of terms.
Table 1 uses vectors with two components as a simple illus-tration of how various distances and dissimilarities behave. Vectors X and Y differ only in their first component by a factor of 2. Aitchison X  X  distance depends only on the ratios of components, that is X 1 : Y 1 and X 2 : Y 2 : these ratios are always the same here and so d A does not change. Other measures depend on both these ratios and the absolute values of each component, and vary from case to case.

Figure 3 shows further aspects of behaviour, based on the distances between two fictional documents. Here X has 999  X  X oise X  terms repeated 20 times each, and one rarer  X  X ignal X  term repeated ten times, that is X = (20 , 20 , . . . , 10). Y has the same first 999 X  X oise X  X erms, but the final X  X ignal X  X s varied from one occurrence up to ten million ( Y 1000  X  [1 , 10 7 the  X  X ignal X  grows large, both cosine and Euclidean distance saturate and do not change appreciably although counts vary over several orders of magnitude.

The right-hand graph illustrates another behaviour. As we halve or double the amount of the  X  X ignal X  term, from 5 through 10 to 20, intuitively it seems that distances should differ. However, the cosine, Euclidean, and Kullback-Leibler measures barely change: the less-important terms  X  X rown out X  the more-important one.

We have several apparently undesirable behaviours, dis-played by three of the five metrics at different times: (1) the distance between two documents which vary only in one component depends on the size of the other (nonvarying) components X  X ut these other components are presumably not interesting; (2) the distance between documents which vary only in one component depends on the number of dimen-sions; (3) the distance can saturate; (4) the distance is not sensitive to even quite large relative changes in the counts Figure 4: Cluster purity as number of clusters varies, using different distance measures to form the clusters. With this data, clusters formed using d EL , d A , or d KL are better than those formed with d E or d C . of uncommon terms, although these changes are presumably interesting.
Thus far, we have looked at the theoretical properties of distances and dissimilarities commonly used in IR and CoDA. This section explores their use in a typical IR task X  clustering X  X n which performance depends on both the distri-bution of the data to be clustered and the way in which we convert differences between multivariate data points into a univariate distance or dissimilarity. The aim here is not some kind of  X  X erformance shoot out X , rather than to see whether and how these measures affect distance-based analyses.
We explore the behaviour of the distances and dissimilari-ties presented so far by using them to cluster Usenet articles from the 20-newsgroups collection. These experiments used 50 articles from each of 20 groups, without headers such as Newsgroups: or Path: which would identify the group, and with no stemming or stopping. 2 Heirarchical clustering used Ward X  X  method, as implemented in R, and a tree was built using each of the distance measures above.

We report the quality of the clustering with purity. Purity for a clustering of 1000 documents is the mean proportion of documents in each cluster which are in the majority class: purity = 1 / 1000 P i max j | c i  X  g j | , where c i is the i th cluster and g j is the j th newsgroup. Purity ranges up to 1, and scores highest when each cluster consists entirely of documents from the same newsgroup.

Figure 4 plots purity when the heirarchies formed with each distance measure were cut to 20, 40, . . . 1000 clusters. At 1000 clusters, each cluster is a single document and purity must be 1; but at all other points, different distance measures produce clusterings of different quality. d C and d E produce very similar distances, and hence very similar clusterings; d EL and d A are also related, and d sits somewhere in between. Aitchison X  X  distance produces somewhat better clusters than the commonly-used cosine and Euclidean, with purity a relative 7% better than d C across all cuts and as much as 20% better when fewer clusters are
This is the  X  X ydate X  collection from Jason Rennie, http: //qwone.com/~jason/20Newsgroups/ . asked for. Clustering with d A never produces worse results than clustering with the standard measures.

Similar effects were seen with different numbers of docu-ments from the same collection, and using the Rand index instead of purity. We also saw similar effects building clusters with complete linkage, although no effect using single linkage.
CoDA is relevant to IR. It provides a well-founded theo-retical basis for analysing data on the relative abundance of document topics and terms, and gives a distance metric that depends solely on these relative abundances.

Our initial clustering experiment suggests logarithm-and ratio-based distances ( d A , d EL , d KL ) warrant further investi-gation as alternatives to distances on the hypersphere ( d d ).
 Our hope is that this paper will encourage exploration of CoDA methods on other IR data sets and tasks, including ranking. We note that both d A and d EL are invariant to re-scaling of components, such as weighting by inverse docu-ment frequency. Our suspicion is that idf-weighting will not improve d E and d C to the point that they surpass logarithm-and ratio-based distances in clustering performance, but this too demands further experiments to verify. [1] J. Aitchison. The Statistical Analysis of Compositional [2] J. Bacon-Shone. A short history of compositional data [3] J. J. Egozcue and V. Pawlowsky-Glahn. Basic concepts [4] S. Kullback and R. A. Leibler. On information and [5] D. Lovell, W. M  X  uller, J. Taylor, A. Zwart, and [6] J. A. Mart  X  X n-Fern  X andez, C. Barcel  X o-Vidal, and [7] J. A. Mart  X  X n-Fern  X andez, J. Palarea-Albaladejo, and [8] J. Xu and W. B. Croft. Cluster-based language models [9] C. Zhai. Statistical Language Models for Information
