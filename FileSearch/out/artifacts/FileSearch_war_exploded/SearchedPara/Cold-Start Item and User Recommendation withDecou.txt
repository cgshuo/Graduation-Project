 A major challenge in collaborative filtering based recom-mender systems is how to provide recommendations when rating data is sparse or entirely missing for a subset of users or items, commonly known as the cold-start problem. In recent years, there has been considerable interest in devel-oping new solutions that address the cold-start problem. These solutions are mainly based on the idea of exploit-ing other sources of information to compensate for the lack of rating data. In this paper, we propose a novel algorith-mic framework based on matrix factorization that simulta-neously exploits the similarity information among users and items to alleviate the cold-start problem. In contrast to ex-isting methods, the proposed algorithm decouples the follow-ing two aspects of the cold-start problem: (a) the completion of a rating sub-matrix, which is generated by excluding cold-start users and items from the original rating matrix; and (b) the transduction of knowledge from existing ratings to cold-start items/users using side information. This crucial difference significantly boosts the performance when appro-priate side information is incorporated. We provide theo-retical guarantees on the estimation error of the proposed two-stage algorithm based on the richness of similarity in-formation in capturing the rating data. To the best of our knowledge, this is the first algorithm that addresses the cold-start problem with provable guarantees. We also conduct thorough experiments on synthetic and real datasets that demonstrate the effectiveness of the proposed algorithm and highlights the usefulness of auxiliary information in dealing with both cold-start users and items.
 H.3.3 [ Information Search and Retrieval ]: Miscella-neous Matrix factorization; Sparsity; Cold-start Problem c  X 
The ever-growing popularity of e-commerce sites and ex-ponential growth of online users and huge collections of items have created a compelling demand for efficient recommender systems to guide users toward items (e.g., products, books, movies, etc). In the past decade, there has been a vast amount of research in this field, mostly focusing on design-ing novel algorithms for generating high quality suggestions (see e.g. [1], for a more recent survey). For instance, Collab-orative filtering (CF) aims to predict preferences of a given  X  X ser X  for some  X  X bjects X  based on her previously revealed preferences X  typically in the form of purchases or ratings X  as well as the revealed preferences of other users.
Despite significant improvements, recommendation sys-tems suffer from a few inherent limitations and weaknesses that need to be addressed and have captured the attention of researchers. Among such limitations are data sparsity and cold-start problems. For instance, for many applications the rating data is usually very sparse or there is no available rat-ing for a subset of users or items (known as cold-start user and cold-start item, respectively) X  making standard collab-orative filtering algorithms infeasible. Therefore, it is of sig-nificant interest to develop algorithms that alleviate the data sparsity challenge and solve the cold-start problem.
Due in part to its importance, there has been an active line of work to address difficulties associated with cold-start users and items, and several researchers have considered a variety of techniques [16, 22, 21, 24, 27, 8, 26, 28, 14, 17], where a common theme among them is to exploit auxil-iary information about users or items besides the rating data that are usually available. The main motivation be-hind these techniques stems from the following observation: other sources of data could potentially provide useful infor-mation about the underlying model and complement the rat-ing data. For instance, users X  decisions on item ratings are very likely to be influenced by other users with whom they have social connections (friends, families, etc). As a result, the availability of auxiliary information such as users X  profile information [2], social context (trust and distrust relations) of users [4], information embedded in the review text [8], and features of items [5] would definitely benefit the rec-ommendations. Therefore, an interesting research question, which is the main focus of this paper is: how to effectively incorporate the side information about users and items to overcome the cold-start problem .

The aim of this paper is to propose an efficient matrix factorization algorithm with similarity information to an-swer the above question. In fact, we study a somewhat more general problem where both cold-start users and cold-start items are present simultaneously and address these two challenges in combination. The dominant paradigm in ex-isting methods such as shared subspace learning [22] and kernelized matrix factorization [28] is to augment the exist-ing methods with side information and perform (a) the com-pletion of rating matrix and (b) the transduction of knowl-edge from existing ratings to cold-start items/users simul-taneously. While this recipe can generate good results in practice, it has a notable drawback: these methods prop-agate the completion and transduction errors repetitively and in an uncontrolled way. Therefore, we propose a two-stage algorithm that decouples completion from transduc-tion. In particular, in the first step we exclude the cold-start users and items from the rating matrix and recover the obtained sub-matrix in a perfect way. Then, the recov-ered sub-matrix along with available similarity side infor-mation about users and items will be utilized to transduct the knowledge to cold-start users/items. We provide the-oretical performance guarantees on the estimation error of the proposed algorithm while most of the existing methods do not provide any theoretical support. We also conduct a set of through experiments to demonstrate the merits of the proposed algorithm and complement our theoretical re-sults. Finally, we compare our algorithm with several other specialized algorithms for cold-start user and item recom-mendation to demonstrates the superiority of our proposed framework over existing methods.

Outline. The remainder of the paper is organized as fol-lows. In Section 2, we begin by establishing notation and providing background on matrix factorization, noting a few of its deficiencies for our setting. We then in Section 3 de-scribe the proposed algorithm and state our main theorem on its estimation error. The experimental results are pro-vided in Section 4 and concluding remarks are given in Sec-tion 5.
Before proceeding to the proposed algorithm, in this sec-tion we establish the notations which are used throughout the paper and formally describe our problem setting. The set of non-negative real numbers is denoted by R + . We use [ n ] to denote a set on integers { 1 , 2 ,  X  X  X  ,n } . Scalars are denoted by lower case letters and vectors by bold face lower case letters such as u . We use bold face upper case letters such as M to denote matrices. The Frobenius and spectral norms of a matrix M  X  R n  X  m is denoted by k M k The transpose of a vector and a matrix denoted by m and M &gt; , respectively. We use ( M )  X  to denote the Moore-Penrose pseudo inverse of matrix M . The dot product be-tween two vectors m and n is denoted by m &gt; n .

In collaborative filtering we assume that there is a set of n users U = { u 1 ,  X  X  X  ,u n } and a set of m items I = { i ,  X  X  X  ,i m } where each user u i expresses opinions about a subset of items. In this paper, we assume opinions are ex-pressed through an explicit numeric rating (e.g., scale from one to five), but other rating methods such as hyperlink clicks are possible as well. We are mainly interested in rec-ommending a set of items for an active user such that the user has not rated these items before. The rating informa-tion is summarized in an n  X  m matrix R  X  R n  X  m , 1  X  i  X  n, 1  X  j  X  m where the rows correspond to the users and the columns correspond to the items and ( p,q )th entry is the rate given by user u p to the item i q . We note that the rating matrix is partially observed and it is sparse in most cases.

An efficient and effective approach for recommender sys-tems is to factorize the user-item rating matrix R by a mul-Figure 1: In our setting we assume that there are many fully unobserved rows and columns in the in-put rating matrix R  X  R n  X  m and only a small sub-matrix M  X  R p  X  q has enough information to be re-covered by the off-the-shelf methods such as matrix factorization or completion. Besides, the similarity matrices A  X  R n  X  n and B  X  R m  X  m about users and items are provided, respectively, that could poten-tially benefit the recommendation. tiplicative of k -rank matrices R  X  UV &gt; , where U  X  R and V  X  R m  X  k utilize the factorized user-specific and item-specific matrices, respectively, to make further missing data prediction. There are two basic formulations to solve this problem: these are optimization approaches(see e.g., [19, 9, 11, 7]) and probabilistic approach [15]. Let  X  R be the set of observed ratings in the user-item matrix R  X  R n  X  m , i.e.,  X 
R = { ( i,j )  X  [ n ]  X  [ m ] : R ij has been observed } , where n is the number of users and m is the number of items to be rated. In optimization based matrix factorization, the goal is to learn the latent matrices U and V by solving the following optimization problem: The optimization problem in Eq. (1) has three main terms: the first term aims to minimize the inconsistency between the observed entries and their corresponding values obtained by the factorized matrices. The last two terms regularize the latent matrices for users and items, respectively. The parameters  X  U and  X  V are regularization parameters that are introduced to control the regularization of latent matri-ces U and V , respectively. After learning the latent features for users and items, the prediction of each missing entry can be computed by the inner product of latent vectors of the corresponding row and the corresponding column.

For new users or items in the system, since there is no ob-served rating data in R , the corresponding rows and columns in the rating matrix is fully unobserved which results in an inability to draw inferences to either recommend existing items to new users or new items to existing users. In par-ticular, it is not feasible for standard Matrix Factorization formulation to fill in a row or a column that is entirely miss-ing in the original rating matrix. In this paper, we assume that there is a sub-matrix M  X  R p  X  q , 1  X  p  X  n, 1  X  q  X  m which includes enough rating data to be fully recovered via standard methods such as matrix factorization or matrix completion. We call the rest of items and users for which the rating data is entirely missing and are not present in M as cold-start. To make recommendations to cold-start users/items we assume that besides the observed entries in the matrix M , there exist two auxiliary similarity matri-ces A  X  R n  X  n and B  X  R m  X  m that capture the pairwise similarity between users and items, respectively as shown in Figure 1. The similarity matrices can be computed from the available information such as users X  profile or social con-text or items X  features. The main focus of this paper is on exploiting available side information to improve the accu-racy of recommendations and resolve the cold-start items and users problems.
We now focus on describing our algorithm and the as-sumptions underlying it. We assume that the rating matrix and the side information matrices are correlated and to some extent. This will be formalized later to share the same latent information; that is, the row and column vectors in R share an underlying subspace spanned by the leading eigen-vectors in the similarity matrices A and B , respectively. This as-sumption follows from the fact that the similarity matrices provide auxiliary information about the users/items, other-wise there would not be any chance to benefit from these information in addressing the cold-start problems.

Before delving into the algorithm, we discuss two alterna-tive methods to exploit the similarity information in matrix factorization and motivate our own algorithm. A straight-forward approach to exploit and transfer knowledge from similarity matrices to the rating data is to cast the problem as a shared subspace learning framework based on a joint matrix factorization to jointly learn a common subset of ba-sis vectors for the rating matrix R and the corresponding similarity matrices A and B for users and items. In partic-ular, the goal is is to factorize the matrices R and A with equal number of rows and the matrices R and B with equal number of columns into four subspaces: two are shared be-tween rating matrix and each of two similarity matrices, and two are specific to the matrices as formulated in the follow-ing optimization problems: with  X  as the regularization parameter for the norms of the solution matrices and common latent space representation is achieved by using the same matrices W  X  R n  X  r and Z .
An alternative approach is to kernelize the matrix factor-ization by assessing the similarity of latent features using the available similarity matrices [28]. In particular, instead of directly solving the optimization problem in Eq. (1), we optimize the following objective where the similarity matrices are explicitly used to regularize the latent features.

However, in our setting there are many rows and columns in the rating matrix R that are fully unobserved, which makes the subspace learning and kernelized factorization methods inapplicable. The main issue with these approaches is that the completion of the unobserved entries in rating ma-trix R and transduction of knowledge from these entries to cold-start users/items via similarity matrices is carried out simultaneously. Therefore, the completion and transduction errors are propagated repetitively and in a uncontrolled way. The issue with error propagation becomes even worse due to Algorithm 1 Matrix Factorization with Decoupled Com-pletion and Transduction (DCT) 1: Input: 2: Complete the sub-matrix M to get c M [Completion ] 3: Decompose c M as c M = P r i =1 b u i b v &gt; i 4: Extract subspaces U A and U B by spectral clustering 5: Compute b a i = b U &gt; A b U A 6: Compute b b i = b U &gt; B b U B 7: Compute b R = U A P r i =1 b a i b b &gt; i U &gt; B [Transduction ] 8: Output: b R the non-convexity of optimization problems in Eq. (2) and Eq. (3) X  jointly in parameters U and V .

In effort to alleviate this difficulty, in this section we pro-pose an alternative approach that diverges from these al-gorithms and transfers information from similarity matrices to rating matrix via the fully recoverable sub-matrix M . In particular, the proposed algorithm decouples the com-pletion from transduction and constitutes of two stages: (i) completion of the sub-matrix M which can be done perfectly with zero completion error, and (ii) transduction of rat-ing data from the recovered sub-matrix to cold-start items and users using similarity matrices. This crucial difference greatly boosts the performance of the proposed algorithm when appropriate side information is incorporated.

With our assumption concerning the correlation of rating and similarity matrices in place, we can now describe our algorithm. To this end, we construct an orthogonal matrix U
A = [ u A 1 ,  X  X  X  , u A s ]  X  R n  X  s whose column space subsumes the row space of the rating matrix. We also construct an-other orthogonal matrix U B = [ u B 1 ,  X  X  X  , u B s ]  X  R column space subsumes the column space of the rating ma-trix. To construct subspaces U A and U B we use the first s eigen-vectors corresponding to the s largest eigen-values of the provided similarity matrices A and B , respectively. We note that the extent to which the extracted subspaces U
A and U B from similarity matrices subsume the corre-sponding row and column spaces of the rating matrix, de-pends on the richness of the similarity information. To formalize this, first from the low-rank assumption of the rating matrix R , it follows that it can be decomposed as R = P r i =1 u i v &gt; i where r is the rank of the matrix. Now we note that the i th latent features vector u i can be decom-posed in a unique way into two parallel and orthogonal parts as u i = u k i + u  X  i , where u k i is the part that is spanned by the subspace U A extracted from the similarity information about users and u  X  i is the part orthogonal to U A .
In a similar way, for the latent features vector of j th item, i.e., v j , we have the following similar decomposition as v v + v  X  i , where v k i is the part that is spanned by the subspace U
B extracted from the similarity information about users and v  X  i is the part orthogonal to U B . We note that the orthogonal components of left and singular vectors u  X  i and v i captures the extent to which the similarity matrices do not provide information about rating data and can not be recovered using these auxiliary information.
To build intuition for the algorithm we propose, we first relate the rating matrix to the similarity matrices. Having decomposed the latent features as above into two parallel and orthogonal components, we can rewrite the rating ma-trix R as:
R = where R  X  is the part of the rating matrix that is fully spanned by the subspaces U A and U B , the matrix R L the part where only the left singular vectors are spanned by U A and the right singular vectors are orthogonal to the subspace spanned by U B , and the matrix R R is the part that the left singular vectors are orthogonal to the subspace spanned by U A and the right singular vectors are spanned by U B . Finally, the matrix R E is the error matrix where both left and singular vectors are orthogonal to the sub-spaces spanned by U A and U B , respectively, which does not benefit form the side information at all. In particular, the error matrix R E can not be recovered from the side in-formation as the extracted subspaces provide no information about the orthogonal parts u  X  i and v  X  i of the singular vec-tors. Therefore, the error contributed by this matrix into the estimation error of final recovered rating matrix is un-avoidable.

In the following subsections, we first devise an effective method to recover the rating matrix R from the sub-matrix M and subspaces U A and U B , and then provide theoretical guarantees on the estimation error in terms of the magnitude of the error matrix k R E k F .
 The Completion Stage. The first step in Algorithm 1 is to complete the sub-matrix M to create the fully recovered matrix c M . To do so, we use the matrix factorization formu-lation in Eq. (1) which has achieved great success and popu-larity among the existing matrix completion techniques [25, 15, 23]. We note that based on matrix completion theory, it is guaranteed that the recovery of the low-rank matrix M is perfect provided the number of observed entries is suffi-ciently high.
 The Transduction Stage. We now turn to recovering the matrix R = P r i =1 u i v &gt; i from the submatrix c M and the sub-spaces U A and U B extracted from the similarity matrices A and B about users and items, respectively. The detailed steps of the proposed completion algorithm are shown in Algorithm 1.

In the second step, the rating information in the recov-ered matrix c M is transducted to the cold-start users and items. To motivate the transduction step, let us focus on the R  X  matrix as defined in Eq. (4). Since u k i and v k fully spanned by the subspaces U A and U B following our construction above, we can write them as: where a i  X  R s ,i = 1 , 2 ,  X  X  X  ,r and b i  X  R s ,i = 1 , 2 ,  X  X  X  ,r are the orthogonal projection of the singular vectors to the corresponding subspaces. By substituting the equations in Eq. (5) into the decomposition of R  X  we get: R From above derivation, we observe that the key to re-cover the matrix R  X  is to estimate the vectors a i 1 , 2 ,  X  X  X  ,r . In the following we show how the recovered rat-ing sub-matrix c M along with the subspaces extracted from the similarity matrices can be utilized to estimate these vec-tors perfectly under some mild condition on the number of cold-start users and items. To this end, first consider the decomposition of the recovered matrix as c M = P r i =1 b The estimation of vectors a i , b i ,i = 1 , 2 ,  X  X  X  ,r in Eq. (6) and equivalently the matrix R  X  is as follows. First, let b U
A  X  R p  X  s be a random submatrix of U A where the sam-pled rows correspond to the subset of rows in the matrix c M . Similarly, we construct a submatrix of U B denoted by b U
B  X  R q  X  s by sampling the rows of U B corresponding to the columns in c M . An estimation of a i , b i ,i  X  [ r ] vectors is obtained by orthogonal projection of left and right singular vectors of c M onto the sampled subspaces b U A and b U B solving following optimization problems: Then, we estimate the R  X  by: b
R where in the last equality we used the fact that b lutions to the ordinary least squares regression problems in Eq. (7). Here (  X  )  X  denotes the Moore-Penrose pseudo inverse of a matrix. The final estimated rating matrix b R is simply set to be b R = b R  X  .
 An upper bound on estimation error. In order to see the impact of similarity information on recovering the rat-ing matrix, we theoretically analyze the estimation error. In particular, the performance of proposed algorithm on esti-mating the rating matrix is stated in the following theorem.
Theorem 3.1. Let R  X  R n  X  m be a low-rank matrix with coherence parameter  X  . Let M  X  R p  X  q be a sub-matrix of R where the rows and columns are uniformly sampled with where r is the rank of the original matrix R . Let recovered matrix by Algorithm 1 using similarity matrices A and B about users and items, respectively. Then with probability 1  X   X  , it holds: We note that in above inequality the parameter  X  is known as incoherence [3, 18] which is the prevailing assumption in analysis of matrix completion algorithms. This parameter states that the singular vectors of R should be de-localized so that they have a small inner product with the standard basis making a full recovery possible.

Remark 3.2. Let us pause to make some remarks con-cerning the results given above. First, one can observe that the recovery error is stated in terms of the norm of error matrix R E which captures the extent to which the similar-ity matrices A and B fail to capture the rating data. This error is unavoidable even if there is no cold-start item or users, p = n and q = m which yields O (1) k R E k F error bound. Also, the error decreases by reducing the number of cold-start items and users as expected.
In this section, we conduct exhaustive experiments to demonstrate the merits and advantages of the proposed al-gorithm. We conduct the experiments on synthetic and two well-known NIPS 1 and MovieLens 2 datasets, aiming to ac-complish and answer the following fundamental questions: In the following subsections, we intend to answer these ques-tions. First we introduce the datasets that we use in our ex-periments and then the metrics that we employ to evaluate the results, followed by our detailed experimental results on both synthetic and real datasets.
We adopt the widely used the Mean Absolute Error (MAE) and the Root Mean Squared Error (RMSE) [6] met-rics for prediction accuracy [6, 13]. Let T denote the set of ratings that we want to predict, i.e., T = { ( i,j )  X  [ n ]  X  [ m ] , R ij needs to be predicted } and let b R denote the prediction matrix obtained by a recommendation algorithm. Then, The RMSE metric is defined as: We would like to emphasize that even small improvements in RMSE are considered valuable in the context of recom-mender systems. For example, the Netflix prize competition offered a $1,000,000 reward for only 10% reduction of RMSE.
To measure the effectiveness of the order of the recom-mended users/items based on their predicted rating, we em-ploy Mean Average Precision (MAP) [12] and Normalized Discount Cumulative Gain (NDCG) measures. Given an item i , let r i be the relevance score of the item ranked at position i , where r i = 1 if the item is relevant to the i and http://www.cs.nyu.edu/~roweis/data.html http://www.grouplens.org/node/73/ r = 0 otherwise. Then we can compute the Average Preci-sion (AP) as where Precision is defined as the number of relevant items (users) divided by the number of items (users) in the dataset. MAP is the average of AP over all the users in the network. The NDCG measure is a normalization of the Discounted Cumulative Gain (DCG) measure. DCG is a weighted sum of the degree of relevancy of the ranked users. The value of NDCG is between [0 , 1] and at position k is defined as: where k is also called the scope, which means the number of top-ranked items presented to the user and Z k is chosen such that the perfect ranking has a NDCG value of 1. MovieLens dataset. As our first dataset, we use the well known MovieLens dataset, which has side information about both users and items that makes it ideal for our setting. This dataset consists of 1000K ratings made by 6040 users for 3706 movies. In addition to rating data, this dataset also contains features for both users and movies. For each movie we used features such as title, year, genre, etc. For each user we extracted features such as gender, age, occupation, and location. The statistics of the dataset is given in Table 4.1. NIPS dataset. We have also applied our algorithm to paper-author and paper-word matrices extracted from the co-author network at the NIPS conference [20]. This dataset has only side information about items. The number of users (i.e., authors) and items (i.e., papers) are 2037 and 1740, respectively. The length of feature vector of each paper is 13649, which is the size of the vocabulary in NIPS (i.e., unique tokens in all articles). The content of the papers is preprocessed such that all words are converted to lower case and stemmed and stop-words are removed. We compute the cosine similarity of the vector representation weighted with TD-IDF of papers with the ones of all other papers. Experiments setup. We separated the dataset into two subsets, a training set that used for training the model and a test set that used for evaluating it. We partition the data into 5 equal subsets and then use one for testing and the rest for training, giving a ratio of testing to training data 1:4, and we repeat the experiment for each of the partitions.
To better evaluate the effect of utilizing the features of users and items in dealing with cold-start problem, we em-ploy different cold-start scenarios.
To evaluate the performance of our proposed DCT algo-rithm, we consider a variety baseline approaches. The base-line algorithms are chosen from three types of categories: (i) those that can only deal with cold-start users, (ii) ones that can deal with cold-start users, and (iii) algorithms that are capable of dealing with both cold-start items and users. In particular, we consider the following basic algorithms:
In this section we present the results of our proposed algorithm on one synthetic dataset. We generated a syn-Figure 2: The RMSE of test data for different p and q values thetic dataset to evaluate our approach before moving for-ward to real datasets. First we generate two matrices U  X  [0 , 1] 4000  X  r and V  X  [0 , 1] 2000  X  r . Then by using U and V we generated a rating matrix R 4000  X  2000 = UV that includes 4000 users and 2000 items. Then we gener-ated a similarity matrix A 4000  X  4000 = UU &gt; for users and a similarity matrix B 2000  X  2000 = VV &gt; for items. Then we added random noise to the all elements of A and B where the noise follows a Gaussian distribution N (0 , 0 . 5). We con-sider A and B as two similarity matrices between users and items, respectably.

In this study we tried different values for p within the range of 100 to 2000 with step-size of 100 and different values for q within the range of 100 to 1000 with same step-size. To generate cold-start users and items in our dataset, we divided the dataset into 4 partitions. We divided users into two groups of p existing users and n  X  p cold-start users. We also selected q items as existing items and m  X  q as cold-start items. Hence Partitions 1 and 2 have p users and partitions 3 and 4 have n  X  p cold-start users; partitions 1 and 3 have q items and partitions 2 and 4 have m  X  q cold-start items. The statistics of the dataset is given in Table 2. The RMSE of test data for different combinations of p and q values is shown in Figure 2. It shows that by increasing p and q , the RMSE decreases. Hence, the fewer unobserved elements we have, the lower the error is. Considering the fact that our theoretical upper bound of error decreases by increasing p m or q n , this observation is completely consistent with our theoretical upper bound.

For our synthetic dataset, we added noise to similarity matrices that follows a zero mean Gaussian distribution with variance of 0.5. To observe the effects of noise variance, we changed the variance from 0 . 1 to 1 with step size of 0 . 1and calculated the accuracy of our algorithm. As Figure 3 shows, by increasing the noise variance, both RMSE and MAE of the results on test dataset increase.
 Table 3: Cold-start users on MovieLens dataset Figure 3: RMSE and MAE of test dataset for dif-ferent noise variances Table 4: Cold-start items and cold-start users on MovieLens
To simulate cold-start user scenario, we divide the users into two training and test disjoint subsets. We use 80% of the users for training and the remaining 20% of users for testing. Then we apply baseline algorithms to predict ratings of cold-start users for different items and evaluate the results using the ground truth, which is the complete rating matrix.

We compared our proposed approach to five alternative recommendation methods: RS, MP, KMF-U and two vari-ations of the LCE algorithm. Since NIPS dataset does not have any information (features) about users, for testing this scenario we only apply these methods on MovieLens dataset. Table 3 shows the average performance of each method across the MovieLens dataset for the first scenario.
Considering the RMSE measure, we observed that DCT and KMF-U behave similarly and outperform others algo-rithms. DCT has also the highest MAP and outperforms others on MovieLens dataset. The LCE-U performs better than MP and the differences between two version of LCE are not significant, but always LCE performs better than LCE-NL. As expected, RS is the least accurate recommender.
To simulate cold-start item scenario, we divide the items into two training and test disjoint subsets. We use 80% of the items for training and the remaining 20% of items for testing. We compared our algorithm to four alternative recommendation methods: CBF, KMF-I, LCE and LCE-NL on NIPS and MovieLens datasets. In the scenarios that we have cold-start items, since there is no historical ratings, MP cannot work. Table 5 shows the average performance Table 5: Experiments with cold-start items on MovieLens and NIPS datasets.
 of each method across the MovieLens and NIPS datasets for cold-start item scenario.

In terms of NDCG, MAP and RMSE measures, DCT outperforms all other approaches on MovieLens and NIPS datasets. Only the MAE of KMF-I is lower than DCT on MovieLensIn. Based on the evaluation measure one can conclude that DCT generally outperforms other approaches. We also observed that DCT works better for cold-start items than cold-start users on MovieLens; and that is because fea-tures of movies are much more richer than those of users in MovieLens dataset.
In this scenario, since there is not any historical informa-tion for nor users neither items, we can only compare our algorithm with RS and KMF on MovieLens. Table 4 shows the results of applying these three algorithms on MovieLens. It is shown that DCT outperforms both RS and KMF in all measures. Hence, DCT is a novel algorithm that can deal with all three types of cold-start problems. We would like to note that DCT is able to predict an initial guess of an item popularity for cold-start items in online recommenders.
The results of Tables 3, 4 and 5 reveal the following findings: 1) RS generally performs significantly worse than the other algorithms in all scenarios for all datasets, which con-firms that it is necessary to carefully use similarity informa-tion of users and items to have a more accurate recommen-dations. 2) The DCT outperformed all other methods by a signif-icant margin in almost all scenarios and datasets except for the cold-start users on the NIPS dataset, in which it attained similar performance as KMF-U. 4) The results for MovieLens are generally better than the results for NIPS for all methods, possibly because of the fact that the items in MovieLens have more and richer features, while the items in NIPS are all papers with just keywords. Another explanation for the better results on MovieLens could be that MovieLens is a richer dataset than NIPS since there are more features and ratings per user/item.
In this paper, we proposed a novel factorization model to explicitly exploit similarity information about users and items to alleviate corresponding cold-start problems. In contrast to exiting methods such as subspace sharing and kernelized factorization methods, in the proposed method the completion of unobserved ratings and transduction of knowledge to cold-start items/users is decoupled. In par-ticular, we first perform a full recovery of the sub-matrix obtained by excluding cold-start items and users, and then exploit the similarity matrices to transduct recovered ratings to cold-start users/items. The performance of the proposed algorithm is theoretically analyzed and empirically verified on synthetic and real datasets. Our results demonstrated that the proposed decoupling idea significantly improves the quality of the recommendations and alleviates the cold-start problem when rich side information about users and items is provided.
This work was supported in part by the National Sci-ence Foundation under Awards IIS-096849, CCF-1117709, and 1331852. The authors would like to thank the anony-mous reviewers for their insightful comments and sugges-tions. We also acknowledge valuable technical discussions with Mehrdad Mahdavi and Dennis Ross during this work. [1] Gediminas Adomavicius and Alexander Tuzhilin.
 [2] Gediminas Adomavicius and Alexander Tuzhilin.
 [3] Jian-Feng Cai, Emmanuel J Cand`es, and Zuowei Shen. [4] Rana Forsati, Mehrdad Mahdavi, Mehrnoush [5] Zeno Gantner, Lucas Drumond, Christoph [6] Jonathan L Herlocker, Joseph A Konstan, Loren G [7] Yehuda Koren, Robert Bell, and Chris Volinsky. [8] Guang Ling, Michael R Lyu, and Irwin King. Ratings [9] Juntao Liu, Caihua Wu, and Wenyu Liu. Bayesian [10] Nathan N Liu, Xiangrui Meng, Chao Liu, and Qiang [11] Hao Ma, Haixuan Yang, Michael R Lyu, and Irwin [12] Christopher D Manning, Prabhakar Raghavan, and [13] Paolo Massa and Paolo Avesani. Trust-aware [14] Aditya Krishna Menon, Krishna-Prasad Chitrapura, [15] Andriy Mnih and Ruslan Salakhutdinov. Probabilistic [16] Seung-Taek Park and Wei Chu. Pairwise preference [17] Ian Porteous, Arthur U Asuncion, and Max Welling. [18] Benjamin Recht. A simpler approach to matrix [19] Jasson DM Rennie and Nathan Srebro. Fast maximum [20] S Roweis. Nips dataset (2002). URL http://www. cs. [21] Laila Safoury and Akram Salah. Exploiting user [22] Martin Saveski and Amin Mantrach. Item cold-start [23] Hanhuai Shan and Arindam Banerjee. Generalized [24] Le Hoang Son. Dealing with the new user cold-start [25] Nathan Srebro, Jason Rennie, and Tommi S Jaakkola. [26] Michele Trevisiol, Luca Maria Aiello, Rossano [27] Ke Zhou, Shuang-Hong Yang, and Hongyuan Zha.
 [28] Tinghui Zhou, Hanhuai Shan, Arindam Banerjee, and
