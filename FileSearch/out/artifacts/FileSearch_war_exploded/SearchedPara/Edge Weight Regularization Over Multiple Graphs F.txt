
Nowadays, relational data are universal and have a broad appeal in many different application domains.The recent upsurge in the amount of text available due to the widespread growth of the Internet has led to the need for large scale, efficient Natural Language Processing (NLP), Information Retrieval (IR) tools for text mining. At the heart of many of the NLP, IR algorithms is the need for a good similarity measure. For example, many of the algorithms for Document Clustering, Word Sense Disambiguation, Document classifi-cation, etc make use of similarity measures.

Similarity graphs are estimated from data and may not accurately reflect the semantic relationship between the objects. It is trivially clear that the more accurate the similarity graph reflects the true semantic relationship better the performance of tasks like classification, clustering and other data mining tasks.

In the past, there was a huge advantage of the text being homogeneous (pure text) and hence it was easier to compute similarity. For example, there exists many different similarity measures for text [18], [12], [13]. But currently, almost all data can be represented using much more than text. For example, publications have many heterogeneous features like text, citations, authorship information, publication venue information, etc. But there are very few similarity measures that use all the different relations together . In most cases, similarity is computed using just one feature type or simi-larity is computed using two feature types individually and then combined in a linear weighted fashion.

In the case of publications, text is the most commonly used feature for computing similarity. Recently, there has been a lot of interest in using all the different feature types to compute similarity. For example, [2] use both citations and text to compute similarity between publications . They compute textual similarity using cosine similarity. Two papers are considered to be similar if one paper cites the other. They combine the similarity between the two paper due to citation (CS) and text (TS) in a linear fashion as follows where,  X   X  1 and S ( x, y ) represents the combined similarity between publications x and y .

This formulation clearly does not take into account any dependency between the different feature spaces (in this case, text and citation space).

Also, even in the case of the same feature space, higher order dependencies are not taken into account. For example, let publication X contain three words { w 1 ,w 2 } and pub-lication Y contain { w 2 ,w 3 } then this indicates that words w 1 and w 3 are similar to each other and should affect the similarity between two other publications containing them.
Nevertheless, [20] takes these dependencies into account and represents each document in the classical vector space model and they use a Naive Bayes classifier to classify text documents. The strength of the classifier lies in the augmented representation of the documents. The new repre-sentation includes the words present in the document as well as words which are present in higher order dependencies. For example, in the classical vector space model, publication X  X  X  vector representation would include only w 1 and w 2 but not w 3 . Whereas, in the new representation all the words are weighted by the number of higher order paths which contain the word. They demonstrate that including higher order dependencies improves classification accuracy and most of the information required for classification is contained in second order dependencies. [21] gives a mathematical proof for the dependence of Latent Semantic Indexing (LSI)[22] on higher order depen-dencies. That is, if two documents X and Y have an initial similarity of zero and if the similarity becomes non-zero after using LSI, then it implies that there exists a higher order dependency between the two documents. Higher order co-occurrences have also been used earlier in Word Sense Disambiguation [23] and stemming [24]
Although, there has been some work on using higher order dependencies it is not easily extended for multiple feature spaces, especially for heterogeneous feature types like citations, text, etc. In the case of using citations and text, it is not immediately clear how to model higher order dependencies when using multiple feature types. Also, another difficulty in extracting higher order dependencies is in including more than one feature space.

One possible approach to use all feature types is to make a big graph using all the features in the same graph. For example, in the case of publications, the set of nodes can include the papers and keywords. The edges could represent citations between papers and the relation between papers and keywords. This approach is equivalent to merging multiple classifiers (each using a different feature type/view) into a single classifier using all the feature types which is not recommended. There could be data points where each of the classifiers are  X  X uthoritative X  on but by merging them this confidence is diluted. Also, [27] proves that a classifier has low generalization error if it agrees on unlabeled data with a second classifier based on different  X  X iew X  of the data. This justifies the search for multiple feature types to be represented separately.

Our work is also quite similar to co-training [26] in the sense of using multiple views (feature types). Also, [25] proposes a greedy algorithm to search for classifiers which have provable advantages over the classifiers found using the co-training algorithm. The greedy algorithm searches for complex classifiers which are essentially a list of atomic rules H , each assigning a label l based on the presence of a single feature h . However, even this approach does not take care of dependencies across feature types and higher-order dependencies between the same feature type. Our algorithm seeks to estimate improved similarity measures by integrating all available sources of similarity.
We seek to motivate the need for an unified similarity measure using an example. In this example, we consider only two different feature types, but it naturally extends to more than two feature types. Consider the problem of classification of publications by research area. Let there be three publications, P 1 , P 2 , P 3 in the area of Machine Translation. Let the publications contain two keywords, k 1 =  X  Statistical Machine Translation X  and k 2 =  X  X ilingual Corpora X . Specifically, publications P 1 contains k 1 , P P 3 contain k 2 . Although the two keywords are enough for a human to clearly see that the two papers are on Machine Translation, no textual similarity measure (without using external knowledge sources) will assign a non-zero similarity value between any pair of publications. In general, this is a very commonly occurring problem in text mining.
However, there are other sources of similarity that can be useful in this case. It is likely that two papers in the same area are similar due to the structure of citation graph. For example, it is possible that P 1 and P 2 are both cited by a lot of papers thereby inducing some similarity between P 1 and P 2 . Assume that P 1 and P 2 are similar due to citation information. This information can be used to induce a small amount of similarity between the keywords contained in the two papers. Thus, keywords k 1 and k 2 have a non-zero similarity value. Once again, this information can be used to induce a small amount of similarity between P 1 and P 3 Therefore, two features which co-occur a lot are similar and similarly, two objects which have a lot of similar features are similar themselves. Thus, we can learn to estimate similarity in one feature space using similarity estimates from other feature space. The major contributions of this paper are given below,
In the next two sections, we formalize and exploit this observation to improve similarity in one feature space using other feature spaces in a principled way.

We assume that the data to be analyzed consists of many heterogeneous feature types. The setup is very general: The objects are represented by many heterogeneous feature types and an initial coarse similarity measure between different feature types as well as objects is estimated from the data. The estimated similarity values are represented using multiple layers of graphs, where each graph corresponds to a particular feature type.
 We now formally define the related concepts,
Definition 1. Objects and Feature Types: we have a set of N objects, O = { o 1 ,o 2 ,...,o N } . Each object is represented using m different feature types, F = { F 1 ,F 2 ,...,F m } . Each feature type, F i consists of a set of features, F Definition 2. Object Graph: The object graph, G 0 = { V 0 ,E 0 } , consists of the set of objects as nodes and the edge weights represent the initial similarity between the objects.
 Definition 3. Feature Graphs: For each feature type F , we create a feature graph, G i =( V i ,E i ) . The graph consists of features F i as nodes, i.e, V i = F i . The edge weights represent the similarity between features. In the case of feature types like citations/links, the set of nodes in the graph is V 0 and similarity can be estimated using the citation/link graph structure using node similarity measures [3], [14].

The object graph and the feature graphs can be viewed as different layers of graphs. These layers are connected using the concept of layer connectivity as defined below,
Definition 4. Layer Connectivity: The two graphs, G i and G j , i = j are connected as follows. There exists an edge between f i j and f k l if they co-occur in some object X  X  feature vector representation. Let Z refer to the layer connectivity function, i.e, Z f ij ,f kl =1 if f i j and f k l co-occur.
Thus we have a set of heterogeneous graphs each with an initial similarity measure. We seek to improve the similarity values in each graph layer using information from all other layers. Specifically, we incorporate higher order dependen-cies in the same feature type as well as dependencies across feature types to improve the initial similarity estimates. we define two features, f ik and f jk of the feature type F k to be similar if and similarly, object similarity can be defined in terms of feature similarity. Therefore, the task is to obtain improved similarity estimates in all feature spaces. We evaluate the estimated similarity measure extrinsically, i.e, using perfor-mance measures for external tasks like classification and clustering which use the improved similarity measure.
Our approach for incorporating information from different heterogeneous feature types is inspired by the standard regularization framework for semi-supervised classification using label propagation [15], [16]. In this framework, there exists a single graph G =( V, E, w ) . The set of nodes is represented as V = { x 1 ,x 2 ,...,x n } . The edge weights, w ij represent similarity between the two nodes i and j . Also, there exists a set of labeled nodes L whose label is given by y ( x ) . The problem is to classify all other nodes using a discriminant function f : X  X  R . To compute f they define an objective function as follows,  X ( f )= Essentially, the first term tries to regularize the node labeling over the network such that similar nodes get similar labels while the second term tries to minimize the difference between the true labels and the predicted labels. The first term can be written in quadratic form as shown below, where f =( f ( x 1 ,f ( x 2 ,...,f ( x n ) and L is the combina-torial graph laplacian matrix, defined as = D  X  W where D is the diagonal matrix with d ii = n j =1 w ij
One reasonable approach to combine the different sources of similarity for performing classification is to linearly com-bine the graph laplacians. For this purpose, we can create m different graphs G 1 ,G 2 ,...,G m , where each graph, G consists of the set of objects as nodes and edge weights represent similarity computed using only feature type F i Let the laplacian corresponding to graph G i be represented as
L i . Then we can perform regularization on all the graphs simultaneously as where i  X  i =1 . This is equivalent to linearly combining the similarity estimates due to different feature types as in 1. Therefore, even this formulation does not use all the sources of similarity efficiently. There are a few problems with the above framework in the context of our problem setting
In this paper, we define a novel regularization framework over edge weights of multiple graphs for improved similarity estimation. Let W refer to the improved weighting function to be computed over the edges in all the graphs, i.e, w f gives the similarity between features f ik and f jk while w refers to the initial similarity value. Also, let V = V o F 2 ...F m refer to the set of vertices in all the graphs. initial similarity. Then, the objective function is defined as, where J ( V i ,V j )=
Then the task is to minimize the objective function. The objective function consists of two parts. The first part ensures the optimized similarity values remain close to the original similarity values while the second term seeks to minimize the dissimilarity between all the graphs. The second term directly models our intuition of features co-occurring with other features are similar. The parameter 0 &lt; X  0 &lt; 1 to control the tradeoff between the two terms. Note that if  X  0 =1 then the optimal solution is the initial solution itself.
The significance of the second term is explained using a simple example. Consider two graphs, G 1 and G 2 . Let G 1 be the graph containing publications as nodes and edge weights representing similarity due to citation graph. Let G be the graph corresponding to keywords and edge weights represent similarity between keywords. There is an edge from a node u 1 in G 1 to a node v 1 in G 2 if the publication corresponding to u 1 contains the keyword corresponding to v 1 . According to this example, minimizing the objec-tive function essentially means updating similarity values between two keywords in proportion to similarity values between the papers they are contained in and vice versa. Therefore, even though keywords like Machine Translation and Word Alignment are not similar according to textual similarity measures, they are assigned a similarity value if two papers which contain them are similar because of citation similarity.
 A. Convex Optimization -Direct Solution
Clearly, the objective function is a convex function in M variables where M is the total number of edges, i.e, M = i =1 | E i | . We can also add the constraint v W u,v =1 for normalization. Then the constrained convex optimization problem can be expressed in the form shown below, where x is a M  X  1 vector representing the edge weights along each dimension of the vector. The Karush-Kuhn-Tucker (KKT) conditions for the above problem are [17], which can be written as, In the above equation, v  X  refers to the dual solution while x  X  refers to the actual solution. Hence, we can directly solve the optimization function to obtain the similarity values by solving the above system of linear equations. Unfortunately, solving the above system of equations requires inverting a 2
M  X  2 M matrix which has a computationally complexity of O ( M 3 ) . This is prohibitively expensive in terms of computational complexity for even moderate size data sets. Hence we need to find alternate optimization methods to minimize the objective function.
 B. An Efficient Algorithm
Although the direct optimization is prohibitively expen-sive, we can solve it using approximate methods such as coordinate descent[1]. In this method, we assume all other dimensions are fixed except one and make a descent along the chosen dimension iteratively. In the context of our problem, we assume all edge weights are fixed except one edge weight and solve the minimization of the objective function. For example, the partial derivative of the objective function with respect to the edge, ( u j ,v k )  X  V i is shown below,
To perform the descent along the edge ( u j ,v k )  X  V i ,we set the above partial derivative to zero which gives us the following expression, where C is defined as
It can be seen that if the original similarity values are bounded in the range [0 , 1] , then they will always remain bounded with C playing the role of a normalization constant. Note that C is a constant for a given pair of vertices and hence, C can be incorporated in w  X  and w to obtain the following equation
If we represent the initial edge weights of graph G i by matrix W  X  i and Z i j represent the layer connectivity matrices for graphs, G i and G j , then the algorithm can be written in terms of iterative matrix equations as follows, where W t i refers to the matrix W i at the end of the t th iteration and Z il T is the transpose of Z il .Now,we can iteratively apply the above equation to minimize the objective function. See appendix for a proof of convergence for the algorithm.

For example, in the context of publications, let T represent the similarity between keywords and P represent the simi-larity between publications due to the citation graph. Then we can combine the two different sources of similarity using the following two iterative equations C. Layered Random Walk
The above algorithm has a nice intuitive interpretation in terms of random walks over different layers of the graph assming the initial similarity weights are transition probability values and Z ij matrices are normalized so that each row sums to 1. Then the similarity between two nodes, u and v in layer i , is computed as the sum of two parts. The first part is  X  0 times the original edge weight. This is necessary so that the newly computed similarity estimates are not too far away from the initial estimates. The second part is a sum of m  X  1 different terms. The j th term is weighted by  X  i j and is the probability of starting a random walk from u and moving to layer j for a random walk of length 1 and then returning back to vertex v in layer i . Thus, the j th term is the probability of a random walk of length 3 with both intermediate vertices of the walk belonging to layer j .

We can see that this algorithm exploits the dependencies across feature spaces to improve similarity estimates. Con-sider the example of publication classification mentioned in section II. When we estimate similarity between the keywords in the keyword feature layer, we perform a random walk over the citation feature layer. Therefore, the similarity between the keywords k 1 and k 2 will be incremented by a value directly proportional to the similarity between the publications due to citations. Also, note that second and higher order dependencies are also taken into account by this algorithm. That is, two papers may become similar because they contain two keywords which are connected by a path in the keyword feature layer, whose length is greater than 1. This is due to the iterative nature of the algorithm. For example, consider a set of semantically similar keywords K = { k 1 ,k 2 ,...,k n } . Assume that the initial keyword similarity estimated the similarity between these keywords is 0 . Then during the first iteration, k 1 and k 2 could become similar because they are contained in publications P 1 and P (similar due to citation similarity). Then k 2 could become similar to k 3 because of another pair of similar papers which contain the keywords. In this fashion, the similarity between all the keywords in the set are improved to a non-zero similarity value.

Also, the  X  ij values can be used to control the user knowledge about the dependency between features. For example, it might be reasonable to assume that there is relatively low dependency between the venue information and authorship information. In this case, we can set the corresponding value of alpha ij to be low. This shows that we can model a large number of dependencies between different feature spaces using the rich representation using heterogeneous graphs which are interconnected.
 It is very hard to evaluate similarity measures in isolation. Thus, most of the algorithms to compute similarity scores are evaluated extrinsically, i.e, the similarity scores are used for an external task like clustering or classification and the performance in the external task is used as the performance measure for the similarity scores. This also helps demonstrate the different applications of the computed similarity measure. Thus, we perform a variety of different experiments on standard data sets to illustrate the improved performance of the proposed similarity measure.

Experiment Set I : We compare our similarity measure against other similarity measures in the context of classifica-tion. We also compare against a state of the art classification algorithm which uses different similarity measures due to different feature types without integrating them into one single similarity measure.

Specifically, we compare our algorithm against three other similarity baselines in the context of classification which are listed below. We use a semi-supervised graph classification algorithm [5] to perform the classification given the different similarity graphs mentioned above.
 We also compare our algorithm against SC-MV : We compare our algorithm against the spectral clustering algorithm for data with multiple views [9]. The algorithm tries to classify data when multiple views of the data are available. The multiple views are represented using multiple homogeneous graphs with a common vertex set, V . In each graph, the edge weights represent similarity between the nodes computed using a single feature type. For our experiments, we used the link similarity graph and the content similarity graph as described above as the two views of the same data.

Experiment Set II : We illustrate the improved performance of our similarity measure in the context of clustering. We compare our similarity measure against the three similarity baselines mentioned above. We use a spectral graph cluster-ing algorithm proposed in [10] to perform the clustering.
For all experiments, we set  X  0 =0 . 4 . This is because the initial similarity estimates are very coarse and computed using naive methods like cosine similarity. Hence, there was a bigger weight on the similarity learned from other feature spaces. We performed our experiments on three different data sets. The three data sets are explained below. TableII summarizes the statistics of the different data sets used in experiments
For all the data sets, we constructed two graphs, the kewyord feature graph and the link similarity graph. The keyword feature layer graph, G f =( V f ,E f ,w f ) is a weighted graph where V f is the set of all features. The edge weight between the features f i and f j represents the similar-ity between the features. The edge weights are initialized to the cosine similarity between their corresponding document vectors. The link similarity graph, G o =( V o ,E o ,w o ) another weighted graph where V o is the set of objects. The edge weight represents the similarity between the objects and is initialized to the similarity between the objects due to the link structure. The link similarity between two objects is computed using the similarity measure proposed by [3] on the object link graph.

We evaluate our algorithm in terms of classification ac-curacy for the classification task. Classification accuracy is the percentage of objects which are correctly classified.
For the clustering task, we use Normalized Mutual In-formation as the measure of clustering accuracy. Mutual Information is a symmetric measure which quantifies the sta-tistical information between two distributions. Let I ( X, Y denote the amount of mutual information between the distri-butions X and Y. Since I(X,Y) has a lower bound of zero and no upper bound, we normalize the quantity using the entropy of X and Y. Thus the normalized mutual information used is,
We estimate NMI using the samples provided by the clusterings. Let n be the total number of objects (nodes) to be clustered. Let k ( A ) be the number of clusters according to clustering A and let k ( B ) be the number of clusters according to clustering B . Let n A h denote the number of objects in cluster C h according to clustering A , and let n l denote the number of objects in cluster C l according to clustering B . Let n h,l denote the number of objects that are in cluster C h according to algorithm A as well as in group C l according to clustering B . Then, NMI is estimated according to 16 This evaluation measure has some nice properties such as NMI ( X, X )=1 . Also NMI ( X, Y )=1 if the clusterings X and Y are the same except for a permutation of the cluster labels. Also, NMI ( X, Y )=0 if the clustering X is random with respect to Y or vice versa.

Therefore, in our setting, we compare the clusterings ob-tained by our algorithm X against the clustering Y , induced by the correct cluster labels using NMI. The higher the value the better the clustering obtained. Thus, we compare our algorithm against the two above mentioned baselines using NMI as the evaluation metric.

Figure V shows the accuracy of the classification obtained using different similarity measures.

It can be seen that the proposed similarity measure outperforms all other baselines by a large margin. Note that although the spectral clustering algorithm on multiple graphs is given the same information as given to our algorithm, our algorithm achieves better performance. We attribute this to the rich representation of the data. In our algorithm, the data is represented as a set of heterogeneous graphs (layers) which are connected together. Thus, we were able to iteratively improve our similarity estimates using similarity estimates from other feature spaces. Whereas, in the case of the algorithm in [9] all the graphs are isolated homogeneous graphs. Hence there is no information transfer across the different graphs.

Table IV shows the NMI scores obtained by the different similarity measures on the different data sets.

We also clustered the keyword feature layer of the AAN data set to show that the proposed algorithm improves the similarity estimates in all the input graphs. Table III shows the most frequent keywords from each cluster. It can be seen that the keyword clusters are very cohesive and are indicative of the research area they belong to. This shows that similarity estimates in the keyword feature layer are also improved using similarity estimates from the citation similarity space.

An important property of this framework is that we en-hance any coarse-grained initial similarity measure. For ex-ample, similarity between keywords can be estimated using more sophisticated methods which use external knowledge bases like WordNet, Wikipedia [6], [7]. Although, there exist lots of similarity measures for computing similarity between keywords and phrases, there are relatively few approaches to compute similarity between entire documents using pair-wise similarities between keywords [8]. Note that using the pairwise similarities between individual keywords alone, we can estimate the similarity between entire documents using the proposed algorithm. We plan to do experiments in future comparing the proposed algorithm to other algorithms in this domain which already use a very sophisticated similarity measure in a single feature space (text).

In this paper, we proposed a novel approach to compute similarity for objects with many heterogeneous features. We formalized the problem of similarity estimation as an optimization problem induced by a regularization framework over edges in multiple graphs. We show that there exists a direct solution to the convex optimization problem, albeit expensive in terms of computational complexity. Therefore, we proposed an alternate algorithm for the optimization us-ing the coordinate descent approach. We also illustrated the improved performance of the proposed similarity measure in tasks like clustering and classification over baseline simi-larity measures and a state-of-the-art classification algorithm which uses the same information as given to our algorithm. The authors would like to thank Vahed Qazvinian, Ahmed Hassan, Kumar Sricharan and Gowtham Bellala for helpful discussions.
 A. Proof of Convergence
We can prove that equations 15 converge as the number of iterations tends to infinity.

Proof : In order to prove convergence, we need to show the following Thus, Substituting this in equation 4, In the above equation, since  X &lt; 1 we can claim that W t  X  W t  X  1 =0 as t  X  X  X  if none of the matrices tend to  X  . W 1  X  W 0 is a constant matrix and ZZ T is a transition probability matrix, where ZZ T ( i, j ) is the probability of random walk of length starting at i and ending at j of length 2 . Hence ( ZZ T ) distribution as t  X  X  X  . Hence, W t  X  W t  X  1 =0 as t  X   X 
