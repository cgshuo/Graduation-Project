 SPECIAL ISSUE PAPER Shijian Lu  X  Tao Chen  X  Shangxuan Tian  X  Joo-Hwee Lim  X  Chew-Lim Tan Abstract This paper presents a scene text extraction tech-nique that automatically detects and segments texts from scene images. Three text-specific features are designed over image edges with which a set of candidate text boundaries is first detected. For each detected candidate text boundary, one or more candidate characters are then extracted by using a local threshold that is estimated based on the surrounding image pixels. The real characters and words are finally identi-fied by a support vector regression model that is trained using bags-of-words representation. The proposed technique has been evaluated over the latest ICDAR-2013 Robust Read-ing Competition dataset. Experiments show that it obtains superior F-measures of 78.19% and 75.24% (on atom level), respectively, for the scene text detection and segmentation tasks.
 Keywords Scene text detection  X  Scene text segmentation  X  Scene text recognition  X  Shape features 1 Introduction Scene text extraction refers to the detection and segmenta-tion of texts in scene images or videos. It is very impor-tant to many vision-related tasks such as guidance for for-eign tourists, unmanned vehicle navigation in urban envi-ronments, and content-based image indexing and retrieval [ 5 , 7  X  9 , 13 ]. Although optical character recognition (OCR) of the scanned document text has become mature, the recog-nition of texts in scenes is still a big challenge for machines.
Besides the final OCR, another two challenging tasks for an end-to-end scene text recognition system are the detection and the segmentation of texts in scenes. Compared with the well-defined document texts, texts in scenes are often more prone to different variations in terms of text size, font, color, orientation, layout, and context as illustrated in Fig. 1 .In addition, texts in scenes usually suffer from different types of degradation such as shadow and low image contrast due to bad illumination, partial occlusion by other objects in scenes, andcompleximagebackground. Duetotheweaktext format-ting and the various types of image degradation, it is much more challenging for the detection and segmentation (sepa-ration of the text from the image background) of scene texts captured by cameras compared with the detection and seg-mentation of texts in the scanned document images. Several benchmarking contests have been organized together with ICDAR 2003 [ 10 ], ICDAR 2005 [ 11 ], ICDAR 2011 [ 25 ], and ICDAR 2013 [ 6 ]. The best accuracy (F-measure) obtained is still lower than 80% for both detection and segmentation tasks, which shows that there is still a large improvement space for both tasks.

A number of scene text detection techniques [ 2 , 3 ]have been reported in the literature. One typical approach is slid-ing window based which scans an image patch by patch, and at each patch, image features are computed to tell how likely the image patch is text or non-text. For example, Chen et al. [ 5 ] build a cascade Adaboost classifier that learns from different texture features such as histogram of intensity and image derivatives. In recent years, the HOG features [ 16 ] have been used for scene text detection with very promis-ing results [ 17  X  19 , 29 ]. To skip the design of handcrafted features, unsupervised feature learning has been proposed [ 20 , 21 ] that first builds a dictionary through K-means clus-tering and then classifies text and non-text regions using mul-tilayer neural networks. The sliding window-based approach is tolerant to image degradation but often computationally intensive and does not segment texts from the image back-ground.

Another typical approach is connected component based [ 12 , 14 , 22  X  24 , 34 ] which first detects a set of character can-didates and then identifies the real text based on different color and shape features. Epshtein et al. [ 12 ] propose a stroke width transform (SWT) operator based on the observation that texts in scenes tend to have more constant stroke width compared with most non-text objects. Gradient and color-based image partition is proposed in [ 24 , 30 ] that first extracts candidate character components and then groups them to text lines based on certain joint layout information. In addition, maximally stable extremal regions (MSERs) [ 26 , 28 ] and its extended extremal regions (ERs) [ 22 ] have been proposed for the extraction of texts in scenes. The connected component-based approach is more efficient and effective in segmenting texts, but the used components such as ERs are often sensitive to image degradation.

In this paper, we propose a scene text detection and seg-mentation technique as illustrated in Fig. 1 . Given a scene image, a set of candidate text boundaries is first detected based on three text-specific features that are extracted from image edges. For each candidate text boundary, one or more candidate characters are then segmented with a local thresh-old that is estimated based on the surrounding image pix-els. The real (binary) texts are finally identified by a support vector regression model that is trained through a bags-of-words (BoW) image representation method. The proposed technique has a number of contributions. First, it designs three text-specific features and integrates them into a text detector that detects texts in scenes reliably. Second, it designs an adaptive scene text segmentation technique that separates text pixels from the image background accurately. Third, it performs both text detection and text segmentation and outperforms state-of-the-art techniques greatly. In partic-ular, the scene text segmentation technique obtains the best F-measure as reported in [ 6 ], and the scene text detection technique significantly outperforms all submissions reported in [ 6 ].

The rest of this paper is organized as follows. Section 2 presents the proposed technique including text-specific edge feature design, feature integration for text detection, scene text segmentation based on text boundary, and the final SVR-based false text removal. Experimental results are then pre-sented in Sect. 3 based on two public datasets. Concluding remarks are finally drawn in Sect. 4 . 2 Detection and segmentation of texts in scenes Figure 2 illustrates the framework of the proposed technique. Given a scene image shown in Fig. 2 a, three text-specific features shown in Fig. 2 c X  X  are first computed over image edges shown in Fig. 2 b that are detected from single-channel image using Canny X  X  edge detector [ 15 ]. The three features are then combined to form a joint feature shown in Fig. 2 f. The joint feature is integrated across three image channels shown in Fig. 2 g and further multiple image scales shown in Fig. 2 h. A set of candidate text boundaries shown in Fig. 2 iis then determined, and candidate character components shown in Fig. 2 j are further extracted based on the detected text boundaries. Finally, candidate characters are clustered into candidate words shown in Fig. 2 k, and the real words shown in Fig. 2 l are identified by a trained support vector regression model. 2.1 Text-specific edge features We design three text-specific features to differentiate text and non-text edges. The first feature is text-specific image contrast which is defined as follows: F where G e stores the gradient magnitude of all pixels of the edge component e , X ( G e ) and  X ( G e ) denote the mean and standard deviation of G e , and  X  denotes a small positive num-ber which ensures a nonzero denominator. Different from the conventional image contrast, the image gradient variance is incorporated in Eq. 1 to discriminate text and non-text edges as texts in scenes often have a more homogeneous image background, and hence a smaller gradient variance along the text boundary.

The second feature captures a text-specific shape structure that a character often has more than one stroke and accord-ingly two edge cuts in either horizontal or vertical direction (an edge cut means an intersection between an edge and a horizontal or vertical scan line as illustrated in Fig. 3 ). This feature is defined as follows: F where W and H denote the edge width and edge height, respectively, i.e., the width and height of the corresponding edge bounding box, and cn i and cn j are the numbers of edge cuts in the i th row and the j th column of the edge, respectively. Function f ( x ) returns 1 if x is larger than 2 and 0 otherwise. The numerator thus gives the number of edge rows and edge columns that have more than two edge cuts. Compared with non-text edges, text edges often have a much larger F 2 as text edges often have more rows or columns that have more than two edge cuts as illustrated in Fig. 3 . Parameter R is used to control the weight of this feature in the finally integrated feature. Extensive experiments on various scene images with texts show that it does not affect the detection performance much when it is set between 10 and 30 (it is fixed at 20 in our implemented system).
Note that there is a smoothing process that sets F 2 of each edge component as the mean F 2 of a number of neigh-boring edge components. The neighboring edge components are determined based on their relative size and distance to the edge component under study. In particular, a neighboring edge component is detected if its size lies within [0.5 2] of the size of the interested edge component, and its distance to the interested edge component is smaller than the sum of the width and height of both edge components. The smoothing process helps to ensure that digits and letters with a simple structure such as 1, i, I will have a high feature value F
The third feature captures another text-specific structure that each character stroke has a pair of edges, and hence, text edges should have a larger number of rows and columns with an even number of edge cuts compared with non-text edges as illustrated in Fig. 3 . This feature is defined by: F where W , H , cn i , and cn j are the same as defined in Eq. 2 . Function g ( x ) returns 1 if x is an even number and 0 if odd. The numerator thus gives the number of edge rows and columns that have an even number of edge cuts. Compared with non-text edges, text edges often have a larger F 3 as they usually have a larger number of rows and columns (normal-ized by edge width and height) that have an even number of edge cuts as illustrated in Fig. 3 .

For the sample image in Fig. 2 a, b shows the Canny edges that are detected from the Y channel image (under the YUV color space) at the original image scale. For Canny edges, we simply use the Canny edge detection function that is imple-mented in MATLAB. The parameters such as the low thresh-old and high threshold are also set by default in MATLAB. For the Canny edges as shown in Fig. 2 b, Fig. 2 b X  X  shows the computed three text-specific features, respectively. 2.2 Feature integration for candidate text detection The three text features are extracted from edges of different channel images at different image scales. We use the YUV color space where channel Y captures the image lightness information and channels U and V capture the image color information. The YUV is used because a certain amount of text in scenes is printed in colors and text features such as contrast may become very weak within the gray-scale image which captures the intensity/lightness only. Note that all three channel images are first normalized to be [0, 1] before the feature computation as described in the last subsection.
In addition, the three text-specific features are computed at images of different scales. The multi-scale approach is helpful in two different aspects. First, it helps to detect texts of different size because text edges may only be detected properly at certain scales and can be neither too large (edges could be broken) nor too small (edge topology could be lost). Second, it helps to improve the robustness to different types ofimagedegradation,astextedgesmissedatoneimagescale, say, due to the complex image background, could be detected properly at another image scale. In our implemented system, we use eight image scales that increase from 0.2 to 1.6 of the original image scale with a step size of 0.2.

For each channel image at one specific image scale, the three text-specific features of each image edge are first com-bined as follows: S where F i , j , k denotes the kth feature that is extracted from edges of the jth channel image at the ith scale. Note that F , j , k is actually a feature image as illustrated in Fig. 2 c X  e where each pixel has a feature value as computed in Eqs. 1  X  3 if the pixel corresponds to an edge pixel. The three features are combined through multiplication to discriminate text and non-text edges, as text edges often have fair/high val-ues across the three features but non-text edges do not.
The text-specific features can thus be integrated across images of different channels at different scales as follows: M = where C is the number of image channels and S is the number of image scales. S i , j denotes the combined feature for the jth channel image at the ith scale as defined in Eq. 4 . Note that the S i , j computed at different image scales need to be scaled back to the original scale before the averaging.

For the sample image in Fig. 2 a, Fig. 2 f shows the joint fea-tures that combine the three text-specific features in Fig. 2 c X  X  by multiplication. As Fig. 2 f shows, the combination of the three text-specific features enhances the discrimination between text and non-text edges greatly. Figure 2 g, h shows the integrated features across Y, U, and V channels at the orig-inal scale and further across multiple image scales, respec-tively. As Fig. 2 g X  X  shows, the integration of features across image channels and scales further enhances the discrimina-tion between the text and non-text edges clearly.

Candidate text boundaries can thus be detected through global thresholding of the integrated text features [ 33 ]. As text edges usually have much higher feature values than non-text edges, we simply set the threshold as the mean of the inte-grated feature image. Besides, we use the external character boundary for scene text segmentation to be described in the next subsection. The internal boundary for characters with holes such as  X  X  X  and  X  X  X  needs therefore to be identified and removed first. The internal boundary can be identified by those that are completely enclosed by another candidate text boundary. Note that some falsely detected candidate text boundaries such as rectangular traffic sign boundary could enclose multiple real text boundaries and accordingly cause problems. In the proposed technique, such falsely detected text boundaries are identified (and filtered out) if they enclose more than three candidate text boundary components (as a real external character boundary seldom encloses more than three internal boundary components). Similar idea has also been proposed in [ 31 ] that makes the use of text boundary topology for false text candidate removal.

For the integrated feature image in Fig. 2 h, Fig. 2 ishows the binary image after the thresholding and filtering where most text boundaries are detected, whereas non-text bound-aries are removed. Note that the detected text boundaries are often more than 1 pixel thick as text edges detected at differ-ent image scales may lie at slightly different positions while scaled back to the original image scale. In addition, some non-text boundaries are also detected most of which can be removed by a SVR model to be described in the ensuing sections. 2.3 Scene text segmentation based on text boundary Segmentation of texts in scenes usually requires adaptive thresholding which estimates a local threshold for each pixel based on the pixels within a neighborhood window. Two crit-ical issues need to be addressed for adaptive thresholding of texts in scenes. First, the relative text brightness needs to be identified as texts in scenes could be either brighter or darker than the background. Second, the size of the neighborhood window which often affects the thresholding performance greatly needs to be estimated adaptively as the size of texts in different scenes could be dramatically different.
We estimate the relative text brightness by detecting a number of text pixels and the surrounding background pixels. In particular, each candidate text boundary is first scanned row by row and column by column as illustrated in Fig. 4 . Take horizontal scanning in Fig. 4 b as an example. For each scanned row, the first and the last text boundary pixels (in red) neighboring to the background are first located. A pair of background pixels (in green color) is then located 1 X 3 pixels on the left and right of the located first and last candidate text boundary pixels. A pair of text stroke pixels (in blue) can then be detected that lie 1 X 3 pixels on the right and left of the text boundary pixels (in yellow) that are dual to the first and last detected text boundary pixels (in red), respectively. The relative text brightness can be estimated based on whether the average intensity of the text pixels is larger than that of the background pixels.

The size of the neighborhood window is very important to the performance of the adaptive thresholding. If the neigh-borhood window is too large, certain neighboring non-text objects could be included which will affect the statistics (such as the mean and variance) of the included pixels and hence the local threshold estimation. If it is too small, certain text pixels may not be segmented properly, e.g., the interior pix-els of character strokes will be missed when the size of the neighborhood is smaller than the stroke width (which often happens as texts in scenes could have an ultra-large size). Ide-ally, the size of the neighborhood window should be adaptive to the size of texts in scenes.

In our proposed technique, the size of the neighborhood window can be adaptively estimated based on the size of the detected candidate text boundary which gives a very close estimation of the size of texts in scenes. In our system, the size of the neighborhood window is set at 1/5 of the height of the text boundary under processing. In addition, we use Niblack X  X  adaptive thresholding technique [ 1 ] that estimates a local threshold based on the mean and variance of the sur-rounding pixels within a neighborhood window as follows: T =  X  + k  X   X  (6) where  X  and  X  refer to the mean and variance of the values of the neighboring pixels, respectively. Parameter k is set at 0.4 in our system. For the detected text boundary in Fig. 2 i, Fig. 2 j shows the segmented texts where most text pixels are extracted correctly. 2.4 SVR-based false text removal Text lines and words can then be located based on the size and spatial layout of the segmented binary components. We first detect text lines through an iterative searching process that exhaustively merges neighboring binary components with similar height, width, and distance. Specifically, two neigh-boring components are merged if the nearest distance and the height difference between them are concurrently smaller than 2 and 1 times of the mean of their heights. Words are then detected based on the distance between neighboring com-ponent pairs within each detected text line. In particular, an inter-word blank is detected when the nearest distance between two neighboring components is larger than 2 times of the median distances between all neighboring component pairs within the detected text line. For the segmented binary text candidates in Fig. 2 j, k shows the detected word can-didates where a certain amount of non-text words are also falsely detected.

The real words are identified from the detected word can-didates through the combination of bag-of-word (BoW) rep-resentation and a support vector regression (SVR) model. In particular, the dense histogram of oriented gradient (HoG) features are extracted from multiscale cells (4  X  4 ; 8  X  12) within each training word bounding box. Bag-of-words (BoW) algorithm is then adopted to cluster these HoGs into a histogram to represent each bounding box. The reason to adopt BoW for bounding box representation is that the detected word bounding boxes could have very different sizes and aspect ratios (e.g., the aspect ratio can vary from 1 to 16 in ICDAR 2013 dataset) and should not be normalized to a fixed size which could introduce severe text shape deforma-tion and distortion.

A code book containing k codewords is constructed by k-means clustering of the dense HoG that are extracted from a set of training word bounding boxes. By varying k in a large range from 100 to 1,000, we found that the best performance is achieved when k is set to 400 as illustrated in Fig. 5 .Anew candidate word bounding box can be quantized into a BoW histogram by matching the extracted HoG with the generated code book. The matching process is described as follows. For each HoG descriptor, the Euclidean distance between each codeword in the codebook and the HoG descriptor is cal-culated, and the codeword with the closest distance is con-sidered as the matched word for the descriptor. The match count for this codeword is accordingly increased by 1. After all HoG features extracted from the bounding box found their matches with the codebook, a BoW histogram that records the appearance number of each codeword in the bounding box is determined. Note that the training bounding boxes in the proposed technique are those candidate word bound-ing boxes that are detected from all training images with the training dataset.

A SVR model is designed and applied where a soft score is computed for each detected bounding box. Different from the conventional approach such as support vector machines (SVM) that labels the training examples in a binary manner, we compute a soft score ranging from 0 to 1 to each detected bounding box, where a higher score indicates that the bound-ing box is more likely to be a real word bounding box. The soft score approach is designed because the detected bound-ing boxes often have only partial overlap with the ground truth word bounding boxes, e.g., some character within a word is missed to be detected or certain amount of neighbor-ing background is included in the detected word bounding box. Compared with SVM, SVR allows the training sam-ples to have different soft scores which reflect their different importance for model training.

The soft score is computed according to the overlap between the detected bounding box and the corresponding ground truth bounding box. Larger scores indicate that the training samples have more overlap with their ground truth texts and thus are more important for SVR training, while lower scores indicate that the training samples have less over-lap and thus less important. The soft score is defined as fol-lows: s (
R , G ) = where R and G refer to the detected bounding box and its ground truth, respectively. The numerator part is thus the intersection area between the two boxes, and the denominator is the union area of the two bounding boxes. The training box with higher overlap with the ground truth will therefore be assigned with a higher score during SVR learning.

With the BoW histograms H i and the soft score s i , i = 1 , 2 ,..., L , where L is the number of training bounding boxes,weadoptsupportvectorregression(SVR)[ 32 ]tolearn a model to correlate the histograms H and the soft scores s for the training samples. The objective functions for SVR are formulated as follows: min Subject to  X  T ( H s  X   X  T ( H  X  , X   X  i  X  0 , i = 1 , 2 ,..., L where  X  is the normal vector of the hyperplane, b is the bias which is a scalar, and  X  i and  X   X  i are the slack variables that are related to prediction errors in SVR. C is a regularization parameter that controls the balance between margin maxi-mization and carried prediction error. Extensive experimen-tation has been carried out by varying C at different values, e.g., 2  X  2 , 2  X  1 ,..., 2 8 . Experiments show that the optimal performance can be obtained when C is set around 128 (2 7 ( H ducing the Lagrange multiplier, the above formula can be transformed into its dual form as follows: subject to 0  X   X  where K is the kernel function. Here, we use norm-2 Chi-square distance function as the kernel which demonstrates great effectiveness for histogram-based distance calculation as described in [ 35 , 36 ].

After solving the above problem, the score d of the BoW histogram H of a detected bounding box can be predicted using following approximation function: d = After obtaining the scores for all the detected bounding boxes, bounding boxes with low scores can be removed, and this reduces the number of false positive boxes and accord-ingly improves the detection precision significantly. Figure 2 l shows the finally identified words where non-text candidates are identified and removed correctly. 3 Experiments The proposed technique has been evaluated and compared with state-of-the-art techniques. Two evaluation datasets are first described. Experimental results on the two datasets are then presented and discussed. 3.1 Dataset The proposed technique has been evaluated extensively by using the ICDAR-2013 public benchmarking dataset that was used in the Robust Reading Competition 2013 1 orga-nized together with the International Conference on Docu-ment Analysis and Recognition. The dataset consists of 229 training images and 233 testing images with resolutions from 422  X  102 to 3 , 888  X  2 , 592 pixels. It includes most images from the ICDAR-2003 dataset [ 10 ] with a number of new images added. All images in the ICDAR-2013 dataset are color images as illustrated in the first row of Fig. 6 . For each image, the text location is provided by the coordinates of the top-left and bottom-right vertices of a bounding box. In addition, text segmentation ground truth is also provided for the training images where a binary image labeling the text regions is given for each training image. In the experiments, the scene text detection is evaluated using the 233 testing images and the scene text segmentation is evaluated using the 229 training images.
 The proposed technique has also been evaluated on the Street View Text (SVT) dataset [ 18 ]. The SVT dataset con-sists of 350 scene images with resolutions from 1 , 024  X  to 1 , 914  X  898 pixels. The images within this dataset are all color images and extracted from the Google Street View. Different from the images in the ICDAR-2013 dataset, the SVT images suffer more from noise, poor lighting, and low image contrast as illustrated in the first row of Fig. 7 . 3.2 Experimental results For the scene text detection task, the evaluation method in [ 27 ] is used which was specifically designed for scene text detection evaluation. This method is widely used because it considers not one-to-one but many-to-one and one-to-many matches with more details described in [ 6 ]. For the scene text segmentation task, the method in [ 4 ] is used which evaluates the segmentation performance from both pixel level and atom level as described in [ 6 ].

Figures 6 and 7 show qualitative results of the proposed technique on the two public datasets. In particular, the four rows in each figure show several sample images from the ICDAR 2013 dataset and SVT dataset (in Figs. 6 , 7 ), the computed overall feature images, the extracted scene texts, and the finally identified words, respectively. As Figs. 6 and 7 show, the proposed technique is robust and tolerant to the variation in text scale, text fonts, lighting, image contrast, and image context.

On the other hand, the proposed technique may not work well under several scenarios. In particular, it could miss detection and/or segmentation when text edges cannot be detectedproperly,typicallywhen(1)textshaveanultra-small character size (e.g., texts within the white color sign in the second image in Fig. 7 ), (2) texts suffer from severe per-spective distortion (e.g., white color texts in the poster at the top-left corner of the last image in Fig. 7 , (3) texts have a very complex background (e.g., text in the advertisement board in the third image in Fig. 6 ), and (4) texts are cluttered by other objects (e.g., street sign text in the fourth image in Fig. 6 ,etc). In addition, non-text objects may be falsely extracted when they demonstrate certain text-like image and shape charac-teristics such as the window structures in the fifth image in Fig. 6 , the wheel structures in the sixth image in Fig. 6 , and the two white color flags in the fifth sample image in Fig. 7 .
Quantitative results are also obtained for the ICDAR-2013 dataset. For the scene text detection task, Fig. 8 shows the receiver operating characteristic (ROC) curves of the proposed technique and several best-performing scene text detection techniques (submitted to the Robust Reading Com-petition 2013 and also listed in Table 1 ) that are determined by multiple combination of precision and recall. In particular, the optimal F-measure is up to 78.19% when the precision and recall are 89.22 and 69.58%. It is clearly higher than the best F-measure as obtained by submissions to the Robust Reading Competition [ 6 ] and also significantly outperforms our submitted method (I2R_NUS_FAR) which just obtains a F-measure of 71.91% as shown in Fig. 8 and Table 1 . The big improvement is largely due to the BoW and SVR-based false alarm reduction as described in Sect. 2.4 where most detected non-text candidates are identified and removed accurately.
For the scene text segmentation task, we obtain the best F-score in both pixel level and atom level as shown in Table 2 as extracted from [ 6 ]. The good text segmentation perfor-mance is largely due to the text boundary-based segmenta-tion approach that is more tolerant to different types of image degradation such as shadow and low image contrast resulting from uneven lighting as described in Sect. 2.3 . As Table 2 shows, the proposed technique outperforms all submissions to the Robust Reading Competition 2013 including our sub-mitted method I2R_NUS_FAR. The segmentation improve-ment over the I2R_NUS_FAR method is largely due to the better scene text localization results as obtained through the SVR-based false alarm reduction as described in Sect. 2.4 .
For the SVT dataset, our experiments show that a F-measure of 45.96% is obtained for the text detection task. This F-measure is much lower compared with those obtained on the ICDAR-2013 dataset. One major cause of the lower F-measure is due to the low text X  X ackground contrast where texts in many SVT images are printed in blue, green, or red colors as illustrated in Fig. 7 . Another major cause of the lower F-measure is the text font where texts in many SVT images use various art fonts for different special effects. These art font texts are often associated with complex bound-ary patterns, shadow effects, pop-up effects, etc., as illus-trated in Fig. 7 , which make the text detection and segmen-tation a much more challenging task. Note that many state-of-the-art techniques also report much lower accuracy on the SVT dataset. For example, a F-measure of 24.1% and 37% is reported in [ 22 , 24 ], respectively. 3.3 Discussion The proposed technique still has several limitations espe-cially for the scene text detection task as illustrated in Figs. 6 and 7 . In particular, the detection may fail to locate texts with a very cluttered background because text edges could be connected to the edge of the surrounding non-text objects. In addition, some non-text objects such as bricks, windows, and car wheels could be falsely detected as some of them have similar shape patterns as texts. Furthermore, it may also fail when scene texts have a very small size (more detection fail-ure is observed when the height of characters is smaller than 15 image pixels) or when the lighting is extremely poor.
Several issues will be further investigated. First, the sys-tem extracts the text boundary through global thresholding which may introduce false alarms especially when the image has no texts. The false alarms could be reduced by training a classifier based on the proposed features together with others such as histogram of gradients (HoG). Second, the current system integrates the three features by multiplication, but it is not clear how much each feature contributes to the final text detection. The contribution of each of the four features will be studied, targeting weighted feature integration for better detection performance. Third, recognition of the segmented scene texts will be studied. One major issue is the integration of gray-level and segmented binary texts for better recogni-tion accuracy.

The computational cost of the proposed technique can be broke down into three parts including one for edge feature extraction for the scene text detection, one for the scene text segmentation together with text line and word grouping, and one for SVR-based false alarm reduction. Currently the edge feature extraction part is most time-consuming as it involves the Canny edge detection and text-specific feature compu-tation at multiple image channels and scales. For the SVR-based false alarm reduction, its computational complexity is quadratic with respect to the number of training samples which only occurs in the offline training stage. In the online testing stage, the complexity is linear to the number of test-ing samples. Overall our system (implemented in MATLAB) takes up to 10s on average for the processing of an image within the ICDAR-2013 dataset. The computation should be much faster when the system is ported in C/C++. 4 Conclusion This paper presents a scene text extraction technique. In the proposed technique, a number of text-specific image and shape features are designed to detect the candidate text regions first. BoW model and SVR-based classifier are then trained to remove the falsely detected text regions. Experi-ments on the ICDAR-2013 dataset show that the proposed technique obtains superior scene text detection and segmen-tation performance.
 References
