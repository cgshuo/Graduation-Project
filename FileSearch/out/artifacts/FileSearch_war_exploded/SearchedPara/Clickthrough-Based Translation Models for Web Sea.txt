 Web search is challenging partly due to the fact that search que-ries and Web documents use diffe rent language styles and voca-bularies. This paper provides a quantitative analysis of the lan-guage discrepancy issue, and expl ores the use of clickthrough data to bridge documents and queries. We assume that a query is pa-rallel to the titles of documents clicked on for that query. Two translation models are trained and integrated into retrieval models: A word-based translation model that learns the translation proba-bility between single words, and a phrase-based translation model that learns the translation probability between multi-term phrases. Experiments are carried out on a re al world data set. The results show that the retrieval systems that use the translation models outperform significantly the system s that do not. The paper also demonstrates that standard statistical machine translation tech-niques such as word alignment, bilingual phrase extraction, and phrase-based decoding, can be adapted for building a better Web document retrieval system. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; I.2.6 [ Artificial Intelligence ]: Learning Algorithms, Experimentation Clickthrough Data, Translation M odel, Language Model, PLSA, Linear Ranking Model, Web Search This paper is intended to addre ss two fundamental issues in in-formation retrieval (IR) by exploiting clickthrough data: synony-my and polysemy . Synonyms are different terms with identical or similar meanings, while polysemy means a term with multiple meanings. onyms lead to the so-called lexical gap problem in document re-trieval: A query often contains te rms that are different from, but related to, the terms in the relevant documents. The lexical gap is substantially bigger in Web search largely due to the fact that search queries and Web documents are composed by a large varie-ty of people and in very different language styles [e.g., 18]. Poly-semy, on the other hand, increases the ambiguity of a query, and often causes a search engine to retrieve many documents that do not match the user X  X  intent. This problem is also amplified by the high diversity of Web documents and Web users. For example, depending on different users, the que ry term  X  X itanic X  may refer to the rock band from Norway, the 1997 Oscar-winning film, or the ocean liner infamous for sinking on her maiden voyage in 1912. Unfortunately, most popular IR methods developed in the re-search community, in spite of their state-of-the-art performance on benchmark datasets (e.g., the TREC collections), are based on bag-of-words and exact term matching scheme s, and cannot deal with these issues effectively [10, 22, 37]. Therefore, the devel-opment of a retrieval system th at goes beyond exact term match-ing and bag-of-words has been a long standing research topic, as we will review later. creating relationships between term s in queries and in documents. Clickthrough data have been exploited for this purpose [3, 34]. However, relationships are cr eated only between single words without taking into account the cont ext, giving rise to an increas-ing problem of noisy proliferation , i.e., connecting a word to a large number of unrelated or weakly related words. In addition, ad hoc similarity measures are often used. ing the statistical translation ba sed approach to IR, proposed by Berger and Lafferty [7]. We estimate the relevance of a document given a query according to how likely the query is translated from the title text of the document 1 . We explore the use of two transla-tion models for IR. Both models are trained on a query-title aligned corpus, derived from one-y ear clickthrough data collected by a commercial Web search engine. The first model, called word-based translation model, lear ns the translation probability of a query term given a word in the title of a document. This model, however, does not address the problem of noisy proliferation. translation probability of a multi-term phrase in a query given a phrase in the title of a document. This model explicitly addresses the problem of noisy proliferati on of translation relationships between single words. In theory, the phrase model, subsuming the word model as a special case, is more powerful because words in the relationships are considered with some context words. More precise translations can be determined for phrases than for words. This model is more capable of dealing with both the synonymy and the polysemy issues in a unified manner. It is thus reasonable to expect that using such phrase translation probabilities as rank-ing features is likely to improve the retrieval results, as we will show in our experiments. relationships between the terms in queries and the terms in docu-ments, most of them rely on a static measure of term similarity (e.g. cosine similarity) accordin g to their co-occurrences across queries and documents. In statistical machine translation (SMT), it has been found that an EM proce ss used to construct the transla-tion model iteratively can signifi cantly improve the quality of the model [9, 27]: A translation model obtained at a later iteration is usually better than the one at an earlier iteration, including the initial translation model corres ponding to a static measure. An important reason for this is that some frequent words in one lan-guage can happen to co-occur often with many words in another language; yet the former are not necessarily good translation can-didates for the latter. The iterativ e training process helps streng-then the true translation relations and weaken spurious ones. The situation we have is very similar: on the one hand, we have que-ries written by the users in some sub-language, and on the other hand, we have documents (or titles) written by the authors in another sub-language. Our goal is to detect possible relations be-tween terms in the two sub-language s. This problem can be cast as a translation problem. The fact that the quality of translation mod-els can be improved using the iter ative training process strongly suggests that we could also obta in higher-quality term relation-ships between the two sub-languages with the same process. This is the very motivation to use principled translation models rather than static, ad hoc , similarity measures. systems that use the translation models outperfor m significantly the systems that do not use them. It is interesting to notice that our best retrieval system, which uses a linear ranking model to incorporate both the word-based and phrase-based translation models, shares a lot of similarities to the state-of-the-art SMT systems described in [23, 27, 28]. Thus, our work also demon-strates that standard SMT techniques such as word alignment, bilingual phrase extraction, and phrase-based decoding, can be adapted for building a better Web document retrieval system. empirical study of learning word-based and phrase-based transla-tion models using clickthrough data for Web search. Although clickthough data has been proved very effective for Web search [e.g., 2, 16, 33], click informatio n is not available for many URLs, especially new and less popular URLs. Thus, another research goal of this study is to investigate how to learn title-query transla-tion models from a small set of popul ar URLs that have rich click information, and apply the models to improve the retrieval of those URLs without click information. search on dealing with the issues of synonymy and polysemy. Section 3 presents a large scale analysis of language differences between search queries and Web documents, which will motivate our research. Section 4 describes the data sets and evaluation methodology used in this study. Sections 5 and 6 describe in de-tail the word-based and phrase-ba sed translation models, respec-tively. The experimental results are also presented wherever ap-propriate. Section 7 presents the conclusions. Many strategies have been pro posed to bridge the lexical gap between queries and documents at the lexical level or at the se-mantic level. One of the simplest and most effective strategies is automatic query expansion, where a query is refined by adding terms selected from (pseudo) rele vant documents. A variety of heuristic and statistical techniques are used to select and (re-)weight the expansion terms [30, 35, 11, 5]. However, directly applying query expansion to a co mmercial Web search engine is challenging because the relevant documents of a query are not always available and generating pseudo relevant documents re-quires multi-stage retrieval, which is prohibitively expensive. LDA [8], take a different strategy . Different terms that occur in a similar context are grouped into the same latent semantic cluster. Thus, a query and a document, repres ented as vectors in the latent semantic space, can still have a high similarity even if they do not share any term. In this paper we will apply PLSA to word transla-tion, and compare it with the othe r proposed translation models in the retrieval experiments. approach [7] does not map differen t terms into latent semantic clusters but learns translation relationships directly between a term in a document and a term in a query. A major challenge is the estimation of the translation models. The ideal training data would be a large amount of query-document pairs, in each of which the document is (judged as) relevant to the query. Due to the lack of such training data, [7] resorts to some synthetic query-document pairs, and [21] simply uses the title-document pairs as substitution for training. In this study we mine implicit relevance judgments from one-year clickthr ough data, and generate a large amount of real query-title pairs for translation model training. between terms in queries and in documents [3, 34]. However, relationships are only created be tween single words by using an ad hoc similarity measure. Translation models offer a way to ex-ploit such relationships in a more principled manner, as we ex-plained earlier. of a polysemous query term. But mo st traditional retrieval models assume the occurrences of terms to be completely independent. Thus, research in this area has been focusing on capturing term dependencies. Early work tries to relax the independence assump-tion by including phrases, in addition to single terms, as indexing units [10, 32]. Phrases are defined by collocations (adjacency or proximity) and selected on the statistical ground, possibly with some syntactic knowledge. Unfo rtunately, the e xperiments did not provide a clear indication whet her the retrieval effectiveness can be improved in this way. various approaches that go beyond unigrams have been proposed to capture some term dependencie s, notably the bigram and tri-gram models [31], the dependence model [14], and the Markov Random Field model [25]. These m odels have shown benefit of capturing dependencies. However, they focus on the utilization of phrases as indexing uni ts, rather than the relationships between phrases. [4] tried to determine su ch relationships using more com-plex term co-occurrences within documents. Our study tries to ex-tract such relationships according to clickthrough data. Such rela-tionships are expected to be more effective in bridging the gap be-tween the query and document sub-languages. To our knowledge, this is the first such atte mpt using clickthrough data. model that determines a probabilit y distribution over  X  X ranslations X  of multi-word phrases from title to query. Our phrases are different from those defined in the previous work. Assuming that queries and documents are composed using tw o different  X  X anguages X , our phrases can be viewed as bilingual phrases (or bi-phrases in short), which are consecutive multi-term seque nces that can be translated from one language to another as un its. As we will show later, the use of the bi-phrases not only bridges the lexical gap between que-ries and documents, but also redu ces significantly the ambiguities in Web document retrieval. Language differences between sear ch queries and Web documents have often been assumed in prev ious studies without a quantita-tive evaluation [e.g., 2, 16, 33]. Following and extending the study in [18], we performed a large s cale analysis of Web and query collections for the sake of quan tifying the language discrepancy between search queries and Web documents. the analysis. The collection is built from the English Web docu-ments, in the scale of trillions of tokens, served by a popular commercial Web search engine. The collection consists of several n -gram data sets built from different Web sources, including the different text fields from the Web documents such as body text, anchor texts, and titles, as well as search queries sampled from one-year worth of search query logs. gram dataset from a different data source. They are the standard word-based backoff n -gram models, where the n -gram probabili-ties are estimated using maximum likelihood estimation (MLE) with smoothing [26].
 how certain a language model trained on one data in one language (e.g., titles) predicts the data in another language (e.g., queries). We use perplexity to measure the certainty of the prediction. Lower perplexities mean higher certainties, and consequently, a higher similarity betw een the two languages. trained on different data sources tested on a random sample of 733,147 queries from the search engine X  X  May 2009 query log. The results suggest several conclusions. First, a higher order lan-guage model in general reduces pe rplexity, especially when mov-ing beyond unigram models. This verifies the importance of cap-turing term dependencies. Second, as expected, the query n-gram Unigram 13242 4164 3633 1754 Bigram 5567 966 1420 289 4-gram 5785 731 1382 168 Table 2: Perplexity results on test queries, using n -gram models with different orders, derive d from different data sources. language models are most predictive for the test queries, though they are from independent query log snapshots. Third, it is inter-esting to notice that although the body language models are trained on much larger amounts of data than the title and anchor models, the former lead to much higher perplexity values, indicat-ing that both title and anchor text s are quantitatively much more similar to queries than body texts. We also notice that in the case of lower order (1-2) models, the ti tle models have lower perplexi-ties than the anchor models, but a higher order anchor model re-duces the perplexity more. This s uggests that title X  X  vocabulary is more similar to that of queries than anchor texts whereas the or-dering in the n -gram word structure captured by the anchor lan-guage models is more similar to the test queries than that by the title language models. guage differences (measured in te rms of perplexity) affect the performance of Web document retrieval. A Web document is composed of several fields of information. The field may be written either by the author of the Web page, such as body texts and titles, or by other authors, such as anchor texts and query clicks. The former sources are called content fields and the latter sources popularity fields [33]. struction of popularity fields is tr ickier because they have to be aggregated over information about the page from other authors or users. Popularity fields are hi ghly repetitive for popular pages, and are empty or very short for new or less popular (or so-called text of all incoming links to the page. The query click field is built from query session data, sim ilar to [16]. The query click data consists of query sessions extracted from one year query log files of a commercial search engine . A query session consists of a user-issued query and a rank of documents, each of which may or may not be clicked by the user. The query click field of a docu-ment d is represented by a set of query-score pairs ( q , Score ( d , q )), where q is a unique query string and Score ( d , q ) is a score as-signed to that query. Score ( d , q ) could be the number of times the document was clicked on for that query , but it is important to also consider the number of times the page has been shown to the user and the position in the ranked list at which the page was shown. Figure 1 shows a fragment of the query click field for the docu-ment http://webmessenger.msn.com , where Score ( d , q ) is com-puted using the heuristic scoring function in [16]. ate query-document pairs for translation model training. As shown in Figure 1, we can form a set of query-title pairs by align-ing the title of the document (e.g., the title of the document http://webmessenger.msn.com is  X  msn web messenger X  ) to each unique query string in the query cl ick field of the same document. In this study, we use titles, inst ead of anchor and body texts, to #unigram 1.2B 60.3M 150M 251.5M #bigram 11.7B 464.1M 1.1B 1.3B Total 1.3T 11.0B 257.2B 28.1B N-gram entries as well as other statistics and model parameters are stored. 
Table 1: Statistics of the Web n -gram language model collection (count cutoff = 0 for all models). These models will be released to the research community at [1]. form training data for two reasons. First, titles are more similar to queries both in length and in vo cabulary (Table 2), thus making word alignment and translation model training more effective. Second, as will be shown later (T able 3), for the pages with an empty query click field, the title field gives a very good single-field retrieval result on our test set, although it is much shorter than the anchor and body fields, and thus it can serve as a reason-able baseline in our experiments. Nevertheless, our method is not limited to the use of titles. It can be applied to other content fields later. data set, called the evaluation data set hencefor th, containing 12,071 English queries sampled from one-year query log files of a commercial search engine. On average, each query is associated with 185 Web documents (URLs). Each query-document pair has a relevance label. The label is human generated and is on a 5-level relevance scale, 0 to 4, with 4 meaning document d is the most relevant to query q and 0 meaning d is not relevant to q . All the retrieval models used in this study (i.e., BM25, language mod-els and linear ranking models) contai n free parameters that must be estimated empirically by trial and error. Therefore, we used 2-fold cross validation: A set of results on one half of the data is obtained using the parameter setti ngs optimized on the other half, and the global retrieval results are combined from those of the two sets. mean Normalized Discounted Cumulative Gain (NDCG) [19]. We report NDCG scores at truncation levels 1, 3, and 10. We also perform a significance test, i.e., a t-test with a significance level of 0.05. A significant difference shoul d be read as significant at the 95% level. ing a single content or popularity fiel d. This is aimed at evaluat-ing the impact of each single field on the retrieval effectiveness. The retrieval results are more or less consistent with the perplexity results in Table 2. The field that is more similar to search queries gives a better NDCG score. Most notable is that the body field, though much longer than the title and anchor fields, gives the worst retrieval results due to the substantial language discrepancy from queries. The anchor field is slightly better than the title field because the anchor field is on average much longer, though in Table 2 the anchor unigram model shows higher a perplexity val-ue than the title unigram model. Therefore it would be interesting Body 0.2798 0.3121 0.3858 Title 0.3181 0.3413 0.4045 Anchor 0.3245 0.3506 0.4117 Query click N/A N/A N/A 
Table 3 : Ranking results of three BM25 models, each using a different single field to represen t Web documents. The click field of a document in the evalua tion data set is not valid. to learn translation models from click-anchor pairs in addition to click-title pairs. We leave it to future work. field, when it is valid, is the most effective for Web search. How-ever, click information is unavail able for many URLs, especially new URLs and tail URLs, leaving their click fields invalid (i.e., the field is either empty or unrel iable because of sparseness). In this study, we assume that each document contained in the evalua-tion data set is either a new URL or a tail URL, thus has no click information (i.e., its click field is invalid). Our research goal is to investigate how to learn title-query translation models from the popular URLs that have rich c lick information, and apply the models to improve the retrieval of those tail or new URLs. Thus, in our experiments, we use BM25 with the title field as baseline. very large amounts of query-title pa irs. For training translation models in this study, we used a randomly sampled subset of 82,834,648 pairs whose documents are popular and have rich click information. We then test the trained models in retrieving documents that have no click information. The empirical results will verify the effectiveness of our methods. Let Q = q 1 ... q J be a query and D = w 1 ... w ment. The word-based translation model [7] assumes that both Q and D are bag of words, and that the translation probability of Q given D is computed as  X   X   X  |  X   X  is the probability of translating w into a query term q . into itself, Equation (1) is reduced to the simple exact term match-ing model. In general, the model allows us to translate w to other semantically related query term s by giving those other terms a nonzero probability. This section describes two methods of estimating the word trans-i.e., the query-title pairs, denoted by  X  X  X  X   X   X ,  X   X  X  X  X 1...  X , X  , derived from the clickthrough data, as described in Section 4. statistical word ali gnment models proposed in [9]. Formally, we optimize the model parameters  X  by maximizing the probabili-ty of generating queries from titles over the training data: Figure 1: A fragment of the query click field for the page http://webmessenger.msn.com [16]. where  X   X   X  |  X , X   X  takes the form of IBM Model 1 [7] as title D . To find the optimal word translation probabilities of Model 1, we used the EM algorit hm [13], running for only 3 itera-tions over the training data as a means to avoid overfitting. The details of the training process can be found in [9]. A sample of the resulting translation probabilities is shown in Figure 2, where a title word is shown together with the ten most probable query terms that it will translate according to the model. This model is considerably simpler and easier to estimate. It does not require learning word al ignments, but approximates  X  X , X | X  X  X  by a variant of the Dice coefficient: where  X   X   X , X   X  is the number of query-title pairs  X  X , X  X  in the training data, where q occurs in the query part and w occurs in the title part, and  X   X   X   X  is the number of query-title pairs where w occurs in the title part. The word-based translation model of Equation (1) needs to be smoothed before it can be applied to document ranking. We fol-low [7] to define a smoothed model as Here,  X   X   X  X | X  X  is a linear interpolation of a background unigram model and a word-based translation model:  X   X   X   X  |  X   X   X  X  X   X   X  |  X   X   X   X   X 1 X   X   X  X   X   X  |  X   X   X   X   X  |  X   X  where  X   X   X 0,1 X  is the interpolation weight empirically tuned,  X   X   X  |  X   X  is the word-based translation model estimated using either of the two methods descri bed in Section 5.1, and  X   X   X  |  X   X  and  X  X | X  X  X  are the unsmoothed backgr ound and document models, respectively, estimated using ma ximum likelihood estimation as where  X  X ; X  X  X  and  X  X ; X  X  X  are the counts of q in the collection and in the document, respectively; and | X | and | X | are the sizes of the collection and the document, respectively. perform well in our retrieval experiments due to the low self-translation problem. This problem ha s also been studied in [36, 20, 24, 21]. Since the target and the source languages are the same, every word has some probability to translate into itself, i.e., P ( q = w | w ) &gt; 0. On the one hand, low self-translation probabilities reduce retrieval performance by giving low weights to the match-ing terms. On the other hand, ve ry high self-probabilities do not exploit the merits of the translation models. translation problem [36, 20, 24, 21]. These approaches assume that the self-translation probabilities estimated directly from data, e.g., using the methods described in Section 5.1, are not optimal for retrieval, and have demonstrat ed that significant improvements can be achieved by adjusting the probabilities. We compared these approaches in our experiments. The best performer is the one proposed by Xue et al. [36], wher e Equation (6) is revised as Equ-ation (9) so as to explicitly adju st the self-translation probability by linearly mixing the transla tion based estimation and maximum likelihood estimation  X   X  X   X   X | X   X   X  X  X   X   X  |  X   X   X   X   X 1 X   X   X  X   X   X  |  X   X   X   X   X  |  X   X  Here,  X   X   X 0,1 X  is the tuning parameter, indicating how much the self-translation probability is adjusted. Notice that letting  X 1 X  in Equation (9) reduces the model to a unigram language model with Jelinek-Mercer smoothing [37].  X  X | X  X  X  in Equation (9) is the unsmoothed document mode l, estimated by Equation (8). So we have  X   X   X  |  X   X   X  X   X   X 0,for . Table 4 shows the main document ranking results using word-based translation models, tested on the human-labeled evaluation dataset via 2-fold cross validation, as described in Section 4. Row 1 is the baseline model. Rows 2 to 5 are different versions of the word translation based retrieval model, parameterized by Equa-tions (5) to (9). All these models achieve significantly better re-sults than the baseline in Row 1. By setting  X 1 X  in Equation (9), the model in Row 2 is equivale nt to a unigram language model with Jelinek-Mercer smoothing. Row 3 is the model where the word translation probabilities are assigned by Model 1 trained by the EM algorithm. Row 4 is similar to Row 3 except that the self-
Figure 2: Sample word translation probabilities after EM training on the query-title pairs. translation probability is not adjusted, i.e.,  X 0 X  in Equation (9). Row 5 is the model where the word translation probabilities are estimated by the heuristic model of Equation (4). simple unigram language model perfo rms similarly to the classical probabilistic retrieval model BM25 (Row 1 vs. Row 2); (2) using word translation model trained on query-title pairs leads to statis-tically significant improvement (Row 3 vs. Row 2); (3) it is bene-ficial to boost the self-translati on probabilities (Row 3 vs. Row 4 is statistically significant in NDCG@1 and NDCG@3); and (4) Model 1 outperforms the heuristic model with a small but statisti-cally significant margin (Row 3 vs. Row 5). Analyzing the varia-tion of the document retrieval performance as a function of the EM iterations in Model 1 training is instructive. As shown in Figure 3, after the first iteration, Model 1 achieves a slightly worse retrieval result than the heuristic model, but the second iteration of Model 1 gives a significantly better result. The phrase-based translation model is a generative model that translates a document title D into a query Q . Rather than translat-ing single words in isolation, as in the word-based translation model, the phrase model translat es sequences of words (i.e., phrases) in D into sequences of words in Q , thus incorporating contextual information. For example, we might learn that the phrase "stuffy nose" can be translated from "cold" with relatively high probability, even though neither of the individual word pairs (i.e., "stuffy"/"cold" and "nose" /"cold") might have a high word translation probability. We assume the following generative story: first the title D is broken into K non-empty word sequences w ,..., w k , then each is translated to a new non-empty word se-quence q 1 ,..., q k , finally these phrases are permuted and concate-nated to form the query Q . Here w and q denote consecutive se-quences of words. tation of D into K phrases w 1,..., w K , and let T denote the K transla-tion phrases q 1 ,..., q K  X  we refer to these ( c i , q Finally, let M denote a permutation of K elements representing the final reordering step. Figure 2 demonstrates the generative proce-dure. If we assume a uniform probabilit y over segmentations, then the phrase-based translation probability can be defined as: Then, we use the maximum a pproximation to the sum: titles to queries, our goal is not to generate new queries, but rather to provide scores over existing Q and D pairs that will be used to rank documents. However, the mode l cannot be used directly for document ranking because D and Q are often of very different lengths, leaving many words in D unaligned to any query term. This is the key difference between our task and the general natural language translation. As pointed out by Berger and Lafferty [7], document-query translation requires a distillation of the document, while translation of natural language tolerates little being thrown away. form the distillation of the document, and assume that a query is translated only from the key title wo rds. In this work, the key title words are identified via word alignment. Let A = a  X  X idden X  word alignment, wh ich describes a mapping from a query term position j to a title word position a the positions of the key title words are determined by the Viterbi alignment A * , which can be obtained using Model 1 (or the heuris-tic model) as follows: tention to those S , T , M triples that are consistent with A we denote as B (C, Q , A * ). Here, consistency requires that if two words are aligned in A * , then they must appear in the same bi-# Models NDCG@1 NDCG@3 NDCG@10 1 BM25 0.3181 0.3413 0.4045 2 WTM_M1 (  X  =1) 0.3202 0.3445 0.4076 3 WTM_M1 0.3310 0.3566 0.4232 4 WTM_M1 (  X  =0) 0.3210 0.3512 0.4211 5 WTM_H 0.3296 0.3554 0.4215 Table 4 : Ranking results on the evaluation data set, where only the title field of each document is used. Figure 3: Variations in (top) NDCG@3 score as a function of the number of the EM iterations for wo rd translation model training. Document ranking is performed by the word translation based retrieval model, pa rameterized by Equations (5) to (9).
 D: ... cold home remedies ... title S: [ X  X old X ,  X  X ome remedies X  X  segmentation T: [ X  X tuffy nose X ,  X  X ome remedy X  X  translation M: (1  X  2, 2  X  1) permutation Q:  X  X ome remedy stuffy nose X  query Figure 4: Example demonstrating the generative procedure behind the phrase-based translation model. phrase ( w i , q i ). Once the word alignment is fixed, the final permu-tation is uniquely determined, so we can safely discard that factor. Thus we rewrite Equation (11) as tion that a segmented query T = q 1 ... q K is generated from left to right by translating each phrase w 1 ... w K independently: where  X  X  X   X   X |  X   X  is a phrase translation probability, the estimation of which will be descri bed in Section 6.1. fined by Equations (10) to (16), can be efficiently computed by using a dynamic programming approach, similar to the monotone decoding algorithm described in [22]. Let the quantity  X  total probability of a sequence of query phrases covering the first j query terms.  X   X   X  |  X   X  can be calculated using the following recur-sion: 1. Initialization:  X   X   X 1 (17) 2. Induction: 3. Total:  X   X   X  |  X   X   X  X   X  (19) This section describes the way  X | X  X  X   X   X  is estimated. We follow a method commonly used in SMT [ 23, 27] to extract bilingual phrases and estimate their translation probabilities. training of Model 1 on query-titl e pairs in two directions: One is from query to title and the other from title to query. We then per-form Viterbi word alignment in each direction according to Equa-tions (12) to (14). The two alig nments are combined as follows: we start from the intersection of the two alignments, and gradually include more alignment links accord ing to a set of heuristic rules described in [27]. Finally, the bilingual phrases that are consistent with the word alignment are extr acted using the heuristics pro-posed in [27]. The maximum phrase length is five in our experi-ments. The toy example shown in Figure 5 illustrates the bilin-gual phrases we can genera te by this process. probability is estimated using relative counts: training data. The estimation of Equation (20) suffers the data sparseness problem. Therefore, we also estimate the so-called lexical weight [23] as a smoothed version of the phrase translation probability. Let  X  X  X  X  X  X  be the word translation probability de-scribed in Section 5.1, and A the word alignment between the 1...| w |, then the lexical weight, denoted by  X   X  puted as shown in Figure 6, where a title phr ase is shown together with the ten most probable query phrases that it will translate into accord-ing to the phrase model. Comparing to the word translation sam-ple in Figure 2, phrases lead to a set of less ambiguous, more pre-cise translations. For example, the term  X  X ista X , used alone, most likely refers to the Microsoft operating system, while in the query  X  X ierra vista X  it has a very different meaning. Similar to the case of the word translation model, directly using the phrase-based query translation model, computed in Equations (17) to (19), to rank documents does not perform well. Unlike the word-based translation model, th e phrase translation model cannot be interpolated with a unigram language model. We therefore resort to the linear ranking model framework for IR in which dif-ferent models are incorpor ated as features [15]. to a real value,  X  X  X  X , X  X  X  . The model has M parameters,  X  for m = 1... M , each for one feature func tion. The relevance score of a document D of a query Q is calculated as Because NDCG is used to measure the quality of the retrieval system in this study, we optimize  X   X  X  for NDCG directly using the Powell Search algorithm [29] via cross-validation. The features used in the linear ranking model are as follows. 
Figure 5: Toy example of (left) a word alignment between two strings "adcf" and "ABCDEF"; and (right) the bilingual phrases containing up to five words that are consistent with the word alignment titanic survivors 0.01537 az 0.00417 titanic pictures 0.00593 dominoes sierra 
Figure 6: Sample phrase translati on probabilities learned from the word-aligned query-title pairs.  X  Phrase translation feature:  X   X  X  X   X   X , X , X   X   X  X | X  X  X  X  X log ,  X  Lexical weight feature:  X   X  X  X   X   X , X , X   X   X  X  X og  X   X  |  X   X  , where  X  Phrase alignment feature:  X   X  X   X   X , X , X   X   X   X   X |  X  Unaligned word penalty feature  X   X  X  X  X   X   X , X , X   X  is defined  X  Language model feature:  X   X  X  X   X   X , X   X   X  X  X og  X   X   X  |  X   X  , where  X  Word translation feature:  X   X  X  X   X   X , X   X   X  X  X og  X   X  |  X   X  , where Table 5 shows the main results of different phrase translation based retrieval models. Row 1 a nd Row 2 are models described in Table 4, and are listed here for comparison. Rows 3 to 5 are the linear ranking models using all the features described in Section 6.2, with different maximum phrase lengths, used in the two phrase translation features, f PT and f LW . The results show that (1) the phrase-based translation model leads to significant improvement (Row 3 vs. Row 2); and (2) using longer phrases in the phrase-based translation models does not seem to produce significantly better ranking results (Row 3 vs. Rows 4 and 5 is no t statistically significant). more detail, we trained a series of linear ranking models that only use the two phrase tran slation features, i.e., f PT and f in Table 6 show that longer phra ses do yield some visible improve-ment up to the maximum length of five. This may suggest that some properties captured by longer phrases are also captured by other features. However, it will still be instructive, as future work, to explore the methods of preserving the improvement generated by longer phrases when more features are incorporated. titles. The phrases are detected us ing the Viterbi algorithm with a maximum length of 5. It is interes ting to see that while the average length of titles is much larger than that of queries, the phrases de-that many long query phrases are translated from short title phrases. There are two possible interpreta tions. First, titles are longer than queries because a title is supposed to be a summary of a web docu-ment which may cover multiple topi cs whereas a user query usually focuses on only one particular topic of the document. Second, title language is more formal and concise whereas query language is more causal and wordy. So, for a specific topic, the description in the title (title phrase) is usually more well-formed and concise than that in queries, as illustrated by the examples in Table 8. ries shown in Table 8 also helps us understand how the phrase-based translation model impacts retrieval results. The phrase model improves the effectiveness of retrieval from two aspects. First, it matches multi-word phrases in titles and queries (e.g., #1, #5, #6 and #7 query-title pairs in Table 8), thus reduces the ambiguities by capturing contextual information. Comparing with the previous approaches that are based on phrase retrieval models [10, 30] and higher-order n -gram models [31, 14], the phrase-based translation model provides an alternative, and in many cases more effective approach to dealing with the polysemy issue. Second, the phrase model is able to identify the phras e pairs that consist of different words but are semantically similar (e.g., #2, #3, #4 and #6 query-title pairs). We notice that these pairs cannot be easily captured by a word-based translation model. Thus , the phrase model is more ef-fective than the word model in bridging the lexical gap between queries and documents. In summary, the results justify that the phrase-based translation model provides a unified solution to deal-ing with both the synonymy and the polysemy issues, as we claim in the introductory section of this paper. # Models NDCG@1 NDCG@3 NDCG@10 1 BM25 0.3181 0.3413 0.4045 2 WTM_M1 0.3310 0.3566 0.4232 3 PTM ( l =5) 0.3355 0.3605 0.4254 4 PTM ( l =3) 0.3349 0.3602 0.4253 5 PTM ( l =2) 0.3347 0.3603 0.4252 Table 5 : Ranking results on the evaluation data set, where only the title field of each document is used. PTM is the linear ranking model of Equation (22), where all the features, including the two phrase translation model features f PT and f LW (with different max-imum phrase length, specified by l ), are incorporated. 
Phrase lengths NDCG@1 NDCG@3 NDCG@10 Table 6 : Ranking results on the evaluation data set, where only the title field of each document is used, using the linear ranking model of Equation (22) to whic h only two phrase translation mod-el features f PT and f LW (with different phrase lengths) are incorpo-rated. 
Table 7: Length distributions of title phrases and query phrases # Queries Titles Bi-phrases 3 jerlon hair products curlaway testimonials [jerlon hair products / curlaway] 5 recipe zucchini nut bread bellypleasers cookbook free recipe zucchini nut 7 washington tulip festival tulip festival komo news seattle washington you-8 cambridge high schools negative impact. An example is shown in #8 in Table 8. The mod-that the  X  school  X  in Q is actually an  X  elementary school  X . One possible reason is that the phrase model tries to learn bi-phrases that are most likely to be alig ned without taking into account whether these phrases are reasonable in the monolingual context (i.e., in D and Q ). Future improvement can be achieved by using an objective function in learning bi-phrases that takes into account both the likelihood of phrase alignment between D and Q, and the likelihood of monolingual phrase segmentation in D and Q . This section compares the translation models with PLSA [17], one of the most studied latent variab le models. Instead of building a full p.d.f. to probabilistically translate words in titles to words in queries, PLSA uses a factored ge nerative model for word transla-tion as where z is a vector of factors that mix to produce an observation [6]. The probabilities  X  X  X  X  X  X  and  X  X  X  X  X  X  are estimated using the EM algorithm on the query-title pairs derived from the click-through data. Empirically, the deri ved factors, frequently called topics or aspects, form a representation in the latent semantic space. Therefore, PLSA takes a different approach than phrase models to enhance the word-based translation model. Whilst the phrase model reduces the transla tion ambiguities by capturing some context information, PLSA smoothes translation probabili-ties among words occurring in similar context by capturing some semantic information. unigram language model, and use the ranking function as  X   X  X   X   X | X   X   X  X  X   X   X  |  X   X   X   X   X 1 X   X   X  X   X  X  X   X   X  |  X   X   X   X   X  |  X   X  Notice that this ranking function has a similar form to that of the word-based translation model in Equations (5) and (9). K in Equa-tion (26) is the number of factors of PLSA. Setting K =1 reduces the PLSA to the word-based transl ation model. In our experiments, we built PLSA models with K = 20, 50, 100, 200, 300, 500, and found no significant difference in retrieval results when K  X  100. lation model, using PLSA alone does not produce good retrieval results (Row 3 vs. Row 4). When mixing with unigram model, PLSA outperforms the word-based translation model by signifi-cant margins, but still slightly underperforms the phrase model. Since PLSA and phrase models use different strategies of improv-ing word models, it will be interesting to explore how to combine their strengths. We leave it to future work. # Models NDCG@1 NDCG@3 NDCG@10 4PLSA ( K =100,  X  =1) 0.3244 0.3505 0.4145 
Table 9: Comparison results of word, phrase translation models and PLSA, tested on the evaluation data set. It has often been observed that search queries and Web documents are written in very different styles and with different vocabularies. In order to improve search results, it is important to bridge queries terms and document terms. Clickt hrough data have been exploited for this purpose in several recent st udies. In this paper, we extend the previous studies by develo ping a more general framework based on translation models and by extending noisy word-based translation to more precise phrase-based translation. This study shows that many techniques deve loped in SMT can be used for IR. lation models, we use query and doc ument title pairs. This choice is motivated by the smaller la nguage discrepancy that we ob-served between queries and document titles. Two translation models are trained and integrated into the retrieval process: a word model and a phrase model. Our experimental results show that the translation models brin g significant improvements to re-trieval effectiveness. In particular, the use of the phrase transla-tion model can bring additional improvements over the word translation model. This suggests the high potential of applying more sophisticated sta tistical machine transl ation techniques for improving Web search. The authors would like to thank Chris Quirk, Xiaolong Li, Kuan-san Wang and Guihong Cao for the very helpful discussions and collaboration. [1] Microsoft web n-gram services. [2] Agichtein, E., Brill, E. and Dumais, S. 2006. Improving web [3] Baeza-Yates, R. and Tiberi, A. 2007. Extracting semantic [4] Bai, J., Nie, J-Y., Cao, G., and Bouchard, H. 2007. Using [5] Bai, J., Song, D., Bruza, P., Nie, J-Y., and Cao, G. 2005. [6] Berger, A., Caruana, R., Cohn, D., Freitag, D., and Mittal, V. [7] Berger, A., and Lafferty, J. 1999. Information retrieval as [8] Blei, D. M., Ng, A. Y., and Jo rdan, M. J. 2003. Latent Di-[9] Brown, P. F., Della Pietra, S. A., Della Pietra, V. J., and [10] Buckley, D., Allan, J., and Salton, G. 1995. Automatic re-[11] Cao, G., Nie, J-Y., Gao, J., a nd Robertson, S. 2008. Selecting [12] Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T., [13] Dempster, A., Laird, N., and Rubin, D. 1977. Maximum [14] Gao, J., Nie, J-Y., Wu, G., and Cao, G. 2004. Dependence [15] Gao, J., Qin, H., Xia, X. and Nie, J-Y. 2005. Linear discri-[16] Gao, J., Yuan, W., Li, X., De ng, K., and Nie, J-Y. 2009. [17] Hofmann, T. 1999. Pr obabilistic latent sema ntic indexing. In [18] Huang, J., Gao, J., Miao, J., Li, X., Wang, K., and Behr, F. [19] Jarvelin, K. and Kekalainen, J. 2000. IR evaluation methods [20] Jeon, J., Croft, W. B., and Le e, J. H. 2005. Finding similar [21] Jin, R., Hauptmann, A. G., an d Zhai, C. 2002. Title language [22] Jones, K. S., Walker S., and Robertson, S. 1998. A probabil-[23] Koehn, P., Och, F., and Marc u, D. 2003. Stat istical phrase-[24] Murdock, V., and Croft, W. B. 2005. A statisitcal model for [25] Metzler, D., and Croft, W. B. 2005. A Markov random field [26] Nguyen, P., Gao, J., and Mahajan, M. 2007. MSRLM: a [27] Och, F. 2002. Statistical machine translation: from single-[28] Och, F., and Ney, H. 2004. The alignment template approach [29] Press, W. H., Teukolsky, S. A. , Vetterling, W. T., and Flan-[30] Rocchio, J. 1971. Relevance feedback in information [31] Song, F., and Croft, B. 1999. A general language model for [32] Sparck Jones, K. 1998. What is the role of NLP in text [33] Svore, K., and Burges, C. 2009. A machine learning ap-[34] Wen, J. Nie, J.Y. and Zha ng, H. 2002. Query Clustering [35] Xu, J., and Croft, W. B. 2000. Improving effectiveness of [36] Xue, X., Jeon, J., and Croft, B. 2008. Retrieval models for [37] Zhai, C., and Lafferty, J. 2001. A study of smoothing me-
