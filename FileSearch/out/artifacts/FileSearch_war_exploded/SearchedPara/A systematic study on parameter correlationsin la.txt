 REGULAR PAPER Shaozhi Ye  X  Ji-Rong Wen  X  Wei-Ying Ma Abstract Although much work has been done on duplicate document detection (DDD) and its applications, we observe the absence of a systematic study on the performance and scalability of large-scale DDD algorithms. It is still unclear how various parameters in DDD correlate mutually, such as similarity threshold, precision/recall requirement, sampling ratio, and document size. This paper ex-plores the correlations among several most important parameters in DDD and curacy and scalability of DDD algorithms. An empirical analysis is conducted on a million HTML documents from the TREC .GOV collection. Experimen-tal results show that even when using the same sampling ratio, the precision of DDD varies greatly on documents with different sizes. Based on this observa-tion, we propose an adaptive sampling strategy for DDD, which minimizes the sampling ratio with the constraint of a given precision requirement. We believe that the insights from our analysis are helpful for guiding the future large-scale DDD work.
 Keywords Duplicate document detection  X  Clustering  X  Sampling  X  Shingling 1 Introduction Duplicate documents and mirrored Web sites are phenomenal on the Web. For ex-ample, it was reported that more than 250 sites mirrored the documents of Linux document project (LDP). 1 Broder et al. clustered the duplicate and nearly dupli-cate ones in 30 million documents and got 3.6 million clusters containing 12.1 million documents [ 5]. Bharat and Broder reported that about 10% of Web sites were mirrored to various extents in a study involving 238,000 sites [ 2]. duplicate and nearly duplicate documents in many applications, such as crawl-the other hand, the tremendous volume of Web pages challenges the performance and scalability of DDD algorithms. For instance, Google 2 announced to have in-dexed eight billion Web pages in April 2005. How can DDD algorithms scale up with the volume of Web and process this amount of pages in acceptable time? tions. To detect the nearly-duplicate documents (i.e., there are some slight dif-ferences between these documents), however, requires much more complex algo-rithms as well as much higher computation cost. Moreover, due to the updated la-tency and site template differences, most mirrored or copied documents are nearly duplicates [ 2]. Therefore, we focus on the nearly-duplicate document detection and the DDD in the rest of this paper refers to nearly-duplicate document detec-tion unless explicitly stated.
 for large-scale document sets in [ 5]. Many applications and the following research, such as [ 2, 3, 12, 14, 23], later adopted this algorithm for its simplicity and effi-ciency.
 tions, little has been explored about the factors affecting their performance and scalability. Meanwhile, to deal with the huge volume of data, all prior work has to make some trade-offs in their implementations. How do these trade-offs affect the result? Although there has been some empirical studies on document clustering models such as [ 26], to the best of our knowledge, no previous work reports any systematic analysis on correlations among different parameters in DDD, and none of them provides a formal evaluation of their trade-off choices.
 correlations. These parameters include similarity threshold, precision/recall re-quirement, sampling ratio, and document size. Among them, sampling ratio is of most interest, for it greatly affects the accuracy and scalability of DDD algorithms. conducted in this paper. The TREC .GOV collection, which includes a million Web pages, is used as our testing dataset. Although the volume of this collec-tion is much smaller than the whole Web, we believe that this collection to some extent represents the Web for DDD algorithms [ 21]. Experimental results show that even when using the same sampling ratio, the precision of DDD on docu-ments of different sizes varies greatly. To be more specific, small sampling ratio greatly hurts the accuracy of DDD on small documents. For example, with 6 . 25% sampling ratio and 0 . 85 similarity threshold, the precision on documents having fewer than 500 words is only 0 . 57 and the overall precision for the entire doc-ument set is 0 . 70. Based on this observation, we propose an adaptive sampling method for DDD which uses dynamic sampling ratio for different document size with the constraint of given precision requirements. With our proposed method, 5 . 55% sampling ratio can achieve the precision of 0 . 85 for all documents with 0 . 85 similarity threshold. We believe that our analysis is helpful for guiding the future DDD work.
 prior work on DDD. Section 3 introduces the DDD algorithm and the document similarity metric used in this paper. Section 4 describes the experiment setup and Sect. 5 presents the experimental results on parameter correlations. Based on our discuss the possible DDD applications in Sect. 7 and conclude this paper with Sect. 8 . 2 Prior work While there are some studies on document structure similarity [ 17 ] and hyperlink connectivity similarity [ 11 ], we focus on the content similarity here. Based on the ways to calculate document similarity, the prior work on DDD can be partitioned into two categories, shingle-based and term-based algorithms, both of which can be applied offline and online. We review these algorithms in this section. 2.1 Shingle-based algorithms Based on the concept of shingle , shingle-based algorithms are widely used in large-scale DDD, such as [2 X 6, 12, 16, 20, 23]. A shingle is a set of contiguous terms in a document. Shingle-based algorithms calculate the similarity between two documents by the number of shingles they share.
 denotes the number of documents, which is not feasible when dealing with large-scale document sets. Broder et al. proposed an O ( N log ( N / m )) algorithm [ 5 ], where m denotes the size of the main memory, which is employed by most large-scale DDD works later. This algorithm can be described as follows: 1. Each document is divided into shingles and a hash value is assigned to each 2. By sorting the kN pairs received from the previous step by their hash values, 3. By scanning the sorted hash(shingle), document ID list, the number of shared 4. Traversing the document ID 1 , document ID 2 , number of shared shingles list, of shingles by their hash values. Moreover, to avoid the alignment problem, the shingling is often processed with a sliding window, which increases the number of shingles for each document (Sect. 3.1 ).
 been proposed to reduce the number of shingles to be compared.  X  Heintze selects shingles with the smallest N hash values for each document  X  Broder et al. sample one of 25 shingles by selecting the shingles whose hash  X  Another more efficient alternative is also proposed in [ 5], which combines  X  In [ 6]and[ 20], exact copies are removed in advance and then, every two or  X  Fetterly et al. use five-gram as a shingle and sample 84 shingles for each doc-sampling strategies, but none of them provides an analysis on how their sampling strategies affect the accuracy of DDD algorithms. On the other hand, sampling has to be used to keep up with the increasing volume of document sets to be examined. Hence, it is important to study the impact of sampling in DDD. 2.2 Term based algorithms Term-based algorithms [7 X 9] use individual terms (words) as the basic unit, in-stead of continuous k -gram shingles, i.e., they do not consider the relative po-sition or ordering among words. Cosine similarity between document vectors is used to calculate the similarity between documents. Many information retrieval (IR) techniques, especially feature selection, are used in these algorithms, which make them much more complex than shingle-based algorithms. The largest set processed by term-based algorithms contains only about 500 K Web pages [ 7 ]. of them also achieve good performance when used online. But for search engines which need to answer over 100 M queries everyday, online methods are not a good choice because of their prohibitive computation cost. Meanwhile, in some appli-cations, we have to do DDD offline; for example, there may be no query available gle based offline approaches and do not discuss more about term-based or online methods. 3 Algorithm Although much work has been done on DDD algorithms and many applications employ DDD techniques, there is no systematic analysis on how the parameters in DDD correlate, such as accuracy, similarity threshold, and sampling ratio, cor-relate. And there is also no formal study on the accuracy and scalability of DDD. This paper aims to explore these problems. We choose the method described in [ 5 ] for analysis since many DDD algorithms and applications follow it. We believe our conclusions can also guide other DDD algorithms, especially in sampling strate-gies.
 the details of two basic issues in this algorithm, shingling and document similarity . 3.1 Sliding window shingling Since the exactly duplicate documents, which have no differences between two documents, are easy to identify by comparing the fingerprints (hashes) of the whole document, this paper focuses on nearly duplicates, which have slight differ-ences between two documents. To find the differences and calculate the document similarity, documents have to be broken into smaller pieces, i.e., shingles. process to divide documents into shingles. First, each document is viewed as a sequence of words and is transformed into a canonical sequence of tokens. This canonical form ignores minor details such as formatting and HTML tags. Then every document D is associated with a set of subsequences of token S ( D ,w) , i.e., shingles.
 unique shingles with size w contained in D . For instance, the 3-shingling of  X  X he ones we don X  X  know we don X  X  know X  is the set {  X  X he ones we X ,  X  X nes we don X  X  X ,  X  X e don X  X  know X ,  X  X on X  X  know we X ,  X  X now we don X  X  X  } . Although the shingle  X  X e don X  X  know X  appears twice, only one is kept.
 between two documents with minor differences.
 shingle size affects the performance of DDD. Generally, greater w results in higher precision and lower recall. In our own experiences, although greater w produces fewer shingles for each document, greater w also hurts the recall of DDD. So a moderate w is chosen to get a balance between precision and recall [ 16]. 3.2 Document similarity We choose the resemblance mentioned in [ 5] as our document similarity metric for its widely usage in DDD. We believe that the conclusions based on this similarity metric can be easily extended to other similarity metrics.
 in S .
 difference between the two documents, the greater is their resemblance .A simi-larity threshold will be set based on application scenarios to determine whether the two documents are duplicate ones, i.e., if the resemblance between the two documents is greater or equal to the similarity threshold , they are identified as duplicate to each other, otherwise not. 4 Experiment setup In this section, we describe our experiment setup, including the testing dataset, preprocessing, the hash function, sampling strategies, and other implementation issues. 4.1 Data description There are several datasets used in prior work, most of which are not publicly avail-able. Chowdhury et al. [ 7] choose 2 GB NIST Web pages and TREC disks 4 and 5 collections as their testing data, but these two sets contain only 240 and 530 K documents, respectively. In this paper, we choose the TREC .GOV collection 3 as our experiment dataset, which contains about a million HTML documents and is widely used in Web-related research. Table 2 summarizes the main properties of this dataset. 4.2 Data preprocessing Firstly, each document is canonicalized by removing all the HTML formatting information. Secondly, special characters, such as Horizontal Tab (HT), Line Feed (LF) and Carriage Return (CR), are converted into spaces, and then continuous spaces are replaced by one space. Thus each document is converted into a string of words separated by single spaces.
 duplicate documents. By calculating MD5 hash for each document, we cluster exactly duplicate documents. For each cluster, only one document is kept as the representative for the cluster and the other documents are removed. As a result, 94,309 documents are removed from the collection and the final set contains 958,725 documents.
 dataset, we rank the documents by their size, in terms of words, shown in Fig. 1 . into 11 groups based on the number of words they have, as shown in Table 3. 4.3 Hash function To speed up the shingling process, 32-bit and 40-bit Rabin [ 19] hash functions areusedinpriorworks[ 2, 3, 5, 6, 20], which can be computed efficiently with the sliding window shingling. However, for large datasets with several millions of documents and several billions of shingles, 32-bit or 40-bit hash may produce many false positives, i.e., different shingles share the same hash value. A 40-bit perfect hash function has the probability 1 / 2 to have a collision (false positive) with about 2 20 (a million) random hashes [ 1]. In this paper, we use the well-known 128-bit MD5 hash function (RFC 1321) for both document fingerprints and shin-gle fingerprints, which generates many less false positives. A 128-bit perfect hash requires 2 64 random hashes to have a collision with 1 / 2 probability. 4.4 Sampling To study the impact of sampling ratio in DDD, the numerical hash value is used to select (sample) shingles. For example, when the sampling ratio is 1 / 2, we run two trials by selecting shingles with odd and even hash value, respectively, and then calculate the average performance based on these two trials. Thus, when the sampling ratio is 1 / n ,werun n trials by selecting the shingles with different re-mainders divided by n , ranging from 0 to n  X  1, and then compute the average performance of all n trials.
 shingles and find that the actual selection ratio is consistent with the given sam-pling ratio. Moreover, there are only slight differences between the performance (in terms of precision/recall) of different trials with the same sampling ratio, which also indicates that MD5 is a good hash function for this sampling task. 4.5 Implementation issues We implement Broder X  X  algorithm and run DDD experiments with different sim-ilarity threshold and sampling ratio combinations for each group. Since the sizes of groups vary greatly, we implement two versions of DDD. For small groups, we use the map in STL (C++ Standard Template Library) to store shingles; thus, all the shingles are kept in memory. For large groups which may produce more than 2 GB shingles, we use the BTREE structure in BerkeleyDB, 4 which is much slower than the map version because of disk I/O.
 2 GHz Xeon CPU and the other two with 3 GHz Xeon CPU. It took two weeks to run about 400 trials with different parameter combinations for each trial. main acceleration trade-offs in their approach. First, they use 1 / 25 sampling ratio and at most 400 shingles are used for each document. They also discard common shingles which are shared by more than 1,000 documents. Second, they divide the data into pieces to fit the main memory. However, [ 5 ] does not give the size of each piece. It just mentions that  X  X he final file containing the list of the documents in each cluster took up less than 100 Mbytes X . Thus, we believe that the size of duplicates across different clusters will be missed. Moreover, although the CPU speed has been greatly improved since then, the speed of RAM and disk does not advances much. So our experiments are very time-consuming, although we use much more powerful hardware than do the previous works. 5 Experimental results In this section, we present our experimental results and discuss the parameter cor-relations among precision/recall, sampling ratio, similarity threshold, and docu-ment size.
 sampling DDD result with this ground truth to calculate the precision/recall. The non-sampling DDD means that all the shingles are kept and used to compute the document similarity. The sampling DDD means that sampling is applied to the shingle set for each document. If two documents are determined as duplicates by sampling DDD while they are not determined as duplicates by non-sampling DDD, it is a false positive pair.
 sampling DDD, and P S denotes the duplicate document pairs detected by sam-pling DDD, the precision of a trial (a run with a given parameter combination) is defined as follows.
 And the recall of the trial is defined as follows.
 5.1 Precision The experimental results of 1 / 4and1 / 16 sampling ratios are shown in Fig. 2(a) and (b).
 ilarity threshold. The curve of Group 0, documents having fewer than 500 words, decreases significantly. In Fig. 2, the highest precision on Group 0 is lower than 0 . 8 as long as the similarity threshold is greater than 0 . 5. Also, the precision on the groups with documents having fewer than 3,000 words drops below 0 . 1when the similarity threshold is higher than 0 . 95. ments are sensitive to sampling and it is hard for them to achieve good precision when small sampling ratio or high similarity threshold is required. On the other when the similarity threshold is high and sampling ratio is small. Our experiments with sampling ratio 1 / 2and1 / 8 also show similar properties as with 1 / 4and1 / 16 sampling ratios.
 the precision of their results. For example, in [ 5 ], 1 / 25 sampling ratio is used, whose precision is no higher than 0 . 55 for 68% documents in our dataset. 5.2 Recall Generally, sampling ratio does not hurt recall because sampling only generates false positives. While for small documents, recall may drop because some of the documents have no shingle or not enough shingles sampled. As shown in Fig. 3 , even with 1 / 16 sampling ratio, the minimum recall is higher than 0.6 and does not drop much with the increasing similarity threshold. 5.3 Summary of parameter correlations Here, we summarize the correlations between precision and other parameters.  X  Sampling ratio : Precision drops with the decrease of sampling ratio, especially  X  Document size : Small documents are more sensitive to similarity threshold 6 Adaptive sampling strategy Based on the observations in Sect. 5, we propose an adaptive sampling strategy for large-scale DDD in this section. The basic idea is to apply small sampling ratio to large documents and large sampling ratio to small documents.
 periment. The TREC .GOV collection is partitioned into 11 groups, as shown in Ta b l e 3. For every group, we minimize the sampling ratio out of (1 / 2, 1 / 4, 1 / 8, 1 / 16), subjected to given precisions ranging from 0 . 5to0 . 99, thus, we minimize the total shingles to process. For example, with the precision requirement 0 . 8and similarity threshold 0 . 6, we choose 1 / 8 sampling ratio for Group 0 and 1 / 16 sam-pling ratio for the other 10 groups, which results in only 8% of the total shingles to process. As shown in Fig. 4, our algorithm greatly reduces the shingles to process, and thus, can deal with larger document sets than the previous unified sampling strategy.
 experiment is 1 / 16, smaller sampling ratios can be used. For example, with the precision requirement 0 . 8 and similarity threshold 0 . 6, we can actually apply 1 / 8 sampling ratio to Group 0 and 1 / 32 sampling ratio to the other 10 groups, which results in only 4 . 72% shingles to process. The optimal sampling ratios and group divisions can be estimated by experiments on representative datasets or a subset sampled from the dataset we want to examine.
 documents consist of a large proportion of the whole document collection. For instance, in the TREC .GOV dataset, the documents having fewer than 500 words consist of 68% of the whole collection. For higher precision, we cannot do small sampling to these small documents, otherwise it would greatly hurt the overall precision. Fortunately these small documents contribute only 17% shingles; thus, although large sampling ratio is applied to these documents, our adaptive sampling strategy greatly reduces the total shingles by applying small sampling ratio to large documents. 7 DDD applications DDD techniques benefit many Web applications due to the high duplication of Web. We discuss some potential applications here.  X  Archiving : By clustering duplicate documents, large-scale search engines can  X  Crawling : As proposed in [ 6 ], DDD can be used to detect duplicate groups  X  Ranking :Yeetal.[ 23 ] report that there are 5 . 5% duplicate entries in the search  X  Noise detection : Clustering duplicate documents also helps noise detec-8Conclusion Although much work has been done on duplicate document detection and many applications employ this technique, little has been explored on its performance and scalability. In this paper, a systematic study on parameter correlations in DDD is conducted and several of the most important parameters of DDD are analyzed. DDD, especially for small documents which consist of a major fraction of the whole Web. Based on this observation, an adaptive sampling strategy is proposed, which minimizes the sampling ratio of documents with a constraint of given pre-cision thresholds, making DDD feasible to process large-scale document collec-future DDD work.
 References Author Biographies
