 Syntactic parsing for Japanese has been dominated by a dependency-based pipeline architecture in which chunk-based dependency parsing is applied and then semantic role labeling is done on the dependencies [Hayashibe et al. 2011; Iida and Poesio 2011; Kawahara and Kurohashi 2011; Kudo and Matsumoto 2002; Sasano and Kurohashi 2011]. This dominance is mainly because chunk-based dependency analysis appar-ently seems most appropriate for Japanese syntax due to its morphosyntactic typol-ogy, which includes agglutination and scrambling [Bekki 2010]. However, it is also true that this type of analysis has prevented us from deeper analysis such as deep parsing [Clark and Curran 2007; Miyao and Tsujii 2008] and logical inference [Bos 2007; Bos et al. 2004].

In this article, we present our work on inducing wide-coverage Japanese resources based on Combinatory Categorial Grammar (CCG) [Steedman 2001]. Our work is basi-cally an extension of a seminal work on CCGbank [Hockenmaier and Steedman 2007], in which the phrase structure trees of the Penn Treebank (PTB) [Marcus et al. 1993] are converted into CCG derivations and a wide-coverage CCG lexicon is extracted from these derivations. Since CCGbank has enabled a variety of outstanding studies on wide-coverage deep parsing for English, our resources are expected to significantly contribute to Japanese deep parsing.

The application of the CCGbank method to Japanese, however, is not trivial, as resources like PTB are not available in Japanese. In Japan, the widely used resources for parsing research are the Kyoto corpus [Kawahara et al. 2002; Kurohashi and Nagao 2003] and the NAIST text corpus [Iida et al. 2007], both of which are corpora with annotations of dependency structures of chunks. An internal structure of a chunk is dependent on the words comprising the chunk. Moreover, a dependency between two chunks can be interpreted as a relation of one chunk to a certain part of the other, as well as between two complete chunks. Therefore, the relation between a chunk-based dependency structure and a CCG derivation is not obvious.

We propose a method for integrating multiple dependency-based corpora into phrase structure trees augmented with predicate argument relations. We can then convert the phrase structure trees into CCG derivations. In the following, we describe the details of the integration method as well as Japanese-specific issues in the conversion. We empirically evaluate the method in terms of the quality of the corpus conversion, the coverage of the obtained lexicon, and the accuracy of parsing with the obtained grammar. Additionally, we discuss problems that remain in Japanese resources from the viewpoint of developing CCG derivations.

There are three primary contributions of this article: (1) we provide the first com-prehensive results for Japanese CCG parsing, (2) we present a methodology for in-tegrating multiple dependency-based resources to induce CCG derivations, and (3) we investigate the possibility of further improving CCG analysis by using additional resources.

Our proposed method is not limited to Japanese. It should be possible to apply a sim-ilar method to other languages for which chunk-based dependency analysis is widely regarded as more appropriate than word-based phrase structure analysis due to mor-phosyntactic features similar to Japanese.

In Section 2, we introduce a CCG-based theory of Japanese syntax and related works on the induction of CCG resources. In Section 3, we explain the details of our method for developing Japanese CCG resources. In Section 4, we present the experimental results on evaluating the obtained resources and conclude the article in Section 5 with a brief summary and a mention of future work. Combinatory Categorial Grammar is a syntactic theory widely accepted in the NLP field [Steedman 2001]. A grammar based on CCG theory consists of categories ,which represent syntactic categories of words and phrases, and combinatory rules , which are rules to combine the categories. Categories are either ground categories such as S and NP or complex categories in the form of X / YorX \ Y, where X and Y are the categories. Category X / Y intuitively means that it becomes category X when it is combined with another category Y to its right, and X \ Y means it takes a category Y to its left. Cat-egories are combined by applying combinatory rules (Figure 2) to form categories for larger phrases. Figure 1 shows a CCG analysis of a simple English sentence, which is called a derivation . The verb give is assigned category S that it takes two NPs to its right, one NP to its left, and finally becomes S. Start-ing from lexical categories assigned to words, we can obtain categories for phrases by applying the rules recursively.
 An important property of CCG is a clear interface between syntax and semantics. As shown in Figure 1, each category is associated with a lambda term of semantic representations, and each combinatory rule is associated with rules for semantic com-position. For example, the first rule in Figure 2 states that the left phrase has semantic representation f , the right phrase has a , and the result of rule application creates se-mantic representation fa . Since the rules in the figure are universal, we can obtain dif-ferent semantic representations by switching the semantic representations of lexical categories. This means that we can plug in a variety of semantic theories with CCG-based syntactic parsing [Bos et al. 2004]. For example, Bos et al. [2004] proposed a system that computes formal semantic representations based on Discourse Represen-tation Theory (DRT) [Kamp and Reyle 1993] by replacing semantic representations of the original CCG parser with discourse representation structures.
 Bekki [2010] proposed a comprehensive theory for Japanese syntax based on CCG. While the theory is based on Steedman [2001], it provides concrete explanations for a variety of morphological and syntactic constructions of Japanese, such as agglutina-tion, scrambling, and long-distance dependencies (Figure 3).

The ground categories in his theory are S, NP, and CONJ (for conjunctions). Syntac-tic features are assigned to categories NP and S (Table I). The feature case represents a syntactic case of a noun phrase. The feature form denotes an inflection form. Table II lists typical lexical categories. Predicates, i.e. , verbs, adjectives, and verbal nouns, are represented as S \ NP ga ,S \ NP ni \ NP ga or S \ NP to \ arguments. For example, S \ NP ga denotes intransitive predicates and S a transitive one. Postpositions are NP ga \ NP nc and NP ni NOM ga X  is represented as NP ga \ NP nc as it takes the left NP to form a nominative NP. Categories for auxiliary verbs require an explanation. In Japanese, auxiliary verbs are extensively used to express semantic information such as tense and modality. The auxiliaries and the main verb are combined in sequential order. For example, a verb  X   X   X  /speak-NEG X  and auxiliaries  X   X  X   X  /not-CONT X  and  X   X   X   X  X  X  X   X   X , which means  X  X id not speak X . This is explained in Bekki X  X  theory by the category S \ S, and the category is combined with a main verb via the function compo-sition rule ( &lt; B in Figure 2) as shown in Figure 4. As stated above, the feature form of category S denotes an inflection form. Since the agglutination of auxiliary verbs is restricted by inflection forms, this form feature is necessary for constraining the gram-maticality of agglutination.
 Our implementation of the grammar basically follows Bekki X  X  theory [Bekki 2010]. However, as a first step in implementing a wide-coverage Japanese parser, we focused on the frequent syntactic constructions that are necessary for computing predicate ar-gument relations, including agglutination, inflection, scrambling, and case alternation. Other details of the theory are largely simplified (Figure 3), coordination and semantic representation in particular. The current implementation recognizes coordinated verbs in continuous clauses (see Figure 6), but the treatment of other types of coordination is largely simplified. For semantic representation, we define predicate argument struc-tures (PASs) rather than the theory X  X  formal representation based on dynamic logic. A PAS consists of a predicate word, set of argument types, and argument phrases. An argument type is represented by a deep syntactic case, ga for nominative, o for accusative, ni for dative, and to for comitative, etc. , all of which are from Japanese postpositions used as case markers. For the example in Figure 3, the PAS for the pred-icate  X   X  X  /participation X  has arguments of ga and ni, where the ga-argument refers to the phrase  X   X  X  X   X  / /ambassador-NOM X  and the ni-argument is  X   X  X  X  /negotiation-DAT X . As a result of defining simple semantics, some of the grammatical distinctions that are required for semantic representations in the theory of Bekki [2010] are ex-cluded from our implementation. Sophisticating our semantic representation is left for future work.

For parsing efficiency, we modified the treatment of some constructions so that empty elements are excluded from the implementation. First, we define type chang-ing rules to produce relative and continuous clauses (shown in Figure 5). A relative clause in Japanese does not have a relativizer. Figure 7 exemplifies a relative and a continuous clause in Japanese. In the original theory, the two types of clauses are ex-plained by using empty elements: p ro, operators r el and defined rules produce almost the same results as the theory X  X  treatment but without using the empty elements. Second, we treat pro-drop and scrambling by simply adding lexical entries to the lexicon. For the sentence in Figure 3, the deletion of the nomina-tive (  X  X  X  X  / /ambassador-NOM), the dative (  X   X  X  /negotiation-DAT), or both, results in valid sentences, and shuffling the two phrases does so as well. Lexical entries with the scrambled or dropped arguments are produced using simple methods, which per-mutate or delete arguments in categories. As described in Section 1, chunk-based dependency analysis has been considered as a standard in Japanese syntactic parsing. Research on Japanese parsing also relies on dependency-based corpora. We used the following resources in this work as they have annotations for the same set of texts and can be used in a complementary way.
Kyoto corpus. This is a news text corpus annotated with morphological information, chunk boundaries, and dependency relations among chunks (Figure 8). The text con-sists of approximately 40,000 sentences from articles appearing in The Mainichi News-paper : all news articles from January 1st to 17th 1995 and all editorials from January to December 1995 [Kurohashi and Nagao 2003; Kawahara et al. 2002]. The morpholog-ical information is annotated basically based on a Japanese grammar book [Masuoka and Takubo 1989]. The dependencies are classified into four types: Para (coordination), A (apposition), I (argument cluster), and Dep (default). Most of the dependencies are annotated as Dep, so the dependencies in the corpus can be regarded as unlabeled relations.

NAIST text corpus. This is a corpus annotated with predicate argument relations for verbs, adjectives, and nouns referring to events [Iida et al. 2007]. It also contains voice information of the predicates and annotations of anaphora and coreference relations. The same set as the Kyoto corpus is annotated. 1 The labels used for predicate argu-ment annotation are deep syntactic cases. In Japanese, arguments of a predicate are typically marked by a postposition, which functions as a case marker. As a result, the label for an argument is basically the same as the postposition following the argument if the sentence has no case alternation. In the example shown in Figure 8,  X   X   X  /am-bassador X  and  X   X  X  / negotiation X  are arguments of the predicate  X   X   X  // participate X  and are followed by postpositions  X   X  / NOM / ga X  and  X   X  Therefore, their labels are ARG-ga and ARG-ni . Figure 9 shows an example involving case alternations. Since the sentence  X  X he government had the ambassador participate in the negotiation. X  is a causative sentence, arguments of the event noun  X  X  partici-pation have altered case markers. As a result,  X   X  X  X  / ambassador X , superficially with the postposition  X   X  / ACC / o X , is labeled with ARG-ga . Note that the label is the same as that in Figure 8. The corpus now focuses on three cases:  X  X a X  (subject),  X  X  X  (direct object), and  X  X i X  (indirect object).

Japanese particle corpus (JP). This is a corpus annotated with distinct grammati-cal functions of the Japanese postposition  X   X  to X  [Hanaoka et al. 2010]. The same set as the Kyoto corpus is annotated. In Japanese, the postposition  X  X o X  has many func-tions, including a complementizer (similar to  X  X hat X ), a subordinate conjunction (simi-lar to  X  X hen X ), a coordinating conjunction (similar to  X  X nd X ), and a case marker (similar to  X  X ith X ). In our work, the case marker information is used to construct to -labeled arguments for predicate argument relations, while other annotations are used for the detection of constructions such as coordination. Research on Japanese deep parsing is fairly limited. Formal theories of Japanese syn-tax were presented by Gunji [1987] based on Head-Driven Phrase Structure Grammar (HPSG) [Sag et al. 2003] and by Komagata [1999] based on CCG. Komagata [1999] has also presented implemented work, although the implementation has not been very suc-cessful in terms of parsing real-world texts. JACY [Siegel and Bender 2002] is a large-scale Japanese grammar based on HPSG that has been used for wide-coverage deep parsing of Japanese. While JACY has been successful in producing precise and detailed semantic representations for realistic sentences, our focus is more on developing deep parsing systems capable of processing a large amount of real-word text. Yoshida [2005], who led the previous studies most similar to our present work, proposed methods for extracting a wide-coverage lexicon based on HPSG from a phrase structure tree-bank of Japanese. We largely extended his work by exploiting the standard chunk-based Japanese corpora based on dependency structures and obtained the first results for Japanese deep parsing with grammar induced from large corpora. In addition, we demonstrated the first results for Japanese CCG parsing of real-world news texts.
Corpus-based acquisition of wide-coverage CCG resources has enjoyed great success for English [Hockenmaier and Steedman 2007]. In that method, PTB is converted into CCG-based derivations from which a wide-coverage CCG lexicon is extracted. CCG-bank has been used for the development of wide-coverage CCG parsers [Clark and Curran 2007]. The same methodology has been applied to German [Hockenmaier 2006], Italian [Bos et al. 2009], Turkish [C  X  ak X c X  2005], and Hindi [Ambati et al. 2013]. These works also suffered from a lack of PTB-like resources and used dependency tree-banks as source resources. Their treebanks are annotated with dependencies of words , the conversion of which into phrase structures is not a big concern. A notable contri-bution of the present work is to propose a method for inducing CCG grammars from chunk-based dependency structures, which is not obvious, as we discuss later in this article.

CCG parsing provides not only predicate argument relations but also CCG deriva-tions, which can be used for various semantic processing tasks. Bos et al. [2004], Bos [2007] proposed a method for computing DRT-based semantic representations from CCG derivations for English, and their system has been applied to NLP tasks such as textual entailment recognition [Bos 2007]. Our work constitutes a starting point for such deep linguistic processing for languages similar to Japanese. For wide-coverage CCG parsing, we need, (a) a wide-coverage CCG lexicon, (b) combi-natory rules, (c) training data for parse disambiguation, and (d) a parser ( e.g. , a CKY parser). Since d) is grammar-and language-independent, all we have to develop for a new language is (a) X (c).

We adopt the methodology of Hockenmaier and Steedman [2007]. In that work, ex-isting linguistic resources (Penn Treebank) are converted into CCG derivations (CCG-bank), which are then used for extracting a wide-coverage CCG lexicon, as well as for training parse disambiguation models [Clark and Curran 2007]. Combinatory rules can be hand-coded because the number of rules and their language dependence are limited. In our case, the number of combinatory rules is nine: four general rules in Figure 2, four type changing rules in Figure 5, and a type changing rule to handle a sentence sequence. 2 Finally, we can obtain all necessary grammatical resources (a), (b), and (c), and CCG-based deep parsing is established.

This methodology relies on a source treebank that will be converted into CCG deriva-tions. The most common resource for parsing research in English is the Penn Treebank [Marcus et al. 1993], which is used as well for the development of the English CCG-bank [Hockenmaier and Steedman 2007]. A critical issue to address is the absence of a Japanese counterpart to PTB. We only have chunk-based dependency corpora, and their relationship to CCG analysis is not clear (see Figure 10).

Our solution is to first integrate multiple dependency-based resources and convert them into a phrase structure treebank that is independent of CCG analysis (Step 1). Next, we translate the treebank into CCG derivations (Step 2). The idea of Step 2 is similar to what has been done with the English CCGbank, but obviously we have to address language-specific issues. We will describe these two steps in detail in the following. We first integrate and convert available Japanese corpora X  X amely, the Kyoto corpus, NAIST text corpus, and JP corpus X  X nto a phrase structure treebank, which is similar in spirit to PTB. As explained in Section 2, the Kyoto corpus provides morphological information and chunk-based syntactic dependencies, while the NAIST and JP corpora provide annotations of syntactic/semantic roles on top of the Kyoto corpus dependen-cies. Our approach is to convert the dependency structures of the Kyoto corpus into phrase structures then augment them with syntactic/semantic roles from the other two corpora.

The conversion involves two steps: 1) recognizing the chunk-internal structures and (2) converting interchunk dependencies into phrase structures. For (1), we do not have any explicit information in the Kyoto corpus. The corpus only provides word segmen-tations and their attributes, although each chunk has internal structures in principle [Vadas and Curran 2007; Yamada et al. 2010]. The lack of a chunk-internal struc-ture makes the dependency-to-constituency conversion more complex than a similar procedure by Bos et al. [2009], which converts an Italian dependency treebank into constituency trees, since their dependency trees are annotated down to the level of each word. For the current implementation, we abandon the idea of identifying exact structures and instead basically rely on the following generic rules (Figure 11).
Nominal chunks. Form each compound noun as a right-branching phrase then attach post-positions to the phrase.
 Verbal chunks. Form left-branching structures for verbal chunks.
 These rules amount to assuming that all but the last word in a compound noun mod-ify the head noun (the last word) and that a verbal chunk is typically in the form VA 1 ... A n , where V is a verb (or other predicative word) and A Figure 11). We chose the left-branching structure as default for a verbal chunk in order to form a similar structure to the CCG derivations in Bekki X  X  analysis. For both cases, phrase symbols are percolated upward from the right-most daughters of the branches (except for a few cases like punctuation) because in almost all cases the syntactic head of a Japanese phrase is the right-most element.

In practice, we have found several patterns of exceptions to these rules. We im-plemented exceptional patterns as small CFGs and determined the chunk-internal structures by deterministic parsing with the generic rules and the CFG. For example, two of the rules we came up with are in the precedence: rule A &gt; B &gt; generic rules. Using these rules, we bracket a com-pound noun  X   X  X  X   X   X   X   X  meaning deaths of approximately one thousand people ,asin Figure 12. We can improve chunk-internal structures to some extent by refining the CFG rules. However, accurate identification of the chunk-internal structure inherently involves the understanding of meanings, and a complete solution, such as the manual annotation by Vadas and Curran [2007], is left for future work.

The conversion of interchunk dependencies into phrase structures may sound trivial, but it is not necessarily easy when the dependency annotation only partially specifies the word-to-word dependency relations. In actuality, the Kyoto Corpus specifies only the dependency structures among chunks . Hence, we cannot know only from the annotation, which word in the head chunk the modifier chunk depends on. This in turn means that we cannot fully determine the phrase structure of the sentences only by the annotation in the Kyoto Corpus.

Setting this uncertainty aside, thanks to the strictly head-final nature of Japanese, we can formulate the process of combining two subtrees (one for a modifier chunk and the other for a head chunk) as follows. First, given two subtrees as shown in Figure 13, we select a node from Y 0 , Y 1 , ... , Y n in the head subtree, which are the nodes on the leftmost path from the root and hence visible from the modifier tree. Let Y k be the selected node, which means the modifier tree syntactically depends on the lexical head of the subtree rooted at Y k . We insert a new node Y the modifier subtree and Y k . The phrase symbol of the new node Y by percolating the phrase symbol of its head daughter, Y k
Figure 14 shows a concrete example. In Figure 14, three chunks are in the depen-dency relation indicated by the solid arrows on the top. The dotted arrows point to the nodes to which the subtrees should be attached. By recursively applying the combining operation in Figure 13, we have the phrase structure shown at the bottom of Figure 14.
The remaining problem is how to choose the sister node for a modifier subtree (how to select Y k in Figure 13). Without any additional annotations, we cannot always correctly determine it. Therefore, as a compromise, we implement approximate heuristic rules to do this. Table III lists examples of such rules. Each row defines a precedence list of patterns on the sister node for a certain type of modifier tree (Modifier-type) and for each dependency relation type (Dep-type). We have four dependency relation types that are used in the Kyoto Corpus: apposition (A), argument cluster (I), coordination (Para), and any other dependency types (Dep). Since the syntactic property of a subtree is largely determined by its lexical head, we classify the modifier subtrees and their sisters using a pair of patterns ( w , t ), where w is a pattern on the word form of the lexical head and t is a pattern on its POS tag.

To select the sister node for the leftmost subtree in Figure 14, for example, we look up the rule table using the dependency type, Para, and the lexical head of the modifier subtree,  X   X   X  /PostP cm  X , as the key. We thus find the precedence list (  X  X  , PostP &gt; ( * , Verb | Aux | Adj) &gt; ( * , * ) in the first row of Table III. Using the precedence list, we select the PP-node on the middle subtree pointed by the dotted arrow because its lexical head (the right-most word)  X   X  X  /PostP cm  X  matches the first pattern in the list. In general, we try to find a matching node for each pattern ( w , t ) in the precedence list in descending order until we find the first match. When a pattern matches more than one node, we select the one at the highest position.

After constructing phrase structures, we augment the trees with predicate-argument annotations from the NAIST and JP corpora. The semantic annotation given in the two corpora is overlaid on the phrase structure trees with slight modifications (Figure 15). In the figure, the annotation given in the two corpora is shown inside the dotted box at the bottom. We converted the predicate-argument annotations given as labeled word-to-word dependencies into the relations between the predicate words and their argument phrases . The results are thus similar to the annotation style of PropBank [Palmer et al. 2005] (see dotted arrows in Figure 15). In the NAIST corpus, each predicate-argument relation is labeled with the argument-type (ga/o/ni) and a flag indicating that the relation is mediated by either a syntactic dependency or a zero anaphora. For a relation of a predicate w p and its argument w the boundary of the argument phrase is determined as follows. (1) If w a precedes w p and the relation is mediated by a syntactic dependency, select (2) If w p precedes w a or the relation is mediated by a zero anaphora, select the In Figure 15,  X   X  /dog  X  /DAT X  is marked as the ni-argument of the predicate  X   X   X  /say X  (Case 1), and  X   X  X  /white  X  /cat X  is marked as its ga-argument (Case 2). Case 1 is for the most basic construction, where an argument PP precedes its predicate. Case 2 covers the relative clause construction, where a relative clause precedes the head NP, the modification of a noun by an adjective, and the relations mediated by zero anaphora.

The JP corpus provides only the function label to each particle  X  X o X  in the text as the label ARG-CLS in Figure 15. For the  X  X o X  particles labeled as ARG-NOM (a marker on a nominal argument) or ARG-CLS (a marker on a clausal argument) in the corpus, we determined the argument phrases marked by the  X  X o X  particles labeled as (nominal or clausal) argument-markers in a similar manner to Case 1 and identified the predicate words as the lexical heads of the phrases to which the PP The latter half of the corpus conversion translates augmented phrase structures to CCG derivations. This step consists of three substeps (Figure 16). (1) Add constraints on categories of tree nodes according to detected constructions and (2) Apply combinatory rules to all branching and obtain CCG derivations. (3) Add feature constraints to terminal nodes. 3.2.1. Local Constraints on Derivations. According to the phrase structures, the first procedure in Step 2 imposes restrictions on the resulting CCG derivations. To describe the restrictions, we focus on some of the notable constructions and illustrate the restrictions for each.

Phrases headed by case marker particles. A phrase of this type must be either an argument (Figure 17, upper) or a modifier (Figure 17, lower) of a predicative. Dis-tinction between the two is made based on the predicate-argument annotation of the predicative. If a phrase is found to be an argument, (1) category NP is assigned to the corresponding node, (2) the case feature of the category is given according to the par-ticle (in the case of the upper part of Figure 17, ni for dative), and (3) the combinatory rule backward function application rule ( &lt; ) is assigned to the branch that combines the particle and the predicative phrase. Otherwise, a category T corresponding modifier node and the rule will be forward function application (
Auxiliary verbs. As described in Section 2.2, an auxiliary verb is always given the category S \ S and is combined with a verbal phrase via &lt; more, we assign the form feature value of the returning category S according to the inflection form of the auxiliary. In the case shown in the figure, S  X   X  /PAST-BASE X  and S cont \ S for  X   X  X  X  /not-CONT X . As a result of this restriction, we can obtain conditions for every auxiliary agglutination because the two form values in S \ S are both restricted after applying the combinatory rules (Section 3.2.2).
Case alternations. In addition to the illustrated argument/adjunct distinction a pro-cess is needed for an argument phrase if the predicate involves case alternation. Such predicates are either causative (see Figure 19) or passive verbs and can be detected by voice annotation from the NAIST corpus. For an argument of that type of verb, its deep case ( ga for Figure 19) must be used to construct the semantic representation, namely the PAS. As well as assigning the shallow case value ( ni in Figure 19) to the argument X  X  category NP, as usual, we assign a restriction to the PAS of the verb so that the semantic argument corresponding to the deep syntactic case is co-indexed with the argument NP. These restrictions are then used for the PAS construction discussed in Section 3.2.3.

Relative clauses. A relative clause can be detected as a subtree that has a VP as its left child and an NP as its right child, as shown in Figure 20. Its conversion consists of, (1) inserting a node on the top of the left VP (see the right-hand side of Figure 20) and (2) assigning the appropriate unary rule to make the new node. The difference between candidate rules RelExt and RelIn (Figure 5) is whether the right-hand NP is an obligatory argument of the VP, which can be determined by the predicate-argument annotation on the predicate in the VP. In the upper example in Figure 20, RelIn is assigned because the right NP  X  X ook X  is annotated as an accusative argument of the predicate  X  X uy X  in the left tree. In contrast, RelExt is assigned in the lower side in the figure (note that restriction for the nodes is different from the upper example) because the right NP  X  X tore X  is not annotated as an argument for the predicate  X  X uy X .
Continuous clauses. Conversion for a continuous clause is similar to that for a rela-tive clause. A continuous clause can be detected as a subtree with a VP of continuous form as its left child and a VP as its right child. Its conversion is similar to that of a relative clause, and only differs in that the candidate rules are Con and ConCoord. ConCoord generates a continuous clause that shares arguments with the main clause while Con produces one without shared arguments. Rule assignment is done by com-paring the predicate-argument annotations of the two phrases (an example of a shared argument as shown in Figure 21). Since a nominative  X   X  X  / he-NOM X  is shared by the predicates of the main clause  X   X   X  X  X  / sang-BASE X  and that for the continuous clause  X   X  X   X  / danced-CONT X , unary rule ConCoord is assigned. 3.2.2. Inverse Application of Combinatory Rules. After assigning local constraints on the trees, combinatory rules are inversely applied. This begins with assigning a category S to the root node. A combinatory rule assigned to each branching is then applied in-versely so that the constraint assigned to the parent transfers to the children. In the upper part of Figure 17, given constraints on the tree  X (I) meet a friend X  are the cat-egory NP ni for the left child and the backward application rule, as illustrated on the left-hand side. After S is given to the root and the combinatory rule is applied, the category for the right child is determined as S \ NP ni . If multiple combinatory rules are assigned in Procedure 3.2.1, the applicable rule with the highest priority is cho-sen. 3 For the subtree in Figure 18, if the category S \ NP only applicable rule is the crossed composition rule , while the assignment mentioned in Section 3.2.1 also allows backward function application . By applying the rule, we obtain S 1 \ NP ga and S base \ S 1 as the children X  X  categories. The same process can be applied to the left subtree. We then obtain three concrete categories for the termi-nal, e.g. , S neg \ NP ga , S cont \ S neg ,and S base \ S  X  X ast-BASE X , respectively. 3.2.3. Constraints on Terminal Nodes. The final process consists of, (a) imposing restric-tions on the terminal category in order to instantiate all the feature values and (b) constructing a PAS for each verbal terminal. An example of process (a) includes set-ting the form features in the verb category, such as S \ NP ing to the voice and inflection form of the verb. For example, if a category S obtained for a verb  X   X  X  /meet-BASE X , the value base is set as S X  X  form feature based on the inflection form of the verb. For (b), arguments in a PAS are given according to the category and partial restriction. For example, if a category S  X   X   X  X  /inquire X  (Figure 19), the PAS for  X  X nquire X  is unary because the category has one argument category (NP ni ), and the category is coindexed with the semantic argument ga in the PAS, due to the partial restriction depicted in Section 3.2.1. However, the case for the argument is nominative ga , according to the deep case annotation. As a result, the lexical entry is obtained as  X   X  S \ NP ni [1]: inquire([1]). Finally, each of the obtained lexical entries is reduced to canonical form. Since words in the corpus (especially verbs) often involve inflection, pro-drop, and scrambling, there are many obtained entries that have slightly varied categories yet share a PAS. We assume that an obtained entry is a variation of the canonical one and register the canonical entries in the lexicon. After collecting canonical entries available from the corpora, new lexical entries for inflection, pro-drop, and scrambling are systematically produced from the canonical ones and are added to the lexicon.
 Inflection basically changes the value of the form feature of the obtained category. For instance, if a category S cont \ NP ga is extracted from  X   X   X  / walk-CONT X , contin-uative form of verb  X   X   X   X , it is regarded as derived from the non-inflected category S
For pro-drop, we now treat only subject deletion because there is not sufficient infor-mation to judge the deletion of other arguments. As described in Section 2.2, almost any argument phrase can be dropped in Japanese. To make the issue more compli-cated, a verb with multiple senses is supposed to have a different set of semantic arguments for each sense. Therefore, if a category S \ NP currence in the corpus, whether the original category is S or S \ NP ga \ NP o \ NP ni (subject and dative deletion) cannot be decided without sense disambiguation and the argument set for the sense. As there is no such annotation available for the text used for now; only the subject drop can be treated.
Scrambling is simply treated as permutation of arguments in the current implemen-tation. Therefore, a canonical form of a verb category is assumed to have canonically ordered arguments.

As a result of the preceding lexical treatment, category S S stem \ NP ga \ NP ni , and the reduced form is registered in the lexicon. We used the following for the implementation of our resources: Kyoto corpus ver-sion 4.0, 4 NAIST text corpus version 1.5, 5 and JP corpus version 1.0. grated corpus is divided into training, development, and final test sets following the standard data split in previous work on Japanese dependency parsing [Kudo and Matsumoto 2002]. The details of these resources are listed in Table IV. Table V shows the number of successful conversions done with our method. we obtained 22,852 CCG derivations from 24,283 sentences (in the training set), resulting in a total conversion rate of 94.1%. The table shows we lost more sentences in Step 2 than in Step 1. This is natural because Step 2 imposes more restrictions on resulting structures and therefore detected more discrepancies including compound-ing errors. Our conversion rate is about 5.5 points lower than the English counterpart [Hockenmaier and Steedman 2007], which suggests that there is room for improve-ment in the current conversion.

Among the failures in Step 1, a very small number of sentences (0.04% in the train-ing set) failed to be converted because of the non-projective dependencies in them. Except for those cases, most of the failures in Step 1 were due to errors in the input corpora such as data format bugs, inconsistent annotations, and annotation errors in various levels (word segmentation, POS tagging, chunk segmentation, and dependency annotation).

In Step 2, 94% of sentence loss occurred in the phase described in Section 3.2.2 due to the contradiction between the local restrictions assigned in Section 3.2.1. We ran-domly sampled 100 sentences from those 1185 sentences and manually investigated the cause of the contradictions. Table VI shows the most frequent causes. Thirty-nine of the 100 sentences were lost due to inappropriate treatment of certain functional expressions and 17 were triggered by conversion mistakes in Step 1, such as errors in recognizing coordinated phrases. A function expression is a multiword expression working as one function unit and often accompanies an irregular phrase structure. Examples include  X (((VP-base  X  X  /expletive noun)  X  X  /adjective) doing (something) X  and  X ((VP 1 -base  X   X  /expletive noun) VP VP 1 . X  Handling function expressions is difficult partly because many of the expressions occur within one chunk and are not marked in the original annotations. Fortunately, three types of function expressions, including the examples above, consist of mostly errors concerning function expressions. Therefore we can expect improvement of the conversion quality by adding rules for the major function expressions (as least when converting the Kyoto corpus.)
For the lexicon extraction from the CCGbank, we obtained 699 types of lexical cate-gories from 616,305 word tokens. After lexical reduction, the number of categories de-creased to 454, which in turn produced 5342 categories by lexical expansion including scrambling. The average number of categories for a word type was 11.68 as a result. To estimate the quality of the obtained derivations, 50 derivations were randomly sam-pled from the Japanese CCGbank and manually investigated. We assumed a gold-standard derivation for each of the sampled sentences, compared it to the obtained derivation, and investigated the causes of the differences if any. When comparing derivations, we focused on their unlabeled structures and the lexical categories. Struc-ture comparison as unlabeled trees was included because the goal of our conversion (especially in Step 1) was to build phrase structures usable for grammar development in other formalisms. Table VII summarizes the results. Forty of the 50 obtained deriva-tions had the same structure as the gold standards, but thirteen of the forty derivations included incorrect lexical categories. The total number of incorrect lexical categories was 37, which accounted for 2.6% of all the lexical categories in the samples.
We further investigated the incorrect lexical categories obtained from derivations that had correct structures (B in Table VII) and structure discrepancies (C and D in the table). For B, 12 of the 19 erroneous categories were caused due to annotation errors in various levels such as POS tags and arguments missing in PASs. Therefore, correction of existing annotations is an important factor in improving accuracy of the obtained lexical categories. The other errors were verb categories with missing arguments due to, for example, mistakes in argument vs. adjunct decisions (see Section 3.2.1.) Some can be fixed by simply improving the algorithm, but others need correcting annotations or resolving discrepancies between the annotations from different corpora. Among ten sentences with structure discrepancy (C and D), annotation errors were not the major cause of the difference. The discrepancies in eight derivations originated in the phase converting interchunk dependencies to phrase structure (see Section 3.1), that is, each of the derivations has a subtree corresponding to a chunk connecting to a wrong point. To obtain more accurate structures, manually correcting the tree structure may be an option as well as improving the conversion rules.

To summarize, 80% of the sampled derivations had the same structure as the gold standards, and 97% of the obtained lexical entries were correct. For the derivations with correct forms, annotation correction is expected to improve the quality of the lexical categories. To decrease structure discrepancy, we need to manually correct the tree structure as well as improve the conversion rules. Following the evaluation criteria in Hockenmaier and Steedman [2007], we measured the coverage of the grammar on unseen texts. First, we obtained CCG derivations for evaluation sets by applying our conversion method and then used these derivations as the gold standard. Lexical coverage indicates the number of words to which the grammar assigns a gold-standard category. Sentential coverage indicates the number of sentences in which all words are assigned gold-standard categories. Since a gold derivation can be logically obtained if gold categories are assigned to all words in a sentence, sentential coverage means that the obtained lexicon has the ability to produce exactly correct derivations for those sentences.

Tables VIII and IX list the evaluation results. 8 Lexical coverage was 99.4% with rare word treatment by using a tag dictionary [Clark and Curran 2007], which is at the same level as the case of the English CCG parser. The columns for  X  X ncovered X  show that most of the uncovered words result from unseen combinations of seen words and seen categories. We also measured sentential coverage in a weak sense, which means the number of sentences that are given at least one analysis (not necessarily correct) by the obtained grammar. This number was 99.4%, and it was 99.3% for the development and test sets, respectively, which is sufficiently high for wide-coverage parsing of real-world texts. In fact, this number is 3.5% higher than the coverage of the Japanese HPSG lexicon [Yoshida 2005]. Finally, we evaluated parsing accuracy. We employed the parser and supertagger of Miyao and Tsujii [2008], specifically, its generalized modules for lexicalized grammars. We trained log-linear models in the same way as Clark and Curran [2007], using the training set as training data. Feature sets were borrowed from an English parse [Miyao and Tsujii 2008]; no tuning was performed. We used the gold morphological information from the annotated corpora described in Section 2.3 and input it into the parser according to the conventions of the research on Japanese dependency parsing. Following Clark and Curran [2007], the evaluation measures were precision and recall over dependencies, where a dependency is defined as a 4-tuple: a head of a func-tor, functor category, argument slot, and head of an argument. A labeled dependency is correct if the four elements match the gold standard. An unlabeled dependency is correct if the heads of the functor and the argument appear together in the gold stan-dard. We assumed that the dependencies derived from the CCG derivations converted from unseen sentences were correct, and used the dependencies as the gold standard in the evaluation.

Table X shows the parsing accuracy on the development and test sets. C&amp;C shows the performance of the English CCG parser [Clark and Curran 2007] on the English CCGbank. While our coverage was almost the same as Clark and Curran [2007], the performance of our parser was 2 X 3% lower. One possible approach to improving the performance is tuning disambiguation models for Japanese. Comparing the parser X  X  performance with previous work on Japanese dependency parsing is difficult because our figures are not directly comparable to theirs. For example, our category dependency includes more difficult problems, such as whether a subject PP is shared by coordinated verbs, while dependency parsers simply ignore the relation between PP and the former verb. Sassano and Kurohashi [2009] reported the accu-racy of their parser as 88.48 and 95.09 in unlabeled chunk-based and word-based F1, respectively. Our score of 90.7 in unlabeled category dependency seems to be lower than their word-based score. Thus, our parser is expected to be capable of real-world Japanese text analysis, but there is room for improvement by tuning the disambigua-tion model for Japanese. In this article, we proposed a method to induce wide-coverage Japanese resources based on CCG, which will lead to deeper syntactic analysis for Japanese and we pre-sented the results of empirical evaluation of the corpus conversion, the coverage of the obtained lexicon, and the parsing accuracy. Although our work is basically in line with CCGbank, in which the conversion of phrase structure trees into CCG derivations is key, the application of the method to Japanese is not trivial due to the fact that the re-lationship between a chunk-based dependency structure and a CCG derivation is not obvious.

Our method integrates multiple dependency-based resources to convert them into an integrated phrase structure treebank. The obtained treebank is then transformed into CCG derivations. Because the treebank is fairly independent of our CCG implementa-tion, the treebank itself can be utilized as a resource and its quality can be improved with additional resources in parallel to the treebank-CCG conversion. The empirical evaluation in Section 4 shows that our corpus conversion successfully converts 94% of the corpus sentences and that the coverage of the lexicon is 99.4%, which is suffi-ciently high for analyzing real-world texts. A comparison of the parsing accuracy with obtained grammar with that of previous works on Japanese dependency parsing and English CCG parsing indicates that our parser can analyze real-world Japanese texts fairly well and that there is room for improvement in disambiguation models.
