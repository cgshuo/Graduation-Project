 1. Introduction algorithms developed in recent years including numeric data clustering and categorical data clustering [1,3 X 6,8,9,13 X  16,18,20 X 23] . However, the problem of determining the number of clusters is still pending. Most of the algorithms mentioned above need a user-specified number of K clusters or implicity cluster number control parameters in advance. a visualization tool for clustered categorical data to find the optimal number of clusters by involving human subjective judgement. But it cannot find the optimal number of clusters automatically.

Recently the best K method BKPlot has been developed at Georgia Tech [12]. The BKPlot method studies the entropy dif-
K s. A hierarchical entropy-based algorithm ACE proposed in [12] to generate high-quality approximate BKPlot, which can capture the candidate best K s with small errors. candidate cluster numbers provided by the BKPlot. However, we noticed two weaknesses of BKPlot on dealing with trans-actional datasets.

First, the BKPlot produces noisy candidate cluster numbers even in a very well-structured transactional dataset. For example, the BKPlot recommends {2,5} for transactional dataset {1100000000,0011000000,0000110000, !0000001100,0000000011}, where  X  X 1100000000 X  means the first two items are in the first transaction. Similarly, for a one-layer structure transactional dataset, which has 1000 items and 20 clusters defined with a structure similar to Fig. 11 , the candidate cluster numbers are {2,5,10,20}. Obviously, these {2,5,10} are not significant structures. Second, the BKPlot becomes time-consuming while BKPlot deals with transaction datasets with large number of items.
According to [12], the time complexity of hierarchical entropy-based ACE, which is used to generate approximate BKPlot, ean data in advance.
 we use this dissimilarity measure to develop an agglomerative hierarchical transactional-clustering algorithm ACTD imental results in Section 4 and summarize our approach in Section 5. 2. Transactional-cluster-modes Dissimilarity 2.1. Notations tional dataset D of size N is defined as follows. Let I  X f I transaction t j (1 6 j 6 N ) is a set of items t j  X f I j 1 action clustering result C K is a partition of D , denoted by f C 2.2. Overview of the SCALE Framework
We briefly describe the design of SCALE (Sampling, Clustering structure Assessment, cLustering and domain-specific Eval-clustering in four steps as shown in Fig. 1 .

SCALE uses the sampling step to handle large transactional dataset. Standard sampling techniques are used in the sam-pling step to generate some sample datasets from the entire large dataset. The framework assumes the primary clustering structure (with small number of large clusters) is approximately preserved with appropriate sample size. approximate BKPlots graph, which can capture the candidate best K s with small errors. The experimental results in [22] show that the predefined class numbers are usually included in the candidate cluster numbers provided by the BKPlot.
However, the ACE becomes time-consuming on dealing with transactional datasets with large number of items at this tional datasets with large number of items.
 clustering structure assessment step outputs the best K s and the cluster seeds at the best K s to the clustering step.
The clustering step applies the WCD [22] clustering algorithm to perform initial cluster assignment. The initial assess-ment result is then used to guide the WCD clustering over the entire dataset in an iterative manner until no transaction
We call the SCALE using BKPlot method as SCALE X  X CE and the SCALE using DMDI method as SCALE X  X CTD . In this paper, we mainly present our design principles of DMDI method and algorithmic details of ACTD algorithm. And our experimental results are the performance and quality comparison of two frameworks in their clustering structure assessment step. 2.3. Concept of coverage density a simple transactional dataset {abc,bc,ac,de,def} can be visualized in Fig. 2 .
 how to order and partition the transactional dataset properly is one of the key issues of clustering algorithm. Bearing this intuition in mind, we give the definition of Coverage Density (CD).
 number of distinct items and number of transactions in a cluster.

M , the items set of C k is I k  X f I k 1 ; I k 2 ; ... ; I kM items in cluster C k is S k , then the Coverage Density of cluster C 2.4. Concept of Transactional-cluster-modes Dissimilarity larity measure in transactional data clustering. Transactional-cluster-modes Dissimilarity is based on the concept of transactional-cluster-mode at first.
 above the user-specified proportion of transactions. Given a transactional cluster C items, suppose the user-specified minimum support is h , then the transactional-cluster-mode CM
CM k  X f I kj j occur  X  I kj  X  P  X  N k h  X  ; 1 6 j 6 M k g . The length of cluster mode CM Definition 3. Transactional-cluster-modes Dissimilarity is the dissimilarity between two transactional-cluster-modes.
Given a pair of clusters C i and C j , suppose the cluster modes of two clusters are CM actional-cluster-modes Dissimilarity between the C i and C
Simplifying the above formula, we get d m  X  C i ; C j  X  X  1 two cluster modes and thus j CM ij j P max fj CM i j ; j CM 0 and 1 2 . Not surprisingly, when two cluster modes have the same set of items, that is CM two complete different cluster modes having no overlapping between the sets of items, that is CM them will result in a maximum dissimilarity d m  X  C i ; C Three examples are given to illustrate the above situations in Figs. 3 X 5 .
 between cluster modes will be small; when the two clusters are very different, merging them will bring large structural dissimilarity measure for transactional data.
 3. Finding significant clustering structure in transactional datasets under the assumption that datasets have some clustering tendency. However, the clustering process is unsupervised, with-decided by the features of datasets and the input parameter values [19].
 application cases. It is also challenging to find all of them.
 Structure .
 significant clustering structure is the most important clustering structure among all optimal clustering structures with different K .

One significant clustering structure corresponds to one optimal number of clusters. Note that there might be multiple actional datasets. 3.1. ACTD: Agglomerative Clustering algorithm with Transactional-cluster-modes Dissimilarity
Having Transactional-cluster-modes Dissimilarity as the measure of inter-cluster dissimilarity, we develop an agglomer-With the common agglomerative clustering process, we briefly describe the ACTD algorithm.
ACTD uses bottom-up process to do clustering. It begins with the scenario where each transaction is a cluster. Then, an iterative process is followed: in each step, the algorithm finds a pair of clusters C d the pair clusters C i and C j and records the d m  X  C i ; C generated during the hierarchical cluster merging process.
 In order to efficiently implement the ACTD, we maintain three tables: clusters summary table T ing the basic information of each cluster of current clustering result. The basic information of cluster C number of transactions T S  X  k N k , the sum occurrences of items T of the input dataset. In the iterative merging procedure, the space used by the merged clusters is freed; The second of current clustering result. For example, T d  X  i  X  j keeps the value of d searches the T d  X  X  , finds and merges the pair clusters with minimum dissimilarity. The third table is Merging Dissim-operations.

Algorithm 1 shows the sketch of the main procedure of ACTD. For the merging pair clusters, the algorithm chooses one is described in Algorithm 2. After each merging operation, there is an updating operation. The updating operation mainly description is given in Algorithm 3.
 Algorithm 1: ACTD.main() Input: Transactional dataset D , Number of transactions N , Min-support h Output: T MDI  X  and all clustering results
Initialize T S  X  N , T d  X  N  X  N , T MDI  X  N ; clusterNo  X  N ; while  X  clusterNo &gt; 1  X  do end while output T MDI  X  and all clustering results;
Algorithm 2: ACTD.merging( i , j ) void merging ( i , j ) { T S  X  i N i  X  X  T S  X  j N j ;
T S  X  i S i  X  X  T S  X  j S j ; for  X  i  X  0 ; i &lt; T S  X  j M j ; i  X  X  X  do end for }
Algorithm 3: ACTD.updating( i , j ) void updating ( i , j ) { free T S  X  j ;
Invalidate T d  X  j :  X  ; (each valid cluster u ) do end for } mode is M . The most time-consuming part of our algorithm is updating the T
O  X  N 2 log N  X  in the worst case. So the overall time complexity is O  X  MN thousands to tens of thousands, while the length of transaction clustering mode M is often around tens. Therefore, the improvement in terms of time complexity is very significant. The space complexity is around the same level of ACE algo-rithm. The clusters summary table T S  X  needs O  X  MN  X  space, the Merging Dissimilarity Indexes Table T space, and pair-clusters dissimilarity table T d  X  X  costs O  X  N 3.2. Plotting Differential MDI curve to find significant clustering structure there such index curves indicating the significant clustering structures for transactional data as well?
We plot all the MDI values in Table T MDI , which is outputted by algorithm ACTD and our results show the curves of MDI boring changes. A conceptual demonstration of  X  X  X gglomerative clustering procedure X  in Fig. 6 can help to understand the reason of MDI curve shape.
 Differential MDI below.

Definition 5. For any two neighboring clustering results C
A small DMDI means the merging operations of two neighboring partition schemes were merging similar clusters and did not change the structure dramatically. A big DMDI means the merging operation might cause the change of the clustering of Differential MDIs Curve for finding significant structures of transactional datasets.
 A DMDI curve may have more than one peak as shown in Fig. 8 . Then which peak indicates the right number of clusters? tering granularity. In addition, some candidate optimal cluster numbers indicated by some peaks may not consistent with the domain knowledge, they may be the valuable K s to be utilized to explore more hidden knowledge. 3.3. Assessing the cluster number of large transactional datasets For the problem of clustering large transactional datasets, we suggest using SCALE X  X CTD framework. Under the SCALE X  significant clustering structures.

Running ACTD on a group of sample datasets will bring one problem, that is, are these DMDI curves of sample datasets
Sample DMDI curves and the DMDI curve of original large dataset as Original DMDI curve . Usually, sample DMDI curves ple DMDI curves will deviate from the original DMDI curve if the sample size is small. How many samples are needed to eratures [17,6] gave rough estimation, which related to the cluster density and the cluster distribution.
In practice, we will try as many sample points as possible that the assessment algorithm can handle in amount of assessment. Therefore, any improvement to the performance of assessment algorithm, as the performance boosting of ACTD to ACE that is used in the BKPlot method, will be preferred. 4. Experiments
We did experiments on both synthetic datasets and real datasets to test if: (1) the SCALE X  X CTD is faster than SCALE X  X CE
K s for experimental datasets; (3) the DMDI method provides less noisy candidate cluster numbers than the BKPlot method
DMDI method has advantages on identifying clustering structures of transactional datasets compared with the traditional statistical method, i.e., Bayesian Information Criterion (BIC) method.

Before we show our experimental results, we introduce the symbols used to annotate the synthetic datasets first. The three primary symbols are the average transaction length T , the total number of items I and the number of transactions there is no specific description. 4.1. Performance evaluation We did performance evaluation experiments to verify our analysis on time complexity of two algorithms ACE and ACTD. the two algorithms on a set of datasets. The detail information of datasets are given below: complexity factor M . Since the purpose of our experiments is studying the relationship between the running time and the transaction length from 5 to 400.

Retail : A real large market basket dataset [7] contains 88,162 transactions and 16,470 items, which is approximately 5 months receipts being collected. The average number of distinct items purchased per receipt is 13 and most customers buy between 7 and 11 items per shopping visit. Retail X  X  sample datasets with different sizes were used to study the ACTD time-complexity factor N and to compare the performance of two algorithms.
 We ran both ACTD and ACE on TxI1000D1k series datasets, the running time for different sizes are shown in Fig. 9 . The than that of ACTD.

We sampled Retail with different sizes from 100 to 2000 and these sample datasets were transformed into categorical datasets with 16,470 dimensions before running ACE. The time spent on these sample datasets are shown in Fig. 10 .We can see that ACTD is much faster than ACE. The performance test results confirm our analysis on the time complexity of
ACE and ACTD: the large number of dimensions d has great impact on running time of ACE algorithm since all transactions have to be transformed to Boolean data, while only the average transaction length has influence on ACTD algorithm. Since much faster than ACE on dealing with a common transactional dataset. 4.2. Quality evaluation
Our experiments have used seven small datasets to evaluate the quality of the DMDI method and the BKPlot method on transactional datasets. Two small synthetic transactional datasets are constructed by ourselves: T50I1000D200 and
In addition, we used five real (non-transactional) datasets: Lenses and Soybean-small are from the UCI machine learning repository 1 ; Tr41, Wap and LA1 are from the documents clustering datasets. The detail information of these seven datasets are given below:
T50I1000D200 is generated with one-layer clustering structure as shown in Fig. 11 . T50I1000D200 has 200 transactions tions in the same cluster have completely same items.

T6I46D200 has two-layer clustering structure as shown in Fig. 12 . T6I46D200 has 200 transactions and the length of each actions having these items.
 the patient should wear.

Soybean-small is a categorical dataset used to classify the soybean diseases. The dataset has 47 records and each record has 35 attributes describing the features of the plant. There are four predefined classes in the dataset.
Tr41 were derived from TREC-5, TREC-6, and TREC-7 collections. class number is 10.

Wap are from the WebACE project and contains 1560 documents. Each document corresponds to a web paper listed in the subject hierarchy of Yahoo!. The total number of terms is 8460 and the predefined class number is 20.
LA1 was obtained from the articles of the Los Angeles Times that was used in TREC-5. It includes the documents from the ments is 3204 and the total number of terms is 31,472.

Before running ACTD algorithm, these two categorical datasets, i.e., Lenses and Soybean-small are transformed to trans-with 9 items and Soybean-small becomes a 72 items transactional dataset.

Since the last three datasets originally are document datasets, whose term frequency information is also included in the datasets. Before running the ACTD, these three datasets are also transformed into transactional datasets by removing the term frequency information.
 Boolean datasets are summarized in Table 1 .
 curves and BKPlot curves. Below we give brief definitions of two measures: include all of the significant K s.

False Discovery Rate : The False Discovery Rate(FDR) is the percentage of the noisy candidate K s indicated by detecting methods. There could be some K s, which are actually not critical but suggested by detecting methods. In order to effi-ciently find the most significant ones, we prefer a detecting method to have less false candidate K s as possible.
The results of two synthetic datasets and five real datasets are summarized in Table 2 . Table 2 shows: (1) both methods covery Rate than the BKPlot method on most of experimental datasets.

We ran ACTD on dataset T50I1000D200 at the support 0.8 and the DMDI curve (Fig. 13 ) clearly shows the candidate opti-mal number of T50I1000D200 is  X 20 X , which is same as the number of clustering structure we constructed. After converting the T50I1000D200 into a 200 1000 Boolean table for the ACE algorithm, we get the BKPlot result as shown in Fig. 14 . The defined number of clusters  X 5 X  and  X 9 X . However, same as the one-layer dataset, the BKPlot method has one more candidate noises than the BKPlot method.
 We did experiments on five real datasets and plotted their DMDI and BKPlot graphs, respectively ( Figs. 17 X 26 ). Again,
Figs. 17 and 18 show that the DMDI result on transactional lenses is less noisy than the BKPlot X  X . The DMDI result given by BKPlot to 3 for Wap. However, the two methods perform equally on the dataset Soybean-small ( Figs. 19 and 20 ).
Here, the setting method of support values is discussed. The support value of algorithm ACTD is a real number between 0 and 1, i.e., (0,1]. It decides the transactional-cluster-mode of each cluster during the whole agglomerative merging feature of a cluster, thus the clustering structure can be found in high probability. Through the above experiments, we the support value 0.8 is appropriate for most datasets in our experiments, while the last three document datasets use different support values according to their sparseness. 4.3. Quality evaluation on sample datasets
We have shown that ACTD is much faster than ACE for BKPlot on transactional datasets, which mean we can apply ACTD ple BKPlot curves.

Two synthetic transactional datasets, which are constructed by ourselves and have predefined clustering structure, were od. The detail information of two synthetic datasets are given below:
T50I1000D1000 is a 5 times duplication dataset of T50I1000D200. The clustering structure of T50I1000D1000 is very clear and transactions within a cluster have same items. The predefined cluster number of T50I1000D1000 is 20 and each cluster has 50 transactions.

T20I500D20K is generated by a modified version of data generator in [2]. The T20I500D20K has predefined 10 clusters, and the coherence of transactions within a cluster is much looser than that of T50I1000D1000. The construction method of T20I500D20K is: (1) the 10 clusters are generated one by one and do not have any item overlapping between any two following parameter values were set: average size of transaction = 20, number of items = 50, number of transac-tions = 2000, average size of patterns = 4, number of patterns = 2000, correlation between consecutive patterns = 0.25.
Uniform sampling method was used to generate sample datasets from original datasets. We use sample sizes {200,400,800} on the dataset T50I1000D1000 and sample sizes {800,1000,2000} on the dataset T20I500D20K to generate sample datasets. For each sample size, we generate 10 sample datasets. After running ACE and ACTD on 10 sample datasets, we average the values generated from 10 sample datasets for the sample BKPlot graph and the sample DMDI graph.
We summarize the results with two measures in Table 3 . Table 3 shows that both methods have 100% Coverage Rate, that is, two methods can indicate the inherent significant clustering structures correctly. However, the DMDI method has much lower False Discovery Rate than the BKPlot method on these sample datasets. Therefore, the DMDI method is same robust as the BKPlot method but less noisy candidate K s. Next, we present these sample DMDI and BKPlot curves in detail.
For the synthetic dataset T50I1000D1000, which has clear clustering structure, the sample DMDI curves ( Fig. 27 ) merge very well and indicate the same cluster numbers as the original DMDI curves. Fig. 28 shows the sample BKPlot curves have more or less deviations from original DMDI curves but have consistent peaks at k  X  20, which indicate the size of merged clusters has influence on the peak values of BKPlot curves.

For the dataset T20I500D20K, the sample DMDI curves and sample BKPlot curves are shown in Figs. 29 and 30 . These two ter numbers. But the sample DMDI curves produce much less noisy candidate K s than the sample BKPlot curves.
In addition to these sample DMDI and BKPlot curves, the average running time on these sample datasets are also pre-again. 4.4. Capability of dealing with noise transactions injected into the T20I500D20K uniformly to generate the mixed datasets. The noise rates are set as 0.5%, 1%, 5% and 10%, ing to the above noise rates.
 of transactions = 2000, average size of patterns = 4, number of patterns = 2000, correlation between consecutive pat-clusters of T20I500D20K.
 The noise dataset is sampled with size = {100,200,1000,2000} and the noise transactions are injected into the
T20I500D20K uniformly to generate four mixed datasets T20I500D20k-N100, T20I500D20k-N200, T20I500D20k-N1000 and T20I500D20k-N2000. Then the four mixed datasets were uniformly sampled with size = {800,1000,2000}. For each sam-od still can identify predefined classes correctly. We find the DMDI method is quite resilient to noise. 4.5. Comparing DMDI with BIC
Among the classical clustering methods, the closest method to our proposed method on transactional data or categorical mixture modeling. Below we give a brief introduction of BIC method.
 lihood of fitting the data to the mixture model. The generic form of BIC is then based on the maximum likelihood and the number of parameters used in estimation: where n is the number of sample records, and w is the number of parameters used in the modeling that include the num-ber of clusters. Usually, the number of cluster corresponding to the minimum BIC is regarded as the optimal number of cluster.
 efficient on most of the experimental transactional datasets. 5. Conclusions
Although the problem of determining the optimal number of clusters is very challenging, we have shown that a coverage highly efficient in finding the optimal number of clusters for transactional datasets.
 Acknowledgements This research is partially supported by NSF CISE CyberTrust, an AFOSR grant, an IBM faculty award, and an IBM SUR grant.
References
