 Given a set of entities, the all-pairs similarity search aims at identifying all pairs of entities that have similarity greater than (or distance smaller than) some user-defined threshold. In this article, we propose a parallel framework for solving this problem in metric spaces. Novel elements of our solution include: i) flexible support for multiple metrics of interest; ii) an autonomic approach to partition the input dataset with minimal redundancy to achieve good load-balance in the presence of limited computing resources; iii) an on-the-fly lossless compression strategy to reduce both the running time and the final output size. We validate the utility, scal-ability and the effectiveness of the approach on hundreds of machines using real and synthetic datasets.
 H.2.8 [ Database Management ]: Database Applications X  Data Mining Algorithms, Performance Similarity Joins; All-Pairs Similarity Search
The all-pair similarity search (APSS) problem seeks to find all pairs of records (or entities) within a dataset that meet a user-defined similarity threshold, based on some def-inition of similarity. This problem has found applications in many domains including community discovery [29], du-plicate detection [16], collaborative filtering [19] and as a preprocessing step for clustering [18].

The motivating application behind this work is the de-tection of click fraud rings [21]. Since an Internet content publisher collects revenue for each click on any ad displayed on her page, she has the incentive to launch click inflation attacks by generating fake clicks. The identification of pub-lisher accounts that have similar features, from an enormous amount of accounts with legitimate traffic, is very effective in detecting such attacks. This can be achieved by represent-ing each account as a point on a multi-dimensional space, and efficiently solving the APSS problem on all such points.
A significant amount of related work has focused on set similarity measures (e.g. Jaccard or Cosine) operating on binary or categorical data [31, 5, 27, 22]. The key to their scalability is effectively leveraging the inherent sparsity in such data [26, 8, 6, 31, 22].

In contrast to the aforementioned sparse APSS problem, we focus on the dense APSS problem [28]. In the motivat-ing application, publishers are typically represented using vectors of numerical features that are reasonably dense i.e., each record has a non-zero value for a significant fraction of the dimensions. To make the problem more tractable, we fo-cus on the MAPSS problem, the APSS flavor of the problem where the similarity measure is a Metric. MAPSS has appli-cations from image search [20] to time series analysis [11]. To make the solution more scalable, we propose a parallel algo-rithm. While our implementation is MapReduce-based [9], the algorithm is generalizable to other forms of parallelizing frameworks, such as MPI and OpenMP.

The proposed MR-MAPSS (MapReduce-based MAPSS) algorithm relies on a carefully engineered partitioning scheme, where the backbone algorithm intelligently routes the input data records into worksets with minimal redundancy. These worksets are independent and each is processed by a MapRe-duce worker. This achieves load-balancing when handling a massive number of high-dimensional records. Repartitioning is employed to further split large worksets that occur when the input data is densely clustered. This improves the load balancing, and enables execution with limited per-machine resource budgets. For efficiency, MR-MAPSS also employs a multi-level indexing structure to combine computing the similar pairs of records with on-the-fly lossless compression of the results. MR-MAPSS is flexible, and caters to diverse measures of similarity, and even handles non-metric simi-larity measures as long as upper and lower bounds on the distances can be established. The main contributions are: 1. We propose MR-MAPSS, an intelligent divide-and-conquer 2. The MR-MAPSS framework automatically detects skewed 3. The MR-MAPSS framework is agile and can adapt to 4. MR-MAPSS employs a novel compression strategy us-5. We establish the effectiveness of the optimizations on Notation: We begin by briefly going over notation used in this paper (see Table 1). Note that the terms  X  X ecord X  and  X  X oint X  X ill be used interchangeably in the rest of the paper. L The dimensionality of the points
D Database of points from R L | D | Number of points in the database p i i t h point in the database dist ( p i , p j ) Distance between p i a nd p j t Distance threshold for a pair to be similar MapReduce infrastructure: M apReduce [9] (MR) has become the de facto platform for big data processing in shared-nothing clusters due to its high scalability and built-in fault tolerance support. It borrows Map and Reduce con-cepts from functional programming. The computation can be represented using two functions: Map : h k 1 , v 1 i  X  [ h k 2 , v 2 i ] Reduce : h k 2 , [ v 2 ] i  X  [ v 3 ] Each input record is a tuple h k 1 , v 1 i . During the execution of a job, each mapper fetches a set of records from the dis-tributed file system, and applies the map function on each single record to produce a list in the form of [ h k 2 , v [ . ] represents a list of elements. The mappers can also out-put the tuples with a secondary key . The shuffler groups the output of the mappers by k 2 and sorts by the secondary key within each group, and sends all tuples with the same k 2 value to the same reducer. The reduce function receives the key and the list of values as input, and emits the results. A combiner, a function similar to reducer, can be used for partial reducing at the mapper X  X  side to reduce the network load. The MR infrastructure allows for specifying initializa-tion operations before the actual Mapper or Reducer starts. Examples of MR applications can be found in [9]. This section describes the backbone algorithm of the MR-MAPSS framework. Optimizations to scale under tight com-putational constraints are discussed in Section 4 and 5. We start by introducing some notations.
 Definition 3.1. SimSet: Given two sets of points S 1 and S , define SimSet ( S 1 , S 2 ) = {h p 1 , p 2 i | dist ( p p  X  S 1  X  p 2  X  S 2  X  p 1 6 = p 2 } .
 The MAPSS problem is to find SimSet ( D, D ) of Dataset D in a metric space .

Definition 3.2. Inner sets, Outer sets, Worksets and APSS division: Assume there exist N sets { I i =1 ,...,N } such that call each W i = h I i , O i i a workset of D . I i , O i are the inner set and outer set of W i respectively. If  X  N i =1 SimSet ( I O ) = SimSet ( D, D ) , then { W i =1 ,  X  X  X  N } is said to define an APSS division of D .

From Definition 3.2, partitioning the APSS workload of D becomes a problem of finding an APSS division. There may
Figure 1: APSS Division and Their Constructions b e multiple APSS divisions. A trivial APSS division is to define inner and outer sets as I 1 = . . . = I N = O 1 = . . . = O N = D . The left part of Figure 1 illustrates the concept of APSS division. In the figure, the nodes are interconnected if similar. Circular and square points represent inner and outer points, respectively. Note that B and C serve as outer points for the upper workset, and serve as inner points for the lower one.

Definition 3.3. Partitions, Centroids, and Radii: D can be divided into N disjoint partitions, P i , . . . , P N , such that P  X  P j =  X   X  i 6 = j and  X  N i =1 P i = D . Let N centroids, c c N be used to partition D . A point, p , belongs to partition P i if and only if c i is its closest centroid. The radius of P is defined as r i = max p i  X  P i dist ( c i , p i ) .
In metric space, one way to form worksets is to let I i , the inner set of workset W i , be P i . O i , the outer set of W is given by { p j | dist ( p j , c i )  X  ( r i + t )  X  p j right part of Figure 1 is an example of constructing workset from a partition consisting of all the circular points. All circles serve as inner points in the generated workset, while squares serve as the outer points. Triangles are irrelevant to this workset and thus discarded.

Theorem 3.4. The worksets formed above define a MAPSS division.

Proof. It suffices to show that all the pairs in SimSet ( I O ) are similar, and that no similar pair is missed. Each pair in SimSet ( I i , I i  X  O i ) is a similar pair, from Defini-tion 3.1. This establishes that  X  N i =1 SimSet ( I i , I contains similar pairs. From Definition 3.3 and Triangle Inequality (TI) , I i  X  O i contains all points similar to any point in I i . Since from Definition 3.3, D =  X  N i =1 I i lows that  X  N i =1 SimSet ( I i , I i  X  O i )  X  SimSet ( D, D ). Hence { W i =1 ,...,N } defines an APSS division for D .
Other approaches, such as the  X  X eneralized hyperplane p artitioning X , can also serve as alternatives to construct in-ner and outer sets. The  X  X eneralized hyperplane partition-ing X  is also implemented in our framework. Interested read-ers are referred to [30, 17] for more details. We omit this discussion here in the interest of space.
F igure 2 depicts the MR-MAPSS framework. It has two major phases: data pre-processing and similarity comput-ing. The data pre-processing phase comprises the Centroid-Sampling and CentroidStats steps. CentroidSampling se-lects random points as centroids. The output samples will be stored on disks and used in the follow-up step. Centroid-Stats step computes partition statistics, such as the radii of the centroids. The Similarity computation phase identi-fies all the similar pairs. It has either one or two chained MapReduce steps, depending on the computing resources and data characteristics. SimilarityMapper reads in the original dataset as well as the centroid statistics, and con-structs the worksets for SimilarityReducer. SimilarityMap-per uses secondary keys so that SimilarityReducer receives inner points before outer points. An optional repartitioning step is employed to split large worksets that do not fit in memory into smaller ones. The repartitioning work is done by the RepartitionReducer. The RepartitionMapper is al-most an identity mapper, except for using secondary keys to provide the same guarantee of SimilarityMapper. Finally, SimilarityReducer computes  X  i SimSet ( I i , I i  X  O i ). Figure 2: The MR-MAPSS Partitioning Framework
I n the rest of the section, we describe the CentroidSam-pling, CentroidStats and Similarity steps without reparti-tioning. Repartitioning will be discussed in the Section 5.
Selecting the right set of centroids(partitions) to minimize the APSS computational cost is a very difficult task. If each data point is treated as a vertex in the graph, the similar pair can be modeled as an edge connecting the correspond-ing vertices. When forming the partitions, a simple but in-tuitive partitioning approach is to find a set of N centroids for which the largest distance of any point to its closest cen-troid in the K-set is minimum. This problem is commonly known as metric K-center , a NP-complete problem [15]. In practice, the algorithm also needs to minimize the number of similar pairs formed by points in two different partitions. Considering the various system constraints makes the prob-lem even harder.

The CentroidSampling step samples N centroids from the input. We tried methods such as KMeans++ [3], random sampling, as well as a seeding approach ensuring the dis-tances between any pair of samples is no less than a thresh-old. It turned out random sampling and KMeans++ have better performance. We therefore chose random sampling due to the simplicity and scalability. CentroidStats com-putes the radius of the partitions. Centroid information is read in every mapper before the actual map operation starts. As a mapper processes a point, it outputs the closest cen-troid id as the key, and the corresponding distance as the value. For each centroid, a reducer receives a list of the distances of the inner points to that centroid, [ dist ( p The reducer computes the radius for each centroid as the maximum distance in this list. Algorithm 1 S imilarity
In the Similarity step (Algorithm 1), all the outer points o f each workset are found. The mappers route the corre-sponding inner and outer points of each partition to the same reducer, and specify the pointT ype of each point as either inner or outer. Each reducer receives one of these worksets, computes all its similar pair of points and outputs them. Secondary keys that guarantee the reducer receives inner points before outer points are not shown for simplicity. Figure 3: The MR-MAPSS Backbone Algorithm
F igure 3 provides an example of four data points that are aligned horizontally on increasing subscript order, such that the distance between any two neighboring points is 0 . 5. Partition P 1 has points p 1 and p 2 . Its centroid, c 1 between p 1 and p 2 . Similarly, partition P 2 contains p p , with centroid c 2 lying between them. Hence, r 1 = r 2 = 0 . 25. For simplicity, the centroids are assumed not to be part of the dataset. The distance threshold is set to 0 . 6. p and p 3 reside in shard S 1 , while the other two points reside in shard S 2 . From Definition 3.3, p 2 and p 3 are outer points of P 1 and P 2 , respectively. During the shuffle phase, p p are routed to W 1 , and p 2 and p 4 are routed to W 2 as inner points. In addition, p 2 and p 3 are routed as outer points to W 1 and W 2 , respectively. The two reducers then compute all the pairs in SimSet ( I i , I i  X  O i ) for i = 1 , 2. Exploting Commutativity: So far, the mappers are as-sumed to send out the outer points of all the worksets. One drawback of this na  X   X ve routing scheme is the existence of redundant computation. In Figure 3, the two points p 2 a nd p 3 were sent to W 1 and W 2 , respectively. This causes the pair h p 2 , p 3 i to be computed by both reducers. More generally, assuming the dataset is N -way partitioned, and OP ( P i , P j ) = { a | a  X  I i  X  a  X  O j } then the number of unnecessary computations in the reduce phase is at least P
The symmetry property of the metric space can be ex-ploited to form worksets more efficiently as defined below.
Definition 4.1. Contributing Partitions: Let { P i =1 ,...,N be the N partitions of D as defined in Definition 3.3. For any pair of partitions, h P i , P j i , let either OP ( P to W j as outer points, or OP ( P j , P i ) be routed to P points, but not both. Define the contributing partitions to P as the set Q i = { P j | OP ( P j , P i ) is routed to P i points of P i } . The worksets are formed such that I i = P
Figure 4 illustrates the inefficiency by showing a 4-way partitioned dataset, where an edge represents OP ( P i , P for partitions P i and P j . The dotted edges represent redun-dant computation. The solid edges incident on any partition represent contributions of outer points from neighboring par-titions that do not result in redundant computation.
Theorem 4.2. The worksets formed according to Defini-tion 4.1 define a non-redundant APSS division where any pair of points is considered in at most one workset.
Proof. The output  X  N i =1 SimSet ( I i , I i  X  O i ) is equal to (  X  i =1 SimSet ( I i , I i ))  X  (  X  N i =1 SimSet ( I i , O i contradiction, assume two points p i  X  P i and p j  X  P j are similar, but not included in the output. We have P i 6 = P otherwise, this point pair will be evaluated in SimSet ( P From the symmetry of the space, since p i and p j are simi-lar, p i  X  OP ( P i , P j ) and p j  X  OP ( P j , P i ). Without loss of generality, we assume OP ( P i , P j ) is routed to P j as an outer point, and the pair ( p i , p j ) is evaluated in SimSet ( I Thus, such pair ( p i , p j ) cannot exist. Hence, the worksets define an APSS division. In addition, ( p i , p j ) will either be routed to workset W i or W j , but not both. Thus, any pair is only considered in at most one workset.

One way to construct the non-redundant APSS division i s to route OP ( P i , P j ) to P j if | P i | &lt; | P j construction minimizes the total computations for forming the APSS division, assuming that the time of evaluating OP ( P i , P j ) is proportional to | P i | . However, this skews the workload, since partitions with larger sizes receive more points. We propose a balancing approach, which is to route OP ( P i , P j ) to P j if ((( P i .id + P j .id ) is odd) XOR ( P P .id )) is true 1 . Hence, each partition gets outer points from roughly half the other partitions, ensuring better load bal-ancing. The SimilarityMapper is outlined in Algorithm 2.
The input of the SimilarityMapper is points rather than partitions. The algorithm first finds the partition P i to which point p i belongs (Line 1). In Line 2-6, the algorithm loops over all the partitions. It outputs p i as an inner point of P i (Line 3-4). Otherwise, the point is checked for being an outer point of P j and output if it is (Line 5-6). The output is sorted based on the secondary key ( X  X nner X  or  X  X uter X ). This ensures the SimilarityReducer processes inner points first to guarantee the correctness of the results in case repartitioning takes place, as discussed in Section 5.
 Algorithm 2 S imilarityMapper Compression of Pairs : To design an efficient Similari-tyReducer, one major challenge is the online compression of enormous APSS output. This happens when the data is highly clustered given the similarity threshold. Existing graph compression algorithms such as WEBGRAPH [7] are not appropriate because they assume the entire data fits in memory. Our solution, Community-based Lossless Com-pression , compresses similar pairs efficiently on the fly, and provides good Interpretation of the resulting pairs better.
We treat the APSS output as a graph , where records are vertices and similar records are connected with edges. Three key community structures are identified  X  cliques, bicliques and hubs (implemented as adjacency lists), as illustrated in Figure 5. By exploiting the community structures in the graph, MR-MAPSS outputs compressed community struc-tures on the fly. The fundamental idea of the compression scheme is to find the community structures when computing the similar pairs, and store them right away. If one record is similar to a set of records, then we say the record and the similar set form an adjacency list. We can thus compress the pairs by factoring the record out. For example, the four similar pairs [( a, b )( a, c )( a, d )( a, e )] can be compressed as [ a, ( b, c, d, e )]. Biclique structure generalizes the adjacency list. We say two sets of records form a biclique if any pair of records, each from a different set, forms a similar pair. For compressed as [( a, b ) , ( c, d )]. To simplify the implementa-tion, cliques are treated as a special bicliques. In terms of compression ratio, adjacency list can compress the data to half of its size, while the biclique representation can com-press the data up to min ( m, n ) times, with m and n being the two set sizes respectively. Clique representation com-presses the data to the O (
Achieving the optimal compression is very difficult -one criteria of optimality is to minimize the number of bicliques collections covering all edges, which is NP-complete [13]. In datasets where no huge cliques and bicliques exist, the ratio is usually between one and two. These compressed struc-tures are identified by sorting the points in their distance to the centroid, and binning them. Then, the bins that have F igure 5: Compression Helps Exhibit the Commu-nity Structure points that fit any of the structures are identified using the Triangle Inequality (TI). Compression results in an order of magnitude difference in the output size when the data is highly clustered given the similarity threshold.

The proposed compression earns us some extra bonus on processing the follow-up steps. For example, shingle cluster-ing [14] and neighborhood density estimation can directly use the compressed data without decompressing, reducing both the IO and the CPU costs.
 Similarity Evaluation : The SimilarityReducer employs the online compression approach while generating all similar pairs. Here we assume the whole workset fits in memory. If the workset cannot fit, our streaming solution takes care of it, as discussed in Section 5.

A tree-based indexing structure is built using hierarchical clustering. In our implementation, this creates partitions at different granularities, and makes both pruning and identi-fying bicliques at different levels easier.
 Algorithm 3 E mitSimSet 4: //Dissimilar partitions are ignored. 6: for childinP B .Children () do 9: for childinP A .Children () do 10: EmitSimSet ( child, P B ) 11: else
In Algorithm 3, EmitSimSet recursively outputs SimSet( P A P B ) . The output formats are adjacency list and biclique. The algorithm first outputs the two partitions if they can form a biclique. The checking only needs radius and centroid information in the indexing structures (Line 1-2). Next, the algorithm continues only when the two partitions may have similar pairs, which prunes away the dissimilar partitions if they have no chance of forming similar pairs. The algorithm exploits the fine-granularity partitioning information by go-ing to the child nodes, and calls itself recursively on one of the child level partitions. If we are already at a leaf node of the indexes, we directly compute and emit the similar pairs (Line 3-12). SimilarityReducer calls EmitSimSet to evaluate similar pairs in each partition.
 Algorithm Applicability : Our MR-MAPSS framework can be applied to any metrics, including Earth Mover Dis-tance(EMD), Hamming distances, string edit distances and so on. When computing pairwise distance, it is common practice to make use of filters. For example, with EMD metric, we can employ filters such as weighted L p norms [4].
It is also worth noting that this generic partitioning frame-work can even cope with some non-metrics. In fact, both cosine similarity and the KS-statistics can work under our framework. For instance, although cosine similarity itself does not conform to TI, the angles between the vectors ac-tually exhibit the TI property [2].
In an imbalanced workload [12], few machines take hours or even days to compute the large worksets while all other machines finish in seconds. It is also possible some worksets are too large to fit in memory, which may make the program crash in the middle of a job. Fortunately, both issues can be greatly relieved by repartitioning the large worksets into even smaller ones iteratively. Figure 6: Improved Similarity Computation Task
F igure 6 highlights the more scalable Similarity computa-tion task with repartitioning. The WorkloadEstimate step, Job 1 in Figure 6, computes the inner and outer sizes for each workset. The fact that partition c 1 has 2 inner and 1 outer points is denoted in Figure 6 as h c 1 , h 2 , 1 ii . Then for each workset, it computes a simple estimate of the processing load. It sorts both inner and outer points first, and con-structs a frequency histogram based on the distance to the centroid. By applying TI, WorkloadEstimate identifies the bins whose points have to be compared against each other. Hence, WorkloadEstimate can estimate an upper bound on the total number of comparisons. If there exists one par-tition whose estimated comparison number is larger than worksets are repartitioned during the Repartition step. Oth-erwise, the similar pairs are computed directly. 2 T he value ut was set to 10 8 in our experiments. (a) Repartitioning, r is the radius of the o riginal worksetInner
M ap 2 is SimilarityMapper. Based on the estimated work-load from WorkloadEstimate, it either continues running SimilarityReducer ( Reduce A ) to compute all the similar pairs when the data is not highly clustered given the similarity threshold, or switches to the RepartitionReducer ( Reduce to split the large worksets into smaller ones. WorkloadEs-timate and SimilarityMapper can run in parallel until the repartitioning decision is reached.

RepartitionMapper ( M ap 3) is simple as it outputs the input as is. It parses the point list and outputs each point individually using the key generated by RepartitionReducer. It also applies a secondary key for the SimilarityReducer to process all the inner points before the outer points.
SimilarityReducer ( Reduce 3) finds the similar pairs and effectively compresses them on the fly. Streaming of data reading can be achieved because the MapReduce infrastruc-ture can stream data from disk when necessary.

When the repartitioning is done, MR-MAPSS checks whether each workset is small enough and repartitions again if there are still some large worksets. We omit this iterative process for simplicity. Next, we study the design of repartitioning, and how it scales even under scarce system memory. RepartitionReducer: A RepartitionReducer reads inner points before outer ones, due to the use of secondary keys by SimilarityMapper. Each reducer evaluates one workset. If the available memory can accommodate the whole workset and the workload is relatively small, the algorithm outputs all the points in the workset as is, deferring the pair evalu-ation to the SimilarityReducer. Otherwise, repartitioning is carried out, as will be discussed next.

To repartition the large worksets into smaller ones, the workset X  X  inner points are first divided into multiple  X  X nner sub-partitions X  using randomly sampled centriods. Each in-ner sub-partition then forms a new sub-workset by picking up some outer points, that could be either inner or outer in the original workset. Figure 7a illustrates this concept by splitting the original workset into two smaller ones. The repartitioning operation results in two smaller sub-worksets by duplicating p 5 , p 9 . Interestingly, p 8 no longer exists in the sub-worksets. The correctness is ensured as SimSet ( I, I  X  O ) = SimSet ( I 1 , I 1  X  O 1 )  X  SimSet ( I 2 , I 2  X  O
So far, repartitioning assumed all inner points fit in mem-ory. Next, we describe a streaming repartitioning scheme that relaxes this assumption, and prove its correctness. Streaming Repartition : After identifying the worksets to be repartitioned, the algorithm processes points in a stream-ing way, thus avoiding crashing if the inner set does not fit in the RepartitionReducer memory. It first randomly samples points from the inner set. Each one of the samples will be a centroid to one of the new sub-worksets .

The streaming operation has two phases -inner point streaming and outer point streaming. If all the inner points fit in memory, calculating the radii of each inner sub-partition is trivial. Otherwise, the exact radii cannot be known un-til it finishes processing the disk-resident inner points. By adding the old centroid in the centroids list, the radius of the original workset is guaranteed to be an upper bound on all the radii of the new worksets. In this case, the original radius is assigned as the tentative radius when streaming the inner points. The observation on the radii upper bound allows us to avoid outputting the point with every centroid as an outer point. After all inner points are processed, the exact radii are adjusted based on the inner point assignment. This adjustment minimizes the redundancies for the outer points streaming.
 Algorithm 4 P rocessInnerPoint 1: point is emitted as inner point of its closest workset W 3: point is emitted as outer point of workset W
The disk-resident inner (to the original centroid) points a re scanned. For each point, the RepartitionReducer emits the point as an inner point to the sub-workset with the clos-est centroid. It also emits the point as an outer point with all the sub-worksets where the point is within r + t of it cen-troid, and r is the radius of the original centroid. Figure 7b shows a state change of the worksets when a new inner point arrives after the memory is filled up. Point p is deemed an outer point for W 1 since dist ( p, c 1 ) does not exceed r + t .
After all such points are scanned, the radii of the new centroids are calculated. The new radii (instead of the radius of the original centroid) are used when scanning the disk-resident outer (to the original centroid) points and assigning them as outer points to the new centroids.

Processing disk-resident outer points is almost identical to the loop in Algorithm 4. If the distance of the point to any new centroid, W i , does not exceed R W i + t , it is output as an outer point with W i . Figure 7c provides an illustration.
Theorem 5.1. Streaming Correctness: StreamRepartition splits the workset W into P worksets W 1 , 2 ,...,P , such that SimSet ( W.I, W.I  X  W.O ) =  X  P i =1 SimSet ( I i , W i ) holds.
Proof. If the inner points fit in memory, the proof is trivial. We only discuss the other case. Due to the sort-ing on point type, all the inner points are processed be-fore any outer points. We divide the streaming into three phases. In the first (second) phase, the RepartionReducer s cans the inner points of the original workset that fit (do not fit) in memory. In the third phase, it scans the outer points of the original workset. Denote the inner (outer) set for workset W i by I ij ( O ij ) after phase j finishes. De-note all the inner (outer) points of W i as I i ( O i ). Thus, I partitions  X  P i =1 I i 1 into P smaller worksets. Also, it outputs i =1 O i 1 . Hence, the output of the first phase is used by SimilarityReducer to compute  X  P i =1 SimSet ( I i 1 , I i 1 larly, the output of the second phase is used by Similari-tyReducer to compute  X  P i =1 SimSet ( I i 2  X  I i 1 , I i 2 the output of the third phase is used by SimilarityReducer to compute  X  P i =1 SimSet ( I i 2 , O i 3 ). By combining the Sim-ilarityReducer computed pairs from the above output, we get SimSet ( W.I, W.I  X  W.O ).

The form h w orksetID, h point, pointT ype, distance ii is used for the output of RepartitionReducer. There could be multi-ple repartitioning steps, though we find repartitioning once is sufficient, in practice, for solving any scalability issues.
We analyze the algorithm, then explain the experiments setup and results.
Given the data D and M machines, assume the data is partitioned with N centroids. We analyze the complexity of each MapReduce job. One notable feature of the framework is that it has minimal redundancy as each pair is considered only once in SimilarityReducer.
 The Sampling step computes statistics on every partition. As each machine works independently on roughly O ( | D | /M ) records, this step is embarrassingly parallel. Usually it could be done in tens of seconds.

The Statistics step computes the centroid statistics. A na  X   X ve approach would compare every point with all the cen-troids, thus having a complexity of O ( K  X | D | /M ). However, by using some indexing on the centroids, we can significantly reduce the number of comparisons.
 In RepartitionMapper, the IO cost takes most of the time. Let I denote the total number of repartitions. In most cases I = 1. Basically, the IO cost is O ( I  X  P K i =1 | workset need to route the worksets to the same Reducer. Reparti-tionReducer time is linear to the workset size if the number of repartitioned worksets is fixed to a constant(20 in our implementation). Fortunately, most sorting can be done in main memory and the shuffling overlaps with the Mapper. The repartition step reduces the workset size at the expense of extra overhead. This tradeoff is evaluated in Section 6.3.
SimilarityReducer is often the most time-consuming oper-ation in the framework. A na  X   X ve implementation would have  X (( N/ ( M  X  I )) 2 ) complexity. However, because a workset is repartitioned when the workload is above the threshold ut , the time complexity is bounded by ut . Usually Similari-tyReduer performance is even better given our indexing and compression techniques.
The principal motivation for this work is to find all simi-lar pairs of records in a dataset of network traffic collected at Google. Each record corresponds to the distribution of a used a subset of this dataset of approximately 2M records where each record is a point in a 30-dimensional space. Ad-ditionally, we report results on a publicly available dataset from the Netflix competition [23]. This dataset contains 480000 users and their ratings on 17770 movies. Each rat-ing is an integer ranging from 1 to 5. We identified a useful subset, namely the 20 most rated movies (dimensions) and the 421144 users (records) who rated them.

We use these datasets to study five issues. First, we in-vestigate the role of repartitioning as a survival mechanism when the computational resources are constrained. Second, we examine repartitioning as a mechanism to mitigate the undersampling problem, where the number of selected cen-troids is relatively small, and hence MR workers are over-loaded. Third, we assess the effectiveness of the compression strategy. Forth, we study the scalability of the MR-MAPSS approach as the number of machines is varied, and finally, we compare our approach with similar efforts.

For all the experiments, a set of default settings is used unless otherwise stated. The MapReduce jobs are run on 500 machines (500 Mappers and 500 Reducers). The pairwise distances are normalized to [0 , 1] 4 . The distance threshold assumes the values 0 . 5, 0 . 2, and 0 . 05. F igure 8: Repartitioning on the Google Dataset
As mentioned above, repartitioning can be autonomously disabled when it is deemed not useful. However, for these experiments, repartitioning is always enabled to study its impact on the overall performance. Figure 8 illustrates this effect when the number of centroids is 500, 1000 and 5000 on the Google dataset. The algorithm with repartitioning is several times faster than the one without repartitioning for almost all configurations. The only exception, under the configuration of 0 . 5 threshold and 5000 centroids, is because we reached the oversampling zone. When oversampling, the number of selected centroids is relatively large, and the over-head of the extra repartitioning MapReduce step does not justify the extra little load balancing. On the other hand, from Figure 9, repartitioning was never useful on the Net-flix data. This is because that data was not highly clustered, thus naturally highly load-balanced.

Another major motivation for repartitioning is when the computational resources are limited. An experiment was conducted on the Google dataset with 50 machines, each prietorial considerations. Earth Movers Distance and KS-Statistic, we report results using the Euclidean distance. The reason is no significant trend differences were detected between the metrics. Figure 9: Repartitioning on the Netflix Dataset, t = 0 . 0 5 with a 500MB memory limit. When running with 100 cen-troids, the repartitioning method always outperformed the non-repartitioning counterpart, as shown in Table 2. The non-repartitioning runs were aborted when the threshold was set to 0 . 5, and failed several times before succeeding when the threshold was 0 . 2. When running the experiments on larger datasets, the non-repartitioning runs crashed im-mediately when the memory was constrained to 3GB, while the repartitioning case completed in reasonable time. Table 2: Run Time (seconds) on the Google Dataset with Limited Memory (500MB)
Threshold Non-Repartitioning Repartitioning 0.5 Failure 23864 0.2 69010 28866 0.05 3019 3639
T o appreciate the effectiveness of compression, we de-signed a na  X   X ve algorithm, Basic, which does brute-force com-putations for each workset, and outputs the similar pairs without compression. For the Google dataset, Basic failed due to the sheer number of pairs. Thus, we only report re-sults on the Netflix dataset in Figure 10. On average, com-pression resulted in an order of magnitude improvement. F igure 10: Compression on the Netflix Dataset
On the Google dataset, we examined the effect of the se-mantic compression technique by comparing different output sizes as the threshold and the number of centroids, N , were varied (see Figure 11). The output sizes reported here are after MapReduce further compresses the output with bzip2. At 0 . 5 we have an interesting situation where most of the pairs are similar. So, there was little benefit from pruning, but there was massive benefit from compression. When the threshold was 0 . 2, many pairs had to be considered, and there was moderate benefit from compression. Notice that the output size when t was 0 . 5 is almost an order of magni-tude smaller than its size when t was 0 . 2, even though the total output comprises more pairs. At 0 . 05 there was the least benefit from compression. However, given the thresh-old is very low, our pruning techniques worked well. It is also worth noting that towards the lower thresholds, as N increases, the compression power is restrained by the smaller workset sizes.
Figure 11: Compression on the Google Dataset We present the scalability results on both the Netflix and Google datasets in Figures 12. We varied the threshold and the number of centroids, N . We selected 500 and 1000 cen-troids for the Netflix dataset and 5000 and 10000 centroids for the Google datasets. From Figures 12, for most of the different combinations of parameter values, the MR-MAPSS framework scaled well with the number of machines. The main exception was the case when 1000 centroids were se-lected for the Netflix dataset when the threshold was 0 . 05. This is mainly because the scale of Netflix dataset is rela-tively small. Running with 1000 centroids on 500 machines does not help much in reducing the overall running time but brings in extra system communication and overheads. F igure 12: Scalability on the Netflix and Google Datasets
We first briefly note that MRSimJoin [28] is distinct from our approach in that it does not address streaming data when input cannot fit in the reducer memory. Moreover, it does not consider compressing the output. The Theta-Join framework [24] enables the comparison of competing strate-gies when the join problem is: i) input-size dominated , when the reducer-input related costs dominate the join com-pletion; ii) output-size dominated , when the reducer-output cost outweighs other costs and iii) input-output balanced , if neither dominates the other. Since MRSimJoin does not address the output size problem (see Section 6.4), h ere we focus only on the input-size comparison 5 .
We also compare these two approaches with an approach called record partitioning (RP). This na  X   X ve approach first splits the data by record into P partitions. Each parti-tion is then routed to all other partitions to form worksets and compute similar pairs. Two synthetic datasets and the Netflix dataset are considered for this evaluation. GAUS-SIAN and CLUSTERED are both 20 dimensional datasets with 3000000 records. For GAUSSIAN, each element in the record follows normal distribution N (1 , 1). For CLUS-TERED, each dimension following 1 + 2  X  U nif orm (0 , 1). For ease of comparison, we apply the same indexes proposed by Jacox and Samet [17], which ensures that the total num-ber of evaluations is roughly identical in both methods.
We now consider the size of intermediate data that is fed to the reducers. As shown in Figure 13, on all datasets, our approach consistently reduces the intermediate data size by half even compared with MRSimJoin, resulting in signifi-cant gains. The rationale is that MRSimJoin is also an N-Way partitioning model with a different routing scheme. For any two partitions P i and P j , it sends both OP ( P i , P OP ( P j , P i ) to the same reducer. Thus, the amount of inter-mediate data to the Reduce phase is the same as the na  X   X ve approach in Figure 4. Essentially, MR-MAPSS delays about half of the overall evaluations to the Reduce phase, thus gen-erating much fewer candidate pairs. Both MRSimJoin and Record Partitioning do not consider output compression at all (described in Section 4) which results in an additional performance benefit for our method. (a) GAUSSIAN F igure 13: Reducer Input for MR-MAPSS(MM), MRSimJoin and Record Partitioning (RP). X axis is threshold.
The set APSS problem, also known as set similarity self-join, has been widely studied in the literatures due to its wide applicability. Several key optimizations, including in-verted indexes [26], prefix filtering [8, 6], suffix filtering [32], were proposed. MapReduce versions also exist [10, 5, 22, 31]. Some optimizations in [32] are adopted in a MapRe-duce setting in [31] for database joins. One recent paper [22] discussed the inefficiencies in [31], and proposed an order of magnitude faster and a more scalable approach. The al-gorithms in [10, 5] approximate the set and multiset simi-larity using cosine similarity. Very recently, a partitioning approach [1] has applied cosine similarity on vectors.
The Metric APSS problem has many applications, e.g., recommender systems [19] and Internet-scale image search [25, 20]. Sequential solutions [17] employed effective indexes for metric similarity joins. Silva et al [28] demostrated a MapReduce-based metric APSS applications on EC2 by ex-tending the  X  X eneralized hyperplane partition X  idea. How-thors do not mention a specific approach for estimating the join matrix efficiently for similarity self joins. ever, they did not consider surviving resource sparsity and output compression.
In this paper, we proposed the novel MR-MAPSS frame-work. The backbone of our approach intelligently routes the input data records into independent worksets processed by independent MapReduce workers such that no pair is evaluated twice. We further enhanced the load-balancing of the partitioning algorithm by using workset repartitioning, and employing compression of the output pairs. Finally, we demonstrated the efficiency, the effectiveness and the scal-ability of the framework using real datasets on hundreds of machines. Future work will study automatic parameter tuning and compare with approaches on Hamming and edit distances. YW and SP acknowledge support from a Google Research Award and NSF Grant IIS-0917070, and the suggestions from Yang Zhang, Dave Fuhry and Tyler Clemons.
