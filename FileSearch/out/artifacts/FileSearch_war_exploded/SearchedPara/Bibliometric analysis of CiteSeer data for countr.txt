 1. Introduction
CiteSeer (CiteSeer) is a vast free Web digital library and search engine of mainly computer science papers that have been automatically acquired from various Web sites, stored, and analyzed to allow for searching and exploring its bibliographic liometric studies particularly due to fears of incomplete and erroneous machine-generated data. We refer to the work by
Fiala (2011) where a detailed overview of CiteSeer X  X  features in the context of other established bibliographic databases is given.

The purpose of this study is to show: (a) where CiteSeer has got its data (i.e. which Web domains it has visited to obtain them), (b) which countries have contributed most to its digital library (in terms of the number of papers published by
We have thoroughly analyzed the CiteSeer data file from December 13, 2005 and have made a quick look at the newer data article (May 2011). 2. Related work
There have been a number of studies of research productivity (publications) and impact (citations) at the level of countries in recent years. There is a growing need for such scientometric indicators because they often reflect the quality of science policy in a specific country and may have influence on changes in science funding. From the many research papers  X  compares the United States to the European Union in a detailed way in various fields of science.

While quite a lot of research efforts have been devoted to bibliometrics of chemistry, biology, or humanities, relatively few scientometric studies have been concerned with the field of computer science. Bakri and Willett (2011) measure the per-formance of computer science research in Malaysia and Gupta, Kshitij, and Verma (2011) analyze the research output of In-dian computer science. Wainer, Xavier, and Bezerra (2009) compared the Brazilian computer science production to twelve research performance of universities around the globe and Guan and Ma (2004) evaluated China and five other countries.
Different sources of bibliographic data for the scientometric evaluation of computer science publications were examined by Bar-Ilan (2010) and by Franceschet (2010a, 2010b) . The latter author also presents an overview of literature comparing ence of computer science journal and conference papers on the scientific community.

Unlike our paper, most of the articles above have mainly exploited the well-known and manually-maintained biblio-graphic database Web of Science (Web of Science) or its variants. As far as CiteSeer as a data source is concerned, some researchers have already used it for bibliometric purposes: Zhou, Councill, Zha, and Giles (2007) explored CiteSeer docu-ments to discover temporal communities of collaborating authors in the domains of databases and machine learning. On the other hand, Hopcroft, Khan, Kulis, and Selman (2004) tracked evolving communities in the whole CiteSeer paper citation graph. An, Janssen, and Milios (2004) conducted a component analysis of the CiteSeer paper citation graph in several re-search domains and CiteSeer X data were used by Wu and Koh (2010) in order to enhance collaborative networks with topic information. Zhao and Strotmann (2007) and Zhao and Logan (2002) analyzed co-citations in CiteSeer documents in the XML research field and a similar study for computer graphics was reported by Chen (2000) . Bar-Ilan (2006) used CiteSeer data for a citation analysis of the works of a famous mathematician. A kind of citation analysis for acknowledgements was also per-formed by Giles and Councill (2004) . Feitelson and Yovel (2004) examined citation ranking lists obtained from CiteSeer and predicted future rankings of authors.

Our study is the first of its kind that attempts to measure the productivity and impact of computer science research con-ducted by countries by analyzing CiteSeer data. 3. Data
The last CiteSeer data originate from December 2005 and they contain roughly 717 thousand publications with 1.8 mil-lion references within CiteSeer. On the other hand, CiteSeer lications with almost 15 million references within CiteSeer and references as edges has become much denser over the past 6 years  X  the mean number of references in a publication increased from 2.5 in 2005 to 11.2 in 2011.
 Let us have a look at a few obvious differences between CiteSeer (CS)/CiteSeer  X  two well-known databases of scientific literature. Both CiteSeer and CiteSeer way: they crawl the Web starting from some seed pages submitted by their engineers or by individual users (authors) and pick up freely accessible documents (mostly PDF or PostScript files) that have the potential to be research papers in com-puter science, mathematics, or related fields. Web crawling as well as information extraction (titles, author names, refer-ences, etc.) occurs automatically, without human intervention. The contents of CiteSeer and CiteSeer on the content and structure of the Web. On the other hand, both Web of Science and Scopus use a great deal of human labor to receive publications (mainly journal issues and conference proceedings) and to index them. Unlike CiteSeer and CiteSeer
WoS and Scopus cover all scientific fields. Which publication sources are indexed and which are not is decided by the edi-torial boards of both  X  X  X uman-made X  X  databases. Another big difference between CiteSeer and CiteSeer and Scopus on the other is that the first two are free whereas the latter two are subscription-based. 4. Methods 4.1. Data collection Data collection methods were different for CiteSeer and for CiteSeer created in December 2005 (the most recent CiteSeer data) that we merely downloaded from the CiteSeer Web site and un-packed into 2 GB of 72 XML-like files. As for CiteSeer X , we were forced to use one of the harvesting tools referenced on its
Web site to gain off-line access to its current repository. The harvest itself took a few days in March 2011 and resulted in a regular 3.7 GB XML file which we further split up into 73 files to process them more smoothly in main memory. We devel-had capabilities to compute more complex values such as HITS and PageRank. 4.2. Internet domains and countries
Gathering statistics about Internet top-level domains (TLD) is quite smooth and accurate given that the  X  X  X ource X  X  property for each document is almost always present and error free. The situation gets considerably worse when we try to assemble
CiteSeer. Therefore, we could not use CiteSeer X data for our experiments with countries. Let us hope that future versions of CiteSeer X (the current one is still a beta) will have such information included. 4.3. Missing data and name unification
In CiteSeer, there is a problem with missing data. For almost each document, there are authors assigned to it but only for out any name unification or disambiguation), we had no address information at our disposal for about 690 thousand or 42% of them, let alone the accuracy of such information.

Thus, to obtain the data shown later in Fig. 2 , we proceeded in the following way: We discarded publications without any address information for any of its authors. This resulted in only 439 thousand being kept. (For these publications, one author at least had some address information included.) Then, we tried to unify country names used in the addresses. This task con-or territories were left. Next, we attempted to unify country names by replacing common synonymic variants of each of those 243 countries with one standard name.
 such occurrences as  X  X  X SA X  X . Other types of unification included considering often independently appearing entities such as
England, Scotland, Wales and Northern Ireland as one country (United Kingdom) or, in contrast, keeping territories of one country separate such as Hong Kong, Taiwan, and Macau from China or Reunion and Martinique from France. Finally, we processed international postal country codes in the addresses as well, thus yielding Czech Republic for an address  X  X  X Z-30416 X  X  with respect to the prefix  X  X  X Z- X  X  as an example. 4.4. Comparison with the Web of Science and Scopus
Since the CiteSeer data we examined were from December 2005, we restricted our analysis to a 10-year period from 1996 to 2005. This decade is the most probable one, in which CiteSeer was collecting its documents. Moreover, Scopus itself does not generally capture citations to documents published before 1996, which is also a good reason for 1996 as a decade X  X  start with regard to possible future comparisons of citations. In September 2010, we were querying on-line Web services of both
WoS and Scopus and generated the rankings in Tables 3 and 4 . As for WoS, we opted to limit our search to the  X  X  X cience Cita-tion Index Expanded X  X  database, to the  X  X  X rticle X  X  document type, and to the publications from the journals included in the seven computer science subject categories of the Journal Citation Reports was easier in that the subject area (computer science) could be specified directly in the query and the exact results number was always disclosed. The final 325 614  X  X  X rticle X  X  documents form 100% for the relative shares in Table 4 . Due to the search limits of both WoS and Scopus, it was sometimes necessary to split up  X  X  X ig X  X  queries into subqueries and to combine their results.

Alternatively, WoS as well as Scopus provide programming interfaces that enable submitting queries and obtaining re-sults without needing to interact with their Web front-ends. However, the basic APIs included in the subscription do have queries and results restrictions that are similar to those on their Web sites. 4.5. Citations and recursive indicators
In addition to measuring shares of individual countries in the publications indexed by CiteSeer, we wished to determine the influence of countries by examining citations they receive. Thus, we derived a citation graph of countries from the cita-tion graph of publications. In the directed publication citation graph, there were 717 thousand nodes (publications) and 1.76 million edges (citations between publications). This accounts for roughly 2.45 citations per paper so, obviously, many cita-proach described earlier. We aggregated citations by the country of the source and target publication. If there were more
Besides first-order methods such as in-degree and citations, there are recursive techniques as well that not only count citations but take also into account whether the citing node itself is frequently cited. Some of these methods are HITS intro-2008 ). We applied these methods to the normalized country citation graph from CiteSeer and present the country rankings obtained in Table 6 . 5. Results and discussion 5.1. Internet domains
One of the properties of each document item indexed by CiteSeer is its source. This is the URL (a Web page) from which the document has originally been downloaded. We were interested in the distribution of Internet top-level domains (TLD) among the sources of CiteSeer documents. This would reveal what regions of the Web the CiteSeer Web crawler has visited covered later.
 Fig. 1 shows the shares of top 20 top-level Internet domains as sources of CiteSeer and CiteSeer domains do not necessarily mean US Web sites, we shall not be too far from the truth if we count them along with . gov as US sites and claim that about a half of all CiteSeer documents have been gathered in the United States with a small increase by several percentage points from 2005 to 2011. In 2005, only 25 documents had no source URL affiliated with them and they are included in those almost 10% of  X  X  X ther X  X  domains. In 2011, this number is considerably higher  X  almost 17 thousand  X  and (from 62 to 41) while the main country-code TLDs remain relatively stable or even slightly decline. There is one remarkable exception, . in , which increases its rank from 37 to 25 and its share from 0.20% to 0.55% between the years 2005 and 2011. In this context, it is interesting to see that the position of . cn (38) remains unchanged in both CiteSeer and CiteSeer
Nowadays, most open access repositories are located within North America and Europe ( Repository66 ) and, therefore, it is logical that even Asian researchers might prefer placing their manuscripts in the repositories of these regions, which further increases the prevalence of American and European top-level Internet domains crawled by CiteSeer. 5.2. Countries
After unifying country names in the available addresses as described in Section 4.3, we tried to assign all 439 thousand publications to one or more country depending on how many authors from which countries they had. About 25 thousand to identify a standard country by the above approach. Thus, only 414 thousand documents (58% of 717 thousand) were finally assigned to one or more country. We counted the assignments to countries and found out country shares that are from those presented in Table 2 .

The relative shares in Fig. 2 sum up to 100% constituted by a total of 449 thousand publication-country assignments, which is not equal to 414 thousand publications due to international co-authorships. ( Albarr X n et al. (2010) call the publi-cation-country assignments  X  X  X xtended articles X  X .) Even though the number of such assignments is only less than 10% greater than that of publications, it does not necessarily imply a relatively low number of international publications in CiteSeer. We may rather assume that addresses in international papers are more difficult to be processed by a machine (CiteSeer) and, therefore, they are often missing or erroneous and do not appear in our cleansed data.
 In Fig. 2 , the top 20 most represented countries take almost 93% of  X  X  X xtended articles X  X . The first country is the United
States with a fourfold greater share (42.59%) than the second most  X  X  X rolific X  X  country  X  Germany (10.65%). At the third posi-tion, there is a tie between France and the United Kingdom (both 5.35%). As a remarkable point, two developing countries have entered the Top 20  X  India and Brazil with shares of 0.67% and 0.64%, respectively. The number (or share) of publica-tions not assigned to any country is not visible in Fig. 2 .

The relative shares in Table 2 are smaller than those in Fig. 2 because the base (100%) is much larger  X  717 thousand, which is the original number of CiteSeer documents. These relative shares are important for they help us compare CiteSeer publication shares with those from the Web of Science and Scopus where the number of all documents can be determined, but the number of publication-country assignments is unknown. The absolute numbers in Table 2 are the numbers of pub-one country, the sum of counts in Table 2 would be approximately 717 thousand and the total share 100% (the rest after rank 100 is negligible). If each document was assigned to two or more countries (i.e. all papers are internationally co-authored), the sum of counts would be more than 717 thousand and the total share more than 100%. A further discussion of the results in Table 2 will follow in the next section along with a comparison to the Web of Science and Scopus. 5.3. Comparison with the Web of Science and Scopus
To get a clue how reliable CiteSeer data are and to see how distant or close to other well-known bibliographic data sources they are, it was necessary to perform a couple of comparisons and measurements. Based on the amount of available information on publication shares of countries from the previous section, we decided to compare these country shares to those obtained from the Web of Science and Scopus  X  two established manually maintained bibliographic databases. The goal was to create rankings of countries by the number of  X  X  X heir X  X  publications in the field of computer science and to com-pare them to the CiteSeer ranking in Table 2 .

In addition to article counts, we also found out numbers of citations to the articles, average citations per article, and h -number of publications and the countries from the top 20 CiteSeer countries (see Table 2 ) are marked with their CiteSeer rank in the second column. When looking at the rankings, we may immediately note that three East Asian countries (main-land China, South Korea, and Taiwan) are under-represented in CiteSeer. Both WoS and Scopus place them in the Top 10 whereas in CiteSeer they are at ranks around 30. The corresponding top-level Internet domains . cn ,. kr , and . tw in Table 1 are also relatively lowly ranked, which might suggest that CiteSeer did not crawl these Web regions so extensively as it should have regarding their real scientific productivity in computer science. Otherwise, we cannot see any striking discrep-ancies between CiteSeer on one side and WoS and Scopus on the other.

Publication shares of the top 20 CiteSeer countries in CiteSeer, WoS, and Scopus are shown in Fig. 3 . There are no evident outliers or differences either, except perhaps for a greater USA share in WoS. In Fig. 4 , we show Spearman X  X  rank correlation coefficients between the rankings of CiteSeer and Scopus, CiteSeer and WoS, and Scopus and WoS for the top 10, 20, 30, 40, ten, which are significant at the 0.05 level. Not surprisingly, the rankings from Scopus and WoS are always very highly pos-in the top 50. We may conclude that the ranking by publications from CiteSeer ( Table 2 ) is relevant and quite competitive compared to the rankings from both WoS and Scopus. As there is no simple way of obtaining the total count of citations to all computer science publications published from 1996 to 2005 from the Web sites of WoS and Scopus, which would be nec-essary to determine the relative citation shares in Tables 3 and 4 , we do not present a comparison plot similar to Fig. 3 for citations. But we do show, in analogy to Fig. 5 , how citation-based rankings correlate with each other in Fig. 5 .Aswe pus and WoS. All the coefficients in Fig. 5 are significant at the 0.01 level (two-tailed). 5.4. Citations and recursive indicators
Finally, the resulting directed graph of citations between countries had 243 nodes (countries) and 2472 edges (citations between them). There were no parallel edges in the graph. Instead, a weight was assigned to each edge denoting from how many parallel edges the edge was created. The sum of weights in the whole graph was about 1.5 million.

In Table 5 , we can see the top 80 countries ordered descendingly by their in-degree in the country citation graph. In the ings place USA, Germany, and the United Kingdom at the top with approximately 48%, 8%, and 6% of all citations, respec-tively. The rank four in in-degree is tied by Canada and France with the same number of citing countries (74) but, in total, France is cited more often by foreign countries and is positioned ahead of Canada in Citations. A similar behaviour may be observed with several other countries. The country rankings in Table 6 were obtained by applying recursive tech-niques, but despite their much higher computational costs they do not seem to provide any striking new information, though. We found the five rankings in Tables 5 and 6 to be very highly positively correlated with each other with Spearman X  X  q between 0.97 and 1 (all significant at the 0.01 level two-tailed). 6. Conclusions and future work
We have presented a thorough study of CiteSeer data with focus on countries and territories with which authors of pub-lications indexed by CiteSeer are affiliated. The main contributions of the study are the following: Based on our analysis, we have obtained the following key results:
The study presented in this paper is the first of its kind that seeks to determine the most influential countries in computer examines publications from the Web of Science and Scopus from 2001 to 2005 and is not at all concerned with citations.
Even less countries (six) are explored by Guan and Ma (2004) for the period of 1993 X 2002. Both studies, in accordance with our results, document a clear superiority of the USA over the rest of the world in computer science research. Unfortunately, there seems to be no previous complex computer science study for countries with which we could compare our findings.
Although CiteSeer data are far from complete and precise (in our experience, some 10% of the existing information might be erroneous), we may conclude that CiteSeer is a free digital library of valuable data and may be successfully used in bibliometric studies, possibly along with other well-known bibliographic databases, as we have shown in this paper. Let us underline in this place that the results we present depend solely on the content and quality of CiteSeer data. If other regions of the Web had been crawled, if Asian paper repositories had been preferred by authors (see Section 5.1), or if the information extraction from papers done by CiteSeer had been more precise and complete, the outcomes of our analysis could have been different. Let us hope in this respect that CiteSeer way and that it will enrich its metadata with the information on addresses and affiliations as well. Our future work on Cite-exploring further differences between the data in CiteSeer and CiteSeer Acknowledgements This work was supported by the European Regional Development Fund (ERDF), Project  X  X  X TIS  X  New Technologies for
Information Society X  X , European Centre of Excellence, CZ.1.05/1.1.00/02.0090. Many thanks are due to the anonymous reviewers for their useful comments.
 References
