 Clustering is ubiquitous in science and engineering, with numerous and diverse application domains, ranging from bioinformatics and medicine to the social sciences and the web [15]. Perhaps the most well-known clustering algorithm is the so-called  X  k -means X  algorithm or Lloyd X  X  method [22], an iterative expectation-maximization type approach, which attempts to address the following objec-of the objective, as well as the good behavior of the associated algorithm (Lloyd X  X  method [22, 28]), have made k -means enormously popular in applications [32].
 In recent years, the high dimensionality of the modern massive datasets has provided a considerable challenge to k -means clustering approaches. First, the curse of dimensionality can make algorithms for k -means clustering very slow, and, second, the existence of many irrelevant features may not such obstacles by introducing feature selection and feature extraction techniques. It is worth not-ing that feature selection selects a small subset of actual features from the data and then runs the clustering algorithm only on the selected features, whereas feature extraction constructs a small set significance of the problem, as well as the wealth of heuristic methods addressing it (see Section 3), there exist no provably accurate feature selection methods and extremely few provably accurate Our work here addresses this shortcoming by presenting the first provably accurate feature selection space, and then selects a small number of features (roughly k log( k ) , where k is the number of clusters) with respect to the computed probabilities. (See Section 2 for a detailed description of our algorithm.) Then, we argue that running k -means clustering algorithms on the selected features returns a constant-factor approximate partition to the optimal. (See Theorem 1 in Section 2.) We now formally define the k -means clustering problem using the so-called cluster indicator matrix. sum of the squares of its elements. (See also Section 4.1 for useful notation.) Given a matrix A  X  R n  X  d (representing n points  X  rows  X  described with respect to d features  X  columns) and a positive integer k denoting the number of clusters, find the n  X  k indicator matrix X opt such that The optimal value of the k -means clustering objective is In the above X denotes the set of all n  X  k indicator matrices X .
 We briefly expand on the notion of an n  X  k indicator matrix X . Such matrices have exactly one in particular X ij = 1 / number of non-zero elements in the j -th column of X ). Note that the columns of X are normalized and pairwise orthogonal so that their Euclidean norm is equal to one, and X T X = I k , where I k is the k  X  k identity matrix. An example of such an indicator matrix X representing three points (rows in X ) belonging to two different clusters (columns in X ) is given below; note that the points the second cluster ( s 2 = 1 ): The above definition of the k -means objective is exactly equivalent with the standard definition of k -means clustering [28]. To see this notice that A  X  XX T A 2 above, A ( i ) and X ( i ) denote the i -th rows of A and X , respectively. Algorithm 1 takes as inputs the matrix A  X  R n  X  d , the number of clusters k , and an accuracy the i -th leverage score equals the square of the Euclidian norm of the i -th row of V k (denoted by ( V a probability distribution over the columns of A since a sampling parameter r that is equal to the number of (rescaled) features that we want to select. section 5). Having r fixed, Algorithm 1 performs r i.i.d random trials where in each trial one column of A is selected by the following random process: we throw a biased die with d faces with each face running time of Algorithm 1 is dominated by the time required to compute the top-k right singular vectors of the matrix A , which is at most O
Input: n  X  d matrix A ( n points, d features), number of clusters k , parameter  X   X  (0 , 1) . Output: n  X  r matrix  X  A , with r =  X ( k log( k/ X  ) / X  2 ) .
 Algorithm 1: A randomized feature selection algorithm for the k -means clustering problem. In order to theoretically evaluate the accuracy of our feature selection algorithm, and provide some a priori guarantees regarding the quality of the clustering after feature selection is performed, we chose to report results on the optimal value of the k -means clustering objective (the F opt of Defi-nition 1). This metric of accuracy has been extensively used in the Theoretical Computer Science community in order to analyze approximation algorithms for the k -means clustering problem. In example, [21, 1] and references therein) invariably approximate F opt .
 be employed as a preprocessing step. Then, an approximation algorithm for the k -means clustering problem would be applied on  X  A in order to determine the partition of the rows of A . In order to formalize our discussion, we borrow a definition from the approximation algorithms literature. A and k , and returns an indicator matrix X  X  that satisfies with probability at least 1  X   X   X  , In the above  X   X   X  [0 , 1) is the failure probability of the algorithm.
 Clearly, when  X  = 1 , then X  X  is the optimal partition, which is a well-known NP-hard objective. If we allow  X  &gt; 1 , then many approximation algorithms exist in the literature. For example, the work the k -means++ method of [1] achieves  X  = O (log( k )) using the popular Lloyd X  X  algorithm and a sophisticated randomized seeding. Theorem 1 (see Section 4 for its proof) is our main quality-of-approximation result for our feature selection algorithm.
 Theorem 1 Let the n  X  d matrix A and the positive integer k be the inputs of the k -means clustering matrix  X  A containing the selected features, where r =  X ( k log( k/ X  ) / X  2 ) .
 If we run any  X  -approximation algorithm (  X   X  1 ) for the k -means clustering problem, whose fail-probability at least 0 . 5  X   X   X  , The failure probability of the above theorem can be easily reduced using standard boosting methods. Feature selection has received considerable attention in the machine learning and data mining com-munities. A large number of different techniques appeared in prior work, addressing the feature selection within the context of both clustering and classification. Surveys include [13], as well as [14], which reports the results of the NIPS 2003 challenge in feature selection. Popular feature [33]. In this section, we opt to discuss only a family of feature selection methods that are closely related to the leverage scores of our algorithm. To the best of our knowledge, all previous feature selection methods come with no theoretical guarantees of the form that we describe here. pal Components Analysis (PCA) corresponds to the task of identifying a subset of k columns from A that capture essentially the same information as do the top k principal components of A . Jol-liffe [18] surveys various methods for the above task. Four of them (called B 1 , B 2 , B 3 , and B 4 in [18]) employ the Singular Value Decomposition of A in order to identify columns that are some-how correlated with its top k left singular vectors. In particular, B 3 employs exactly the leverage scores in order to greedily select the k columns corresponding to the highest scores; no theoretical results are reported. An experimental evaluation of the methods of [18] on real datasets appeared in [19]. Another approach employing the matrix of the top k right singular vectors of A and a Procrustes-type criterion appeared in [20]. From an applications perspective, [30] employed the methods of [18] and [20] for gene selection in microarray data analysis. From a complementary viewpoint, feature selection for clustering seeks to identify those features that have the most dis-criminative power among the set of all features. Continuing the aforementioned line of research, many recent papers present methods that somehow employ the SVD of the input matrix in order to select discriminative features; see, for example, [23, 5, 25, 26]. Finally, note that employing the leverage scores in a randomized manner similar to Algorithm 1 has already been proven to be accurate for least-squares regression [8] and PCA [7, 2]. 3.1 Connections with the SVD A well-known property connects the SVD of a matrix and k -means clustering. Recall Definition 1, and notice that X opt X T opt A is a matrix of rank at most k . From the SVD optimality [11], we immediately get that (see section 4.1 for useful notation) A more interesting connection between the SVD and k -means appeared in [6]. If the n  X  d matrix A is projected on the subspace spanned by its top k left singular vectors, then the resulting n  X  k matrix  X  A = U k  X  k corresponds to a mapping of the original d -dimensional space to the optimal (the columns of U k ) correspond to the constructed features (  X  k is a simple rescaling operator). Prior to the work of [6], it was empirically known that running k -means clustering algorithms on the low-dimensional matrix  X  A was a viable alternative to clustering the high-dimensional matrix A . k -means partition on  X  A , i.e., then using this partition on the rows of the original matrix A is a 2-approximation to the optimal partition, a.k.a., The above result is the starting point of our work here. Indeed, we seek to replace the k artificial features. On the positive side, an obvious advantage of feature selection vs. feature extraction is the immediate interpretability of the former. On the negative side, our approximation accuracy is slightly worse ( 2 +  X  , see Theorem 1 with  X  = 1 ) and we need slightly more than k features. This section gives the proof of Theorem 1. We start by introducing useful notation; then, we present a preliminary lemma and the proof itself. 4.1 Notation  X 
A  X  F and  X  A  X  2 denote the Frobenius and the spectral norm of a matrix A , respectively. A +  X  min ( X ) denote the largest and the smallest non-zero singular values of a matrix X , respectively. A useful property of matrix norms is that for any two matrices X and Y ,  X  XY  X  F  X  X  X  X  X  F  X  Y  X  2 and  X  XY  X  F  X  X  X  X  X  2  X  Y  X  F ; this is a stronger version of the standard submultiplicavity property abbreviate  X  X ndependent identically distributed X  to  X  X .i.d X  and  X  X ith probability X  to  X  X .p X . 4.2 Sampling and rescaling matrices We introduce a simple matrix formalism in order to conveniently represent the sampling and rescal-ing processes of Algorithm 1. Let S be a d  X  r sampling matrix that is constructed as follows: S entry which is set to one) is appended to S . Also, let D be a r  X  r diagonal rescaling matrix con-of A is selected, then the next diagonal entry of D is set to 1 / this paragraph, Algorithm 1 outputs the matrix  X  A = ASD  X  R n  X  r . 4.3 A preliminary lemma and sufficient conditions Lemma 1 presented below gives upper and lower bounds for the largest and the smallest singular argues that the matrix ASD can be used to provide a very accurate approximation to the matrix A k . Lemma 1 provides four sufficient conditions for designing provably accurate feature selection al-gorithms for k -means clustering. To see this notice that, in the proof of eqn. (4) given below, the results of Lemma 1 are sufficient to prove our main theorem; the rest of the arguments apply to all sampling and rescaling matrices S and D . Any feature selection algorithm, i.e. any sampling matrix S and rescaling matrix D , that satisfy bounds similar to those of Lemma 1, can be employed to design a provably accurate feature selection algorithm for k -means clustering. The quality of such an approximation will be proportional to the tightness of the bounds of the three terms of Lemma 1 ( || V T the bottleneck in the approximation accuracy of a feature selection algorithm would be to find a || ( V T k S ) + || 2 appeared to be the bottleneck in the design of provably accurate column-based low-rank approximations (see, for example, Theorem 1.5 in [17] and eqn. (3.19) in [12]). It is evident from the above observations that other column sampling methods (see, for example, [17, 3, 2] and references therein), satisfying similar bounds to those of Lemma 1, immediately suggest themselves for the design of provably accurate feature selection algorithms for k -means clustering. Finally, equations (101) and (102) of Lemma 4.4 in [31] suggest that a sub-sampled randomized Fourier transform can be used for the design of a provably accurate feature extraction algorithm for k-means clustering, since they provide bounds similar to those of Lemma 1 by replacing the matrices S and D of our algorithm with a sub-sampled randomized Fourier transform matrix (see the matrix R of eqn. (6) in [31]). Lemma 1 Assume that the sampling matrix S and the rescaling matrix D are constructed using Algorithm 1 (see also Section 4.2) with inputs A , k , and  X   X  (0 , 1) . Let c o and c 1 be absolute constants that will be specified later. If the sampling parameter r of Algorithm 1 satisfies then all four statements below hold together with probability at least 0 . 5 : To simplify notation, we set  X  =  X  Proof: First, we will apply Theorem 3.1 of [29] for an appropriate random vector y . Toward that end, y  X  R k as follows: for i = 1 , ..., d Pr [ y = y y . This definition of y and the definition of the sampling and rescaling matrices S and D imply that V k SDDS also that E [ yy T ] = 1 . Our choice of r allows us to apply Theorem 3.1 of [29], which, combined with the Markov X  X  inequality on the random variable z = V T k SDDS T V k  X  I k 2 implies that w.p at least 1  X  1 / 6 , for a sufficiently large (unspecified in [29]) constant c o . Standard matrix perturbation theory re-sults [11] imply that for i = 1 , ..., k the first two statements of the Lemma hold w.p at least 1  X  5 / 6 . To prove the third statement, we the second condition of the Lemma imply that  X  k ( V T k SD ) &gt; 0 . To prove the fourth statement: A k  X  ASD ( V T k SD ) + V T k The first term of eqn. (9) is bounded by In the above, in eqn. (10) we replaced A k by U k  X  k V T k , and in eqn. (11) we set ( V k SD )( V eqn. (9) is bounded by be dropped without increasing a unitarily invariant norm such as the Frobenius matrix norm. If the first three statements of the lemma hold w.p at least 1  X  5 / 6 , then w.p at least 1  X  1 / 3 , union bound implies that all four statements hold together with probability at least 0 . 5 . 4.4 The proof of eqn. (4) of Theorem 1 We assume that Algorithm 1 fixes r to the value specified in Lemma 1; note that this does not violate the asymptotic notation used in Algorithm 1. We start by manipulating the term A  X  X ~  X  X T ~  X  A 2 in eqn. (4). Replacing A by A k + A  X   X  k , and using the Pythagorean theorem (the subspaces spanned without increasing a unitarily invariant norm. Now eqn. (5) implies that We now bound the first term of eqn. (14): matrix and can be dropped without increasing a unitarily invariant norm. In eqn. (17) we used submultiplicativity (see Section 4.1) and the fact that V T k can be dropped without changing the spectral norm. In eqn. (18) we replaced X ~  X  by X opt and the factor of the matrix ASD , and any other n  X  k indicator matrix (for example, the matrix X opt ) satisfies ( In eqn. (19) we first introduced the k  X  k identity matrix I k = ( V T k SD ) + ( V T k SD ) duced V T k without changing the Frobenius norm. We further manipulate the term  X  5 of eqn. (20): In eqn. (21) we used Lemma 1 and the triangle inequality. In eqn. (22) we replaced A k by AV k V T k and Definition 1. Combining equations (20), (23), (5), Lemma 1, and the fact that  X   X  1 , we get sufficiently large constant c 1 , it is thus Combining eqn. (24) with eqns. (14) and (15) concludes the proof of eqn. (4). Using asymptotic or the  X  -approximation k -means clustering algorithm fail, which happens w.p at most 0 . 5 +  X   X  . We present an empirical evaluation of Algorithm 1 on two real datasets. We show that it selects the 1 is a sufficient -not necessary -condition to prove our theoretical bounds. Indeed, a much smaller choice of r , for example r = 10 k , is often sufficient for good empirical results.
 We first experimented with a NIPS documents dataset (see http://robotics.stanford. edu/  X  gal/ and [10]). The data consist of a 184  X  6314 document-term matrix A , with A ij de-noting the number of occurrences of the j -th term in the i -th document. Each document is a paper that appeared in the proceedings of NIPS 2001, 2002, or 2003, and belongs to one of the following three topic categories: (i) Neuroscience, (ii) Learning Theory, and (iii) Control and Reinforcement Learning. Each term appeared at least once in one of the 184 documents. We evaluated the accuracy of Algorithm 1 by running the Lloyd X  X  heuristic 1 on the rescaled features returned by our method. In order to drive down the failure probability of Algorithm 1, we repeated it 30 times (followed by the Lloyd X  heuristic each time) and kept the partition that minimized the objective value. We report the percentage of correctly classified objects (denoted by P , 0  X  P  X  1 ), as well as the value of features suffices to approximately reproduce the partition obtained when all features were kept. In Figure 1 we plotted the distribution of the leverage scores for the 6314 terms (columns) of A ; we also highlighted the features returned by Algorithm 1 when the sampling parameter r is set to 10 k . We observed that terms corresponding to the largest leverage scores had significant discriminative power. In particular, ruyter appeared almost exclusively in documents of the first and third cate-gories, hand appeared in documents of the third category, information appeared in documents of the first category, and code appeared in documents of the second and third categories only. We also experimented with microarray data showing the expression levels of 5520 genes (features) for mal tumor, 12 with leiomyosarcoma, and 9 with synovial sarcoma. Table 1 depicts the results from our experiments by choosing k = 3 . Note that the Lloyd X  X  heuristic worked almost perfectly when r was set to 10 k and perfectly when r was set to 20 k . Experimental parameters set to the same values as in the first experiment.
