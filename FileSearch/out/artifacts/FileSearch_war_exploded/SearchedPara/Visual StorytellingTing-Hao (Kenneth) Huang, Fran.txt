 Beyond understanding simple objects and concrete scenes lies interpreting causal structure; making sense of visual input to tie disparate moments to-gether as they give rise to a cohesive narrative of events through time. This requires moving from rea-soning about single images  X  static moments, de-void of context  X  to sequences of images that depict events as they occur and change. On the vision side, progressing from single images to images in context allows us to begin to create an artificial intelligence (AI) that can reason about a visual moment given what it has already seen. On the language side, pro-gressing from literal description to narrative helps to learn more evaluative, conversational, and abstract language. This is the difference between, for ex-ample,  X  X itting next to each other X  versus  X  X aving a good time X , or  X  X un is setting X  versus  X  X ky illumi-nated with a brilliance... X  (see Figure 1). The first descriptions capture image content that is literal and concrete; the second requires further inference about what a good time may look like, or what is special and worth sharing about a particular sunset.
We introduce the first dataset of sequential im-ages with corresponding descriptions, which cap-tures some of these subtle but important differ-ences, and advance the task of visual storytelling. We release the data in three tiers of language for the same images: (1) Descriptions of images-in-isolation (DII) ; (2) Descriptions of images-in-sequence (DIS) ; and (3) Stories for images-in-sequence (SIS) . This tiered approach reveals the ef-fect of temporal context and the effect of narrative language. As all the tiers are aligned to the same images, the dataset facilitates directly modeling the relationship between literal and more abstract visual concepts, including the relationship between visual imagery and typical event patterns. We additionally propose an automatic evaluation metric which is best correlated with human judgments, and establish sev-eral strong baselines for the visual storytelling task. Work in vision to language has exploded, with re-searchers examining image captioning (Lin et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Xu et al., 2015; Chen et al., 2015; Young et al., 2014; Elliott and Keller, 2013), question an-swering (Antol et al., 2015; Ren et al., 2015; Gao et al., 2015; Malinowski and Fritz, 2014), visual phrases (Sadeghi and Farhadi, 2011), video under-standing (Ramanathan et al., 2013), and visual con-cepts (Krishna et al., 2016; Fang et al., 2015).
Such work focuses on direct, literal description of image content. While this is an encouraging first step in connecting vision and language, it is far from the capabilities needed by intelligent agents for nat-uralistic interactions. There is a significant differ-ence, yet unexplored, between remarking that a vi-sual scene shows  X  X itting in a room X   X  typical of most image captioning work  X  and that the same vi-sual scene shows  X  X onding X . The latter description is grounded in the visual signal, yet it brings to bear information about social relations and emotions that can be additionally inferred in context (Figure 1). Visually-grounded stories facilitate more evaluative and figurative language than has previously been seen in vision-to-language research: If a system can recognize that colleagues look bored , it can remark and act on this information directly.

Storytelling itself is one of the oldest known hu-man activities (Wiessner, 2014), providing a way to educate, preserve culture, instill morals, and share advice; focusing AI research towards this task there-fore has the potential to bring about more human-like intelligence and understanding.
 Extracting Photos We begin by generating a list of  X  X toryable X  event types. We leverage the idea that  X  X toryable X  events tend to involve some form of pos-session, e.g.,  X  X ohn  X  X  birthday party, X  or  X  X habnam  X  X  visit. X  Using the Flickr data release (Thomee et al., 2015), we aggregate 5-grams of photo titles and de-scriptions, using Stanford CoreNLP (Manning et al., 2014) to extract possessive dependency patterns. We keep the heads of possessive phrases if they can be classified as an EVENT in WordNet3.0, relying on These terms are then used to collect albums using 50 photos where all album photos are taken within a 48-hour span and CC-licensed. See Table 1 for the query terms with the most albums returned.

The photos returned from this stage are then pre-sented to crowd workers using Amazon X  X  Mechani-cal Turk to collect the corresponding stories and de-scriptions. The crowdsourcing workflow of devel-oping the complete dataset is shown in Figure 2. Crowdsourcing Stories In Sequence We develop a 2-stage crowdsourcing workflow to collect natu-ralistic stories with text aligned to images. The first stage is storytelling , where the crowd worker selects a subset of photos from a given album to form a photo sequence and writes a story about it (see Fig-ure 3). The second stage is re-telling , in which the worker writes a story based on one photo sequence generated by workers in the first stage.

In both stages, all album photos are displayed in the order of the time that the photos were taken, with a  X  X toryboard X  underneath. In storytelling , by clicking a photo in the album, a  X  X tory card X  of the photo appears on the storyboard. The worker is in-structed to pick at least five photos, arrange the or-der of selected photos, and then write a sentence or a phrase on each card to form a story; this appears as a full story underneath the text aligned to each im-age. Additionally, this interface captures the align-ments between text and photos. Workers may skip an album if it does not seem storyable (e.g., a col-lection of coins). Albums skipped by two workers are discarded. The interface of re-telling is simi-lar, but it displays the two photo sequences already created in the first stage, which the worker chooses from to write the story. For each album, 2 work-ers perform storytelling (at $0.3/HIT), and 3 work-ers perform re-telling (at $0.25/HIT), yielding a total of 1,907 workers. All HITs use quality controls to ensure varied text at least 15 words long.
 Crowdsourcing Descriptions of Images In Iso-lation &amp; Images In Sequence We also use crowdsourcing to collect descriptions of images-in-isolation (DII) and descriptions of images-in-sequence (DIS), for the photo sequences with sto-ries from a majority of workers in the first task (as Figure 2). In both DII and DIS tasks, workers are asked to follow the instructions for image caption-ing proposed in MS COCO (Lin et al., 2014) such as describe all the important parts . In DII, we use we use the storyboard and story cards of our story-telling interface to display a photo sequence, with MS COCO instructions adapted for sequences. We recruit 3 workers for DII (at $0.05/HIT) and 3 work-ers for DIS (at $0.07/HIT).
 Data Post-processing We tokenize all sto-rylets and descriptions with the CoreNLP tok-enizer, and replace all people names with generic entities with their entity type (e.g., location ). The data is released as training , validation , and test following an 80%/10%/10% split on the stories-in-sequence albums. Example language from each tier is shown in Figure 4. Our dataset includes 10,117 Flickr albums with 210,819 unique photos. Each album on average has 20.8 photos (  X  = 9.0). The average time span of each album is 7.9 hours (  X  = 11.4). Further details of each
We use normalized pointwise mutual information to identify the words most closely associated with each tier (Table 3). Top words for descriptions-in-isolation reflect an impoverished disambiguat-ing context: References to people often lack so-cial specificity, as people are referred to as simply  X  X an X  or  X  X oman X . Single images often do not convey much information about underlying events or actions, which leads to the abundant use of pos-ture verbs ( X  X tanding X ,  X  X itting X , etc.). As we turn to descriptions-in-sequence, these relatively uninfor-mative words are much less represented. Finally, top story-in-sequence words include more storytelling elements, such as names ( [ male ] ), temporal refer-ences ( today ) and words that are more dynamic and abstract ( went , decided ). Given the nature of the complex storytelling task, the best and most reliable evaluation for assessing the quality of generated stories is human judgment. However, automatic evaluation metrics are useful to quickly benchmark progress. To better understand which metric could serve as a proxy for human eval-uation, we compute pairwise correlation coefficients between automatic metrics and human judgments on 3,000 stories sampled from the SIS training set.
For the human judgements, we again use crowd-sourcing on MTurk, asking five judges per story to rate how strongly they agreed with the statement  X  X f these were my photos, I would like using a story like take the average of the five judgments as the final score for the story. For the automatic metrics, we use and Skip-Thoughts (Kiros et al., 2015) to compute similarity between each story for a given sequence. Skip-thoughts provide a Sentence2Vec embedding which models the semantic space of novels.

As Table 4 shows, METEOR correlates best with human judgment according to all the correlation co-efficients. This signals that a metric such as ME-TEOR which incorporates paraphrasing correlates best with human judgement on this task. A more detailed study of automatic evaluation of stories is an area of interest for a future work. We report baseline experiments on the storytelling task in Table 7, training on the SIS tier and testing on half the SIS validation set (valtest). Example out-put from each system is presented in Table 5. To highlight some differences between story and cap-tion generation, we also train on the DII tier in iso-lation, and produce captions per-image, rather than in sequence. These results are shown in Table 7.
To train the story generation model, we use a sequence-to-sequence recurrent neural net (RNN) approach, which naturally extends the single-image captioning technique of Devlin et al. (2015) and Vinyals et al. (2014) to multiple images. Here, we encode an image sequence by running an RNN over the fc7 vectors of each image, in reverse order. This is used as the initial hidden state to the story decoder model, which learns to produce the story one word at a time using softmax loss over the training data vocabulary. We use Gated Recurrent Units (GRUs) (Cho et al., 2014) for both the image encoder and story decoder.

In the baseline system, we generate the story us-ing a simple beam search (size=10), which has been successful in image captioning previously (Devlin et al., 2015). However, for story generation, the re-sults of this model subjectively appear to be very poor  X  the system produces generic, repetitive, high-level descriptions (e.g.,  X  X his is a picture of a dog X ). This is a predictable result given the label bias prob-lem inherent in maximum likelihood training; recent work has looked at ways to address this issue di-rectly (Li et al., 2016).

To establish a stronger baseline, we explore sev-eral decode-time heuristics to improve the quality of the generated story. The first heuristic is to lower the decoder beam size substantially. We find that using a beam size of 1 (greedy search) significantly increases the story quality, resulting in a 4.6 gain in METEOR score. However, the same effect is not seen for caption generation, with the greedy caption model obtaining worse quality than the beam search model. This highlights a key difference in generat-ing stories versus generating captions.

Although the stories produced using a greedy search result in significant gains, they include many repeated words and phrases, e.g.,  X  X he kids had a great time. And the kids had a great time. X  We intro-duce a very simple heuristic to avoid this, where the same content word cannot be produced more than once within a given story. This improves METEOR by another 2.3 points.

An advantage of comparing captioning to story-telling side-by-side is that the captioning output may be used to help inform the storytelling output. To this end, we include an additional baseline where  X  X isually grounded X  words may only be produced if they are licensed by the caption model. We define the set of visually grounded words to be those which occurred at higher frequency in the caption training than the story training:
We train a separate model using the caption an-notations, and produce an n-best list of captions for each image in the valtest set. Words seen in at least 10 sentences in the 100-best list are marked as  X  X icensed X  by the caption model. Greedy decod-ing without duplication proceeds with the additional constraint that if a word is visually grounded, it can only be generated by the story model if it is licensed by the caption model for the same photo set. This results in a further 1.3 METEOR improvement.
It is interesting to note what a strong effect rel-atively simple heuristics have on the generated sto-ries. We do not intend to suggest that these heuris-tics are the right way to approach story generation. Instead, the main purpose is to provide clear base-lines that demonstrate that story generation has fun-damentally different challenges from caption gener-ation; and the space is wide open to explore for train-ing and decoding methods to generate fluent stories. We have introduced the first dataset for sequen-tial vision-to-language , which incrementally moves from images-in-isolation to stories-in-sequence. We argue that modelling the more figurative and so-cial language captured in this dataset is essential for evolving AI towards more human-like understand-ing. We have established several strong baselines for the task of visual storytelling, and have moti-vated METEOR as an automatic metric to evaluate progress on this task moving forward.
 Beyond understanding simple objects and concrete scenes lies interpreting causal structure; making sense of visual input to tie disparate moments to-gether as they give rise to a cohesive narrative of events through time. This requires moving from rea-soning about single images  X  static moments, de-void of context  X  to sequences of images that depict events as they occur and change. On the vision side, progressing from single images to images in context allows us to begin to create an artificial intelligence (AI) that can reason about a visual moment given what it has already seen. On the language side, pro-gressing from literal description to narrative helps to learn more evaluative, conversational, and abstract language. This is the difference between, for ex-ample,  X  X itting next to each other X  versus  X  X aving a good time X , or  X  X un is setting X  versus  X  X ky illumi-nated with a brilliance... X  (see Figure 1). The first descriptions capture image content that is literal and concrete; the second requires further inference about what a good time may look like, or what is special and worth sharing about a particular sunset.
We introduce the first dataset of sequential im-ages with corresponding descriptions, which cap-tures some of these subtle but important differ-ences, and advance the task of visual storytelling. We release the data in three tiers of language for the same images: (1) Descriptions of images-in-isolation (DII) ; (2) Descriptions of images-in-sequence (DIS) ; and (3) Stories for images-in-sequence (SIS) . This tiered approach reveals the ef-fect of temporal context and the effect of narrative language. As all the tiers are aligned to the same images, the dataset facilitates directly modeling the relationship between literal and more abstract visual concepts, including the relationship between visual imagery and typical event patterns. We additionally propose an automatic evaluation metric which is best correlated with human judgments, and establish sev-eral strong baselines for the visual storytelling task. Work in vision to language has exploded, with re-searchers examining image captioning (Lin et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Xu et al., 2015; Chen et al., 2015; Young et al., 2014; Elliott and Keller, 2013), question an-swering (Antol et al., 2015; Ren et al., 2015; Gao et al., 2015; Malinowski and Fritz, 2014), visual phrases (Sadeghi and Farhadi, 2011), video under-standing (Ramanathan et al., 2013), and visual con-cepts (Krishna et al., 2016; Fang et al., 2015).
Such work focuses on direct, literal description of image content. While this is an encouraging first step in connecting vision and language, it is far from the capabilities needed by intelligent agents for nat-uralistic interactions. There is a significant differ-ence, yet unexplored, between remarking that a vi-sual scene shows  X  X itting in a room X   X  typical of most image captioning work  X  and that the same vi-sual scene shows  X  X onding X . The latter description is grounded in the visual signal, yet it brings to bear information about social relations and emotions that can be additionally inferred in context (Figure 1). Visually-grounded stories facilitate more evaluative and figurative language than has previously been seen in vision-to-language research: If a system can recognize that colleagues look bored , it can remark and act on this information directly.

Storytelling itself is one of the oldest known hu-man activities (Wiessner, 2014), providing a way to educate, preserve culture, instill morals, and share advice; focusing AI research towards this task there-fore has the potential to bring about more human-like intelligence and understanding. Extracting Photos We begin by generating a list of  X  X toryable X  event types. We leverage the idea that  X  X toryable X  events tend to involve some form of pos-session, e.g.,  X  X ohn  X  X  birthday party, X  or  X  X habnam  X  X  visit. X  Using the Flickr data release (Thomee et al., 2015), we aggregate 5-grams of photo titles and de-scriptions, using Stanford CoreNLP (Manning et al., 2014) to extract possessive dependency patterns. We keep the heads of possessive phrases if they can be classified as an EVENT in WordNet3.0, relying on manual winnowing to target our collection efforts. 2 These terms are then used to collect albums using the Flickr API. 3 We only include albums with 10 to 50 photos where all album photos are taken within a 48-hour span and CC-licensed. See Table 1 for the query terms with the most albums returned.

The photos returned from this stage are then pre-sented to crowd workers using Amazon X  X  Mechani-cal Turk to collect the corresponding stories and de-scriptions. The crowdsourcing workflow of devel-oping the complete dataset is shown in Figure 2. Crowdsourcing Stories In Sequence We develop a 2-stage crowdsourcing workflow to collect natu-ralistic stories with text aligned to images. The first stage is storytelling , where the crowd worker selects a subset of photos from a given album to form a photo sequence and writes a story about it (see Fig-ure 3). The second stage is re-telling , in which the worker writes a story based on one photo sequence generated by workers in the first stage.

In both stages, all album photos are displayed in the order of the time that the photos were taken, with a  X  X toryboard X  underneath. In storytelling , by clicking a photo in the album, a  X  X tory card X  of the photo appears on the storyboard. The worker is in-structed to pick at least five photos, arrange the or-der of selected photos, and then write a sentence or a phrase on each card to form a story; this appears as a full story underneath the text aligned to each im-age. Additionally, this interface captures the align-ments between text and photos. Workers may skip an album if it does not seem storyable (e.g., a col-lection of coins). Albums skipped by two workers are discarded. The interface of re-telling is simi-lar, but it displays the two photo sequences already created in the first stage, which the worker chooses from to write the story. For each album, 2 work-ers perform storytelling (at $0.3/HIT), and 3 work-ers perform re-telling (at $0.25/HIT), yielding a total of 1,907 workers. All HITs use quality controls to ensure varied text at least 15 words long.
 Crowdsourcing Descriptions of Images In Iso-lation &amp; Images In Sequence We also use crowdsourcing to collect descriptions of images-in-isolation (DII) and descriptions of images-in-sequence (DIS), for the photo sequences with sto-ries from a majority of workers in the first task (as Figure 2). In both DII and DIS tasks, workers are asked to follow the instructions for image caption-ing proposed in MS COCO (Lin et al., 2014) such as describe all the important parts . In DII, we use the MS COCO image captioning interface. 4 In DIS, we use the storyboard and story cards of our story-telling interface to display a photo sequence, with MS COCO instructions adapted for sequences. We recruit 3 workers for DII (at $0.05/HIT) and 3 work-ers for DIS (at $0.07/HIT).
 Data Post-processing We tokenize all sto-rylets and descriptions with the CoreNLP tok-enizer, and replace all people names with generic MALE / FEMALE tokens, 5 and all identified named entities with their entity type (e.g., location ). The data is released as training , validation , and test following an 80%/10%/10% split on the stories-in-sequence albums. Example language from each tier is shown in Figure 4. Our dataset includes 10,117 Flickr albums with 210,819 unique photos. Each album on average has 20.8 photos (  X  = 9.0). The average time span of each album is 7.9 hours (  X  = 11.4). Further details of each tier of the dataset are shown in Table 2. 6
We use normalized pointwise mutual information to identify the words most closely associated with each tier (Table 3). Top words for descriptions-in-isolation reflect an impoverished disambiguat-ing context: References to people often lack so-cial specificity, as people are referred to as simply  X  X an X  or  X  X oman X . Single images often do not convey much information about underlying events or actions, which leads to the abundant use of pos-ture verbs ( X  X tanding X ,  X  X itting X , etc.). As we turn to descriptions-in-sequence, these relatively uninfor-mative words are much less represented. Finally, top story-in-sequence words include more storytelling elements, such as names ( [ male ] ), temporal refer-ences ( today ) and words that are more dynamic and abstract ( went , decided ). Given the nature of the complex storytelling task, the best and most reliable evaluation for assessing the quality of generated stories is human judgment. However, automatic evaluation metrics are useful to quickly benchmark progress. To better understand which metric could serve as a proxy for human eval-uation, we compute pairwise correlation coefficients between automatic metrics and human judgments on 3,000 stories sampled from the SIS training set.
For the human judgements, we again use crowd-sourcing on MTurk, asking five judges per story to rate how strongly they agreed with the statement  X  X f these were my photos, I would like using a story like this to share my experience with my friends X . 7 We take the average of the five judgments as the final score for the story. For the automatic metrics, we use METEOR, 8 smoothed-BLEU (Lin and Och, 2004), and Skip-Thoughts (Kiros et al., 2015) to compute similarity between each story for a given sequence. Skip-thoughts provide a Sentence2Vec embedding which models the semantic space of novels.

As Table 4 shows, METEOR correlates best with human judgment according to all the correlation co-efficients. This signals that a metric such as ME-TEOR which incorporates paraphrasing correlates best with human judgement on this task. A more detailed study of automatic evaluation of stories is an area of interest for a future work. We report baseline experiments on the storytelling task in Table 7, training on the SIS tier and testing on half the SIS validation set (valtest). Example out-put from each system is presented in Table 5. To highlight some differences between story and cap-tion generation, we also train on the DII tier in iso-lation, and produce captions per-image, rather than in sequence. These results are shown in Table 7.
To train the story generation model, we use a sequence-to-sequence recurrent neural net (RNN) approach, which naturally extends the single-image captioning technique of Devlin et al. (2015) and Vinyals et al. (2014) to multiple images. Here, we encode an image sequence by running an RNN over the fc7 vectors of each image, in reverse order. This is used as the initial hidden state to the story decoder model, which learns to produce the story one word at a time using softmax loss over the training data vocabulary. We use Gated Recurrent Units (GRUs) (Cho et al., 2014) for both the image encoder and story decoder.

In the baseline system, we generate the story us-ing a simple beam search (size=10), which has been successful in image captioning previously (Devlin et al., 2015). However, for story generation, the re-sults of this model subjectively appear to be very poor  X  the system produces generic, repetitive, high-level descriptions (e.g.,  X  X his is a picture of a dog X ). This is a predictable result given the label bias prob-lem inherent in maximum likelihood training; recent work has looked at ways to address this issue di-rectly (Li et al., 2016).

To establish a stronger baseline, we explore sev-eral decode-time heuristics to improve the quality of the generated story. The first heuristic is to lower the decoder beam size substantially. We find that using a beam size of 1 (greedy search) significantly increases the story quality, resulting in a 4.6 gain in METEOR score. However, the same effect is not seen for caption generation, with the greedy caption model obtaining worse quality than the beam search model. This highlights a key difference in generat-ing stories versus generating captions.

Although the stories produced using a greedy search result in significant gains, they include many repeated words and phrases, e.g.,  X  X he kids had a great time. And the kids had a great time. X  We intro-duce a very simple heuristic to avoid this, where the same content word cannot be produced more than once within a given story. This improves METEOR by another 2.3 points.

An advantage of comparing captioning to story-telling side-by-side is that the captioning output may be used to help inform the storytelling output. To this end, we include an additional baseline where  X  X isually grounded X  words may only be produced if they are licensed by the caption model. We define the set of visually grounded words to be those which occurred at higher frequency in the caption training than the story training:
We train a separate model using the caption an-notations, and produce an n-best list of captions for each image in the valtest set. Words seen in at least 10 sentences in the 100-best list are marked as  X  X icensed X  by the caption model. Greedy decod-ing without duplication proceeds with the additional constraint that if a word is visually grounded, it can only be generated by the story model if it is licensed by the caption model for the same photo set. This results in a further 1.3 METEOR improvement.
It is interesting to note what a strong effect rel-atively simple heuristics have on the generated sto-ries. We do not intend to suggest that these heuris-tics are the right way to approach story generation. Instead, the main purpose is to provide clear base-lines that demonstrate that story generation has fun-damentally different challenges from caption gener-ation; and the space is wide open to explore for train-ing and decoding methods to generate fluent stories. We have introduced the first dataset for sequen-tial vision-to-language , which incrementally moves from images-in-isolation to stories-in-sequence. We argue that modelling the more figurative and so-cial language captured in this dataset is essential for evolving AI towards more human-like understand-ing. We have established several strong baselines for the task of visual storytelling, and have moti-vated METEOR as an automatic metric to evaluate progress on this task moving forward.

