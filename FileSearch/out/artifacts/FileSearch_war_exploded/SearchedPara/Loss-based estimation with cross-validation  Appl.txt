 Current statistical inference problems in genomic data anal-ysis involve parameter estimation for high-dimensional mul-tivariate distributions, with typically unknown and intricate correlation patterns among variables. Addressing these in-ference questions satisfactorily requires: (i) an intensive and thorough search of the parameter space to generate good candidate estimators; (ii) an approach for selecting an opti-mal estimator among these candidates; and (iii) a method for reliably assessing the performance of the resulting es-timator. We propose a unified loss-based methodology for estimator construction, selection, and performance assess-ment with cross-validation. In this approach, the parameter of interest is defined as the risk minimizer for a suitable loss function and candidate estimators are generated using this (or possibly another) loss function. Cross-validation is ap-plied to select an optimal estimator among the candidates and to assess the overall performance of the resulting esti-mator. This general estimation framework encompasses a number of problems which have traditionally been treated separately in the statistical literature, including multivari-ate outcome prediction and density estimation based on ei-ther uncensored or censored data. This article provides an overview of the methodology and describes its application to the prediction of biological and clinical outcomes (possibly censored) using microarray gene expression measures. Censored data; classification; comparative genomic hybridiza-tion; cross-validation; density estimation; estimation; loss function; microarray; model selection; multivariate outcome; prediction; regression trees; risk; survival analysis; variable selection.  X 
SD and MvdL contributed equally to this work. Our general estimation methodology was motivated by cur-rent statistical inference problems in the analysis of genomic data, such as: the prediction of biological and clinical out-comes (possibly censored) using microarray gene expression measures, the identification of regulatory motifs (i.e., tran-scription factor binding sites) in DNA sequences, and the genetic mapping of complex traits using single nucleotide polymorphisms (SNP).
 Microarrays are high-throughput biological assays that can be used to measure the abundance of nucleic acids (DNA or RNA) on a genomic scale in different biological samples. In cancer research, for example, interest is in relating microar-ray measures of gene expression to biological and clinical outcomes in order to gain a more thorough understanding of the molecular basis of the disease and eventually develop better diagnosis and treatment strategies. Outcomes (phe-notypes) of interest include tumor class, response to treat-ment, patient survival, and can be either polychotomous or continuous, censored or uncensored. Explanatory variables (genotypes), or features , include measures of transcript (i.e., mRNA) levels or DNA copy number for thousands of genes, treatment, epidemiological and histopathological variables. An important and immediate question is the choice of a good predictor , i.e., a function of the explanatory variables that has low error (i.e., risk) when used to predict the out-come. Should one use linear discriminant analysis, trees, support vector machines (SVMs), neural networks, or some other approach to construct this predictor? Predictor se-lection includes the related problem of variable selection , or feature selection , that is, the identification of a subset of marker genes to be used in the predictor function. Esti-mator selection problems in microarray data analysis follow the so-called  X  X mall n , large p  X  paradigm: thousands of ex-planatory variables are measured for each observational unit (e.g., patient), but the sample sizes available for estimation purposes are comparatively small.
 A dominating feature in the above and other statistical infer-ence problems in genomic data analysis is that they involve parameter estimation for high-dimensional multivariate dis-tributions, with typically unknown and intricate correlation patterns among variables. Accordingly, statistical models for the data generating distribution correspond to large pa-rameter spaces. For instance, for the prediction of clinical outcomes using microarray measures of gene expression, the parameter space may consist of the set of all possible linear combinations of tensor products of univariate polynomial basis functions of the explanatory variables (i.e., thousands of gene expression measures), in order to allow for higher order interactions among these variables. Even if it were possible to minimize a suitable error measure (empirical or cross-validated risk) over the entire parameter space, the resulting estimators would be too variable and ill-defined. Instead, we approximate the parameter space by a sequence of subspaces of increasing dimension and generate candi-date estimators for each subspace. This approach therefore requires: (i) an intensive and thorough search of the pa-rameter space to generate good candidate estimators; (ii) a procedure for selecting an optimal estimator among these candidates; and (iii) a method for reliably assessing the per-formance of the resulting estimator. Parameter estimation problems can be formulated generally and abstractly as follows. The data consist of realizations of random variables, X 1 ,...,X n , from an unknown data gen-erating distribution , F X, 0 . The goal is to use these data to estimate a parameter  X  0 of the distribution F X, 0 , where  X  is defined as some function of F X, 0 . That is, we wish to ob-tain an estimator , or function of the data,  X   X  , that is close (in risk distance) to the parameter  X  0 . For example, in cancer microarray studies, X i could consist of a pair X i = ( W measured on a patient i , where W i is a d = 5000-dimensional vector of microarray measures and Z i is a possibly censored survival time. The parameter of interest  X  0 could corre-spond to the d  X  d correlation matrix for the gene expression vector W or to the conditional expected value of the survival time Z given the gene expression vector W .
 Our general strategy for data-adaptive estimation is driven by the choice of a loss function and relies on cross-validation for estimator selection and performance assessment. Our proposed estimation road map, which covers censored data situations, can be stated in terms of the following three main steps. Section 2 elaborates on each of these steps. 1. Definition of the parameter of interest in terms of a 2. Construction of candidate estimators based on a loss 3. Cross-validation for estimator selection and performance Note that we use the term estimation in a broad sense, to provide a unified treatment of multivariate prediction and density estimation based on either uncensored or censored data. Each of these problems can be dealt with according to the road map by the choice of a suitable loss function. A number of common estimation procedures follow the road map in the full data situation, but depart from it when faced with the obstacle of evaluating the loss function in the pres-ence of censoring (e.g., classification and regression trees, where candidates in Step 2 are obtained by recursive binary partitioning of the covariate space). Here, we argue that one can, and should, also adhere to the above estimation road map in censored data situations. All that is required is to replace the full (uncensored) data loss function by an ob-served (censored) data loss function with the same expected value, i.e., with the same risk.
 We note also that existing methods for Step 2 are not ag-gressive enough for the types of datasets encountered in ge-nomics. In order to account for higher-order interactions among many variables (e.g., thousands of gene expression measures in microarray experiments), one needs to consider large parameter spaces. However, standard approaches ei-ther only accommodate variable main effects or are too rigid to generate a good set of candidate estimators. For exam-ple, while regression trees allow interactions among vari-ables, the candidate tree estimators are generated accord-ing to a limited set of moves, amounting to forward selec-tion (node splitting) followed by backward elimination (tree pruning). Instead, we recommend more aggressive and flex-ible algorithms, such as the D/S/A algorithm of (Molinaro and van der Laan, 2003; Sinisi and van der Laan, 2003), that at each step allow not only node splitting, but also node collapsing and substitutions (Section 2.3). The present article provides an overview of our general data-adaptive loss-based estimation methodology with cross-validation and describes applications to the analysis of genomic data. Section 2 discusses the main features of our estimation road map, including the choice of a loss function (Step 1), the new D/S/A algorithm for generating candidate estimators (Step 2), cross-validation estimator selection and performance as-sessment (Step 3), and a novel loss-based approach for mea-suring variable importance. Section 3 describes applications of the methodology to the prediction of biological and clini-cal outcomes (possibly censored) using microarray gene ex-pression measures.
 Our general framework and its theoretical foundations are established in van der Laan and Dudoit (2003). This earlier manuscript proposes a unified cross-validation methodology for estimator construction, selection and performance as-sessment, and in particular provides finite sample results and asymptotic optimality results concerning cross-validation es-timator selection for general data generating distributions, loss functions (possibly depending on a nuisance parame-ter), and estimators. These new theoretical results have the important practical implication that cross-validation se-lection can be used in intensive searches of large parame-ter spaces, even in finite sample situations. Special cases and applications are described in a collection of related arti-cles: estimator selection and performance assessment based on uncensored data (Dudoit and van der Laan, 2003), es-timator selection with censored data (Kele  X s et al., 2003a), likelihood-based cross-validation (van der Laan et al., 2003), tree-based estimation with censored data (Molinaro et al., 2004), D/S/A algorithm for generating candidate estimators (Molinaro and van der Laan, 2003; Sinisi and van der Laan, 2003), supervised detection of regulatory motifs in DNA se-quences (Kele  X s et al., 2003b). In many applications of interest, the full data structure will simply consist of a pair, X = ( W,Z ), where W = ( W 1 ,...,W is a d -vector of explanatory variables (e.g., microarray ex-pression measures for thousands of genes) and Z is a scalar outcome (e.g., survival time, tumor class, quantitative phe-notype). However, to cover general estimation problems, we define the full data structure as a stochastic process, denotes either a fixed endpoint or a random survival time, R ( t )  X  I( T  X  t ), and L ( t ) is a covariate process. Denote the distribution of the full data structure X by F X, 0 , where F
X, 0 is assumed to belong to a certain model M F , possibly non-parametric and very large. The covariate process L ( t ) may contain time-dependent and time-independent covari-ates. Denote the time-independent covariates by W = L (0), a d -dimensional vector, measured at baseline. For random T , let Z = log T denote the log survival time. For fixed T , one may also be interested in monitoring Z ( t ), t  X  { t 0 ,...,t m  X  1 = T } , an m -dimensional outcome process in-cluded in X ( t ), such as T-cell counts at different timepoints. In the observed data world, one rarely sees all of the relevant variables in the process X =  X  X ( T ) = { X ( t ) : 0  X  t  X  T } . Rather, one observes the full data process X ( t ) up to the minimum,  X  T  X  min( T,C ), of the survival time T and a univariate censoring variable C . In a clinical setting, this missing, or censored, survival data situation can be due to drop-out or the end of follow-up. The observed data struc-ture can be written as O  X  (  X  T,  X  ,  X  X (  X  T )), where  X  is the censoring indicator ,  X   X  I( T  X  C ), equal to one for uncen-sored observations and to zero for censored observations. The random variable O for the observed data has a distri-bution P 0 = P F X, 0 ,G 0 , indexed by the full data distribution, F
X, 0 , and the conditional distribution, G 0 (  X | X ), of the cen-soring time C given full data X . The survivor function for the censoring mechanism is denoted by  X  G 0 ( c | X )  X  Pr 0 ( C &gt; c | X ) and referred to as censoring survivor func-tion . We make the standard coarsening at random (CAR) assumption for the censoring mechanism. If X = ( W,Z ), that is, X does not include time-dependent covariates, then, under CAR, the censoring time C is conditionally indepen-dent of the survival time T given baseline covariates W . Thus,  X  G 0 ( c | X ) =  X  G 0 ( c | W ) and the censoring survivor function only depends on the observed baseline covariates W . Gill et al. (1997), van der Laan and Robins (2002) (Sec-tion 1.2.3, in particular), and Robins and Rotnitzky (1992) provide further, thorough explanations of CAR. Parameters. For distributions F X  X  M F , define parame-ters  X   X   X ( F X ) in terms of a mapping,  X  : M F  X  D ( S ), from the model M F into a space D ( S ), where elements of D ( S ) are functions from a Euclidean space S into the real line IR . Thus, a parameter  X  =  X ( F X ) is itself a mapping  X  : S  X  IR . Let  X   X {  X  =  X ( F X ) : F X  X  X  F } denote the corresponding parameter space . Note that the use of upper case  X  and lower case  X  allows us to distinguish between two types of mappings: the mapping  X  : M F  X  X  ( S ), that defines a parameter  X  =  X ( F X ) for a particular distribu-tion F X , and the mapping (i.e., realization)  X  : S  X  IR , corresponding to this parameter. For example, for a full data structure X = ( W,Z ), the space S is typically a sub-set of IR d , corresponding to the explanatory variables W , and the parameter for a distribution F X could be defined as the conditional expected value of the response Z given W ,  X  ( W ) =  X ( F X )( W ) = E F X [ Z | W ].
 Estimators. Assume that we have a sample, or learning set , of n independent and identically distributed (i.i.d.) ob-servations, X 1 ,...,X n , from the distribution F X, 0  X  M Let P n denote the empirical distribution of X 1 ,...,X n P n places probability 1 /n on each realization X i . Our goal is to use the sample to estimate the parameter  X  0  X   X ( F X, 0 corresponding to the unknown data generating distribution F
X, 0 . An estimator  X   X   X   X   X ( P n ) is simply a function of the empirical distribution P n , that is, an algorithm one can ap-ply to the data X 1 ,...,X n .
 Loss functions and risk. We define a full data loss func-tion , L ( X, X  ), such that its expected value, or risk , under F
X, 0 is minimized at the parameter  X  0 . That is,  X  0 is such that To simplify notation, we may use the subscript 0 to refer to parameters of the underlying data generating distributions F X, 0 (and G 0 in censored data situations), that is, write E
X, 0 [ L ( X, X  )] = E 0 [ L ( X, X  )]. Note that we do not require uniqueness of the risk minimizer, rather, we simply assume that there is a loss function whose risk is minimized by the parameter of interest  X  0 . In addition, depending on the pa-rameter of interest, there could be numerous loss functions from which to choose and one should adopt the loss function that corresponds to the desired measure of performance for the estimation of  X  0 . Loss functions for common estimation problems are listed in Table 3. For instance, regression prob-lems typically involve minimizing risk for the squared error loss function (a.k.a. mean squared error), while classification often involves minimizing risk for the indicator loss function (a.k.a. classification error), and density estimation is con-cerned with minimizing risk for the negative log-likelihood loss function (a.k.a. entropy).
 Risk estimation. Since the data generating distribution F
X, 0 is typically unknown, one cannot directly minimize risk as in equation (1). That is, the conditional risk , for an estimator  X   X  =  X   X ( P n ), is typically unknown and needs to be estimated from the data, i.e., using the empirical dis-tribution P n . A naive risk estimator is the empirical risk , or resubstitution estimator , where the unknown F X, 0 is simply replaced by the empirical P n However, it is well-known that estimator construction and selection methods aimed at optimizing the empirical risk do not produce estimators that minimize the true unknown risk and often suffer from over-fitting, i.e., are too data-adaptive. Instead, we turn to cross-validation to provide consistent risk estimators for use in estimator selection and performance assessment. As detailed in Section 2.4, the cross-validated risk estimator is obtained by constructing estimators  X   X ( P 0 n,S n ) on training sets of size n (1  X  p estimating risk based on empirical distributions P 1 n,S n validation sets of size np n (in place of the unknown F X, 0 in equation (2)) In this representation, E S n indicates averaging of the vali-dation set risks over the different splits of the learning set into training and validation sets. Thus, the risk definitions in equations (2), (3), and (4), differ in the choice of distribu-tions for constructing the estimator (empirical distributions P n or P 0 n,S n ) and for evaluating the loss function ( F Example 1. Prediction. In univariate (in the outcome) prediction problems, the full data structure is X = ( W,Z )  X  F
X, 0  X  X  F , where W is a d -dimensional vector of explana-tory variables and Z a scalar outcome. For continuous out-comes Z , the parameter of interest is typically the condi-tional expected value ,  X  0 ( W ) =  X ( F X, 0 )( W ) = E F W ], of the outcome given the explanatory variables. The parameter space is  X  = {  X ( F X ) :  X ( F X )( W ) = E F X W ] , F X  X  M F } and the loss function is the squared error loss function , L ( X, X  ) = ( Z  X   X  ( W )) 2 . The familiar ordi-nary least squares (OLS) regression approach corresponds to minimizing the empirical risk for the squared error loss function over linear combinations of individual explanatory variables.
 Example 2. Density estimation. Likewise, the widely used maximum likelihood estimation (MLE) framework in-volves minimizing the empirical risk for the negative log-likelihood loss function over densities  X   X   X  = {  X   X  :  X   X   X  } , indexed by a parameter  X   X   X . For example,  X   X  could rep-resent the density for the multivariate normal distribution N (  X ,I d ), with mean vector  X  and d  X  d identity covariance matrix I d . For full data structure X  X   X   X  0 and loss function L ( X, X  ) =  X  log  X  ( X ), the parameter of interest is (For simplicity, we define densities  X  with respect to the Lebesgue measure. However, one could define densities more generally, with respect to some dominating measure  X  , to accommodate discrete distributions as well.) The MLE then minimizes the negative log-likelihood function In the observed (censored) data world, we have a learning set of n i.i.d. observations, O 1 ,...,O n , from the right-censored data structure, O i  X  P 0 = P F X, 0 ,G 0 . Let P n denote the empirical distribution of O 1 ,...,O n , where P n places prob-ability 1 /n on each realization O i . The goal remains to find an estimator for a parameter  X  0 defined in terms of the risk for a full data loss function L ( X, X  ), e.g., a predictor of the log survival time Z based on covariates W . An immedi-ate problem is that loss functions such as the quadratic loss, L ( X, X  ) = ( Z  X   X  ( W )) 2 , cannot be evaluated for an observa-tion O with censored survival time, i.e., for which Z = log T is not observed ( X  = 0). Risk estimators based on only uncensored observations, such as 1 n P i L ( X i , X  ) X  i , are bi-ased for E 0 [ L ( X, X  )] and, in particular, estimate instead the parameter of interest  X  0 .
 The general estimating function methodology of van der Laan and Robins (2002) can be used to link the observed data world to the full data world. The general solution is to replace the full (uncensored) data loss function , L ( X, X  ), by an observed (censored) data loss function , L ( O, X  |  X  with the same expected value, i.e., with the same risk , As detailed below,  X  0 denotes a nuisance parameter for the data generating distribution P 0 = P F X, 0 ,G 0 . A simple ob-served data loss function is the inverse probability of censor-ing weighted (IPCW) loss function where  X  = I( T  X  C ) is the censoring indicator and  X  0 =  X ( P 0 ) denotes the nuisance parameter corresponding to the censoring survivor function,  X  G 0 , for the censoring time C given full data X . Under the coarsening at random (CAR) assumption,  X  G 0 ( T | X ) =  X  G 0 ( T | W ) only depends on the ob-served data and can be estimated, for example, using the Cox proportional hazards model. For an uncensored ob-servation ( X  = 1), the IPCW observed data loss function is simply the full data loss function weighted by the in-verse of the probability  X  G 0 ( T | W ) of no censoring before T ; for a censored observation ( X  = 0), the loss function is zero. We stress that in the absence of censoring, i.e., when  X  G ( t | w )  X  1  X  t , the IPCW observed data loss function re-duces to the full data loss function, L ( O, X  |  X  0 ) = L ( X, X  ). This ensures that the censored and full data estimators co-incide when there is no censoring.
 Example 1. Prediction. In the case of the squared error loss function used in regression, the empirical risk based on the IPCW loss function becomes a weighted mean squared error (MSE) Z where  X   X  n =  X   X ( P n ) represents  X  G n , an estimator of the nui-sance parameter  X  G 0 derived under the CAR assumption for the censoring mechanism.
 Example 2. Density estimation. Similarly, for the neg-ative log-likelihood loss function used in density estimation, the empirical risk based on the IPCW loss function is The ability to map from a full data loss function into an observed data loss function with the same risk offers sev-eral important practical advantages. Firstly, this allows us to directly extend full data estimation methodology to cen-sored data situations. This in contrast to common censored data estimation approaches, such as survival trees, which by-pass the risk estimation problem for censored outcomes by altering the node splitting, tree pruning, and performance assessment criteria in manners that are specific to censored survival data (Molinaro et al., 2004). In general, the split-ting and pruning criteria seem to be chosen based on conve-nience for handling censored data and do not reduce to the preferred choice for uncensored data. Most tree-based re-gression and density estimation procedures rely on the nega-tive log-likelihood loss function, with the explicit or implicit goal of estimating the conditional survivor function given explanatory variables, and differ mainly in their choice of model for the observed data likelihood within nodes. This general difficulty in evaluating risk for censored observations results in a discontinuity between the full and observed data worlds. Secondly, as shown in Molinaro et al. (2004), gains in accuracy can be achieved by employing a loss function that is specific to the parameter of interest (e.g., by using the squared error loss function for regression rather than the negative log-likelihood loss function typically used in survival trees). Finally, the IPCW estimating function ap-proach allows us to assess performance on censored data for arbitrary loss functions. Current methods typically rely on the negative log-likelihood loss function or lead to biased risk estimators by ignoring censored observations altogether. Having defined the parameter of interest in Step 1, as the risk minimizer for a particular loss function, Step 2 of the road map is concerned with generating a sequence of can-didate estimators by minimizing the empirical risk (for the same or possibly another loss function as in Step 1) over sub-spaces of increasing dimension approximating the complete parameter space. In general, it is not feasible to consider all possible elements of the subspaces and one needs an efficient search algorithm for optimizing risk over these subspaces. Tree-structured estimators, such as CART (Breiman et al., 1984), correspond to one such procedure, whereby candidate estimators are obtained by recursive binary partitioning of the covariate space. However, as discussed in Molinaro et al. (2004) and Molinaro and van der Laan (2003), trees do not provide an exhaustive enough search of the subspaces: the candidate estimators are generated according to a limited set of moves, amounting to forward selection (node splitting) followed by backward elimination (tree pruning). Instead, we favor more aggressive and flexible algorithms, such as the D/S/A algorithm of Molinaro and van der Laan (2003) and Sinisi and van der Laan (2003), that at each step al-low not only node splitting, but also node collapsing and substitutions. Consider a full data structure of the form X = ( W,Z ), where W is a d -vector of explanatory variables and Z is a possibly multivariate outcome. Define a countable set of basis func-tions , {  X  j : j  X  IN } , indexed by the non-negative integers IN , such that every parameter  X   X   X  can be arbitrarily well approximated by finite linear combinations of these basis functions (or some known function of linear combinations of basis functions, such as the logit function in binary classifi-cation or the exponential function in the Cox proportional hazards model). That is, define regression functions ,  X  I, X  as where I  X  I denotes a countable index set and I is a col-lection of subsets of IN . For a given index set I  X  I , the regression coefficients  X  = (  X  1 ,..., X  | I | ) are assumed to be-long to B I  X {  X  :  X  I, X   X   X  } X  IR | I | .
 The choice of basis functions {  X  j : j  X  IN } depends on the estimation approach. In polynomial regression, the  X  j are polynomial functions of the explanatory variables (Sinisi and van der Laan, 2003). In histogram regression (e.g., regres-sion trees), for a given index set I  X  I , the {  X  j : j  X  I } are indicators for sets { S j : j  X  I } that form a partition of the covariate space (Molinaro et al., 2004; Molinaro and van der Laan, 2003). The basis functions  X  j may themselves be defined as tensor products of univariate basis functions , e ,e 1 ,e 2 ,... , such as polynomial powers (e.g., e 0 ( x ) = 1, e ( x ) = x , e 2 ( x ) = x 2 ,...), spline basis functions, or wavelets basis functions. Given a d -vector ~p = ( p 1 ,...,p d )  X  IN  X  ( W ) = e p 1 ( W 1 )  X  ...  X  e p d ( W d ) denote the tensor product of univariate basis functions identified by ~p . For instance, for polynomial basis functions, the multivariate basis func-tions are  X  ~p ( W ) = W p 1 1 ...W p d d .
 The collection of basis functions {  X  j : j  X  IN } (or {  X  IN d } , in the tensor product representation above), provides a basis for the complete parameter space  X  , which can be represented by One can then define a sieve , {  X  k } , of subspaces  X  k  X   X  , of increasing dimension approximating the complete parameter space  X  . For example, Our approach is to seek, for each index set size k , the esti-mator that minimizes the empirical risk over the subspace  X  k . We tackle this risk optimization problem in two steps: optimization over regression coefficients  X   X  B I for a given index set I (e.g., least squares estimation as in Section for the squared error loss) and optimization over index sets I (e.g., D/S/A algorithm in Section 2.3.3 , below). One can further reduce the number of candidates  X  I, X  in  X  k by imposing constraints on the basis functions  X  j or regres-sion coefficients  X  j . For instance, in polynomial regression, one can enforce constraints on the degree of the polynomial bases, such as: P d j =1 I( p j 6 = 0)  X  k 0 or P d j =1 p particular restrictions can be chosen by cross-validation. Given index sets I  X  X  , define I -specific subspaces For each subspace  X  I , the regression coefficients  X  are esti-mated by minimizing the empirical risk, that is, where  X   X  n =  X   X ( P n ) is an estimator of the nuisance param-eter  X  0 =  X ( P 0 ) for the observed data loss function (Sec-tion 2.2.2 ). Denote the resulting I -specific estimators by  X   X  In the special case of the squared error loss function, with full data,  X   X  I is simply the least squares linear regression estimator corresponding with the variables identified by the index set I . That is, We propose a new algorithm for minimizing risk over sub-sets of basis functions, i.e., over index sets I , according to three types of moves for the elements of I : deletions, sub-stitutions, and additions. We refer to this algorithm as the Deletion/Substitution/Addition algorithm , or D/S/A algo-rithm . The main features of this novel approach are summa-rized below for tensor product basis functions  X  j (e.g., tensor products of univariate polynomial basis functions in polyno-mial regression). The reader is referred to Sinisi and van der Laan (2003) for a more complete discussion and simulation studies assessing the performance of this general search pro-cedure. Adaptations to histogram regression, with indicator basis functions, are discussed in Molinaro and van der Laan (2003).
 The D/S/A algorithm for minimizing risk over index sets I is defined in terms of three functions, DEL ( I ), SUB ( I ), and ADD ( I ), which map an index set I  X  I of size k into sets of index sets of size k  X  1, k , and k +1, respectively (Box 1). That is, for deletion moves, DEL : I  X  I  X  DEL ( I )  X  I , with | I  X  | = | I | X  1 for I  X   X  DEL ( I ).
 Box 1. Deletion/Substitution/Addition moves.

Consider index sets I  X  IN d and let I denote a collection of subsets of IN d .

Deletion moves. Given an index set I  X  I of size k = | I | , define a set DEL ( I )  X  I of index sets of size k  X  1, by deleting individual elements of I . This results in k possible deletion moves, i.e., | DEL ( I ) | = k .
Substitution moves. Given an index set I  X  I of size k = | I | , define a set SUB ( I )  X  I of index sets of size k , by replacing individual elements ~p  X  I by one of the 2 d vectors created by adding or subtracting 1 to any of the d components of ~p . That is, for each ~p  X  I , consider moves ~p  X  ~u j , where ~u j denotes the unit d -vector with one in position j and zero elsewhere, j = 1 ,...,d . This results in up to k  X  (2 d ) possible substitution moves, i.e., | SUB ( I ) | = k  X  (2 d ).

Addition moves. Given an index set I  X  X  of size k = | I | , define a set ADD ( I )  X  I of index sets of size k + 1, by adding to I an element of SUB ( I ) or one of the d unit vectors ~u j , j = 1 ,...,d . This results in up to k  X  (2 d ) + d possible addition moves, i.e., | ADD ( I ) | = k  X  (2 d ) + d . Next, we describe how the three basic moves of the D/S/A algorithm can be used to generate index sets I k ( P n ), that seek to minimize the empirical risk function, f E ( I ), over all index sets I of size less than or equal to k , k = 1 ,...,m (Box 2). For an index set I  X  X  , the empirical risk of the I -specific estimator  X   X  I =  X   X  I ( P n ) (as defined in Section is where  X   X  I =  X   X  I ( P n ) and  X   X  n =  X   X ( P n ) are estimators based on the empirical distribution P n for the entire learning set . In the special case of the squared error loss function, with full data, the empirical risk function is simply the mean squared error (cf. residual sum of squares) for  X   X  I : f empirical risk) index set I of size less than or equal to k , k = 1 ,...,m , by The D/S/A algorithm in Box 2 returns for each k , an index set I k ( P n ) that approximates I ? k ( P n ). Denote the resulting then be used to select the optimal index set size k , as detailed in Section 2.4, k ( P n )  X  argmin on the empirical distributions P 0 n,S n for the training sets only . Denote the final estimator corresponding to the index set size  X  k = k ( P n ) by  X   X  =  X   X ( P n )  X   X   X  k ( P n
Box 2. Deletion/Substitution/Addition algorithm for optimizing the empirical risk function. The D/S/A algorithm is such that BEST ( k ) is decreasing in k , since addition moves only occur when they result in a decrease in risk over the current index set size. Thus, the best subset of size k is also the best subset of size less than or equal to k .
 Each step of the D/S/A algorithm is linear in the dimension d of the covariate space and in the current size k of the index set. An interesting open question is the number of iterations for substitution moves.
 Finally, note that the D/S/A algorithm for generating candi-date estimators is completely defined by the following choices: the loss function, the basis functions  X  j defining the param-eterization  X   X ,I of the parameter space, and the sets of dele-tion, substitution, and addition moves. Consequently, the D/S/A algorithm can be adapted straightforwardly to ad-dress a broad range of estimation problems, with different objective functions.
 Example 1. Prediction: D/S/A moves with poly-nomial basis functions. Suppose we wish to predict a continuous outcome Z based on a 4-dimensional vector of explanatory variables, W = ( W 1 ,W 2 ,W 3 ,W 4 ), d = 4. We illustrate the three main moves in the D/S/A algorithm, with basis functions defined as tensor products of univariate polynomial basis functions,  X  ~p ( W ) = e p 1 ( W 1 )  X  e e 3 ( W 3 )  X  e p 4 ( W 4 ) = W IN I are subsets of IN 4 .
 Suppose the current index set in the D/S/A algorithm is I k = | I 0 | = 2. This index set corresponds to basis functions  X  1 ( W ) = W 1 W 2 W 3 and  X  ~p 2 ( W ) = W 2 W set, DEL ( I 0 ), contains two index sets of size k = 1 The substitutions set, SUB ( I 0 ), contains thirteen index sets, I = = { ~p  X  1 ,~p  X  2 } , of size k = 2, where either ~p  X  (here, the cardinality of SUB ( I 0 ) is less than k  X  (2 d ) = 16, because moves that result in negative powers are not al-lowed, e.g., ~p 2  X  ~u 1 ).
 The additions set, ADD ( I 0 ), contains 13+4 = 17 index sets, I thirteen new 4-vectors introduced in the above substitutions set SUB ( I 0 ) or one of the four unit vectors ~u j , j = 1 ,..., 4. For instance, one of the seventeen index sets I +  X  ADD ( I is I + = { ~p 1 = (1 , 1 , 1 , 0) ,~p 2 = (0 , 1 , 0 , 5) ,~p (0 , 1 , 1 , 5) } . This set corresponds to the three basis functions  X  1 ( W ) = W 1 W 2 W 3 ,  X  ~p 2 ( W ) = W 2 W W 2 W 3 W 5 4 , and hence, to the following candidate predictor for the outcome Z :  X  I + , X  ( W ) =  X  1 W 1 W 2 W 3 +  X  2  X  Search procedures, such as the D/S/A algorithm for opti-mizing the empirical risk function f E ( I ) over same size in-dex sets I , can be used to construct a sequence of candidate estimators,  X   X  k =  X   X  k ( P n )  X   X  , k  X  { 1 ,...,K n } , for the parameter  X  0 . The next step is to select an optimal esti-mator among these candidates. Specifically, the estimator selection problem involves choosing a data-adaptive selec-tor  X  k = k ( P n ), so that the risk distance , or risk difference , d (  X   X   X  k , X  0 ), between the estimator  X   X   X  k and the parameter  X  converges to zero at an asymptotically optimal rate. Here, d Ideally, one would like to choose  X  k as the optimal benchmark selector ,  X  k n , which minimizes the risk distance d n that is, where  X   X  n ( k ) is the conditional risk for the candidate estima-tor  X   X  k =  X   X  k ( P n ) However, the risk distance d n (  X   X  k , X  0 ), and hence the optimal benchmark selector  X  k n , depend on the unknown data gener-ating distribution P 0 . Thus, in practice, the selection prob-lem involves estimating the conditional risk  X   X  n ( k ) for each candidate estimator  X   X  k  X   X  , k  X  { 1 ,...,K n } , and seeking k that minimizes this risk estimator. Cross-validation is a general approach for risk estimation and estimator selection. Example 1. Prediction. For the squared error loss func-tion, L ( X, X  ) = ( Z  X   X  ( W )) 2 , the risk difference simplifies to that is, a squared bias term for  X   X  k as an estimator of  X  Example 2. Density estimation. For the negative log-likelihood loss function, the risk difference is the Kullback-Leibler divergence , or relative entropy , between densities and  X  0 The main idea in cross-validation (CV) is to divide the avail-able learning set into two sets: a training set and a validation set . Observations in the training set are used to compute (or train ) the estimator(s) and the validation set is used to assess the performance of (or validate ) this estimator(s) in terms of a loss function.
 To derive a general representation for cross-validation, we introduce a binary random n -vector, or split vector , S n { 0 , 1 } n , independent of the empirical distribution P alization of S n = ( S n, 1 ,...,S n,n ) defines a particular split of the learning set of n observations into a training set and a validation set, where S n,i = 0 indicates that the i th ob-servation is in the training set and S n,i = 1 indicates that it is in the validation set. The particular distribution of the split vector S n defines the type of cross-validation pro-cedure. This representation covers many types of CV pro-cedures, including leave-one-out cross-validation (LOOCV), V -fold cross-validation, Monte-Carlo cross-validation, and bootstrap-based cross-validation (van der Laan and Dudoit, 2003). Let P 0 n,S n and P 1 n,S n denote the empirical distribu-tions of the training and validation sets, respectively, and let p = p n = n 1 /n be the proportion of observations in the validation set, where n 1 = P n i =1 I( S n,i = 1). Given a candidate estimator  X   X  k =  X   X  k ( P n ), the cross-validation risk estimator of the conditional risk  X   X  n ( k ), using the full data loss function, is The cross-validation selector  X  k = k ( P n ) is chosen so that, among all K n candidate estimators,  X   X   X  k has the best perfor-mance on the validation sets A selector  X  k = k ( P n ) is said to be asymptotically equivalent with the optimal benchmark  X  k n if the ratio of risk distances In particular, then  X  k is asymptotically optimal . van der Laan and Dudoit (2003) derive finite sample and asymptotic optimality results concerning the cross-validation selector for general data generating distributions, loss func-tions (possibly depending on a nuisance parameter,  X  0 , as in the IPCW loss function), and estimators. The asymptotic optimality result states that, under mild regularity condi-tions, the cross-validation selector,  X  k , performs asymptoti-cally as well as the optimal benchmark selector,  X  k n , based on the unknown data generating distribution P 0 (see van der Laan and Dudoit (2003) for full statements and proofs of the results). These new theoretical results have the important practical implication that, even finite sample situations, one can use cross-validation to engage in an intensive search of a large parameter space. It is important to note that risk estimators from cross-validation relate only to the aspects of estimation that were cross-validated. Hence, it is essential to perform cross-validation on the entire estimation, or training, process, including fea-ture selection and other choices, such as the number of neigh-bors k in nearest neighbor classification ( k -NN) and the ker-nel in support vector machines (SVM). Otherwise, risk es-timators can be severely biased downward, i.e., overly op-timistic. Cross-validation has been widely-used in genomic data analysis, to compare estimators and for overall esti-mator performance assessment. For instance, in cancer mi-croarray studies, estimates of classification error (i.e., risk for the indicator loss function) are often reported to sup-port statements such as  X  X linical outcome X for cancer Y can be predicted accurately based on microarray gene expres-sion measures. X  However, it is common practice in these studies to screen genes and fine-tune predictor parameters (e.g., number of neighbors in k -NN, kernel in SVMs) us-ing the entire learning set and then perform cross-validation only on the final portion of the predictor building process. The resulting error rates are therefore biased downward and give an overly optimistic view of the predictive power of microarray expression measures. Suppose that an estimator  X   X  =  X   X ( P n ), of the parameter  X  has been selected as described above by cross-validation on a learning set of n observations. The overall performance of this  X  X inal X  estimator now needs to be assessed based on an independent test set . A double , or nested , cross-validation study can be performed, in which the learning set is obtained from a partition of a complete dataset of n ? observations into a learning set and a test set. Let P n ? denote the empirical distribution of the complete dataset of n ? observations. The CV risk estimator of the overall performance of the selected estimator is given by where S ? n refers to binary split vectors for the entire dataset distribution P n of a learning set of n observations. Note that the entire estimation procedure (i.e., all three steps in the road map) is now applied to each learning set, i.e., each P The risk estimators from cross-validation are statistics , i.e., they are functions of the empirical distribution P n , and thus vary from sample to sample. It is therefore natural to study the sampling distribution of these statistics and derive con-fidence intervals for the risk they are estimating. Dudoit and van der Laan (2003) prove that cross-validated risk es-timators are consistent and asymptotically linear for the risk based on the true underlying distribution, and use these re-sults to derive confidence intervals for the unknown risk. An approximate asymptotic (1  X   X  )100% confidence interval for the conditional risk  X   X  n , defined in equation (2), is given by where  X   X  2 n  X  R ( IC ( x | P n )) 2 dP n ( x ), IC ( x | P R
L ( x,  X   X ( P n )) dP n ( x ), and  X ( z  X / 2 ) = 1  X   X / 2 for the stan-dard normal cumulative distribution function  X (  X  ). A common and practical question in prediction problems is to assess the importance of a variable (or set of variables) in terms of its predictive ability for an outcome of interest. For instance, in microarray experiments, one is interested in determining how important each gene (or set of genes) is for the prediction of a particular biological or clinical outcome. Measures of variable importance can then assist in the iden-tification of a subset of marker genes for the outcome. We propose to define variable importance in terms of a loss function (Teng et al., 2003). Consider a full data struc-ture X = ( W,Z )  X  F X , where W is a d -dimensional vector of explanatory variables and Z is an outcome of interest. Let J  X  { 1 ,...,d } and  X  J refer to a subset of explana-tory variables and its complement, respectively. Denote the reduced data structure, based on only the explanatory variables indexed by J , using X ( J ) = ( W ( J ) ,Z ), where W ( J ) = ( W j : j  X  J ), and the corresponding distribution by F X ( J ) . Consider J -specific parameter spaces,  X  ( J )  X  {  X  J ( F X ) =  X ( F X ( J ) ) : F X  X  M F } , where given a subset J  X  { 1 ,...,d } and distribution F X  X  M F , the parame-ters  X  J ( F X ) =  X ( F X ( J ) ) are well-defined analogs of the full parameter  X ( F X ). For instance, one can extend the | J | -dimensional distributions F X ( J ) to d -dimensional distri-butions, that are degenerate for W (  X  J ), i.e., assign mass one to some constant value for W (  X  J ). When such extended dis-tributions belong to the model M F , then  X  ( J )  X   X  . For example, in regression problems, the parameters of interest are conditional expected values of the outcome given sets of Given a loss function L ( X, X  ), such as the squared error loss, L ( X, X  ) = ( Z  X   X  ( W )) 2 , one can then define the J -specific parameters ,  X  J 0 =  X ( F X ( J ) , 0 ), as the risk minimizers over  X  ( J ), that is, In particular, the full parameter  X  0 =  X ( F X, 0 ) corresponds to J = { 1 ,...,d } .
 The variable importance parameter ,  X  J 0 =  X  J ( F X, 0 ), for the set of variables indexed by J , can now be defined as the difference between the risk for the  X  J -specific parameter  X  defined without the variables indexed by J , and the risk for the parameter  X  0 , based on all d explanatory variables. That is, Note that in most applications,  X  (  X  J )  X   X  , so that the im-portance parameters  X  J 0 are non-negative. Thus  X  J 0 mea-sures the increase in risk (error) resulting from omitting explanatory variables W ( J ) = ( W j : j  X  J ) from the es-timation process. We stress that this general definition of variable importance applies to sets of variables and there-fore allows examination of not only variable main effects (i.e., individual j  X  { 1 ,...,d } ), but also higher order in-teractions among variables. In particular, in the context of microarray experiments, this general definition can be used to assess the importance of gene clusters in terms of their predictive power for an outcome of interest. In addition, in high-dimensional problems, one could consider variable importance measures for orthogonal transformations of the explanatory variables (e.g., from singular value decomposi-tion).
 The variable importance parameters can be estimated us-ing variable importance statistics ,  X   X  J =  X   X  J ( P n ), that are functions of the empirical distribution P n , using either the empirical risk or the cross-validated risk. For the empirical risk where the  X   X  J =  X   X  J ( P n ) are estimators of the J -specific parameters based on the empirical distribution P n , and  X  corresponds to J = { 1 ,...,d } .
 Example 1. Prediction. The full parameter is  X  0 ( W ) = E [ Z | W ] and the J -specific parameters are conditional expected values of the outcome Z given explanatory vari-ables W ( J ),  X  J 0 ( W ) = E 0 [ Z | W ( J )] = E 0 [  X  The variable importance parameters, defined in terms of the squared error loss function, are In the special case of linear conditional expectations, i.e.,  X   X  For single variables, i.e., J = { j } , the importance parameter  X  { j } 0 =  X  2 j 0 V ar 0 [ W j ] for independent explanatory variables. To evaluate our proposed loss-based estimation methodol-ogy and demonstrate its application to tree-structured esti-mation with censored data, we present the following results from a simulation study and analysis of breast cancer sur-vival and CGH copy number data. Preliminary simulation results for a new D/S/A algorithm for histogram regression are also provided in this section. More detailed results and discussion can be found in Molinaro et al. (2004) and Moli-naro and van der Laan (2003). The proposed survival tree approach based on the IPCW loss function was compared to that of LeBlanc and Crowley (1992), which is implemented as a default for censored data in the R rpart function (Therneau and Atkinson, 1997). The loss function for the survival trees of LeBlanc and Crow-ley (1992) is based on the observed data negative log-likelihood for a Cox proportional hazards model with the same baseline hazard for each node. Trees based on the IPCW loss func-tion can be grown using the rpart function, by setting the method argument to  X  anova  X  and by providing the IPCW weights for individual observations through the weights ar-gument. The censoring survivor function,  X  G 0 , used in the IPCW loss function, is estimated separately for each training set. In what follows, Method 1 and Method 2 refer, respec-tively, to the survival trees of LeBlanc and Crowley (1992) and to trees grown using the proposed IPCW loss function. The two approaches differ in the choice of loss function for splitting and pruning and thus lead to two different par-titions of the covariate space, i.e., to different assignments of observations to terminal nodes. Given such a final par-tition, we then consider two survival estimation methods for the terminal nodes: the IPCW mean survival time and the Kaplan-Meier (KM) median survival time. These two types of estimators correspond to full data parameters de-fined in terms of the squared and absolute error loss func-tions, respectively. The two different loss functions and the two different within-node estimation methods thus produce four different predictors of survival (namely, Method 1 with IPCW mean, Method 1 with KM median, Method 2 with IPCW mean, Method 2 with KM median), which were com-pared by simulation as described below.
 The following model was considered for the full data struc-ture: Z  X  log T = W 2 + , where W and are indepen-dent random variables with W  X  U (0 , 1),  X  N (0 , X  2 ), and  X  = 0 . 25. Thus, E 0 [ Z | W ] = Median 0 [ Z | W ] = W the conditional survivor function is given by S 0 ( z | W ) = Pr 0 ( Z  X  z | W ) = 1  X   X (( z  X  W 2 ) / X  ), where  X (  X  ) de-notes the standard normal cumulative distribution function. Censoring times C were simulated using mixtures of three uniform distributions. The censoring survivor function,  X  used in the IPCW loss function, was estimated separately for each training set, by fitting a Cox proportional hazards model to the survival time T and covariate W .
 One hundred simulated learning sets were generated from an observed data distribution with 20% censoring, for sample sizes n = 250, 600, 1250, and 6000. Risk estimates, based on test sets of size N = 5000 generated from the full data distribution, were computed for each of the four predictors, using the L 2 loss function for the IPCW within-node mean estimation method and the L 1 loss function for the KM me-dian estimation method. Within each sample size, the four test set risk estimates were averaged over the B = 100 repe-titions. Method 1 and Method 2 were compared by forming the ratio of Method 2 X  X  average risk to that of Method 1, separately for each of the two within-node estimation meth-ods.
 Ratios of average test set risk are displayed in Table 1 for both the KM median and IPCW mean estimation meth-ods; ratios less than one correspond to improved accuracy for Method 2, i.e., for trees based on the new IPCW loss function. The results illustrate the impact on accuracy of the choice of loss function used for node splitting and tree pruning. As expected, when the parameter of interest is the conditional mean survival, the risk is smaller for parti-tions generated by Method 2 ( X  X PCW Mean X  column). The IPCW loss function also corresponds to lower risk when in-terest is in estimating the median survival. The difference in risk decreases with increasing sample size. Our censored regression tree method was also applied to a dataset from a comparative genomic hybridization (CGH) study of breast cancer patients. Data were collected on 152 Table 1: Simulation study: survival trees. Comparison of survival trees grown with Method 1 ( rpart  X  X  default) and Method 2 (proposed IPCW loss function). Ratios of average risk for Method 2 to Method 1 are displayed for the KM me-dian and IPCW mean within-node estimation methods for four sample sizes, n . Individual entries of the table are ratios of average test set risk (1 /B ) P B b =1 R L ( x,  X   X ( P where  X   X  refers to one of the four survival predictors, P P
N denote, respectively, the learning set and test set empir-ical distributions in the b th simulation, N = 5000, B = 100. For the KM median within-node estimation method (col-umn 2), L is the absolute error loss, and for the IPCW mean within-node estimation method (column 3), L is the squared error loss.
 patients, all with initial occurrences of breast cancer; 52 sub-sequently recurred. Time to event (in years) was defined as time to recurrence. Patients with no recurrence at the time of death or of final follow-up are censored. Explanatory variables include epidemiological variables (e.g., age at di-agnosis, race), histopathological variables (e.g., tumor stage, grade), and DNA copy number measures from a CGH mi-croarray with 2254 bacterial artificial chromosomes (BACs). The 152 observations were split at random into a learning set and a test set of 128 and 24 (i.e., five sixths and one sixth) observations, respectively, while retaining the appro-priate level of censoring. Trees were grown using the learning set and their overall performance assessed on the test set. Five-fold cross-validation of the learning set was used to se-lect the  X  X est X  tree (again, retaining the appropriate level of censoring). The censoring survivor function,  X  G 0 , used in the IPCW loss function, was estimated separately for each of the five training sets in the cross-validation, by fitting a Cox proportional hazards model to the epidemiological and histopathological variables. The full learning set tree is shown in Figure 1, with filled circles for the two-split sub-tree. Each terminal node is described by the IPCW mean log survival time (in years) and the number of observations. The legend in the bottom left corner indicates the chromo-somal location of each BAC. The first two splits are based on BACs that fall in chromosomal regions known to contain genes related to breast cancer (personal communication with Joe Gray and Fred Waldman).
 This preliminary analysis illustrates limitations of single trees based on microarray measures: they typically involve a very small number of splits and therefore only provided limited biological insight. Improved prediction accuracy and more information on chromosomal regions related to breast cancer survival may be obtained from aggregation methods such as bagging and boosting and the use of loss-based variable im-portance measures as proposed in Section 2.6. In addition, Figure 1: Breast cancer survival and CGH copy number data analysis. Survival tree built from the learning set of 128 patients, using the IPCW squared error loss function. Each terminal node is described by the IPCW mean log survival time (in years) and the number of observations. we are exploring more aggressive procedures, based on the D/S/A algorithm, that include  X  X R X  statements in addition to the  X  X ND X  statements of tree estimators (Molinaro and van der Laan, 2003). We report the following preliminary results from a simu-lation study comparing a new D/S/A algorithm for his-togram regression (i.e., as in Section 2.3, using indicator basis functions) to standard regression trees. We consider the following (full data) model: Z  X  W 2 + , where W and are independent random variables with W  X  N (0 , 1),  X  N (0 , X  2 ), and  X  = 0 . 25. The parameter of interest is  X  0 ( W ) = E 0 [ Z | W ] = W 2 and the loss function is the squared error loss. One hundred simulated learning sets were generated from the above distribution, for sample sizes n = 250, 500, and 1000. Regression trees were grown us-ing the R rpart function, selecting tree size using five-fold cross-validation, with the 1-SE rule and without the 1-SE rule (Breiman et al., 1984; Therneau and Atkinson, 1997). The D/S/A algorithm of Molinaro and van der Laan (2003), with indicator basis functions, was applied to generate a se-quence of candidate estimators (i.e., partitions of the real line) and five-fold cross-validation was used to select the op-timal number of sets in the partition.
 Risk estimates, based on test sets of size N = 1000 were computed for each of the three predictors, using the squared error loss function. Summary statistics over the B = 100 simulations are displayed in Table 2 for the test set risks and the partition sizes (i.e., number of indicator basis func-tions for final predictor).
 Table 2: Simulation study: D/S/A algorithm for histogram regression. Test set risk (mean squared error) for histogram regression with D/S/A algorithm ( DSA ), regression trees with 1-SE rule ( tree1SE ), and regression trees without 1-SE rule ( tree0SE ). The following summary statistics are re-ported for each prediction method and each sample size n , over B = 100 simulations: risk average (Risk Avg.), risk standard deviation (Risk SD), average partition size (Avg. Size), and average risk over average DSA risk (Ratio). n Method Risk Avg. Risk SD Avg. Size Ratio 250 tree1SE 0.45305 0.14195 5.69 .577 500 tree1SE 0.27216 0.08574 9.55 .696 1000 tree1SE 0.18489 0.05206 13.02 .762 These preliminary results demonstrate that significant gains in accuracy can be achieved, even in univariate situations, by selecting partitions using the D/S/A algorithm rather than standard tree methods. We anticipate even greater gains in accuracy from the D/S/A algorithm in multivariate situations. In addition, the results raise questions regarding the benefits of the widely-used 1-SE rule. In our simple example, where the parameter  X  0 ( W ) = E 0 [ Z | W ] = W is a quadratic function of the explanatory variable W , the D/S/A algorithm is able to exploit the symmetry of the parameter  X  0 , while the more rigid tree algorithms cannot recognize this symmetry. Trees built without the 1-SE rule (i.e., tree0SE for which tree size is obtained by minimizing the cross-validated risk) result in roughly twice as many sets in the final partition compared to the D/S/A algorithm. We would like to thank Joe Gray, Dan Moore, and Fred Waldman (Comprehensive Cancer Center, University of Cal-ifornia, San Francisco) for graciously providing the CGH dataset, biological insight, and fruitful discussions. We are also grateful to Terry Therneau and Elizabeth Atkinson (Mayo Clinic) for their thorough explanation of the R rpart package.
 L. Breiman, J. H. Friedman, R. Olshen, and C. J. Stone.
Classification and regression trees . The Wadsworth statis-tics/probability series. Wadsworth International Group, 1984.
 S. Dudoit and M. J. van der Laan. Asymptotics of cross-validated risk estimation in model selection and perfor-mance assessmen. Technical Report 126, Division of Bio-statistics, University of California, Berkeley, 2003. URL www.bepress.com/ucbbiostat/paper126/ .
 R. D. Gill, M. J. van der Laan, and J. R. Robins. Coarsening at random: Characterizations, conjectures and counter-examples. In D. Y. Lin and T. R. Fleming, editors, Pro-ceedings of the First Seattle Symposium in Biostatistics, 1995 , Springer Lecture Notes in Statistics, pages 255 X 294, 1997.
 S. Kele  X s, M. J. van der Laan, and S. Dudoit. Asymptot-ically optimal model selection method for regression on censored outcomes. Technical Report 124, Division of Bio-statistics, University of California, Berkeley, 2003a. URL www.bepress.com/ucbbiostat/paper124/ .
 S. Kele  X s, M. J. van der Laan, S. Dudoit, M. B. Eisen, and B. Xing. Supervised detection of regulatory motifs in DNA sequences. Statistical Applications in Genetics and Molecular Biology , 2(1), 2003b. Article 5.
 M. LeBlanc and J. Crowley. Relative risk trees for censored survival data. Biometrics , 48:411 X 425, 1992.
 A. M. Molinaro, S. Dudoit, and M. J. van der Laan. Tree-based multivariate regression and density estimation with right-censored data. Journal of Multivariate Analysis , 2004. (Accepted).
 A. M. Molinaro and M. J. van der Laan. A Dele-tion/Substitution/Addition algorithm for partitioning the covariate space in prediction. Technical report, Division of Biostatistics, UC Berkeley, 2003. (In preparation). J. Robins and A. Rotnitzky. Recovery of information and adjustment for dependent censoring using surrogate mark-ers , chapter AIDS Epidemiology, Methodological issues. Bikhauser, 1992.
 S. Sinisi and M. J. van der Laan. A general Deletion/Substitution/Addition algorithm in prediction.
Technical report, Division of Biostatistics, UC Berkeley, 2003. (In preparation).
 S. L. Teng, S. Dudoit, and M. J. van der Laan. Loss-based measures of variable importance. Technical report, Di-vision of Biostatistics, University of California, Berkeley, 2003. (In preparation).
 T. Therneau and E. Atkinson. An introduction to recursive partitioning using the rpart routine. Technical Report 61, Section of Biostatistics, Mayo Clinic, Rochester, 1997. M. J. van der Laan and S. Dudoit. Unified cross-validation methods for selection among estimators: Finite sample results, asymptotic optimality, and applications. Tech-nical Report 130, Division of Biostatistics, University of California, Berkeley, 2003. URL www.bepress.com/ ucbbiostat/paper130/ .
 M. J. van der Laan, S. Dudoit, and S. Kele  X s. Asymp-totic optimality of likelihood based cross-validation. Tech-nical Report 125, Division of Biostatistics, University of California, Berkeley, 2003. URL www.bepress.com/ ucbbiostat/paper125/ .
 M. J. van der Laan and J. Robins. Unified Methods for Cen-sored Longitudinal Data and Causality . Springer, 2002.  X  )  X  L ( X, X  )  X   X  G L ( X, X  ) L ( X, X  ) = 1  X   X  ( X ) : l = 0 ,...,m  X  1)  X  IR m
