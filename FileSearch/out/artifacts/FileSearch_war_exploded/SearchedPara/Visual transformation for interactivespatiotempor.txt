 REGULAR PAPER Ya n g C a i  X  Richard Stumpf  X  Timothy Wynne  X  Michelle Tomlinson  X  Daniel Sai Ho Chung  X  Xavier Boutonnier  X  Matthias Ihmig  X  Rafael Franco  X  Nathaniel Bauernfeind Abstract Analytical models intend to reveal inner structure, dynamics, or rela-tionship of things. However, they are not necessarily intuitive to humans. Conven-tional scientific visualization methods are intuitive, but limited by depth, dimen-sion, and resolution. The purpose of this study is to bridge the gap with transfor-mation algorithms for mapping the data from an abstract space to an intuitive one, which include shape correlation, periodicity, multiphysics, and spatial Bayesian. We tested this approach with the oceanographic case study. We found that the interactive visualization increases robustness in object tracking and positive de-tection accuracy in object prediction. We also found that the interactive method enables the user to process the image data at less than 1 min per image versus 30 min per image manually. As a result, our test system can handle at least 10 times more data sets than traditional manual analyses. The results also suggest that min-imal human interactions with appropriate computational transformations or cues may significantly increase the overall productivity.
 Keywords Vision  X  Visualization  X  Interaction  X  Data Mining 1 Introduction According to Constructionism, people not only discover things, but also construct them and interact with them. For over half a century, people have been developing computing systems for discovering lawful patterns in letters, numbers, words, and images. The research has expanded into the computational study of the process of scientific discovery, producing such well-known classic AI programs as BA-CON [ 13] and DENDRAL [ 16]. However, autonomous discovery systems have rarely been used in the real world. While many factors have contributed to this, the most chronic difficulties seem to always fall into two categories: the represen-tation of the prior knowledge that people bring to their tasks, and the awareness of new context. Many difficult scientific discovery tasks can only be solved in interactive ways, by combining intelligent computing techniques with intuitive and adaptive user interfaces. It is inevitable that human intelligence is used in scientific discovery systems. For example, the human eyes can capture complex patterns and detect anomalies in a data set. The human brain can easily manipulate perceptions (shape, color, balance, time, distance, direction, speed, force, similar-ity, likelihood, intent, and well-being) to make decisions. The process consists of perception and interaction and is often ubiquitous and autonomous. We refer to this kind of intelligence as Ambient Intelligence (AmI) [1, 3].
 that are sensitive and responsive to the presence of people. Since its introduc-tion in the late 1990s, the field has matured and expanded into cognitive as-pects of interactions. Ambient Intelligence is about human interaction with in-formation in a way that permits humans to spot interesting signs in massive data sources X  X uilding tools that capitalize on human strengths and compensate for human weaknesses in order to enhance and extend discovery capabilities. For example, people are much better than machines at detecting patterns in a vi-sual scene, while machines are better than people at manipulating streams of numbers. 1.1 Studies in interactive visualization The human X  X omputer interactive discovery systems have risen from many pro-fessional applications. Zudilova and Sloot investigate the practical deployment of virtual reality systems in the medical environment [ 34]. They explore the multimodal interaction of virtual reality and desktop computers in Virtual Ra-diology Explorer. Pryke and Beale present their interactive data mining sys-tem that helps users gain insight from the dynamically created virtual data space [ 17]. Cowell et al. present the architecture of a next-generation analyt-ical environment for scientific discovery within continuous, time-varying data streams [ 7]. Devaney et al. develop a high-end CAVE-based virtual labora-tory for scientific discovery. Minimizing the time between the generation of a scientific hypothesis and the test of that idea, it enables scientific investi-gations at the speed of thought. Cai et al. [ 4] develop a game-based interac-tive scientific visualization system for interdisciplinary biological discovery and education.
 everyday work and life can eventually help us design more effective discovery systems. Hubona and Shirah investigate how various spatial depth cues, such as motion, stereoscopic vision, shadows, and scene background, are utilized to pro-mote the perceptual discovery and interpretation of the presented imagery in 3D scientific visualization [ 11]. 1.2 Studies in spatiotemporal data mining Spatiotemporal data are widely used in the remote sensing community. The imagery data usually contain multispectrum information such as visible, in-frared, UV, or microwave signals. Scientists need to find out the physical properties behind the data flow and predict the future trends such as global warming, flood, harmful algae blooms, or river plumes. Spatiotemporal dy-namics are not only about frames but also multidimensional interaction and evolution. Spatiotemporal data mining is often a computationally hard prob-lem. For example, a colocation mining problem for over 10,000 locations would take several hours to solve by a computer. In many cases, the inverse physics process is complicated. Heuristics have to be used for approximate estimations.
 main knowledge. Many models are based on statistics and geospatial applica-tions. For example, the Bayesian model is used to geographically profile serial killers; neural networks are used to model urban development dynamics, where the spatiotemporal patterns and space X  X ime transitions are extracted. Spatial au-toregression is developed for spatial data mining. Furthermore, Variogram is used for spatial variation analyses. However, spatial data are often sparse and noisy. For example, normally over 80% of optical remote sensing satellite images over Florida contain clouds so that the objects beneath remain unseen. There are many artifacts in the images due to the noise in data acquisition and processing. Further-more, our knowledge about the spatial dynamics is often limited. Therefore, fully autonomous data mining is nearly impractical. Human intervention is necessary. It is desirable to use computers X  numerical power and humans X  pattern recognition capabilities.
 sions from massive data. Models, such as regression, neural network, and statistic models, can be extended to spatiotemporal domains. These models are quantita-tive and programmable. However, they assume that the future is what it used to be. As a result, they only work in a well-controlled environment with  X  X lean data. X  They are better for short-term prediction, because they need less knowledge and parameters.
 example, deformable finite meshed templates are used to model faces [ 20]. Par-ticle filters [ 33 ] are used to describe the spatiotemporal feature distributions as particle dynamics. Particle filters assume the particle distribution follows a sta-tistical pattern and a linear or nonlinear transformation trajectory. Unfortunately, many spatiotemporal data include overwhelming noises and missing data that pre-vent using statistical assumptions properly. Object tracking sometimes fails due to the discontinuity of the patterns.
 analogy. For example, Leyton discovers that shape is energy [ 15]. His shape pro-cess theory reveals the rules for recovering original shapes based on symmetry and other properties. Lewin [ 2] uses metaphors from well-known systems (e.g., physics), in order to understand and explain a rather unknown system (e.g., psy-chology). In explaining phenomena around the concept of behavior, Lewin uses the metaphor of a  X  X orce field X  such as psychological tension, psychological force, and fluidity.
 artificial life forms [10, 22, 26 X 31], as well as at the large scale, such as urban and economic dynamics [ 9]. Furthermore, CA has been employed in simulating and verifying various types of physical interactions, such as the expansion patterns of different lattice gases, surface tension of liquids, and diffusion-limited aggre-gation [ 8]. Although CA X  X  rules are simple and local, they can simulate complex spatiotemporal interactions that are challenging to other computational methods. 1.3 Objectives of this study Analytical models intend to reveal inner structure, dynamics, or relationship of things. However, they are not necessarily intuitive to humans. Conventional sci-entific visualization methods are intuitive, but limited by depth, dimension, and resolution. To bridge the gap, transformation algorithms are needed to map the data from an abstract space to an intuitive one. For example, a spectrogram maps an invisible sound to a visible frequency X  X ntensity X  X ime space. The conver-gence of scientific visualization and data mining creates a new domain for visual data mining. Sensing and understanding together enables humans to gain insight from a large amount of data. This approach integrates a human X  X  Ambient In-telligence and analytical computation to form a coherent knowledge discovery environment.
 that incorporate computer vision, multiphysics simulation, and user interaction models to solve the tracking and prediction problems across multiple databases (see Fig. 1). Embedding vision into the conventional data mining algorithms en-ables us to automate the data prepreprocessing and increase accuracy in the over-all process. In contrast to the conventional data mining algorithm that is normally context-free and independent from the physical constraints, we introduced a vi-sual multiple physics model such as Cellular Automata to simulate the physical, biological, and chemical phenomena. This provides a mechanism to explore the deep knowledge about spatiotemporal patterns while the accurate computational models are unavailable or impractical. For example, the current data assimilation for weather models takes several hours so it prevents real-time weather forecast-ing. Machine learning has been a key feature in many data mining algorithms. In this project, we extend learning algorithms to spatial and temporal space and in-tegrate them with vision and simulation models. As learning models can be data mining tools alone, they also support vision and simulation models for calibration or optimization of parameters. Moreover, human X  X omputer interaction and sci-entific visualization methods are introduced as a comprehensive interface, which increases the robustness of the process.
 domain-oriented knowledge discovery. No single visualization method or single interaction can cover the whole process. Instead, multiple visualization methods and human X  X omputer interactions are embedded inside the complex process. To bridge the analytical models and human visual perception, we have developed a platform of the visual transformation models that enable the two-way visual interactions, which are subtle but profound.
 predicting the spatiotemporal dynamics in an imagery flow. The model consists of a set of parameters and rules that control spatiotemporal dynamics in the form of a shape, such as diffusion, transport, and collision. In this model, cells not only interact with their neighbors locally , but also contain the holistic shape of a clus-ter of cells as an object or a lump . Figure 2 shows an illustration of the system architecture where the cellular automata model consists of Bayesian inference, ar-tificial life rules, and multiphysics rules such as multibody collision, etc. The visu-alization functions include fuzzy blobs, isosurfaces, correlation values, periodicity transform, animation, and polarized glasses. On the other side, the microinterac-tions include shape adjustment, parametric control, rotation, pan, zoom, cut, and slide windows.
 we try to find the object at t = n + 1 and beyond. For prediction problems, given databases of historical data and current physical and biochemical conditions, we try to predict the occurrence of the object of interest at a particular time and location. Figure 3 illustrates a generalized spatiotemporal data mining problem. with the real world databases: tracking and prediction of harmful algal blooms in theGulfofMexicoarea. 2 Shape correlation transform Given a selected object in a very large imagery database, tracking the movement of the object over time with multiple sensors is a challenging task for both humans and computers. For example, it takes about 30 min for scientists to analyze one frame of the multispectrum satellite image. On the other hand, computer vision-based tracking is faster, but it is not very reliable in many cases. The goal of the in-teractive object-tracking is to combine the machine vision with human X  X omputer interaction so that the computer takes care of most of the time-consuming iterative task and humans contribute the minimal initiation and evaluation processes. the tracking task. In the protocol, the user identifies the tracking target. The com-puter takes over the tracking frame by frame and superimposes the tracking results on top of the original images. At a certain point, the computer stops the tracking and sends the signal to the user for more information. After the re-initiation, the computer moves on till the next break point. The key element in this protocol is the context-awareness algorithm. In this study, we use the correlation filter to measure the similarity of the object in the current frame and the object in the pre-vious frame. The correlation filter is built by using Fast Fourier Transform and Correlation Theorem: where a is the test image while b is the reference object in the previous image to be tracked. X = FFT ( x ) and x = IFFT ( X ) represent Discrete Fast Fourier Transform and Inverse Discrete Fast Fourier Transform respectively.
 where  X  N = e (  X  2  X  i ) fN , N denotes the length of the vector and symbol. The as-terisk symbol (  X  ) denotes array multiplication and conj returns the complex con-jugate of the argument. The pixel that gives the highest correlation value has the highest confidence that the reference object is at that pixel location. The correla-tion filter alone cannot reliably track the object over a long period of time because it faces a dilemma when the object breaks into pieces. The computer would select one of the pieces and track it. The scenario probably is not what we want for object tracking due to lack of control of the process. Figure 4 shows the scenario where the object breaks into two pieces and the computer is attracted to the small piece at the fourth frame.
 the similarity of the objects between images. If there is a large difference in the values given by the correlation filter between two consecutive images, we can con-clude that the objects have changed significantly during this time period. However, we could not determine whether the object in the previous frame has disappeared in the subsequent frame or if the object has just deformed significantly. Scien-tists X  experience and intuition would help determine if the object still appears in the next image in this case. We performed an experiment to track an object in a series of 84 images (see Figs. 5 and 6). In frame 48, the correlation dropped by more than 50% from the previous frame. Originally, the algorithm would stop at this point because it cannot identify the object being tracked. However, instead of terminating the tracking, the computer prompted the user to select a particular object in the new image so that the tracking could go on. In our experiment, the user selected an object that was most alike the previous object and the tracking progress continued until the end of series. There is also another application that human interaction may help. When the object breaks into more than one piece, the original algorithm would only try to find the single best match of the previous tracking object. With the interaction method, the user can select more than one object and multiple targets will be under the tracking process. Table 1 shows a comparison of the results. 3 Spatial periodicity transform Although humans are capable of identifying spatiotemporal patterns, such as the repetitions in textures and the rhythms in music, human perceptions often miss the hidden patterns if the data set is complex. Computational transforms are needed for semiautomatically identifying underlying patterns. Perhaps the best known is the Fourier Transform, which attempts to explain a data set as a weighted sum of sinusoidal basis elements. However, none of the available methods directly search for periodicities, repetitions, or regularities in the data. Sethares builds on a clas-sical technique, called the Buys-Ballot table, for the determination of hidden pe-riodicities, the Periodicity Transform [ 18]. In his method, the algorithm searches for the best periodic characterization of the length N signal x . The underlying technique is to project x onto a periodic subspace.
 two-dimensional space of the cellular automata. We construct spatial and tem-poral windows to allow the user to select the region of interest (in the red box of longitude and latitude coordinates), on a particular time frame. The algorithm starts with cropping the data within the ROI (Region Of Interest) box and sort-ing the data by time. Then it takes the Fast Fourier Transform (FFT) and extracts the highest value. For each point, we take the average value of the corresponding time. We then simply calculate the remaining signal and reprocess it. The outputs include the detected periods and their energy. This method enables the user to interact with spatial periodicity transform in an interactive mode, which is very important to many applications such as oceanographic studies.
 the highest energy. Then we turn this frequency ( f ) into a period ( p ) with the formula: dimensional colored cells to represent the power (energy) for a particular period component, e.g., 365.25 days. In this case, a user needs to select the period com-ponent as an input. Figure 7 illustrates an example of the two-dimensional power ratios in a database of Karenia Brevis cell counts in the Gulf of Mexico for 50 years. The brighter the cell, the higher the power ratio, which means that the prob-ability of finding the target every year at the same time is higher. Due to the het-erogeneity of the data set, preprocessing is necessary. That consists of computing a space mean of the number of cells and the estimation of missing data spots where we do not have any measurement. Based on the periodicity transform and human X  X omputer interaction, we can see in Fig. 9 that for each set of data we have a 1-year period component.
 diagram into a two-dimensional matrix. Each cell has its periodicities along with their powers. This matrix is called cellular periodigram and is represented in Fig. 8. The advantage of this method is its all-in-one solution. However, its scalability is limited by the number of cells per screen.
 transform. We assign an interactive window for two-dimensional data points. In another window, we display the associated results of the periodicity transform, where the x -coordinate is the period and the y -coordinate is the power ratio of the component. Figure 9a X  X  are examples of the interactive data mining results. The data is drawn from 50 years of the Karena Brevis cell counts from Florida coast. Each piece of data contains the geographical coordinates and levels of concentra-tion. 4 Multiphysics transform Conventional data mining models do not involve physical parameters. On the other hand, traditional physical models only deal with particular phenomena with re-quired calibration process. In this study, we try to merge the two with a simple multiphysics model and an interaction process that overlays the results from the simulated model on top of the collected data.
 Given the results obtained from the previous sections, we create scenarios of object movements by modeling the artificial life, with diffusion, growth, shrink, transport, and collision. The growth diffusion function changes the object shape by a predefined structuring element. The dilation or erosion morphology is based on the assumption of a glass-like surface where no friction exists. They can only deform the outline of a shape uniformly based on the population. It does not reflect the intrinsic, external nonlinear, or random factors that contribute to the shape pro-cess. In order to simulate the nonuniform shape growth, we investigate percolation clustering [ 8]. Figure 10 shows an example of the percolation process that started from a given shape.
 uninhibitedly in an infinite amount of space; rather, their growths and movements are often affected by environmental factors such as external forces like winds and currents, and the presence of obstacles. A collision causes deformation. The extent of a deformation depends on the elasticity of an object. For a rigid object, it will stop at the collision contact point and keep the same shape. We classify objects as rigid, elastic, and flexible. The two extreme cases are straightforward. Rigid objects do not deform after the impact. Flexible objects completely deform along the outline of the collided hard objects. For the case of an elastic object and a rigid object, we use the following rules to model the deformation: 1. Detect the collision points 2. Move a portion of the collided cells to sideways, the ratio is proportion to 3. Continue to deform until the motion stops.
 which starts at the left frame with a very small region. Through some internal fac-tors, the life grows larger. But once the growth hits the wall, the growth spreads to the side starting from the third frame. The frames mentioned earlier were gener-ated using a Cellular Automata simulation with a relatively low elasticity. is implemented for the growth and shrinking of cells, collision of cells onto a boundary, and wind translations. The simulation rules are listed in the Appendix. to analyze the dynamics of artificial life. So far, the Bayesian prediction model is a better prediction than most other models because of its simplicity. Figure 12 shows the interactive parameter estimation.
 ture shape deformations, collisions, and wind translations using surface current conditions, such as temperature which would affect how the shape deforms, grows, and shrinks. The dynamics are detailed by simulating predictions and comparing them to the actual movement of artificial life. The error of simulation compared to actual movements will allow humans to better understand the dynamics of the internal and external factors of artificial life. With better understanding, the CA model can be improved to incorporate more rules dealing with artificial life. This cycle can be repeated for the computer and the human to learn together. system, where we display the simulated results on the left channel and the ground truth data on the right channel. The polarized display system displays two data sets simultaneously. Furthermore, the multimodal display system is scalable from a desktop monitor to a large projected screen. Figure 13 shows a front view of the stereo projector system. 5 Spatial Bayesian transform Here we define a Bayesian cellular automata model to represent the spatial stochastic formation of an artificial life  X  X lga. X  The alga lives in a two-dimensional space divided by a grid of cells. In this model, we consider the location of an alga ( x , y ), time ( t ), and other physical stressors. For historical images, c denotes the presence of an alga. Given the location in ( x , y ), and time ( t ), the following equa-tion can be used to find the probability that location is inhabited by alga at that time Sometimes, when additional evidence e such as the surface temperature is present, it is necessary to use the recursive property of Bayesian to get a more accurate value of the probability: Calculating each of the probabilities is straightforward: of pixels in all images, and N xc is the number of pixel occurrences of c at location P ( e | c ) .
 alize this prediction, we need a threshold  X  to determine the pixel of the output image. The output pixel O ( x , y ) is a Boolean value decided by the equation: For images outputting the probability, the value of each grid represents the proba-bility the grid is inhabited by artificial life. The higher the probability, the darker the region is on the probability image. Probability images are used to predict the location of life given the location, time, and any additional evidence. 5.1 Gray area rendering and interaction The probabilities under those certain conditions/evidence, such as time and tem-perature, can be calculated. Figure 14 shows examples of the estimated target locations.
 cold temperature because when the temperature falls below 15 X C, no alga is de-tected. In Fig. 14 , a and c are poor probability models because of the lack of evidence for the Bayesian model to make an accurate prediction; this is indicated by the large number of cells in each image with very low probability (light colored cells). When the model is fed with both temperature and time: 15 X C and January, it becomes much more accurate, as shown in Fig. 15 . 5.2 Isosurface rendering and interaction Isosurface is an interactive tool for volumetric visualization. The isosurface plots in Fig. 16 have latitude, longitude, and time. Each slice of the blue isosurface with respect to the time axis represents the predicted image at time t = z with some constant  X  which is the probability threshold constant or confidence level. Humans can interact with the isosurface plot and be able to make more accurate predictions by adjusting the value of  X  . This visualization tool can be used by the user to determine what confidence level, or  X  to use.
 dicted image. Each time slice of the blue isosurface represents the predicted location of life. For the figure mentioned earlier,  X  = 0 . 4 was used for the prediction.
 the choice of choosing the time interval to view the plot, and the  X  threshold. Through visualizing different slices of the plot with different  X  values, the user can interact with the Bayesian probability model to produce more accurate images by adjusting the  X  value for each time period.
 algal bloom (HAB) of Karena Brevis . In this case, the HABs are lumps, rather than individual cells. The input data are satellite images and cell counts. The pre-diction model uses the satellite images as additional evidence. A Spatiotemporal Bayesian prediction model is implemented using a modified version of the equa-tions mentioned earlier. The inputs for the prediction are the location in latitude and longitude, the time, and the patch of the image for the region of interest. For the prediction, the probability of HAB present is compared to the probability of not present. The higher probability will determine the classification. Because the denominator for calculating both the probabilities is the same, the prediction equa-tion summarizes to: where I is input image and I x , y is the c  X  C patch for the region of interest. See the next section titled Usability Study for more details. 6 Usability study We investigate the visual transformation methods in the context of our real-world scientific research projects in oceanographical studies. Therefore, our approach has to make sense to scientists who conduct day-to-day analyses and discovery tasks. Table 2 summarizes the advantages and disadvantages of the visualization modes and interaction modes in a matrix form.
 of cell counts from the Gulf of Mexico area and SeaWiFS satellite images from NASA. We quantified our results, for example, positive accuracy, which describes the percentage of predictions which were correct when Harmful Algal Bloom cell count, was greater than or equal to 5,000. Tables 3 and 4 show summaries of the results. The measurements are defined as following:  X  Positive accuracy is the percent of the cases in which target is present and the  X  Positive accuracy = confirmed positive / (confirmed positive + false negative).  X  Positive detection is the percent of all predictions that are correct.  X  Positive detection = (confirmed positive + confirmed negative) / (sum). 7Conclusion Analytical models intend to reveal inner structure, dynamics, or relationship of things. However, they are not necessarily intuitive to humans. Conventional sci-entific visualization methods are intuitive but limited by depth, dimensions, and resolutions. The purpose of this study is to bridge the gap with transformation algorithms for mapping the data from an abstract space to an intuitive one. physics simulation, and machine learning models to solve the tracking and predic-tion problems across multiple databases. In particular, the visual transformers in-clude shape correlation for interactive tracking, periodicity for periodical patterns, multiphysics for simulating the complex surface dynamics, and spatial Bayesian for spatiotemporal prediction.
 ables us to automate the data preprocessing and increase accuracy in the overall process. context-free and independent from the physical constraints, we introduced a vi-sual multiple physics model Cellular Automata to simulate the physical, biolog-ical, and chemical phenomena. This provides a mechanism to explore the deep knowledge of spatiotemporal patterns while the accurate computational models are not available or impractical.
 mining process with multimodal interfaces such as desktop computers and stereo projectors. The interactive visualization is not only a display but also a learning and problem solving process.
 NASA X  X  8-year SeaWiFS satellite database along with NOAA X  X  50-year in situ cell count databases. We found that the interactive visualization increases robust-ness in object tracking and positive detection accuracy in object prediction. We found that the interactive method enables the user to process the image data at less than 1 min per image versus 30 min per image manually. As a result, our test sys-tem can handle significantly more data sets (e.g., 5,000 samples) than traditional manual analysis (e.g., 188 samples).
 tions can improve the data mining quality. However, where, when, and how to en-gage the visual interaction are key factors in the process. The results also suggest that minimal human interactions with appropriate computational transformations or cues may significantly increase the overall productivity.
 Appendix: CA simulation rules The growth rule is as follows given the growth amount N : 1. Pick an arbitrary cell neighboring the region of life. 2. If the sum of neighboring cells  X  2 and the arbitrary cell is  X 0 X  then make the 3. Repeat steps 1 and 2 until the desired N has been reached.
 The shrink rule is as follows given the shrink amount M : 1. Pick an arbitrary cell on the edges of the region of life. 2. If the sum of neighboring cells  X  2 and the arbitrary cell is  X 1 X  then make the 3. Repeat steps 1 and 2 until the desired N has been reached, or a maximum The collision rule is as followed given elasticity E : 1. If any cells cross the boundary, proceed to the next step. 2. Treat the rows and columns outside of the boundary as  X 1. X  3. If the sum of neighboring cells around any cell is  X  4 then make the cell 4. Repeat this step E times since the more elastic the collision, the more the cells The wind translation rules are as follows given wind speed W and direction: 1. Using a constant for speed of translation with relationship to wind speed. (e.g., 2. Move the shape at the speed of one iteration at a time in the x and y directions. 3. If there is a collision with a boundary, spread along the boundary, but continue References Author Biographies
