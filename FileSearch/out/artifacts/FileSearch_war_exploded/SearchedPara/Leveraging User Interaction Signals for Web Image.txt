 User interfaces for web image search engine results differ sig-nificantly from interfaces for traditional (text) web search results, supporting a richer interaction. In particular, users can see an enlarged image preview by hovering over a result image, and an  X  X mage preview X  page allows users to browse further enlarged versions of the results, and to click-through to the referral page where the image is embedded. No ex-isting work investigates the utility of these interactions as implicit relevance feedback for improving search ranking, beyond using clicks on images displayed in the search re-sults page. In this paper we propose a number of implicit relevance feedback features based on these additional inter-actions: hover-through rate,  X  X onverted-hover X  rate, referral page click through, and a number of dwell time features. Also, since images are never self-contained, but always em-bedded in a referral page, we posit that clicks on other im-ages that are embedded on the same referral webpage as a given image can carry useful relevance information about that image. We also posit that query-independent versions of implicit feedback features, while not expected to capture topical relevance, will carry feedback about the quality or attractiveness of images, an important dimension of rele-vance for web image search. In an extensive set of ranking experiments in a learning to rank framework, using a large annotated corpus, the proposed features give statistically significant gains of over 2% compared to a state of the art baseline that uses standard click features.
  X  Information systems  X  Web search engines; Con-tent ranking; Image search; Web Image Search, User Behavior, Ranking
Although multimedia search in general, and image search in particular, have been very active research areas for well over a decade, there has been relatively little work in un-derstanding how user behavior, based on interactions with image search engine interfaces, the traces of which are stored in search engine logs, can be used to provide implicit rele-vance feedback that can improve the rankings for commer-cial image search engines. Instead, standard approaches, developed with general web search in mind, are applied to multimedia search without any effort to adapt them to the special case of image search and the rich, and unique, user interactions that the interfaces of commercial image search engines facilitate. On the other hand, image search has been shown to be very important within web search: for exam-ple, a recent study shows that queries showing image intent were second only to the navigational intent 1 queries on desk-tops and tablets, and they were the most popular (42%) for mobile phone devices [22].

In this work, we address this gap by leveraging the rich user interactions with web image search results to extract implicit relevance feedback information and improve search engine rankings. Interaction features that are found in image search, but not in general web search, include: (1)  X  X over-ing X  over result images to see a preview; (2) browsing en-larged previews of many images after a click on a search result, via next/previous buttons; (3) the ability to further click through, from this image preview, to the host webpage where the image is embedded. In this work, we propose a number of novel implicit user feedback features for image search based on the above interface features, and evaluate their usefulness for image search ranking on a large anno-tated image search corpus, showing an improvement in rel-evance over a strong baseline based on click data.

The main contributions of this paper are as follows: navigational queries = queries that seek a single website or web page of a single entity
Although other work has explored dwell time, to the best of our knowledge this is the first work to use dwell time for a large scale, general search relevance task.
 The rest of the paper is organized as follows: in the next Section we introduce the related work, before going on to describe the unique features of web image search user inter-faces in Section 3. In Section 4 we introduce novel implicit relevance feedback features for web image search. In Section 5 we describe the data that we use in our experiments, and then we go on to describe our experiments and results in Section 6. Finally, we conclude the paper in Section 7.
Click information from the search engine logs has long been used as an implicit relevance feedback signal to improve relevance for web search. Features like click-through rate are calculated from the log data and used as a signal for rele-vance, under the assumption that relevant documents have more clicks than non-relevant ones [12, 13, 1]. Agichtein et al. [1], for example, show that adding such implicit user be-havior information can give as much as a 30% improvement for relevance in web search. Other work tries to better un-derstand user behavior by creating click models that aim to account for biases caused by the display position and other factors [5]. While most of this has focused on web search, and treats the results as a simple ranked list, at least one recent work studied aggregated search on a web search re-sults page, where video and image results, among others, are displayed alongside traditional web results [25].

There has been relatively little work that has aimed at leveraging user behavior information to improve results for multimedia search, either for images or videos. Craswell et al. [4] explore random walk models on the click graph for propagating click information to URLs which have not been clicked. They are not specifically interested in image search, however, but use image data because it has features that suit the research questions on that paper. They show that 75% of clicked images are relevant to the query, which means that basic click data is a very strong indicator of rel-evance for image search. Jain &amp; Varma [9] use click data to train query-dependent re-ranking models, but do not focus on exploiting click data as a feature for ranking. Smith &amp; Ashman [21] study click-through data for image search and, consistently with Craswell et al., showed that it was  X  X onsid-erably more accurate in general than document based search click-through data X , although the reliability of the clicks was shown to be dependent on factors such as query type and the quality (in terms of precision) of the results shown. Tsikrika et al. [24] have used image clicks to automatically create ground truth labels for training visual classifiers, while other work has combined click data and visual features for im-age ranking [30]. Other researchers have studied web image search interaction logs to understand how interaction with image search engines differs from general search, without ex-ploring the utility of these interactions as implicit relevance information [10, 2, 17, 19].
 In traditional web search, beyond standard clicks on the Search Results Page (SRP), some researchers have explored post-SRP clicks for improving relevance [3, 26], although this work relies on user-installed browser toolbars to log the post SRP-click user trails, while other work has explored user branching behavior using tabs [8]. Some recent work has investigated using dwell time on search result pages to model user satisfaction [15], although they do not use dwell time as a signal to predict relevance. Other work has ex-plored using dwell time for personalization of web search results [28, 27], while Liu et al. [16] showed that dwell time is a good indicator of document usefulness for a variety of tasks. Yi et al. use dwell time as a novel feature in a learning to rank framework for recommendation [29]. With respect to images, Trevisiol et al. [23] leveraged extended user brows-ing traces on the Flickr photo sharing platform to create a query independent image authority measure.

This work focuses on novel features for implicit relevance feedback beyond the standard SRP clicks. While  X  X overs X , and clicks from the image Preview Page to the image Re-ferral Page , are totally unexplored due to their uniqueness to image search, our use of dwell time features is related to the work cited above. We emphasize, however, that the rich interaction with multiple search results after an SRP click, but still within the search engine experience (and therefore these interactions are stored in the search engine logs), are unique to image search, providing much more dwell time data than would be available in general web search. Also, we note that we explore propagating click data beyond the query image url level, again unique to image search and to this work. To the best of our knowledge, the is the first work to use dwell time features extracted from large scale log data for a search relevance ranking task (although Yi et al. [29] did so for a recommendation task).
Image search interfaces differ greatly from traditional web search, supporting a number of interactions not associated with traditional search. Figure 1 (first row) shows the Search Results Page for the three major U.S. search engines (Bing, Yahoo and Google). The main section of the layout presents a grid of clickable thumbnails. Two out of the three search engines allow a form of interaction that is specific to image search: the  X  X over X  (see pointer 1 in Figure 1). After hover-ing the mouse cursor on a search result image for about 1-2 seconds, the user can see an enlarged version of the thumb-nail with some additional information about the context of the image (e.g., the website it was crawled from, the image resolution).

When the user clicks on a thumbnail (or its enlarged ver-sion), the Preview Page is loaded (see Figure 1, second row). This page is generally displayed as a new view, or it can un-fold from the thumbnail. In either case, the Preview Page extends over the whole width of the screen, and shows a larger version of the image, not necessarily in the original size. In addition to a link to the actual image (i.e., a full size version hosted in its original server), a Preview Page typically provides other elements for interaction:
The user can additionally  X  X lose X  the Preview Page to go back to the Search Results Page .
Images are not standalone content items on the web, but instead they are embedded in web pages, that link to the servers where the images are actually hosted. Images are typically discovered by search engine crawlers in the context of the web pages in which they are embedded, and the search engines will store details of that page along with the details of the image. Also, images are not canonical resources pages link to, which means that there is a very high level of repli-cation. Two identical images can be hosted in two different servers (and hence have two different URLs), and any of those locations can be linked to by any number of websites. Additionally, images do not carry any information about what they represent, so search engines need to extract this information from their context, i.e., the surrounding text in the website they are linked from. This means that the infor-mation that an image is associated with is heavily dependent on the content of its Referral Page .

As the same image (in terms of content or location) can be embedded in different contexts, it is important to take into account the website the image was crawled from when it is shown to the user as a result for a particular query. In other words, much of the information we have available to decide on the relevance of an image to a given query depends heavily on the textual content and metadata of its Referral Page , and its power to accurately describe what the image depicts. This means that, when considering implicit user feedback, we should also consider the relevance of the image referral page since, when a user interacts with an image re-sult, they are not only validating the relevance of the image to the query, but also indirectly validating the relevance of its Referral Page to that same query.
Work on user expectations for relevance in image search in commercial web search engines has shown that, in addition to topical relevance , users consider image quality , or attrac-tiveness , to be of paramount importance. Geng et al [7] have shown that all of the top requirements for image search re-sults are directly related to attractiveness or quality:  X  X igh quality X ,  X  X olorful X ,  X  X harp X ,  X  X eautiful X ,  X  X ppealing X ,  X  X ivid X . These aspects were actually rated as more important than relevance although, as the authors note, this is likely due to the design of the study, where the users were presented with the top results from commercial search engines. In that setting, there were unlikely to be any problems with topical relevance , and so these users did not identify this as a major issue. Nevertheless, that work clearly emphasizes that image quality and attractiveness are, alongside topical relevance, the most important user requirements in image search. For this reason, in this paper we will focus our evaluation of image search ranking algorithms on topical relevance and image quality (we use quality in the general sense, to cap-ture objective image quality, beauty and appealingness).
In this section we introduce new implicit relevance feed-back features for Image Search, first describing features ex-tracted from the Search Results Page and the image Preview Page , and then describing how we propagate these features to the Referral Page level and the query-independent level.
The standard way to exploit user behavior in web search is to use the click-through rate (or click probability) on the Search Results Page (SRP). This is calculated by di-viding the number of times an image has been clicked for a given query by the number of times it has been displayed, or viewed, for that query:
In Section 3.1 we also described how some web image search engines also support a  X  X over X  event, where leaving the mouse hovering over an image for a number of seconds results in an enlarged version of the image being displayed (see Figure 1). Park et al. [19] have shown that image search interfaces elicit 8 to 10 times more hover interactions than clicks, and that the images that are hovered on are moder-ately to highly correlated with the images that are clicked on. This suggests that, although potentially more noisy than clicks, this noise could be compensated by the larger volume, and hovers could be a strong additional signal for relevance. To exploit the interaction for improving relevance, we pro-pose the hover-through rate (HTR) feature, as follows:
Also, in many sessions, the display of an enlarged  X  X over X  image may be followed by a standard click on the image, if the user wants to see a bigger version of the image in the Preview Page 2 . We can interpret this interaction as the user first inspecting the enlarged images for relevance and, if they judge this image as being relevant, they then click through to the image Preview Page . Based on this, we also propose a new feature, which we call converted hover rate (CHR), the proportion of hover events that are followed by clicks the same session (i.e. hovers that are  X  X onverted X  to clicks):
From the image Preview Page , we derive two types of features: (i) features based on clicks from the image Pre-view Page to the original Referral Page where the image is hosted, and (ii) features on the dwell time on images in this page. Interactions with this page can be very important, since users often interact with images in this view that they did not click on in the Search Results Page : one recent study [19] showed that, for each click on an image from the Search Results Page (which opens the Preview Page ), an average of 17 additional images are viewed by the user in the Pre-view Page via navigation using the previous/next buttons.. Given this, we propose that these interactions carry much additional implicit relevance information. All of the major U.S. search engines provide, in the Image Preview page, a link to the original referral page where the image was crawled from, allowing the user to view the image in its original context. We posit that this deeper method of
Note that, although these clicks may take place very soon after a hover, the search logging instrumentation is very reli-able at successfully logging, and distinguishing, both events. interaction should be strongly correlated with relevance, and that it can be used as a feature for image relevance. We can calculate the Referral Page Click-through Rate (RP-CTR), as follows:
Given that a number of images are consumed on each user visit to the Preview Page , we know the time when the user started viewing an image (when they clicked through from the Search Results Page , or when they accessed it via the next button), and when they finished viewing the im-age (when they clicked the next/ previous button, or exiting the Preview Page ). With this information, we calculate the amount of time that the user spent viewing each image, or the image dwell time . Previous work has shown that dwell time can be a strong indicator of user satisfaction for many tasks [16, 15], and it has been used successfully as a fea-ture to improve recommendation [29]. We hypothesize that larger dwell times on a individual images indicates a deeper interaction with results, reflecting higher quality and/or top-ical relevance. Once we know the dwell time on an individual image, we can calculate a number of features based on it. We calculate three main types of dwell time features (see Table 1 for a complete list):
To create our final representation of the dwell time fea-tures, we calculate the average value of each feature across all sessions. Normally, click-through data is calculated at the query-URL level. That is, when a user clicks on a result, the click is associated with the URL of that result. In web image search, this is the URL of the image result. In image search, however, each image is also hosted in the context of the re-ferral webpage where it has been shown, and this webpage can also exhibit relevance to the query; it could be an article or fanpage dedicated to the topic of the query, for example, and may contain multiple images relevant to the query. For this reason, unique to image search, there are two separate URLs that we can associate search result clicks with: the image URL and the referral page URL . In Sections 4.1 and 4.2 we proposed new features at the image URL level. We now propose to propagate these features to the referral page URL level or, equivalently, to associate clicks with the refer-ral page URL instead of the image URL . This should cap-ture referral page relevance information, and also increase coverage of the implicit feedback features for images which have not been clicked, but for which other images from the same Referral Page have been clicked. So, in Equations 2, 3 and 4, this simply means replacing ( image url ,query ) with ( referral page url, query) . Similarly, for this version of the dwell time features, we associate the features with the refer-ral page URL rather than the image URL .

It is also possible to ignore the query completely, and con-sider the user behavior features at the query-independent level. Unlike propagating to the Referral Page level, this is not strictly unique to image search. However, we propose that query independent features will capture the quality of images, independent of topical relevance. As discussed in Section 3, Geng et al. [7] previously showed that image attractiveness or quality very important for image search. Given this, we posit that propagating click features to the query independent level will capture implicit user feedback about image quality, and so we also calculate all of our pro-posed features independent of the query.

In summary, we have three levels of propagation for the user behavior features:
We take the complete search engine log data, covering all queries across all device types (mobile, desktop, etc) from the Yahoo search engine, covering a 6 month period from July 2014 to December 2014 3 . From this log, we extract all user interaction events from the Search Results Page and the image Preview Page .

We extract the following fields from each entry in the log: session id, timestamp, query string, anonymous user identi-fier, page type, and event type (i.e. pageview, click). For pageview events we extract the URLs of all thumbnail result images (and their referral pages) displayed on the page. For click events, we have information about the type of click (e.g. click, hover) and the URL of the clicked images and their referral pages. We then aggregate this data and calculate all of the features described in Section 4 for all query-url pairs, query-referral page pairs, and all image urls in the data.
To create a corpus that we can use to train and evaluate web image search ranking models, we take a traffic based query sample. That is, we calculate the query volume (the number of times the query was issued) for each query in the logs, and sort the queries by volume. Since in this work we are interested in head and torso queries, for which user be-havior information is available in the log data, we restrict our corpus to queries from the top 50% of this traffic distri-bution (i.e. the most popular queries, covering 50% of the overall traffic volume), taking as our sample a total of 9,272 queries from the top 50% of queries.
The two main facets of relevance for web image search are topical relevance and image quality/attractiveness [7]. In or-der to measure each of these facets separately, for candidate image URLs for each query, we gather separate judgements for topical relevance and quality. All of these annotations are carried out by professional editors, trained to use the topical relevance and quality scales outlined below.
For topical relevance , we gather judgements on a 3 point-scale:
For image quality, we gather judgements on the following 5-point scale:
We do not reveal further details about this initial dataset (e.g. size, etc) as such information is commercially sensitive.
Finally, we create a single combined relevance + qual-ity judgement that incorporates both topical relevance and quality, and maps to a standard 5-point PEGFB scale (Per-fect, Excellent, Good, Fair, Bad). We do this using a simple heuristic which gives precedence to topical relevance:
To avoid biasing our sample towards the features of the images ranked highly by the ranking algorithm used, we take a sample of a 15 images per query from the top 500 results of a baseline ranking algorithm, and have these annotated by editors. After this initial round of annotation, a prelim-inary analysis showed that some frequent queries had few or no negatively annotated (i.e. moderately/non relevant) images. For these cases, to ensure a good balance of annota-tions in our corpus, we identified queries for which the initial annotations suffered from this bias, and sampled additional image URLs to be annotated for these queries. This gave a variable number of image URLs annotated per query, result-ing in a total of 221,920 annotations over the 9,272 queries in our sample, an average of almost 24 image URLs per query, with the labels (from the combined mapping) distributed as follows: 1,195 Perfect , 46,926 Excellent , 30,571 Good , 67,669 Fair and 75,559 Bad .
Using the corpus described in the previous subsection, Ta-ble 2 shows the coverage of each feature in terms of the per-centage of judged query-url paris in the corpus for which we have a non-null value, or a positive (i.e., non-zero) value. In order to remove noise from sparse data, when calculating the implicit feedback features, we treat them as undefined if they are based on less than a minimum number of im-pressions in the Search Results Page or the Preview Page (we set the threshold at 20). In Table 2, a non-null value indicates that the corresponding feature can be calculated, whereas a null value means there are too few impressions to calculate the feature reliably, so we treat it as undefined (for the feature vectors used in MLR models, undefined features are assigned the value -1). While the absolute coverage val-ues are influenced by the amount of log data available for processing and by the method used to sample image urls for each query, we are mostly interested in the relative gain in coverage from adding new features and from propagating those features, which should be quite robust against these factors. The coverage of HTR is slightly lower than that of CTR due to the fact that the impressions accounted for the former are desktop-only, since there is no hover action on mobile. The coverage of the Dwell Time (DT) and the RP-CTR features is based on the number of impressions on the Preview Page , which must always be preceded by a click on the Search Results Page . DT has slightly lower coverage because the dwell time can not be computed for those im-which the feature has non-zero values. ages viewed at the end of the session (e.g., if the users closes the browser or navigates away from the Preview Page ). The non-zero coverage shows the proportion of query-URLs in the corpus with non-zero values for this metric: that is, there has been at least one click, so this is poten-tially a positive relevance signal (the non-zero coverage does not apply to the DT features, since by definition the dwell time, if defined, will always be non-zero). Here HTR has a higher coverage than CTR, despite of the fact that it does not include interactions on mobile devices. We can also see that the overall coverage (last row, calculated as proportion of the corpus where at least one of the features is non-zero) increases when we consider all implicit feedback features, and that corpus coverage increases significantly when prop-agating features. In fact, we see that the coverage increases for all the features with the adoption of the propagation schemes, and it doubles at the query-independent level com-pared with the query-URL level. This is important because is shows that by adding new features, and propagating them, the number of image URLs that can potentially benefit from these features increases significantly. This also emphasizes how the introduction of image specific user feedback signals can better capture the peculiarities of user behavior in image search, compared with generic web search.
To evaluate our proposed features we adopt a learning to rank framework. We employ the state of the art GBRank ranking algorithm [31], which uses a pairwise loss function based on Gradient Boosting Regression Trees (GBDT), as our ranking function. We convert the combined relevance + quality judgements to a numeric learning target as follows: Perfect -4, Excellent -3, Good -2, Fair -1, Bad -0.
We calculate over 2,000 features as input to the learning algorithm to train two separate baseline models, B1 and B2 . We evaluate the effectiveness of our proposed features by adding them as additional features on top of each of the baselines, and measure their effect on search ranking. We compare against the first baseline to confirm that our pro-posed features carry significant relevance information, and against the second to show that our methods can outperform a strong baseline based on standard click features: Table 3: Basic statisistics for the Web Image Search training and evaluation corpus.
We use Normalized Discounted Cumulative Gain (NDCG) as the metric to assess search relevance performance. NDCG has been widely used to assess relevance in the context of search engines [11]. For a ranked list of N documents, Dis-counted Cumulative Gain (DCG) is calculated as follows: where G i represents the weight assigned to the label of the document at position i , i.e., 10 for Perfect, 7 for Excellent, 3 for Good, 0.5 for Fair, and 0 for Bad. NDCG normal-izes DCG by dividing it by the ideal DCG that would be significantly better than B2. obtained from a perfect ranking: NDCG has the advantage that it is easier to interpret, in that its score always falls between 0 and 1, with 1 indicating a perfect ranking. We use the symbol NDCG to indicate the average of this value over a set of testing queries in our experiments. In the fol-lowing sections, we will report NDCG1, NDCG3, NDCG5 and NDCG10, referring to NDCG calculated for the top 1 ranked document only (NDCG1), the top 3 ranked docu-ments only (NDCG3), etc. For our main evaluation metric, we calculate NDCG based on the combined relevance + qual-ity scale. For a supplemental evaluation, we use the topical relevance labels only, on a 3-point scale, where we map rele-vant to Good, moderately relevant to Fair, and non-relevant to Bad, and calculate NDGC based on this.

Taking the set of annotated queries described in Section 5.1, we partition them into training, tuning and testing sets. The partitioning is query-based (i.e., all judgements for a given query will be assigned to the same partition) as fol-lows: 80% training, 5% tuning, and 15% test. More details of the evaluation corpus can be found in Table 3. The train-ing partition is used to train the models, the tuning partition is used to conduct a parameter search for the optimal param-eters of the GBRank algorithm (i.e., the parameter set with the best NDCG score on the tuning partition is selected), and then the model trained with these optimal parameters is tested against the test set to produce our evaluation results. The NDCG results for Relevance+Quality are shown in Table 4. Firstly, we can see that all of the proposed features, except for the RP-CTR, give significant improvements over B1 . For NDCG5, for example, this improvement is 8.2% for the HTR, 2.5% for the CHR, and 2.8% for the DT, and all of these differences are highly statistically signifi-cant ( p &lt; 0 . 01). The B2 + HTR model outperforms B2 by over 2% for NDCG1, showing a highly significant im-provement over B2 for NDCG3, NDCG5 and NDCG10. In fact, all of the proposed new features show at least signif-icant improvements ( p &lt; 0 . 05) over B2 of around 1% for Table 5: Relevance + Quality NDCG Results for Image Search with and without hover-based Implicit Feedback features.  X  -significantly better than B1.  X  X  -highly significantly better than B1.  X  : signifi-cantly better than B2.  X  X  : highly significantly better than B2.
 NDCG3, highly significant improvements of around 1% for NDCG5, and smaller, but highly significant, improvements for NDCG10. It is also noteworthy that the RP-CTR, which did not improve over B1 , achieves improvements over B2 that are comparable with the other proposed features. We hypothesize that the lack of coverage for this feature means that it is not a reliable signal for relevance in the absence of other click data, but since it provides complementary infor-mation to Search Results Page clicks, this feature can im-prove relevance in combination with other features. We can also see that the combination of all features gives the best overall performance, even though the improvement gained by adding each of the individual features to B2 is relatively minor.

These results show that the proposed features can give sig-nificant improvements over the strong, state of the art base-line (B2) although the improvements are relatively small at approximately 1% or less for most metrics, with the excep-tion of NDCG1 which shows an improvement of over 2%. Looking at the results for the propagated features, we get further improvements, with all metrics showing highly signif-icant improvements over B2 of 2% or greater. This confirms that propagating the click features also gives important im-provements for image search relevance, and the fact that these improvements can be seen at a greater ranking depth (NDCG10) also shows that these features will give more ro-bust ranking at depth, which is important in image search, where users explore search results much more deeply than in web search, often examining several pages of results [2, 19].

To understand which among the 25 proposed dwell time features are most useful, we take the relative importance score of each feature in the best B1 + DT model, which is calculated by accumulating decrease of loss of each tree splitting point [6]. From this, we find that the following, in order of importance, are the most important dwell time features for this model: gtMedian , DT z-score , DT/Mean , DT/Total , DT/Median . Four of these features are nor-malised features, and one is a categorical features. These results seem to confirm that our proposed normalized and categorical dwell time features are important if we are to make the best use of dwell time information.
We use NDCG for Relevance+Quality as our main eval-uation metric because we believe that it captures topical relevance while also boosting the scores for methods that give higher rank to the most attractive, high-quality im-ages. However, to verify that the improvements in quality do not come at the expense of topical relevance, we also re-port, in Table 6, results for topical relevance only NDCG. The results in Table 6 show very similar improvements over the baselines: the best method shows around 1.5% improve-ment for NDCG1, NDCG5 and NDCG10, and this improve-ment is highly significant for NDCG5 and NDCG10. These results demonstrate that, whether we focus on topical rele-vance only, or relevance + quality , our proposed features for image search ranking give non-trivial, and highly significant improvements over a strong baseline.
At the time of the study, two of the three major U.S search engines had a  X  X over X  interaction in their web image search SRP interface. Since then, one of these removed this feature, meaning that only one of these three search engine now sup-ports this interaction, which raises questions as to whether the results reported so far in this paper are generalizable, or whether they are dependent on transient user interface features. The other web image search user interaction fea-tures highlighted in Section 3 on which our proposed features are based, however, have been stable features of all of these search engines for a number of years now, and this can be expected to continue. Table 5 shows results for our meth-ods without the hover-based features, as the results without these features are based on more stable, and therefore more generalizable, features. We can see that removing the hover-based features has a relatively small impact on the NDCG; the improvement over the baseline is still more than 2% (and statistically significant) better than B2 for NDCG1, and it is highly significantly, and over 1.5% better, for NDCG5 and NDCG10.
In this paper we have proposed a number of novel user be-havior features for improving web image search ranking: (1) hover-through rate based on  X  X overs X  in the Search Results Table 6: Topical Relevance NDCG Results for Im-age Search with Implicit Feedback features.  X  -sig-nificantly better than B1.  X  X  -highly significantly better than B1.  X  : significantly better than B2.  X  X  : highly significantly better than B2.
 Page ; (2) click-through from the image Preview Page to the Referral Page where the images are embedded; (3) dwell time features from the image Preview Page ; and (4) propa-gating user behavior features to the query-referral page and the query-independent level. We created a large scale cor-pus to evaluate the usefulness of these features in a learning to rank framework, and show that the new features achieve highly statistically significant improvements of over 2% be-yond a strong, state of the art baseline based on standard click-based features.

While this study focused on image search, many of the user interactions discussed here are shared by web video search interfaces, and so we plan to explore the usefulness of the same features for video search. Also, the latest image search interfaces now support clicking through from the Pre-view Page directly to a full size view of the image. This was not available at the time of our study, but we expect that it will provide similar relevance information as click through from the Preview Page to the Referral Page , and plan to explore this in the future. [1] E. Agichtein, E. Brill, and S. Dumais. Improving web [2] P. Andre, E. Cutrell, D. S. Tan, and G. Smith. [3] M. Bilenko and R. W. White. Mining the search trails [4] N. Craswell and M. Szummer. Random walks on the [5] N. Craswell, O. Zoeter, M. Taylor, and B. Ramsey. An [6] J. H. Friedman. Greedy function approximation: A [7] B. Geng, L. Yang, C. Xu, X.-S. Hua, and S. Li. The [8] J. Huang, T. Lin, and R. W. White. No search result [9] V. Jain and M. Varma. Learning to re-rank: [10] B. J. Jansen, A. Spink, and J. O. Pedersen. The effect [11] K. J  X  arvelin and J. Kek  X  al  X  ainen. Cumulated gain-based [12] T. Joachims. Optimizing search engines using [13] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and [14] D. Kelly and N. J. Belkin. Display time as implicit [15] Y. Kim, A. Hassan, R. W. White, and I. Zitouni. [16] C. Liu, J. Liu, N. Belkin, M. Cole, and J. Gwizdka. [17] S. Maniu, N. O X  X are, L. Aiello, L. Chiarandini, and [18] D. Metzler and W. B. Croft. A markov random field [19] J. Y. Park, N. O X  X are, R. Schifanella, A. Jaimes, and [20] S. Robertson and H. Zaragoza. The probabilistic [21] G. Smith and H. Ashman. Evaluating implicit [22] Y. Song, H. Ma, H. Wang, and K. Wang. Exploring [23] M. Trevisiol, L. Chiarandini, L. M. Aiello, and [24] T. Tsikrika, C. Diou, A. P. de Vries, and [25] C. Wang, Y. Liu, M. Zhang, S. Ma, M. Zheng, [26] R. W. White and J. Huang. Assessing the scenic [27] S. Xu, H. Jiang, and F. C. M. Lau. Mining user dwell [28] S. Xu, Y. Zhu, H. Jiang, and F. C. M. Lau. A [29] X. Yi, L. Hong, E. Zhong, N. N. Liu, and S. Rajan. [30] J. Yu, D. Tao, M. Wang, and Y. Rui. Learning to rank [31] Z. Zheng, K. Chen, G. Sun, and H. Zha. A regression
