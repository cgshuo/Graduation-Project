 Conversational Recommendation mimics the kind of dialog that takes between a customer and a shopkeeper involving multiple interactions where the user can give feedback at every interaction as opposed to Single Shot Retrieval , which corresponds to a scheme where the system retrieves a set of items in response to a user query in a single interaction.
Compromise refers to a particular user preference which the recommender system failed to satisfy. But in the context of conversational systems, where the user X  X  preferences keep on evolving as she interacts with the system, what consti-tutes as a compromise for her also keeps on changing. Typ-ically, in Single Shot retrieval, the notion of compromise is characterized by the assignment of a particular feature to a particular dominance group such as MIB (higher value is better) or LIB (lower value is better) and this assignment remains true for all the users who use the system.

In this paper, we propose a way to realize the notion of compromise in a conversational setting. Our approach, Flexi-Comp, introduces the notion of dynamically assigning a feature to two dominance groups simultaneously which is then used to redefine the notion of compromise. We show experimentally that a utility function based on this notion of compromise outperforms the existing conversational rec-ommenders in terms of recommendation efficiency.
 H.3.3 [ Information Search and Retrieval ]: Information filtering Algorithms, Performance, Experimentation Recommender Systems; Personalization; Knowledge Based Recommendation; Conversational Recommenders; Compro-mise
In this day and age of huge data floating on the web, with a large number of choices available to the user, recommender systems play an extremely important role in matching users to products or items that they might find interesting. The scenarios where the user does not have much domain knowl-edge and only has a vague idea of her preferences present unique set of challenges for recommender systems. In such situations, the preferences of the user keep on evolving in light of the available options that are shown to him. Typi-cally, knowledge based approaches to recommendation use a weighted similarity model to compute the similarity between two products or between a product and a query. These sys-tems calculate utility of a product with respect to a given query(or product) by using a weighted linear combination of the local similarities between the query(or product) and the product concerned( weighted similarity model ) [5]. These local similarities are usually defined by experts or could be learned from the data as well. where localsim ( P i ,Q i ) is the local similarity between the i attributes of products P and Q and w i refers to the weight assigned to the i th attribute. In this way, these systems use similarity as a proxy for utility of a product.

Knowledge based approaches to recommendation can be further divided into -Single Shot Retrieval and Conversa-tional Recommendation [7]. Single Shot Retrieval systems use various kinds of extensions to the utility function de-scribed in Eq. 1 to return a set of items in response to a user query in a single user interaction. If a user is unhappy with the shown items, she has to tweak her query to get a different set of items. Single Shot Retrieval systems as-sume that the user has a fair idea of what she wants, which might not hold true always. As an alternative to Single Shot Retrieval systems, Conversational Systems engage the user in an extended dialog, showing her a set of items and then seeking her preference information by allowing her to give feedback on the shown items. Thus, for novice users who do not know much about their preferences upfront (do not know much about the domain), conversational recommender sys-tems provide them with an effective way to navigate through the product space, educating them on the way.
 In the realm of Single Shot systems, Compromise Driven Retrieval (CDR) [4] was proposed, which introduced the no-tion of compromises to better approximate the utility of a product for a user. Compromises are defined as the pref-erences of the user that the recommender system fails to satisfy [3]. To identify if a particular feature has been com-promised or not, dominance criteria like MIB (More is Bet-ter) and LIB (Less is Better) are used. For example -let us consider Price of a camera to be an LIB feature and assume that a user specifies  X  X  X  as her preferred Price . If a prod-uct P in the database has Price greater than  X  X  X , then we say that P involves a compromise across the Price feature with respect to the query. CDR ranks those products higher which are maximally similar to the specified query and in-volve the minimum number of compromises. But CDR has this inherent assumption that the user is completely sure of her preferences which she can sufficiently specify upfront, which is seldom the case. To our knowledge, there has been little work done to model compromises in a conversational setting. As we discussed earlier, when the user only has a vague idea of what she wants, her preferences evolve as she navigates through the product space. In such a scenario, what constitutes as a compromise for the user, would keep on varying with her changing preferences.

In this paper, we propose Flexi-Comp, a flexible and dy-namic approach to model compromise in a conversational recommendation setting. Specifically, we use the product selected by the user at every iteration from among k shown products, to infer the changing notion of compromise for the user (discussed in Section 3). We introduce the notion of a particular feature belonging to both  X  X ominance X  groups -MIB and LIB simultaneously, with varying degree of mem-berships. This simultaneous belongingness of a feature to both MIB and LIB helps redefine the notion of compromise for the user at every interaction with the system.
Compromise driven retrieval (CDR) [4], a single shot re-trieval scheme, defines a notion of compromise to better ap-proximate the utility of a product for a user. Apart from the differences already mentioned in the Section 1, another difference between the two approaches is that Flexi-Comp takes into the extent of compromise into account as well. While in CDR, Price will always be treated as LIB, in our approach, the degree of belongingness of Price to both MIB and LIB represents the user X  X  confidence in going with LIB as opposed to MIB and vice versa. Various conversational approaches to recommendation differ in the way they elicit user feedback. Flexi-Comp is a conversational recommenda-tion strategy which uses Preference Based feedback(PBF) as the mode of eliciting user feedback. In PBF, the user is shown k items at every interaction from which she ex-presses her preference by selecting one and rejecting the oth-ers. More Like This (MLT) [2] is a commonly used strategy based on Preference Based Feedback in which the user selects a product P in each recommendation cycle. In the next cy-cle, treating P as the new query, those products are shown to the user which are most similar(according to weighted similarity model ) to P . Our approach differs from MLT in that it provides a way to characterize compromises in a con-versational setting. wMLT [2], another approach based on PBF, dynamically weighs attribute values while interacting with the user, based on the difference of the attribute value of the selected product to those of the rejected products. Adaptive Selection (MLT-AS) [8], another Preference Based Feedback based conversational approach uses an explicit di-versity component on top of weighted similarity model and preference carrying mechanism to generate effective rec-ommendations. Flexi-Comp, on the other hand does not include an explicit diversity metric in its utility computa-tion and uses a mechanism to characterize the compromises which contributes to its superior performance over MLT-AS which uses an explicit diversity metric.
We explain how our approach, Flexi-Comp, generates effi-cient recommendations with the help of an example. Let us assume that a user is shown products 1,2,3,4 from Table 1 at a particular iteration and she chooses product 2. We can use this selection information to infer the degree of mem-bership of each feature to the two groups -MIB, LIB. Let the degree of membership of a feature f to MIB be denoted by m f and its degree of membership to LIB be denoted by l . Notice that products 1 and 3 have a Price value lesser than that of selected product 2, whereas product 4 has a higher Price than product 2. Hence, when we look at prod-uct 2 X  X  Price in comparison with products 1 and 3 X  X  Price , we might conclude that Price belongs to the group MIB with 2/3 confidence ( Price of the selected product is more than 2 out of the 3 rejected products). Similarly, comparing product 2 X  X  Price with that of product 4, gives a confidence of 1/3 ( Price of the selected product is less than 1 out of the 3 rejected products) to Price belonging to the LIB group. The total amount by which Price belongs to the MIB group can then be given by: and to LIB group can be gives as where P i denotes the Price of product i. A similar procedure is then followed to calculate m f and l f for all the numeric features f . We can see that to calculate m f and l feature f requires an ordering over all possible values of f , which makes it difficult to work for nominal features where there is no clear cut ordering as there is between numbers. The procedure to calculate m f and l f for a nominal feature f is discussed below.

Assume we want to define an ordering between two val-ues f 1 and f 2 of a particular nominal feature f . Let us say that f 1 and f 2 belong to two products P 1 and P 2 respec-tively. The basic intuition of our approach to order nominal features is -if two products are sufficiently similar across all the features barring f and Price , then the f 1 is greater than f 2 if the Price of P 1 is lesser than the the Price of P Let us assume that the feature set associated with prod-ucts in a database be given by F. For example -for products shown in Table 1, F = { Price , Resolution , Manufacturer } . Let us suppose that we want an ordering over two of the values( f i and f j ) of a nominal feature f . Let F c denote the set which consists of all the elements in F except Price and f itself. Therefore for the database shown in Table 1, if we want an ordering over two Manufacturer values, then F c = { Resolution } . Now, the amount by which a particular nominal value f i (value corresponding to nominal feature f for the i th product) is greater than another nominal value f can be calculated as: greater ( f i ,f j ) = ( X where P i and P j are the Price values of product i and j respectively. w g value for each feature g is a number between 0 to 1 denoting the importance associated with that feature and all the w g values sum up to 1. Suppose we want to define an ordering between Kodak (product 5) and Sony (product 4) in Table 1. Since the Price of product 4 is lesser than the Price of product 5, therefore, Sony (S) is greater than Kodak (K) by an amount equal to : assuming weight associated with Resolution to be 1. Once we define an ordering over the nominal features in this way, it is easy to define m f and l f for nominal feature f in a way similar to that described in Eq. X  X  2 and 3. In this way, we are able to define membership of a particular feature simultaneously to the two groups of MIB and LIB, rather than having it X  X  membership confined to only one kind of dominance group at a time. What usually happens in a rec-ommender system is that, there is a fixed association of a feature to a dominance group. For example -Price usually belongs to the LIB (Less is Better) group, implying that a product P i having lesser Price than another product P j has a higher utility than P j , given all the other attributes remaining the same for both the products. Thus, a prod-uct having a higher Price than the query value is said to have been compromised and is given a negative utility (w.r.t Price ), whereas a product having a lesser Price than the query value gets a positive utility w.r.t. to Price . Hence the utility of a product with respect to a reference product or a reference query depends on the dominance group (MIB or LIB) it is associated with. Assigning a feature to a par-ticular dominance group is done by experts who bring with them a wealth of domain specific  X  X ominance X  knowledge. Flexi-Comp, on the other hand, does not require domain specific  X  X ominance X  knowledge as it implements a flexible membership to each dominance group based on the selection of a product by a user at every iteration; not a static notion of a feature belonging to a particular dominance group.
The next step in the recommendation process is to es-timate the utility of all the products in the database and present the top-k products having the highest utility in the next iteration to the user. Let us suppose that the user has selected product S in the current iteration, then we can cal-culate the values of m f and l f for all features f belonging to S as discussed earlier. The utility of a product P with respect to the new query S is then estimated as: Utility ( P ) = (  X   X  weightsim ( S,P )) + (  X   X  pull ( S,P )) (6) This process of utility estimation takes place at every inter-action and keeps going on until the user settles on a product of her choice. Here weightsim ( S,P ) is calculated according to the weighted similarity model where the weights at ev-ery interaction with the user are adjusted according to the wMLT algorithm [2]. pull ( S,P ) corresponds to the pull ex-erted by product P on S . The higher the pull, the higher the utility of product P . pull ( S,P ) is estimated as: pull ( S,P ) = where F is the set of all features associated with the prod-ucts. F for products in Table 1 is -F = { Price , Resolution , Manufacturer } . S f and P f refer to the value of feature f in product S and P respectively. grt ( S f ,P f ) = where N f is a normalizing constant for feature f . Note the use of grt ( S f ,P f ) for the case when MIB and grt ( P when LIB in Eq. 7. In the case where Less is Better (LIB) for feature f , grt ( P f ,S f ) returns a positive value for a value S f lesser than P f which is desirable. The value of  X  and  X  was set to 0.5 in our experiments to have equal contributions from both the sources of utility. In the actual implementa-tion of our system, we take care of the history of previous selections made by the user in coming to the current selected product. Given S is the currently selected product, to esti-mate the utility of a product P , the equation for pull ( S,P ) actually used is pull ( S,P ) = where k represents the interaction cycle and S f k represents the feature value of feature f in the product which was se-lected in the k th iteration. At the k th interaction cycle, we take into account the user selections in the cycles prior to the current cycle. The factor b k is used to control the con-tribution of the previous interactions in utility estimation. Higher value of k implies higher value of b k . Similar history profiling is done for the weightsim term in Eq. 6. As was discussed earlier, the values m f and l f have a role to play on defining the notion of compromise. When we say that attribute f has been compromised.
We compare Flexi-Comp with well known preference based approaches -MLT [2], wMLT [2] and MLT-AS [8]. We use two standard datasets in our experiments -Camera 1 and PC [2]. The Camera dataset contains 210 cameras with 10 attributes and the PC dataset contains 120 PC X  X  with 8 attributes. We use a leave one out methodology simi-lar to the one used in [8], where each product is removed from the products database and used to generate a partic-ular query of interest. For each query, that product is con-sidered as target, which is most similar (according to the weighted similarity model ) to the product from which the query is generated. We report in Tables 2 through 5, the average number of cycles and unique products presented to a simulated user en route a target product during a recom-mendation dialog [1, 8]. From the standpoint of cognitive load experienced by a user, an ideal algorithm for conversa-tional recommendation would incur less number of cycles as well as show fewer unique items in the process of reaching a target item. For both the datasets, we generated queries of length 1, 3 and 5 to distinguish between difficult , mod-erate and easy queries respectively [6]. We refer to queries of length 1, 3 and 5 as Q-1, Q-3 and Q-5 respectively. Q-1 is considered to be difficult because it is an underspecified query and gives us very little information about the target item as opposed to Q-5 which gives reasonable amount of information about the target. Also, Q-1 can be seen to rep-resent a user who does not know much about the domain and hence is not able to specify her need sufficiently. On the other hand, Q-5 can be seen to represent a user who knows quite a lot about the domain and is therefore able to specify her information need sufficiently. A total of 3938 and 2382 queries were generated for the Camera and PC dataset respectively. In all our experiments, 4 products are presented in every recommendation cycle. Also, in all the algorithms, the simulated user selects that product in every cycle (among the 4 shown) which is most similar (governed by the weighted similarity model) to the target.
 Algorithm/Approach Q-1 Q-3 Q-5 MLT 11.705 7.035 3.54
MLT-AS 7.82 5.19 2.99 wMLT 6.66 3.95 2.56 Flexi-Comp 4.86 3.37 2.295 Algorithm/Approach Q-1 Q-3 Q-5 MLT 22.145 14.23 8.59
MLT-AS 16.85 11.69 7.92 wMLT 19.268 12.21 8.52 Flexi-Comp 14.94 10.88 7.86 Algorithm/Approach Q-1 Q-3 Q-5 MLT 30.4 19.17 15.74
MLT-AS 16.84 12.9 10.91 wMLT 15.22 9.87 7.6 Flexi-Comp 7.87 5.51 4.77 Table 5: Avg. Unique Items -Camera Dataset Algorithm/Approach Q-1 Q-3 Q-5 MLT 46.76 30.04 25.19
MLT-AS 27.53 20.84 17.97 wMLT 39.75 27.28 21.63 Flexi-Comp 23.04 16.23 14.18 For query Q-1 on the PC dataset, Flexi-comp achieves a re-duction of 58.47% in terms of number of cycles as compared to MLT, whereas MLT-AS and wMLT achieve a reduction of 33.19% and 43.1% respectively. This shows that when the user does not have much of domain knowledge, Flexi-Comp is better suited to guide her towards an acceptable product as compared to the other algorithms. An interesting aspect which shows the robustness of Flexi-Comp is that wMLT, which is closest to Flexi-Comp in terms of average number of cycles on the PC dataset, shows a high number of unique items en route the target(19.268 as shown in Table 3). But Flexi-Comp is able to show fewer unique items indicating that it is adept in reducing both the number of cycles and the number of unique items simultaneously. A similar trend can be observed for the Camera dataset as well.
In this paper, we proposed Flexi-Comp, which character-izes the notion of compromises in a conversational setting. Instead of pre-assigning each feature to a particular domi-nance group (like MIB or LIB), and maintaining that assign-ment for each type of user, we proposed a flexible and dy-namic notion of dominance, wherein a particular feature can be simultaneously assigned to both the dominance groups with varying degrees of membership. This dynamic notion of membership to each of the different dominance groups(which gets updated at every user interaction) helps in redefining what constitutes as a compromise for the user and feeds into the overall utility function for the user. Our general notion of compromises can be adopted by other approaches. We show that Flexi-Comp which takes care of compromises in this way outperforms the existing conversational approaches based on Preference Based Feedback. [1] M. S. Llorente and S. E. Guerrero. Increasing retrieval [2] L. McGinty and B. Smyth. Comparison-based [3] D. McSherry. Coverage-optimized retrieval. In [4] D. Mcsherry. Similarity and compromise. In In [5] F. Ricci, L. Rokach, and B. Shapira. Introduction to [6] M. Salam  X o, J. Reilly, L. McGinty, and B. Smyth. [7] B. Smyth. Case-Based Recommendation. In The [8] B. Smyth and L. Mcginty. The power of suggestion. In
