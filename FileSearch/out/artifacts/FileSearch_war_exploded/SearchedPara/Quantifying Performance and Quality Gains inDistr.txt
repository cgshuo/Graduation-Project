 Distributed search engines based on geographical partition-ing of a central Web index emerge as a feasible solution to the immense growth of the Web, user bases, and query traf-fic. However, there is still lack of research in quantifying the performance and quality gains that can be achieved by such architectures. In this paper, we develop various cost mod-els to evaluate the performance benefits of a geographically distributed search engine architecture based on partial in-dex replication and query forwarding. Specifically, we focus on possible performance gains due to the distributed nature of query processing and Web crawling processes. We show that any response time gain achieved by distributed query processing can be utilized to improve search relevance as the use of complex but more accurate algorithms can now be en-abled for document ranking. We also show that distributed Web crawling leads to better Web coverage and try to see if this improves the search quality. We verify the validity of our claims over large, real-life datasets via simulations. H.3 [ Information Systems ]: Information Storage and Re-trieval Experimentation, Performance Data centers, distributed query processing, index partition-ing, Web crawling
Designing a large-scale Web search engine requires many decisions to be made. These decisions are affected mainly by the users of the search engine, queries submitted, evolution of the Web, and trends in hardware. The goal in a successful design is to have a high-performance search engine that sat-isfies user needs in terms of the quality of presented results. In this context, performance typically refers to achieving high query processing throughput with low response times. Quality refers to precision and recall.

Achieving high performance and result quality becomes more challenging as the Web grows and evolves, user bases and query traffic increase, distribution of users and hence queries become geographically more diverse, and user ex-pectations get higher. Scaling with all these ever-growing factors necessitates decentralization of the search process [1]. Decentralization can be achieved by distributing resources of a central search engine over multiple, geographically dis-tant data centers. However, distributing the search process brings new design issues such as the optimum number and placement of data centers, assignment of crawling, index-ing and query processing tasks to data centers as well as clustering and mapping of user queries to data centers.
In this work, we adopt a geographically distributed search architecture [3, 14], based on partial index replication and selective query forwarding between search sites. In this ar-chitecture, unlike centralized search engines, where the index is stored in a single, large data center, the index is parti-tioned into smaller, non-overlapping parts and distributed over multiple, geographically distant search sites. Partition-ing of the index is in compliance with the geographical lo-cality of data centers (e.g., a data center in France crawls and indexes documents in Europe). Additionally, popular or commonly accessed parts of the index are replicated on all search sites. Query processing is also different than central-ized search engines, where all user queries are submitted to a single data center. In this architecture, users are mapped to local data centers according to their geographical local-ity (e.g., European queries are submitted to the data center in France). Local sites may forward queries to other search sites that are likely to contain relevant results for the query.
The search architecture we discuss differs from federated search architectures [5, 6] in the literature in a number of ways. First, the search sites are not autonomous as it is typically the case in federated search, but are parts of a single, large, collaborative system. Second, resources and algorithms are more homogeneous, e.g., all sites execute the same ranking algorithm. Third, there are infrastructural advantages, e.g., we can have a dedicated network between search sites. Fourth, distribution of content in data centers is explicitly managed, i.e., there is an offline mechanism for distributing the content over the search sites.

A related but still different architecture is P2P networks [4, 15]. A geographically distributed search engine is different than a P2P system in a number of ways. First, data cen-ters are static and have high availability, unlike the peers that are quite dynamic and volatile in P2P search. Second, the number of data centers is not very high (e.g., several), whereas the number of peers can grow drastically (e.g., mil-lions) in P2P. Third, P2P systems are low cost systems, but constructing data centers can be quite expensive and hence require more intelligent design choices to be made.
The focus of this work is on performance and quality gains achievable by geographically distributing the search and crawling processes. In Section 2, we describe a tax-onomy of geographically distributed search architectures. Later, we investigate the validity of the following claims:
In this section, we briefly summarize the query evaluation process in search engines, introduce a taxonomy of search architectures, present our assumptions about resource allo-cation, and discuss the trade-offs in different architectures.
In large-scale search engines, due to tight response time requirements (e.g., a few hundred milliseconds), queries are processed on large parallel systems, i.e., clusters composed of many processors. As an offline process, an inverted in-dex [10, 16] is built over a Web page collection. This index is distributed over the processors in a cluster. For distribut-ing the index, two possible approaches taken are term-based or document-based partitioning [13]. In practice, due to bet-ter load balancing and higher index availability, document-based partitioning is adopted by almost every major search engine. In this approach, the entire Web collection is par-titioned such that each processor will have a sub-collection and construct a local index over it. To achieve concurrency and increase query throughput, search clusters are repli-cated. Also, results of popular queries and frequently ac-cessed parts of the index are cached in memory to decrease the query processing time.

Processing of a query on a search cluster follows several steps. A master processor receives the query and checks the result cache. If the query result is cached, the query is an-swered without any processing. Otherwise, the master sub-mits the query to all worker processors in the cluster. Each worker fetches from disk the inverted lists corresponding to query terms that are not already cached in memory. Due to strict processing time requirements, large-scale engines em-ploy a boolean AND model, i.e., inverted lists are intersected so that only the documents that contain all query terms are selected. Then, selected documents are scored using various techniques, and top-scored k documents are returned to the master. Finally, the master sorts all results in decreasing order of scores and returns the top k results to the user.
In query processing, there are two types of costs: a fixed cost independent of the number of processors and a cost linearly scaling with increasing processor count. The average processing time T can be represented by the simple equation where a is the fixed time cost, b is the time cost of pro-cessing a byte of an inverted list, I avg is the average size (in bytes) of an inverted list in the global index, and K is the number of processors in the cluster. If inverted lists are in memory, decompression of postings and list intersection forms the scalable cost. If inverted lists are stored on disk, query processing time is dominated by accessing the data on disk. In this case, the fixed cost is the cost of disk seeks and rotational latency while the scalable cost is the cost of transferring lists into memory. We will use equation (1) throughout the cost analysis in Section 3.2.
Based on the way the index is stored, we classify search engine architectures into three as centralized (SE-C), repli-cated (SE-R), and partitioned (SE-P). Fig. 1 illustrates these architectures. In the figure, inner, small boxes represent search clusters, and patterns represent different ways of in-dex replication/partitioning. Later, in this section, we show that these three architectures are special cases of a more general architecture based on partial replication and query forwarding (SE-H).

The centralized architecture (SE-C) has been heavily used in the past by large Web search engines and is still adopted by small-scale engines. In this architecture, the entire Web index is stored in one, central site. The index is replicated Table 1: Allocation of computational resources in different search engine architectures locally over multiple search clusters located within the site. All user queries are submitted to this single site, where each query is completely processed by an individual cluster.
The architecture based on index replication (SE-R) is cur-rently adopted by some major Web search engines. The entire index is replicated over multiple (e.g., several), geo-graphically distant data centers. The index is also locally replicated in each data center as in the case of SE-C. There is a static, one-to-one mapping between user queries and search sites, i.e., each site is responsible for processing a subset of user queries. Typically, this mapping is based on geographical proximity of users to data centers. The query evaluation is over the entire index, and each query is pro-cessed independently by a separate cluster as in SE-C.
In the SE-P architecture, the global document collection is partitioned into smaller, non-overlapping sub-collections such that each data center has a different sub-collection as-signed to it. Hence, the entire index is partitioned and dis-tributed over multiple, geographically distant search sites. Each site replicates its local sub-index on its clusters. Map-ping of queries to sites and processing is similar to SE-R.
Despite its performance advantages, low relevance is a lim-itation for SE-P. In theory, given a perfect ranking function, search relevance in SE-P can never be better than that of SE-C as, in SE-P, only a subset of the documents are eval-uated for relevance. There are two ways to alleviate this problem: taking popular data to queries or taking some queries to sites with relevant data. The first is an offline process that requires replicating a portion of the index (e.g., globally popular documents) on data centers. The second is an online process based on forwarding some queries (e.g., queries that are expected to have relevant documents on other sites) to non-local data centers for additional process-ing. These two solutions lead to a hybrid architecture [3, 14], which we refer to as SE-H. We should note that SE-R is a special case of SE-H with full replication. Also, SE-P can be seen as a simple version of SE-H with no replication and query forwarding. We will go into more detail on SE-H in Section 3.
To be able to make a fair trade-off analysis, we need to have some assumptions about the allocation of computa-tional resources in different architectures. We assume that the centralized architecture has an index of size I , replicated over C search clusters, each containing K processors. We also assume that C and K values are optimally chosen for best performance, i.e., we assume the optimality of resource allocation in the centralized architecture.

In practice, selecting the optimal K value depends on many factors such as the average inverted list size, hard-Table 2: Performance of the three search engine ar-chitectures in various criteria Arch. Through. time (using a perfect ranker) SE-C C/T ` C + T R SE  X  C ware parameters, and properties of queries. In any case, the optimal value is the minimum K value that satisfies a given maximum tolerable average query processing time T  X  for a percentage (herein, we assume 100%) of queries, i.e., it should always take T  X  T  X  units of time to process queries on a cluster of K processors, on average. Choosing the opti-mal K value requires a study on its own as query processing time does not linearly scale due to the fixed time costs men-tioned before. If K is known, choosing the optimal C value, which depends on T and query arrival rate, is relatively eas-ier as it can be chosen such that a given constraint on the minimum permitable query throughput is satisfied.
In the rest of the analysis, to be fair, we fix the number of processors to C  X  K so that the total computational power available to each architecture is the same. In SE-R and SE-P architectures, to simplify the analysis, we do not consider the load imbalance in search sites; instead, we assume that search sites have similar index sizes and query traffic.
Given these assumptions, Table 1 summarizes the opti-mal distribution of computational resources in different ar-chitectures. In SE-R, since every search cluster stores and processes the entire index, clusters are composed of K pro-cessors. Hence, there are S sites each with C/S clusters. In SE-P, on the other hand, search clusters have K/S pro-cessors as each site has an index of size I/S , and there are C search clusters. Note that this allocation preserves our assumption about optimality of the K value in SE-C. We delay the discussion about SE-H to Section 3.
The success of a search engine is proportional to the num-ber of users attracted. Attracting more users requires good performance, which is usually determined by high query throughput, low query response time, and high search rele-vance. Table 2 displays a comparison of the search architec-tures in terms of these factors, which we discuss below.
Response time is the time passed between the submission of a query and the display of the results. Typically, search engines try to keep it below a few hundred milliseconds. In general, the response time is composed of two components: network latency and query processing time.

In our context, network latency accounts for the time spent over the network to receive the query from the user plus the time spent over the network to send the search results to the user. Latency is determined by both the ge-ographical distance and the network distance between the user and the search site. Query processing time, on the other hand, is determined by the architecture of the search engine, available computational resources, and the query.
For our response time analysis, we assume that, in SE-C, the average query processing time is T , which we define as the average time needed to process a single user query over an index of size I using a single search cluster containing K processors. Since the I/K ratio is fixed, query processing time is T also for SE-R and SE-P architectures. However, since there are multiple, geographically scattered search sites and assignment of user queries to sites is based on proximity, the average network latency ( ` D ) of distributed architectures SE-R and SE-P is lower than the average network latency ( `
C ) of SE-C. Therefore, SE-R and SE-P have lower query response times, on average.
Query throughput is the number of queries processed per unit of time by the search engine. The peak, sustainable throughput determines the query drop rates under high traf-fic. Throughput is typically increased by having replicas of the same search index on multiple, identical search clusters.
Among the architectures, SE-C and SE-R can process at most C queries concurrently (assuming no multi-threading) since there are C search clusters in these architectures. On the other hand, in SE-P, it is possible to achieve a rela-tive throughput increase by a factor of S because, although smaller, there are more ( S  X  C ) clusters, which can process queries at the same rate provided by SE-C and SE-R.
In measuring relevance of search results, various metrics can be used (e.g., precision at k , mean average precision, and discounted cumulative gain). Search relevance is a very important criterion for the success of a search engine as it is directly related to user satisfaction and revenues.
Among the architectures, SE-C and SE-R are expected to achieve the same search relevance as queries are processed under the same conditions, i.e., evaluated over exactly the same index. Assuming a perfect ranking algorithm is avail-able, SE-P is expected to produce less relevant results than SE-C no matter how the index is partitioned and users are assigned to sites. This is due to the fact that queries are eval-uated locally over only a small portion of the entire index and relevant documents available on non-local sites cannot be included in query results.
In SE-H, index replication requires deciding on what parts of the index should be replicated on which data centers. Sim-ilarly, query forwarding requires deciding on which queries should be forwarded to which data centers. A recent work [3] has proposed a na  X   X ve technique for index replication and a novel algorithm for query forwarding in SE-H. The replica-tion algorithm used is na  X   X ve because it is simply based on replicating frequently accessed parts of the index on all data centers, without taking the relevance relation between the queries and documents into account. The query forwarding algorithm is novel because it has a filtering step which pre-vents forwarding of queries to sites with irrelevant content. In SE-H, given a replication factor  X  , a subset of size  X I of the global index is replicated on all sites while the rest of the index is disjointly partitioned such that a sub-index of size (1  X   X  ) I/S is assigned to each center. Hence, the total index size stored on a site is I (  X  + (1  X   X  ) /S ). In addition to partial replication of the index, a fraction  X  of non-local queries is forwarded to  X  remote sites for further processing.
In SE-H, queries are processed as follows. According to assignment of users to data centers, the user query is sub-mitted to a local search site. This site evaluates the query over both the local and replicated index generating the local top k results for the query. Depending on the query and a forwarding criterion, the local site may decide to forward the query to one or more remote sites. One forwarding criterion is whether a remote site holds documents with higher scores than some of the local top results for the same query. If the query is not forwarded, the local top k results are simply returned to the user. If the query is forwarded, the remote sites that receive the query evaluate the query over their lo-cal indices and each site generates a remote top k result set. These results are returned to the local site, where they are merged into a global top k list, which is then returned to the user. Fig. 2 illustrates query processing in SE-H.
Replication and query forwarding both help improving search relevance in SE-H. However, increasing the size of the replicated index results in an increase in index sizes. Hence, search clusters in SE-H now have to process a larger index, which means an increase in average query processing time. Query forwarding, on the other hand, incurs both query processing overhead on remote sites and a network latency overhead on the response time of the forwarded queries. In what follows, we develop analytical models to compare SE-H and SE-C architectures. Such a comparison is possible either by fixing available hardware resources and investigat-ing the query processing performance or by fixing the perfor-mance and observing the savings in hardware. Here, we take the first approach while the authors of [3] take the latter. In what follows, we compare the two architectures in terms of query processing performance and network latencies.
In SE-H, a search site processes its queries over an index of size I ((1  X   X  ) /S +  X  ). Also, for every query submitted to the search system, the search site has to process additional  X  X  queries over an index of size I (1  X   X  ) /S . These additional queries are those forwarded by other search sites. There is no need to evaluate these queries over the replicated part of the index as this is already done at the local site.
According to the discussion above, on average, a search cluster needs to process an index of size I ((1  X   X  ) /S +  X  ) +  X  X  (1  X   X  ) /S ) per query submitted to the distributed system. For the SE-H architecture, let K 0 , C 0 , and T 0 respectively be the number of processors per cluster, number of clusters per data center, and average processing time for a query. By (1), we obtain the average query processing time in SE-H as Figure 3: Number of queries concurrently processed in SE-H with varying replication rate (  X  ) and query forwarding rate (  X  ) under the query processing time constraint T 0 = T .

Using this formula in our analyses requires setting param-eters such as a , b , and I , which are hardware-and collection-specific. Instead of fixing these parameters to specific values, we assume a generic centralized search engine setup, where the average query response time is T =100ms, and each clus-ter (we assume C = 10) is composed of K = 100 processors. For SE-H, we use a reasonable value of S = 15 search sites and assume  X  = 1. To measure the relative gains in SE-H, we gradually increase the cost component a and observe the performance with varying  X  and  X  values.

Since SE-H is supposed to work on a smaller index but with the same computational power available in SE-C, we can expect a performance improvement over SE-C. For per-formance analysis, one alternative is to fix the average query processing time ( T 0 = T ) and observe the increase in the number of concurrently processed queries. This is possible by constructing many, small-size clusters in SE-H.
We know that, in SE-C, C queries can be concurrently processed and answered in T units of time on clusters com-posed of K processors. If we fix the average query processing time in SE-H, then SC 0 concurrent queries are answered in T unit of time on clusters composed of K 0 processors. Setting T = T and using equation CK = SC 0 K 0 (the total hardware is fixed), we obtain the number of queries that SE-H can concurrently process as
Fig. 3 shows the behavior for various replication and query forwarding rates for the number of queries that can be pro-cessed concurrently in SE-H. Note that, for SE-C, this num-ber is fixed and equal to C = 10 for our setup. For SE-H, if a is small, then performance is mainly determined by the replication rate. With sample values of  X  = 0 . 1 and  X  = 0 . 1, it is possible to concurrently process six times more queries than SE-C. Performance degrades only for very high a values and when query forwarding rate increases. Figure 4: Effect of different configurations of replica-tion rate (  X  ) and query forwarding rate (  X  ) on query processing time ( T 0 ) in SE-H (assuming T =100 ms).
Although this analysis is useful, it is more interesting to develop a cost model that fixes the query throughput to that of the centralized architecture and compute the average query processing time. Furthermore, it may not be that useful to increase the throughput unless query traffic also increases. By reducing the response time, however, we may obtain more time to utilize complex ranking functions and increase the relevance in SE-H.

This analysis requires fixing the concurrently processed query count. Now, in SE-H, C concurrent queries are an-swered in T 0 units of time using C 0 clusters each containing K 0 processors. This can be achieved by fixing the number of clusters in SE-H to C 0 = C/S so that the number of concur-rently processed queries is equal in both architectures. This enforces K 0 = K . Since we know K 0 , we can now plot the query processing time in SE-H using equation (2).
Fig. 4 shows the behavior for sample  X  and  X  values as a varies. In practice, if posting lists are in memory, a is expected to be small (e.g., a&lt; 0 . 10 T ). If they are on disk, a is relatively larger due to the overhead of disk seeks, which is not parallelizable. As seen in the figure, for low a values, considerable time gain is possible in query processing.
We should note that query processing times can be further decreased by using fewer clusters consisting of larger number of processors ( K 0 &gt; K ) without violating the throughput constraint (via queuing of queries), but we do not go into this analysis due to space limitations.
The cost analysis for network latency is relatively sim-pler. In the centralized architecture SE-C, network latency is simply twice the latency between the user and the search site, i.e., L = 2 ` C . In SE-H, on the other hand, the time spent on the network depends on the type of the query. For forwarded queries, there is the overhead of network latency between the local site and remote search sites. If the aver-age latency between two search sites is denoted by ` R , the average network latency cost in SE-H is L 0 = 2 ` D + 2  X ` Hence, the latency difference between SE-H and SE-C is Figure 5: Minimum ` D values required (feasible net-work latency) in SE-H to have a gain in network latency against SE-C (assuming ` C = 100 ms) with increasing  X  , under the worst-case scenario, where ` R = ` D + ` C .

To make a further, worst-case scenario analysis, assume that the data center in SE-C is used as one of the local data centers in SE-H (a quite reasonable assumption). We can simply assume Euclidean distances in place of latency values as round-trip time correlates better with geographical distance [11]. The worst case scenario is observed when ` ` + ` C , i.e., when the user is on the line between the local site and the central site. To have a gain in average latency, we must have L 0  X  L&lt; 0, which implies According to this equation, for example, if half of the queries can be processed locally (  X  = 0 . 5), the average latency be-tween the user and a local site in SE-H must be at least three times less than the average latency between the user and the central site in SE-C. Fig. 5 displays the feasible ` values necessary to have a latency gain in SE-H relative to SE-C (assuming ` C =100ms) as  X  varies.
As illustrated in the previous two sections, it is possible to have gains in both query processing time and network la-tency in the SE-H architecture, relative to SE-C. These gains can now be used to improve the search relevance by em-ploying more costly but accurate ranking functions in query processing. That is, given the same query response time con-straint, we can show that SE-H has higher relevance than SE-C since it can afford a high-quality ranking function.
To simulate a cheap ranker for SE-C, we used a linear combination of a BM25 variant with a link analysis metric. For the complex ranker in SE-H, we used a machine learned ranking system with many scorers. We adjusted the com-plexity of this ranker by simply increasing the number of scorers. The execution cost linearly scales with the increase in complexity as the scorers has similar cost.

To form a base-line set of top documents, we collected the top 20 results for 5000 queries from a commercial search en-gine. To evaluate the performance of the ranking functions, Figure 6: Variation in precision@20 as the cost of the ranking function increases. we blended 200 documents selected by the cheap scoring function into the base-line set. We ranked all 220 docu-ments using a complex ranking function composed of 1000 scorers, at every 100 scorer measuring precision@20 values.
Fig. 6 shows the behavior for precision@20 as the com-plexity of the ranking system increases. Zero complexity corresponds to the case where we use the cheap ranking function. Full complexity corresponds approximately to an execution cost of 100ms. This result indicates that consid-erable relevance improvements are achievable in SE-H even by introducing little complexity into the ranking function utilizing the gains in response time (i.e., the gains in both query processing time and network latency).
Large Web search engines continuously crawl the Web and build search indices. In SE-C, the entire Web is crawled by a central system and stored in a single data center. An ad-vantage of distributed search architectures over SE-C is the distributed nature of the crawling process, i.e., data cen-ters crawl the Web pages that are of interest to their users. Hence, the distances between search sites and crawled Web sites are lower compared to the centralized case. As pages will be downloaded faster, page refresh rates can be im-proved and Web coverage can be increased. Here, we are interested in the latter.

A recent study [7] has investigated performance gains in distributed Web crawling via real-life performance experi-ments over geographically distributed sites based on sam-pling of above-mentioned latency and bandwidth values. It reports considerable throughput increase for systems dis-tributing the crawling process. Obviously, given a fixed amount of crawling time, the speedup in download rates leads to better coverage of the Web.

Herein, we investigate the claim that better Web cover-age in distributed search engine architectures leads to better search quality. In practice, both Web crawling and process-ing of queries are continuous. For evaluating the claim, we can assume two timelines, each starting at the same time. The first timeline contains the occurrence of queries. The second contains the discovery of Web pages. At a specific time-point t (checkpoint), by measuring the  X  X verlap X  be-Figure 7: Overlap between future query results and documents crawled in the past. tween the search results that will be displayed within the time interval [ t . . .  X  ) and the pages crawled during the time interval [0 . . . t ], the effect of Web coverage on search rele-vance can be measured. In the example in Fig. 7, 50% of the future query results can be served by the repository, i.e., an overlap of 0.5 at time point t . For our analysis, however, we assume that queries are repeated and evaluate the relevance at every checkpoint over the entire set of queries.
We use a random sample of 102 million Web pages, crawled in September 2007. Pages are partitioned into five distinct geographical regions, based on the location of the Web server holding the document and the content of the document.
As a ground-truth for relevant results, we use clicks ob-tained from a commercial search engine. We assume that a document that was clicked for a given query is relevant. For simplicity, we ignore the rank at which the document was clicked. We collect pairs of queries and clicked documents for those queries by users from a particular geographic re-gion. For example, queries from users whose IP address is in France, or queries submitted to the French site. We form our document collection such that it contains all relevant documents for queries we process.

We measure search relevance as the average reciprocal rank of relevant documents. Web pages are ranked using a linear combination of a BM25 variant and a link analysis metric. The parameters of BM25 and the weights for the linear combination have been set to maximize the average reciprocal rank achieved by the centralized system.
In the experiments, we investigate the effect of increased crawling throughput, crawling order, and region boosting on search quality. We report the relevance values at 10 equally spaced time points (checkpoints). The 10th checkpoint cor-responds to the time that the fastest system downloaded all Web pages. That is, at this checkpoint, only a subset of the collection will be downloaded by slower systems.
For throughput experiments, we use four different down-load throughput ( D ) values for the centralized crawling sce-nario and one for the distributed case (obtained from [7]). Fig. 8 shows the accuracies achieved with varying through-put. According to the results, by the time a distributed en-gine downloads all pages, the fastest centralized engine only has about 3 / 4 of the accuracy of the distributed engine.
The previous experiment considers a random crawling or-der. However, more intelligent crawling orders can be em-ployed so that more important pages are downloaded ear-lier [2, 8, 9, 12]. In our experiments, we try different crawl-
Figure 8: Effect of throughput on result quality. ing orders: random, decreasing page size, increasing page size, increasing URL depth, and decreasing importance (as measured by a link analysis metric).

Fig. 9 shows the impact of different crawling strategies on relevance. In general, ordering by URL depth or page importance achieve good relevance rates very early. Specif-ically, 95% of the accuracy is already achieved by the page importance metric when only 10% of pages are crawled. We also display the results for varying throughput rates in Ta-ble 3. This table shows that centralized engines can also achieve high precision values by employing a link analysis metric, approaching distributed engine X  X  performance. This is due to the power-law distribution in important pages, i.e., most important pages can be downloaded very early without being affected much from the download throughput.
Partitioning documents geographically results in some form of regional document boosting if no query forwarding is em-ployed. But, the same effect can also be achieved by the centralized engine. We conduct an experiment to observe the effect of region boosting. Fig. 10 shows the performance of different architectures (with D = 48 . 1). It is seen that SE-P performs better than SE-C since many irrelevant doc-uments are automatically filtered due to partitioning. How-ever, SE-C with explicit boosting of documents performs slightly better.
We have shown via analytical models that there are con-siderable performance gains possible by distributed search architectures relative to a centralized architecture. Also, it is shown that the performance gains may lead to gains in search quality as they let more complex ranking functions be employed. Finally, we have investigated the effect of dis-tributed Web crawling on search relevance.
 Although, in this work, the distributed architecture (SE-H) is shown to be superior to the centralized architecture (SE-C) in terms of performance, this mainly depends on the amount of redundant work performed, i.e., the processing on the replicated index and remote query processing. Hence, there are two main research directions that can be pursued for further improving the performance in SE-H. First, novel replication models are needed. These models should take into account the current mapping of users to data centers Figure 9: Effect of crawling order on result quality. Table 3: Average reciprocal rank with different or-dering strategies and download rates Ordering strategy 18.5 23.5 27.6 30.9 Decreasing page length 0.041 0.058 0.072 0.086 Random 0.085 0.100 0.113 0.123 Increasing page length 0.130 0.143 0.150 0.154 URL depth 0.153 0.156 0.158 0.159
Link analysis metric 0.156 0.158 0.157 0.159 and also the relevance relationship between documents and user queries while replicating the data. The data should be replicated such that each data center should store the parts of the index that are most relevant to its users. Second, novel query forwarding techniques are needed. This problem is somewhat similar to the collection selection problem in federated IR. However, due to the collaborative nature of search sites, it is possible to devise superior algorithms. [1] R. Baeza-Yates, C. Castillo, F. Junqueira, [2] R. Baeza-Yates, C. Castillo, M. Marin, and [3] R. Baeza-Yates, A. Gionis, F. Junqueira, [4] M. Bawa, G. S. Manku, and P. Raghavan. Sets: search [5] J. Callan. Distributed information retrieval. In
Figure 10: Result quality with region boosting. [6] J. P. Callan, Z. Lu, and W. B. Croft. Searching [7] B. B. Cambazoglu, F. Junqueira, V. Plachouras, and [8] J. Cho and H. Garcia-Molina. Parallel crawlers. In [9] J. Cho, H. Garcia-Molina, and L. Page. Efficient [10] D. Harman and G. Candela. Retrieving records from a [11] B. Huffaker, M. Fomenkov, D. J. Plummer, D. Moore, [12] M. Najork and J. L. Wiener. Breadth-first crawling [13] D. Puppin, F. Silvestri, and D. Laforenza.
 [14] C. Sarigiannis, V. Plachouras, and R. Baeza-Yates. A [15] C. Tang, Z. Xu, and M. Mahalingam. Peersearch: [16] I. H. Witten, A. Moffat, and T. C. Bell. Managing
