 Recent work on language models for information retrieval has shown that smoothing language models is crucial for achieving good retrieval performance. Many different effec-tive smoothing methods have been proposed, which mostly implement various heuristics to exploit corpus structures. In this paper, we propose a general and unified optimization framework for smoothing language models on graph struc-tures. This framework not only provides a unified formula-tion of the existing smoothing heuristics, but also serves as a road map for systematically exploring smoothing methods for language models. We follow this road map and derive several different instantiations of the framework. Some of the instantiations lead to novel smoothing methods. Empiri-cal results show that all such instantiations are effective with some outperforming the state of the art smoothing methods. Categories and Subject Descriptors: H.3.3 [Informa-tion Search and Retrieval]: Retrieval Models General Terms: Algorithms Keywords: Language modeling, smoothing, graph struc-ture, document and word graph
Language models have attracted much attention in the in-formation retrieval community recently due to their success in a variety of retrieval tasks [17, 5]. Fundamental to all the language models used for retrieval is the issue of smooth-ing, which has been shown to affect retrieval performance significantly [22]. Indeed, in the basic language modeling approaches [17, 7, 16, 24], the entire retrieval problem is es-sentially reduced to the problem of estimating a document language model which can be further reduced to how to smooth the document language model. In other more so-phisticated use of language models, such as relevance mod-els [13] and model-based feedback methods [23], improved Copyright 2008 ACM 978-1-60558-164-4/08/07 ... $ 5.00. smoothing of document language models is also shown to improve performance [14, 20].

Because of the importance of smoothing, it has been at-tracting attention ever since Ponte and Croft X  X  pioneering work on applying language models to retrieval [17]. Since then many smoothing approaches have been proposed and tested. In early days, most smoothing methods relied on using a background language model, which is typically esti-mated based on the whole document collection, to smooth a document language model [17, 7, 16, 24]. Recently, corpus graph structures have been exploited to provide more accu-rate smoothing of document languages. The basic idea is to smooth a document language model with the documents similar to the document under consideration through either clustering or document expansion [14, 9, 10, 6, 18, 11, 20]. Such a local smoothing strategy can leverage document sim-ilarity structures to offer  X  X ustomized X  smoothing for each individual document; this is in contrast to the simple global smoothing strategy which smoothes all documents with the same background model.

The local smoothing strategy has been shown to be quite effective in several recent studies [14, 20, 11] and is the best smoothing strategy known so far. In virtually all these local smoothing methods, the smoothing formula eventu-ally boils down to some form of interpolation of the origi-nal unsmoothed document language model (i.e., the max-imum likelihood estimate of the unigram language model) and some  X  X upporting X  language models estimated based on documents similar to that document. Different smoothing methods differ in how they assign weights to all these dif-ferent component language models. It is known that this weighting of component language models can significantly affect retrieval performance, and sometimes even a small dif-ference in weighting can lead to visible difference in perfor-mance [14]. Unfortunately, none of the existing work offers a formal framework to optimize these weights; as a result, there is no guidance on how to further improve these exist-ing smoothing methods or develop new (potentially better) smoothing methods. For example, it is unclear whether we can exploit other structures such as word similarity graphs to improve smoothing.

In this paper, we propose a general unified optimization framework for smoothing language models on graph struc-tures. While not explicitly stressed in the existing work, we believe that graph structures are fundamental to smoothing; indeed, we can interpret smoothing intuitively as to make the language models of those nodes close to each other on a graph structure similar to each other, so that the surface representing all the language models would be  X  X mooth. X 
Our framework unifies two major heuristic goals in smooth-ing within a single objective function: (1)  X  X idelity X : A smoothed language model should not deviate too much from the original non-smoothed language model; (2)  X  X mooth-ness X : After smoothing, the nodes that are close to each other on the graph would have similar (smoothed) language models; the closer the nodes are, the more similar their lan-guage models are.

This framework not only provides a principled formula-tion of the existing smoothing heuristics but also serves as a road map for systematically exploring smoothing meth-ods for language models. We follow this road map and derive several different instantiations of the framework, in-cluding smoothing on document graphs and smoothing on word graphs, as well as smoothing document language mod-els and query language models. Although document graphs have been used for smoothing in the previous work, to the best of our knowledge, no previous work has studied how to smooth language models with a word graph. Moreover, ex-isting work on smoothing has mostly attempted to smooth document language models, while we also study how to use a word graph to smooth a query language model.

These smoothing methods are evaluated using several stan-dard TREC test collections. The results show that all the derived smoothing methods can improve over the baseline global smoothing method significantly, indicating that our framework is reasonable. The derived smoothing methods either outperform or perform similarly to the corresponding state of the art local smoothing methods.

Given the importance of smoothing in the language mod-eling approach to information retrieval and given the gen-erality of our framework, we hope that the framework can open up some promising new directions for finding an opti-mal way of smoothing language models.

The rest of the paper is organized as follows. In Section 2, we propose the general optimization framework for smooth-ing language models with graph structure, and introduce a unified solution. In Section 3, we follow the framework and introduce four instantiations of the general framework. We discuss the properties of those instantiations in Section 4 and evaluate them with empirical experiments in Section 5. Finally, we discuss related work and conclude in Section 6 and 7, respectively.
To motivate our framework, we start with a brief discus-sion of the intuitions behind the major smoothing heuristics.
Given a non-smoothed document language model P M L ( w | d ) (i.e., a word distribution), all smoothing methods attempt to generate a smoothed language model P ( w | d ) that can bet-ter represent the topical content of document d . An obvious improvement over P M L ( w | d ) that almost all the smoothing methods would do is to avoid assigning zero probabilities to words that are not seen in d . Indeed, this was a major motivation for smoothing discussed in [17]. In general, how-ever, the purpose of smoothing is to improve the accuracy of the estimated language model, not just avoiding zero prob-abilities. The most useful resources for smoothing so far have been documents that are similar to d , and methods exploiting such document graph structures are among the best performing methods [20, 9, 14].

The general procedure of all the smoothing methods using corpus structures is as follows: 1. Construct a graph of documents where documents are 2. For every document, estimate a structure-based lan-3. Combine the structure-based language model with the
How are these methods different from traditional global methods, such as the Jelinek-Mercer(JM) and the Dirichlet smoothing [24]? Intuitively, they all went beyond the initial goal of giving non-zero probabilities to unseen words. But why do they all take such general steps? What are they essentially trying to optimize? To explain this formally, let us first look at what each step is trying to achieve.
By constructing a similarity graph of documents, one en-sures that similar documents are located close to each other. Using the argument of data manifold in machine learning, if we project the documents onto a hyperplane, we expect that documents with the largest similarity have the smallest distance on the hyperplane [1].

By estimating a document language model based on the neighbor documents or the closest clusters, one ensures that the language model representation of a document is not sig-nificantly different from the documents close to it.
However, with this objective alone, the smoothed lan-guage model could dramatically deviate from the original document. Indeed, an easiest solution is making all docu-ments share the same representation. The combination with original language model ensures that the smoothed language model is not far from its original contents.

Thus we see that there are two major intuitive assump-tions made in all such work:  X  documents in the same cluster should have similar representation  X  [21, 14], and  X  neighbor documents should have similar representation  X  [9, 20]. Such assumptions also appear in the literature of semi-supervised learning [25, 27], where the first one is referred as  X  global consistency  X  and the latter as  X  local consistency  X .
Indeed, if we project the graph structure of documents on a hyperplane, the language model { P ( w | d ) } can be plotted as surfaces on top of the hyperplane.

Figure 1 visualizes such an intuitive explanation with syn-thetic data. The hyperplane shows a manifold structure of documents and the surface plots P ( w | d ) for a given word w over different documents. The left figure shows the maxi-mum likelihood estimate of P ( w | d ), which has an unsmoothed surface; in contrast, the figure on the right side presents a smoothed surface (i.e., smoothed P ( w | d )) with the ba-sic shape of the surface preserved (i.e., being  X  X oyal X  to P
M L ( w | d )).
Although the graph structure is not explicitly mentioned in cluster-based methods [21, 14], this is a reasonable gen-eralization. Figure 1: Illustration of smoothing language mod-els on a graph structure. Left figure: unsmoothed model; Right figure: smoothed model
We can now see that graph structures are fundamental to smoothing and a better graph would presumably lead to better smoothing. For example, the simple smoothing strat-egy of interpolating with the collection language model can be regarded as smoothing on a clearly non-optimal graph where document d is assumed to have equal distance to all other documents in the collection (i.e., a  X  X tar X  structure). Thus it should not be surprising that smoothing on a graph better reflecting the real semantic similarities between doc-uments would be better, which has indeed been shown in the literature [9, 20].

A major challenge in any smoothing method is how to deal with the tradeoff between  X  X idelity X  (stay close to the non-smoothed model) and  X  X moothness X  (be the same as neighbors). In general, we have such a surface for each word w 0 ( P ( w 0 | d )). Thus conceptually we have as many surfaces as the number of words in our vocabulary, and optimizing this tradeoff for all the words can be very difficult without an explicit objective function.

How can we ensure that the tradeoffs for different nodes in the graph are all reasonable? In general, how can we smooth all such surfaces in a principled way and achieve the smoothing effect in Figure 1? The hyperplane in Figure 1 illustrates a manifold of documents. Does that have to be a graph of documents? We can imagine that the hyperplane presents other types of graphs, such as a word graph, and plot the language models as different surfaces on that hy-perplane. In the following section, we introduce a general framework to smooth language models on a general graph structure.
From Figure 1, we see that the notion of smoothing dis-cussed in this paper is different from  X  X ssigning non-zero probabilities to unseen words, X  but aims at  X  X chieving con-sistency on a graph structure. X  For continuous functions like time series, smoothing is usually done with a regularizer with well-defined derivatives. A graph structure, however, defines a discrete domain. Discrete regularization has been cast as an optimization problem in machine learning litera-ture [26]; we apply it here to define a general optimization framework for smoothing language models.

Formally, let us introduce the following definitions:
Note that G can be either directed or undirected. In this paper, we only focus on the undirected case, and propose the following general optimization framework for smoothing language models on the graph structure.

This objective function generalizes the intuitions in Sec-tion 2.1 well: The first term guarantees that the smoothed language model does not deviate too much from its origi-nal value, especially for more important vertices (controlled by w ( u )); the second term, also known as a harmonic func-tion in semi-supervised learning, guarantees the consistency of the language model on the graph. The right term can also be written as  X  P u  X  V P v  X  V w ( u, v )( f u  X  f v w ( u, v ) = 0 when there is no edge between u, v .
This framework is general. Clearly, a different selection strategy. This flexibility provides us a road map for under-standing the existing smoothing strategies, as well as explor-ing novel smoothing methods.

To use such a road map, any reasonable instantiation of the framework must satisfy the following constraints. 1. f u and f v are comparable, that is, when f is fully 2. w ( u ) offers a reasonable weighting that captures the 3. w ( u, v ) offers a reasonable weighting that captures the
When we are smoothing language models, we could in-stantiate f u as the probability of a word given a document, i.e., P ( w | d ), and associate u with d , or w , or both. In this case, there is an additional constraint, i.e., P w P ( w | d ) = 1. What are reasonable instantiations of w ( u ) and w ( u, v )? Intuitively, w ( u, v ) could be instantiated as the closeness of two vertices in the graph, or the similarity of u and v . Using the similarity of two vertices to weight an edge has been commonly adopted in existing literature ([9, 20]). How about w ( u )? There are also many studies in the context of link analysis to model the importance of a vertex on a graph, e.g., in-degree, PageRank [2], and HITS [8]. In this paper, we use the degree of vertex u as the importance weight of u :
Minimizing O ( C ) in Equation 1 will achieve the smooth-ness of the surface in Figure 1. For instance, if we se-lect { G = D, f u = P ( w | d u ) ,  X  f u = P M L ( w | d Cosine ( d u , d v ) } , the smoothing framework boils down to smoothing language models with document similarity graph, where edges are weighted with cosine similarity. To smooth the language models { P ( w | d ) } d  X  C , one needs to find a solu-tion of { P ( w | d u ) } d u  X  C = arg min { f u } ject to the constraint P w P ( w | d ) = 1, which achieves the smoothness of multiple surfaces.
Generally, to minimize O ( C ) in Equation 1, we can com-pute the first-order partial derivatives of O ( C ), that is,  X  X  ( C ) Clearly, a solution of Equation 3 could minimize O ( C ) in Equation 1. To get such a solution, we can start with f u  X  f u and execute Equation 3 until converging. In practice, we do not need to wait until complete convergence; a few iterations of Equation 3 can already give an improved O ( C ). We further discuss leave convergence in Section 4.
In Section 2, we have shown that smoothing document language models with a document graph [14, 9, 20] is an instantiation of the general framework. It is by no means the only one. Indeed, any reasonable instantiation of { G, f u w ( u ) , w ( u, v ) } which satisfies the predefined constraints will lead to a variant strategy of smoothing. In this section, we explore different instantiations of the framework.
The most common instantiation is smoothing with docu-ment graphs (i.e., V = D ). Although not in a unified way, many heuristics have been proposed to smooth using docu-ment graphs. Document language models are adjusted by receiving weights from the cluster it belongs to [21, 14], or by propagating weights from the nearest neighbors [9, 18, 20, 19]. A commonly used instantiation of w ( u, v ) is the co-sine similarity of two documents. As shown in Section 2.2, a reasonable instantiation of f u and  X  f u is f u = P ( w | d  X  f = P M L ( w | d u ). Plugging these in Equation 3, we have Smooth Document Language Models ( f u = P ( w | d u ) ):
P ( w | d u ) = (1  X   X  ) P M L ( w | d u ) +  X  X One may notice that we did not utilize the constraint P w P ( w | d ) = 1. As a matter of fact, when we start with after every iteration of Equation 4.

Another interesting instantiation of f u is simply the rele-vance score of a document d u for a query q .

Smooth Relevance Scores ( f u = s ( q, d u ) ):
This process bypasses language models, but smoothes the relevance score directly. Similar heuristics appear in existing work where corpus structure is utilized to rerank documents with score propagation and regularization [10, 6, 18, 11]. In our experiments, we will show that this instantiation is not as effective as smoothing document language models.
Recent work in natural language processing has intro-duced new ways to represent documents with graph struc-tures of words [15]. Indeed, in many scenarios we are acces-sible to a graph of terms (e.g., a word similarity graph, an entity-relation graph, or an ontology graph). Although most existing work smoothes language models with a document graph, we show that the general smoothing framework can also be instantiated with word graphs, which leads to novel smoothing procedures.

In such smoothing procedures, G is now a word graph (i.e., V = W ). w ( u, v ) can be instantiated with either co-occurrence or mutual information of two words. We intro-duce the following novel instantiations:
P ( w u | d ) = (1  X   X  ) P M L ( w u | d ) +  X  X
Note that unlike a document graph where all documents are treated equally, there is significant prior knowledge of using different words. Some words tend to be used more than others even if they are highly related (e.g., car v.s. vehicle). Some words are more important than others and thus should be assigned a larger value. In this instantia-and f v comparable. Please note that Deg ( u ) is just one choice of denominator which captures the importance of a word (which is proportional to the pagerank value on G ). One could use other choices such as idf . This results in a different denominator from the one used in Equation 4 and Equation 6 ( Deg ( u ) v.s. Deg ( v )). P w P ( w | d ) = 1 is now naturally satisfied after any iteration of Equation 6. This smoothing strategy is not well studied in existing IR literature. Interestingly, if we use a similar instantiation, f u = P ( d u | w ) /Deg ( u ) on a document graph based on doc-ument similarity, we would be able to derive the term count propagation smoothing method proposed in [19].

The instantiations discussed so far aim at smoothing doc-ument language models, where f u  X  P ( w u | d ). When sys-tematically examining variations of the framework, one may naturally ask whether this is the only way to instantiate f In language modeling retrieval models, the query language model is also an important component. Can we also lever-age the framework to smooth query language models? The
P ( w u | q ) = (1  X   X  ) P M L ( w u | q ) +  X  X
This smoothing method is related to query expansion (i.e., to add new terms to the query and adjust the weights of query words). In model-based feedback [23], where a new query model is estimated from feedback documents, we ex-pect that this novel smoothing strategy can also be applied.
The objective function of smoothing in Equation 1 is re-lated to existing work of discrete regularization in semi-supervised learning and manifold learning [27, 1, 25, 26]. Many different regularizers and objective functions have been proposed in that context. We use Equation 1 instead of oth-ers because it is general, has many nice properties, and has natural connections to many existing concepts and models. Some recent work [6] also used one of them for the problem of retrieval score regularization.
Equation 6 and Equation 7 look similar to the updating formula of PageRank [2], except that P M L ( w | d ) is used in-stead of the uniformed jumping probability 1 /N . Indeed, Equation 6 can be rewritten as
P ( w u | d ) = X which is essentially computing a stationary distribution of a positive-recurrent Markov chain, where which is guaranteed to converge. Note that the initial value of u would not be forgotten like in PageRank, because P M L has been embedded into transition probabilities. Intuitively, we can imagine that when an author is composing a docu-ment, he would randomly walk along such a Markov chain of words, and write down a word whenever he passes it.
PageRank-like propagation has been used in [10, 18] to rerank top retrieved documents, combined with other fea-tures in a heuristic way.

What about Equation 4 and 5 for the document graph, or more generally Equation 3? They are not like PageRank now because a different denominator Deg ( u ) is used instead of Deg ( v ). What is this essentially modeling? In fact, when we transform the graph in a certain way (by adding nodes and reassigning transition probabilities), one could see that Equation 4 is actually computing the  X  X bsorption proba-bility X   X  the probability of each node to be absorbed to a termination state in such a transformed Markov chain. This is also guaranteed to converge.

Note that if we only apply Equation 4 once, the smoothing procedure is very similar to the method proposed in [20]. The difference is that they are using f u = c ( w, d u ), which we have shown to be not as reasonable as P ( w | d u ).
How about language model smoothing with collection dis-tribution (e.g., Jelinek-Mercer smoothing) and cluster struc-tures [14, 11]? Intuitively, these models are smoothing doc-ument language models with global structures (e.g., collec-tion, clusters), so that the estimated language models could satisfy global consistency (i.e., documents in the same global structure has similar representation). [20, 9] explored near-est neighbors, which guarantees local consistency of lan-guage models. A regularization framework like Equation 1, as shown in [25], satisfies both local and global consistency on a manifold structure.
The smoothing framework we proposed is general and or-thogonal to the selection of graphs. As long as the graph is constructed in a reasonable way (two related vertices are connected with a higher weighted edge, and are expected to have similar representations), the objective function in Equation 1 can be applied.

Thus either a fully connected graph (with well scaled edge weights) [25, 26], or a k-nearest-neighbor (kNN) graph [9, 10, 20] can be used for smoothing. We show our experi-mental results with kNN graphs. On the other hand, any reasonable distance/similarity measure could be applied to compute w ( u, v ).

In scenarios that the number of documents/terms is too large, as in the case of a commercial search engine, one may think of using smaller subgraphs. Our proposed framework can be easily adapted to subgraphs. Of course, we need to ensure that the smoothed value of vertices in the subgraph are comparable with the values of the unsmoothed vertices (i.e., vertices not in the subgraph). The representations of both smoothed documents and unsmoothed documents with Equation 4 are all language models, which do not have a scaling problem. As for term graphs (e.g., Equation 6), it is easy to prove that
This formula indicates that the probability of smoothed terms would not affect the probability mass of unsmoothed terms.
In Section 2.2 and Section 3, we proposed a general frame-work of smoothing language models and introduced various instantiations with document graph and word graph, result-ing in several different smoothing strategies. In this sec-tion, we evaluate the effectiveness of these strategies empir-ically. Experiment results show that all the instantiations of the smoothing framework outperform the non-structural smoothing methods. Two instantiated approaches also out-perform the state-of-the-art graph-based smoothing meth-ods. We use four representative TREC data sets: AP88-90, LA (LA Times), SJMN (San Jose Mercury News 1991), and TREC8. They are identical to the data sets used in [20], with the same source, query, and preprocessing procedure. The first three data sets are also identical to the data sets used in [14]. The basic statistics of the data sets are pre-sented in Table 1. We used the title field of a query/topic description to simulate short keyword queries in our exper-iments.
For every data set, we construct a k-Nearest-Neighbor (kNN) graph of all documents, as well as a kNN graph of words. The edge weight of two documents is measured with the cosine similarity, formally .

The edge weight of two words is set to their mutual in-formation [4]. We do not include the most frequent terms (which appear in &gt; 50% documents) or the most infrequent terms (which appear in &lt; 15 documents in TREC8, or &lt; 7 documents in other data sets). This provides us with a word graph of 40 k  X  60 k vertices. We control the density of the graph by adjusting the number of nearest neighbors. To en-sure that the graph is undirected, we add an edge between u and v if either u is in v  X  X  k-nearest neighbors, or the converse.
After we smooth all the document language models and obtain the smoothed P 0 ( w | d ) and possibly also a smoothed query language model P ( w | q ), we use further smooth P using Dirichlet prior [22] and obtain We then use the KL-divergence retrieval model to rank docu-ments, where each document is scored based on the negative KL-divergence of the query language model and P 00 ( w | d ) [12].

The additional Dirichlet smoothing is to model noise in the query and has been used in most existing work on smooth-ing language models with corpus structure [14, 9, 20]. When P ( w | d ) is unsmoothed (i.e., the maximum likelihood esti-mate), it boils down to the Dirichlet prior model, which is used as our baseline.
In Section 3, we introduced four different instantiations of smoothing, namely: smoothing document language model with document graph ( DMDG ); smoothing relevance score with document graph ( DSDG ); smoothing document lan-guage model with word graph ( DMWG ); and smoothing query language model with word graph ( QMWG ).

We compare these four methods in Table 2 along with the best results of the Dirichlet smoothing. In all our ex-periments, the cutoff of relevant documents is set as 1000. We see that all the four smoothing instantiations outperform the non-structural smoothing baseline. DMDG and DMWG outperform Dirichlet prior consistently and significantly.
Among the four proposed methods, we see that smooth-ing document language model tends to achieve better per-formance than smoothing query language model or the rel-evance score. One possible explanation is that smoothing document models is superior to smoothing query language models, since that the short keyword query only conveys very sparse information about the user X  X  information need. Expanding a query in a wrong direction could hurt the re-trieval performance.

Regularizing relevance scores has been discussed in exist-ing literature [6], where a similar approach to DSDG is used. Clearly, we see that smoothing relevance scores alone is not as effective as smoothing the document language models. In-deed, by dealing with the richer representation of document language models, one has more flexibility in controlling the core retrieval modules to achieve better performance.
Using a document graph vs. a query graph to smooth document language models perform similarly, which is ap-pealing since smoothing with word graph has not been well explored in the existing literature, suggesting potential room for further leveraging a word graph for smoothing.
The selection of smoothing strategies would introduce a different set of new parameters. Generally, when a kNN graph is used, the size of the graph could be controlled by tuning the parameter k . In Equation 1, we introduced a pa-rameter  X  to balance the consistency of the language models on the graph, and the fidelity to the maximum likelihood estimates. Figure 2 presents the sensitivity of the retrieval performance to these introduced parameters.

From the left plot, we see that the performance is rela-tively stable over different k , even if a document only has a few (e.g., 10) neighbors. This is different from the observa-tion made in [20], where smoothing with a larger number (  X  100) of neighbors significantly outperforms a small num-ber (  X  50) of neighbors. This is because when propagations are processed iteratively, a few closest neighbors could well capture the local consistency.

Similar patterns are observed from other data sets and smoothing methods. We set the number of neighbors to 100 for a document graph and 50 for a word graph, unless specifically noted.

From the plot in the middle of Figure 2, we see that when  X  is larger, the estimated language models would achieve more consistency on the surface at the expense of deviating more from the original estimates. When  X  is set smaller, less smoothing effect is added to the language models. Setting  X  in the range of 0 . 3  X  0 . 7 usually yields good retrieval performance.

Finally, how many iterations do we have to run for the equations proposed in Section 3? In fact, we could even find a closed form solution for such a regularizer [25, 6]. However, its computation is time consuming. Intuitively, most smoothing effect occurs in the first a few iterations, which is indeed confirmed in the right plot of Figure 2, where we see that the performance becomes quite stable after a few iterations. Thus we may just run the algorithm for a few iterations in practice.
We now compare our methods with some state of the art smoothing methods.
 Table 3: Performance (MAP) comparison with ex-isting smoothing methods
Liu and Croft [14] proposed a method (denoted as CBDM) to smooth document language models with cluster language models. [20] also proposed a smoothing strategy (denoted as DELM) to expand a document with its nearest neigh-bors. Both methods utilized the document similarity and are related to the DMDG instantiation we proposed. As we use the identical data sets and preprocessing procedures with their work, we compare the performance of our DMDG method with the best results reported in their papers.
We see that DMDG consistently outperforms CBDM. It also outperforms DELM on three data sets except for LA, but with a smaller improvement. This is not surprising, be-cause the DELM method is very similar to our DMDG in-stantiation if only one iteration of Equation 4 is processed. The difference is that DELM also expands the document length besides smoothing the language models. The per-formance of DMDG with only 1 iteration is also presented in Table 3, from which we see that running more iterations improves retrieval performance. Similar improvements are achieved with the DMWG method. This comparison shows that the optimization framework of smoothing performs bet-ter than exploring either local or global consistency alone.
Pseudo feedback has been proved to be effective to update the query model using collection information [23]. It is inter-esting to see whether pseudo feedback could bring additional improvements to the graph-based smoothing methods, and whether it could benefit from a graph structure. [20] has already proved that combining pseudo feedback and document graph achieves better results than both the feedback model and the smoothing model alone. In this work, we intend to explore whether combining the feedback model and the word graph would improve performance, be-cause smoothing with a word graph is our novel exploration. Model-based feedback [23] combines the original query lan-guage model with a feedback model estimated from the top ranked documents. One natural thought is to smooth the feedback model with a word graph and Equation 7. An al-ternative way is to use the updated query model to retrieve the smoothed documents (i.e., with DMWG method) while the document language models are smoothed. We explored both strategies, and summarize the results in Table 4.
We use the model-based method in [23] to estimate a feedback model from the top 5 retrieved documents. In this process, we fix the noise parameter to be 0.9. We tune the combination parameter  X  (as in [23]) to get the optimal performance of pseudo-feedback. From Table 4, we see that both ways of combination improved performance. Combin-Table 4: Performance (MAP) of combination of word graph and pseudo feedback ing pseudo feedback and DMWG significantly improves both DMWG and pseudo feedback. For the FB+DMWG model, we simply reuse the optimal parameters for Feedback and DMWG individually, without further tuning.
Most of the related work has already been discussed in the previous sections of the paper. Here we give a brief summary of all the related work.

Smoothing language models [3] for information retrieval has been a fundamental and challenge problem in IR re-search. Traditional smoothing methods explore the collec-tion information in an unstructured way [17, 3, 24]. Our work lies in the context of language model smoothing, but we utilize the graph structures from the collection to smooth document and query language models.

Recent research has explored document similarity graphs to smooth language models [21, 14, 9, 18, 20]. Cluster struc-tures [21, 14], nearest neighbors [9, 20], and propagation-based methods [18, 11] have been proposed with various heuristic solutions. Our proposed optimization framework is a reasonable generalization of all such approaches, which provides a unified solution to this problem, as well as a road map for exploring novel smoothing approaches.
One of the concrete instantiations of our proposed frame-work, smoothing relevance scores with document graph, is related to score reranking work, such as [10, 6, 11]. [6] explores a regularization approach which is related to the objective function we introduced. However, they focus on regularizing the relevance scores, while we explore the gen-eral problem of smoothing language models. Experimental results show that smoothing relevance score alone is not as effective as smoothing document language models.
The proposed optimization framework is also related to the graph-based learning theme in machine learning. Similar objective functions have been explored in [27, 25, 26]. Their main focuses are machine learning problems such as semi-supervised learning and spectral clustering, while we explore graph structures to smooth language models for retrieval.
In this paper, we proposed a general optimization frame-work for smoothing language models with graph structures. The proposed framework not only gives a principled justifi-cation for many of the heuristics and a unified solution to smoothing language models with graphs, but also provides a road map for the exploration of novel smoothing methods. Following such a road map, we introduced four instantia-tions of smoothing methods, including two novel methods of smoothing document and query models with a word graph. Empirical results show that all proposed instantiations sig-nificantly improve over the unstructured smoothing meth-ods. The two methods of smoothing document language models outperform the state of the art smoothing methods.
The proposed framework opens up many interesting ques-tions for future research: Can we combine a document graph and a word graph to enhance the performance? Can we ex-plore other types of graphs, like a query-dependent graph? How can we explore traditional IR heuristics in this unified framework? All these questions should be further studied. We thank the anonymous reviewers for their useful com-ments. This work is in part supported by the National Science Foundation under award numbers IIS-0347933, IIS-0713571, and IIS-0713581. The first author is supported by a Yahoo! PhD Fellowship. [1] M. Belkin and P. Niyogi. Laplacian eigenmaps for [2] S. Brin and L. Page. The anatomy of a large-scale [3] S. F. Chen and J. Goodman. An empirical study of [4] K. W. Church and P. Hanks. Word association norms, [5] W. B. Croft and J. Lafferty, editors. Language [6] F. Diaz. Regularizing ad hoc retrieval scores. In [7] D. Hiemstra and W. Kraaij. Twenty-one at TREC-7: [8] J. M. Kleinberg. Authoritative sources in a [9] O. Kurland and L. Lee. Corpus structure, language [10] O. Kurland and L. Lee. Pagerank without hyperlinks: [11] O. Kurland and L. Lee. Respect my authority!: Hits [12] J. Lafferty and C. Zhai. Document language models, [13] V. Lavrenko and B. Croft. Relevance-based language [14] X. Liu and W. B. Croft. Cluster-based retrieval using [15] R. Mihalcea and D. R. Radev, editors. Textgraphs: [16] D. H. Miller, T. Leek, and R. Schwartz. A hidden [17] J. M. Ponte and W. B. Croft. A language modeling [18] T. Qin, T.-Y. Liu, X.-D. Zhang, Z. Chen, and W.-Y. [19] A. Shakery and C. Zhai. Smoothing document [20] T. Tao, X. Wang, Q. Mei, and C. Zhai. Language [21] J. Xu and W. B. Croft. Cluster-based language [22] C. Zhai and J. Lafferty. A study of smoothing [23] C. Zhai and J. Lafferty. Model-based feedback in the [24] C. Zhai and J. Lafferty. A study of smoothing [25] D. Zhou, O. Bousquet, T. N. Lal, J. Weston, and [26] D. Zhou and B. Sch  X olkopf. Discrete regularization. [27] X. Zhu, Z. Ghahramani, and J. D. Lafferty.

