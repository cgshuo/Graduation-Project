 We present a novel interpretation of Clarity [5], a widely used query performance predictor. While Clarity is com-monly described as a measure of the  X  X istance X  between the language model of the top-retrieved documents and that of the collection, we show that it actually quantifies an addi-tional property of the result list, namely, its diversity. This analysis, along with empirical evaluation, helps to explain the low prediction quality of Clarity for large-scale Web col-lections.

Many query performance predictors were proposed over the years [2]. Clarity [5] is a well known, commonly used, state-of-the-art predictor that measures the  X  X oherence X  of the top-retrieved documents with respect to the collection. Specifically, the more distinguishable the language used in the retrieved documents from the general language used in the collection, the better the retrieval is assumed to be C larity was shown to be highly effective for most TREC benchmarks [6]. However, low prediction quality is observed when using Clarity for large scale, noisy, Web corpora [1].
We present a novel formal analysis of Clarity that sheds some light on its underlying components and the properties of the result list of top-retrieved documents that it quanti-fies. While Clarity is commonly described as a measure of the  X  X istance X  between a language model induced from the result list and that induced from the collection, we show that Clarity actually quantifies an additional property of the re-sult list, namely, its diversity. Our empirical analysis shows that the diversity of the result list has a negative correla-tion with retrieval performance for older TREC benchmarks and a positive correlation for the new ClueWeb collection. These findings, along with the formal analysis, help to ex-plain the poor prediction quality of Clarity over ClueWeb.
There are are several variants of clarity, among which is a p re-retrieval method (SCS [7]) that considers only the query and the corpus and not the retrieved documents.
 In addition, the novel interpretation we present suggests new integration approaches of Clarity X  X  building blocks.
Let q and D denote a query and a corpus of documents, respectively. In what follows we use p ( w | x ) to denote the probability assigned to term w by a unigram (smoothed) language model induced from x .

The query likelihood (QL) retrieval method scores doc-ument d by log p ( q | d ) def = log Q q t erm in q . Let D [ k ] q denote the result list of the k h ighest ranked documents. The assumption behind Clarity is that the higher the divergence of a model of D [ k ] q from that of the c orpus, the higher the effectiveness of D [ k ] q is, and thereby, t he better the quality of the QL-based retrieval. The KL divergence between a relevance language model, R , induced from D [ k ] q , and a language model induced from D , is used to quantify this divergence. R is a weighted linear mixture of the models of documents in D [ k ] q [5]. Accordingly, the C larity of q is defined as: Clarity ( q ) def = K L p (  X | R ) p (  X |D ) = X
As is the case for any two probability distributions, we can write the KL divergence as follows: KL p (  X | R ) p (  X |D ) = CE p (  X | R ) p (  X |D )  X  H ( p (  X | R )) ; CE is the cross entropy between R and D , H is the entropy of the relevance model,
Under this decomposition, Clarity integrates two mea-sures (building blocks). The first is the  X  X istance X  of R from the corpus, as measured by the cross entropy. The more distant R from D , the higher the cross entropy is. We use CDistance (for  X  X orpus distance X ) to refer to the cross en-tropy between R and D .
 The second measure used by Clarity is the entropy of R . High entropy means that R assigns relatively low weights (i.e., probabilities) to a large number of terms; thereby, D is highly diverse. In contrast, low entropy means that only a few terms are highly weighted, hence D [ k ] q is more focused. W e use LDiversity (for  X  X ist diversity X ) to refer to R  X  X  en-tropy. Next we study the prediction quality of each of these building blocks and compare it with that of Clarity which amounts to their equal-weight linear interpolation:
We conducted experiments using the following TREC bench-marks (disks and topics are indicated in the parentheses): TREC4 (disks 2-3; 201-250), TREC5 (disks 2,4; 251-300), WT10G (WT10G; 451-550), Robust (disks 4-5 -{ CR } ; 301-450, 601-700), GOV2 (GOV2; 701-850), and the ClueWeb collection (category B). Two sets of topics were used for ClueWeb: Clue09 (1-50) and Clue10 (51-100). We applied Porter stemming and stopword removal upon queries and documents using the Lemur/Indri toolkit.

Previous work hypothesized that the low prediction qual-ity of Clarity over Web collections is due to the large amount of noise (e.g. spam) [6]. To address spam effects in ClueWeb, we filtered out the spammiest documents from the result lists (those assigned a spam score below 50 by Waterloo X  X  clas-sifier [4]) and retained the original ranking for the residual corpus. Thus, we get two additional experimental setups for ClueWeb: Clue09+SpRM and Clue10+SpRM.
 We use the predictors to predict the performance of the QL retrieval method specified above; unigram Dirichlet-smoothed document language models are used with the smoothing parameter set to 1000. We study three predictors: CDis-tance, LDiversity, and Clarity which interpolates the two (see Equation 1). Following the common practice to evalu-ating prediction quality [2], we report Pearson X  X  correlation between the values assigned by the predictor and retrieval effectiveness measured by average precision computed using TREC X  X  relevance judgments. The size of the result list, k , was set to 500 following previous recommendations [6]. The relevance models were clipped to use only the 100 highest weighted terms. The language models of documents used to construct the relevance model were not smoothed.
 Figure 1: Comparing the prediction quality of Clar-ity and its building blocks.

Figure 1 presents our main results. To simplify the presen-tation, we refer to the ClueWeb benchmarks as  X  X lueWebs X  (on the left of the graph) and to all the other benchmarks as  X  X mallScales X  (on the right). The differences of the patterns observed for  X  X lueWebs X  and  X  X mallScales X  are as follows. First, CDistance is not very effective over  X  X lueWebs X  in comparison to  X  X mallScales X . We attribute this finding to the low quality of the collection statistics in a noisy Web setup that affects the estimation of the corpus language model. Evidently, spam removal did not improve prediction quality over ClueWeb.

Second, LDiversity and retrieval performance have posi-tive correlation for  X  X lueWebs X , yet negative correlation is observed for  X  X mallScales X . We presume that list coherence can attest to improved retrieval effectiveness, as is implied by the findings for  X  X mallScales X , which are mainly com-posed of unambiguous (coherent) queries. On the other hand, list diversity might correspond to improved retrieval performance, as is implied by the findings for  X  X lueWebs X , for ambiguous queries, if it attests to coverage of various query aspects. It was found that for Clue09 topical diver-sity and retrieval performance are strongly correlated [3].
Following the observations made above, we can explain the low effectiveness of Clarity for  X  X lueWebs X ; Clarity, as presented in Equation 1, is the subtraction of prediction values assigned by two predictors which are both positively correlated with retrieval effectiveness. Usually, two predic-tors that are positively correlated with retrieval performance are integrated by multiplication or summation [2]. Thus, the subtractive integration of CDistance and LDiversity, as implemented by Clarity, yields low quality prediction over  X  X lueWebs X .
We showed that Clarity amounts to an equal weight in-terpolation of two predictors; one measures the  X  X istance X  of the result list from the collection, while the second measures the list X  X   X  X iversity X . We used this formal finding to help ex-plain the low prediction quality of Clarity over ClueWeb, in contrast to its high effectiveness over other TREC bench-marks. Preliminary results of using non-equal weights for the interpolation mentioned above, and independently op-timizing free-parameter values for each predictor, attest to the merits of these future directions. (Actual numbers are omitted due to space considerations.) Acknowledgments. We thank the reviewers for their com-ments. This paper is based upon work supported in part by the Israel Science Foundation under grant no. 557/09, by IBM X  X  SUR award, by an IBM Ph.D. Fellowship and by Miriam and Aaron Gutwirth Memorial Fellowship. Any opinions, findings and conclusions or recommendations ex-pressed in this material are the authors X  and do not neces-sarily reflect those of the sponsors.
