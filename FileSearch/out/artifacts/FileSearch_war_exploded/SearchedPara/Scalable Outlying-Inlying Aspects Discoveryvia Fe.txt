 Nguyen Xuan Vinh 1( In this paper, we are interested in the novel and practical problem of investigat-ing, for a particular query object, the aspects that make it most distinguished mining , although it has also been known as outlying subspaces detection [ 15 ], Outlying aspects mining has many practical applications. For example, a home buyer would be highly interested in finding out the features that make a par-ticular suburb of interest stand out from the rest of the city. A recruitment panel may be interested in finding out what are the most distinguishing merits of a particular candidate compared to others. An insurance specialist may want to find out what are the most suspicious aspects of a certain insurance claim. A natural complementary task to outlying aspects mining is inlying aspects min-ing , i.e., what features make the query most usual. we present the outlying-inlying aspects returned by our proposed approach X  OARank  X  X or player Kyle Korver in the NBA Guards dataset (data details given in Section 4.1 ). NBA sports commentators are interested in the features that make a player most unusual (and maybe also usual). If we take the kernel density rank as an outlyingness measure, then it can be observed that in the top 2D and 3D-inlying subspaces, Kyle has very low density ranking (198th and 168th over 220 players respectively). The attributes in which Kyle appears most usual are  X  X ebound (Offensive) X ,  X  X ree throw (Percentage) X  and  X  X ield goal (Percentage) X . On the other hand, in the top outlying subspaces, Kyle has very high density rank (2nd and 5th over 220 players respectively). Kyle is indeed very good at 3-points scoring:  X 3-points (Attempted) X ,  X 3-points (Made) X , and  X 3-points (Percentage) X .
 In this context, we only focus on the query object, which itself may or may not that make it most outlying. We are also not interested in other possible outliers in the data, if any. In contrast, outlier detection aims to identify all possible outlying objects in a given data set, often without explaining why such objects are considered as outliers. Outlier explanation is thus a complementary task to Thus, in this paper, we shall employ the term outlying aspects mining ,whichis more generic than outlier explanation. 1.1 Related work The latest work on outlying aspects mining can be categorized into two main search approaches [ 5 ]. tion. More specifically, the two classes are defined as the query point (positive the positive class is over-sampled with samples drawn from a Gaussian distri-bution centred at the query point, while the negative class is under-sampled, keeping k full-space neighbors of the query point and some other data points from the rest of the data. Similarly in [ 12 ], the positive class is over-sampled while keeping all other data points as the negative class. The feature subsets that result in the best classification accuracy are regarded as outlying features and selected for user inspection.

A similar approach to feature selection is feature transformation [ 2 ], which identifies a linear transformation that best preserves the locality around the neighborhood of the query point while at the same time distinguishing the query from its neighbors. Features with high absolute weights in the linear transformation are deemed to contribute more to the outlyingness of the query.  X  X n score-and-search based methods , a measure of outlyingness degree is needed.
The outlyingness degree of the query object will be compared across all pos-sible subspaces, and the subspaces that score the best will be selected for further user inspection. In [ 5 ], the kernel density estimate was proposed as an outlyingness measure. It is well known, however, that the raw density mea-sure tends to be smaller for subspaces of higher dimensionality, as a result of increasing sparseness. For this reason, the rank statistic was used to calibrate the raw kernel density to avoid dimensionality bias. Having defined an outly-ingness measure, it is necessary to search through all possible subspaces and enumerate the ones with lowest density rank. Given a dataset of dimension the number of subspaces is (2 d  X  1). If the user specifies a parameter the maximum dimensionality, then the number of subspaces to search through is in the order of O ( d d max ). 1.2 Contribution In this paper, we advance the state of the art in outlying aspects mining by making two important contributions. First, we show an insightful theoretical result connecting the two seemingly different approaches of density-based score-and-search and feature selection for outlying aspects mining. In particular, we show that by using a relevant measure of mutual information for feature selec-tion, namely the quadratic mutual information, density minimization can be regarded as contributing to maximizing the mutual information criterion. Sec-ond, as exhaustive search for subspaces is expensive, our most important con-tribution in this paper is to propose an alternative scalable approach, named OARank , in which the features are ranked based on their potential to make the query point having low density. The top-ranked features are then selected either for direct user inspection, or for a more comprehensive score-and-search with the best-scored subspaces then reported to the user. The feature ranking procedure takes only quadratic time in the number of dimensions and scales linearly w.r.t the number of data points, making it much more scalable and suit-able for mining large and high dimensional datasets, where a direct enumeration strategy is generally infeasible. In the feature selection approach, the problem of explaining the query is first posed as a two-class classification problem, in which we aim to separate the query q (positive class c 1 ) from the rest of the data O (negative class n objects { o 1 ,..., o n } , o i  X  R d .Let D = { D 1 ,...,D a Gaussian distribution centred at the query. The task is then to select the top features that distinguish the two classes. These features are taken as outlying features for the query.
 has a close connection to density based approaches. Let us form a two-class data set Note that here we have over-sampled the positive class simply by duplicating q n times, so that the classification problem is balanced. Mutual information based feature selection aims to select a subset of information shared between the data and the class variable is maximized, i.e., space S and C is the class variable. We will show that by using a particular measure of entropy coupled with the Gaussian kernel for density estimation, we arrive at a formulation reminiscent of density minimization. In particular, we as: Havrda-Charvat X  X  entropy reduces to Shanon X  X  entropy in the limit when hence it can be viewed as a generalization of Shannon X  X  entropy [ 7 , 9 ]. version of Havrda-Charvat X  X  entropy with  X  = 2, also known as quadratic Havrda-Charvat X  X  entropy H 2 ( X )=1  X  f ( x ) 2 d x (with the normalizing con-stant discarded for simplicity). Using the Gaussian kernel G ( x  X  X i , X  2 )=(2  X  X  2 )  X  d/ 2 probability density of X is estimated as  X  f ( x )= 1 2 n quadratic entropy of X can be estimated as: wherein we have employed a nice property of the Gaussian kernel, which is that the convolution of two Gaussian remains a Gaussian [ 14 ]: The conditional quadratic Havrda-Charvat X  X  entropy of X given a (discrete) variable C is defined as H 2 ( X | C )= K k =1 p ( c k ) H then
H mutual information between X and C is estimated as: where An interesting interpretation for the quadratic mutual information is as fol-between two data points, which can be called the information potential [ 3 ]. The within the negative class and 2 n i,j = n +1 G ( X i  X  X j of intra-class interaction within the positive class (within-class total informa-tion potential), thus CC is a measure of class compactness . On the other hand, mation potential), thus CS is a measure of class separability . For maximizing I ( X ,C ), we aim to maximize intra-class interaction while minimizing inter-class interaction.
 Theorem 1. Density minimization is equivalent to maximization of class sep-arability in quadratic mutual information based feature selection. Proof. Note that since the positive class contains only q (duplicated we have maximizing class separability.
 density-based score-and-search and feature selection based approaches for out-lying aspects mining. Minimizing the density of the query will contribute to maximizing class separation, and thus maximizing the mutual information cri-terion. This insightful theoretical connection also points out that the mutual-information based feature selection approach is more comprehensive, in that it also aims to maximize the class compactness. The relevance of class-compactness to outlying aspects mining is yet to be explored. We now present the main contribution of this paper X  OARank  X  X  hybrid app-roach for outlying aspects mining that leverages the strengths of both the feature selection and the score-and-search paradigms. This is a two-stage approach. In the first stage, we rank the features according to their potential to make the query outlying. In the second (and optional) stage, score-and-search can be per-formed on a smaller subset of the top-ranked m d features. 3.1 Stage 1: OARank X  X utlying Features Ranking We aim to choose a subset of m features S  X  D such that the following criterion is minimized: where K ( x  X   X , h )=(2  X h 2 )  X  1 / 2 exp { X  ( x  X   X  ) 2 Gaussian kernel with bandwidth h and center  X  ,and C ( m )= normalization constant.
 function in SS can be seen as a kernel density estimate at the query point q .To see this, we first develop a novel kernel function for density estimation, which is the sum of 2-dimensional kernels. Herein, we employ the Gaussian product kernel recommended by Scott [ 11 ], defined as: where h j  X  X  are the bandwidth parameters in each individual dimension. We note that, in the product kernel ( 4 ), a particular dimension (feature) can be  X  X e-emphasized X , by assigning its corresponding 1D-kernel to be the  X  X niform X  kernel, or more precisely, a rectangular kernel with sufficiently large bandwidth Note that  X  u can be chosen to be arbitrarily large, so that we can assume that any query point of interest q will lie within  X  u -distance from any kernel center o in any dimension. In our work, we normalize the data (including the query) so that o ij  X  [  X  1 , 1] in any dimension, thus  X  u could simply be chosen as From the product kernel ( 4 ), if we de-emphasize all dimensions, but keeping only two  X  X ctive X  dimensions { t, j } , then we obtain the following Averaging over all d ( d  X  1) / 2 pairs of dimensions yields the following kernel: probability density function.
 Proof. It is straightforward to show that K ( q  X  o i , h ) 1 Employing this new kernel to estimate the density of the query point q in a subspace S  X  D , we obtain exactly the objective function in minimized will minimize the density at the query q , i.e., making q most outlying. 3.2 Solving the Outlying-Inlying Aspects Ranking Problem The subset selection problem SS can be equivalently formulated as a quadratic integer programming problem as follows: lently, we can rewrite it in a maximization form as where Q tj =  X   X  Q tj and  X  = max i,j Q ij . While ( 8 ) and ( 9 ) are equivalent, the Hessian matrix Q is entry-wise non-negative, a useful property that we will exploit shortly. The parameter m specifies the size of the outlying subspace we wish to find. It is noted that SS and QIP are not monotonic with respect to m , i.e., with two different m values, the resulting outlying subspaces are not necessarily subsets of one another.
 domain, as follows. Note that with w i  X  X  0 , 1 } , w i =  X  m . We shall now drop the integral 0-1 constraint, which in fact causes NP-hardness, while keeping the norm constraint: The additional non-negativity constraints w i  X  0 ensure that the relaxed solu-tion can be reasonably interpreted as feature  X  X otential X  in making q outlying. Also note that we can replace w 2 = the optimal relative weight ordering (all the weights w i multiplicative constant 1 / Observe that since Q ij  X  0, the solution to this problem is simple and straight-forward: it is the dominant eigenvector associated with the dominant eigenvalue of the Hessian matrix Q [ 13 ]. Note that with this relaxation scheme, the param-eter m has been eliminated, thus OARank will produce a single ranking. The outcome of this quadratic program can be considered as feature potentials: fea-tures that have higher potentials contribute more to the outlyingness of the query. Features can be ranked according to their potentials. The top-ranked features will be chosen for the next score-and-search stage.
 (subspaces) in which the query point appears to be most usual . 3.3 Stage 2: OARank+Search Having obtained the feature ranking in stage 1, there are two ways to proceed: (i) One can take the top k -ranked features as the single most outlying subspace of size k . A more flexible way is to (ii) perform a comprehensive score-and-search on the set of top-ranked m d features, and report a list of top-scored subspaces for user inspection. The search on the filtered feature set is however much cheaper than a search in the full feature space. 3.4 Complexity Analysis Ranking stage : The cost of building the Hessian matrix Q is of the ranking process is O ( d 2 n ).
 Score-and-Search stage : If we employ the density rank measure, the cost for scoring (i.e., computing the density rank for the query) in a single subspace is O ( n ) time. The number of subspaces to score is O (2 m  X  1) for exhaustive search, or O ( m d max ) if a maximum subspace size is imposed. Note that for practical applications, we would prefer the subspaces to be of small dimensionality for improved interpretability, thus it is practical to set, for example, overall complexity of this stage is O ( n 2 m d max ) Overall, the complexity of both stages is O ( d 2 n posed two-stage approach. For comparison, a direct density rank based score-and-search approach on the full space costs O ( n 2 d d max d is moderately large.
 Techniques for further improving scalability : While the ranking phase of OARank is generally very fast, the search phase can be slow, even on the reduced feature set. To further speed up the search phase, one can further prune the search space. In particular, the search space can be explored in a stage-wise manner, expanding the feature sets gradually. In exhaustive search, every feature set of size k will be expanded to size k + 1 by adding 1 more feature. We can improve the efficiency of this process, sacrificing comprehensiveness, by only choosing a small subset of most promising subspaces (i.e., highly-scored) of size k to be expanded to size k +1. In this section, we experimentally evaluate the proposed approaches, OARank and OARank+Search . We compare our approaches with the density rank based approach in [ 5 ] and Local Outlier with Graph Projection (LOGP) [ 2 ]. LOGP is the closest method in spirit to OARank, in that it also learns a set of weights: features with higher weights contribute more to the outlyingness of the query. These weights are from a linear transformation that aims to separate the query from its full-space neighborhood. LOGP was proposed as a method for both detecting and explaining outliers. We implemented all methods in Mat-lab/C++ except LOGP for which the Matlab code was kindly provided by the authors. The parameters for all methods were set as recommended in the original articles [ 2 , 5 ]. The bandwidth parameter for OARank was set according to [ 5 ], i.e., h =1 . 06 min {  X , R R being the difference between the first and third quartiles of the data distribu-tion. In order to improve the scalability of score-and-search based methods, we apply a stage-wise strategy as discussed in Section 3.4 where only at most 100 top-scored subspaces are chosen for expansion at each dimension All experiments were performed on an i7 quad-core PC with 16Gb of main mem-ory. The source code for our methods will be made available via our website. 4.1 The NBA Data Sets We first test the methods on the NBA data available at http://sports.yahoo. com/nba/stats . This data set was previously analyzed in [ 5 ], where the authors collected 20 attributes for all NBA guards, forwards and centers in the 2012-2013 season, resulting in 3 data sets. We compare the quality of the ranking returned by OARank and LOGP. More specifically, for each player, we find the top 1, 2 and 3 inlying and outlying features, and then compute the density rank for the player in his outlying-inlying subspaces. in Fig. 2 . It can be clearly seen that OARank is able to differentiate between inlying and outlying aspects. More precisely, in the outlying subspaces (of the top-ranked 1/2/3 features), all players tend to have higher density rank than their ranks in the inlying subspaces (of the bottom-ranked 1/2/3 features). On the same data set, LOGP ranking does not seem to differentiate well between outlying and inlying features. In particular, the rank distribution appears to be uniform in both inlying and outlying subspaces. Thus, in this experiment, qualitatively we can see that OARank is more effective at identifying inlying-outlying aspects. The same conclusion applies for the NBA Guards and Centers data sets, for which we do not provide detailed results due to space restrictions. The feature weights and ranking returned by OARank for Kyle Korver can be inspected in Fig. 3 (e). 4.2 Mining Non-Trivial Outlying Subspaces 1000 data points and 19 to 136 outliers. These outliers are challenging to detect, as they are only observed in subspaces of 2 to 5 dimensions but not in any lower dimensional projection. We note again that our task here is not outlier detection, but to explain why the annotated outliers are designated as such. For this data set, since the ground-truth (i.e., the outlying subspace for each outlier) is available as part of Keller et al. X  X  data, we can objectively evaluate the performance of all approaches. Let the true outlying subspace be the retrieved subspace be P . To evaluate the effectiveness of the algorithms, we employ the Jaccard index Jaccard ( T,P ) | T  X  P | / | T  X  P | for different approaches on all datasets are reported in Figure 3 (a,b). We can observe that OARank and the density based score-and-search app-roach both outperform LOGP. OARank (without search) obtains relatively good results, slightly better than density rank score-and-search at higher dimensions. OARank (with search) did not seem to improve the results significantly on this of magnitude faster than density rank score-and-search. It can also be observed that the OARank+Search approach admits a near-flat time complexity profile with regards to the number of dimensions. This agrees well with the theoreti-cal complexity of O ( d 2 n ) for ranking and O ( n 2 m d max sets, the ranking time was negligible compared to search time, while the search complexity of O ( n 2 m d max ) is independent of dimensionality. 4.3 Scalability We tested the method on several large datasets. We pick the largest of Keller X  X  data sets of 1000 points in 100 dimensions, and introduce more synthetic exam-ples by drawing points from a Gaussian distribution centred at each data points, resulting in several large data sets of size ranging from 50,000 to 1 million data points. The run time for OARank and LOGP is presented in Figure 3 (d). It is noted that for these large datasets, the search phase using the density score is computationally prohibitive, due to quadratic complexity in data size LOGP and OARank deal well with large datasets, with linear time complexity in the number of data points. This observation matches well with OARank X  X  theoretical complexity of O ( d 2 n ) and demonstrates that OARank is capable of handling large data sets on commodity PCs.
 for applications on streaming data: as data come in, entries in the Hessian matrix Q can be updated gradually. Feature weights can also be updated on-the-fly in real time, given that there exist very efficient algorithms for computing the dominant eigenvector of symmetric matrices. In this paper, we have made two important contributions to the outlying aspects mining problem. First, we have made an insightful connection between the den-sity based score-and-search and the mutual information based feature selection approach for outlying aspects mining. This insight can inspire the development of further hybrid approaches, which leverage the strengths of both paradigms. Second, we proposed OARank, an efficient and affective approach for outlying We show that OARank is suitable for mining very large data sets.

