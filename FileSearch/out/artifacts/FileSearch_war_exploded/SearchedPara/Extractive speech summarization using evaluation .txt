 1. Introduction
Due to the rapid development and maturity of multimedia technology, large volumes of information content have been mation about multimedia content. However, unlike text documents, which are structured with titles and paragraphs and are thus easier to retrieve and browse, the associated spoken documents of multimedia content are only presented with video or spoken documents are automatically transcribed into words, incorrect information (resulting from recognition errors and inaccurate sentence or paragraph boundaries) and redundant information (generated by disfluencies, fillers, and repetitions) would prevent them from being accessed easily. Speech summarization, which attempts to distill important information and remove redundant and incorrect content from spoken documents, can facilitate users to review spoken documents effi- X 
Automatic summarization of text documents dates back to the early 1950s ( Baxendale, 1958 ). Nowadays, the research is extended to cover a wider range of tasks, including multidocument, multilingual, and multimedia summarization. Broadly speaking, summarization can be either extractive or abstractive. The former selects important sentences or paragraphs from an original document according to a target summarization ratio and concatenates them to form a summary; the latter, on the other hand, produces a concise abstract of a certain length that reflects the key concepts of the document, thus requiring highly sophisticated natural language processing techniques, like semantic inference and natural language generation, to name a few. Thus, in recent years, researchers have tended to focus on extractive summarization. In addition to being extrac-tive or abstractive, a summary may also be generated by considering factors from other aspects like being generic or query-oriented. A generic summary highlights the most salient information in a document, whereas a query-oriented summary presents the information in a document that is most relevant to a user X  X  query. Interested readers may refer to ( Mani &amp; Maybury, 1999 ) for a comprehensive overview of the principal trends and the classical approaches for text summarization.
This paper focuses exclusively on generic, extractive speech summarization since it usually constitutes the essential building block for many other speech summarization tasks. It should also be mentioned that speech summarization presents opportunities that do not exist for text summarization; for instance, information cues about prosody/acoustics and emotion/ speakers can help the determination of the importance and structure of spoken documents ( Christensen, Gotoh, &amp; Renals, 2008; McKeown, Hirschberg, Galley, &amp; Maskey, 2005 ). We thus set the goal at selecting the most representative sentences non-lexical features, to form the summary for a given spoken document. In particular, we have recently introduced a new perspective on the problem of speech summarization, saying that some potential defects of the existing supervised speech summarizer to the evaluation metric ( Lin, Chang, Liu, &amp; Chen, 2010 ).

Our work in this paper continues this general line of research, including exploring and comparing more speech summa-rizers developed along this line of research and providing more in-depth elucidations of their modeling characteristics and summarizer in a pair-wise rank-sensitive manner ( Burges et al., 2005; Cao et al., 2006; Herbrich, Graepel, &amp; Obermayer, spoken document, but also at the correct ordering (preference) relationship of each sentence pair in accordance with their respective importance to the document. Nevertheless, it turns out that this attempt in essence would be loosely related to the evaluation metric. In this regard, the other attempt is instead to train the summarizer by directly maximizing the eval-uation. Furthermore, we extensively study and evaluate the utility of augmenting the feature set of supervised summarizers with more indicative features derived from various unsupervised summarizers.

The remainder of this paper proceeds as follows. Section 2 reviews the conventional approaches to speech summariza-how they can be exploited for speech summarization. Section 4 describes a variety of features that are generated to represent spoken documents and sentences. Then, the experimental settings and a series of summarization experiments are presented in Section 5 . Finally, Section 6 concludes our presentation and discusses avenues for future work. 2. Related work 2.1. Supervised summarizers
As to the development of speech summarizers, quite several machine-learning methods have been explored with some 2007 ), and they may broadly fall into two main categories: supervised and unsupervised speech summarizers. Supervised summarizers usually formulize the speech summarization task as a two-class (summary/non-summary) sentence-classifica-tion problem: Each sentence S i in a spoken document to be summarized is associated with a set of M indicative features
X class-specific importance (or decision) score to each sentence S the document can be iteratively selected into the summary based on their scores until the length limitation or a desired sum-marization ratio is reached. During the training phase, a set of training spoken documents D ={ d of N documents and the corresponding handcrafted summary information, is given. The summarizer is trained in the sense of reducing the classification (labeling) errors of the summarizer made on the sentences of these training spoken document exemplars. It is expected that minimizing these errors caused by the summarizer would be equivalent to maximizing the lower bound of the summarization evaluation score (usually, the higher the score, the better the performance). Representa-tive techniques include, but not limited to, Bayesian classifier (BC), support vector machine (SVM), and conditional random fields (CRF) ( Lin, Chen, &amp; Wang, 2009 ). Among them, support vector machines (SVM) has prominently used to formulate and 2007 ). An SVM summarizer is developed under the basic principle of structural risk minimization (SRM) in the statistical learning theory. If the dataset is linear separable, SVM attempts to find an optimal hyper-plane by utilizing a decision function that can correctly separate the positive and negative samples, and ensure the margin is maximal. In the nonlinear separable case, SVM uses kernel functions or defines slack variables to transform the problem into a linear discrimination problem. In this paper, we use the LIBSVM toolkit ( Chang &amp; Lin, 2001 ) to construct a binary SVM summarizer, and adopt the radial basis function (RBF) as the kernel function. The posterior probability of a sentence S mary class S can be approximated by the following sigmoid operation: where the weights a and b are optimized by the development set, and g ( X summarizer. Once the SVM summarizer has been properly constructed, the sentences of the spoken document to be sum-marized can be ranked by their posterior probabilities of being in the summary class. The sentences with the highest prob-abilities are then selected and sequenced to form the final summary according to different summarization ratios.
The imbalanced-data (or skewed-data) problem, however, might strongly affect the performance of a supervised speech summarizer. This problem stems from the fact that the summary sentences of a given training spoken document usually are in a smaller portion (e.g., 10%) as compared to non-summary ones. When training a supervised summarizer on the basis of such an imbalanced-data set, the resulting summarizer tends to assign sentences of the document to be summarized to the class of non-summary sentences (viz. the majority class), thereby leading to high classification accuracy over the class of non-summary sentences but poor accuracy over the class of summary sentences (viz. the minority class). Several heuristic methods have been proposed to relieve this problem, like re-sampling (up-sampling, down-sampling, or both) or re-hand, higher sentence classification accuracy does not always imply better summarization quality. This is mainly because that the summarizer usually classifies each sentence individually with little consideration of relationships among the sen-tences of the document to be summarized. 2.2. Unsupervised summarizers
Another stream of thought attempts to conduct document summarization based on some statistical evidences between each sentence and the document, without recourse to manually labeled training data. We may name them unsupervised 2005 ), Markov Random Walk (MRW) ( Wan &amp; Yang, 2008 ) and so on, conceptualize the document to be summarized as a network of sentences, where each node represents a sentence and the associated weight of each link represents the lexical similarity relationship between a pair of nodes. Document summarization thus relies on the global structural information embedded in such conceptualized network, rather than merely considering the local features of each node (sentence). Put simply, sentences more similar to others are deemed more salient to the main theme of the document. Some other studies investigate the use of probabilistic models to capture the relationship between sentences and the document ( Daum X III &amp;
Marcu, 2006; Nenkova, Vanderwende, &amp; McKeown, 2006 ). Yet, there is a recent attempt that employs a probabilistic ranking framework for speech summarization, where the summarization task is conducted in a purely unsupervised manner ( Chen generating the document content or the probabilistic distance between each sentence model and the document model ( Lin,
Yeh, &amp; Chen, 2011 ). Even though the performance of the abovementioned unsupervised summarizers is usually worse than that of supervised summarizers, their domain-independent and easy-to-implement properties still make them attractive. 3. Proposed speech summarization methods 3.1. Learning to rank with pair-wise preference information for example, can be simply set to 2 representing that a sentence can have the label of being either a summary ( l summary ( l 2 ) sentence. The elements in the rank set have a total ordering relationship l zation, which considers not only the importance of sentences to a training spoken document but also the order of each sen-2005 ). Each of these methods has its own merits and limitations; however, to our knowledge, this criterion has not yet been extensively explored in the context of speech summarization. Thus, in this paper we take Ranking SVM ( Cao et al., 2006 )as an example to implement this strategy for speech summarization, since it has shown to offer consistent improvements over summarization, the training objective of Ranking SVM is to find a ranking function that can correctly determine the prefer-ence relation between any pair of sentences: where l ( ) denotes the label of a sentence and f ( ) denotes the decision value of a sentence provided by Ranking SVM. For a more thorough and entertaining discussion of Ranking SVM, interested readers can refer to ( Cao et al., 2006 ). 3.2. Training summarizers with objectives related the evaluation metric
Although reducing the sentence classification (e.g., SVM) or ranking (e.g., Ranking SVM) errors would be equivalent to maximizing the lower bound of the performance evaluation score of a given summarization system, it is still not closely re-lated enough to the final evaluation metric for speech summarization. Recently, quite a few approaches have been proposed to train an IR system by directly maximizing the associated evaluation score. For instances, Joachims (2005) presented an
SVM-based method for directly optimizing multivariate nonlinear performance measures like the F1-score or Precision/Re-call Breakeven Point (PRBEP) adopted in document classification. On the other hand, Cossock and Zhang (2006) discussed the issue of learning to rank with preference to the top scoring documents of a given training query. More recently, Xu and Li (2007) proposed an ensemble-based algorithm that can iteratively optimize an exponential loss function based on various speech summarization, and the AdaRank training algorithm and two novel discriminative training objectives are taken as the initial attempts. 3.2.1. The AdaRank training algorithm
The fundamental premise of AdaRank basically lies in that ensemble-based systems may produce more favorable results set of weak rankers (or ranking functions) and integrates them through a linear combination to form the final ranking model emphasize those training spoken documents having more sentences incorrectly ranked by the previously selected weak ran-kers, which actually is evidenced by the corresponding summarization performance of the training spoken documents. Con-secutive rankers are concentrated on dealing with those  X  X  hard-to-summarize  X  X  training spoken documents. AdaRank, therefore, belongs to a kind of the  X  X  X irect optimization X  X  training algorithms.

A bit of terminology: Given a set of training spoken documents H  X f X  d of sentences in a document d n provided by human subjects, AdaRank will select, at each iteration t , a single summarization feature x t ( cf. Section 5 ) that has the best overall evaluation performance on the training documents: where P ( d n , x t ) is the automatically generated summary with a specific ordering of selected sentences; E ( P ( d notes the summarization evaluation performance on the document d from 0 to 1 (the higher the value the better the performance); w iteration t , which can be further expressed by where E  X  ^ P  X  d n ; X t 1  X  ; Y n  X  is the summarization performance on the document d from iterations 1 to t 1, i.e., X t 1 ={ x 1 , x 2 , ... , x ment having higher summarization performance with the features selected so far (or during the previous t 1 iterations) will play a less pronounced role at the current iteration. Also noteworthy is that as a specific summarization feature x being selected at iteration t , its corresponding weight a
At the end, with the completion of iteration t 0 , we can rank a spoken sentence S expressed by where g ( S i , x m ) is the corresponding decision value of the selected feature x 3.2.2. Discriminative training of speech summarizers
In recent years, there has been a growing interest in developing discriminative training algorithms for reranking of hypotheses output from a baseline speech recognition system in an attempt to optimize the final performance measure resemblance to Ranking SVM and AdaRank in their functionality, and are therefore anticipated to carry over well to extrac-tive speech summarization. However, such a conception of discriminative training has never been extensively explored for speech summarization, as far as we know. Hence, in this paper, we investigate to leverage discriminative training to estimate give a decision score to an arbitrary sentences S i of a spoken document d ability P GCLM  X  S i d n j  X  which is approximated by where X i is the M -dimensional feature vector X i of S i product of X i and f ; and L n is the total number of sentences in d training paradigm for speech summarization, we define and optimize the following training objective so as to estimate the parameter vector f of GCLM: where Summ n is the reference summary of a training document d tained by comparing a sentence S l of d n to Summ n with a desired evaluation metric that will return a score ranging between the posterior probabilities of the summary sentences of all training spoken documents given the summarization model (viz. with the parameter vector f ), but also to boost negative impact of those sentences that have inferior summarization perfor-mance (or are more dissimilar from the reference summary) on the training objective, thus generating more confusable data to the other discriminative training objectives that have been studied and practiced in the acoustic modeling for speech rec-ognition, such as boosted maximum mutual information estimation (boosted MMIE) ( Povey et al., 2008 ) and conditional maximum likelihood estimation (CMLE) ( Roark et al., 2007 ).

In this paper, we also explore the use of an alternative training objective for GCLM, which aims to maximize the expected summarization evaluation scores of all sentences of the training spoken documents: We can see from (9) that by training the GCLM model with the objective F higher posterior probabilities, and vice versa for the non-summary sentences (or those sentences that are more dissimilar from the reference summary). The training objective shown in (9) is close in spirit to those that had ever used in minimum speech recognition and machine translation. 4. Features for speech summarization
Although the above approaches can be applied to both text and spoken documents, the latter presents unique difficulties, such as recognition errors, problems with spontaneous speech, and the lack of correct sentence or paragraph boundaries. To avoid redundant or incorrect content while selecting important and correct information, multiple recognition hypotheses, confidence scores, language model scores, and other grammatical knowledge can be utilized. In addition, acoustic features (e.g., intonation, pitch, energy, and pause duration) can provide important clues for summarization; although reliable and efficient ways to use these acoustic features are still under active research ( Chen &amp; Lin, 2012; Lin et al., 2009; Zhang et al., 2007 ).

In this paper, we use a set of 29 features to characterize a spoken sentence, including the structural feature, the lexical mation of a spoken sentence; lexical features represent the linguistic characteristics; acoustic features describe more about how things are said than what is said, and may provide additional important information for summarization; and relevance features evaluate the relevance between a document and each one of its sentences. For each kind of acoustic features, the minimum, maximum, mean, difference value and mean difference value (indexed from 1 to 5) of a spoken sentence are ex-tracted. The difference value is defined as the difference between the minimum and maximum values of the spoken sen-tence, while the mean difference value is defined as the mean difference between a sentence and its previous sentence.
Semantic Analysis) ( Gong &amp; Liu, 2001 ) and WTM (Word Topic Model) ( Chen, 2009 ) are different unsupervised summarizers, respectively, producing single summarization (relevance) features. VSM represents each sentence of a document, and the whole document, in vector form. In this approach, each dimension specifies the weighted statistics, for example the product of the term frequency (TF) and inverse document frequency (IDF), associated with an indexing term (or word) in the spoken sentence or document. Sentences with the highest relevance scores to the whole document (usually calculated by the cosine score of two vectors) are included in the summary accordingly. VSM solely based on matching the literal words that are pres-ent in the sentences and the document would sometimes fail to include enough relevant sentences in the summary because of the word mismatch problem. LSA is a natural extension of VSM that represents each sentence of a document to be sum-marized in a latent semantic space. To accomplish this, singular value decomposition (SVD) is performed on the  X  X  X erm-sen-tence X  X  matrix of the document, for which the right singular vectors with larger singular values represent the dimensions of the more important latent semantic concepts in the document. Therefore, the sentences with the largest index values in each of the top R right singular vectors are included in the summary. LSA thus exhibits some sort of concept matching.
On the other hand, WTM regards each word w j of the language as a generative M occurrence of another word. To get to this point, all words are assumed to share a same set of K latent topic distributions { T , , T k , , T K }, but have different weights over these topics P  X  T distribution P ( w | T k ) for observing an arbitrary word w of the vocabulary:
Each sentence S of a document d (to be summarized) can be viewed as a composite WTM model for generating the document: as a kind of language model for translating any word occurring in S to an arbitrary word of d . Important sentences are thus selected according to their associated document-likelihoods P introduction to the theoretical background (including the training) and some practical applications of WTM to speech rec-ognition and information retrieval.

Each of the above features is further normalized by the following equation: where l m and r m are, respectively, the mean and standard deviation of a feature x depend on the epoches and genres of spoken documents ( Christensen et al., 2008; Lin et al., 2010 ).
 5. Experiments
In this section, we will describe the experimental setup and then present a series of experiments conducted to assess summarization performance as a function of manual/recognition transcripts, features used for sentence ranking, and differ-ent supervised summarizers that are taken as the vehicle for combining features. 5.1. Experimental setup 5.1.1. Speech and text corpora
The speech data set used in this research is the MATBN corpus ( Lin et al., 2010; Wang, Chen, Kuo, &amp; Cheng, 2005 ), which contains approximately 200 h of Mandarin Chinese TV broadcast news collected by Academia Sinica and the Public Televi-sion Service Foundation of Taiwan between November 2001 and April 2003. The content has been segmented into separate stories and transcribed manually. Each story contains the speech of one studio anchor, as well as several field reporters and interviewees. A subset of 205 broadcast news documents (spoken documents that covered a wide range of topics) compiled between November 2001 and August 2002 was reserved for the summarization experiments. Twenty-five hours of gender-balanced speech from the remaining speech data were used to train the acoustic models for speech recognition. The data was first used to bootstrap the acoustic model training with the MLE criterion. Then, the acoustic models were further optimized 2002 ). The average Chinese character error rate (CER) obtained for the 205 spoken documents was about 35% ( Liu et al., 2007 ). Some basic statistics of the 205 spoken documents are given in Table 2 .

Additionally, a large number of text news documents collected by the Central News Agency (CNA) between 1991 and 2002 (the Chinese Gigaword Corpus released by LDC) were used. The documents collected in 2000 and 2001 were used to train N -gram language models for speech recognition with the SRI Language Modeling Toolkit ( Stolcke, 2005 ). A subset of about 14,000 text news documents, compiled during the same period as the broadcast news documents to be summarized, was used to calculate the IDF statistics of VSM and estimate the parameters of WTM, as mentioned in Section 4 . 5.1.2. Evaluation metric
Three subjects were asked to create summaries of the 205 spoken documents for the summarization experiments as references (the gold standard) for evaluation. The summaries were generated by selecting 50% of the most important sentences in the reference transcript of a spoken document, and ranking them by importance without assigning a score to each sentence. To assess the goodness of the automatically generated summaries, we used the ROUGE measure as the evaluation metric ( Lin, 2003; Liu &amp; Liu, 2010 ). The ROUGE measure evaluates the quality of the summarization by counting the number of overlapping units, such as N -grams, longest common subsequences or skip-bigram, between the automatic summary and a set of reference (manually-annotated) summaries. Three widely used variants of the ROGUE measure were adopted to assess the utility of the summarization methods presented in this paper. They are, respectively, the ROUGE-1 (unigram) measure, the ROUGE-2 (bigram) measure and the ROUGE-L (longest common subsequence) mea-sure. Generally speaking, the ROUGE-1 measure is to evaluate the informativeness of automatic summaries while the
ROUGE-2 measure is to estimate the fluency of automatic summaries. On the contrary, ROUGE-L does not reward for fixed-length N -grams but instead for a combination of the maximal substrings of words, which works well in general for evaluating both content and grammaticality. The summarization ratio, defined as the ratio of the number of sentences in the automatic (or manual) summary to that in the manual transcript of a spoken document, was set to 10% in this study.

Table 3 shows the levels of agreement between the three subjects for important sentence ranking. Each of these values was obtained by using the summary created by one of the three subjects as the reference summary, in turn for each subject, while those of the other two subjects as the test summaries, and then taking their average. These observations seem to reflect the fact that people may not always agree with each other in selecting the important sentences for representing a given document.
 5.2. Experimental results 5.2.1. Baseline results by using single features
At the outset, we examine the summarization performance when sentence ranking acts on different single features (or unsupervised summarizers, cf. Table 1 ) that were derived based on the recognition transcripts along with their correspond-ing audio segments (denoted by SD, spoken documents). The associated results are graphically illustrated in Fig. 1 . In addi-tion, the results based on the manual transcripts of spoken documents (denoted by TD, text documents) are also sketched in
Fig. 1 for reference. For the TD case, the acoustic features were obtained by performing word-level forced alignment of the audio segments of the spoken documents to their corresponding manual transcripts. Inspection of Fig. 1 reveals two partic-the fact that the various ROUGE measures are based on counting the number of overlapping units between the automatic summary and the reference summary. Even though the summary sentences can be correctly selected or identified, the eval-to be more effective than the other simple (or raw) features. This is because the relevance features, to some extent, are de-signed for capturing the importance (or relevance) of a sentence to the whole document or/and the relevance between sen-tences. They thus might be more closely related to the notion of identifying important or relevant sentences from a spoken document.

To take a step further, WTM and MRW are competitive to each other. WTM performs slightly better than MRW when using manual transcripts (i.e., the TD case); however, an opposite phenomenon is witnessed when using recognition tran-scripts (i.e., the SD case). One possible speculation is that, unlike MRW, the model parameters of WTM are all estimated from an outside set of text news documents (cf. Section 5.1.1 ), which somewhat makes WTM unable to faithfully capture the top-ical relationship among words in the imperfect recognition transcripts ( Chen, 2009 ).

Nevertheless, the performance of almost all the features compared here is more or less plagued by speech recognition errors. It has been shown that speech recognition errors are the dominating factor for the performance degradation of spoken document summarization when using recognition transcripts instead of manual transcripts, whereas erroneous sentence boundaries cause relatively minor problems ( Christensen et al., 2008 ). A straightforward remedy, apart from the many ap-proaches improving recognition accuracy, might be to develop more robust representations for spoken documents. For example, multiple recognition hypotheses, beyond the top scoring ones, obtained from M -best lists, word lattices, or confu-sion networks, can provide alternative (or soft) representations for the confusing portions of the spoken documents ( Chelba, well as the pairing of words and subword units, for representing the spoken documents has also been proven beneficial for spoken document summarization ( Chen, Yu, Wang, &amp; Chen, 2006 ). 5.2.2. Summarization using sets of features
Building on the observations made on the above experimental results, in the next set of experiments, we attempt to group the simple (or raw) features, viz. the structural, lexical and acoustic features, together to make them more competitive (de-noted by SET 1). The remaining four relevance features are also grouped together to form another feature set (denoted by SET (for characterizing a spoken sentence) to SVM, and the associated class-specific score output by SVM (cf. Section 2 ) is used for sentence ranking accordingly. Further, the proportions of summary sentences in a training spoken document being used are set in accordance with different ratios (viz. 10%, 20% and 30%) of all the sentences in the document. The corresponding results are presented in Table 4 , in terms of ROUGE-1, ROUGE-2 and ROUGE-L measures. SVM appears to perform better when the numbers of labeled summary and non-summary sentences become more balanced (e.g., 30% summary labels), but its performance will degrade significantly when the numbers of labeled summary and non-summary sentences become more imbalanced (e.g., 10% summary labels). Meanwhile, it is interesting to mention that combining the structural, lexical and acoustic features together (SET 1) tends to provide more indicative cues than combining relevance features together (SET sets of features (ALL) leads to substantial improvements than using each set of features separately. This evidence suggests that these two sets of features seem to be complementary to each other. 5.2.3. Summarization using evaluation metric-related training criteria
In the third set of experiments, we turn our attention on evaluating the utility of Ranking SVM, AdaRank and two variants of GCLM (viz. GCLM-I and GCLM-II), with respect to different feature sets and evaluation metrics being used. The results for the SD case are shown in Table 5 , in terms of ROUGE-1, ROUGE-2 and ROUGE-L measures ( Lin, 2003 ); the corresponding re-sults of SVM are also listed for comparison. Notice here that all these models are learned from the training spoken documents of the development set along with 10% summary labels and then tested on the spoken documents of the evaluation set. As can be seen, the two summarization models stemming from the IR community, viz. Ranking SVM and AdaRank, provide sub-stantial improvements over SVM in the speech summarization task studied here, while AdaRank outperforms Ranking SVM be achieved by AdaRank. The gaps between the actual and the best results are mainly due to that the final ranking model for
AdaRank is optimized by using the development set rather than the evaluation set. Such performance mismatch (in ROUGE-1 that GCLM-I (cf. (8)) and GCLM-II (cf. (9)) are quite comparable to each other and perform on par with AdaRank when using fewer features to represent the spoken sentences (viz. SET 1 or SET2). However, GCLM-I is substantially better than AdaRank and GCLM-II when more (all) features are being used (viz. ALL). This seems to confirm the merit of the GCLM-I training objec-tive. GCLM-I aims not only to maximize the posterior probabilities of training summary sentences but also to emphasize the negative impact of the non-summary sentences that have higher posterior probabilities on the training objective, which can more confusing non-summary sentences, for better model estimation and generalization.

To recap, the superiority of the supervised summarizers (like SVM, Ranking SVM, AdaRank and GCLM) over the unsuper-vised summarizers (like VSM, LSA, WTM and MRW) stem from two factors. The first is that the supervised summarizers make use of the manually-annotated document-reference summary information for model training, whereas the unsuper-vised summarizers do not utilize such information. The second is that most of the unsupervised summarizers rely merely on lexical features (TF-IDF, word or topic unigrams, etc.), whereas the supervised summarizers integrate more indicative fea-speech summarization. They also have the side effect of mitigating the imbalanced-data problem as compared to the tradi-tional SVM approach. 6. Conclusions
In this paper, we have investigated various kinds of summarization features and training criteria for training a speech summarizer; the evaluation metric-related training criteria not only can deal with the imbalanced-data problem but also can boost the summarizer X  X  performance by maximizing the associated evaluation score or optimizing an objective that is clude: (1) investigating more elaborate acoustic features that can be used for speech summarization, (2) seeking other alter-native approaches to optimizing a summarizer X  X  performance (Chen and Lin, 2012), (3) exploring better ways to represent the recognition hypotheses of spoken documents beyond the top scoring ones ( Lin et al., 2011 ), (4) extending and applying the proposed model training paradigms to multi-document summarization tasks, and (5) incorporating the summarization results into audio indexing for better retrieval and browsing of spoken documents. Additionally, how to make effective use of semi-supervised (or even unsupervised) learning to improve the performance of supervised summarizers without recourse to manual annotation and specialized linguistic expertise might also be an important issue for spoken document summarization.
 Acknowledgements
This work was sponsored in part by  X  X  X im for the Top University Plan X  X  of National Taiwan Normal University and Ministry of Education, Taiwan, and the National Science Council, Taiwan, under Grants NSC 99-2221-E-003-017-MY3, NSC 98-2221-E-003-011-MY3, NSC 100-2515-S-003-003, and NSC 99-2631-S-003-002.
 References
