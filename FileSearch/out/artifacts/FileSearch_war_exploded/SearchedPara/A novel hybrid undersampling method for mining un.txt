  X  if  X  then  X  rules on Insurance and 10 rules on churn prediction datasets, 1. Introduction
While deploying machine learning algorithms to binary classi-fi cation problems, data imbalance has inevitably become a chal-lenge to data analysts. In many real world applications such as fraud detection, default prediction, churn prediction, oil-spill detection and network intrusion detection, the class distributions follow 90%:10% proportion and beyond ( Vasu and Ravi, 2011 ). In such problems, machine learning algorithms wrongly predict all the samples of the minority class to be those of majority class. However, this minority class usually will be the more important class. In other words, algorithms tend to be overwhelmed by the majority class and ignore the minority class. Its importance increased as more and more researchers realized that this imbalance causes suboptimal classi fi cation performance, by most algorithms ( Vasu and Ravi, 2011 ).The study of class imbalance problem has been a hot topic in machine learning in recent years ( Diamantini and Potena., 2009 ).
Insurance fraudulent claims blow a severe loss over billions for the insurance companies. These fraudulent claims negatively not only effect the insurance fi rms but also hurt the socio-economic structures ( Xu et al., 2011 ). Fraudulent claims hurt the insurance companies a lot and if they can be detected either in on-line or off-line manner, it could reduce the loss to a great extent. Most of the insurance companies publish few data on the occurrence of insurance fraud ( Lang and Wambach, 2013 ). Consequently, detec-tion becomes a challenge. Further, insurance frauds grow rapidly in fi elds like Telecom, e-business, accounts, credit cards, etc., ( Phua et al., 2007 ). Chan et al. (1999) mentioned various reasons for increase in credit card frauds like lack of deploying ef fi technique to handle massive millions of transactions, due to highly skewed data, and further each transaction has different speci amount, and attributing the same misclassi fi cation cost rate to all transactions is yet another potential fl aw in cost based mining techniques. Most of the analyses agree that approximately 20% of all insurance claims are fraudulent in some way ( Sublej et al., 2011 ).
But most of these claims go unnoticed, as fraud investigation is usually performed manually by a domain expert or investigator and he/she rarely takes support of a computer ( Sublej et al., 2011 ).
Inappropriate data representation is also a common problem, making the job for fraud investigators extremely dif fi cult ( Chan et al., 1999 ; Phua et al., 2004 ). Hence, these types of frauds should be addressed by developing models that can trigger early warnings or red fl ags. Jensen (1997) observed several technical dif fi culties indetecting fraud. Firstly, only a small portion of accident claims are fraudulent (skewed class distribution) making them extremely dif fi cult to detect. Next, there is a severe lack of labeled data sets as labeling is expensive and time consuming. Any approach for detecting such fraud should thus be founded on moderate resources (data sets) in order to be applicable in practice. In addition to this, fraudsters discover innovative types of frauds continually. existing customers cease doing business with a bank/ fi nancial insti-tution/service provider. Over the decade and half, the number of customers with banks and fi nancial companies is increasing. Banks provide services through various cha nnels, like ATM, debit cards, credit cards, internet-banking, etc. The enormous increase in the number of customers has made banks conscious of the quality of service they offer. This phenomenon triggered tremendous competition amongst various banks resulting in a signi fi cant increase in reliability and quality of service from banks. The problem of customers defecting or shifting loyalties from one bank to another has become common.
Churn occurs owing to reasons such as availability of latest technology, customer-friendly bank staff, low interest rates, proximity of geogra-phicallocation,variedservicesoffered,etc.Hence,thereisapressing need to develop models that can predict which existing  X  loyal  X  customer is going to churn out or attrite in near future. customer churn in credit cards by fi rst proposing a combination of machine learning algorithms to rectify the data imbalance pro-blem and then invoking a few classi fi ers for classi fi cation purpose. We propose k -reverse nearest neighborhood ( k RNN) and one-class
SVM (OCSVM) respectively for outlier detection and undersam-pling of majority class of a dataset. The article makes an in-depth description, evaluation and analysis of the proposed system. work reported in the related areas. Section 3 describes proposed methodology while Section 4 presents the experimental metho-dology followed in the study. Section 5 discusses the results obtained and Section 6 concludes the paper. 2. Literature review presented for detecting the fraud, by merging the results obtained from local fraud detection tools at different corporate sites to yield a more accurate global tool. This work was extended by Chan et al. (1999) and Stolfo et al. (2000) . They proposed a scalable distributed data mining model to evaluate classi fi cation techniques using a realistic cost model. They reported signi fi cant improvement by partitioning a large data set into smaller subsets to generate classi using different algorithms, experimenting with fraud/non-fraud dis-tributions within training data and using stacking to combine multi-ple models. Stefano and Gisella (2001) proposed a fuzzy logic control (FLC) model to evaluate an  X  index of suspects  X  on each claim ef fi ciently for highlighting fraudulent claims. Brockett et al. (2002) introduced a mathematical technique for an apriori classi objects when no training data with target variable exists in the context of fraud detection in body injury claims in automobile insurance. They reported that by using principal component analysis of Relative to Identi fi ed Distribution (RIDIT) scores, an insurance fraud detector can reduce uncertainty and increase the chances of targeting theappropriateclaims.Later, Phua et al. (2004) proposed a fraud detection method which makes use of a single meta-classi fi (stacking) to choose the best base classi fi ers, and then combine these base classi fi ers' predictions (bagging) to improve cost savings. They called this method as stacking  X  bagging. In this method, they used
MLP together with Na X ve Bayesian (NB) and C4.5 algorithms, on data partitions derived from minority over-sampling with replacement anddemonstratedthatstacking  X  bagging performs slightly better than the best performing bagged algorithm, C4.5 in terms of cost savings on a publicly available automobile insurance fraud dataset.
Phua et al. (2007) conducted an exhaustive survey of data mining based fraud detection methods hi ghlighting higher cost savings.
Several researchers proposed some standard data mining algo-rithms like neural networks, fuzzy logic, genetic algorithms, support vector machines, logistic regression, classi fi cation trees to handle fraud detection ( Bolton and Hand, 2002; Brockett et al., 2002; Viaene et al., 2002, 2005; Perez et al., 2005; Estevez et al., 2006; Yang and Hwang, 2006; Hu et al., 2003; Kirkos and Spathis, 2007;
Rupnik et al., 2007; Quah and Sriganesh, 2008; Sanchez et al., 2009 ). Recently, Ngai et al. (2011) conducted a comprehensive literature review on fi nancial fraud detection and various data mining techniques handling the problem. They reported that the main data mining techniques used in this domain are logistic models, neural networks, the bayesian belief network, and decision trees. Zhu et al. (2011) proposed Nonnegative matrix factorization approach for health care fraud detection. Xu et al. (2011) proposed Random Rough subspace based Neural Network Ensemble for
Insurance fraud detection and used rough set reducts to improve the consistency in the datasets. They reported a detection rate of fraud cases as 88%. They concluded that other ensemble strategies can further improve the problem solution. Sublej et al. (2011) proposed a graph based network approach for detecting automobile insurance frauds. They proposed Iterative Assessment Algorithm based on Graph components. However, most of these above ment-ioned studies did not handle the class imbalance problems of insurance data. Benard and Vanduffel (2014) studied mean-vari-ance ef fi cient portfolios. It is a quantitative approach where mean returns get maximized and variance of risk is minimized. Then they derived bounds on Sharpe ratios and demonstrated that this will be useful for fraud detection.

Regarding Churn prediction, Mozer et al. (2000) analyzed sub-scriber data of a major wireless carrier, by applying logistic regres-sion, na X ve neural network and a sophisticated neural network. They concluded that using a sophisticated neural net, $93 could be saved per subscriber. Smith and Gupta (2000) employed multilayer percep-tron, Hop fi eld neural networks and self-organizing neural networks to solve churn problems. Larivie're and Van den Poel (2004) concluded that in the fi nancial services industry, two  X  periods can be identi fi ed: the early years after becoming a customer and a second period after being a customer for some 20 years.
Ferreira et al. (2004) analyzed wireless dataset from Brazil using multilayer perceptron, C4.5 decision trees, hierarchical neuro-fuzzy systems and a data-mining tool named rule evolver based on genetic algorithms (GAs). Larivie're and Van den Poel (2004) studied
Application of fuzzy ARTMAP for churn prediction 431 the defection of SI customers of a large Belgian fi nancial services provider using survival estimates, hazard ratios and multinomial probit analysis.
Recently, Kumar and Ravi (2008) conducted the most comprehensive investigation on the credit card churn prediction problem in bank credit cards by resorting to data mining. They employed the bala-ncing method such as SMOTE, undersampling, oversampling and combination of undersampling and oversampling before invoking multilayer perceptron, logistic regression, decision tree (J48), random forest, radial basis function network and support vector machine.
Naveen et al. (2009) designed a new feature s election method called the  X  union method  X  by considering the union of feature subsets selected by t -statisticandmutualinformationandemployedFuzzy
ARTMAP with the selected feature s for predicting churn in the same dataset.

In the following, we review the past work reported in under-sampling methods. Hart (1968) proposed an undersampling method,
Condensed Nearest Neighbor (CNN). This method fi rst randomly draws one example from the majority class to be combined with all examples from the minority class to form a training set S ,thenusea 1-NN over S to classify the examples in the training set and move every misclassi fi ed example from the training set to S . Laurikkala (2001) proposed Neighborhood Cleaning Rule (NCR). It employed the Wilson's Edited Nearest Neighbor Rule to remove selected majority class examples. Further, Chawla et al. (2002) proposed Synthetic Minority Oversampling Technique (SMOTE) approach, in which the minority class samples are oversampled by generating arti ples rather than just oversampling with replacement. Japkowicz (2000) , Japkowicz and Stephen (2002) and Japkowicz (2003) used a synthetic dataset to study the class imbalance problem and found that independent of the training size, linearly separable domains are not sensitive to imbalance. Then they compared the effectiveness of (a) oversampling, (b) undersampling and (c) cost modifying for tackling the imbalance problem. Th ey concluded that the bad perfo-rmance of the unbalanced dataset is usually caused by small sub-cluster that cannot be classi fi ed accurately. Further, outliers skew the performance of a model. So, outliers need to be carefully handled fi rst ( Lee et al., 2013 ). Taking cue from Lee et al. (2013) ,webelieve that outlier elimination plays a vital role in the insurance fraud detection. Soujanya et al. (2006) proposed with the concept of k reverse nearest neighborhood, hav ing the capability of eliminating outliers simultaneously while building the clusters. It implies that it removes the noisy records which lie distantly from the normal records. Further, we agree with their point that outliers may be considered as noisy points lying outside a set of de fi ned clusters. In our close analogy with our fraud dataset, outliers are the records which exhibit dissimilarity with the de fi nedsetofclustersandthey cannot be part of representatives while undersampling the data and further noise needs to be eliminated to enhance the data quality. Hence, in the proposed methodology, we chose k RNN as a fi in the process of undersampling. 3. Proposed method
We fi rst extracted the validation dataset, comprising 20% of the original unbalanced dataset using strati fi ed random sampling from the dataset and left it intentionally untouched so as to validate the ef fi ciency of the proposed model in a real life scenario. The rema-ining 80% dataset is subjected to the proposed novel under sampling approach. We propose a novel hybrid under sampling approach by eliminating the outliers fi rst from the majority class using k RNN and from the resulting outlier-free dataset, we extract the support vectors using OCSVM. We particularly chose SVM here as it performs row dimensionality reduction (by way of picking up
Support Vectors) while classifying the datasets. Therefore, we implicitly accomplish undersampling by way of selecting a few, important samples of the majority class. These resulting samples are then merged with the minority class samples thereby yielding a modi fi ed balanced dataset. The block diagram of the proposed approach is depicted in Fig. 1 . 3.1. Algorithms used in the proposed approach 3.1.1. kReverse Nearest Neighbor (kRNN) ( Soujanya et al., 2006 )
Let X be d-dimensional data set, X  X f x 1 ; x 2 ; x 3 ; ... ; where n is size of the data set and x i ; x j are any two points (samples) in X .If d ij is the distance between two samples x i and x of x g ,whereforagivenpoint x i ,the k th smallest distance after sorting all the distances from x i to the remaining points, k th smallest distance is the k th nearest distance of x j .

Then, k Reverse Nearest Neighbor set k RNN( x i )isde fi ned as { x | x
A k NN( x j )}, the set of all points x j that consider x nearest neighbor. A point x j belongs to k RNN( x i ) if and only if x
A k NN( x j ). Note that, in case of k NNs, for a given k value, each point in the dataset will have at least k nearest neighbors ( case of ties), but the set of k RNNs of a point could have zero or more elements. The set of k RNNs of point x i gives a set of points has higher number of k RNNs than another point x j , then we can say that x i has a denser neighborhood than x j . In other words, less the numbers of k RNNs, the farther apart are the points in the dataset to x j , i.e. the neighborhood is sparse.

According to k RNN concept, outlier point is de fi ned as a point that has less than k number of k RNNs, i.e., | k RNNs ( x less the number of k RNNs, the more distant is the point from its neighbors. To fi nd the noisy samples/outliers, we need to set the optimal combinations of k 1 , k 2 values, which depend on the nature of the data. Here k 1  X  k th smallest distance and k 2  X  | reverse neighbors. A sample is called outlier if it has less no of reverse neighbors ( k 2 ) than k 1 . The higher the difference between k and k 2 the higher is the probability of a sample being an outlier.
But if we choose too small value for k 1 and too large value for k then we may not identify all the outliers. The values of k chosen for the two datasets are presented in Section 5 . 3.2. Overview of classi fi cation techniques (MLP), and Decision Tree are too popular to be described here. So we present a brief overview of Support Vector Machines (SVM), Probabilistic Neural Network (PNN) and Group Method of Data
Handling (GMDH). 3.2.1. Support vector machines (SVM) classi fi cation by constructing an N -dimensional hyperplane that optimally separates the data into two categories. SVM models are closely related to neural networks. Using a kernel function, SVMs are an alternative training method for polynomial, Radial Basis
Function (RBF) networks and MLP classi fi ers, in which the weights of the network are found by solving a quadratic programming problem with linear constraints, rather than by solving a non-convex, unconstrained minimization problem, as in standard neural network training. The goal of SVM modeling is to fi optimal hyperplane that separates samples, in such a way that the samples with one category of the target variable should be on one side of the plane and the samples with the other category are on the other side of the plane. The samples near the hyperplane are the support vectors. An SVM analysis fi nds the hyperplane that is oriented so that the margin between the support vectors is maximized. One idea is that performance on test cases will be good if we choose the separating hyperplane that has the largest margin.
 belongs to only one class. It builds a boundary that separates the class from the rest of the feature space ( Tax and Duin, 2004 ). Given tion, the primary problem is 0 where v is an upper bound on the fraction of outliers and lower bound on the fraction of support vectors. The dual problem is min  X  1 2  X  T Q  X  X  ; such that r  X  i r 1 l ; i  X  1 ;:::; l point x i from the origin with margin  X  and  X  i accounts for possible errors. 3.2.2. Group method of data handling (GMDH)
Ivakhnenko (1968) as an inductive learning algorithm for model-ing of complex systems. It is a self-organizing approach based on sorting-out of gradually complicated models and evaluation of them using some criterion on separate parts of the data sample ( Srinivasan, 2008 ). The GMDH was partly inspired by research in perceptrons and learning fi lters. GMDH has in fl uenced the devel-opment of several techniques for synthesizing (or  X  self-organizing ) networks of polynomial nodes. The GMDH attempts a hierarchical solution, by trying many simple models, retaining the best, and building on them iteratively, to obtain a composition (or feed-forward network) of functions as the model. The building blocks of
GMDH, or polynomial nodes, usually have the quadratic form: z  X  w 0  X  w 1 x 1  X  w 2 x 2  X  w 3 x 2 1  X  w 4 x 2 2  X  w 5 for inputs x 1 and x 2 ,coef fi cient (or weight) vector w , and node output, z .Thecoef fi cients are found by solving the Linear Regres-sion equations with z  X  y , the response vector.

The GMDH network learns in an inductive way and tries to build a function (called a polynomial model), which would result in the minimum error between the predicted value and expected output. The majority of GMDH networks use regression analysis for solving the problem. The fi rst step is to decide the type of polynomial that the regression should fi nd. The initial layer is simply the input layer. The fi rst layer created is made by comput-ing regressions of the input variables and then choosing the best ones. The second layer is created by computing regressions of the values in the fi rst layer along with the input variables. This means that the algorithm essentially builds polynomials of polynomials.
Again, only the best are chosen by the algorithm. These are called survivors. This process continues until a pre-speci fi ed selection criterion is met. 3.2.3. Probabilistic neural network (PNN)
PNN was introduced by Specht (1990) . It is an implementation of the statistical algorithm called kernel discriminant analysis in which the operations are organized into multilayer feed forward network with four layers: input layer, pattern layer, summation layer, and output layer. It is a pattern classi fi cation network based on the classical Bayes classi fi er, which is statistically an optimal classi fi er that seeks to minimize the risk of misclassi pattern classi fi er places each observed data vector x  X  [ x x ]
T , into one of the prede fi ned classes c i , i  X  1, 2,..., m where m is the number of possible classes. The classical Bayes pattern classi-fi er implements the Bayes conditional probability rule that the probability P  X  c i = x  X  of x being in class c i is given by p x i  X  where P  X  c i = x  X  is the conditional probability density function of x given set, Pc i  X  X  is the probability of drawing data from class c
Vector x is said to belong to a particular class c i ,if Pc of the patterns in the pattern layer. The summation layer com-putes the probability P  X  c i = x  X  of the given input x to be in each of the classes c i . The output layer selects the class for which the highest probability is obtained in the summation layer. The input is then made to belong to this class. Effectiveness of the network depends on the smoothing parameter r . 4. Experimental methodology
The effectiveness of the proposed hybrid approach was demon-strated on an insurance fraud detection dataset and a customer credit card churn dataset taken from literature. 4.1. Insurance fraud detection dataset description
The insurance fraud detection dataset is taken from Phua et al. (2004) . This dataset mainly comprises the information regarding the various automobile insurance claims during the period 1994
The dataset, described in Table 1 , comprises 31 predictor variables and 1 class variable. It consists of 15,420 samples of which 14,497 are non-fraudulent and 923 are fraudulent, which means there are 94% genuine samples and 6% fraudulent samples. Hence, the dataset is highly unbalanced in terms of the proportion of fraudulent and non-fraudulent samples.
 4.2. Data preprocessing
It is observed that the age attribute in the dataset appeared twice in numerical and categorical form as well. Hence, the age attribute with numerical values is removed from the data to reduce the complexity caused by too many unique values it possesses. Further, the attributes Year , Month , Week of the month and Day of week represent the date of the accident and the attributes Month claimed , Week of the month claimed and Day of week claimed represent the date of the insurance claim. Thus, a new attribute Gap is derived from seven attributes such as Year , Month , Week of the month , Day of week , Month claimed , Week of the month claimed and Day of week claimed . The attribute Gap represents the time difference between the accident occurrence and insurance claim. Thus 24 variables which included some derived variables are selected for further study. Hence, we have 15,420 samples with 24 predictor variables and 1 class variable. The attributes of the preprocessed dataset are presented in Table 2 . 4.3. Churn prediction dataset description
The churn prediction dataset is taken from a Latin-American bank, which suffered an increasing number of credit card custo-mer churn and decided to improve its retention system. The dataset consists of two groups of features for each customer: socio-demographic and behavioral data, which are described in Table 6 . The dataset consists of 21 features and 1 class label. It consists of 14,814 records of which 13,812 are non-churners and 1002 are churners, which means there are 93.24% loyal customers and mere 6.76% are churners. It is clear that the dataset is highly unbalanced in terms of proportion of churners vs. loyal customers ( Business Intelligence Cup, 2004 ). 4.4. Proposed undersampling methodology
First, the data is partitioned using the strati fi ed random sam-pling method into two subsets in the ratio 80% and 20% to ensure that each subset has the same proportion of positive and negat-ive samples as in the original data. The 80% data (11,597 non-fraudulent records and 738 fraudulent records) is used for under-sampling and to build the model, while the 20% validation data (with 2900 non-fraudulent records and 185 fraudulent records) is set aside to validate the effectiveness of the model, as it is close to real world scenario (see Fig. 1 ).

This 80% of original unbalanced dataset is fed to different classi fi ers mentioned in Section 4 to build the model. To obtain statistically reliable results, 10-fold cross validation method is used throughout this study. In other words, the training dataset (80%) is divided into 10 subsets of equal size using strati fi ed random sampling method. For all the possible choices of 9 subsets, the union of 9 subsets (training data) is used for training, and the remaining subset (test data) is used for testing (see Fig. 2 ). Then the results on test fold are averaged. Finally, the model is validated using validation data (20%).
 combinations of k 1 , k 2 values, which depend on the nature of the data. Here k 1  X  k th smallest distance and k 2  X  | k RNNs reverse neighbors. A sample is called outlier if it has less number of reverse neighbors ( k 2 ) than k 1 . First, we need to increase the values of k 2 from 1 to k 1 . As we increased the values of k 2 from 1 to k 1, the number of outliers increased. When there is a steep increase in the number of outliers over previous k 2 particular previous k 2 can be considered. If we keep k 1 there shall be more number of outliers or in other words more number of samples shall be labeled as outliers. For both the datasets, based on the above discussion, we chose k 1 , k tuning them in the identi fi cation of number of outliers. and accordingly 4569 samples are identi fi ed as outliers. Then, the identi fi ed outliers are removed from the majority class in the training dataset and the residual majority class has 7028 samples.
On this residual majority class, we employed one-class SVM (OCSVM) to extract the support vectors. We trained the OCSVM with the samples of reduced majority class and extracted support vectors. Consequently, the class distribution ratio of majority class versus minority class in this modi fi ed training dataset became almost perfectly balanced, i.e., 703 majority samples (non-fraudu-lent records  X  which are indeed support vectors) and 723 minority samples (fraudulent records, since 80% of minority samples are left untouched).
 remove the outliers, we applied k RNN with k 1 , k 2 being chosen as 5 and respectively 1. Thus, we obtained 2336 outliers and on the residual 8713 samples, we employed OCSVM to extract the support vectors (720 in number) and derived the new training dataset by merging with minority class samples (80% of untouched minority class samples). Now, the training class is almost equally distributed i. e., 720 majority class samples and 783 minority class samples. For both the datasets, we chose k 1 , k 2 based on the above discussion. undersampling. The innovation in the proposed methodology lies in the use of OCSVM in place of K-Means Clustering algorithm, in order to perform the 2nd level of undersampling (after kRNN).
This is distinctly advantageous over the use of K-Means because we extracted support vectors from OCSVM, which are very few in number and also they are the actual samples, forming a subset of the dataset that resulted from the application of the kRNN met-hod. However, in the case of K-Means, the cluster centers, which are not the actual samples from the majority class dataset, are considered to accomplish under-sampling. Thus, in the proposed method, arti fi cial samples, in the form of cluster centers, do not enter the modi fi ed dataset.
 dataset and tested with the test data. We performed the 10 fold cross validation method using the modi fi ed training dataset. Finally, the model is validated on the validation data (20%) which is unb-alanced and represents the realistic scenario present in the data.
The quantities employed to measure the quality of the classi sensitivity, speci fi city, accuracy and Area Under ROC Curve (AUC), which are de fi ned as follows ( Fawcett and Provost,1996 ). Sensitivity is the measure of proportion of the true positives (TP), which are correctly identi fi ed by a classi fi er.
 Sensitivity  X  TP = TP  X  FN  X  X 
Speci fi city is the measure of proportion of the true negatives (TN), which are correctly identi fi ed by a classi fi er. Specificity  X  TN = TN  X  FP  X  X 
Accuracy is the measure of proportion of true positives and true negatives, which are correctly identi fi ed.

Accuracy  X  TP  X  TN  X  X  = TP  X  TN  X  FP  X  FN  X  X  where TP stands for true positive , TN stands for true negative ,FP stands for false positive , FN stands for false negative. Further, we computed the AUC for each classi fi er and ranked the classi the descending order of AUC ( Ravi et al., 2007 ). Further, we repr-esented the confusion matrix as well in Fig. 2 . 5. Results and discussion
We performed 10-fold cross validation throughout the study and the average results obtained against validation data using original unbalanced data and the modi fi ed (balanced) data using proposed undersampling method are presented in Table 4 . All classi the proposed undersampling methodology outperformed their coun-terparts in the case of unbalanced dataset. Further, when compared the results of Table 3 , our proposed novel methodology outper-formed well in identifying fraudulent records. One can notice the variation in sensitivity when compared to previous results also. We employed the classi fi ers Decision Tree and Support Vector Machines (SVM) as implemented in the open source data mining tool Rapid-miner ( www.rapidminer.com ). We employed Multi-Layer Percep-tron, Group Method of Data Handling (GMDH) and Probabilistic
Neural Network (PNN) is implemented in NeuroShell ( www.neuro shell.com/ ). Logistic Regression is implemented in Weka embedded
Knime ( www.knime.org ). For implementing OCSVM to extract sup-port vectors for the majority class, we used libsvm module supported by OpenCV ( http://docs.opencv.org/modules/ml/doc/support_vector_ machines.html ).

Various parameters are considered for different classi fi we chosen the important parameters for both the datasets such as
Momentum  X  0.01 .InDT, con fi dence factor  X  0.5 , 0.1 , minimum no of samples for leaf  X  12 , minimum split size  X  20 , maximal depth  X  30 and gain_ratio is chosen as criterion, In formationGainischosenascriterion for Fraud detection dataset and Churn prediction dataset .ForSVM, kernel type  X  anova, a statistical decomposition method for deriving kernal, Kernal gamma  X  1 and 5, kernel degree  X  1, cost parameter  X  0 and convergence epsilon  X  0.01, maximum number of iter-ations  X  10,000 are chosen for Insurance Fraud and Churn prediction dataset. Max no of variables in connections  X  3,maxvariabledegreein connections  X  3, max variables in product team  X  3, selection criter-ion  X  full and model optimization  X  high arechoseninGMDHforboth datasets. For PNN, Genetic pool size is set to 300, smoothing factor is chosen as 0.8 .ForOCSVM, nu value  X  0.1,  X   X  0.04, kernel function is sigmoid, coef fi cient of 10 is chosen and the parameter combination that is mentioned above is chosen after conducting experiments involving several combinations for fi ne tuning them and it yielded the highest sensitivity in both datasets.

Even though fraud occurrence rate is low, still it costs a huge loss to an organization. Therefore, many business decision makers place high emphasis on sensitivity alone thereby trying to minimize the losses and sustaining in the competi tion. In many real-life problems such as fraud detection in credit cards, fraud detection in telecom services, bankruptcy prediction, in trusion detection in computer net-works, cancer detection in humans based on their genetic pro etc. detecting and predicting positive cases is accorded higher pre-ference. Obviously, to misclassify a churner (fraudulent record in our case) is, on average, far more expensive than to misclassify a non-churner (non-fraudulent record) ( Glady et al., 2009 ). At the same time, we accorded same priority in evaluating the classi fi receiver operating curve (AUC).

Consequently, in this study, sensitivity is accorded top priority ahead of speci fi city. Therefore, we discuss the performance of our proposed approach with respect to sensitivity alone. However, if management is interested in correctly predicting both fraudulent and genuine claims, then both sensitivity and speci fi city should be given equal priority. In other words AUC can be considered in ran-king classi fi ers and sensitivity alone. From Table 3 , it is observed that LR, MLP, RBF, Decision Tree, GMDH and PNN all yielded almost 0% sensitivity, 100% speci fi city and 94% accuracy on the original unbalanced insurance fraud detection data ( Vasu and Ravi, 2011 ). This result is not surprising and the reason for the worst perfor-mance is the imbalance present in the dataset. However, SVM yielded considerably higher sensitivity of 70.76%, speci fi 63.2% and accuracy of 63.65% on original unbalanced data. From the results of SVM, we can observe that SVM is not highly affected by the class imbalance since the classi fi cation in SVM is accom-plished by using the support vectors (which lie on the classi tion boundary) from majority class and minority class by ignoring the samples from both classes far away from the classi fi boundary.

Amodi fi ed dataset with almost 50%:50% proportion of genuine and fraudulent claims are generated using the proposed hybrid under sampling approach. This modi fi ed data is used to train sev-eral classi fi ers and then performance of the classi fi ers is validated using validation data and results (validation set) are presented in
Table 3 .From Table 3 , it is observed that by following proposed under sampling approach, classi fi ers LR, MLP, DT, SVM, GMDH and
PNN yielded uniformly higher sensitivity compared to that of unbalanced case. It is observed that SVM yielded highest sensitivity of 91.89% and DT yielded 90.74% on the validation data which is unbalanced. If management is interested in correctly predicting both fraudulent and genuine claims, then both sensitivity and speci fi city should be given equal priority. Further, we performed t -test to see whether SVM's performance is statistically signi or not. We found that all other models except DT are statistically signi fi cantly different with respect to sensitivity at 18 degrees of freedom. Between DT and SVM, with respect to sensitivity, com-puted t -statistic value is 1.23, which indicates that there is no signi-fi cant difference between DT and SVM. Therefore, any of them can be recommended to the management. Consequently, we recom-mend DT, as it is much faster to train and more importantly, it yields  X  if  X  then  X  rules indicating the business knowledge extracted from the dataset.

In this context, we compare our results with that of the previous studies conducted on the same dataset. Phua et al. (2004) proposed a fraud detection method on the same dataset. The main objective of their research was to improve cost savings for which they used stacking  X  bagging approach. Since they did not report sensitivity, speci fi city and accuracy, our results are strictly not comparable to theirs. Secondly, Padmaja et al. (2007) also worked on the same dataset. However, quite surprisingly, they undersampled the min-ority class before applying SMOTE technique on it. We, however, do not subscribe to that line of thinking, since in our opinion, we should not undersample the minority class, how much ever noisy it may be as they are too rare to be removed. Kubat and Matwin (1997) also held the same opinion. Further, they adopted hold-out method whereas we evaluated our approach using 10 fold cross validation, which is more authentic. Thus, once again, our results cannot be compared with theirs.

When compared to Vasu and Ravi (2011) , our approach yielded better sensitivity while maintaining the speci fi city and accuracy.
We presented AUC results in Table 3 which re fl ects that we succeeded in identifying fraudulent records without drastic fall in other measures like speci fi city and accuracy. We further compared our results with that of Farquad et al. (2012) , where they deployed SVM-Recursive Feature Elimination for feature selection and employed active learning methods for synthetic data generation. But they achieved a maximum sensitivity of only 88.16% (see Table 4 ). Our proposed methodology, with a sensitivity of 91.89 % using SVM, outperformed their method.

As regards rules, we have extracted just 4 rules using the decision tree as presented in Table 5 . When compared with the rules obtained by Vasu and Ravi (2011) and Farquad et al. (2012) , we obtained less number of rules that are completely different and without actually compromising on the sensitivity. This is a signi fi cant outcome of the study. Vasu and Ravi (2011) by applying decision tree achieved 83.6% sensitivity with 19 rules having the same root variable viz.,  X  Base Policy  X  . But they obtained additional variables like V_Price , Accident Area , Gender and Past no of claims , etc. Then, Farquad et al. (2012) by applying decision tree achieved 88.16% sensitivity with 12 rules having the root of  X  Policy Holder and they got additional variables of Vehicle Category, Age of Vehicle, Accident Area, Manufacturer, and Marital status.

Further, in the process of validating the proposed methodology, we worked on Churn prediction dataset. We presented the results obtained by the proposed methodology in Table 7 . When com-pared with Vasu and Ravi (2011) , we observed GMDH, SVM, LR, DT yielded signi fi cant results with respect to sensitivity (83.1%, 87.7%, 83.45% and 91.2%) and AUC. Further, when we observed the AUC results of these classi fi ers, GMDH yielded superior results among the rest of the classi fi ers in the proposed methodology. We performed t -test as done earlier in the case of Insurance data.
With respect to sensitivity of GMDH, we noticed that statistically there is no signi fi cant difference between SVM and LR. But as
GMDH has higher AUC when compared with the other classi fi we concluded that GMDH contributed signi fi cant outcome.
Further, we extracted Decision rules for the Churn dataset and we observed that we obtained 10 rules yielding higher sensitivity (91.2%) when compared with Vasu and Ravi (2011) , where they achieved 12 rules with 91% sensitivity. When compared with Farquad et al. (2012) , the proposed methodology yielded signi cant results. This can be observed by comparing Tables 7 and 8 . Further, rules extracted are presented in Table 9 . 6. Conclusion and future work
In this paper, we proposed a novel hybrid approach for under-sampling the majority class in largely skewed unbalanced datasets in order to improve the performance of classi fi ers. This paper demonstrates the signi fi cance of eliminating outliers (noisy) and redundant samples from the majority class in the case of highly skewed unbalanced datasets. In the proposed approach, we fi applied k RNN to detect and remove the noise present in the form of outliers in the majority class. Later, we applied OCSVM for extracting support vectors in the majority class.

Finally, we combined these noisy and redundancy free majority class samples with the original minority samples and carried out experiments with the modi fi ed dataset thus obtained. We tested the effectiveness of our approach with different classi fi MLP, PNN, LR, SVM, DT, and GMDH using the validation dataset which is unbalanced and close to real environment. The effective-ness of the proposed hybrid approach was demonstrated on the dataset namely fraud detection dataset taken from Phua et al. (2004) in insurance sector. We achieved 90.74% and 91.89% fraudulent claims detection rate (sensitivity) on Insurance fraud detection data by DT and SVM respectively. Regarding Churn prediction dataset, GMDH yielded signi fi cant result of 83.1% sensitivity. The results show that using our hybrid undersampling approach, the classi fi ers performed better compared to when original unbalanced data was presented to them. Finally, the proposed methodology can be applied to other problems of intruder detection in computer networks, terminal diseases pre-diction in humans, default prediction in banks, etc. The scope can be further extended by determining whether the presence of k RNN has its in fl uence on the proposed model. If not so, we feel that OCSVM can itself undersample the majority class. We would like to investigate further in that perspective.
 References
