 Sentiment analysis deals with the treatment of opinion, sentiment, and subjec-tivity in text such as product reviews and microblogs [1 X 3], which has gained sig-nificant popularity over the past few years in natural language processing (NLP) and natural language understanding (NLU). In financial domain, sentiment anal-ysis has been proved to be useful in many economic and financial applications, such as market dynamic prediction [4], movie box office forecasting [5], company stock prices forecasting [6], etc. In order to predict stock market, many research studies have been carried out on different data sources, such as microblogs [7, 8], reviews [9], Tweets [10 X 12] and financial news [9, 12, 13], etc. And a variety of methods have been adopted in these studies, such as sentiment-based trading strategy [9], time series models [8, 9] and machine learning methods [11 X 13]. ment of target company, i.e., bullish or bearish, which is not exactly the same as the positive or negative emotions in product reviews. For example, in product reviews,  X  increasing price  X  usually indicates bad news (i.e., negative emotion) but in financial domain  X  increasing price  X  of stock is welcome news. To make it easier to look at stock news, the social media services Twitter and StockTwit-s have been curating a $-tagged stock information  X  X ashtag X  (a  X $ X  followed by a ticker symbol, e.g., $GOOG). For example, a message from StockTwits (ID:7744550)  X  X ollowing this morning X  X  EPS, I blew out $ ZMH and added to $ DNKN (longs). X  , expressed negative and positive sentiments for two cashtags $ ZMH and $ DNKN , respectively.
 croblogs and news. That is, given above example, we not only identify their sentiment orientation but also predict their sentiment scores (in this case, the sentiment scores for two cashtags are marked as  X  0 . 462 and 0 . 528 by human annotators). To address it, we propose a gated and attention-based neural net-work model GABi-LSTM. Specifically, we utilize a convolutional neural network to extend the widely-used word embedding into fine-grained character-level em-bedding in order to capture morphological information and alleviate the rare word problem (such as symbols, numbers, appearing frequently in financial do-main) as well. Moreover, to effectively integrate character embedding into word embedding for word representation, we propose a novel gate mechanism to adap-tively choose the most appropriate way to represent each word rather than s-traightforward concatenation operation. Most important, we propose an atten-tion mechanism to select out the most relevant part of the contexts in terms of the target company (i.e., cashtag in microblogs or company name in news) and to assign them the higher weight for sentence representation. In this way, our proposed model not only takes financial domain into consideration but also embeds target-dependent information into representation.
  X  Our proposed novel gated and attention-based neural network model GABi- X  Comparative experiments on two benchmark datasets in financial microblogs A vast amount of research has been dedicated to detect the potential correla-tions between media data and the stock market. For example, [14] proved that investor sentiment is positively associated with future stock price crash risk and [7] revealed the effectiveness of microblogs for stock market prediction. Previous studies performed financial sentiment analysis on different data sources, such as microblog data [7, 8], film reviews [9], Tweets [10 X 12] and financial news [9, 12, 13]. Moreover, a variety of methods have been examined, for example, the sentiment-based trading strategy in [9] and time series models in [8, 10]. tures combined with supervised machine learning algorithms. For example, in [11 X 13, 15 X 17]. Besides, a hybrid method with machine learning and lexicon-based approach [16, 12] has been widely used and [12] achieved the state-of-the-art performance recently. Meanwhile, with the development of deep learning methods, a variety of neural network models have been proposed in [16, 18 X 20]. For example, [18] combines the Convolution Neural Network (CNN) and Long Short-Term Memory (LSTM) and [19] adopts a Bidirectional Gated Recurrent Unit (Bi-GRU). However, above neural network models are still in infancy to deal with fine-grained sentiment analysis on financial microblogs and news and the performance of the above methods are lower than machine learning method. 3.1 Motivation The proposed architecture of GABi-LSTM model is motivated by the observa-tions and analysis we made on as follows.
 dependent task. The words  X  put  X ,  X  call  X  and  X  long  X  regarded as neutral words in traditional sentiment analysis but express a strong sentiment tendency in stocks prediction, which results in many pre-defined sentiment lexicons in traditional NLP tasks not suitable for financial sentiment analysis.
 several characteristics: (1) the sentence length is relatively short; (2) they usually contain a lot of numbers and non-English characters, e.g.,  X $,%, +,  X  , #, @, - X ; (3) many sentences are incomplete, such as  X  Worst performers today  X . These differences inspire us to encode words in both character-level and word-level embeddings and then integrate them using a gate mechanism.
 dict the sentiment score associated with a target company. In order to capture the relationship between the target company and contexts, an attention mechanism is proposed to embed target company information into sentence representation. 3.2 The Overview of Model Architecture Given a sentence associated with a target company as input, the model is de-signed to return a sentiment score reflecting the bullish or bearish sentiment of the target company. Figure 1 shows the architecture of the proposed model GABi-LSTM, which consists of three modules. First, a word representation module is to map each word in the given input sentence into a vector through:  X  Word-level Embedding Layer maps each word into a vector using a pre- X  Charcter-level Embedding Layer maps each word into a vector at the  X  Gate Layer adaptively integrates word-level and character-level represen-Then, a sentence representation module is to learn sentence vector through:  X  Bi-LSTM Layer models word interactions based on word representations.  X  Attention Layer produces an attention (i.e., weight) vector over hidden  X  Sentence Embedding Layer obtains sentence vector representation.
 Finally, a Linear Regression Layer is to perform sentiment score prediction. The detailed description of these three modules in GABi-LSTM is as follows. 3.3 Word Representation with Gated Char-and Word-Embedding Word representation module aims to map each word of the input sentence to a low-dimensional vector. For each word, 1) a pre-trained word lookup table is used to get the word-level embedding; 2) a character-level convolutional neural network (Char-CNN) is adopted to get the character-level embedding; 3) then, a gate mechanism is followed to adaptively integrate word-level and char-level embeddings to represent the word.
 Word-level Embedding With the prevalence of deep learning methods in NLP, a word can be represented as a low dimensional, continuous and real-valued vector by training on the text corpus using a neural network architecture. In this module, each word w i in the input sentence is projected to a low-dimensional E
V w i , where | V | is the vocabulary size, d w is the dimension of a word vector and V w i  X  R | V | is the one-hot representation of w i .
 Character-level Embedding The sentences from financial microblogs and news usually comprise many different forms of numbers, but the frequency of a particular number appears very low, such as  X +5% X ,  X $30 X , etc. Only consider-ing the word-level embeddings might suffer from the out-of-vocabulary (OOV) problem when encountered these rare numbers or other unseen words during test time. Therefore, we also encode words at the character level, which has been pre-viously shown to be beneficial for alleviating rare words problem and has the capacity for capturing morphological information, like the prefixes and suffixes of words. To map each word of the input sentence into a vector, a character-level CNN is adopted by the following two steps: 1. Each character in the input word is first projected into a character vector 2. Then the character matrix C is fed into a CNN to obtain the character level Gated Mechanism Word-level embedding can capture the semantic and syn-tactic information and character-level embedding can capture the morphological information well. Considering that each of the two embeddings have their own advantages and they can not take the place of each other, we propose a gate mechanism to adaptively choose the most appropriate way to represent each word based on these two types of word vectors, defined as: into the same dimension space as X word w as the final representation for the input word w i . 3.4 Sentence Representation with Attention-based Bi-LSTM Sentence Representation Module is used to represent the input sentence as a fixed size vector. LSTM is proved to be particularly useful for modeling sequential data and can specifically address the long-term dependency problem compared with the conventional recurrent neural network (RNN). However, LSTM can only capture the interaction between the current word and preceding words but ignore successive words which are significant for sentence representation. Therefore, we utilize bidirectional LSTM (Bi-LSTM) to get the sentence representation, which has a forward LSTM and a backward LSTM used to capture the information from both past and future words. Moreover, in order to capture the most relevant part from the sentence according to the target company, we proposed to introduce the attention mechanism to embed target company into sentence representation. Bi-LSTM Given the input sentence S = [ X w 1 , X w 2 ,..., X w N ], for each time step i , the step of LSTM computation corresponds to: where denotes element-wise multiplication. i i , f i , o i , c i denote the input gate, forget gate, output gate, memory cell respectively. Moreover, since a Bi-LSTM is used in our model, at each position i of the sequence, we obtain a forward hidden state concatenate them to produce the intermediate state, i.e., h i = [ Attention Mechanisms As we mentioned before, the sentiment score is asso-ciated with a target company in the input sentence, and it varies widely with respect to different companies. Therefore, we design an attention mechanism which is capable of capturing the most relevant part of sentence in response to a given company and gives them a higher weight.
 lookup table E . The attention mechanism will produce an attention weight vec-output by Bi-LSTM. Each attention weight p i for h i is computed by following, (probability) vector over the inputs and can be viewed as the weights of the words measuring to what degree our model should pay attention to. Next, we sum over the state h i weighted by the attention vector p to compute the representation 3.5 Linear Regression Since the sentiment scores for the given sentences are continuous, this prediction task requires a regression. Instead of using a softmax classifier, we use a linear function in the output layer, defined as, where W r  X  R 2 d h and b r  X  R are the trainable weight and bias parameters respectively,  X  y is the sentiment score predicted by the model. We define the training loss (to be minimized) as the mean square error (MSE) between the predicted  X  y and ground-truth y : where where y ( k ) and  X  y ( k ) are gold and predicted labels, m is instances count. 3.6 Parameter Learning We fix the lengths of sentences (number of words) to be 50 and the lengths of words (number of characters) to be 30, and apply truncating or zero-padding when necessary. The dimensions for character embeddings, word embeddings and target embeddings are 30, 300 and 300 respectively. The word embeddings are initialized by pre-trained word vectors using Google Word2vec [21]. The other parameters are initialized by sampling from a uniform distribution U (  X  0 . 5 , 0 . 5). In the Char-CNN module, we choose three groups of 50 filters, with filter window sizes of (2, 3, 4). The dimensions of the hidden states d h in Bi-LSTM is set to 200 and 100 for microblog and news data respectively. We use AdaGrad [22] with a learning rate of 0.001 and a minibatch size 64 to train the model. 4.1 Datasets We use the two datasets of SemEval 2017 Task 5 3 , consisting of financial mi-croblogs messages (from Twitter or StockTwits) and news statements or head-lines collected by [23]. Each instance is associated with a target company (i.e., cashtag in microblogs or company name in news). Specially, the data derived from microblog messages is sentence fragments (namely  X  Spans  X ) which are ex-tracted from the raw messege according to the target company, whereas the data derived from news statements or headlines is complete sentences (namely  X  Title  X ). Besides, for microblog messages, we rebuild the raw messege (name-ly  X  Text  X ) of training and test set respectively with the official API of Twitter and StockTwits. In our experiments, for both data sources we randomly selected 80% as training dataset and the remaining 20% as development dataset. Table 1 shows the statistics of the datasets.
 4.2 Evaluation Measure The evaluation measure is weighted cosine similarity ( WCS ). The scores are conceptualised as vectors, where each dimension represents a company within a given microblog message or news text. Note that the both vectors will have the same number of dimensions as the companies for which sentiment needs to be assigned will be given in the input data. Cosine similarity and cosine weight are calculated as in Equation(14), where G and P are vectors of gold-truth and predicted scores, m is the number of instances and | P | is the dimension of the vector P . The final score is the product of the cosine and the weight (i.e., WCS = cosine weight  X  cosine ( G,P )). 4.3 Experimental Results Table 2 shows the experiments of our proposed models and several baselines on microblog and news test datasets. Standard CNN, LSTM and Bi-LSTM with word-only embeddings serve as three baselines. The CNN utilizes the vector after the max pooling layer, the LSTM and Bi-LSTM adopt the average of hidden state vectors to represent the input sentence. We see that these models have consistent results on both microblog and news datasets and observe the following findings.
 outperforms other two standard CNN and LSTM with only word level embed-dings. This demonstrates the importance of taking into account both past and future information of the context in encoding the sentence, which is consistent with previous studies. Meanwhile, we also see that LSTM performs better than CNN, which proves the effectiveness of LSTM in modeling sequential sentence. tence representation model and investigate the performance of different word representations. We find that the integration of character level and word lev-el embeddings performs better than any of the individual embedding. This is not surprising since the integration can take into account both the advantages of character and word level embeddings for word representation. That is, the word-level embedding is able to capture semantic and syntactic information and the character-level embedding takes the word morphological information and rare symbols into consideration, which is crucial for sentiment analysis in finan-cial domain. Furthermore, regarding integration strategy, the gate mechanism outperforms the simple and straightforward concatenation of these two types of embeddings, which indicates that the gate mechanism does help to choose a better way for word representation.
 word representation is taken, attention-based Bi-LSTM achieves substantially higher performance over the Bi-LSTM. This is because the attention mechanism captures the most relevant part of the sentence in terms of the target company and embeds the target information into the final sentence representation. This demonstrates the effectiveness of the attention mechanism for capturing interac-tion between target company and context. Overall, GABi-LSTM model achieves the best performance using the proposed attention-based Bi-LSTM model incor-porating with Gated Word &amp; Char embeddings, which indicates the effectiveness of the proposed method.
 microblog dataset, we find that Spans achieves results highly superior to the Text . The reasons are from: 1) The Text contains more noises and non-standard words, such as the hashtag, slangs and elongated words; 2) The Text may contain more than one companies and their sentiment are mixed in one message. 4.4 Comparison with the State-of-the-art Systems Table 3 shows the comparison between our proposed GABi-LSTM model and the top systems reported in SemEval 2017 Task 5, where ML, DL and Lex denote Machine Learning, Deep Learning and Lexicon respectively. Most studies have adopted a combination method of deep learning and pre-defined sentiment lexicon [16, 18 X 20], or a method of traditional NLP features with pre-defined sentiment lexicon applied to machine learning algorithms [12, 25]. Several studies adopt a single deep learning [24] or a pure machine learning method [15, 17]. the-art systems on both datasets. Specificly, the proposed GABi-LSTM system improves approximately 2% on microblog dataset, and 3% on news dataset com-pared with the reported best system. Overall, the experimental results show the superiority of the proposed model GABi-LSTM in dealing with the fine-grained financial target-dependent sentiment analysis. Another advantage of the pro-posed model is it does not require expensive hand-crafted features or external resources (pre-defined lexicon). 4.5 Qualitative Visualization Analysis To further understand the work of the attention mechanism within our model, we performed visualized analysis on a randomly selected example from the mi-croblog dataset:  X  Worst performers today: $ RIG -13%, $ EK -10%, $ MGM $ IO -6%, $ CAR -5.5% / best stock: $ WTS +15%. X  to target companies for the individual words in the example sentence. Darker shades of blue indicate stronger attention values. We see that the words like  X  X orst X , X $MGM X , X -6 X  and  X % X  are assigned higher attention values when the target company is  X $MGM X , while words such as  X  X est X ,  X $WTS X  and  X +15 X  contribute greatly to the target company  X $WTS X . Additionally, we observe that words such as  X  X oday X , which are rather irrelevant with respect to the target company, indeed have lower attention scores. The results show the effectiveness of our attention mechanism in capturing the most relevant fragments to the target company in the sentence. To address fine-grained financial target-dependent sentiment analysis from finan-cial microblogs and news, we proposed an effective neural network model GABi-LSTM. In consideration of the characteristics of tweets and financial domain, this model utilizes a CNN to achieve fine-grained character-level embedding in order to capture morphological information and alleviate the rare word problem and a gated mechanism to integrate character embedding into word embedding for word representation. Moreover, its attention-based Bi-LSTM component embeds target-dependent information into sentence representation and make the target-dependent sentiment score prediction possible. The comparative experiments on financial benchmark datasets show that our model outperforms baseline systems by a large margin and achieves the state-of-the-art performance so far.
