 Traditionally, research in identifying structured entiti es in documents has proceeded independently of document cat-egorization research. In this paper, we observe that these two tasks have much to gain from each other. Apart from direct references to entities in a database, such as names of person entities, documents often also contain words that are correlated with discriminative entity attributes, suc h age-group and income-level of persons. This happens nat-urally in many enterprise domains such as CRM, Banking, etc. Then, entity identification, which is typically vulner a-ble against noise and incompleteness in direct references t o entities in documents, can benefit from document catego-rization with respect to such attributes. In return, entity identification enables documents to be categorized accord-ing to different label-sets arising from entity attributes w ith-out requiring any supervision. In this paper, we propose a probabilistic generative model for joint entity identifica tion and document categorization. We show how the parame-ters of the model can be estimated using an EM algorithm in an unsupervised fashion. Using extensive experiments over real and semi-synthetic data, we demonstrate that the two tasks can benefit immensely from each other when per-formed jointly using the proposed model.
 H.2.8 [ Database Applications ]: Data Mining; I.5.1 [ Pattern Recognition ]: Models X  Statistical ; I.5.4 [ Pattern Recog-nition ]: Applications X  Text processing Algorithms, Theory document categorization, entity identification, probabil istic generative model
The importance of unstructured data as an information source is increasing steadily, both in the scientific and bus i-ness communities. Recent Gartner 1 research reports sug-gest that 80% of data in today X  X  enterprises is unstructured . In the growing competitive landscape, enterprises cannot a f-ford to let such huge amounts of data and insights hidden therein go untapped. There has also been a lot of activ-ity in the research community towards deriving insight from unstructured documents and integrating heterogeneous dat a sources. In this paper, we focus on two tasks in unstructured data  X  entity identification and document categorization  X  that are of critical importance, but have traditionally bee n investigated independently.

Entity identification in unstructured data forms a criti-cal component of data integration, which has attracted sig-nificant amount of research in recent years[7, 5]. Given a structured database containing descriptions of various en ti-ties, and a collection of unstructured documents mention-ing or referring to one or more entities from this database, entity identification attempts to identify the most relevan t database entity (or set of entities) for a given document. The task is often made challenging when the mentioned at-tributes do not exactly match the entity descriptions in the database due to misspellings and other kinds of noise, or when the evidence available in the document is insufficient for pinning down any single entity.

To illustrate, consider a movie database such as IMDB 2 and movie reviews written by various users. Figure 1 shows sample movies from the database. The database contains movies with information such as movie name, actors, di-rectors, writers, genre of the movie, and its rating, among myriad other information. Figure 2 shows some snippets of reviews written by different online users for movies from thi s database.

In this scenario, the goal of entity identification is to tag movie reviews with the most appropriate movie from the database. This is typically achieved by first annotating named entities in the documents and then identifying en-tities based on these annotations. The annotations in our example reviews are highlighted in Figure 2. For example, review (a) has two annotations,  X  X arrison Ford X  and  X  X avid Twohy X . These annotations uniquely map the review to the fourth movie entity, Fugitive , in the database. However, for the remaining reviews, the annotations do not have enough information to unambiguously identify a unique movie. Figure 2: Snippets of Movie Reviews from IMDB
An equally important and well-researched task over un-structured document collections is that of document cate-gorization[11, 16]. This involves automatically tagging d oc-uments in a collection with labels from a pre-defined label set. In our movie example, it is desirable to automatically organize all reviews according to the genres for easy brows-ing and retrieval. Note that it may be equally useful and meaningful to organize the same collection of reviews ac-cording to some other label-set, for example, the popularit y of the movies or the demographics of the writer. In the classification setting, a critical requirement is sufficient and accurate supervision, which is typically provided in the fo rm of training samples for the different labels. However, desig n-ing meaningful label-sets and building training data is tim e consuming, error prone and highly human-intensive. In the document categorization setting, we investigate if this ne ed for supervision can be alleviated.

In this paper, we consider these two text mining tasks to-gether and motivate how they can mutually reinforce each other under certain scenarios. In our example, entity iden-tification based solely on the annotated terms in the doc-ument often fails to unambiguously identify an entity. We observe that the unstructured words in the document that are not annotated, and are typically thrown away during the entity identification process, often carry useful informat ion. Although they do not directly mention an entity attribute, they may provide significant indirect evidence for inferring database column values. As an example, for the review in Figure 2(b), words such as adventurer , quest and search are suggestive of a movie from the  X  X dventure X  genre. When combined with the evidence from the annotated terms, it clearly points to the first movie, Indiana Jones and the Last Crusade . In enterprise settings, such as Banking and CRM, the words used in a customer communication are of-ten clearly indicative of her age-group and income level. However, discriminating these categories from words in the document requires that we have trained classifiers based on interesting database column values.

Now, consider the document categorization side of the story. We have seen that supervision in the form of labeled training samples is a critical requirement for effective cat e-gorization. However, if we are able to automatically and ac-curately identify entities for documents, then the databas e columns values provide labels for the documents. In our example, if all the reviews are correctly linked to their cor -responding movies, then review (a) would be labeled as  X  X c-tion X , review (c) as  X  X omedy X  and so on. Then, by aggregate analysis, any classifier would discover that words such as ad-venturer and quest are indicative of  X  X ction X  movies, while funny and laugh are suggestive of the  X  X omedy X  genre. In the CRM setting, it is possible to discover vocabularies use d by different categories of customers.

Thus, quite clearly, the two tasks can be combined to their mutual benefit. In this paper, we explore how the two tasks can be formally related, and propose a probabilistic model for jointly performing entity identification and docu -ment type categorization.
 Our contributions: We motivate the problem of jointly addressing the two tasks of entity identification and docu-ment categorization and show how they can naturally bene-fit each other in many domains. We propose a probabilistic generative model for documents that accounts for unstruc-tured words and mentions of entity attributes, and infer-ence in this model jointly addresses the two tasks. We show how parameters can be estimated in this model in a com-pletely unsupervised fashion from a document collection an d a backend structured database. We demonstrate using ex-periments over real and semi-synthetic data that our model improves entity identification over state-of-the-art base lines and enables unsupervised document categorization to com-pete with supervised approaches.
 Outline: Next, we formulate the joint problem and explore ways to relate the two tasks in Section 2. Then, we describe our joint probabilistic model in Section 3 and the unsuper-vised parameter estimation algorithm for it in Section 4. We describe experimental results over different datasets in Se c-tion 5. We discuss our contributions in the light of related research in Section 6 and finally conclude in Section 7.
In this section, we formulate the technical problem of jointly addressing entity identification and document cat-egorization. We start by formulating the two tasks inde-pendently, and then motivate why and how they can be addressed jointly.
 Entity Identification: The first problem is entity identi-fication , where we are given a structured database that con-tains a relation with k columns C = { C i } . We will refer to each row of the relation as an entity e , having its own values e.C i for the k columns. We also have a collection D = { d of documents, such that each document is about a specific entity from the database. More formally, each document d contains a set of attribute terms d.A = { a i } . If e i is the central entity for this document, then each attribute term a corresponds to some column value e i .C j of entity e i . For instance, in our previous IMDB movie database example, the columns in the relation are Actor1, Director1, Genre, etc. Each row in the database corresponds to a movie entity which has its own values for these columns. Our document collection consists of movie reviews, each of which is about a specific movie from the database and mentions its actors, ac-tresses, director, among other things. These tokens form th e attribute terms of the document. Given this bag-of-terms, our goal is to identify the central entity from the database relation. In our example, the goal is to identify the movie from the review assuming enough evidence is present for the inference. The major challenge is that explicit identifiers of the entity, such as a unique movie id, or sometimes even movie name, may not be available in reviews. This holds true especially for noisy documents where even a i may not exactly match its corresponding column value of e.C j . In our example, the database contains fully qualified names of people which are unlikely to be used in a review. Just  X  X eorge Lucas X  is ambiguous in the IMDB database and the full name of the director of our interest is  X  X eorge Walton Lucas Jr. X . Misspellings and abbreviations are also common , all adversely affecting recall. Precision can also be affecte d by noisy and partial information in the document if it leads to an incorrect entity being identified.
 Document categorization: A critical corpus-level task that needs to be performed over the document collection is document categorization . Let W be the vocabulary of unstructured words that are used in the document collec-tion. Each document d i in the collection has its own set of words d i .W = { w i 1 , w i 2 . . . } drawn from the vocabulary. Categorization groups documents in the corpus into differ-ent classes or categories based on the words they contain. This is helpful in organizing the document collection and making it amenable for navigation, browsing, and retrieval .
In the clustering formulation, the categorization process is unsupervised, but categories are not pre-specified. Instea d, documents are grouped according to how similar/dissimilar they are in terms the words they contain using projections in some vector space model. In contrast, the classification ap-proach supervises how the documents should be categorized by specifying a label-set and providing examples of docu-ments for each label. An interesting and important aspect of classification is its multi-faceted nature. The same docu -ment collection can be categorized differently by providing a different set of labels. For example, the same collection of movie reviews can be classified equally meaningfully ac-cording to genre, rating, budget, country of origin, awards etc. This, however, comes at the cost of having to pro-vide sufficient amount of labeled training data for each facet (label-set), which is costly, time-consuming, and often in fea-sible for some applications. Clustering, on the other hand, comes for free in terms of providing supervision, but can only produce a single facet based on the dominant features of the collection for a given similarity measure. For exam-ple, if the best grouping of the review collection is based on the movie ratings, then that is the separation that clusteri ng would yield.
 Relating the two tasks: In many scenarios, categorization and entity identification can be performed jointly and can benefit from one another. Imagine documents containing both attribute terms d i .A mentioning entities and unstruc-tured words d i .W , such that the words are strongly corre-lated with the column values of the central entity. This hap-pens naturally in many real-world domains. As motivated in the introduction, the words used in a movie review are differ-ent for movies of different genres, as also for movies of differ -ent popularity ratings. In the enterprise CRM setting, diffe r-ent customers may complain differently even about the same products, depending on their demographic background.
When such correlations exist, both entity identification and document categorization can benefit. If some column values of the entity can be predicted from the words con-tained in the document, then these words provide additional evidence for identifying the entity. Additionally, since t he column value is inferred using aggregate statistics, it pro -vides robustness against noise. For categorization, it ope ns up the possibility of getting the best of both the classifica-tion and clustering worlds. If structured entities from the database can be associated with documents, then the corre-lated column values can act as multiple label sets for these documents. This  X  X upervision X  can be used to perform multi-faceted categorization over the corpus without explicitly us-ing labeled training data.

We next explore possible ways to associate the vocabu-lary and entities in the database and enable their mutual reinforcement. Observe that associating words with specifi c entities is not helpful in any direction. It does not help en-tity identification unless the words are known for the specifi c entity. Nor does it help document categorization as the size of the label set would be huge.
 Relating words and entities: Instead of associating words with specific entities, we relate words to groups of similar entities in the database. We describe two natural ways of doing this. The first scenario is the relation-based-vocabulary scenario where the entities are partitioned over multiple r e-lations R = { R i } , with directly corresponding columns. We have different word distributions associated with dif-ferent relations R i . For movies, imagine that we have or-ganized (English) movies under different relations accordi ng to their country of origin. We have words associated with each country (or, equivalently each relation) and the words d .W in a movie review d i are picked from the vocabulary of the relation containing the movie. The second scenario is the column-value-based-vocabulary scenario, where all enti-ties belong to the same relation R . This relation has special columns T = { T i } which are called the  X  X ype columns X , and the words in a document depend on the values { e.T i } in the type columns for the central entity e i from R .
 Equivalence of the formulations: The relation-based-vocabulary scenario can easily be mapped to the column-value-based-vocabulary scenario. In the movie domain, we can add all the movies from the different relations in R into a single relation R with a new type column T  X  which in-dicates the original relation R i  X  X  for the movie entity. The words in a movie review are determined by the value e .T  X  for the movie e i . This reduction uses a single entity type column, but we can show that the multi-column set-ting is also not any more expressive. We can reduce it to the relation-based-vocabulary scenario by creating one relat ion R i in R for every combination of values in the type columns. We can then populate these relations with entities that have that specific (entity, type value) combination. The relatio n-based and column-value-based scenarios are thus equally ex -pressive; the single-column model is also as expressive as t he multi-column model.

For the rest of this paper, we will focus on the single-column-value-based-vocabulary scenario, without any los s in generality. Specifically, we have all entities in a single re-lation R with k columns C i . In addition to these k columns, we have one  X  X ype column X  T containing values { t i } . Each value t i for T has different words from the vocabulary W associated with it. Each document d i in the collection D is about a central entity e i . It contains attribute terms d derived from the column values e i .C j of the central entity. Additionally, the document contains words d i .W from the unstructured vocabulary W that are associated with the type value e i .T of the central entity. Our goal is then to identify the central entity from the document d i and also categorize the documents according to type values t i . The challenge is that no labeled data is provided for learning th e word distributions for the type values.
In this section, we propose a joint probabilistic model that unifies entity identification and document categorization b y relating word distributions with entities in the structure d database. The probabilistic model is shown in the plate representation in Figure 3. It encapsulates three plates, o r repetitive processes. The outermost plate describes the ge n-eration process for each document. This repeats N times, once for each document in the collection. Among its pa-rameters, the model has a prior probability P ( T = t ) over possible values for the type column T . The generation pro-cess for each document first chooses a value t for the type column T using the distribution P ( T ). This chosen value t for the type column plays the key role in relating the at-tribute terms and the unstructured words in the document.
On the unstructured side, this type value determines the words from the unstructured vocabulary W that are cho-sen for the document. This is done using a distribution P ( W | T = t ) over words w  X  X  for each value t of the type column. The smaller plate on the right shows the generation process for the unstructured words in the document. The process repeats m times, once for each word in the docu-ment. In each iteration, a word is chosen from W using the distribution P ( W | T = t ) for the chosen type column value.
Let us now see how the attribute terms are chosen. For each type value t , the model has a distribution P ( e | T = t ) over entities e from the database that have value t for their type column. The model first chooses an entity e using the distribution P ( E = e | T = t ). This is the central entity e in the document. Additionally, there is a prior distributio n P ( C = c ) over the k columns of the relation. These two dis-tributions allow us to generate the attribute terms a in the document. This process in captured using the smaller plate on the left. It repeats n times, once for each attribute term a in the document. In each iteration, the model chooses a column c from the prior distribution P ( C = c ) over columns. This determines the specific entity column e.C to be men-tioned as the attribute term. Then the attribute term a is generated using a noisy process from the column value e.C of the chosen entity e .

In summary, the model parameters  X  include the following distributions  X  P ( C ) over columns in the relation, P ( T ) over values for the type column, P ( E | T ) over entities in the database and P ( W | T ) over words in the unstructured vocabulary for each value t of the type column. The observed variables in the model are the unstructured words w  X  d.W and the attribute terms a  X  d.A contained in the document. They are shown shaded in Figure 3. The entity e , the type value t and the columns c for the attribute terms are all hidden and need to be inferred from the observed variables.
The probability P ( a | E = e, C = c ) describes the noise process that generates the attribute terms based on the col-umn values of the entity in the database. While it is possible to incorporate this as a parameter in the model, this has cer-Figure 3: Probabilistic generative model for docu-ments with attribute terms and unstructured words tain disadvantages. This needs to capture all different ways in which noise can be introduced in the domain, and ex-plicitly model all of them. This would tailor the model too closely to specific domains, and make it hard to generalize. Enumerating and assigning probabilities to all different wa ys for transforming an entity column value also is an extremely laborious process. Instead, we can make use of the fact that the document collection has already been provided to us. So, we can safely make a closed world assumption, and for any column value in the database, consider only those transformations for it that occur in the documents in the collection. For example, for the name  X  X arrison Ford X , the transformations that need to be considered for it are only  X  X ord X ,  X  X arrison Ford X ,  X  X . Ford X  and  X  X arrison X , if those are the only  X  X imilar X  names that occur in the collection. Thoug h  X  X arry Ford X  can be a valid candidate in general, we do not need to assign a probability to it, if it never occurs in the re -views that we have in hand. The issue here is in determining which terms in the document are  X  X imilar X . We handle this by considering as input a similarity measure for each column c  X  X  in the relation. Then, given any column value from the database, the similarity measure specified for that col-umn automatically defines a distribution over all attribute terms in the document collection. As an additional advan-tage, specifying the similarity measures decouples the mod el from the underlying domain.

We will now quickly run over the generation process to see how movie reviews are generated by our model in the exam-ple movie domain. Here, the type column corresponds to genres of movies, so the model has a prior distribution over movie genres. So first, a genre, say  X  X ction X , is picked using the distribution P ( Genre = x ). For each genre, the model has two distributions, P ( Movie = m | Genre ) over movies , and P ( W ord = w | Genre ) over unstructured words. Once the genre is chosen, the unstructured words (such as adven-turer , quest , justice , etc.) are generated for the document using the word distribution for the selected genre  X  X ction X  . On the structured side, a movie, say  X  X aiders of the Lost Ark X , is chosen from the  X  X ction X  genre using the distribu-tion over  X  X ction X  movies. Finally, attribute terms are gen -erated for this movie. This is done by repeatedly choosing a column from the movie table, such as  X  X ctor X ,  X  X irector X ,  X  X roducer X ,  X  X roduction House X , etc. using a prior distrib u-tion over movie columns, and then generating a noisy form of the chosen column for the movie. For example, for  X  X aiders of the Lost Ark X , we may first choose  X  X ctor X , get to  X  X ar-rison Ford X  and generate  X  X ord X  as the attribute term, then choose  X  X irector X , select  X  X eorge Lucas X  and generate  X  X u-cas X  as the second attribute term, and so on.

Consider a document d i containing m unstructured words w i = { w ij } and n attribute terms a i = { a ij } . The hidden variables for this document are the entity label e i , the type value t i and the columns labels c i = { c ij } , j = 1 ..n for each attribute term. The joint probability distribution over th ese variables given the model parameter  X  is defined as: = P ( t i , e i , c i , a i , w i |  X ) = P ( t i ) P ( e i | t i ) P ( c i ) P ( a i | e i , c i = P ( t i ) P ( e i | t i ) = P ( t i ) P ( e i | t i ) Y where we have factorized the joint distribution according to the independence assumptions in the model, and n c i and n i are the number of times column label c and word w respectively occur in the document d i .

If the document collection D contains N documents { d i } , then, assuming that each document is generated indepen-dently of the others, the likelihood of the document collec-tion given the model parameters  X  can be given as P ( D |  X ) = Q Formalizing the intuition: The factored joint distribu-tion in Eq. (1) helps to explain the intuition behind how the two tasks of entity identification and document categoriza-tion reinforce each other. Traditional entity identificati on P ( e i ) over the entities, for choosing the most likely entity for the document. In our model, the words in a document come into play to determine the  X  X ype X  for the document, and entities that belong to that particular type are more likely to be right entity for the document. This helps to reduce the ambiguity in identifying entities. On the document cat-egorization side, standard document categorization would consider P ( t i ) P ( w i | t i ) for choosing the most likely type for the document. But, in Eq. (1), the attribute terms in the document play a role in identifying entity candidates for th e document, and only the entities identified for the document contribute relevant type labels for it. We will see in Sectio n 5 that the experimental results back up this intuition.
In this section, we discuss how the model parameters, de-scribed in Section 3 are estimated from a given document collection and the algorithmic challenges that are involve d. Note that we do not require labeled training data for es-timating the model parameters, and instead resort to un-supervised estimation using the Expectation Maximization approach. Using standard EM derivation and incorporating the sum constraints for each probability distribution, the update rules for the model parameters look as follows: P ( w | t i = x ) = P
In order to do these updates, we need the posterior prob-abilities of the hidden variables computed for each docu-ment. They can be computed in a straight forward manner by marginalizing over the joint distribution:
P ( t i , e i , c i | a i , w i ,  X ) = P ( t i , e i , c i where P ( a i , w i ) = P t P e P c P ( t i = t, e i = e, c The basic EM algorithm is described using high-level pseudo code in Figure 4. However, it poses several scalability and other challenges which we describe and resolve next. Scalability Issues: We first consider the scalability issues that arise in the E-step of the algorithm. This is clearly brought out by lines 3-9 in Figure 4. Consider a document d that has n attribute terms. Then this document has n + 2 latent variables  X  t for the type, e for the entity and a c variable for each of the n attribute terms, indicating the column that it corresponds to. The unrestricted space of assignments to these n +2 variables is clearly intractable. If there are E entities in the database, T possible values for the type column, k different columns, then there are E  X  T  X  k possible value assignments to the n + 2 hidden variables for each document. Fortunately, very few of these assignments have non-zero posterior probability for the document. So ou r aim is to identify a significantly smaller space of assignmen ts per document that need to be evaluated.
First, observe that the exploring the entire E  X  T space is clearly not necessary. We consider domains where each en-tity has only one value for the type column. This drastically reduces the number of assignments for the entity and type values to E from E  X  T .

We next focus on restricting the number of entities that we need to consider for any document d . For each at-tribute term a i in the document d , we identify the candi-date matches for this term across the various columns of the database using the similarity measure that has been spec-ified for this attribute. Each candidate match for a term a consists of an entity index e i , a column index c i and a score s i . For example, the attribute term  X  X ohnny Gray X  in a movie review returns ( director, 7395 , 0 . 789) as a match, in-dicating that it matches the director column of movie 7395 (which has value X  X ray, John III X  X n the database) with score 0.789. Note that the score is determined by the similarity measure that is employed for that attribute. The matches for all the terms in a document allow us to identify what assignments to the hidden entity and column values are rele-vant for this document. We extract the set of all entities tha t occur in a candidate match for at least one term in the doc-ument. These form the set of candidate entities eCand ( d ) for the document.

Now, we look at the column assignment space for attribute terms. For each entity e in eCand ( d ), clearly not all at-tribute terms in document d can map to all columns for e . From the candidate matches for each term a i , we find the columns that are matched by this term for entity e . Po-tentially, there can be multiple such columns for the same entity. The term  X  X ergman X  in a review matches the di-rector  X  X ngmar Bergman X , the writer  X  X ngmar Bergman X  and the actress  X  X ngrid Bergman X  columns for the Swedish movie  X  X   X  ostsonaten X . But barring such few and exceptional cases, a term matches at most one column for any entity. So if we get a maximum of e max entities in the candidate set eCand ( d ) for any document, then we have reduced E  X  T  X  k n possible assignments to a maximum of e max assignments to evaluate. Note that e max is a significantly smaller number than E , and grows extremely slowly with increasing number of entities i n the database.
 Initialization: Initialization is a critical issue to avoid lo-cal maxima in EM. While initializing all probabilities uni-formly is an option, it usually does not work very well. We make use of the candidate matches for attribute terms and their corresponding scores for model initialization. Inst ead of first initializing the parameters, we first initialize the pos-teriors using the candidate match scores, and then take es-timates of the parameters from the initial posteriors. For each entity e in eCand ( d ) described above, we compute its score for the document d by summing over its scores from each attribute term a in the document: score ( e, d ) = P a  X  d.A max c score ( e, a, c ). The candidate matches provide us with score ( e, a, c ), and for each attribute term, we take the maximum contribution towards each entity over all its column matches. Then the initial entity posteriors are com-puted by normalization: These initial estimates do not consider the words in the doc-ument, and as such, serve as a baseline for comparison with our joint model. We score the columns for each document using the candidate matches in a similar way, and them nor-malize them to compute the column marginals P init ( c | d ). To compute the marginals over type values, we uniformly dis-tribute the posterior for each entity over all its correspon ding type values, and then appropriately normalize to compute P init ( t | d ) and P init ( t, e | d ). For the words, we score words and type values via the documents that contain the words, ber of occurrences of word w in document d . Normalizing the scores provides initial estimates for P init ( w, t | d ). Start-ing from these estimates for the document posteriors, we get initial estimates for the model parameters.
In this section, we evaluate entity identification and doc-ument categorization performance of our model over dif-ferent datasets and compare against state-of-the art base-lines for both tasks. We have developed this model in a re-search engagement in a Customer Relationship Management (CRM) setting in a leading financial institution. Our huge unstructured document collection consisted of customer e-mails and agent call logs (summaries of interactions betwee n customers and agents) from the financial institution X  X  con-tact (call) center. The back-end structured data warehouse of customers, accounts and transaction details spans over 10 relations and 150 columns. We addressed the task of identifying the customer entity for each call log or email, and also categorizing the documents according to customer demographics, and the type of financial instrument such as credit card, debit card, savings or salary account used by them. We were able to achieve very good performance over both tasks inspite of very noisy textual data. But, owing to the confidential nature of this data, we are unable to publish the experimental details for it. As a shareable and repeat-able alternative, we consider the IMDB dataset and focus on the tasks of identifying movies from reviews and categoriz-ing reviews according to the movie genre. Additionally, to gain further insight into the performance of our model, and investigate how our results can generalize to datasets from other domains, we perform experiments on semi-synthetic data, where we can control data characteristics that influ-ence performance of our model. Processed versions of the IMDB dataset used in our experiments are made available for download 3 as per repeatability guidelines.
We now describe the various baselines for the entity iden-tification and document categorization tasks, and their pa-rameters.
 Document Classification Baseline: We use a state-of-the-art document classification baseline that makes use of all tokens in the review. We combine the attribute terms and the unstructured words in a review to create a bag-of-words for the classifier to use. We use linear-kernel SVMs (SVMLight 4 ) with default parameters We denote this as DC-Base ).
 Entity Identification Baseline: We use as our entity identification baseline the approach outlined in Eq. (2). Th is is in line with state-of-the-art entity identification tech niques [5] when applied to single tables. Recall that this only make s use of the attribute terms in the document and the candi-date matches for them. It does not make use of the list of unstructured words in the document. It does not involve any tunable parameters. We denote this is EI-Base . Joint Model: This is our joint model described in this paper. We denote this as JM . One very desirable aspect of the model is that, like the baseline above, it does not involv e any tunable parameters. We run at most 4 EM iterations, since loglikelihood mostly stabilizes by then.

For the entity identification task, in addition to accuracy, we also consider average decision entropy as a metric. For any document having k entity choices, we measure decision the predicted probability for entity e i . For any entity identi-fication approach, we can then take the average over decision entropy for all documents as a performance measure.
The IMDB dataset 5 consists of movies, actors, actresses, directors and writers lists, among other details. To create our unstructured document collection, we crawled the IMDB site and downloaded the first 10 reviews (user comments) for the top 50 movies for 25 IMDB genres. This led to 12 , 500 reviews in all. On the structured side, we created a sin-gle movie table containing movie details with 6 columns for Actor 1, Actor 2, Actress 1, Actress 2, Director 1, Director 2, W riter 1 and W riter 2. We use the actor rank information available in IMDB to populate the 2 actor (and actress) columns with the top 2 actors and actresses for the movie. No ranking being available for directors and writers, we pic k the first two that are listed for the movie. Note that in or-der to make the entity identification task challenging, we do not include the movie name as a column. We populated this movie table with the top 1250 movies from the 25 gen-res. Additionally, we added 25 , 000 randomly drawn movies from IMDB spanning all 25 genres. This resulted in a to-tal of 26 , 250 movies in our structured database. One issue with the IMDB dataset is that each movie is associated with multiple genres. We get around this issue by randomly pick-ing one of the genres for each movie and repeating this over several runs for all experiments.
 Pre-processing: Each review document was processed to obtain its list of unstructured words and the list of attribu te terms and their corresponding column matches in the databas e. The list of unstructured words is found simply by removing stop-words from the document. To find the attribute terms from the document, we need to annotate mentions of ac-tors, actresses, directors and writers from movie reviews. We use a state-of-the-art rule-based named entity annotato r [18] to annotate all person names. For this, we added a set of regular-expression based rules that uses capitalization p at-terns combined with First and Last Name dictionaries. To ensure high recall, we configured the dictionaries by adding all the first and last names of actors, actresses, directors a nd writers in the IMDB database.

Next, we compute the candidate matches for each at-tribute term that we extracted by matching against the six  X  person name  X  columns in our database. As already men-tioned, we need to account for name variations and spelling
Figure 5: Doc categorization accuracy on IMDB mistakes. Therefore, we need to perform similarity match-ing between attribute terms and database values. We experi-mented with various string similarity measures from the Sec -ondString package 6 . In our final experiments, we used the Soft-TFIDF measure coupled with the Jaro-Winkler metric. We built a wrapper around it to handle initializations, etc. for names. To address similarity matching in a scalable fash -ion, we indexed database names using first and last name tokens. This helps in quickly looking up candidate matches from the database for each attribute term. We then compute the similarity scores for these pairs.
 Table 1: Ent Identification Performance for IMDB Results for IMDB: We plot the results of our experiments on the IMDB dataset in Figure 5 and Table 1. First, in Fig-ure 5, we compare document categorization accuracy of JM against that achieved from the SVM baseline (DC-Base). Here we measure SVM accuracy on a separate held out set containing 20% of the reviews. On the X-axis, we vary the amount of training data provided for training. Of course, JM being unsupervised, it X  X  accuracy remains constant. Ob-serve that the supervised baseline catches up with JM only when it is provided with  X  35% of the document collec-tion as training samples. For JM , this  X  X upervision X  comes for free via automatic identification of entities for the doc -uments. As an added benefit, there are gains on the entity identification side as well, as can be seen from Table 1. We can see that JM improves entity identification performance over the baseline from 38 . 5% to 40 . 8% in terms of accuracy. But this is not the complete picture. For documents where JM is not able to correctly nail down the right entity, it is nevertheless greatly successful in reducing the confusion over the possible choices. This can be seen by comparing the the average decision entropy over candidate entities, which is re-duced to 0 . 067 for JM from 2 . 359 for the baseline. Much of this improvement is made possible by correctly inferring th e type value (genre) for the movie entity, though the actual movie could not be identified. Note that this is in perfect agreement with the intuition that we formalized at the end of Section 3.

Though JM shows some improvement in entity identifica-tion for IMDB, it is greatly below our expectation. This is in part because enough matches could not be found for the attribute terms in the documents. Recall that our back-end movie database does not have the names of the movies, and only contains names of the two actors and actresses, direc-tors and writers. It is quite challenging to identify movies based on just this information, as the baseline performance shows. We expected JM to improve over this by correctly identifying the genres based on the words in the reviews. But the multiple genres originally associated with each movie turns out to be a big hindrance for this. Recall that for each run, we randomly picked one genre for the movie. (Numbers reported in Figure 5 and Table 1 are averaged over 10 runs) This results in significant overlap across genres in terms of words, and makes genre identification very challenging for JM . Note that DC-Base achieves only  X  65% accuracy, even after using all the training data.

In summary, IMDB satisfies our motivational setting where words are related to column values in the database, but is not a perfect fit because of the multiple genre values asso-ciated with each movie. Unfortunately, the enterprise data that we worked on cannot be publicly disclosed, and we are not aware of publicly available data that has both the struc-tured component and unstructured documents labeled with structured entities (that we need for evaluation). As a viab le alternative, we experimented on semi-synthetic data, wher e we can control the degree of overlap between different cate-gories and evaluate our model under different settings. This also provides insights into how our model is expected to perform in domains with varying characteristics. Next, we describe the semi-synthetic data generation process and ou r experiments on it.
In our semi-synthetic data, we keep the structured movie table untouched. Of the list of genres available for each movie, we randomly pick one in each run of the experiment, as before. On the unstructured side, we re-create the docu-ments in two stages. First, we artificially partition the wor ds in our vocabulary between the genres with different degrees of overlap. This is achieved using an overlap probability p . Each word in the vocabulary is first assigned to one ran-domly chosen genre from the list. Additionally, it is assign ed to each of the remaining genres with probability p o . Clearly, higher values of p o result in the same word belonging to more genres, leading to less separability between genres. For ea ch genre, we assume all words assigned to it to be equally likely . Once the words have been assigned to genres, we re-create the reviews in our collection. For each review, we keep the n attribute terms untouched. We replace the m original un-structured words with m words sampled uniformly from the genre of the corresponding movie. Effectively, only the un-structured words in each review are replaced by new words. Everything else stays exactly the same as before.
In our experiments on semi-synthetic data, we vary the de-gree of overlap between the words assigned to different gen-res, and analyze the performance of JM . We first consider the entity identification results plotted in Figure 6(a). We can see that for very high overlap among the words in differ-ent genres, improvement in entity identification performan ce over the baseline is minimal. This mirrors the original IMDB scenario, where the words do not have enough information for genres to be correctly assigned to reviews. However, as the words begin to get spread out more clearly between gen-res, entity identification performance improves significan tly. Figure 6: (a) Entity Identification and (b) Docu-ment Categorization results on semi-synthetic data In fact, it increases by as much as 55 . 8% when words have medium overlap (from 38 . 5% to 60% for p o = 0 . 6), and by 80 . 3% (from 38 . 5% to 69 . 4% for p o = 0 . 1) when they are clearly indicative of the genre. This clearly underlines th e huge potential that categorization using words lends to en-tity identification. In Figure 6(b), we compare the document categorization performance of JM against that of DC-Base. Baseline performance also naturally improves when words are better separated between genres, but the crux is that document categorization happens for free in JM , where as 80% training data is used for the baseline. Most interest-ingly, for large overlap between genres, JM actually outper-forms the baseline in categorizing documents by leveraging attribute matches with the structured database. This is pre -cisely the mutual reinforcement that we had expected from the model. We had explained this intuitively at the end of Section 3, and these empirical results lend further credenc e to our claims. In summary, these experiments clearly show that the two tasks can benefit each other immensely when conditions are favorable, which we believe happens natural ly for many enterprise tasks.
One related body of work looks at integrating data across different structured data sources[17]. Several rule based a nd learning based techniques have been proposed for schema matching [14] that aims at matching related schema, and en-tity resolution/deduplication/tuple matching [9, 20, 1] t hat focuses on matching individual duplicate records in databa ses. Some effort has also gone into integrating information acros s unstructured resources[12, 13]. There has been some re-cent research on identifying structured entities from unst ruc-tured documents using pre-defined contexts[5]. However, the words in the documents that do not match against the database are ignored. In contrast, our goal is to use the unstructured words in the document in addition to the at-tribute terms to identify structured entities.

One way to deal with the issue of labeled data is unsu-pervised clustering using generative models. Topic mixtur e models of increasing complexity have been proposed for doc-uments over the years [15, 10, 2]. In comparison, we use a relatively simple model for the unstructured words in the document. It will be interesting to investigate if more com-plex mixture models can bring further improvements for en-tity identification. Also, all these generative models cons ider only unstructured words in documents, whereas we look to generate noisy entity mentions as well. The PRM frame-work [8] is a probabilistic generative model over values in a structured database that takes into account dependencies across different columns. Our focus is on documents that mention these values rather than the back-end data itself.
At a high level, the idea of using information or knowl-edge from one learning task to help another has been used in various forms. Semi-supervised methods are the most popu-lar among approaches that bootstrap learning on insufficient labeled data by exploiting abundance of unlabeled data[16] . Co-training in multi-view learning [3] uses two independen t feature partitions (views) on the same label-set and train-ing data. The goal is to iteratively add training data to one classification task and re-learn models using the curren t labeling from the other. In co-clustering[6], two clusteri ng tasks reinforce each other. More specifically, co-clusteri ng looks to simultaneously cluster instance rows and feature columns by iteratively transferring knowledge from the row s to the columns (and vice-versa). Cross-training[19] explo its mappings between classes across related but non-identical classification tasks to overcome lack of training data in any individual task. Multi-task learning aims to improve gener -alization for a learning task by using higher order domain knowledge from related tasks. It has been used for cluster-ing[21] where similarity measures are tweaked according to those in related clustering tasks, and in neural networks[4 ].
Our proposed model for joint entity identification and document categorization is similar in spirit, but unique in that we combine two hitherto unrelated tasks. We show how structured entity identification can benefit from un-structured words in documents via document categoriza-tion. Conversely, we also perform document categorization without an explicit label-set or training data where mappin g documents to entities implicitly provides supervision via the column values in the database.
In summary, we have considered two critical tasks over document collections, namely entity identification and doc -ument categorization. We have observed that the two tasks can be related naturally in many domains so that they can mutually benefit each other. We have explored different ways to formally relate the two tasks and proposed, as a solution, a probabilistic model for documents that generat es both entity mentions and unstructured words. We demon-strated that this model not only competes with supervised document classification approaches, but can also improve entity identification accuracy immensely in many scenarios . We believe that this paper will open up new directions of research in integration and analysis over unstructured doc -ument collections. [1] I. Bhattacharya and L. Getoor. Collective entity [2] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [3] A. Blum and T. Mitchell. Combining labeled and [4] R. Caruana. Multitask learning. Machine Learning , [5] V. T. Chakaravarthy, H. Gupta, P. Roy, and [6] I. S. Dhillon. Co-clustering documents and words [7] X. Dong, A. Halevy, and J. Madhavan. Reference [8] L. Getoor, N. Friedman, D. Koller, and B. Taskar. [9] M. Hern  X andez and S. Stolfo. The merge/purge [10] T. Hofmann. Probabilistic latent semantic indexing. I n [11] T. Joachims. Text categorization with support vector [12] X. Li, P. Morie, and D. Roth. Semantic integration in [13] A. McCallum and B. Wellner. Conditional models of [14] R. E. Melnik S., Garcia-Molina H. Similarity flooding: [15] K. Nigam, A. K. McCallum, S. Thrun, and [16] K. Nigam, A. K. McCallum, S. Thrun, and T. M. [17] B. P. A. Rahm Erhard. On matching schemas [18] G. Ramakrishnan. Bridging chasms in text mining [19] S. Sarawagi, S. Chakrabarti, and S. Godbole. [20] P. Singla and P. Domingos. Object identification with [21] S. Thrun and J. O X  X ullivan. Discovering structure in
