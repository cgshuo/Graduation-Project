 Measuring the error in rating prediction has been by far the domi-nant evaluation methodology in the Recommender Systems litera-ture. Yet there seems to be a gene ral consensus that this criterion alone is far from being enough to assess the practical effective-ness of a recommender system. In formation Retrieval metrics have started to be used to ev aluate item selection and ranking rather than rating prediction, but considerable divergence remains in the adoption of such metrics by different authors. On the other hand, recommendation utility includes other key dimensions and concerns beyond accuracy, such as novelty and diversity, user engagement, and business perform ance. While the need for fur-ther extension, formalization, cl arification and standardization of evaluation methodologies is rec ognized in the community, this need is still unmet for a larg e extent. The RUE 2012 workshop sought to identify and better understand the current gaps in rec-ommender system evaluation met hodologies, help lay directions for progress in addressing them, and contribute to the consolida-tion and convergence of experime ntal methods and practice. H.3.3 [ Information Search and Retrieval ]: information filtering . Algorithms, Measurement, Perfo rmance, Experimentation, Standardization, Theory. Utility, evaluation, methodology, metrics, recommender systems. Measuring the error in predicting held-out user rating values has been by far the dominant offline evaluation methodology in the Recommender Systems (RS) literature [6,9]. Yet there seems to be a general consensus in the community that this criterion alone effectiveness of a recommender system in matching user needs [12]. The end users of recommendations receive lists of items rather than rating values, whereby recommendation accuracy metrics  X  X s surrogates of the ev aluated task X  should target the quality of the item selection, rather than the numeric system scores that determine this selection. Furthermore, as far as the order of recommended items determin es the set of elements that the user will actually consider for consumption, effectiveness assessment methodologies should ta rget item rankings. For this reason, metrics and methodologies from the Information Retrieval (IR) field  X  X here ranking evaluation has been studied and stand-ardized for decades [11] X  have started to be adopted by the RS community [3,4,6,9]. Gaps remain between the methodological formalization of tasks in both fi elds though, which result in diver-gences in the adoption of IR methodologies for RS, hindering the interpretation and comparability of empirical observations by different authors. only one among several relevant dimensions of recommendation effectiveness. The value of novelty, for instance, has been recog-nized as a key dimension of recommendation utility for users in real scenarios, in-as-much as the purpose of recommendation is inherently linked to discovery in many application domains quality to enrich the user X  X  experience and enhance his array of relevant choices [1,8]. Novelty and diversity are generally positive for businesses as well, by favoring the diversity of sales and help-ing leverage revenues fro m market niches [5]. As a matter of busi-ness performance enhancement, th e value added by recommenda-tion can be measured more direc tly in terms of on-line clickthrough rate, conversion rate, sales order si ze increase, returning customers, networks commonly face multiple objective optimization problems related to user engagement, requi ring appropriate evaluation meth-odologies for optimizing along the entire recommendation funnel, effective recommendations for c onsumers and providers may in-clude confidence, coverage, risk, cost, robustness, etc. While the need for further extension, formalization, clarification and standardization of evaluation methodologies is recognized in engaging in evaluation work, researchers and practitioners are still often faced with experimental design questions for which there are currently not always precise and consensual answers. Room remains for further methodologi cal development and conver-gence, which motivated this workshop. RUE 2012 gathered researchers and practitioners interested in developing better, clearer, and/or more complete evaluation methodologies for recommender syst ems  X  X r just seeking clear guidelines for their experimental needs. The workshop provided an informal setting for exchanging and discussing ideas, sharing experiences and viewpoints. RU E sought to identify and better understand the current gaps in recommender system evaluation methodologies, help lay directi ons for progress in addressing them, and contribute to the consolidation and convergence of experimental methods and practice. As a particular focus of inter-est, the workshop sought a better understanding of the methodo-logical connections from confluent disciplines (IR, Machine spectives. Specific questions raised and addressed by the workshop includ-ed, among others, the following:  X  What are the unmet needs and challenges for evaluation in the 
RS field? What changes would we like to see? How could we speed up progress?  X  What relevant recommendation utility and quality dimensions should be cared for? How can th ey be captured and measured?  X  How can metrics be more clearly and/or formally related to the task, contexts and goals for which a recommender application is deployed?  X  How should IR metrics be app lied to recommendation tasks? 
What aspects require adjustment or further clarification? What further methodologies should we draw from other disciplines?  X  What biases and noise should experimental design typically watch for?  X  Can we predict the success of a recommendation algorithm with our offline experiments? What offline metrics correlate better and under which conditions?  X  What are the outreach and limitations of offline evaluation? How can online and offline experiments complement each other?  X  What type of public datasets and benchmarks would we want to have available, and how can they be built?  X  How can the recommendation effect be traced on business outcomes?  X  How should the academic evaluation methodologies improve their relevance and usefulness for industrial settings?  X  How do we envision the evalua tion of recommender systems in the future? The accepted papers and the discussions held at the workshop addressed, among others, the following topics:  X  Recommendation quality dimensions.  X  Effective accuracy, ranking quality.  X  Novelty, diversity, unexpectedness, serendipity.  X  Utility, gain, cost, risk, benefit.  X  Robustness, confidence, coverage, usability, etc.  X  Matching metrics to tasks, needs, and goals.  X  User satisfaction, user pe rception, human factors.  X  Business-oriented evaluation.  X  Multiple objective optimization, user engagement.  X  Quality of service, quality of experience.  X  Evaluation methodology and experimental design.  X  Definition and evaluation of ne w metrics, studies of exist- X  Adaptation of methodologies fro m related fields: IR, Ma- X  Evaluation theory.  X  Practical aspects of evaluation.  X  Offline and online experimental approaches.  X  Simulation-based evaluation.  X  Datasets and benchmarks.  X  Validation of metrics. The workshop opened with a keynote talk, followed by the presentation of accepted papers and open discussions. The accept-ed papers and a summary of discussions are available in the work-shop proceedings, which can be reached from the workshop web-site at http://ir.ii.uam.es/rue2012. [1] Castells, P., Wang, J., Lara, R., Zhang, D. Workshop on nov-[2] Celma, O. and Herrera, P. A New Approach to Evaluating [3] Cremonesi, P., Garzotto, F., Ne gro, S., Papadopoulos, A. V., [4] Cremonesi, P., Koren, Y., a nd Turrin, R. Performance of [5] Fleder, D. M. and Hosanagar, K. Blockbuster Culture X  X  Next [6] Herlocker, J. L., Konstan, J. A., Terveen, L. G., and Riedl, J. [7] Lathia, N., Hailes, S., Capra, L., and Amatriain, X. Temporal [8] McNee, S. M., Riedl, J., and Konstan, J. A. Being Accurate [9] Shani, G. and Gunawardana, A. Evaluating Recommendation [10] Vargas, S. and Castells, P. Rank and Relevance in Novelty [11] Voorhees, E. M. and Harman, D. K. TREC: Experiment and [12] Willemsen, M. C., Bollen, D. G. F. M., Ekstrand, M. D. 
