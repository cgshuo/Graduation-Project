 Association rule mining has become one of the most popular data exploration techniques, allowing users to generate unexpected rules from  X  X arket basket X  data. Proposed by Agrawal et al. [1, 2], association rule mining discovers all rules in the data that satisfy a user-specified minimum support (minsup) and mini-mum confidence (minconf). Minsup represents the minimum amount of evidence (that is, number of transactions) we require to consider a rule valid, and minconf specifies how strong the implication of a rule must be to be considered valuable. tional databases. Let I = { i 1 ,i 2 ,...,i m } be a set of items and D be a set of transactions, where each transaction T is a set of items such that T  X  I .An association rule is an implication of the form X  X  Y , where X  X  I , Y  X  I , and X  X  Y =  X  . X is referred to as the antecedent of the rule, and Y as the consequent .Therule X  X  Y holds in the transaction set D with confidence c % if c % of transactions in D that contain X also contain Y .Therule X  X  Y has support of s % in the transaction set D ,if s % of transactions in D contain X  X  Y [2]. One measure of the predictive strength of a rule X  X  Y is its lift value, calculated as confidence( X  X  Y ) / support( Y ). Lift indicates the degree to which Y is more likely to be present when X is present; if lift is less than 1 . 0, Y is less likely to be present with X than Y  X  X  baseline frequency in D . The task of generating association rules is that of generating all rules that meet minimum support and minimum confidence, and perhaps meet further requirements such as having lift greater than 1 . 0.
 mining methods. However, several authors have pointed out that the Apriori algorithm, by definition, hinders us from finding rules with low support and high confidence [3, 4, 5]. Apriori generates frequent itemsets (i.e. those that will produce rules with support higher than minsup) by joining the frequent itemsets of the previous pass and pruning those subsets that have a support lower than minsup [2]. Hence, to generate rules that have low support, minsup must be set very low, drastically increasing the running time of the algorithm. This is known as the rare item problem . It means that, using the Apriori algorithm, we are unlikely to generate rules that may indicate events of potentially dramatic consequence. For example, we might miss out rules that indicate the symptoms of a rare but fatal disease due to the frequency of incidences not reaching the minsup threshold. Some previous solutions to this problem are reviewed in Section 2. high confidence rules effectively. We call such rules  X  X poradic X  because they represent rare cases that are scattered sporadically through the database but with high confidence of occurring together. In order to find sporadic rules with Apriori, we have to set a very low minsup threshold, drastically increasing the algorithm X  X  running time. In this paper, we adopt an Apriori-Inverse approach: we propose an algorithm to capture rules using a maximum support threshold. First, we define the notion of a perfectly sporadic rule, where the itemset forming the rule consists only of items that are all below the maximum support threshold. To enable us to find imperfectly sporadic rules, we allow maximum support to be increased slightly to include itemsets with items above maximum support. Finally, we demonstrate that Apriori-Inverse lets us find sporadic rules more quickly than using the Apriori algorithm. The most well-known method for generating association rules is the Apriori algorithm [2]. It consists of two phases: the first finds itemsets that satisfy a user-specified minimum support threshold, and the second generates association rules that satisfy a user-specified minimum confidence threshold from these  X  X requent X  itemsets. The algorithm generates all rules that satisfy the two thresholds and avoids generating itemsets that do not meet minimum support, even though there may be rules with low support that have high confidence. Thus, unless minimum support is set very low, sporadic rules will never be generated. There are several proposals for solving this problem. We shall discuss the MSApriori (Multiple Supports Apriori), RSAA (Relative Support Apriori Algorithm) and Min-Hashing approaches.
 they cannot contribute to rules generated by Apriori, even though they may participate in rules that have very high confidence. They overcome this problem with a technique whereby each item in the database can have a minimum item support (MIS) given by the user. By providing a different MIS for different items, a higher minimum support is tolerated for rules that involve frequent items and lower minimum support for rules that involve less frequent items. The MIS for each data item i is generated by first specifying LS (the lowest allowable minimum support), and a value  X , 0  X   X   X  1 . 0. MIS( i ) is then set according to the following formula: finding some rare-itemset rules. However, the actual criterion of discovery is determined by the user X  X  value of  X  rather than the frequency of each data item. Thus Yun et al. [5] proposed the RSAA algorithm to generate rules in which significant rare itemsets take part, without any  X  X agic numbers X  specified by the user. This technique uses relative support : for any dataset, and with the support of item i represented as sup ( i ), relative support ( RSup ) is defined as: Thus, this algorithm increases the support threshold for items that have low frequency and decreases the support threshold for items that have high frequency. Like Apriori and MSApriori, RSAA is exhaustive in its generation of rules, so it spends time looking for rules which are not sporadic (i.e. rules with high support and high confidence). If the minimum-allowable relative support value is set close to zero, RSAA takes a similar amount of time to that taken by Apriori to generate low-support rules in amongst the high-support rules.
 significant rules without any constraint on support. Transactions are stored as a 0/1 matrix with as many columns as there are unique items. Rather than searching for pairs of columns that would have high support or high confidence, Cohen et al. search for columns that have high similarity , where similarity is defined as the fraction of rows that have a 1 in both columns when they have a 1 in either column. Although this is easy to do by brute-force when the matrix fits into main memory, it is time-consuming when the matrix is disk-resident. Their solution is to compute a hashing signature for each column of the matrix in such a way that the probability that two columns have the same signature is proportional to their similarity. After signatures are calculated, candidate pairs are generated, and then finally checked against the original matrix to ensure that they do indeed have strong similarity.
 tion will produce many rules that have high support and high confidence, since only a minimum acceptable similarity is specified. It is not clear that the method will extend to rules that contain more than two or three items, since m C r checks for similarity must be done where m is the number of unique items in the set of transactions, and r is the number of items that might appear in any one rule. Removing the support requirement entirely is an elegant solution, but it comes at a high cost of space: for n transactions containing an average of r items over m possible items, the matrix will require n  X  m bits, whereas the primary data structure for Apriori-based algorithms will require n  X  log 2 m  X  r bits. For a typical application of n =10 9 , m =10 6 and r =10 2 , this is 10 15 bits versus approximately 2  X  10 12 bits.
 out having to wade through a lot of rules that have high support (and are there-fore not sporadic), without having to generate any data structure that would not normally be generated in an algorithm like Apriori, and without generating a large number of trivial rules (e.g. those rules of the form A  X  B where the support of B is very high and the support of A rather low). In the next section, we propose a framework for finding certain types of sporadic rules. In the previous section, the techniques discussed generate all rules that have high confidence and support. Using them to find sporadic rules would require setting a low minsup. As a result, the number of rules generated can be enormous, with only a small number being significant sporadic rules. In addition, not all rules generated with these constraints are interesting. Some of the rules may correspond to prior knowledge or expectation, refer to uninteresting attributes, or present redundant information [6]. 3.1 Types of Sporadic Rule We refer to all rules that fall below a user-defined maximum support level (max-sup) but above a user-defined minimum confidence level (minconf) as sporadic rules . We further split sporadic rules into those that are perfectly sporadic (have no subsets above maxsup) and those that are imperfectly sporadic . We then demonstrate an algorithm, which we call Apriori-Inverse, that finds all perfectly sporadic rules.
 and no member of the set of A  X  B may have support above maxsup. Perfectly sporadic rules thus consist of antecedents and consequents that occur rarely (that is, less often than maxsup) but, when they do occur, tend to occur together (with at least minconf confidence).
 certainly does not cover all cases of rules that have support lower than max-sup. For instance, suppose we had an itemset A  X  B with support( A ) = 12%, support( B ) = 16%, and support( A  X  B ) = 12%, with maxsup = 12% and min-conf = 75%. Both A  X  B (confidence = 100%) and B  X  A (confidence = 75%) are sporadic in that they have low support and high confidence, but neither are perfectly sporadic, due to B  X  X  support being too high. Thus, we define imperfectly sporadic rules as the following: and minconf but has a subset of its constituent itemsets that has support above maxsup. Clearly, some imperfectly sporadic rules could be completely trivial or uninteresting: for instance, when the antecedent is rare but the consequent has support of 100%. What we should like is a technique that finds all perfectly sporadic rules and some of the imperfectly sporadic rules that are nearly perfect. 3.2 The Apriori-Inverse Algorithm In this section, we introduce the Apriori-Inverse algorithm. Like Apriori, this algorithm is based on a level-wise search. On the first pass through the database, an inverted index is built using the unique items as keys and the transaction IDs as data. At this point, the support of each unique item (the 1-itemsets) in the database is available as the length of each data chain. To generate k -itemsets under maxsup, the ( k  X  1)-itemsets are extended in precisely the same manner as Apriori to generate candidate k-itemsets. That is, a ( k  X  1)-itemset i 1 is turned into a k -itemset by finding another ( k  X  1)-itemset i 2 that has a matching prefix of size ( k  X  2), and attaching the last item of i 2 to i 1 . For example, the 3-but { 1 , 3 , 4 } and { 1 , 2 , 5 } will not produce a 4-itemset due to their prefixes not matching right up until the last item.
 at least meet a minimum absolute support requirement (say, at least 5 instances) and are pruned if they do not (the length of the intersection of a data chain in the inverted index provides support for a k-itemset with k larger than 1). The process continues until no candidate itemsets can be generated, and then association rules are formed in the usual way.
 we have simply inverted the downward-closure principle of the Apriori algorithm; rather than all subsets of rules being over minsup, all subsets are under max-sup. Since making a candidate itemset longer cannot increase its support, all extensions are viable except those that fall under our minimum absolute sup-port requirement. Those exceptions are pruned out, and are not used to extend itemsets in the next round.
 Algorithm Apriori-Inverse Input: Transaction Database D , maxsup value Output: Sporadic Itemsets (1) Generate inverted index I of (item, [TID-list]) from D . (2) Generate sporadic itemsets of size 1: (3) Find S k , the set of sporadic k -itemsets where k  X  2 : considers itemsets that have support above maxsup; therefore, no subset of any itemset that it generates can have support above maxsup. However, it can be extended easily to find imperfectly sporadic rules that are nearly perfect: for instance, by setting maxsup i to maxsup/minconf where maxsup i is maximum support for imperfectly sporadic rules and maxsup is maximum support for reported sporadic rules. 3.3 The  X  X ess Rare Itemset X  Problem It is, of course, true that rare itemsets may be formed by the combination of less rare itemsets. For instance, itemset A may have support 11%, itemset B support 11%, but itemset A  X  B only 9%, making A  X  B sporadic for a maxsup of 10% and A  X  B a valid imperfectly sporadic rule for minconf of 80%. However, Apriori-Inverse will not generate this rule if maxsup is set to 10%, for A  X  B is an imperfectly sporadic rule rather than a perfectly sporadic one.
 note, however, that not all imperfectly sporadic rules are necessarily interesting: in fact, many are not. One definition of rules that are trivial is proposed by Webb and Zhang [7], and a similar definition for those that are redundant given by Liu, Shu, and Ma [8]. Consider an association rule A  X  C with support 10% and confidence 90%. It is possible that we may also generate A  X  B  X  C , with support 9% and confidence 91%: however, adding B to the left hand side has not bought us very much, and the shorter rule would be preferred. Another situation in which trivial rules may be produced is where a very common item is added to the consequent; it is possible that A  X  C has high confidence because the support of C is close to 100% (although in this case, it would be noticeable due to having a lift value close to 1 . 0). Therefore, we do not necessarily wish to generate all imperfectly sporadic rules.
 duce rules that are not-too-far from being perfect. We refer to the modifications as  X  X ixed Threshold X ,  X  X daptive Threshold X , and  X  X ill Climbing X . In general, we adjust the maxsup threshold to enable us to find at least some imperfectly sporadic rules: specifically, those that contain subsets that have support just a little higher than maxsup.
 Fixed Threshold: In this modification, we propose adjusting the maximum support threshold before running Apriori-Inverse to enable us to find more rare itemsets. The maximum support threshold is adjusted by taking the proportion of the maximum support threshold and the minconf threshold. For example, given a minsup threshold of 0 . 20 and a minconf of 0 . 80, the new minsup thresh-old would be set to 0 . 2 / 0 . 8=0 . 25. However, during the generation of rules, we only consider itemsets that satisfy the original maximum support threshold. Rules that have supports which are higher than the original maxsup are not generated.
 Adaptive Threshold: In this modification, we propose changing the maximum support by a small increment  X  (typically 0 . 001) at each value of k during the generation of sporadic k -itemsets. The threshold is increased until the number of itemsets in the current generation does not change when compared to the previous generation. In general, we search for a plateau where the number of itemsets found does not change.
 Hill Climbing: Hill Climbing is an extension of Adaptive Threshold; it adjusts the maximum support threshold by adding an increment that is the product of a rate-variable  X  (like the learning constant for a gradient descent algorithm; but typically 0 . 01) and the gradient of the graph of the number of itemsets generated so far. Like the previous method we modify the threshold until the number of itemsets reaches a plateau. Using this method in a large dataset the plateau is likely to be found sooner, since the increment used becomes greater when the gradient is steep and smaller when the gradient becomes less steep. In this section, we compare the performance of the standard Apriori algorithm program with the proposed Apriori-Inverse. We also discuss the results of three the different variation of Apriori-Inverse. Testing of the algorithms was carried out on six different datasets from the UCI Machine Learning Repository [9]. algorithms. Each row of the table represents an attempt to find perfectly spo-radic rules X  X ith maxsup 0 . 25, minconf 0 . 75, and lift greater than 1 . 0 X  X rom the database named in the left-most column. For Apriori-Inverse, this just involves setting maxsup and minconf values. For the Apriori algorithm, this involves set-ting minsup to zero (conceptually; in reality, the algorithm has been adjusted to use a minimum absolute support of 5), generating all rules, then pruning out those that fall above maxsup. In each case, this final pruning step is not counted in the total time taken. In the first three cases, Apriori was able to generate all frequent itemsets with maxsup greater than 0 . 0, but for the final three it was not clear that it would finish in reasonable time. To give an indication of the amount of work Apriori is doing to find low-support rules, we lowered its minsup thresh-old until it began to take longer than 10 , 000 seconds to process each data set. the Teaching Assistant Evaluation dataset, Bridges dataset, and Zoo dataset. However, using Apriori on the Flag dataset and Mushroom dataset, we could only push the minimum support down to 0 . 11 and 0 . 15 respectively, before hitting the time constraint of 10 thousand seconds. Compare this to Apriori-Inverse, finding all perfectly sporadic rules in just a few minutes for the Mushroom database. For the Soybean-Large dataset, no rules below a support of 43% could be produced in under 10 thousand seconds.
 databases such as the first three in Table 1, a method such as Apriori-Inverse is required if sporadic rules under a certain maximum support are to be found in larger or higher-dimensional datasets. We also note that Apriori is finding a much larger number of rules under maxsup than Apriori-Inverse; this is, of course, due to Apriori finding all of the imperfectly sporadic rules as well as the perfectly sporadic rules. To take the Teaching Evaluation Dataset as an example, Apriori finds whereas, from this particular grouping, Apriori-Inverse only finds support, lift, and confidence values as the first, they both count as trivial ac-cording to the definitions given in [8] and [7]. Apriori-Inverse has ignored them (indeed, has never spent any time trying to generate them) because they are imperfect.
 to find some imperfectly sporadic rules. The Fixed Threshold method finds the largest number of sporadic rules, because it is  X  X vershooting X  the maxsup thresh-olds determined by the two adaptive techniques, and therefore letting more item-sets into the candidate group each time. As a result, it requires fewer passes of the inverted index, but each pass takes a bit longer, resulting in longer running times. However, the times for the Fixed Threshold version seem so reasonable that we are not inclined to say that the adaptive techniques give any signifi-cant advantage. Determining a principled way to generate imperfectly sporadic rules X  X nd determining a good place to stop generating then X  X emains an open research question. Nevertheless, we note that the time taken to generate all of the imperfectly sporadic rules by all three methods remains very much smaller than the time taken to find them by techniques that require a minimum support constraint.
 Existing association mining algorithms produce all rules with support greater than a given threshold. But, to discover rare itemsets and sporadic rules, we should be more concerned with infrequent items. This paper proposed a more efficient algorithm, Apriori-Inverse, which enables us to find perfectly sporadic rules without generating all the unnecessarily frequent items. We also defined the notion of imperfectly sporadic rules, and proposed three methods of finding them using Apriori-Inverse: Fixed Threshold, Adaptive Threshold, and Hill Climbing. Apriori-Inverse are X  X t best X  X euristic. More importantly, there are some types of imperfectly sporadic rule that our methods will not find at all. Our future work will involve ways of discovering rules such as A  X  B  X  C where neither A nor B is rare, but their association is, and C appears with A  X  B with high confidence. This is the case of a rare association of common events ( A and B ) giving rise to a rare event ( C ). It is a particularly interesting form of imperfectly sporadic rule, especially in the fields of medicine (rare diseases) and of process control (disaster identification and avoidance).

