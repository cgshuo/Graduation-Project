 In machine learning problems the data often live in a vector space, typically a Euclidean space. However, there are many other kinds of non-Euclidean spaces suitable for data outside this conven-tional context. In this paper we focus on the domain where each data sample is a linear subspace of vectors, rather than a single vector, of a Euclidean space. Low-dimensional subspace structures are commonly encountered in computer vision problems. For example, the variation of images due to the change of pose, illumination, etc, is well-aproximated by the subspace spanned by a few  X  X igenfaces X . More recent examples include the dynamical system models of video sequences from human actions or time-varying textures, represented by the linear span of the observability matrices [1, 14, 13].
 Subspace-based learning is an approach to handle the data as a collection of subspaces instead of the usual vectors. The appropriate data space for the subspace-based learning is the Grassmann manifold can define positive definite kernels on the Grassmann manifold, which allows us to treat the space as if it were a Euclidean space. Previously, the Binet-Cauchy kernel [17, 15] and the Projection kernel [16, 6] have been proposed and demonstrated the potential for subspace-based learning problems. On the other hand, the subspace-based learning problem can be approached purely probabilistically. Suppose the set of vectors are i.i.d samples from an arbitrary probability distribution. Then it is possible to compare two such distributions of vectors with probabilistic similarity measures, such as the KL distance 1 , the Chernoff distance, or the Bhattacharyya/Hellinger distance, to name a few [11, 7, 8, 18]. Furthermore, the Bhattacharyya affinity is indeed a positive definite kernel function on the space of distributions and have nice closed-form expressions for the exponential family [7]. In this paper, we investigate the relationship between the Grassmann kernels and the probabilis-tic distances. The link is provided by the probabilistic generalization of subspaces with a Factor Analyzer which is a Gaussian  X  X lob X  that has nonzero volume along all dimensions. Firstly, we show that the KL distance yields the Projection kernel on the Grassmann manifold in the limit of zero noise, whereas the Bhattacharyya kernel becomes trivial in the limit and is suboptimal for subspace-based problems. Secondly, based on our analysis of the KL distance, we propose an set of affine as well as scaled subspaces.
 We will demonstrate the extended kernels with the Support Vector Machines and the Kernel Dis-criminant Analysis using synthetic and real image databases. The proposed kernels show the better performance compared to the previously used kernels such as Binet-Cauchy and the Bhattacharyya kernels. In this section we will consider the two well-known probabilistic distances, the KL distance and the Bhattacharyya distance, and establish their relationships to the Grassmann kernels. Although these is considered as i.i.d. samples from the i -th Factor Analyzer the full covariance C nor invert it.
 More importantly, we use the FA distribution to provide the link between the Grassmann manifold look at the limits of the KL distance and the Bhattacharyya kernel under this condition. 2.1 KL distance in the limit The (symmetrized) KL distance is defined as e Z = 2  X  1 / 2 [ e Y 1 e Y 2 ] . In this case the KL distance is to We can ignore the multiplying factors which do not depend on Y 1 or Y 2 , and rewrite the distance as Projection distance [2, 16, 6], with the corresponding Projection kernel 2.2 Bhattacharyya kernel in the limit Jebara and Kondor [7, 8] proposed the Probability Product kernel which includes the Bhattacharyya kernel as a special case.
 of (7) is well-defined. In contrast, the diagonal terms of k Prob become which diverges to infinity as  X   X  0 . This implies that after normalizing the kernel by the diagonal terms, the resulting kernel becomes a trivial kernel including the Bhattacharyya kernel, loses its discriminating power as the distributions become close to subspaces. Based on the analysis of the previous section, we will extend the Projection kernel (4) to more general spaces than the Grassmann manifold in this section. We will examine the two directions of extension: from linear to affine, and from homogeneous to scaled subspaces. 3.1 Extension to affine subspaces An affine subspace in R D is a linear subspace with an  X  X ffset X  . In that sense a linear subspace is simply an affine subspace with a zero offset. Analogously to the (linear) Grassmann manifold, we can define an affine Grassmann manifold as the set of all m -dim affine subspaces in R D space 2 . The affine span is defined from the orthonormal basis Y  X  R D  X  m and an offset u  X  R D by condition for the equivalent of representations: Definition 1 (invariance to representations) . where Y  X  is any orthonormal basis for the orthogonal complement of span( Y ) .
 Similarly to the definition of Grassmann kernels [6], we can now formally define the affine Grass-mann kernel as follows. Let k : ( R m  X  D  X  R D )  X  ( R m  X  D  X  R D )  X  R be a real valued symmetric function k ( Y 1 , u 1 , Y 2 , u 2 ) = k ( Y 2 , u 2 , Y 1 , u 1 ) . Definition 2. A real valued symmetric function k is an affine Grassmann kernel if it is positive definite and invariant to different representations: aspan( Y 1 , u 1 ) = aspan( Y 3 , u 3 ) and aspan( Y 2 , u 2 ) = aspan( Y 4 , u 4 ) . With this definition we check if the KL distance in the limit suggests an affine Grassmann kernel. The KL distance with the homogeneity condition only Y 0 1 Y 1 = Y 0 2 Y 2 = I m becomes, J we will denote as the  X  X inear X  kernel to emphasize the underlying assumption: The second term give rise to a new  X  X ernel X  term is not invariant under the invariance condition unfortunately. We instead propose the slight modification: The proof of the proposed form being invariant and positive definite is straightforward and is omit-ted. Combined with the linear term k Lin , this defines the new  X  X ffine X  kernel As we can see, the KL distance with only the homogeneity condition has two terms related to the two separate positive kernels for subspaces and for offsets, we can add or multiply them together to construct new kernels [10]. 3.2 Extension to scaled subspaces We have assumed homogeneous subspace so far. However, if the subspaces are computed from the PCA of real data, the eigenvalues in general will have non-homogeneous values. To incorpo-rate these scales for affine subspaces, we now allow the Y to be non-orthonormal and check if the resultant kernel is still valid.
 Ignoring the multiplicative factors, the limiting (  X   X  0 )  X  X ernel X  from (3) becomes which is again not well-defined.
 The second term is the same as (12) in the previous subsection, and can be modified in the same way to k u = u 0 1 ( I  X  b Y 1 b Y 0 1 )( I  X  b Y 2 b Y 0 2 ) u 2 .
 following form among other possibilities.
 The sum of the two modified terms, is the proposed  X  X ffine scaled X  kernel: This is a positive definite kernel which can be shown from the definition. Summary of the extended Projection kernels The proposed kernels are summarized below. Let Y i be a full-rank D  X  m matrix, and let b Y = Y i ( Y 0 i Y i )  X  1 / 2 the orthonormalization of Y i as before. We also spherize the kernels so that k ( Y 1 , u 1 , Y 1 , u 1 ) = 1 for any Y 1 and u 1 .
 There is a caveat in implementing these kernels. Although we used the same notations Y and b Y for computed from data assuming u = 0 , whereas for affine kernels the Y and b Y are computed after removing the estimated mean u from the data. 3.3 Extension to nonlinear subspaces A systematic way of extending the Projection kernel from linear/affine subspaces to nonlinear spaces former kernels. Note that the proposed kernels (15) can be computed only from the inner products of the column vectors of Y  X  X  and u  X  X  including the orthonormalization procedure. If we replace the this implicitly defines a nonlinear feature space. This  X  X oubly kernel X  approach has already been proposed for the Binet-Cauchy kernel [17, 8] and for probabilistic distances in general [18]. We can adopt the trick for the extended Projection kernels as well to extend the kernels to operate on  X  X onlinear subspaces X  3 . In this section we demonstrate the application of the extended Projection kernels to two-class clas-sification problems with Support Vector Machines (SVMs). 4.1 Synthetic data we generate three types of data  X   X  X asy X ,  X  X ntermediate X  and  X  X ifficult X   X  from MFA distribution, which cover the different ranges of data distribution.
 A total of N = 100 FA distributions are generated in D = 10 dimensional space. The parameters of each FA distribution p i ( x ) = N ( u i , C i ) are randomly chosen such that of the remaining distributions p i are determined from whether they are close to p + or p  X  . The distances are measured by the KL distance J KL . 4.2 Algorithms and results We compare the performance of the Euclidean SVM with linear/ polynomial/ RBF kernels and the performance of SVM with Grassmann kernels. To test the original SVMs, we randomly sampled tion by holding out one set and training with the other N  X  1 sets. The polynomial kernel used is k ( x 1 , x 2 ) = (  X  x 1 , x 2  X  + 1) 3 .
 To test the Grassmann SVM, we first estimated the mean u i and the basis Y i from n = 50 points of the probabilistic PCA [12], although they can also be estimated by the Expectation Maximization approach.
 Six different Grassmann kernels are compared: 1) the original and the extended Pro-jection kernels (Linear, Linear Scaled, Affine, Affine Scaled), 2) the Binet-Cauchy k rithms with leave-one-out test by holding out one subspace and training with the other N  X  1 subspaces.
 Table 1: Classification rates of the Euclidean SVMs and the Grassmann SVMs. The BC and Bhat are short for Binet-Cauchy and Bhattacharyya kernels, respectively.
 Table 1 shows the classification rates of the Euclidean SVMs and the Grassmann SVMs, averaged for 10 trials. The results shows that best rates are obtained from the extended kernels, and the Euclidean kernels lag behind for all three types of data. Interestingly the polynomial kernels often perform worse than the linear kernels, and the RBF kernel performed even worse which we do not report. For the  X  X ifficult X  data where the means are zero, the linear SVMs degrade to the chance-level ( 50% ), which agrees with the intuitive picture that any decision hyperplane that passes the origin will roughly halve the points from a zero-mean distribution. As expected, the linear kernel is inappropriate for data with nonzero offsets ( X  X asy X  and  X  X ntermediate X ), whereas the affine kernel performs well regardless of the offsets. However, there is no significant difference between the homogeneous and the scaled kernels. The Binet-Cauchy and the Bhattacharyya kernels mostly underperformed.
 We conclude that under certain conditions the extended kernels have clear advantages over the orig-inal linear kernels and the Euclidean kernels for the subspace-based classification problem. In this section we demonstrate the application of the extended Projection kernels to recognition problems with the kernel Fisher Discriminant Analysis [10]. 5.1 Databases The Yale face database and the Extended Yale face database [3] together consist of pictures of 38 subjects with 9 different poses and 45 different lighting conditions. The ETH-80 [9] is an object database designed for object categorization test under varying poses. The database consists of pic-tures of 8 object categories and 10 object instances for each category, recored under 41 different poses. These databases have naturally factorized structures which make them ideal to test subspace-based learning algorithms with. In Yale Face database, a set consists of images of all illumination con-ditions a person at a fixed pose. By treating the set as a point in the Grassmann manifold, we can perform illumination-invarint learning tasks with the data. For ETH-80 database, a set consists of Grassmann manifold, we can perform pose-invariant learning tasks with the data.
 There are a total of N = 279 and 80 sets as described above respectively. The images are resized to the dimension of D = 504 and 1024 respectively, and the maximum of m = 9 dimensional subspaces are used to compute the kernels. The subspace parameters Y i , u i and  X  are estimated from the probabilistic PCA [12]. 5.2 Algorithms and results We perform subject recognition tests with Yale Face, and categorization tests with ETH-80 database. Since these databases are highly multiclass (31 and 8 classes) relative to the total number of sam-ples, we use the kernel Discriminant Analysis to reduce dimensionality and extract features, in con-junction with a 1-NN classifier. The six different Grassmann kernels are compared: the extended Projection (Lin/LinSc/Aff/Affsc) kernels, the Binet-Cauchy kernel, and the Bhattacharyya kernel. The baseline algorithm (Eucl) is the Linear Discriminant Analysis applied to the original images in the data from which the subspaces are computed.
 Figure 1 summarizes the average recognition/categoriazation rates from 9-and 10-fold cross vali-dation with the Yale Face and ETH-80 databases respectively. The results shows that best rates are achieved from the extended kernels: linear scaled kernel in Yale Face and the affine kernel in ETH-80. However the difference within the extended kernels are small. The performance of the extended kernels remain relatively unaffected by the subspace dimensionality, which is a convenient property in practice since we do not know the true dimensionality a priori. However the Binet-Cauchy and the Bhattacharyya kernels do not perform as well, and degrade fast as the subspace dimension increases. The analysis of the poor performance are given in the thesis [5]. In this paper we analyzed the relationship between probabilistic distances and the geometric Grass-mann kernels, especially the KL distance and the Projection kernel. This analysis help us to under-stand the limitations of the Bhattacharyya kernel for subspace-based problems, and also suggest the extensions of the Projection kernel. With synthetic and real data we demonstrated that the extended kernels can outperform the original Projection kernel, as well as the previously used Bhattacharyya and the Binet-Cauchy kernels for subspace-based classification problems. The relationship between other probabilistic distances and the Grassmann kernels is yet to be fully explored, and we expect to see more results from a follow-up study.
 Figure 1: Comparison of Grassmann kernels for face recognition/ object categorization tasks with kernel discriminant analysis. The extended Projection kernels (Lin/LinSc/Aff/ AffSc) outperform the baseline method (Eucl) and the Binet-Cauchy (BC) and the Bhattacharyya (Bhat) kernels.
