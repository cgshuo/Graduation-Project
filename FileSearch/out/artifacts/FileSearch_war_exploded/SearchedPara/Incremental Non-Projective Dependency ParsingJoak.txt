 Data-driven dependency parsing has been shown to give accurate and efficient parsing for a wide range of languages, such as Japanese (Kudo and Mat-sumoto, 2002), English (Yamada and Matsumoto, 2003), Swedish (Nivre et al., 2004), Chinese (Cheng et al., 2004), and Czech (McDonald et al., 2005). Whereas most of the early approaches were limited to strictly projective dependency structures, where the projection of a syntactic head must be contin-uous, attention has recently shifted to the analysis of non-projective structures, which are required for linguistically adequate representations, especially in languages with free or flexible word order.

The most popular strategy for capturing non-projective structures in data-driven dependency parsing is to apply some kind of post-processing to the output of a strictly projective dependency parser, as in pseudo-projective parsing (Nivre and Nilsson, 2005), corrective modeling (Hall and Nov  X  ak, 2005), or approximate non-projective parsing (McDonald and Pereira, 2006). And it is rare to find parsers that derive non-projective structures directly, the no-table exception being the non-projective spanning tree parser proposed by McDonald et al. (2005).
There are essentially two arguments that have been advanced against using parsing algorithms that derive non-projective dependency structures di-rectly. The first is that the added expressivity com-promises efficiency, since the parsing problem for a grammar that allows arbitrary non-projective depen-dency structures has been shown to be NP complete (Neuhaus and Br  X  oker, 1997). On the other hand, most data-driven approaches do not rely on gram-mars, and with a suitable factorization of depen-dency structures, it is possible to achieve parsing of unrestricted non-projective structures in O ( n 2 ) time, as shown by McDonald et al. (2005).

The second argument against non-projective de-pendency parsing comes from the observation that, even in languages with free or flexible word order, most dependency structures are either projective or very nearly projective. This can be seen by con-sidering data from treebanks, such as the Prague Dependency Treebank of Czech (B  X  ohmov  X  a et al., 2003), the TIGER Treebank of German (Brants et al., 2002), or the Slovene Dependency Treebank (D  X  zeroski et al., 2006), where the overall proportion of non-projective dependencies is only about 2% even though the proportion of sentences that con-tain some non-projective dependency is as high as 25%. This means that an approach that starts by de-riving the best projective approximation of the cor-rect dependency structure is likely to achieve high accuracy, while an approach that instead attempts to search the complete space of non-projective de-pendency structures runs the risk of finding struc-tures that depart too much from the near-projective norm. Again, however, the results of McDonald et al. (2005) suggest that the latter risk is minimized if inductive learning is used to guide the search.
One way of improving efficiency, and potentially also accuracy, in non-projective dependency parsing is to restrict the search to a subclass of  X  X ildly non-projective X  structures. Nivre (2006) defines degrees of non-projectivity in terms of the maximum number of intervening constituents in the projection of a syn-tactic head and shows that limited degrees of non-projectivity give a much better fit with the linguistic data than strict projectivity, but also enables more ef-ficient processing than unrestricted non-projectivity. However, the results presented by Nivre (2006) are all based on oracle parsing, which means that they only provide upper bounds on the accuracy that can be achieved.

In this paper, we investigate to what extent con-straints on non-projective structures can improve accuracy and efficiency in practical parsing, using treebank-induced classifiers to predict the actions of a deterministic incremental parser. The parsing al-gorithm used belongs to the family of algorithms de-scribed by Covington (2001), and the classifiers are trained using support vector machines (SVM) (Vap-nik, 1995). The system is evaluated using treebank data from five languages: Danish, Dutch, German, Portuguese, and Slovene.

The paper is structured as follows. Section 2 defines syntactic representations as labeled depen-dency graphs and introduces the notion of degree used to constrain the search. Section 3 describes the parsing algorithm, including modifications neces-sary to handle degrees of non-projectivity, and sec-tion 4 describes the data-driven prediction of parser actions, using history-based models and SVM clas-sifiers. Section 5 presents the experimental setup, section 6 discusses the experimental results, and sec-tion 7 contains our conclusions. A dependency graph is a labeled directed graph, the nodes of which are indices corresponding to the to-kens of a sentence. Formally: Definition 1 Given a set R of dependency types (arc labels), a dependency graph for a sentence x = ( w 1 , . . . , w n ) is a labeled directed graph G = ( V, E, L ) , where: 1. V = { 0 , 1 , 2 , . . . , n } 2. E  X  V  X  V 3. L : E  X  R The set V of nodes (or vertices ) is the set of non-negative integers up to and including n . This means that every token index i of the sentence is a node ( 1  X  i  X  n ) and that there is a special node 0 , which will always be a root of the dependency graph. The set E of arcs (or edges ) is a set of ordered pairs ( i, j ) , where i and j are nodes. Since arcs are used to represent dependency relations, we will say that i is the head and j is the dependent of the arc ( i, j ) . The function L assigns a dependency type (label) r  X  R to every arc e  X  E . We use the notation i  X  j to mean that there is an arc connecting i and j (i.e., ( i, j )  X  E ); we use the notation i r  X  j if this arc is labeled r (i.e., (( i, j ) , r )  X  L ); and we use the notation i  X   X  j and i  X   X  j for the reflexive and transitive closure of the arc relation E and the corresponding undirected relation, respectively. Definition 2 A dependency graph G is well-formed if and only if: 1. The node 0 is a root, i.e., there is no node i such 2. G is weakly connected, i.e., i  X   X  j for every 3. Every node has at most one head, i.e., if i  X  j AuxZ
Sb
AuxP The well-formedness conditions are independent in that none of them is entailed by any (combination) of the others, but they jointly entail that the graph is a tree rooted at the node 0. By way of example, figure 1 shows a Czech sentence from the Prague Dependency Treebank (B  X  ohmov  X  a et al., 2003) with a well-formed dependency graph according to Defi-nitions 1 and 2.
 The constraints imposed on dependency graphs in Definition 2 are assumed in almost all versions of dependency grammar, especially in computational systems, and are sometimes complemented by a fourth constraint: 4. The graph G is projective, i.e., if i  X  j then Most theoretical formulations of dependency gram-mar regard projectivity as the norm but recognize the need for non-projective representations to cap-ture non-local dependencies (Mel X   X  cuk, 1988; Hud-son, 1990). Finding a way of incorporating a suit-ably restricted notion of non-projectivity into prac-tical parsing systems is therefore an important step towards a more adequate syntactic analysis, as dis-cussed in the introduction of this paper.

In order to distinguish classes of dependency graphs that fall in between arbitrary non-projective and projective, Nivre (2006) introduces a notion of degree of non-projectivity, such that projective graphs have degree 0 while arbitrary non-projective graphs have unbounded degree.
 Definition 3 Let G = ( V, E, L ) be a well-formed dependency graph, let G ( i,j ) be the subgraph of G defined by V ( i,j ) = { i, i +1 , . . . , j  X  1 , j } , and let min( e ) be the smallest and max( e ) the largest ele-ment of an arc e in the linear order &lt; : 1. The degree of an arc e  X  E is the number of 2. The degree of G is the maximum degree of any To exemplify the notion of degree, we note that the dependency graph in figure 1 has degree 1 . The only non-projective arc in the graph is (5 , 1) and G (2 , 4) contains three connected components, each consist-ing of a single root node (2, 3, 4). Since exactly one of these, 3, is not dominated by 5 in G (1 , 5) , the arc (5 , 1) has degree 1.

Nivre (2006) presents an empirical study, based on data from the Prague Dependency Treebank of Czech (B  X  ohmov  X  a et al., 2003) and the Danish De-pendency Treebank (Kromann, 2003), showing that more than 99.5% of all sentences occurring in the two treebanks have a dependency graph with a max-imum degree of 2; about 98% have a maximum de-gree of 1; but only 77% in the Czech data and 85% in the Danish data have degree 0 (which is equivalent to assuming P ROJECTIVITY ). This suggests that lim-ited degrees of non-projectivity may allow a parser to capture a larger class of naturally occurring syn-tactic structures, while still constraining the search to a proper subclass of all possible structures. 1 Covington (2001) describes a parsing strategy for dependency representations that has been known since the 1960s but not presented in the literature. The left-to-right (or incremental) version of this strategy can be formulated in the following way: L
INK ( i , j ) is a nondeterministic operation that adds the arc i  X  j (with some label), adds the arc j  X  i (with some label), or does nothing at all. In this way, the algorithm builds a graph by systematically trying to link every pair of nodes ( i, j ) ( i &lt; j ). We assume that L INK ( i , j ) respects the R OOT and S
INGLE -H EAD constraints and that it does not in-troduce cycles into the graph, i.e., it adds an arc i  X  j only if j 6 = 0 , there is no k 6 = i such that k  X  j , and it is not the case that j  X   X  i . Given these constraints, the graph G given at termination can always be turned into a well-formed dependency graph by adding arcs from the root 0 to any root node in { 1 , . . . , n } .

Assuming that L INK ( i , j ) can be performed in some constant time c , the running time of the al-gorithm is P n i =1 c ( i  X  1) = c ( n 2 2  X  n 2 ) , which in terms of asymptotic complexity is O ( n 2 ) . Checking R
OOT and S INGLE -H EAD in constant time is easy, but in order to prevent cycles we need to be able to find, for any node k , the root of the connected component to which k belongs in the partially built graph. This problem can be solved efficiently us-ing standard techniques for disjoint sets, including path compression and union by rank, which guaran-tee that the necessary checks can be performed in average constant time (Cormen et al., 1990).
In the experiments reported in this paper, we mod-ify the basic algorithm by making the performance of L INK ( i , j ) conditional on the arcs ( i, j ) and ( j, i ) being permissible under different degree constraints: The function P ERMISSIBLE ( i , j , d ) returns true if and only if i  X  j and j  X  i have a degree less than or equal to d given the partially built graph G . Setting d = 0 gives strictly projective parsing, while d =  X  corresponds to unrestricted non-projective parsing. With low values of d , we will reduce the number of calls to L INK ( i, j ) , which will reduce the overall parsing time provided that the time re-quired to compute P ERMISSIBLE ( i, j, d ) is insignif-icant compared to the time needed for L INK ( i, j ) . This is typically the case in data-driven systems, where L INK ( i, j ) requires a call to a trained classi-fier, while P ERMISSIBLE ( i, j, d ) only needs access to the partially built graph G . 2 History-based parsing uses features of the parsing history to predict the next parser action (Black et al., 1992). In the current setup, this involves using fea-tures of the partially built dependency graph G and the input x = ( w 1 , . . . , w n ) to predict the outcome of the nondeterministic L INK ( i, j ) operation. Given that we use a deterministic parsing strategy, this re-duces to a pure classification problem.

Let  X ( i, j, G ) = (  X  1 ,. . . , X  m ) be a feature vec-tor representation of the parser history at the time of performing L INK ( i, j ) . The task of the history-based classifier is then to map  X ( i, j, G ) to one of the following actions: 1. Add the arc i r  X  j (for some r  X  R ). 2. Add the arc j r  X  i (for some r  X  R ). 3. Do nothing.
 Training data for the classifier can be generated by running the parser on a sample of treebank data, us-ing the gold standard dependency graph as an ora-cle to predict L INK ( i, j ) and constructing one train-ing instance ( X ( i, j, G ) , a ) for each performance of L INK ( i, j ) with outcome a .

The features in  X ( i, j, G ) = (  X  1 , . . . ,  X  m ) can be arbitrary features of the input x and the partially built graph G but will in the experiments below be restricted to linguistic attributes of input tokens, in-cluding their dependency types according to G .
The history-based classifier can be trained with any of the available supervised methods for func-tion approximation, but in the experiments below we will rely on SVM, which has previously shown good performance for this kind of task (Kudo and Mat-sumoto, 2002; Yamada and Matsumoto, 2003). The purpose of the experiments is twofold. First, we want to investigate whether allowing non-projective structures to be derived incrementally can improve parsing accuracy compared to a strictly projective baseline. Secondly, we want to examine whether restricting the degree of non-projectivity can im-prove efficiency compared to an unrestricted non-projective baseline. In order to investigate both these issues, we have trained one non-projective parser for each language, allowing arbitrary non-projective structures as found in the treebanks during training, but applying different constraints during parsing: 1. Non-projective ( d =  X  ) 2. Max degree 2 ( d = 2 ) 3. Max degree 1 ( d = 1 ) These three versions of the non-projective parser are compared to a strictly projective parser ( d = 0 ), which uses the same parsing algorithm but only con-siders projective arcs in both training and testing. 3
The experiments are based on treebank data from five languages: the Danish Dependency Treebank (Kromann, 2003), the Alpino Treebank of Dutch (van der Beek et al., 2002), the TIGER Treebank of German (Brants et al., 2002), the Floresta Sint  X  actica of Portuguese (Afonso et al., 2002), and the Slovene Dependency Treebank (D  X  zeroski et al., 2006). 4 The data sets used are the training sets from the CoNLL-X Shared Task on multilingual dependency parsing (Buchholz and Marsi, 2006), with 20% of the data reserved for testing using a pseudo-random split. Ta-ble 1 gives an overview of the five data sets, showing the number of tokens and sentences, the presence of different kinds of linguistic annotation, and the amount of non-projectivity.

The features used in the history-based model for all languages include the following core set of 20 features, where i and j are the tokens about to be linked and the context stack is a stack of root nodes k in G ( i +1 ,j  X  1) , added from right to left (i.e., with the top node being closest to i ): 1. Word form: i , j , j +1 , h ( i ) . 2. Lemma (if available): i . 3. Part-of-speech: i  X  1 , i , j , j +1 , j +2 , k , k  X  1 . 4. Coarse part-of-speech (if available): i , j , k . 5. Morphosyntactic features (if available): i , j . 6. Dependency type: i , j , l ( i ) , l ( j ) , r ( i ) . In the specification of features, we use k and k  X  1 to refer to the two topmost tokens on the context stack, and we use h (  X  ) , l (  X  ) and r (  X  ) to refer to the head , the leftmost dependent and the rightmost dependent of a token  X  in the partially built dependency graph. 5 In addition to the core set of features, the model for each language has been augmented with a small number of additional features, which have proven useful in previous experiments with the same data set. The maximum number of features used is 28 (Danish); the minimum number is 23 (German).
The history-based classifiers have been trained using SVM learning, which combines a maximum margin strategy with the use of kernel functions to map the original feature space to a higher-dimensional space. More specifically, we use LIB-SVM (Chang and Lin, 2001) with a quadratic kernel K ( x i , x j ) = (  X x T i x j + r ) 2 . We use the built-in one-versus-one strategy for multi-class classification and convert symbolic features to numerical features us-ing the standard technique of binarization.

Parsing accuracy is measured by the unlabeled at-tachment score (AS), i.e., the proportion of words that are assigned the correct head (not counting punctuation). Although the parsers do derive labeled dependency graphs, we concentrate on the graph structure here, since this is what is concerned in the distinction between projective and non-projective dependency graphs. Efficiency is evaluated by re-porting the parsing time (PT), i.e., the time required to parse the respective test sets. Since both training sets and test sets vary considerably in size between languages, we are primarily interested in the rela-tive differences for parsers applied to the same lan-guage. Experiments have been performed on a Sun-Blade 2000 with one 1.2GHz UltraSPARC-III pro-cessor and 2GB of memory. Table 2 shows the parsing accuracy of the non-projective parser with different maximum degrees, both the raw attachment scores and the amount of error reduction with respect to the baseline parser. Our first observation is that the non-projective parser invariably achieves higher accuracy than the pro-jective baseline, with differences that are statisti-cally significant across the board (using McNemar X  X  test). The amount of error reduction varies be-tween languages and seems to depend primarily on the frequency of non-projective structures, which is not surprising. Thus, for Dutch and German, the two languages with the highest proportion of non-projective structures, the best error reduction is over 35% and over 20%, respectively. However, there seems to be a sparse data effect in that Slovene, which has the smallest training data set, has the smallest error reduction despite having more non-projective structures than Danish and Portuguese.
Our second observation is that the highest score is always obtained with an unbounded degree of non-projectivity during parsing. This seems to corrobo-rate the results obtained by McDonald et al. (2005) with a different parsing method, showing that the use of inductive learning to guide the search dur-ing parsing eliminates the potentially harmful ef-fect of increasing the size of the search space. Al-though the differences between different degrees of non-projectivity are not statistically significant for the current data sets, 6 the remarkable consistency across languages suggests that they are nevertheless genuine. In either case, however, they must be con-sidered marginal, except possibly for Dutch, which leads to our third and final observation about accu-racy, namely that restricting the maximum degree of non-projectivity to 2 or 1 has a very marginal effect on accuracy and is always significantly better than the projective baseline.

Turning next to efficiency, table 3 shows the pars-ing time for the different parsers across the five lan-guages. Our first observation here is that the pars-ing time can be reduced by restricting the degree of non-projectivity during parsing, thus corroborat-ing the claim that the running time of the history-based classifier dominates the overall parsing time. As expected, the largest reduction is obtained with the strictly projective parser, but here we must also take into account that the training data set is smaller (because of the restriction to projective potential links), which improves the average running time of the history-based classifier in itself. Our second ob-servation is that the amount of reduction in parsing time seems to be roughly related to the amount of non-projectivity, with a reduction of about 50% at a max degree of 1 for the languages where more than 20% of all sentences are non-projective (Dutch, German, Slovene) but significantly smaller for Por-tuguese and especially for Danish. On the whole, however, the reduction in parsing time with limited degrees of non-projectivity is substantial, especially considering the very marginal drop in accuracy.
In order to compare the performance to the state of the art in dependency parsing, we have retrained the non-projective parser on the entire training data set for each language and evaluated it on the final test set from the CoNLL-X shared task (Buchholz and Marsi, 2006). Thus, table 4 shows labeled at-tachment scores, the main evaluation metric used in the shared task, in comparison to the two highest scoring systems from the original evaluation (Mc-Donald et al., 2006; Nivre et al., 2006). The incre-mental non-projective parser has the best reported score for Danish and outperforms at least one of the other two systems for four languages out of five, although most of the differences are probably too small to be statistically significant. But whereas the spanning tree parser of McDonald et al. (2006) and the pseudo-projective parser of Nivre et al. (2006) achieve this performance only with special pre-or post-processing, 7 the approach presented here de-rives a labeled non-projective graph in a single incre-mental process and hence at least has the advantage of simplicity. Moreover, it has better time complex-ity than the approximate second-order spanning tree parsing of McDonald et al. (2006), which has expo-nential complexity in the worst case (although this does not appear to be a problem in practice). In this paper, we have investigated a data-driven ap-proach to dependency parsing that combines a deter-ministic incremental parsing algorithm with history-based SVM classifiers for predicting the next parser action. We have shown that, for languages with a non-negligible proportion of non-projective struc-tures, parsing accuracy can be improved signifi-cantly by allowing non-projective structures to be derived. We have also shown that the parsing time can be reduced substantially, with only a marginal loss in accuracy, by limiting the degree of non-projectivity allowed during parsing. A comparison with results from the CoNLL-X shared task shows that the parsing accuracy is comparable to that of the best available systems, which means that incremen-tal non-projective dependency parsing is a viable al-ternative to approaches based on post-processing of projective approximations.
 The research presented in this paper was partially supported by a grant from the Swedish Research Council. I want to thank Johan Hall and Jens Nils-son for their contributions to MaltParser, which was used to perform the experiments. I am also grateful to three anonymous reviewers for finding important errors in the preliminary version and for suggesting several other improvements for the final version.
