 Graphical models including independence graphs, directed acyclic graphs (DAG) and Bayesian networks have been applied widely to many fields, such as data mining, pattern recognition, artificial intelligence and causal discovery [4, 8, 9, 10]. Graphical models can be used to co pe with uncertainty for a large system with a great number of variables. Structural learning of graphical models from data is an important and difficult problem, and has been discussed by many authors [4, 5, 8, 9, 10, 11]. There are two main kinds of structural learning methods. One is constraint-based learning and the other is score-based learn-ing. Most of structural learning approaches deal with only one database with completely observed data. With the development and popularity of computers, various databases have been built, which may contain different sets of variables and overlap each other. For example, in medical research, a researcher collects data of these variables, another researcher may collect data of other variables, and they have some common variables.

In this paper, we discuss how to learn the structures of independence graphs from multiple databases with different and overlapped variables. In our approach, we first learn a local subgraph from each database separately, and then we com-bine these subgraphs together to construct a global graph over all variables. Several theoretical results are shown for the validity of our algorithm. Our ap-proach can validly discover independence graphs from multiple databases. The advantage of our local discovery approach is that each independence test is per-formed conditionally on a small set of variables rather than on all other variables so that the tests become more powerful and less computational complex. This approach can also utilize the prior knowledge of conditional independencies to reduce the number of variables in each conditional set.

Section 2 gives notation and definitions. In Section 3, we show how to con-struct the independence graph with multiple databases. We give an example in Section 4 to illustrate our approach for recovering an independence graph. Finally Section 5 discusses the advantages and complexity of the proposed algo-rithm. Agraphisapair G =( V, E )where V = { x 1 ,x 2 ,...,x n } is a finite set of vertices and E is a subset V  X  V of distinct vertices, called the set of edges. An edge is an edge between vertices x and y is undirected, denoted by ( x, y ) and depicted by a line in the graph. A graph is undirected if it contains only undirected edges. In this paper, we concentrate only on undirected graphs. For an undirected graph G , vertices x and y are adjacent if there is an undirected edges between x and y . Let ne ( x ) denotes all the vertices that are adjacent with x , called the neighbor An undirected graph is called complete if each pair of vertices are connected by an edge in the graph.

The vertex set V in a graph G is used to denote an n -dimensional vector of random variables. An independence graph, or more precisely a conditional independence graph, for the variable set V is an undirected graph G =( V, E ) in which ( x, y ) /  X  E if and only if x and y are independent conditionally on all other variables, denoted by x y | V \{ x, y } [12].

A hypergraph is a collect ion of vertex sets [2, 3]. Multiple databases C = {
C 1 ,...,C H } are depicted as a hypergraph where a hyperedge C h is an observed variable set C h is treated as a sample from a marginal distribution of the variable sets. Given a collection of databases C , the graphical model with the edge set E =  X  h E h is the saturated gra phical model where E h = C h  X  C h is the edge set of the complete graph over the vertex set C h since there is no information on higher interactions over different databases. For the saturated graphical model, can also be used to depict the prior knowledge of conditional independencies [11]. a hypergraph, as shown in Fig. 1 (a). We can get that D 1 = { 1 , 3 , 4 } , D 2 = { 1 , 3 , 6 } and D 3 = { 4 , 6 } . The saturated graphical model corresponding to C is shown in Fig. 1 (b). From the saturated graphical model, we can see that ( C For multiple databases C = { C 1 ,...,C H } , it should be noted that there is no information on the association among variables that are never observed together, and thus parameters that relate to the association are inestimable without other assumptions. The condition to make our algorithm correct for structural learn-ing from multiple databases C is that C must contain sufficient data such that parameters of the underlying independence graph are estimable. For an inde-pendence graph, its parameters are estimable if, for each clique g i ,thereisa database C h in C which contains g i . Thus multiple databases C have sufficient such that C h contains g i in the underlying independence graph. Every database C h can be seen as a maximum complete undirected graph to depict possible association among variables in C h . We assume that all independencies inferred from multiple databases are true for the underlying independence graph. In this section, we propose an approach for structural learning of independence graphs. In our approach, we first learn a subgraph from each database, and then we combine these subgraphs together to construct a global graph over all variables. Below we give the theoretica l results which ensures the correctness of this approach. By definition of an independence graph, the existence of an edge between x and y can be determined by testing conditional independence x y | V \{ x, y } . Usually all databases are needed to calculate the statistics for testing this independence. In the following theorems, we show that this may not be needed for some edges. First we give a l emma to be used in proofs of theorems. Lemma 1. Properties of conditional independence: 1. ( X Y | Z )  X  ( Y X | Z ) ; 2. ( X YW | Z )  X  ( X Y | Z ) ; 3. ( X YW | Z )  X  ( X Y | ZW ) ; 4. ( X Y | Z )&amp;( X W | ZY )  X  ( X YW | Z ) ; 5. ( X W | ZY )&amp;( X Y | ZW )  X  ( X YW | Z ) .
 Proof. See page 11 of [9] for the proof.
 Theorem 1. Let A , B and C be a partition of all variables in V ,andvariables x and y be contained in A . Suppose that A B | C .Then x y | V \{ x, y } if and only if x y | ( A  X  C ) \{ x, y } .
 Proof. We first prove the sufficiency. Because A B | C , we can get from the third property of Lemma 1 From the sufficient condition, the right hand side of the above formula can be rewritten as Thus we proved the sufficiency.

Next we prove the necessity. Because A B | C , we can get from the third property of Lemma 1 Because x y | ( A \{ x, y } ,B,C ), the right of the above formula is equal to Thus we proved Theorem 1.
 Let A = C h \ D h , which is a vertex set that only appears in the database C h . According to Theorem 1, we can see th at the existence of an edge whose two vertices fall into only one database can be determined validly by using the database C h only.
 Theorem 2. Let A , B and C be a partition of all variables in V ,andvariables x and y be contained in A and C respectively. Suppose A B | C .Then x y | V \ { x, y } if and only if x y | ( A  X  C ) \{ x, y } . Proof. Because A B | C , we can get from the third property of Lemma 1 that and x B | ( y, A \{ x } ,C \{ y } ), we can get from the fourth property of Lemma 1 that x ( y, B ) | ( A \{ x } ,C \{ y } ). From the third property of Lemma 1, we can get x y | ( A \{ x } ,B,C \{ y } ). Thus we proved the sufficiency.
 { x } ,C \{ y } ), we can get from the fifth property of Lemma 1 that x ( y, B ) | ( A \ { x } ,C \{ y } ). From the second property of Lemma 1, we can get x y | ( A \{ x } ,C \ { y } ). Thus we proved Theorem 2.
 According to Theorem 2, the existen ce of an edge whose one of two vertices is contained only by one database can also be determined validly by using the database only.

From the above two Theorems, we know that an edge whose at least one of two vertices is contained only by one database C h can be determined by using the marginal distribution of C h without requirement of the other databases.
Now we consider how to determine an edge ( x, y ) both of whose vertices are contained by at least two databases. Let C xy =  X  { h : x  X  C Theorems 1 and 2, it can be shown that the existence of edge ( x, y )canbe determined by testing whether x and y are independent conditionally on C xy \ { x, y } . But the union set C xy may contain a large number of variables. Below we discuss how to reduce variables from C xy .
 For a database C h , suppose that both x and y are contained in D h ,where D the independence graph obtained by using database C h . From Theorems 1 and 2 we can get that NE h is contained in ne ( D h )of G which is the independence graph for the whole variable set V .Let C xy =  X  { h : x  X  C From the property of multiple databases, we have that  X  { h : x  X  C independent of V \ C xy conditionally on  X  { h : x  X  C x nor y is contained in  X  { h : x  X  C determine the existence of edge ( x, y ) by the marginal distribution of C xy ,which is a subset of C xy .

Now we give the algorithm for structural learning of independence graphical models.
 Algorithm: Construct an independenc e graph from multiple databases 1. Input: Multiple databases C = { C 1 ,...,C H } . 2. Construct a local independence graph G h from database C h separately for 3. Construct the global independence graph G V : 4. Output: the independence graph G V .

According to Theorems 1 and 2, the independence graph constructed by the above algorithm is validly, and the statistical inference is more efficient than the traditional approach in which each edge ( x, y ) is determined by testing x y | V \ { x, y } since the conditional set V \{ x, y } for independence tests is much larger than C h \{ x, y } and C xy \{ x, y } .

Step 3 in the algorithm becomes much simpler if we assume that the condi-tional independence x y | A implies x y | B for all A  X  B . The assumption is similar to the faithfulness assumption for DAGs [10]. Under this assumption, we ensure that edges are deleted validly at Step 2, and thus an edge between x and y should be absent in G V if it is absent in any subgraph G h . In this section, we illustrate our algorithm using the ALARM network in Fig. 2 that is often used to evaluate structural learning algorithms [1, 7, 10]. The ALARM network in Fig. 2 describes associ ations among 37 variables in a medical diagnostic system for patient monitoring. Using the network, some researchers generate continuous data from normal distributions and others generate discrete data from multinomial distributions [7, 10]. Our approach is applicable for both continuous and discrete data. Since the validity of our algorithm can be en-sured by Theorems 1 and 2, the algorithm is illustrated by using conditional independencies from the underlying independence graph in Fig. 2 rather than conditional independence tests from simulated data.
 Suppose that we have three databases as depicted by the hypergraph in Fig. 2. 16 , 19 } . At Step 2, the local independence graphs are obtained separately from the three databases, as shown in Fig. 3 (a), (b) and (c) respectively. From Fig. 3, 25 , 26 } .

At Step 3, we first initialize the edge set E of the global graph G V as the union of all edge sets of G h for h =1 , 2 , 3, as shown in Fig. 4. For any pairs of vari-ables contained in every D h , we must redetermine the e xistence of corresponding C 19 } . Finally we get the global independence graph G is the same as the underlying graph in Fig. 2. There are several obvious advantages of our approach for structural learning. Firstly, independence tests are performed only conditionally on smaller sets con-tained in a database C h or C xy rather than on the full set of all other variables. Thus our algorithm has higher power for statistical tests.

Secondly, the theoretical results proposed in this paper can be applied to scheme design of multiple databases. Without loss of information on structural learning of independence graphs, a joint data set can be replaced by a group of incomplete data set based on the prior knowledge of conditional independencies among variables.

Thirdly, for complexity, our approach tests as many times of conditional in-dependence as the ordinary approaches. However, for the ordinary approaches, each test is performed conditionally on n  X  2 variables. For a large n , an indepen-our approach, these tests are taken conditionally only on smaller sets of variables such that they become more practical. On the other hand, the EM algorithm over all n variables is required for each test in the ordinary approaches, which makes computation more complex. In our approach, only those conditional in-dependence tests for edges falling in separators requires the EM algorithm over a smaller variable set. Thus our approach is less computational complex than the ordinary approaches.
Finally, we discuss the validity of the decomposition approach proposed in this paper. For a given collection of databases C , the decomposition approach can obtain the same independence graph as that obtained by using the ordinary approaches if there were no errors of independence tests. In the decomposition approach, observed data are collapsed into marginal data, and thus indepen-dence tests are more efficient. If each clique g i of the underlying independence graph is contained by some database C h in C , then the joint distribution can be identified from these marginal distributions of observed variables, and thus the decomposition approach is valid for r ecovering the correct structure of the underlying independence graph.
 This research was supported by NSFC, MSRA, NBRP 2003CB715900 and PSFC 20060400365.

