 Factor graphs are a widely used representation for modeling complex dependencies amongst hidden variables in structured prediction problems. There are two common inference problems: learning (setting model parameters) and decoding ( maximum a posteriori (MAP) inference). MAP inference is the problem of finding the most probable setting to the graph X  X  hidden variables conditioned on some observed variables.
 For certain types of graphs, such as chains and trees, exact inference and learning is polynomial time [1, 2, 3]. Unfortunately, many interesting problems require more complicated structure rendering exact inference intractable [4, 5, 6, 7]. In such cases we must rely on approximate techniques; in particular, stochastic methods such as Markov chain Monte Carlo (e.g., Metropolis-Hastings) have been applied to problems such as MAP inference in these graphs [8, 9, 10, 11, 6]. However, for many real-world structured prediction tasks, MCMC (and other local stochastic methods) are likely to struggle as they transition through lower-scoring regions of the configuration space. For example, consider the structured prediction task of clustering where the MAP inference problem is to group data points into equivalence classes according to some model. Assume for a moment that Figure 1: The figure on the left shows the sequence of states along an optimal path beginning at a single-cluster configuration and ending at the MAP configuration (F1 scores for each state are shown). The figure on the right plots the F1 scores along the optimal path to the goal for the case where the MAP clustering has forty instances (twenty per cluster) instead of 5. MCMC must make many downhill jumps to reach the MAP configuration. For example, Figure 1 shows the F1 scores of each state along the optimal path to the MAP clustering (assuming each MCMC jump can reposition one data point at a time). We can see that several consecutive downhill transitions must be realized before model-scores begin to improve.
 Taking into account the above discussion with an emphasis on the delayed feedback nature of the MAP inference problem immediately inspires us to employ reinforcement learning (RL) [12]. RL is a framework for solving the sequential decision making problem with delayed reward. There has been an extensive study of this problem in many areas of machine learning, planning, and robotics. Our approach is to directly learn the parameters of the log-linear factor graph with reinforcement learning during a training phase; MAP inference is performed by executing the policy. Because we develop the reward-structure to assign the most mass to the goal configuration, the parameters of the model can also be interpreted as a regularized version of maximum likelihood that is smoothed over neighboring states in the proposal manifold.
 The rest of this document is organized as follows: in  X  2 we briefly overview background material. In  X  3 we describe the details of our algorithm and discuss a number of ideas for coping with the combinatorial complexity in both state and action spaces. In  X  4.3 we present our empirical results, and finally in  X  6 we conclude and lay out a number of ideas for future work. 2.1 Factor Graphs random variables and factors as nodes. Let X be a set of observed variables and Y be a set of hidden variables. The factor graph expresses the conditional probability of Y = y given X = x discriminatively: Where Z X is an input-dependent normalizing constant ensuring that the distribution sums to one,  X  is the set of factors, and  X  ( x,y i ) are factors over the observed variables x and a set of hidden variables y i that are the neighbors of the factor (we use superscript to denote a set). Factors are to find a setting of the parameters  X  that explains the data. For example, maximum likelihood sets the parameters so that the model X  X  feature expectations matches the data X  X  expectations. 2.2 Reinforcement Learning problems in which an agent interacts with the environment and the objective is to learn a course of actions that optimizes a long-term measure of a delayed reward signal. The most popular realization of RL has been in the context of markov decision processes (MDPs).
 R : S X A X S  X  IR is the reward function, i.e. R ( s,a,s 0 ) is the expected reward when action a is taken in state s and transitions to state s 0 , and P : S X A X S  X  [0 , 1] is the transition probability function, i.e. P a ( s,s 0 ) is the probability of reaching state s 0 if action a is taken in state s . the probability of choosing action a (as the next action) when in state s . Following a policy on an MDP results in an expected discounted reward R  X  t accumulated over the course of the run, where R t = P Given a Q-function ( Q : S  X A  X  IR ) that represents the expected discounted reward for taking action a in state s , the optimal policy  X  ? can be found by locally maximizing Q at each step. Meth-ods of temporal difference (TD) [13] can be used to learn the optimal policy in MDPs, and even have convergence guarantees when the Q-function is in tabular form. However, in practice, tabular representations do not scale to large or continuous domains; a problem that function approximation techniques address [12]. Although the convergence properties of these approaches have not yet been established, the methods have been applied successfully to many problems [14, 15, 16, 17]. When linear functional approximation is used, the state-action pair  X  s,a  X  is represented by a feature vector  X  ( s,a ) and the Q value is represented using a vector of parameters  X  , i.e. Instead of updating the Q values directly, the updates are made to the parameters  X  : notice the similarity between the linear function approximator (Equation 2) and the log-linear factors (right-hand side of Equation 1); namely, the approximator has the same form as the unnormalized log probabilities of the distribution. This enables us to share the parameters  X  from Equation 1. In our RL treatment of learning factor graphs, each state in the system represents a complete as-signment to the hidden variables Y = y . Given a particular state, an action modifies the setting to a subset of the hidden variables; therefore, an action can also be defined as a setting to all the hidden variables Y = y 0 . However, in order to cope with complexity of the action space, we introduce a pro-poser (as in Metropolis-Hastings) B : Y  X  Y that constrains the space by limiting the number of possible actions from each state. The reward function R can be defined as the residual performance improvement when the systems transitions from a current state y to a neighboring state y 0 on the manifold induced by B . In our approach, we use a performance measure based on the ground truth labels (for example, F1, accuracy, or normalized mutual information) as the reward. These rewards ensure that the ground truth configuration is the goal. 3.1 Model reward function R and transition probability function P ; we can now reformulate MAP inference and learning in factor graphs as follows:  X  States: we require the state space to encompass the entire feasible region of the factor graph. Therefore, a natural definition for a state is a complete assignment to the hidden variables Y  X  y and the state space itself is defined as the set S = { y | y  X  DOM ( Y ) } , where DOM( Y ) is the domain space of Y , and we omit the fixed observables x for clarity since only y is required to uniquely identify a state. Note that unless the hidden variables are highly constrained, the feasible regional will be combinatorial in | Y | ; we discuss how to cope with this in the following sections. constrained set of modifications to a subset of the hidden variable assignments. We constrain the action space to a manageable size by using a proposer , or a behavior policy from which actions neighboring states s 0 given a state s . In context of the action space of an MDP, the proposer can be viewed in two ways. First, each possible neighbor state s 0 can be considered the result of an action a , leading to a large number of deterministic actions. Second, it can be regarded as a single highly stochastic action, whose next state s 0 is a sample from the distribution given by the proposer. Both of these views are equivalent; the former view is used for notation simplicity.  X  Reward Function The reward function is designed so that the policy learned through delayed reward reaches the MAP configuration. Rewards are shaped to facilitate efficient learning in this combinatorial space. Let F be some performance metric (for example, for information extraction tasks, it could be F 1 score based on the ground truth labels).
 The reward function used is the residual improvement based on the performance metric F when the system transitions between states s and s 0 : this reward can viewed as learning to minimize the geodesic distance between a current state and the MAP configuration on the proposal manifold. Alternatively, we could define a Euclidean reward as highest, that is s ? = arg max s F ( s ) .  X  Transition Probability Function: Recall that the actions in our system are samples generated from a proposer B , and that each action uniquely identifies a next state in the system. The function 3.2 Efficient Q Value Computations how Q values can be derived from the factor graph (Equation 1) in a manner that enables efficient computations.
 As mentioned previously, a state is an assignment to hidden variables Y = y and an action is another assignment to the hidden variables Y = y 0 (that results from changing the values of a subset of the variables  X  Y  X  Y ). Let  X  y be the setting to those variables in y and  X  y 0 be the new setting to those variables in y 0 . For each assignment, the factor graph can compute the conditional probability y some algebraic manipulation so redundant factors cancel yields: Where the partition function Z X and factors outside the neighborhood of  X  y cancel. In practice an action will modify a small subset of the variables so this computation is extremely efficient. We are from Equation 2. 3.3 Algorithm Now that we have defined MAP inference in a factor graph as an MDP, we can apply a wide variety of RL algorithms to learn the model X  X  parameters. In particular, we build upon Watkin X  X  Q(  X  ) [18, 19], a temporal difference learning algorithm [13]; we augment it with function approximation as described in the previous section. Our RL learning method for factor graphs is shown in Algorithm 1. Algorithm 1 Modified Watkin X  X -Q (  X  ) for Factor Graphs ing Y = y 0 ). Then, during each step of the episode, the maximum action is obtained by repeatedly s with high probability ( 1  X  ), or transitions to a random state instead. We also include eligibility traces that have been modified to handle function approximation [12].
 Once learning has completed on a training set, MAP inference can be evaluated on test data by executing the resulting policy. Because Q -values encode both the reward and value together, policy execution can be performed by choosing the action that maximizes the Q -function at each state. We evaluate our approach by training a factor graph for solving the ontology alignment problem. Ontology alignment is the problem of mapping concepts from one ontology to semantically equiv-alent concepts from another ontology; our treatment of the problem involves learning a first-order probabilistic model that clusters concepts into semantically equivalent sets. For our experiments, we use the the dataset provided by the Illinois Semantic Integration Archive (ISIA) 1 . There are two ontology mappings: one between two course catalog hierarchies, and another between two company profile hierarchies. Each ontology is organized as a taxonomy tree. The course catalog contains 104 concepts and 4360 data records while the company profile domain contains 219 concepts and 23139 records. For our experiments we perform two-fold cross validation with even splits.
 The conditional random field we use to model the problem factors into binary decisions over sets of concepts, where the binary variable is one if all concepts in the set map to each other, and zero otherwise. Each of these hidden variables neighbors a factor that also examines the observed concept combinatorial in the number of concepts in the ontology, and it cannot be full instantiated even for small amounts of data. Therefore, we believe that this is be a good dataset demonstrate the scalability of the approach. 4.1 Features concepts over entire sets. These features are described more detail in our technical report [17]. The pairwise feature extractors are the following:  X  TFIDF cosine similarity between concept-names of c i and c j  X  TFIDF cosine similarity between data-records that instantiate c i and c j  X  TFIDF similarity of the children of c i and c j  X  Lexical features for each string in the concept name  X  True if there is a substring overlap between c i and c j  X  True if both concepts are the same level in the tree The above pairwise features are used as a basis for features over entire sets with the following first order quantifiers and aggregators:  X  X  X  : universal first order logic quantifier  X  X  X  : existential quantifier  X  Average: conditional mean over a cluster  X  Max: maximum value obtained for a cluster  X  Min: minimum value obtained for a cluster  X  Bias: conditional bias, counts number of pairs where a pairwise feature could potentially fire. The real-valued aggregators (min,max,average) are also quantized into bins of various sizes corre-sponding to the number of bins= { 2,4,20,100 } . Note that our first order features must be computed on-the-fly since the model is too large to be grounded in advance.
 4.2 Systems In this section we evaluate the performance of our reinforcement learning approach to MAP infer-ence and compare it current stochastic and greedy alternatives. In particular, we compare piecewise [20], contrastive divergence [21], and SampleRank [22, 11, 23]; these are described in more detail below.  X  Piecewise (GA-PW): the CRF parameters are learned by training independent logistic regression classifiers in a piecewise fashion. Inference is performed by greedy agglomerative clustering.  X  Contrastive Divergence (MH-CD1) with Metropolis-Hastings the system is trained with con-trastive divergence and allowed to wander one step from the ground-truth configuration. Once the parameters are learned, MAP inference is performed using Metropolis-Hastings (with a proposal distribution that modifies a single variable at a time).  X  SampleRank with Metropolis-Hastings (MH-SR): this system is the same as above, but trains the CRF using SampleRank rather than CD1. MAP is performed with Metropolis-Hastings using a proposal distribution that modifies a single variable at a time (same proposer as in MH-CD1).  X  Reinforcement Learning (RL): this is the system introduced in the paper that trains the CRF with delayed reward using Q(  X  ) to learn state-action returns. The actions are derived from the same proposal distribution as used by our Metropolis-Hastings (MH-CD1,MH-SR) systems (modifying a single variable at a time); however it is exhaustively applied to find the maximum action. We set the RL parameters as follows:  X  =0.00001,  X  =0.9,  X  =0.9.  X  GLUE: in order to compare with a well-known system on the this dataset, we choose GLUE [24]. In these experiments contrastive divergence and SampleRank were run for 10,000 samples each , SampleRank were run for more steps to compensate for only observing a single action at each step (recall RL computes the action with the maximum value at each step by observing a large number of samples). 4.3 Results In Table 1 we compare F1 (pairwise-matching) scores of the various systems on the course catalog and company profile datasets. We also compare to the well known system, GLUE [24]. Sam-pleRank (MH-SR), contrastive divergence (MH-CD1) and reinforcement learning (RL) underwent ten training episodes initialized from random configurations; during MAP inference we initialized the systems to the state predicted by greedy agglomerative clustering. Both SampleRank and rein-forcement learning were able to achieve higher scores than greedy; however, reinforcement learning outperformed all systems with an error reduction of 75.3% over contrastive divergence, 28% over SampleRank, 71% over GLUE and 48% over the previous state of the art (greedy agglomerative in-ference on a conditional random field). Reinforcement learning also reduces error over each system on the company profile dataset.
 After observing the improvements obtained by reinforcement learning, we wished to test how robust the method was at recovering from the local optima problem described in the introduction. To gain more insight, we designed a separate experiment to compare Metropolis-Hastings inference (trained with SampleRank) and reinforcement learning more carefully.
 In the second experiment we evaluate our approach under more difficult conditions. In particular, type of local optima discussed in the introduction). We then compare greedy MAP inference on a model whose parameters were learned with RL, to Metropolis-Hastings on a model with parameters learned from SampleRank. More specifically, we generate a set of ten random configurations from the test corpus and run both algorithms, averaging the results over the ten runs. The first two rows of Table 2 summarizes this experiment. Even though reinforcement learning X  X  policy requires it to be greedy with respect to the q-function, we observe that it is able to better escape the random initial configuration than the Metropolis-Hastings method. This is demonstrated in the first rows of Table 2. Although both systems perform worse than under these conditions than those of the previous experiment, reinforcement learning does much better in this situation, indicating that the q-function learned is fairly robust and capable of generalizing to random regions of the space. After observing Metropolis-Hasting X  X  tendency to get stuck in regions of lower score than reinforce-ment learning, we test RL to see if it would fall victim to these same optima. In the last two rows of Table 2 we record the results of re-running both reinforcement learning and Metropolis-Hastings (on the SampleRank model) from the configurations Metropolis-Hastings became stuck. We notice that RL is able to climb out of these optima and achieve a score comparable to our first experiment. MH is also able to progress out of the optima, demonstrating that the stochastic method is capable of escaping optima, but perhaps not as quickly on this particular problem.
 Table 2: Average pairwise-matching precision, recall and F1 over ten random initialization points, and on the output of MH-SR after 10,000 inference steps. The expanded version of this work is our technical report [17], which provides additional detail and motivation. Our approach is similar in spirit to Zhang and Dietterich who propose a reinforcement learning framework for solving combinatorial optimization problems [25]. Similar to this approach, we also rely on generalization techniques in RL in order to directly approximate a policy over un-seen test domains. However, our formulation provides a framework that explicitly targets the MAP problem in large factor graphs and takes advantage of the log-linear representation of such models in order to employ a well studied class of generalization techniques in RL known as linear function approximation. Learning generalizable function approximators has been also studied for efficiently guiding standard search algorithms through experience [26].
 There are a number of approaches for learning parameters that specifically target the problem of MAP inference. For example, the frameworks of LASO [27] and SEARN [28]) formulate MAP in the context of search optimization, where a cost function is learned to score partial (incomplete) rather than explore the solution space itself. As shown in [28] these frameworks have connections to learning policies in reinforcement learning. However, the policies are learned over incomplete con-figurations. In contrast, we formulate parameter learning in factor graphs as an MDP over the space of complete configurations from which a variety of RL methods can be used to set the parameters. Another approach that targets the problem of MAP inference is SampleRank [11, 23], which com-putes atomic gradient updates from jumps in the local search space. This method has the advantage of learning over the space of complete configurations, but ignores the issue of delayed reward. We proposed an approach for solving the MAP inference problem in large factor graphs by using reinforcement learning to train model parameters. RL allows us to evaluate jumps in the configu-ration space based on a value function that optimizes the long term improvement in model scores. Hence  X  unlike most search optimization approaches  X  the system is able to move out of local optima while aiming for the MAP configuration. Benefitting from log linear nature of factor graphs such as CRFs we are also able to employ well studied RL linear function approximation techniques for learning generalizable value functions that are able to provide value estimates on the test set. Our experiments over a real world domain shows impressive error reduction when compared to the other approaches. Future work should investigate additional RL paradigms for training models such as actor-critic.
 This work was supported in part by the CIIR; SRI #27-001338 and ARFL #FA8750-09-C-0181, CIA, NSA and NSF #IIS-0326249; Army #W911NF-07-1-0216 and UPenn subaward #103-548106; and UPenn NSF #IS-0803847. Any opinions, findings and conclusions or recommendations ex-pressed in this material are the authors X  and do not necessarily reflect those of the sponsor.
