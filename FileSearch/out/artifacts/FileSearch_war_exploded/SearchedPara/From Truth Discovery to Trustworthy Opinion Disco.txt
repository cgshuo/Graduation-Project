
In this era of information explosion, conflicts are often en-countered when information is provided by multiple sources. Traditional truth discovery task aims to identify the truth  X  the most trustworthy information, from conflicting sources in different scenarios. In this kind of tasks, truth is regarded as a fixed value or a set of fixed values. However, in a num-ber of real-world cases, objective truth existence cannot be ensured and we can only identify single or multiple reliable facts from opinions. Different from traditional truth dis-covery task, we address this uncertainty and introduce the concept of trustworthy opinion of an entity, treat it as a random variable , and use its distribution to describe consis-tency or controversy, which is particularly difficult for data which can be numerically measured, i.e. quantitative infor-mation. In this study, we focus on the quantitative opinion, propose an uncertainty-aware approach called Kernel Den-sity Estimation from Multiple Sources ( KDEm ) to estimate its probability distribution, and summarize trustworthy in-formation based on this distribution. Experiments indicate that KDEm not only has outstanding performance on the classical numeric truth discovery task, but also shows good performance on multi-modality detection and anomaly de-tection in the uncertain-opinion setting.

Truth Discovery; Source Reliability; Kernel Density Esti-mation
In this era of information explosion, numerous claims about the same object can be collected from multiple sources. Ex-amples include city weather information found through dif-ferent websites, product rating scores collected from dif-Figure 1: General Workflow: from Truth Discovery to Trustworthy Opinion Discovery . ferent customers, and gun control comments provided by different political parties. However, these claims are usu-ally not consistent and conflicts may appear from different sources. Therefore, how to integrate and summarize con-flicting claims and to find out trustworthy information from multiple sources becomes a challenge.
 Truth Discovery . To solve this problem, a series of truth discovery models were developed, where the concept of truth is implicated as a fact or a set of facts which can be con-sistently agreed . A straightforward approach to solve this problem for categorical data is to take the majority as the truth. For numeric data, mean or median can be regarded as the truth. These straightforward methods regard differ-ent sources as equally reliable, which may fail in scenarios where data are not clean enough and inputs are contam-inated by unreliable sources, such as out-of-date websites, faulty devices and spam users. Therefore, several methods have been proposed to overcome this weakness by estimat-ing source reliability and trustworthy information simulta-neously [4, 5, 9, 10, 13, 15 X 20, 24 X 27].
 Truth or Trustworthy Opinion? We notice that be-cause of the objectivity of truth , the output for an entity from most existing truth discovery models is a fixed value while other pieces of information are discarded. However, the objective truth may not be found or the existence of it cannot be ensured for a number of cases. For example, the Table 1: Example 1: A toy example for trustworthy opinion discovery.
 Figure 2: Probability density estimation for Entity 2 in Example 1. exact decline time for Maya civilization remains a mystery and the number for Apple Watch sales is kept secret to the public. For such category of problems, answers of multiple versions from multiple sources stay active, which greatly in-validate the power of traditional truth discovery approaches. In these cases, we can only summarize reliable facts from opinion claims provided by multiple sources. Some of these entities may have only one dominant fact while others may have multiple reliable representative opinion instances. We can provide several real-world scenarios as follows. Since truth can only be represented by a fixed value or a set of fixed values, to model all above scenarios, we need to replace the concept of truth by the concept of trustworthy opinion ( opinion as shorthand) of an entity. To preserve the uncertainty of opinion, we will regard the opinion as a random variable and find its distribution to describe the consistency or the controversy.
 Uncertainty of Quantitative Opinion . Most truth dis-covery models designed for categorical data can provide a trustworthiness score to each claim and assign the one with the largest score to be the truth of this entity. This score in-deed can be regarded as a reflection of probability for a claim of being chosen as truth. Therefore, it could be straight-forward to extend these methods to model the distribution of categorical opinion. However, we notice that there are numerous cases where data are numeric or can be quantita-tively measured. It is nontrivial to model this kind of quan-titative information in an uncertainty-aware way. In existing numeric truth discovery models, it is believed that the truth is a single value and the uni-modal distribution for the truth is assumed or implicated. For entities of which trustworthy opinions are controversial, this uni-modal assumption may cause a loss of valuable information.
 When we apply traditional numeric truth discovery models on Example 1, due to the uni-modal implication, the truth estimation for Entity 2 may shrink to a value between two modes  X 3 X  and  X -3 X  and source reliability cannot be estimated appropriately. Even if we can specify the multi-modality of the data, it is difficult to identify the number of modes so that parametric approaches can hardly be applied to solve this problem.

To solve the problem of trustworthy quantitative opinion discovery from multiple sources, we need to overcome sev-eral challenges as follows. Firstly, how can we preserve the uncertainty of opinion and model the source reliability si-multaneously? Secondly, if the reliable underlying opinion distribution is obtained, how can we find the truth from this if we know truth exists for an entity? Also, how can we summarize representative quantitative opinion instances if we are not sure about the truth existence?
In this study, we propose an uncertainty-aware method to summarize trustworthy quantitative information from mul-tiple sources. The general workflow is described as follows.  X  Firstly, we estimate opinion distributions of entities which are represented as p robability d ensity f unctions ( pdf ).
Specifically, we introduce a nonparametric model of Ker-nel Density Estimation from Multiple Sources ( KDEm ), which is also the core algorithm of this study.  X  Secondly, if truth exists, we estimate the truth from ob-tained opinion distribution; if truth existence cannot be ensured, then we report representative opinion instance(s) based on estimated opinion distribution.
 Figure 1 describes the workflow which is applied on afore-mentioned Example 1.

The philosophy of KDEm is similar to the standard Ker-nel Density Estimation ( KDE ) [14]. To model the shape of a probability density function ( pdf ), a straightforward ap-proach is drawing histogram, which is usually not smooth enough for numeric data. However, by applying kernel tech-nique, we can add continuity over bins and obtain a smooth pdf estimation  X  kernel density estimation ( KDE ). Using kernel, we can transform each claim from a real value to a single component function. For each entity, standard KDE is to find a function which is similar to all the component functions. Then the multi-modality of opinion distribution can be preserved through this technique. Below is an exam-ple illustrating this idea.
 Table 2: Truth Discovery v.s. Trustworthy Opinion Discovery.
 We also notice that Source 5 consistently contaminates the data but in KDE model, component functions are equally weighted. Thus in this study, we believe that reliable source provides trustworthy claims. Then our KDEm can be re-garded as an optimization framework to find a target func-tion which can minimize the weighted difference between the target function and each single component function, where the weight reflects corresponding source reliability. Here we illustrate how the proposed KDEm is able to capture source reliability.
 Once the reliable pdf of the quantitative opinion is obtained from KDEm , we can cluster claims based on this function. Within each cluster, we can regard the mode or the claim with the largest pdf value as a representative candidate. If we know truth exists for the entity, the most trustwor-thy candidate will be reported as the truth and others will be treated as outliers. Through this approach, KDEm is robust to outliers and can naturally detect outliers. Thus different from other numeric truth discovery models, no ad-ditional outlier detection procedure is needed for KDEm . If the truth existence cannot be ensured, by setting a con-fidence threshold, we can report single or multiple repre-sentative values as trustworthy opinion instances, identify uni-modality/multi-modality and detect anomaly observa-tions.
 Contributions of this Study . The classical truth dis-covery task and our trustworthy opinion discovery task are compared in Table 2. Generally, these two kinds of prob-lems take the same input and both involve source reliabil-ity. However, their output formats are different and ideally an trustworthy opinion discovery model can be compatible with classical truth discovery task when truth existence can be ensured. Now we conclude contributions of this study as follows.  X  Different from previous truth discovery models, we raise a new but closely related problem  X  trustworthy opinion discovery . We replace the concept of truth by the concept of trustworthy opinion , model the uncertainty of quantita-tive opinion and regard the opinion as a random variable;  X  A nonparametric approach KDEm is proposed to esti-mate opinion distribution and source reliability score si-multaneously, which can model different shapes of density functions and perceive multiple modes;  X  KDEm is compatible with traditional numeric truth dis-covery task, and could be significantly robust to outliers;  X  Based on the opinion distribution estimation from KDEm , we can summarize one or more representative values, dis-tinguish controversial entities from consistent entities (uni-modal/multi-modal detection) and identify abnormal claims (anomaly detection).
 The rest of the paper is organized as follows. We illustrate definitions and problem formulation in Section 2. Section 3 describes our model for this trustworthy quantitative opin-ion discovery task. Section 4 presents our experimental re-sults. We then introduce related works in Section 5 and provide conclusions and future directions in Section 6.
In this section, we formally define the trustworthy opinion discovery task. We first define some basic terms:
Definition 2.1.  X  An entity is an object of interest.  X  A claim is a value provided by a source for an entity.  X  A trustworthy opinion is a random variable whose dis-tribution describes the trustworthy information of an en-tity.  X  A truth is a fixed value regarding an entity which can be consistently agreed. If truth exists for an entity, it can be distinguished based on the distribution of trustworthy opinion.  X  The representative value (s) of an opinion could be one or more significant trustworthy values summarized based on the opinion distribution.  X  The confidence of a representative value is a score that measures the significance level of the representative value of an opinion. Higher confidence indicates this rep-resentative value is more trustworthy and vice versa.  X  A source reliability score describes the possibility of a source providing trustworthy claims. Higher source relia-bility score indicates that the source is more reliable and vice versa.
 Notice that in this study, we only discuss the quantitative opinion with single value setting, which means the trustwor-thy opinion is a numeric random variable. Then we define the trustworthy opinion discovery task as follows: Definition 2.2. (Trustworthy Opinion Discovery) For a set of entities N of interest, claims are collected from a set of sources S . The uncertainty-aware trustworthy opinion discovery task is to estimate the probability density function of the trustworthy opinion of each entity, and identify the reliability level of each source simultaneously.
 To better understand the estimated opinion distribution, we can summarize the representative values and associated con-fidence scores based on the estimated probability density function of the opinion. Details about this procedure will be introduced in Section 3.2.

All the notations used in this study has been summarized in Table 3.
Generally, the method for uncertainty-aware quantitative trustworthy information summarization can be divided into two steps: 1) estimating the density function of the opinion of each entity; and 2) summarizing the trustworthy informa-tion based on estimated opinion distribution.
In this section, we first introduce the intuition and a den-sity estimation method without distinguishing sources. Then we introduce our model and the algorithm.
Suppose the claim set for the i -th entity is denoted by { x ij  X  R d ,j  X  S i } . For the traditional truth discovery task, a straightforward estimation of the truth is the sam-ple mean. By introducing the concept of source reliability, the format of weighted sample mean is applied in several existing numeric truth discovery methods [9, 10]. Here the weights correspond to source reliability scores.

As discussed before, in our uncertain-opinion setting, to model the uncertainty of opinion, we need to map truths and claims from real values/vectors to functions. Therefore, we define this mapping for the i -th entity as where K h i is a translation invariant, symmetric, positive semi-definite kernel function with bandwidth h i ( h i for the i -th entity. K h i needs to satisfy K h i (  X  , x )  X  0 and R
K h i ( x 0 , x ) d x 0 = 1, so that it can be ensured as a probabil-ity density. A typical kernel example is Gaussian kernel : which is used in all the experiments in this study. If Gaus-sian kernel is applied, we notice that the function transfor-Gaussian distribution.

By applying this kind of mapping, we have following analo-gies of previous sample mean and weighted sample mean: where  X  i ( x ij ) = K h i (  X  , x ij ) and P j  X  X  mean function in (3) can be written as which is the standard Kernel Density Estimation ( KDE ) [14] of the opinion t i . By considering source trustworthiness, we have the extended weighted sample mean function in (4). The major task in our KDEm model is to find the specific pdf estimation in this format.

In preparation for subsequent analysis, we need to look at the kernel technique in detail and define inner product, norm and distance for this function space. Each positive semi-definite kernel K h i is associated with a reproducing kernel Hilbert space (RKHS) H i [1]. For x  X  R d , we have  X  ( x ) = K h i (  X  , x )  X  H i , 1 m Inner Product. Based on the reproducing property, for g  X  X  i , x  X  R d , we have the definition of inner product [1] Specially, by taking g = K h i (  X  , x 0 ) =  X  i ( x 0 ), we have Norm and Distance. Then we have the definition of the norm k X k : and the definition of distance between two functions f,g  X  H :
We now define our model  X  Kernel Density Estimation from Multiple Sources ( KDEm ), by introducing the source weight and minimizing the loss on different entities together.
Particularly, we need to find a set of functions f i  X  H i = 1 ,...,n and a set of numbers c j  X  R + , j = 1 ,...,m , which can minimize the total loss function
J ( f 1 ,...,f n ; c 1 ,...,c m ) = where m i is the number of provided claims for the i -th entity, and c 1 ,...,c m satisfy where n j is the number of claims provided by S j . Suppose  X  f i is the output for f i from this framework. Then is defined as the density estimation for t i , the trustworthy opinion of the i -th entity ( i = 1 ,...,n ).

In (10), c j reflects the trustworthiness level of source S and k  X  i ( x ij )  X  f i k H i measures the distance between the opinion density f i and  X  i ( x ij ), the function transformation of the claim x ij provided by S j . If S j is reliable, it will give large penalty to the distance and vice versa. We use the con-straint (11) to ensure the number of solutions for c 1 ,...,c is finite and this optimization problem is convex if f 1 ,...,f are given. In (11), n j is used to model the involvement level of source S j .

To minimize the total loss function (10) with constraint (11), we further convert the problem into an optimization problem without constraint. That is to find a set of func-tions f i  X  H i for i = 1 ,...,n , a set of numbers c j  X  j = 1 ,...,m , and a real number  X  to minimize the new loss function For R d and the function F : R d  X  R , the Gateaux differen-tials of F at x  X  R d with incremental h  X  R d is dF ( x ; h ) = lim if the limit exists for all h  X  R d . Then a necessary condition for F to achieve a minimum at x 0 is dF ( x 0 ; h ) = 0 for  X  h  X  R d . We thus have the following lemma:
Lemma 3.0.1. For  X  i  X  { 1 ,...,n } , given { c 1 ,...,c R + } , { f j  X  H j | j = 1 ,...,n,j 6 = i } and  X   X  R , the Gateaux differential of Q at f i  X  X  i with incremental h  X  X  i can be given by where V i ( f i ) = 2 m We can prove this lemma by applying similar technique in [7]. Given c 1 ,...,c m  X  R + , a necessary condition for f  X  f it, we have the following theorem for f i  X  X  i :
Theorem 3.1. Suppose c 1 ,...,c m  X  R + are fixed, the es-timation for f i  X  X  i , i = 1 ,...,n can be given by a weighted kernel density estimation where w ij = c j / ( P Notice that if sources are equally reliable, we have c 1 = ... = c m and the estimated pdf from (15) is the same output from standard KDE .

If f i  X  H i ,  X  i = 1 ,..,n are fixed, by solving the equations 1 ,...,m , we have the following theorem for c 1 ,...,c m
Theorem 3.2. Suppose f i  X  X  i ,i = 1 ,...,n are fixed, the objective problem Q is a convex optimization problem. The optimal solution for c j  X  R + , j = 1 ,...,m is Therefore, we can apply a block coordinate descent [2] iter-ative method, which can keep reducing the total loss func-tion (10), to obtain the estimated densities  X  f i ,i = 1 ,...,n and source weight scores c j ,j = 1 ,...,m . This method is concluded as Algorithm 1.
 Algorithm 1 KDEm Algorithm
The general principle of Algorithm 1 is that we start with the opinion density functions obtained from standard KDE and then iteratively update the opinion distributions and the source reliability scores. If obtained opinion densities are closer to the real trustworthy opinion distributions, then preciser source reliability scores can be obtained based on (16). On the other hand, if the updated source reliability scores are more accurate, then we can obtain preciser trust-worthy opinion densities based on (15). Therefore, these two updating procedures can mutually enhance each other.
Specifically in Algorithm 1, since where  X  ij is the shorthand of  X  i ( x ij ), we have In our model, h i can be decided either based the data or based on prior knowledge. Details about bandwidth selec-tion will be introduced in the experiment part. Here we show how the algorithm works on the aforementioned example. Figure 3: Source reliability score c j in each iteration for Example 1.
In each iteration, for each entity, the most time consum-ing part is to compute k  X  i ( x ij )  X   X  f ( k +1) i O ( m 2 i ) time, where m i is the number of claims for the i -th and O ( k P n i =1 m 2 i ) for the whole KDEm model, where k is the number of iterations ( k &lt; 10 in our experiments).
In some real cases, although data are numeric and the number of claims is significantly large, the possible values are limited. For example, the values of rating scores are usually integers from 1 X 5 or from 1 X 10. In such cases, for each entity, we can easily map these claims to corresponding values. Then we can compute the kernel basis K ( x ij , x ues. The time cost for each iteration thus becomes P n i =1 where v i is the number of claimed values for the i -th entity.
Once the opinion density estimation  X  f kdem i for each en-tity N i is obtained, we can use DENCLUE 2.0 [6] to cluster claims { x ij ,j  X  X  i } and calculate the center of each cluster based on the opinion distribution. Then from these clusters, we can summarize the representative values and correspond-ing confidence values based on different user preferences. Clustering Claims. DENCLUE 2.0 [6], a hill-climbing procedure which assigns each claim to its nearest mode based on the density function, is applied in this part. Specifically, taking Gaussian kernel as example, the gradient of  X  f kdem is given by By setting it to zero, we obtain an update rule: For each entity N i , the procedure starts at each claim x and iteratively update it based on (18) until convergence. For claims which converge to the same mode  X  x ik , we cluster them together. The cluster is denoted as C ik and the confi-dence of this cluster is defined as c ik = P j : x Summarizing Trustworthy Information. We first sum-marize the representative candidate value within each clus-ter. Then we screen these candidates based on certain cri-teria and report representative values of the opinion based on different user preferences. Here we introduce two sets of user preferences regarding these two steps respectively as follows.  X   X  X iscrete X  vs.  X  X ontinuous X . Although our model is designed for numeric data, in real cases, e.g.,  X  X he num-ber of Solar System planets X , numeric claims may share discrete property as well and users may believe that a rep-resentative value should be from provided claims. In this case, for each entity N i , within each cluster C ik , the claim with the largest density value is regarded as a represen-tative candidate (  X  X iscrete X  ):
However, if users believe that a representative candidate may not be claimed or observed by any sources, then the associated mode  X  x ik can be regarded as a representative candidate (  X  X ontinuous X  ):  X   X  X ingle X  vs.  X  X ultiple X . If users believe truth exists for an entity, we only report the candidate with largest associated cluster confidence as the truth. Thus the set of reported single truth is (  X  X ingle X  )
However, as we discussed before, if the truth existence cannot be ensured, then single or multiple representative values of opinion may be reported. If we are given a threshold thr  X  0, then we only keep those candidates whose confidences are larger than thr and re-normalize their confidence scores. In this case, the set of reported representative values of opinion is (  X  X ultiple X  )
In the above two scenarios, we mark those claims within deleted candidates X  associated clusters as outliers or anomaly observations.
In this section, we test our proposed model KDEm on several synthetic datasets and real world applications 1 These experiments can be categorized as two kinds of tasks: 1. Traditional truth discovery from contaminated data (sin-gle truth existence can be ensured) 2. Multi-modality detection and anomaly detection (truth existence cannot be ensured).
As discussed before, KDEm is compatible with tradi-tional single truth discovery task and can be more robust to outliers compared with traditional methods. Therefore, in this section, we conduct several sets of experiments to verify the capability and superiority of KDEm regarding this task.
 Datasets. A set of synthetic datasets Synthetic(unimodal) and a real world dataset Population(outlier) [16] are used for this task. Table 4: Basic statistics of Population(outlier) and Tripadvisor datasets and time cost from KDEm on these datasets. For both of them, we normalize the original claims { x ij by its mean (  X  x i = P x ij /m i ) and standard deviation ( sd p P k x ij  X   X  x i k 2 /m i ). Then we use the normalized z-score methods. When we obtain the output, we use the denor-malized truths ( sd i  X  t i +  X  x i ) for evaluation. Performance Measures. For this task, we assume that truth existence can be ensured. Thus for each entity we have only one real truth t  X  i and one estimated value User preference for this kind of experiments should be  X  X is-crete X + X  X ingle X  or  X  X ontinuous X + X  X ingle X  and we try both in our experiments. We can use the Mean Absolute Error (MAE) and Rooted Mean Squared Error (RMSE) to mea-sure the performance of models, which are defined as Smaller MAE or RMSE indicates better performance.
 Baselines. In addition to our model KDEm , we con-duct standard kernel density estimation ( KDE ) [14] and robust kernel density estimation ( RKDE ) with Hampel X  X  Figure 4: Results of experiments on synthetic uni-modal datasets Synthetic(uni) . loss function [7] as two baselines. RKDE is a state-of-art M-estimation based kernel density estimation method which is more robust with outliers than standard KDE . For KDE , RKDE and KDEm , Gaussian kernel is applied and h i is set to be the Median Absolute Deviation (MAD) of data X In practice, we will add a smooth item 10  X  10 sd i to this value and h i can be given by the modified MAD, which indicates Then we have MAD  X  i = 0 iff { x ij } j  X  X  i are all the same. For these three models, we conduct experiments based on both  X  X iscrete X + X  X ingle X  preference ( KDEm d , KDE d , RKDE d ) and  X  X ontinuous X + X  X ingle X  preference ( KDEm c , KDE c , RKDE d ). Since truth existence can be ensured, we could apply several state-of-art truth discovery models on these datasets. Particularly, the following models are ap-plied as additional baselines on both Synthetic(unimodal) and Population(outlier) : Mean , Median , TruthFinder [23], AccuSim [3], GTM [24], CRH [10] and CATD [9]. Details about these methods will be introduced in Section 5. For Population(outlier) , in addtion to these numeric truth discovery models, we add another baseline Voting where data are regarded as categorical and the majority value is assigned as the truth.
 Results. Results of experiments on Synthetic(unimodal) are showed in Figure 4. From Figure 4, we can conclude that KDEm d and KDEm c generally outperform other base-lines based on MAE and RMSE. We notice that KDEm d and KDEm c always have better performance than KDE d , RKDE d and KDE c , RKDE c , which indicate that source quality is important in our uncertain-opinion assumption. We notice that results from traditional numeric truth dis-covery models GTM , CRH , and CATD are not very good while results from TruthFinder and AccuSim are better, which are originally designed for categorical data and ex-tended to handle numeric claims. One possible reason could be for TruthFinder and AccuSim , claims are regarded as separated facts so that the effect of outlier can be alleviated if no more trustworthy claim supports it. However, in other numeric truth discovery models, truth is regarded as a fixed value. Since real-value based distance is usually sensitive to extreme values, additional outlier detection is always needed for those methods. However, if truth is regarded as a ran-dom variable and its density function is estimated by kernel Table 5: Results of experiments on the Popula-tion(outlier) dataset. methods, the effect of those extreme values can be weaken since we are only interested in the dominant mode. Thus KDEm , KDE and RKDE are robust to outliers compared with traditional numeric truth discovery models.

Results of experiments on Population(outlier) are showed in Table 5. Average time cost of each iteration in our KDEm model on this dataset is reported in Table 4. Similar to ex-periments on Synthetic(unimodal) , KDEm has the best performance and the performance of KDE can be improved by considering source quality. Also, traditional numeric models cannot estimate the truth precisely since they are too sensitive to outliers but results from TruthFinder and AccuSim are relatively good.
A major feature of our KDEm model is that it can detect the controversy of the opinion distribution through multi-modality detection. For each entity, the number of reported representative values may indicate the number of modals of the opinion distribution. If this number is larger than one, the opinion of this entity may be controversial. More-over, outliers can be naturally detected based on the esti-mated opinion distribution. Thus we can apply KDEm for anomaly detection. In this section, we conduct experiments on a set of synthetic datasets to verify the capability and su-periority of our KDEm regarding multi-modality detection and anomaly detection on data from multiple sources. In addition, we provide a real world application, review rating summarization, to discover the controversy and consistency of users X  feedback regarding products.
 Datasets. A set of synthetic datasets Synthetic(mix) are used to verify the capability and superiority of KDEm and a set of real world datasets Tripadvisor [21, 22] are used for the users X  rating summarization. Performance Measures. For each entity N i in this kind of experiments, we may have multiple representative values of the opinion. Thus user X  X  preference for this task should be  X  X iscrete X + X  X ultiple X  or  X  X ontinuous X + X  X ultiple X . Notice that for multi-modality detection and anomaly detection, results based on these two kinds of preferences are the same because this task is only related to the clustering proce-dure. Since we only have groundtruth for Synthetic(mix) , we can only measure the performance on Synthetic(mix) . For Tripadvisor , we provide description analysis instead.
For experiments on Synthetic(mix) , if thr is a fixed pa-rameter, we have For multi-modality detection, suppose M is the number of modals we are interested in, K i is the number of represen-tative values reported from the model and K  X  i is the true number of representative values for the i -th entity. Then Similarly, for anomaly detection, suppose Then we have If we set the parameter thr to different values from 0 to 1, we can obtain a set of values { FPR k , TPR k , k = 1 , 2 ,...,k which are sorted based on FPR k . Here we arbitrarily set FPR 0 = TPR 0 = 0 and FPR k  X  +1 = TPR k  X  +1 = 1 and an ROC curve can be obtained. Then we use the area under the ROC curve (AUC) to evaluate the performance: Baseline. Similarly, we only introduce the baseline meth-ods for experiments on Synthetic(mix) . For Synthetic(mix) , single truth existence cannot be ensured since a half of enti-ties are bi-modal. Therefore, in addition to KDEm , we only apply KDE and RKDE on this kind of datasets and com-pare their multi-modality detection and anomaly detection Figure 5: Results of experiments on synthetic mixed multi-modal datasets Synthetic(mix) .
 Table 6: Number of detected multimodal entities in Tripadvisor datasets. capabilities. Gaussian kernel is applied for these methods and h i = MAD  X  i for all entities in Sythetic(mix) . Results. Results of experiments on the synthetic mixed multi-modal datasets Synthetic(mix) are showed in Figure 5. Based on Figure 5, for uni-modal, bi-modal, and anomaly detection, our model KDEm always has better performance than KDE and RKDE based on AUC. We also notice that RKDE has difficulty in distinguishing multi-modality and anomaly observations in this set of datasets. A possible reason could be that it tends to predict minority opinion instances as outliers when the number of claims is limited.
For Tripadvisor , since we don X  X  have groundtruth, we only apply KDEm to estimate the trustworthy rating dis-tribution and reliability scores of sources and use description analysis to evaluate the results. Since the values of claims for Tripadvisor can only be selected from { 1 , 2 , 3 , 4 , 5 } , we set a fixed bandwith h i = 0 . 8 and thr = 0 . 2 for all the entities. Since the rating distributions of some entities in Tripadvi-sor may be multi-modal and the representative values need to be continuous, the user preference for KDEm on these datasets should be  X  X ontinuous X + X  X ultiple X .

Average time costs of each iteration in KDEm on this set of datasets are reported in Table 4. The numbers of detected multi-modal entities in Tripadvisor are displayed in Table 6. From Table 6, we notice that the number of detected multi-modal entities in Tripadvisor(location) is much smaller while the number of detected multi-modal entities in Tripadvisor(business service) is larger than others. This indicates that users X  opinions tend to agree on the location of a hotel while their feedbacks are diverse re-garding the hotel service. In Figure 6, we provide histograms and estimated truth densities of one of uni-modal entities, one of bi-modal entities and one of tri-modal entities from Tripadvisor(location) .

We can obtain eight sets of source reliability scores and eight sets of number of predicted modals regarding differ-ent aspects respectively. For these two kinds of measures, the correlations between each pair of these eight datasets are calculated and displayed in Figure 7. From this figure, we notice that the correlations of source reliability scores be-tween each pair of aspects are relatively strong while those of rating consistency are much weaker, which means source re-Figure 6: Histograms for examples of detected uni-modal, bi-modal and tri-modal entity examples in the Tripadvisor(location) dataset. Figure 7: Pairwise correlation of source reliability scores and predicted numbers of modals for the Tri-padvisor datasets. liability tends to be consistent among different aspects while the consistency of claims tends to be independent of aspects.
The major technique in this study is inspired by ker-nel density estimation. Standard kernel density estimation ( KDE ) [14] is a non-parametric approach to estimate den-sity function of a random variable. Since standard KDE may be sensitive to outliers, robust kernel density estimation ( RKDE ) [7], which is based on the idea of M-estimation, is proposed to overcome this limitation. However, the weight of each component function in RKDE is estimated based on a single entity rather than all the provided entities. There-fore, RKDE cannot estimate source reliability scores as pre-cise as our KDEm model does.

Besides, various truth discovery models have been pro-posed to handle different scenarios [3 X 5, 9, 10, 13, 15 X 20, 23, 24, 26, 27] and these methods are summarized in a recent survey [12]. TruthFinder [23] is a Bayesian based iterative approach to estimate the truth and source reliability. The source consistency assumption in TruthFinder has been broadly applied in following-up studies. Then source depen-dency is considered in [3] and another model AccuCopy is proposed to solve this problem. As most truth discov-ery models, TruthFinder and AccuCopy are designed for categorical data but they both can be extended to handle nu-meric data by applying a similarity measure between claims. The extended version of AccuCopy is called AccuSim . In addition, particularly for the numeric data, a Bayesian framework GTM [25] is proposed to infer the real-valued truth and source reliability level. TBP [13] can be regarded as an extension of GTM to handle different difficulty lev-els of questions and to eliminate source bias. In [10], an optimization framework CRH can be applied on hetero-geneous data, where categorical and numeric data can be modeled together. It is noticed that most sources provide a few claims while only limited sources provide a number of claims. Thus in [9], this long-tail phenomenon is studied and a model CATD is proposed, in which the confidence interval of the source reliability is adopted to tackle this problem.
LTM [24] is a probabilistic graphical model where multi-ple values of truth are allowed. Notice that our uncertain-opinion assumption is different from this multiple truth as-sumption. In LTM , a reliable claim needs to include correct values and exclude wrong values as often as possible. How-ever, in our study, trustworthy opinion is a random variable and multiple representative values can be summarized. A reliable claim can contain either one of these values. Funda-mentally we do not estimate the  X  X ecall X  of a source.
Apart from the truth discovery, some existing studies fo-cused on the problem of statement truthfulness discovery. T-verifyer [11] is proposed to verify the truthfulness of fact statements. However, rather than finding out the truth from different claims, it determines whether a given statement is true by means of submitting the it to search engines.
Furthermore, this paper is much different from traditional opinion extraction and summarization task. Traditional opin-ion extraction and summarization is to extract informative words, summarize sentiments and associated degrees from given documents [8], where documents are treated indepen-dently and equally. In contrast, this paper focuses on iden-tifying the trustworthiness of opinion, which is based on extracted informative opinion claims instead of raw docu-ments. Specifically, our task is to find reliable opinion dis-tribution from claims provided by multiple sources.
In this study, an uncertainty-aware model  X  KDEm , is introduced to estimate the probability density function of the trustworthy opinion from multiple sources. Based on the estimated distribution, representative opinion instances can be summarized as well. Experiments on synthetic and real-world datasets not only indicate that KDEm is more robust to extreme values claimed in multiple sources than traditional truth discovery models if the single truth exis-tence can be ensured, but also shows that KDEm is good at detecting multi-modality and anomaly observations.
In the future, more loss functions and kernels can be the-oretical studied to improve the accuracy and efficiency of KDEm . We only focus on quantitative information in this study but categorical data can be modeled by encoding them as high dimensional binary claims as well.

Research was sponsored in part by the U.S. Army Re-search Lab. under Cooperative Agreement No.W911NF-09-2-0053 (NSCTA), National Science Foundation IIS-1017362, IIS-1320617, IIS-1319973, IIS-1553411 and IIS-1354329, HDTRA1-10-1-0120, and grant 1U54GM114838 awarded by NIGMS through funds provided by the trans-NIH Big Data to Knowledge (BD2K) initiative (www.bd2k.nih.gov), and UIUC. The views and conclusions contained in this doc-ument are those of the author(s) and should not be in-terpreted as representing the official policies of the U.S. Army Research Laboratory or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copy-right notation hereon.
