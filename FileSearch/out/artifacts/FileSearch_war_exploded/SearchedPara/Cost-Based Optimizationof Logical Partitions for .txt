 Shu Peng 1 ,JunGu 1 ,X.SeanWang 1 , Weixiong Rao 2 ,MinYang 1 ,andYuCao 3 Over the past few years, Hadoop [1] has been widely used for data mining and data analysis due to its two advantages. First, the Hadoop parallel programming framework allows users to comfortably write MapReduce [4] jobs on a big cluster of commodity machines. Second, with the h elp of Hadoop distributed file system (HDFS) to distribute input files across the clustered machines, the MapReduce jobs can then process local file chunk s with very low communication cost.
The main processing mode in using HDFS to evaluate queries is to scan the entire data. For an efficient use of the clu ster of machines, the HDFS splits large data files into data blocks and then distributes the blocks randomly and evenly across the machines. This policy can balance the workload of the machines when the scan is done. When all the data need s to be scanned, this is indeed a very effective method.

Optimization over the above basic sca nning method has been introduced in the literature, especially for data in a relational table form. RCFile [7] was designed as a data format that stores the column-wise compressed data in order to reduce the I/O cost. However, often a query do es not need to use all the data in a data file. For example, a select-project quer y may only need to access a small portion of the rows and a few of the columns in a relation. A general approach in the literature (e.g. CoHadoop [5]) is to put the data needed by the same query onto the same physical machines to reduce data shuffling. However, this approach may lead to unbalanced workload across the cluster, reducing the efficiency of the whole cluster.

In this paper, we propose a novel approach, namely a condition based partition scheme (CPS). The main idea of CPS is to analyze the query workload and then to place correlated data into the same logical partitions, and each logical partition is instead physically stored in clustered machines. Here,  X  X orrelated data X  means the data that are used by the same query. In detail, by analysing the queries, we divide correlated rows of input tables into the logical partitions. When the raw table data is uploaded to HDFS, the HDFS policy then still randomly and evenly distributes the data across the cluster, without incurring unbalanced workload across the cluster. When the queries are processed, only those logical partitions needed by the queries are accessed. In this way, we can avoid the full scan of the whole input data and access only the needed partitions, and yet do not introduce any imbalance in processing tasks among the cluster nodes. Consequently, the CPS greatly optimizes the query processing time.
In summary, in this paper, we make the following contributions: 1. We introduce a logical partitioning approach to avoid full scan of data, yet 2. We design a method that does the logical partitioning correctly and auto-3. We design a prototype to implement (based on Hive [2]) the logically par-4. We conduct experiments to compare the implemented prototype with the The rest of the paper is organized as follows. First we introduce the background and motivation of our work in Section 2 and Section 3, respectively. Next, we present the design of our partitioning scheme in Section 4.After that, we evaluate the performance of the dev eloped prototype in Sectio n 5, and review the related works in Section 6. Finally Section 7 concludes the paper.
 To provide a technical background of this paper, we give an overview of two systems, namely Hadoop and Hive, and an i mportant concept used in this paper: logical partition. 2.1 Hadoop and Hive The Hadoop system mainly contains two components, namely HDFS and MapRe-duce. Before processing the input files, Hadoop needs to distribute the files ran-domly and evenly across a cluster of co mmodity machines. The files are then maintained on HDFS with data blocks of 64 MB by default. Next, to initiate a data query task, Hadoop starts map tasks (mappers) and reduce tasks (reduc-ers) concurrently on the clustered mach ines. The mappers sequentially read the input files from HDFS with each data block as a unit and shuffle the interme-diate results to the reducers. When the jobs are finished, the reducers write the output back to HDFS.

To support queries, Hive provided an open-source data warehousing solution atop Hadoop. To process the queries written by an SQL alike declarative lan-guage, Hive compiles the queries into MapReduce jobs that are executed by Hadoop. Besides, Hive provides commands and third party application inter-faces, such that users can execute queries in a flexible way. 2.2 Logical Partition Hive supports the concept of logical partition. That is, for a given table, the data belonging to the same logical partition is placed into one directory on HDFS. However, the files are physically distributed onto multiple nodes in a cluster. For example, Hive uploads the table raw files onto HDFS by executing MapReduce jobs, and then splits the raw files into N partitions. Next, the N partitions are evenly placed onto M datanodes of a Hadoop cluster. For a given table T in data warehouse, in the simplest case, we assume that only one selection query refers to T . We can directly select all rows from T satisfying the query condition into a partition, and let all remaining rows into another partition. Intuitively, by treating the query condition as a sub-range R in the universe U , the partitioning scheme is to separate the universe U into the sub-range R and the opposite U \ R .

In the general case, there instead could exist multiple queries referring to the table T . As an example of the general case, t hree following queries refer to the table Lineitem in the TPC-H [3] benchmark (we will use the queries throughout this paper).
Given the general case, the key of the proposed partitioning scheme is to generate a best partition plan with the minimal query processing time. Therefore, we may independently consider each of the queries, and then follow the above simplest case (with only one query) to partition the input table T . However, the approach could lead to too many partitions and MapReduce jobs during processing queries and still cannot guarantee the minimal query processing time.
To this end, we need to carefully find a best way to partition the table T for the minimal query processing time. Here, we will define a partition plan that is used to partition T (Section 4.1). Given multiple queries, each of the queries might involve an associated plan to partition the table T , and the whole queries involve the various combinations of such plans (Sections 4.2). Consequently, based on a cost model, we find the best one with the least cost among all possible candidates for the minimal processing time (Section 4.3). 4.1 Definition of Partition Plan For a given table T with schema T s , we consider a set of queries over T . Such queries involve the number n of query conditions  X  i with 1  X  i  X  n . Based on such In general, partition p i is determined by an associated condition  X  i , such that the data of p i satisfies the condition  X  i = True .When p i are ready, to process a query having multiple conditions, we will consider only those partitions corresponding to such conditions for query evaluation.

Basedonthethreequeries Q 1 ,Q 2 and Q 3 , we give an example of a valid partition plan in Table 1. Just for simplicity, we only consider the predicate conditions referring to the attribute l shipdate . As shown in this table, each partition has an unique ID, and the data in each of the partitions is determined by selecting the rows from Lineitem satisfying the associated conditions. 4.2 Generate Candidate Plans We note that the query conditions could involve disjunctive and (or) conjunctive forms of individual predicates. Thus, we need to transform the disjunctive and (or) conjunctive combinations of such predicates. The purpose of such transfor-mations is to simply the query combinations and meanwhile should not falsely miss any query results needed by the que ries (and thus should correctly answer the queries without falsely missing any results).

With equivalent transformations in logical theory, we can transform query conditions into their the major conjunctive normal form (CNF), and each ele-ment in CNF refers to one attribute. Ba sed on all such elements, we then use the algorithm in Section 4.2 to generate the candidates of partition plans.
Before transforming the predict conditions in queries, we first formally define the candidate partition plan as follows. For the table schema T s , the candidate partition plan about an attribute a contains all selective conditions involving a and their corresponding negative conditions. To avoid overlapping conditions, a candidate plan satisfies  X   X  i , X  j  X   X ,  X  i  X   X  j =  X  .

Next, we consider the following typical data warehouse query, and would like to find out all the candidate plans.
 where A denotes an aggregate function over attributes of table T ,and P denotes a set of predicates in the query conditions. Denote  X  ( P ) to be the attributes appearing in the conditions. For each query in given workload, we can extract the attributes set and union all the sets to figure out the attributes appearing in the queries. Then, for every attribute, we combine the query conditions refer to it. Finally, we figure out a candidate plan for each attribute as shown in Table 2. 4.3 Cost Model of Partition Plan In this section, we give a cost model to measure the cost of each plan. Consider table T =( a 1 ,a 2 ,...,a k )where a i with 1  X  i  X  k is the i -th attribute of T and M ( T )isthesizeof T . Suppose we have a candidate plan P = { p 1 ,p 2 ,...,p n } de-Denote r (  X  ) is the selective ratio of T , when filtering T according to  X  . Then, the size of the i -th partition of table T is Next,weusethefunction f ( T,i,q ) to determine whether or not the i -th partition is hit by q ,where  X  ( q ) is the set of attributes in where clause of q and c q is the whereclauseof q : In addition, if aggregation operations existing in query, there incurs two MapRe-duce jobs to figure out query result, the first one executes selection operation and the other one processes its result fo r aggregation. We accumulates the input size and use function g ( q ) to determine whether q having aggregations:
Then, given the query set Q ( T ) contains all the queries refer to T in current workload, the cost of fetching input data of T is
With the above cost model, we then can calculate the cost of each candidate plan, and the one with the least cost is chosen as the final plan. We evaluate the performance of CPS on thr ee aspects: (i) the usefulness of our cost model, (ii) the advantages of CPS over previous works.

We conduct our experiments on a Hadoop cluster with one master node and 5 slave nodes. Each node has the same hardware and software configuration with 64-bit Linux and 750 GB hard disk. We implement CPS service atop Hadoop 1.2.1 and Hive 0.11.0. Each mapper/reducer task uses 1024 MB memory and the block size is 64 MB by default. We use dbgen in TPC-H to generate the synthetic dataset and the scale factor is 30. 5.1 Usefulness of the Proposed Cost Model In section 4.1, we generate 3 candidate partition plans for table Lineitem .Next, for each of these plans, we first use the cost model to compute the theoretical cost, and next compute the empirical results based on our experiments. Instead of precisely computing the theoretical c ost, we alternatively use the size of the data used by the plans. Denote M to be the total size of an input table, and r  X  M then indicates the size of the data used by a plan to process the query.
In Table 3, for each of the candidate plans with respect to the rows l shipdate , l discount and l shipmode , the 2nd -4th columns indicate the data size used by each query from Q 1 to Q 3 (because of aggregation, the ratio of input size to table size maybe larger than 1), and the 5th column sums the data size. The rightmost column instead gives the real query time of executing all three queries. Though the result given by the proposed c ost modal does not precisely indicate the absolute cost of each candidate plan, this table does verify that the empirical value (i.e., the real query time) is roughly consistent with the theoretical value computed by the cost model (i.e., th e total cost in the 5-th column). 5.2 Comparison between CPS and Previous Works In this experiment, we compare CPS with two alternative approaches (a special file format RCFile and physically clustering). In detail, we first use RCFile as the typical example of optimize the desig n of file format. Second, we purposely move the data blocks belonging to the same (logical) partitions onto the same physi-cal nodes, such that we emulate the physical clustering approach (CoHadoop). Finally, we combine the two techniques (the RCFile and the CPS partitioning scheme) together in order to provide the utmost performance. In this experiment, we extract 12 selection queries 1 about table Lineitem in TPC-H and processing queries on those tables. Based on these approaches, we conduct the experiment by 10 times and then measure the average processing time given in Figure 1(a).
As shown in this figure, the case without any partition obviously uses the longest time. Next, compared with RCFile, the CPS partition scheme saves about 20% time. The physical clustering method has the longer execution time than RCFile does. Instead, the combination of the two techniques (i.e., RCFile and CPS) can achieve the best result by sav ing around 40% time. This experiment clearly verifies the advantages of CPS ov er the previous works, and in partic-ular, the combination of CPS and RCFile achieves the best result among all approaches. First, CoHadoop extended Hadoop by placing data blocks (and their replicas) of correlated files onto the same data nodes. In this way, the correlated files can be accessed locally inside the same machine, and the network traffic is reduced. Un-fortunately, this approach could lead to unbalanced workloads in clustered ma-chines, in case that the correlated data i s associated with very high (or very low) volume size. Different from CoHadoop, some previous works specially designed file formats to optimize Hadoop query processing time. The typical examples of such works include CFile , [6] and RCFile . CFile supported a column-wise file format based on the entire file level, while RCFile was an in-block column-wise storage. The column-store allowed faster query processing time by less I/O cost. However, these data formats do not cluster semantically related data together. In this paper, we proposed a new partit ion scheme, CPS, to optimize query processing, based on the Hive data warehouse and the Hadoop distributed file system. Our experimental results have successfully verified that the CPS scheme can improve the performance of query pro cessing over the state of the art meth-ods. As future work, we will extend our partition scheme to support more com-plex query conditions, such as those having multiple attributes conditions, and the query operators like Join and Order By .

