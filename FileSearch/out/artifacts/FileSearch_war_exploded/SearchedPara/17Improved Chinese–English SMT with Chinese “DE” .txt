 Syntactic structure-based reordering has been shown to be significantly helpful for handling word order issues in phrase-based machine translation (PB-SMT) [Chang et al. 2009; Collins et al. 2005; Du and Way 2010a, 2010b; Elming 2008; Jiang et al. 2010; Li et al. 2007; Wang et al. 2007; Xia and McCord 2004]. Generally, PB-SMT has an independent reordering model because the PB-SMT phrases themselves do not have an ability to perform word reordering. However, as regards hierarchical PB-SMT (HPB-SMT) [Chiang 2005] and syntax-based SMT such as SAMT [Zollmann and Venugopal 2006], they possess an inherent word reordering capability because each rule is a hierarchical structure which contains the sub-phrases or sub-rules. The order of a sub-rule pair in a source X  X arget rule is decided by the positions of non-terminals when generating the hierarchical phrase or a rule based on the word alignment links between the source and target language sentences. Generally, there is no specific re-ordering model inside the HPB and SAMT systems. When the rule is expanded during the decoding process, the word order is computed automatically according to the po-sitions of the corresponding non-terminal symbols and the synchronous context-free grammars (SCFG) [Aho and Ullman 1969] between the source and target phrases.
The rules in HPB and SAMT systems are generally variable-based phrases, that is, short phrases are substituted in a long phrase or rule with variables or non-terminals based on SCFG. The method of constructing variable-based phrases facilitates learning of the different orderings between the source and target languages to some extent. Therefore, in some sense, it can be said that the variable-based rules contain a hidden reordering model. However, this hidden model cannot handle some flexible syntactic structures such as some cases of (DE) in Chinese very well, especially for HPB-SMT which does not consider syntactic knowledge in its rules.

It is well known that in MT, it is difficult to translate between Chinese and En-glish because of the different word orders (cf. the different orderings of head nouns and relative clauses). Wang et al. [2007] pointed out that Chinese differs from En-glish in several important respects, such as relative clauses appearing before the noun being modified, prepositional phrases often appearing before the head they modify, etc. Chang et al. [2009] argued that many of the structural differences are related to the ubiquitous Chinese structural particle phrase (DE) construction, used for a wide range of noun modification constructions (both single word and clausal) and other uses. They pointed out that DE is a major source of word order error when a Chinese sen-tence is translated into English due to the different ways that the DE construction can be translated. As a result, Jiang et al. [2010] exploited some functional words includ-ing DE constructions for source-side syntactic reordering of Chinese X  X nglish SMT in a non-deterministic approach. They extract syntactic reordering patterns from source-side parse trees and then transform the input sentence into word lattices based on the pre-extracted reordering patterns. Finally word lattice decoding was performed to generate the final result. The results show that the reordering of functional words such as DE play an important role in improving translation quality and reducing word order errors.

From the grammatical perspective, DE in Chinese represents the meaning of  X  X oun modification X  which generally is shown in the form of a noun phrase (NP) [A DE B]. A includes all words in the NP before DE and B contains all words in the NP after DE. Wang et al. [2007] first introduced a reordering of the DE construction based on a set of rules which were generated manually and achieved significant improvements. Chang et al. [2009] extended this work by classifying DE into five more fine-grained cate-gories using a log-linear classifier [Toutanova and Manning 2000] with rich features. The purpose of DE classification is to capture more fine-grained translation patterns based on the different ways that DE can be translated in order to achieve higher accu-racy both in terms of reordering and lexical choice. Their experiments showed that a higher accuracy of DE classification improved the accuracy of reordering component, and further indirectly improved the translation quality in terms of BLEU [Papineni et al. 2002] scores.
 In this article, we focus on two aspects: (1) improving the classification accuracy of DE constructions and the translation quality of SMT systems using the discriminative probabilistic latent variable model (DPLVM) [Morency et al. 2007; Sun and Tsujii 2009]; and (2) evaluating word alignment and phrase table on quality of original data as well as the DE reordered. Motivated by the idea of DE classification and the fact that classification accuracy could have a significant impact on SMT performance, we regard the DE classification as a labeling task, and hence propose a new model to la-bel the DE construction using DPLVM, which uses latent variables to carry additional information that may not be expressed by those original labels and capture more com-plicated dependencies between DE and its corresponding features. We also propose a new  X  X ree-pattern X  feature which can automatically learn the reordering rules rather than use manually generated ones.

In our task, the DE classifier is used to pre-process both the training and test data by explicitly labeling (DE) constructions, as well as reordering phrases. Then we retrain word alignment using GIZA++ [Och and Ney 2003] and build a reordered phrase table. The experimental results on PB-SMT, HPB-SMT, and SAMT systems are reported on the NIST 2005 and 2008 Chinese X  X nglish evaluation data in terms of BLEU [Papineni et al. 2002], NIST [Doddington 2002], and METEOR [Banerjee and Lavie 2005] scores.

The remainder of this article is organized as follows. In Section 2, we introduce the diversity of DE constructions to be translated into English and types of word order errors caused by translating the DE construction as well as the reordering mecha-nisms of HPB-SMT and SAMT. Section 3 describes the closely related work on the DE construction. We also detail the Stanford log-linear DE classifier, the five more fine-grained classes and the features used in this section. In Section 4, we describe our proposed DPLVM algorithm and its adaptation to our task. We also detail the feature templates as well as the proposed new feature used in our model. In Section 5, the classification experiments are conducted to compare the proposed classification model with a log-linear model [Chang et al. 2009]. Section 6 reports comparative experi-ments conducted on the NIST 2005 and 2008 data sets using sets of reordered and non-reordered data. Meanwhile, in Section 7, we carry out an analysis of the impact of the syntactic DE reordering by evaluating the quality of the word alignment and phrase table. In Section 8, we give some examples to show how the DE reordering approach works for PB-SMT and HPB-SMT systems. Section 9 concludes and gives avenues for future work. It is well known that in MT, it is difficult to translate from Chinese to English because of the different word orders (cf. the different orderings of head nouns and relative clauses). Chang et al. [2009] pointed out that many of these structural differences are related to the ubiquitous Chinese structural particle phrase (DE) construction, used for a wide range of noun modification constructions (both single word and clausal) and other uses. Examples in Figure 1 show the diversity of DE construction [A DE B] in Chinese when it is translated into English.

From Figure 1, we can see that the same form of [A DE B] in Chinese can be trans-lated in many different ways, where some have the same order, but others see a re-ordering of A and B in English, such as the last two examples. Therefore, if the DE construction under different contexts can be classified into a certain class and can be distinguished in terms of reordering vs. non-reordering, then it might reduce the word order errors caused by the DE construction translation. In order to highlight the importance of studying the DE construction, some examples shown in Figure 2 illustrate the errors of translation results from three different MT systems, and many errors relate to incorrect reordering for the (DE) structure. These three translations are from different HPB-SMT systems. Although HPB-SMT has an inherent reordering capability, none of them reordered  X  X ad reputation X  and  X  X iddle school X  around the DE. Chang et al. [2009] analyzed that this is because it is not sufficient to have a formalism which supports phrasal reordering. They claimed it is necessary to have sufficient linguistic modeling, so that the system knows when and how much to rearrange.

To solve this problem, Wang et al. [2007] proposed a syntactic reordering approach to deal with structural differences and to reorder source-language sentences to be much closer to the order of target-language sentences. They presented a set of syn-tactic rules to determine whether a (DE) construction should be reordered or not before translation. The deficiency of their algorithm is that they did not fully consider the flexibility of the DE construction, such that it could be translated in many different ways. Chang et al. [2009] extended the work of Wang et al. [2007] and characterized the DE structures into five more fine-grained classes based on their behaviour.
Figure 3 gives an example illustrating how reordering DE construction influences the translation quality of a Chinese sentence. We can see that if we can properly recognize the DE construction [A DE B] and perform the appropriate reordering, we can achieve a closer word order to the English and hence a better English translation even if it is quite literal.

Although the HPB-SMT and syntax-based systems have a strong reordering capa-bility in their generalized phrases, it still cannot process some complicated and flexible cases of DE construction like those in Figure 2. Therefore, a lot of work has gone into word reordering before decoding so that the Chinese sentences have a closer word or-der to their corresponding English sentences. The variable-based rules such as hierarchical phrases in HPB-SMT and syntactic rules in SAMT not only have a powerful generalization ability, but also have a strong re-ordering capability because of their special phrasal structures. The idea of presenting the variable-based phrases is to learn reordering of phrases in the same way that the phrases are good for learning the reordering of words. Let us revisit Figure 3 as an illustration to show the mechanism of HPB-style reordering. SAMT has a similar re-ordering style to that of HPB-SMT.

In Figure 3, there are three hierarchical phrase pairs (lexicalized synchronous grammar rules) which are related to syntactic reordering [Chiang 2005], namely,  X  &lt; yu X
Chinese PPs almost always modify VPs on the left, whereas English PPs usually modify VPs on the right. It can be found that this rule is a phrase-reordering rule which generalizes the reordering of verb phrases and prepositional constituents.  X  &lt; X
This rule indicates that Chinese relative clauses modify NPs on the left while En-glish relative clauses modify NPs on the right. Therefore, this reordering rule gen-eralizes the reordering of relative clauses and the DE structure.  X  &lt; X
This rule captures the construction zhiyi in English word order which is different from Chinese word order.
 Based on the reordering rules above, the translation goal for HPB-SMT is to rotate the noun head and the preceding relative clause around (DE), so that  X  X one of few countries] [have diplomatic relations with North Korea] X  can be correctly translated. However, it cannot process some complicated and flexible cases of DE construction like those in Figure 2. Therefore, in the next section, we introduce DE classification and use it as a necessary complementary component to improve the HPB system. To address the word order problems of the DE construction, Wang et al. [2007] pro-posed a syntactic reordering approach to deal with structural differences and to re-order source-language sentences to be much closer to the order of target-language sentences. They presented a set of manually generated syntactic rules to determine whether a (DE) construction should be reordered or not before translation [Wang et al. 2007]), namely,  X  For DNPs (consisting of  X  X P+DEG X ):  X  Reorder if XP is PP or LCP;  X  Reorder if XP is a non-pronominal NP.  X  For CPs (typically formed by  X  X P+DEC X ):  X  Reorder to align with the  X  X hat+clause X  structure of English.
 The deficiency of their algorithm is that they did not fully consider the flexibility of the DE construction, as it can be translated in many different ways.

Chang et al. [2009] carefully investigated the syntactic behaviour of the DE struc-tures and then categorized them into five more fine-grained classes. They argued that one possible reason why the DE construction remains problematic is that previous work has paid insufficient attention to the many ways that the DE construction can be translated, as well as the rich structural cues which exist for these translations.
For a Chinese noun phrase [A B], Chang et al. [2009] categorized it into one of the following five classes (cf. [Chang et al. 2009] for some real examples of each class):  X  A B (label: DE AB )
In this category, A on the Chinese side is translated as a pre-modifier of B. In most cases A is an adjectival form.  X  B preposition A (label: DE BprepA )
There are several cases that are translated into the form B preposition A.  X  A X  X  B (label: DE AsB )
In this class, the English translation is an explicit s-genitive case. This class occurs much less often but is still interesting because of the difference arising from the of-genitive.  X  Relative clause (label: DE relc )
In this class, the relative clause would be introduced by a relative pronoun or be a reduced relative clause.  X  A preposition B (label: DE AprepB )
This class is another small one. The English translations that fall into this class usually have some number, percentage or level word in the Chinese A.

In these five classes, DE BprepA and DE relc are the two classes that A and B should be reordered between Chinese and English. The Stanford DE classifier uses a statistical classifier trained on various features to predict for a given Chinese (DE) construction whether it will reorder on the English side.

Chang et al. [2009] used six kinds of features for DE classification, namely part-of-speech tag of DE (DEPOS), Chinese syntactic patterns appearing before DE (A-pattern), unigrams and bigrams of POS tags(POS-ngram), suffix unigram and bigram of word (Lexical), semantic class of words (SemClass), and reoccurrence of nouns (Top-icality), namely,  X  Part-of-speech tag of DE (DEPOS): In the Chinese Treebank guideline [Xia 2000], there are two types of POS tag for
DE in NPs, namely DEC and DEG. However, Chang et al. [2009] used a trained parser to parse the data sets instead of a gold-standard Chinese treebank, so that the DEPOS feature might have values other than DEC and DEG.  X  Chinese syntactic patterns appearing before DE (A-pattern): Indicator functions that fire when some syntactic rules are satisfied, such as  X  X  is
ADJP if A+DE is a DNP with the form of  X  X DJP+DEG X  X  etc.  X  Unigrams and bigrams of POS tags (POS-n-gram):
This feature includes all unigrams and bigrams in A and B as well as the bigram pair across DE.  X  Lexical features:
In order to avoid the sparseness issue of only using full word identity, one-character suffix of each word and suffix unigrams and bigrams are used as lexical features.
Apart from the suffix gram features, there are other three lexical features to process some special grammatical occasions.  X  Semantic class of words (SemClass):
This feature uses the Chinese thesaurus CiLin [Mei et al. 1984] to look up the se-mantic classes of the words in [A DE B].  X  Reoccurrence of nouns (Topicality):
This feature is used to disambiguate s -genitive and of -genitive. The feature is ap-proximated by caching the nouns in the previous two sentence, and fired when the noun appears in the cache.
 The contribution of each feature to the classification accuracy is reported in Section 5.
In order to further improve the classification accuracy and the translation qual-ity, we propose a new model X  X PLVM X  X o classify the DE constructions using similar features. Based on the discussion so far, we can see that:  X  syntactic reordering of the DE construction in Chinese is an effective way to improve translation quality;  X  classifying the DE construction into more fine-grained categories can achieve better reordering and translation performance;  X  classification accuracy of the DE construction in Chinese has a significant impact on SMT performance.
 Driven by these three points, especially the third one, we propose a DPLVM-based clas-sifier to improve classification accuracy. In natural language processing (NLP) such as sequential labeling [Sun and Tsujii 2009], DPLVM demonstrated excellent capability of learning latent dependencies of the specific problems, and have outperformed sev-eral commonly used conventional models, such as support vector machines, conditional random fields, and hidden Markov models.

In our task of DE classification in a Noun phrase [A DE B], the fact that DE is labeled as reordered or non-reordered strongly depends on the relationships between A and B, which in some sense can be represented by latent variables. As a result, we propose using a DPLVM algorithm to model our classifier to label DE based on different categories. In this section, we theoretically introduce the definition and mathematical description of the DPLVM algorithm used in NLP tasks [Sun and Tsujii 2009].

Given a sequence of observations x = { x 1 , x 2 ,..., x m } and a sequence of labels y = { y is a member of a set Y of possible class labels. DPLVM also assumes that a sequence of latent variables h = { h 1 , h 2 ,..., h m } is hidden in the training examples. The DPLVM is defined as in Equation (1) [Morency et al. 2007; Sun and Tsujii 2009]: where are the parameters of the model. It can be seen that the DPLVM equates to a CRF model [Lafferty et al. 2001] if it has only one latent variable for each label.
For the sake of efficiency, the model is restricted to having disjoint sets of latent variables associated with each class label. Each h j is a member in a set H y latent variables for the class label y j . We define H as the union of all H y sequences which have any h j  X  H y model can be rewritten as in Equation (2): where P ( h | x , ) is defined by the usual conditional random field formulation, as in Equation (3): in which f ( h , x ) is a feature vector. Given a training set consisting of n labeled se-quences ( x i , y i ), for i =1 ... n , parameter estimation is performed by optimizing the objective function in Equation (4): The first term of this equation is the conditional log-likelihood of the training data. The second term is a regularizer that is used for reducing over-fitting in parameter estimation.

For decoding in the testing stage, given a test sequence x , we want to find the most probable label sequence y  X  , as in Equation (5): Sun and Tsujii [2009] argued that for latent conditional models like DPLVMs, the best label path y  X  cannot be generated directly by the Viterbi algorithm because of the in-corporation of hidden states. They proposed a latent-dynamic inference (LDI) method based on A  X  search and dynamic programming to efficiently decode the optimal label sequence y  X  . For more details of the LDI algorithm, refer to Sun and Tsujii [2009].
In our experiments, we use the open source toolkit of DPLVM 2 and adapt it to our special requirements based on the different features and scenarios. We use the five classes of DE of Chang et al. [2009] shown in Section 3 to label DE using our DPLVM model. In order to fairly compare the classification performance between that of Chang et al. [2009] and our proposed classifiers, we use the same data sets and conditions to train and test the classifier. The data sets are the Chinese Treebank 6.0 (LDC2007T36) and the English X  X hinese Translation Treebank 1.0 (LDC2007T02). For more details about the data sets, refer to Chang et al. [2009]. There are 3523 DEs in the data set, with 543 of them in the  X  X ther X  category which do not belong to any of the five predefined classes. In the classification experiments, the  X  X ther X  class is excluded 3 and 2980 DEs remain, each of which is manually annotated with DE labels for the purposes of classifier training and evaluation. In order to match the training and testing conditions, we used a parser trained on CTB6 excluding files 1-325 to parse the data sets with DE annotation and extract parse-related features rather than using gold-standard parses (same conditions as in Chang et al. [2009]). It is worth noting that in the Chinese Treebank, there are two types of POS tag for DE in NPs, namely DEC and DEG. However, as a result of using a trained parser, to parse the data sets instead of gold-standard ones, the DE POS tags might have values other than DEC and DEG. In our data set, there are four other POS tags, namely { AS, DER, DEV, SP } . In our task, we use the five class labels of DE constructions in NPs, namely DE AB , DE it is different from traditional sequence labeling tasks such as POS tagging, parsing, etc. We only need to label one word in the NP structure, that is, the (DE) in a Chinese NP [A DE B]. Therefore the sequence labeling task is efficient when using the DPLVM algorithm.

Based on our task, the mathematical conditions for DE classification in a sequence of [A DE B] are denoted as follows. the Chinese character (DE), and B= { x k ,..., x m } .  X  Latent Variables. h = h 1 , h 2 ,..., h m , where m = 3 in our task. 4
We also employ five features in the DPLVM model, namely DEPOS, POS-gram, lexical features, SemClass as well as a new feature  X  X ree-pattern, X  which is discussed below. The tree-pattern feature is distinct from the A-pattern feature used in Chang et al. [2009] in terms of automatic learning capability from the training data.
We did not add the sixth feature used in Chang et al. [2009] X  X opicality X  X n our classifier because we do not consider it to be very useful in a data set in which the sen-tences which are randomly stored. In such a corpus, the content between any adjacent sentences is irrelevant in many cases.

The new feature and the templates of all features used in our task are defined as follows.

DEPOS. As mentioned in Section 4.3, there are six kinds of DE POS tags. Thus, the feature template is defined in Equation (6):
Tree-pattern. Chang et al. [2009] used an A-pattern feature which is an indicator function that fires when some syntactic rules are satisfied, such as  X  X  is ADJP if A+DE is a DNP with the form of  X  X DJP+DEG X  X , etc. These rules are induced manually based on the grammatical phenomena at hand. Considering that such syntactic rules might have different properties in different contexts, here we propose a more generalized fea-ture  X  X ree-pattern X  to automatically learn the reordering from the training data. We consider all the sub-tree structures around DE without any word POS tags. For exam-ple, consider the parse structure (an example in Chang et al. [2009]) in Equation (7): where the tree-pattern is  X  X P NP CP IP VP ADVP VP DEC NP X . We do not use the word POS tag (except DE) in this feature, such as NR, AD, VA, etc. The intention of this feature is to enable the classifier to automatically learn the structural rules around DE. Given that the position of DE in the parsing of [A DE B] is i , then the feature template is defined as in Equation (8): where i is the position of DE in the tree-pattern, l is the length of the left side of the POS sequence centered by DE-POS while m is the length of the right side. We do not restrain the window size of the tree pattern. T tree u is the sequence of unigrams in connection with DE, and T tree b is the sequence of bigrams related to DE; l and m are the window sizes of A and B respectively. Generally, we use all the unigrams and bigrams in the parsing of A and B in our experiments. We argue that the biggest advantage of this feature is that it does not depend on manually generated rules, but rather it learns and generalizes the reordering rules from the training data directly. POS-gram. The POS-ngram feature adds all unigrams and bigrams in A and B. Given that the position of DE is i in [A DE B], the feature template is defined as in Equation (9): where T pos u and T pos b are unigrams and bigrams in A and B. In the unigrams, we exclude the POS of DE; in the bigrams, we include a bigram pair across DE.
Some other features such as lexical features and SemClass (cf. Chang et al. [2009] for details) can be defined using similar feature templates. In this section, we compare the performance of DE classifiers between the DPLVM and log-linear methods.

The accuracy of classification is defined as in Equation (10):
Table I shows the comparison of accuracy, where  X 5-A X  and  X 2-A X  represent the accu-racy of the five-class and two-class respectively. The two-class is the categorized classes of DE in Wang et al. [2007] which are defined as  X  X eordered X  and  X  X on-reordered X  cate-gories. Compared to our previous work in Du and Way [2010a], in this article, we also use the feature  X  X -pattern X  in the DPLVM model rather than remove it, so that the final result is better than that in Du and Way [2010a]. It can be seen that our DPLVM classifier outperforms the log-linear classifier by 2.8 and 2.6 absolute points (3.71% and 2.99% relative improvement respectively) both on five-class and two-class classi-fications. Furthermore, we see that the DPLVM achieves significantly better perfor-mance than the log-linear model only with the simple feature of  X  X EPOS X . Comparing the contribution of feature  X  X -pattern X , we can see that it achieves the improvement of 13.1 and 12.7 points (23.91% and 17.89% relative) on  X 5-A X  and  X 2-A X  respectively using the log-linear model, while it improves 13.7 and 13.1 points (24.38% and 18.12% relative) on  X 5-A X  and  X 2-A X  respectively using the DPLVM model. From applying the two features  X  X EPOS X  and  X  X -pattern X  to these two models, it can be seen that the DPLVM model outperforms the log-linear model. As to the new feature  X  X ree-pattern X  in DPLVM model, we can see that it contributes the improvements of 2.4 and 1.4 points (3.44% and 1.64% relative) on  X 5-A X  and  X 2-A X  respectively when we apply the feature  X  X -pattern X . This improvement may be attributed to the good learning ability of DPLVM as well as the strong generalization capability of our tree-pattern feature.
From above statistics, we know that the boundaries of A and B are detected by the parser, therefore the accuracy of the parse would have a significant impact on the performance of the DE classification. In order to examine the problem of boundary detection, we will perform two experiments in future work 1) how the different levels of the accuracy of the parser impact the accuracy of the classifier? 2) how would the phrases outside A DE B influence the translation of A DE B construction? In terms of speed, in our task we only need to label the Chinese character DE in the NP structure [A DE B] rather than label the whole sentence, so that we have a feature matrix of n  X  1 for each DE. Accordingly, the DPLVM classifier can run efficiently with low memory usage. For our SMT experiments, we used three systems, namely Moses [Koehn et al. 2007], Moses-chart and Moses-samt. 5 The first one is the state-of-the-art PB-SMT system, the second is a new extended system of the Moses toolkit re-implementing the hier-archical PB-SMT (HPB) model [Chiang 2005], while the last one is also a newly re-implemented syntax-augmented SMT model [Zollmann and Venugopal 2006]. Word alignment is carried out by GIZA++ [Och and Ney 2003] and then we symmetrized the word alignment using the grow-diag-final 6 heuristic for PB-SMT and grow-diag-final-and heuristic for HPB-SMT and SAMT. Parameter tuning is performed using Minimum Error Rate Training [Och 2003] for the optimization of BLEU scores.
The training data contains 1.7m sentence pairs including the ISI parallel data, FBIS and other LDC data. 7 The five-gram language model is trained on the English part of the parallel training data using the SRILM toolkit [Stolcke 2002]. The development set (devset) is the NIST 2006 test set which contains 1,664 sentences, and the test sets are the NIST 2005 and 2008  X  X urrent X  test sets which contain 1082 and 1357 sentences respectively. All the results are reported in terms of BLEU [Papineni et al. 2002], NIST [Doddington 2002] and METEOR (MTR) [Banerjee and Lavie 2005] scores. All the dev and test sets have four references per source sentence.

Moses-samt is a string-to-tree SMT system. Based on the default configuration, we firstly use maximum-entropy based POS (MaxEntPOS) tagger 8 [Ratnaparkhi 1996] for POS tagging in the English side, and then use Collins parser 9 [Collins 1997] to parse the tagged English data. Finally we use the annotation wrapper in Moses package to convert the parsed data to the XML format used in Moses-samt. 10
To run the DE classifiers, we use the Stanford Chinese parser [Levy and Manning 2003] to parse the Chinese side of the MT training data, the devset, and test set. For DE-annotated MT experiments, after parsing the training data, the devset, and test sets, we separately use two DE classifiers to annotate the DE constructions in NPs in all of the parsed data. The five classes are represented by AB , AsB , BprepA , relc and AprepB to replace the original (DE) character. Once the DE data are labeled, we pre-process the Chinese data by reordering the sentences only with BprepA and relc annotations. Table II lists the statistics of the DE classes in the MT training data, devset and test set using our DPLVM classifier.  X  X E non  X  denotes the unlabeled (DE) which does not belong to any of the five classes. 11 It can be seen that the classes that should be reordered X  X E BprepA and DE relc  X  X ccount for more than 68%, 67%, and 65% of the training data, devset, and test set respectively, which indicates that the majority of DE constructions in Chinese should be reordered. 12 Based on this finding, we also carried out a series of experi-ments in which all DE constructions in NPs are annotated as only one reordering class so that we can verify the effectiveness and necessity of the automatic DE classification. The details are described in Section 6.3.

After this preprocessing, we restart the whole MT pipeline: we run GIZA++ [Och and Ney 2003] to align the reordered data, build phrase tables, tune the MT system and finally evaluate the translation output. Experimental results from the PB-SMT, HPB and SAMT systems separately using the DPLVM and log-linear classifiers are shown in Table III.

The 95% confidence intervals (CI) for BLEU scores are independently computed on each of the three systems, while the  X  X air-CI X  is computed over the baseline system for other systems with either the log-linear classification model or the DPLVM classi-fication model. All significance tests use bootstrap and paired-bootstrap re-sampling normal approximation methods [Zhang and Vogel 2004]. 13 Improvements are consid-ered to be significant if the left boundary of the confidence interval is larger than zero in terms of the  X  X air-CI X .

The baseline systems indicate that the data is neither categorized into DE classes nor reordered on the Chinese side. We can observe the following.  X  The  X  X L-C X  and  X  X V-C X  systems significantly outperformed  X  X L X  in terms of BLEU scores based on the confidence intervals of  X  X air-CI 95% X  for PB-SMT and HPB-SMT.
However, as to the SAMT systems, neither the  X  X L-C X  nor the  X  X V-C X  is significantly better than the the  X  X L X  in terms of BLEU scores.  X  Regarding PB-SMT and HPB-SMT, the confidence intervals of the  X  X V-C X  over  X  X L-
C X  are [-0.11, +0.73] and [-0.28, +0.74] respectively, which indicates that although our DPLVM based system outperforms the log-linear based system by 0.31 absolute (1.05% relative) and 0.23 absolute (0.74% relative) BLEU points respectively, it is not significantly better.  X  The  X  X V-C X  method also achieved improvements in terms of NIST and MTR scores compared to the  X  X L X ,  X  X L-C X  systems for PB-SMT, HPB-SMT, and SAMT.  X  The  X  X V-M X  and  X  X L-M X  outperformed the  X  X L X  systems as well in terms of BLEU and most MTR scores for the PB-SMT and HPB-SMT 14 . Furthermore, the  X  X V-M X  performs best in the two manually reordering methods  X  X L-M X  and  X  X V-M X  com-pared to the  X  X L X  systems for PB-SMT and HPB-SMT in terms of the BLEU and
NIST scores. This comparison also indicates that since the majority of DE construc-tions in the NPs are featured as reordering, even if all of them are converted to a reordered class, it still can benefit the translation quality.  X  All the results from  X  X L-C X  and  X  X V-C X  are better than those from  X  X L-M X  and  X  X V-
M X  respectively in terms of BLEU scores, which indicates that the automatic clas-sification models are helpful to distinguish the non-reordered from reordered DE constructions. Results on the NIST 2008 test set for these three types of SMT systems with different classification models are shown in Table IV.

Findings on the NIST 2008 test set are quite similar to those on the NIST 2005 test set. Specifically, we observe the following.  X  The  X  X V-C X  system outperformed the  X  X L X  and  X  X L-C X  by 1.11 absolute (4.64% rel-ative), 0.47 absolute (1.91% relative) BLEU points respectively for PB-SMT, and by 0.94 absolute (3.92% relative), 0.24 absolute (0.97% relative) BLEU points re-spectively for HPB-SMT systems, and by 0.25, 0.01 absolute points respectively for
SAMT. In terms of the  X  X air-CI 95% X , the  X  X L-C X  and  X  X V-C X  systems are signifi-cantly better than the  X  X L X  systems for PB-SMT and HPB-SMT but not for SAMT.  X  The  X  X V-C X  systems also beat the  X  X L-C X  and  X  X L X  systems in terms of NIST and
MTR scores for PB-SMT, HPB-SMT and SAMT.  X  Regarding PB-SMT and HPB-SMT, the confidence intervals of the  X  X V-C X  over  X  X L-
C X  are respectively [+0.02, +0.93] and [+0.01, +0.82] which indicate that our DPLVM based system significantly outperforms the log-linear based system on NIST 2008 test set for PB-SMT and HPB-SMT.  X  The  X  X V-M X  and  X  X L-M X  outperformed the  X  X L X  systems as well in terms of BLEU for the PB-SMT and HPB-SMT, which shows the consistency of the property on the NIST 2008 test set where the majority of the DE constructions are featured as reordered.  X  The results of the  X  X L-C X  and  X  X V-C X  systems are better than those of the  X  X L-M X  and  X  X V-M X  respectively on the NIST 2008 test set, which confirms our conclusion that the automatic classification models are more efficient in distinguishing the non-reordered and reordered DE constructions.  X  As to the SAMT systems, the  X  X L X  and  X  X V X  based methods still did not significantly outperform the  X  X L X  systems.

Based on the analysis above, we conclude that (1) using DE classification and re-ordering on the source-side is helpful in improving translation quality in terms of PB-SMT and HPB-SMT; (2) the results using DPLVM achieve better translation quality than that of the  X  X L X  processed data, which indirectly shows that DPLVM outperforms the log-linear classification model in our task; (3) the DE classification methods do not perform well on SAMT; and (4) the improvements on both PB-SMT and HPB-SMT show that the effectiveness of DE reordering is consistent for different types of MT systems without syntactic knowledge, but inconsistent on syntax-based SMT. In order to better verify how much impact DE reordering on the source side has on translation quality, we also carried out experiments using monotone decoding for PB-SMT systems. Results on NIST 2005 and 2008 test sets are shown in Table V.
It can be seen that the  X  X V-C X  and  X  X L-C X  systems with DE constructions reordered in the source side significantly achieve improvements by 1.14 absolute (4.21% rela-tive), 0.99 absolute (3.65% relative) and by 0.99 absolute (4.27% relative), 0.88 abso-lute (3.80% relative) BLEU points on NIST 2005 and 2008 test sets respectively than the  X  X L X  systems. Moreover, the  X  X V-C X  performs better than the  X  X L-C X  on these two test sets which indicates that our DPLVM model is more helpful in improving the translation quality in terms of PB-SMT systems. Wang et al. [2007] and Chang et al. [2009] discussed how the syntactic rules and DE annotation affect translation from the perspective of (1) syntactic reordering rules transforming the word order on the source-side to be closer to English; (2) the extent to which the reordered data produces better word alignment and better phrase pairs for SMT. The first aspect is easily understood, and can be evaluated quite simply. How-ever, the second one is not that easy to verify because it might be argued that incorrect reordering could introduce more noise and hurt the alignments as well as the con-tents of the phrase table [Wang et al. 2007]. In this section, we plan to evaluate how DE reordering contributes to the improvement in translation quality for PB-SMT and HPB-SMT from two respects, namely word alignment and phrase table. In addition, we try to explain to the failure of the DE construction reordering methods for SAMT systems. We create a word alignment test set which includes 500 sentences with human align-ment annotation, and then add this test set to the MT training corpus. Accordingly, the DE-reordered test set is added to the reordered training corpus as well. Thus, we run GIZA++ using the same configurations for these two data sets and symmetrize the bidirectional word alignment using the grow-diag-final heuristic. The word alignment performance is evaluated with the human annotation via Precision, Recall, F1 and AER measures. The results are reported in Table VI.

In the bottom row of Table VI, the number before the slash represents the gain in  X  X V-reordered X  alignment quality compared to the  X  X on-reordered X  alignment links, and the number after the slash indicates the gain in  X  X V-reordered X  alignment qual-ity compared to the  X  X L-reordered X  alignment. We can see that in terms of the four measures, the word alignment produced by the LV-reordered data achieved the best performance with the highest F1 and lowest AER. Accordingly, DE reordering seems to be helpful in improving the word alignment quality of the training data, and our DPLVM model is more helpful in improving the word alignment quality than the log-linear model. Wang et al. [2007] proposed one way to indirectly evaluate the phrase table by giving the same type of input to the baseline and reordered systems, with the consideration that if the reordered system learned a better phrase table, then it should outperform the baseline on non-reordered inputs despite the mismatch and vice versa. However, they did not settle the question as to whether the reordered system can learn better phrase tables.

We also try to use the idea of Wang et al. [2007] to carry out phrase table evalua-tion on PB-SMT, HPB-SMT, and SAMT, that is, we tune the baseline on a reordered devset and then evaluate on a reordered test set; tune the reordered system on a non-reordered devset and then evaluate on a non-reordered test set. Results for PB-SMT, HPB-SMT, and SAMT on NIST 2005 and 2008 test sets are shown in Tables VII, VIII, and IX, respectively.

We find that for PB-SMT systems, (1) given the non-reordered test set, the DE re-ordered system performs better than the baseline system, which is consistent when different DE classifiers are applied; (2) given the reordered test set system, the re-ordered set produces a better result than the baseline, which is also consistent when different DE classifiers are applied; and (3) the results from the DPLVM-based (LV-C) reordered data are better than those from the LL-based reordered data.

Therefore, for PB-SMT, the reordered system has learned a better phrase table and the reordered test set addresses the problem of word order. What about HPB-SMT and SAMT? The experimental results on HPB-SMT and SAMT are shown in Tables VIII and IX respectively.

However, for HPB-SMT and SAMT systems, it is a different situation: given the non-reordered test set, the reordered SMT system did not beat the baseline. Mean-while, given the non-reordered SMT systems, that is, the baseline, the reordered test set did not perform better than the non-reordered test set. We argue that the special variable-based phrase or rule structure might be an important factor to influence mis-matched/matched conditions. The reordering capability of the rule might be disordered by the mismatched input. Since the DE-annotated approach is to label the Chinese sentences and then reorder the DE constructions with labels BprepA and relc , knowing what percentage of sen-tences are altered and comparing the results of altered sentences will be a useful mea-sure of how much impact DE reordering has on system performance. In the NIST 2005 test set, there are 724 out of 1,082 sentences (66.91%) that have DEs under NPs, and there are 648 out of 724 sentences (89.5%) that have BprepA or relc labels and are reordered. For the NIST 2008 test set, there are 839 out of 1357 sentences (61.8%) that have DEs under NPs, and there are 664 out of 839 sentences (79.1%) that have BprepA or relc labels and are reordered. These show that (1) the preprocessing affects the majority of the sentences; and (2) there is a significant amount of reordering in the majority of the affected sentences.

The experimental results of the altered sentences in terms of BLEU scores are shown in Table X.

We can see that both the  X  X V-C X  and  X  X L-C X  systems outperformed the baseline re-garding the DE altered sentences in terms of BLEU scores on both NIST2005 and 2008 test sets for all three types of SMT systems except the one underlined. Therefore, as for the altered sentences, the DE-annotated approach is helpful in choosing better English translations. To sum up, from the SMT results and the evaluation results on the word alignment and the phrase table as well as the altered sentences, we can conclude that the DE re-ordering methods contribute to significant improvements in translation quality for PB-SMT and HPB-SMT. Using DE reordered data can also achieve better word alignment. Based on evaluating phrase tables, we see that the source-side reordered PB-SMT sys-tems can learn a better phrase table than the non-reordered baseline. However, for the HPB-SMT and SAMT, due to the variable-based nature of their rules, we need more careful investigation to have a more proper evaluation of the phrase table. Our intention is to use the DE classifier to annotate and reorder DEs to improve trans-lation quality. However, this approach has demonstrated different characteristics on different types of SMT systems. In this section, we try to take examples of DE reorder-ing annotations of DE BprepA and DE relc to explain some phenomena especially those reflected from the HPB-SMT and SAMT experiments. DE BprepA is one of two annotations which would have to be reordered in Chinese sen-tences. This annotation indicates that the Chinese  X  X  X  is translated into a prepositional phrase. Figure 4 is an example of DE BprepA which is a translation from the NIST 2008 test set.
 In Figure 4, the position of Chinese A  X   X  and the position of Chinese B X   X  are swapped in the 4 English references. The Chinese A is translated into a prepositional phrase in English side. We can see that none of the baseline systems of PB-SMT, HPB-SMT and SAMT correctly translated the phrases A and B in terms of the word order, that is, they are not swapped on the English side. However, in the translation of the source-side reordered systems (PB LV-C, HPB LV-C and SAMT LV-C), the (DE) is translated into the preposition  X  X f X  and the positions of A and B are correctly adjusted on the English side. DE relc is another annotation which requires reordering of the Chinese sentences. This annotation indicates that the Chinese  X  X  X  is translated into a relative clause. Figure 5 is an example of DE relc which is also from the NIST 2008 test set.

In Figure 5, the position of Chinese A  X   X  X ndthe position of Chinese B  X   X  are swapped in the four English references in which the translation of DE in the first two references leads to a relative clause re-spectively, while the translation of DE in the last two references leads to prepositional phrases. The DPLVM classification model identifies the DE as the annotation  X  X elc X . We can see that like the example in Figure 4, none of the baseline systems of PB-SMT, HPB-SMT and SAMT (except SAMT BL) in this example can translate the phrases A and B in the correct positions. However, the source-side reordered systems (PB LV-C, HPB LV-C and SAMT LV-C) correctly translated the DE constructions in terms of the word order. In this article, we presented a new classifier: DPLVM to classify the Chinese DE con-structions in NPs. The classifier first categorized DE constructions into five classes according to its different constituents, and then reordered the contexts of the DE con-structions with labels of BprepA and relc so as to better match the English word order. We also proposed a new and effective feature  X  X ree-pattern X  to automatically learn the reordering rules using the DPLVM algorithm. A series of comparative experiments on NIST 2005 and 2008 test sets were carried out between the DPLVM and the log-linear classifiers and the baseline system for different types of SMT systems, namely PB-SMT, HPB-SMT, and SAMT. The experimental results showed that our DPLVM classifier outperformed the log-linear based method in terms of both classification ac-curacy and MT translation quality. Results from SAMT systems show that our DE-annotated approach does not work significantly. We argue that our DE reordering method do not necessarily lead to complete grammatical tree structures. In addi-tion, the evaluation of the experimental results in Section 7 indicates that the DE-reordering approach is helpful in improving the accuracy of the word alignment, and can also produce better phrase pairs and thus generate better translations for PB-SMT and HPB-SMT.

As for future work, we have the following plan (1) examine and classify the DE con-structions in other syntactic structures such as VP, LCP, etc., and specify the distribu-tion of DE under NP, VP as well as other syntactic categories; (2) use other excellent classifiers such as SVM to perform DE experiments and compare with DPLVM and log-linear methods; (3) manually examine how many DE constructions are wrongly classified in test sets so that we can properly evaluate the real accuracy of our DPLVM classifier and log-linear method; (4) examine in detail why the DE-annotated approach cannot perform well for SAMT; (5) examine the situation that how the phrases outside A DE B influence the translation of A DE B; and (6) intend to improve classification ac-curacy of the DE classifier with richer features to further improve translation quality.
