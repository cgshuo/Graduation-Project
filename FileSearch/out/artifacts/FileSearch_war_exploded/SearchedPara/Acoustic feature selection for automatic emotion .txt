 Jia Rong, Gang Li * , Yi-Ping Phoebe Chen * 1. Introduction
Emotional recognition is a common instinct for human beings, which has been studied by researchers from different dis-effects of vocal emotion expression on interpersonal interaction. Understanding the emotional state of the speaker during communication can help the listeners to catch more information than is represented by the content of the dialogue sen-2003; Schuller et al., 2005 ).

Although machine learning and data mining techniques have obtained flourishing applications ( Mitchell, 1997 ), only a few works have utilized these powerful tools and achieved better performance in emotion recognition from speech. Here a serious encumbrance is the lack of available emotional speech data. There is only few public benchmark databases avail-able for research purpose.

A sufficient number of training examples is the premise for most machine learning and data mining algorithms to work tise, how many training examples will be adequate is task-dependent; for example, the task of learning the XOR function, many machine learning and data mining algorithms (Vapnik, 1995 ).

There are two obvious ways to overcome the problem of data scarcity: one is to collect more data while the second is to design techniques that can deal with small data sets. Considering the fact that further data collection is manual, cost intensive and hard to achieve, it is more feasible and desirable for the second way. Based on this recognition, this paper presents a novel feature selection algorithm, ERFTrees , to extract effective features from small data sets. There are two facets of benefits by using this algorithm for emotion recognition: firstly, the irrelevant data can be removed and the dimensionality of the training data can be reduced; secondly, with a reduced data set, most existing machine learning algorithms which do not work well on small data set, can now produce better recognition accuracy. The empirical re-sults on Chinese (Mandarin) emotional data sets indicate that the presented algorithm outperforms other linear and non-linear dimensionality reduction methods, including Principle Component Analysis (PCA), Multi-Dimensional Scaling (MDS), and ISOMap.

The rest of the paper is organized as follows: we introduce the background and the related work in Section 2. The algo-in Section 5, we conclude the paper with a perspective analysis of possible future work. 2. Background and related work 2.1. Theory of human emotions
Constructing an automatic emotion recognizer depends on a sense of what emotion is. Most people have an informal understanding, but there is a formal research tradition which has probed the nature of emotion systematically. It has been chological tradition X .

In psychology, the theories of emotion are grouped into four main traditions, each making different basic assumptions about what is central to the nature of emotion. As summarized by Cornelius (1996) , there are four perspectives focusing emotion; and social constructivists are interested in the social-psychological and sociological organization of emotion.
In the field of emotion recognition from speech, different research groups usually use different emotion states, as shown in Table 1 . Considering the general agreement in the Darwinian and the Jamesian traditions of emotion research that some full-blown emotions are more foundational than others, it might be more desirable to focus on those foundational emotions. Based on the study of Russell (1980) and Banse and Scherer (1996) , the most foundational emotions are defined as follows:
In this work, we are going to use the above five emotional states for emotion recognition from speech. 2.2. A machine learning framework for emotion recognition
Depending on the understanding of various researchers, different emotion recognition projects usually adopt different methods. However, most existing work on emotion recognition can be summarized into the following general procedure, as shown in Fig. 1 :
Feature extraction stage to extract the whole acoustic feature set from the original speech corpus and transform these
Data preprocessing stage to select the most relevant subset of the whole candidate feature set or reduce the size of the
Emotion recognition stage to apply machine learning methods on the processed speech data set from the previous stage to 2.2.1. Feature extraction
In the feature extraction stage, the raw speech wave data examples are preprocessed to extract the basic acoustic or lin-high-dimensional speech features.

Other researchers, like Kwon, Chan, Hao, and Lee (2003) tried to do more. They transformed an original speech wave from Bu, Chen, &amp; Li, 2004).

Table 3 gives a summary of the previous study of acoustic features which have been used to encode emotional states by psychologists and human behavior biologists since 1930 X  X . The correlation between the speech signals and the archetypal states of the four emotions can also be found in the table.

Although there are many unsolved problems in psychology, Table 3 is still widely used as the most important theoretical foundation by a large number of computing-background research groups who have strenuously engaged with the emotional speech recognition in the past decade. The basic acoustic features were the primary choices in the early days. Most of the 1969), while it can also cause a positive emotion in others (Bezooijen, 1984 ). 2.2.2. Data preprocessing
Unlike the previous stage which extracts the related speech features with potential value for emotion recognition, the second stage aims to reduce the size of the speech data set by selecting the most relevant subset of features and removing tive in reducing dimensionality, removing irrelevant data, increasing learning accuracy, and improving comprehensibility
Sequential Forward Selection (SFS) was employed by Bhatti, Wang, and Guan (2004), whom achieved improved results than prior work without a feature selection process.

Another category of data preprocessing methods is dimensionality reduction, this contains linear methods such as Prin-(Kohavi &amp; John, 1997 ). 2.2.3. Emotion recognition as a classification problem
The last stage in this procedure is to train and build a classification model using machine learning algorithms to predict the emotional states in the speech data instances. The key task of this stage is to choose an appropriate method which can provide an accurate predicted result for emotion recognition efficiently.

In 1990s, most of the emotion recognition models are based on the simple Maximum Likelihood Bayes algorithm (MLB) (NN) classification methods, which became popular around 2000, is also a good choice for emotion recognition applications (Nicholson,Takahashi,&amp;Nakatsu,1999;Petrushin,2004;Park,WookLee,&amp;Sim,2002;Parketal.,2003 ).After2002,moreatten-tionwasturnedto SupportVectorMachine(SVM) (Chuang&amp;Wu,2004;Hoch,Althoff,McGlaun,&amp;Rigoll,2005; Lee,Narayanan,&amp; and to find out valuable patterns that can predict the speech data instance to a certain emotional state accurately. 2.3. Summary
Machine learning is an important method to achieve automatic emotion recognition from speech. The performance is highly related to the training data set and data preprocessing.

However in the area of emotion recognition from speech, as the size of training data is relatively small considering the number of features, existing work is not as efficient as expected. (1) Most attentions were focused on how to improve the accuracy of emotional speech recognition by building a better (3) PCA and MDS are widely used dimensionality reduction methods in the emotion recognition research, both of which
In this paper, we aim to develop solutions for application with small data sets to produce better data pre-processing re-sults for the future classification process. 3. The feature selection algorithm: ERFTrees
From the last section, we know that a data preparation step is important for the performance of emotion recognition algo-of many existing feature selection algorithms which requires adequate samples. To address this challenge, a novel method, forest ensemble and simple decision tree algorithm together. The structure of ERFTrees model is shown in Fig. 2 .
The Ensemble Random Forest to Trees (ERFTrees) model consists of two major components: feature selection and voting strategy. The feature selection strategy uses two algorithms ( C4.5 Decision Tree and Random Forest ensemble) to select to combine these two subsets of candidate features to work out the final output of feature selection model. 3.1. Feature selection strategy select the  X  X est X  attributes that contain most of the available information.
 sion tree classifier to grow a decision tree, which returns a set of selected features F F
DT can not be considered as the output of our feature selection model, because the decision tree algorithm has some weak-rithm is applied. Unfortunately, the training data set used in this study is a small data set with high-dimensional speech
Therefore, other methods need to be involved to solve this problem. Our focus is centralized on the random forest ensemble as described in RF2TREE method ( Zhou, 2004 ).

In random forest ensemble, a set of base learners generated by the RandomForests (Breiman, 2001 ) algorithm will be trained using the  X  X agging X  ensemble strategy to avoid the overfitting problem caused by the small training data set. This ensemble is used to enlarge the training data set through randomly generating new virtual instances, classifying them by the ensemble and putting these new instances into the original training data set. The generated random forest consists of many decision trees, and each tree is grown in the same way: suppose the number of training examples is n , each example has m variables; and then randomly sample n examples with a replacement from the original training data set; specify a number l m so that at each tree node, l variables are randomly selected out of the m variables and the best one of these random forest ensemble. Then, each of the selected candidate features will be ranked by a voting-by-majority strategy. 3.2. Voting strategy
The voting strategy is used to combine the multiple results obtained from the feature selection strategy and determine which subset of the features will be selected as the final output of the feature selection model. Here, we use the voting-by-majority method to complete the voting task.

In the voting-by-majority , each learner has the same priority or importance, and it will contribute one voice to the can-didate features. The voting result is determined by the number of votes received and the features with the majority vote. For example, in this case, we have two learners: a single C4.5 decision tree and a random forest ensemble. Let F resulting subset of features, which is selected by the single decision tree algorithm, and F is selected by the random forest ensemble. If one candidate feature f ( t 3.3. Justification ability to approach F can be expressed as where err T denotes the error rate of the decision tree. In the same way described in (Zhou, 2004 ), err contained in the training data set; while err s T is an error term caused by the limitation of the finite samples.
Since err c T can be extremely small, and the noise can be removed by data pre-processing, it is obvious that the perfor-samples to capture the target distribution. That is, err T obtain the probability to approach the function F E implemented by a random forest ensemble trained on the same training data set with the following equations:
However, the error rate caused by the limited learning ability may be enhanced in the generated training data set, which does not contain all possible feature vectors. That is, assuming the noise error rate can be ignored, err
If we use F TE as the function implemented by the combined model for both RF2TREE and decision tree, then the probability to approach F TE can be expressed as where According to the above justification, P F TE can be greater than either P
Therefore, it shows that the performance can be improved if we combine a decision tree with the random forest ensemble method used in RF2TREE together. The experiments described in the next section also verify this. 4. Experiment design and result analysis In this section, we evaluate the performance of the presented feature selection algorithm against other common methods.
According to the common sources of collecting emotional speech data, the data used in this work contains two speech cor-pora from different sources: (1) acted speech corpora and (2) natural speech corpora. The language spoken in both speech corpus is Chinese (Mandarin). 4.1. Data sets and original acoustic features
The first data corpus contains speech examples with acted emotions which are expressed by a group of well-selected ac-tors; while the second speech corpus contains speech examples with natural emotions recorded from the daily dialogues male, male and both) which are separated based on the gender of the speakers, and the purpose is to test the importance of gender-dependency in the emotion recognition task.

No matter what features were used in the previous work by any research group, they are all derived from a set of basic features, which will be used to represent the speech data samples in all 9 data sets as shown in Table 4 . 4.2. Evaluation of feature selection results
Following the three-stage framework as shown in Fig. 1 , once the original speech data examples are represented by the 84 reported for comparison.

The intuition of the experimental design is that: if the ERFTrees algorithm performs well, it will produce a reduced data 4.2.1. Selected features see that: save storage memory and processing time, but also make it possible for many machine learning algorithms to work efficient.
 among all the features, which is the same conclusion as mentioned in psychology studies. 11 out of 16 selected features are from these three groups; None of the Phase -related features has been selected by the algorithm;
Only three transformed features are identified as relevant by the algorithm. 4.2.2. Quality of the selected features
In order to evaluate the quality of the selected features, we compare it with the original 84 features. Two classification lected 16-feature data sets. Both classification algorithms run on each data set for 100 runs, and the average accuracy is shown in Table 7 .

In Table 7 , the numbers in bold show which feature set has better performance by comparing the emotion recognition vide better performance than the whole feature set for emotional speech recognition. 4.2.3. Gender dependency recognition performance on gender-dependent data sets is better than the performance on gender-independent data sets.
The classification accuracy on most of the data sets when considering gender dependency were improved by about 3.99% data sets (data sets 5, 6), which gave remarkable improvements of 8.28% (84-feature set) and 9.25% (16-feature set) by the RandomForests algorithm. Therefore, it has advantages to involve gender information which provides better performance on emotion recognition.

In addition, the results on the 84-feature and 16-feature data sets both imply that male and female speakers express their ers (from 57.61% to 71.22%). 4.2.4. Acted vs. natural emotions
In general, the correct recognition accuracy was higher for natural data sets (data sets 4 X 6) than for acted data sets (data sets 1 X 3). The results of recognizing natural emotions on data sets with both female and male speakers was approximately 3.18% better than the results of acted emotions. For this reason, if the available speech data instances are limited, it is possible to apply the model trained based on the acted emotional speech data to recognize natural emo-tions as well.

Furthermore, unlike the recognition performance on the data sets of acted emotions, the natural data sets gave a lower is also possible to have other emotions hidden inside. For example, people may feel both sad and fear when they do some-thing wrong that might hurt the others. Therefore, it needs more information to recognize the certain natural emotional states, contained in the speech examples than to recognize the states contained in the acted emotions. 4.2.5. Analysis of confusion matrix
Generally, the ability to recognize an individual emotional state is improved by using the subset of the selected 16 fea-tures rather than the whole acoustic feature set. The 5 confusion matrices displaying correct recognition accuracy of each diagonal are the correct recognition rate of each emotional states on the special data set.
 features, it achieved the highest correct rate of 82.54%; while the  X  X appy X  emotion performed worst, which only has 16% on such a short time. 4.2.6. Summary
In summary, the feature selection process with the presented ERFTrees algorithm, is well suited for the task of emotion recognition. A well-selected acoustic feature subset can represent the original speech data examples using fewer features but containing most of the useful available information for an emotion recognition task. The experiment results also show sification tasks. 4.3. Comparison with other methods
PCA/MDS and ISOMap are widely used dimensionality reduction methods in emotional speech recognition and related areas. In this part, we compare the performance of our presented algorithm with PCA/MDS and ISOMap . All compared algo-algorithm found 16 features, we set the lower-dimension for both PCA/MDS and the ISOMap algorithms as 16. The k-NN and the results are used for performance comparison to the presented ERFTrees algorithm. In our experiment, we tune the gives the classification results on data sets preprocessed by different algorithms.

When comparing the results from PCA/MDS and ISOMap algorithms with the original data sets, we can see that these two ing the performance of the linear dimension reduction method, PCA/MDS , and the non-linear dimension reduction method outperforms by 2%, this may come from the fact that natural speech data is more complex than acted speech data, accord-ingly the non-linear method is more suitable for these cases.
 The last column ( ERFTrees ) of the Table 9 gives the accuracy of k-NN algorithm on the data sets preprocessed by the ERF-data sets. On average, the performance has been improved 8% over those data sets preprocessed by the dimensionality reduction methods. Moreover, the comparison between the 5th column ( None ) and the 8th column ( ERFTrees ) further con-ognition accuracy. 5. Conclusion
It is no doubt that introducing advanced machine learning techniques into emotion recognition is beneficial. However, since collecting a large set of emotion speech samples is time consuming and labor intensive, machine learning algorithms (ERFTrees) is presented, which can be used to extract effective features from small data sets.

The small size of the training data sets has been identified as an important factor, impacting the learning performance decision tree algorithm and the restricted learning ability of the random forest ensemble.

A series of experiments were done on 9 emotional speech data sets where the language is Chinese (Mandarin), and 16 acoustic features were selected out from the original 84 feature set. Most of them are the basic pitch -related and inten-can provide a higher recognition accuracy than those with the original 84-feature set. The recognition performance was im-Forest classifier.
 Acknowledgements
The related work is partially supported by Deakin CRGS grant 2008. The authors would like to thank Sam Schmidt for proof reading the English of the manuscript.
 References
