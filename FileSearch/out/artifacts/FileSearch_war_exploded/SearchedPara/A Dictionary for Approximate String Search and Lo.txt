 In this paper we propose a dictionary data structure for string search with errors where the query string may differ from the expected matching string by a few edits. This data structure can also be used to find the database string with the longest common prefix with fe w errors. Specifically, with a database of n random strings, each of length of O ( m ), we show how to perform string search on a query string that dif-fers from its closest match by k editsusingadatastructure of linear size and query time equal to  X  O (log n 2log n This means that if k&lt; m log  X  O (1). This is of significant in practice as there are several applications where k is small relative to m . Our approach converts strings into bit vectors so that similar strings can map to similar bit vectors with small hamming distance. A simple reduction can be used to obtain similar results for approximate longest prefix search.
 H.3.3 [ Information Search and Retrieval ]: data struc-tures, Hashing Algorithms, Experimentation Embeddings, Hamming Space
String matching allowing errors, which is also known as approximate string matching is an integral part of an infor-mation retrieval system whose design is based on efficient access of specific information from large collections of data. There is a plethora of work in the area of efficient processing of online queries. The size of today X  X  data collections makes exact string matching very expensive. Approximate string matching allows for skips while  X  X canning X  the database for pattern matches. Another area where approximate string work done while the author was at Ebrary, Inc.
 Copyright 2006 ACM 1-59593-433-2/06/0011 ... $ 5.00. matching can be applied is where the database itself has erroneous data. On of the popular applications for approx-imate string matching is spell-checking.

In this work, we apply approximate string matching to the following queries -
In all the above queries, we allow for a specified number of errors while computing the matches. The error model, in general, depends on the application. The basic idea is to represent the error as a distance between the two strings  X  the query and the possible match  X  and return the clos-est match to the query that minimizes this distance. The edit distance isacommonmetricusedtodiscoveraclose variant of the input string. For an excellent exposition of approximate string matching, the reader is referred to [15] and references within.
 Definition 1. The edit distance between two strings S 1 and S 2 is the minimum number of edit operations required to transform S 1 to S 2 . The edit operations include insertions and deletions.

Edit distance algorithms can be specialized depending on the application. An interesting special case is the hamming distance [12].

Definition 2. A special case of the edit distance, where the strings are of equal length and the edit operations are limited to substitutions, is the hamming distance .
The basis for all problems involving string comparisons is the classical pattern matching problem wherein one searches for a pattern p in a given text t . The above defined metrics form the cornerstone of many approximate pattern matching problems.

Our approach converts strings into bit vectors so that sim-ilar strings can map to similar bit vectors with small ham-ming distance.
 Application areas -Some of the application areas of ap-proximate string matching are computational biology, signal processing, information retrieval, and pattern recognition.
In computational biology, DNA sequences can be seen large texts over an alphabet of four characters (A, T, G, C). Looking up matches to a query sequence is an impor-tant aspect of genomics. For example, an exact match could give information about the position of the gene sequence in the genome. An approximate match could give other in-formation such as number of mutations (edits) in the gene sequence. Studying differences between two DNA sequences forms the basis of many studies in evolutionary biology.
In information retrieval, spell checking is an old but im-portant application of approximate string matching. Look-ing up substrings in a large text or collection of documents often use approximate string matching to obtain the offsets of the substring or a close match in the large text. An-other important application is optical character recognition wherein the digitized texts often have 7  X  16% errors. Search-ing with errors, either in the query string or the database string, uses approximate string matching.

Other areas such as security employ approximate string matching in password checkers wherein the passwords are checked against a list of  X  X ow-security X  password strings that are deemed vulnerable to cracking by malicious users.
In this section, we present the state of the art in approx-imate string matching. We broadly classify existing litera-ture on approximate string matching into three categories. Sketch based schemes -Several studies suggest methods to map strings to bit-vectors in hamming space [2, 16]. The essential idea being that given such an embedding, tech-niques for approximate search in hamming space can be carried over to strings. The measure of the goodness of an embedding is usually referred to as distortion .Thisis also known to be the error (as a multiplicative factor) in-troduced in distances while transforming from one metric space to another. Broder [5] suggests a simple approach based on string shingling to convert strings into sets. The idea is to view a document as a set of all its substrings of a certain length. They argue that similar documents are likely to map to similar sets. Bar-Yossef et al [2] extend this technique to produce bit-vectors with theoretical guar-antees: they achieve a distortion of O ( d 3 / 7 ). Recently, this was improved by Ostrovsky et al [16] to O (2 using methods based on recursive shingling. Andoni and In-dyk [1] use locality sensitive hashing (LSH) to construct a text and c is a distortion factor and  X  O ( n 1 /c + n 1+ o (1) where M is the upper bound for the query length.
 Trie based schemes -There is a plethora of work that is uses tries (suffix trees) to store the text. The search algo-rithm is adapted to deal with possible errors using dynamic programming [19, 17]. The main advantage of tries is the search time is independent of the size of the text. Thus, they find many uses in searching large collections. The worst-case query time for a trie is O ( kn ) while the average case is O ( m ). The space complexity is linear. On the same token, one of the issues with tries is that the performance is tied to query length. Therefore large queries result in a poorer perfor-mance compared to sketch based techniques. Cole et al [7] Bloom Filter based schemes -Another data structure that has captured attention of research is the Bloom fil-ter [3]. It has been widely used in network applications [4, 8, 18]. It has been used string processing [14, 13]. All of these studies proposed the Bloom filter as a space-efficient data structure to answer membership queries. The data structure offers a succinct representation of a set and allows for membership queries with ve ry small false positives and no false negatives, i.e., the structure will never return true when querying for a member that does not exist in the set. Kirscsh and Mitzenmacher [10] propose a generalization of the Bloom filter to answer queries of the form  X  X s x close to an element of the filter? X  where closeness or similarity is defined under the hamming metric. Hamming distance does not recognize similarity of strings that are produced by a few insertions or deletions from a given string.
In this study, we analyze the problem of approximate string matching by mapping strings into the hamming space and looking at approximate matches in the hamming space. The mapping of strings to bit vectors uses shingling tech-niques due to Broder [5] that convert a string into a set of shingles. In a following step we convert the set of shingles into a bit vector. We then construct a hash table to store the strings with the bit vectors as the keys.

Our main result is a data structure that uses linear space random strings with high probability. We define the notion of random strings in the next section. We demonstrate the efficiency of our data structure by experiments on a large set of strings.
The approximate version of the string matching problem involves allowing upto k errors when computing the nearest match to a given query. We now formally define this prob-lem. Let  X  denote a finite alphabet over which the strings in the database are defined. For clarity, let a = |  X  | . For any string s  X   X   X  ,wedenotethelengthofthestringby | s | and s as the i -th character of s ,for i  X  [1 , | s | ]. A substring of s from character i to character j is denoted by s i..j .Givena database text T  X   X   X  whose length is n and a query string Q  X   X   X  whose length is m , we define the k -approximate string match as:
Definition 3. Given T , Q , and an error parameter k , return all close variants of Q in T which satisfy d ( Q, T k for 1  X  i  X  n and j  X  i ,where d ( ., . ) is the distance function. In this study, we use the edit distance as a measure of string similarity.

In our definition of this problem, we have a dictionary D of d terms, a query Q asks for all terms within a hamming distance k of Q . Thus, for large texts where n&gt;&gt;m , T canbedecomposedintoasetof d strings and stored in a dictionary which can then be queried.

A different formulation of approximate string matching problem, that often appears in literature, deals with large static texts such as books wherein the goal is to find all locations I in T such that d ( Q, T i..i + m )  X  k for all i In such cases the entire text can be pre-processed to build an index to support efficient dictionary queries. In fact, using the same process described above, this problem can be reduced to the approximate matching problem by breaking up the text into smaller length shingles and treating each shingle as a term in the dictionary. Known efficient methods preprocess the text in O ( n log n ) time and answer queries in time O ( m log log n + r )time.

In this study, we define a data structure that can be used to solve both the above problems. We describe our data structure in Section 3.
The k -approximate longest prefix match is an approxima-tion to the longest common prefix problem. Here, we allow upto k errors when matching the longest prefix common to both database term(s) and the query. Under the same conditions defined in Section 2.1. we define k -approximate longest prefix match as -
Definition 4. Given T , Q , and an error parameter k , return longest common prefix of Q and T which satisfy the distance function. Here Q 1 .. 1 . 1 i denotes the prefix of Q with length 1 . 1 i . Again, we use the edit distance as the similarity measure between two strings.

Basically, we compute the approximate match of all valid prefixes of the input query with the prefixes of the database string. The valid prefixes are pre-computed and stored in different hash tables, one table for each prefix length. To reduce the overhead of storing prefixes of all possible lengths, we store only prefixes of lengths that are powers of 1 . 1. The approximate longest common prefix is the largest index j is dependent on the allowable error in the closest match computation. If the error is small, the power is small. For example, with a value of 1 . 1, the algorithm truncates no more than 10% of the characters in a string.
In this section, we define the data structure for the k -approximate string match. We will show how this data structure can be extended to solve a natural relaxed ver-sion of the k -approximate longest prefix match problem.
First, if the query is of length d , we can assume that all strings are of length d  X  k . Any solution to such a problem can be easily extended to database strings of ar-bitrary length by simply partitioning strings by rounding their lengths to a nearest multiple of 2 k and searching in at most two partitions. We will detail this later in this section. First, we present the data structure for the specific case of k -approximate string match for equal length database strings and queries.
To simplify the analysis, we consider all strings to be ap-proximately of the same length. Later we show that it is simple to extend the analysis to strings of varying lengths. Let all database strings and the query string to be of length d  X  k . The key step in this algorithm is to map database strings to bit-vectors of length approximately 2 d . The map-ping uses the well-known embedding of edit distance metric to the hamming metric [5, 11]. The mapping defined by Broder in [5] involves a simple computation using document shingles that is easy to implement in practice. Bar Yossef et. al. [2] showed how such a shingling method can be used to achieve an embedding to hamming space with distortion at most O ( d 3 / 7 ). Ostrovsky and Rabani [16], on the other hand, give a lower distortion of 2 O ( algorithms are complex.

Mapping from strings to bit vectors -The mapping from strings of length approximately d is composed of two steps.

Mapping strings to set of shingles -The first step in the mapping scheme involves reducing the string to a set of shin-gles W . This is similar to the method proposed by Broder [5]. Each shingle is a substring of length w of the input string. The length of the shingle should be chosen with care. The length w has a direct consequence on the ex-pected number of unique shingles in a string. To ensure random shingles in expectation, we need to choose w such that a w &gt;&gt; l ,where l is the length of the resulting bit vec-tor and a is the size of the alphabet. We denote this bit vector as the sketch of a given string. In this study, we set l =2 d . Thus, the length of a shingle is w  X  log a d .
Embedding a set of shingles into the hamming space -The second step involves mapping the shingle set to bit-vector of size  X d ,where  X   X  2. The length of the bit-vector is derived using a Bloom filter technique [3, 6] as follows.

Initially, the bit vector is initialized with all bits set to 0. We then use a hash function that maps a given shingle into an index in the bit vector. Thus, the range of the hash functionsis[ l ]where l is the length of the bit vector. We now present a simple analysis of computing the size of bit vector.

We set the length to be such that each bit takes a value 0 or 1 with probability 1 / 2. Lemma1givesalength d ln 2 .
Finally, we sample b  X  log n bits from the bit-vector to generate the mapping of the given string to hamming vectors of size  X  log n bits. We define map( S ) as the sketch of string S resulting from the mapping scheme.

Lemma 1. Assuming a string s has y distinct shingles, for a bit-vector of length x = y ln 2 ,the map( s ) takes a random value  X  X  0 , 1 } x .

Proof. Assuming there are y distinct shingles, we set thesizeofthebit-vector x n such a way that the bit-vector takes a random value  X  X  0 , 1 } x . For this the probability of turning on a bit equal to (1  X  1 x ) y must be 1 / 2giving
Note that for a random string, with high probability, the number of distinct shingles is concentrated close to d  X  w for w&gt; log a d +  X (1).

Lemma 2. For two strings S 1 , S 2 , E [ H ( map ( S 1 ) ,map ( S 2 ED ( ., . ) is the edit distance between the two given strings.
Proof. Let e = ED ( S 1 ,S 2 ). Each edit results in w shin-gles being different. So the symmetric difference of shingle sets W 1 and W 2 is at most 2 ew elements. We know that since each shingle affects at most one bit in the bit vector, the hamming distance is also not more than the symmetric dif-ference. Since the mapping from sets to bit vectors preserves the set differences in expectation, we have the proof.
In the rest of the analysis, we will round  X  to 2. As we can see in the above result, a lower value of w is better. However, too small a value of w will restrict the size of the set to the size of the alphabet making the bit vector sparse. Our objective is to ensure that a random string of length d results in a random bit vector of length d ln 2 . To this end, w needs to be set in such a way where all the shingles are nearly distinct for a random string. For an alphabet of size a , setting w =log a d + c will ensure this, where c is a small integral non-zero constant.

Data structure -Lemma 2 implies the following data structure. Map all strings to bit-vectors of length b .Con-struct a hash table storing the original string and indexed by its sketch. To search a given query Q , we compute its map, map( Q ), and search it in the table. Although, it is not necessary that map( Q ) coincides with the map of its near-est neighbor, lemma 2 implies that they differ in bits kwb expectation. So, if the entry corresponding to map( Q )does not contain a nearby string, we search in all buckets close to map( Q ), i.e., all bit-vectors that differ from map( Q )in at most kwb d bits.

Instead of storing the entire string in the table, we can store again a B -bit vector sketch of the string. The sketch is computed using the same mapping scheme. Instead of sampling b bits, we sample b + B bits. We then use b bits as a key to lookup the table and the B bits for comparing the strings. Let us note that having a sketch of size B produces a collision probability of 1 2 B . Thus, choosing B =8sets the probability of collision to 1 / 256. Note that we can also compute the length of B as a function of the number of strings, n , in each range, say, log n . In this study, we set the value of B to a constant. A comparison between the query and a potential match is set to be close if they differ in at most kwb d bits as implied by lemma 2. The space complexity of the data structure is clearly linear.

Query time complexity -Since we search in all entries that differ from map( Q )inatmost kwb d bits, the number of entries searched is less than b kwb d . In particular, if kwb &lt; d ,thequerytimeis  X  O (1).
For a tighter and more precise analysis, we assume the strings are random. The above discussion observed that d is expected number of bits on which map( Q )andthe map of its nearest neighbor differed. It is possible that the actual difference is greater than this. But we show that the probability of it being much greater is small. Formally, the probability that it differs by more than twice the above difference is at most 1 / 2 by Markov X  X  inequality. So, by using O (log n ) independent instances of the data structure, the algorithm can be made to succeed with high probability.
Also, in the query time complexity computation, we as-sumed that the number of entries in a bucket is constant. Again, this may not be true with a small probability. Since each string (assuming the d  X  w shingles are nearly distinct) maps to a random bit-vector, the expected number of en-tries per bucket is less than 1. Again, by Markov X  X  inequal-ity, with probability 1 / 2, the bucket containing the nearest neighbor has at most two entries. So, if we limit our search to only two entries per buckets, then again, with O (log n ) instances of the data structure, we succeed with high prob-ability. We summarize with the following theorem.

Theorem 1. Assuming strings are random, that is the d  X  w shingles are near distinct, our k -approximate string match can be implemented using a near linear size data high probability.

Note that the length of a shingle is w =log a d ,where d is the length of the input string and a is the size of the alphabet.
We now summarize the algorithms for approximate string matching. The input to the algorithm is a set of strings, S , extracted from a large text or collections of documents, thesizeofashingle, w , in a string, and the size of sketch, B , of each string. The strings are assumed to be fairly ran-dom and non-repetitive, that is, no string generates a shin-gle more than once. Informally, our algorithm computes a sketch for each string and uses the sketch as a key in a hash table. The length of the sketch generated depends on the length of the string. Specifically, we group strings by their lengths in intervals of size 2 k where k is the maximum al-lowable error in finding closest matches. Specifically, we add astring s toagroup i where the index of the group is com-puted from | s | / 2 k . We use index( s ) to denote this grouping operation, where s  X  X  and index( . ) returns the group id given a string. In this study, we use the hamming distance to represent the error between a potential match in the hash table and the query string. During querying, we similarly compute the sketch of a query and use the sketch to lookup the appropriate hash table for a match. The hash table to search is indexed by i such that the length of the query is in the range [2 ki, 2 k ( i +1)). The number of ranges is dependent on the maximum length of a string in the database. With-out loss of generality, let us assume the number of groups to be G . This number is computed from | s max | / 2 k where s max is the string with maximum length. Each group is represented by a hash table, H i , that is indexed by keys of length log N i + c ,where N i are the number of strings in the range. Using the extra c bits ensures that the database of N i strings does not result in all possible combinations of bit vector of that length. During the pre-processing step, we partition the database strings into appropriate ranges and keep a count of strings in each range. We store the counts in an array count.

We now present the algorithms for constructing the dic-tionary data structure.

Remark 4.1. The query algorithm takes in a query q , the shingle width w , and the allowable error k in finding a match. If the computed key for the query does not exist in the appropriate hash table, we flip bits in the key for 1 differ from the query key in at most i bits. One way to do this is to try all the x = mentation is to try random i -bit flip where each bit is flipped x times with probability 1 / 2 . A coupon collection argument would imply that doing such random flips x log x times cov-ers all combinations with high probability. In reality, with kx trials, the probability of missing it is at most e  X  k Algorithm 1 ApproxStringInsert( S ,w,B ) 1: for s  X  X  do 2: count[index( s )] + + { count # of string in each group 3: end for 4: for i =1to G do 5: N i  X  count[ i ] 6: end for 7: for all s  X  X  do 8: d  X  X  s | 9: i  X  index( s ) 10: produce a shingle set W ( s )ofsize d  X  w { use shingles 11: bitvector( s )  X  hash( W ) {  X d number of bits } 12: key( s )  X  sample b (= log N i + c )bits 13: value( s )  X  sample B bits 14: add entry key( s ) , value( s ) in H i 15: end for Algorithm 2 ApproxStringQuery( q, k, w ) 1: d  X  X  q | 2: i  X  index( q ) 3: produce a shingle set W of size d  X  w 4: bitvector( q )  X  hash( q ) 5: key( q )  X  sample b (= log N i + c )bits 6: value( q )  X  sample B bits 7: sketch  X  X  i (key( q )). 8: if H (value( q ) ,sketch )  X  k then 9: return value 10: else 11: for i =1to kwb d do 12: ntrials  X  13: for j =1to ntrials do 14: flip upto i bits in all 15: sketch  X  X  i (key( q )). 16: if H (value( q ) ,sketch )  X  k then 17: return value 18: end if 19: revert the flipped bits 20: end for 21: end for 22: end if 23: return failure
We use the dictionary for approximate string matching to compute a k -approximate longest prefix match between a given string and the database strings. Instead of storing all possible prefixes for a given a string, we keep a sketch of a string X  X  prefixes by truncating a prefix to its nearest power of 1 . 1. Thus, a prefix of length 2 d is rounded to length 1 . 1 log 1 . 1 2 d by truncating the extra characters. A dictio-nary is then created for each prefix length and all prefixes of the same length are stored in the appropriate dictionary. To find the approximate longest common prefix, we compute the prefixes of the query Q and starting with the longest prefix, we find the closest match for each prefix using the approximate string matching algorithm.

The memory required to compute the approximate longest common prefix is O ( n log 1 . 1 d ). The running time is longer by a factor of O (log 1 . 1 d ). One of the disadvantages of this approach is that most of the k allowed edits are in the trailing characters that can be truncated if the length of database string and/or the query are not powers of 1 . 1. However, the number of truncated characters is never more than 10% of the prefix length. Thus, given a prefix of length 2 d bits, the truncated length is no less than 2 d  X   X  ,where  X  =2 d  X  1 . 1 log 1 . 1 2 d in the number of truncated characters. Assuming random strings and uniform selection of the log n bits that form the map( . ) of the given prefix, we loose no more than k  X  2 d edits to truncation. In fact, our experiments show that the information loss is minimal in reality.
In this section, we illustrate the efficiency of the proposed dictionary data structure in approximate string matching. We test the algorithms with different values of parameters  X  X eysizevariable c , size of string sketch B , error k ,length of queries d , number of strings in the database n  X  X ndtheir effect on query times, accuracy of a match, number of hash table lookups, and memory.
We set the maximum length of a string to be indexed in the dictionary to be 128 characters. This limits the number of index ranges to 20. We assume all strings are at least of length 6 characters. Furthermore, we group all strings 10 characters and less into one index range. This range can be handled separately by small string approximation algorithms such as those based on dynamic programming. For the rest of this section, we do not deal with small strings. The shingle size w issetto3. Thesizeofabit-vectortomap d  X  3 shingles of a string of length d is set to 6 i +10 the index number. The hash( s ) function is a simple function for hashing strings based on Horner X  X  rule.

In the querying phase, to make sure we have enough num-ber of bits to flip in case of not finding a search key in the hash table, we set the number of flips to 1 . 5 kwb d +1. The number of trials for each set of flips is set to 2 thenumberofbitstoflip. Wenotethatatnotimewehad to flip more than three bits to find an approximate match. The strings to be inserted in the dictionary were randomly generated from a set of 115000 unique words by concate-nating a randomly choosing a specified number of words. We set this number to 5. The number of strings was also parametrized and set to a million for our experiments. Set-ting the number of words to 5 generated strings of lengths varying from 17 to 74 with a mean length of 42.

Note that using small number of lookups (  X  2), the prob-ability of finding a n earest neighbor in a bucket is 1 / 2. In order to increase the success probability to say 90%, one way would be to increase c and hence the number of lookups into the table. The expected number of lookups is dependent on the length of the key b as number of lookups (exponential in c ). Another approach would be to work on multiple copies of the data structure, upto log n copies in fact. However, in reality we could do with lot lesser number of copies. For a 90% success prob-ability, we would need 4 copies of the data structure, since (1  X  0 . 5 4 ) &gt; 0 . 9.
Inthefirstsetofexperimentsweanalyzethetimeand space requirements of our data structure. We compare the memory taken by our data structure to a compressed trie. We insert a unique list of 1000000 strings with an average length of 42 characters. The compressed Trie took about 272M of memory to store all the phrases while the dictionary took 199M -about 36 . 7% reduction. This memory size was obtained with c =3bitsand B =20bits. Figure1shows the running times for querying the dictionary for different values of the error parameter. Clearly, the running time increases with the number of errors in the query string. As Figure 1 shows, the algorithm performs very well in terms of running time as it takes only 0 . 22 ms to find an approximate match for a query averaging 42 characters in length. The response times shown in Figure 1 were averaged over 10000 queries on a dictionary containing 1000000 strings.
Analogous to the r unning time is the number of lookups a query takes to return an approximate match. Our simula-tion results show that the number of lookups for approx-imate string search is small. For a database of 1000000 strings of length about 42 characters each, successful queries with at most 4 errors can be performed in under 15 mem-ory lookups. At the same time the success rate of finding a  X  X orrect X  approximate match for the input query with at most 4 errors is 48 . 2%. This success rate can be increased significantly by using multiple copies of the data structure and running the query independently on each of the copies. For example, by running the query on 4 copies of the data structure, we saw the success rate increase to 86 . 0%. In the following analysis, we highlight our observations in more de-tail.

An important metric to measure is the number of lookups as a function of the error. We vary the error from 1 to 5 for the query strings for different values of B .Weset c =3 for this run. Figure 2 shows that the number of lookups in-creases with the error for B = 30 and 40. This is because the number of flips in the query algorithm is increasing. Again, the number of strings in the database is 1000000. Further-more, the number of trials are proportional to the parameter B . Figure 2 illustrates this behavior. The parameter B was varied from 30 to 40. In general, for a fixes error param-eter k , the number of lookups increase as we increase the parameter B . In this set of experiments, we set the number of copies of the data structure to 1.
In another sets of experiments we illustrate the number of lookups performed as a function of the parameters c and the length of query strings d . The number of lookups was averaged over a 10000 queries. Figure 3 illustrates the fact the number of lookups is exponential in c .Wefix B =30 and k = 2. The number of copies of the data structure is set to 1.

Clearly, as Figure 3 shows, the number of lookups in-creases from nearly 1 to very large (  X  1800) for c = 12. This motivates us to consider the alternative of using copies of the data structure and using small values of c to achieve the desired success rate.

Figure 4 shows the affect the number of copies of the data structure has on the query accuracy which is expected as the query outcome for each data structure is independent. The failure probability drops exponentially with the number of copies. We fix B =30and c = 3. From the figure, we can see that using just 3 copies will give us an error probability of around 10%.

The parameters c and B are very important to the per-formance of our data structure. Figures 5 and 6 show the failure probability as a function of these parameters. The number of copies is set to 1. The error value was changed from1to6. Inthefirstexperiment B = 30 while in the second experiment c = 3. Figure 5 shows that for any fixed value of c , the failure probability decreases with the error pa-rameter. For any fixed k , the failure probability decreases with increase in c and this is more pronounced for larger values of k . For small errors, the number of extra bits used in the sketch do not have have the same impact as they do for large errors. This has a consequence on how much the failure probability varies for different values of c .
Figure 6 shows a similar correlation between the sketch size B and the failure probability as one found between the number of extra bits and the failure probability. In fact, both B and c contribute toward the total number of bits used to represent a key in our dictionary. Hence it is not surprising we observe similar behavior w.r.t failure proba-bility when one of the parameters is changed. Figure 4: Failure probability vs. Number of copies
An interesting observation suggesting the use of multiple copies of the data structure instead of larger sketch sizes is the fact that with each independent run, the number of lookups, and hence the query times, decrease significantly as most of the work seems to be done in the earlier runs. Note that we stop querying the dictionary in the following runs once we have a successful hit for a query. At the same time, the success rate increases significantly as the number of trials are increased. Figure 7 illustrates the above observations as the trials progress. In this run, the number of errors was set to 3, B = 20, and c = 3. The times were averaged over 10000 queries. Observe that the response time falls significantly by round 5 while at the same time the success rate increases to almost 92%.

Finally we studied the effect of the query length on the number of lookups. Intuitively, it seems that the number of lookups should increase with the query length. This is, how-ever, not the case. Initially, as the query length increases, so do the number of lookups. For queries of larger lengths, as our analysis in Section 3 suggests, we need fewer lookups to find a match. Finally, we studied the effect of using multiple copies of the dictionary on the failure probability. Clearly this decreases with the number of copies uemployed. Fig-ure 8 illustrates this trend.
We proposed a dictionary data structure for string search with errors where the query s tring may differ from the ex-pected matching string by a few edits. This data structure can also be used to find the database string with the longest common prefix with few errors. F or few errors, our methods produce very few lookups, especially if the query strings are large. [1] Alexandr Andoni and Piotr Indyk. Efficient algorithms [2] Ziv Bar-Yossef, T. S. Jayram, Robert Krauthgamer, Figure 7: Effect of the number of trials on the suc-cessrateandlookuptime Figure 8: Effect of the query length on the number of lookups [3] Burton H. Bloom. Space/time trade-offs in hash [4] A. Broder and M. Mitzenmacher. Network [5] Andrei Broder. On the resemblance and containment [6] Andrei Z. Broder, Moses Charikar, Alan M. Frieze, [7] Richard Cole, Lee-Ad Gottlieb, and Moshe [8] Cristian Estan and George Varghese. New directions [9] Piotr Indyk and Rajeev Motwani. Approximate [10] Adam Kirsch and Michael Mitzenmacher.
 [11] Eyal Kushilevitz, Rafail Ostrovsky, and Yuval Rabani. [12] Gad M. Landau and Uzi Vishkin. Efficient string [13] M. D. McIllroy. Development of a spelling list. IEEE [14] James K. Mullin and Daniel J. Margoliash. A tale of [15] Gonzalo Navarro. A guided tour to approximate string [16] Rafail Ostrovsky and Yuval Rabani. Low distortion [17] H. Shang and T. H. Merrettal. Tries for approximate [18] Ion Stoica, Robert Morris, David Karger, Frans [19] E. Ukkonnen. Finding approximate pattern in strings.
