 University of Sussex
Techniques that exploit knowledge of distributional similarity between words have been proposed in many areas of Natural Language Processing. For example, in language modeling, the sparse proposed. Within this framework, the problem of finding distributionally similar words is cast with the way they are measured in document retrieval. As will be shown, a number of popular existing measures of distributional similarity are simulated with parameter settings within the asymmetric relationship? Second, are some co-occurrences inherently more salient than others the extent to which each word occurs in each co-occurrence type? varying the parameters within the CR framework rather than using other existing distributional and therefore also in lexical distributional similarity. 1. Introduction
Over recent years, approaches to a broad range of natural language processing (NLP) applications have been proposed that require knowledge about the similarity of words.
The application areas in which these approaches have been proposed range from speech recognition and parse selection to information retrieval (IR) and natural language generation. For example, language models that incorporate substantial lexical knowl-plausible combinations of events are not seen in corpus data. Brown et al. (1992) report that one can expect 14.7% of the word triples in any new English text to be unseen in a training corpus of 366 million English words. In our own experiments with grammatical relation data extracted by a Robust Accurate Statistical Parser (RASP) (Briscoe and
Carroll 1995; Carroll and Briscoe 1996) from the British National Corpus (BNC), we found that 14% of noun-verb direct-object co-occurrence tokens and 49% of noun-verb direct-object co-occurrence types in one half of the data set were not seen in the other utterance impossible.

Markovitch 1993; Pereira, Tishby, and Lee 1993; Dagan, Lee, and Pereira 1999) provides an intuitively appealing approach to language modeling. In order to estimate the prob-ability of an unseen co-occurrence of events, estimates based on seen occurrences of similar events can be combined. For example, in a speech recognition task, we might predict that cat is a more likely subject of growl than the word cap , even though neither co-occurrence has been seen before, based on the fact that cat is  X  X imilar X  to words that do occur as the subject of growl (e.g., dog and tiger ), whereas cap is not. their semantic similarity, e.g., the components of meaning they share by virtue of both being carnivorous four-legged mammals? Or are we referring to their distributional similarity, e.g., in keeping with the Firthian tradition, occur as the arguments of the same verbs (e.g., eat , feed , sleep ) and tend to be modified by the same adjectives (e.g., hungry and playful ).
 might be usefully retrieved that use synonymous terms or terms subsuming those speci-fied in a user X  X  query (Xu and Croft 1996). In natural language generation (including text simplification), possible words for a concept should be similar in meaning rather than just in syntactic or distributional behavior. In these application areas, distributional sim-ilarity can be taken to be an approximation to semantic similarity. The underlying idea is based largely on the central claim of the distributional hypothesis (Harris 1968), that is: ilarity has given rise to a large body of work on automatic thesaurus generation (Hindle 1990; Grefenstette 1994; Lin 1998a; Curran and Moens 2002; Kilgarriff 2003). There are inherent problems in evaluating automatic thesaurus extraction techniques, and much research assumes a gold standard that does not exist (see Kilgarriff [2003] and
Weeds [2003] for more discussion of this). A further problem for distributional similarity methods for automatic thesaurus generation is that they do not offer any obvious way to distinguish between linguistic relations such as synonymy, antonymy, and hyponymy (see Caraballo [1999] and Lin et al. [2003] for work on this). Thus, one may question 440 manually constructed thesauri (e.g., WordNet [Fellbaum 1998], Roget X  X  [Roget 1911], the
Macquarie [Bernard 1990] and Moby 2 ). Automatic techniques give us the opportunity to model language change over time or across domains and genres. McCarthy et al. (2004) investigate using distributional similarity methods to find predominant word senses within a corpus, making it possible to tailor an existing resource (WordNet) to specific domains. For example, in the computing domain, the word worm is more likely
This domain knowledge will be reflected in a thesaurus automatically generated from a computing-specific corpus, which will show increased similarity between worm and virus and reduced similarity between worm and caterpillar .
 prepositional phrase attachment ambiguity resolution, it is necessary to decide whether the prepositional phrase attaches to the verb or the noun as in the examples (1) and (2). 1. Mary ((visited their cottage) with her brother). 2. Mary (visited (their cottage with a thatched roof)).
 events (the verb, the object, the preposition, and the prepositional object). However, a statistical model built on the basis of four lexical events must cope with extremely sparse data. One approach (Resnik 1993; Li and Abe 1998; Clark and Weir 2000) is to induce probability distributions over semantic classes rather than lexical items. For example, a cottage is a type of building and a brother is a type of person , and so the co-occurrence of any type of building and any type of person might increase the probability that the PP in example (1) attaches to the verb.
 are induced need to be semantic or whether they could be purely distributional. If we know that two words tend to behave the same way with respect to prepositional phrase attachment, does it matter whether they mean similar things? Other arguments for using semantic classes over distributional classes can similarly be disputed (Weeds 2003). For example, it is not necessary for a class of objects to have a name or symbolic label for us to know that the objects are similar and to exploit that information. Distri-butional classes do conflate word senses, but in a task such as PP-attachment ambiguity resolution, we are unlikely to be working with sense-tagged examples and therefore it is for word forms that we will wish to estimate probabilities of different attachments.
Finally, distributional classes may be over-fitted to a specific corpus, but this may be beneficial to the extent that the over-fitting reflects a specific domain or dialect. similarity may perform as well on this task as those based on semantic similarity. Li (2002) shows that using a fairly small corpus (126,084 sentences from the Wall
Street Journal ) and a distributional similarity technique, it is possible to outperform a state-of-the-art, WordNet-based technique in terms of accuracy, although not in terms of coverage. Pantel and Lin (2000) report performance of 84.3% using an unsuper-vised approach to prepositional phrase attachment based on distributional similarity techniques. This significantly outperforms previous unsupervised techniques and is drawing close to the state-of-the-art supervised techniques (88.2%).
 to formulate it. As we have said, two words are distributionally similar if they appear in similar contexts. We therefore need to consider what is meant by context . For example, same sentence, the same document, or the same grammatical dependency relation. The effect of the type of context used is discussed by Kilgarriff and Yallop (2000). They show that the use of sentence-level and document-level context leads to  X  X ooser X  thesauri more akin to Roget X  X  , whereas the use of grammatical dependency relation level context leads to  X  X ighter X  thesauri more akin to WordNet. The use of grammatical dependency similar words to those that are plausibly inter-substitutable (Church et al. 1994), giving us the following definition of distributional similarity: butional similarity and semantic similarity, since semantic similarity can be thought of as the degree of synonymy that exists between two words, where synonymy is defined (Church et al. 1994) as follows: such as synonymy , antonymy and hyponymy that might be found in a tighter thesaurus such as WordNet. Hence, the proposed framework is based on the concept of substi-tutability, and we use grammatical dependency relations as context. However, since the framework is based on features , there is no reason why someone wishing to find topical relationships between words, as might be found in Roget X  X  , could not use the framework. We simply do not repeat the earlier work of Kilgarriff and Yallop (2000).
 1. Is lexical substitutability and therefore distributional similarity 2. Are all contexts equally important? For example, some verbs, e.g., have and 442 3. Is it necessary to consider the difference in extent to which each word proach to evaluation that is based on the assumption that we want to know which words are distributionally similar because particular applications can make use of this information.
 with high performance in another application area (Weeds and Weir 2003a). Thus, it is not clear that the same characteristics that make a distributional similarity measure useful in one application will make it useful in another. For example, with regard to the question about symmetry, in some applications we may prefer a word A that can be substituted for word B in all of the contexts in which B occurs. In other applications, we may prefer a word A that can be substituted for word B in all of the contexts in which A occurs. For example, asked for a semantically related word to dog ,wemightsay animal , since animal can generally be used in place of dog , whereas we might be less likely to say dog for animal ,since dog cannot generally be used in place of animal . This preference in the direction of the relationship between the two words is not necessarily maintained when one considers language modeling in the face of sparse data. If we want to learn what other contexts animal can occur in, we might look at the co-occurrences of words such as dog , since we know that dog can generally be replaced by animal .Ifwewantto learn what other contexts dog can occur in, we are less likely to look at the co-occurrences of animal , since we know that animal can occur in contexts in which dog cannot. measure, or propose using a radically different distributional similarity measure in each possible application, we propose a flexible, parameterized framework for calculating distributional similarity (Section 2). Within this framework, we cast the problem of finding distributionally similar words as one of co-occurrence retrieval (CR), for which we can measure precision and recall by analogy with the way that they are measured in document retrieval . Different models within this framework allow us to investigate how frequency information is incorporated into the distributional similarity measure.
Different parameter settings within each model allow us to investigate asymmetry in similarity. In Section 3 we discuss the data and the neighbor set comparison tech-nique used throughout our empirical work. In Section 4 we discuss a number of existing distributional similarity measures and discuss the extent to which these can be simulated by settings within the CR framework. In Section 5 we evaluate the CR framework on a semantic task (WordNet prediction) and on a language modeling task (pseudo-disambiguation). 2. Co-occurrence Retrieval
In this section, we present a flexible framework for distributional similarity. This frame-work directly defines a similarity function, does not require smoothing of the base language model, and allows us to systematically explore the questions about similarity raised in Section 1. In our approach, similarity between words is viewed as a measure of how appropriate it is to use one word (or its distribution) in place of the other. Like relative entropy (Cover and Thomas 1991), it is inherently asymmetric, since we can measure how appropriate it is to use word A instead of word B separately from how appropriate it is to use word B instead of word A.
 or co-occurrence types and these co-occurrence types have associated frequencies that may be used to form probability estimates. Throughout our discussion, the word for which we are finding neighbors will be referred to as the target word .Ifweare computing the similarity between the target word and another word, then the second word is a potential neighbor of the target word. A target word X  X  nearest neighbors are the potential neighbors that have the highest similarity with the target word. 2.1 Basic Concepts
Let us imagine that we have formed descriptions of each word in terms of the other words with which they co-occur in various specified grammatical relations in some corpus. For example, the noun cat might have the co-occurrence types dobj-of, feed and ncmod-by, hungry . Now let us imagine that we have lost (or accidentally deleted) the description for word w 2 , but before this happened we had noticed that the description of word w 2 was very similar to that of word w 1 . For example, the noun dog might also have the co-occurrence types dobj-of, feed and ncmod-by, hungry . Hence, we decide that we can use the description of word w 1 instead of the description of word w are hopeful that nobody will notice. How well we do will depend on the validity of substituting w 1 for w 2 , or, in other words, the similarity between w analogy with information retrieval, where there is a set of documents that we would like to retrieve and a set of documents that we do retrieve, we have a scenario where there is a set of co-occurrences that we would like to retrieve, the co-occurrences of w and a set of co-occurrences that we have retrieved, the co-occurrences of w the analogy, we can measure how well we have done in terms of precision and recall, where precision tells us how much of what was retrieved was correct and recall tells us how much of what we wanted to retrieve was retrieved.
 occurrence retrieval. As the distribution of word B moves away from being identical precision, but B may remain a high-recall neighbor. For example, we might expect the noun animal to be a high-recall neighbor of the noun dog . When B does not occur in contexts that A does occur in, the result is a loss of recall but B may remain a high-precision neighbor. For example, we might expect the noun dog to be a high-precision neighbor of the noun animal . We can explore the merits of symmetry and asymmetry in a similarity measure by varying the relative importance attached to precision and recall. This was the first question posed about distributional similarity in Section 1. retrieval model (CRM). Additive models are based on the Boolean concept of two objects either sharing or not sharing a particular feature (where objects are words and features are co-occurrence types). Difference-weighted models incorporate the differ-ence in extent to which each word has each feature. Exploring the two types of models, both defined on the same concepts of precision and recall, allows us to investigate the third question posed in Section 1: Is a shared context worth the same, regardless of the difference in the extent to which each word appears in that context? 444 ent weight functions within each type of model. Weight functions decide which co-occurrence types are features of a word and determine the relative importance of features. In previous work (Weeds and Weir 2003b), we experimented with weight functions based on combinatorial, probabilistic, and mutual information (MI). These allow us to define type-based, token-based, and MI-based CRMs, respectively. This work extends the previous work by also considering weighted mutual information (Fontenelle et al. 1994), and an approximation to the log-likelihood ratio (Manning and
Sch  X  utze 1999) as weight functions. 2.2 Additive Models occurrence retrieval, we now formulate this formally in terms of an additive model. trieved, or predicted, by it and, conversely, required in a description of it. We will refer to these co-occurrence types as the features of w , F ( w ): where D ( w , c ) is the weight associated with word w and co-occurrence type c . Possible weight functions will be described in Section 2.3.
 Positives, TP ( w 1 , w 2 ), which will be abbreviated to TP in the rest of this article: are shared by both words, where each feature is weighted by its relative importance according to w 1 : are shared by both words, where each feature is weighted by its relative importance according to w 2 : w  X  X  retrieval of w 2 is equal to the precision of w 2  X  X  retrieval of w
P add ( w 2.3 Weight Functions occurrences of w 1 and w 2 are important enough to be considered part of their descrip-tion, or by analogy with document retrieval, which co-occurrences we want to retrieve for w 2 and which co-occurrences we have retrieved using the description of w then used to weight contexts by their importance. In the latter case, D ( w retrieval process X  X  perceived relevance of co-occurrence type c ,and D ( w actual relevance of co-occurrence type c . The weight functions we have considered so far are summarized in Table 1. Each weight function can be used to define its own CRM, which we will now discuss in more detail.

Additive type-based CRM (D type ). In this CRM, the precision of w proportion of co-occurrence types occurring with w 1 that also occur with w recall of w 1  X  X  retrieval of w 2 is the proportion of verb co-occurrence types (or distinct verbs) occurring with w 2 that also occur with w 1 . In this case, the summed values of D are always 1, and hence the expressions for precision and recall can be simplified: 446
Additive token-based CRM (D tok ). In this CRM, the precision of w proportion of co-occurrence tokens occurring with w 1 that also occur with w recall of w 1  X  X  retrieval of w 2 is the proportion of co-occurrence tokens occurring with w that also occur with w 1 . Hence, words have the same features as in the type-based CRM, but each feature is given a weight based on its probability of occurrence. Since F ( w ) = { c : D ( w , c ) &gt; 0 } = { c : P ( c | w ) &gt; 0 } , it follows that the expressions for precision and recall can be simplified: Additive MI-based CRM (D mi ). Using pointwise mutual information (MI) (Church and
Hanks 1989) as the weight function means that a co-occurrence c is considered a feature of word n if the probability of their co-occurrence is greater than would be expected if words occurred independently. In addition, more informative co-occurrences contribute more to the sums in the calculation of precision and recall and hence have more weight. Additive WMI-based CRM (D wmi ). Weighted mutual information (WMI) (Fung and
McKeown 1997) has been proposed as an alternative to MI, particularly when MI might assigned to low-probability events.
 statistical test that has been proposed for collocation analysis. It measures the (signed) difference between the observed probability of co-occurrence and the expected prob-ability of co-occurrence, as would be observed if words occurred independently. The difference is divided by the standard deviation in the observed distribution. Similarly to MI, this score obviously gives more weight to co-occurrences that occur more than would be expected, and its use as the weight function results in any co-occurrences that occur less than would be expected being ignored.
 to the t-test. However, using the z-test, the (signed) difference between the observed probability of co-occurrence and the expected probability of co-occurrence is divided by the standard deviation in the expected distribution.
 frequencies of co-occurrences and individual words occurring under the null hypoth-esis, that words occur independently, and under the alternative hypothesis, that they do not.
 of w occurring in any context, f ( c ) is the total frequency of c occurring with any word, and N is the grand total of co-occurrences, then the log-likelihood ratio can be written: which we term the ALLR weight function. We use an approximation because the terms that represent the probabilities of the other contexts (i.e., seeing f ( c ) hypothesis) tend towards  X  X  X  as N increases (since the probabilities tend towards zero).
Since N is very large in our experiments (approximately 2,000,000), we found that using the full formula led to many weights being undefined. Further, since in this case the probability of seeing other contexts will be approximately equal under each hypothesis, it is a reasonable approximation to make.
 tion is that it is always positive, since the observed distribution is always more probable than the hypothesized distribution. All of the other weight functions assign a zero or negative weight to co-occurrence types that do not occur with a given word and thus these zero frequency co-occurrence types are never selected as features.
This is advantageous in the computation of similarity, since computing the sums the words is (1) very computationally expensive and (2) due to their vast number, the effect of these zero frequency co-occurrence types tends to outweigh the effect of those co-occurrence types that have actually occurred. Giving such weight to these shared non-occurrences seems unintuitive and has been shown by Lee (1999) to be undesirable in the calculation of distributional similarity. Hence, when using the 448
ALLR as the weight function, we use the additional restriction that P ( c , w ) &gt; 0 when selecting features. 2.4 Difference-Weighted Models
In additive models, no distinction is made between features that have occurred to the same extent with each word and features that have occurred to different extents with each word. For example, if two words have the same features, they are considered identical, regardless of whether the feature occurs with the same probability with each word or not. Here, we define a type of model that allows us to capture the difference in the extent to which each word has each feature.
 extent to which w 1 goes with c and which may be, but is not necessarily, the same as the weight function D ( n , w ). Possible extent functions will be discussed in Section 2.5.
Having defined this function, we can measure the precision and recall of individual features. The precision of an individual feature c retrieved by w both words go with c divided by the extent to which w 1 goes with c . The recall of the retrieval of c by w 1 is the extent to which both words go with c divided by the extent to which w 2 goes with c .
 ution, lie in the range [0,1]. We can now redefine precision and recall of a distribution as follows: co-occurrence type is not a black-and-white matter. Features that are shared to a similar extent are considered more important in the calculation of distributional similarity. 2.5 Extent Functions
The extent functions we have considered so far are summarized in Table 2. Note that weighted CRMs. For example, in the difference-weighted MI-based model we get the expressions: z-test based CRM ,andthe ALLR-based CRM . An interesting special case is the difference-weighted token-based CRM . In this case, since F ( w ) P ( c expressions for precision and recall: 450 have arrived at the same expression for both in this model. As a result, this model is symmetric.
 difference-weighted type-based CRM . This is because there is no difference between types and tokens for an individual feature; i.e., their retrieval is equivalent. In this case, the following expressions for precision and recall are derived: every token is effectively considered in this model, tokens are not weighted equally. In this model, tokens are treated differently according to which type they belong. proportion of the tokens for its particular type that it constitutes. 2.6 Combining Precision and Recall
We have, so far, been concerned with defining a pair of numbers that represents the similarity between two words. However, in applications, it is normally necessary to compute a single number in order to determine neighborhood or cluster membership.
The classic way to combine precision and recall in IR is to compute the F-score; that is, the harmonic mean of precision and recall: precision and high recall of the target distribution. It may be that, in some situations, occurrences (i.e., it is a high-precision neighbor) or it may be one that retrieves all of the required co-occurrences (i.e., it is a high-recall neighbor). The other factor in each case may play only a secondary role or no role at all.
 high precision and high recall are required for high similarity by computing a weighted  X  X  Special Case arithmetic mean of the harmonic mean and the weighted arithmetic mean of precision and recall: 4 where both  X  and  X  lie in the range [0,1]. The resulting similarity, sim ( w lie in the range [0,1] where 0 is low and 1 is high. This formula can be used in combi-nation with any of the models for precision and recall outlined earlier. Precision and recall can be computed once for every pair of words (and every model) whereas sim-ilarity depends on the values of  X  and  X  . The flexibility allows us to investigate em-pirically the relative significance of the different terms and thus whether one (or more) might be omitted in future work. Table 3 summarizes some special parameter settings. 2.7 Discussion
We have developed a framework based on the concept of co-occurrence retrieval (CR). Within this framework we have defined a number of models (CRMs) that allow us to systematically explore three questions about similarity. First, is similarity between words necessarily a symmetric relationship, or can we gain an advantage by considering it as an asymmetric relationship? Second, are some features inherently more salient than others? Third, does the difference in extent to which each word takes each feature matter? conversely, a high-recall neighbor is not necessarily a high-precision neighbor) and therefore we are not constrained to a symmetric relationship of similarity between 452 words. Second, the use of different weight functions varies the relative importance attached to features. Finally, difference-weighted models contrast with additive models in considering the difference in extent to which each word takes each feature. 3. Data and Experimental Techniques
The rest of this paper is concerned with evaluation of the proposed framework; first, by comparing it to existing distributional similarity measures, and second, by evaluating performance on two tasks. Throughout our empirical work, we use one data-set and one neighbor set comparison technique, which we now discuss in advance of presenting any of our actual experiments. 3.1 Data
The data used for all our experimental work was noun-verb direct-object data extracted from the BNC by a Robust Accurate Statistical Parser (RASP) (Briscoe and Carroll 1995;
Carroll and Briscoe 1996). We constructed a list of nouns that occur in both our data set and WordNet ordered by their frequency in our corpus data. Since we are interested in the effects of word frequency on word similarity, we selected 1,000 high-frequency nouns and 1,000 low-frequency nouns. The 1,000 high-frequency nouns were selected as the nouns with frequency ranks of 1 X 1,000; this corresponds to a frequency range of [586,20871]. The low-frequency nouns were selected as the nouns with frequency ranks of 3,001 X 4,000; this corresponds to a frequency range of [72,121].
 data and the other 20% was set aside as test data. compute similarity scores between all possible pairwise combinations of the 2,000 nouns and to provide (MLE) estimates of noun-verb co-occurrence probabilities in the pseudo-disambiguation task. The test data provides unseen co-occurrences for the pseudo-disambiguation task.
 with verbs in the direct-object position, the generality of the techniques proposed other grammatical relations, and other types of context. We restricted the scope of our experimental work solely for computational and evaluation reasons. However, we could have chosen to look at the similarity between verbs or between adjectives. chose nouns as a starting point since nouns tend to allow less sense extensions than verbs and adjectives (Pustejovsky 1995). Further, the noun hyponymy hierarchy in
WordNet, which will be used as a pseudo-gold standard for comparison, is widely recognized in this area of research.
 single grammatical relation (e.g., Lee 1999), whereas other work has considered multiple grammatical relations (e.g., Lin 1998a). We consider only a single grammatical relation because we believe that it is important to evaluate the usefulness of each grammatical relation in calculating similarity before deciding how to combine information from different relations. In previous work (Weeds 2003), we found that considering the a pseudo-disambiguation task.
 occurring in the corpus. This restriction was for computational efficiency and to avoid computing similarities based on the potentially unreliable descriptions of very low-frequency words. However, since our evaluation is comparative, we do not expect our results to be affected by this or any of the other restrictions. 3.2 Neighbor Set Comparison Technique
In several of our experiments, we measure the overlap between two different similarity measures. We use a neighbor set comparison technique adapted from Lin (1997). so that each neighbor is given a rank score of k  X  rank . Potential neighbors not within a given rank distance k of the noun score zero. This transformation is required since scores computed on different scales are to be compared and because we wish to only consider neighbors up to a certain rank distance. The similarity between two neighbor sets S and S is computed as the cosine of the rank score vectors: where s ( w )and s ( w ) are the rank scores of the words within each neighbor set S and S respectively.
 neighbor sets for each noun according to each pair of measures under consideration, we computed the mean similarity across all high-frequency nouns and all low-frequency nouns. However, since the use of the CR framework requires parameter optimization, here, we randomly select 60% of the nouns to form a development set and use the remaining 40% as a test set. Thus, any parameters are optimized over the development set nouns and performance measured at these settings over the test set. 4. Alternative Distributional Similarity Measures
In this section, we consider related work on distributional similarity measures and the extent to which some of these measures can be simulated within the CR framework. more extensive review, see Weeds (2003). Here, we concentrate on a number of more popular measures: the Dice Coefficient, Jaccard X  X  Coefficient, the L divergence measure, Hindle X  X  measure, and Lin X  X  MI-based measure. 4.1 The Dice Coefficient
The Dice Coefficient (Frakes and Baeza-Yates 1992) is a popular combinatorial similarity measure adopted from the field of Information Retrieval for use as a measure of lexical 454 distributional similarity. It is computed as twice the ratio between the size of the inter-section of the two feature sets and the sum of the sizes of the individual feature sets: is zero and the similarity between words with identical feature sets is 1. However, as harmonic mean of precision and recall (or F-score) using the additive type-based CRM. exactly replicated. 4.2 Jaccard X  X  Coefficient
Jaccard X  X  Coefficient (Salton and McGill 1983), also known as the Tanimoto Coefficient (Resnik 1993), is another popular combinatorial similarity measure. It can be defined as the proportion of features belonging to either word that are shared by both words; that is, the ratio between the size of the intersection of the feature sets and the size of the union of feature sets: occurrences is zero and the similarity between words with identical features is 1. Fur-ther, as shown by van Rijsbergen (1979), the Dice Coefficient and Jaccard X  X  Coefficient are monotonic in one another. Thus, although in general the scores computed by each will be different, the orderings or rankings of objects will be the same. In other words, for all k and w ,the k nearest neighbors of word w according to Jaccard X  X  Coefficient will be identical to the k nearest neighbors of word w according to the Dice Coefficient and the harmonic mean of precision and recall in the additive type-based CRM. 4.3 The L 1 Norm
The L 1 Norm (Kaufman and Rousseeuw 1990) is a member of a family of measures known as the Minkowski Distance, for measuring the distance in space. The L 1 Norm is also known as the Manhattan Distance, the taxi-cab distance, the city-block distance, and the absolute value distance, since it represents the distance traveled between the two points if you can only travel in orthogonal directions. When used to calculate lexical distributional similarity, the dimensions of the vector space are co-occurrence types and the values of the vector components are the probabilities of the co-occurrence types given the word. Thus the L 1 distance between two words, w and w 2 , can be written as: ing basic probability theory, we can rewrite the L 1 Norm as follows: based CRM. The constant and multiplying factors are required, since the CRM defines a similarity in the range [0,1], whereas the L 1 Norm defines a distance in the range [0,2] (where 0 distance is equivalent to 1 on the similarity scale). 4.4 The  X  -skew Divergence Measure The  X  -skew divergence measure (Lee 1999, 2001) is a popular approximation to the
Kullback-Leibler divergence measure 8 (Kullback and Leibler 1951; Cover and Thomas 1991). It is an approximation developed to be used when unreliable MLE probabilities 456 would result in the actual Kullback-Leibler divergence measure being equal to defined (Lee 1999) as: for 0  X   X   X  1, and where: always being non-zero when the r distribution is non-zero. The parameter  X  controls the extent to which the measure approximates the Kullback-Leibler divergence measure.
When  X  is close to 1, the approximation is close while avoiding the problem with zero probabilities associated with using the Kullback-Leibler divergence measure. This theoretical justification for using a very high value of  X  (e.g., 0.99) is also borne out by empirical evidence (Lee 2001).
 is calculated. For the purposes of this paper, we will find the neighbors of w optimizing: 9
CRMs to exactly simulate it. However, this measure does take into account the differ-ences between the probabilities of co-occurrences in each distribution (as a log ratio) and therefore we might expect that it will be fairly closely simulated by the difference-weighted token-based CRM. Further, the  X  -skew divergence measure is asymmetric. dist  X  ( w 1 , w 2 ) measures the cost of using the distribution of w calculated over the verbs that occur with w 2 . As such, we might expect that dist a high-recall measure, since recall is calculated over the co-occurrences of w the 200 nearest neighbors according to dist  X  and different parameter settings within the CR framework for 1,000 high-frequency nouns and for 1,000 low-frequency nouns, using the data and the neighbor set comparison technique described in Section 3. Table 4 shows the optimal parameters in each CRM for simulating dist development set, and the mean similarity at these settings over both the development set and the test set. From these results, we can make the following observations. set are minimal. Thus, performance of the models with respect to different parameter settings appears stable across different words.
 weighted token-based CRM achieves a fairly close approximation to dist
CRM  X  X  X  X  overall best approximation is achieved by the additive t-test based CRM. Although none of the CRMs are able to simulate dist  X  exactly, the closeness of approximation of overlap observed between other measures of distributional similarity. Weeds, Weir, and McCarthy (2004) report an average overlap of 0.4 between neighbor sets produced using dist  X  and Jaccard X  X  Measure and an average overlap of 0.48 between neighbor sets produced using dist  X  and Lin X  X  similarity measure.
 levels of recall for both high-and low-frequency nouns. For example, Figure 1 illustrates the variation in mean similarity between neighbor sets with the parameters  X  and  X  for the additive t-test based model. As can be seen, similarity between neighbor sets is significantly higher at high recall settings (low  X  ) within the model than at high-precision settings (high  X  ), which suggests that dist  X  has high-recall CR characteristics. 4.5 Hindle X  X  Measure
Hindle (1990) proposed an MI-based measure, which he used to show that nouns could 458
Hindle X  X  Measure proposed by (Lin 1998a), which overcomes the problem associated with calculating MI for word-feature combinations that do not occur: expressions for precision and recall in the difference-weighted MI-based CRM: since TP = T ( w 1 )  X  T ( w 2 ). However, we also note that the denominator in the expres-sion for recall depends only on w 2 , and therefore, for a given w the target word, it will remain the same as we calculate each neighbor set. Accordingly, the value of recall for each potential neighbor w 1 divided by a constant. Hence, neighbor sets derived using sim obtained using recall (  X  = 0,  X  = 0) in the difference-weighted MI-based CRM. 4.6 Lin X  X  Measure
Lin (1998a) proposed a measure of lexical distributional similarity based on his information-theoretic similarity theorem (Lin 1997, 1998b): two words w 1 and w 2 can be written according to Lin X  X  measure as: where T ( w ) = { c : I ( w , c ) &gt; 0 } . There are parallels between sim measures compute a ratio between what is shared by the descriptions of both nouns and the sum of the descriptions of each noun. The major difference appears to be the use of
MI, and hence we predicted that there would be a close relationship between sim the harmonic mean in the additive MI-based CRM. This relationship is shown below:
Now if TP I ( w 1 , c ) = TP I ( w 2 , c ), it follows:
TP I ( w 1 , c ) = TP I ( w 2 , c ) holds, the CR framework reduces to sim last necessary condition for equivalence is not one we can expect to hold for many (if any) pairs of words. In order to investigate how good an approximation the harmonic mean is to sim lin in practice, we compared neighbor sets according to each measure using the neighbor set comparison technique outlined earlier.
 parameters  X  and  X  .At  X  = 1, the average similarity between neighbor rankings was 0.967 for high-frequency nouns and 0.923 for low-frequency nouns. This is significantly higher than similarities between other standard similarity measures. However, the optimal approximation of sim lin was found using  X  = 0 . 75 and  X  = 0 . 5 in the additive
MI-based CRM. With these settings, the development set similarity was 0.987 for high-460 frequency nouns and 0.977 for low-frequency nouns. This suggests that sim more compensation for lack of recall by precision and vice versa than the harmonic mean. 4.7 Discussion
We have seen that five of the existing lexical distributional similarity measures are (ap-proximately) equivalent to settings within the CR framework and for one other, a weak approximation can be made. The CR framework, however, more than simulates existing measures of distributional similarity. It defines a space of distributional similarity mea-sures that is already populated with a few named measures. By exploring the space, we can discover the desirable characteristics of distributional similarity measures. It may be that the most useful measure within this space has already been discovered, or it may be that a new optimal combination of characteristics is discovered. The primary goal, however, is to understand how different characteristics relate to high performance in different applications and thus explain why one measure performs better than another.
In the next section, we consider what characteristics of distributional similarity mea-sures are desirable in two different application areas: (1) automatic thesaurus generation and (2) language modeling. 5. Application-Based Evaluation As discussed by Weeds (2003), evaluation is a major problem in this area of research.
In some areas of natural language research, evaluation can be performed against a gold standard or against human plausibility judgments. The first of these approaches is taken by Curran and Moens (2002), who evaluate a number of different distributional similarity measures and weight functions against a gold standard thesaurus compiled from Roget X  X  ,the Macquarie thesaurus, and the Moby thesaurus. However, we argue that this approach can only be considered when distributional similarity is required as an approximation to semantic similarity and that, in any case, it is not ideal since it is not clear that there is a single  X  X ight answer X  as to which words are most distributionally similar. The best measure of distributional similarity will be the one that returns the most useful neighbors in the context of a particular application and thus leads to the best performance in that application. This section investigates whether the desirable characteristics of a lexical distributional similarity measure in an automatic thesaurus generation task (WordNet prediction) are the same as those in a language modeling task (pseudo-disambiguation). 5.1 WordNet Prediction Task
In this section, we evaluate the ability of distributional similarity measures to predict semantic similarity by making comparisons with WordNet. An underlying assumption of this approach is that WordNet is a gold standard for semantic similarity, which, as is discussed by Weeds (2003), is unrealistic. However, it seems reasonable to suppose that a distributional similarity measure that more closely predicts a semantic measure based on WordNet is more likely to be a good predictor of semantic similarity. We chose
WordNet as our gold standard for semantic similarity since, as discussed by Kilgarriff and Yallop (2000), distributional similarity scores calculated over grammatical relation level context tend to be more similar to tighter thesauri, such as WordNet, than looser thesauri such as Roget X  X  . 5.1.1 Experimental Set-Up. There are a number of ways to measure the distance be-tween two nouns in the WordNet noun hierarchy (see Budanitsky [1999] for a review).
In previous work (Weeds and Weir 2003b), we used the WordNet-based similarity measure first proposed in Lin (1997) and used in Lin (1998a): direct super-classes of concept c in WordNet, and P ( c ) is the probability that a randomly selected word refers to an instance of concept c (estimated over some corpus such as SemCor [Miller et al. 1994]).

Pedersen 2003; McCarthy, Koeling, and Weeds 2004), it has been shown that the distance measure of Jiang and Conrath (1997) (referred to herein as the  X  X C measure X ) is a superior WordNet-based semantic similarity measure:
WordNet-based measure and each of the distributional similarity measures using the technique discussed in Section 3. We have carried out the same experiments using both the Lin measure and the JC measure. Correlation between distributional similarity mea-sures and the WordNet measure tends to be slightly higher when using the JC measure 462 between distributional similarity measures remain approximately the same. Here, for brevity, we present results just using the JC measure. 5.1.2 Results. As before, we present the results separately for the 1,000 high-frequency target nouns and for the 1,000 low-frequency target nouns. Table 5 shows the optimal parameter settings for each CRM (computed over the development set) and the mean similarities with the JC measure at these settings in both the development set and the test set. It also shows the mean similarities over the development set and the test set for each of the existing similarity measures discussed in Section 4. For reference, we also present the mean similarity for the WordNet-based measure wn sim of comparison, the test set correlation values for each distributional measure are also illustrated in Figure 3.
 the observed test set mean similarities were all less than 0.1, and thus any difference between mean scores of greater than 0.016 is significant at the 99% level, and differences greater than 0.007 are significant at the 90% level. Thus, from the results in Table 5 we can make the following observations.
 prediction, for both high-and low-frequency nouns, are the MI-based and the t-test based CRMs. The additive MI-based CRM performs the best for high-frequency nouns and the additive t-test based CRM performs the best for low-frequency nouns. How-ever, the differences between these models are not statistically significant. These CRMs perform substantially better than all of the unparameterized distributional similarity measures, of which the best performing are sim hind and sim and dist  X  1 for low-frequency nouns. Second, the difference-weighted versions of each model generally perform slightly worse than their additive counterparts. Thus, the difference in extent to which each word occurs in each context does not appear to be a factor in determining semantic similarity. Third, all of the measures perform signifi-cantly better for high-frequency nouns than for low-frequency nouns. However, some of the measures ( sim lin , sim jacc and sim dice ) perform considerably worse for low-frequency nouns.
 of variation across the CRMs was very similar. This pattern is illustrated using one of the best-performing CRMs ( sim add mi ) in Figure 4. With reference to this figure and to the results for the other models (not shown), we make the following observations. 464 low values of  X  than for high values of  X  . In other words, neighbors according to the WordNet based measure tend to have high-recall retrieval of the target noun X  X  co-occurrences. Second, a high value of  X  leads to high performance for high-frequency nouns but poor performance for low-frequency nouns. This suggests that WordNet-derived neighbors of high-frequency target nouns also have high-precision retrieval of the target noun X  X  co-occurrences, whereas the WordNet-derived neighbors of low-frequency target nouns do not. This also explains why particular existing measures (Jaccard X  X  / the Dice Coefficient and Lin X  X  Measure), which are very similar to a  X  = 1 setting in the CR framework, perform well for high-frequency nouns but poorly for low-frequency nouns. 5.1.3 Discussion. Our results in this section are comparable to those of Curran and
Moens (2002), who showed that combining the t-test with Jaccard X  X  coefficient outper-formed combining MI with Jaccard X  X  coefficient by approximately 10% in a comparison against a gold-standard thesaurus. However, we do not find a significant difference between using the t-test and MI in similarity calculation. Further, we found that using a combination of precision and recall weighted towards recall performs substantially better than using the harmonic mean (which is equivalent to Jaccard X  X  measure). In our experiments, the development-set similarity using the harmonic mean in the additive
MI-based CRM was 0.312 for high-frequency nouns and 0.153 for low-frequency nouns, and the development-set similarity using the harmonic mean in the additive t-test based
CRM was 0.294 for high-frequency nouns and 0.129 for low-frequency nouns. 5.2 Pseudo-Disambiguation Task Pseudo-disambiguation tasks have become a standard evaluation technique (Gale, Church, and Yarowsky 1992; Sch  X  utze 1992; Pereira, Tishby, and Lee 1993; Sch  X  utze 1998; Lee 1999; Dagan, Lee, and Pereira 1999; Golding and Roth 1999; Rooth et al. 1999; Even-
Zohar and Roth 2000; Lee 2001; Clark and Weir 2002) and, in the current setting, we may use a noun X  X  neighbors to decide which of two co-occurrences is the most likely.
Although pseudo-disambiguation is an artificial task, it has relevance in at least two application areas. First, by replacing occurrences of a particular word in a test suite with a pair of words from which a technique must choose, we recreate a simplified version of the word sense disambiguation task; that is, we choose between a fixed number of homonyms based on local context. The second is in language modeling where we data problem, it is often the case that a possible co-occurrence has not been seen in the training data. 5.2.1 Experimental Set-up. A typical approach to performing pseudo-disambiguation is as follows. A large set of noun-verb direct-object pairs is extracted from a corpus, of which a portion is used as test data and another portion is used as training data.
The training data can be used to construct a language model and/or determine the distributionally nearest neighbors of each noun. Noun-verb pairs ( n , v two verbs is the most likely to take the noun as its direct object. Performance is usually measured as error rate. We will now discuss the details of our own experimental set-up. from the BNC for each of 2,000 nouns was used to compute the similarity between nouns and is also used as the language model in the pseudo-disambiguation task, and pseudo-disambiguation task.
 aside for each target noun and modified it as follows. We converted each noun-verb pair ( n , v 1 ) in the test data into a noun-verb-verb triple ( n , v from the verbs that have the same frequency, calculated over all the training data, as v plus or minus 1. If there are no other verbs within this frequency range, then the test instance is discarded. This method ensures that there is no systematic bias towards v being of a higher or lower frequency than v 1 . We also ensured that ( n , v been seen in the test or training data. Ten test instances target noun in a two-step process of (1) while more than ten triples remained, discarding duplicate triples and (2) randomly selecting ten triples from those remaining after step 1. At this point, we have 10,000 test instances pertaining to high-frequency nouns and 10,000 test instances pertaining to low-frequency nouns, and there are no biases towards the higher-frequency or lower-frequency nouns within these sets. Each of these sets was split into five disjoint subsets, each containing two instances for each target noun. We use these five subsets in two ways. First, we perform five-fold cross validation.
In five-fold cross validation, we compute the optimal parameter settings in four of the subsets and the error rate at this optimal parameter setting in the remaining subset. This is repeated five times with a different subset held out each time. We then compute an average optimal error rate. We cannot, however, compute an average optimal parameter setting, since this would assume a convex relationship between parameter settings and error rate. In order to study the relationship between parameter settings and error form a test set. The development set is used to optimize parameters and the test set 466 between error rate and parameter settings, it is the error rate in this development set that is shown. In the case of the CRMs, the parameters that are optimized are  X  ,  X  ,and k (the number of nearest neighbors). 12 For the existing measures, the only parameter to be optimized is k .
 use the nearest neighbors of noun n (as computed from the training data) to decide given a vote that is equal to the difference in frequencies of the co-occurrences ( m , v we distinguish between cases where a neighbor occurs with each verb approximately the same number of times and where a neighbor occurs with one verb significantly more often than the other. The votes for each verb are summed over all of the k nearest neighbors of n , and the verb with the most votes wins. Performance is measured as error rate.
 where T is the number of test instances and a tie results when the neighbors cannot decide between the two alternatives. 5.2.2 Results. In this section, we present results on the pseudo-disambiguation task for all of the CRMs described in Section 2. We also compare the results with the six existing distributional similarity measures (Section 4) and the two WordNet-based measures (Section 5.1).
 backs-off to the unigram probabilities of the verbs being disambiguated. By construction of the test set, this should be approximately 0.5. The actual empirical figures are 0.553 for the high-frequency noun test set and 0.586 for the low-frequency noun test set. The deviation from 0.5 is due to the unigram probabilities of the verbs not being exactly observed when all 1,999 potential neighbors are considered. In this case, we obtain target noun in the training data, but a noun is not considered as a potential neighbor of itself.
 rates for each measure, and for high-and low-frequency nouns, calculated using five-fold cross validation. For ease of comparison, the cross-validated average optimal error rates are illustrated in Figure 5. Standard deviation in the mean optimal error rate across significant at the 99% level and differences greater than 0.012 are significant at the 90% level. From the results, we make the following observations. cantly outperforms all but one (the z-test based CRM) of the other measures for high-frequency nouns. For low-frequency nouns, slightly higher performance is obtained using the additive MI-based CRM. This difference, however, is not statistically signifi-cant. Second, all of the distributional similarity measures perform considerably better than the WordNet-based measures 13 at this task for high-and low-frequency nouns.
Third, for many measures, performance over high-frequency nouns is not significantly higher (and is in some cases lower) than over low-frequency nouns. This suggests that distributional similarity can be used in language modeling even when there is relatively little corpus data over which to calculate distributional similarity.
 use the development set to determine the optimal parameters, we consider performance on the development set as each parameter is varied. Table 7 shows the optimized pa-rameter settings in the development set, error rate at these settings in the development set, and error rate at these settings in the test set. For the CRMs, we considered how the performance varies with each parameter when the other parameters are held constant at their optimum values. Figure 6 shows how performance varies with  X  , and Figure 7 shows how performance varies with  X  for the additive and difference-weighted t-test based and MI-based CRMs. For reference, the optimal error rates for the best performing existing distributional similarity measure ( sim lin ) is also shown as a straight line on each graph.
 fairly similar for all measures and is as would be expected. To begin with, considering 468 more neighbors increases performance, since more neighbors allow decisions to be made in a greater number of cases. However, when k increases beyond an optimal value, a greater number of these decisions will be in the wrong direction, since these words are not very similar to the target word, leading to a decrease in performance. In a small number of cases (when using the ALLR-based CRMs or the WMI-based CRMs for high frequency nouns), performance peaks at k = 1. This suggests that these measures may be very good at finding a few very close neighbors.

CRMs, perform significantly better at low values of  X  (0.25-0.5) and high values of  X  (around 0.8). This indicates that a potential neighbor with high-precision retrieval of informative features is more useful than one with high-recall retrieval. In other words, it seems that it is better to sacrifice being able to make decisions on every test instance with a small number of neighbors in favor of not having neighbors that predict incorrect verb co-occurrences. This also suggests why we saw fairly low performance by the  X  -skew divergence measure on this task, since it is closest to a high-recall setting in the additive t-test based model. The low values of  X  indicate that a combination of precision and recall that is closer to a weighted arithmetic mean is generally better than one that is closer to an unweighted harmonic mean. However, this does not hold for the t-test based CRMs for low-frequency nouns. Here a higher value of  X  is optimal, indicating that, in this case, requiring both recall and precision results in high performance. 6. Conclusions and Future Directions
Our main contribution is the development of a framework, first presented in a prelim-inary form in Weeds and Weir (2003b), that is based on the concept of lexical substi-tutability. Here, we cast the problem of measuring distributional similarity as one of co-occurrence retrieval (CR), for which we can measure precision and recall by analogy with the way they are measured in document retrieval. This CR framework has then allowed us to systematically explore various characteristics of distributional similarity measures.
 end, we have explored the merits of symmetry and asymmetry in a similarity measure by varying the relative importance attached to precision and recall. We have seen that as the distribution of word B moves away from being identical to that of word A, its similarity with A can decrease along one or both of two dimensions. When B occurs in contexts that word A does not, precision is lost but B may remain a high-recall neighbor of word A. When B does not occur in contexts that A does, recall is lost but B may remain a high-precision neighbor of word A. Through our experimental work, which is more thorough than that presented in Weeds and Weir (2003b), we have shown that the kind of neighbor preferred appears to depend on the application in hand. High-precision neighbors were more useful in the language modeling task of pseudo-disambiguation and high-recall neighbors were more highly correlated with WordNet-derived neighbor sets. Thus, similarity appears to be inherently asymmetric. Further, it would seem 470 unlikely that any single, unparameterized measure of distributional similarity would be able to do better on both tasks.
 distributional similarity. To this end, we have explored the way in which frequency information is utilized using different co-occurrence retrieval models (CRMs). Using different weight functions, we have investigated the relative importance of different co-occurrence types. In earlier work (Weeds and Weir 2003b), we saw that using MI to weight features gave improved performance on the two evaluation tasks over type-based or token-based CRMs. Here, we have seen that further gains can be made by using the t-test as a weight function. This leads to significant improvements on the pseudo-disambiguation task for all nouns and marginal improvements on the WordNet predic-tion task for low-frequency nouns. To some extent, this supports the findings of Curran and Moens (2002), who investigated a number of weight functions for distributional similarity and showed that the t-test performed better than a number of other weight functions including MI.
 each word appears in each context. To this end, we have herein proposed difference-weighted versions of each model in which the similarity of two words in respect difference-weighted CRMs to their additive counterparts and shown that difference-weighting does not seem to be a major factor and does not improve results when using the best-performing CRMs.
 understanding of existing distributional similarity measures. By comparing existing measures with the CR framework, we can analyze their CR characteristics. As discussed in Weeds and Weir (2003b), the Dice Coefficient and Jaccard X  X  Coefficient are exactly simulated by  X  = 1 in the additive type-based model and Lin X  X  Measure is almost equivalent to the harmonic mean of precision and recall in the additive MI-based model. Here, we also show that the L 1 Norm is exactly simulated by the (unparame-terized) difference-weighted token-based model, Hindle X  X  Measure is exactly simulated by  X  = 0,  X  = 0 in the additive MI-based model, and the  X  -skew divergence measure is most similar to high-recall settings in the additive t-test based CRM. Knowing that
Lin X  X  Measure is almost equivalent to the harmonic mean of precision and recall in the additive MI-based model explains why this measure does badly on the WordNet prediction task for low-frequency nouns. We have seen that recall is more important than precision in the WordNet prediction task, whereas the nearest neighbors of a target noun according to Lin X  X  Measure have both high precision and high recall. Conversely, knowing that the  X  -skew divergence measure is most closely approximated by high-recall settings in the additive t-test based model explains why this measure performs poorly on the pseudo-disambiguation task, since we have seen that high precision is required for optimal performance on this task.
 and we have shown that the performance of distributional similarity techniques for low-frequency nouns is not significantly lower than for high-frequency nouns. This suggests that distributional techniques might be used even when there is relatively little estimation techniques for rare words with greater confidence. In the semantic domain, we might be able to use distributional techniques to extend existing semantic resources to cover rare or new words or automatically generate domain-, genre-, or dialect-specific resources.
 although the set of CRMs defined here is more extensive than that defined in Weeds and
Weir (2003b), it is still not exhaustive, and other models might be proposed. Further, it would be interesting to combine CRMs with the feature reweighting scheme of
Geffet and Dagan (2004). These authors compare distributional similarity scores with human judgments of semantic entailment and show that substantial (approximately 10%) improvements over using Lin X  X  Measure can be achieved by first calculating similarity using Lin X  X  Measure and then recalculating similarity using a relative feature focus score, which indicates how many of a word X  X  nearest neighbors shared that feature.
 evaluate CRMs and distributional similarity methods in general. In particular, we see potential for the use of distributional similarity methods in prepositional phrase attach-ment ambiguity resolution. This task has been previously tackled using semantic classes to predict what is ultimately distributional information. Accordingly, we believe that it should be possible to do better using the CR framework. 472 butional techniques need to be able to distinguish between different semantic relations such as synonymy, antonymy, and hyponymy. These are important linguistic distinc-tions, particularly in the semantic domain, since we are unlikely, say, to want to replace a word with its antonym. Weeds, Weir, and McCarthy (2004) give preliminary results on the the use of precision and recall to distinguish between hypernyms and hyponyms in sets of distributionally related words.
 Acknowledgments References 474
