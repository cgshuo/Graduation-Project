 We propose three heuristics to determine the country of ori-gin of a person or institution via text-based IE from the Web . We evaluate all methods on a collection of music artists and bands , and show that some heuristics outperform ear-lier work on the topic by terms of coverage, while retaining similar precision levels. We further investigate an extension using country-specific synonym lists .
 Categories and Subject Descriptors: H.3.1 [Informa-tion Storage and Retrieval]: Content Analysis and Indexing I.7.m [Document and Text Processing]: Web Mining General Terms: Algorithms, Measurement Keywords: information extraction, country of origin detec-tion, term weighting, music information research, evaluation The country of origin of a person or institution represents an interesting aspect of his/her/its life/existence. It plays a vi-tal role in understanding a person X  X  background and context. As a semantic descriptor , the country of origin of a music artist or band, both of which we will refer to as X  X rtist X  X n the following, can be used to query music collections based on learned associations between acoustic features and textual features  X  cf. [2, 6]
The  X  X ountry of origin X  of an artist is defined as the coun-try where he or she was born, or the band was founded. What makes this task a challenge is foremost that the origin is neither always unambiguous, nor well-known. Consider, for example, Farrokh Bulsara , later known as Freddie Mer-cury . He was born in Zanzibar, Tanzania. However, he relocated to the UK at the age of 17, where he later became world famous as co-founder of the band Queen . Mercury X  X  country of origin is nevertheless Tanzania, whereas Queen X  X  is the UK. This example highlights the problem of determin-ing the origin in cases where the main country of musical activity differs from the place of birth.

Earlier work on predicting the origin of a music artist mainly consists of [3, 4]. In contrast to our approaches that may  X  at least in theory  X  use the whole Web as data source, Govaerts and Duval focus on specific Web sites, such as last.fm , Wikipedia , and Freebase , and apply heuristic func-vital to the quality of the prediction. For the evaluation experiments described next, we use the following scheme to describe a setting: { key 1 , , key n }, ipl , af . We further looked into using synonyms for countries and nationalities, extracted from Thesaurus.com . A complete list of the used mappings country 7 X  h syn 1 , , syn n i is countries, such as the  X  X nited States X  (of America), are of-ten wrongly predicted due to their ambiguity , and abundant presence on the Web. Since there exists no standardized data set for this kind of task and we did not have access to the one used in [3], we manually extracted 578 artists and their country of origin from sources such as allmusic.com , last.fm , and Wikipedia . 3 We included artists from 69 distinct countries of the world.
Table 1 shows the evaluation results in terms of coverage (or recall ), precision , and F-measure [7]. Coverage denotes the share of artists for which a country could be determined, precision is the share of artists whose origin is correctly pre-dicted among the number of artists for which a prediction was made, and the F-measure aggregates precision and re-call via the weighted harmonic mean. The best-performing approaches within each category are printed in bold.
The page counts approach seems to be too simple to cap-ture the country of origin correctly. The term weighting approaches yield overall the best results. Interestingly, tf and df measures outperform tf idf . Even though tf idf is the standard approach in text-based IR, it underperforms in this specific IE task. This is likely a result of tf idf  X  X  penalization of terms that occur in a large number of docu-ments. Suppressing such terms does make sense in most IR tasks. In our IE task, however, general and popular terms should not be given less weight. The text distance approach performs worse than expected. The reason for this bad per-formance may be an unfavorable set of key terms. We will investigate this as part of future work.

Table 2 shows the best evaluation results from Govaerts and Duval [3]. Comparing Tables 1 and 2, our approaches perform, in general, better with respect to coverage and F-measure. In terms of precision, the picture is more diversi-fied. While Govaerts and Duval X  X  combined method reaches about 77% precision (at a 59%-recall-level), our best method in terms of precision achieves about 71% (but at a 100%-recall-level).
 Employing the Wilcoxon signed-rank test on each pair of ap-proaches (with and without synonyms) revealed significant difference for tf idf -based approaches. Furthermore, three of the approaches based on text distances perform signifi-cantly worse if synonyms are used. This may be explained by ambiguous synonyms, such as  X  X S X  or  X  X am X .
 Friedman X  X  two-way analysis of variance revealed highly sig-nificant differences between all categories of approaches. We further employed the post-hoc Wilcoxon signed-rank test to analyze which settings differ within their category. In Ta-ble 1 the settings that significantly differ from the best per-
