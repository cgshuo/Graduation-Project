 nptdat@mi.ci.i.u-tokyo.ac.jp Wikipedia ( www.wikipedia.org ) has emerged as the world X  X  largest online encyclopedia. Because the ency-clopedia is managed by the Wikipedia Foundation, and because numerous collaborators in the world continu-ously develop and edit its articles, its contents are be-lieved to be quite reliable despite its openness.
This study is intended to deal with the problem of extracting binary relations between entity pairs from Wikipedia X  X  English version. A binary relation is defined as a triple ( e p , rel , e s ) in which e p and e s are entities and rel indicates a directed relationship of e p and e s . Current experiment limits entities and relations to a reasonable size in that an entity is classifiable as person , organiza-tion , location , artifact , year , month or date ; and a rela-tion can be founder , chairman , CEO , COO , president , director , vice chairman , spouse , birth date , birth place , foundation , product and location .

To our knowledge, only one recent work has at-tempted relation extraction on Wikipedia: (Culotta et al., 2006) presents a probabilistic model to integrate extrac-tion and mining tasks performed on biographical text of Wikipedia. Some other works (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002) rely on the abundance of web data to obtain easy patterns and learn such patterns based mostly on lexical information. Rather than analyzing dependency path between entity pair proposed in (Bunescu and Mooney, 2006; Cui et al., 2005), our method analyzes a subtree derived from the dependency structure. Such subtree contains more evi-dence of the entities X  inter-relation than the path in some cases. We propose a new feature obtained from the sub-tree by using a subtree-mining technique.
 In addition, we also make use of the characteristics of Wikipedia to allocate the mentions of entities and further identify their types to help the relation extraction process. Due to the encyclopedic style , each Wikipedia article mainly provides information for a specific entity and fur-ther mentions other entities related to it. Culotta et al. (2006) defines the entities as principal entity and sec-ondary entity respectively. We predict only relationships between the principal entity and each mentioned sec-ondary entity that contains a link to its descriptive article.
We put some assumptions in this study: a relation-ship can be expressed completely in one sentence. Fur-thermore, a relationship between an entity pair might be expressed with the implication of the principal entity in some cases. Thus, for an article, only sentences contain-ing at least a secondary entity are necessarily analyzed.
An interesting characteristic of Wikipedia is the cate-gory hierarchy that is used to classify articles according to their content. Additionally, those articles for famous en-tities provide summary sections on their right side, which are created by human editors. Finally, the first sentence of an article often defines the principal entity. Figure 1 delineates our framework for relation extrac-tion. First, Wikipedia articles are processed to remove HTML tags and to extract hyperlinks that point to other Wikipedia articles. Raw text is submitted to a pipeline including a Sentence Splitter , a Tokenizer and a Phrase Chunker supplied by the OpenNLP 1 tool set. The in-stances of the principal entity and secondary entities are then anchored in the articles. The Secondary Entity De-tector simply labels the appropriate surface texts of the hyperlinks to other Wikipedia articles, which are proper nouns as secondary entities. The Principal Entity Detec-tor will be explained in the following subsection.
After the entities are anchored, sentences that include at least one mention of secondary entities will be selected by a Sentence Detector . Each mention of the secondary entities is considered as a relation candidate between the underlying entity and the principal entity. Secondary en-tities are always explicit, although the principal entity is sometimes implicit in sentences containing no mention.
Keywords that provide clues for each relation label will be identified by a Keyword Extractor . Parallely, an Entity Classifier module classifies the entities into types. The Relation Extractor extracts subtree feature from a pair of the principal entity and a mention of secondary entity. It then incorporates subtree feature together with entity type feature into a feature vector and classifies relations of the entity pair using SVM-based classifiers . 3.1 Principal Entity Detector This module detects all referring expressions of the prin-cipal entity in an article. All occurrences of identified expressions are labeled as mentions of the principal en-tity. We adopt (Morton, 2000) to classify the expressions into three types: (1) personal pronoun (2) proper noun (3) common nouns. Based on chunking information, we propose a simple technique to identify a set of referring expressions of the principal entity, denoted as F : (i) Start with F = {} . (ii) Select the first two chunks for F : the proper chunk (nounphase with at least one proper noun) of the article title and the first proper chunk in the first sentence of the article, if any. If F is still empty, stop. (iii) For each remaining proper chunk p in the article, if p is derived from any expressions selected in (ii), then F  X  p . Proper chunk p if all its proper nouns appear in p 2 . (iv) In the article, select c as the most frequent subjective pronouns , find c X  as its equivalent objective pronoun and add them to F . (v) For each chunk p with the pattern [DT N 1 ... N k ] where DT is a determiner and N k  X  X  are a common nouns , if p appears more frequently than all the selected pro-nouns in (iv), then F  X  p .
 Table 1 shows some extracted referring expressions. The third column indicates in which step the expressions are selected. Supported by the nature of Wikipedia, our technique provides better results than those of the coref-erence tool in LingPipe library 2 and OpenNLP tool set. 3.2 Entity Classifier Entity type is very useful for relation extraction. For in-stance, the relation label between a person and an orga-nization should be founder , chairman , etc., but cannot be spouse , product , etc. We first identify year , month and date entities by directly examining their surface text. Types of other entities are identified by classifying their corresponding articles. We develop one SVM-based clas-sifier for each remaining type using the following fea-tures: category feature (categories collected when trac-ing from the article upto k level of its category structure), pronoun feature (the most frequent subjective pronoun in the article) and singular noun feature (singular nouns of the first sentence of the article). 3.3 Keyword Extractor Our hypothesis in this research is that there exist some keywords that provide clues for the relationship between ES denotes the secondary entity. a pair. For example, to express the founder relation, a sentence should contain one keyword such as: found , founder , founded , co-founders , or establish , etc. We iden-tify such keywords by using a semi-automatic method. First, we automatically extract some true relations from summary sections of Wikipedia articles. Then, we map entities in such relations to those in sentences to build sample sentences for each relationship . Tf-idf model is exploited to measure the relevance of words to each re-lationship for those on the dependency path between the entity pair. Finally, we choose the keywords manually from lists of candidates ranked by relevance score with respect to each relation. Table 2 shows our result selected from ranked lists of total 35,820 keyword candidates us-ing only one hour of human labor. 3.4 Subtree Feature from Dependency Path In this subsection, we will describe how to obtain effi-cient features for extracting relation using subtree min-ing. We extend the idea of Bunescu et al. (Bunescu and Mooney, 2006) suggesting the analysis of dependency path between the entities for extracting relation, in that paths between the secondary entity and the keywords of r will be added to the dependency path between the entities to create a tree. The expanded tree is defined as core tree of r because it attempts to capture the clues for r . Steps to extract the core tree C of a relationship r from a sentence s are described as follows: (i)] Initialize the core tree C as blank. (ii) Derive the dependency tree D from s . (iii) Label the group of nodes corresponding to words of secondary entity by an ES node in D . (iv) If the principal entity appears in s , apply (iii) to re-place principal entity with EP . Then extract P 0 as shortest path from ES to EP in D and add P 0  X  C . (v) For each keyword w of r , extract P w as the shortest path from ES to node of w and add P w  X  C .

Figures 2c &amp; 2d present exemplary core trees of CEO relationship derived from the dependency trees in Figures 2a &amp; 2b. To analyze both words and relations of a core tree uniformly, we transform it into a uniform graph for-mat (Figures 2e &amp; 2f) in which core tree words and rela-tions are also represented as graph nodes.

We define a basic element of a relationship r as a key pattern that commonly appears in various core trees of r . As an example, the core trees in Figures 2e &amp; 2f share a common pattern in Figure 2g. Intuitively, this subtree shares the core trees of sentences that express the idea of  X  X oined the company as CEO X  or  X  X oined the company and do something as CEO X  .
 We denote T = ( V , E ) as a directed tree, in which V is a set of nodes and E is a set of directed edges. Node y is an ancestor of node x , denoted by x  X  y , if ( x , y )  X  E or  X  i 1 ,..., i k ( k  X  N and k  X  1) such that ( x , i 1 ) , ( i 1 , i 2 ) ,..., ( i k  X  1 , i k ) , ( i tree S = ( V S , E S ) is a subtree of T if and only if: (i) V and (ii)  X  ( x , y )  X  E S , we have x  X  y in T . We use a subtree as a feature for relation extraction. From a set of training sentences with respect to a relation-ship r , we derive the core trees. A frequent tree-mining algorithm (Zaki, 2002) is used to generate subtrees from that set of core trees to form the feature space. Each mined subtree corresponds to a value of the feature. In this experiment, 5,975 articles are selected, in which 45 articles are for testing and 5,930 articles for train-ing. We apply the framework in Figure 1 on the train-ing articles to extract keywords and select relation candi-dates. Subsequently, 3,833 positive instances (each con-tains at least one relation) and 805 negative instances (the ones containing no relation) from the candidates are an-notated to train the Relation Extractor . Among 39,467 Table 3: Compare our proposed system and baselines Table 4: Result of Entity Classifier with various levels ( k value) of exploited category structure entities collected from all principal and secondary enti-ties, we randomly select 3,300 entities and manually an-notate their types for the Entity Classifier . Finally, we use 3,100 entities for training and 200 entities for testing.
We develop two baseline systems to evaluate our method, which use bag-of-words model. The second sys-tem (B1 in Table 3) works like the Keyword Extractor on training instances in that it calculates tf-idf scores for words on the dependency path between the entities with respect to each relation. During testing, it accumulates tf-idf scores of words on the path and chooses the relation label that gives the highest score for the entity pair. The only difference between the two baseline systems is that the first one (B0 in Table 3) focuses on all the words be-tween the entities in sentence text, not dependency path.
In our experiments, dependency graphs are obtained by Minipar parser (Lin, 1998), classifiers are trained by SVM Light (Joachims, 1999) with 2 nd -order polynomial kernel, subtrees are mined by FREQT 3 tree miner.
On the basis of preliminary experiments, we report the performance of our system compared with those of base-line systems in Table 3. The result shows that our pro-posed method gives a substantial improvement over the baselines. Although the recall is quite adequate, preci-sion is low. Data analysis reveals that although the mined subtrees capture key features for relationships, they also generate many irrelevant features which degrade the per-formance. It is necessary to carry out feature selection step for subtree feature. One more reason of the poor precision is that our system suffers from the error accu-mulation in a long pipeline of entity detection , entity clas-sification , dependency parsing and relation classification .
Table 4 shows the effectiveness of different values of k parameter in Entity Classifier . The classifier works best when we trace four levels on category system. An inter-esting fact is that Wikipedia can be used as an external knowledge source for Named Entity Recognition. We have presented a method to extract relations between entities from Wikipedia articles by incorporating infor-mation from the Wikipedia structure and by the analysis of Wikipedia text. The key features of our method in-clude: (1) an algorithm to build the core syntactic tree that reflects the relation between a given entity pair more accurately; (2) the use of a tree-mining algorithm to iden-tify the basic elements of syntactic structure of sentences for relationships; (3) method to make use of the nature of Wikipedia for entity allocation and entity classification. E. Agichtein and L. Gravano. 2000. Snowball: Ex-tracting relations from large plain-text collections. In the 5 th ACM International Conference on Digital Li-braries .
 S. Brin. 1998. Extracting patterns and relations from the world wide web. In Proceedings of the 1998 Inter-national Workshop on the Web and Databases , pages 172 X 183.
 R.C. Bunescu and R.J. Mooney. 2006. Extracting rela-tions from text: From word sequences to dependency paths. In Anne Kao and Steve Poteet, editors, Text Mining and Natural Language Processing .
 H. Cui, R. Sun, K. Li, M.-Y. Kan, and T.-S. Chua. 2005. Question answering passage retrieval using de-pendency relations. In Proceedings of SIGIR .
 A. Culotta, A. McCallum, and J. Betz. 2006. Integrating probabilistic extraction models and data mining to dis-cover relations and patterns in text. In Proceedings of the HLT-NAACL-2006 .
 T. Joachims. 1999. Making large-scale svm learning practical. In B. Sch  X  olkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods -Support Vector Learning . MIT-Press.
 D. Lin. 1998. Dependency-based evaluation of minipar. In Proceedings of the Workshop on the Evaluation of
Parsing Systems, 1 st International Conference on Lan-guage Resources and Evaluation .
 T. Morton. 2000. Coreference for nlp applications. In Proceedings of the ACL-2000 .
 D. Ravichandran and E. Hovy. 2002. Learning surface text patterns for a question answering system. In Pro-ceedings of the ACL-2002 , pages 41 X 47.
 M.J. Zaki. 2002. Efficiently mining frequent trees in a forest. In Proceedings of 8th ACM SIGKDD .
