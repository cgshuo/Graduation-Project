 This paper presents a SVM (Support Vector Machine) classi-fication system which divides contact-center call transcripts into  X  X reeting X ,  X  X uestion X ,  X  X efine X ,  X  X esearch X ,  X  X esolu-tion X ,  X  X losing X  and  X  X ut-of-topic X  sections. This call sec-tion segmentation is useful to improve search and retrieval functions and to provide more detailed statistics on calls. We use an off-the-shelf automatic speech recognition (ASR) system to generate call transcripts from recorded calls be-tween customers and service representatives.

We first classify an individual utterance into a call section by applying the SVM classifier and then merge adjacent ut-terances classified into a same call section. We experiment with the proposed system on 100 automatically transcribed calls. The 10-fold cross validation shows 87.2% classification accuracy. we also compare the p roposed algorithm with two other approaches  X  the most frequent section only method and a maximum entropy-based segmentation. The evalua-tion shows that our system X  X  accuracy is 12% higher than the first baseline system and 6% higher than the second baseline system respectively.
 H.4 [ Knowledge Management ]: Mining and representing text, Classification Algorithm, Design, Experimentation Call Section Segmentation, Natural Language Processing, Text Mining, Classification, Contact Center Analytics, Speech Analytics  X  A full version of this paper is available as Author X  X  Guide to Preparing ACM SIG Proceedings Using L A T E X 2 and BibTeX at www.acm.org/eaddress.htm
Most medium to large size businesses operate customer contact centers or call centers to provide services to cus-tomers. The most important focus of these contact centers is on reducing operations costs while improving customer satisfaction. With the advancement of Internet technology, many modern contact centers support various channels of customer interactions including telephony, emails, web-page fill-in forms and instant messaging. The majority of the current contact center interactions, however, are still over telephone conversations between contact center agents and customers.

Customer and agent conversations are a valuable source of insights into the contact center operations and also the company X  X  overall business. For instance, deep analysis of such conversations can enable measuring customer satisfac-tion, identifying up-sell/cross-sell opportunities and moni-toring contact center performance. However, faced with a huge volume of calls and immature spoken language under-standing technology, companies are not able to fully utilize the information. So far, much of the effort applying nat-ural language processing (NLP) techniques in this domain has focused on automatic call routing through an interactive voice response system and call topic classification based on a predefined domain taxonomy [6, 15, 18, 20, 7].

This work aims to address the need through a call sec-tion segmentation system which analyzes automatic speech transcripts and divides a call transcript into different call sections. The target data comprise spontaneous speech con-versations between customers and service representatives. The target call sections used in this work are  X  X reeting Sec-tion X ,  X  X uestion Section X ,  X  X efine Section X ,  X  X esearch Sec-tion X ,  X  X esolution Section X ,  X  X losing Section X  and  X  X ut-of-topic Section X . We select these seven call sections because they tend to appear in all contact center calls. The sections, however, can vary across companies and industries. We use a machine learning-based approach, thus it is easily adapt-able to different set of call sections. Note also that not all call sections appear in every transcript. For instance,  X  X ut-of-topic X  section appears in a small fraction of calls. Also, a call section may appear multiple times during a call. For instance, some calls have more than one  X  X reeting X  section when the calls were transferred to other agents.

To our knowledge, there has been no previous research on automatic call segmentation for ASR transcripts. This call section segmentation can improve search and retrieval func-tions and also provide more detailed call statistics enabling many interesting applications for business intelligence. For instance, if a contact center agent can find a previous call that is identical or very similar to an in-coming call, he can reduce the call handling time by extracting the solution to the customer X  X  question from the previous call. By match-ing only the  X  X uestion X  sections of the in-coming call and previous calls, we can more easily find calls for identical or very similar problems. Another motivating application identifies highly frequent calls or calls that have very short  X  X uestion X  and/or  X  X esolution X  sections. These calls might be good candidate for self-service thus saving the contact center operation costs.

To this end, we propose a call segmentation system based on support vector machines. More specifically, we first iden-tify utterances in a call transcript, and apply the SVM clas-sifier to assign one of the target call sections to each ut-terance. Utterance boundaries are automatically recognized based on a statistical approach. Then, we merge utterances that are adjacent and are classified into a same call sec-tion. We experiment with the proposed system on automat-ically transcribed calls. In this work, we use a state-of-the-art automatic speech recognition (ASR) system described in [10] for transcribing contact center calls. The ASR system uses a large US English vocabulary and works in speaker-independent mode. The ASR system exhibits about 42% word error rate (WER) for our data.

More specifically, the call section segmentation system comprises the following three steps. 1. Utterance Boundary Detection  X  segment a call into 2. Utterance Classification  X  classify each utterance into 3. Utterance Merge  X  merge adjacent utterances that were
In this work, we train and test our system with contact center calls from an IT (information technology) Help Desk of a large company. Most calls are related to problems with network connectivity, user passwords and various software tools. The transcripts contain additional information about the recognized words such as the starting time, the duration of the word and speaker turns. We show that our system achieves 87.2% segmentation accuracy for automatic tran-scripts.

The remainder of the paper is structured as follows. Sec-tion 2 describes how to recognize utterance boundaries in automatic speech transcripts. In section 3, we describe the SVM-based call section segmentation system in more de-tails. In section 4, we show the experimental results and performance evaluation of the proposed system. Finally, we discuss related work in section 5 and present the conclusion and possible future work in section 6.
We use an individual utterance as the base unit for call section segmentation task. However, speech transcripts gen-erated by the ASR system used in this work do not have utterance boundary information. In this section, we de-scribe automatic methods for identifying utterances and the speaker identification in a speech transcript.
Automatically identifying utterance boundaries in an au-tomatic speech transcript is very challenging for the follow-ing reasons. Firstly, the transcripts have no punctuation marks or case information, which are often used as cues for sentence boundary recognition. Secondly, the speaker turns in automatic speech transcripts are often inaccurate and un-reliable. One speaker turn often contains speeches from both speakers. Thirdly, the automatic speech recognizer has a high (about 42%) word error rate. The word error rate tends to be higher for domain-specific words because many domain-specific words are out-of-vocabulary. This makes conducting text analysis on these data more difficult.
In this work, we define the task of utterance boundary detection as a binary classification problem. For each word boundary, the system decides if it is an utterance boundary. Note that our primary goal in this work is to identify call sections, and utterances are only used as a base unit for finding correct sectio n boundaries. Our goal for utterance detection, therefore, is to generate exclusive utterances. In other words, we are more tolerant of short utterances that don X  X  include part of other utterances than of long utterances that include part of other utterances. With this goal in mind, we apply a maximum entropy classification method for utterance boundary detection. We used the OpenNLP Maxent machine learning package to build the maximum entropy-based utterance boundary detection system [1].
We train the system on 2,236 manual transcripts which contain quite accurate utterance changes and speaker turns. The training data consist of about 1.7 million words. Table 1 describes the features for utterance boundary detection.
The word feature set W is a set of words appeared in the training corpus after the following normalization steps. 1. Normalize all filler words into an artificial generic word 2. Normalize all numeric tokens and special expressions 3. Retrieve the lemma forms, { L 1 ,  X  X  X  ,L n } ,ofallother More formally, W is defined as { F, E, L 1 ,  X  X  X  ,L n } .Note that the method is based only on textual cues and the pause length between two words. Our system is not integrated with the ASR system and thus is not able to use other prosodic features, which are very useful for recognizing ut-terance boundaries [9, 12].

To evaluate the performance of the utterance boundary classifier, we divided the 2,236 manual transcripts into a training set and an evaluation set. The training set contains 2,000 manual transcripts, and the evaluation set contains the remaining 236 transcripts. Table 2 shows precision, recall and F-measure of the utterance boundary detection system. being appeared as a bigram within utterances.
 and the start of w j Table 1: The feature set for utterance boundary recognition. w feature set W described in section 2.1. The bigram correlation C ( w Table 2: Performance evaluation results for utter-ance boundary detection. F-measure was computed on 2,000 manual transcripts and tested on 236 man-ual transcripts. The training set and the test set contain 160,217 and 16,224 utterances respectively.
We recognize speaker identification based on two observa-tions from contact center calls. Firstly, contact center calls have two speaker types  X  customer and agent. More than one agent is often involved in a call, but it is not important for the purpose of this work to distinguish different agents. In this work, we classify each utterance into  X  X gent X  speech and  X  X ustomer X  speech.

Secondly, some words are more frequently used by one speaker than by the other speaker. We call these words speaker-identifying keywords. Some examples of such speaker-identifying keywords are  X  X  X  (customer),  X  X ou X (agent),  X  X old X  (agent), and  X  X ppreciate X  (customer). We recognize the speaker of an utterance based on these speaker-identifying keywords in the utterance.

We extract speaker-identifying keywords automatically in the following way. For each word w that appears 10 times or more in the training data (i.e., 2,326 manual transcripts in this work), we compute p ( agent | w )and p ( customer Note that the manual transcripts are marked with accurate speaker identification. If the difference of the two probabili-ties is greater than 0.2, we regard w as a speaker-identifying keyword. Furthermore, w is regarded as an agent keyword if p ( agent | w ) is greater than p ( customer | w )andviceversa. Some examples of agent keywords and customer keywords are shown in Table 3. The table shows the 20 most frequent keywords for agent and customer.

The speaker of an utterance is determined based on the number of agent keywords and customer keywords in the utterance. If an utterance contains more keywords in the agent keyword list, the agent is regarded as the speaker of the utterance. In the same way, the customer is regarded as the speaker if the utterance contains more keywords in the customer keyword list. If the frequencies for the two key-word sets are equal, the opposite speaker from the previous utterance is assigned.
 We conducted evaluation of the speaker identification al-
Agent Keywords Customer Keywords Table 3: The 20 most frequent agent keywords and customer keywords. These words were extracted from 2,236 manual transcripts for an IT help desk. gorithm with manual transcripts. We divided the manual transcripts into a training set and a test set. The train-ing set contains 2,000 manual transcripts, and we extracted the agent keyword list and the caller keyword list from the training set. We then applied speaker identification on the remaining 236 manual transcripts. The proposed algorithm shows 74.67 % of F-score on average. The performance eval-uation results are shown in Table 4 in more detail.
In this work, we apply support vector machines for iden-tifying the call section type for utterances. The main idea of SVMs is to find a hyperplane which splits the positive examples from negative examples with the largest distance in between the two example sets [21]. SVMs have been suc-cessfully applied to many classification and regression tasks such as text categorization [11] and pattern recognition [2].
In this work, we use the LIBSVM implementation de-scribed in [4]. Particularly we use C-support vector clas-sification (C-SVC) with a radial basis function (RBF) ker-Perf. for Customer 71.04% 79% 74.81% Average Performance 74.90% 74.88% 74.67% Table 4: Performance evaluation results on 236 man-ual transcripts, which contain 16,224 utterances. Perf.forAgent and Perf.forCustomer are per-formance results for agent utterances and customer utterances respectively. F-measure was computed nel [21]. C-SVC solves the following problem: given training vectors x i  X  R n ,i =1 ,...,l and an answer vector y  X  R l . C-SVC is designed for two class classifi-cation problems. For multi-class ( k ) classification like our problem, LIBSVM uses the  X  X ne-against-one X  approach clas-sificationinwhich k ( k  X  1) 2 classifiers are constructed and each one trains data from two different classes.
We use the following features for utterance classification.
The utterance classification result is then used to parti-tion a call transcript into different sections. The partition is conducted by merging adjacent utterances by the following two steps.

First, we merge the utterances which are adjacent and are classified into a same call section type into a call section. This step can be done in a very straightforward way. The next step is to find very short call sections which are located between two long sections. The length of an utterance is determined in two ways. If the utterance has fewer than  X  words or the time duration of the utterance is shorter than  X  seconds, the utterance is regarded as short. In most cases, these call sections are result s of classification error due to small number of words in the utterances. We merge these short call sections into the previous section. In this work,  X  issetto5and  X  is set to 3 based on experimental results.
We describe initial experimental results and the perfor-mance of the presented system in this section.
Initial experiment was conducted with 100 ASR transcripts from an IT Help Desk contact center for a company. The ASR transcripts were randomly selected from the data set made available for our research. The test set comprises 13.2 hours of customer calls consisting of about 5,350 utterances. Table 5 shows more information on the experimental data.
An example of call segmentation result for an automatic call transcript is shown in Appendix A. The system found  X  X reeting X ,  X  X uery X ,  X  X esolution X , and  X  X losing X  sections in the given transcript. Utterance changes and the speaker identification information are also marked.

The call section segmentation system enables interesting analysis on contact center calls. For instance, it provides more detailed understanding on how a call was handled in terms of how long time was spent on each section. A call with short query section can mean a simple user question, and may be a good candidate for self-service. This kind of time analysis can also be done over entire calls a con-tact center handles over a time period. The analysis can help contact center managers to better understand on the operation and to optimize their contact center operations. Figure 1 and Figure 2 depict these scenarios respectively. Figure 1 shows the duration times for the call sections in the transcript shown in 6. Figure 2 shows comparative analy-sis results. The call X  X  handling times were compared with about 250,000 other calls. The comparison was done with calls handled by the same agent, with calls that belong to the same topic, and finally with all 250,000 calls. Figure 1: Call duration time for each call section found
For performance evaluation, we extracted utterances in the transcripts and manually annotated call section types to the utterances. We then conducted 10-fold cross valida-tion using 90% of the data for training and the rest 10% of the data for evaluation. The presented system showed 87.2% classification accuracy on average. The classification accuracy was computed as shown below.
 Figure 2: This chart shows a comparison result of a cur-Even though the system was trained on a small data set, the presented system shows reasonably good performance. We also compared our system with two other systems. The first system is an imaginary system which classifies all utterances into the most frequent section type (i.e.,  X  X eso-lution X  section for our data). The second system is a maxi-mum entropy-based segmentation system which is a widely used approach in natural language processing [17]. In this work, we built the maximum entropy-based segmentation by using the OpenNLP Maxent machine learning package [1]. The same set of features used for the SVM-based segmenta-tion system was used for the maximum entropy-based seg-mentation system. Figure 3 depicts the performance com-parison results. While the system accuracy varies, note that the SVM-based approach consistently outperforms the other two methods. The average accuracy of the SVM-based ap-proach is 87.2%, while the accuracy of  X  X esolution X -only method and maximum entropy-based segmentation is 75.4% and 81.2% respectively. It shows that the proposed sys-tem performance is about 12% higher than X  X esolution X -only method and 6% higher than the maximum entropy-based segmentation.

Lastly, we analyzed the significance of the feature set used in the experiment. Figure 4 shows the contribution of each feature type for call section segmentation. As we can see from the figure, all the selected features make strong contri-butions for call segmentation task. Especially, the section type of the previous utterance and the positions of the ut-terance in the call turn out to be the most important fea-tures. The reason for the relatively weak contribution by the and 80,160 tokens.
 Figure 3: The comparison of call section segmentation speaker identification feature is related to the performance of the automatic speaker identification recognition.
Most of previous work on spoken data processing has been focused on information retrieval and topic categorization of spoken documents. NIST SDR (Spoken Document Re-trieval) tracks implement evaluations of Spoken Document Retrieval (SDR) technology within a broadcast news do-main. NIST SDR involves the search and retrieval of ex-cerpts from spoken audio recordings using a combination of automatic speech recognition and information retrieval technologies [8]. Most of the ASR systems used in the tracks showed relatively high recognition accuracy (lower than 30% WER) because the broadcast news were professionally cre-ated. However, the mean average precision (MAP) of the participant systems were around 0.55 showing that SDR is still a difficult task.

Recently, text analytics on contact center calls have gained much attention from researche rs. Most of the work focuses on call classification and information retrieval.
Mamou et al. [13] presented an information retrieval sys-tem from contact center speech data. The task is expected Figure 4: The performance of the call section segmen-to be more difficult than the NIST SDR tasks due to lower ASR accuracy for such noisy data. The work exploits the additional information provided by word confusion networks (WCNs) to improve retrieval performance and shows that the mean average precision is improved using WCNs com-pared to the raw word transcripts.

Tang et al. [20] described a call-type classification system for an Information Technology (IT) Help Desk call center. The system assigns recorded conversations into a hierarchi-cal taxonomy of the call types. Similar to our approach, they also apply a classifier based on support vector machines.
While Tang et al. used a manually created topic taxon-omy, Roy and Subramaniam [19] proposed an automated method for building a topic taxonomy from contact center call recordings. In addition to topics, the topic taxonomy contains typical question and answers and general call statis-tics such as the average call handling time for the topics.
Busemann et al. [3] also presented an automatic topic clas-sification system for contact center messages. They used shallow text processing for feature extraction and machine learning systems for model creation and classification. How-ever, the target data for the system are e-mail messages not speech transcripts.
Christensen et al. [5] presented a maximum entropy ap-proach to find utterance and topic boundaries in news broad-casts. Similar to our approach, they use cue words and the pause length as features to recognize utterance boundaries.
Mishne et al. [14] described a system to find out-of-topic parts in call transcripts. The difference between the system and our system is two folds. First, the work finds only the out-of-topic section in the call. Second, the system works on manually transcribed call transcripts that have almost perfect word recognition.
This paper presents an automated system to segment con-tact center calls into seven different sections such as  X  X reet-ing X ,  X  X uestion X ,  X  X efine X ,  X  X esearch X ,  X  X esolution X ,  X  X los-ing X  and  X  X ut-of-topic X . The target data set is automatically transcribed calls for spontaneous speech conversations. The state-of-art speech recognition systems have a high word er-ror rate for contact center calls due to a large number of different speakers and foreign accent. The automatic speech transcripts used in the experiment show about 42% word error rate.

Even though the transcripts are very inaccurate, we have shown that a supervised machine-learning approach can be effectively used to find structure in those calls. The ex-perimental results show that the system works well with a relatively small amount of training data. The average ac-curacy of 10-fold cross validation on 100 automatic speech transcripts is 87.2%. The evaluation also shows that the system performance is about 12% higher than a baseline system which classifies all utterances into the most frequent section, and 6% higher than a maximum entropy-based seg-mentation.

The call segmentation system can provide contact centers with more detailed knowledge on the calls. Thus, it enables many applications for improving agent productivity and for identifying cost-reduction strategies. We have integrated the presented system into two contact center applications. The first application is a search system that finds previous calls which are identical or very similar to an in-coming call, and retrieves the solution given in the previous calls. The tool uses  X  X uestion X  section to identify similar calls and uses the  X  X esolution X  section to find the solution. The second application is topic categorization of contact center calls. The topic categorization shows a better result by using only  X  X uestion X  section than by using the entire transcript.
The future plans for performance improvement are in the following. Firstly, we plan to integrate our techniques for identifying utterance boundaries and speakers in the ASR system. Since we used an off-the-shelf ASR system, the tools can not utilize acoustic and prosodic features. Cur-rently, our system uses only pause length which is available in transcripts. However, incorp orating acoustic features and lexical features will produce more accurate utterance bound-aries and speaker identification.

Secondly, we plan to investigate the impact of the ASR X  X  word error rate on the call section segmentation system. We can can simulate different ASR systems with different WERs by arbitrarily introducing word mis-recognitions at a specific WER in manual transcripts. We then run the presented system in the manual transcripts and compare the results. [1] J. Baldridge. The opennlp project. [2] C. Burges. A tutorial on support vector machines for [3] S. Busemann, S. Schmeier, and R. G. Arens. Message [4] C. Chang and C. Lin. Libsvm: a library for support [5] H. Christensen, B. Kolluru, Y. Gotoh, and S. Renals. [6] J. Chu-Carroll and B. Carpenter. Vector-based natural [7] A. S. G. Tur, D. Hakkani-Tur and E. Shriberg. [8] J. Garofolo, G. Auzanne, and E. Voorhees. The trec [9] Y. Gotoh and S. Renals. Sentence boundary detection [10] L. M. D. P. G. S. H. Soltau, B. Kingsbury and [11] T. Joachims. Text categorization with support vector [12] Y. Liu, M. Harper, E. Shriberg, and A. Stolcke. Using [13] J. Mamou, D. Carmel, and R. Hoory. Spoken [14] G. Mishne, D. Carmel, and R. Hoory. Automatic [15] G. T. P. Haffner and J. Wright. Optimizing svms for [16] Y. Park, R. J. Byrd, and B. K. Boguraev. Automatic [17] A. Ratnaparkhi. Maximum entropy models for natural [18] G. Riccardi, A. Gorin, A. Ljolje, and M. Riley. A [19] S. Roy and L. Subramaniam. Automatic generation of [20] M. Tang, B. Pellom, and K. Hacioglu. Call-type [21] V. Vapnik. The Nature of Statistical Learning Theory .
