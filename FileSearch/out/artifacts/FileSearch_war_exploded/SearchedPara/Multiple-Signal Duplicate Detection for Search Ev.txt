 We consider the problem of duplicate document detection for search evaluation. Given a query and a small num-ber of web results for that query, we show how to detect duplicate web documents with precision  X  0 . 91 and recall  X  0 . 77. In contrast, Charikar X  X  algorithm, designed for du-plicate detection in an indexing pipeline, achieves precision  X  0 . 91 but with a recall of  X  0 . 58. Our improvement in re-call while maintaining high precision comes from combining three ideas. First, because we are only concerned with dupli-cate detection among results for the same query, the number of pairwise comparisons is small. Therefore we can afford to compute multiple pairwise signals for each pair of doc-uments. A model learned with standard machine-learning techniques improves recall to  X  0 . 68 with precision  X  0 Second, most duplicate detection has focused on text analy-sis of the HTML contents of a document. In some web pages the HTML is not a good indicator of the final contents of the page. We use extended fetching techniques to fill in frames and execute Javascript. Including signals based on ourricherfetchesfurtherimprovestherecallto  X  0 . 75 and the precision to  X  0 . 91. Finally, we also explore using signals based on the query. Comparing contextual snippets based on the richer fetches improves the recall to  X  0 . 77. We show that the overall accuracy of this final model approaches that of human judges.
 H.3.3 [ Information Storage and Retrieval ]: [Informa-tion Search and Retrieval] Algorithms, Measurement Compression distance, duplicate detection, machine learn-ing, search evaluation, similarity hash, web search Copyright 2007 ACM 978-1-59593-597-7/07/0007 ... $ 5.00.
Detection of duplicate or near-duplicate web pages is both an important problem for web search engines, and a very dif-ficult problem. Several algorithms have been proposed and evaluated in recent years [11, 8, 3, 9, 2, 3]. Most approaches can be characterized as different types of distance or overlap measures operating on the HTML strings fetched for pairs of URLs being compared. State-of-the-art algorithms, such as Broder et al. X  X  [2] and Charikar X  X  [3], achieve reasonable precision and/or recall, but do not approach human-level accuracy.

The problem we address in this paper is duplicate detec-tion as a part of search evaluation. Highly accurate du-plicate detection is critical for obtaining accurate relevance measurements. To see this, suppose we are interested in measuring the aggregate relevance of results served by var-ious search engines, for a set of sample queries. We obtain a relevance score from a human judge for each of the top N results of each query, from each engine being measured. We can roll these scores up to an aggregate relevance met-ric for each engine, using a technique such as nDCG [10]. Duplicates can affect our measurement in two ways. First, an engine may receive undue credit for returning duplicate results for a given query. Unaccounted for, an engine that returns four copies of the (highly rated) appropriate home-page for a navigational query will receive credit four times over. In reality, users of the engine will be annoyed, not four times happier, and the useless duplicates that are returned will push other potentially good results down in the results list. Second, rating noise can be introduced when two du-plicate results from different engines are not detected, and thus are judged separately and potentially receive different relevance scores. Similarly, rating noise is introduced if two non-duplicates are wrongly detected as duplicates, assum-ing this means only one of them will be judged and they will receive an identical relevance score. Therefore, accurate relevance measurement in this framework requires duplicate detection with both high precision and high recall.
Duplicate detection approaches used by modern search engines are designed for very high scalability. As part of building an index of web pages, an engine must efficiently identify duplicates among the billions of URLs being crawled and indexed. Because of this, recent approaches have fo-cused on transforming the string returned by a URL into a single compact hash or  X  X ingerprint. X  Comparing these fingerprints is very inexpensive and gives a measure of sim-ilarity/distance between the content of the corresponding web pages. Typically, these algorithms operate on content that is directly fetched via the URL (what is fetched by wget or curl )  X  which in many cases is very different from what a user will see in a browser, due to constructs such as frames, Javascript, and other dynamic content. Fully  X  X endering X  the page like a browser does, by fetching embedded frames, executing Javascript, and so on would simply be too costly.
For search evaluation, howe ver, we can afford to explore more expensive approaches. We only need to find dupli-cates among the top search results for a given query, from some small number of search engines being measured (for instance, the top five results from three engines, giving up to 15 total URLs); and, for search evaluation, we only need to do this for hundreds or thousands of sample queries. Even accounting for measurement over time (where we might see a new URL tomorrow that is a duplicate of one we rated today), the numbers are not large.

We present an approach to duplicate detection for search evaluation that achieves precision and recall approaching human-level accuracy. The approach has three key elements:
We evaluate performance using a collection of 34,577 URL pairs derived from major search engines X  top results on 500 randomly-selected queries; each pair is labeled by multiple human judges as duplicate or non-duplicate. Because we want to maximize both precision and recall in a balanced way, we use a balanced F-score (2PR/(P+R)) as an over-all accuracy measure, where P represents precision and R recall. Taking Charikar X  X  [3] random-hyperplane hashing al-gorithm as a baseline, we find that using multiple signals combined via a decision-tree classifier improves the F-score by approximately 6 percent. Adding in signals derived from rendered content gives an additional 5 percent improvement, and adding query-based signals yields another percent. In aggregate, these methods reach an accuracy level that ap-proaches the average accuracy of individual human judges.
Section 2 describes related work. Section 3 presents our duplicate detection method and describes several of the sig-nals we use. Section 4 describes our evaluation methodology and the human-scored data we collected. Experimental re-sults and a comparison of our method X  X  accuracy to previous algorithms appear in Section 5. We provide conclusions and possible extensions in Section 6.
The majority of the work on duplicate document detec-tion has focused on the problem of finding all duplicate doc-ument pairs without comparing all pairs of documents in the corpus. Along these lines, Manber [11] introduced the idea of considering all k-byte sequences in the text and fin-gerprinting the document with the sequences that hash to a bit sequence ending in c 0 X  X  where c is a parameter. This work was built on by Heintze [8] to develop a system for plagiarism detection. Broder et al. [2] applied a shingling approach similar to Manber [11] and Heintze [8] to the web. This approach was extended by Schleimer et al. [14] so that two documents that overlap for a long enough sequence are guaranteed to have at least one matching fingerprint stored.
Brin et al. [1] and Shivakumar and Garcia-Molina [15, 16] explored schemes for detecting plagiarism using sentence overlap and word frequency comparisons. These schemes compute multiple fingerprints per document and two doc-uments are considered similar if a large fraction of their fingerprints agree. Shivakumar and Garcia-Molina looked at applying these schemes to the web where naively consid-ering all pairs that have at least one fingerprint match is inefficient [17].

Charikar [3] developed a technique for fingerprinting doc-uments that allows one to estimate the cosine similarity be-tween documents quickly. The idea is to choose a set of ran-dom hyperplanes and then record for each document which half-space of each hyperplane contains the document term vector. We use an implementation of Charikar X  X  algorithm as one of the signals for our system.

Recently, Henzinger [9] compared Broder et al. X  X  algo-rithm and Charikar X  X  to determine the precision of each at a fixed recall. Henzinger adjusted the parameters of Charikar X  X  algorithm to have the same storage needs as that of Broder et al. X  X  and achieve the same recall. The preci-sion of each algorithm was then estimated. Henzinger found that both algorithms had higher precision when comparing documents from different sites. Since Charikar X  X  algorithm identified a higher percentage of duplicate pairs from dif-ferent sites, it also achieved higher precision than Broder X  X  algorithm. Henzinger then used the algorithms in combina-tion to produce a new algorithm that gets a better trade-off between precision and recall.

Calibrasi and Vitanyi [4] worked on clustering groups of similar documents using distance measures based on com-pression algorithms. Given a compression algorithm, the normalized compression distance between two documents A and B is defined as where AB is the concatenation of A and B and c ( X )isthe compressed size of document X . Intuitively, if two docu-ments are similar, the compression of their concatenation is similar in size to each of their individual compressed sizes, i.e., one document does not add much content beyond what is already in the other. For this study, one of our signals is based on the normalized compression distance.
 Finally, in the area of query-specific duplicate detection, Gomes and Smith suggested using text around the query terms as a means for comparing documents [7]. This method provides another signal we use in our system.
Our approach to duplicate detection differs from the pre-vious approaches in three respects. First, instead of relying on a single predictive signal we make use of multiple signals. The intuition is that each signal uses a different dimension to compare pages, and the combination of these signals is a better predictor than any signal alone. Second, we use richer fetching techniques to bring in a rendered body which in some cases is more representative of the page as rendered in a browser. Third, since our focus is on duplicate detec-tion in the setting of search evaluation, we consider signals that estimate how similar the pages are with respect to a specific query.
When considering whether two pages are duplicates, pre-vious approaches have focused on the HTML that can be fetchedwitha wget command. In a significant percentage of cases, the contents of the HTML bodies (fetches) do not reflect what is rendered in a browser. In these situations, algorithmic duplicate determination based on the original fetches is usually not accurate.

We address two situations in which the fetched HTML is not representative of the rendered appearance of the page: Redirects: In some cases the initial page returned with a Frames: A fetch of a web page using frames returns the
Our approach is to render pages with a Mozilla-based fetcher similar to Crowbar [6]. After the page is loaded in the fetcher, we execute the equivalent of  X  X iew source X  in Firefox to generate the effective HTML of the URL. We do not wait for redirects with long delays. We incorporate frames by grafting their contents into the surrounding doc-ument at the place of the frame declaration; this produces invalid HTML, but is a good base for our content analysis.
Fetching and rendering is much more expensive than sim-ply fetching the content of the initial URL: with external frames and scripts many more URLs need to be retrieved and rendering a page in a browser is CPU intensive. To reduce bandwidth requirements, we do not fetch images. Moreover, we observe that for many URLs, the effective HTML does not differ much from the original HTML. There-fore an interesting area of future work is to develop heuristics for whether expensive rendering of a page is worthwhile or not.
For a pair of URLs, we compute a number of different signals based on the URLs, the HTML bodies, the rendered bodies and the text of the query used to score the relevance of the URLs. We use only one URL-based signal, namely an exact match on the domain of the URL. In addition, we com-pute body-based signals from both the fetched HTML and therenderedbodiesaswellas query-specific signals based on the text appearing near the query terms in the page. We begin by describing the body-based signals and then discuss the query-specific signals.
For each pair of URLs, we fetch both an HTML body and a rendered body. (See Section 3.1.) For each of these bodies, we compute a number of distance metrics. Some of these metrics have been explored in the literature as stand-alone duplicate detection algorithms but our focus here is on using them as one of many signals in a larger duplicate detection system.

Exact title match: The simplest signal we use is a check of whether the titles of the two documents exactly match. Despite the simplicity of this signal, it turns out be very useful for finding pairs of documents that are incorrectly considered duplicates by more sophisticated algorithms.
Similarity-hash distance: We use Charikar X  X  algorithm for hashing document term vectors to a 64-bit hash [3]. This hashing scheme ensures that the fraction of bits that agree between two documents X  hashes is, in expectation, equal to the cosine similarity between the documents X  term vectors.
Tf-idf distance: The tf-idf score of a term, namely the product of the frequency of the term in a document divided by its frequency in the corpus, is a commonly used con-cept in information retrieval. For duplicate detection, we compare the top 100 tf-idf terms in each document, using a logarithmic idf table. Let L X be the list of terms from document X and pos ( w,L X ) be the position of term w in L
X .Fortwodocuments A and B , we compute a distance between L A and L B which is biased towards the movements of top terms in the lists. In particular, for documents A B and a term w , we define their weight as: weight A,B ( w )= 1
Without loss of generality, assume that | L A | X | L B | .Then the tf-idf distance between A and B is given by:
Note that if | L B | &gt; | L A | then we normalize the tf-idf distance based on L B instead.
 Body-length distance: Let len ( X ) be the length of the HTML body for document X . Then we define the body-length distance between documents A and B as bld( A,B )= Modified normalized compression distance (mcd): We apply the concept of compression distance to our du-plicate detection problem using the gzip algorithm as the underlying compression algorithm. We modify the formula for compression distance to increase the accuracy for docu-ments that are similar to each other. We define the modified normalized compression distance between documents A and B as mcd ( A,B )= max
Note that the modified normalized compression distance is 0 if and only if c ( AA )= c ( AB )= c ( BB ). In contrast, the normalized compression distance may be non-zero for identical documents since c ( AA ) is not guaranteed to equal c (
A ). Our modification means that we can distinguish be-tween document pairs that are identical and pairs that are very similar to each other at the cost of decreasing the ac-curacy of the measurement for v ery dissimilar pairs. This is acceptable because the difficult part of accurately detect-ing duplicate documents lies in distinguishing pairs that are duplicates from pairs that are merely very similar to each other.
In search evaluation, the question of whether two URLs are duplicates is made in the context of the query for which these two URLs were returned as good search results. Two pages are duplicates if their content is essentially the same in light of the query. For each URL we extracted contextual snippets from the document that appeared near the query words. This text was similar to the text that users see on a search engine X  X  results page as a description accompanying a search result. However we found that a few modifications allowed the snippets to be more accurate for duplicate detec-tion. First, we used much longer snippets than would typi-cally be presented to a user in a search result display. Sec-ond, search engines sometimes draw snippets from sources other than the visible text of the page, such as meta tags or ODP entries [12]. In our case, we restricted the snippets used as a duplicate detection signal to be from the visible text of the page.
To generate training and test data, we first randomly sam-pled 500 English queries from Google X  X  US querystream in mid 2006. Each query was run on three search engines, and the first five search results were recorded, yielding up to 15 unique URLs for each query. We then asked four human judges to indicate duplicate/non-duplicate for each pair of URLs associated with each query.

In reality, judging whether two URLs are duplicates is of-ten very difficult; there are many  X  X dge X  cases. After some initial trials, so that we could get consistent data, we pro-duced fairly detailed instructions to guide our human judges on some of the more common hard cases. Examples that we indicated should be considered duplicates included: Examples that we indicated should not be considered dupli-cates included:
Using these instructions, four judges per URL pair had full agreement on 98.5% of pairs. The remaining 504 pairs were reviewed by additional judges (and in the hardest cases, by the authors), until we had a final judgment for every pair. Our data set included 537 duplicates out of 34,577 pairs. Note that the small number of duplicate pairs in the sample is partly due to the fact that we filtered out exact duplicate URLs returned from two different engines since these are trivial to label correctly. It should be noted that the rater agreement for the class of duplicate pairs was only 83.0% whereas for the non-duplicate pairs it was 98.8%. This demonstrates that the difficulty of detecting duplicate documents, even for human raters, lies in distinguishing the duplicate pairs from the pairs that are merely very similar to each other.

Treating the final judgments as ground truth, an interest-ing accuracy baseline is the precision and recall of individual judges. We took the first, second and third rater who rated each pair and compared their rating to the final rating. The best F-score obtained was 85.5% with a precision of 77.4% and a recall of 95.4%. Since these ratings were also used to determine the final duplicate/non-duplicate judgement, this is possibly a higher score than an independent rater could achieve. Interestingly, humans do better at recall but have a lower precision than either Charikar X  X  algorithm [3] or our machine-learning approach.
Based on preliminary results we did not feel that a single duplicate detection algorithm could effectively distinguish duplicate documents with both high precision and high re-call. Our goal was t o understand how much each of the following three techniques improve duplicate detection: 1. use many signals computed from standard fetches and 2. do extended fetching to generate rendered bodies which 3. use signals based on the query.

To study this question, we used the publicly available data-mining toolbench Weka [18] to analyze our data. This tool enabled us to experiment quickly with a large variety of classification algorithms. In our initial tests, we were able to obtain much better results with classifiers that were tree-based or rule-based. This matched our intuition that the class of duplicates was not inherently linear, and that there were probably subclasses of duplicates definable by boolean conditions over various combinations of signals. To compare various feature sets, we used the average F-score from two classifiers: Jrip, a rule-based classifier ( Weka  X  X  implemen-tation of the Ripper algorithm [5]) and J48, a decision-tree classifier (a Weka implementation of the C4.5 algorithm [13]). Each classifier was run using 10-fold cross-validation a total of nine times each using different randomization seeds for learning and cross-validation splits. The numbers reported are for the models with median F-score.

In order to understand the usefulness of our three ideas for improving duplicate detection we considered the follow-ing groups of signals in Table 1. Group F alone represents a set of signals that can be easily computed from a stan-dard fetch. We use this set to evaluate how much improve-ment can be made over a single duplicate detection algo-rithm by combining multiple signals. Group R are the sig-nals that can be computed from rendered bodies. We added group R to group F to evaluate the combined effect of us-ing multiple signals and richer fetching techniques. When evaluating query-based signals we discovered that snippets derived from the fetched and rendered bodies behave dif-ferently. Therefore they are listed separately as groups FS and RS respectively. Note that body-length distance was not included in sets F or R . It turns out that body-length distance is not useful when modified normalized compres-sion distance is included in the set of signals. However, in practice, body-length distance is a much faster signal to compute and therefore in Section 5.3 we discuss the effects of replacing modified normalized compression distance with body-length distance.

We first show how much improvement can be gained from each of these three techniques for our US English-based dataset. We then discuss using this model on different lan-guages and queries from outside the US.
We first consider the predictive strength of the individ-ual signals, with precision and recall calculated over data 100 90 80 70 60 Figure 1: F-score of duplicate detection techniques: similarity hashing (simhash), Tf-Idf, F signals only, F + R signals, F + R + RS signals, and a human rater. for which the appropriate bodies were available (in some cases, we were unable to fetch the rendered bodies). The re-sults appear in Table 2, ordered by decreasing F-score. The strongest two signals are dr awn from the rendered bodies. This matches our intuition that these rendered bodies more closely match what the user sees, and when available often provide better predictors than signals based on the direct fetches. However, it should be noted that since only 90% of the pairs had rendered bodies for both URLs, their recall and F-scores over the full set of URL-pairs would be lower.
Figure 1 summarizes the improvement in duplicate de-tection accuracy as we cumulatively add groups of signals corresponding to our three techniques. We use the similar-ity hash distance as a baseline since it is commonly used in search-engine indexing systems. We also show the F-score for tf-idf distance since it is actually 2.2 points higher and represents the best F-score obtained by an individual signal computed from a standard fetch. Taking a combination of all fetched-body signals provides a 3.8 point improvement beyond tf-idf distance. Further adding signals computed from the rendered bodies adds another 5.3 points to the F-score. Finally, the addition of signals based on the rendered snippets (text surrounding the query terms in the rendered body) also appears to help (  X  1 . 4 points). Table 3 gives the breakdown of precision and recall for each of these signal sets. One interesting thing to note is that the improvement in F-score comes largely as a result of improving the recall while maintaining a high precision.

In Section 5.3, we repeat this analysis with the signals ordered by increasing computational cost.

The precision-recall curve for the model including all the signals in F , R ,and RS is shown in Figure 2. While the overall F-score our algorithm achieves is close to that of a human rater, it does so with noticably higher precision but lower recall. For our algorithm to match human-level recall, its precision would be below 20%. We suspect this may be because our algorithms do not detect bolierplate content au-for which rendered bodies were available.
 Figure 2: The solid line is the Precision-Recall curve for F + R + RS signals. The dotted line is the F-contour for the max achieved F-score of 84.0, just below that of the best human rater. tomatically whereas humans can quickly eliminate irrelevant sections of a web page and determine that the underlying core content of two pages is the same.

Somewhat surprisingly, the addition of snippets generated from the fetched HTML of the documents does not improve the F-score. Results appear in the second column of Table 4, where each row shows the average F-score first with and then without signals based on fetched-body snippets.

Our models were built from data based on queries that originated in the US and were made in English. A natural question is how well such models extend to queries for other locations and in other languages.
 We tested these classifiers on two data sets from Google X  X  United Kingdom and Chinese querystreams. We obtained our test sets in the same way we built our US training data, except that we sampled only 200 queries for each test set. For UK data we had 204 duplicates out of 14,328 pairs, and for Chinese data a significantly smaller 120 duplicates out of 14,377 pairs. Our results appear in Table 5. The aver-age F-score of 83.8 for UK queries indicates that the mod-els trained over US English queries fit well for UK English queries. However, these models do not translate as well to Chinese-query duplicate results, producing a maximum F-score of only 74.8. The small sample size in this dataset (only 120 duplicates) may be having an effect. In any event, more analysis is required to explain the discrepancy. Per-haps for double-byte languages, the results can be improved by explicitly training on double-byte language training sets. Figure 3: Performance gain while adding signals of increasing computational complexity. Signals com-puted from the rendered body are indicated by  X (r) X . Errors bars denote the lower and upper quartile of F-scores per signal set.
The increased performance of our duplicate detection al-gorithm comes at a computational cost. In a real implemen-tation, there are three classes of computationally intensive tasks, given below: 1. Obtaining the page content, which is made up of fetch-2. Processing individual pages, which includes the cost of 3. Duplicate determination. We first consider the sim-
Based on the above considerations, the signals can be or-dered by increasing computational cost. When building a system, a natural approach is to choose only enough signals to reach a desired F-score. F igure 3 shows the marginal gain in F-score obtained by adding increasingly expensive signals to our model. We grouped same title and same do-main together because individually they each provide very low F-score. Interestingly, combining just the three cheap-est signals  X  same domain, same title and body-length dis-tance  X  already gives an F-score above 70 which is as good as many of the more expensive signals alone. Adding sim-ilarity hash brings the F-score above 75. Computing the compression distance for fetched bodies does improve the F-score but a bigger increase is realized by adding signals based on the rendered content. This again matches our in-tuition that the rendered content is more indicative of what users see, and that it is beneficial to combine different kinds of signals (in this case, those from both fetched bodies and rendered bodies).

A practical embodiment of our methodology handling hun-dreds of queries and tens of thousands of URLs a minute is feasible. Figure 4 shows an example model when replacing the expensive modified normalized compression distances on bodies with body-length distance in the signal set F + R + RS .
Our experimental results show that the accuracy of du-plicate detection algorithms can be significantly improved by combining multiple text-based signals and computing those signals over both fetched and rendered bodies. We be-lieve the quality of our models could be increased further by (1) detecting and removing boilerplate from document bod-ies, (2) more fine-grained feature selection, (3) using more sophisticated URL-matching signals, and (4) training over larger data sets.

The increased performance we gained comes at a compu-tational cost. The resultant algorithm is only feasible over modest-sized problems. It would be interesting to explore the idea of combining multiple signals in other duplicate de-tection domains with stricter computational limits, such as web crawling and indexing.
 =&gt; class=notdup [1] S. Brin, J. Davis, and H. Garcia-Molina. Copy [2] A.Z.Broder,S.C.Glassman,M.S.Manasse,and [3] M. Charikar. Similarity estimation techniques from [4] R. Cilibrasi and P. Vitanyi. Clustering by [5] W. W. Cohen. Fast effective rule induction. In [6] Crowbar. http://simile.mit.edu/wiki/Crowbar . [7] B. Gomes and B. Smith. Detecting query-specific [8] N. Heintze. Scalable document fingerprinting. In [9] M. R. Henzinger. Finding near-duplicate web pages: a [10] K. J  X  arvelin and J. Kek  X  al  X  ainen. Cumulated gain-based [11] U. Manber. Finding similar files in a large file system. [12] Open directory project. http://www.dmoz.org . [13] J. R. Quinlan. C4.5: Programs for Machine Learning . [14] S. Schleimer, D. S. Wilkerson, and A. Aiken. [15] N. Shivakumar and H. Garcia-Molina. Scam: A copy [16] N. Shivakumar and H. Garcia-Molina. Building a [17] N. Shivakumar and H. Garcia-Molina. Finding [18] I. H. Witten and E. Frank. Weka: Practical machine
