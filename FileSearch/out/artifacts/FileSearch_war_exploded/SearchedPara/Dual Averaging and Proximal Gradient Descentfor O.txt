 Taiji Suzuki s-taiji@stat.t.u-tokyo.ac.jp Stochastic and online optimization approach is one of the most promising approaches to efficiently process learning tasks on big data. These days, the size of data is rapidly increasing in various domains such as natural language processing, image recognition, signal processing, and bio-informatics. We often encounter such huge data that can not fit in memory. In that sit-uation, sequential learning procedures such as stochas-tic and online optimizations are quite powerful tools. Moreover high dimensionality is also a common fea-ture of recent data. To cope with high dimensionality, recent developments of online learning algorithms have successfully involved sparse regularizations into on-line methods, e.g., Forward-Backward-Splitting (FO-BOS) and Regularized Dual Averaging method (RDA) ( Duchi and Singer , 2009 ; Xiao , 2009 ).
 The efficiency of online algorithms with sparse reg-ularizations relies on the efficiency of computing the proximal operation : min x {k x  X  q k 2 / 2 +  X  ( x ) } where  X  is a regularization function. However, for structured sparsity regularizations, it is often diffi-cult to compute the proximal operation. Structured sparsity is an important notion to capture complex structures of data. Examples of that include over-lapped group lasso, low rank tensor estimation, and graph lasso ( Jacob et al. , 2009 ; Signoretto et al. , 2010 ; Tomioka et al. , 2011 ). Alternating Direction Multi-plier Method (ADMM) is a promising optimization method to deal with these kinds of structured regular-izations ( Gabay and Mercier , 1976 ; Boyd et al. , 2010 ; Qin and Goldfarb , 2012 ). The most favorable prop-erty of ADMM is its generality. These days, ADMM has been gathering much attentions because it has a broad class of applications in machine learning, image processing, and statistics (see the survey of Boyd et al. ( 2010 )). However, ADMM is basically a batch method which needs to store the whole data in memory. To re-solve this issue, Wang and Banerjee ( 2012 ) proposed a new method that combines the two notions; stochastic optimization and ADMM. Theoretically, the method posses a favorable convergence property. However, the method needs to solve an exact non-linear optimiza-tion in each iteration.
 In this paper, we propose a couple of new online variants of ADMM: Online Proximal Gradient de-scent type method (OPG-ADMM) and Regularized Dual Averaging type method (RDA-ADMM). The core of our proposal is linearization of the loss func-tion. In the existing analysis of online ADMM ( Wang and Banerjee , 2012 ), the optimization of the loss function is assumed to be easily carried out or is given as oracle. However, in practice, that is not nec-essarily easy and should be carefully addressed. Our proposal gives a solution to this issue by utilizing lin-earization of the loss function. The main features of our algorithms are as follows:  X  (Efficiency) The computations for every update  X  (Generality) They are applicable to a wide class  X  (Easiness of implement) The algorithms are quite Moreover we show that the convergence rates of both algorithms achieve O (1 / timal ( Nemirovskii and Yudin , 1983 ). We also show that OPG-ADMM achieves O (log( T ) /T ) convergence for a strongly convex loss. Finally, we present numer-ical experiments to show the effectiveness of the pro-posed methods.
 Independently of our study, Ouyang et al. ( 2013 ) de-veloped the same algorithm as our OPG-ADMM. They gave the convergence rate of the expected risk as in our study. Moreover their analysis includes the tail proba-bility of the risk which is not addressed in this paper. In machine learning, we often encounter the following stochastic regularized risk minimization problem: where x is the optimization variable (usually called weight vector) contained in a closed convex set X  X  R m , w is a sample generated from an (unknown) underlying distribution (for example, w is an input-output pair in supervised learning settings), f ( x,w ) is a loss function that measures error of x for a sample w , and  X   X  ( x ) is a regularization function that is a penalty on complexity of x . We assume both f (  X  ,w ) and  X   X  (  X  ) are convex. In typical machine learning settings, we have only finite number of samples, and consider an empirical approximation of the expected risk: where { w t } T t =1 are i.i.d. samples drawn from the (un-known) distribution. This formulation includes several machine learning tasks such as SVM, logistic regres-sion, ridge regression and Lasso.
 To solve such a regularized risk minimization problem, a lot of  X  X atch X  type algorithms have been proposed ( Beck and Teboulle , 2009 ; Figueiredo and Nowak , 2003 ; Combettes and Wajs , 2005 ; Tomioka et al. , 2012 ). Since batch type methods maintain all ob-served samples during the optimization, such methods do not work when data are so large that they cannot fit in memory. Therefore we need an alternative approach to tackle large size problems.
 Stochastic optimization is a promising approach to deal with such large scale data. This approach se-quentially draws samples, one at a time, and ade-quately update the weight vector based on the sin-gle sample observed at the latest iteration. Here we introduce two representative stochastic optimization methods on the basis of which we develop new meth-ods. The first method has several different names including online proximal gradient descent, forward-backward splitting (FOBOS) and online mirror de-scent ( Duchi and Singer , 2009 ; Duchi et al. , 2010 ). Here we utilize the terminology online proximal gradi-ent descent (OPG). Let x t be the weight vector at the t -th step, and g t be a member of the sub-gradient of f (  X  ,w t ) evaluated at x t : g t  X   X  x f ( x,w t ) | x = x the update rule of OPG at the t -th step is as follows: (OPG) x t +1 =argmin where  X  t is a step size parameter. OPG achieves the minimax optimal regret bound. Typically  X  t is set to be decreasing, thus the step size shrinks as the itera-tion proceeds. The second method, Regularized Dual Averaging (RDA), is developed on an opposite spirit. the t -th step is as follows: (RDA) x t +1 = argmin In this approach  X  t is typically increasing, and the reg-ularization for the new step is vanishing. Thus RDA does not down-grade the importance of newly observed samples. RDA also achieves the minimax optimal re-gret, and it is reported that RDA well captures the regularization effect, that is, for sparse learning, RDA usually produces a sparser solution than OPG ( Xiao , 2009 ).
 The efficiency of these algorithms heavily relies on the fact that the proximal operation corresponding to the regularization function  X   X  can be efficiently computed. Here the proximal operation corresponding to a func-tion  X   X  is the map defined by the following display ( Rockafellar , 1970 ): For example, if the regularization function is L 1 -norm  X   X  ( x ) = C P m j =1 | x j | , then the corresponding proximal operation is the well-known soft-thresholding operation:  X  x = prox( q |  X   X  ) is given as  X  x j sign( q j ) max( | q j |  X  C, 0).
 However, the proximal operation can not be efficiently computed for structured regularizations as presented in Section 5 unless we develop a specifically tailored optimization method for each regularization function. In this article, we overcome this problem utilizing the idea of Alternating Direction Multiplier Method (ADMM). Here we describe the concept of ADMM. We once turn back to the batch situation. Instead of considering the naive optimization problem Eq. ( 1 ), we transform the problem into the following linear constraint optimiza-tion problem  X  : where  X   X  ( x ) =  X  ( Ax ) with a matrix A  X  R l  X  m and Y  X  R l is a convex set such that Ax  X  Y for all x  X  X . Here we assume it is easy to compute the proximal operation corresponding to  X  . ADMM splits the op-timizations with respect to x and y utilizing the aug-mented Lagrangian technique. The iterative scheme of ADMM for the problem ( 2 ) is as follows: x y  X  where  X  t is the dual variable and  X  is a given param-eter. One can see that in ADMM the optimizations with respect to x and y are separated into ( 3a ) and ( 3b ). ADMM can be seen as an approximated ver-sion of the method of multiplier that minimizes the augmented Lagrangian instead of executing ( 3a ),( 3b ) ( Hestenes , 1969 ; Powell , 1969 ; Rockafellar , 1976 ): min where the optimizations for x and y are not split, but are jointly optimized. On the other hand, in ADMM, thanks to the splitting technique, the update is easily carried out. As for the convergence prop-erties of ADMM, O (1 /n ) convergence was proven by He and Yuan ( 2012 ) and the linear convergence for strongly convex functions was shown by Deng and Yin ( 2012 ).
 These days, ADMM has been gathering much atten-tions because it has a broad class of applications in machine learning, image processing, and statistics (see the survey of Boyd et al. ( 2010 )). However, ADMM is basically a batch algorithm. In the next section, we develop two online versions of ADMM. In this section, we propose two algorithms, RDA-ADMM and OPG-ADMM, as online versions of ADMM. We give the convergence rates of these meth-ods. For a positive definite matrix Q , let k x k Q be p x  X  Qx . 4.1. RDA-ADMM We first introduce Regularized Dual Averaging ADMM (RDA-ADMM) which is a combination of on-line RDA and ADMM. Here we denote by G t an arbitrary matrix that is used in the t -th update in RDA-ADMM. G t can depend on any information ob-served until t -th step. We define  X  x t ,  X  y t and  X   X  x Then the procedure of RDA-ADMM is summarized in Algorithm 1 .
 The only difference from the batch-version ADMM is the update rule of x t (Eq. ( 4 )). The loss func-tion 1  X  g x . This can be seen as a linear approximation of the loss function, which makes the computation of the up-date much easier. y t and  X  t are replaced with their averaged versions  X  y t and  X   X  t . The averaging technique to access the past information at the current update. There is a regularization term 1 2  X  the step size. The term  X  2 t k Ax k 2 is a bit tricky. Un-like the original ADMM, there is 1 /t factor in front of k Ax k 2 . This is needed for a technical reason to show Algorithm 1 RDA-ADMM Input:  X  &gt; 0 , {  X  t } T  X  1 t =1
Initialize x 1 = 0 , y 1 = 0 ,  X  1 = 0 . for t = 1 to T  X  1 do end for return  X  x T := 1 O (1 / are same as the original ADMM.
 At the first glance, it seems that we need to solve a linear equation to obtain x t +1 . However, by setting G t =  X I  X   X  X  t t A  X  A with a sufficiently large  X  such that G t is positive definite, the update rule becomes drastically simple: where  X  X ( x ) is a projection of x onto the con-vex set X . The technique to employ a special G t to cancel the term k Ax k 2 is called linearization of ADMM and has been used also for the batch-type ADMM ( Zhang et al. , 2011 ). This is a quite advanta-geous point compared with the existing online ADMM method ( Wang and Banerjee , 2012 ).
 The update rule of y t is just a proximal operation cor-responding to  X  . Indeed that can be rewritten as Therefore, under our assumption that the proximal op-eration corresponding to  X  is easily computed, the up-date of y t is also efficiently carried out.
 Convergence Analysis Here, we give convergence analysis of of RDA-ADMM. To derive convergence rates, we assume the following conditions.
 Assumption 1. (A1) X and Y are compact convex sets with radius R , (A2) The sub-gradients of f (  X  ,w ) is bounded by G , i.e., (A3) The sub-gradients of  X  is bounded by L  X  , i.e., In this section, we suppose that G t =  X I  X   X  X  t A  X  A/t , and  X , X , X  t are chosen so that G t I for simplicity. The particular choice of G t is not essential, but just for simplicity. For general G t , we also obtain a similar bound with slight changes of expressions.
 Moreover we suppose that  X  t /t is non-increasing. Then, we have the following regret bound.
 Theorem 1. Let  X   X   X  R l be arbitrary. Then there exists a constant K depending on R,G,L  X  , X ,A, X  1 , X   X  such that, for all x  X   X  X ,y  X   X  Y satisfying Ax  X  = y  X  1 T  X  h  X   X  ,A  X  x T  X   X  y T i +  X  1 T The proof can be found in the supplementary material. Basically the proof is a combination of those of the original RDA ( Xiao , 2009 ) and ADMM ( He and Yuan , 2012 ).
 Here, we should notice the fact that ( x t ,y t ) produced by RDA-ADMM does not necessarily satisfies the lin-ear constraint Ax t = y t (see Eq. ( 2 )). Therefore we need to modify x t or y t so that the constraint holds. A naive approach is to use y  X  t := Ax t instead of y t . On the other hand, if A is invertible, it is also a natu-ral strategy to use x  X  t := A  X  1 y t instead of x t . We can show that both strategies achieve the minimax optimal rate for the expected risk. For notational simplicity, we define (i) Convergence analysis for the pair ( x t ,y  X  t ) . The pair ( x t ,y  X  t ) achieves the following convergence rate of the expected risk. Now let  X  y  X  t := 1 t P t  X  =1 y  X   X  be the concatenation ( w 1 ,...,w t ).
 Theorem 2 (Convergence rate of RDA-ADMM) .
 There exists a constant K depending on R,L  X  , X , X  1 ,A such that, for all x  X   X  X ,y  X   X  Y such that Ax  X  = y  X  , the expected risk of RDA-ADMM is bounded as The proof can be found in the supplementary material. It is shown by using Theorem 1 with a specifically chosen  X   X  and Jensen X  X  inequality. We again would like to remark that  X  x T and  X  y  X  T always satisfies the linear constraint A  X  x T =  X  y  X  T .
 The convergence rate is basically same as that of the standard RDA. The first term in the upper bound ex-presses how aggressively the estimator adapt to the newly observed data, and, on the other hand, the second term expresses a regularization on the esti-mator X  X  fluctuation. Roughly speaking, these two terms express explore and exploit trade-off. The third term is O (1 /T ) and is asymptotically negligible. Here  X  =  X  0 der this setting, one can show that RDA-ADMM yields O (1 / that The convergence rate O (1 / minimax optimal rate ( Nemirovskii and Yudin , 1983 ). Therefore our proposed algorithm achieves the opti-mal rate. Moreover, we observe that (  X  x t ,  X  y t ) approx-Indeed, Theorem 2 gives k A  X  x T  X   X  y T k 2 = O p (1 / for  X  t =  X  0 (ii) Convergence analysis for the pair ( x  X  t ,y t ) . In the same way as the convergence analysis for the pair ( x t ,y  X  t ), we also obtain the convergence rate for the linear constraint Ax  X  t = y t . Letting  X  x  X  t := 1 we have the following convergence bound (the proof can be found in the supplementary material).
 Theorem 3. Suppose that A is invertible. For all x  X   X  X ,y  X   X  Y such that Ax  X  = y  X  , there exists a constant K  X  depending on R,G,A, X , X  1 such that In particular, for  X  t =  X  0 bounded by C 2 / on R,G,A,B,L  X  , X , X  0 , X  .
 Comparing Theorems 2 and 3 , the difference of the up-per bounds is only the O (1 /T ) term, which is negligi-ble. Therefore, we have the (almost) same convergence rate for both strategies, (  X  x t ,  X  y  X  t ) and (  X  x  X  4.2. OPG-ADMM Here we describe our second proposal, OPG-ADMM, that is a combination of OPG and ADMM. The pro-cedure is summarized in Algorithm 2 .
 Algorithm 2 OPG-ADMM Input:  X  &gt; 0 , {  X  t } T  X  1 t =1
Initialize x 1 = 0 , y 1 = 0 ,  X  1 = 0 . for t = 1 to T  X  1 do end for The only difference from RDA-ADMM is the update rule of x t (Eq. ( 5 )). Instead of utilizing the averaged gradient  X  g  X  t x , the gradient at the current state g is used for the linearized loss function, and there is a proximal term 1 2  X  penalty to let x t +1 close to the previous state x t . The update rule of OPG-ADMM is more similar to the original batch ADMM than that of RDA-ADMM.
 Here again, we can avoid solving linear equation in the update of x t by choosing G t appropriately. If we set G t =  X I  X   X  X  t A  X  A with a sufficiently large  X  such that G t is positive definite, the update rule becomes as follows: x As in the case of RDA-ADMM, we have a similar con-vergence rate also for OPG-ADMM. Moreover, if we assume strong convexity on the loss function, we can show a tighter bound for OPG-ADMM. To incorporate strong convexity, we introduce a modulus of strong convexity,  X  , that is a non-negative real such that f ( x  X  ,w )  X  f ( x,w )+( x  X   X  x )  X   X  x f ( x,w )+ for all w and x,x  X   X  X .
 Theorem 4 (Convergence rate of OPG-ADMM) .
 Suppose G t =  X I  X   X  X  t A  X  A , and  X , X , X  t are chosen so that G t I . Under Assumption 1 , there exists a constant K depending on R,G,L  X  , X , X  1 ,A such that, for all x  X   X  X ,y  X   X  Y such that Ax  X  = y  X  , the ex-pected risk of OPG-ADMM is bounded as  X  2 T The proof can be found in the supplementary mate-rial. Now if we set  X  t =  X  0 / that OPG-ADMM shows O (1 / exists a constant C  X  1 such that E Moreover, if  X  &gt; 0, by letting  X  t =  X  there exists a constant C  X  X  1 such that vergence results as in the case of RDA-ADMM. See the supplementary material for the detailed proofs. 4.3. Related Work Recently, Wang and Banerjee ( 2012 ) proposed a simi-lar algorithm that is also an online version of ADMM. The different point from our algorithm is that, in the update of x , they don X  X  utilize the linear approxima-tion of loss function, but minimizes the loss function directly as follows: x This update also achieves O (1 / a general loss and O (log( T ) /T ) for a strongly con-vex loss. However we need to go through an ex-act non-linear optimization. This sometimes requires much additional computational cost. In particular, the optimization is not efficient when each draw is sub-batch , i.e., w t at each iteration is a bunch of sub-samples w t = { z t, 1 ,...,z t,M } and the loss function is a concatenation of the loss for each sub-sample, f ( x,w t ) = 1 function for sub-samples. The sub-batch technique is often used to stabilize the solution. In this situ-ation, the optimization requires more computational cost than one sample optimization. On the other hand, our method is hardly affected by the increasing size of sub-batch.
 Independently of our study, Ouyang et al. ( 2013 ) de-veloped the same algorithm as our OPG-ADMM.
 They also gave the convergence rate of the expected risk such as O (1 / and O (log( T ) /T ) for a strongly convex loss function. Moreover their analysis includes the tail probability of the risk which is not addressed in this paper. There are several applications of ADMM. In this sec-tion, we present some examples of structured sparse regularizations for which our online type ADMM method is effective.
 Overlapped group lasso The group lasso imposes a group sparsity as where G is a set of subsets (groups) of indexes, and x g is a restriction of x onto the index set g ( x g = ( x i ) If groups { g } g  X  G have no overlap, then the proxi-mal operation corresponding to the group lasso reg-ularization is analogous to the soft-thresholding op-eration. However, if there are overlaps, the proxi-mal operation can not be straightforwardly computed ( Jacob et al. , 2009 ; Yuan et al. , 2011 ). This difficulty can be simply avoided by setting A and  X  as follows. Divide G into sets G 1 ,..., G m each of which consists of non-overlapped groups, let Ax be concatenation of m -repetitions of x , that is, Ax = [ x ; ... ; x ], and let  X  ([ x 1 ; ... ; x m ]) = C P m i =1 k x i k G that  X  ( Ax ) = C P m i =1 k x k G the proximal operation corresponding to  X  can be ef-ficiently computed because, for q = [ x 1 ; ... ; x m ], Thus we can apply the ADMM scheme. See Qin and Goldfarb ( 2012 ) for applications of the batch ADMM to the overlapped group lasso.
 Graph regularization Assume that we are given a graph G . We put each coordinate of the weight vec-tor x on each vertex of G . In graph regularization, we impose variables on adjacent vertexes are similar. To do so, we consider the following type of regularization:  X  in the graph G and h is a penalty function on the discrepancy between adjacent variables. Fused lasso ( Tibshirani et al. , 2005 ) and Graph lasso ( Jacob et al. , 2009 ) are special cases of this formulation. Here we set A as the adjacent matrix which is a |E| X  dim( x ) matrix where each row of A corresponds to an edge ( i,j )  X  E and has 1 at the i -th component,  X  1 at the j -th com-ponent and 0 otherwise, i.e., Ax = ( x i  X  x j ) ( i,j )  X  X  Then, for  X  ( y ) = P e  X  X  h ( y e ) ( y  X  R |E| ), we have  X  observe that the proximal operation corresponding to  X  can be carried out by a concatenation of proximal operation for h on each edge e  X  E as in the previous examples. In this section, we demonstrate the performance of the proposed methods through synthetic data and real data. We compare our stochastic ADMMs (RDA-ADMM and OPG-ADMM) with the conventional stochastic optimization methods such as OPG, RDA and the Non-Linearized online ADMM (NL-ADMM) given by Eq. ( 6 )  X  . Through this section, we fix  X  0 = 0 . 01,  X  = 1 and  X  = 1. All problems are classification problems, and we employed logistic loss.
 In the experiments, we utilize overlapped group reg-ularizations. To run OPG and RDA, we need to di-rectly compute the proximal operation for the over-lapped group lasso penalty. To compute that, we em-ployed the-state-of-the-art dual formulation proposed by Yuan et al. ( 2011 ). As for stochastic ADMMs, we used the decomposition technique explained in Section 5 , and the pair (  X  x t ,  X  y  X  t ) = (  X  x t ,A  X  x t t -th step estimator. 6.1. Simulated Sparse Classification Tasks Here we compare the performances in synthetic data where an overlapped group lasso regularization is im-posed. We generated N = 512 input feature vectors { a n } N n =1 with dimension d = 32  X  32 = 1024 (each a n is of 1024 dimension). Each feature is generated from an i.i.d. standard normal. The training output mal distribution with the standard deviation 3.Here the true weight vector x  X  is constructed as follows: we generated a 32  X  32 sparse matrix such that only the first column is non-zero (generated from i.i.d. stan-dard normal) and other columns are zero, and set x  X  as the vectorization of that matrix.
 As the sample draw w t in the t -th step, we drew sub-batch of size 10, that is, we randomly picked up 10 samples { ( a n Then we imposed an overlapped group lasso regular-ization defined as follows. We converted a weight vec-tor x  X  R 1024 into a 32  X  32 matrix, denoted by X , and imposed column-wise and row-wise group regu-larizations:  X   X  ( x ) = C ( P 32 i =1 k X i,  X  k + P 32 C k x k block where C = 0 . 025. This is an overlapped group lasso regularization. Here the expected risk of a weight vector x is  X  ( x ) := 1 N P N n =1  X  ( c n ,a  X  n We independently repeated the experiments 10 times and averaged the excess expected risk:  X  ( X  x )  X  min x  X  ( x ). In Figure 1 , the excess expected risk is depicted against the CPU time. In this dataset, OPG-ADMM and RDA-ADMM show almost the same per-formances while NL-ADMM, RDA and OPG show slower convergence. The main reason for the slow con-vergence of NL-ADMM is because the nonlinear op-timization required in each iteration ( 6 ) takes much longer time than the iteration of our methods. We observe that RDA and OPG take even longer time to achieve a certain precision. This is because the proximal operation solved by the dual formulation ( Yuan et al. , 2011 ) consumes much time since it re-quires executing a constrained optimization.
 6.2. Real Data Set Finally, we show the experimental results on a real dataset,  X  X dult X   X  . Adult dataset consists of N = 32 , 561 training samples and 16 , 281 test samples with d = 123 dimensional feature vector with 0/1 values. In addition to the original 123 features, we took prod-ucts of features to give additional 15 , 129 (= 123 2 ) fea-tures. Then we concatenated the original features and the newly produced features to obtain total 15 , 252 features. The weight vector x is also 15 , 252 dimen-sional and we divided the weight vector into two parts x = [ x (1) ; x (2) ] corresponding to the original features and the product features respectively ( x (1) is 123 di-mensional and x (2) is 15 , 129 dimensional). We im-posed L 1 -regularization on x (1) and the block wise overlapped regularization introduced in the synthetic data on x (2) :  X   X  ( x ) = C ( k x (1) k 1 + k x (2) k block with C = 0 . 01.
 We again drew sub-batch of size 10 for each step t . We repeated the experiments 10 times and averaged the classification error on the test set. Figure 2 shows the averaged classification error as a function of CPU time. Here, OPG was excluded from the figure be-cause OPG showed much worse performance than the listed methods. We can see that RDA-ADMM shows the best performance followed by NL-ADMM, OPG-ADMM and RDA. Here again we observe that a heav-ier computation of each iteration of NL-ADMM causes a slower convergence than RDA-ADMM. On the other hand, RDA-ADMM requires a quite light computa-tion for each iteration that leads to a fast convergence. RDA-ADMM also outperforms OPG-ADMM. This is because RDA-ADMM induces a sparser solution that leads to a better generalization error. RDA showed quite slow convergence since the proximal operation required heavy computation because of the high di-mensionality. It did not go through sufficient number of iterations in 10 3 seconds, and as a result, did not reach a stable convergence phase. In this paper, we proposed two online variants of ADMM: OPG-ADMM and RDA-ADMM. The pro-posed methods are applicable to a wide range of struc-tured regularizations, efficiently computed and easy to implement. We have shown that both methods achieve O (1 / function, and OPG-ADMM achieves O (log( T ) /T ) for a strongly convex loss. In numerical experiments, our methods showed nice convergence behaviors. Over-all RDA-ADMM showed favorable performances com-pared with OPG-ADMM.
 An interesting future work is to justify whether RDA-ADMM can also achieve O (log( T ) /T ) convergence rate for a strongly convex loss or regularization func-tion.
 We would like to thank Hua Ouyang, Niao He, Long Q. Tran, and Alexander Gray for the communication. TS was partially supported by MEXT Kakenhi 22700289, Global COE Program  X  X he Research and Training Center for New Development in Mathematics, X  and the Aihara Project, the FIRST program from JSPS, initiated by CSTP.
 A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM J. Imaging Sciences , 2(1):183 X 202, 2009. S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eck-stein. Distributed optimization and statistical learn-ing via the alternating direction method of multipli-ers. Foundations and Trends in Machine Learning , 3:1 X 122, 2010.
 P. L. Combettes and V. R. Wajs. Signal recovery by proximal forward-backward splitting. Multiscale Modeling and Simulation , 4(4):1168 X 1200, 2005. W. Deng and W. Yin. On the global and linear convergence of the generalized alternating direction method of multipliers. Technical report, Rice Uni-versity CAAM TR12-14, 2012.
 J. Duchi and Y. Singer. Efficient online and batch learning using forward backward splitting. Journal of Machine Learning Research , 10:2873 X 2908, 2009. J. C. Duchi, S. Shalev-Shwartz, Y. Singer, and A. Tewari. Composite objective mirror descent. In
Proceedings of the Annual Conference on Computa-tional Learning Theory , 2010.
 M. A. T. Figueiredo and R. Nowak. An em algorithm for wavelet-based image restoration. IEEE Trans. Image Process , 12:906 X 916, 2003.
 D. Gabay and B. Mercier. A dual algorithm for the solution of nonlinear variational problems via finite-element approximations. Computers &amp; Mathematics with Applications , 2:17 X 40, 1976.
 B. He and X. Yuan. On the O (1 /n ) convergence rate of the Douglas-Rachford alternating direction method. SIAM J. Numerical Analisis , 50(2):700 X 709, 2012. M. Hestenes. Multiplier and gradient methods. Jour-nal of Optimization Theory &amp; Applications , 4:303 X  320, 1969.
 L. Jacob, G. Obozinski, and J.-P. Vert. Group lasso with overlap and graph lasso. In Proceedings of the 26th International Conference on Machine Learn-ing , 2009.
 A. Nemirovskii and D. Yudin. Problem complexity and method efficiency in optimization . John Wiley, New York, 1983.
 H. Ouyang, N. He, L. Q. Tran, and A. Gray. Stochas-tic alternating direction method of multipliers. In Proceedings of the 30th International Conference on Machine Learning , 2013.
 M. Powell. A method for nonlinear constraints in mini-mization problems. In R. Fletcher, editor, Optimiza-tion , pages 283 X 298. Academic Press, London, New York, 1969.
 Z. Qin and D. Goldfarb. Structured sparsity via al-ternating direction methods. Journal of Machine Learning Research , 13:1435 X 1468, 2012.
 R. T. Rockafellar. Convex Analysis . Princeton Uni-versity Press, Princeton, 1970.
 R. T. Rockafellar. Augmented Lagrangians and ap-plications of the proximal point algorithm in convex programming. Mathematics of Operations Research , 1:97 X 116, 1976.
 M. Signoretto, L. D. Lathauwer, and J. Suykens. Nu-clear norms for tensors and their use for convex mul-tilinear estimation. Technical Report 10-186, ESAT-SISTA, K.U.Leuven, 2010.
 R. Tibshirani, M. Saunders, S. Rosset, J. Zhu, and
K. Knight. Sparsity and smoothness via the fused lasso. Journal of Royal Statistical Society: B , 67(1): 91 X 108, 2005.
 R. Tomioka, T. Suzuki, K. Hayashi, and H. Kashima.
Statistical performance of convex tensor decomposi-tion. In Advances in Neural Information Processing Systems 25 , 2011.
 R. Tomioka, T. Suzuki, and M. Sugiyama. Super-linear convergence of dual augmented lagrangian al-gorithm for sparsity regularized estimation. Journal of Machine Learning Research , 12:1537 X 1586, 2012. H. Wang and A. Banerjee. Online alternating direction method. In Proceedings of the 29th International Conference on Machine Learning , 2012.
 L. Xiao. Dual averaging methods for regularized stochastic learning and online optimization. In Ad-vances in Neural Information Processing Systems 23 , 2009.
 L. Yuan, J. Liu, and J. Ye. Efficient methods for over-lapping group lasso. In Advances in Neural Infor-mation Processing Systems 24 , 2011.
 X. Q. Zhang, M. Burger, and S. Osher. A unified primal-dual algorithm framework based on Bregman iteration. Journal of Scientific Computing , 46(1):
