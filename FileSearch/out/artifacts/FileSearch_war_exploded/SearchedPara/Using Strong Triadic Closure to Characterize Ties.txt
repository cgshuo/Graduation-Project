 In the past few years there has been an explosion of social networks in the online world. Users flock these networks, creating profiles and linking themselves to other individuals. Connecting online has a small cost compared to the physi-cal world, leading to a proliferation of connections, many of which carry little value or importance. Understanding the strength and nature of these relationships is paramount to anyone interesting in making use of the online social net-work data. In this paper, we use the principle of Strong Triadic Closure to characterize the strength of relationships in social networks. The Strong Triadic Closure principle stipulates that it is not possible for two individuals to have a strong relationship with a common friend and not know each other. We consider the problem of labeling the ties of a social network as strong or weak so as to enforce the Strong Triadic Closure property. We formulate the prob-lem as a novel combinatorial optimization problem, and we study it theoretically. Although the problem is NP-hard, we are able to identify cases where there exist efficient al-gorithms with provable approximation guarantees. We per-form experiments on real data, and we show that there is a correlation between the labeling we obtain and empirical metrics of tie strength, and that weak edges act as bridges between different communities in the network. Finally, we study extensions and variations of our problem both theo-retically and experimentally.
 J.4 [ Computer Applications ]: Social and behavioral sci-ences; H.2.8 [ Database Applications ]: Data Mining; H.4 [ Information Systems Applications ]: Miscellaneous Strong Triadic Closure, Social Networks, Approximation Al-gorithms
The past few years have been marked by the emergence and explosive growth of online social networks. Facebook, LinkedIn, and Twitter are three prominent examples of such online networks, which have become extremely popular, en-gaging hundreds of millions of users all over the world. On-line social networks grow much faster than physical social networks since the  X  X ost X  of creating and maintaining con-nections is much lower. The average user in Facebook has a few hundreds of friends, and a sizeable fraction of the net-work has more than a thousands friends [20]. The social cir-cle of the average user contains connections with true friends, but also with forgotten high-school classmates, distant rel-atives, and acquaintances made through brief encounters. Many of the online connections correspond to weak, or no relationships in the physical world.

Understanding the strength and nature of online relation-ships is paramount to anyone interested in extracting some utility out of the online social network data. For moneti-zation purposes, knowing which relationships correspond to true friendships is of critical importance to advertisers who want to profile users based of their social circle and initiate viral marketing campaigns. For sociologists, knowing the relative importance of online relationships can have a signif-icant effect on the way they model and interpret dynamics and norms in the social network. For friendship suggestion algorithms, knowing which friends matter more can have an important impact on the produced link recommendations.
The problem of understanding the strength and nature of social ties has been studied in the past [12, 7, 6, 22]. Previous approaches rely on user characteristics in order to estimate the true affinity between two users. In this work we use solely the graph structure in order to derive the charac-terization of the ties within a social network. To this end we make use of the Strong Triadic Closure (STC) principle [4]. The STC principle has its roots into early works in Psychol-ogy [3, 15, 8], and it has been used in the study of social networks [4, 8]. Informally, the STC principle assumes that there are two types of ties, strong and weak, and it stip-ulates that it is not possible for two individuals to have a strong relationship with a common friend and not know each other. That is, it is not possible to have an open triangle in the network graph where both edges of the triangle are labeled strong.

We use the STC property to characterize the ties of a so-cial network by asking for a labeling of the edges of the social graph into strong and weak such that the STC property is satisfied. There is a trivial solution to this problem which is to label all edges weak. However, we believe that creating strong relationships is the main motivation for users to join, and actively engage with a social network, online or other-wise. Therefore, we look for a labeling that also maximizes the number of strong ties (or minimizes the number of weak ties).

We thus obtain the following two problems: the maxSTC problem where we ask for a labeling of the graph such that the STC property holds and the number of strong edges is maximized, and the minSTC problem where we seek to min-imize the number of weak edges. These are two novel com-binatorial optimization problems that are of independent theoretical interest. Both problems are NP-hard, and we thus look for efficient algorithms with approximation guar-antees. We show that this is not possible for the maxSTC problem. For the minSTC problem, we show that it can be expressed as a graph vertex cover problem on an appropri-ately defined graph, a problem known to have a constant factor approximation algorithm.

We also extend our formulation to capture more complex problems where new edges may be added to the graph, or ties in the network may be of different types. Of particular interest is the minMultiSTC problem where we seek to en-force a variant of the STC property in the presence of multi-ple types of strong edges. The problem of understanding the  X  X ype X  of an edge is something that arises naturally in prac-tice. Although there are specialized online social networks catering to different needs of the users (social, professional, informational), the boundaries between these different cir-cles are not always clear. It is often the case that there are multiple types of relationships in a single network. There-fore, it is important to be able to not only distinguish be-tween strong and weak ties, but also to differentiate between different types of relationships and social circles. We test our algorithms experimentally on real datasets. Our experiments demonstrate that the labeling we obtain based on the structural graph property of Strong Triadic Closure correlates well with empirical measures of tie strength. Furthermore, our labeling agrees with the celebrated  X  X trength of weak ties X  observation [8]. The edges between different communities in the social network are usually labeled weak, while the strong edges concentrate among the nodes of the communities.

In summary, in this work we make the following contribu-tions.
The rest of the paper is structured as follows. In Section 2 we review some of the related work. In Section 3 we formally define the problems we will study. In Section 4 we study the complexity of our problem, and in Section 5 we consider approximation algorithms. Section 6 considers extensions to the basic problem. Section 7 contains the experimental evaluation, and Section 8 concludes the paper.
In this paper we build upon the Strong Triadic Closure principle from Psychology. Strong Triadic Closure was first defined by Granovetter [8] in his seminal paper  X  X he Strength of Weak Ties X . Previously, Davis [3] and Newcomb [15] dis-cuss some evidence that this property exists in social net-works. The Strong Triadic Closure is discussed in detail in the book of Easley and Kleinberg [4]. They discuss the effect of the property on the structure of the network, and possible relaxations, but they do not consider the problem of labeling the edges of the graph to enforce the property. In the dis-cussion they also consider recent experiments [10, 16] which demonstrate a correlation between structural properties of an edge and a notion of strength measured in practice. We perform similar experiments in Section 7 where we study the correlation between the label of an edge and the empirical tie strength.

Recent work has considered the problem of assessing the link strength in a social network, using data from e-mails [14], phone calls [16], and social media [7]. Kahanda and Neville [12] develop a supervised learning approach to predict link strength from transactional information (communication, file trans-fers, etc) and differentiate between strong and weak rela-tionships in large-scale social networks. Gilbert and Kara-halios [7] develop a model for characterizing ties in a social network using features about the similarity and interaction between users. They validate their model on small-scale data collected with questionnaires. In a follow-up work, Gilbert [6] explores how well a tie-strength model developed for one social medium adapts to another in order to find relationships which transcend a particular medium. In ad-dition, Xiang et al. [22] develop an unsupervised model to estimate relationship strength from interaction activity and user similarity. They also handle heterogenous relationship strength (e.g. acquaintances, best friends). Their work is motivated by the theory of homophily from sociology, which postulates that people tend to form ties with other people who have similar characteristics. Jones et al. [11] found that the frequency of online interaction is a good predictor of strong ties.

Another direction of related research focuses on charac-terizing the type of a relationship between users. Tang, et al. [19] use user and link characteristics to build a genera-tive model which assigns the most likely type to a specific relationship. In a follow-up work, Tang et al. [18] extend their model for classifying the type of social relationships by learning across heterogenous networks. Their model incor-porates ideas from social theories such as structural balance and social status. Backstrom et al. [1] use only the structure of the Facebook graph to identify the romantic partner of a user. They propose dispersion as a new network measure for estimating tie strength.

Our work is similar to this line of research in the sense that we are also trying to characterize the strength and type of social ties. However, prior work relies heavily on user and link characteristics to derive this characterization. In our case we only make use of the graph structure. We use the Strong Triadic Closure principle to formulate our labeling problem as a discrete optimization problem.
Let G = ( V,E ) be an undirected graph that represents a social network, where the set of vertices V corresponds to individuals, and the set of edges E corresponds to the connections (ties) between these individuals. The goal is to produce a labeling of the ties in the social network as either strong or weak . We will denote this labeling as a function L G : E  X  { W,S } , which maps each edge e  X  E to a label W (Weak), or S (Strong). Abusing the notation, we will sometimes use L E to refer to the labeling of the set of edges in E .
 The goal is to find a labeling that satisfies the Strong Triadic Closure (STC) property, which is defined as follows.
Definition 1 (Strong Triadic Closure). Given a graph G , a labeling L G of the graph satisfies the Strong Tri-adic Closure (STC) property, if there exists no pair of edges ( u,v ) and ( u,w ) , such that L G ( u,v ) = S and L ( u,w ) = S , and ( v,w ) 6 X  E .
 Informally, the STC property requires that for every node u , it is never the case that u has strong ties with both v and w , yet there is no tie between v and w .

We want to label the edges of the graph into strong or weak, such that the labeling satisfies the STC property. It is easy to see that a trivial solution to this problem is to label all edges in the network as weak. However, we believe that people build social networks with the goal to create strong ties with other people, therefore, we ask for a labeling that satisfies the STC property while maximizing the number of strong ties. Equivalently, we can ask to minimize the number of weak ties in the network, while satisfying the STC property. Let S ( L G ) and W ( L G ) denote the number of strong and weak ties respectively produced by the labeling L G . We are interested in the following two problems. Problem 1 (Maximum Strength STC (maxSTC)).
 Given a graph G , find a labeling L G that satisfies the STC property and maximizes S ( L G ) .
 Problem 2 (Minimum Weakness STC (minSTC)).
 Given a graph G , find a labeling L G that satisfies the STC property and minimizes W ( L G ) .

In the following we show that the problems are NP-hard, and we consider approximation algorithms.
To establish hardness it is sufficient to consider one of the two variants. We will show that the maxSTC problem is NP-hard, since the proof is easier.

Before going into the hardness proof, we introduce some notation and provide some intuition about the problem. Let ( u,v ) , ( u,w )  X  E denote pair of edges in the graph G that share a common endpoint u . We say that the edges define an open triangle  X  ( u,v ) , ( u,w )  X  incident on u , if ( v,w ) 6 X  E . The first observation is that, according to the definition of the STC property, a labeling L G violates the STC prop-erty if and only if there exists at least one open triangle  X  ( u,v ) , ( u,w )  X  such that both ( u,v ) and ( u,w ) are labeled strong. In this case, we say that the open triangle violates the STC property; otherwise, we say that the open triangle satisfies the STC property. It is clear that a labeling L satisfies the STC property, if there is no open triangle that violates the STC property. Furthermore, it is also clear an edge ( u,v ) that does not belong to any open triangle should be labeled strong. The labeling of ( u,v ) does not affect the labeling of the remaining edges, since it cannot cause the STC property to be violated. Thus, when looking for the optimal solution, we only need to consider edges that par-ticipate in at least one open triangle.

For our reduction we will consider a special type of net-work: the ego-network G u of a user u . Assuming an under-lying social network G , consider a single user u  X  V of the network, and let N u be the friends of u . Let E u denote the set of edges from u to the nodes in N u , and E N the set of edges between the nodes in N u . The ego-network of a user u is defined as G u = ( { u } X  N u ,E u  X  E N ).

We will now define a simpler variant of the maxSTC prob-lem, which we will show that it is as hard as the maxSTC problem. In our new problem, given an ego-network G u we ask for a labeling of just the edges E u incident on the node u such that the STC property is not violated. That is, there is no open triangle incident on u that has both edges labeled as strong.

Problem 3 (maxEgoSTC). Given the ego-graph G u of user u , find a labeling L E u of the edges incident on node u that satisfies the STC property and maximizes S ( L E u
It is easy to show that the maxSTC problem is at least as hard as the maxEgoSTC problem.

Lemma 1. There is a polynomial-time reduction from the maxEgoSTC problem to the maxSTC problem.

Proof. Let G u be the ego-network of node u that is given as input to the maxEgoSTC problem. The reduction is straightforward: we create an instance of the maxSTC problem, using the the graph G u as input and asking for a labeling L E that maximizes the number of strong edges. The key observation is that whether or not the labeling L of the edges in E u satisfies the STC property is indepen-dent of the labeling L E N of the edges in E N . This follows from the fact that there exists no open triangle in G u that contains an edge from E u and an edge from E N . Such a tri-angle would have to be of the form  X  ( u,v ) , ( v,w )  X  , missing the edge ( u,w ). However this is not possible, since by con-struction ( u,w )  X  E u . Therefore, all open triangles in G contain either two edges from E u , or two edges from E Thus, we can label the edges E u and E N independently. Finding a labeling L E that maximizes the number of strong edges in G u while respecting the STC property requires to find labelings L E u and L E N that maximize the number of strong edges in E u and E N respectively. The labeling L is a solution of the maxEgoSTC problem. Note also that finding a labeling L E N that maximizes the number of strong edges in E N is an instance of the maxSTC problem, with the graph G N = ( N u ,E N ) as input More formally, consider the decision problem for the max-EgoSTC problem, where given a graph G u , we ask if there is a labeling L E u that has S ( L E u )  X  k . Consider also the de-cision version of the maxSTC problem, where given a graph G , we ask if there is a labeling L G that has S ( L Given the graph G u we create the graph G N and using bi-nary search on the value of ` we find the labeling L  X  G N maximizes S ( L G N ). Let S ( L  X  G N ) =  X  . Now we give the graph G u as input to the maxSTC problem and we ask if there is a labeling L G u such that S ( L G u )  X  k +  X  . Since the labeling of E u and E N are independent, there is a labeling L u with S ( L E u )  X  k , if and only if there is a labeling L with S ( L G u )  X  k +  X  .
 Lemma 2. The maxEgoSTC problem is NP-hard.

Proof. We will now show that the maxEgoSTC prob-lem is NP-hard by reducing the maxClique problem to it. Given an input graph G = ( V,E ) and a value k , the deci-sion version of the maxClique problem asks if there exists a subset V c  X  V of vertices of size at least k , such that the induced subgraph G c = ( V c ,E c ) forms a clique, where E c = { ( u,v )  X  E : u  X  V c ,v  X  V c } .

Given the input graph G = ( V,E ) to the maxClique problem, we create an instance of the maxEgoSTC prob-lem, by creating an ego-network G u consisting of an addi-tional node u and edges E u that connect node u to all the nodes V of G . That is, G u = ( { u } X  V,E u  X  E ). We ask if there is a solution of the maxEgoSTC problem of size at least k .

Let S  X  E u be the subset of edges in E u that are labeled strong according to a labeling L E u . Each edge ( u,v )  X  S in the ego-network defines a unique vertex v  X  V . Let V S  X  V denote the set of vertices defined by the set of edges S . The labeling L E u satisfies the STC property, if and only if the set of vertices V S defines a clique in the graph G . If the labeling of the edges in S satisfies the STC property, then no pairwise combination of edges from S can create an open triangle. Therefore, for every pair of edges ( u,v ) , ( u,w )  X  S , we have that ( v,w )  X  E , and thus V S defines a clique. On the other hand, if the labeling of the edges in S does not satisfy the STC property, then there must exist at least one pair of edges ( u,v ) , ( u,w )  X  S that define an open triangle. Therefore, the edge ( v,w ) 6 X  E and hence the set V S does not define a clique.

Therefore, there exists a labeling L E u for the maxEgoSTC problem such that S ( L E u )  X  k , if and only if, there exists a clique of size at least k in graph G .
Given that the two problems we consider are NP-hard, we look for approximation algorithms. The reduction from maxClique to maxEgoSTC preserves the approximation, so, following the result in [9] we cannot approximate the maxEgoSTC solution within a factor better than O ( n 1  X  ). Fortunately, we can do better for the case of the minSTC problem.

Theorem 1. There exists a 2-approximation algorithm for the minSTC problem.

Proof. Recall that a labeling L G satisfies the STC prop-erty, if there exists no open triangle that violates the STC property. That is, there is no open triangle  X  ( u,v ) , ( u,w )  X  such that both ( u,v ) and ( u,w ) are labeled strong. There-fore, for every open triangle, at least one of the edges of the open triangle must be labeled weak. We say that this edge covers the open triangle. The goal is to find the minimum set of edges that cover all open triangles in the graph.
Using this intuition we will show how the minSTC prob-lem can be mapped to the Minimum Vertex Cover ( minVer-texCover ) problem. Given a graph G = ( V,E ), a subset of vertices C  X  V is a vertex cover of the graph G , if for every edge ( u,v )  X  E , u  X  C or v  X  C . The minVertexCover problem, given a graph G looks for a vertex cover of G with the smallest number of vertices.

The mapping from minSTC to minVertexCover pro-ceeds as follows. Given a graph G = ( V,E ) that is input to the minSTC problem, let T denote the set of all open triangles in G . We create a dual graph G T = ( V E ,E T is input to minVertexCover as follows. For every edge e  X  E we create a vertex v e  X  V E . For every open triangle  X  e 1 ,e 2  X  X  X  T , we create an edge ( v e 1 ,v e 2 )  X  E T .
Given a labeling L G we define the set C to be the set of vertices v e  X  V E such that the corresponding edge e  X  E is labeled weak. If L G satisfies the STC property, then for every triangle  X  e 1 ,e 2  X  X  X  T at least one of the edges e must be labeled weak. Therefore, for every edge ( v e 1 ,v E T at least one of the two endpoints is included in the set C , and hence C is a vertex cover for the graph G T .
Furthermore, given a minimum vertex cover C  X  V E of the graph G T we can create a labeling L G by labeling as weak every edge e  X  E , such that v e  X  C , and the remaining edges as strong. Since C is a vertex cover for G T , by construction of G T it follows that every open triangle in G is covered by at least one edge labeled weak. Therefore, the labeling L
G respects the STC property. If C is the minimum vertex cover, then L G is the labeling with the minimum number of weak edges. If C is an  X  -approximate solution for the minimum vertex cover, then L G is an  X  -approximation of the minimum number of weak edges. It is well known that there is a 2-approximation algorithm for the minVertexCover problem [21, 2], which implies a 2-approximation algorithm for the minSTC problem.

In our experiments we consider two different approxima-tion algorithms for the minSTC problem: A 2-approximation algorithm that relies on finding a maximal matching for the dual graph G T ; A greedy O (log n )-approximation algorithm that constructs a vertex cover of G T by always selecting the node that covers the most uncovered edges. We discuss the details of the algorithms in Section 7.
We will now discuss some extensions and variations to the basic minSTC problem.
Consider the case that the graph G that we want to label consists of a full clique of n nodes, missing a single edge ( u,v ). Then, the best labeling we can obtain has n  X  2 weak edges (all the edges incident to either u or v ). However, we could obtain a labeling with all the edges labeled strong if we simply added the missing edge ( u,v ). Thus, we consider a new minimization problem, where, in order to guarantee the STC property, except for labeling existing edges as weak, we can also add new edges to the graph. The goal is to minimize the number edges added to the graph, and the number of edges in the original graph that are labeled weak. We refer to this problem as minSTC+ .

Problem 4 (minSTC+). Given a graph G = ( V,E ) , identify a set of additional edges E 0  X  V  X  V \ E and a labeling L G 0 of the graph G 0 = ( V,E  X  E 0 ) that satisfies the STC property such that W ( L E ) + | E 0 | is minimized.
The minSTC+ problem is also NP-hard. However, we can again view it as a coverage problem, and exploit the fact that there is a known approximation algorithm.
Lemma 3. There is a O (log n ) -approximation algorithm for the minSTC+ problem.

Proof. We will show that our problem can be modelled as an instance of the Minimum Hitting Set ( minHitSet problem. The minimum hitting set problem is defined as follows. Given a universe of elements U and a collection of subsets of U , S = { S 1 ,...,S n } , we want to find a subset C  X  U of minimum size, such that for each S i  X  S , S i  X  C 6 =  X  , that is, each set S i  X  X  is hit by C .

We can transform an instance of the minSTC+ prob-lem to an instance minHitSet problem using a construc-tion very similar to the one we used for transforming min-STC to the minVertexCover problem. Given the graph G = ( V,E ) the universe U is defined as the set of all pairs of the form ( u,v ) where u,v  X  V . For every open tri-angle t =  X  ( u,v ) , ( u,w )  X  of the graph G , we create a set S t = { ( u,v ) , ( u,w ) , ( v,w ) } . The goal is to find the small-est subset C of pairs in U such that we hit all the sets S Given a hitting set, we define the set E 0 as the set of pairs ( w,u )  X  C such that ( w,u ) 6 X  E . The remaining pairs in C correspond to edges in E and are labeled weak. The la-beling of graph G 0 satisfies the STC property since every open triangle in G is either covered by a weak edge, or it is closed by an edge in E 0 . We can assume that the additional edges are labeled weak, therefore, they do not create any new violating open triangles.

Given a solution to the minSTC+ problem, we can define a hitting set, by adding to the set C the pairs in E 0 , and the edges in E that are labeled weak. Since all open triangles in G are covered, this defines a hitting set.

The minHitSet problem is NP-hard, but the simple greedy algorithm that always selects the element that hits most sets that are not already hit is known to have a O (log n ) approx-imation ratio. Therefore, there exists a O (log n ) approxima-tion algorithm for the minSTC+ problem.
In the minSTC (or maxSTC ) problem, there are only two types of edges: strong and weak. We assume that the weak edges are the  X  X ess important X  ones, and we want to produce a labeling that maximizes the strong ones. We now consider the scenario where there are multiple types of strong ties. For example, when considering the social network of a spe-cific individual it would be useful to understand which links correspond to strong family ties, strong work ties, or strong friendship ties. In this case we want to identify the strong edges of each type.
 We model this problem using a natural extension of the STC property. Similar to before, the goal is to have as many strong edges as possible, such that there is no violating open triangle with both edges labeled strong. The difference is that with multiple types of strong edges, an open triangle is violating if both its edges are labeled strong and they are both of the same type. That is, it is ok for a user u to have strong relationships with users v , w and v , w to not be connected, as long as the type of relationship of ( u,v ) and ( u,w ) is different.

More formally, we assume a fixed number k of strong relationship types. We can view these types as k labels { S 1 ,...,S k } . We also have the additional label W for the weak edges. Given graph G = ( V,E ), we want to produce a labeling L E : E  X  X  W,S 1 ,...,S k } of the edges of a graph G . The labeling must satisfy the multi-Strong Triadic Closure property (multi-STC) which is defined as follows.

Definition 2 (multi-Strong Triadic Closure). Gi-ven a graph G , a labeling L E : E  X  { W,S 1 ,...,S k fies the multi-Strong Triadic Closure (multi-STC) property, if there exists no pair of edges ( u,v ) and ( u,w ) , such that ( v,w ) 6 X  E and L ( u,v ) = L ( u,w ) = S i , for some i  X  [1 ,k ] .
Similar to the STC property, we can trivially satisfy the multi-STC property by labeling all edges as weak. Our goal is again to maximize the number of edges labeled strong (of any type), or minimize the number of edges labeled weak. The maximization problem runs again into the problem of finding the maximum clique, so we study the minimization problem.

Problem 5 (minMultiSTC). Given a graph G , and k strong edge types, find a labeling L E that satisfies the multi-STC property and minimizes W ( L E ) .
 We can now prove the following theorem.

Theorem 2. The minMultiSTC problem is NP-hard for any k  X  2 . There is a O (log n ) -approximation algorithm for k = 2 . The problem is hard to approximate for k  X  3 , unless P = NP.
 Proof. For the proof, we will make use of the dual graph G
T that we constructed in Section 5. We can model our problem as a coloring problem on the dual graph G T , where we want to color the nodes of G T with k + 1 colors. There are k  X  X trong X  colors { S 1 ,...,S k } , one for each strong label, plus an additional  X  X hite X  color W for the weak label. We want a legal coloring of the nodes of the graph G T , where no two adjacent nodes can be colored with the same strong color. It is ok if we have two adjacent nodes having a white color. We want a legal coloring that minimizes the number of white nodes.

For k = 2 our problem is equivalent to the odd-cycle traversal problem [17], which given a graph asks for the min-imum number of vertices to be removed, so that the result-ing graph becomes bipartite. This problem is also known to be NP-hard, but there is a O (log n )-approximation algo-rithm [5].

For k  X  3 we can show that the problem is not only NP-hard, but also hard to approximate unless P = NP . The proof follows easily by observing that for a k -colorable graph, the optimal solution to the minMultiSTC problem has cost zero, that is, we do not need to use the white color. This implies that if there was an algorithm with bounded ap-proximation ratio, then for an input instance for which the optimal algorithm has cost zero, the algorithm would be able to produce a solution with zero cost as well; otherwise the approximation ratio is infinite. However, for k  X  3, finding a k -coloring of a k -colorable graph is NP-hard. Therefore, it is hard to decide if there is a solution to the minMulti-STC problem that has cost greater than zero. Therefore, the problem is hard to approximate, unless P = NP .
We note that for k = 2, the O (log n )-approximation al-gorithm makes use of linear programming for deriving the solution. We propose a simpler heuristic in Section 7.
The goal of the experiments is to study if the labeling we obtain by enforcing the STC property correlates with an intuitive measure of tie strength in practice. We perform a variety of experiments towards this end. Our experiments are on real data, and demonstrate the practical utility of our formulation and of the proposed algorithms. We use five different datsets in our experiments: Actors , Authors , Les Miserables , Karate Club and Amazon Books . Table 1 shows some statistics about our datasets. The col-umn  X  X eights X  indicates whether we can compute weights for the edges of the graph. The weight of an edge corre-sponds to the empirical strength of the connection. The col-umn  X  X ommunity Structure X  indicates whether there exists a known community structure in the graph.
 Les Miserables 7 7 2 54 Y es N o A mazon Books 1 05 4 41 N o Y es We now describe the datasets in detail.

The Actors dataset: We create a graph from a movie dataset collected from IMDB 1 , consisting of 3,125 movies made from 1945 to 2010, and 2,171 actors that participate in these movies. The actor graph contains a node for each actor in the data, and there is an edge between two actors if they have collaborated in at least one movie. For each node of the graph we also have information about the set of movies in which the actor has played. We prune actors who participated in less than 5 movies since we do not consider them to be significant members of the network.

The Authors dataset: This dataset was obtained from data downloaded from the DBLP site 2 . It consists of a col-lection of authors that have published papers in one of the major Data Mining, Databases or Theory conferences dur-ing the period between 1994 and 2013. The author graph contains a node for each author in the data, and there is an edge between two authors if they have collaborated in at least one paper. For each node in the graph we also have http:www.imdb.com http://dblp.uni-trier.de/xml/ information about the set of papers the author has written. We prune authors who wrote less than 3 papers since we do not consider them to be significant members of the network.
The Les Miserables dataset: This dataset contains the network of co-appearances of characters in Victor Hugo X  X  novel  X  X es Miserables X  [13]. Nodes represent characters of the novel, and there is an edge between two nodes if the pair of characters appear in the same chapter of the book. For each edge we have the number of such co-appearances between the two characters.

The Karate Club dataset: Zachary X  X  Karate Club dataset [23] is a social network of friendships between 34 members of a karate club at a US university in the 1970s. The information about the friendship was derived by ques-tionnaires filled out by the members of the club.

The Amazon Books dataset: This dataset contains a set of books about US politics published around the time of the 2004 presidential election which are sold by the online bookseller Amazon.com 3 . Edges between books represent frequent co-purchasing of the books. In addition, each node (book) is labeled as  X  X iberal X ,  X  X eutral X , or  X  X onservative X , depending on its political viewpoint. There are 43 liberal, 13 neutral and 49 conservative books in this dataset.
In Section 5, we proved that minSTC problem on the graph G can be mapped to the minVertexCover problem on the dual graph G T . Given the graph G , the dual graph G
T is constructed by creating a node for every edge of G , and connecting two nodes if the corresponding edges form an open triangle. The algorithms we consider work by con-structing an approximate solution to the minVertexCover problem. We now describe them in detail.

The Greedy Algorithm : The input to the algorithm is the graph G and its dual G T , and the output is a labeling of the edges of the graph G as strong or weak. The algorithm works by constructing a vertex cover of graph G T in a greedy fashion. Recall that a vertex cover of a graph is a set of vertices such that every edge of the graph has at least one endpoint in the set. Let C denotes the set of nodes which are selected by our algorithm. Initially C =  X  . At every step the algorithm selects the node v with the maximum degree in G
T , and adds it to the set C . It then deletes node v and all edges incident on v from graph G T . The process is repeated until there are no more edges in the graph G T . Given the set of nodes in C , we label the corresponding edges of graph G as weak. The remain edges are labeled strong. This algorithm is known to be a O (log n )-approximation algorithm [21].
If at any step of the algorithm more than one nodes have the same degree, we break ties by choosing the node that corresponds to the edge in G that participates in the fewest closed triangles in the graph G . This way, our algorithm tends to label as weak edges that participate in many open triangles and few closed triangles, a principle that agrees with our intuition of what a weak edge should be.

The MaximalMatching Algorithm: The MaximalMatch-ing algorithm also produces a vertex cover of the graph G by constructing a maximal matching for the dual graph G T A matching of a graph is a collection of non-adjacent edges of the graph, while a maximal matching is one where no additional edges can be added. The algorithm constructs the matching one edge at the time. Let M denote the set
Available by V. Krebs at http://www.orgnet.com/. of edges selected by our algorithm. Initially M =  X  . The algorithm selects the next edge to add to the set M by first selecting the node u with the highest degree in G T and then the neighbor v of u with the highest degree. If more than one nodes have the same degree then we break ties in the same way as in the Greedy Algorithm. We add edge ( u,v ) to M , and delete u , v and all edges incident on u or v from G The algorithm terminates when there are no more edges in the graph G T . Let C denote the set of vertices that are endpoints of the edges in M . Similar to before, we label as weak the corresponding edges of G , while the remaining edges are labeled as strong. This algorithm is known to be a 2-approximation algorithm [21].

Note that for both algorithms if there are vertices in the graph G T that have no incident edges, then the correspond-ing edges in the graph G will be labeled strong. These cor-respond to edges that participate only in closed triangles, or that are isolated in the graph G .

Table 2 shows the number of edges labeled weak and strong for the two algorithms on the five datasets we con-sider in this paper. Despite the better approximation ra-tio the MaximalMatching algorithm always produces a larger number of weak edges.
 Table 2: Number of strong and weak edges for Greedy and MaximalMatching algorithms.
 Les Miserables 128 126 106 148
Amazon Books 114 327 71 370
In this section we study the relationship between the as-signed labels and a notion of tie strength measured in prac-tice. Our experiments follow the line of experimentation in prior work [16, 10] where they study how structural features of an edge correlate with empirical tie strength.

For this experiment, we use the three datasets for which we can compute weights for the edges: the Actors dataset, the Les Miserables dataset and the Authors dataset. The weights on the edges correspond to the strength of the re-lationships: a strong and enduring collaboration between two nodes in the case of the Actors and Authors datasets, and high affinity in the storyline of the novel in the case of the Les Miserables dataset. Specifically, for the Actors dataset, the weight of an edge is the number of times that the two actors have collaborated; for the Authors dataset it represents the number of papers that they have written together; for the Les Miserables dataset, it is the number of co-appearances between two characters in the same chapter. The goal of this experiment is to test the validity of the edge labeling, by examining if there is a correlation between the assigned label and the weight of the edge. Mathematically, we will show that there is a statistically significant difference between the mean weight of strong and weak edges.

Table 3 shows the mean weight for the strong and weak edges for all the three datasets, using the Greedy and Maxi-malMatching algorithms. Clearly, for all of the datasets the strong edges have higher weight than the weak ones. The t -test reveals that the difference is statistically significant at a 5% confidence level. We can thus conclude that the label-ing of our algorithm agrees with the  X  X rue X  strength of the network ties.
 Table 3: Mean count weight for strong and weak edges for Greedy and MaximalMatching algorithms.

The frequency of common activity (e.g. collaboration) be-tween two users is obviously a strong indicator of tie strength. However it may also be an artifact of the general frequent activity of the two users. For example, two highly prolific researchers may collaborate on higher-than-average number of papers, but this may be simply due to the fact that they produce a lot of publications in general. An alternative mea-sure of tie strength is the fraction of the activity of the two users that is devoted to their relationship. We use Jaccard similarity to capture this idea. Recall that Jaccard similarity between two sets is defined as the ratio of their intersection over their union. In our case the sets correspond to the sets of activities in which the two users engage (e.g., movies, publications, etc), and the Jaccard similarity measures the fraction of their activities that are common.

For this experiment we use the Actors and the Authors datasets. For the Actors dataset the weight of an edge be-tween two actors is the number of movies in which they have played together, over the total number of movies in which at least one of the two actors has participated. Similarly, the weight of an edge between two authors is defined as the number of papers that they have written together over the total number of their papers. We cannot compute Jaccard similarity for the Les Miserables dataset, since we do not have the exact chapter appearances for each character.
Table 4 shows the mean Jaccard similarity for the strong and weak edges using Greedy and MaximalMatching algo-rithms. Again, for all of the datasets the strong edges have higher weight than the weak ones and the t -test reveals that this difference is statistically significant at a 5% confidence level. We note that in the case of Jaccard similarity, the gap between strong and weak edges is larger than before. It seems that our labeling is more adept at capturing this focused measure of tie strength.
 Table 4: Mean Jaccard similarity for strong and weak edges for Greedy and MaximalMatching algo-rithms.

Comparing the MaximalMatching and the Greedy algorithm we observe that they behave very similarly in terms of the mean weights of strong and weak edges. However, the Greedy algorithm produces consistently a larger number of strong edges, and it is intuitively more appealing.
Granovetter, in his seminal paper [8], demonstrated the importance of weak social ties in connecting individuals with information that is not readily available in their close social circle, such as new work opportunities. A possible expla-nation to this observation is nicely articulated in the book of David Easley and Jon Kleinberg [4], where they postu-late that weak ties act as bridges between communities in the graph. Communities hold different types of information, and the only way for an individual to obtain access to infor-mation from a community different than her own is through weak ties.

In accordance to this interpretation, given a labeling of the edges of a graph with known community structure, we would like most of the inter-community edges to be labeled weak, while most of the strong labels to be confined to intra-community edges. That is, edges that bridge communities should be labeled weak, while strong edges should serve as a backbone of the communities.
 Formally, let G = ( V,E ) denote the input graph, and let C = { C 1 ,...,C k } denote a partition of the nodes of the graph into k communities, which is also given as part of the input. Let E inter denote the set of edges ( u,v ) such that u  X  C v  X  C j for some i 6 = j , and let E intra denote the set of edges ( u,v ) such that u,v  X  C i for some i . Also given the labeling L
G of the graph G let W denote the set of edges labeled weak, and let S denote the set of edges labeled strong. We define the precision P W and recall R W for the weak edges as follows: Similarly, we define precision P S and recall R S for strong edges as follows: The numbers we are mostly interested in are R W and P S , that is, we want the bridging edges to be labeled weak, and the strong edges to be confined within the communities.
To test our hypothesis we need graphs with known com-munity structure. To this end, we use the Karate Club and Amazon Books datasets. For the Karate Club dataset it is well known [4] that there were two fractions within the members of the club, centered around the two trainers, that eventually led to the breakup of the club. For the Amazon Books dataset the communities are given by the political viewpoint of the books.
 Table 5: Precision and Recall for strong and weak edges for Greedy and MaximalMatching algorithms.
 Table 5 shows the results for the two datasets for the Greedy and MaximalMatching algorithms. The two algo-rithms behave similarly, but the Greedy algorithm performs better overall in terms of both precision and recall. We now study the labeling of the Greedy algorithm in more detail.
For the Karate Club dataset we observe that we have per-fect precision for the strong edges, and perfect recall for the weak edges. We visualize the results of the Greedy algorithm in Figure 1. The nodes are colored white and gray depend-ing on the community to which they belong. The thick red edges correspond to the edges labeled strong, and the thin blue edges to the edges labeled weak. We can see that strong edges appear only between nodes of the same group, while all edges that cross communities are labeled weak.
 Figure 1: Karate Club graph. Blue light edges rep-resent the weak edges, while red thick edges repre-sent the strong edges.

For the Amazon Books dataset the Greedy algorithm char-acterizes 114 edges as strong, out of which 92 connect books of the same type, thus yielding precision P S = 0 . 81. On the other hand, there are 70 edges that connect nodes from different groups, and 48 of those are labeled weak, yielding recall R W = 0 . 69. Of the remaining 22 edges that cross communities and are labeled strong, 20 are edges with one of the two endpoints being a book labeled as neutral. It is intuitive that people would co-purchase books of neutral viewpoint with liberal or conservative books, thus leading to strong connections. There are only two edges that connect a liberal and a conservative pair of books, and are labeled strong by our algorithm. These pairs are: ( X  X merica Un-bound X ,  X  X ise of the Vulcans X ), and ( X  X he Choice X ,  X  X ise of the Vulcans X ). After some investigation, we found out that, for the first pair, although the books  X  X merica Unbound X  and  X  X ise of the Vulcans X  belong to different categories (lib-eral and conservative respectively), they are both about the exact same issue: George W. Bush X  X  foreign policy. There-fore, there is a different latent dimension that groups them together, which can explain the strong relationship between them.
In this section we conduct experiments for the minSTC+ problem, where except for labeling edges as strong or weak, we can also add edges to the graph. To this end we use the greedy algorithm we described in Section 6. The algorithm works iteratively. At each step of the algorithm a pair of nodes ( u,v ) is selected which covers the most remaining open triangles. This pair is either an edge not currently in the graph, which, when added, closes the most remaining open triangles, or an existing edge, which, when labeled weak, covers the most remaining open triangles. We refer to this algorithm as the Greedy+ Algorithm.

Table 6 shows the number of strong, weak and added edges using the Greedy+ algorithm. We can see that, as expected, using added edges the number of strong edges increases. Ta-ble 7 shows the mean weight for the strong and weak edges for the Greedy+ algorithm. It is still the case that strong edges have higher mean weight than the weak edges, how-ever, compared to the results in Table 3, the strong edges have lower mean weight while weak edges have higher mean weight. Therefore, although the Greedy+ algorithm labels more edges as strong, it seems that some of these edges are of low weight.
 Table 6: Number of strong, weak and added edges for the Greedy+ algorithm.
 Table 7: Mean count weight for strong and weak edges for the Greedy+ algorithm.

To obtain further insight into the effect of added edges we look at the labeling produced by the Greedy+ algorithm for networks with known community structure. Table 8 shows the results for the Amazon Books and Karate Club datasets. We observe that the recall value R S of the Greedy+ algo-rithm is higher compared to that of the Greedy algorithm, while the P S value is almost the same. This means that the algorithm is successful at introducing strong edges within the communities as it was supposed to. We also compute the precision P A of the added edges, shown in Table 8. We define P A as the fraction of added edges that fall within the community. Clearly, the added edges serve the purpose of  X  X trengthening X  an existing community.
 Table 8: Precision and Recall for strong and weak edges for the Greedy+ algorithm
We also visualize the effect of the added edges for the case of the Karate Club dataset in Figure 2. For this dataset, the Greedy+ algorithm adds only one edge (the dashed black edge in the figure). Compared to Figure 1, the algorithm maintains all the inter-community weak edges, and all the intra-community strong edges produced by the Greedy algo-rithm, and it adds an additional four strong edges within the grey community. The added edge reveals the existence of a near-clique of seven users in the network, which can now be labeled strong.
 Figure 2: Karate Club graph. Blue light edges rep-resent the weak edges, red thick edges represent the strong edges, and the black dashed edge represents the added edge
Therefore, we can conclude that the Greedy+ algorithm is better than the Greedy algorithm in revealing the backbone of an existing community in the graph. However, this comes at the price of labeling as strong more low-weight edges.
We now consider the minMultiSTC problem where we have k different types of strong ties. For this problem, there is an approximation algorithm for k = 2, however, it makes use of Linear Programming, making it complex to imple-ment. There is no known algorithm for k &gt; 2.

We propose a heuristic algorithm that works for any k by making iterative calls to the Greedy algorithm. The algo-rithm starts by running the Greedy algorithm on the graph G , producing sets S 1 and W 1 of strong and weak edges re-spectively. We label the edges in S 1 as  X  X trong 1 X . Given the set W 1 we compute the subgraph G W 1 induced by the edges in W 1 . We can now repeat the same process on the graph G W 1 to obtain a new set of edges S 2 to label  X  X trong 2 X , and a new subgraph G W 2 . We continue iteratively until all k labels have been utilized.

More formally, the i -th iteration of the algorithm takes the graph G W i  X  1 as input (where G W 0 = G ), runs the Greedy algorithm, and produces the sets of edges S i and W i . The set S i is labeled as  X  X trong i  X , and the set W i is used to produce the graph G W i for the next iteration. This process is repeated until k iterations are completed. The set W in the final iteration is labeled as  X  X eak X . We refer to this algorithm as the MultiGreedy Algorithm.

We now experiment with the MultiGreedy algorithm. For simplicity we only consider the case where k = 2, that is, we only have two types of strong edges. For this experiment, we use as input graph an ego-graph , that is, the relationships of a single individual, and the edges between them. This is a common scenario in online social networks, where we want to be able to discriminate between different types of relationships of a specific individual.

Using the Authors dataset, we create two ego-networks centered around Jon Kleinberg, and Ravi Kumar, two re-searchers with diverse interests and collaborations. We prune co-authors with whom the author has less than 3 papers to-gether, so that we focus on the more meaningful collabora-tions. Tables 9 and 10 show the results. For Kleinberg we observe that the  X  X trong 1 X  edges correspond to collabora-tions related to the early Web research, and his association with IBM Almaden, while the  X  X trong 2 X  collaborations cor-respond to more theoretical publications. For Ravi Kumar, the  X  X trong 1 X  ties correspond to his time at IBM Almaden, while the  X  X trong 2 X  ties to the time at Yahoo. Our al-gorithm is able to differentiate between these two distinct types of relationships.
 Strong 1 R. Kumar, S. Rajagopalan, A. Tomkins, A.
 Strong 2 M. Sudan, D. P. Williamson Strong 1 A. Tomkins, D. Sivakumar, E. Upfal, P. Strong 2 V. Josifovski, S. Vassilvitskii, A. Z. Broder
In this paper we addressed the problem of characterizing the connections in a social network. We made use of the Strong Triadic Closure property, and we formulated a novel optimization problem where we look for a labeling of the edges of a graph into strong or weak, such that the STC property is satisfied and the number of weak edges is mini-mized. We studied the complexity of the ensuing problems, and we showed that for the minimization problem we can provide a constant approximation algorithm. We also con-sidered extensions of the basic formulation, to account for edge additions and networks where ties may have several different types. The experimental results demonstrate that the labeling we obtain makes sense in practice.

Our work leaves room for further research. More specifi-cally, it would be interesting to consider further relaxations of the STC property. It would also be interesting to have a stochastic model where each edge has a probability of be-ing strong or weak, rather than belonging exclusively to one class or another. These questions become more interesting when we have more than one types of strong edges. This work is supported by the Marie Curie Reintegration Grant project titled JMUGS which has received research funding from the European Union. [1] L. Backstrom and J. M. Kleinberg. Romantic [2] K. L. Clarkson. A modification of the greedy [3] J. A. Davis. Clustering and hierarchy in interpersonal [4] D. Easley and J. Kleinberg. Networks, Crowds, and [5] N. Garg, V. V. Vazirani, and M. Yannakakis.
 [6] E. Gilbert. Predicting tie strength in a new medium. [7] E. Gilbert and K. Karahalios. Predicting tie strength [8] M. Granovetter. The strength of weak ties. American [9] J. Hastad. Clique is hard to approximate within [10] B. A. Huberman, D. M. Romero, and F. Wu. Social [11] J. J. Jones, J. E. Settle, R. M. Bond, C. J. Fariss, [12] I. Kahanda and J. Neville. Using transactional [13] D. E. Knuth. The stanford graphbase: A platform for [14] G. Kossinets and D. Watts. Empirical analysis of an [15] T. M. Newcomb. The Acquaintance Process . New [16] J.-P. Onnela, J. Saramaki, J. Hyvonen, G. Szabo, [17] B. Reed, K. Smith, and A. Vetta. Finding odd cycle [18] J. Tang, T. Lou, and J. Kleinberg. Inferring social ties [19] W. Tang, H. Zhuang, and J. Tang. Learning to infer [20] J. Ugander, B. Karrer, L. Backstrom, and C. Marlow. [21] V. V. Vazirani. Approximation algorithms . Springer, [22] R. Xiang, J. Neville, and M. Rogati. Modeling [23] W. W. Zachary. An information flow model for
