 Click logs present a wealth of evidence about how users in-teract with a search system. This evidence has been used for many things: learning rankings, personalizing, evaluating ef-fectiveness, and more. But it is almost always distilled into point estimates of feature or parameter values, ignoring what may be the most salient feature of users X  X heir variability. No two users interact with a system in exactly the same way, and even a single user may interact with results for the same query di  X  erently depending on information need, mood, time of day, and a host of other factors. We present a Bayesian approach to using logs to compute posterior distri-butions for probabilistic models of user interactions. Since they are distributions rather than point estimates, they nat-urally capture variability in the population. We show how to cluster posterior distributions to discover patterns of user interactions in logs, and discuss how to use the clusters to evaluate search engines according to a user model. Because the approach is Bayesian, our methods can be applied to very large logs (such as those possessed by Web search en-gines) as well as very small (such as those found in almost any other setting).
 H.3.4 [ Information Storage and Retrieval ]: Systems and Software X  Performance Evaluation test collections, user logs, evaluation
Every user approaches search in their own way. Some users only ever use web search engines like Google, and they trust these engines implicitly to give them the only result they need at rank one. Others use web search engines for more involved tasks, clicking through several links, reformu-lating queries, and using advanced features of the system. Still more use other search systems (such as those that in-dex newswire, case law, or biomedical research) frequently; their approach in those systems is probably di  X  erent from their approach on the web. And of course these groups do not begin to cover the whole space, not to mention the fact that a single user may have di  X  erent approaches depend-ing on their information need, mood, and a host of other unmeasurable factors.

Recently there has been interest in developing measures that explicitly model user interaction, notably rank-biased precision (RBP) [10], expected reciprocal rank (ERR) [3], ex-pected browser utility (EBU) [16], and time-calibrated mea-sures [14]. These measures use parameterized probabilistic models of actions users may take when browsing ranked re-sults. But when the parameters are set to a single fixed value, these measures lose the ability to reflect variability in the user space. They essentially assume that however that single value is decided on, whether it be based on logs or based on pure rational thought, it reflects users as an ag-gregate. Like traditional measures, they may miss out on segments of the user population that experience search re-sults in very di  X  erent ways from the majority.

Fortunately, logs contain a lot of information about vari-ability in how users are interacting with the system. With the right methods, we can take advantage of that variability to more precisely measure the e  X  ectiveness of the system to di  X  erent segments of the user base. We present a Bayesian method for using logged user data to estimate probabilistic models of user interaction: starting with a prior distribu-tion over users, we use a log to update it and produce a posterior distribution representing variability over the user space. We then show how we can use such distributions to evaluate systems, find separate clusters of behavior types for more targeted analysis, and analyzing the log to formulate hypotheses about how users interact with the engine.
One advantage of our approach is that it does not require huge logs, and thus can be used for search systems or tasks that have much smaller user bases than web search (which is the  X  X raditional X  domain for applications of mining user logs). This contrasts with other log-based approaches to evaluation and analysis such as bucket testing [7] and inter-leaving [11], which rely on large amounts of data to draw statistically-powerful conclusions.

This paper is organized as follows: in Section 2, we intro-duce the idea of forming a probability distribution based on a structural user model and informed by user logs and show how to compute it. In Section 3, we illustrate the use of this idea to partition a log into smaller segments and analyze behavior within those segments. Section 4 takes this a step further, using clustering to automatically partition the log into segments that may not be obvious to people analyzing a log, and showing that some of the clusters that emerge are quite interesting. In Section 5 we provide evidence that evaluating system quality on the basis of the appropriate segments of user logs can o  X  er new insights into the perfor-mance of retrieval systems. We conclude in Section 6 with some directions for future work.
There are many e  X  ectiveness evaluation measures repre-sented in the information retrieval literature. Almost all of them can be seen as encoding a model of a user that in-teracts with ranked results by stepping down the ranking one-by-one, looking at each document, and deriving util-ity from those that are relevant. For example, precision at rank k could be seen as modeling a user that always goes to rank k (never beyond) and finds every relevant document to that rank. Average precision (AP) can be understood as modeling a user that browses the retrieved documents from top to bottom and is equally likely to stop at any rel-evant document. When the user stops at a particular rel-evant document d , average precision assumes that she has found every relevant document to the rank at which d is re-trieved [12] (this may be an unrealistic user model, but it is a user model nevertheless). Discounted cumulative gain has a more explicit user model, using graded judgments to model relative di  X  erences in utility amongst relevant docu-ments and a reciprocal-log discount to model waning interest on the part of the user in continuing down the ranking.
Recently there has been interest in describing user in-teractions with a parameterized probabilistic model. The first such example may be Cooper X  X  Expected Search Length (ESL) [4], which can be described as the average number of documents a user must examine before observing all doc-uments relevant to the query. But the probabilistic com-ponent in ESL is related more to a system X  X  approach to breaking ties than to anything about a user X  X  behavior. More recently, rank-biased precision (RBP) was the first measure to be based on an explicit probabilistic model of the user: it models a user who, at each rank, flips a biased coin to de-cide whether to continue to the next document. If the coin comes up tails, the user continues; otherwise, the user aban-dons the search. This results in the rank of abandonment k having a geometric distribution: P ( k )=(1  X  ) k 1  X  , where  X  is the probability that the coin comes up heads. Then RBP is defined as a sum over ranks k : Another way of understanding  X  is as the user X  X  impatience for continuing down the ranked results. The higher  X  is, the less patient the user is and the more likely they are to
Or patience; we use the two terms interchangeably. If  X  represents patience, 1  X  represents lack thereof, and the model can always be reparameterized. abandon the search early; likewise, the lower  X  is, the more documents they are likely to see during their session.
RBP X  X  model is very simple; there is much about user behavior that it does not model. The cascade model is a more complex model that has been shown to more accu-rately model behavior [5]. Like RBP, it assumes a user step-ping down ranked results one by one. Unlike RBP, it does not assume that the probability of continuing is constant over the ranking. In the most general form of the model in-troduced by Craswell et al. [5], each URL can have its own probability of continuing; these probabilities (which we will call  X  d ) could be based on features of the URL, the summary shown for it [16], prior distributions of dwell times [14], and even global features of the ranking or interface. Then the probability that a user stops at rank k is: Using a fixed  X  such that  X  d =  X  for all d produces RBP.
Chapelle et al. X  X  expected reciprocal rank (ERR) is an ef-fectiveness measure based on the cascade model and graded judgments. It defines  X  d as a function of the relevance grade of d , so that the probability of a user continuing depends on the relevance of the document they are currently looking at [3]. Thus we can say  X  d g =  X  g for all documents d judged by a human assessor to have relevance grade g , thereby re-ducing the parameter space to |G| , the number of grades. ERR is then defined as the expectation of reciprocal ranks: Chapelle et al. set  X  g =(2 g 1) / 2 g max ,with g 2 { 0 , 1 ,...,g [3]. This is a heuristic, though, and not based on direct em-pirical evidence pertaining to utility.

Rather than one parameter like RBP, ERR has as many parameters as there are grades in the evaluation. When there are five grades, there are five parameters (  X  0 &lt;  X   X  &lt;  X  3 &lt;  X  4 ) each representing the probability that a user stops after seeing a document of that grade. The highest grade in a five-grade system (usually called  X  X erfect X ) has the highest probability of stopping, modeling a user that is very unlikely to look at anything else after seeing the perfect result. These can also be thought of as representing  X  X atience X , but they are now more specifically patience given a particular level of relevance.

Expected browser utility (EBU) [16] is another evalua-tion measure that is based on the cascade model. Similar to ERR, EBU also assumes that the probability of a user con-tinuing after reaching a particular rank k depends on the relevance of the document at rank k , but adds even more complexity, including the probability that a user clicks on a link given the attractiveness of the snippet (summary) of the document at rank k . Further complexity is possible; the only limiting factors are computation time and the ability to estimate model parameters.
Since the parameters in a user model are meant to reflect user behavior, it makes sense to use logs of user behavior X  query logs X  X o estimate them [17, 18, 16, 9]. Most work focuses on estimating point values of parameters. But point estimates fail to capture variability in the population. Con-sider RBP X  X  patience parameter: some information needs will naturally require less patience than others, for exam-ple the query  X  X oogle X  versus  X  X inux guide X . Furthermore, some users will naturally have more patience than others for the same query; using a point estimate assumes that the model captures all users equally, i.e. each user has equal chances of being satisfied and stopping at the first rank of the results for all queries. We argue that it is more inter-esting to look at a distribution of a parameter and compute distributions of e  X  ectiveness evaluation measures over the distribution of the parameter rather than point estimates of evaluation measures.

In general, let  X  be a vector of parameters. Let P ( action |  X  ) be a model of user actions parameterized by  X  . For the RBP model, we could define action = stop at rank k, k 2 1 , 2 ,... and P ( k |  X  )=(1  X  ) k 1  X  . Here  X  is the probability that a user goes on to look at result k + 1 given that they are currently looking at result k , and can be thought of as  X  X m-patience X  just as we described above. Given no information about user patience, we may initially assume that all values of  X  are equally likely, e.g.  X   X  Unif (0 , 1). Then we could compute a distribution of RBPs over the population of users by sampling a  X  (which models a user) and then computing RBP with that  X  .
Of course, a uniform distribution does not come close to modeling the actual population of user patience. Fortu-nately we have a large source of data from which to glean information about user patience: a query log. Given a log L containing queries and clicks, we will want a posterior for  X  conditional on the log:
Carterette et al. [2] showed how to compute P (  X  | L )with a single pass over a log. First, for each individual search (unique query/user pair at a unique time) registered in the log, they count c (the number of clicked URLs) and r (the number of unclicked URLs before the last rank clicked); c is modeled as a function of r and  X  using the negative binomial distribution. Using a beta prior for P (  X  ), which is conjugate to the negative binomial distribution, gives the following three equations to compute P (  X  | L ) in one pass: Here M r is the total number of searches with r unclicked URLs and the sum searches with r unclicked URLs.  X  and are hyperparame-ters which we define to be 1. There is a special case when r and c are both 0: since there are no clicks from which to esti-mate patience, we also keep a flat prior that has probability of being sampled from equal to the proportion of searches for which r and c are both 0. The pseudocode in Algorithm 1 illustrates this.
 Algorithm 1 Compute posterior distribution P (  X  | L ) for ERR with a single pass over a log L .
 Require: Alog L in which each line has a user/cookie ID, Ensure: A multivariate distribution P (  X  | L ). 1: for each line ` in L do 2: c total number of clicks 3: k last rank of deepest click 4: for each grade g with a URL in ` do 5: if c =0 then 6: M g [ null ] M g [ null ]+1 7: end if 8: k g min rank of URL with grade g 9: c g number of clicks on documents with rank k g 10: if c g &gt; 0 then 12: M g [ r ] M g [ r ]+1 13: C g [ r ] C g [ r ]+ c g 14: end if 15: end for 16: end for 17: for each grade g do 18: for B trials do 19: sample a value r from indexes of M g according to 20: if r = null then 21: sample  X  g from Beta (1 , 1) 22: else 23: sample  X  g from Beta (1 + C g [ r ] , 1+ r  X  M g [ r ]) 24: end if 25: end for 26: end for
To compute P (  X  | L ) with a single pass, we will assume that each line in the log has (at a minimum) a user or cookie ID, a query ID, and the ranks of clicked URLs. From that information we extract the deepest rank clicked k last , the total number of clicks c , and then compute r = k last c . We increment M r by one and increment C r by c . When complete, we know the total number of queries with r no-clicks as well as the total number of clicks for those queries; this is what we need to compute P (  X  | r, L ). To do that, we repeatedly sample a value of r from the M r distribution and use the corresponding M r and C r to compute parameters for a beta distribution from which we sample to generate  X  s.
In ERR, the parameter  X  represents the probability of a user continuing to browse conditional on seeing a document of a particular grade. We will need to estimate a distribution P (  X  | L, g ) for each grade g . As with our RBP distribution, we sum over r to obtain: We will understand the distribution P ( r | L, g ) as the proba-bility of observing a query with r unclicked documents given that there is a document of grade g in its results. P (  X  | r, L, g ) is the probability that the user will have patience to continue browsing past the document of grade g in those results.
We change the RBP algorithm outlined above to only Table 1: Two example rankings of URLs with rele-vance judgments (given as a numeric grade g )and clicks ( X ). count clicks that occur on or below a document of a par-ticular grade; Algorithm 1 gives complete pseudocode. The first 16 lines compute numbers from the log, storing counts of queries and total numbers of clicks for each r and each grade in the M g [] and C g [] arrays. The special case when r = 0 and c = 0 is held in a special place in M g given a null index in this pseudocode. The last 10 lines actually perform the sampling.

As an example of how the algorithm works, suppose we have a log with two lines shown in Table 1 (as columns). The first, ` 1 , has a X  X erfect X  document at rank 1 and a  X  X air X  X oc-ument at rank 6. The user has clicked at rank 1, and that is the last registered click. There are no unclicked documents, so r =0. M 4 [0], the number of lines with at least one  X  X er-fect X  document in the ranking and no unclicked documents, is incremented by 1; C 4 [0], the total number of clicks for lines with at least one  X  X erfect X  document and no unclicked documents, is also incremented by 1. Since there were no clicks on or after the  X  X air X  document, we simply assume the user never saw it, and thus do not take it into consideration; M 1 and C 1 are unchanged (as are M 2 , C 2 , M 3 , and C 3
The second line, ` 2 , has three  X  X air X  documents (at ranks 1, 3, and 5) and three  X  X ood X  documents (at ranks 2, 4, and 7). The user has clicked on ranks 1, 4, 7, 8, and 10. For g = 1, there are 5 unclicked documents and 5 total clicks on or below the first rank with a  X  X air X  document; M 1 [5] is incremented by 1, and C 1 [5] is incremented by 5. For g =2, k g = 2. Then, treating the document at rank 1 as unclicked, there are 6 unclicked documents and 4 total clicks, so M 2 [6] is incremented by 1 and C 2 [6] is incremented by 4. The click at rank 1 provides no evidence for whether a user will continue after rank 2, so treating the document at rank 1 as unclicked functions as a prior for the user having the patience to get to rank 2 in the first place (independently of the relevance judgments). To fit one of the models above, we need query log data. For RBP, we only require queries and ranks of clicks. For ERR we need query, ranks of clicks, and relevance judg-ments. The amount of query log data is somewhat inciden-tal; we can compute a posterior whether the log is very large or very small.

We have accumulated a rather large amount of log data from di  X  erent search engines. The AOL data is well-known; Figure 1: Posterior distribution for RBP X  X  patience parameter  X  computed from a web search engine log. Note that higher values of  X  reflect less patience for browsing down a ranked list. Figure 2: Posterior distributions for ERR X  X  param-eters  X  1 ..  X  4 computed from a web search engine log. Note that higher values of  X  reflect greater likeli-hood of stopping after encountering a document of that grade. it has queries, user IDs, and click ranks, but no relevance judgments. Yahoo! recently released log data under its Aca-demic Relations Webscope program 2 consisting of anony-mized query IDs, cookie IDs, click ranks, and graded rele-vance judgments (on a five-point scale). We have an addi-tional proprietary log that is similar to the Yahoo! log but that also contains human judgments of query type. Most of the results we present are based on the Yahoo! log.
We have elected to define  X  0  X  0, i.e. we assume that a user will never abandon the search after seeing a  X  X ad X  URL (they will always continue to look at the next one). This is partly because ERR will not use judgments of nonrele-vance otherwise, and partly because there are many missing judgments in our log data. These missing judgments likely a  X  ect the distribution of  X  0 more than any other parameter, since there are far fewer documents that have actually been judged  X  X ad X  versus  X  X air X .

Figure 1 shows a posterior distribution for RBP X  X   X  based on a web search engine log. Note the high peak at  X   X  1; this means that it is very common for users to only click on the top-ranked result. Figure 2 shows four posterior distribu-http://webscope.sandbox.yahoo.com tions, one for each of ERR X  X  parameters for four relevance grades. Note that the peak is present in all four, but its prominence decreases dramatically as relevance decreases, while the prominence of the hump to the left increases as relevance decreases. This supports the assumption of the cascade model that users are less likely to continue brows-ing after seeing more highly-relevant documents.
We note a few peculiar qualities of the distribution plots in Figures 1 and 2. First, there are clear peaks at certain points. Second, the peaks begin to disappear below 0.4, with the distribution becoming more smooth. Third, the distribution is quite flat in between the peaks above 0.4.
All three of these qualities are emergent from the Bayesian updating of the negative binomial model. The key param-eters are c , the number of clicks, and r , the number of unclicked URLs. Consider an infinitely-long log in which every query has the same values of c and r . Using our ap-proach, a posterior distribution of  X  given this log will have all its mass at a single fixed value of  X  . The negative bi-nomial distribution is such that for most values of c and r , those  X  s occur below 0.4; thus if c and r were uniformly dis-tributed through the log, the distribution would have most of its mass to the left. This is counteracted by the fact that low values of c and r are far more common X  X n particular, c = 1 and r =0or r = 1 (i.e. clicks at ranks 1 and 2) X  X nd result in higher values of  X  .For c =1 ,r = 0 (a single click at rank 1) in an infinite log,  X  will be 1; since this case is common in logs, there is a peak there. For c =1 ,r =1(a single click at rank 2), it will be 0.5, and the peak near 0.5 is again explained by the commonness of that case. As c in-creases and r stays low (i.e. as users click more and more of the ranked URLs), values of  X  increase towards 1, but these cases are rare in the logs, and therefore mostly subsumed by the flat prior. As r increases (and users click fewer of the ranked URLs), low values of  X  become more frequent and less discriminant, and therefore the distribution is smoother in the lower range.

So while the distributions look somewhat strange, the pe-culiarities are explainable from the model and the nature of the log. Furthermore, a log that looks fundamentally di  X  er-ent (by not having so many queries for which users abandon early, for instance) will have a di  X  erent distribution. We shall see some examples below.
Once we have a posterior distribution, we can form a marginal distribution for an evaluation measure reflecting e  X  ectiveness over the population. For each possible value of RBP for a topic, we compute the probability of getting that value if we were to sample  X  from the posterior P (  X  | L ): where P ( RBP = y |  X  ) is 1 if RBP computed with parameter  X  is y and 0 otherwise. This looks a bit complicated but it is really quite simple; we can compute it by sampling  X  s from P (  X  | L ), computing RBP based on that  X  , and over many trials forming the density P ( RBP ). When we have many topics to evaluate, we just do this for each one, then average the RBPs for each value of  X  . The marginal distribution is then over mean RBP. Figure 3: Distribution of mean RBP over the distri-bution of  X  shown in Figure 1. Figure 4: Distribution of mean ERR over the distri-bution of  X  sshowninFigure2.
 Similarly, P ( ERR = y )= Since  X  is a vector of |G| parameters, there is an integral over each parameter. Still, computing P ( ERR ) is as simple as computing P ( RBP ); we sample a vector of parameters  X  from distributions such as those in Figure 2 and compute ERR with those parameters. Note that we are not integrat-ing over  X  0 . As discussed above, we fix  X  0 = 0, since ERR does not make sense otherwise.

Figure 3 shows the distribution of mean RBP for one of our web search engines based on P (  X  | L ) computed from its log. Note the peak near RBP = 0.9; this reflects the fact that for most users only the first document is examined, and for many queries that is indeed good enough. Figure 4 shows the distribution of mean ERR for the same engine based on the distributions in Figure 2. ERR X  X  more precise user model suggests that while this engine has a high ERR for most users, there may be some for which the user experience is substantially less positive. Figure 5: Left: posterior distribution for RBP X  X  pa-rameter  X  for the query  X  X oogle X  in the AOL log.
 Right: the same for the query  X  X eather X .
Above we showed how to compute a distribution for an e  X  ectiveness measure parameter given a complete log. The log encompasses a great variety of di  X  erent types of users, queries, and tasks or information needs. Instead of comput-ing one distribution for the whole log, it may make sense to compute di  X  erent distributions for di  X  erent segments of the log, then evaluate separately using those distributions.
For instance, returning to the example of  X  X oogle X  and  X  X inux guide X , one would expect that for queries similar to  X  X oogle X , most users would interact with the search engine in a similar way, mainly clicking on the top returned result. Hence, we expect a narrow distribution of the parameter  X  around high values. On the other hand, for queries similar to  X  X inux guide X  X e expect users to demonstrate a high variabil-ity in their interaction with the returned results. It makes more sense for queries similar to  X  X oogle X  that the retrieval system to be evaluated against a narrow distribution of the parameters  X  , while for queries similar to  X  X inux guide X  it be evaluated against a di  X  erent and possibly high-variance distribution of the parameters  X  . (Note that similarity will be defined more carefully in Section 4.)
In the limit, we could compute a distribution for a single user interacting with results for a single query; this would reflect the possibility that a user could interact with results di  X  erently depending on many factors that are not captured in the log, not known to researchers currently, or cannot be measured in any practical way.
We can take a log and partition it into multiple smaller logs, one for each unique query. We can then use these smaller logs (which we will call L q ) to inform a posterior dis-tribution for  X  , which we can denote as P (  X  q | L q ). Looking at a posterior distribution for a query tells us something about how users are interacting with the system for that particular query; comparing distributions between two queries tells us something about di  X  erences in user interactions for the two. We have done this for all of our logs, but the only one that has original query text is the AOL log, so in this section we focus on that. Note that since the AOL log has no relevance judgments, we can only look at the RBP parameter, not the ERR parameters.

Figure 5 shows the RBP distribution for the queries X  X oogle X  and  X  X eather X  in the AOL query log. The former is clearly highly navigational: 75% of users who input the query clicked on rank 1 and stopped, and another 22% didn X  X  click any-thing. The remaining 3% went deeper into the ranking, with about half of those looking at rank 3 and the other half going beyond rank 3. Figure 6: Posterior distributions for RBP X  X  param-eter  X  for two users in the AOL log. Both users submitted more than 1,000 queries in the log.

The latter is more varied. It has some navigational qual-ities, as 74% of users that clicked something clicked on the top-ranked result. But only about 36% of users with this query clicked anything; 64% did not click any of the ranked results. This may suggest that the query is poorly-formed (some users may be looking for a site like weather.com , oth-ers may be looking for local weather). It may also suggest that there was a weather vertical in the interface (like there is in modern search engines) that gave the user everything they needed without requiring a click. Clearly using the same distribution of the parameter  X  when evaluating a sys-tem over these two queries is not appropriate.
Similarly, we can partition the log into multiple smaller logs, one for each unique user. We will call these L u , and again we can use them to inform a behavioral profile for an individual user. Looking at these can tell us something about how individual users experience results (conditional on the queries they are submitting).

Again, the AOL log is the only one with user information that can be directly linked to queries. The other logs have dissociated user information from queries and so cannot be used for these illustrations.

Figure 6 shows the RBP parameter distributions for two di  X  erent users in the AOL query log (both of whom have more than 1,000 queries in the log). These users are quite di  X  erent; both are patient enough to have significant proba-bility mass in the left part of the distribution, though both submit a significant number of queries for which they only click at rank 1. The right plot shows a user that clicks often enough that there is only a small dip at  X  u =0 . 8; as we discussed in Section 2.3.1, most of these distributions have agapbetween  X  u  X  1 and  X  u  X  0 . 5 because few users click more times than they do not click (i.e. c is rarely greater than r except when c = 1 and r = 0). This particular user does have a tendency to click on many results, and that shows up in the shape of the distribution.
If we have some indication of user task, we can aggre-gate over user/queries that match that task. One example alluded to above is informational and navigational , a very common classification scheme for web queries. We have hu-man navigational and informational judgments for one of our search engine logs; the posterior parameter distributions for the two classes are shown in Figure 7. The navigational class clearly shows a high peak near  X  = 1 with almost nothing below that. The informational class also has a peak near  X  = 1, but it is a smaller peak and there is still significant Figure 7: Profiles for informational and navigational classes. density in the lower part of the domain. These reflect the expected behavior for these two query types.

An interesting question here is whether there are queries that have been assigned a particular task class but have par-ticular instances for which user behavior reflects a di  X  erent class. For example,  X  X oogle X  is almost certainly a naviga-tional query, and indeed 76% of users with that query in the AOL log clicked only on rank 1. But there is a small group of users X  X bout 3% X  X hat click as far down as rank 5; for these, it might be argued that they have a more infor-mational need that spurred that query. The remaining 21% did not click anything.
Clustering query logs is useful for discovering patterns in the ways users are interacting with an engine. Most work on clustering within logs focuses on clustering queries, us-ing similarities among clicked URLs [15, 1] or similarities in terms used in query refinements [13]. Such work could be applied to users as well, forming clusters based on similar query formulations [8].

Behavior profiles give us the ability to cluster the log ac-cording to a user model and the variability of interaction with the engine rather than anything about text or spe-cific URLs. Text-based clustering cannot capture some of the most fundamental divisions that have been identified in logs, particularly informational versus navigational needs X  there is not much in the way of term similarity to connect two randomly-chosen navigational queries, for instance, nor is there much that would allow a clustering algorithm to put two randomly-chosen informational queries together. By clustering on distributions, we use click position information, but we also take advantage of variability in click positions. Furthermore, we can cluster along any dimension of inter-est we can identify in the log (e.g., clustering per query, per user, per session, etc.).

Apart from revealing interesting behavior profiles, clus-tering can be helpful in evaluating retrieval systems. If we could collect clicks for every query in a test collection such as those produced by TREC, then we could infer a patience distribution for each query separately and evaluate retrieval systems using the appropriate parameter distribution. In systems-based experimentation, we do not have the ability to observe users interacting with the retrieval systems. If query clustering based on behavior profiles could provide a classification of queries with meaningful semantics (e.g. navigational vs informational queries, or precision-oriented vs recall-oriented queries), then human assessors could iden-tify the class for each query in the collection and use a single parameter distribution for each such class.
As mentioned above, we can form posterior distributions for a single instance of a query, all instances of a single unique query, any single search performed by a user, all searches performed by one user, or any other groups of queries or users. In this section we assume we have some number m of posterior distributions P (  X  m | L ) and our goal is to cluster them.

In order to cluster the distributions, we use the clustering algorithm proposed by Kejzar et al. [6] for clustering dis-crete distributions. The proposed algorithm is based on a modification to the standard K -means (leaders) clustering algorithm. Instead of using the standard squared Euclidean distance as the distance metric, it uses the relative error metric , which its authors have shown to provide more infor-mative results than the Euclidean distance.

Our distributions are (strictly speaking) continuous, not discrete, so to use this algorithm we must transform them to discrete distributions. We do this by sampling and binning: for each value of  X  sampled from a posterior P (  X  | L ), we place it into one of eleven bins by rounding it to the nearest tenth. Therefore we are essentially clustering on 11 features, each of which represents the probability that a sampled  X  would have (roughly) a particular value, or in other words, the probability that a sampled  X  X ser X  would have a given degree of patience for browsing results. We tried other discretiza-tions into more bins, but they did not produce substantially di  X  erent clusters than what we show here.

Once the algorithm has placed each posterior distribution into a cluster C i , we re-compute a posterior P (  X  C i | L ) for the cluster by aggregating log data. When the clustering is complete, we have K posterior distributions, each associated with one of our clusters.
We first computed posterior distributions P (  X  q | L ) for ev-ery unique query in our proprietary log. We then formed increasing numbers of clusters of queries to look for pat-terns. Our first two clusters are shown in Figure 8. Note the similarity to the posterior distributions for the naviga-tional and informational classes shown in Figure 7: our first cluster looks very much like the navigational class, and the second looks like the informational class only with less of a peak at  X  C 2  X  1.

For our proprietary log we have human labels of naviga-tional/informational intent, and we compared the queries in each cluster to those judgments. The first cluster in Fig-ure 8 consists of about 1600 queries, of which about 1200 (75%) have been labeled  X  X avigational X  by a human asses-sor. The second cluster consists of about 3900 queries, with 700 (18%) labeled  X  X avigational X . About 62% of all navi-gational queries fell into the first cluster, with the remain-ing 38% in the second. Thus, while the clusters seem simi-lar to navigational and informational intents, they are also somewhat di  X  erent. It is gratifying to see that the informa-tional/navigational split is  X  X eal X  (indeed seemingly the pri-mary dimension along which queries are clustered), but also interesting that there are queries that have been labeled as one class but have behavioral patterns more like the other. It may be that instead of an informational/navigational split, the more salient description is  X  X igh-precision X  versus  X  X igh-recall X , with users issuing the former satisfied by one highly-relevant document (which may be a Wikipedia page for an correlated to the clusters. informational query) and users issuing the latter requiring more relevant documents and thus progressing deeper into the ranking.

We also tried forming three clusters from this data (not shown). In this case there are two clusters that are even more clearly navigational and informational (comprising 83% and 15% navigational queries respectively), with a third cluster that is somewhere in between (37% navigational). This third cluster seems to consist of lower-volume queries for which there is not much click evidence to move the prior, but it may still suggest that there are types of intents other than the two described above.

Figure 9 shows two clusters of queries for the Webscope log, this time with the distributions superimposed. These distributions look quite similar to those in Figure 8: one has a very high peak near 1 with little density elsewhere, while the other has a smaller peak at 1 that is only slightly higher than the other high points in the distribution. We do not have navigational/informational class labels for this data, but since we have relevance judgments we can look at the proportion of queries that have at least one  X  X erfect X  docu-ment that appear in each cluster (assuming that a query that has an associated document labeled as  X  X erfect X  is likely to have navigational intent). By that measure, the first cluster has 63% of the queries with at least one  X  X erfect X ; the other has about 37%.
 Figure 10 shows four clusters of queries for the same log. The first two clusters look very similar to the two in Figure 9, and the next two look very flat, with only a slight rise in cluster C 4 near  X  C 4  X  0 . 8. Their flatness suggests that they are clusters of queries with low click volume; there does not seem to be information to update the prior. Looking at them closer shows that they have peaks and valleys much like others we have seen, and thus are capturing something about behavior, but at this scale those peaks and valleys are not visible. It turns out, however, that they are meaningful; we will see this in Section 5 below.
The next question is whether the evaluation of a sys-tem di  X  ers depending on which data it has been evaluated Figure 9: Posterior distributions computed from click information for two clusters of queries. Figure 10: Posterior distributions computed from click information for four clusters of queries. against, and in particular whether the choice of cluster af-fects conclusions drawn from the evaluation. If we reach substantially di  X  erent conclusions about a system depending on the cluster we evaluate against, it suggests that system evaluation should not be performed with the same uniform set of parameters, but instead should be adapted for the particular query, user, or task.

To perform these evaluations, we used the cluster distri-butions shown above in Eq. 1 and 2 to generate distribu-tions for mean RBP and mean ERR; that is, we select a pa-tience cluster, sample a value of  X  (either a scalar or a vector value, depending on measure) from that cluster, evaluate all queries in the log using that value of  X  in RBP or ERR, and then average the RBP/ERR values over queries. Over many such trials we obtain a marginal distribution for the measure over the parameter posterior distribution in each chosen cluster. Note that all of our clusters are based on the RBP parameter distributions; we formed posterior dis-tributions for ERR parameters by computing them from the queries and clicks in each existing cluster based on RBP.
Figure 11 shows evaluation results given the two clusters in Figure 9. The left plot shows distributions for mean RBP: it is always quite high (above 0.8), but it also clearly tracks the di  X  erences between the two clusters. RBP calculated against the first cluster has a high peak near RBP  X  1, since most queries that fall into that cluster have a  X  X erfect X  document at rank 1 that is the only one that users click on. There are still enough queries in this cluster with clicks deeper in the ranking that some of the users it represents are more patient; these patient users find few relevant docu-ments after the top-ranked one and therefore find the system has a lower RBP (the peak around 0.75). RBP calculated against the second cluster has a much smaller peak at RBP  X  1 and more mass lower in the distribution. For more pa-tient users, the system represented by this log is significantly underperforming on average.
 The right plot in Figure 11 shows distributions for mean ERR. They are clearly very di  X  erent from each other. The distribution based on the first cluster evaluates the sys-tem highly on average, but has a great deal of variability, with values below 0.5 not uncommon. The peak around ERR  X  0 . 8 reflects the high probability of users abandoning after the first  X  X erfect X  document. The distribution based on the second cluster tends to evaluate the system much worse, with an expected ERR around 0.55. This strongly suggests that the data used to evaluate the engine is impor-tant, and di  X  erent sources of data can lead to very di  X  erent conclusions.

We also looked at evaluation against four patience clus-ters. Recall from Figure 10 that two of the clusters seemed quite flat, with little apparent di  X  erent between them, sug-gesting that they may be artifactual rather than reflect some real di  X  erence in user behavior. But as Figure 12 shows, the evaluation results for the four clusters di  X  er greatly. The first two clusters result in similar RBP distributions as those in Figure 11, suggesting that queries in the other two clus-ters are not important to defining the shape of the first two clusters (in some sense supporting the idea that those two clusters reflect something real about behavior). The two new clusters are similar, but one clearly has a higher peak at RBP  X  0 . 8, reflecting the very slight bump in the posterior distribution near  X  C 4  X  0 . 8above  X  C 3 . Furthermore, these two distributions are both very di  X  erent from the other two.
Figure 12 also shows distributions for mean ERR. Again the first two are similar to those in Figure 11, and the two new distributions are both di  X  erent from each other as well as di  X  erent from the original two.

These results point to a very high variability in how users will perceive a system X  X  e  X  ectiveness depending on both the query and the users themselves. There is therefore much potential for improving e  X  ectiveness. distribution for parameters of a probabilistic user model from log data. In particular, we showed how to apply the method to the user models that the evaluation measures RBP and ERR are based on and compute posterior distribu-tions for RBP X  X   X  X atience X  parameter and ERR X  X  relevance grade-based parameters with a single pass over a log. Since our method is Bayesian (and thus involves updating a prior based on evidence), it can be used with very small amounts of log data as well as very large. We used it to form dis-tributions for unique queries, unique users, and individual submissions of a query by a user. With these sorts of dis-tributions, we can cluster instances in the log according to similarity in browsing behavior; by examining these clusters we found that evaluation results can depend heavily on the data used to estimate evaluation measure parameters.
As noted above, this reveals a great deal of opportunity for better understanding and improving e  X  ectiveness. Tech-niques could include using di  X  erent retrieval algorithms for queries or personalizing the retrieval algorithm for users, in both cases depending on which patience cluster they fall into. Our results also support the idea that a simple evalu-ation by RBP or ERR using point estimates of parameters is not enough to understand system e  X  ectiveness; there is still too much variance in users that is not modeled by those parameter values.

We further intend to extend the models we have presented to more complex models of user behavior, such as those pre-sented recently using summary quality [16] or user dwell time [14]. There has been much work on this in the infor-mation science and information retrieval literature, and we believe we have a novel contribution to the body of litera-ture. Figure 9. Figure 10.
