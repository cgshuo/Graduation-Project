 In federated text retrieval systems, the query is sent to mul-tiple collections at the same time. The results returned by collections are gathered and ranked by a central broker that presents them to the user. It is usually assumed that the col-lections have little overlap. However, in practice collections may share many common documents as either exact or near duplicates, potentially leading to high numbers of duplicates in the final results. Considering the natural bandwidth re-strictions and efficiency issues of federated search, sending queries to redundant collections leads to unnecessary costs.
We propose a novel method for estimating the rate of over-lap among collections based on sampling. Then, using the estimated overlap statistics, we propose two collection selec-tion methods that aim to maximize the number of unique relevant documents in the final results. We show exper-imentally that, although our estimates of overlap are not inexact, our suggested techniques can significantly improve the search effectiveness when collections overlap. H.3.3 [ Information Storage and Retrieval ]: Informa-tion Search and Retrieval; H.3.4 [ Information Storage and Retrieval ]: Distributed Systems; H.3.7 [ Information Storage and Retrieval ]: Digital Libraries Algorithms, Design, Experimentation Resource selection, federated search, distributed informa-tion retrieval, overlap estimation, overlapped collections
In federated information retrieval (FIR), the query is sent simultaneously to several collections. Each collection eval-Copyright 2007 ACM 978-1-59593-597-7/07/0007 ... $ 5.00. uates the query and returns the results to the broker. As there is no need to directly access the index of the collec-tions, FIR methods can search the hidden web. FIR can also provide a search service over the latest version of docu-ments in collections without consuming costly resources for crawling and indexing.

For each query, the broker selects the collections that are most likely to return relevant documents. This creates the collection selection problem [Callan et al., 1995; Nottelmann and Fuhr, 2004; Hawking and Thomas, 2005; Si and Callan, 2003a]. To be able to select suitable collections, the broker acquires some knowledge about the contents of each collec-tion, creating the collection representation problem [Baillie et al., 2006; Callan and Connell, 2001]. The knowledge of the broker about each collection may vary from detailed vocab-ulary statistics to a limited number of sampled documents. Once the selected collections return their results, the broker merges them and presents them to the user. This final step is the result merging problem [Callan et al., 1995; Si and Callan, 2003b].

Our paper focuses on the first problem: collection selec-tion. FIR techniques assume that the degree of overlap among collections is either none or negligible [Si and Callan, 2003b]. However there are many collections such as biblio-graphic databases or news resources that have a significant degree of overlap. In such scenarios, selecting collections that are likely to return the same results not only wastes costly resources, but also degrades search effectiveness by introducing duplicate documents into the final results.
We propose a method that estimates the degree of overlap among collections by sampling from each collection using random queries. In addition, we introduce two collection selection techniques that use the estimated overlap statistics to maximize the number of unique relevant documents in the final results.

Our experiments on three testbeds suggest that, compared to the state-of-the-art methods, our techniques return fewer duplicate documents. They also significantly outperform the current alternatives in terms o f the final search precision.
Many collection selection techniques are based on the as-sumption that the federated search environment is cooper-ative , that is, collections provide the broker with their in-dex statistics and other useful information. CORI [Callan et al., 1995], GlOSS [Gravano et al., 1994], and CVV [Yu-wono and Lee, 1997] are examples of such methods, among which CORI has been suggested to be the most effective [Craswell et al., 2000; Powell and French, 2003], although there is also evidence that is poor [D X  X ouza et al., 2004].
In practice, federated search systems may be uncoopera-tive . In this case, collections do not make their index statis-tics available to the broker. Therefore, the broker samples documents from each collection and uses them for collec-tion selection. Recent collection selection algorithms were developed for uncooperative environments.

Si et al. [2002] proposed a language modelling framework for collection selection and result merging and showed that it can slightly outperform CORI in most cases. ReDDE [Si and Callan, 2003a] estimates the number of relevant docu-ments in collections according to their sampled documents. Collections are ranked according to the number of their sam-pled documents that are ranked highly by a central model.
Nottelmann and Fuhr [2003] introduced a decision the-oretic framework (DTF) for co llection selection. It tries to minimize the overall costs of federated search including money, time, and retrieval quality. However, the effective-ness of DTF, in particular for short queries, has been found to be inferior to that of CORI [Nottelmann and Fuhr, 2003]. Nottelmann and Fuhr [2004] showed that combining DTF with CORI can increase its general effectiveness.
In a similar approach, Si and Callan [2004] proposed a unified utility maximization framework (UUM) for resource selection. As in ReDDE, UUM runs queries on an index of all sampled documents. It uses training queries to learn the probabilities of relevance for the sampled documents accord-ing to their central scores. Using these probabilities, UUM selects collections that are likely to maximize either the final precision or final recall.

RUM [Si and Callan, 2005] is an enhanced version of UUM that also considers the search effectiveness of resources. That is, collections are ranked according to the number of relevant documents that they are expected to return, instead of the number of relevant documents that they contain. Training queries are used to model the search effectiveness of collec-tions. Si and Callan [2005] showed that the results produced by RUM are at least as good as those of UUM.

Hawking and Thomas [2005] suggested a hybrid approach that combines federated search with centralized techniques. In their method, the link anchor text available in a set of crawled documents is used to provide a description of collec-tions that are not crawled. Collections are ranked according to the similarities of crawled pages referring to them. They showed that their technique can outperform ReDDE and CORI for some tasks.
 Collection representation . The broker needs to gather de-scriptions of each collection before collection selection, . Col-lections are selected according to the similarities of their de-scriptions with the query. In coo perative environments, col-lections provide the broker with their own descriptions. In uncooperative environments, where collections communicate with the broker only in response to queries, a query-based sampling (QBS) technique [Callan and Connell, 2001] can be used to obtain collection descriptions. In QBS, probe queries are submitted to collections. The documents returned are downloaded and used as descriptions.

Several strategies have been suggested for selecting the probe queries. Callan and Connell [2001] chose the probe queries from a set of dictionary words or existing sample doc-uments. Gravano et al. [2003] suggested that probe queries be selected from the nodes of a hierarchical classification tree. We found that selecting the probe queries from query logs can lead to better samples and significantly improves the search effectiveness [Shokouhi et al., 2007]. Hedley et al. [2004] selected the initial probe queries from the search in-terface of collections. Query-based sampling may be static or adaptive. In static QBS, a fixed number of documents (say 300) is downloaded from each collection [Callan and Connell, 2001]. In adaptive QBS [Baillie et al., 2006; Caver-lee et al., 2006; Shokouhi et al., 2006a], sampling terminates according to the size of collections or the number of new terms in the most recent samples.
 Result merging . Once the selected collections return their results, the broker merges them into a single list and presents them to the user. CORI [Callan et al., 1995] and SSL [Si and Callan, 2003b] are two well-known examples of such algorithms. We use SSL for our experiments, as it has found to be more effective than CORI [Si and Callan, 2003b]. Duplicate detection in FIR . Typically, federated search tech-niques assume that collections are independent and overlap-free. There are only two previous works that have explored the problem of overlapped collections.

We introduced a method for detecting duplicate and near-duplicate documents among the re turned result s[Bernstein et al., 2006]. In this approach, collections send a hash vector with each document they return to the broker. The broker detects the duplicate and near-duplicate documents by com-paring the returned hash vectors and discards them during merging. The major drawback with this technique is that it may not be applicable for uncooperative environments, as it expects collections to use the same hash functions and to return a hash value per document.

COSCO is an overlap-aware collection selection method proposed by Hernandez and Kambhampati [2005] for unco-operative environments, which uses a large number of train-ing queries to measure the degree of overlap among collec-tions. For collection selection, each query is matched with the previous training queries . Then, according to the over-lap statistics stored for the training queries, collections are selected in a way that is expected to minimize the number of duplicate documents in the final results. If the terms avail-able in a new query do not exist in the previous training queries, COSCO cannot effectively estimate the overlap and select suitable collections. Also, Hernandez and Kambham-pati [2005] test COSCO on a set of bibliographic datasets; its effectiveness for heterogeneous datasets and typical web pages has not been investigated.
Documents downloaded by query-based sampling are the only source of information about collections in uncooper-ative environments, so it is necessary to extract as much information as possible from the samples. Considering the efficiency restrictions and bandwidth limits, it is advanta-geous if the extra information is extracted from documents that are already downloaded from collections. Methods for estimating the size of collections such as sample-resample [Si and Callan, 2003a] and capture-history [Shokouhi et al., 2006b] already have such characteristics. They can estimate the size of collections with a small number of probe queries and sample documents. In this section, we introduce a novel method for estimating the degree of overlap among collec-tions. Our technique uses the documents downloaded by query-based sampling for estimating the rate of overlap and does not require any additional information.

Suppose that we have two collections C 1 and C 2 ,andthere are K overlap documents between them. We gather one sample from each collection using query-based sampling, S from C 1 and S 2 is from C 2 .Let m 1 and m 2 represent the documents from S 1 and S 2 that are in K .Thatis, m 1 and m 2 are the subsets of sampled documents that are selected from the overlapped documents between C 1 and C 2 : Assuming that the samples are random, we can estimate the sizes of m 1 and m 2 as follows: Here, | C x | is the size of collection C x that can be estimated by the capture-history technique [Shokouhi et al., 2006b]. From another perspective, m 1 and m 2 can be regarded as two random samples from the population of overlapped doc-uments. The probability of any given document from m 1 to be available in m 2 is  X  = | m 2 | K . Therefore, the probability of any given document from m 1 not to be available in m 2 calculated as 1  X   X  . The expected number of documents in m 1 that are available in m 2 can be calculated as below: where the possible number of overlap documents is 0 &lt;i&lt; | m 1 | and P ( i ) is the probability of having exactly i docu-ments in m 1 that are also available in m 2 . (Note that by definition E ( D ) is the expected number of documents in m that are available in m 1 .) Since P ( i ) follows a binomial distribution, for the expected value of D we have: That is: =  X  where  X  = | m 1 | X   X  . By substituting i  X  1withanother variable j we have: =  X  According to the binomial theorem: Algorithm 1 Relax resource selection 1: Relax-Selection ( G, w, s ) 2: Initialize-Graph ( G, S ) 3: S  X  X  X  4: Q  X  V [ G ] 5: while Q =  X  do 6: u  X  Extract-Max ( Q ) 7: S  X  S  X  X  u } 8: for each vertex v  X  Adj [ u ] do 9: d [ v ]  X  d [ u ]  X  w e ( u, w ) //Relaxing 10: end for 11: end while which gives: Thus:
Equation (2) shows the expected number of documents in m 1 that are common with m 2 . Similarly, E ( D )istheex-pected number of documents in m 2 that are common with m 1 . Thus, the number of overlap documents is independent of the collection sizes. Having the number of duplicate doc-uments ( D ) within two samples it is possible to estimate the value of K as:
Once the number of duplicate documents among collec-tions is estimated, it can be used in collection selection algorithms for maximizing the number of unique relevant documents in the final results. In the following sections, we introduce two methods that use the estimated overlap statistics for collection selection.
A federated search environment in the presence of over-lapped documents among collections can be represented by agraph G . In this graph, each vertex is a collection and each edge indicates the existence of overlap documents between a pair of collections.

We aim to minimize the number of duplicate documents in the final merged list. For this purpose, the selection algo-rithm has to avoid simultaneously selecting collections that are likely to return the same answers.

In our Relax collection selection technique, initially the number of relevant documents in each collection (vertex) is estimated. As in ReDDE [Si and Callan, 2003a] and UUM [Si and Callan, 2004], we rank all the sampled documents from collections for each query. Assuming that the top  X  documents returned in this central ranking are relevant (we used  X  = 150), the number of relevant sampled documents for each collection is counted. Then according to the esti-mated size of collection and the number of sampled docu-ments, the total number of relevant documents in collection C is computed as  X  R = r  X | C | | S | . Here,  X  R is the estimated number of relevant documents in collection C and | C | is the estimated size of collection. The number of sampled docu-ments from collection C that are ranked in the top  X  results by the central retrieval model is represented by r . Finally, |
S | is the number of sampled documents from collection C . Relax uses the estimated values for  X  R as the weights of the collections.

In the next step, the weight w e ( u, v ) of an edge between two given collections u and v is calculated according to the approximated number of common relevant documents be-tween u and v as follows: Here, | C u  X  C v | represents the total number of documents in both collections.  X  R u and  X  R v are respectively the esti-mated number of relevant documents in collections u and v .  X  K is the estimated number of common documents between collections u and v that is calculated by Eq. (3).
At each stage, the collection with the highest number of relevant documents is selected. The weights of other collec-tions are updated by subtracting the estimated number of their overlapped relevant documents with the selected collec-tion (that is, by relaxing). In summary, our Relax selection method (Algorithm 1) is as follows: 1. Documents are downloaded from each collection using 2. The size of collections and the number of overlapped 3. The federated environment is represented by a graph, 4. The collection with the highest estimated number of 5. Relax updates the graph by relaxing all collections 6. Stop if there are no more edges or enough collections
Figure 1 shows a simple example of four overlapped col-lections. At the first step (A), the number of relevant doc-uments in each collection R is estimated. At the next stage (B), the collection with the highest number of relevant doc-uments ( C 4) is selected. The graph is relaxed by subtract-ing the estimated number of common relevant documents between the top collection and the connected collections. After each update, the edges connected to the most recent selected collection are removed. This process continues until there is no edge in the graph (C) X (E). That is, Relax se-lects collections according to the number of their unvisited relevant documents.
Another strategy for avoiding duplicates in the final re-sults is to remove collections with a high degree of overlap from the resource selection rankings. That is, initially the degree of overlap between collection pairs is estimated. Then for each query, collections are ranked using a resource selec-tion method such as ReDDE [Si and Callan, 2003a]. Each collection at rank  X  is compared with the other collections at the higher ranks. Collection C  X  is pruned from the origi-nal rank list if it has a large estimated overlap with at least Figure 2: The overlap among collection pairs in the Qprobed-280 testbed. one of the other collections at higher ranks. Our filtering method for ReDDE, which we refer to as F-ReDDE, can be summarized as below: 1. The overlaps among collections are estimated as de-2. Collections are ranked using a resource selection algo-3. Each collection is compared with the previously se-
The effectiveness of this method is expected to strongly depend on the underlying collection selection technique that is used. Also, the optimum value for  X  may vary between testbeds. In the following sections, we compare the effec-tiveness of our selection methods with other techniques in the presence of overlap among collections.
The effectiveness of FIR methods tends to vary substan-tially on different testbeds [D X  X ouza et al., 2004; Si and Callan, 2003a]. Unfortunately, in the standard FIR testbeds [Powell and French, 2003; Si and Callan, 2003b], there is no overlap among collections. Thus, we create three new testbeds with overlapping collections based on the docu-ments available in the TREC GOV dataset. We do not claim that our strategies for creating these testbeds are perfect or argue that the testbeds entirely reflect the characteristics of web collections. However, considering the available datasets for evaluating information retrieval experiments, we believe that the suggested testbeds are acceptable.
 Qprobed-280 . For this testbed, we used the 360 most fre-quent queries in in a query log provided by a major search engine, of queries with a highly ranked answer in the .gov domain. Each selected query is passed as a probe query to an index of TREC GOV documents. For each probe query, Figure 3: The overlap among collection pairs in the Qprobed-300 testbed. a random number of documents (between 5 000 and 20 000) are downloaded as a collection. Queries that return less than 5 000 answers are discarded.

In total, 280 collections with average size of 12 194 docu-ments were generated. The largest and smallest collections in this testbed respectively contain 19 860 and 5 001 docu-ments. Documents in each collection match for the same query and are likely to have somewhat similar topicality.
Figure 2 depicts the degree of overlap among collections in this testbed. There are 74 492 collection pairs that have less than 10% overlap while there are only 79 pairs with more than 90% of their documents in common. The overall rate of overlap among collections is low; only 1.1% of collection pairs in this testbed have more than 50% overlap. Qprobed-300 . Starting from the first collection in the pre-vious testbed, every twentieth collection is merged into a single large collection. The same procedure is applied to ev-ery twentieth collection starting from the next initial 13 col-lections (collections 2 , 3 , ..., 14) in the Qprobed-280 testbed. In total, the testbed is comprised of 300 collections. Fig-ure 3 illustrates the degree of overlap among collection pairs. About 1.9% of collection pairs have more than 50% overlap which is higher than the Qprobed-280 testbed.

The collections in this testbed vary in size from 5001 to 180 985 documents with an average of 20 908 documents. Sliding-115 . We used a sliding window of 30 000 documents on the TREC GOV documents to generate 112 collections. Each collection has a random percentage of overlap P % (25 &lt;P &lt; 100) with the previous collection. Then, starting from the first collection, every tenth collection is collapsed into a single large collection. The same procedure is applied on every tenth collection starting from the second and third collections, forming two additional large collections. In to-tal, there are 115 collections. Figure 4 illustrates the degree of overlap among collection pairs on this testbed. About 2.5% of collection pairs have more than 50% overlap.
We expect that the impact of using overlap statistics be-comes more significant as the overall overlap in the testbeds increases. The experimental results reported in the following sections confirm this hypothesis.
 Figure 4: The overlap among collection pairs in the Sliding-115 testbed. Figure 5: The accuracy of overlap estimation for col-lection pairs in the Sliding-115 testbed.
The accuracy of our method for estimating the rate of overlap among collection pairs is measured using an average estimation error metric, defined as below: where Here, n is the total number of collections, D ( i, j )isthe proportion of documents in collection i that are common with collection j .Thevalue | C i  X  C j | is equivalent to the K value in Eq. (3) and | C i | represents the size of collection i .
In our preliminary experiments, the initial estimated val-ues for D ( i, j ) suggested that the degree of overlap among collections is usually overestimated. This observation can be easily explained; document retrieval models are biased to-wards returning some popular documents for many queries [Garcia et al., 2004]. In addition, we found evidence that samples produced by query-based sampling are not random [Shokouhi et al., 2006b]. Therefore, the number of common documents between collection samples is often higher than the random scenario causing the overestimation of overlap in Eq. (3). Thus, we divide the estimated overlaps by the largest overlap value for normalization. In the rest of this paper and for all of our experiments, we use the normalized overlap values.

The AEE values computed for collections in the Qprobed-300, Qprobed-280, and Sliding-115 testbeds are respectively 0.91, 0.93, and 0.70.

Figure 5 shows the accuracy of estimations for overlapped collections in the Sliding-115 testbed. The horizontal axis represents the collection pairs sorted according to their ac-tual overlap degree. It can be seen that the estimated values and the actual overlap rates correlate. The errors in estima-tions are largely due to two factors: (1) the query-based sampling technique does not produce random samples from collections [Shokouhi et al., 2006b] and (2) the size of sam-ples are so small, so that they do not capture any duplicate document for estimating the degree of overlap. The trends for the other two testbeds were similar and we do not present them here.

In the rest of this section, we show that although our es-timates of overlap are not perfectly accurate, our suggested methods can significantly improve the search effectiveness in the presence of overlap among collections.
 Search effectiveness . To make our results comparable with previous work [Nottelmann and Fuhr, 2003; 2004; Si and Callan, 2003a;b; 2004; 2005], we run traditional static query-based sampling on each collection. Probe queries are se-lected randomly from the set of sampled documents and sampling terminates once 300 documents are downloaded. ReDDE is one of the most successful collection selection techniques that does not require training data. Therefore, we use it as the baseline of our experiments. We also com-pare the results with CORI.
 For simplicity, we assume that all collections use the IN-QUERY [Callan et al., 1992] retrieval model and return at most 100 answers per query. We apply SSL [Si and Callan, 2003b] X  X he best current FIR merging method X  X o merge the results and compared methods by their precision values at early recall points ( P @ n ). The statistical significant de-tected by the t-test for the differences between ReDDE and other methods at the 90% and 95% confidence intervals are respectively specified by  X  and  X  . The duplicate documents in the final merged lists are considered as irrelevant.
Table 1 shows the precision values obtained by running the TREC topics 551 X 600 on the Qprobed-280 testbed. The cutoff values represent the number of collections selected per query. For the cutoff values 5 and 10, there is little difference between the effectiveness of methods, and only for P@5, Relax has a small advantage over the alternatives. When 20 collections are selected, the gaps become more apparent. Relax significantly outperforms ReDDE for precision at 5 and 10. F-ReDDE is at least as good as ReDDE and CORI produces a better P@5 value than ReDDE.

Table 2 includes the precision values produced by the selection methods on the Qprobed-300 testbed. The dif-ferences between ReDDE and Relax are often significant. Other methods have no significant advantage against each other, but ReDDE is usually the best among them.

On the Sliding-115 testbed, Relax produces the best re-sults. It significantly outperforms ReDDE in 5 of 12 cases. CORI 0.163 0.149 0.134 0.113 0.167 0.142 0.121 0.110 0 . 195 Relax 0 . 187  X  0.142 0.114 0.107 0.183 0.161 0.126 0.112 0 . 208 Relax 0 . 204  X  0 . 177  X  0.140 0.128 0 . 187  X  0.153 0.130 0 . 121 Table 4: The average number of duplicate docu-ments returned by each method per query for the Qprobed-280 testbed. Cutoff values (CO) represent the number of collections selected.
 Table 5: The average number of duplicate docu-ments returned by each method per query for the Qprobed-300 testbed. Cutoff values (CO) represent the number of collections selected.
 F-ReDDE also produces better results than ReDDE in gen-eral. However, the differences are smaller and only signifi-cant at two cases. The precision values for CORI for cut-off=5 are specified in italic to indicate that they are sig-nificantly worse than ReDDE. This is consistent with ob-servations of Si and Callan [2003a] suggesting that CORI is poorer than ReDDE on testbeds with skewed distribu-tions of collection sizes. The differences between CORI and ReDDE become negligible for larger cutoff points.
As the extent of overlap among collections in the testbeds increases, the impact of using an overlap-aware collection selection method becomes more apparent. While there is no significant difference between methods on the Qprobed-280 testbed, the overlap-aware methods outperform the FIR baselines on the other two testbeds. This confirms our hy-pothesis that, as the overlap grows, there is a more notice-able need for use of overlap-aware selection methods. Table 6: The average number of duplicate docu-ments returned by each method per query for the Sliding-115 testbed. Cutoff values (CO) represent the number of collections selected.
 Number of duplicates . For each cutoff value, we compare the average number of duplicate documents returned by meth-ods per query. Table 4 suggests that when the rate of overlap among collections is low, the number of duplicate documents returned by CORI, F-ReDDE, and ReDDE are usually com-parable. Relax performs better than ReDDE and manages to reduce the number of duplicate documents by 11% and 15% respectively for cutoff values 5 and 10.

Tables 5 and 6 suggest that, compared to ReDDE, the overlap-aware selection methods can significantly reduce the chance of visiting a duplicate document in the final results. In both testbeds, CORI returns the lowest number of du-plicate documents. This is due to the poor performance of CORI for testbeds with skewed distribution of collection sizes. Compared to the other methods, CORI selects the three large collections in the Sliding-115 testbed for fewer queries. The same observation can be made for the 14 large collections of the Qprobed-300col testbed. As these collec-tions cause the highest overlap in the testbeds, missing them during collection selection significantly reduces the number of duplicate documents in the final results. However, Ta-bles 2 and 3 show that the effectiveness of CORI on these testbeds is poor even when the number of duplicate docu-ments is low. This is mainly because the large collections missing by CORI have a high density of relevant documents.
The average number of duplicates returned by ReDDE and F-ReDDE are similar on the Qprobed-300 testbed. On the Sliding-115 testbed, F-ReDDE returns 13% and 14% fewer duplicates respectively when 5 and 20 collections are
Relax 0 . 171  X  0 . 154  X  0.122 0.115 0.187 0 . 156  X  0.133 0.115 0.154 0.152 0 . 140  X  0 . 133  X  selected. On the latter two testbeds, the number of dupli-cates returned by Relax is substantially less than ReDDE. Relax returns respectively 12% and 18% less documents than ReDDE for cutoff values 5 and 20 on the Sliding-115 testbed. On the Qprobed-300 testbed, the number of du-plicates for Relax at CO=10 and CO=20 are respectively 22% and 14% less than that for ReDDE.

Overall, Relax produces the highest precision values. It also returns a lower number of duplicate documents than all methods but CORI. The F-ReDDE approach works well on some testbeds but on others is not significantly better than the alternatives. This might be due to the poor choice of  X  , which was chosen based on our preliminary experiments.
We have introduced a novel technique for estimating the degree of overlap among uncooperative collections. Our method uses the sampled documents downloaded for collec-tion selection and does not require any additional informa-tion. We have also proposed two overlap-aware collection selection techniques that consider the overlap statistics of resources for collection selection. Our experimental results show that, in the presence of overlap, our techniques can sig-nificantly outperform previous collection selection methods in terms of search effectiveness. They also lead to a smaller number of duplicate documents in the final merged results.
Several open directions remain as our future work. For our experiments in this paper, we used static query-based sampling and downloaded 300 documents for each collection. It is interesting to investigate the impact of sample size and sampling strategies on the accuracy of overlap estimations and the final search effectivene ss. Moreover, we arbitrarily set  X  to 30% according to preliminary experiments, but our results suggest that the best value for  X  varies on different testbeds. Finally, although in theory the methods discussed in this paper are applicable for avoiding near-duplicate doc-uments, they have not been tested for such a scenario.
