
Neural Networks are successful in acquiring hidden knowledge in datasets. Their biggest weakness is that the knowledge they ac-quire is represented in a form not understandable to humans. Re-searchers tried to address this problem by extracting rules from trained Neural Networks. Most of the proposed rule extraction methods requked specialized type of Neural Networks; some re-quired binary inputs and some were computationally expensive. 
Craven proposed extracting MofN type Decision Trees from Neu-ral Networks. We believe MofN type Decision Trees are only good for MofN type problems and trees created for regular high dimen-sional real world problems may be very complex. In this paper, we introduced a new method for extracting regular C4.5 like Deci-sion Trees from trained Neural Networks. We showed that the new method (DecText) is effective in extracting high fidelity trees from trained networks. We also introduced a new discretization tech-nique to make DecText be able to handle continuous features and a new pruning technique for finding simplest tree with the highest fidelity. 
Neural Networks have good generalization capability. The most important weakness of Neural Networks is that they are like black boxes. Understanding the reasoning behind a Neural Network's output is not easy. Knowledge acquired by a Neural Network is represented by its topology, by the weights on the connections and by the activation functions of the hidden and output nodes. These representations are not easily understandable. 
Symbolic learning techniques produce more understandable out-puts but they are not as good as connectionist learning techniques in generalization [I, 8, 6, 10]. In this paper, we present a new method for extracting Decision 
Trees from trained Feedforward Neural Networks. We show that the new method is able to create Decision Trees that are close in accuracy to the Neural Networks they are extracted from and pro-duce similar outputs (with high fidelity) like the Neural Network. Fidelity is the ability of the extracted Decision Tree to imitate the Neural Network it is extracted from. Fidelity is important if the 
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to permission and/or a fee. SIGKDD '02 July 23-26, 2002, Edmonton, Alberta, Canada. 
Copyright 2002 ACM 1-58113-567-X/02/0007 ...$5.00. parent's model is used at the node else, local model is created. Cre-ating local models by using MofN type constraints is much more difficult and expensive than by using regular constraints. 
TREPAN USeS a heuristic search which aims to increase the fi-delity for constructing MofN splits. It first finds a binary split by using information gain. TREPAN splits tWO valued features into two. If a feature has more than two values, binary splits based on each value are considered (e.g. computer = desktop? computer = laptop?). The binary split selected is used as a seed for the search process. Information gain is used as the evaluation function in the search. Candidate splits are created by adding a new member to the set and keeping m constant or by adding a new member and incrementing the value ofm by one. 
TREPAN uses two stopping criteria. If a node covers the in-stances of a single class with high probability, it is made a leaf. 
It finds the proportion of examples (Pc) (the proportion of the num-ber of instances classified as the most common class to the number of total instances at the node) and the confidence interval for the are parameters to the algorithm. TREPAN also stops tree generation by limiting the depth of the tree. 
DecText extracts regular Decision Trees from trained feedfor-ward Neural Networks. The Decision Tree extracted from~the net-work mimics the network's behavior. It is the network's represen-tation in a more understandable form. / 
We have developed new splitting techniques, a new discretiza-lion method which uses Neural Network for making DecText be able to handle continuous variables and a new tree pruning tech-nique which tries to optimize fidelity while minimizing tree size. 
One of the problems with classical Decision Tree creation al-gorithms is that splitting tests toward the leave are selected using fewer instances than splitting tests toward the top of the tree. There-fore splitting tests toward the leave are less reliable. DecText can use unlabeled instances and find he classification for the unlabeled instances by using the trained Neural Network. By using unlabeled instances, we can use fixed number of instances to choose splitting features at any level in the tree. In some problem domains unla-beled instances are readily available (e.g. data from sensors etc.). 
If we have a mathematical model of the problem, we can create new instances by using that model. If we have neither, we can cre-ate a model from the dataset and we can create random instances by using the model. 
Continuous features are handled by using the new discretization method we developed. Discretization can be done globally (before creating the Decision Tree) or locally at each node of the tree. Dis-cretization algorithm creates N + 1 discrete values for the feature. 
If a continuous feature is used for splitting, N + 1 subnodes are created and that feature is not used again. 
Random instances are used only for choosing the best splitting feature and for labeling nodes which have no training data instances available to label them. 
All leave are labeled by using the outputs of Neural Network on training data instances. If there are no training instances available at a node, n random instances are created (by using its parent's data model if local data modeling is being used or by using global data model if global data modeling is being used) and their outputs from the Neural Network are used for labeling the leaf. n is by default equal to the number of instances in the training dataset. 
By default, we do not use stopping criteria. We stop creating teaching output. For unlabeled (i.e. random) instances, we find the output by testing them on the network. We find the classification for all the instances. Then, we find the dominating class in the dataset. 
We assume that all the instances in that partition should have been classified as the dominating class. Then, we set the teaching out-put to dominating class. If the instances are all classified the same with high confidence by the Neural Network then SSE will be low. If they are classified as different classes with low confidence then SSE will be high. 
In SSE Split, for feature f~, data subsets (S~j) for each parti-tion are found. Then, for each S~j, the dominating class is found by using Neural Network's outputs. Expected classification is set to dominating class for S~j and SSE~j is calculated for S~. To find SSEi value for feature .fl, all SSEi~ values are added for all partitions. Feature with the smallest SSE~ value will be used for splitting. 
Like SSE split ClassDiff split also measures the quality of the partitions created by each feature. The reason we call this new algorithm ClassDiff splitting is that we use the difference between the maximum average NN output value and the second maximum average NN output value in the subsets to measure how good a partition is. 
In Neural Networks using one-of-m representation for classifica-tion, the difference between the highest output unit value and the second highest output unit value gives a good measure of the con-fidence of classification. ClassDiff value for a subset is calculated like this: 'First, a 
TotalOut vector with nC (nC: number of classes) members is created and each member is initialized to 0. Then, each instance is tested on the Neural Network to find nC output values(OutD. 
Out~ is added to TotalOut~. TotalOuti is divided by ISl Number of instances in the dataset) to find AverageOuti. 
Then, the maximum valued AverageOut (AverageOutMax) and second maximum valued AverageOut (AverayeOutMax2) are found. ClassDiff value is the difference between these two For finding the splitting feature fi at a node, for each partition 
Sij of f~ elassDiff~ is calculated. Then, eclassDiffParti is found by adding all [(IS~ul/ISl) * dassDiffij] values for all the partitions of feature fi. IS~Jl is the number of instances in the jth partition of the ith feature. The feature with the highest classDiff-Part value is the most relevant feature. Here is another representation of the method we used: 
TotalOut~j[O to nC -1]: Vector with size nC for feature i partition j. Each element is initialized to 0. (nC : Number of classes) TotalOut~j[k] = ~fo,-~u i.,t ..... O,j[k] I Sij 1: # of instances in the jth partition 
Discretization is similar to splitting. The cutpoints found by the discretization algorithm should maximize the probability of having one class dominate the subsets created by the cutpoints. 
For finding the cutpoints, instances are first sorted by increasing value of the feature E The mid points of instances where classifica-tion of the Neural Network change are found. For each candidate cut point, the dataset is divided into two subsets and ClassDiff value is calculated for each subset. 
To find the ClassDiff (CD) value for a cut point, the ClassDiff values for the left and right subsets (Sl and S~) created by the cut-point are found. The final value is calculated by using the formula: 
The cut point with the maximum CD I, value is chosen as the final cut point. The subsets which have a ClassDiff value smaller than DiscStop are further partitioned recursively. DiscStop is user definable and the default is 0.4. This is a heuristic value. It gener-ally gives good results with not too many cutpoints. 
After the tree is extracted, it is first checked if any of the non-terminal nodes has children all classified the same. Such non-terminal nodes are made leave and children's label is assigned to them as their label. Checking non-terminal nodes starts from the bottom of the tree and it goes toward the root. This kind of pruning does not change fidelity of the tree. 
Fidelity Pruning tries to optimize fidelity of the tree wh!le trying to make it as simple as possible, m random instances by using the data model for the whole training dataset are created first, m is user defined. The default value for m is the number of instances in the training dataset. For all the non-terminal nodes of the tree the node is made a leaf and fidelity of the new tree is tested by using the random dataset. If fidelity is better than maxFidelity, new fidelity is assigned to maxFidelity and tree is saved in best tree. At the end best tree is returned as the pruned tree. 
While extracting the Decision Tree, the dominating class at each non-terminal node is saved. While pruning when a non-terminal node is turned into a leaf it is labeled by using the dominating class saved while extracting the tree. 
In this section, we will show that DecText extracts high fi-delity Decision Trees from trained Neural Networks. We will show that our discretization algorithm performs better than entropy dis-cretization. We will also show that Fidelity Pruning reduces num-ber of nodes and tree size without affecting fidelity or accuracy much. First, we report accuracy results for Neural Network, C4.5 and DecText for each dataset. We also compare number of nodes in the 
C4.5 trees and DecText trees. To be able to test the capabilities of the new algorithm better, for any of the tests we did not use pruning (except for the tests of the pruning algorithm). For comparing tree sizes and accuracy, we created unpruned trees by using C4.5 (we used the command c4.5 -ml -cl00 -f file base -u).. 
We used the datasets Vote, Vote-3, Heart, and Housing. Vote dataset contains votes from U.S. House of Representatives during the 98th congress. Heart dataset is concerned about heart disease diagnosis. The data was collected from the Cleveland Clinic Foun-dation. In this dataset we changed the feature "number of major creates more nodes. This is mainly because C4.5 uses binary splits for continuous valued features. Although the number of nodes de-creases if binary splits are used, depth of the tree increases. Table 4: # nodes and tree size for C4.5 and DecText. 
We first tested new splitting techniques we developed. We also tested gain splitting technique to compare the results we get by us* ing the new techniques. We report the fidelity results in percentages and we also report the tree depth and the number of nodes in the trees. All the results are the averages of 10-fold cross validation tests. 
As it can be seen from the tables below, SetZero split gives much better results for the Vote and Vote-3 datasets. These datasets have more relevant features. SetZero sorts the features by their relevancy to the classes in the problem. The other splitting techniques sort the features by the quality of the partitions created. Therefore, if there are highly relevant features in the dataset SetZero will create much better results. If there are not any relevant features in the dataset other splitting techniques will create better results. Gain split creates the smallest trees. Table 5: Comparison of new splitting techniques.' 
In this section, we report the tests on DecText by using random instances. Random instances are created while choosing the best splitting feature or while labeling a node if there are no training data instances available at the node. The number of instances to be used at each node is set to the number of instances in the training dataset. We tested creating random instances by using local model, and global model. 
Creating random instances for finding the splitting features and for labeling empty nodes improves fidelity results in almost all of 
Another advantage of using ClassDiff Discretization is that the Table 8: Entropy vs ClassDiff Discretization. Method Heart (%) Heart (%) ] Housing (%) Housing (%) SetZero 84.5 84.8 86.2 87.4 SSE 88.6 88.3 87.4 88.0 ClassDiff 88.0 88.3 87.8 88.2 Fidelity 86.2 86.6 87.2 88.2 
Gain 84.8 83.5 ! 86.6 87.6 
Number of random instances used at each node for finding the Vote SetZero 435 Global 97.2 Vote-3 SetZero 500 Global 96.4 Heart SSE 5000 Local ClassDiff 94.4 Housing ClassDiff 506i Local ClassDiff 91.0 
Here is the comparison of the results we reported in the begin-Method [[ Vote (%) Vote-3 (%) Heart (%) [ Housing (%) Basic] 96.0 94.2 84.5 I 88.0 
Best 97.2 96.4 94.4 91.0 
Two of the most important criteria used for evaluating inductive 
Several researchers addressed the problem by extracting rules Craven addressed the problem by extracting MorN Decision 
We developed a new Decision Tree extraction technique 
