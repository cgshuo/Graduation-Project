 Novi Quadrianto novi.quad@gmail.com Alex J. Smola alex.smola@gmail.com Tiberio S. Caetano tiberio.caetano@gmail.com Quoc V. Le quocle@stanford.edu Computer Science Department, Stanford University Assume that a web services company wants to increase its profit in sales. Obviously sending out discount coupons will increase sales, but sending coupons to customers who would have purchased the goods any-way decreases the margins. Alternatively, failing to send coupons to customers who would only buy in case of a discount reduces overall sales. We would like to identify the class of would-be customers who are most likely to change their purchase decision when receiv-ing a coupon. The problem is that there is no direct access to a sample of would-be customers. Typically only a sample of people who buy regardless of coupons (those who bought when there was no discount) and a mixed sample (those who bought when there was dis-count) are available. The mixing proportions can be reliably estimated using random assignment to control and treatment groups. How can we use this informa-tion to determine the would-be customers? Likewise, consider the problem of spam filtering. Datasets of spam are likely to contain almost pure spam (this is achieved e.g. by listing e-mails as spam bait), while user X  X  inboxes typically contain a mix of spam and non-spam. We would like to use the inbox data to improve estimation of spam. In many cases it is possible to estimate the proportions of spam and non-spam in a user X  X  inbox much more cheaply than the actual labels. We would like to use this informa-tion to categorize e-mails into spam and non-spam. Similarly, consider the problem of filtering images with  X  X mproper content X . Datasets of such images are read-ily accessible thanks to user feedback, and it is rea-sonable to assume that this labeling is highly reliable. However the rest of images on the web (those not la-beled) is a far larger dataset, albeit without labels (af-ter all, this is what we would like to estimate the labels for). That said, it is considerably cheaper to obtain a good estimate of the proportions of proper and im-proper content in addition to having one dataset of images being of likely improper content. We would like to obtain a classifier based on this information. In this paper we present a method to estimate labels directly in such situations, assuming that only label proportions be known. In the above examples, this would be helpful in identifying potential customers, spam e-mails and improper images. We prove bounds indicating that the estimates obtained are close to those from a fully labeled scenario. The formal setting though is more general than the above examples might suggest: we do not require any label to be known, only their proportions within each of the involved datasets. Also we are not restricted to the binary case but in-stead can deal with large numbers of classes. Problem Formulation Assume that we have n sets of observations X i = x i 1 ,...,x i m ple sizes m i (our calibration set) as well as a set X = { x 1 ,...,x m } (our test set). Moreover, assume that we know the fractions  X  iy of patterns of labels y  X  Y ( | Y |  X  n ) contained in each set X i and assume that we also know the marginal probability p ( y ) of the test set X . 1 It is our goal to design algorithms which are able to obtain conditional class probability esti-mates p ( y | x ) solely based on this information. As an illustration, take the spam filtering example. We have X 1 =  X  X ail in spam box X  (only spam) and X 2 =  X  X ail in inbox X  (spam mixed with non-spam). The test set X then may be X 2 itself, for example. The goal is to find p (spam | mail) in X 2 . Note that (for general  X  iy this is more difficult than transduction, where we have at least one dataset with actual labels plus an unla-beled test set where we might have an estimate as to what the relative fractions of class labels might be. Our idea relies on uniform convergence properties of the expectation operator and of corresponding risk functionals (Altun &amp; Smola, 2006). In doing so, we are able to design estimators with the same perfor-mance guarantees in terms of uniform convergence as those with full access to the label information. 2.1 Exponential Families Denote by X the space of observations and let Y be the space of labels. Moreover, let  X  ( x,y ) : X  X  Y  X  H be a feature map into a Reproducing Kernel Hilbert Space (RKHS) H with kernel k (( x,y ) , ( x 0 ,y 0 )). In this case we may state conditional exponential models via where the normalization g is called the log-partition function. For { ( x i ,y i ) } drawn iid from a distribution p ( x,y ) on X  X  Y the conditional log-likelihood is where  X  XY is defined as in Table 2. In order to avoid overfitting one commonly maximizes the log-likelihood penalized by a prior p (  X  ). This means that we need to solve the following optimization problem For instance, for a Gaussian prior on  X  , i.e. for  X  log p (  X  ) =  X  k  X  k 2 + const . we have  X   X  = argmin The problem is that in our setting we do not know the labels y i , so the sufficient statistics  X  XY cannot be computed exactly. The only place where the labels enter the estimation process is via the mean  X  XY . Our strategy is to exploit the fact that this quantity, how-ever, is statistically well behaved and converges under relatively mild technical conditions at rate O ( m  X  1 2 ) to its expected value (see Theorem 2) Our goal therefore will be to estimate  X  xy and use it as a proxy for  X  XY , and only then solve (5) with the es-timated  X   X  XY instead of  X  XY . We will discuss explicit convergence guarantees in Section 3 after describing how to compute the mean operator in detail. 2.2 Estimating the Mean Operator In order to obtain  X   X  we would need  X  XY , which is impossible to compute exactly, since we do not have Y . However, we know that  X  XY and  X  xy are close. Hence, if we are able to approximate  X  xy this, in turn, will be a good estimate for  X  XY .
 Our quest is therefore as follows: express  X  xy as a linear combination over expectations with respect to the distributions on the datasets X 1 ,...,X n (where n  X  | Y | ). Secondly, show that the expectations of the distributions having generated the sets X i (  X  set x [ i,y see Table 2) can be approximated by empirical means (  X 
X [ i,y bine both steps to provide guarantees for  X  XY . It will turn out that in certain cases some of the al-gebra can be sidestepped, in particular whenever we may be able to identify several sets with each other (e.g. the test set X is one of the training datasets X i ) or whenever  X  ( x,y ) factorizes into  X  ( x )  X   X  ( y ). Mean Operator: Since  X  xy is a linear operator map-ping p ( x,y ) into a Hilbert Space we may expand  X  xy  X  where the shorthand  X  class x [ y,y ] is defined in Table 2. This means that if we were able to compute  X  class x [ y,y ] we would be able to  X  X eassemble X   X  xy from its indi-vidual components. We now show that  X  class x [ y,y ] can be estimated directly.
 Key to our assumptions is that p ( x | y,i ) = p ( x | y ). In other words, we assume that the conditional distribu-tion of x is independent of the index i , as long as we know the label y . This yields the following: This allows us define the following means  X  x [ i,y Note that in order to compute  X  set x [ i,y 0 ] we do not need any label information with respect to p ( x | i ). However, since we have at least | Y | of those equations and we assumed that  X  has full rank, they allow us to solve a linear system of equations and compute  X  class x [ y,y ] from  X  set x [ i,y 0 ] for all i . That is, we may use  X  x =  X  X  to compute  X  class x [ y,y ] for all y  X  Y . Whenever  X   X  R n  X  n is invertible (8) reduces to  X  class x =  X   X  1 With some slight abuse of notation we have  X  class x and  X  x represent the matrices of terms  X   X  x [ i,y Algorithm 1
Input datasets X , { X i } , probabilities  X  iy and p ( y ) for i = 1 to n and y 0  X  Y do end for Compute  X   X  XY = P y  X  Y p ( y ) X   X  class x [ y,y ]
Solve the minimization problem Return  X   X   X  .
 Obviously we cannot compute  X  set x [ i,y 0 ] explicitly, since we only have samples from p ( x | i ). However the same convergence results governing the convergence of  X 
XY to  X  xy also hold for the convergence of  X  set X [ i,y to  X  set x [ i,y 0 ]. Hence we may use the empirical average  X 
X [ i,y find an estimate for  X  XY (see Algorithm 1). 2.3 Special Cases In some cases the calculations described in Algorithm 1 can be carried out more efficiently.
 Minimal number of sets, i.e. | Y | = n : Provided that the inverse can be computed more directly. Testing on one of the calibration sets, i.e. X = X : This means that X is one of the training sets. We only need one less set of observations. This is particularly useful for factorizing feature maps. Special feature map  X  ( x,y ) =  X  ( x )  X   X  ( y ) : In this case the calculations of  X   X  class x [ y,y 0 ] and  X  set X greatly simplified, since we may pull the dependency on y out of the expectations. Defining  X  class x [ y ] , X  and  X  set X [ i ] as in Table 2 allows us to simplify A significant advantage of (10) is that we only need to perform O ( n ) averaging operations rather than O ( n  X | Y | ). Obviously the cost of computing (  X  &gt;  X  ) remains unchanged but the latter is negligible in com-parison to the operations in Hilbert Space.
 Binary classification: One may show that the fea-ture map  X  ( x,y ) takes on a particularly appealing form of  X  ( x,y ) = y X  ( x ) where y  X  { X  1 } . This follows since we can always re-calibrate  X   X  ( x,y ) , X   X  by an offset in-dependent of y such that  X  ( x, 1) +  X  ( x,  X  1) = 0. If we moreover assume that X 1 only contains class 1 and X 2 = X contains a mixture of classes with labels 1 and  X  1 with proportions p (1) =:  X  and p (  X  1) = 1  X   X  respectively, we obtain the mixing matrix Plugging this into (10) and the result in (9) yields Consequently taking a simple weighted difference be-tween the averages on two sets, e.g. one set containing spam whereas the other one containing an unlabeled mix of spam and non-spam allows one to obtain the sufficient statistics needed for estimation. The obvious question is how well  X   X  XY manages to ap-proximate  X  XY and secondly, how badly any error in estimating  X  XY would affect the overall quality of the solution. We approach this problem as follows: first we state the uniform convergence properties of  X  XY and similar empirical operators relative to  X  xy . Secondly, we apply those bounds to the cases discussed above, and thirdly, we show that the approximate minimizer of the log-posterior has a bounded deviation from what we would have obtained by knowing  X  XY exactly. 3.1 Uniform Convergence for Mean Operators In order to introduce the key result we need to intro-duce Rademacher averages: Definition 1 (Rademacher Averages) Let X be a domain and p a distribution on X and assume that X := { x 1 ,...,x m } is drawn iid from p . Moreover, let F be a class of functions X  X  R . Furthermore denote by  X  i Rademacher random variables, i.e. { X  1 } valued with zero mean. The Rademacher average is This quantity measures the flexibility of the function class F  X  in our case linear functions in  X  ( x,y ). Theorem 2 (Convergence of Empirical Means) Denote by  X  : X  X  B a map into a Banach space B , denote by B  X  its dual space and let F the class of linear functions on B with bounded B  X  norm by 1 . Let R &gt; 0 such that for all f  X  F we have | f ( x ) |  X  R . Moreover, assume that X is an m -sample drawn from p on X . For  X  &gt; 0 we have that with probability at least 1  X  exp(  X   X  2 m/ 2 R 2 ) the following holds: For k  X  0 we only have a failure probability of 1  X  exp(  X   X  2 m/R 2 ) .
 Theorem 3 (Bartlett &amp; Mendelson (2002)) Whenever B is a Reproducing Kernel Hilbert Space with kernel k ( x,x 0 ) the Rademacher average can be bounded from above by R m ( F )  X  m  X  1 2 [ E x [ k ( x,x )]] Our approximation error can be bounded as follows. From the triangle inequality we have: For the second term we may employ Theorem 2 di-rectly. To bound the first term note that by linearity where we define the matrix of coefficients Now note that all  X  [ i,y 0 ] also satisfy the conditions of Theorem 2 since the sets X i are drawn iid from the distributions p ( x | i ) respectively. We may bound each term individually in this fashion and subsequently ap-ply the union bound to ensure that all n  X  | Y | com-ponents satisfy the constraints. Hence each of the terms needs to satisfy the constraint with probability 1  X   X / ( n | Y | ) to obtain an overall bound with probabil-ity 1  X   X  . To obtain bounds we would need to bound the linear operator mapping  X  into . 3.2 Special Cases A closed form solution in the general case is not par-ticularly useful. However, we give an explicit analy-sis for two special cases: firstly the situation where  X  ( x,y ) =  X  ( x )  X   X  ( y ) and secondly, the binary clas-sification setting where  X  ( x,y ) = y X  ( x ) and X i = X , where much tighter bounds are available.
 Special feature map We only need to deal with n rather than with n  X | Y | empirical estimates, i.e.  X  set vs.  X  set X [ i,y 0 ]. Hence (14) and (15) specialize to Assume that with high probability each  X  [ i ] satisfies k  X  [ i ] k  X  c i (we will deal with the explicit constants c later). Moreover, assume for simplicity that | Y | = n and that  X  has full rank (otherwise we need to follow  X   X  1 ). This implies that bounds we have the following theorem: Theorem 4 Assume that we have n sets of observa-tions X i of size m i , each of which drawn from distri-butions with probabilities  X  iy of observing data with label y . Moreover, assume that k (( x,y ) , ( x 0 ,y Finally, assume that m = | X | . In this case the mean operator  X  XY can be estimated by  X   X  XY with probability at least 1  X   X  with precision Proof We begin our argument by noting that both for  X  ( x,y ) and for  X  ( x ) the corresponding Rademacher averages R m for functions of RKHS norm bounded by 1 is bounded by m  X  1 2 . This is a consequence of all kernels being bounded by 1 in Theorem 3 and k  X  0. Next note that in Theorem 2 we may set R = 1, since for k f k  X  1 and k (( x,y ) , ( x,y ))  X  1 and k ( x,x )  X  1 it follows from the Cauchy Schwartz inequality that | f ( x ) |  X  1. Solving  X   X  exp  X  m 2 for yields  X  m Finally, note that we have n + 1 deviations which we need to bound: one between  X  XY and  X  xy , and n for each of the [ i ] respectively. Dividing the failure probability  X  into n + 1 cases yields bounds of the form m  X  1 2 h 2 + p log (( n + 1) / X  ) i and m error terms into (18) and summing over terms yields the claim and substituting this back into the triangle inequality proves the claim.
 Binary Classification Next we consider the special case of binary classification where X 2 = X . Using (11) we see that the corresponding estimator is given by Since  X   X  XY shares a significant fraction of terms with  X  XY we are able to obtain tighter bounds as follows: Theorem 5 With probability 1  X   X  (for 1 &gt;  X  &gt; 0 ) the following bound holds: k  X   X  XY  X   X  XY k  X  2  X  h 2 + p log(2 / X  ) i h m  X  1 2 1 + m m + is the number of observations with y = 1 in X 2 . Proof Denote by  X  [ X + ] and  X  [ X  X  ] the averages over the subsets of X 2 with positive and negative labels respectively. By construction we have that Taking the difference yields 2  X  [  X  set X [1]  X   X  [ X + ]]. To prove the claim note that we may use Theo-bound and summing over terms proves the claim. The bounds we provided show that  X   X  XY converges at the same rate to  X  xy as  X  XY does, assuming that the sizes of the sets X i increase at the same rate as X . 3.3 Stability Bounds To complete our reasoning we need to show that those bounds translate in guarantees in terms of the mini-mizer of the log-posterior. In other words, estimates using the correct mean  X  XY vs. its estimate  X   X  XY do not differ by a significant amount. For this purpose we make use of (Altun &amp; Smola, 2006, Lemma 17). Lemma 6 Denote by f a convex function on H and let  X ,  X   X   X  H . Moreover let  X  &gt; 0 . Finally denote by  X  ,  X  H the minimizer of with respect to  X  and  X   X   X  the minimizer of L (  X   X ,  X   X  ) re-spectively. In this case the following inequality holds: This means that a good estimate for  X  immediately translates into a good estimate for the minimizer of the approximate log-posterior. This leads to the following bound on the risk minimizer.
 Corollary 7 The deviation between  X   X  , as defined in (4) and  X   X   X  , the minimizer of the approximate log-posterior using  X   X  XY rather than  X  XY , is bounded by Finally, we may use (Altun &amp; Smola, 2006, Theorem 16) to obtain bounds on the quality of  X   X   X  when con-sidering how well it minimizes the true negative log-posterior. Using the bound yields the following bound for the log-posterior: Corollary 8 The minimizer  X   X   X  of the approximate log-posterior using  X   X  XY rather than  X  XY incurs a penalty of at most  X   X  1 k  X   X  XY  X   X  XY k 2 . Note that our analysis so far focused on a specific set-ting, namely maximum-a-posteriori analysis in expo-nential families. While this is a common and popular setting, the derivations are by no means restricted to this. We have the entire class of (conditional) models described by Altun &amp; Smola (2006); Dud  X  X k &amp; Schapire (2006) at our disposition. They are characterized via minimize Here p is a distribution, H is an entropy-like quantity defined on the space of distributions, and  X  ( z ) is some evaluation map into a Banach space. This means that the optimization problem can be viewed as an approxi-mate maximum entropy estimation problem, where we do not enforce exact moment matching of  X  but rather allow slack. In both Altun &amp; Smola (2006) and Dud  X  X k &amp; Schapire (2006) the emphasis lay on unconditional density models: the dual of the above optimization problem. In particular, it follows that for H being the Shannon-Boltzmann entropy, the dual optimiza-tion problem is the maximum a posteriori estimation problem, which is what we are solving here.
 In the conditional case, p denotes the collection of tation operator on the set of observations. Finally, cal observations. We have two design parameters: Function Space: Depending on which Banach Space norm we may choose to measure the deviation between  X  and its expectation with respect to p in terms of e.g. the ` 2 norm, the ` 1 norm or the `  X  norm. The latter would lead to sparse coding and convex combinations. Entropy and Regularity: Depending on the choice of entropy and divergence functionals we obtain a range of diverse estimators. For instance, if we were to choose the unnormalized entropy instead of the en-tropy, we would obtain AdaBoost style problems. We may also use Csiszar and Bregmann divergences. The key point is that our reasoning of estimating  X  XY based on an aggregate of samples with unknown la-bels but known label proportions is still applicable. This means that it should be possible to design boost-ing algorithms and sparse coding methods which could operate on similarly restricted sets of observations. Transduction G  X artner et al. (2006) and Mann &amp; Mc-Callum (2007) performed transduction by enforcing a proportionality constraint on the unlabeled data via a Gaussian Process model. At first glance these methods might seem applicable for our problem but as stated in Section 1, they do require that we have at least some labeled instances of all classes at our disposition which need to be drawn in an unbiased fashion. This is clearly not the case in our setting.
 Self consistent proportions K  X uck &amp; de Freitas (2005) introduced a more informative variant of the binary multiple-instance learning, in which groups of instances are given along with estimates of the fraction of positively-labeled instances per group. This is then used to design a hierarchical probabilistic model which will generate consistent fractions. The optimization is solved via a MCMC sampler. While only described for a binary problem it could be easily extended to multi-class settings. Chen et al. (2006) and Musicant et al. (2007) use a similar approach with similar drawbacks, since we typically only have as many sets as classes. Conditional Probabilities A seemingly valid alter-native approach is to try building a classifier for p ( i | x ) and subsequently recalibrating the probabilities to ob-tain p ( y | x ). At first sight this may appear promising since this method is easily applicable in conjunction with most discriminative methods. The idea would be to reconstruct p ( y | x ) by However, this is not a useful estimator in our setting for a simple reason: it assumes the conditional inde-pendence y  X  X  X  x | i , which obviously does not hold. A simple example illustrates the problem. Assume that X , Y = { 1 , 2 } and that p ( y = 1 | x = 1) = p ( y = 2 | x = 2) = 1. In other words, the estimation problem is solvable since the classes are well separated. More-over, assume that  X  is given by will only exceed random guessing by at most . Reduction to Binary For binary classification and real-valued classification scores we may resort to yet another fairly straightforward method: build a classi-fier which is able to distinguish between the sets X 1 and X 2 and subsequently threshold labels such that the appropriate fraction of observations in X 1 and X 2 respectively has its proper labels. Unfortunately, multi-class variants of this reduction are nontrivial and experiments show that even for the binary case this method is inferior to our approach.
 Density Estimation Finally, one way of obtaining the probabilities p ( x,y | i ) is to perform density estima-tion for each set of observations X i . Subsequently we may use to re-calibrate the probability estimates. Bayes X  theo-rem is finally invoked to compute posterior probabili-ties. This tends to fail for high-dimensional data due to the curse of dimensionality in density estimation. Datasets: We use binary and three-class classifica-tion datasets from the UCI repository 2 and the Lib-SVM site. 3 If separate training and test sets are avail-able, we merge them before performing nested 10-fold cross-validation. Since we need to generate as many splits as classes, we limit ourselves to three classes. For the binary datasets we use half of the data for X 1 and the rest for X 2 . We also remove all instances of class 2 from X 1 . That is, the conditional class proba-bilities in X 2 match those from the repository, whereas in X 1 their counterparts are deleted.
 For three-class datasets we investigate two different partitions. In scenario A we use class 1 exclusively in X 1 , class 2 exclusively in X 2 , and a mix of all three generate X 3 . In scenario B we use the following splits  X   X  Here the constants c 1 ,c 2 and c 3 are chosen such that the probabilities are properly normalized. As before, X 3 contains half of the data.
 Model Selection: As stated, we carry out a nested 10-fold cross-validation procedure: 10-fold cross-validation to assess the performance of the es-timators; within each fold, 10-fold cross-validation is performed to find a suitable value for the parameters. For supervised classification, i.e. discriminative sort-ing, such a procedure is quite straightforward be-cause we can directly optimize for classification error. For kernel density estimation (KDE), we use the log-likelihood as our criterion.
 Due to the high number of hyper-parameters (at least 8) in MCMC, it is difficult to perform nested 10-fold cross-validation. Instead, we choose the best parame-ters from a simple 10-fold crossvalidation run. In other words, we are giving the MCMC method an unfair ad-vantage over our approach by reporting the best per-formance during the model selection procedure. Finally, for the re-calibrated sufficient statistics  X   X  XY we use the estimate of the log-likelihood on the valida-tion set as the criterion for cross-validation, since no other quantity, such as classification errors is readily available for estimation.
 Algorithms: For discriminative sorting we use an SVM with a Gaussian RBF kernel whose width is set to the median distance between observations (Sch  X olkopf, 1997); the regularization parameter is chosen by cross-validation. The same strategy applies for our algo-rithm. For KDE, we use Gaussian kernels with diag-onal densities. Cross-validation is performed over the kernel width. For MCMC, 10 3 samples are generated after a burn in period of 10 3 steps (K  X uck &amp; de Freitas (2005)).
 Optimization: Bundle methods (Smola et al., 2007; Teo et al. , 2007) are used to solve the optimization problem in Algorithm 1.
 Results: The experimental results are summarized in Table 3. Our method outperforms KDE and discrimi-native sorting. In terms of computation, our approach is somewhat more efficient, since it only needs to deal with a smaller sample size (only X rather than the union of all X i ). The training time for our method is less than 2 minutes for all cases, whereas MCMC on average takes 15 minutes and maybe even much longer when the number of active kernels and/or observations are high. However, for large number of partitions n , the MCMC procedure might potentially have an edge over our method as we do not take full advantage of this setting. However, this can be achieved easily by optimizing the condition number of the pseudoinverse of the redundant system of linear equations.
 Our method also performs well on multiclass datasets. As described in Section 3.2, the quality of our mini-mizer of the log-posterior depends on the mixing ma-trix and this is noticeable in the reduction of perfor-mance for the dense mixing matrix (scenario B) in comparison to the better conditioned sparse mixing matrix (scenario A). In other words, for ill conditioned  X  even our method has its limits, simply due to numer-ical considerations of effective sample size. We have showed a rather surprising result, namely that it is possible to consistently reconstruct the labels of a dataset if we can only obtain information about the proportions of occurrence of each class (in at least as many data aggregates as there are classes). In par-ticular, we prove that up to constants, our algorithm enjoys the same rates of convergence afforded to meth-ods which have full access to all label information. This has a range of potential applications in e-commerce, spam filtering and improper content de-tection. It also suggests that techniques used to anonymize observations, e.g. demographic data, may not be really safe. Experiments show our algorithm is fast and outperforms competitive methods.
 Altun, Y., &amp; Smola, A. (2006). Unifying divergence minimization and statistical inference via convex du-ality. COLT X 06 , LNCS, 139 X 153. Springer.
 Bartlett, P. L., &amp; Mendelson, S. (2002). Rademacher and Gaussian complexities: Risk bounds and struc-tural results. JMLR , 3 .
 Chen, B., Chen, L., Ramakrishnan, R., &amp; Musicant,
D. (2006). Learning from aggregate views. ICDE X 06 , 3 X 12.
 Dud  X  X k, M., &amp; Schapire, R. E. (2006). Maximum en-tropy distribution estimation with generalized regu-larization. COLT X 06 , Springer.
 G  X artner, T., Le, Q., Burton, S., Smola, A., &amp; Vish-wanathan, S. (2006). Large-scale multiclass trans-duction. NIPS X 06 K  X uck, H., &amp; de Freitas, N. (2005). Learning about individuals from group statistics. In UAI X 05 , 332 X  339.
 Mann, G., &amp; McCallum, A. (2007). Simple, robust, scalable semi-supervised learning via expectation regularization. In ICML X 07 .
 Musicant, D., Christensen, J., &amp; Olson, J. (2007). Su-pervised learning by training on aggregate outputs. In IEEE ICDM .
 Sch  X olkopf, B. (1997). Support Vector Learning . Old-enbourg Verlag.
 Smola, A., Vishwanathan, S. V. N., &amp; Le, Q. (2007). Bundle methods for machine learning. In NIPS X 07 . Teo, C.H., Le, Q.V, Smola, A.J., &amp; Vishwanathan,
S.V.N. (2007). A Scalable Modular Convex Solver for Regularized Risk Minimization. In KDD X 07 . Errors are reported in mean  X  standard error. The best result and those not significantly worse than it, are highlighted in boldface. We use a one-sided paired t-test with 95% confidence.
 MM : Mean Map (our method); KDE : Kernel Density Es-timation; DS : Discriminative Sorting (only applicable for binary classification); MCMC : the sampling method; BA : Baseline, obtained by predicting the major class.  X  : Program fails (too high dimensional data -only KDE ).  X  : Program fails (large datasets -only MCMC ). Acknowledgments We thank Hendrik K  X uck and Choon Hui Teo. NICTA is funded by the Aus-tralian Government X  X  Backing Australia X  X  Ability and the Centre of Excellence programs. We received fund-ing of the FP7 Network of Excellence by the European
