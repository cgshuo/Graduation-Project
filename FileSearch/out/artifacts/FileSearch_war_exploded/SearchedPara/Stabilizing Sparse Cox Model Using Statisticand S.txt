 Stability is fundamental to prognosis. Besides good performance, a prognostic model needs to be interpretable and stable to warrant clinical adoption. This translates to a small group of succinct predictors that are consistent in the face of data re-sampling. Hence strong feature selection is key when deriving clinical models.
 causes instability in linear [ 1 ] and survival models [ 2 ]. These aspects are intrin-sic to modern healthcare data. Medical events often co-occur, especially in aged cohorts. Comorbidities or diseases that co-exist with the primary disease in a patient, cause multiple diagnoses that are strongly correlated to each other. For example, Fig. ( 1 a) 1 shows the common complications in a diabetic cohort. These correlations can be visualized as a network, as in Fig. ( 1 b). When deriv-ing a prediction model from such data, we have to account for the complex interconnectedness of the features.
 Integrating domain knowledge to improve learning has been gaining much attention recently [ 3 ]. Biological understanding of gene-disease networks, for example, has enabled discovery of what genes contribute to a disease, and what proteins would bind with a particular chemical compound [ 4 ]. However, little has been explored in networks derived from the healthcare processes and their contribution to prediction models. Domain knowledge, represented as networks, should ideally guide the feature selection process in clinical prediction. In this paper, we address the problem of stabilizing a high dimensional model derived from routinely collected EMR data. We focus on minimizing the vari-ance in feature subsets and model estimation parameters. We construct a feature graph with nodes as EMR features and edges as relationship between the fea-tures. We look at three feature relationships: (i) Jaccard score between features (ii) aggregate of Jaccard score and the semantic EMR link used in [ 5 ] (iii) Jaccard scores between features transferred from a related cohort. A random walk reg-ularization of the proposed graphs is used to stabilize a sparse Cox model that predicts time to readmission. Our experiments are conducted on 2 real world hospital datasets: a heart failure cohort and a diabetes cohort. We measure fea-ture stability using the Consistency index and model estimation stability using signal-to-noise ratio (SNR). Our proposed method, when compared with elastic net and recently introduced semantic EMR graph regularization [ 5 ], confirmed better feature and model stability when validated against both cohorts. 1. Representation of medical domain knowledge as feature graphs that embed 2. A random walk regularizer based on the proposed feature graphs to stabi-3. Demonstration of improved feature stability as measured by the Consistency 4. Demonstration of improved stability, using transfer learning on related coho-The significance of our study lies in understanding the importance of incorporat-ing underlying feature relationships into model learning. This promotes stable feature selection in a clinical setting. Stabilizing clinical prediction has received little attention, partly because most models are built using a small subset of well-defined predictors, chosen either by domain experts or from prior knowledge. In most high dimensional models, the primary regularizer of choice is the lasso because of its convexity and sparsity inducing property [ 6 ]. However, when data is correlated, as in the healthcare domain, lasso regularized models are susceptible to data variations resulting in loss of stability [ 1 ]. The inconsistency of Lasso to handle data correlation has been demonstrated for linear models [ 7 ] and recently for survival models [ 2 ]. When prior knowledge about feature relationships are available, Sandler et al . proposed additional regularization using graph networks where the nodes are fea-tures and edges represent relationships [ 3 ]. Graph regularization ensures statis-tical weight sharing among related features. In such scenarios, the first challenge is to identify useful prior information.
 databases like KEGG, Pathway Commons and BioCarta to extract context spe-cific data to build feature graphs [ 8 ]. Recent linear classification models have started using such gene-pathway networks and protein interaction networks to improve prediction accuracy and model interpretability [ 9 ]. For genomic data, a recent study employed a quadratic Laplacian regularizer into Cox regression, where the Laplacian graph was derived from prior gene regulatory network infor-mation [ 10 ]. Another study investigated the significance of including domain knowledge as network information into Cox model for identifying biomarkers in breast cancer [ 11 ]. Specifically, the study compared eight network based regu-larizers and three non-network based regularizers for Cox regression. All meth-ods were validated on five public breast cancer datasets. The study observed no significant advantage for network-based approaches over non-network-based approaches in terms of prediction performance or signature stability. In the healthcare domain, elastic net regularized Cox model showed superior performance over Lasso for prostate cancer dataset [ 12 ]. Though elastic net reg-ularization can handle correlated features, it cannot incorporate structural rela-tionships. Vinzamuri et al . introduced a modification to the elastic net involving an RBF kernel for handling feature correlations to improve accuracy and reduce redundancy [ 13 ]. In contrast, our work focuses on handling correlations to stabi-lize the model estimation and top predictors. Feature similarity is captured by building a Jaccard similarity graph for additional regularization. Recent stud-ies use semantic graphs constructed from ICD-10 code relations and temporal relations in medical events to stabilize linear readmission models derived from electronic medical records [ 5 ]. We compare our approach with this EMR graph regularization. Further, we investigate the effect of aggregating the semantic EMR graph with the statistical Jaccard graph on model stability. 3.1 Sparse Cox Model We use Cox regression to model risk of readmission (hazard function) at a future time instance, based on data from EMR. Unlike logistic regression where each patient is assigned a nominal label, Cox regression models the readmission time directly [ 13 ]. The proportional hazards assumption in Cox regression assumes a constant relationship between readmission time and EMR-derived explanatory variables. Let D = { x n ,y n } N n =1 be the training dataset with ordered on increasing y n , where x n  X  R K denotes the feature vector for index admission and y n is the time to next unplanned readmission. When a patient withdraws from the hospital or does not encounter readmission in our data during the follow-up period, the observation is treated as right censored. Let M observations be uncensored and R ( t n ) be the remaining events at readmission time t Since the data D is high dimensional (possibly K N ), we apply lasso regu-larization for sparsity induction [ 14 ]. The feature weights by maximizing the 1 -penalized partial likelihood: where w 1 = k | w k | ,  X &gt; 0 is the regularizing constant, and log partial likelihood [ 15 ] computed as: However, sparsity induction is known to cause instability in feature selection [ 16 ]. Instability occurs because Lasso randomly chooses one in two highly correlated features. Each training run with slightly different data could result in a different feature from the correlated pair. The nature of EMR data further aggravates this problem. The EMR data is, by design, highly correlated and redundant. Also, features in the EMR data maybe weakly predictive for some task, thereby limiting the probability that they are selected. These sum up to lack of repro-ducibility between model updates or external validations, hindering the method credibility and adoption by clinicians.
 modifies the likelihood function in Eq. ( 1 )as: Here, the ridge regression term k w 2 k tends to give equal weights to corre-lated features, while the lasso term k | w k | introduces sparsity. However, this formulation ignores domain knowledge. 3.2 Stabilization Using Feature Graph Medical events often co-occur, especially in aged cohorts. For example, the pres-ence of comorbidities causes multiple diagnoses at the same time. We capture feature correlation in a knowledge network, with features as nodes and relations between features as edges. Let the adjacency matrix of the feature graph be where G ij = g  X  (0 , 1) represents the weighted similarity score between features and j . We ensure all features have equal prominence by constraining the out-links of each node to sum to one. The medical events linked together in the feature graph should have similar weights.We introduce a random walk regularizer [ 3 ]: where I is the identity matrix. The graph stabilized model likelihood can be written as: Here the 1 regularizer introduces sparsity by pushing weak features towards zero, while the random walk regularizer distributes smoothness equally among correlated features. The gradient of Eq. ( 4 ) becomes: Parameter estimation is done by maximizing the likelihood in Eq. ( 4 )using L-BFGS algorithm [ 17 ]. We build and compare different feature graphs to sta-bilize our model. Each feature graph differs in the construction of its adjacency matrix G . A recent study [ 5 ] introduced a semantic EMR graph, where nodes denoted features and edges denoted a temporal relation or ICD-10 structural relation between features (Fig. 2 b). Using this as a baseline, we construct G using the following methods. First, we represent the edges using the Jaccard index between features, as in Fig. 2 a. Second, we aggregate the baseline seman-tic EMR graph and the Jaccard graph. Here, each edge is the maximum of Jaccard and semantic scores between the features (Fig. 2 c). Finally, we inves-tigate transferring the adjacency matrix between related cohorts. Specifically, the Jaccard similarity scores between features in one cohort is transferred to a related cohort (Fig. 2 d). We detail these methods below.
 Jaccard Graph. The Jaccard index measures the percentage of agreement between components among feature vectors. Given two feature vectors F , the pairwise Jaccard score reads: where a is the number of non-zero components in F i and F non-zero components in F i but not in F j and c is number of non-zero components F1 in F j but not in F i . We construct an undirected graph with nodes as features and edges representing the Jaccard score between features.
 Graph Aggregation. We investigate the effect of combining the semantic EMR graph with Jaccard graph on model stability and feature stability. The semantic EMR graph captures the general relationship between diagnostic codes based on the ICD-10 structures, while the Jaccard graph is cohort specific. Here, we use a simple aggregation technique to construct the final EMR; Jaccard graph as: Transfer Learning. Finally, we examine the capability of our proposed method in transfer learning. Knowledge from one domain can be transferred to a related domain when data is scarce or expensive to collect [ 18 ]. Getting high quality training data is often difficult, particularly in a medical setting. Cohorts that share comorbidities and diagnoses, as in diabetes and cardiovascular diseases, are likely to have similar correlations among features. Accordingly, we propose to stabilize a Cox model derived from one cohort using the Jaccard similarity graph constructed from a related cohort. We denote the transferred graph as: TL-Jaccard graph. Further, we use TL-Jaccard graph to construct the aggre-gated graph: Here, the temporal and hierarchical feature relations in the cohort are captured by the EMR graph. The statistical relations among features, which can be expen-sive to calculate, are transferred from the related cohort using TL-Jaccard graph. In this section, we evaluate feature and model stability of our framework. The results are reported on two cohorts: heart failure (HF) and diabetes (DB), pro-vided by Barwon Health, a regional health service provider which has been serving more than 350 , 000 residents in Victoria, Australia spective data for heart failure and diabetes patients from the hospital EMR database. The heart failure cohort contains all patients with at least one ICD-10 diagnosis code I50, while the diabetes cohort includes all patients with at least one diagnosis code between E10-E14. This resulted in 1 , 885 heart failure admis-sions and 2 , 840 diabetes admissions between January 2007 and December 2011. Patients of all age groups were included whilst inpatient deaths were excluded. We focus our study on emergency attendances and unplanned admissions of patients.
 We use the one-sided convolutional filter bank introduced in [ 19 ]toextract a large pool of features from EMR databases. The filter bank summarizes event statistics over multiple time periods and granularities. The feature extraction process resulted in 3 , 338 features for heart failure cohort and 7 for diabetes cohort. The extracted features are used to derive a sparse Cox model. Our proposed feature graphs capture correlations between these features to stabilize model learning. 4.1 Evaluation Protocol The baseline regularization methods for Cox regression are chosen to be (i) lasso (ii) elastic net (iii) semantic EMR graph (as in [ 5 ]). Based on the construction of the feature graph, we arrive at four different models: (i) Jaccard graph regu-larized model: feature graph is the Jaccard similarity graph among features in the given cohort (ii) EMRJaccard regularized model: feature graph is the aggre-gation of Jaccard graph with semantic EMR graph, as in Eq. ( 7 ) in the given cohort (iii) TL Jaccard regularized model: feature graph is the Jaccard similar-ity graph transferred from a related cohort (iv) EMR; TL Jaccard regularized model: feature graph is the aggregation of semantic EMR graph from the given cohort and Jaccard graph transferred from a related cohort.
 Temporal Validation. We ensure that the training and testing sets are com-pletely separated in time. This validation strategy is chosen because it better reflects the common practice of training the model in the past and using it in the future.We gather admissions which have discharge dates before September 2010 for heart failure and before 2009 for diabetes patients to form the training set and after that for testing. Next we specify the set of unique patients in the training set. We then remove all admissions of such patients in the testing set to guarantee no overlap between two sets. The statistical characteristics of two cohorts are summarized in Table 1 .
 Model performance is evaluated using measures of AUC (area under the ROC curve) with confidence intervals based on Mann-Whitney statistic. The AUC is computed from the ranking of hazard rates of the patient readmissions. Measuring Model Stability. We use the Consistency index [ 20 ]tomeasure stability of feature selection process. The Consistency index ( ture selection in obtaining several desirable properties, i.e., monotonicity, limits and correction for chance. To simulate data variations due to sampling, we cre-ate B data bootstraps of original size n . For each bootstrap, a model is trained and a subset of top k features is selected. Features are ranked according to their importance, which are product of feature weight and standard deviation. Finally, we obtain a list of feature subsets S = { S 1 ,S 2 , ..., S The Consistency index corrects the overlapping due to chance. Considering a pair of subsets S i and S j , the pairwise Consistency index in which | S a  X  S b | = r and d is the number of features. The stability for the set S = { S Consistency index is bounded in [  X  1 , +1].
 We further our investigations on the model stability. The model estimation stability is defined as variance in parameters. A measure is the signal-to-noise ratio (SNR): SNR ( i )=  X  w i /  X  i in which  X  w i is the mean feature weight across bootstraps for feature i ,and  X  i is its standard deviation. We take the average of the 20 highest SNR values. Higher score indicates better stability. 4.2 Results Our models are designed using two hyper-parameters: lasso regularization parameter  X  and graph regularization parameter  X  . We empirically tune these parameters to improve feature stability without hurting model discrimination. Overall, feature stability depended more on graph parameter discrimination was more sensitive to  X  . A good tradeoff was achieved at and  X  =0 . 8. All models are externally validated against (i) heart failure cohort with a 6-month horizon (ii) diabetes cohort with a 12-month horizon. Table 2 reports the AUC scores with confidence intervals for the different models. The predictive performance of our proposed graph stabilization models and transfer learning techniques are comparable with the baselines.
 Stabilization Using Statistical and Semantic Graphs. Graph regularized models consistently produced more stable features than lasso and elastic net reg-ularized models. When comparing different graph regularizations, we found the semantic EMR graph to be more effective for small feature subsets (see Fig. 3 ). For increasing feature subset sizes ( &gt; 100), Jaccard graphs proved effective. The temporal and structural relations of diagnosis codes have stronger effect for small set of features, while Jaccard index was effectual on larger sets. This behavior suggests aggregating statistical and semantic structures. For the top 100 predic-tors, EMRJaccard graph stabilization demonstrated the highest feature stability in both cohorts (see Fig. 3 ).
 Fig. 5 , each model is represented by average of its 20 highest SNR values. The Jaccard graph regularized model proved to be most robust in both cohorts. Interestingly, model stability using EMRJaccard graph was similar to elastic net and was not able to improve upon semantic EMR graph or Jaccard graph. Stabilization Using Transfer Learning. We investigate transfer of feature graphs between related cohorts. For the heart failure cohort, TL Jaccard graph represents the Jaccard scores transferred from diabetes cohort, while EMR;TL Jaccard graph is the aggregation of the semantic EMR graph of heart failure cohort and Jaccard graph transferred from diabetes cohort. The same technique is applied to stabilize diabetes cohort, where the Jaccard scores are transferred from heart failure cohort. We compare the transferred graph stabilizations with lasso and elastic net. Figs. ( 4 ; 5 ) show that the cross-domain graphs also help the stabilities of feature selections and model estimation. Novel methods in feature selection often concentrate on model performance and overlook stability [ 21 , 22 ]. Stability facilitates reproducibility between model updates and generalization across medical studies. Stable predictors inspire con-fidence in prognosis, as they are often subjected to further examinations. In this paper, we utilize statistical and semantic relations in EMR data to stabilize a sparse Cox model for predicting readmission. The model is validated on two dif-ferent retrospective cohorts. The random walk regularization of the aggregated feature graph promotes group level selection and rare-but-important features. On two stability measures, the proposed method has demonstrated largely improved stability. In related cohorts, when collecting data becomes expensive, transferring domain knowledge using TL-Jaccard graph was also found to improve stability. Also, our proposed model is derived entirely from commonly available data in medical databases. All these factors suggest that our model could be easily inte-grated into the clinical pathway to serve as a fast and inexpensive screening tool in selecting features and patients for further investigation. Future work includes applying the same technique for a variety of cohorts and investigating other latent correlations in EMR to enhance feature stability.

