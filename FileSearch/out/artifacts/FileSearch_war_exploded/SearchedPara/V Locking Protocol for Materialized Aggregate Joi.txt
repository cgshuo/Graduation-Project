 Materialized views are widely used in da tabase systems and Web-based information systems to improve query performance [4]. As real-time decision making is increas-ingly being needed by enterprises [14], th e requirement of immediate materialized view maintenance with transactional cons istency is becoming more and more neces-sary and important for providing consistent and up-to-date query results. Reflecting real world application demands, this requirement has become mandatory in the TPC-R benchmark [13]. Graefe and Zwilling [6] also argued that materialized views are like indexes. Since indexes are always maintained immediately with transactional consistency, materialized views should fulfill the same requirement. A few detailed examples for this requirement on materialized views are provided in [6]. 
In a materialized aggregate join view AJV , multiple tuples are aggregated into one group if they have identical group by attribute values. If generic concurrency control mechanisms are used, immediate maintenance of AJV with transactional consistency can cause significant performance degradation in the database system. Since different tuples in a base relation of AJV can affect the same aggregated tuple in AJV , the addi-tion of AJV can introduce many lock conflicts and/or deadlocks that do not arise in the absence of AJV . The smaller AJV is, the more lock conflicts and/or deadlocks will occur. In practice, this deadlock rate can easily become 50% or higher [9]. A detailed deadlock example is provided in [9]. 
To address this lock conflict/deadlock issue, we previously proposed the V+W locking protocol [9] for materialized aggregate join views. The key insight is that the COUNT and SUM aggregate operators are associative and commutative [7]. Hence, during maintenance of the materialized aggregate join view, we can use V locks rather than traditional X locks. V locks do not conflict with each other and can increase con-currency, while short-term W locks are used to prevent  X  X plit group duplicates X   X  multiple tuples in the aggregate join view for the same group, as shown in Section 2.2 below. [9] described how to implement the V+W locking protocol on both hash indi-ces and B-tree indices. replaced by a latch (i.e., semaphore) pool solution, which is more efficient because acquiring a latch is much cheaper than acquiri ng a lock [5]. This leads to the V lock-ing protocol for materialized aggregate join views presented in [10]. There, V locks are augmented with a  X  X alue-based X  latch pool. Traditionally, latches are used to pro-tect the physical integrity of certain data structures (e.g., the data structures in a page [5]). In our case of materialized view maintenance, no physical data structure would be corrupted if the latch pool were not used. The  X  X alue-based X  latch pool obtains its name because it is used to protect the logical integrity of aggregate operations rather than the physical integrity of the database. [10] showed how to implement the V locking protocol on hash indices and formally proved the correctness of the imple-mentation method. [10] also performed a simulation study in a commercial RDBMS, demonstrating that the performance of the V locking protocol can be two orders of magnitude higher than that of the traditional X locking protocol. 
This paper improves upon our previous work by addressing the particularly thorny problem of implementing the V locking protocol on B-tree indices. Typically, imple-menting high concurrency locking modes poses special challenges when B-trees are considered, and the V locking protocol is no exception. We make three contributions. First, we present the method of implementing the V locking protocol on B-tree indi-ces. Second, we show that our techniques are all necessary. Third, we formally prove the correctness (serializability) of our implementation method. 
In related work, Graefe and Zwilling [6] independently proposed a multi-version concurrency control protocol for materialized aggregate join views. It uses hierarchi-cal escrow locking, snapshot transactions, key-range locking, and system transactions. The key insight of that multi-version concurrency control protocol is similar to that of our V locking protocol. Nevertheless, that multi-version concurrency control protocol cannot avoid split group duplicates in materialized aggregate join views. Instead, spe-cial operations have to be performed during materialized aggregate join view query proposed techniques are all necessary. [6] al so gave no rigorous proof of the correct-ness of its proposed concurrency control protocol. 
Our focus in this paper is materialized aggregate join views. In an extended rela-tional algebra, a general instance of such a view can be expressed as AJV=  X  (  X  (  X  ( R 1  X  R  X  ...  X  R COUNT , SUM , AVG , MIN , and MAX . However, because MIN and MAX cannot be maintained incrementally (the problem is deletes/updates  X  e.g., when the MIN / MAX value is deleted, we need to compute the new MIN / MAX value using all the values in the aggregate group [3]), we restrict our attention to the three incrementally update-able aggregate operators: COUNT , SUM , and AVG . In practice, AVG is computed using COUNT and SUM , as AVG=SUM/COUNT ( COUNT and SUM are distributive while AVG is algebraic [2]). In the rest of the paper, we only discuss COUNT and SUM , whereas our locking techniques for COUNT and SUM also apply to AVG . Moreover, by letting n=1 in the definition of AJV , we include aggregate views over single relations. In this section, we present our method of implementing the V locking protocol on B-tree indices. On B-tree indices, we use value locks to refer to key-range locks. To be consistent with the approach advocated by Mohan [11], we use next-key locking to implement key-range locking. We use  X  X ey X  to refer to the indexed attribute of the B-(key value, row id list). 2.1 The V Locking Protocol In the V locking protocol for materialized aggregate join views [10], we have three kinds of elementary locks: S, X, and V. The compatibilities among these locks are listed in Table 1, while the lock conversion lattice is shown in Fig. 1. 
In the V locking protocol for materialized aggregate join views, S locks are used for reads, V locks are used for associative and commutative aggregate update writes, while X locks are used for transactions th at do both reads and writes. These locks can be of any granularity, and, like traditional S and X locks, can be physical locks (e.g., tuple, page, or table locks) or value locks. 2.2 Split Groups and B-Trees We consider how split group duplicates can arise when a B-tree index is declared over build a B-tree index I B on attribute a . Also, assume there is no tuple (1, 2, X ) in AJV , another new join result tuple (1, 2, 4) into AJV (by insertion into R ). Using standard transaction will execute something like the following operations: (1) Obtain an X value lock for t 1 .a on I B . This value lock is held until the transaction (2) Make a copy of the row id list in the entry for t 1 .a of I B . (3) For each row id in the row id lis t, fetch the corresponding tuple t 2 . Check whether Suppose now we use V value locks instead of X value locks and the two transactions T and T  X  above are executed in the following sequence: (1) T obtains a V value lock for a=1 on the B-tree index I B , searches the row id list in (2) T  X  obtains a V value lock for a=1 on I B , searches the row id list in the entry for Now the aggregate join view AJV contains two tuples (1, 2, 3) and (1, 2, 4) instead of a single tuple (1, 2, 7). Hence, we have the split group duplicate problem. 2.3 The Latch Pool To enable the use of V locks while avoiding split group duplicates, we use the same latch pool for aggregate join views as that described in [10]. The latches in the latch pool guarantee that for each aggregate group, at any time, at most one tuple corre-sponding to this group exists in the aggregate join view. For efficiency we pre-allocate a latch pool that contains N&gt;1 X (exclusive) latches. We use a hash function H that maps key values into integers between 1 and N . We use latch in the latch pool. 
We ensure that the following properties always hold for this latch pool. First, during request another latch in the latch pool. Second, to request a latch in the latch pool, a transaction must first release all the other latches in the RDBMS (including those latches that are not in the latch pool) that it currently holds. Third, during the period that a transaction holds a latch in the latch pool, this transaction does not request any lock. The first two properties guarantee that there are no deadlocks between latches. The third property guarantees that there ar e no deadlocks between latches and locks. These properties are necessary, because in an RDBMS, latches are not considered in deadlock detection. We define a false latch conflict as one that arises due to hash conflicts (i.e., latch conflicts. It does not affect the correct ness of the V locking protocol. In practice, concurrently running transactions in the RDBMS, the probability of having false latch conflicts should be small. 2.4 Implementing V Locking with B-Trees Implementing a high concurrency locking scheme in the presence of indices is diffi-cult, especially if we consider issues of recoverability. Key-value locking as proposed by Mohan [11] was perhaps the first published description of the issues that arise and their solution. Unfortunately, we cannot directly use the techniques in Mohan [11] to implement V locks as value (key-range) locks. Otherwise as shown in [9], serializabil-ity can be violated. 2.4.1 Operations of Interest To implement V value locks on B-tree indices correctly, we need to combine those techniques in Mohan et al . [11, 5] with the technique of logical deletion of keys [12, 8]. In our protocol, there are five operations of interest: (1) Fetch : Fetch the row ids for a given key value v 1 . (2) Fetch next : Given the current key value v 1 , find the next key value v 2 &gt;v 1 existing (3) Put an X value lock on key value v 1 . (4) Put a first kind V value lock on key value v 1 . (5) Put a second kind V value lock on key value v 1 . 
Transactions use the latches in the latch pool in the following way: (1) To integrate a new join result tuple t into an aggregate join view AJV (e.g., due to (2) To remove a join result tuple from AJV (e.g., due to deletion from some base re-We show why this is by an example. Suppose a B-tree index is built on attribute a of an aggregate join view AJV . Assume we insert a tuple into some base relation of AJV and generate a new join result tuple t . The steps to integrate t into AJV are as follows: 
Once again, we do not know whether we need to update an existing aggregate group in AJV or insert a new aggregate group into AJV until we read AJV . However, we do know that we need to acquire a second kind V value lock on t.a before we can integrate t into AJV . Similarly, suppose we delete a tuple from some base relation of AJV . We compute the corresponding join result tuples. For each such join result tuple t , we execute the following steps to remove t from AJV : 
In this case, we do not know whether we need to update an aggregate group in AJV or delete an aggregate group from AJV in advance. However, we do know that we need to acquire a first kind V value lock on t.a before we can remove t from AJV . 
The ARIES/KVL method described in Mohan [11] for implementing value locks on a B-tree index requires the insertion/deletion operation to be done immediately after a transaction obtains appropriate locks. Also, in ARIES/KVL, the value lock implemen-tation method is closely tied to the B-tree implementation method, because AR-IES/KVL strives to take advantage of both IX locks and instant locks to increase con-currency. In the V locking mechanism, high concurrency has already been guaranteed by the fact that V locks are compatible with themselves. We can exploit this advantage so that our method for implementing value locks for aggregate join views on B-tree indices is more general and flexible than the ARIES/KVL method. Specifically, in our method, after a transaction obtains appropriate locks, we allow it to execute other op-erations before it executes the insertion/deletion/update/read operation. Also, our value lock implementation method is only loosely tied to the B-tree implementation method. 2.4.2 Operation Implementation Method Our method for implementing value locks for aggregate join views on B-tree indices is as follows. Consider a transaction T . 
Op1. Fetch : We first check whether some entry for value v 1 exists in the B-tree in-
Op2. Fetch next : We find the smallest value v 2 in I B such that v 2 &gt;v 1 . Then we put an S lock for v 2 on I B . Op3. Put an X value lock on key value v 1 : We first put an X lock for value v 1 on I . Then we check whether some entry for v 1 exists in I B . If no such entry exists, we 
Op4. Put a first kind V value lock on key value v 1 : We put a V lock for value v 1 on I B . 
Op5. Put a second kind V value lock on key value v 1 : We first put a V lock for exists, we do the following: (c) We release the short-term V lock for value v 2 on I B . Table 2 summarizes the locks acquired during different operations. 
During the period that a transaction T holds a first kind V (or second kind V, or X) value lock for value v 1 on the B-tree index I B , if T wants to delete the entry for value v , T needs to do a logical deletion of keys [12, 8] instead of a physical deletion. That 1. If the delete is rolled back, the delete_flag is reset to 0. If another transaction inserts the entry for v 1 is reset to 0. This is to avoid potential write-read conflicts discussed at the beginning of Section 2.4. The physical deletion operations are necessary, other-wise I B may grow unbounded. To leverage the overhead of the physical deletion op-erations, we perform them as garbage collection by other operations (of other transac-tions) that happen to pass through the affected nodes in I B [8]. 
In Op4 (put a first kind V value lock on key value v 1 ), usually an entry for value v 1 exists in the B-tree index. However, the situation that no entry for v 1 exists in the B-tree index is still possible. To illustrate this, consider an aggregate join view AJV that is defined on base relation R and several other base relations. Suppose a B-tree index I is built on attribute d of AJV . If we insert a new tuple t into R and generate several new join result tuples, we need to acquire the appropriate second kind V value locks on I B before we can integrate these new join result tuples into AJV . If we delete a tu-join result tuples that are to be removed from AJV . These join result tuples must have been integrated into AJV before. Hence, when we acquire the first kind V value locks for their d attribute values, these d attribute values must exist in I B . 
However, there is an exception. Suppose attribute d of the aggregate join view AJV comes from base relation R . Consider the following scenario (see [10] for details). There is only one tuple t in R whose attribute d=v , but no matching tuple in the other base relations of AJV that can be joined with t . Hence, there is no tuple in AJV whose attribute d=v . Suppose transaction T executes the following SQL statement: 
In this case, to maintain AJV , there is no need for T to compute the corresponding join result tuples that are to be removed from AJV . T can execute the following  X  X irect propagate X  update operation: will find that no entry for value v exists in I B . 
In Op4 (put a first kind V value lock on key value v 1 ), even if no entry for value v 1 this case. This is because the first kind V value lock can only be used to remove a join result tuple from the aggregate join view AJV . In the case that no entry for v 1 currently other transaction inserts an entry for v 1 into I B ), because no join result tuple currently exists for v 1 . Then the first kind V value lock on key value v 1 is used to protect a null is clearer from the correctness proof in Section 4. The preceding section is admittedly dense and intr icate, so it is reasonable to ask if all this effort is really necessary. Unfortunately the answer appears to be yes  X  we use the following aggregate join view AJV to illustrate the rationale for the techniques introduced in Section 2.4. The schema of AJV is ( a , sum ( b )). Suppose a B-tree index I is built on attribute a of AJV . We show that if any of the techniques from the previ-ous section are omitted (and not replaced by other equivalent techniques), then we cannot guarantee serializability. Technique 1. As mentioned above in Op5 (put a second kind V value lock on key why. Suppose originally the aggregate join view AJV contains two tuples that corre-a new join result tuple (3, 5) into AJV . T  X  integrates a new join result tuple (2, 6) into AJV . T  X  X  reads those tuples whose attribute a is between 1 and 3. Suppose no latch on v is requested. Also, suppose T , T  X  , and T  X  X  are executed as follows: In this way, T  X  X  can start execution even before T  X  finishes execution. This is incorrect due to the write-read conflict between T  X  and T  X  X  (on the tuple whose attribute a=2 ). Technique 2. As mentioned above in Op5 (put a second kind V value lock on key value v 1 ), if the V lock for value v 2 on the B-tree index I B is acquired as an X lock, we illustrates why. Suppose originally the aggregate join view AJV contains only one result tuple (3, 6) into AJV . T  X  integrates a new join result tuple (2, 5) into AJV . Sup-pose the V lock for v 1 on I B is not upgraded to an X lock. Also, suppose T and T  X  are executed as follows: In this way, T  X  can start execution even before T finishes execution. This is incorrect due to the read-write conflict between T and T  X  (on the tuple whose attribute a=2 ). Technique 3. As mentioned above in Op5 (put a second kind V value lock on key aggregate join view AJV contains two tuples that correspond to a=1 and a=5 . Con-reads those tuples whose attribute a is between 1 and 3. Suppose we do not insert an entry for v 1 into I B . Also, suppose T , T  X  , and T  X  X  are executed as follows: In this way, T  X  X  can start execution even before T finishes execution. This is incorrect due to the write-read conflict between T and T  X  X  (on the tuple whose attribute a=2 ). In this section, we prove the correctness (serializability) of our key-range locking strategy for aggregate join views on B-tree indices. Suppose a B-tree index I B is built on attribute d of an aggregate join view AJV . To prove serializability, for any value v 1 also considered), we only need to show that there is no read-write, write-read, or write-write conflict between two different transactions on those tuples of AJV whose tive and commutative properties of the addition operation. Furthermore, the use of the latches in the latch pool guarantees that for each aggregate group, at any time at most one tuple corresponding to this group exists in AJV . We enumerate all the possible cases to show that write-read and read-write conflicts do not exist. Since we use next-key locking, in the enumeration, we only need to focus on v 1 and the smallest existing value v 2 in I B such that v 2 &gt;v 1 . 
Consider the following two transactions T and T  X  . T updates (some of) the tuples in the aggregate join view AJV whose attribute d has value v 1 . T  X  reads the tuples in AJV whose attribute d has value v 1 (e.g., through a range query). Suppose v 2 is the smallest (or second kind V, or X) value lock for d=v 1 on I B . T  X  needs to obtain an S value lock for d=v 1 on I B . There are four possible cases: (1) Case 1: An entry E for value v 1 already exists in the B-tree index I B . Also, trans-(2) Case 2: An entry E for value v 1 already exists in the B-tree index I B . Also, trans-Hence, for any value v 1 , there is no read-write or write-read conflict between two dif-ferent transactions on those tuples of the aggregate join view AJV whose attribute d has value v 1 . As discussed at the beginning of this section, write-write conflicts do not exist and thus our key-range locking protocol guarantees serializability. Implementing the V locking protocol for (non-aggregate) join views in the presence of B-tree indices is tricky. For example, suppose we do not use the latches in the latch pool. That is, we only use S, X, and V value locks on join views. Suppose we imple-ment S, X, and V value locks for join views on B-tree indices in the same way as de-scribed in Section 2.4. Also, suppose a B-tree index I B is built on attribute a of a join nately, this approach does not work. The reason is similar to what is shown for Tech-lock greatly reduces concurrency.) 
To implement value locks for join views on B-tree indices with high concurrency, we can use the latches in the latch pool and treat join views in the same way as aggre-gate join views. For join views, we still use four kinds of value locks: S, X, first kind V, and second kind V. For example, suppose a B-tree index I B is built on attribute a of a join view JV . As described in Section 2.4, to insert a new join result tuple t into JV , from JV , we first put a first kind V value lock for t.a on I B . For join views, all the four different kinds of value locks (S, X, first kind V, and second kind V) can be imple-mented on B-tree indices in the same way as described in Section 2.4. The only ex-ception is that we no longer need the latc h on the group by attribute value of tuple t . The correctness (serializability) of the implementation can be proved in a way similar to that described in Section 4. Note here, for join views, the latches in the latch pool are used for a different purpose from that for aggregate join views. 
