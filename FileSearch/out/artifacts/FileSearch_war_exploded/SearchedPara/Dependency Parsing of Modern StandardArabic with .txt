 Nuance Communications Center for Computational Learning Systems, Columbia University Center for Computational Learning Systems, Columbia University parsing of Arabic, a morphologically rich language with complex agreement patterns. Using con-morphological features in two input conditions: machine-predicted condition (in which POS tags gold condition, but may be detrimental in the predicted condition, where they are outperformed person, number, gender, and undiacritized lemma) that improve parsing quality in the predicted ( X  X umanness X ) feature, are more helpful for parsing than form-based gender and number. We in a combined gold+predicted condition. We experimented with two transition-based parsers,
MaltParser and Easy-First Parser. Our findings are robust across parsers, models, and input other parsers and morphologically rich languages. 1. Introduction
For Arabic X  X s for other morphologically rich languages X  X he role of morphology is often expected to be essential in syntactic modeling, and the role of word order is less important than in morphologically poorer languages such as English. Morphology interacts with syntax in two ways: agreement and assignment. In agreement , there is coordination between the morphological features of two words in a sentence based on their syntactic configuration (e.g., subject X  X erb or noun X  X djective agreement in
GENDER and/or NUMBER ). In assignment , specific morphological feature values are assigned in certain syntactic configurations (e.g., CASE direct object of a verb). 1 the syntax and choose among different parses. The choice of optimal linguistic features depends on three factors: relevance, redundancy, and accuracy. A feature has relevance if it is useful in making an attachment (or labeling) decision. A particular feature may or may not be relevant to parsing. For example, the GENDER the Arabic phrase / it should attach to the masculine door , resulting in the meaning  X  X he car X  X  new door X ; if the-new is feminine ( ), it should attach to the feminine the-car , resulting in  X  X he door of the new car. X  Conversely, the ASPECT feature does not constrain any syntactic decision. Even if relevant, a feature may not necessarily contribute to optimal perfor-mance because it may be redundant with other features that surpass it in relevance. For example, as we will see, the DET and STATE features alone both help parsing because they help identify the idafa construction, but they are redundant with each other and the DET feature is more helpful because it also helps with adjectival modification of nouns. predictions out of all predictions) of course affects the value of a feature on unseen text.
Even if relevant and non-redundent, a feature may be hard to predict with sufficient accuracy by current technology, in which case it will be of little or no help for parsing, even if helpful when its gold values are provided. As we will see, the very relevant and not redundant, but it cannot be predicted with high accuracy and overall it is not useful.
 various tradeoffs among these three factors. In the past, it has been shown that if we can recognize the relevant morphological features in assignment configurations well enough, then they contribute to parsing accuracy. For example, modeling improves Czech parsing (Collins et al. 1999): CASE is relevant, not redundant, and can be predicted with sufficient accuracy. It has been more difficult showing that agreement morphology helps parsing, however, with negative results for dependency parsing in several languages (Eryigit, Nivre, and Oflazer 2008; Nivre, Boguslavsky, and Iomdin 2008; Nivre 2009).

Modern Standard Arabic (MSA). For MSA, the space of possible morphological features is fairly large. We determine which morphological features help and why. We further determine the upper bound for their contribution to parsing quality. Similar to previous 162 results, assignment features, specifically CASE , are very helpful in MSA, though only under gold conditions: Because CASE is rarely explicit in the typically undiacritized machine-predicted (real, non-gold) condition. In contrast with previous results, we show agreement features are quite helpful in both gold and predicted conditions. This is likely a result of MSA having a rich agreement system, covering both verb X  X ubject and noun X  X djective relations. The result holds for both the MaltParser (Nivre 2008) and the Easy-First Parser (Goldberg and Elhadad 2010).
 speech (POS) tagging has concentrated on the morphemic form of the words. Often, however, the functional morphology (which is relevant to agreement, and relates to the meaning of the word) is at odds with the  X  X urface X  (form-based) morphology; a well-known example of this are the  X  X roken X  (irregular) plurals of nominals. We show that by modeling the functional morphology rather than the form-based morphology, we obtain a further increase in parsing performance (again, both when using gold and when using predicted POS and morphological features). To our knowledge, this work is the first to use functional morphology features in MSA processing.
 dicted POS and morphological features, training on a combination of gold and pre-dicted POS and morphological feature values outperforms the alternative training scenarios.
 their representation in the annotated corpus we use, and variations of abstraction (Section 3), and describe our basic experiments in Section 4. We first explore the con-tribution of various POS tag sets, (form-based) morphological features, and promising combinations thereof, to Arabic dependency parsing quality X  X n straightforward fea-ture engineering design and combination heuristics. We also explore more sophisticated extended exploration of functional features. This includes using functional and GENDER feature values, instead of form-based values; using the non-form-based rationality ( RAT ) feature; and combinations thereof. We additionally consider the appli-cability of our results to a different parser (Section 6) and consider combining gold and predicted data for training (Section 7). Section 8 presents a result validation on unseen test data, as well as an analysis of parsing error types under different conditions. We conclude and provide a download link to our model in Section 9. Last, we include an appendix with further explorations of PERSON feature engineering,  X  X inning X  of Arabic number constructions according to their complex syntactic patterns, and embedding useful morphological features in the POS tag set. Much of Sections 2 X 5 was presented in two previous publications (Marton, Habash, and Rambow 2010, 2011). This article extends that previous work by: 5. providing an extended discussion and comparison of several notable and best 2. Experimental Data and Relevant Linguistic Concepts
In this section, we present the linguistic concepts relevant to our discussion of Arabic parsing, and the data we use for our experiments. We start with the central concept of the morpheme followed by the more abstract concepts of the lexeme and lexical and inflectional features. Throughout this section, we use the term feature in its linguistic sense, as opposed to its machine learning sense that we use in Section 4. Discussions of the challenges of form-based (morpheme-based) versus functional features on the one hand, and morpho-syntactic interactions on the other hand, follow. Finally, we present the annotated corpus we use, and the various POS tag sets, that are extracted from this corpus (in varying degrees of abstraction and lexicalization), and which we use in the rest of the article. 2.1 Morphemes
Words can be described in terms of their morphemes (atomic units bearing mean-ing); in Arabic, in addition to concatenative prefixes and suffixes, there are templatic (non-contiguous) morphemes called root and pattern . The root is typically a triplet additional consonants, and place-holders for the root radicals. The root conveys some base meaning, which patterns may modify in various ways. A combination of a root and a pattern is called a stem . More on root and pattern can be found in Section 2.2.
Arabic also includes a set of clitics that are tokenized in all Arabic treebanks, with the exception of the Arabic definite article, We consider the definite article a prefix, and its presence affects the value of the feature in models containing it (see Section 4.3). An example of morphological analysis to the level of morphemes is the word has one prefix and one suffix (which at a deeper level may be viewed together as one circumfix), in addition to a stem composed of the root the pattern 1A2i3 . 3 2.2 Lexeme, Lexical Features, and Inflectional Features
Arabic words can also be described in terms of lexemes and inflectional features. We define the lexeme as the set of word forms that only vary inflectionally among each other. A lemma is one of these word forms, used for representing the lexeme word set.
For example, Arabic verb lemmas are third-person masculine singular perfective. We explore using both a diacritized LEMMA feature, and an undiacritized lemma (hereafter
LMM ). Just as the lemma abstracts over inflectional morphology, the root abstracts over both inflectional and derivational morphology and thus provides a deeper level generally complementary abstraction, sometimes indicating semantic notions such as 164 causation and reflexiveness, among other things. We use the pattern of the lemma, not of the word form. We group the ROOT , PATTERN , LEMMA ,and as lexical features (see Section 4.4). Nominal lexemes can be further classified into two groups: denoting rational (i.e., human) entities, or irrational (i.e., non-human) entities.
The rationality (or RAT ) feature interacts with syntactic agreement and other inflectional features (discussed next); as such, we group it with those features in this article. with a lexeme. Words 4 vary along nine dimensions: GENDER nominals and verbs); ASPECT , VOICE ,and MOOD (for verbs); and state, idafa ), and the attached definite article proclitic features abstract away from the specifics of morpheme forms. Some inflectional features affect more than one morpheme in the same word. For example, changing the value of the ASPECT feature in the earlier example from imperfective to perfective yields the word form suffix, and pattern. 2.3 Form-Based vs. Functional Features
Some inflectional features, specifically gender and number, are expressed using dif-ferent morphemes in different words (even within the same POS). There are four sound gender-number suffixes in Arabic: 5 +  X  (null morpheme) for masculine singular, + + for feminine singular, + +wn for masculine plural, and plural. Form-based GENDER and NUMBER feature values are set only according to these four morphemes (and a few others, ignored for simplicity). There are exceptions and alternative ways to express GENDER and NUMBER , however, and functional feature values take them into account: Depending on the lexeme, plurality can be expressed using sound plural suffixes or using a pattern change together with singular suffixes.
A sound plural example is the word pair daughter/granddaughters.) On the other hand, the plural of the inflectionally and morphemically feminine singular word madAris+  X  ( X  X chools X ), which is feminine and plural inflectionally, but has a masculine singular suffix. This irregular inflection, known as broken plural , is similar to the English mouse/mice , but is much more common in Arabic (over 50% of plurals in our training data). A similar inconsistency appears in feminine nominals that are not inflected using sound gender suffixes, for example, the feminine form of the masculine singu-lar adjective inconsistency in the correspondence between inflectional features and morphemes, and inspired by Smr X  (2007), we distinguish between two types of inflectional features: form-based (a.k.a. surface, or illusory) features and functional features. based ( X  X urface X ) inflectional features, and do not mark rationality; this includes the
Penn Arabic Treebank (PATB) (Maamouri et al. 2004), the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005; Habash,
Rambow, and Roth 2012). The Elixir-FM analyzer (Smr X  2007) readily provides the functional inflectional number feature, but not full functional gender (only for adjectives and verbs but not for nouns), nor rationality. In this article, we use an in-house system which provides functional gender, number, and rationality features (Alkuhlani and
Habash 2012). See Section 5.2 for more details. 2.4 Morpho-Syntactic Interactions
Inflectional features and rationality interact with syntax in two ways. In agreement relations , two words in a specific syntactic configuration have coordinated values for specific sets of features. MSA has standard (i.e., matching value) agreement for subject X  verb pairs on PERSON , GENDER ,and NUMBER , and for noun X  X djective pairs on
GENDER , CASE ,and DET . There are, however, three very common cases of exceptional agreement: Verbs preceding subjects are always singular, adjectives of irrational plural nouns are always feminine singular, and verbs whose subjects are irrational plural are also always feminine singular. See the example in Figure 1: the adjective,
Al X kyAt ( X  X mart X ), of the feminine plural (and rational) ters X ) is feminine plural; but the adjective, of the feminine plural (and irrational)
This exceptional agreement is orthogonal to the form-function inconsistency discussed earlier. In other words, having a sound or broken plural has no bearing on whether the noun is rational or not X  X nd hence whether an adjectival modifier should agree with it by being feminine-singular or -plural. Note also that all agreement rules, including the exceptional agreement rules, refer to functional number and gender, not to form-based number and gender.
 166 of dependents have different CASE , for example, verbal subjects are always marked NOMINATIVE (for a discussion of case in MSA, see Habash et al. [2007]). marker on nouns; when a noun heads an idafa construction, its C ASE and STATE are rarely explicitly manifested in undiacritized MSA. The D plays an important role in distinguishing between N-N construct ( idafa ), in which only the last noun bears the definite article, 7 and N-A (noun-adjectival modifier), in which both elements generally exhibit agreement in definiteness (and agreement in other features, too). Although only N-N may be followed by additional N elements in Idafa relation, both constructions may be followed by one or more adjectival modifiers.
Instead, bilexical dependencies are used to model semantic relations that often are the only way to disambiguate among different possible syntactic structures. 2.5 Corpus, CATiB Format, and the CATIB 6POSTagSet
We use the Columbia Arabic Treebank (CATiB) (Habash and Roth 2009). Specifically, we use the portion converted from Part 3 of the PATB to the CATiB format, which en-riches the CATiB dependency trees with full PATB morphological information. CATiB X  X  dependency representation is based on traditional Arabic grammar and emphasizes syntactic case relations. It has a reduced POS tag set consisting of six tags only (hence-forth CATIB 6). The tags are: NOM (non-proper nominals including nouns, pronouns, adjectives, and adverbs), PROP (proper nouns), VRB (active-voice verbs), VRB-PASS (passive-voice verbs), PRT (particles such as prepositions or conjunctions), and PNX (punctuation). CATiB uses a standard set of eight dependency relations: SBJ and OBJ for subject and (direct or indirect) object, respectively (whether they appear pre-or post-verbally); IDF for the idafa (possessive) relation; MOD for most other modifications; and other less common relations that we will not discuss here. For other PATB-based POS tag sets, see Sections 2.6 and 2.7.
 categories of orthographic clitics, but not the definite article + the experiments reported in this article, we use the gold segmentation. Tokenization in-volves further decisions on the segmented token forms, such as spelling normalization, which we only briefly touch on here (in Section 4.1). An example CATiB dependency tree is shown in Figure 1. For the corpus statistics, see Table 1. For more information on
CATiB, see Habash and Roth (2009) and Habash, Faraj, and Roth (2009). 2.6 Core POS Tag Sets
Linguistically, words have associated POS tags, e.g.,  X  X erb X  or  X  X oun, X  which further abstract over morphologically and syntactically similar lexemes. Traditional Arabic grammars often describe a very general three-way distinction into verbs, nominals, and particles. In comparison, the tag set of the Buckwalter Morphological Analyzer (Buckwalter 2004) used in the PATB has a core POS set of 44 tags ( phological extension. 8 Cross-linguistically, a core set containing around 12 tags is often assumed as a  X  X niversal tag set X  (Rambow et al. 2006; Petrov, Das, and McDonald 2012). We have adapted the list from Rambow et al. (2006) for Arabic, and call it here contains the following tags: verb (V), noun (N), adjective (AJ), adverb (AV), proper noun (PN), pronoun (PRO), relative pronoun (REL), preposition (P), conjunction (C), particle (PRT), abbreviation (AB), and punctuation (PNX). The CATIB a further reduction, with the exception that CATIB 6 contains a passive voice tag (a mor-phological feature); this tag constitutes only 0.5% of the tags in the training, however. 2.7 Extended POS Tag Sets
The notion of  X  X OS tag set X  in natural language processing usually does not refer to not only the core POS, but also the complete set of morphological features (this tag set is still fairly small since English is morphologically impoverished). In PATB-tokenized
MSA, the corresponding type of tag set (core POS extended with a complete description of morphology) would contain upwards of 2,000 tags, many of which are extremely rare (in our training corpus of about 300,000 words, we encounter only 430 POS tags with complete morphology). Therefore, researchers have proposed tag sets for MSA useful size computationally. These tag sets are hybrids in the sense that they are neither simply the core POS, nor the complete morphologically enriched tag set, but instead they selectively enrich the core POS tag set with only certain morphological features. Habash (2010).
 sets CORE 44 and the newly introduced CORE 12; (b) CATiB Treebank tag set ( (Habash and Roth 2009) and its newly introduced extension of simple regular expressions on word form, indicating particular morphemes such as the prefix on predicted values as reported in Section 4; (c) the PATB full tag set with complete morphological tag ( BW ) (Buckwalter 2004); and two extensions of the PATB reduced tag set (P ENN POS, a.k.a. RTS, size 24 [Diab, Hacioglu, and Jurafsky 2004]), both outperforming it: (d) Kulick, Gabbard, and Marcus (2006) X  X  tag set (K one of whose most important extensions is the marking of the definite article clitic, and (e) Diab and Benajiba X  X  (in preparation) E XTENDED RTS tag set ( gender, number, and definiteness, size 134. 3. Related Work
Much work has been done on the use of morphological features for parsing of morpho-logically rich languages. Collins et al. (1999) report that an optimal tag set for parsing
Czech consists of a basic POS tag plus a CASE feature (when applicable). This tag set (size 58) outperforms the basic Czech POS tag set (size 13) and the complete tag set did not yield any improvements. The results for Czech are the opposite of our results for Arabic, as we will see. This may be due to CASE rate in Czech (5.0%) (Haji X  c and Vidov X -Hladk X  1998) compared with Arabic (
Spanish morphological features (number for adjectives, determiners, nouns, pronouns, and verbs; and mode for verbs) outperforms other combinations. Our approach is 168 comparable to their work in terms of its systematic exploration of the space of mor-that extending POS and phrase structure tags with definiteness information helps unlexicalized PCFG parsing.

Gabbard, and Marcus 2006; Diab 2007; Green and Manning 2010), the Prague Depen-dency Treebank (PADT) (Buchholz and Marsi 2006; Nivre 2008) and the CATiB (Habash and Roth 2009). Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses. Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al. 2007), trained on the
PADT. His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.
 definite article is helpful for parsing. We go beyond previous work, however, and explore additional lexical and inflectional features. Previous work with MaltParser in
Russian, Turkish, and Hindi showed gains with CASE but not with agreement features (Eryigit, Nivre, and Oflazer 2008; Nivre, Boguslavsky, and Iomdin 2008; Nivre 2009).
Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task. Furthermore, we demonstrate that our results carry over successfully to another parser, the Easy-First Parser (Goldberg and Elhadad 2010) (Section 6).
 21 languages using features that encode whether the values for certain attributes are because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs. 9 We expect this kind of feature to yield lower gains for Arabic, unless: 4. Basic Parsing Experiments
We examined a large space of settings. In all our experiments, we contrasted the results obtained using machine-predicted input with the results obtained using gold input (the (including POS tag sets) and their prediction accuracy. We then explored various feature combinations in a hill-climbing fashion. We examined these issues in the following order: 1. the contribution of POS tag sets to the parsing quality, as a function of the amount 2. the contribution of numerous inflectional features in a controlled fashion, using (c) 3. the contribution of the lexical features in a similar fashion, again using (f) gold input 4. (i) certain feature combinations and (j) the embedding of the best combination in the 5. (k) further feature engineering of select useful features.
 of our findings across these frameworks. In Section 7 we explore alternative training methods, and their impact on key models.
 parent word and the type of dependency relation to it, abbreviated as L used for greedy (hill-climbing) decisions for feature combination. Unlabeled attachment accuracy score (U AS ) and label accuracy (dependency relation regardless of parent, L are also given. For statistical significance, we use McNemar X  X  test on non-gold L implemented by Nilsson and Nivre (2008). We denote p &lt; 0 . 05 and p &lt; 0 . 01 with , respectively. 4.1 Data Sets and Parser For all the experiments reported in this article, we used the training portion of PATB
Part 3 v3.1 (Maamouri et al. 2004), converted to the CATiB Treebank format, as men-tioned in Section 2.5. We used the same training / devtest split as in Zitouni, Sorensen, opment (dev) set and a blind test set. For all experiments, unless specified otherwise, we used the dev set. 10 We kept the test unseen ( X  X lind X ) during training and model development. Statistics about this split (after conversion to the CATiB dependency format) are given in Table 1.
 parser MaltParser v1.3 (Nivre 2003, 2008; K X bler, McDonald, and Nivre 2009), a transition-based parser with an input buffer and a stack, which uses SVM classifiers 170 to predict the next state in the parse derivation. All experiments were done using the Nivre  X  X ager X  algorithm. 11 text: word ID (ordinal position in the sentence), word-form, POS tag, head (parent word ID), and deprel (the dependency relation between the current word and its parent).
There are default MaltParser features (in the machine learning sense), classifiers. The most commonly used feature functions are the top of the input buffer (next word to process, denoted buf[0]), or top of the stack (denoted stk[0]); following items on buffer or stack are also accessible (buf[1], buf[2], stk[1], etc.). Hence MaltParser
McDonald, and Nivre (2009) describe a  X  X ypical X  MaltParser model configuration of attributes and features. 13 Starting with it, in a series of initial controlled experiments,
POS tags. For features of new MaltParser-attributes (discussed later), we used buf[0] + dependents of the specified argument). This new MaltParser configuration resulted in gains of 0.3 X 1.1% in labeled attachment accuracy (depending on the POS tag set) over the default MaltParser configuration. We also experimented with using normalized word-forms ( Alif Maqsura conversion to Ya , and Hamza removal from each Alif )asis small decrease in performance, so we settled on using non-normalized word-forms. All experiments reported here were conducted using this new configuration. To recap, it has the following MaltParser attributes (machine learning features): 4 word-form attributes, 7 POS tag attributes, and 5 deprel attributes (some of which are not useful for the Nivre  X  X ager X  algorithm), totaling 16 attributes and two more for every new feature described in Section 4.3 and on (e.g., CASE ). 4.2 The Effect of POS Tag Richness on Parsing Quality
In this section, we compare the effect on parsing quality of a number of POS tag sets varying in their richness, in both gold and predicted settings.

Gold POS tag values. We turn first to the contribution of POS information to parsing quality, as a function of the amount of information encoded in the POS tag set (i.e., the relevance of a tag set). A first rough estimation for the amount of information is the actual tag set size, as it appears in the training data. For this purpose we compared the POS tag sets discussed in sections 2.6 and 2.7. In optimal conditions (using gold POS tags), the richest tag set ( BW ) is indeed the best performer (84.0%), and the poorest ( the worst (81.0%). Mid-size tag sets are in the high (82%), with the notable exception of
KULICK , which does better than ERTS , in spite of having one fourth the tag set size; more-over, it is the best performer in unlabeled attachment accuracy (86.0%), in spite of being less than tenth the size of BW . Our extended mid-size tag set, performer as expected. Columns 2 X 4 in Table 2 show results with gold input, and the rightmost column shows the number of tag types actually occurring in the training data.
Predicted POS tag values. So far we discussed optimal (gold) conditions. But in prac-tice, POS tags are annotated by automatic taggers, so parsers get predicted POS tags as input, as opposed to gold (human-annotated) tags. 14 The more informative the tag set, the less accurate the tag prediction might be, so the effect on overall parsing quality is unclear. Put differently, we are interested in the tradeoff between relevance and accu-racy . Therefore, we repeated the experiments with POS tags predicted by the MADA toolkit (Habash and Rambow 2005; Habash, Rambow, and Roth 2012) 172 columns 5 X 7). It turned out that BW , the best gold performer but with lowest POS pre-diction accuracy (81.8%), suffered the biggest drop (11.4%) and was the worst performer with predicted tags. The simplest tag set, CATIB 6, and its extension, from the highest POS prediction accuracy (97.7%), and their performance suffered the least. C ATIB E X was the best performer with predicted POS tags. Performance drop and POS prediction accuracy are given in columns 8 and 9.
 if not more important, than its relevance. In other words, when designing a parsing model, one might want to consider that in the tradeoff, mediocre accuracy may be worse than mediocre relevance. Later we see a similar trend for other features as well (e.g.,
CASE in Section 4.3). In Section 7 we also present a training method that largely mitigates (but doesn X  X  resolve) this issue of mediocre accuracy of relevant features. 4.3 Inflectional Features and Their Contribution to Parsing Quality
Experimenting with inflectional features is especially important in Arabic parsing, lexical information in a controlled manner, we focused on the best performing core ( X  X orphology-free X ) POS tag set, CORE 12, as baseline; using three different set-ups, we added nine inflectional features (with either gold values, or with values predicted by MADA): DET (presence of determiner), PERSON , ASPECT ,
NUMBER , STATE ,and CASE . For a brief reminder and examples for each feature, see the rightmost column in Table 3, or for more details refer back to Section 2.
 additional MaltParser attributes); in set-up Sep , we augmented the baseline model with each of these features, one at a time, separately; and in set-up Greedy , we combined them in a greedy heuristic (since the entire feature space is too vast to exhaust): starting with the most gainful feature from Sep , adding the next most gainful feature, keeping it if it helped, or discarding it otherwise, and repeating this heuristics through the least gainful feature. See Table 4.

Gold feature values. We applied the three setups ( All, Sep ,and Greedy ) with gold POS tags and gold morphological tags, to examine the contribution of the morphological features in optimal conditions. The top left section of Table 4 shows that applying all inflectional features together yields gains over the baseline. Examining the contribution of each feature separately (second top left Sep section), we see that STATE and DET , were the top contributors. Performance of from the predicted conditions (see following discussion). No single feature outper-formed the All set-up in gold. Surprisingly, only CASE and set-up (85.4%, our highest result in gold), although one might expect feature helped, too (since it is highly relevant: It participates in agreement, and interacts with the idafa construction). This shows that there is redundancy in the information provided by DET on the one hand and CASE and STATE on the other, presumably because both sets of feature help identify the same construction, idafa .

Predicted feature values. We re-applied the three set-ups with predicted feature values (right-hand side half of Table 4). Set-up All hurts performance on the machine-predicted input. This can be explained if one examines the prediction accuracy of each feature (top half, third section of Table 3). Features which are not predicted with very high accuracy, such as CASE (86.3%), can dominate the negative contribution, even though they are paragraph). The determiner feature ( DET ), followed by the individual contributors in set-up Sep . Adding the features that participate in agreement, namely, DET and the PNG features ( PERSON , NUMBER , GENDER yielded a 1.4% gain over the CORE 12 baseline. These results suggest that for a successful feature combination, one should take into account not only the relevance of the features, but also their accuracy. 174 4.4 Lexical Features and Their Contribution to Parsing Quality
Next, we experimented with adding the lexical features, which involve semantic ab-straction to some degree: the diacritized LEMMA , the undiacritized lemma (
ROOT ,andthe PATTERN (which is the pattern of the LEMMA semantically related words. We experimented with the same set-ups as above: All, Sep , and Greedy .

Gold feature values. The left-hand side half of Table 5 shows that adding all four however, no proper subset of the lexical features beats the set of all lexical features.
Predicted feature values. The right-hand side of Table 5 shows that adding all four features yielded a minor gain in set-up All . LMM was the best single contributor, closely followed by ROOT in Sep . CORE 12+ LMM + ROOT (with or without greedy combination in set-up Greedy , and also provides the best performance of all experiments with lexical features only. Due to the high redundancy of (only 0.01% absolute gain when adding LEMMA in the Greedy set-up, which appears larger only due to rounding in the table), we do not consider tions from this point on. Note, however, that LEMMA  X  X nd all the lexical features X  X re predicted with high accuracy (top half, second section of Table 3). All
Sep
Greedy 4.5 Inflectional and Lexical Feature Combination and Its Contribution to Parsing Quality
We now combine morphological and lexical features. Following the same greedy heuristic as in the previous sections, we augmented the best inflection-based model CORE 12+ DET + PNG with lexical features, and found that the undiacritized lemma ( improved performance on predicted input (80.2%) (see Table 6). Adding more lexical features does not help, however, suggesting that some of the information in the lexical features is redundant with the information in the morphological features. See the Ap-pendix, Section A.1, for our attempt to extend the tag set by embedding the best feature combination in it. 4.6 Additional Feature Engineering
So far we have experimented with morphological feature values as extracted from the PATB (gold) or predicted by MADA; we also used the same MaltParser feature configuration for all added features (i.e., stk[0] + buf[0]). It is likely, however, that from a machine-learning perspective, representing similar categories with the same tag, or 176 taking into account further-away tokens in the sentence, may be useful for learning.
Therefore, we next experimented with modifying some inflectional features that proved most useful in predicted input.
 see Section 2), we attempted modeling the DET values of previous and next elements (as MaltParser X  X  stk[1] + buf[1], in addition to the modeled stk[0] + buf[0]). This vari-ant, denoted DET 2, indeed helps: When added to the improves non-gold parsing quality by more than 0.3%, compared to
Table 7. This variant yields a small improvement also when used in combination with the PNG and LMM features, as shown in the second part of Table 7 X  X ut only in gold. determiner feature, and its redundancy with the PNG features (and note that all fea-tures involved are predicted with high accuracy). A possible explanation might be that form-based feature representation is inherently inadequate here, and therefore its high accuracy may not be very indicative. We explore non-form-based (functional) feature
Section A.2. 5. Parsing Experiments with Functional Features
Section 4 explored the contribution of various POS tag sets, (form-based) morphological features, and promising combinations thereof, to Arabic dependency parsing quality X  in straightforward feature engineering design and combination heuristics. This section explores more sophisticated feature engineering: using functional feature values, instead of form-based values; using the non-form-based rationality ( feature; and combinations thereof. For additional experiments regarding alternative representation for digit tokens, and the  X  X inning X  Arabic number constructions accord-ing to their complex syntactic patterns, see the Appendix, Section A.3. 5.1 Functional Feature Representation for Broken Plurals (using ElixirFM)
The NUMBER feature we have thus far extracted from PATB with MADA only reflects form-based (as opposed to functional ) values, namely, broken plurals are marked as singular. This might have a negative effect for learning generalizations over the complex agreement patterns in MSA, beyond memorization of word pairs seen together in training. To address this issue, one can use the Arabic morphological tool ElixirFM containing a lemma and a functional NUMBER (and other features). We replaced the surface NUMBER value for all nominals marked as singular in our data with ElixirFM X  X  functional value, using the MADA-predicted lemma to disambiguate multiple ElixirFM analyses. These experiments are denoted with F N N UM . In training, of the lemma types sent to ElixirFM for analysis, about 20% received no analysis (OOV). A manual observa-tion of a small sample revealed that at least half of those were proper names (and hence their NUMBER value would have stayed singular). Almost 9% of the ElixirFM-analyzed types (over 7% of the tokens) changed their NUMBER value. In the dev set, the OOV rate was less than 9%, and almost 11% of the ElixirFM-analyzed types changed their NUMBER value. This amounts to 4.4% of all tokens.
 feature. We used this feature in our best model so far, CORE of the form-based NUMBER feature. 17 The ElixirFM-based models yielded small gains of up to 0.1% over this best model on predicted input. We then modified the ElixirFM-based best model to use the enhanced DET 2 feature. This variation yielded a similarly small gain, altogether less than 0.2% from its ElixirFM-free counterparts. 5.2 Functional Gender and Number Features, and the Rationality Feature The ElixirFM lexical resource used previously provided functional values but no functional GENDER values, nor RAT (rationality, or humanness) values.
To address this issue, we use a version of the PATB3 training and dev sets manually annotated with functional gender, number, and rationality (Alkuhlani and Habash 2011). 18 This is the first resource providing all three features (ElixirFm only provides functional number, and to some extent functional gender). We conducted experiments tures, obtained from training a simple maximum likelihood estimation classifier on this resource (Alkuhlani and Habash 2012). 19 The first part of Table 8 shows that the (rationality) feature is very relevant (in gold), but suffers from low accuracy (no gains in machine-predicted input). The next two parts show the advantages of functional gender and number (denoted with a F N * prefix) over their surface-based counterparts.
The fourth part of the table shows the combination of these functional features with the other features that participated in the best combination so far (
DET 2, and PERSON ); without RAT , this combination is at least as useful as its form-based counterpart, in both gold and predicted input; adding RAT 0.4% (absolute) gain in gold, offering further support to the relevance of the rationality feature, but a slight decrease in predicted input, presumably due to insufficient accuracy again. The last part of the table revalidates the gains achieved with the best controlled feature combination, using CATIB E X  X  X he best performing tag set with predicted in-put. Note, however, that the 1% (absolute) advantage of CATIB features) over the morphology-free CORE 12 on machine-predicted input (Table 2) has 178 shrunk with these functional feature combinations to 0.3%. We take it as further support to the relevance of our functional morphology features, and their partial redundancy with the form-based morphological information embedded in the 6. Evaluation of Results with Easy-First Parser
In this section, we validate the contribution of key tag sets and morphological features X  and combinations thereof X  X sing a different parser: the Easy-First Parser (Goldberg and
Elhadad 2010). As in Section 4, all models are evaluated on both gold and non-gold (machine-predicted) feature values.
 however, it does not attempt to attach arcs  X  X agerly X  as early as possible (as in previous sections), or at the latest possible stage (an option we abandoned early on in preliminary experiments). Instead, the Easy-First Parser keeps a stack of partially built treelets, and attaches them to one another in order of confidence (from high confidence,  X  X asy X  in a second pass, with a separate training step, after all attachments have been decided (the code for which was added after the publication of Goldberg and Elhadad (2010), which only included an unlabeled attachment version).
 as for MaltParser, but it gives the feature designer greater flexibility. For example, the
POS tag can be dynamically split (or not) according to the token X  X  word-form and/or the already-built attachment treelets, whereas in MaltParser, one can meld several
The Easy-First Parser X  X  first version comes with the code for the features used in its first publication. These include POS tag splitting and feature melding for prepositional attachment chains (e.g., parent-preposition-child). For greater control of the contribu-a better  X  X pples-to-apples X  comparison with MaltParser (as used here), we disabled these features, and instead used features (and selected feature melding) that were as equivalent to MaltParser as possible.
 consistently higher than the corresponding results with MaltParser, with similar trends for the various features X  contribution: Functional GENDER tribute more than their form-based counterparts, in both gold and predicted conditions; rationality ( RAT ) as a single feature on top of the POS tag set helps in gold (and with Easy-First Parser, also in predicted conditions) X  X ut when used in combination with
PERSON , LMM , functional GENDER ,and NUMBER , it actually slightly lowers parsing scores in predicted conditions (but with Easy-First Parser, it helps in gold conditions);
DET is the most useful single feature in predicted conditions (from those we tried here); and the best performing model in predicted conditions is the same as with MaltParser:
CORE 12 hold also for C ATIB E X , the best performing tag set on predicted input. Inter-estingly, with this parser, the greater 1.6% (absolute) advantage of additional features) over the morphology-free CORE 12 on machine-predicted input (compare with only 1% in MaltParser in Table 2) has shrunk completely with these functional feature combinations. This suggests that Easy-First Parser is more resilient to accuracy errors (presumably due to its design to make less ambiguous decisions earlier), and hence can take better advantage of the relevant information encoded in our functional morphology features. 7. Combined Gold and Predicted Features for Training
So far, we have only evaluated models trained on gold POS tag set and morphological feature values. Some researchers, however, including Goldberg and Elhadad (2010), train on predicted feature values instead. It makes sense that training on predicted because good predictions of feature values are reinforced (since they repeat the gold patterns that do not repeat the gold). 21 To test our hypothesis, we start this section by comparing three variations: 180 values yields better scores when evaluated on gold, too (although later we see this is not always the case). More interestingly, when evaluated on predicted feature values, training on predicted feature values yields better parsing scores than when training on gold, and training on g+p yields best scores, in support of our hypothesis. Therefore, in the rest of the table (and in the rest of the experiments), we apply the g+p training variant to the best models so far, both in MaltParser and Easy-First Parser. The next part in Table 10 shows that this trend is consistent also with the best feature combinations so far. Interestingly, the RAT feature contributes to improvement only in the g+p condition, presumably because of its low prediction accuracy.
 and BW (best performers on predicted and gold input, respectively). We can see in  X  X orphology-free X  CORE 12 (in Table 10) outperforms C ATIB ing CORE 12+ DET 2+ LMM + PERSON +F N * NGR our best MaltParser model on predicted feature values. Similarly, the Easy-First Parser model F N * NG outperforms its CATIB E X counterpart ( CATIB E X + resulting in our best model on the dev set in machine-predicted condition (82.7%). performer on predicted input, had the most dramatic gains from using g+p: more than 5% (absolute) for L AS on predicted input with MaltParser (and over 3% with Easy-
First Parser). Although much improved, BW models X  performance still lags behind the leading models.
 the alternatives (independently of parser choice) due to making the parser more resilient to lower accuracy in the input. It also suggests that g+p training enables the parser to better exploit relevant data when represented in  X  X leaner X  separate features, as opposed to when the POS tags are split into ambiguous form-based cases as in C experimentation is needed in order to test this latter conjecture. 8. Result Validation and Discussion 8.1 Validating Results on an Unseen Test Set
Once experiments on the development set were done, we ran the best performing form-based non-gold-based models from Section 4 on a previously unseen test set. This set 182 is the test split of part 3 of the PATB (hereafter PATB3-TEST; see Table 1 for details).
Table 12 shows that the same trends held on this set too, with even greater relative gains, up to almost 2% absolute gains.
 Sections 5 X 7 on PATB3-TEST. Here, too, the same trends held. Results are shown in
Table 13. 8.2 Best Results on Length-Filtered Input
For better comparison with work of others, we adopt the suggestion made by Green and Manning (2010) to evaluate the parsing quality on sentences up to 70 tokens long.
We report these filtered results in Table 14. Filtered results are consistently higher (as expected). Results are about 0.9% absolute higher on the development set, and about 0.6% higher on the test set. The contribution of the RAT feature across sets is negligible about 0.15% gain on the test set. For clarity and conciseness, we only show the best model (with RAT ) in Table 14. 8.3 Error Analysis
We perform two types of error analyses. First, we analyze the attachment accuracy by attachment relation type on PATB3-DEV. Our hypothesis is that the syntactic re-lations which are involved in agreement or assignment configurations will show an improvement when the relevant morphological features are used, but other syntactic 184 relations will not. Second, we analyze the grammaticality of the obtained parse trees with respect to agreement and assignment phenomena. Here, our hypothesis is that when using morphological features, the grammaticality of the obtained parse trees will increase.
 Attachment accuracy by relation type. Our first hypothesis is illustrated in Figure 2. On the left, we see the parse provided by our baseline system (MaltParser using only CORE 12), which has two errors: hand side of Figure 2) if functional morphological features were available to the parser, including the rationality feature, and if the parser could learn the agreement rule for non-rational subjects, as well as the requirement that the head of an idafa construction cannot have a definite article.
 detail: from adding morphological features to a  X  X orphology-free X  baseline, which in all cases we take to be the MaltParser trained on the gold machine-predicted input (except for Table 17, where the Easy-First Parser is trained and evaluated instead).
 training. The accuracy by relation type is shown in Table 15. Using just see that some attachments (subject, modifications) are harder than others (objects, features designed to improve idafa and those relations subject to agreement, subject, and nominal modification ( DET 2, PERSON , NUMBER , GENDER subject, nominal modification, and idafa reduce error by substantial margins (error reduction over CORE 12 is greater than 10%; in the case of idafa it is 21.8%), and all other relations (including object and prepositional attachment) improve to a lesser degree (error reduction of 7.1% or less). We assume that the non-agreement relations (object 186 and prepositional attachment) improve because of the overall improvement in the parse due to the improvements in the other relations.
 we see a further reduction in the agreement-related attachments, namely, subject and nominal modification (error reductions over baseline of 13.7% and 12.9%, respectively).
Idafa decreases slightly (because this relation is not affected by the functional features), whereas object stays the same. Surprisingly, prepositional attachment also improves, with an error reduction of 8.1%. Again, we can only explain this by proposing that the improvement in nominal modification attachment has the indirect effect of ruling out some bad prepositional attachments as well.
 relations affected by agreement or assignment perform worse than without the ratio-nality feature. In contrast, all other relations improve. The decrease in performance can be explained by the fact that the rationality ( RAT accuracy; because it interacts directly with agreement, and because we are training on gold annotation, the models trained do not correspond to the seen data. We expect (We have no explanation for the improvement in the other relations.)
POS and morphological feature values (g+p; Section 7). The accuracy by relation is trained only using gold CORE 12 features. First, we see that using the same single feature, but training on gold and predicted tags, we obtain an across-the-board improvement, with error reductions between 3.6% and 9.3%, with no apparent patterns. (Prepositional modifications even show a slight decline in attachment accuracy). This row (using only
CORE 12 and training on gold and predicted) now becomes our baseline for subsequent discussion of error reduction. If we then add the form-based features, we again find that the error rate decrease for subject, nominal modification, and idafa (the relations affected by agreement and assignment) is greater than that for the other relations; with this train-ing corpus, however, the separation is not as stark, with subject decreasing its error rate by 9.6% and prepositional modification by 8.7%. Notably, idafa shows the greatest error rate reduction we have seen so far: 30.2%. When we turn to functional features, we again see a further increase in performance across the board. And, as expected, the penalty for using the rationality feature disappears because we have trained on predicted features affected by agreement and assignment, with subject reducing error by 13.4% now, nominal modification by 14.7%, and idafa by 34.0%. The tree on the right in Figure 2 is the parse tree returned by this model, and both the subject and the idafa relation are correctly analyzed. Note that the increase in the accuracy of idafa is probably not related to the interaction of syntax and morphology in assignment, because assignment in the idafa construction is not affected by rationality. Instead, we suspect that the parser can exploit the very different profile of the rationality feature in the dependent node of the idafa and modification constructions. Looking just at nominals, we see in the gold corpus that 62% of the dependents in a modification relation have no inherent rationality (this is the case notably for adjectives), whereas this number for idafa is only 18%. In contrast, the dependent of an idafa is irrational 66% of the time, whereas for modification that number is only 16%.
 relation is shown in Table 17. When we switch from MaltParser to Easy-First Parser, we get an overall error reduction of 4.2%, which is reflected fairly evenly among the relations, with two outliers: subjects improve by 9.0%, whereas idafa increases its error is usually considered an  X  X asy X  relation (no word can intervene between the linked words), as reflected in the high accuracy numbers for this relation. Furthermore, when we inspect the unlabeled accuracy scores (not shown here), we see that the unlabeled attachment score for idafa also decreases. Thus, we must reject a plausible hypothesis, namely, that the parser gets the relations right but the labeler (which in the Easy-First
Easy-First Parser on gold and predicted, we see a similar improvement pattern over just training on gold as we did with the MaltParser; one exception is that the idafa relation improves greatly again. Finally, we add the functional morphological features (training 188 on gold and predicted). Again, the pattern we observe (by comparing error reduction against using Easy-First Parser trained only using CORE 12 on gold and predicted) are very similar to the pattern we observed with the MaltParser in the same conditions.
One difference stands out, however: whereas the MaltParser can exploit the rationality feature when trained on gold and predicted, the Easy-First Parser cannot. Object and prepositional modification perform identically with or without rationality, but subject and idafa perform worse; only nominal modification performs better (with overall per-formance decreasing). If we inspect the unlabeled attachment scores for subjects, we do detect an increase in accuracy (from 85.0% to 85.4%); perhaps the parser can exploit the rationality feature, but the labeler cannot.

Grammaticality of parse trees. We now turn to our second type of error analysis, the evaluation of the grammaticality of the parse trees in terms of gender and number agreement patterns. We use the agreement checker code developed by Alkuhlani and Habash (2011) and evaluate our baseline (MaltParser using only ing model (Easy-First Parser using CORE 12 + DET + LMM + PERSON the gold reference. The agreement checker verifies, for all verb X  X ominal subject relations and noun X  X djective relations found in the tree, whether the agreement conditions are met or not. The accuracy number reflects the percentage of such relations found which gold syntax. For all three trees, however, we used gold morphological features for because we want to see to what extent the predicted morphological features help find the correct syntactic relations, not whether the predicted trees are intrinsically coherent given possibly false predicted morphology. The results can be found in Table 18. We note that the grammaticality of the gold corpus is not 100%; this is approximately equally
Nominal modification has a smaller error band between baseline and gold compared with subject X  X erb agreement. We assume this is because subject X  X erb agreement is more complex (it depends on their relative order), and because nominal modification can have multiple structural targets, only one of which is correct, although all, however, are plausible from the point of view of agreement. The error reduction relative to the gold topline is 62% and 76% for nominal agreement and verb agreement, respectively.
Thus, we see that our second hypothesis X  X hat the use of morphological features will reduce grammaticality errors in the resulting parse trees with respect to agreement phenomena X  X s borne out.
 ical features in particular) improve parsing, but they improve parsing in the way that we expect: (a) those relations affected by agreement and assignment contribute more than those that are not, and (b) agreement errors in the resulting parse trees are reduced. 9. Conclusions and Future Work
We explored the contribution of different morphological features (both inflectional and lexical) to dependency parsing of Arabic. Starting with form-based morphological features, we find that definiteness ( DET ), PERSON , NUMBER lemma ( LMM ) are most helpful for Arabic dependency parsing on predicted (non-gold) input. We further find that functional gender, number, and rationality features (F
N * GENDER ,F N * NUMBER , RAT ) improve over form-based-only morphological fea-tures, as expected when considering the complex agreement rules of Arabic. To our knowledge, this is the first result in Arabic NLP using functional morphological fea-tures, and showing an improvement over form-based features.
 1. We observe a tradeoff among the three factors (relevance, redundancy, and ac-2. Lexical features do help parsing, and the most helpful in predicted condition is 3. GENDER and NUMBER and their functional variants are the most useful for parsing 4. When evaluating in the machine-predicted input condition, training on data with 5. All of these results carry over successfully to another parser (Easy-First Parser), 190 the evaluation framework we presented and many of our conclusions will carry over to other languages (particularly, Semitic and morphology-rich languages) and syntactic representations (e.g., phrase structure). Some of our conclusions are more language independent (e.g., those involving the use of predicted training conditions). features X  X specially RAT  X  X n order to improve dependency parsing accuracy in pre-dicted condition. We also intend to investigate how these features can be integrated into other parsing frameworks; we expect them to help independently of the framework. The ability to represent the relevant morphological information in a manner that is useful to attachment decisions is, of course, crucial to improving parsing quality.
 Acknowledgments References 192 A. Appendix: Additional Feature Engineering
The following sections describe additional experiments, with negative or small gains, presented here for completeness.
 A.1 Embedding Morphological Features Within the POS Tags
After discovering our best form-based feature combination, we explored whether mor-phological data should be added to an Arabic parsing model as stand-alone machine learning features, or whether they should be used to enhance and extend a POS tag set.
We created a new POS tag set, CORE 12E X , size 81 (and 96.0% prediction accuracy), by extending the CORE 12 tag set with the features that most improved the
DET and the PNG -features. But CORE 12E X did worse than its non-extended (but feature-enhanced) counterpart, CORE 12+ DET + PNG . Another variant, which used both the extended tag set and the additional not improve over CORE 12+ DET + PNG either.
 A.2 Extended PERSON Feature
After extending the determiner feature ( DET 2), the next gainful feature that we could alter was P ERSON . We changed the values of proper names from  X  X /A X  to  X 3 X  (third-person). But this change resulted in a slight decrease in performance, so it was abandoned.
 A.3 Digit Tokens and Number Binning
Digit tokens (e.g., 4, as opposed to four) are marked singular by default. They don X  X  show surface agreement with a noun, even though the corresponding number-word token would. Therefore we replaced the digit tokens X  denoted these experiments with N UM D GT . 24 bers (Dada 2007). Therefore, we alternatively experimented with binning the digit tokens X  NUMBER value accordingly: digit tokens in the training set, and 1.2% in the dev set.
 value conditions (results not shown), so it was abandoned as well.
