 Textual entailment is defined as a relation between texts where a human reading text t 1 would infer that another text t 2 is most likely true [Bar-Haim et al. 2006]. Textual entailment recognition is considered as a fundamental NLP task to determine seman-tic equivalence among texts and has been shown to contribute to the improvement of semantically oriented applications such as question answering and automatic summa-rization [Androutsopoulos and Malakasiotis 2010].

Extensive research on textual entailment recognition owes much to publicly avail-able data sets such as those developed in PASCAL/TAC RTE challenges [Bar-Haim et al. 2006; Bentivogli et al. 2009, 2010; Dagan et al. 2006; Giampiccolo et al. 2007, 2009]. These data sets provide text pairs and their labels,  X  X  X  or  X  X  X , which indicate whether the entailment relation holds for each pair of texts. Conventionally, these evaluation campaigns presuppose applications such as question answering and auto-matic summarization, and textual entailment recognition has been regarded as a com-ponent for improving these applications. For example, in question answering, textual entailment should be applied to answer validation, whereas in automatic summariza-tion textual entailment is applied to redundancy detection. Therefore, these evaluation data were created by simulating such use cases.

The present article explores a new framework for evaluating textual entailment recognition based on a real-world scenario in which a person finds propositions (facts/non-facts) inferred from texts. We focus on the National Center Test for Univer-sity Admission in Japan (Center Test), 1 which is a standardized achievement test used in university admissions. Although this test involves a variety of subjects, we found that questions in several fields, such as history and politics, primarily test examinee X  X  knowledge. A typical question is shown in Figure 1. 2 In this question, the individual is asked to judge whether each statement is a historically true fact. Answering this type of question can be regarded as a direct application of textual entailment recognition, that is, judging whether a statement can be inferred from a textbase that describes facts, such as textbooks and Wikipedia. In the present article, we demonstrate that a significant portion of such tests can be recast as textual entailment recognition and that the tests are useful resources for the evaluation of textual entailment recognition methods.

One advantage of this task setting is that textual entailment recognition is directly connected to the real-world situation in which a person judges the factuality of a given statement. This means that the data has a natural distribution of linguistic phenom-ena that humans rely on in such situations. Another interesting fact is that distractors can be used as negative instances of entailment relations. Distractors are not artifi-cially created or chosen for NLP purposes but are carefully designed for quantifying the level of achievements of high school studies. In addition, the performance of textual entailment recognition can be mapped to scores of university entrance examinations, which are intuitively understandable and comparable with actual human records.
This endeavor is a part of the grand challenge project that aims at developing an artificial intelligent system that can pass university entrance examinations. The main purpose of this project is to integrate heterogeneous artificial intelligence technologies, such as natural language processing, image processing, mathematical manipulation, logical systems, etc., as well as to discover missing links in the actual implementation of a fully end-to-end system. Through extensive analysis of actual examinations, we found that finding propositions from texts is an essential technology that appears ev-erywhere when answering questions, and it can be solved fundamentally by textual entailment recognition. The present work constitutes a first step of this project, and focuses on a specific type of questions, that is, questions to ask examinees X  knowledge, in which proposition finding is directly connected to answering questions.

The data set developed in this work was provided for a subtask of the NTCIR RITE, which is a task to evaluate systems for recognizing inference in text [Shima et al. 2011a]. Sixteen systems from six teams were evaluated using this data. In the fol-lowing, we describe our method for converting university entrance examinations into textual entailment recognition, and report empirical evaluation results obtained in the NTCIR RITE.

Contributions of this article are listed here.  X  This work demonstrates that examinations for humans can be a suitable source for the evaluation of textual entailment recognition.  X  This work reveals how many of the actual questions can be reduced into textual entailment recognition. Textual entailment recognition aims to compute semantic equivalence between texts and is considered to be an integrated framework for semantic processing, involv-ing syntactic/semantic parsing, coreference resolution, paraphrasing, ontology induc-tion, logical inference, etc. Various approaches have been proposed, including ma-chine learning-based methods, such as tree kernels, and methods relying on a theorem prover for predicate logic. For an extensive survey of textual entailment recognition, refer to Androutsopoulos and Malakasiotis [2010].

Extensive research on textual entailment recognition is made possible by the avail-ability of standard evaluation data. PASCAL/TAC workshops provided evaluation data sets [Bar-Haim et al. 2006; Bentivogli et al. 2009, 2010; Dagan et al. 2006; Giampiccolo et al. 2007, 2009], which enable fair empirical evaluation of a variety of methods. Start-ing from the simplest artificial setting, in which manually edited sentence pairs are given and their entailment relations are judged [Dagan et al. 2006], the task setting has become more complicated and more similar to real-world tasks/applications, such as the detection of contradiction and recognizing entailment in context [Bentivogli et al. 2009, 2010; Giampiccolo et al. 2007]. Although research has focused primarily on English, evaluation efforts also took place in other languages: for example, Italian [Bos et al. 2009] and Japanese [Odani et al. 2008].

Most of the previous works cited considered textual entailment recognition as a fundamental component to be embedded in an application, such as question answering and automatic summarization. Therefore, evaluation data were created by simulating actual use cases. For example, in the case of question answering, text pairs are created by retrieving texts that may validate candidate answers. In the case of automatic summarization, text pairs are created by retrieving texts that may describe redundant information. Entailment labels are annotated manually or are transferred from the results of end applications.

The task setting of the present study is different from the above studies in the fol-lowing aspects. First, both t 1and t 2 are real-world sentences (although necessary post-editing is performed as explained in Section 3.4), and their entailment relations are directly connected to the factuality of t 2. Note that negative instances are taken from distractors of actual questions, which are carefully designed to evaluate the examinee X  X  knowledge. Second, entailment labels are automatically derived from questions, and no manual decisions are required. Third, the performance of entailment recognition can be mapped to the test scores, which are comparable with human performance for the same task.

Dagan et al. [2006], Kasahara et al. [2010], and Pe  X  nas et al. [2011] investigated the concept of using reading comprehension tests for the evaluation of textual entail-ment recognition. They considered reading comprehension tests to be intended to test human ability to compute entailment relations, and they considered such tests to be applicable to the evaluation of automatic systems. Although this sounds similar to the setting of the present study, an essential difference is that questions targeted in the present research are not intended originally to assess the ability of entailment recognition. The questions considered herein concern facts/non-facts that are natu-rally derived from knowledge. It is expected that most people can correctly judge the entailment relations evaluated in the present study. The setting of the present study is focused on an unperceived ability to derive propositions from texts, which is trivial for ordinary people but not for NLP systems. Another important difference is that the above studies created test sentences specifically for this evaluation, which indicate the data is artificially designed for this specific NLP task. In contrast, sentences we adopt in the present work come from real-world texts that are not intentionally developed for NLP purposes. Therefore, we can say that our data reflects a natural distribution of entailment relations in the real-world situation to find propositions in texts. A recently proposed framework is the knowledge base population (KBP) [Ji and Grishman 2011]. This task attempts to populate a Wikipedia infobox by extracting facts from a corpus. Textual entailment has been applied to answer validation in this task. Since this task attempts to extract facts from a corpus, the motivation appears similar to the setting of the present study. However, a significant difference is that the KBP focuses on entities such as persons and organizations and their attributes are extracted. This means that the information structures are limited. An ontology-based approach to question answering [Angele et al. 2003] also shares the motivation of deriving facts from an existing resource, although the representations of these facts are also limited. In contrast, the questions considered in the present article are in the form of free text, which can express any proposition. In fact, as we will exemplify later, statements asked in actual examinations describe complex events in which multiple clauses are involved. NTCIR is a workshop on information retrieval that specifically focuses on shared eval-uation [Ishikawa et al. 2011]. Every year and a half, organizers and participants ar-range tasks, develop evaluation data, evaluate systems, and hold a workshop. Each workshop has different tasks, and the NTCIR-9 workshop running from 2010 to 2011 included tasks like geo-temporal information retrieval and search intent analysis, as well as the RITE task that focuses on the recognition of entailment, paraphrasing, and contradiction in texts [Shima et al. 2011a]. In the task setting of RITE, given two sen-tences, t 1and t 2, systems are asked to identify semantic relations between them. In the example in Figure 2, it is required to output that  X  t 1 entails t 2 X .

The RITE task involves two task settings, one of which focuses on the binary clas-sification (BC) between  X  X ntailment X  and  X  X ot entailment X , while the other setting requires the multi-class classification (MC) among  X  X orward entailment X ,  X  X ackward entailment X ,  X  X idirectional entailment (paraphrasing) X ,  X  X ontradiction X , and  X  X o rela-tion X . As we exemplify below, true-or-false questions in the Center Test can be reduced to the task to judge whether or not the two texts have an entailment relation, that is, the same setting as BC. This article addresses a method to convert questions of the National Center Test into the evaluation data for the BC setting. The National Center Test for University Admission (Center Test) is a standardized achievement test for high school students who wish to enter universities/colleges in Japan. The test has been held nationwide since 1990, and more than 500,000 students take the test each year. Most universities/colleges in Japan use the Center Test for admissions in some manner, where each university/college is supposed to determine how to use the results of the Center Test. Some universities/colleges use the scores for selected subjects, while others have their own test and judge admissions according to a combination score of the two tests. In order for universities/colleges to design their own admission policy using the Center Test, various types of tests for a variety of subjects are provided.

Since the Center Test is a nationwide event, questions are carefully designed. Ques-tions and choices are carefully edited to ensure that similar expressions do not exist anywhere in textbooks or on the Web. Questions never exceed the scope of the knowl-edge that the subjects should have been learned in high schools. That is, contents are fairly limited and restricted, while a variety of linguistic expressions appear. There-fore, the Center Test is an interesting resource for investigating how humans process and understand texts. Another desirable feature is that all questions are presented in a multiple-choice style (typically questions have four choices), which allows for auto-matic scoring.

While the Center Test covers a variety of subjects and question types, the present study focuses on true-or-false questions in which choices are presented in the form of sentences. In the following, we demonstrate that such questions can be recast as textual entailment recognition. In the present study, we assume that questions to test an examinee X  X  knowledge can be reduced to the task of textual entailment recognition. For example, consider the world history question shown in Figure 1. For this question, we can find relevant descriptions in Wikipedia, as shown in Figure 3.

From these texts, humans can naturally conclude that statement 1 is true, whereas statement 2 is false. This process of answering questions can be interpreted as judging the entailment relation between a text describing facts and a statement of the ques-tion. That is, we can say that statement 1 can be inferred from the first sentence of Figure 3, whereas statement 2 cannot be inferred from the second sentence (although they are not logically contradictory).

From this example, we can create text pairs for evaluating textual entailment recog-nition, as shown in Figure 4, where t 1 is from a Wikipedia description, whereas t 2is from the choice of the question. In this way, we can reduce the process of solving a question into textual entailment recognition, and inversely, if a system of textual en-tailment recognition can judge the entailment relation correctly, the system can also correctly answer the question. This indicates that textual entailment recognition is strongly connected to the real-world task of solving this type of question.
As an evaluation challenge of textual entailment recognition, this work follows the simplest setting: a pair of sentences is given, and a system is asked to judge whether or not the pair has an entailment relation. As described above, the Center Test includes a variety of types and difficulty levels of questions, and it allows for the automatic scoring, it is a suitable resource for creating data sets for the empirical evaluation of textual entailment recognition.

Note, however, that these questions attempt to measure fundamentally different criteria from what we have evaluated to be an NLP task. The original tests were in-tended to evaluate the amount of knowledge, and not to test the ability of computing entailment relations. Nevertheless, comparison with humans in the proposed frame-work is meaningful, because the focus is on an ability most ordinary people have un-consciously. In other words, this is a challenge to replicate the ability of semantic processing that all people already have.

In the present study, we adopt Wikipedia as a textbase for the retrieval of evidential texts. This is mainly due to a copyright issue, but also because Wikipedia contains rich information about history, politics, economics, and society, which are the target subjects of the present work. The present study focuses on questions for which the correct answers can be derived by entailment relations from textual knowledge. Actual tests may include questions that are not in this form. Therefore, we must first classify questions by type and filter out questions that are not appropriate for evaluating textual entailment recognition.
In a preliminary investigation, we found that questions of the Center Test can be classified into the following five types.
 lations from texts that describe facts, such as Wikipedia and textbooks. We will convert this type of question into evaluation data.
 does not apply (Figure 5). Although such questions could be reduced into factoid ques-tion answering, this is beyond of the scope of the present study. 3 (e.g., pictures) were excluded from consideration (Figure 6).
 or interpretation of the text is requested. Inherently, answers cannot be derived from external resources for this type of question. Although such questions may be reduced into textual entailment between a given text and choices [Dagan et al. 2006; Kasahara et al. 2010; Pe  X  nas et al. 2011], these cases were excluded from the present study. ple of an abstract concept (Figure 7). This type of questions involve the identification of text relationships, but cannot be reduced into textual entailment. Hence, they must be excluded for the present work. After extracting appropriate questions, converting the questions into textual entail-ment data is relatively straightforward. The first step is to determine the relation label,  X  X  X  or  X  X  X . Unlike previous methods, labels are determined automatically from the intent of questions and the correct answers. In principle, if a statement refers to a fact, the label is  X  X  X , otherwise the label is  X  X  X . When a question requests the iden-tification of a true statement (e.g.,  X  X hoose the correct statement X  or  X  X hoose the ap-propriate sentence X ), a correct answer is assigned  X  X  X , whereas incorrect answers are assigned  X  X  X . In contrast, when a question requests the identification of a false state-ment (e.g.,  X  X hoose the incorrect statement X  or  X  X hoose the inappropriate sentence X ), a correct answer is assigned  X  X  X , whereas incorrect answers are assigned  X  X  X .
The next step is to retrieve a relevant text from Wikipedia. This process is per-formed for each statement independently. For a statement with the entailment label  X  X  X , we must find a text that is evidence of the statement. In other words, a text that entails the statement must be retrieved. The situation is not evident for statements with the label  X  X  X . The label  X  X  X  indicates that the statement should not be entailed from any text in Wikipedia. This involves two cases. The first is that Wikipedia con-tains a text that is contradictory to the statement, and the second is that Wikipedia does not contain any texts that can infer or is contradictory to the statement. Since we cannot know beforehand which is the case for each false statement, we took the following approach. (1) Search for a text that is contradictory to the statement. If an appropriate text is (2) If such a text cannot be found, retrieve a text that includes as many keywords in
When an annotator finds multiple texts that satisfy these criteria, we let the anno-tator pick one of them arbitrarily. Sentences extracted from questions and Wikipedia are accepted as is, as far as pos-sible, as t 1and t 2. However, sentences are originally placed in a context and so might lack some necessary information. For example, consider the example shown in Figure 8. Each choice in the question (Figure 8 top) does not explicitly refer to  X  X t-toman Empire X ; instead,  X  X ttoman Empire X  is referred to implicitly (in the choice 1), or by an anaphoric relation (in the choice 2). 4 Without the context (i.e., asking about the Ottoman Empire), it is impossible to judge the factuality of these sentences. In such cases, we let annotators add necessary terms to the original sentences so that they form natural sentences allowing for true-or-false judgment (see t 2 in the bottom of Figure 8).

A similar situation occurs with texts extracted from Wikipedia. As shown in the middle of Figure 8, each text does not explicitly mention  X  X   X  uleyman X  or  X  X ttoman Empire X . Rather, the texts use anaphoric expressions and coreferences (e.g.,  X  X is X ,  X  X e X ,  X  X he Empire X ). In the manner described above, we allow annotators to add necessary terms to these texts (see t 1 in the bottom of Figure 8).

Another issue is that Wikipedia does not necessarily describe relevant information in a single sentence. Consider the example shown in Figure 9. We could not find a single sentence that entails all of the information of this statement. Rather, we found the two sentences as shown in Figure 10. In such cases, we allowed annotators to retrieve all necessary sentences and summarize these sentences into a single natural sentence. In the case of Figure 10, we created the sentence shown in Figure 11 as t 1. For data development, we used questions from the Center Tests for 2007 and 2009. The data obtained from the 2009 test will be used as the development set, whereas the data obtained from the 2007 test will be used as the final test set in the experiments in Section 4.2. The selected subjects are world history (WH), Japanese history (JH), modern society (MS), and politics and economics (PE). World history and Japanese history are separated into two subject fields, namely A and B, 5 according to the time period they cover. In total, we have six subject fields. Seven undergraduate students were recruited for the data development. A total of 315.5 person-hours were required to process all of the data.

Table I shows the results of question filtering and conversion into text pairs for the development set. Entailment, non-sentential, non-text, comprehension, and exempli-fication indicate the question types introduced in Section 3.2.  X  X o evidence X  denotes the number of questions for which annotators could not find evidential texts for some of the choice sentences. Such questions are not used in the final evaluation of exami-nation scores. These results reveal that more than half of the questions of the Center Test could be reduced into textual entailment recognition, which demonstrates the effectiveness of using the Center Test as a resource for the evaluation of entailment recognition.

Table II shows a summary of the data used to evaluate the textual entailment recog-nition systems. Since the majority of questions ask to  X  X hoose the correct statement X , the data distribution is biased towards  X  X  X . The test set contains 108 questions, 70 of which are the type of  X  X hoose the correct statement X , 23 of which are the type of  X  X hoose the incorrect statement X , and 15 of which directly ask the correctness of state-ments. Figure 12 shows text pairs sampled from the development data. The data is released in XML format. The tag pair denotes a text pair, and the attribute label indicates the entailment relation ( X  X  X  or  X  X  X ). The tags t1 and t2 denote t 1and t 2, respectively. These samples reveal that target texts represent fairly complex events and concepts that are not limited to entities and their attributes, although humans can easily determine their entailment relations. The evaluation data developed herein was provided for the NTCIR RITE subtask. Six teams listed in Table III participated in this evaluation. Refer to the system descrip-tion papers for the details of these systems. Table IV shows the accuracy, precision, recall, and F -score achieved by each of the systems. 6 The precision, recall, and F -score are computed for  X  X  X  labels. The table also shows the scores for three baseline systems: one outputs  X  X  X  to all pairs, another outputs  X  X  X  to all pairs, and the other outputs random labels. The results reveal that the best systems outperform the baselines by a large margin with respect to accuracy, whereas only one of the systems (System IBM-1) achieved a better score than the All Y baseline in F -score. One possible reason for this is that the systems are tuned for accuracy and tend to output  X  X  X , which is the majority class.

While it is unclear from Table IV the degree to which these results impact a real task, an advantage of the proposed framework is that we can interpret the results in terms of the examination scores. The outputs of the systems are mapped into answers to the questions using confidence values, which indicate the confidence of each output label. The algorithm for converting textual entailment labels into answers to questions is described as follows. (1) When a system outputs  X  X  X  to only one of the choices, output this choice as the (2) If a system outputs  X  X  X  to more than one choice, output the choice assigned the (3) If a system outputs  X  X  X  for all choices, output the choice assigned the lowest confi-When a question asks to choose an incorrect statement, the same algorithm is applied while  X  X  X  and  X  X  X  are inverted.

Table V shows the scores for the examinations. 7 The number of correct answers for each subject is shown. The correct answer ratios for all of the subjects are shown in the right-most column. The results clearly demonstrate that the top systems outperform the baseline by a large margin. This result proved that current state-of-the-art meth-ods for textual entailment recognition contribute significantly to the task of judging factuality of propositions, while the score is still far from perfect. In this section, we discuss problems encountered during the process of data development.
 dividuals to identify the correct combination of an event and its time. For distractors of such questions, annotators tended to retrieve a Wikipedia description of the same event that includes correct time information. Such text pairs can be easily judged as  X  X  X  by simply checking the consistency of time information. It is reported that IBM-1 gained five points in accuracy by adding a feature that detects the consistencies and inconsistencies of temporal expressions (e.g., year and century). The difficulty in this type of question is retrieving such a text that mentions the same event. In order to avoid such easy-to-judge instances, we must modify the strategy for retrieving t 1for this type of question.
 post-editing of Wikipedia texts were performed. In particular, in modern science and politics and economics, statements do not refer to events, but rather describe abstract concepts, for which annotators could not find an exact single sentence that mentions the statements directly. In such cases, multiple text fragments had to be retrieved. Since we let annotators summarize multiple texts into a single sentence, this resulted in extensive editing of original texts in some cases. Such editing may have made en-tailment recognition unjustifiably easy. For example, in the example of Figures 10 and 11, original texts are retrieved from different pages, and only relevant expressions are extracted and edited into a sentence. As a result, the obtained t 1 is very similar to t 2, although the original texts are considerably different from them. The present article investigated the proposed approach to the development of evalu-ation data for textual entailment recognition using the National Center Test for Uni-versity Admission. We extracted questions to test the examinee X  X  knowledge from six subject fields: world history A/B, Japanese history A/B, modern society, and politics and economics. Based on the tests for 2007 and 2009, we created 941 text pairs. This data set is provided for the subtask of NTCIR RITE, and sixteen systems for tex-tual entailment recognition were empirically evaluated using this data. The experi-ments demonstrated that the best system could output correct answers for 56% of the questions. This result is far beyond the random baseline while it is still far behind humans. 8
The proposed framework should be able to be applied to tests of similar style, which are common in the real world. Similar entrance examinations are used in several countries, and similar tests are used in other fields, such as examinations for medi-cal and judicial licenses. This means that we can readily develop evaluation data in various languages/domains by following the proposed methodology.
 In the future, the task should be made to more closely reflect actual examinations. At present, we manually retrieve evidential texts and post-edit the texts to include necessary information. We can eliminate these human interventions by not providing t 1 explicitly. Instead, we provide a textbase, such as Wikipedia or a textbook, in its entirety, and ask a system to retrieve relevant texts by itself. This setting is similar to textual entailment recognition in context [Bentivogli et al. 2009, 2010] and the knowl-edge base population [Ji and Grishman 2011]. The development of evaluation data for this setting appears to be easy. We can simply provide a textbase and sentential choices extracted from questions, and most of the work of the annotators used in the present work is unnecessary. Note that this setting is completely free from the two problems raised in Section 4.3. Another possible extension is cross-lingual textual entailment, which was recently proposed [Mehdad et al. 2010, 2011]. Several subjects, including world history and politics and economics, should be independent of languages. There-fore, we can easily set up the task by using questions in one language and finding evidence in another language.

