 theory of support vector machine to unsupervised learning, has been attracting considerable attention recently. The existing approaches mainly focus on reducing the compu-tational complexity of MMC. The accuracy of these meth-ods, however, has not always been guaranteed. In this pa-per, we propose to incorporate additional side-information, which is in the form of pairwise constraints, into MMC to further improve its performance. A set of pairwise loss functions are introduced into the clustering objective func-tion which effectively penalize the violation of the given constraints. We show that the resulting optimization prob-lem can be easily solved via constrained concave-convex procedure (CCCP). Moreover, for constrained multi-class MMC, we present an efficient cutting-plane algorithm to solve the sub-problem in each iteration of CCCP. The exper-iments demonstrate that the pairwise constrained MMC al-gorithms considerably outperform the unconstrained MMC algorithms and two other clustering algorithms that exploit the same type of side-information. 1 creasing interest recently in both machine learning and data mining communities [15, 18, 19, 22, 23, 24]. The key idea of MMC is to extend the maximum margin principle in su-pervised learning to unsupervised scenario. Given a set of examples, it performs clustering by finding the maximum margin separating hyperplanes in the data. Recent studies have shown that MMC is superior to conventional cluster-ing methods.
 support vector machines (SVMs), which usually result in convex optimization problems, unsupervised large margin learning leads to non-convex integer programs, which are much more difficult to solve. Recently, different optimiza-tion techniques have been used to handle this problem, such as semidefinite programs (SDP) [15, 18, 19], alternating op-timization [22] and cutting-plane method [23, 24]. have improved the efficiency of MMC, the accuracy of these methods has not always been guaranteed. For example, as an iterative approach, the iterSVR algorithm [22] begins with assigning a set of initial labels to the examples. The quality of this initialization is crucial for the performance. Random initialization will usually result in poor clustering outputs (see Figure 2).
 setting of the class balance constraint, i.e . the relative size of clusters. Specifically, for binary clustering, the mean of the labels y i  X  X  X  1 } is required to lie in a small interval around zero, i.e . 1 n i y i  X  [  X  l, l ] , where l is the param-eter that controls the class balance. This constraint is not only necessary for preventing the trivially  X  X ptimal X  solu-tion which assigns all examples to the same cluster, it may also greatly influence clustering accuracy. Figure 1 depicts how the performance of iterSVR varies with l for two data sets. As can be seen, a relatively accurate estimation of the size of each cluster is required for getting a satisfactory clustering. However, this kind of prior knowledge is usually not easy to obtain in practice. The situation is even worse for multi-class clustering, in which it is generally difficult to effectively enforce the class balance constraints even if we have the required prior knowledge.
 consider augmenting the data with additional information to aid the unsupervised clustering. Specifically, we are inter-ested in incorporating constraints between pairs of exam-ples into MMC. The pairwise constraints indicate whether two examples belong to the same cluster or not, usually known as must-link constraints and cannot-link constrains.
Figure 1. Clustering accuracy of iterSVR vs. l . K-means is used for initialization.
 Such pairwise constraints are weaker and more general than the explicit labels on the examples, since we cannot infer la-bels from only pairwise constraints. It has been suggested that this kind of constraints may arise from domain knowl-edge automatically or can be obtained with little human ef-fort in some scenarios [20].

The idea of incorporating pairwise constraints has been studied a lot in the context of semi-supervised clustering, leading to two different lines of research. In the first line, the constraints are incorporated directly into the clustering algorithms [2, 11, 12, 16]. This kind of methods enforce the constraints during the clustering process and produce solu-tions that comply with the given constraints. In the second line, the constraints are used to learn an informative distance function, which is then combined with some clustering al-gorithms, such as K-means [1, 3, 17]. It has been shown by previous studies that appropriate incorporation of pairwise constraints can often improve the clustering performance.
The main contributions of this paper are three-fold. First, we propose to incorporate pairwise constraints into max-imum margin clustering (both for binary and multi-class clustering). A set of pairwise loss functions are introduced into the clustering objective functions in an attempt to pe-nalize the violation of the given constraints. Second, we formulate the constrained MMC problem as an optimiza-tion problem with a convex quadratic objective function and concave-convex constraints, which can be solved with constrained concave-convex procedure (CCCP) [14, 21]. By using CCCP, we iteratively solve a series of standard quadratic programs (QPs), which makes computational ef-ficiency close to the recently proposed MMC algorithms, such as iterSVR [22]. Finally, for multi-class clustering, we present a fast cutting-plane algorithm to solve the QP prob-lem in each iteration of CCCP.
 The remainder of the paper is organized as follows. We first present the pairwise constrained maximum margin clustering algorithm for binary clustering in Section 2. Then in Section 3, we extend it to multi-class maximum margin clustering. Experiment results are provided in Section 4. Finally, we conclude this paper in Section 5.
In this section, we present the pairwise constrained max-imum margin clustering algorithm for binary clustering.
The key idea of maximum margin clustering is to ex-tend the principle of maximum margin to unsupervised learning. For binary clustering, given a set of examples U = { x 1 ,..., x n } , the algorithm aims to find, among all possible binary labels Y = { y 1 ,...,y n } ,y i  X  X  X  1 } ,the one that a SVM trained on U X Y yields the largest mar-gin. Specifically, let f ( x i )= w T  X  ( x i )+ b , it solves the following optimization problem, where the last constraint is the class balance constraint in-troduced to avoid the trivially  X  X ptimal X  solution that as-signs all examples to the same cluster and achieves a very large margin. l is a constant controlling the class balance.
In Eq.(1), we intend to maximize the margin by optimiz-ing both the unknown y and the unknown SVM parame-ters ( w ,b ) , while in SVM, the labels y are known for train-ing. This difference makes Eq.(1) a combinatorial problem, which is much more difficult to solve than the QP problem for SVM. However, similar with the formulation for trans-ductive SVM [5], we can approximate it by finding a sepa-rating hyperplane which forces the examples to be as far as possible from the margin, Here a slightly relaxed constraint is used to maintain the class balance, which is much easier to handle than the orig-inal one in Eq.(1).
In pairwise constrained clustering, we are given a set of is the label assigned to the example pair ( x i 1 , x i 2 ) .The constraints with label z i =1 will be called must-link con-straints which means x i 1 and x i 2 belong to the same cluster. The constraints with z i =  X  1 are cannot-link constraints in-dicating that x i 1 and x i 2 belong to different clusters.
To incorporate pairwise constraints into MMC, we would like to introduce a loss function, which will penalize the violation of the given constraints. While loss functions violation of the pairwise constraints, they are generally dif-ficult to handle in an optimization problem. We therefore introduce a relaxed loss function which is convex and more flexible. Specifically, we choose to measure the pairwise the Laplacian loss | y i  X  f ( x i ) | in regression. It is expected that the difference between f ( x i 1 ) and z i f ( x i 2 ) can mea-sure how likely the pairwise constraints would be violated.
By using this loss function we intend to minimize the difference between f ( x i 1 ) and f ( x i 2 ) if the example pair ( x i 1 , x i 2 ) belong to the same cluster. When the two exam-ples belong to different clusters, the loss function becomes | f ( x i 1 )+ f ( x i 2 ) | , the minimum of which is achieved when the separating hyperplane passes through the middle point same absolute value but opposite signs. The trivial case of f ( x i 1 )= f ( x i 2 )=0 can be avoided by the margin con-straints that keep the examples away from the separating hyperplane. Similar pairwise loss function has been incor-porated into SVMs by Yan et al . in [20]. However, they fo-cused on classification problem, where the labels for some examples are known during training. Besides, in [20], no margin constraints are imposed to the examples whose la-bels are unknown, which may lead to the trivial solution.
By introducing this pairwise loss function, we have the following optimization problem for constrained MMC, Note that, in Eq.(3), we exclude the class balance constraint. It is expected that the cannot-link constraints can already prevent the trivially  X  X ptimal X  solution of assigning all ex-amples to the same cluster. Moreover, by omitting the bal-ance constraint we can assess the effect of the pairwise con-straints more accurately.
The objective function in Eq.(3) is convex and the in-equality constraints regarding the labeled example pairs are linear w.r.t. the variables. The margin constraint, how-ever, is non-convex but a difference of convex functions. To solve this problem, we turn to constrained concave-convex procedure (CCCP), which is proposed recently to solve problems with a concave-convex objective function and concave-convex constraints [14, 21]. Given an ini-Taylor expansion at w ( t ) ,b ( t ) and optimizes the result-ing convex problem. Notice that | f ( x i ) | is a non-smooth function at w ( t ) ,b ( t ) . To use CCCP, we should replace the gradient with the subgradient when computing the tan-| f By replacing | f ( x i ) | in Eq.(3) with this first order approxi-mation, we have the following relaxed convex optimization problem for each iteration of CCCP: The optimization problem in Eq.(5) is a standard QP prob-lem which can be solved efficiently. Following CCCP, the solution ( w ,b ) of Eq.(5) is then used as w ( t +1) ,b ( t +1) and the iteration continues until convergence.

For initialization, we are actually interested in the sign of f (0) ( x i ) . Therefore, we can directly initialize the labels for the examples. Just like in iterSVR [22], we can use a simple clustering algorithm such as K-means to get a rela-tively good initialization. However, we find that by incor-porating the pairwise constraints, the iterative algorithm be-comes less sensitive to initialization. We can achieve good clustering result even with random initialization. only handle binary clustering. In the following, we discuss how to perform constrained MMC for multi-class data set. method to handle data with multiple clusters, we employ the multi-class SVM formulation proposed by Crammer and Singer in [6]. In multi-class classification scenario, each label y i is an integer from the set { 1 ,...,k } , where k is the number of classes. A separate weight vector w r is defined for each class r and the classifier classifies example x i by The value of the inner-product w T r x i is called the similarity score of x i for class r . According to [6] the optimization problem for multi-class SVM can be formulated as where  X  p,q equals 1 if p = q and 0 otherwise. The con-straints state that for each example ( x i ,y i ) , the similarity score w T y i x i for correct y i must be greater than the scores w r x i for all incorrect r . Otherwise, a loss is incurred. aims to find, among all possible labels, the one with which a multi-class SVM trained will yield the largest margin. With a slight modification to Eq.(7), we formulate the multi-class MMC as min s.t .  X  i  X  X  1 ,...,n } ,r  X  X  1 ,...,k } : where y  X  i is determined by Eq.(6). The problem in Eq.(8) is comes from the sharing of the slack variable  X  i among the margin constraints for the same example x i . To tackle this problem, we propose to introduce a separate slack variable  X  ir for each constraint, which can dispense with the need for  X  i ,r . Specifically, we reformulate the multi-class MMC as
Note that we should also incorporate the class balance constraints in Eq.(9) just like in binary clustering. How-ever, it is quite difficult to impose effective constraints in this multi-class setting.
In the following, we incorporate pairwise constraints into multi-class MMC. Just like in binary case, the problem would generally be difficult to handle if we impose exact loss for violation of the constraints. We therefore use re-laxed loss functions to accommodate these prior knowledge. For must-link constraints, we can use similar loss functions as in Eq.(3), i.e . for each w r , we minimize the difference between the similarity scores of the examples that are sup-posed to belong to the same cluster. For cannot-link con-straints, however, we should use a slightly different loss similarity scores of x i 1 and x i 2 for class r . A cannot-link constraint is violated when the two examples achieve max-imum similarity scores on the same class. In this case, the gap between the max and average of  X  ir for the pair should be maximized, i.e . we will get a relatively large value for therefore minimize this gap to avoid label collision between cannot-link constrained examples. These lead to the follow-ing optimization problem for constrained multi-class MMC: where m p and m n are the total number of must-link and cannot-link constraints respectively. Note that when k =2 , the loss function for cannot-link constraints in Eq.(10) is ac-tually equivalent with the pairwise loss used in Eq.(3) (See Appendix A). Therefore, the pairwise loss functions intro-duced for multi-class clustering can be regarded as consis-tent extension of the loss functions used for binary case. Just like in binary case, the first set of constraints in Eq.(10), which maintain the margin between the max simi-larity score and the other scores for each example, are non-convex. Once again, we can use CCCP to solve this opti-mization problem. In order to compute the first order Tay-first introduce the vectors  X  w and  X  x ip ,i =1 ,...,n,p = 1 ,...,k , where  X  w is the concatenation of the vectors w p , and  X  x ip is a vector of the same length as  X  w such that all of its entries are 0 except the subranges corresponding to class p , which are set to x i . It is straightforward to verify that the term w T p x i can be written as  X  w T  X  x ip .
For pointwise maximum function h ( x )= hull of the union of subdifferentials of the  X  X ctive X  func-tions at x , i.e .  X  X  ( x )= conv  X  p  X  S ( x )  X  X  p ( x ) , where conv denotes the convex hull, and S ( x ) is the index set of all p  X  X  1 ,...,k } for which h ( x )= h ( x ) [13]. Thus, we have that for i =1 ,...,n ,  X  where  X  with k p =1  X  ip =1 . Considering that each example will be assigned to only one cluster during clustering, therefore for each example x i , we keep only one of the indices on which the max similarity score according to current  X  w are achieved. For convenience, we pick the subgradient with  X  iu i =1 where u i is the minimum of the indices that sat-are set to 0. At the t th iteration of CCCP, denote the cur-
By replacing the maximum function in Eq.(10) with the above first order Taylor expansion, we obtain the following convex optimization problem for each iteration of CCCP: The problem in Eq.(13) is a standard QP problem. We can iteration continues until convergence. For initialization, we can adopt similar strategy as for binary clustering.
Although the problem in Eq.(13) is a standard QP prob-lem, it is not immediately obvious that it can be solved efficiently, since it has a large number of slack variables and constraints. Inspired by the recent work for structural SVMs [9, 10], in the following, we present a cutting-plane method for solving Eq.(13). The algorithm is based on an alternative, but equivalent formulation of Eq.(13), i.e ., min
The optimization problem in Eq.(14) has only three slack variables, each of which is shared across a set of constraints. Each constraint that involves  X  corresponds to the sum of a subset of constraints from Eq.(13) that maintain the margins for the examples. { c ir } are binary variables that select the constraint subset. Each of the constraint involving  X  corre-sponds to one of all possible binary vectors s  X  X  X  1 , 1 } m p . And the constraints in the last set correspond to possi-ble combinations of labels { r i } i : z i =  X  1  X  X  1 ,...,k } m n . The problem in Eq.(14) is equivalent with the problem in Eq.(13) in the following sense.
 Theorem 1 Any solution { w  X  1 ,..., w  X  k } of the problem in Eq.(14) is also a solution of the problem in Eq.(13) (and vice
Proof. See Appendix B. Algorithm 1 Cutting-Plane algorithm for solving Eq.(14) 1: Input: { ( x i ,u 2: W  X   X  X  X  , W  X   X  X  X  , W  X   X  X  X  3: repeat 4: ( w 1 ,..., w k , X , X , X  )  X  5: Find c , s and r for the most violated constraints by 6: W  X   X  X   X   X  X  c } , W  X   X  X   X   X  X  s } , W  X   X  X   X   X  X  r } 7: until No constraint is violated by more than . 8: return ( w 1 ,..., w k , X , X , X  )
By moving from Eq.(13) to Eq.(14), we have success-fully reduced the number of slack variables to only three. However, the number of constraints increased exponentially new problem appear intractable. Fortunately, by adapting the cutting-plane algorithm, we can greedily find a polyno-mially sized subset of the constraints, with which the solu-tion of the reduced problem fulfills all constraints in Eq.(14) up to precision [9, 10].

The cutting-plane algorithm is illustrated in Algorithm 1. Starting with three empty sets of constraints, the cutting-plane algorithm iteratively constructs three constraint sub-sets W  X  , W  X  , W  X  (working sets), which correspond to the three slack variables  X ,  X  and  X  respectively. In each itera-tion, it first computes the optimum over the current working sets. Then it finds the most violated constraint in each con-straint set and adds them to the corresponding working sets. The algorithm terminates when no constraint can be found that is violated by more than the desired precision .The most violated constraints are those that require the largest  X ,  X  and  X  to make them feasible given the current estima-tion of ( w 1 ,..., w k ) . The corresponding c , s and r for these constraints can be calculated as follows: Therefore, in each iteration, only three constraints are added into the working sets.

At the beginning, ( w 1 ,..., w k ) can be initialized with the solution of Eq.(14) obtained during the last iteration of CCCP. To solve Eq.(14) for the first iteration of CCCP, the initial working set is constructed with randomly generated c , s and r . By applying similar analytical skills, we can get similar theoretical results about the correctness and the time complexity of this cutting-plane algorithm as those in [9, 10]. Due to the lack of space, we do not provide detailed discussions here.

Note that although Zhao et al . [23, 24] have also pro-posed to use cutting-plane and CCCP to solve the MMC problem (without pairwise constraint), they directly used the cutting-plane algorithm to solve the original non-convex problem. And CCCP was used to solve the sub-problem in each iteration of the cutting-plane algorithm. In our work, we first use CCCP to decompose the non-convex problem into a series of QP problems. Then we use cutting-plane algorithm to solve the convex QP problem. The form of the optimization problem we get is much simpler than zhao et al . X  X  work. Moreover, the convergence of cutting-plane algorithms for convex problems can be more easily guaran-teed than for non-convex problems [8].
In the experiment, we evaluate the performance of the proposed algorithms on a number of data sets from the UCI repository (ionosphere, digits, letter and satellite), the LIB-SVM data 2 (svmguide1-a and mnist), and another bench-mark repository 3 (image). The binary class data sets are cre-ated following the same setting as in [22]. We also created several multi-class data sets from the digits, letter and mnist data. The letter and mnist data sets are relatively large. So we select the first 200 samples from each of the correspond-ing classes and create several subsets of data sets. We sum-marize the data sets in Tabel 1.

For binary clustering, we implement the algorithm pro-posed in Section 2. The implementation is based on Uni-verSVM 4 , which applied CCCP to transductive SVM [5]. For multi-class clustering, we use the cutting-plane based algorithm. To use kernel in the cutting-plane algorithm, we use low-rank approximations of the kernel matrix as sug-gested in [10]. A set of examples are randomly chosen as the basis functions. In the following, we will simply refer to the proposed algorithms as CMMC.

Following the previous works [15, 18, 22], we mea-sure the quality of the clustering solutions by cluster purity. Specifically, we first take a set of labeled examples, remove the labels and run the clustering algorithms. Then each of the resulting clusters is labeled with the majority class ac-cording to the original training labels. Finally, the accuracy of the predictions are measured.
We first examine the sensibility of the maximum mar-gin clustering algorithms to the initial class labels. Both CMMC and other recently proposed maximum margin clus-tering algorithms [22, 23, 24] are iterative methods, which should begin with assigning a set of initial labels to the ex-amples. Instead of using K-means to generate the initial labels, in this experiment, we run CMMC (binary edition) and iterSVR 5 [22] with random initialization. In Figure 2, for each data set, 10 random initializations are generated, starting from which CMMC and iterSVR are then run re-spectively. For iterSVR, we set l to be 0 . 05 for the first three data sets which are balanced and let l =0 . 4 for satellite (The mean of its training labels is 0.3712). For CMMC, we randomly generate 50 pairwise constraints (25 for each kind) for each data set. As can be seen, although iterSVR works well when initialized with K-means (shown by  X  X terSVR Kmeans X ), its performance is not quite good and is extremely unstable when initialized randomly (shown by  X  X terSVR rand X ). CMMC, on the other hand, steadily achieved very good results, which means it is less sensi-tive to the initialization. This is a very pleasant property and shows the value and power of introducing pairwise con-straints in MMC.
We then compare the pairwise constrained maximum margin clustering algorithm with two other clustering meth-ods that exploit the same type of side information. The first algorithm we compare with is constrained EM [12], which uses constraints between pairs of examples to aug-ment the data when computing the Gaussian mixture model. Constrained EM has shown superior performance over some other semi-supervised clustering methods that also directly incorporate the constraints such as constrained K-means [16] and constrained complete linkage [11]. We also compare the proposed algorithm with Discriminative Component Analysis (DCA) [7], which, combined with K-means, can be regarded as a representative of the sec-ond kind of semi-supervised clustering algorithms. DCA is a distance metric learning algorithm that can exploit both must-link and cannot-link constraints between data points. It has shown that for the task of image retrieval, the dis-tance learned by DCA can achieve better result than some other related work such as Relevant Component Analysis (RCA) [1] and Xing X  X  work [17].
 In Figure 3, we compare the clustering accuracy of CMMC, constrained EM and DCA under different numbers of pairwise constraints. In this experiment, we initialized CMMC with the result of K-means, because it would con-verge to desired clustering more quickly if a relatively good initialization is given. We test each data set under five dif-ferent numbers of constraints. For each specific number, five different realizations of constraints are randomly gen-erated. Then the same constraints are applied to different algorithms. Under each realization of constraints, the clus-tering algorithms are run five times. The average results are reported. The results obtained by K-means are shown as the baseline. For binary data, we also show the accuracy of iterSVR. We can see from Figure 3 that by introducing pair-wise constraints, we can dramatically improve the accuracy of MMC. Moreover, CMMC also consistently and signifi-cantly outperforms constrained EM and DCA in all the data sets under different settings of the pairwise constraints. This demonstrates the superiority of using the maximum margin principle to perform semi-supervised clustering.
Finally, we explore an desirable property of maximum margin clustering that has not been investigated by previ-ous work. One of the most important advantages of SVM is its ability to achieve relatively small generalization er-ror on new data. By applying the principle of maximum margin to the clustering problem, we are actually expecting to inherit this property. Therefore, in the final experiment, we are interested in examining the generalization ability of CMMC. To serve this purpose, we save the models learned by CMMC when conducting the experiments in the previ-ous section. Then we use these models to group new data that has not been considered during the clustering process. The results on letter and mnist data, for which only a small subset has been used in previous experiments, are shown in Figure 4. The test data are obtained by randomly draw-ing 200 new samples for each of the corresponding classes. As can be seen from Figure 4, the clustering accuracy on out-of-sample examples (shown by  X  X est X ) is quite similar with that on the given examples (shown by  X  X rain X ), which confirmed our expectation. This may suggest an important application scenario for maximum margin clustering where for a large data set, we can perform the clustering process only on a small subset of the data and then use the model learned to directly group the remaining data points.
In this paper, we have presented a new maximum mar-gin clustering algorithm, which incorporates pairwise con-straints into the clustering process. We propose to in-troduce a set of pairwise loss functions into the cluster-ing objective function which penalize the violation of the given constraints. We show that the non-convex optimiza-tion problem for constrained maximum margin clustering can be solved with constrained concave-convex procedure, which decomposes the problem into a series of QP prob-lems. For multi-class clustering, a cutting-plane algorithm is presented for solving the QP problem in each iteration of CCCP. Experiments on a number of real world data sets demonstrate that by incorporating the pairwise constraints, we can greatly improve the accuracy of maximum mar-gin clustering. The pairwise constrained maximum mar-gin clustering algorithms also considerably outperform two other semi-supervised clustering algorithm with the same amount of pairwise constraints. In the future, we will fur-ther investigate the efficiency of the proposed algorithms, especially on large scale data sets.

Yang Hu and Nenghai Yu are supported in part by Na-tional Natural Science Foundation of China (60672056) and the Research Fund for the Doctoral Program of Higher Ed-ucation (20070358040).

For the multi-class formulation in Eq.(10), when k =2 ,let  X  w = w 1  X  w 2 , then the classifier can be written as y  X  i =1 if  X  w T x i &gt; 0 , otherwise y  X  i =2 . To impose the cannot-link constraints, we intend to minimize the following loss function which equals,
This is equivalent with the loss function for cannot-link pairs in Eq.(3) if we let f ( x i )= w T x i .
 Proof of Theorem 1
Generalizing the proofs from [9, 10], we will show that the two optimization problems in Eq.(14) and Eq.(13) have the some objective value and equivalent set of constraints.
Particularly, for every { w 1 ,..., w k } the smallest feasi- X 
For a given { w 1 ,..., w k } ,  X  ir ,  X  ir and  X  i can be opti-mized individually. The optimums are achieved as follows
For the problem in Eq.(14), the optimal  X  for a given { w 1 ,..., w k } is  X  = max
Since Eq.(23) is linear in c ir , each c ir can be optimized independently, i.e .,  X  = 1
Similarly, the optimal  X  and  X  for the problem in Eq.(14) are  X  = max = =  X  = max = =
Therefore, for any { w 1 ,..., w k } , the objective functions of the problems in Eq.(13) and Eq.(14) are equal given the optimal  X  ir , X  ir , X  i and  X ,  X ,  X  .
