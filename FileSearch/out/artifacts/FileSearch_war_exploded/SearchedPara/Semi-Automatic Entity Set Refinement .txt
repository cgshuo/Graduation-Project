 Sets of named entities are extremely useful in a variety of natural langua ge and information re-trieval tasks. For example, companies such as Ya-hoo! and Google maintain sets of named entities such as cities, products and celebrities to improve search engine relevance. named entities is expensiv e and laborious. In re-sponse, many automatic and semi-automatic me-thods of creating sets of named entities have been proposed, some are supervised (Zhou and Su, 2001), unsupervised (Pantel and Lin 2002, Nadeau et al. 2006), and others semi-supervised (Kozareva et al. 2008). Semi-supervi sed approaches are often used in practice since they allow for targeting spe-cific entity classes such as European Cities and French Impressionist Painters . Methods differ in complexity from simple ones using lexico-syntactic patterns (Hearst 1992) to more compli-cated techniques based on distributional similarity (Pa  X  ca 2007a). rors inevitably occur and manual refinements are necessary for most practical uses requiring high precision (such as for query interpretation at com-mercial search engines). Looking at expansions from state of the art syst ems such as GoogleSets 1 , we found systematic errors such as those resulting from ambiguous seed instances. For example, con-sider the following seed in stances for the target set Roman Gods : GoogleSet X  X  expansion as well others employing distributional expansion techniques consists of a mishmash of Roman Gods and celestial bodies, originating most likely from the fact that Neptune is both a Roman God and a Planet . Below is an excerpt of the GoogleSet expansion: The inherent semantic similarity between the errors can be leveraged to quickly clean up the expan-sion. For example, given a manually tagged error  X  asteroid  X , a distributional similarity thesaurus such as (Lin 1998) 2 can identify comet as similar to asteroid and therefore potentially also as an error. This method has its limitations since a manually tagged error such as Earth would correctly remove Moon and Sun , but it would also incorrectly re-move Mars , Venus and Jupiter since they are also similar to Earth 3 . prove the precision of automatically expanded enti-ty sets by using minimal human negative judgments. The algorithms leverage the fact that set expansion errors are systematically caused by ambiguous seed instances which attract incorrect instances of an unintended entity type. We use dis-tributional similarity and sense feature modeling to identify such unintended entity types in order to quickly clean up errors with minimal manual labor. We show empirical evidence that average R-precision over random entity sets improves by 26% to 51% when given from 5 to 10 manually tagged errors. Both proposed refinement models have li-near time complexity in set size allowing for prac-tical online use in set expansion systems. lows. In the next section we review related work and position our contribution within its landscape. Section 3 presents our task of dynamically model-ing the similarity of a set of words and describes algorithms for refining sets of named entities. The datasets and our evaluation methodology used to perform our experiments are presented in Section 4 and in Section 5 we describe experimental results. Finally, we conclude with some discussion and future work. There is a large body of work for automatically building sets of named entities using various tech-niques including supervised, unsupervised and semi-supervised methods. Supervised techniques use large amounts of training data to detect and classify entities into coar se grained classes such as People , Organizations , and Places (Bunescu and Mooney 2004; Etzioni et al. 2005). On the other hand, unsupervised methods require no training data and rely on approaches such as clustering, targeted patterns and co-o ccurrences to extract sets of entities (Pantel and Lin 2002; Downey et al. 2007). practice since they allow for targeting specific enti-ty classes. These methods rely on a small set of seed examples to extract sets of entities. They ei-ther are based on distributional approaches or em-ploy lexico-syntactic patterns to expand a small set of seeds to a larger set of candidate expansions. Some methods such as (Riloff and Shepherd 1997; Riloff and Jones 1999; Banko et al. 2007;Pa  X  ca 2007a) use lexico-syntactic patterns to expand a set of seeds from web text and query logs. Others such as (Pa  X  ca et al. 2006; Pa  X  ca 2007b; Pa  X  ca and Durme 2008) use distributional approaches. Wang and Cohen (2007) use structural cues in semi-structured text to expand sets of seed elements. In all methods however, expa nsion errors inevitably occur. This paper focuses on the task of post processing any such syst em X  X  expansion output using minimal human judgments in order to re-move expansion errors. formance is a common theme within many infor-mation retrieval and machine learning tasks. One form of user feedback is active learning (Cohn et al. 1994), where one or more classifiers are used to focus human annotation efforts on the most benefi-cial test cases. Active learning has been successful-ly applied to various natural language tasks such as parsing (Tang et al. 2001), POS tagging (Dagan and Engelson 1995) and providing large amounts of annotations for common natural language processing tasks such as word sense disambigua-tion (Banko and Brill 2001). Relevance feedback is another popular feedback paradigm commonly used in information retrieval (Harman 1992), where user feedback (either explicit or implicit) is used to refine the search results of an IR system. Relevance feedback has b een successfully applied to many IR applications including content-based image retrieval (Zhouand Huang 2003) and web search (Vishwa et al. 2005). Within NLP applica-tions relevance feedback has also been used to generate sense tagged examples for WSD tasks (Stevenson et al. 2008), and Question Answering (Negri 2004). Our methods use relevance feedback in the form of negative examples to refine the re-sults of a set expansion system. The set expansion algorithms discussed in Section 2 often produce high quality entity sets, however inevitably errors are intr oduced. Applications re-quiring high precision sets must invest significant-ly in editorial efforts to clean up the sets. Although companies like Yahoo! and Google can afford to routinely support such manual labor, there is a large opportunity to reduce the refinement cost (i.e., number of required human judgments). Gods from Section 1. Key to our approach is the hypothesis that most expansion errors result from some systematic cause . Manual inspection of ex-pansions from GoogleSets and distributional set expansion techniques revealed that most errors are due to the inherent ambiguity of seed terms (such as Neptune in our example) and data sparseness (such as Sonne in our example, a very rare term). The former kind of error is systematic and can be leveraged by an automatic method by assuming that any entity semantically similar to an identified error will also be erroneous. veraging this hypothesis. In the first method, de-scribed in Section 3.1, we use a simple distributional thesaurus and remove all entities which are distributionally similar to manually iden-tified errors. In the second method, described in Section 3.2, we model the semantics of the seeds using distributional features and then dynamically change the feature space according to the manually identified errors and rerank the entities in the set. Both methods rely on the following two observa-tions: a) Many expansion errors are systematically b) Entities which are similar in one sense are Task Outline : Our task is to remove errors from entity sets by using a minimal amount of manual judgments. Incorporating feedback into this process can be done in multiple ways. The most flexible system would allow a judge to iteratively remove as many errors as desired and then have the system automatically remove other errors in each iteration. Because it is intractable to test arbi-trary numbers of manually identified errors in each iteration, we constrain the judge to identify at most one error in each iteration. errors in an entity set, it is also possible to improve expanded sets by using feedback to add new ele-ments to the sets. We consider this task out of scope for this paper. 3.1 Similarity Method (SIM) Our first method directly models observation a ) in the previous section. Following Lin (1998), we model the similarity between entities using the dis-tributional hypothesis, which states that similar terms tend to occur in similar contexts (Harris 1985). A semantic model can be obtained by re-cording the surrounding contexts for each term in a large collection of unstructured text. Methods dif-fer in their definition of a context (e.g., text win-dow or syntactic relations ), or a means to weigh contexts (e.g., frequency , tf-idf , pointwise mutual information ), or ultimately in measuring the simi-larity between two context vectors (e.g., using Euc-lidean distance , Cosine , Dice ). In this paper, we use a text window of size 1, we weigh our contexts using pointwise mutual information, and we use the cosine score to compute the similarity between context vectors (i.e., terms). Section 5.1 describes our source corpus and extraction details. Compu-ting the full similarity matrix for many terms over a very large corpus is computationally intensive. Our specific implementation follows the one pre-sented in (Bayardo et al. 2007). directly used to refine entity sets. Given a manual-ly identified error at each iteration, we automatical-ly remove each entity in the set that is found to be semantically similar to th e error. The similarity threshold was determined by manual inspection and is reported in Section 5.1. we expect that this method will perform poorly on entity sets such as the one presented in our exam-ple of Section 1 where the manual removal of Earth would likely remove correct entities such as Mars , Venus and Jupiter . The method presented in the next section attempts to alleviate this problem. 3.2 Feature Modification Method (FMM) Under the distributional hypothesis, the semantics of a term are captured by the contexts in which it occurs. The Feature Modification Method (FMM), in short, attempts to automatically discover the incorrect contexts of the unintended senses of seed elements and then filte rs out expanded terms whose contexts do not overlap with the other con-texts of the seed elements. ous expanded instance e . In the SIM method of Section 3.1 all set elements that have a feature vec-tor (i.e., context vector) similar to e are removed. The Feature Modification Method (FMM) instead tries to identify the subset of features of the error e which represent the unintended sense of the seed terms S . For example, let S = { Minerva , Neptune , Baccus , Juno , Apollo }. Looking at the contexts of these words in a large corpus, we construct a cen-troid context vector for S by taking a weighted av-erage of the contexts of the seeds in S . In Wikipedia articles we see contexts (i.e., features) such as 4 : Given an erroneous expansion such as e = Earth , we postulate that removing the intersecting fea-tures from Earth  X  X  feature vector and the above feature vector will remove the unintended Planet sense of the seed set caused by the seed element Neptune . The intersecting features that are re-moved are bolded in the above feature vector for S . The similarity between this modified feature vector for S and all entities in the expansion set can be recomputed as described in Section 3.1. Entities with a low similarity score are removed from the expanded set since they are assumed to be part of the unintended semantic class ( Planet in this ex-ample). method is more stable wi th respect to observation b ) in Section 3. We showed that SIM would incor-rectly remove expansions such as Mars , Venus and Jupiter given the erroneous expansion Earth . The FMM method would instead remove the Planet features from the seed feature vectors and the re-maining features would still overlap with Mars , Venus and Jupiter  X  X  Roman God sense. Efficiency : FMM requires online similarity com-putations between centroid vectors and all ele-ments of the expanded set. For large corpora such as Wikipedia articles or the Web, feature vectors are large and storing them in memory and perform-ing similarity computations repeatedly for each editorial judgment is computationally intensive. For example, the size of the feature vector for a single word extracted from Wikipedia can be in the order of a few gigabytes. Storing the feature vec-tors for all candidate expansions and the seed set is inefficient and too slow for an interactive system. The next section proposes a solution that makes this computation very fast, requires little memory, and produces near perfect approximations of the similarity scores. 3.3 Approximating Cosine Similarity There are engineering optimizations that are avail-able that allow us to perform a near perfect approx-imation of the similarity computation from the previous section. The proposed method requires us to only store the shared features between the cen-troid and the words rather than the complete fea-ture vectors, thus reducing our space requirements dramatically. Also, FMM requires us to repeatedly calculate the cosine similarity between a modified centroid feature vector and each candidate expan-sion at each iteration. Without the full context vec-tors of all candidate expansions, computing the exact cosine similarity is impossible. Given, how-ever, the original cosine scores between the seed elements and the candidate expansions before the first refinement iteration as well as the shared fea-tures, we can approximate with very high accuracy the updated cosine score between the modified centroid and each candidate expansion. Our me-thod relies on the fact that features (i.e., contexts) are only ever removed from the original centroid  X  no new features are ever added. seed instances. Given an expansion error e , FMM creates a modified centroid by removing all fea-tures intersecting between e and  X  . Let  X  ' be this modified centroid. FMM requires us to compute the similarity between  X  ' and all candidate expan-sions x as: where i iterates over the feature space. do not have for calculating the exact cosine simi-larity is the norm of x , x . Given that we have the original cosine similarity score, cos ( x ,  X  ) and that we have the shared features between the original centroid  X  and the candidate expansion x we can calculate x as: Combining the two equations, have: In the above equation, the modified cosine score can be considered as an update to the original co-sine score, where the update depends only on the shared features and the original centroid. The above update equation can be used to recalculate the similarity scores without resorting to an expen-sive computation involving complete feature vec-tors. can be approximated instead from only the shared features between the centroid and all instances in the expanded set. We empirically tested this ap-proximation by comparing the cosine scores be-tween the candidate expansions and both the true centroid and the approximated centroid. The aver-age error in cosine score was 9.5E-04  X  7.83E-05 (95% confidence interval). We evaluate our algorithms against manually scraped gold standard sets, which were extracted from Wikipedia to represent a random collection of concepts. Section 4.1 discusses the gold standard sets and the criteria behind their selection. To present a statistically significant view of our results we generated a set of tria ls from gold standard sets to use as seeds for our seed set expansion algo-rithm. Also, in section 4.2 we discuss how we can simulate editorial feedback using our gold standard sets. 4.1 Gold Standard Entity Sets The gold standard sets form an essential part of our evaluation. These sets were chosen to represent a single concept such as Countries and Archbishops of Canterbury . These sets were selected from the List of pages from Wikipedia 5 . We randomly sorted the list of every noun occurring in Wikipe-dia. Then, for each noun we verified whether or not it existed in a Wikipedia list, and if so we ex-tracted this list  X  up to a maximum of 50 lists. If a noun belonged to multiple lis ts, the authors chose the list that seemed most appropriate. Although this does not generate a perfect random sample, diversity is ensured by the random selection of nouns and relevancy is ensured by the author adju-dication. website and they went through a manual cleanup process which included merging variants. . The 50 sets contain on average 208 elements (with a min-imum of 11 and a maximum of 1116 elements) for a total of 10,377 elements. The final gold standard lists contain 50 sets including classical pianists , Spanish provinces , Texas counties , male tennis players , first ladies , cocktails , bottled water brands , and Archbishops of Canterbury 6 . 4.2 Generation of Experimental Trials To provide a statistically significant view of the performance of our algorithm, we created more than 1000 trials as follo ws. For each of the gold standard seed sets, we created 30 random sortings. These 30 random sortings were then used to gener-ate trial seed sets with a maximum size of 20 seeds. 4.3 Simulating User Feedback and Baseline User feedback forms an integral part of our algo-rithm. We used the gold standard sets to judge the candidate expansions. The judged expansions were used to simulate user feedback by marking those candidate expansions that were incorrect. The first candidate expansion that was marked incorrect in each editorial iteration was used as the editor X  X  negative example and was given to the system as an error. at each iteration in the editorial process for our two methods described in Section 3. Our baseline me-thod simply measures the gains obtained by re-moving the first incorrect entry in a candidate expansion set at each iteration. This simulates the process of manually cleaning a set by removing one error at a time. 5.1 Experimental Setup Wikipedia 5 served as the source corpus for our al-gorithms described in Sections 3.1 and 3.2. All articles were POS-tagged using (Brill 1995) and later chunked using a variant of (Abney 1991). Corpus statistics from this processed text were col-lected to build the similarity matrix for the SIM method (Section 3.1) and to extract the features required for the FMM method (Section 3.2). In both cases corpus statistics were extracted over the semi-syntactic contexts (chunks) to approximate term meanings. The minimum similarity thresholds were experimentally set to 0.15 and 0.11 for the SIM and FMM algorithms respectively. 4.2, which consists of a set of seed instances of one of our 50 random semantic classes, was expanded using a variant of the distributional set expansion algorithm from Sarmento et al. (2007). The expan-sions were judged against the gold standard and each candidate expansion was marked as either correct or incorrect. This set of expanded and judged candidate files were used as inputs to the algorithms described in Sections 3.1 and 3.2. Choosing the first candida te expansion that was judged as incorrect simulated our user feedback. This process was repeated for each iteration of the algorithm and results are reported for 10 iterations. judged against the gold standard lists and the per-formance was measured in terms of precision gains over the baseline at various ranks. Precision gain for an algorithm over a baseline is the percentage increase in precision for the same values of para-meters of the algorithm over the baseline. Also, as the size of our gold standard lists vary, we report another commonly used statistic, R-precision. R-precision for any set is the precision at the size of the gold standard set. For example, if a gold stan-dard set contains 20 elements, then R-precision for any set expansion is measured as the precision at rank 20. The average R-precision over each set is then reported. 5.2 Quantitative Analysis Table 1 lists the performance of our baseline algo-rithm (Section 4.3) and our proposed methods SIM and FMM (Sections 3.1 and 3.2) in terms of their R-precision with 95% confidence bounds over 10 iterations of each algorithm. method in terms of R-precision reaching a maxi-mum value of 0.322 after the 10 th iteration. For small numbers of iterations, however, the SIM me-thod outperforms FMM since it is bolder in its re-finements by removing all elements similar to the tagged error. Inspection of FMM results showed that bad instances get ranked lower in early itera-tions but it is only after 4 or 5 iterations that they get pushed passed the similarity threshold (ac-counting for the low marginal increase in precision gain for FMM in the first 4 to 5 iterations). age of 4% increase in performance (13% im-provement after 10 iterations). However both the FMM and the SIM method are able to outperform the baseline method. Using the FMM method one would achieve an average of 17% improvement in R-precision over manually cleaning up the set (32.5% improvement after 10 iterations). Using the SIM method one would achieve an average of 13% improvement in R-precision over manually clean-ing up the set (17.7% improvement after 10 itera-tions). 5.3 Intrinsic Analysis of the SIM Algorithm Figure 1 shows the precision gain of the similarity matrix based algorithm over the baseline algo-rithm. The results are shown for precision at ranks 1, 2, 5, 10, 25, 50 and 100, as well as for R-precision. The results are also shown for the first 10 iterations of the algorithm. ranks and increases in gain throughout the 10 itera-tions. As the number of iterations increases the change in precision gain le vels off. This behavior can be attributed to the fact that we start removing errors from top to bottom and in each iteration the rank of the error candidate provided to the algo-rithm is lower than in the previous iteration. This results in errors which are not similar to any other candidate expansions. These are random errors and the discriminative capacity of this method reduces severely. the similarity matrix algorithm over the baseline algorithm is higher at ranks 1, 2 and 5. Also, the performance increase drops at ranks 50 and 100. This is because low ranks contain candidate expan-sions that are random errors introduced due to data sparsity. Such unsystematic errors are not detecta-ble by the SIM method. 5.4 Intrinsic Analysis of the FMM Algorithm The feature modification method of Section 3.2 shows similar behavior to the SIM method, how-ever as Figure 2 shows, it outperforms SIM me-thod in terms of precision gain for all values of ranks tested. This is because the FMM method is able to achieve fine-grained control over what it removes and what it doesn X  X , as described in Sec-tion 5.2. strated in the R-precision curve. There is a sudden jump in precision gain after the fifth iteration of the algorithm. In the first iterations only few errors are pushed beneath the similarity threshold as cen-troid features intersecting with tagged errors are slowly removed. As the feature vector for the cen-troid gets smaller and sma ller, remaining features look more and more unambiguous to the target entity type and erroneous example have less chance of overlapping with the centroid causing them to be pushed pass the conservative similarity threshold. Different conservative thresholds yielded similar curves. High thresholds yield bad performance since all but the only very prototypi-cal set instances are removed as errors. call as a function of the target coverage of each set. We also directly measured recall at various ranks and FMM outperformed SIM at all ranks and itera-tions. 5.5 Discussion In this paper we proposed two techniques which use user feedback to remove systematic errors in set expansion systems caused by ambiguous seed instances. Inspection of expansion errors yielded other types of errors. expansion sets by noise from various pre-processing steps involved in generating the expan-sions. Such errors cause incorrect contexts (or fea-tures) to be extracted for seed instances and ultimately can cause erroneous expansions to be produced. These errors do not seem to be systemat-ic and are hence not discoverable by our proposed method. ture space can be very large, the difference in simi-larity between a correct candidate expansion and an incorrect expansion can be very small for sparse entities. Previous approaches have suggested re-moving candidate expansions for which too few statistics can be extracted, however at the great cost of recall (and lower R-precision). In this paper we presented two algorithms for im-proving the precision of automatically expanded entity sets by using minimal human negative judgments. We showed that systematic errors which arise from the sema ntic ambiguity inherent in seed instances can be leveraged to automatically refine entity sets. We proposed two techniques: SIM which boldly removes instances that are dis-tributionally similar to errors, and FMM which more conservatively removes features from the seed set representing its unintended (ambiguous) concept in order to rank lower potential errors. precision over random entity sets improves by 26% to 51% when given from 5 to 10 manually tagged errors. These results were reported by testing the refinement algorithms on a set of 50 randomly chosen entity sets expanded using a state of the art expansion algorithm. Given very small amounts of manual judgments, the SIM method outperformed FMM (up to 4 manual judgments). FMM outper-formed the SIM method given more than 6 manual judgments. Both proposed refinement models have linear time complexity in set size allowing for practical online use in set expansion systems. moving erroneous entities from expanded entity sets. A complimentary way to improve perfor-mance would be to investigate the addition of rele-vant candidate expansions that are not already in the initial expansion. We are currently investigat-ing extensions to FMM that can efficiently add new candidate expansions to the set by computing the similarity between modified centroids and all terms occurring in a large body of text. ings of this work to a priori remove ambiguous seed instances (or their ambiguous contexts) before running the initial expansion algorithm. It is our hope that most of the errors identified in this work could be automatically discovered without any manual judgments.

