 The lack of sufficient labeled Web pages in many languages, especially for those uncommonly used ones, presents a great challenge to traditional supervised classification methods to achieve satisfactory Web page classification performance. To address this, we propose a novel Nonnegative Matrix Tri-factorization (NMTF) based Dual Knowledge Transfer (DKT) approach for cross-language Web page classification, which is based on the following two important observations. First, we observe that Web pages for a same topic from dif-ferent languages usually share some common semantic pat-terns, though in different representation forms. Second, we also observe that the associations between word clusters and Web page classes are a more reliable carrier than raw words to transfer knowledge across languages. With these recogni-tions, we attempt to transfer knowledge from the auxiliary language, in which abundant labeled Web pages are avail-able, to target languages, in which we want classify Web pages, through two different paths: word cluster approxima-tions and the associations between word clusters and Web page classes. Due to the reinforcement between these two different knowledge transfer paths, our approach can achieve better classification accuracy. We evaluate the proposed ap-proach in extensive experiments using a real world cross-language Web page data set. Promising results demonstrate the effectiveness of our approach that is consistent with our theoretical analyses.
 H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing Algorithms, Experimentation Cross-language Classification, Knowledge Transfer, Nonneg-ative Matrix Factorization
With the rocketing growth of Internet in recent years, an ever-increasing number of Web pages are now available in many different languages. As of April 2011, over 131.1 mil-lion web sites are actively in operation 1 , with billions of Web pages created in almost all human languages. As a result, cross-language information retrieval becomes unprecedent-edly important for organizing and mining information stored in Web pages in various languages.

A potential problem in categorizing Web pages, especially for those written in uncommonly used languages, is the lack of sufficient labeled data. This prevents us from training an effective classification model, which usually requires a large amount of labeled data. Statistically speaking, the more labeled training data one can obtain, the more accurate and robust the classification model is. Fortunately, due to many reasons, there exists a lot of labeled Web pages in several most commonly used languages, such as English. It is hence useful and intriguing to make use of these labeled Web pages in one language, called as auxiliary language ,to help to classify Web pages in another language, called as target language . This problem is called as cross-language Web page classification [15]. In this paper, we explore this important, yet challenging, problem by proposing a novel Nonnegative Matrix Tri-factorization (NMTF) based Dual Knowledge Transfer (DKT) approach. One of the most widely used strategy in cross-language Web page (text) classification is using language translation [15, 16, 18 X 21, 23]. One can either translate test data into the auxiliary language, or translate training data into the target language, and then train and classify the resulted data in one single language. Although this straightforward method may be feasible, it suffe rs from a number of critical problems that impede its practical use [15, 19, 20]. In this subsection, we examine the challenges in cross-language Web page classification and seek opportunities to overcome them, which motivate our approach.
 Cultural discrepancy. The first difficulty in cross-language Web page classification is due to cultural discrepancies, which heavily impact the classification performance in spite of a perfect translation [15, 19]. Given that a language is the way of expression of a cultural and socially homogenous community, Web pages from a same category but different languages may concern very different topics. For example, we consider Web pages that report sports news in France (in French) and in USA (in English). While the former typ-ically pays more attention to soccer, rugby and cricket, the latter is more interested in basketball and American foot-ball. From machine learning perspective of view, this cor-responds to the situation where the training data and test data are drawn from different distributions, which makes it a challenge for traditional supervised and semi-supervised classification algorithms to achieve satisfactory Web pages classification performance.

Moreover, even we have sufficiently many labeled data in the target language, due to the differences of culture and so-cial focus, they might not cover all the Web page categories. Consider that, for example [16], the English speakers tend to contribute more to some topics than their Czech coun-terparts ( e.g ., to discuss  X  X ondon X  more than  X  X rague X ), so that, having only data in English, we may expect them to do poorly at identifying topics like  X  X rague X . Czech speak-ers, on the other hand, often talk about  X  X rague X , so that by leveraging Czech data, we may expect to improve on de-tecting topic  X  X rague X  in English Web pages.

To overcome this problem, instead of simply combining the data, we consider to transfer labeling information con-tained in Web pages in the auxiliary language to those in the target language [17]. Our approach is based on the observa-tion that Web pages in different languages from a same cat-egory often share the same semantic information, although they are in different representation forms, e.g ., French words and English words [15]. Therefore, we may abstract the prior knowledge in the auxiliary language into semantic patterns, and make use of them to help to classify Web pages in the target language. To transfer knowledge across languages, the most natural carrier is the basic linguistic representa-tion unit  X  words. We give an example to illustrate the usefulness of knowledge transfer by words in Web page clas-sifications as in Figure 1.

Given a data set with four Web pages ( W 1, W 2, W 3 and W 4) as shown in Figure 1(a), we represent them as a word-document matrix as shown in Figure 1(b). Because (a) A synthetic data set of 4 Web pages in target language. (c) Transformed representation of the data set by incorpo-rating the prior knowledge learned from an auxiliary lan-guage, which leads to the meaningful clustering results. Figure 1: An illustrative example to demonstrate the usefulness of leveraging the prior knowledge learned from an auxiliary language when clustering Web pages in a target language. in practice we usually do not have labels for Web pages in the target language, we clusters them (the rows of the data matrix) based on cosine similarity, which results in two clus-ters, ( W 1and W 3) as a cluster and ( W 2and W 4) as a clus-ter. This result, however, is not meaningful. If we use the learned knowledge from the auxiliary language to guide this clustering process, we can transform the data matrix with 3 semantic features as in Figure 1(c). That is,  X  X lustering X  X nd  X  X lassification X  belong to  X  X earning X ,  X  X llumination X  and  X  X ex-ture X  belong to  X  X raphics X , and  X  X ebpage X  and  X  X yperlink X  belong to  X  X eb X . Clustering on this new transformed data matrix, we obtain ( W 1and W 2) as a cluster and ( W 3and W 4) as a cluster. This is a very meaningful result, because the former is concerned with  X  X nformation retrieval X , while the latter is interested in  X  X ision X . More theoretical analysis for this example will be given later in Section 3.2.
In our approach, the first path to transfer knowledge across languages is by word cluster approximations, which is schemat-ically shown by the red lines in Figure 2.
 Translation ambiguity. In the process of language trans-lation, the ambiguities introduced by dictionaries is another important challenge in cross-language Web page classifica-tion. For example, the word  X   X  (reading materi-als) X  in Chinese Web pages could be reasonably translated as  X  X extbooks X ,  X  X equired reading list X ,  X  X eference X  and so on. Since the linguistic habits in expressing a concept are different in different languages, the phrases for a same con-cept may have different probabilities in different languages. Therefore, transferring knowledge by the raw words some-times are not reliable. However, the concept behind the phrases may have the same effect to indicate the class labels of the Web pages in different languages. In the same exam-ple, a Web page is more probable to be course-related if it contains the concept of  X  X eading materials X . In other words, only the concept behind raw words are stable in indicat-ing taxonomy, and the association between word clusters and Web page categories is independent of languages [25]. Therefore, we use it as the second bridge to transfer knowl-edge across different languages, which is illustrated by the green paths in Figure 2.
 Data diversity. One more challenge in cross-language Web page classification is the data diversity. As illustrated in Figure 2, although we may have a lot of training Web pages in one language, usually not all of them are fully labeled. Similarly, even the labeled resources in the target language are scarce, we may still have a small number of Web pages in this language labeled by limited human effort. As a result, we can not rigidly assume the Web pages in the auxiliary language are always labeled and the Web pages in the target language are not labeled at all. Namely, model flexibility must be addressed to handle real world cross-language Web page classification tasks. Taking into account the three challenges in cross-language Web page classification, through a novel joint NMTF frame-work, we abstract the prior knowledge contained in Web pages in the auxiliary language, including both labeling in-formation by human efforts and latent language structures, in two forms represented by the two factor matrices F a and S of NMTF respectively, and then transfer them to the target language to guide the classification therein. The whole idea is summarized in Figure 2. Because we employ a two-way knowledge transfer, we call our proposed approach as Dual Knowledge Transfer (DKT) approach, which is interesting from a number of perspectives as following.
In this section, we first briefly review NMTF and reveal how it transfers knowledge between data and features within a same data set, from which we will develop our approach.
Traditional Nonnegative Matrix Factorization (NMF) aims to find two nonnegative matrices whose product can well ap-proximate the original nonnegative data matrix X  X  R p  X  n i.e ., X  X  FG T ,where F  X  R p  X  k + and G  X  R n  X  k + .The columns of X are data points and the rows of X are observa-tions. An appropriate objective of NMF is to minimize [12]: where  X  denotes the Frobenius norm of a matrix. Accord-ing to [5], NMF defined in Eq. (1) corresponds to simultane-ous K -means clustering of the rows (features) and columns (data points) of X ,where F can be considered as the clus-tering indictions for features and G can be considered as the clustering indications for data points. Because co-clustering the both sides of an input data matrix makes use of the in-terrelatedness between the data points and features, NMF based co-clustering methods usually report superior perfor-mance [5, 7]. In the context of Web page classification, the intrinsic linguistic structures of a language is described by X for a set of Web pages, and the prior knowledge by human efforts could be encoded in G , both of which are transformed into F as word (feature) clustering patterns through Eq. (1).
Because two-factor NMF in Eq. (1) is restrictive, which often gives a rather poor low-rank matrix approximation, we may introduce one more factor S  X  R k 1  X  k 2 + to absorb the different scales of X , F and G , which leads to NMTF [7] minimizing the following objective:
J NMTF = X  X  FSG T 2 s.t. F  X  0 ,G  X  0 ,S  X  0 , (2) where F  X  R p  X  k 1 + and G  X  R n  X  k 2 + . S provides increased degrees of freedom such that the low-rank matrix represen-tation remains accurate while F gives row clusters and G gives column clusters. Most importantly, S is a condensed view of X [13] and represents the associations between word clusters and Web page clusters [25].

Obviously, F and S convey two types of transformed knowl-edge, which we exactly expect to transfer across languages as outlined in Section 1. However, Eqs. (1 X 2) are both designed for one single data set, while in cross-language Web page classification we have two separate data sets, one in auxil-iary language and the other in target language. Moreover, both of them are unsupervised where prior labeling knowl-edge is not utilized. Therefore, we further develop NMTF in Eq. (2) and propose a novel joint NMTF approach to trans-fer knowledge across languages to address the challenges in cross-language Web page classification to achieve improved classification performance.
In this section, we develop a novel NMTF based DKT approach for cross-language Web page classification, which transfers knowledge from the auxiliary language to the tar-get one by two paths: (1) word cluster approximations and (2) the associations between word clusters and Web page classes. An efficient algorithm to solve the proposed objec-tive with rigorous convergence proof is presented.
For a cross-language Web page classification problem, we have two Web page data sets, one in the auxiliary language X a = x a Web page in the auxiliary language and x i t represents that in the target language. Thus X a and X t can be seen as the document-word co-occurrence matrices of the auxiliary data and target data respectively, or their tf-idf normalized counterparts. We assume that the both data sets are using a same vocabulary with m words: if the vocabularies differ, we may simply pad zeros in the feature vectors and re-express them under a same unified vocabulary so that the indices of the feature vectors from the both data sets correspond to the same word. Let V  X  R m  X  m be a diagonal matrix with V ii ) =1ifthe i -th word occurs in the both data sets, and V ii ) =0otherwise.

Typically a large amount of Web pages in the auxiliary language are manually labeled, which can be described by belongs to the k -th class, and Y a ( ik ) = 0 otherwise. Some-times, though not always, we also have a limited number of labeled Web pages in target language. We similarly describe the k -th class, and Y t ( ik ) = 0 otherwise. Again, we assume that the two data sets share a same set of classes. If not, we can pad the zero columns to Y a or Y t , or the both, such that the column indices of the both matrices correspond to the same classes. Our goal is to predict labels for the unlabeled Web pages in the target data set.
 Throughout this paper, we denote the real number set as R and the nonnegative real number set as R + . The element at the i -th row and j -th column of a matrix M is denoted as M ij ) . Frequently used notations are summarized in Table 1.
GiventheWebpagedata X a in auxiliary language and their labels Y a , adopting the idea of NMTF, we may factorize X a by minimizing the following objective [25]:
J where tr (  X  ) denote the trace of a matrix. In Eq. (3),  X &gt; 0 is a parameter that determines to which extent we enforce the prior labeling knowledge in the auxiliary language, i.e ., G if x i a is labeled by the i -th row of Y a ,and C a ( ii ) erwise. Note that, if C = I , all the Web pages in auxiliary language are completely labeled and specified by Y a . Transfer knowledge via word cluster approximations by F a and F t . Solving the optimization problem in Eq. (3), F a and S a contains information of the data in the auxiliary language which is to be transferred to those in the target language. We achieve this by minimizing: where F a  X  is obtained by solving Eq. (3). The second term in Eq. (4) acts same as that in Eq. (3), which enforces labeling information in the target domain if it is available. Here,  X &gt; 0, and C t  X  R n t  X  n t is a diagonal matrix whose entry C 1 if Web page x i t is labeled by the i -th row of Y t ,and C 0 otherwise. When the labels for all the Web pages in target language are not available, we have C t =0 n t  X  n t ,whichisa zero matrix. The key part is the third term. It enforces the constraint that word clusters in X t is approximately close to F a , learned from X a . The extent of this approximation is determined by  X &gt; 0. As a result, the label information contained G a of X a is transferred to the label assignments G t in X t via the semantic word structures F a and F t ,which is schematically shown red paths in Figure 2.

In order to demonstrate the usefulness of knowledge trans-fer via word cluster approximations, we give more theoretical analysis on the example in Figure 1. Suppose the knowledge in auxiliary language is certain, we may set  X  in Eq. (4) as  X  . In order to see the real effect of prior knowledge to im-prove classification performance, we temporarily ignore the training information in the target language by minimizing: which is identical to the following problem [5,7]: By the equivalence between K -means clustering and prin-cipal component analysis (PCA) [4, 24], the clustering of Eq. (6) uses X T t F a F T a X t as the pairwise similarity, whereas K -means clustering uses X T t X t as the pairwise similarity. For the example in Figure 1, we have and K -means clustering will produce (W1, W3) as a cluster and (W2, W4) as another cluster.

Now, with the knowledge F a learned from auxiliary lan-guage, we have where we assume we already learned F a from auxiliary lan-guage, which is Clearly, using the similarity in Eq. (8), K -means cluster-ing will generate (W1, W2) as a cluster and (W3, W4) as another cluster, which is more meaningful as in Figure 1(c).
We may see more directly how knowledge in the word space from auxiliary language is transformed into the Web page space in target language. Let the square root of the semi-definite positive matrix be P : F a F T a = P T P .We have X T t F a F T a X t =( PX ) T ( PX )whichmeanswecluster the Web pages using the transformed data  X  X t = PX =
F
For the example in Figure 1, we have
It is obvious that on this transformed data, W1 and W2 will be clustered into one cluster, W3 and W4 will be clus-tered into another cluster. This analysis shows how the knowledge in the word space learned from auxiliary language is transformed into the Web page space in target language. Transfer knowledge via the association between the word clusters and Web page classes by S . As discussed earlier in Section 1, compared to words, the association be-tween word clusters and Web pages classes are more reliable to convey semantic relationships across different languages. Formally, we achieve this by minimizing: where S a  X  is obtained by solving Eq. (3). As a result, S learned from the auxiliary data set, is used as supervision to classify the target data. Namely, S a  X  bridges the source and target languages such that prior labeling knowledge can be transferred from the former to the latter, which is schemat-ically shown green paths in Figure 2.
 Our optimization objective. Finally, we may combine the three optimization problems Eqs. (3 X 11) into a joint op-timization objective to minimize: J +  X  tr ( G a  X  Y a ) T C a ( G a  X  Y a ) + tr  X  ( G t  X  Y t ) T C t ( G t  X  Y t )+  X  ( F t  X  F a ) s.t. F a  X  0 ,G a  X  0 ,S  X  0 ,F t  X  0 ,G t  X  0 . In this formulation S is shared in the two matrix factoriza-tions for both auxiliary and target data, which is used as a bridge to transfer knowledge between them. In addition, through the constraint in the last term, the two data sets are connected by F a and F t .

Note that, the last term of Eq. (4) only applies to the com-mon words of X a and X t , which are encoded by V .When the auxiliary data set and the target data set do not share any word, i.e ., V =0 m  X  m is a zero matrix, there will be no knowledge transfer through word cluster approximation path. Similarly, if the auxiliary data set and the target data set do not share common classes, there will be no knowledge transformation in the optimization problem of Eq. (11), be-cause it is decoupled into two independent subproblems, one for auxiliary data and the other for target data. However, these two cases rarely happen simultaneously. As a result, our model is flexible and can always transfer knowledge in Eq. (12) through the either path, or the both.

Upon solving Eq. (12), the class label for the x i t in target language is determined by Solving Eq. (12) and assign labels to the unlabeled Web pages in the target language using Eq. (13), our cross-domain Web page classification approach is proposed. Because Eq. (12) transfers knowledge in two different paths, we call it as Dual Knowledge Transfer (DKT) approach.
In the following, we derive the solution to Eq. (12) and present an alternating scheme to optimize the objective J Specifically, we will optimize one variable while fixing the rest variables. The procedure repeats until convergence. First we expand the objective in Eq. (12) as follows,
J ( F a ,G a ,S,F t ,G t )= tr  X  2 X T a F a SG T a +  X F T t VF t  X  2  X F T t VF a +  X F T a VF a , where constant terms are discarded.
 Computation of F a . For the constraint F a  X  0, following standard theory of constrained optimization, we introduce the Lagrangian multiplier U  X  R m  X  k 1 , thus the Lagrangian function is Setting  X  X  ( F a ) / X  X  a =0,weobtain
U =  X  2 X a G a S T +2 F a SG T a G a S T  X  2  X V F t +2  X V F
Using Karush-Kuhn-Tucker condition U ( ij ) F a ( ij ) =0,we get which leads to the following updating formula: Computation of G a , S , F t and G t . Following the same derivations in Eqs. (15 X 18), we obtain the updating rules for the rest variables of J DKT as following:
In summary, we present the iterative multiplicative up-dating algorithm of optimizing Eq. (12) in Algorithm 1.
In this section, we will investigate the convergence of Al-gorithm 1. We use the auxiliary function approach [12] to prove the convergence of the algorithm.

Lemma 1. [12] Z ( h, h ) is an auxiliary function of F ( h ) if the conditions Z ( h, h )  X  F ( h ) and Z ( h, h )= F ( h ) are satisfied. [12] If Z is an auxiliary function for F ,then F is non-increasing under the update h ( t +1) =argmin h Z ( h, h ) . Algorithm 1: Algorithm to solve J DKT in Eq. (12).
Lemma 2. [7] For any matrices A  X  R n  X  n + , B  X  R k  X  k S  X  R n  X  k + and S  X  R n  X  k + ,and A and B are symmetric, the following inequality holds Theorem 1. Write J in Eq. (14) w.r.t. F a , we have then the following function H F a ,F a =  X  2 is an auxiliary function for J ( F a ) .Furthermore,itisa convex function in F a and its global minimum is Proof. According to Lemma 2, we have Because z  X  1+log z,  X  z&gt; 0, we have tr F T t VF a  X  Summing over all the bounds in Eqs. (27 X 30), we can obtain H ( F a ,F a ), which clearly satisfies (1) H ( F a ,F a ) and (2) H ( F a ,F a )= J ( F a ).

Then, fixing F a , we minimize H ( F a ,F a ).  X  X  ( F a ,F a ) and the Hessian matrix of H ( F a ,F a )is  X  H ( F a ,F a )
F which is a diagonal matrix with positive diagonal elements. Therefore H ( F a ,F a ) is a convex function of F a ,andwe can obtain the global minimum of H ( F a ,F a ) by setting  X  X  ( F a ,F a ) / X  X  a ( ij ) = 0 and solving for F a ,fromwhich we get Eq. (26). This completes the proof of Theorem 1. Theorem 2. Using Algorithm 1 to update F a , J ( F a ) in Eq. (24) will monotonically decreases.
 Proof. By Lemma 1 and Theorem 1, we can get that J F a 0 = H F a 0 ,F a 0  X  H F a 1 ,F a 0  X  J F a 1 ... Therefore J ( F is monotonically decreasing.
 Theorem 3. Using Algorithm 1 to update G a , S , F t and G , the respective objectives will monotonically decrease. Theorem 3 can be similarly proved as Theorems (1 X 2). Because J in Eq. (12) is obviously lower bounded by 0, Algorithm 1 is guaranteed to converge by Theorems (2 X 3).
In this section, we review several prior researches that are mostly related to our work, including transfer learning, cross-language classification and NMTF.
 Transfer learning. From machine learning perspective of view, our work belongs to the topic of transfer learning (also called as domain adaption in some research papers), which deals with the case where training and test data are ob-tained from different resources thereby in different distribu-tions [13 X 16, 18 X 21, 23, 25]. For a comprehensive survey of transfer learning, we refer readers to [17].
 Cross-language classification. Cross-language Web page and document classification has attracted increased atten-tion in recent years due to its importance in information re-trieval. Bel et al . [1] studied English-Spanish cross-language classification problem. Two scenarios are considered in their work. One scenario assumes to have training documents in both languages, and the other is to learn a model from the text in one language and classify the data in another lan-guage by translation. Our work follows the first strategy. [16] employed a general probabilistic English-Czech dictionary to translate Czech text into English and then classified Czech Table 2: Description of testing data sets. English is used as auxiliary language in all the testing data sets.

Data Target D1 German 3,500 0 D2 German 3,500 1,000 D3 French 3,500 0 D4 French 3,500 1,000 D5 Japanese 3,500 0
D6 Japanese 3,500 1,000 documents using the classifier built on English training data. Ling et al . [15] classify Chinese Web pages using English data source by utilizing the information bottleneck theory. Other cross-language text classification researches include [23] (Chinese-English), [19] (English-Spanish-French), [20] (English-Chinese-French), etc ., to be mentioned. NMTF. NMF is a useful learning method to approximate a nonnegative input data matrix by the product of factor matrices [11,12], which has been applied to solve many real world problems including dimensionality reduction, pattern recognition, clustering and classification [3,5 X 8,13,14,22,25]. Recently, Ding et al . extended NMF [7] to NMTF and ex-plored its relationships to K -means/spectral clustering [5,7]. Due to its mathematical elegance and encouraging empiri-cal results, NMTF method is further developed to address a variety of aspects of unsupervised and semi-supervised learn-ing [3,8,13,14,22,25], among which [13] and [25] are closely related to our work. The former investigated cross-domain sentiment classification, which transfers knowledge by shar-ing information of word clusters. This is similar to our ap-proach to transfer knowledge through word cluster approx-imations. While they dealt with two separate tasks of ma-trix factorizations, first on the source domain and then on the target domain, our approach optimizes a combined and collaborative objective, which leads to extra values in clas-sification as shown later in our experimental evaluations. In addition, they assume there exist no label information in tar-get domain, which restricts its capability to solve real world problems. The latter considered the cross-domain document classification via transferring knowledge by the associations between word clusters and document classes, which, how-ever, did not use the important information contained in words as both our approach and [13]. Again, they restrict that the data in the source domain are completely labeled while no data labeling information in the target domain. In summary, our approach has very close relationships to [13] and [25], but enjoys the advantages of both of them, with additional flexibilities to allow training data appearing in various forms.
In this section, we evaluate the proposed Dual Knowl-edge Transfer (DKT) approach in cross-language Web page classification, and compare it with several state-of-the-art supervised, semi-supervised and transfer learning classifiers.
We conduct our empirical evaluations on a publicly avail-respect to a wide range of parameters settings. able multi-lingual Web page data set  X  cross lingual sen-timent corpus 2 [18]. This data set contains about 800,000 web pages from Amazon web site for product reviews in four languages: English, German, French and Japanese. The crawled part of the corpus contains more than 4 millions of Web pages in the three languages other than English from amazon.{de|fr|co.jp} . Besides the original Web pages, all the Web pages in German, French and Japanese are trans-lated into English. The corpus is extended with English Web pages provided by Blitzer et al . [2]. All the Web pages in the corpus are divided into three categories upon the prod-uct they describe: books, DVDs and music.

In our experiments, we randomly pick up 5,000 Web pages from each language. Same as [18], we use English as auxil-iary language and the rest three as target languages sep-arately. Therefore we end up with three language pairs for testing: English-Germen, English-French, and English-Japanese. Because in real world applications not all the Web pages in auxiliary language are labeled, we randomly pick up 70% of English Web pages for each class as labeled data. On the other hand, because in real world applications the Web pages in target language are mostly unlabeled, we sim-ulate two different cases: (1) no labeled Web pages in target languages and (2) we randomly pick up 20% Web pages from each class as labeled data in the concerned target language. As a result, we end up with six testing data sets, which are summarized in Table 2. For each testing data set, our task is to classify the unlabeled Web pages in the corresponding target language.
We compare the proposed DKT approach against the su-pervised learning method (1) Support Vector Machine (SVM) method, and the semi-supervised learning method (2) Trans-ductive SVM (TSVM) method [9] as baselines. We also compare to the two closely related cross-domain learning methods based on NMTF: (3) Knowledge Transformation by Words (KTW) method [13], (4) Matrix Tri-factorization based classification framework (MTrick) [25]; and a very re-cent cross-language Web classification method using infor-mation bottleneck theory (IB) [15].
SVM and TSVM methods can use either the labeled data in target language or the labeled data in both auxiliary and http://www.webis.de/research/corpora/ target language. We therefore refer to SVM T, TSVM T as the former case, and SVM ST, TSVM ST as the latter case. For the latter case, the data from the both auxil-iary and target languages are used in a homogeneous way. This is equivalent to assume the Web pages from different laguanges are drawn from a same distribution, which, how-ever, is not true in reality. Following previous works, for the both methods, we train one-versus-others classifiers, with the fixed regularization parameter C = 1. Gaussian kernel is used ( i.e ., K ( x i , x j )=exp  X   X  x i  X  x j 2 )where  X  is set as 1 /m . SVM and TSVM are implemented by SVM light [10].
The parameters of KTW and MTrick are set as optimal following their original works [13,25]. The iteration number of IB method is set as 100.

For our approach, due to the nature of our optimization objective in Eq. (12), we always use S , i.e ., the associations between word clusters and Web page classes, to transfer knowledge. In order to test the flexibility of our approach, we consider two different cases of our approach for using words to transfer knowledge: (1) not use words transfer de-noted as  X  X KT ( S only) X , i.e ., set  X  = 0 in Eq. (12); and (2) use words transfer denoted as  X  X KT X . Upon some prelimi-nary test, for our appraoch we set the tradeoff parameters  X  =  X  =1,and  X  =1 . 5, the number of word clusters is set to same as Web page classes k 1 = k 2 = 3, the error threshold in Algorithm 1 to determine convergence is set  X  =10  X  11 and the maximum iterating number is 100.
Two widely used classification performance metrics in sta-tistical learning and information retrieval are used in our experiments: macro-average precision and F 1 -measure. Let f be the function which maps from document d to its true class label c = f ( d ), and h be the function which maps from document d to its prediction label c = h ( d )givenbythe classifiers. The macro-average precision P and recall R are defined as:
The F 1 measure is the harmonic mean of precision and recall, which is defined as follows:
Table 3 presents the classification performances measured by macro-average precision and F 1 score of the compared methods on six different test data sets. From the results we have the following observations. First, the proposed DKT approach consistently outperforms the other compared methods. DKT ( S only) method is always worse than DKT approach, which confirms the usefulness of the knowledge transfer path by word cluster approximations. Second, from the experimental results of SVM ST and TSVM ST meth-ods, we can see that considering Web pages from different languages as homogenous typically leads to unsatisfactory classification performance. Because the cross-domain meth-ods, including ours, are generally better than these two methods, knowledge transfer from the auxiliary language to the target one is important to improve the classification performance. Third, our DKT approach is able to trans-fer knowledge in two paths, i.e ., word cluster approxima-tions and the associations between word clusters and Web page classes, thus it achieves encouraging classification per-formance on all the six test data sets. In contrast, KTW method can only transfer knowledge through word cluster approximations, and MTrick method only transfers knowl-edge through the associations between word clusters and Web page classes, their performances are generally not as good as other transfer learning methods. Last, but not least, our approach is able to exploit labeling information in both auxiliary and target data, whereas KTW method and MTrick method cannot benefit from labeling informa-tion in target domain, and SVM T method and TSVM T method cannot work with labeling information in auxiliary data. The more labeled data in target domain, the bet-ter classification performance our approach can achieve. In summary, all the above observations demonstrate the effec-tiveness of the proposed DKT approach in cross-language Web page classification.
The proposed DKT approach has three parameters  X  ,  X  ,  X  in Eq. (12). Although it is tedious to seek an optimal com-bination of them, we can demonstrate that the performance of our DKT approach is not sensitive when the parameters are sampled in some value ranges. We bound the parameters in the ranges of 1  X   X   X  10, 1  X   X   X  10 and 0 . 5  X   X   X  3 upon preliminary tests and evaluate them on data set D2 as it has labeled Web pages in both auxiliary language and target language.

From Figure 3 we can see that the average performance of all the parameter settings is almost the same as the results from the default parameters. Morever, the variance of all the parameter settings is small. It shows that the performance of our approach is stable with respect to the parameters in a considerably large range.
Because our DKT approach employs an iterative algo-rithm, an important issue is its convergence property. In Section 3.4, we have already theoretically proved the con-vergence of the algorithm. Here, we empirically check the convergence property of the proposed iterative algorithm. The classification accuracy in each iteration to classify D1 test data set is shown in Figure 4(a), where the x-axis rep-resents the number of iterations, and the y-axis denote the Figure 4: Number of iterations vs . the performance of the proposed approach measured by macro-average precision and Objective Value. macro-average precision. From the figure, it be can seen that DKT approach converges within about 10 iterations, which indicates that our algorithm is fast. In addition, the objective values of our algorithm in each iteration are plot-ted in Figure 4(b), which shows that the objective value of our algorithm keep to decrease along with iterative process, which is in accordance with our theoretical analysis.
In this paper, we proposed a novel NMTF based DKT approach for cross-language Web page classification. Our approach adopts the idea of transfer learning to pass knowl-edge across languages, instead of simply combining the Web page data from different languages. By carefully examine the cross-language Web page classification problem, we ob-served that common semantic patterns usually exist in Web pages for a same topic from different languages. More-over, we also observed that the associations between word clusters and Web page classes are more reliable to transfer knowledge than using raw words. With these recognitions, our approach is designed to transfer knowledge across lan-guages through two different paths: word cluster approx-imations and the associations between word clusters and Web pages classes. With this enhanced knowledge trans-fer, our approach is able to address the main challenges in cross-language Web page classification: cultural discrepan-cies, translation ambiguities and data diversity. Extensive experiments using a real world cross-language Web page data set demonstrated encouraging results from a number of aspects that validate our approach.
 This research was supported by NSF-CNS 0923494, NSF-IIS 1041637, NSF-CNS 1035913. demonstrate the advantage of the proposed DKT approach. Data Metrics Compared methods D1 Precision  X  0.682  X  0.689 0.673 0.695 0.691 0.697 0.716 D2 Precision 0.679 0.692 0.682 0.701 0.675 0.699 0.703 0.718 0.730 D3 Precision  X  0.670  X  0.675 0.663 0.682 0.680 0.683 0.701 D4 Precision 0.663 0.682 0.670 0.687 0.669 0.681 0.690 0.702 0.718 D5 Precision  X  0.662  X  0.668 0.656 0.674 0.672 0.679 0.688 D6 Precision 0.651 0.676 0.663 0.676 0.658 0.672 0.681 0.695 0.707 [1] N. Bel, C. Koster, and M. Villegas. Cross-lingual text [2] J. Blitzer, R. McDonald, and F. Pereira. Domain [3] G. Chen, F. Wang, and C. Zhang. Collaborative [4] C. Ding and X. He. K-means clustering via principal [5] C. Ding, X. He, and H. Simon. On the equivalence of [6] C.Ding,T.Li,andM.Jordan.Convexand [7] C. Ding, T. Li, W. Peng, and H. Park. Orthogonal [8] Q. Gu and J. Zhou. Co-clustering on manifolds. In [9] T. Joachims. Transductive inference for text [10] T. Joachims. SVMLight: Support Vector Machine. [11] D. Lee and H. Seung. Learning the parts of objects by [12] D. Lee and H. Seung. Algorithms for non-negative [13] T. Li, V. Sindhwani, C. Ding, and Y. Zhang.
 [14] T. Li, V. Sindhwani, C. Ding, and Y. Zhang. Bridging [15] X.Ling,G.Xue,W.Dai,Y.Jiang,Q.Yang,and [16] J. Olsson, D. Oard, and J. Haji X  c. Cross-language text [17] S. Pan and Q. Yang. A survey on transfer learning. [18] P. Prettenhofer and B. Stein. Cross-language text [19] G. Ram  X   X rez-de-la Rosa, M. Montes-y G  X  omez, [20] L. Shi, R. Mihalcea, and M. Tian. Cross language text [21] X. Wan. Co-training for cross-lingual sentiment [22] F. Wang, T. Li, and C. Zhang. Semi-supervised [23] K. Wu and B. Lu. A refinement framework for cross [24] H. Zha, X. He, C. Ding, H. Simon, and M. Gu. [25] F. Zhuang, P. Luo, H. Xiong, Q. He, Y. Xiong, and
