 Many online communities use tags  X  community selected words or phrases  X  to help people find what they desire. The quality of tags varies widely, from tags that capture a key dimension of an entity to those that are profane, useless, or unintelligible. Tagging systems must often select a sub-set of available tags to display to users due to limited screen space. Because users often spread tags they have seen, se-lecting good tags not only improves an individual X  X  view of tags, it also encourages them to create better tags in the fu-ture. We explore implicit (behavioral) and explicit (rating) mechanisms for determining tag quality. Based on 102,056 tag ratings and survey responses collected from 1,039 users over 100 days, we offer simple suggestions to designers of online communities to improve the quality of tags seen by their users.
 H.5.3 [ Information Interfaces and Presentation ]: Group and Organization Interfaces X  Collaborative computing ; H.5.2 [ Information Interfaces and Presentation ]: User Inter-faces Design, Experimentation, Human Factors tagging, moderation, user interfaces Member contributions power many online communities. Users upload images to flickr, bookmark pages on del.icio.us, and author encyclopedia entries at Wikipedia. These member-maintained communities harness their users X  effort to amass collections of millions of pictures, articles, and bookmarks. Navigating these large collections can be difficult. How should a user on flickr go about finding a freely available, Copyright 2007 ACM 978-1-59593-845-9/07/0011 ... $ 5.00. high quality image of a marine iguana among the 65 million uploaded photos? Computer vision algorithms cannot yet do a good job of selecting photos based on the wide variety of image features that are of interest to people [5].
Tags  X  words or phrases that describe items  X  have emerged as a flexible, rich means to navigate these corpuses. Tagging systems draw on contributions from ordinary users to out-scale expert maintained taxonomies. For example, in 200 years of existence the Library of Congress has applied their expert-maintained taxonomy to 20 million books 1 . In con-trast, in just four years, flickr X  X  users have applied their ad hoc tagging vocabulary to over 25 million photographs [16].
The resulting system is powerful. The search for  X  X arine iguana X  in the Creative Commons section of Flickr returns 19 photos  X  several strikingly good  X  free for use with at-tribution. The only returned photo not of an iguana shows the house of Senator John Warner of Virginia, who was once married to Elizabeth Taylor, who appeared in a 1964 movie called  X  X n the Trail of the Iguana X . Every other photo is found in the search because of a tag added by a Flickr user.
Tagging systems scale well, but contributions from non-experts may reduce the quality of a system X  X  vocabulary of tags. For example, in the online community we study in this paper, users find that only 21% of the tags are worthy of gen-eral display. Low quality tags cluttering an interface may be useless or worse, they may be misleading, inappropriate, or offensive. Good tags, however, can make a system better by tying entities to one another to enhance browsing or search, or may serve as a source of descriptive information.
The lack of quality control on displayed tags is particu-larly dangerous given the self-reinforcing nature of tagging vocabularies. Conformity theory predicts that the tags that users see from other users will influence the tags that they in turn assign [2]. Conformity has been observed in practice. Golder and Huberman [12] and Cattuto [6] independently show that tagging vocabularies reach a stable equilbrium: once a tag becomes popular it remains popular. Sen et al. show that users tend to create tags resembling other tags they see in the community [17]. Systems that can select good tags not only improve the experience of the user who sees the tags, they also encourage those users to create good tags in return.

Selecting the right tags for display can be challenging for a number of reasons. Unlike data rich entities such as web pages and wikipedia articles, tags usually consist of a single unstructured word. As we mentioned earlier, tag quality can be quite poor. Moreover, tagging systems do not have much http://www.loc.gov/about/reports room for error. Due to limited screen space, many systems can only display a few tags from among the many users have applied. With all these challenges, how should del.icio.us select the five tags it displays for the for the website digg from among the 10,688 unique tags users have applied to it?
Our goal in this research is to understand methods for selecting high quality tags for display while suppressing low quality tags. We explore several lightweight interfaces for collecting member feedback about tags, and examine which interfaces lead to the richest metadata for understanding the quality of individual tags. We then develop several ap-proaches for predicting tag quality based on either implicit system usage data or on explicit member feedback. We structure our paper around five research questions. Our first two research questions explore the effects of the rating interface on the tags displayed in a system. Rating interfaces that evaluate tag quality based on explicit ratings can only be effective for those tags that have been rated. Our first research question examines the relationship be-tween rating interface and rating quantity:
RQ1: Which rating interfaces lead to the most ratings?
Increased rating quantity is only valuable to the extent that it improves the tags displayed by a tagging system. Our second research question examines this relationship directly.
RQ2: Which tag rating interfaces should designers implement to better select the tags they show to users?
Of course, tag ratings do not inherently determine which tags are displayed -a system must implement a tag selec-tion method drawing on both tag ratings and non-ratings tag data. Our remaining research questions explore three fundamental signals a tag selection method may use:
RQ3: Can we determine the tags a user wants to see based on other users X  behavior?
RQ4: Can we determine the tags a user wants to see based on a user X  X  own ratings?
RQ5: Can we determine the tags a user wants to see based on other users X  ratings?
The rest of the paper is organized as follows. In section 2 we summarize existing research related to tag selection methods. Section 3 presents our tag rating implementation and describes our experimental setup. Section 4 discusses RQ1, which relates the tag rating interface to rating quan-tity. Sections 5, 6, and 7 explore RQ3, RQ4, and RQ5, which investigate specific tag selection methods. We conclude in section 8 with a return to RQ2, design implications, discus-sion, and several ideas for future research. Although public bookmarking systems such as Fab [3], Knowledge Pump [11], and Pharos [4] have been available since the 1990 X  X , Millen et al. point to tagging as a key reason current social bookmarking systems have enjoyed greater success [15]. As tagging systems became more pop-ular, Shirky [18] was among the first of many bloggers and technology critics who argued that traditional controlled on-tologies improperly describe the way in which information is now organized. Our work furthers early studies of tagging communities by analyzing the quality of tags created in an online community.
 In early academic research on tagging communities, Mac-Gregor and McCullogh [14] explore the relative merits of controlled versus evolved vocabularies, arguing that evolved ontologies engage users but lack the precision of their con-trolled counterparts. Golder and Huberman indicate that the proportions of tags applied to a given item in del.icio.us appear to stabilize over time, and suggest that community members may be influenced by what they see [12]. Cat-tuto furthers their work by presenting a generative model for users X  tagging that predicts the rate at which both particu-lar users and entire communities re-use tags [6]. In earlier work, we show that the tags a user sees influence the tags they create themselves [17]. We also classify tags as gen-erally factual, subjective, or personal (intended for the tag creator themselves), and find that users generally prefer fac-tual tags over subjective tags and strongly dislike personal tags. Our research extends earlier work describing how users choose tags to the novel problem of how systems might se-lect tags to show a user from among a large collection of tags that have been applied by other users.

Several researchers have studied moderation in online com-munities. Cosley et al. find that  X  X iki-like X  systems that immediately display user contributions lead to more contri-bution than systems that require members to review contri-butions before they are displayed [7]. In other work, Cosley et al. show that intelligent task routing can be used to help users find tasks they might complete to improve the sys-tem [8]. Lampe and Resnick analyze the moderation system utilized on the online forum slashdot 2 [13]. They find that although the community perceives that forum moderations are generally fair, comments that are assigned low scores, or posted late in a conversation are often overlooked by moder-ators. Arnt and Zilberstein explore machine learning tech-niques for predicting moderation scores in online forums [1]. Our research differs from the general work on moderation of contributions in that we focus on a type of contribution (tags), and investigate ways in which user interfaces may improve moderation.
As a platform for our analyses, we used the MovieLens 3 movie recommendation system. MovieLens members can tag movies, and use tags contributed by others in the com-munity to find or evaluate movies. Since we introduced tagging features to MovieLens in January 2006, MovieLens users have created 52,814 tag applications resulting in 9,055 distinct tags. (A tag is a particular word or phrase used in a tagging system. A tag application is the result of associ-ating a tag with a system entity.) 2,344 users have applied at least one tag (13.5% of active users). Further details of MovieLens and the MovieLens tagging system can be found in [17].

In order to study explicit tag feedback, we introduced tag ratings to the MovieLens community. Our design of a tag rating system was based on two guiding principles: users should be able to rate tags with a single click, and the ratings interface should require minimal screen space. Since a star-based rating system requires too much space, we selected http://www.slashdot.org name anonymized Figure 1: Tags as they appear on the Movie-Lens search results screen, next to the experimental thumbs up and thumbs down ratings widgets. a thumbs up / thumbs down rating system, similar to that used in many commercial applications such as Amazon 4 , TiVo 5 , and reddit 6 .

While many commercial applications incorporate both thumbs up and thumbs down ratings, several only employ one or the other. For example, BoardGameGeek originally em-ployed thumbs up and down moderation, but shifted to only thumbs up moderation to  X  X ake it harder for people to  X  X ang up X  X  and  X  X educe hurt feelings. X  7 In sites such as YouTube, users provide positive feedback about items by marking items as  X  X avorites. X  Other sites allow solely neg-ative feedback. Users of Google Video, for example, may mark tags as  X  X pam X  but have no means of providing posi-tive feedback.

To investigate the utility of different rating interfaces, we randomly split users into four experimental groups repre-senting possible combinations of positive (thumbs up) and negative (thumbs down) ratings widgets: 1. Control group C was not shown any tag rating widgets. 2. Group U was only shown the thumbs up tag rating 3. Group D was only shown the thumbs down tag rating 4. Group UD was shown both the thumbs up and thumbs
The tag rating interface appeared alongside all tag ap-plications appearing on the MovieLens search results page (Figure 1) and movie details page (Figure 2). Search results pages displayed up to three tags per movie, while the movie details page displayed up to twenty tags. MovieLens ran-domly selected and ordered tags for display from among the tags applied to a movie.

To help motivate users to provide tag ratings, we imple-mented simple user interface responses to rating actions. Tags shift to the front of a movie X  X  tag list in response to a positive rating, and tags move to the end of the movie details page list and are hidden from the search results page in response to a negative rating. We incorporated AJAX javascript controls to enable fast, lightweight rating inter-actions. We enabled the tag rating features on January 21, 2007 and collected data for one hundred days.

While the thumbs up and down ratings provided coarse data about tag quality, we also wanted a  X  X old standard X  data set for evaluating our techniques for selecting tags to
Amazon.com uses thumb ratings for meta-reviewing.
The TiVo digital video recorder collects user feedback through a thumb-based interface
The news aggregation service reddit.com allows users to click an up-arrow or a down-arrow for each article. http://www.boardgamegeek.com/thread/156510 Figure 2: Tags and the experimental ratings widgets as they appear on the movie details screen.
 Figure 3: We asked users to rate tags for five movies on a one-to-five scale. We instructed them that MovieLens would only choose to show them tags rated 3, 4, or 5 stars. display. To this end, we emailed 2,531 active MovieLens users and asked them to complete an online survey in which they provide feedback on tag quality. Users were asked to rate up to twenty tags applied to five movies on a five star scale. The five selected movies consisted of: Figure 3 shows an example screen from the survey. As a point of reference, users were instructed that MovieLens would only display tags rated 3,4, and 5 stars. 577 users responded to the survey (22.8% response rate) and rated at least one tag application. 546 users rated tags for all five movies. We gave users the option of continuing to rate tags after they completed rating tags for their first five movies.
Users provided 74,987 one-to-five ratings. Two users pro-vided more than 1,000 ratings, while 253 users provided 100 or more ratings. The distribution of the one-to-five ratings is shown in Table 1. Users deemed 38% of rated tag appli-cations worthy of display. The average tag rating was 2.17.
Other than the tag applications themselves, we base our analyses on three types of data. We study whether patterns in aggregate user behavior, such as searches for tags, indicate Table 1: Distribution of one-to-five tag ratings by rating value. The average overall rating is 2.17 tag rating value 1 2 3 4 5 percentage of ratings 46% 14% 21% 10% 7% that tags should be displayed or hidden. We use thumbs up and thumbs down tag ratings to evaluate the relative utility of different rating interfaces, and explore their predictive power for tag selection methods. The one to five star survey ratings serves as a  X  X old standard X  for evaluating selection methods and to better understand users liking for tags.
We have argued that up/down ratings interfaces are preva-lent in modern systems, and we believe they are an appropri-ate light-weight interface for soliciting feedback about tags (and other ubiquitous system entities). One decision that a designer of such a ratings system faces is whether to include both a positive and a negative ratings widget, or if one or the other alone will provide sufficent data to build accurate models of quality. Designers of commercial systems are di-vided on this issue, even within the same domain: the social news site reddit 8 has both up and down arrows, whereas the social news site digg 9 has a  X  X igg X  button in a highly visi-ble place on the interface, with a less visible  X  X ury X  button elsewhere. In this section, we investigate this decision by examining data collected in a field study of several variants of a tag ratings system in MovieLens.

Methods for selecting tags to display depend on data  X  either implicit data about user behavior, or data collected explicitly from users. Many explicit ratings-based systems find collecting sufficient data a challenge. Thus, a key ques-tion is which interfaces attract the most ratings. Our first research question is:
RQ1: Which rating interfaces lead to the most ratings?
Table 2 shows a summary of up/down ratings applied dur-ing the experimental period by users in the different groups. In total, 460 users (7.3% of active users during the time pe-riod) generated a total of 27,069 tag ratings. 72% of tag ratings occured from the search results page, while 28% oc-cured on the movie details page. A small number of users supplied the majority of tag ratings. For example, the top rater provided 10.4% of all tag ratings (2,823), and the top 20% of raters provided 93.5% of all tag ratings (25,322) 10 51.5% of raters applied 3 or fewer ratings.

Our first finding is that the presence of different ratings interfaces leads to significant differences in ratings contri-butions. The descriptive statistics from Table 2 give an in-tuitive feel for the results. Users in Group UD rated more times (13,841) than users in Group D (11,903) or in Group U (1,325). Also, more users in Group UD rated one or more times (14.2%) as compared with users in Group D (9.7%) or Group U (5.1%).

These differences are statistically significant. Because the distribution of work per-user is strongly skewed, we must www.reddit.com www.digg.com
This distribution is common in member-maintained com-munities. For instance, in Wikipedia the most profilic 10% of users generate 80% of all edits [19]. apply non-parametric statistical tests to determine differ-ences. To measure the differences in per-user ratings be-tween groups, we examine the ratings of all users who log in to the system during the experimental period. We test for differences using a one-way Wilcoxon test, and report sig-nificance based on the p-value resulting from a Chi-Square approximation. Users in Group UD rated more than users in Group D ( n = 3181, means 8.65 vs. 7.53, ChiSquare = 14 . 64, DF = 1, p &lt; 0 . 001), and they also rated more than users in Group U ( n = 3176, means 8.65 vs. 0.84, ChiSquare = 75 . 85, DF = 1, p &lt; 0 . 001). Users in Group D rated more than users in Group U ( n = 3157, means 7.53 vs. 0.84, ChiSquare = 25 . 04, DF = 1, p &lt; 0 . 001).
We also find that more users from Group UD contributed one or more tag ratings than from either of the other exper-imental groups. To test for significance, we conduct a likeli-hood ratio Chi-Square test. We find that users in Group UD were more likely to rate one or more tags than users in Group D (14.19% vs. 9.68%, ChiSquare = 15 . 47, p &lt; 0 . 001), and they were also more likely to rate than users in Group U (14.19% vs. 5.08%, ChiSquare = 78 . 45, p &lt; 0 . 001). Users in Group D were more likely to rate one or more tags than users in Group U (9.68% vs. 5.08%, ChiSquare = 24 . 84, p &lt; 0 . 001).

Although on average, users in group D generated more negative ratings per-user as compared with users in group UD (means 7.53 vs. 6.13), this difference is not statistically significant using a Wilcoxon test ( n = 3181, ChiSquare = 0 . 05, df = 1, p = 0 . 82). The difference in the means might be attributed to the presence of the most prolific rater in Group D, who singlehandedly rated 2,823 times.

Interestingly, we do find that users are more likely to rate tags positively in the presence of a thumbs-down rating wid-get. This is demonstrated by the fact that users in Group UD gave a thumbs up to an average of 2.5 tag applications, while users in Group U gave a thumbs up to just 0.8 tag applications. This difference is statistically significant, us-ing a Wilcoxon test ( n = 3176, ChiSquare = 36 . 24, df = 1, p &lt; 0 . 001).

We thought the additional up ratings in the UD group might be due to tag  X  X hurn X  introduced by negative tag ratings (negatively rated tags disappear and the user is pre-sented with additional tags to rate). To test this hypothesis, we measured the tag-specific probabilities that a displayed tag would be rated positively across both Groups U and UD. We then calculated each group X  X  expected number of up rat-ings based on their displayed tags. We find that the number of up ratings in Group UD is 1.5 times the expected number, while the U group is half the expected number. Therefore, we cannot attribute the extra positive ratings in the Group UD to tag churn. Apparently there is something about the presence of both ratings in the interface that leads to more up ratings.

Overall, we find that the interface containing both up and down ratings widgets led to the greatest levels of contribu-tions. We later return to the impact of these contributions on tag selection methods. However, the general message is that greater contributions leads to greater coverage, and therefore more successful interfaces for displaying high qual-ity tags.

Our second finding is that users contributed more negative ratings than positive ratings, especially among users who rated more than three tags. Across all three experimental groups, we collected more than four times as many negative ratings as positive ratings (21,717 vs. 5,352).

The difference in the quantity of down ratings versus up ratings is awkward to statistically verify. As stated above, Group UD rated more positively than Group U. We might therefore speculate that this difference is due to some aspect of the up-only interface which makes it less attractive to provide ratings. Therefore, we cannot fairly factor Group U into the comparison between up and down ratings. We are left with a paired Wilcoxon test among users of Group UD.
When looking only at Group UD, we find that a major-ity (119 vs. 101) of users actually rated more positively than negatively. The remainder (7 users) rated equal num-bers up and down. There is no statistical difference across these users in per-user up ratings vs. down ratings using a Wilcoxon test ( n = 1600, W = 12, p = 0 . 99).

However, when we look only at the 108 users in Group UD who have rated more than three times, we find significance ( n = 108, W = 522, p = 0 . 045), accounting for the large overall difference in Table 2 (9,814 down ratings vs. 4,027 up ratings). We might state that committed raters contribute more down ratings than up ratings. However, as we discuss in section 8, this result is likely the result of the overall quality of tags in the MovieLens system, rather than the result of an innate preference for rating things down.
While  X  X ormal X  tag raters produce similar quantities of positive and negative ratings,  X  X ower X  tag raters strongly favor negative ratings. Differences in the number of positive or negative ratings may impact the effectiveness of certain tag selection methods. For instance, if users rate more nega-tively than positively, systems might be able to identify bad tags more easily than good ones. RQ2 directly explores the relationship between tagging interface and selection quality in the context of specific tag selection methods. We now move on to explore selection methods, but will return to RQ2 in section 8.
Systems such as flickr and del.icio.us have attracted mil-lions of users and generated vast amounts of behavioral data about the tags users created, searched for, and browsed. Ide-ally, designers of existing online tagging communities might estimate tag quality by analyzing existing behavioral data without having to collect explicit feedback about tags. In this section, we form predictions based on implicit measures of tag quality, such as the number of users who have applied a tag. We test those predictions against the gold standard of the user surveys in an effort to answer:
RQ3: Can we determine the tags a user wants to see based on other users X  behavior? Figure 4: Average tag quality grouped by the num-ber of tag applications, number of users who applied the tag, and number of users who searched for the tag.

Perhaps users apply higher quality tags more often than low quality tags. If so, then the number of times a tag has been applied might be a reasonable proxy for its quality. A tagging system might wish to preferentially display tags ap-plied many times, or hide tags that have been applied fewer than some minimum number of times. Motivated by this possibility, we examine the num-apps tag selection method which predicts tag quality based on the number of times a tag has been applied.

Figure 4 shows the average tag survey rating (a five star scale) grouped by the number of tag applications. Tags ap-plied once have the lowest average rating (1.89), tags applied 16-31 times have the highest average rating (2.53), and the tags applied most often (256 or more times) have an average rating of 2.14.

We might expect the most often applied tags to be the highest rated, but this is not the case -users gave low average ratings for several of the most frequently applied tags. This may be attributable to an abundunce of  X  X ersonal X  tags intended solely for their creator; four of the five most rated tags applied 256 or more times are personal:  X  X vd X ,  X  X wn X ,  X  X een at the cinema X , and  X  X ric X  X  dvds. X 
Personal tags appear to reduce the accuracy of the previ-ous tag selection method. Sen et al. showed that personal tags are generally used frequently by only a few users [17]. Systems might show fewer personal tags by normalizing each user X  X  influence over the selection method. We now explore the num-users selection method, which predicts tag quality based on the number of users who have applied each tag.
Figure 4 shows the average tag rating grouped by the number of users who have applied the tag. The average Figure 5: Precision of selection methods based on other users X  behavior top-n ranked survey ratings. Ratings of 3, 4, and 5 stars are viewed as desirable. rating of the tag used by the most users (32 or more) reaches 2.63 out of 5 stars. A clear upward trend is apparent: tags applied by more users are rated higher than tags applied by fewer users.

Perhaps users search for, and click on, good tags more frequently than bad tags. We analyzed 19,458 tag search and click events, and found that a few users who search for the same tag many times bias the number of searches per tag. For this reason, the num-searches selection method normalizes each user X  X  weight by focusing on the number of users who search for a tag.

Figure 4 shows average tag rating grouped by the number of users who clicked on each tag. As with the number of users who applied each tag, we see a gentle upward trend. Tags searched for by 16-31 users have an average rating of 2.58, while those searched for by 32 or more users have an average rating of 2.42. The small decline in average rating can be accounted for by two  X  X ersonal X  tags many users clicked on:  X  X een more than once X , and  X  X rlend X  X  dvds. X 
Unlike the first two selection methods, the search-based selection method can only generate predictions for tags users have searched for. The selection method achieves a predic-tion coverage of 70.2% of the tag survey ratings.
Average rating is just one possible measure of tag selec-tion quality. Perhaps users wish to minimize the number of low-quality tags displayed. Inspired by this proposition, we ranked all survey ratings outputted by the previous three selection methods, and measured the precision (fraction of 3, 4, or 5 star ratings) at different thresholds. Figure 5 shows each selection method X  X  precision at different thresh-olds. We used a logarithmic scale on the x-axis to faciliate later comparisons with methods having lower coverage.
Num-searches performs well at the high end: 87% of the 128 tag survey ratings for the most searched-for tags were rated three, four or five stars. Num-users also performs con-sistently: more than half of the top ranked 16,384 survey ratings are rated three or higher. Num-apps, on the other hand, performs erratically.

The top-ranked precision numbers may seem higher than expected based on our earlier analysis using average survey ratings. For instance, while num-users achieves a precision of 56% for the top 8,192 survey ratings, the average of these ratings is only 2.63 (below the display threshold). This dif-ference can be explained by the distribution of survey rat-ings: while ratings are divided relatively evenly among three, Figure 6: Mapping between thumbs up/down tag ratings and one-to-five survey ratings. Thumbs down ratings are mostly rated 1, while thumbs up ratings are evenly split between 3,4 and 5. four, and five stars, there are far more one star ratings than two star ratings. Low survey ratings affect survey averages disproportionately more than high survey ratings.
We find selection methods normalizing each user X  X  influ-ence, such as num-users and num-search, to be more robust than methods which can be biased by a few power users (such as num-apps). Although the three selection methods appear to correlate with user liking for tags, none of them seem to be individually sufficient. Even in num-users, the overall top performer, users approve of barely half of the top ranked 22% of survey ratings (n=16384).

Although most real-world tagging systems do not have access to survey data as a gold standard, we believe that our use of them is justified. We hope that our conclusions will provide general insights into the way in which users evaluate tags. We also believe that many large tagging sites would eagerly conduct a small survey if it improved the quality of displayed tags.
In the last section we showed that tag selection methods such as num-users and num-searches based on implicit be-havior have some predictive power. As a more direct alterna-tive, systems may use thumbs up or thumbs down feedback to select the tags a user wants to see. Research question 4 explores tag selection methods based solely on a user X  X  own ratings:
RQ4: Can we determine the tags a user wants to see based on a user X  X  own ratings?
In MovieLens, users rate specific applications of tags to movies. For instance, suppose Sally rates the tag  X  X ombies X  on the movie  X 28 Days Later X  positively. Our first ratings-based tag selection method, user-rating , simply concludes that users like the tag applications they rate thumbs up and dislike the tag applications they rate thumbs down.
We begin by better understanding what Sally X  X  thumbs up rating for  X  X ombies X  on  X 28 Days Later X  tells us about her survey rating (survey ratings use a higher-precision five star scale). Figure 6 shows the number of thumbs up and down ratings that were mapped to each survey rating. The meaning of thumbs up and down are clearly distinct. Down ratings map to a one or two star rating, while up ratings map to a rating of three stars or higher. While 80% of thumbs down ratings received a survey rating of 1 star, the thumbs Figure 7: Precision of selection method X  X  based on user X  X  own ratings. Ratings of 3, 4, and 5 stars are viewed as desirable. up ratings where equally likely to be rated 3,4, or 5 stars. The less extreme mapping for thumbs up ratings may be due to our annotation of the one-to-five survey scale: we told MovieLens users that 3, 4, and 5 star ratings would be displayed.

Figure 7 shows the precision of the application-based se-lection method. While the method achieves a precision of 81% for the top 64 survey ratings (those survey ratings also rated a thumbs up), it can only generate predictions for tag applications with an associated thumb rating. The tag se-lection method can only predict for one out of two hundred survey ratings, leading to a coverage of 0.5%.
 Suppose that Sally rates the tag  X  X ombies X  on both  X 28 Days Later X  and  X  X awn of the Dead. X  If Sally rates tags  X  X ombies X  consistently, her second rating of zombie may be wasting her valuable effort. To reduce Sally X  X  effort a system might assume that she will rate the tag  X  X ombies X  positively for all movies.

We evaluate this broader interpretation of tag ratings by measuring the effectiveness of a user X  X  average tag rating as a tag selection method ( user-avg ). We encoded thumbs up ratings as +1 and thumbs down ratings as -1 (we use this encoding throughout the rest of the paper). Tags with more than one user rating are weighted more heavily by adding one  X  X eutrally X  rated (0) tag to every average calculation (this is equivalent to using an uniform beta prior [10]). For instance, two positive and one negative rating will result in an adjusted rating of 0+1+1  X  1 4 = 0 . 25.

The average-based selection method achieves a precision of almost 70% for the top 256 survey ratings. The coverage improves by a factor of four to 1.9%, but still remains quite low.

In summary, a user X  X  tag ratings serve as strong predictors of their liking for particular tags. This precision comes at the expense of coverage -even if we extend user ratings of tag applications to apply to all occurrences of the tag we only cover 1.9% of survey ratings. Our results also indicate that systems may want users to rate tags instead of tag applications. The intra-rater reliability of the one to five star survey data also supports this conclusion; the average variance for a user X  X  rating of the same tag is only 0.175 on a five point scale.
Sally is not the only MovieLens user who likes the tag  X  X ombies. X  In fact, 81% of all thumb ratings for  X  X ombies X  Table 3: Top 10 most controversial tags based on thumb ratings as measured by expected entropy (Appendix A). in MovieLens are thumbs up ratings. If several raters agree on a tag X  X  quality, a system may be able to conclude that most users have similar opinions of the tag, increasing the tag selection method X  X  coverage to all users. In this section we explore how systems might select tags for display based on aggregate user thumb ratings.

RQ5: Can we determine the tags a user wants to see based on other users X  ratings?
To capture users X  aggregate tag opinions, we considered the global-avg selection method which ranks tags by their overall average rating across all users. As in the user-avg, we smoothed average ratings by adding a single neutral rating. Figure 9 shows that the precision for the highest ranked tags is slightly lower than selection methods based on a user X  X  own ratings. However, the decreased precision is offest by a 49x improvement in coverage to 93%.

Users obviously don X  X  agree on all tags. Table 3 lists the most controversial tags as measured by expected entropy (Appendix A). Controversial tags appeared to contain in-formation that is already displayed in MovieLens (comedy, sci-fi, steven spielberg), subjective (classic, stylized, quirky), or about a controversial topic (nudity -full frontal).
We wondered whether certain types of tags lead to differ-ent levels of agreement across users. We discovered a differ-ence in agreement for  X  X ood X  and  X  X ad X  tags. We divided tags into those with one-to-five means above and below the 3 star display threshold, and measured the average variance across all users X  ratings for the tag. While the average vari-ance for low-rated tags was 0.72, the average variance for highly-rated tags was 1.15. Users clearly agreed more about bad tags than good tags.

Perhaps Sally provided MovieLens X  X  fifth positive rating for  X  X ombies X  and no users had rated the tag negatively. This high level of initial agreement offers a promising signal for tag quality that can be easily implemented by system designers.

The previous four ratings for  X  X ombie X  may have all come from the same user (we know from section 6 that a user will generally rate a tag consistently). To be sure that the initial consecutive ratings are independent confirmation, a designer may want to require that they come from different users.

Based on these scenarios, we now examine the consec-apps selection method, which ranks tags based on the num-ber of initial identical ratings, and the similar consec-users , which requires that the ratings come from different users. Figure 8: Percent of remaining ratings that, after an initial number of identical ratings, remain positive or negative.
 Figure 9: Precision of selection method X  X  based on other users X  ratings. Ratings of 3, 4, and 5 stars are viewed as desirable.

We begin with an intuitive analysis of repeated ratings that translates easily into system implementation. Figure 8 shows the percent of ratings that, after a certain number of initial consecutive positive or negative ratings, remain positive or negative. The graph presents both the count by-application and by-user metrics. Both metrics for negative consecutive ratings serve as accurate predictors. After a tag receives four consecutive thumbs down ratings (regardless of user), 90% of the remaining ratings will be thumbs down. On the other hand, even after a tag receives strictly thumbs up ratings by 9 different users, only 71% of the remaining tags are positive.

Both consec-apps and consec-users perform similarly on the rank / precision analysis that we used to evaluate prior selection methods. Consec-users yields a precision of 67% for the 64 top-ranked tags, compared to 64% for consec-apps. Both methods achieve over 50% accuracy for tags that were initially rated thumbs up.

In general, we find selection methods based on aggre-gate ratings to achieve slightly lower precision than meth-ods based on a user X  X  own ratings, but with much higher coverage. The precision of the aggregate rating methods seems similar to the precision of methods based on aggre-gate implicit behavior. This does not mean that ratings-Table 4: Tag Selection Method X  X  Coverage of Tag Survey Ratings Per Experimental Group.
 based methods do not provide additional benefit. If the im-plicit and explicit selection methods excel at different types of tags, systems may draw on both methods to construct a more accurate hybrid selection method. We investigate one such method in the next section.
In the previous three sections we presented seven differ-ent tag selection methods. Ensemble learning methods that combine the outputs from different  X  X xperts X  can lead to improved overall performance [9]. Inspired by these meth-ods, we evaluated the predictive power of a simple ensem-ble method that averages the percentile rankings produced by six of the previous tag selection methods (we call this method hybrid ) 11 We did not include num-apps in the en-semble due to its poor performance.

Table 5 shows a detailed comparison of the precision of all the selection methods we evaluated, including hybrid. Al-though hybrid yielded lower precision than other methods for the top 256 survey ratings, it out-performed the other methods beyond this threshold. The performance of the hybrid selection method is probably more desirable to sys-tem designers: systems will want to show more tags than those associated with the top 256 (0.3%) survey ratings. The performance of this simple hybrid suggests that more sophisticated ensemble learners should be able to provide substantially improved performance.

Now that we have presented our selection methods, we return to research question two:
RQ2: Which tag rating interfaces should designers implement to better select the tags they show to users?
The ratings-based selection methods can only be effective to the extent they have ratings data. Table 4 compares the coverage of each selection method when restricted to only a group X  X  thumb and survey ratings. The group with both up and down ratings achieves the greatest coverage of the three groups.

In many applications, a method with medium coverage but excellent precision may be more desirable than one with full coverage and low precision. To directly test the effects of different interfaces on selection quality, we constructed a different hybrid method from each of the four experimental groups using only the group X  X  thumb ratings and used the selection method to predict the group X  X  survey ratings.
Figure 10 presents the precision results of the hybrid se-lection method for each of the experimental groups. The Our ensemble method can be viewed as a Bayesian Voting Method [9] in which all ensemble members have similar ac-curacy and percentile rankings correspond to the probability that a tag is rated positively. Since the probability that a survey rating is rated positively overall is 0.38 (reasonably close to 0.5), this has some emperical justification. Figure 10: Precision of the hybrid selection method for each of the four experimental groups. high precision of the top 128 ranked ratings for the U group (dotted line with triangle markers) may suggest that thumbs up ratings uncover the best tags, but the poor precision of lower rankings implies that positive ratings do not weed out mediocre and bad tags. The control group without ratings (the solid line with a circle marker) performs well at upper ranks but poorer at middle and lower rankings. The groups with just down ratings (solid line with a square marker) and both up and down ratings (solid line with a diamond marker) both perform quite well at medium and lower rankings.
As stated earlier, precision at lower rankings (e.g. larger n) is more valuable to systems that want to show a large fraction of tags. The down interface, and the interface that used both up and down ratings should be particularly valu-able for such systems.

Our results translate into four simple guidelines for de-signers of tagging systems: 1. Systems that support positive ratings should also support negative ratings. We found that users generate more positive ratings when they could also rate negatively. We also showed that increased rating quantity leads to improved coverage for many tag selection methods. Finally, selection methods using negative ratings, and both positive and negative ratings, performed better than those that just use positive ratings or no ratings at the lower rank-ings (larger n ) critical to real-world systems. This finding is in direct conflict with the policies on many sites that avoid negative ratings for fear that they will drive away users. Our data do not provide tools for directly comparing the bene-fits of negative ratings for decision-making with the costs of hurting users X  feelings. We believe that most systems should support negative ratings for objects such as tags, even if they do not support negative ratings for people . 2. Use tag selection methods that normalize each user X  X  influence. We found that tag selection methods such as the number of searches or applications per tag are skewed by a small group of  X  X ower X  users. Tag selection methods that normalize by user, such as the number of users who applied a tag perform better than those that do not. 3. Incorporate both behavioral and rating-based tag selection methods. We found both behavioral and ratings-based tag selection methods to be effective. Table 5 compares precision results across different types of selection methods. Methods based on a user X  X  own ratings achieved high precision but very low coverage. Methods based on ag-gregate community behavior and aggregate community rat-ings performed similarly. Selecting tags based on the num-ber of users who searched for them was particularly precise for those tags ranked highest (87% for the top 128 survey ratings). The hybrid method performs well at the lower rankings (larger n ) important for real-world systems. 4. Assume that a user X  X  rating for a particular tag application extends to other applications of the tag. We found that users generally rate the same tag consistently, regardless of the item it was applied to. For example, 91% of thumb ratings for the same tag, by the same user, but for different items were identical. Although systems may want to allow users to rate individual tag applications, they should interpret a rating for a tag application as strong ev-idence for a user X  X  general feeling like for a tag.
We also discovered two surprising characteristics of tag rating in MovieLens. First, users tend to agree more about  X  X ad X  tags than  X  X ood X  tags. We saw evidence for this in tag selection methods (consecutive negative ratings were much more predictive than positive ones), survey results (inter-user agreement was higher among  X  X ad X  tags), and general use of the five-star survey scale (there were 7x more one star ratings than two star ratings, but distribution on the high end of the scale was even). Second, in the UD group, although more tag raters rated positively than negatively, a few power users caused the group to generate twice as many negative ratings as positive ones. These results may be specific to communities such as MovieLens that have many low quality tags.
 Our research presents several opportunities for future work. Although we focus our analysis of tag selection methods to three basic types of signals (implicit user behavior, a user X  X  own ratings, aggregate user ratings), more complex tech-niques may lead to improved accuracy. Our goal was to present system designers with intuitive tag selection meth-ods that they may easily implement, and to offer both prac-titioners and researchers insights into several fundamental signals of tag quality. We leave the exploration of more complex algorithms, such as those based on machine learn-ing techniques, as future research.

Second, we would like to validate our techniques using other tagging applications. Surveyers felt that the quality of most MovieLens tags was low enough that they should not be displayed. It would be particularly useful to validate our results in a domain that has a higher ratio of good to bad tags.

Finally, we would like to know whether the design prin-ciples we present generalize to other types of community-contributed content such as images, articles and bookmarks. As the size of member-maintained communities grows, com-munities will require better tools to separate good contribu-tions from bad ones. We would like to thank Dan Frankowski, Shyong Lam, Al Mamunar Rashid, Jilin Chen, and S. Andrew Sheppard for their help in planning the tag rating studies, Sara Drenner for her initial suggestion of rating tags, the rest of Grouplens for their discussion and input, and our MovieLens users for their exuberant tag ratings and survey responses.
This work is funded in part by National Science Founda-tion, grants IIS 03-24851 and IIS 05-34420.
Entropy measures the amount of uncertainty associated with a random variable. Entropy is calculated by summing over all possible outcomes x 1 ...x n : In our application, we wish to measure the amount of dis-agreement in the thumb ratings for a particular tag. Thus, suppose, a tag has 2 positive votes and 3 negative votes. The entropy of the ratings for the tag would be Now suppose that the tag has 20 positive ratings and 30 neg-ative ratings. Since the ratio of positive to negative ratings is the same, the entropy will be the same.

But do we really expect the amount of disagreement to be the same in both cases? In the first example, it is easy to imagine that the  X  X rue X  underlying ratio of positive to negative ratings is 0.2, 0.5, or 0.7. On the other hand, we have a fair degree of confidence in the entropy measurement for the second example due to its fifty ratings.

A Bayesian approach to entropy calculation treats the up to down ratio itself as a random variable. If we assume that all up/down ratios for tags are equally likely (this is not far from actual reality), then, given u up ratings and d down ratings, the probability of a particular ratio q being f is:
Based on this probability calculation, we can calculate the expected entropy of the ratings by combining a  X  X eighted average X  of the entropies for all possible ratios f weighted by the probability of each f as calculated in equation 3: Using this formulation, we get an expected entropy of 0.84 for the example with five votes and 0.96 for the example with fifty votes, which seems more reasonable.
