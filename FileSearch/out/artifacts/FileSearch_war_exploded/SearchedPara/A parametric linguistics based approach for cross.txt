 1. Introduction
The World Wide Web has become a valuable knowledge resource and the amount of information written in different languages and the number of non-English speaking users has been increasing tremendously.
Although 68% of web content is still in English, this trend is changing and a large number of web pages are being written in almost every popular language. This increasing diversity has created a tremendous wealth of multi-lingual resources on the Web. However, one of the major difficulties in accessing these knowledge sources is the language barrier. Moreover, these resources are highly unstructured, contain a diverse and dynamic collection of contexts, and are used by individuals who have little knowledge of search techniques.
While some of the existing search engines support limited translation capabilities, the potential to tap into multi-lingual Web resources has not been materialized. Current search engines are primarily geared towards
English and have several limitations. In addition to the language constraint, the search engines use different syntax and rely on keyword based techniques, which do not comply with the actual needs and requirements of users as reported in [1] . In particular, expert users such as the librarians at the Library of Congress need to search a database of documents by making use of meaningful associations. Other expert user communities such as lawyers, researchers, scientists, academics, and doctors have reported similar requirements. Ideally, users should be able to formulate search queries such as  X  X  X he list of students who took my database course last quarter X  X ,  X  X  X he e-mail that John sent me the day I came back from Athens X  X , or  X  X  X he papers where we acknowledged a particular grant X  X  that make use of concepts, relationships and terms in his/her native natural language.

Multi-Lingual Information Retrieval (MLIR) facilitates searching for documents in more than one lan-guage. Typically, two approaches are used in MLIR  X  translating the query into all target document languages or translating the document information to match the query language. As pointed out in [2] , the output quality of these techniques are still not very satisfying because they rely on automated machine translation and do not consider contextual information to correctly perform word sense disambiguation. In addition, these tech-niques have been developed based on standard Text Retrieval Conference (TREC) collections, which are somewhat homogeneous and structured. However, the Web documents are highly heterogeneous in structure and format. Hence, multi-lingual querying on the Web is more complicated and requires a systematic approach.

The overall objective of our work is to integrate the salient features of Web search technology, distributed database querying, and information retrieval with respect to multi-lingual aspects of querying. In particular, the specific objectives of this research are to (a) develop a flexible and scaleable methodology for concept based cross-lingual querying on the Web and (b) develop an architecture for a system that implements the methodology.

The contributions of this research are threefold. First, unlike the MLIR techniques, which are mostly designed for specific pairs of languages, our approach is general enough and flexible that it can be applied to any language, since it is grounded in the parametric theory of linguistic diversity. This refers to both the parsing (query acceptance) and the query translation (query generation) problem. With respect to the query translation problem, the injection of linguistic knowledge structure of a submitted query increases the semantic precision of the translated query and contributes in solving the problem of ambiguity in translation.

Second, our approach is not dependent on the structure and format of the target documents or the existence of parallel or comparable corpora for query translation. Hence it is scaleable and well suited for searching documents on the Web. Third, the architecture that we have developed relies on a more general definition of a space of words as the underlying conceptual structure, which also incorporates existing resources such as a linguistic ontology (e.g., WordNet [26] ). This structure also provides space for accommodation of new words, which might extend or construct networks of words in any natural language. Semantic similarity issues across words in different natural languages are also addressed within this structure, especially for the query translation problem.

The remainder of the paper is organized as follows. Section 2 provides some background and related work in information retrieval, parametric theory in linguistics as well as language automata theory and state machines. Section 3 introduces the proposed approach for cross-lingual Web querying. Section 4 discusses the system architecture and a prototype implementation in terms of components for the query processing as well as the data or conceptual structures in conjunction with an example query and an evaluation scenario.
Finally, Section 5 summarizes the main aspects of the paper and outlines future work. 2. Background and related work
This section provides a brief discussion on the following three areas relevant to our work: (a) concept based querying as an emerging search paradigm within the realm of cross-lingual information retrieval, (b) the para-metric theory in linguistics, which underlies the structural aspects of the potential existence of a universal grammar for natural languages, and (c) the relationship between our proposed query language and natural and computer languages in terms of state machines. 2.1. Information retrieval
Search by important associations to some related concepts or user context and profile has been addressed by the emerging concept or knowledge based querying approaches [3,4] . With the emergence of the Semantic Web, information retrieval in terms of search by associations within a more domain specific context has been addressed as a way to alleviate the problems in locating the most relevant links and reduce the search space of links and or documents to be examined. There has also been an attempt to focus on generic solutions as suggested by [5] , and therefore, improve the effectiveness of search on the Semantic Web.

In general, these approaches make use of domain specific ontologies and semantic annotations in order to augment and improve query semantics either interactively [6] or as performed by the system [3,4] for different purposes, e.g., search engines and information retrieval [4] , mediation across heterogeneous data sources [3] .
However, they mostly rely on intelligent techniques and knowledge-based approaches for mappings across concepts and for query expansion. Querying by integrating semantic associations among entities, instances, properties, etc., into a conceptual search or query language has not been addressed, especially when cross-lin-gual natural language based querying is concerned.

The idea of performing information retrieval with techniques other than keyword based has been addressed by several approaches that merge DB and IR [9] . These try either to tie together the two systems by treating them as a black box or by extending one system with the features of the other with considerable integration drawbacks and difficulties. This kind of querying and search is mostly defined as  X  X  combining search technol-ogies and knowledge about the query and the user context into a single framework to find the most appropriate answer for the user X  X  information need  X  X  [10] .

Context based and personalized IR/DB querying paradigms, however, aim at either a contextual proactive content delivery or information filtering according to a model reflecting user behavior and profile [9] . A cross-lingual, natural language based querying by associations and restrictions, however, is still out of the scope of these approaches.

The only approach for a concept-based natural language query system , to our knowledge, has been addressed by [11] , in particular, the CQL/NL query language. It relies on the premise that concept-based or conceptual query interfaces reduce the cognitive load in querying databases by allowing users to directly use constructs from conceptual schemas. Unlike other concept-based query languages, which require users to specify each query-path in its entirety, CQL requires users to specify only the end points, i.e., the starting and terminating entities and relationship roles of query-paths.

The CQL system automatically deduces the correct intermediate entities to use on a given query-path. It also performs logical operations such as conjunction or disjunction on the derived paths. A controlled natural language interface like the one we propose in this paper and the methodology is also provided. The CQL/NL system combines natural language interface methods with the CQL querying approach into a single approach that is referred to as the semantic grammar (SG) approach.

Despite the similarities, our approach considerably differs from [11] in that we facilitate parsing and trans-formation (translation) of multi-lingual queries, with common underlying data and concept structures across natural languages as well as a universal query language grammar, whereas [11] only deals with imperfect infor-mation in concept-based querying.

Even within the realm of cross-lingual information retrieval (CLIR) [7,8] , a number of challenges have been reported, especially the problem of query translation [8] . With respect to the query translation problem, existing CLIR systems mostly rely on bilingual dictionaries or parallel/comparable corpora for query translation.
Some hybrid methods also exist. With bilingual dictionaries, queries submitted in a source language are trans-lated into a target language by means of simple dictionary lookup. However, besides the problem of incom-pleteness of the dictionary, we are also faced with the problem of ambiguity in translation, i.e., multiple translations are stored in the dictionary for the same word. If a dictionary is not available, query translation is performed by corpus-based techniques [7] in which translation equivalents are extracted from parallel corpuses.

However, many of these methods suffer from the mismatch of the domains of the training corpora and the search corpora. In addition, we cannot assume that parallel corpora are always available and in any natural language. In order to improve semantic precision in query translation, statistical models, e.g., decaying co-occurrence model , noun phrase (NP) translation model , dependency translation model , for query translation have been proposed, where the dependencies among words are also exploited [21,22] . However, this method has been applied for particular pairs of natural languages only, e.g., English X  X hinese CLIR, and has not been generalized for all natural languages.

The major difference, however, with our approach is that query translation takes place by also considering linguistic knowledge structure of the query as being a sentence and not by only considering a word-by-word translation, either through a dictionary or statistical methods. The latter is also known as  X  X  X ag of words X  X  translation approach, which does not take into consideration the context of a particular word and might lead to ambiguous translations. 2.2. Universal grammar and parametric theory in linguistics
The idea of a universal grammar underlying all natural languages is as old as Chomsky X  X  minimalist pro-gramme [15] and his school of grammar known collectively as transformational-generative grammar. This idea relies on principles and parameters by which children learn a language innately and has even been taken to reduce grammar to a genetic mechanism for acquiring a language. To this extent, it also triggered controver-sial philosophical discussions on how close the universal grammar and a potential Language Acquisition Device are supposed to be.

Despite all controversial discussions and apart from any biological implications they embarked on, the idea of a universal grammar as based on principles and parameters is motivated by the attempt to illumi-nate two puzzles: how adults can use language creatively and how it is possible for children to acquire their first language. The term  X  X  X reative X  X  is meant as the inherent capability for speakers to utter and understand any of an infinite set of possible sentences, many of which are created anew and on an appropriate occasion.

Given that the set is infinite and the utterances may contain recognizable mistakes, possible sentences can-not be simply memorized. They should be the result of a set of mentally represented rules, which appear to be surprisingly complex and similar from language to language. The complexity of this knowledge, however, gave rise to the problem of how we can acquire a language in a few years at our disposal. An answer has been given in terms of a language faculty, which must result from the interplay of the innate and the learned, leaving aside the question of what precisely is innate.

The framework known as  X  X  X rinciples and parameters X  X  [14] gives some explanation and provides knowl-edge of how languages can differ from each other. Parameters are conceived like switches, which can be set one way or the other on the basis of a simple evidence of the language we are dealing with. Parametric Theory is considered as the unification theory in Linguistics in a role similar to the role of Mendeleyev X  X 
Periodic Table in Chemistry or the role of the  X  X  X rand theory and unification formula X  X  in Physics. To this extent, combination of certain parameters determines the grammatical pattern of a natural language and, therefore, its semantics and syntax, even for those natural languages, which are unknown or undiscovered yet.

The products of the language faculty, however, such as representations of the sounds and meaning of sen-tences, derive many of their properties from the interfaces with which they have to interact: systems of percep-tion and production , on the one hand, and systems of thought and memory on the other. A crucial goal has always been to determine what is unique to language and what is unique to humans.

In our approach, we are concerned with what is unique to language rather than to humans. Our approach parallels the products of the language faculty, however, from a mechanistic point of view. The proposed sys-tem of perception (parsing) and production (translation) of multi-lingual queries is equipped with systems of thought and memory (partially constructed queries, contextual knowledge, linguistic ontology such as Word-Net and linguistic parameters), which are common to all natural languages.

It is worth noting that the interaction aspects of the proposed approach is an important means of deriving the meaning of queries either for query completion or translation. The main reason is that it goes beyond  X  X  X et another human-computer interface X  X  into the very principles of a Turing Machine as a theoretical (mathemat-ical) foundation of the computing discipline. 2.3. Turing machines and interactive models
Formally speaking, the models of natural or computer languages are defined in terms of four classes of lan-guages according to Chomsky X  X  hierarchy theorem : recursively enumerable, context-free, context-sensitive, and regular . These classes of languages can be formally specified by an automaton (state machine), which lies within the range of Finite State Automaton (FSA) and Turing Machine (TM), if acceptance of a sequence of words is emphasized, e.g., in a system of perception.

The specification of different automata, however, underlie the same philosophy having, for example, some finite set S q of states, some alphabet R called the input alphabet , a transition function d as a mapping
S  X  R ! R , some start state s 0 and some set of accepting or final states F S . In other words, M can be roughly defined as M =( S q ={ q 1 , ... , q n }, R , d , s
Alternatively, a type of grammar in terms of (a) a finite set N of non-terminal symbols , (b) a finite alphabet R of terminal symbols , disjoint from N , (c) a start symbol S , and (d) a finite set of some set P of productions of the form n ! x , is usually specified when the emphasis is put on generation rather than acceptance of sequences of words, e.g., in a system of production. To this extent, a language L ( G ) can also be defined by a grammar G ={ N , R , S , P } meaning that all words can be derived by S by repeated applications of the productions P .
Moreover, generation might be turned into an acceptance mechanism when parsing . Therefore, we might formally define the parser of a language L ( M ) in terms of an automaton M , which is equivalent to some gram-of elements of R and can be accepted by some automaton M . In addition, either the whole set of production rules P or a particular set of words in R * as accepted by M can be represented by a tree, which is called the derivation or parse tree. Normally, the vertices of a derivation tree are labeled with terminal or non-terminal (variable) symbols.

There has always been three specific issues that give state machines trouble [16] : (1) Grammatical does not imply sensible and vice versa, e.g.,  X  X  X ows flow supremely X  X  is grammatically acceptable but makes no sense, (2)
There are order requirements between the words of a sentence, i.e., a state machine has no way to encode or remember what order it saw previous symbols, for example, the expression  X  X ( a [ i + 3)] X  X  is immediately recog-nized by programmers as invalid, and (3) there are dependencies between the words of a sentence, for example, the design of a state machine that has paths to generate any number of pairs of brackets would be indefinitely large.

Designing a state machine, which is capable of coping with and, perhaps, solving problems (1), (2) and (3) as stated above should rely on other premises than merely stepping through the states of a state machine. For instance, we need to design a machine that generates and accepts language by structure rather than word sequence and, consequently, having a memory system that can pair up dependent words and enforce nested structure. The key idea would be to see a sentence as not a cleverly combined sequence of words, but rather groups of words and groups of groups. In other words, sentences have structure like a book has chapters, sec-tions and alike.

It is these objectives that underlined the design of Meaning Driven Data Query Language (MDDQL) [13] as a state  X  X  X hoice X  X  machine. This contrasts with the assertions of the theoretical computer science in the 1960s that TMs provide a complete model of computer problem solving. Turing X  X  assertion, however, was focusing on functional algorithmic problems [20] . Interactive forms of problem solving are not being addressed by TM X  X  computational model. These forms of problem solving depend on the behavior of the world rather than a pri-ori human beliefs.

Furthermore, in the pursuit of semantic rules rather than only syntactic ones, in order to make or determine a query as meaningful as possible, dependencies among the words of a query are driven by a linguistic or domain ontology, which is partly done interactively. This complies with the  X  X  X nteraction as computational model X  X  as advocated by [17] as well as the proponents of artificial intelligence, object-and agent-oriented pro-gramming, coordination and concurrency. For example, interaction has been proposed as an alternative to the
Japanese logical fifth generation computing model during the early 1990s as a more powerful tool than algo-rithms [18] . Interactive computing has also been proposed as an empiricist model that expands computational problem solving from algorithmic Turing Machine (TMs) models and functional input-output to broader concepts of interleaved dynamic streams and observable interaction with the environment [19] . To this extent,  X  X  X hoice machines X  X  extend (TMs) to interactive computation representing a distinct form of computing not modeled by TMs.

Given our intention to turn the designated state  X  X  X hoice X  X  machine to a Universal Query Language Autom-aton, the state machine should also be capable of dealing with word dependencies driven by linguistic struc-tures within a query. This is particularly helpful for the query translation phase. Therefore, using the elements of the parametric theory in linguistics, we extract structure from exemplars such as those dictated by the Eng-lish language or any particular natural language, while switching from one language to another. To this extent, the MDDQL state  X  X  X hoice X  X  machine relies on cross-lingual syntactic and semantic rules.

In the following, we present our approach that is heavily influenced by the design of a state  X  X  X hoice X  X  machine for cross-lingual web querying. 3. Proposed approach
Our approach builds on the methodology presented by Burton-Jones et al. [12] for semantic augmentation of web queries. They have developed a heuristic-based methodology for building context aware web queries.
Our approach also uses the query language described by Kapetanios et al. in [13] . They present a high level query language (called Meaning Driven Data Query Language  X  MDDQL) for databases, which relies on an ontology driven automaton. We provide a brief overview of the formal specification of a cross-lingual
MDDQL in Section 3.1 before describing our cross-lingual web querying approach in Section 3.2 . 3.1. Formal specification of a cross-lingual MDDQL automaton
As already stated in Section 2.3 , a key objective of the MDDQL design and specification as a concept based query language for well structured data as managed by databases [13] has been to design a state machine that generates and accepts query statements by both structure and meaning of dependent words rather than bags of keywords. Consequently, there should be a memory system that can pair up dependent words and, perhaps, enforce nested structures according to the intended meaning of a query. Satisfying the same key objective for any natural language begs the question as to how such a state machine, i.e. a Uni-versal Query Language Automaton , should be specified. It should be general enough in order to be used as a flexible and scalable acceptor (query parsing) and generator (query expansion and translation) of cross-lin-gual queries.

The formal specification of a cross-lingual parser for a concept based query language such as MDDQL needs to be done at a higher level of abstraction and, therefore, be independent of and easily adaptable to any con-trolled natural language, even those with still unknown grammar. In other words, the formal specification of
MDDQL(M) or MDDQL(G) is driven by universal features holding for all kinds of natural languages, i.e., some atoms of languages as expressed in terms of parameters such as word type order and head directionality . Formally speaking, a cross-lingual MDDQL is defined as MDDQL ( M
MDDQL ( G L ( V P 1  X   X  P n , A L )) meaning that the underlying automaton M or grammar G is a func-tional mapping of an input alphabet (set of words) A as a subset of a lexicon L for a particular language and a set or subset V of the Cartesian product P 1  X   X  P n of values of a set of the applicable parameters
P ={ P 1 , ... , P n }, which roughly characterize the given natural language L in which a concept based query has been constructed, to the specification elements of M or G .

In other words, given the inherent nature of the MDDQL concept based querying paradigm, with the exception of a small set of non-terminal or variable symbols as common to all natural languages such as noun the rules, which reflect a deeper understanding of the atoms  X  other than words  X  and structure of natural lan-guages in terms of parameters and patterns rather than specific rules for each language. Therefore, the
MDDQL parser becomes subject to a dynamic specification. To this extent, there is no need for specifying a new automaton or grammar each time a natural language is used, even if it is an unanticipated one. There-fore, besides lowering the overall system complexity , flexibility and scalability become an important issue as well.
Let us consider, for example, the parameter P 1 = word type order as one parsing dimension, with the fol-lowing patterns subject-verb-object, subject-object-verb, no word type order , which apply to English, Japanese, Walrpiri, respectively.

If M L = English ( v ={ P 1 = subject-verb-object } 2 V , A L ), then the starting symbol S word from A standing for a noun or adjective, whereas the parser is expecting a verb phrase to follow the noun phrase. However, when M L = Japanese ( v ={ P 1 = subject-object-verb } 2 V , A L ) holds, the starting sym-bol S 0 could be any Japanese word from A standing for a noun or adjective and the parser expecting another noun phrase, this time the object of the phrase, to be followed by a verb phrase. In the third case M
L = Walrpiri ( v ={ P 1 = no-word-type-order } 2 V , A L ), where special markers are used in order to dis-tinguish the semantic roles of words within a Warlpiri sentence, the parser should be guided according to the indications of those special markers. Further parameters such as the null subject might be considered as parsing dimensions which might also have an impact on grammatical patterns to be determined dynamically.

It is out of the scope of this paper to explain the impact of all these parameters on parsing of a concept (controlled natural language) based query language. It is, however, important to realize that given a set of all potential parameters S shaping a multi-dimensional space,the formal specification of MDDQL is a func-tional mapping of a subspace P S to all potential specification elements of the automaton or the grammar, with P as a multi-dimensional sub-space of potential parameters within which a particular natural language can be characterized.

The mathematical notation of the MDDQL automaton for cross-lingual, concept based querying takes the form M ={ S q ( V ), R ( A ), d ( V ), s 0 ( A ), F ( V )} rather than the conventional M =( S functional dependency of potential states, input alphabet, transition function, start symbol and set of finite acceptable states, respectively, from multi-dimensional space V , formed by a set of linguistic parameters (uni-rather than the conventional form G ={ N , R , S , P }.
 Additionally, the derivation or parse tree, as the outcome of a parsed query, takes the form G
MDDQL ={ V , E }, with V representing words together with their semantic roles. Therefore, it is independent of a particular set of production rules P and, therefore, natural language. It reflects compositional semantics of the query. For example, G MDDQL ={( h v 1 : car ( noun , class ), v tree of  X  X  car wheels  X  X  or  X  X  wheels of cars  X  X , in Japanese and English, respectively.

Given that the specified automaton and the computational model of the MDDQL parser can be considered as an abstract machine , which consists of more than one input tape , each of which has a finite control and a head to be moved either to the right or the left of the input tape (alphabet) and, therefore, causing a transition from a state q i 1 to a state q i , it is necessary to augment this automaton with stacks memorizing not only what are the words, which have been scanned at a state q i , but also the syntactical/semantic structure at that par-ticular state q i .

The algorithm presented in Table 1 exemplifies the transition function and logic of this abstract machine and only the parameter P : word-order-type as a dimension of the space V , are considered. 3.2. MDDQL-based web querying approach
Given that the transition function and logic of the MDDQL state machine needs to be determined by not only patterns as provided by elements (parameters) of a universal grammar, but also generally applicable semantic rules, our concept based cross-lingual web querying adapts the semantic query augmentation method discussed in [12] as well as the interaction logic for semantic query augmentation discussed in [13] . To this extent, our approach aims at setting up a framework in terms of required data and conceptual structures as well as query processing phases, in order to accept and translate (generate) query statements, and facilitate scalable and flexible cross-lingual Web querying.

In the following, we discuss the five query processing phases: (1) MDDQL Query Parsing, (2) Query Expan-sion and translation, (3) Query Formulation, (4) Search Knowledge Sources, and (5) Consolidate Results. These phases are briefly described below. In particular, the first three phases address the syntactic and semantic aspects of query acceptance and translation with the help of universally holding linguistic structures and knowledge. 3.2.1. Phase 1  X  MDDQL query parsing
This phase involves parsing the natural language query as specified by the user. The query is segmented and the MDDQL parser parses the segments and creates the query graph. This represents major aspects of the meaning of the query in a conceptual structure, which is natural language independent. The conceptual query graph can either be translated into English (or any other target language) prior to or following the query expansion as described in the following phase. Table 1 exemplifies the parsing algorithm as based on the for-mal specification given in Section 3.1 . 3.2.2. Phase 2  X  query expansion and translation
The output of the MDDQL parsing phase is a conceptual query graph (CQG) with a set of initial query terms which becomes the input to the query expansion and translation phase. During this phase, the CQG might get expanded either with linguistic knowledge as provided by lexicons and ontologies, in order to dis-ambiguate the meaning of query terms (words), or by adding appropriate user X  X  personal information as well as contextual information.

Formally speaking, the CQC is defined as G MDDQL ={ V , E } with V and E being the vertices and edges, respectively, reflecting the semantics of the query. However, the CQG follows an n-level knowledge representation model in accordance with the models for inter-lingual machine translation in [28 X 31] . Cur-rently, we foresee a 4-level knowledge representation model addressing syntactic, lexical-semantics, aspectual and contextual information of the query. All levels are integrated with each other and contribute to constrain the choice of the target language terms.

Disambiguating the meaning of query terms (words) not only improves the precision of search results in a source language but also the precision of query translation outcome. For instance, translating the query pat-tern  X  X  X BM buys Apple X  X  without disambiguating the meaning of the word  X  X  X pple X  X   X  in this particular context to be translated as a company and not as a fruit  X  might lead to meaningless translations. Furthermore, the problem of ambiguity in translation, i.e., multiple translations stored in a dictionary for the same word, has been extensively reported in the literature [21,22] .

Given that the parametric theory in linguistics applies not only to the syntactic level but also to the lexical-semantics and aspectual levels [28,29] , there is an analogous parsing (phase 1) method in query translation and generation using the G MDDQL . The more information available at each CQG level, the more precise the trans-lation will become.

For instance, in order to constrain the translation of the query  X  X  X apanese writers who won Nobel prizes X  X , for each query term, we need to identify the proper semantics of the term at the lexical-semantic, aspectual and contextual levels, given the user X  X  query context. For example, the verb  X  X  X in X  X  has many different meanings in English and Japanese. In order to constrain the target-language terms, the word senses from a lexicon such as WordNet are first used. For the term  X  X  X in X  X , synonym sets might be extracted and added to the MDDQL CQG at the lexical-semantic level. However, the appropriate word sense of  X  X  X in X  X  could be further constrained by the aspectual level, where the verb  X  X  X in X  X  is provided a thematic signature such as {win, THING}, {win, HEART}, {win, FAME}, {win, FAVOUR}, {win,
OVER}, {win, VICTORIES}, {win, THROUGH}. Based on these aspectual information, a different translation and, eventually, a complement for the verb  X  X  X in X  X  in Japanese will follow such as  X  X  X achinuku X  X  for Nobel prize.
 The following translation algorithm for relations as expressed by verbs could be applied: Detect thematic signature from a database of Lexical Conceptual Structure and Synsets Determine translation
A further example of what could be expressed at the contextual level of the CQG as query knowledge rep-resentation model is some negative information for what needs to be excluded. To ensure precise query results, it is important to filter out pages that contain incorrect senses of each term. Thus, a synonym from the unse-lected synset with the highest frequency is added as negative knowledge to the query. Since ontologies contain domain specific concepts, appropriate hypernym(s) and hyponym(s) are added as mandatory terms to improve precision. In this phase, personal information and preferences relevant to the query is also added. For exam-ple, the user may limit the search to certain geographical area or domain. Such information helps narrow down search space and improve precision.

In general, our approach is targeted at selecting the best set of translation words by solving ambiguities according to as many layers on the CQG as possible. While taking into consideration the syntactic informa-tion or linguistic knowledge of the structure in a sentence does improve semantic precision in machine trans-lation [22,23] , however, is not sufficient for precise query translations.

To this extent, embedding linguistic structure and context into the query translation mechanism, in princi-ple, leads to better results than translating a query in a  X  X  X ag of words X  X  mode. Given the formal specification of the cross-lingual MDDQL automaton (Section 3.1 ), the linguistic knowledge structure for the query transla-tion in any target language can also be provided by the linguistic parameters X  framework as a universal lin-guistic structure.
 3.2.3. Phase 3  X  query formulation
The output of the query expansion and translation phase is the expanded and translated query. In this phase, the query is formulated according to the syntax of the search engine used. It might take the form of a phrase or sentence to be searched in a corpus or a set of terms including the initial query terms, synonyms, negative knowledge, hypernyms, hyponyms, and personal preference information. Depending on the type of search, appropriate boolean operators are used to construct the query depending upon the type of term added.
For each query term, the synonym is added with an OR operator (e.g. query term OR synonym). Hyper-nym and hyponym are added using the AND operator (e.g. query term AND (hypernym OR hypernym)). Personal preference information is also added using the AND operator (e.g. query term AND preference).
The negative knowledge is added using the NOT operator. The first synonym from the highest remaining syn-set not selected is included with the NOT operator (e.g. query term NOT synonym).
 The following algorithm is used in constructing the expanded query:
For example, consider the following query: doctors who provide physical therapy WordNet provides four senses for the term Doctor and Doc is a synonym from the selected word sense.  X  X  X octor of the Church X  X  is the synonym from the highest word sense that was not selected. This is added as negative knowledge.  X  X  X hys-iotherapy X  X  is a synonym for physical therapy. Applying the algorithm for query expansion to this example and assuming the user chooses the  X  X pecialist X  hyponym for  X  X octor X  and the  X  X edical practice X  and  X  X herapy X  hyper-nyms for  X  X hysical therapy, X  the query constructed is 3.2.4. Phase 4  X  search knowledge sources
This phase submits the query to one or more web search engines (in their required syntax) for processing using the API provided by them. Our query construction heuristics work with most search engines. For exam-ple, AltaVista allows queries to use a NEAR constraint, but since other search engines such as Google and
AlltheWeb do not, it is not used. Likewise, query expansion techniques in traditional information retrieval systems can add up to 800 terms to the query with varying weights. This approach is not used in our meth-odology because web search engines limit the number of query terms (e.g. Google has a limit of ten terms). The // Expand each base query term with additional information let query _ expn = NULL let WS be a word senses set, WS ={ WS 1 , WS 2 , ... WS n let each word sense set WS i contain one or more synonyms, WS for each term TN i end for search query can also be submitted to internal knowledge sources within an organization that may contain relevant documents. Of course, this requires that knowledge repositories provide a well defined set of APIs for applications to use. 3.2.5. Phase 5  X  consolidate results
In the final phase, the results from the search engine (URLs and  X  X nippets X  provided from the web pages) are retrieved and presented to the user. The user can either accept the results or rewrite and resubmit the query to get more relevant results. This phase also integrates the search results from multiple language sources and takes care of the differences in formatting. The results are organized based on the knowledge sources used, target language, or domain. 4. System architecture and implementation The architecture of the proposed system is shown in Fig. 1 . It consists of the following components: (a)
MDDQL Query Parser, (b) Query Expansion Module, (c) Query Constructor, (d) Query API Manager, and (e) Results Integration Module. These components implement the proposed query processing phases as discussed in the previous section. They also support the data and conceptual structures needed to drive query processing towards semantically more precise results. It involves the creation of: (1) an information space for setting up the linguistic parameters, (2) word spaces such as dictionaries and linguistic ontologies, and (3) user profiles and personalized context.

A proof-of-concept prototype is currently under development using J2EE technologies with client-server architecture. The user specifies search queries in natural language. The server contains Java application code and the WordNet database. The prototype will provide an interface to commercial search engines such as Google ( www.google.com ) and AlltheWeb ( www.alltheweb.com ). The MDDQL parser component is implemented using Java, where abstract data types have been defined for the construction and manip-ulation of the MDDQL query trees. A repository (knowledge base) for representing the parameters (dimensions), as well as a classification mechanism of a natural language into this parametric space is provided.

The heuristics within the query expansion module are being implemented as servlets and the rules and rea-soning capability using Java Expert System Shell, Jess ( http://herzberg.ca.sandia.gov/jess ). The prototype will also interface with ontology sources such as DAML and ResearchCyc and relational databases that store per-sonal profiles and user preferences for query refinement. The query API manager component uses URL encoding to communicate with the knowledge sources and the API provided by the search engines. The results integration module returns the first twenty pages from the search engines in the format they provide. The results contain the title of the page, the snippet, and the URL for each page. Each of the components and their functionalities are briefly described below.

Implementation of MDDQL as a Universal Query Language parser and translator/generator has been accomplished as a command line based prototype on a Java 1.5.0_06 SE platform. The classes used in the pro-totype are shown in Fig. 2 , some of which are briefly described below.
 The MDDQL conceptual query tree is defined as a generic QueryTermNode class. The instance variables
TUI, Word, etc. represent the query term as an object rather than a single string value. In order to distinguish among the ontological role of nodes and, therefore, the included query terms on the tree (instance variable:
Word), subclasses such as PropertyTermNode, RelationTermNode, ValueTermNode can be specified as inheriting from the generic QueryTermNode class, which indicate that a particular term is being considered as a Property, Association, Value, respectively. Of course, various other subclasses can be added, if needed. The class QueryTreeContainer provides all the methods for the construction and manipulation of the
MDDQL query tree based on its generic type QueryTermNode together with the initialization of some context for a particular query tree. The QtagQueryParser class currently receives as input the tokenized query as per-formed by the Part-of-Speech (POS) probabilistic tagger (QtagTokenFactory) and provides the natural lan-guage settings for the n -dimensional parametric space.

The class QueryTreeMapping implements the conversion of the tokenized query based on the POS list of tags and the natural language settings of the n -dimensional parametric space, into an MDDQL conceptual query tree. It takes into consideration what terms have been put into the various stacks, which mainly refer to the semantic role of clauses, i.e., a subject, an association, an object, within the query. Therefore, the sequence of query terms as they appear within a query does not have any impact on the order of appearance of the terms, i.e., query term nodes on the query tree.

In order to verify the validity of our assumptions, and to clarify the internal workings of the system as pre-sented in Fig. 1 , we describe in detail how a query is parsed, passed to the query expansion and translation module, then to the query constructor, which will transform the query to be submitted to the query API manager.

Let us assume that a user submits the following Japanese query: asking both pages in English and Japanese (the phrase in italics is not sent to the search engine, it is added here to help the reader understand the query). Since, Google relies on keywords, it will not be able to retrieve any links.

Obviously the phrase needs to be segmented into morphemes. The need for a segmentation tool does not arise for European languages, where at this stage a lexical analyzer like WordNet (for English) will suffice to help create the conceptual graph. There are many segmentation tools available for Japanese (e.g. Juman), with various levels of accuracy. In this example, we assume that the system being implemented can interface with such a tool and move on to actual parsing. The segmented query is ( writer ) ( syntactic role marker ).

Note that the initial query has not been translated into English. The intention is to postpone it as much as possible, since all translations are prone to ambiguities if the context is not clearly specified and the words are not chosen carefully from a lexicon. According to the parsing algorithm described in Section 3.1 , the MDDQL parser will start by setting the language to Japanese, and define its parameters. For this scenario two param-eters are considered: the head-directionality and the topic-prominent .

For maximum flexibility, the parser makes no assumptions about the underlying grammar of the lan-guage, but depending on the parameters it will adjust its behavior. The importance of this design deci-sion is that it allows the system to dynamically vary the language it is tokenizing without changing the parser itself.

The head-directionality is a linguistic pattern of the Japanese language that manifests by the use of markers (postpositions added to noun phrases to emphasize their role  X  subject or object) [14] . This parameter plays an important role in identifying the words in their dictionary form during parsing. The topic-prominent param-eter, characterizes phrases such as the following:  X  X  Nobel prize, prize winning it did Japanese writer  X  X  with an initial noun phrase (topic) followed by a complete clause as a comment on the topic. This type of phrase con-structions is not accepted in English, and it was obtained by translating each word after the segmentation step.
In this case direct translations introduce verbiage to the query and will negatively impact the retrieval results, hence the need for further syntactic and semantic analysis.

A first level of syntactic parsing of the query identifies the following groups of words that represent the object (Nobel prize), the subject (Japanese writer), and the verb (prize win-ning, it did), respectively. The later group of words is stemmed from the verb  X  X  X in X  X .

Secondly, for the verb that expresses the relation between subject and object, the system will attempt to identify against a resource of lexical conceptual structures the one that best matches the query X  X  context, i.e. the agent is a writer that wins a prize, as opposed to {win, war}, {win, love}, etc, This information gath-ered from this stage will be used to annotate the corresponding vertex for  X  X  X in X  X  and will be used during the tree translation phase.
 Below, the algorithm given in Section 3.1 is instantiated.
 Input: an A L = Japanese .
 A = { X  X  (Japanese) X  X ,  X  X  (writer) X  X ,  X  X  (win) X  X ,  X  X  (Nobel) X  X ,  X  X  (prize) X  X ,  X  X  (book) X  X  X .
 Output: An MDDQL parse tree G MDDQL .
 SET L = Japanese ; SET V with h P1 : head-last case marker , P2 : topic-prominent i .

The topic-prominent parameter helps correctly identify the subject phrase and disregard  X  X  X hey X  X  from query, without loss of meaning. The head-directionality parameter helps add syntactic roles to query tree and identify the right word from the lexicon.
 Generating the conceptual graph
Vertices: G SU :( h v 1 : writer ( noun , subject , class ), v D = SU , or
Connecting edges that respect the ontological constraints:
Due to the ontological constraints that the query parser abides by when building this graph, the outcome of the algorithm will be a conceptual tree, which is a natural language independent representation of the intended meaning of the query. This tree might be extended by the Query Expansion and Translation Module for word meaning disambiguation and the extended conceptual tree is read and translated into a query by the Query Constructor with appropriate syntax.

Once the conceptual graph (tree) is created, the Query Expansion and Translation Module clones and trans-lates each of the vertices into English taking into account the lexical conceptual information that has been annotated at the beginning of this stage for disambiguation. In this particular scenario the user asked for pages in both English and Japanese, but scaling up the system with other languages will only mean repeating the cloning and the generic translation process, from conceptual structures to adequate linguistic structures of the target natural language.

Translating from conceptual structures is fast and adds less ambiguity in the translation process, and it also preserves the semantic relationships between concepts. During the translation of the vertices into another nat-ural language, the system will interface with a set of chosen lexicons, ontologies, and a translation service to perform the mapping from the input language to the other natural languages requested. To this conceptual tree, the Query Expansion and Translation Module will add a set of a priori global constraints that apply to all queries and local constraints based on the query context. These constraints are derived from the user X  X  pro-file and they will enhance the queries precision.

In the scenario described so far, the global constraints refer to the language restrictions (English and Jap-anese). The local constraints will also become active since in the query X  X  structure the key-concept  X  X  X riter X  X  arises. This user is a book reviewer, and he expects that his search will provide pointers to both authors X  biog-raphies, as well as titles of books they wrote. In the user X  X  profile the association (semantic relationship) between the concepts  X  X  X riter X  X  and  X  X  X ook X  X  is stored. The Query Expansion and Translation Module will detect this association and the concept  X  X  X ook X  X  will be automatically added to the query tree. The conceptual graph will expand with: (a) a new vertex v 6 : book ( noun , object , class ), and (b) a new edge h e 5 :  X  X  X  X  i connecting v 1 : writer (noun, object, property) with v
After the query tree is completed, the Query Constructor plays the role of a query factory that will read the query tree and generate queries for a set of heterogeneous resources (e.g. enumeration of the vertices for the search engines, or SQL queries for databases available via web services). The flexibility of this module is due to the high-level representation we have chosen to use for query representations.
 The final task of the cross-lingual web querying system is to assemble the results obtained after the Query
Constructor delegated the queries to different resources. The Results Integration Module will wrap the answers to the search by taking into consideration user preferences for content and formatting. 5. Preliminary validation
The scope of this paper is to develop a parametric linguistics based approach for cross-lingual web querying and to investigate the contribution of a universal query language as an adapting mechanism to a new language environment. Hence, we discuss only the theoretical aspects of the proposed approach and the preliminary validation that has been conducted. Our future work includes a more comprehensive empirical validation of the approach and further refinements to our system.

We have run a series of preliminary experiments to validate our approach. This process entailed collecting comparative data representing the number of hits obtained by submitting a translation of the query as a whole, disregarding any semantic aspect, and the hits provided by simulating the steps performed by our pro-posed MLIR system. The main elements needed for our experiments were a repository of multi-lingual doc-uments and a search engine that can be linked to our chosen corpus.

A parallel corpus of English and Japanese news pages has been extracted from the web using [24] and linked to Hyper Estraier, a full-text search engine [25] . The latter supports Unicode search algorithms based on a hybrid mechanism of N -gram and morphological analyzer. The main focus of our experiments was verifying that parsing and translating a query through our proposed approach and algorithm produces relevant hits. By working with a parallel corpus, we wanted to test if by submitting the same query in English, and Japanese, the search engine would output the corresponding pair of documents.

We decided to use only simple, two or three words expressions that have a semantic relationship between them. The steps undertaken to expand and translate a short query, can be recursively repeated when the query is more complex, as long as the query terms are grouped into meaningful phrases (denoted by quotes). Hyper
Estraier has indexed 35,500 pairs of html documents in English and Japanese. The Japanese pages were not machine translated, and sometimes as we discovered in our experiments, not a perfect mapping of the English document.

Let us consider the query  X  X  X nformation overload X  X . The output of our query was a set of 15 documents writ-ten in English, where the two words were co-occurring. Now, in order to run the Japanese query we approached the translation from three angles: (a) translate the query as a whole through Altavista X  X  Babelfish; (b) translate each word and submit it; and (c) use the MLIR system.

The result we obtained in the first case was zero hits, which is not surprising. In the second case, the search engine displayed a list of 44 hits. These were only partially incorporated by the hits from the English query result list (three out of 15). This search will also return zero results if we concatenate the two translated words (since in Japanese there are no spaces in between words).

By examining the corresponding Japanese documents we noticed that the word  X  X  X verload X  X  was very often not mapped to its expected Japanese translation. We identified that the word  X  X  X lood X  X  was used, as well as  X  X  X ass X  X . From WordNet and a thesaurus, we determined the relationships between them:  X  X  X lood X  X  belongs to the synset of the word  X  X  X verload X  X , while  X  X  X ass X  X  is a hyponym of  X  X  X lood X  X . At this stage of our experiment the MLIR X  X  Query Expansion component is required to provide suggestions for automatic expansion. This is a classic problem for information retrieval systems, and we can assume from previous experiments [27] that using WordNet and a thesaurus is a good combination for obtaining more specific expanded queries. Though our scenarios deal with short queries, the retrieval performance should not be affected by the submission of long Boolean queries.

The initial query modified to the Japanese equivalent:  X  X  X nformation flood X  X  j  X  X  X nformation mass X  X  ( j ) does not improve relevance. This is due to the fact that the translation did not con-sider any characteristics of the Japanese language. We replace  X  X  X nformation flood X  X  and  X  X  X nformation mass X  X  with the translations for  X  X  X lood of information X  X  ( ) and  X  X  X ass information X  X  ( ), because the Japanese head parameter requires the head of a phrase to be in the final position. Comparing these translations with the previous version, we notice that they are not very different. However, the rele-vancy of the query results improved to 10 out of 15. This tweak will affect any other keyword based search engine as well.

After modifying the query, the search engine delivered 74 Japanese documents instead of 44, out of which we could easily identify 10 as being part of the group of English documents. We also inspected the last five documents and a pattern emerged. The expression  X  X  X nformation overload X  X  in three out of five was translated completely differently than before, due to its context. In one instance, the word  X  X  X nformation X  X  did not even appear in the pair document, being replaced with the word  X  X  X esources X  X .
 We have run the experiment with  X  X  X lood of information X  X  in Japanese. This expression was translated into
English and the word  X  X  X lood X  X  was expanded to  X  X  X verload X  X . We used the head first parameter to set the query to  X  X  X lood of information X  X  j  X  X  X nformation overload X  X . Since the Japanese WordNet is still work in progress, we had to perform the translation first, and than use the head directionality parameter to rebuild the expression. The results of this query incorporated the Japanese matching pair.

We had similar results when testing other short queries using the same method. So far, we have noticed a 20% improvement in precision when adding an extra layer of linguistic processing before or after the actual translation. We have not verified this in a larger scale, but further experiments will involve testing of this approach on the WebCLEF corpus. Finding flexible ways of performing query expansion without using
WordNet will make it generic. Further experiments will focus on the cost of adding more linguistic parame-ters, and determining the minimum set of parameters that can help alleviate translation of structural changes and translation mismatch. 6. Conclusion and future work
Current CLIR techniques for querying the Web lack a generic approach and methodology to cope with any natural language in a flexible and scalable manner without increasing overall system complexity. In this paper, we have presented an approach and suggested a methodology for cross-lingual querying on the Web, which primarily addresses the problems of constructing as semantically precise as possible query statements and results for knowledge resources across natural languages. Our approach relies on initial specification and implementation of a state  X  X  X hoice X  X  machine underlying the MDDQL query language. It follows the principles of considering syntactic and semantic dependencies across words rather than independent words for query parsing and translation as well as query expansion and augmentation. The universality of this approach and state machine is underlined by the common syntactic structures, as provided by the paramet-ric theory in linguistics, and the common semantic structures, as provided by linguistic ontologies (e.g.,
WordNet). We have discussed the architecture of a system that implements our methodology. A proof-of-concept prototype is currently under development to demonstrate the feasibility of the approach. Our future work includes completing the prototype, further experimental validation, and refinement of the meth-odology towards shaping and creating multi-lingual word spaces in order to cope with query translation problems, especially when there are no dictionaries or linguistic ontologies available in a particular natural language.

References
