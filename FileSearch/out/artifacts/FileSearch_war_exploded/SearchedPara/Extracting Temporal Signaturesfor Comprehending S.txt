 Systems biology has made massive strides in recent years, with capabilities to model complex systems including cell division, stress response, energy metabolism, and signaling pathways. Concomitant with their improved modeling capa-bilities, however, such biochemical network models have also become notoriously complex for humans to comprehend. We propose network comprehension as a key problem for the KDD community, where the goal is to create explainable representations of complex biological networks. We formu-late this problem as one of extracting temporal signatures from multi-variate time series data, where the signatures are composed of ordinal comparisons between time series com-ponents. We show how such signatures can be inferred by formulating the data mining problem as one of feature se-lection in rank-order space. We propose five new feature selection strategies for rank-order space and assess their se-lective superiorities. Experimental results on budding yeast cell cycle models demonstrate compelling results comparable to human interpretations of the cell cycle.
 G.3 [ Probability and Statistics ]: Time series analysis; I.5.2 [ Design Methodology ]: Feature evaluation and se-lection; H.2.8 [ Database Applications ]: Data mining Algorithms, Measurement, Experimentation.  X  Slotta is now with the National Institute of Mental Health, National Institutes of Health, Bethesda, MD 20892. Temporal signatures, systems biology, feature selection, rank-order spaces, biological networks.
Systems biology is an immensely successful enterprise [19] that focuses on modeling biological processes with math-ematical formalisms (e.g., ordinary differential equations (ODEs)) and uses numerical simulation and analysis tools to recreate the dynamics of biology. Key processes such as cell division, stress response, energy metabolism, and sig-naling pathways can now be satisfactorily modeled using systems biology tools. Capabilities such as SBML (Systems Biology Markup Language) and software such as JigCell [24] and COPASI [10] have eased the modeling and simulation process for biologists who might not be trained in the un-derlying mathematics and algorithmics.
 As a case in point, consider the cell division cycle (see Fig. 1 (top left)) of budding yeast ( S. cerevisiae ), which is a model eukaryotic organism. The set of chemical reac-tions and steps included in the most updated yeast cell cycle mathematical model is highly complex (see Fig. 1 (top left)) involving close to 140 parameters, 48 molecules, and close to 50 equations. The cycle consists of four phases [23]: G1 (Gap 1), S (DNA synthesis), G2 (Gap 2), and M (mitosis). These phases are carefully orchestrated using different pro-tein complexes such as Cdk/cyclin complexes, Cdh1, Sic1, and Cdc6.

Richard Feynman is famously known to have said that X  X v-erything that living things do can be understood in terms of the jiggling and wiggling of atoms. X  Although this can be dismissed as a physicist X  X  reductionist view, the reality is not too far from this quote: the progression through cell cycle phases requires the successive activation and inactivation of protein molecules and complexes. For instance, in the G1 phase, proteins like Cdh1, Sic1, and Cdc6 are more abundant than Cdk/clb complexes, whereas in the S/G2/M phases it is the opposite. Hence, one key way to comprehend the cell cycle is to understand which molecules overpower which oth-ers (and when), which molecules are decaying versus which others are increasing in concentration, and which molecules regulate the production/demise of others.

Unfortunately, this type of information is quite difficult to Chen et al. [5] and (Top right) figure motivated by diagram in [17]. gather from the diagram of Fig. 1 (top left). Because bio-chemical reaction networks are conceptualizations of dynam-ical processes, even skilled scientists cannot eyeball these di-agrams and comprehend them. One approach is to simulate these networks, generate multi-variate time course data cor-responding to the set of molecules, and use data analysis tools to study the time series. This, of course, simply exac-erbates the issue, since, without sophisticated data mining tools, comprehending time series of the scale envisaged here is a herculean task.

We hence propose network comprehension (Fig. 1) as a key problem for the KDD community: given a biochem-ical network, mine the multivariate time course data sim-ulated by the network to gain insight into the network X  X  functioning. The key form of insight we seek is understand-ing of the  X  X ace X  conditions between the proteins and how these race conditions drive the internal state of the organ-ism. In particular, which protein molecules contribute to the notion of internal state? What relationships must ex-ist between these molecules in each stage of the dynamical process? We view these relationships as temporal signa-tures , since they serve to summarize the underlying state of the dynamical system.

The problem of extracting temporal signatures can be viewed as feature selection in rank-order space [20]. Here, proteins are the features, instances are vectors of concen-tration values of proteins, and the classes are the phases of the biological system (e.g., G1, S, G2, M phases). Because the dynamics are driven by relationships between protein concentrations, it is not meaningful to work directly with the amount of proteins in an attribute-value sense. (For instance, in understanding the overall dynamics of the sys-tem, it is incomprehensible to work with a measurement quantifying 300-500 molecules of Clb2 as being present in a yeast cell whose volume is just 50e-15 liter.) Instead we rank the features and think of each instance as a total order over the features. The network comprehension problem then reduces to conducting feature selection in rank-order space and constructing temporal signatures by composing ordinal comparisons between protein molecules.

There are many applications that highlight the impor-tance of feature selection in rank-order datasets. In biomed-ical instrumentation, the desire is to select a subset of elec-trodes from an EEG dataset and use profiles of relative sig-nal strength as indicators of patient health [26]. Here the instances are the patients, the classes are the diagnoses, and the features denote signal strength as measured using differ-ent electrodes. In large-scale gene expression (microarray) assays [2], one possible aim is to classify an experimental condition using expression changes only across a  X  X alient X  subset of genes. For instance, by observing a handful of genes (features) and ranking them by their expression lev-els, it is possible to qualitatively characterize the cellular transcriptional state (class) for a given condition (instance). In decision-making referendums, one goal is to identify key voting indicators to infer political biases of constituencies. Here, the instances are the constituencies, the classes are political party strongholds, and the features might be socio-economic indicators. On a lighter vein, it appears possible to classify a movie as an art film or a mass market flick by ranking critics! Given such a widespread prevalence of ap-plications where rank order is pertinent [15], it is surprising that this feature selection problem has received little atten-tion so far.

While order-theoretic considerations have been studied in different guises in machine-learning research, our formula-tion is different from the more traditional settings where instances or classes are ordered or the feature values are ranked across instances [8, 13, 14]. These types of formu-lations are pertinent in applications such as recommender systems or information retrieval [6, 11], where the goal is to learn and mine a ranking or to infer total orders from given preferences. In our case, features can be ranked within an instance and the goal is to use ordinal comparisons as de-scriptors for classes.

The key contributions of this paper are: 1. Formulation of the biochemical network comprehen-2. Five feature selection strategies X  X reedyKL, KS, Spoil-3. Experimental results on both synthetic data (to assess
Let S = { S 1 ; S 2 ; : : : ; S n } be a set of biochemical species (features), and let D be the common domain of S i . In the case of the yeast cycle, S includes molecules such as { Cln 2 ; Clb 2 ; Clb 5 ; Sic 1 ; Cdc 6 } , and D denotes non-negative reals, to capture molecular concentrations. For simplicity, we assume that a total order is defined on D . A feature in-stance s defines an order on S by the rule S i &lt; S j if s Let C = { C 1 ; C 2 ; : : : ; C l } be a set of classes. For the yeast cycle, the classes denote the phases, i.e., C = { G1,S,G2,M } . Then a dataset T is a nonempty multiset of pairs { ( s ; c ) } , where s  X  D n and c  X  C . Suppose, S consists of the molecules Cln 2 ; Clb 2 ; Clb 5 ; Sic 1, and Cdc 6, which have con-centrations of 0.07, 0.15, 0.05, 0.02, and 0.11 respectively in
We recast the given dataset T into a rank-order dataset, whose feature values capture the relative order between pairs of feature values of a feature instance s . We define the rank-order feature set for S to be F = { F 1 ; F 2 ; : : : ; F { 1 ; 2 ; : : : ; n } be a permutation that sorts s into non-decreasing order. If two features have the same value, we then choose  X  s arbitrarily from the permutations that satisfy the above condition. Then the rank-order feature instance f for s has values given by f i =  X  s ( i ). For example, given a feature rank-order feature instance is f = (3 ; 5 ; 2 ; 1 ; 4).
The rank-order dataset is the multiset T R = { ( f ; c ) | ( s ; c )  X  T } . Each dataset T R implies a probability distri-bution P as follows: Moreover, if a feature instance f occurs in at least one pair of T , then the conditional probability is defined for all c  X  C . We will use the term P ( C | f ) to refer to the conditional probability distribution of C given the features.
A classical definition of irrelevant and redundant features is that put forth by Blum and Langley [3]. These notions apply to regular datasets (such as T ), rather than rank-order datasets (such as T R ). A feature S i is irrelevant if it has no effect on the class distributions and is independent of other features. A feature S i is strongly relevant if there exists at least two feature instances s 1 and s 2 in T that differ only in their values assigned to S i and have different classifications, if they appear in T multiple times [3].

These definitions have to be modified for the rank-order case. A feature F i in rank-order space is a strongly relevant feature if there exists at least two data instances ( f p ( f ; C n )  X  T R such that when F i is removed, f p and f q col-lapse to the same permutation. For example, the feature F 1 in Fig. 2 is a strongly relevant feature because removing F 1 makes Row 1 and Row 2 of the table to collapse to the same permutation. A feature F i is a weakly relevant feature if there exists a nonempty subset of features F  X   X  F such that removing F  X  makes F i strongly relevant. For example, the feature F 2 in Fig. 2 is a weakly relevant feature because it becomes strongly relevent if F 1 is removed. We call a feature redundant if it is weakly relevant, but not strongly relevant.

S 20 33 65 40 a 40 25 60 30 b 50 35 20 70 c 80 55 35 40 d Figure 2: Dataset T (left table) with its corresponding rank-order dataset T R (right table).
We define the boolean-order feature set for F to be B = { B i;j | 1  X  i &lt; j  X  n } , where the domain of each B i;j { true; f alse } . Given a rank-order feature instance f , the Figure 3: Boolean order dataset T B corresponding to the rank-order dataset T R in Fig. 2. corresponding boolean-order feature instance b has values given by,
The boolean-order dataset for T R is a multiset T B = { ( b ; c ) | ( f ; c )  X  T R } . An example of T B is illustrated in Fig. 3. We will evaluate classification performance of the algorithms using T B rather than T R because we need to perform classification on orders and not on the absolute val-ues. While the boolean table might contain redundant fea-tures due to transitivity, such transitivity is local to each instance. Given features B 1 ; 2 ( F 1 &lt; F 2 ), B 2 ; 3 stance, then B 1 ; 3 will be true. But in another instance, B and B 1 ; 3 can be true without B 1 ; 2 being true. Hence, B is not redundant here. Thus, it is not possible to universally remove a feature based on transitivity.
Numerous criteria can be applied for removing redundant and irrelevant features which result in a reduction of F to a subset F  X   X  F . Such criteria include improved accuracy of predictive modeling, smaller description length for learned mappings, or preservation of as much of the relationship between class distributions and features as possible. We will use the latter criterion in this paper.

For f  X  T R , let f F  X  be the projection of f onto the features in F  X  . For a dataset T R , let T R F  X  = { f F  X  | f  X  T projection of T R using the feature set F  X  . Our goal is to approximate P ( C | f ) with P ( C | f F  X  ). A popular approach to characterizing the difference between two distribution is the KL-divergence [7]. The KL-divergence between distri-butions P and Q is KL ( P; Q ) = P x  X  X P ( x ) log P ( x ) Koller and Sahami defined two feature subset divergence quantities,  X  F  X  and  X  F  X  using KL-divergence [12]. For each data instance f  X  T R , divergence is  X  F  X  ( f ) = KL ( P ( C | f ) ; P ( C | f F  X  ). Now, the feature subset divergence distribution of data instances in T R . We can utilize the feature subset divergence in two ways. One way is to define such that  X  F  X  is at most that threshold. Another is to search for a subset in all F  X   X  F of a fixed size that minimizes  X 
In the context of a rank-order dataset, a feature instance f is a permutation, and we require methods to measure dis-tances between permutations. We adopt two established approaches for defining a distance function between two per-mutations. The Spearman X  X  distance [21] between two rank-order feature instances f i and f j is: where k is the index of the k th feature.

We use Kendall tau distance as our second distance func-tion between two permutations. The Kendall tau distance between two rank-order feature instances f i and f j is, kd ( f i ; f j ) = | ( x; y ) : x &lt; y; ( f i ( x ) &lt; f
Let I be a multiset of rank-order instances that belong to a single class in T R and let I  X  be the set I without feature F . We define the spoiler count sp ( F i ; I ) for feature F respect to I as follows. Since the instances belong to the same class and order is presumably indicative of class membership, these permuta-tions ought to be similar in order. The feature that has the highest spoiler count contributes most to the differences in order and is therefore a good candidate for removal. For each class c  X  C , let T R ( c ) represent the set of rank-order instances that belong to c in T R . The total spoiler count tsp ( F i ) of a rank-order feature F i is
A final notion we will find useful is measures of central tendency around permutations. Let M be a set of permu-tations. Then the center ctr ( M ) of M is a permutation  X  (not necessarily in M ) that minimizes P  X   X  M sd (  X ;  X  c gorithmically, we can compute the permutation ctr ( M ) by summing the ranks in each position, across all permutations, and deriving ctr ( M ) from the order of the resulting sums (if there are duplicate sums, ties are broken arbitrarily). Each class C i for which M i = { f | ( f ; C i )  X  T R } is nonempty, ctr ( M i ) is a permutation at the center of a smallest hyper-sphere containing all the permutations of M i . If C i and C are distinct classes, then ctr ( P i ) and ctr ( P j ) are represen-tatives of the two classes that can be used to measure a distance between the two classes. In addition, a center pro-vides an estimate of the expected ranks of each feature in that class, but, more importantly the expected relative or-ders of the features; a fact which we shall make use of later on. A feature selection algorithm might choose to remove a feature that yields distances that are closest to the original distances.
Finally, a temporal signature can be viewed as a descrip-tive summary of the rank-order relationships across classes (typically, such summaries are made after feature selection). There are many ways to define such signatures. In this pa-per, we take the temporal signature S k ( c i ) of a class label c parameterized by k , as a set { T 1 ; :::; T k } of k permutation centers derived from all the instances associated with the label. The centers of a class are computed using a k-means algorithm where the distance measure between permutations 3.5 4.2 5.7 a 3.5 4.2 1.5 b 3.5 1.5 2.4 c 3.5 2.4 5.7 d Figure 4: Datasets T (left table) and T R (right table) where a removable feature in T does not apply in T R . is the Spearman X  X  distance. The signatures thus represent a set of k distinct expected orders capturing the essence of all the instances associated with a class. In the experiments de-scribed here, we set k = 1; we did not experience significant gain in information with higher values of k .
In this section, we investigate relationships among feature spaces, rank-order spaces, and boolean-order spaces. Let S k be a feature in S , F k be the corresponding rank-order feature in F , S  X  = S  X  { S k } , and F  X  = F  X  { F k } . We say that F k is a removable feature if  X  F  X  = 0. We now examine the relationship between  X  S  X  and  X  F  X  . The next two conjectures and their counterexamples illustrate that the relationship is not simple or easily exploited.
Conjecture 1. Let T be a dataset with feature set S and suppose S k is a feature such that  X  S  X  X  S k } = 0 . Then,  X 
Counterexample: Figure 4 provides an example of a dataset T and corresponding rank-order dataset T R . Here, S 1 is a removable feature in dataset T (values assigned to S are the same for all instances). This means  X  S  X  X  S 1 } = 0. However,  X  F  X  X  F 1 } 6 = 0 in T R , because there is at least one instance f (such as f = (3 ; 1 ; 2)) such that  X  F  X  ( f ) 6 = 0.
Conjecture 2. Let T be a dataset with feature set S , and let T R with feature set F be its corresponding rank-order dataset. Suppose F k is a feature such that  X  F  X  = 0 . Then,  X  S  X  = 0 in T .

Counterexample: Figure 5 provides an example of a dataset T and corresponding rank-order dataset T R . Here, F 1 can be removed (as can any single feature) while retaining the same capacity to classify so that  X  F  X  = 0 in T R , where F  X  = F  X  X  F 1 } . However,  X  S  X  6 = 0 for S  X  = S  X  X  S 1 } since S 1 is in fact the feature that distinguishes the two instances in the dataset (  X  S  X  ( s ) 6 = 0 for both instances). Figure 5: Datasets T (left table) and T R (right table) where a removable feature in T R does not apply in T .

We now present the following result, which demonstrates that rank-order datasets and boolean-order datasets contain the same order-theoretic information with respect to feature selection.
 Lemma 1. Let T R be a rank-order dataset with feature set F , and let T B with feature set B be its corresponding boolean order dataset. Furthermore, let F  X   X  F . Define B  X   X  B to be the set of all features B i;j such that F i ; F j  X  F  X  . Then,  X 
Proof. From the definition of boolean space, it suffices to f . The equality P ( f ) = P ( b ) follows directly from how the boolean order set was constructed since there is a one-to-one correspondence between rank-order instances ( f ) and boolean order instances ( b ). We obtain that and are equal by the observation that the projections performed on each of the datasets are essentially equivalent. For a given f , P ( C | f ) and P ( C | b ) obviously yield the same distributions, again because of the one-to-one transforma-tion. For the distributions P ( C | f  X  F ) and P ( C | b  X  other hand, we note that a projection in rank-order space preserves the relative order of the features even with the (possible) update in rank values. This in turn corresponds to the boolean order features that are projected in boolean order space. Thus, P ( C | f  X  F ) = P ( C | b  X  B ), and the result follows.

Lemma 1 implies that it is sufficient to consider selec-tion strategies on rank-order datasets and that analogous strategies using boolean order datasets will yield the same results.
We present four feature selection strategies (two taking an information-theoretic approach and the remaining two using discrete mathematics concepts), all of which follow the standard backward stepwise selection framework [9]. In this meta-algorithm, the boolean function cond ( F i ) ei-ther monitors subset size or subset divergence. The func-tion h ( F i ) is the feature selection function for this selection strategy. This function returns a feature from F i in regu-lar feature space, but uses rank order space in its selection process.

Alg 1: (GreedyKL) Use rank-order space, and greed-ily choose the feature that yields the minimum feature sub-set divergence when compared against the original dataset. That is, choose h ( F i ) = F k that minimizes  X  F i  X  X  F
Alg 2: (KS) Adapt the Koller-Sahami algorithm [12] for use in rank-order space. First, find (approximate) Markov blankets for all features in the Bayesian network of rank-order features (and class) implied by T R . A Markov blan-ket for a set of features F  X  is another set of features G whose values, if known, render F  X  independent of all others (i.e., F  X  F  X   X  G ) [16]. This term arises from the graphi-cal models literature where a network encodes conditional independencies, and random variables satisfying the above definition form a  X  X lanket X  around the given set of features. In the Koller-Sahami approach, we remove the feature F k , whose Markov blanket M k yields the minimum feature sub-set divergence when compared against M k  X  F k . That is, h ( F i ) = F k that minimizes  X  M k with respect to M k  X  F
Alg 3: (Spoilers) A third technique uses the notion of spoilers defined earlier. We use rank-order space and remove the feature with the highest spoiler count. That is h ( F F k that maximizes tsp ( F k ).

Alg 4: (CDV) The CDV algorithm aims to represent the dataset using the temporal signatures of the classes, and then proceeds to remove features based on their ability to maintain this representation. To construct it we create a set S =  X  i S k ( c i ) containing the centers of all the classes and then construct a distance vector, D , of all the pairwise dis-tances of the centers in S . D contains k  X  | C | 2 components. Then, for each feature we remove it and recompute this dis-tance vector. In the meta algorithm h ( F i ) = F k , where F is the feature that yields the minimum Euclidian distance between the recomputed distance vector and D , the original distance vector.
We now present experimental studies with the above fea-ture selection strategies, including descriptions of datasets, interpretation of results, and discussion.
We generate synthetic datasets based on the definitions of irrelevant and redundant features presented earlier. We begin by creating a base set of strongly relevant features and then add a number of redundant and irrelevant features to see if our feature selection algorithms identify and remove them correctly.

Collapsible permutations. Two permutations  X  i and  X  j are collapsible for k if removing  X  i ( k ) and  X  j ( k ) causes the resulting ranks of the permutations to be equal. For example, let  X  i = (4 ; 1 ; 3 ; 2) and  X  j = (3 ; 1 ; 4 ; 2) then p p are collapsible for k = 3 (i.e., the third element) because the ranks after removal of p i (3) and p j (3) yields (3 ; 1 ; 2).
To design a feature F i that is strongly relevant, we simply ensure that removing it will cause the collapse of one or more instances and that the other features are irrelevant when F is strong. Each strong feature F i is designed pertinent to a class distribution made up of two classes c i 1 and c i 2 algorithm to generate the set of base features is: Algorithm 1 Generate-Synthetic-Data ( n , r ) Input: n : number of strongly relevant features; Output: returns n  X  ( n + 1) instances 1. for i  X  1 to n do 2. p  X  pick a random feature permutation 3. q  X  gen permutation to collapse with p for k = i 4. ins  X  gen n permutations by swapping i with all 5. ins  X   X  replicate ins  X  X  p; q } r times 6. insc  X  assign classes c i 1 or c i 2 randomly to ins  X  Irrelevant features are incorporated into the base dataset by considering each instance separately and by uniformly inserting the new feature into the existing order implied by the instance.

A redundant feature is incorporated into the dataset by picking a subset of existing strong features F  X  and using a pure function distorted by normal noise g ( F  X  )+  X  to generate the ranks for the new feature. In this paper we insert linear features of the form aF i + b +  X  or non-linear features of the form a sin( bF i + c )+  X  . Normal noise is generated with a = 0,  X  = 1. For the following experiments, we selected n = 12 strong features and r = 3 repetitions as input to the base features generation algorithm. We then added irrelevant and redundant features as necessary to this base set of features and instances.

Our first experiment compares the performance of all the algorithms in terms of the number of redundant features they are correctly able to identify. The data consists of 8 redundant features, in which 4 are linear and 4 are non-linear. The linear redundant features are generated using F r = aF i + b +  X  , where a and b are random variables tak-ing values between (1 ; 5) and  X  is a normal distribution with = 0 and  X  = 1. Non-linear redundant features are gen-erated using F r = a sin( bF i + c ) +  X  where a , b and c are random variables taking values between (2 ; 6), (1 ; 5), and (1 ; 5) respectively. We generate 100 trials and calculate the percentage of features correctly identified as redundant, F (the dependent); and the percentage of features incorrectly identified as redundant, F i . We see from Fig. 6 (a larger yel-low strip is better) that CDV (96%) and Spoilers (87.3%) are the only feature selection strategies that perform well in detecting the actual redundant variables, whereas all the other strategies tend either to detect F i instead of F r none at all. Although, Spoilers performs as well as CDV it is important to note that Spoilers itself is intractable with a larger number of features because of the use of Kendall tau distance to compare two permutations.

Our second experiment compares the performance of all the algorithms in terms of the number of irrelevant features they are correctly able to identify. Here CDV (37.5%) leads in the ability to detect irrelevant features correctly, while all the other algorithms fail to detect even one (see Fig. 6).
While CDV performs well on redundant features it does not perform as well in detecting irrelevant features. To get an intuition on why this should be true, consider the na-ture of the permutation centers and the induced pairwise distance vector. A center of a class is the expected ordering of the features and if a feature has little variance within a class, its removal will have little effect on the resulting cen-ter. However, if the feature varies a lot then its removal will completely alter the expected order of the features. This is the case with irrelevant features. Removing them will cause a large change in distances between centers. Since CDV re-moves features that exhibit the smallest change from the original distance vector, the irrelevant feature continues to remain in the dataset possibly tainting the resulting signa-tures. This forms the motivation for an improved feature selection approach.

To determine irrelevancy of a feature F r we adopt a tradi-tional information-theoretic method to rank the features by the reduction in entropy gained from conditioning the data on F r . We calculate this as where H is the entropy. The higher the value of Z ( F r ) the higher the likelihood that F r is independent of the features and the class distribution and therefore irrelevant. In the meta-algorithm h ( F r ) = F k , where F k is the feature that yields the maximum Z value.

Alg 5: (CDV+) To mitigate the problems caused by irrelevant features, we first remove features based on Z as defined above, and then perform CDV on the rest of the dataset. We refer to this algorithm as CDV+ .

We now test all the algorithms under the presence of both 4 redundant and 4 irrelevant features on top of the 12 strong features. Over a 100 trials we find that CDV has a positive identification rate of 67.7% while CDV+ has a positive iden-tification rate of 82.9% showing a clear indication that CDV is improved by removing irrelevant features first using a sep-arate information-theoretic algorithm (see Fig. 6). We thus use CDV+ as the feature selection algorithm to process the yeast cell cycle data described next.
The yeast cell cycle data comes from the mathematical model of the budding yeast cell cycle by Chen et al. [5, 4]. As stated earlier, biochemical interactions are converted to ODEs, which are simulated for different time steps. The concentrations of 45 molecules (recall that these are the fea-tures) are recorded for 1000 time steps. In addition to the regular cell cycle data (known as  X  X ormal X  or  X  X ild type X ), we also have access to several mutant cell cycles (where one or more key molecules are perturbed and the cell adopts an aberrant state).

The cell cycle is a highly regulated sequence of events: it consists of DNA replication (S phase) and replication of the cell (M phase) separated by two gap phases (G1 and G2). The entire machinery is controlled by four types or categories of molecules [22] as indicated in Fig. 8 (A) (as  X  X K X ,  X  X P X ,  X  X DK X  and  X  X nemies X ).

The entire cell cycle is orchestrated by the coordination among these four kinds of molecules, and that is shown in Fig. 7 (A) where the small squares mark the cell cycle phase. Some of the 45 molecules that we have in our dataset fall un-der those categories. So our primary task is to see if we can detect the molecules that belong to those four categories. In other words, through our experiments, we seek to determine if conclusions of the form shown in Fig. 8 can be automati-cally obtained.

Fig. 8(B) shows how the different types of molecules are active at the four phases of the cell cycle. The cell cycle machinery works like a  X  X ee saw X  balance between the G1 phase and G2/M/S phases [22]. In other words, most of the molecules that are active in G1 are inactive in the other phases and vice versa. So, the mutual antagonistic nature of the cell cycle phases is expected to be evident from the rank ordering of molecules from our feature selection algorithm. Figure 8: (A) Groups of molecules that control and regulate the cell cycle. (B) Concentrations of these key molecules across all four phases: G1, S, G2 and M. Figure courtesy John Tyson and Bela Novak [22].

In applying feature selection to the cell cycle data, the questions we seek to answer are: 1. Which are the biochemical molecules that play signif-2. Is the relative rank order of the reported molecules
Category of molecules in Fig. 8 CDK Clb2-Cdc28, Clb5-Cdc28 Enemies Cdh1, Sic1, Cdc6 SK Cln1,Cln2,Cln3-Cdc28 EP Cdc20, Cdc14 Figure 9: Descriptions of molecule categories from Fig. 8.
Category of molecules identified as signatures for each phase CDK Clb2T, Clb5T Enemies Cdh1, Sic1, Cdc6
SK SBF (which is a transcription
EP Mcm1 (which is a transcrip-Figure 10: Descriptions of molecule categories from Fig. 8 identified by our approach. 3. Can we apply the same analysis to mutant cells (where From analysis of the wild-type (normal) cell cycle data, CDV+ X  X  results correspond excellently to three of the groups reported in Fig. 8(A). The reported sets of molecules for each of the category in Fig. 8(A) are listed in Fig 9. We report the minimal set of significant molecules that are sufficient to distinguish among the four phases of the cell cycle. Compare Fig. 9 with Fig. 10. One of the molecules missed by our anal-ysis is Cln2, which represents the category  X  X K X . However, this fact was nullified by the presence of another molecule (SBF), which controls Cln2. Comparing Fig. 9 with Fig. 10 reveals that CDV+ successfully identifies all four categories of molecules that the cell cycle machinery is composed of. A mutant cell basically means an abnormal cell. Four mu-tants for the cell cycle were generated by changing parame-ters in the ODEs corresponding to the molecule being knocked out [5]. These four mutants cause the cell cycle to arrest at different phases.

The collected dataset comprises 5000 instances with 45 feature-values, representing the concentration of each of the molecules at a given time-point. We ran CDV+ with a single centroid in each class. After removing each feature we ran a naive Bayes classifier and tracked its accuracy. Fig. 12 illustrates that the accuracy of the naive Bayes clas-sifier remains almost uniform until the algorithms remove 35 features, which gives us a sense of when to stop re-moving features. We also note that Fig. 12 shows similar accuracies when running the other algorithms. However, GreedyKL and KS do not remove the correct molecules. On the contrary, the insufficiency of purely information-based removal becomes evident when we inspect the actual molecules removed X  X ic1, Cdh1, Mcm1, Cdc20, and Cdc14, many of the key players themselves. Figure 12: Accuracy of naive Bayes classifier vs feature re-moved.
 We validate our results using precise biological facts. Read-ers who are slighly unfamiliar with the exact molecule names that are mentioned here can picture them as placehold-ers from Figure 10 and focus on just the interconnection between them resulting in the observed behavior. Names of molecules are written exactly as they appear in Fig. 8 through Fig 11.
We have introduced the novel KDD problem of network comprehension and cast it as feature selection in rank-order space followed by summarizing the remaining features into temporal signatures. We have presented five specific feature selection algorithms for removing redundant and irrelevant features in rank-order data: our results demonstrate that rank-order data are better analyzed using an order-theoretic algorithm (CDV+) versus traditional information-theoretic algorithms (e.g., KS). Both our synthetic and real-world studies reveal that CDV+ is the best performing algorithm in terms of picking out the redundant and irrelevant features, and not just maintaining a high classification accuracy rate.
Our future work is focused on delving further into net-work comprehension goals. While temporal signatures yield a great deal of insight into the functioning of cellular biol-ogy, taking into account the regulatory mechanisms, for ex-ample, can help us assess if the rank order changes from one phase to another are triggered by or correlate with changes in some regulatory molecules. Ultimately, just like an elec-trical engineer uses metaphors of amplifiers, oscillators, and switches to comprehend an unknown circuit, we wish to de-sign decompositions of biochemical circuits as compositions of input-output signals mediated by protein molecules. The initial steps toward these ideas have been taken [18, 19] and this area is ripe for the development and application of data mining methods.
 This work is supported in part by US NSF grants CCF-0937133, CNS-0615181, ITR-0428344, and the Institute for Critical Technology and Applied Science (ICTAS), Virginia Tech. [1] A. Amon, M. Tyers, B. Futcher, and K. Nasmyth. [2] M.N. Arbeitman et al. Gene expression during the life [3] A.L. Blum and P. Langley. Selection of relevant [4] K.C. Chen, A. Csikasz-Nagy, B. Gyorffy, J. Val, [5] K.C. Chen et al. Integrative analysis of cell cycle [6] W.W. Cohen, R.E. Schapire, and Y. Singer. Learning [7] T.M. Cover and J.A. Thomas. Elements of [8] X. Geng, T. Liu, T. Qin, and H. Li. Feature selection [9] T. Hastie, R. Tibshirani, and J. Friedman. The [10] S. Hoops, S. Sahle, R. Gauges, C. Lee, J. Pahle, [11] T. Kamishima. Nantonac collaborative filtering: [12] D. Koller and M. Sahami. Toward optimal feature [13] G. Lebanon and J. Lafferty. Cranking: Combining [14] G. Lebanon and J. Lafferty. Conditional models on the [15] J.I. Marden. Analyzing and Modeling Rank Data . CRC [16] J. Pearl. Probabilistic Reasoning in Intelligent [17] W. K. Purves, G.H. Orians, and H.C. Heller. Life: [18] N. Ramakrishnan and U.S. Bhalla. Memory switches [19] N. Ramakrishnan, U.S. Bhalla, and J.J. Tyson. [20] D.J. Slotta, J.P. Vergara, N. Ramakrishnan, and L.S. [21] C. Spearman. The proof and measurement of [22] J.J. Tyson and B. Novak. Temporal organization of [23] J.J. Tyson, B. Novak, K.C. Chen, and J. Val. [24] M. Vass, N. Allen, C.A. Shaffer, N. Ramakrishnan, [25] R. Visintin, S. Prinz, and A. Amon. CDC20 and [26] X.L. Zhang, H. Begleiter, B. Porjesz, W. Wang, and
