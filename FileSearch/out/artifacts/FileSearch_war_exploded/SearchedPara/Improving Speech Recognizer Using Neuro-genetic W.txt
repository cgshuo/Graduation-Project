 Speech-driven text retrieval or also known as spoken query information retrieval aims to search information in a textual document collection using a spoken query. Interests in providing speech access to web contents have increased steadily as the web has become an important source of information. Speech is natural for most people, and thus it can provide a more usable and favorable interaction. Different approaches have been proposed to allow access to web conten ts using speech. Spoken dialogue system is a traditional option; however the textual information lacks the required structure. The limitations of the speech channel are also a problem, because it is not possible to send much information over it. One solution is to extend an existing web browser using speech [1]. Other approaches are based on the automatic generation of dialogue systems for specific web content [2]. Finally, spoken queries information retrieval (IR) systems that use speech as input to an IR engi ne provided a natural solution to overcome the major limitations of the speech channel. Speech recognition which decodes human voice to generate transcri ptions has of late become a practical technology. It is feasible that speech recognition is used in real world computer-based applications, specifically, those associated with human language. 
A related area of research is spoken document retrieval (SDR), whose aim is inverse: to index and retrieve relevant items from a collection of spoken audio good results have been obtained [3]. However, spoken query information retrieval is a more difficult task, because spoken queries contain less redundancy to overcome speech recognition errors. Initiated partially by the TREC-6 spoken document retrieval (SDR) track [4], various methods have been proposed for spoken document retrieval. Barnett et al. [5] performed co mparative experiments related to spoken query text retrieval, where an existing speech recognition system was used as an input interface for the INQUERY text retrieval system. They used as test inputs 35 queries collected from the TREC 101-135 topics, dictated by a single male speaker. Crestani [6] also used the above 35 queries and showed that conventional relevance feedback techniques marginally improved the accuracy for spoken query text retrieval. 
The abovementioned cases focused solely on improving text retrieval methods and did not address problems of improving speech recognition accuracy. In fact, an existing speech recognizer was used with no enhancement. In other words, speech recognition and text retrieval modules were fundamentally independent and were simply connected by way of an input/output protocol. However, since most speech recognizers are trained based on specific domains, the accuracy of speech recognition across domains is not satisfactory. Thus, as can easily be predicted, in cases of Barnett et al. [5] and Crestani [6], a relatively high speech recognition error rate were detected thus considerably decreased the retrieval accuracy. Additionally, speech recognition with a high accuracy is essential for interactive retrieval. 
Motivated by these problems, we pr oposed a method to integrate speech recognition and retrieval methods where our aim is to improve speech recognizer and then allow the IR engine to retrieve text documents relevant to the given spoken queries. Since spoken queries language is dependent on the target collections, we adapt statistical language models used fo r speech recognition based on the target collection, so as to improve both the r ecognition and retrieval accuracy. The direction of this work is composed into several sections. Section 2 provides an overview of the speech recognizer. The details of the meth ods and implementation are described in Section 3. Section 4 describes the experimental and performance evaluation of the speech recognizer and retrieval. In Section 5, the experimental results are reported and discussed. Finally, the conclusion is drawn in Section 6. In recent years, there has been an increasing interest in classification approaches to improve the recognition of speech sounds. Various approaches have been experimented to develop the speech recognizer or classifier and over the years, three speech recognition approaches are established. Dynamic time warping (DTW) is the oldest approach and is an algorithm for measuring similarity between two speech sequences which may vary in time or speed [7][8]. However, this technology has been replaced by the more accurate Hidden Markov Model (HMM) that has become the primary tool for speech recognition since 1970s. Hidden Markov Model (HMM) is a statistical model in which the system being modeled is assumed to be a Markov process with unknown parameters. This algorithm is often used due to its simplicity and feasibility of use.

However in late 1980s, artificial intelligent (AI) based approaches were considered for training the system to recognize speech using artificial neural network (ANN) algorithms. This technology is capable of solving much more complicated recognition tasks, and can handle low quality, noisy data, and speaker independence. Researchers have started to consider ANN as an a lternative to HMM approach in speech recognition due to two broad reasons: 1) speech recognition can basically be viewed as a pattern classification problem, and 2) ANN can perform complex classification tasks [9]. Given sufficient input-output data, ANN is able to approximate any continuous function to arbitrary accuracy. However, the main obstacle faced by NN model is a longer learning time when the data set becomes larger. NN learning is highly important and is still undergoing intense research in both biological and artificial networks. A learning algorithm is the heart of the NN based system. Error Back-Propagation (EBP) [10] is the most cited learning algorithm and a powerful method to train ANN model [11]. However, there are several drawbacks in the EBP learning algorithms; where the main basic defect is the convergence of EBP algorithms which are generally slow since it is based on gradient descent minimization method. Gradient search techniques tend to get trapped at local minima. 
Recently, many researchers tried to overcome this problem by using stochastic algorithm, such as Genetic Algorithms (GA) [12], since they are less sensitive to local minima. Genetic Algorithm (GA) based learning provides an alternative way of learning for the ANN, which involves controlling the learning complexity by adjusting the number of weights of the ANN. However, GA is generally slow compared to the fastest version of gradient -based algorithms due to its nature of finding a global solution in the search space. Thus, to have better convergence time, we proposed the fusion of global search GA method with matrix solution second order gradient based learning algorithm known as Conjugate Gradient (CG) in a two-layer Feed-Forward NN architecture. Our proposed neuro-genetic weights connection strategy method combined GA in the first layer and CG in the second layer to achieve optimum weights for the feed-forward network. Our algorithm aims to combine the capacity of GA and CG in avoiding local minima and the fast execution of the NN learning algorithm. The general idea towards this work is to generate a speech recognizer capable of producing accurate text transcription of isolated spoken Malay utterances for textual documents retrieval. Malay language is a branch of the Austronesian (Malayo-Polynesian) language family, spoken as a native language by more than 33,000,000 persons distributed over the Malay Peninsula, Sumatra, Borneo, and numerous smaller islands of the area and widely used in Malaysia and Indonesia as a second language [13]. The improved speech recognizer is done by implementing genetic algorithm (GA) with Artificial Neural Network (ANN) to determine the suitable network architecture and to improve the recognition performance in an offline mode. The overall process of this model is described as a block diagram as shown in Fig. 1.
All the waveform speech input went through the first block of speech processing techniques that involved spectral analysis, speech boundary or endpoint detection methods, time axis normalization and featur e extraction to form cepstral coefficients of vector input signals. The pre-processing block designed in speech recognition aims towards reducing the data complexity before the next stage start to work with the data. parameters: Energy Zero-Crossing Feature (EZF), Energy-Entropy Feature (EEF) and variance Energy-Entropy (vEE) [14][15]. These different feature parameters data were words segment detection [14][15]. A two-layer feed-forward neural network with one hidden layer and one output layer was used in this work. Only one hidden layer was utilized as it has been proven that an ANN with one hidden layer was sufficient in performing process mapping arbitrarily [16]. Our approach combined genetic algorithm (GA) with matrix solution methods to achieve optimum weights for hidden and output layers. The proposed method is to apply genetic algorithm (GA) in the first layer and conjugate gradient (CG) method in the second layer of the FF ANN architecture as depicted in Fig. 2. These two methods are combined together using proposed dynamic connection strategy, where a feedback mechanism exists for both algorithms. 
In this study, trial and error approach was used to determine the optimum topology of the network. It was found that the optimum topology of the network could be best estimated using a network with 20 hidden neurons. Using this network topology, the training and validation errors were 1.9143 x 10 -5 and 1.6126 x 10 -4 respectively. 
In this work, we proposed two variations of genetic algorithm (GA) that can be applied for weights searching in the first layer of ANN. The first strategy is the primitive or straight GA which is applied to ANN phenotype using a direct encoding scheme and we followed exactly the work done by [17]. This GA methodology used the standard two point crossover or interpolation as recombination operator and Gaussian noise addition as mutation operator. Meanwhile, as the second strategy, we proposed slight variations of the standard GA known as m utation G enetic A lgorithm ( mGA ), where the only genetic operator to be considered is mutation. The mutation is applied using a variance operator. The stepwise operation for mGA can be described as follows: Step 1: Uniform distribution technique will be used to initialize all the hidden layer Step 2: The fitness for the population is calculated based on the phenotype and the Step 3: Each individual population vector ( i w , i  X  ), i =1, 2,...,  X  creates a single Step 4: Repeat step 2 , if the convergence for the mGA is not satisfied. 
Meanwhile, the weights for the output layer is computed using the conjugate gradient (CG) method where the output of the hidden layer is computed as sigmoid function [f(.)] for the weighted sum of its input. The CG algorithm is a numerical optimization technique designed to speed up the convergence of the back-propagation algorithm. It is in essence a line search technique along any set of conjugate descent approach. The power of the CG algorithm comes from the fact that it avoids the calculation of the Hessian matrix or seco nd order derivatives, yet it still converges to the exact minimum of a quadratic function with n parameters in at most n steps [11]. The conjugate gradient algorithm starts by selecting the initial search direction as the negative of the gradient as in Equation (7) and (8). where x is the vector containing the weights and biases and function, that is the mean square error (MSE). The search directions ( conjugate with respect to a positive definite Hessian matrix if, where A represents the Hessian matrix [ The above condition can be modified to avoid the calculation of the Hessian matrix for practical purposes and is given as in Equation (10). The new weights and biases are computed by taking a step with respect to the learning rate ( i  X  ) along the search direction that mini mizes the error as in Equation (11). where, the learning rate ( i  X  ) for the current step is given by Equation (12). where the scalar ( i  X  ) which can be viewed as a momentum added to the algorithm [16] is given by one of three common choices where we adopted Fletcher and Reeves formula for the current implementation. the minimum, or a predefined error criterion is achieved. As is obvious from the above steps, the conjugate gradient algor ithm requires batch mode training, where weight and bias updates are applied after the whole training set is passed through the network, since the gradient is computed as an average over the whole training set [11]. In this work, since the network architecture is a two-layer feed-forward ANN, the input nodes in the first layer started with the range compression for the applied input (based on pre-specified range limits) so that it is in the open interval (0,1) and The hidden nodes performed a weighted sum on its input and then passed through the sigmoidal activation function before sending the result to the next layer, which is the output layer. The output layers also performed the same weighted sum operation on its input and passed through the sigmoidal activation function to produce the final result. The vital and challenging task is to find suitable rules of joining two different techniques for the given ANN architecture. The combination of the GA and the CG method provides much possibilities of joining the two different methods [17]. 
The above flowchart (Fig. 3) illustrates our proposed method: dynamic connection strategy for combining these two methods, where the CG method is called after one generation run for GA/mGA method. The best fitness population is halved and the upper half is saved as the weights for output layers. Then, the GA/mGA ran for the remaining generation. We conducted 100 experiments to evaluate the performance of the spoken query information retrieval. First, we described the experimental set-up and the results of our proposed method compared to the st andard ANN error back-propagation (EBP) baseline system. We also compared the standard GA+ANN model against our proposed mGA+ANN model. For evaluating speech recognition performance we used the standard word error rate (WER) as our metric. On the other hand, we used precision and recall (Eq. 14-17) with respect to manual transcription to evaluate the retrieval performance. Let Correct ) ( q be the number of times the query q is found correctly, Answer ) ( q be the number of answers to the query q , and Reference ) ( q be the number of times q is found in the reference. We computed precision and recall rates for each query and reported the average over all queries. The measurement was not weighted by frequency, for example, each query Q q  X  is presented to the system only once, independent of number of occurrences of q in the transcriptions. All experiments were conducted on the hansard documents of Malaysian House of Parliament. The hansard documents consists of Dewan Rakyat (DR) Parliamentary debates session for the year 2008. It contains spontaneous and formal speeches and it is the daily records of words spoken by 222 elected members of DR. The hansard documents comprises of 51 live videos and audio recording files (.avi form) of daily parliamentary session and 42 text files (.pdf form). Each part of parliamentary session contains six to eight hours spoken speeches surrounded with medium noise condition or environment (  X  30 dB), speakers interruption (Malay, Chinese and Indian races) and different speaking styles (low, medium and high intonation or shouting). The reason of choosing this kind of data is due to its naturalness and spontaneous speaking styles during each session. 
For the purpose of this study, eight hours of one day Parliament session document was selected as our experimental data. The document collection consists of 12 topics, 120 speakers and a total of 148,411 spoken words. After thorough data analysis, the most frequently words were determined from eight hours of one day Parliament session document. The quantitative information shows that only 50 words were most commonly used by the speakers with more than 25 repetitions. The selection of 50 words are the root words formed by joining one or two syllables structures (CV/VC  X  consonant or vowel structure) that can be pronounced exactly as it is written and can control the distribution of the Malay language vocalic segments. However, the vocabulary used in this study consisted of seven words as presented in Table 1 for the purpose of spoken queries input. Each word was selected according to their groups of syllable structure with maximum 25 repetitions and spoken by 20 speakers. Thus, the speech data set consists of 3500 utterances of isolated Malay spoken words. For the experiments, all the audio files were re-sampled at a sampling rate of 16 kHz with frame size of 256 kbps. All signal data were converted into a form that is suitable for further computer processing and analysis. A total of 3500 data collection were used as inputs for modeling purposes. The data were equally divided into training and testing set. The network was obtained after undergoing a series of training using two different algorithms. In order to improve network generalization ability, early stopping techniques was applied to CG training. In this technique, validation error was monitored during the training process. When the validation error increased over a specified number of iterations, the training was stopped to prevent over fitting. For weights evolved using GA, the number of generation was used to stop the iteration. 
The word recognition results obtained based on 95% confidence interval for the training and testing sets of all the methods used in the study is depicted in Fig. 4. Fifty experiments were done to choose perfect Hidden Neurons Number (HNN) for the ANN model. We identified the network configuration with the best HNN is (50-20) of multilayer feed-forward (FF) network structure. 
The maximum number of epochs for network training was set to 1,000 after observing that convergence was reached within the range. As can be seen from Fig. 4, the proposed algorithm using mutation Genetic Algorithm (mGA) and Conjugate Gradient (CG) yielded 95.39% of overall classification rate. The proposed method outperformed the other two training networks where 91.57% was obtained from the fusion of standard GA and CG. Meanwhile, standard ANN using EBP algorithm yielded 89.54% that is the lowest among the other two algorithms. Although the difference in overall classification performances between standard GA and CG (GA+CG) and the mGA with CG (mGA+CG) may deemed insignificant, the difference between the two algorithms becomes more significant when the individual confusions matrices and 95% confidence interval plots are examined. 
The degradation in recognition is very noticeable on all the vocabulary words except for the word  X  X DA X  and  X  X ANG X . The spreads in confidence intervals of the words  X  X OLEH X  and  X  X NTUK X  obtained with the GA+CG algorithm are 16.25% and 18.55% respectively. Whereas the spreads for the same words obtained with the mGA+CG method are 5.6% and 3.7% respectively. Therefore, the mGA+CG lead to more accurate and reliable learning algorithm for training of FF network than the standard GA+CG algorithm. Since the calculation started with random initial weights, each run produced different results even though the network architecture was maintained. Thus, in order to obtain an optim al solution, repeated runs were practiced and only the best result was recorded. 
As mentioned earlier, precision and recall methods were used to evaluate retrieval performances. Prior to evaluation, the spoken query was transcribed by the speech recognizer and the best hypothesis was processed by the IR engine to obtain the list of documents relevant to that query. Each of the speech recognizer X  X  performance is measured and the result is given in Table 2. As expected, we obtained very good performance on the proposed method of mutation GA and CG (mGA+CG) fusion. It is interesting to note that as precision and recall rates increase indicating a better retrieval performance, the WER reduces indicating improved word recognition accuracy. Owing to this fact, GA combined with CG is proven to possess probable advantages. Moreover, the performances in the validation sets were considered better than standard ANN using EBP algorithm and this proved that mGA+CG fusion is a reliable learning algorithm and has high potential for further improvement for spoken query information retrieval.
 Speech recognizer model WER Precision Recall This paper reports on an experimental study of improving speech recognition engine for a better retrieval of text document retrieval based on spoken queries. Despite the limitations of the experimentation presented here, the results showed that the use of speech recognizers at the front end of spoken query information retrieval is quite robust with low error rate of 4.62%. Based on the results obtained in this study, ANN is an efficient and effective empirical mode ling tool for estimating the speech process variable by using other easily available process measurements. The use of multilayer feed-forward network with delay values in model input variables is sufficient to give estimation to any arbitrary accuracy. Even though the conventional EBP method is widely used, GA is preferable as the optimal solution searching is population based using gradient information. As presented in this study, integrating mutated GA with CG as second order gradient based learning method can also improve mean square error (MSE) to reduced WER of the speech recognizer and increased the recognition rate up to more than 95%. Despite the success of the proposed neuro-genetic weight connection strategy, the speech recognition model has room for improvement. Much effort is needed to improve GA method for speeding up the learning process in ANN model for the purpose of spoken query IR systems. Acknowledgements. This research was supported by Research Management Instiitute (RMI), UiTM under research grant: 600-RMI/DANA 5/3/RIF (403/2013). 
