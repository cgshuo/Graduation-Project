 ORIGINAL PAPER Solen Quiniou  X  Mohamed Cheriet  X  Eric Anquetil Abstract In this paper, we present a framework to han-dle recognition errors from a N -best list of output phrases given by a handwriting recognition system, with the aim to use the resulting phrases as inputs to a higher-level appli-cation. The framework can be decomposed into four main steps: phrase alignment, detection, characterization, and cor-rection of word error hypotheses. First, the N -best phrases are aligned to the top-list phrase, and word posterior proba-bilities are computed and used as confidence indices to detect word error hypotheses on this top-list phrase (in comparison with a learned threshold). Then, the errors are characterized into predefined types, using the word posterior probabilities of the top-list phrase and other features to feed a trained SVM. Finally, the final output phrase is retrieved, thanks to a correc-tion step that used the characterized error hypotheses and a designed word-to-class backoff language model. First exper-iments were conducted on the ImadocSen-OnDB handwrit-ten sentence database and on the IAM-OnDB handwritten text database, using two recognizers. We present first results on an implementation of the proposed framework for han-dling recognition errors on transcripts of handwritten phrases provided by recognition systems.
 Keywords Handwriting recognition  X  Error processing  X  Confidence score  X  Word posterior probability  X  Error characterization  X  Language model 1 Introduction With the emergence of new devices ( e.g. smartphones, inter-active whiteboards, or digital pens) and the increase in infor-mation channels, more and more multimedia data (audio, video, electronic texts, handwritten texts...) are produced days after days. In order to exploit this amount of information, high-level applications need to be designed. In recent years, applications based on natural language processing such as information retrieval, information extraction, summarization or categorization have been investigated for  X  X lean X  text doc-uments [ 39 ]. However, multimedia data, such as blogs, SMS, e-mails, or transcripts from recognition systems (automatic speech recognition systems, handwriting recognition sys-tems, or statistical machine translation systems) are generally  X  X oisy X . Thus, it is more difficult to apply the aforementioned techniques to these noisy data.

Recently, there has been an interest for studying problems relating to processing text data from noisy sources. Tech-niques from natural language processing applications thus need to be adapted to deal with such noisy data. In the field of noisy data processing, works have been focusing more partic-ularly on dealing with text documents [ 39 ], such as blogs..., or transcripts from automatic speech recognition (ASR) sys-tems [ 13 , 36 ]. However, there have been few works that deal with transcripts from handwriting recognition (HWR) sys-tems [ 26 , 35 ]. The main issue with transcripts, whether they are from an ASR or a HWR system, is that they contain tran-scription errors. This is even more a problem when the tran-scripts are used as inputs to higher-level applications, such as information retrieval or text summarization. It is, then, of main interest to detect these recognition errors so that the higher-level system can deal with them, and/or to correct them so as to improve the performance of the higher-level system.

In this paper, we set our work in the case of handling rec-ognition errors on handwritten transcripts, in order to make it easier for a higher-level system to process these transcripts that are given as input. Thus, we present a framework to han-dle recognition errors on phrase transcripts, given as output by a handwriting recognition system (here, a phrase may be either a sentence or a text). The proposed error handling framework consists of four steps: 1. alignment of the transcription results, given in a N -best 2. detection of error hypotheses on the top-list transcription, 3. characterization of error hypotheses into predefined 4. correction of error hypotheses, based on the error types In the proposed framework, we want to be as independent as possible to the recognition system used to provide the input transcripts. The only constraint on the recognizer output is that it has to be a N -best list of phrase hypotheses (usually, the list is ordered based on a recognition score); additional information on each phrase may also be given ( e.g. its rec-ognition score, the recognition score of each of its words, or information on the phrase segmentation into its words). Here, we use on-line handwriting recognition systems, but off-line handwriting recognition systems may also be used, or even automatic speech recognition systems (in this latter case, rec-ognition error types may have to be changed, to reflect speech recognition errors). A preliminary version of this error han-dling framework was presented in [ 34 ]; here, we generalize it to consider recognizers as  X  X lack boxes X , without any further assumptions on the information they provide.
 The remainder of this paper is organized as follows. Section 2 discusses previous work on the processing of rec-ognition errors in handwriting recognition (on-line and off-line), but also in the field of speech recognition, while Sect. 3 presents the proposed framework. Sections 4 , 5 , 6 , and 7 describe the various parts of the framework, i.e. the align-ment of the phrase hypotheses from a N -best list into a word graph, the detection of word error hypotheses, the character-ization of these error hypotheses, and the correction of the initial top-list phrase, respectively. Section 8 gives an experi-mental evaluation of the proposed framework. Finally, some conclusions are drawn in Sect. 9 . 2 Related work In this section, we present an overview of previous works dealing with recognition errors, in the context of off-line and on-line handwriting recognition. We also review works in the field of automatic speech recognition, since most of the approaches proposed in that domain can be applied to hand-writing recognition (in fact, many approaches used in hand-writing recognition come from speech recognition). 2.1 Handwriting recognition In handwriting recognition (whether off-line or on-line), there have been few work on the detection, characteriza-tion and/or correction of recognition errors, especially at the phrase level.

In fact, some handwriting recognition systems detect rec-ognition errors by rejecting recognition results considered not sufficiently reliable. This rejection step can be performed using  X  X nti-models X  as in [ 27 ], where  X  X nti-letter models X  are used to identify incorrectly recognized words in hand-written sentences. The approach most commonly used is to compute confidence indices on words and then to compare them with a threshold to decide whether or not the words have to be rejected. In [ 30 ], several confidence measures are presented, both at the letter and word levels but no linguistic information is used in the confidence measures considered. In [ 3 ], rejection strategies based on the N -best list obtained by varying the weight of a language model are considered; the authors rely on the fact that incorrectly recognized words are more sensitive to this weight variation. However, in all these approaches, words are only detected as correct or incor-rect: there is no further characterization of the errors, nor is there a correction step.

Other work has involved combining the outputs of various recognition systems: this approach can be viewed as a cor-rection step on the output of one of these recognition systems (usually, the one that achieves the best performance). Thus, in [ 2 ], the outputs of different recognition systems are com-bined into a transition network and a language model is used to help retrieve the most likely output text. However, sev-eral recognition systems are needed to achieve this. In other recognition systems, the use of several language models dur-ing the recognition step can be viewed as a correction step on a baseline recognition system with only one LM. Thus, in [ 29 , 32 ], a n -class LM is combined with another n -class model and with a n -gram model, respectively, whereas in [ 45 ], a stochastic context-free grammar is combined with a n -gram model: in both cases, the recognition system bene-fits from the use of the added language model. Finally, an original correction approach has been proposed in [ 12 ]. In this work, the correction task is considered as a translation task in which the source language is the output of the rec-ognition system (which may contain recognition errors) and the target language is the corrected output. Since the recog-nition system is considered as a black box that only gives an N -best list of candidate words for a handwritten word, the probabilities used in the correction model are estimated using the relative frequencies of each word and its trans-lated equivalents. Nonetheless, these probabilities strongly depend on the vocabulary of the handwritten sentences and need to be trained again if the vocabulary changes. Thus, this system may not be well adapted for applications where the vocabulary is not known, e.g. a freeform note-taking appli-cation. In such handwritten notes, out-of-vocabulary words are more likely to occur than in notes from off-line applica-tions where the whole vocabulary may be known in advance. Recent works also analyze the impact of recognition errors ( i.e.  X  X oise X ) on recognition tasks [ 26 ], or on higher-level tasks such as text categorization [ 35 ]. 2.2 Speech recognition In the field of automatic speech recognition, confidence mea-sures are frequently associated with the words of the recog-in machine translation tasks to assess the quality of the trans-lation, as in [ 40 ], where various measures are compared. Among all these confidence measures, word posterior prob-abilities have been shown to be among the best [ 5 , 23 , 41 ], and they can be combined with other information sources in a neural network [ 23 ],inaSVM[ 21 ], or using conditional random fields [ 13 ] to achieve even better results. These confi-dence measures can also be used to detect recognition errors by rejecting words, the value of which is below a considered threshold. In [ 44 ], the error detection step is composed of two levels: first, incorrect phrases are detected (using a SVM and various features at the sentence level), and then, for the phrases detected in this way, the words are classified as either correct or incorrect (also using an SVM, and this time vari-ous features at the word level). In our proposed framework, we use these word posterior probabilities to detect recogni-tion errors, but also to characterize them into different types (using other features in an SVM as well).

The ROVER framework [ 14 ] has been proposed in order to combine the outputs of several recognizers and then to cor-rect the output of the best of them. From the alignment of out-puts considered, this framework introduces a voting scheme to make the final choice among competing words from the various recognizers. The score given to each word combines the confidence indices given by all the recognition systems and the number of systems that outputted the word consid-ered (in [ 2 ], this framework is extended to take a language model into account during the voting step). Furthermore, as speech recognition systems are usually multi-pass systems, they use several language models with increasing complex-ity in various passes, since each recognition pass decreases the size of the search space. But, although different types of language models have been tried, n -gram models remain the most widely used. In [ 28 ], an interesting LM, called the word-to-category backoff LM , is presented: the category-based LM is used when the current word with its associated history is not estimated in the word-based LM (it is showed that it achieves better results than using only the word-based LM). Inspired by this work, in the final step of our framework, we combine a n -gram LM and a n -class LM (into a word-to-class backoff LM ) to correct the errors identified. Thus, rather than com-bining LMs, as previous work in handwriting recognition, we use an adequate LM based on the error type of the current word. 3 Architecture of the error handling framework Our framework, illustrated in Fig. 1 , is aimed at handling recognition errors on a N -best list of phrases given by a handwriting recognition system (phrases can correspond to sentences or texts, depending on the recognition system). This framework can be divided into four parts: alignment of the input phrases, detection of the word error hypotheses on the top-list phrase, characterization of the detected error hypotheses, and correction of those errors. The various parts are presented in the following sub-sections, and they will be described in greater detail in the rest of the paper. 3.1 Alignment of the input sentences First, the phrases of the N -best list given by the recogni-tion system need to be aligned. For this purpose, we use an incremental alignment algorithm based on a string matching algorithm. The resulting output of this alignment module is a word graph which will be used by the following modules. The alignment algorithm is presented in Sect. 4 . 3.2 Detection of the word error hypotheses To detect error hypotheses on the top-list phrase, a confidence index is computed for each of its words. Here, word posterior probabilities are computed from the input N -best list and are used as confidence indices, as presented in Sect. 5 . Each word in the top-list phrase is finally labeled as either an error hypothesis or a correct word, by comparing its confidence index with a learned threshold. 3.3 Characterization of the word error hypotheses Previously detected word error hypotheses are then charac-terized according to predefined error types: a word with an incorrect segmentation ( errSeg ), a word that is a substitution of the correct word that appears in the graph ( errSubst ), and a word that is a substitution of the correct word that does not appear in the graph ( errAbs ). As described in Sect. 6 , the confidence index for each error hypothesis and other fea-tures are given as inputs to a SVM classifier which is learned to output the corresponding error type for each word error hypothesis. Then, each word error hypothesis of the top-list phrase is labeled according to its characterized error type. Furthermore, these error types are used to prune the word graph, so that alternative word hypotheses only remain for words identified as errors that may be corrected. 3.4 Correction of the word error hypotheses Finally, a language model that efficiently combines a n -gram LM and a n -class LM (called word-to-class backoff LM ) is used on the pruned word graph to retrieve the corrected phrase. This phrase is the output of the whole framework. The combined language model, as well as its use on the pruned word graph, is presented in Sect. 7 . 4 Multiple phrase alignment using string edit operations In order to use information from the other phrases of the N -best list, these phrases need to be aligned with the top-list phrase. A word graph is thus built. In this section, we describe how the standard edit distance is used to match pairs of phrases so as to build the word graph that represents the alignment of all the phrases in the N -best list.

The word graph is built by iteratively aligning each com-peting phrase of the N -best list with the top-list phrase. First, an initial word graph is built with the top-list phrase: an edge is created for each of its words. Then, for each of the other phrases in the list, the Levenshtein edit distance [ 24 ] is computed between the top-list phrase and the phrase con-sidered: this edit distance computes the minimum number of edit operations (among substitutions , insertions , and dele-tions ) used to transform the phrase considered into the top-list phrase. The corresponding sequence of edit operations is used to create new edges and nodes, according to these operations (see Fig. 2 ):  X  substitution : an edge labeled y j is created, parallel to the  X  insertion : an edge labeled y j is created to be inserted  X  deletion :a null edge is created, parallel to the x i edge. In the case of the substitution operation, a new edge with a label y j is only created if it does not already exist in the word graph. If it does, information for the current word y as given for the current phrase, is added to the correspond-ing edge ( e.g. recognition score, where the recognition score associated with the edge becomes the maximal score among the corresponding words). In the upper part of Fig. 1 , there is an example of a word graph construction, from a 3-best list of phrase hypotheses.

This iterative alignment algorithm does not guarantee an optimal alignment with minimal edit costs. However, in prac-tice, it gives an adequate solution, as a trade-off between accuracy and computational complexity. 5 Detection of error hypotheses using confidence measures on words A common approach to detecting error hypotheses is annotat-ing confidence at the word level. Confidence indices are thus computed for each word of the N -best phrase list, using the N -best list itself and the word graph built from it (see Sect. 4 ). These confidence indices are then used to detect potential recognition errors on the top-list phrase. In this section, we first describe how word posterior probabilities are computed, to be used as word confidence indices. Then, we present an approximation of these word posterior probabilities, depend-ing on the information provided by the recognition system (as given in the N -best lists). Finally, we introduce the general error detection approach, which compares word confidence indices to a learned threshold. 5.1 Word posterior probabilities as confidence indices The posterior probability of a word corresponds to the sum of probabilities of all the phrases of the N -best list that contains this word, at the same position. Word posterior probabilities are commonly used as confidence indices in tasks such as speech recognition or machine translation, where they were shown to be among the best confidence measures [ 5 , 23 ]. Nevertheless, these probabilities are not widely used in the field of handwriting recognition, whereas they could also be useful as confidence measures on words.

The word posterior probabilities can be computed either from the N -best list of phrases or on the word graph. Here, we chose to compute them on the N -best list, by also using the word graph to obtain the alignment of the words at a given position of the phrases. The word posterior probability P Eq. 1 : P with w of the phrases W ( k ) that contains w i at the same position. P W ( k ) | S is the probability for the phrase W ( k ) given the signal S corresponding to the handwritten phrase. Using the Bayes formula, P W ( k ) | S can be rewritten as given by Eq. 2 : P W ( k ) | S = P S | W ( k )  X  P W ( k ) (2) which can then be rewritten as Eq. 3 , using a decomposition of the phrase into its words: P W ( k ) | S = with P w ( k ) j | h ( k ) j being the probability of word its history h ( k ) j in the phrase, which is given by a language model (see Sect. 7.2 ), and P s ( k ) j | w ( k ) j being the probabil-ity of part s ( k ) j of the handwritten phrase, given the word which is given by the handwriting recognition system. These two probabilities, P s ( k ) j | w ( k ) j and P w ( k ) j needed to compute the word posterior probabilities. Never-theless, the handwriting recognition probabilities may not be always provided in the N -best list given by the recognition system. That is why we need to compute an approximation to these recognition probabilities so as to compute an approxi-mation of the word posterior probabilities. 5.2 Approximation of the word posterior probabilities In [ 40 ], the authors proposed various word-level confidence measures for machine translation. Here, we use two of these confidence measures as an approximation P ( s i | w i ) of the word recognition probability. Thus, we can combine this probability with the probability given by the language model to obtain an approximation P post (w i ) of the word posterior probability.

The first approximation corresponds to the relative fre-quency of a word w i in the N -best list of phrases and is given by Eq. 4 : f with  X  w ( k ) j ,w i being the Kronecker function, which equals 1 when w ( k ) j = w i and 0 otherwise ( w ( k ) j w j from the phrase W
The second approximation extends the first one by also taking into account the rank of each hypothesis phrase in the N -best list. This rank-weighted frequency of a word w i is given by Eq. 5 : f 5.3 Error hypothesis detection by comparison Word error hypotheses can easily be detected by comparing their confidence index with a threshold  X  err (the optimal value of which is found on a validation set, as can be seen in the experiments in Sect. 8.5 ): if the confidence index of the considered word is below the learned threshold, the word is detected as an error hypothesis. The confidence index of each word of the top-list phrase is used to label the top-list phrase words, as defined by Eq. 6 : class reco (w i ) =
Here, we use the word posterior probabilities P post (w i (or the approximate posterior probabilities, P post (w i ) confidence indices Conf (w i ) . 6 Characterization of error hypotheses into predefined types In addition to detecting error hypotheses, it may be of inter-est to also characterize these error hypotheses into various types. This means that the error hypotheses could then be processed differently, according to their type (to try to cor-rect them, for example). In this section, we present the error types we chose to consider. Then, we describe the various features that we are using as inputs to a SVM, to characterize each error hypothesis into its likeliest type. Finally, we show how these error types are used to prune the word graph that will then be used to perform the final correction step (see Sect. 7 ). 6.1 Recognition error types In this paper, we consider three types of errors that may cause an incorrect recognition of a considered word:  X  segmentation errors ( errSeg ): the recognition error is  X  substitution errors ( errSubst ): the considered word is not  X  absent substitution errors ( errAbs ): the considered word 6.2 Feature sets for the error characterization To characterize the detected error hypotheses into the error types defined in the previous sub-section, we use a classifier. We have chosen to use a SVM, both because SVMs are effi-cient and because they are able to deal with unbalanced clas-ses (in terms of the number of training examples). The SVM is aimed at characterizing each word of the top-list phrase that has been detected as a word error hypothesis, using some fea-tures of the considered word as its inputs (each word of the top-list phrase is then further labeled with its retrieved error type).

Here, we consider various features providing different types of information such as from a graphic model, or a language model, which have been proven to be useful to detecting error hypotheses [ 8 ]. Furthermore, we con-sider three different feature sets, based on an incrementally enlarged context from which features are extracted for the considered word (see Fig. 4 , where each context is shown with a different color): word context ( wSet ), local context ( lSet ), and neighboring context ( nSet ). This can be compared to work by [ 36 ], where features are also divided into different groups corresponding to different contexts of words. This is aimed at showing the improvement obtained when enlarging the word context. These three feature sets are described in the following sub-sections. 6.2.1 Word context feature set ( wSet ) In this baseline set, wSet , we consider a context restricted to the current word itself. Hence, the features for a word correspond to information on this word only. The 5 following features are considered:  X  posterior probability ( wWordPosteriorProba ): the poster- X  unigram probability ( wWordUnigramProba ): the uni- X  length ( wWordLength ): the length of the word (in number  X  position ( wWordPhrasePos ): the position of the word in  X  phrase length ( wPhraseLength ): the length of the top-list The first 3 features are used classically in several works on speech recognition [ 8 , 17 , 18 , 21 , 44 ] and on handwriting rec-ognition [ 30 ] to detect recognition errors. The last 2 features were inspired by [ 21 ]. 6.2.2 Local context feature set ( lSet ) In the set lSet , additional information on the competing words of the current word in the word graph are considered. So, this set contains 11 features, i.e. the 5 previous ones as well as the following 6 new features:  X  number of competing words ( lConcurrNbWords ): the  X  competing null edge ( lConcurrNullEdge ): a Boolean fea- X  posterior probability mean and variance ( lConcurrPoste- X  unigram probability mean and variance ( lConcurrUnig-The mean and variance of some score features are frequently used. The use of the first feature was again inspired by [ 21 ]. In that work, they also have features similar to the second one here, but they used it for the competing words of the pre-vious and next words to consider ( i.e. to know whether the competing edges of the previous and next considered words contain a null edge). So, we add this kind of feature, but by considering the current competing edges, anticipating that it may be helpful for characterizing segmentation errors. 6.2.3 Neighboring context feature set ( nSet ) Finally, we extend the set lSet to the set nSet , in which we also consider information on the neighboring words (the pre-vious one and the next one in the top-list phrase). Indeed, a recognition error can often lead to a recognition error on a neighboring word. This last set contains 16 features, i.e. the 11 previous ones and the following 5 new features:  X  bigram probability ( nWordBigramProba ): the bigram  X  previous posterior probability ( nPrevWordPosterior  X  next posterior probability ( nNextWordPosteriorProba ):  X  error on the previous word ( nPrevWordError ): a Boolean  X  error on the next word ( nNextWordError ): a Boolean fea-The first feature is an extension of the unigram probability of the baseline set, to include the context of the previous word. The second and third features were inspired by [ 8 , 17 ], while the last two features are based on our previous work [ 34 ]. 6.3 Word graph pruning by the error characterization types The word graph is finally pruned according to the type of each word in the top-list phrase (a correct label or a characterized error type):  X  segmentation error : the edge x i of the current word is  X  substitution error : the edge x i of the current word is kept,  X  absent substitution error : only the edge x i of the current  X  correct word : only the edge x i of the correct word is kept In Fig. 5 , it can be seen that the word  X  X ouble X  has been removed from the graph, as the corresponding word of the top-list phrase (word  X  X rouble X  ) has been detected as a correct word.

The pruning step is performed to decrease the size of the graph, which in turn will reduce the complexity of the cor-rection step that uses this graph. The pruning mainly con-sists of removing edges that compete with words detected as correct ones, thereby ensuring that their initial recog-nition will not change. Competing edges of error hypoth-eses characterized as absent substitution errors are also removed, ensuring that their incorrect recognition will not have as bad an impact as before on the recognition of the neighboring words. Indeed, this kind of error leads to the substitution of the current word by a wrong word, which has been shown to cause recognition errors on neighbor-ing words, essentially due to the use of a language model during the recognition process [ 4 ]. In fact, only competing edges of segmentation and substitution errors remain in the graph (in addition to edges corresponding to words of the top-list phrase), which will be used to correct the top-list phrase. 7 Correction of error hypotheses using a word-to-class backoff language model The characterization of error hypotheses into different types can now be used to try to correct the top-list phrase, depend-ing on its characterized words. So, the pruned word graph can be exploited to retrieve the corresponding corrected phrase. In this section, we introduce the maximum a pos-teriori (MAP) approach that is used to find the likeliest sentence in a word graph, using a language model. Then, we briefly recall the concept of statistical language mod-eling. Finally, we describe the language model used in this correction step that combines a n -gram LM and a n -class LM, from its creation to its use on the pruned word graph. 7.1 Post-processing correction using a language model To retrieve the corrected phrase that is given as the output of the whole error handling framework, we use the classic MAP decoding. This decoding is aimed at finding the likeli-est phrase  X  W correct among the phrases W ( k ) = w ( k ) given the handwritten signal S (it is efficiently performed on the word graph with the Viterbi algorithm [ 15 ]):  X  W = arg max with P S | W ( k ) being the probability of the handwritten signal S for the given sentence W ( k ) (it is classically given by the recognition system), P W ( k ) the probability of the phrase W ( k ) given by a language model and weighted by  X  and N ( k ) the number of words in the sentence W ( k ) that is weighted by  X  . Equation 7 extends Eq. 2 by weighting each probability and by also taking into account the number of words in the phrase. Equation 7 can be further decomposed for each word of the sentence, as given by Eq. 8 :  X  W = arg max where the probability P s ( k ) j | w ( k ) j is either given by a hand-writing recognition system or computed as an approximation (as explained in Sect. 5.2 ), and the probability P w ( k is given by a language model. This language model can be a simple model (as the ones described in Sect. 7.2 )oramore complex one (as the combined language models presented in Sect. 7.3 ). 7.2 Statistical language modeling Before presenting the combined LM, we recall the principle of statistical language modeling and the two models most commonly used: n -gram LMs and n -class LMs. 7.2.1 General definition Statistical language modeling is aimed at capturing the reg-ularities of a language by the use of statistical inference on a corpus of that language. The probability of a sequence of n words W = w n 1 = w 1 ...w n is thus given by Eq. 9 : P (
W ) = where h i = w 1 ...w i  X  1 is called the history of word w practice, there are too many different histories, which leads to a tremendous number of probabilities to estimate. Fur-thermore, most of these probabilities occur too infrequently in the corpus to be estimated reliably. A solution would be to merge histories into equivalence classes, which results in n -gram LMs. 7.2.2 n-gram language models In n -gram LMs, histories ending with the same n  X  1 words are considered to belong to the same equivalence class. Equation 9 can thus be rewritten into Eq. 10 : P (
W ) = where n is called the order of the LM. The n -gram prob-abilities P w w i | w i  X  1 i  X  n + 1 are estimated using relative fre-quencies obtained from a text corpus. Hence, the probability estimations depend on the corpus, and the probabilities of non-occurring n -grams ( i.e. sequences of n words) will be estimated to be zero. One way to overcome this problem is to apply a smoothing to the n -gram LM probabilities. The principle of the smoothing is first to reduce the probabilities of the n -grams occurring in the corpus and then to redis-tribute this mass of probabilities to unseen n -grams. Here, we use the Kneser-Ney smoothing, which has been shown to be very efficient [ 19 ]. Nonetheless, when words are out of the vocabulary associated with the LM, their probabilities will remain equal to zero. In that case, a solution may be to use n -class language models, where words are grouped into equivalence classes. Thus, if we can find the class of an out-of-vocabulary word, its linguistic probability will not be equal to zero. 7.2.3 n-class language models Depending on the approach used to create the considered classes (using a statistical criterion or by considering prede-fined categories), a word may belong to one or more clas-ses. For example, when considering the grammatical nature of words (also called Part-Of-Speech or POS tags), which is our concern here, a word may belong to several classes, the correct one depending on the context of the word. Two approaches can be used to take into account the various pos-sible classes of words: either consider all the possible class sequences associated with a given word sequence or only consider the likeliest class sequence. We chose the latter approach, so that we could retrieve the class of an unknown word (an OOV word or a word characterized as an absent substitution error , for example). The probability P (w i thus based not only on the words but also on their classes, as given by Eq. 11 : P with C i i  X  n + 1 = C i  X  X  X  X  X  C i  X  n + 1 , and c j being a class in the class set C j associated with the word w j .
 In conclusion, n -gram LMs are more accurate than n -class LMs but the latter have better generalization power. This is why we have combined efficiently these two types of lan-guage models into what is called a word-to-class backoff LM , as presented in the next sub-section. 7.3 Word-to-class backoff language model In this sub-section, we first describe how a n -gram language model is efficiently combined with a n -class language model to create the word-to-class backoff language model and then we present how to use this new language model on the pruned word graph, to correct the top-list phrase. 7.3.1 Definition of the word-to-class backoff language As seen in the previous sub-sections, n -gram LMs are more accurate to provide a linguistic probability to a word, but this word and the words in its history have to belong to the vocabu-lary associated with the language model. In the case when the word or one word of its history is out of the vocabulary, it may be of interest to use instead a n -class LM that can provide a probability to this OOV word based on its class. Conse-quently, in order to associate an accurate linguistic probabil-ity with each word, we combine a n -gram LM and a n -class LM, as inspired by [ 28 ]. We call this model a word-to-class backoff LM , because the n -class LM is used instead of the n -gram LM when the current word w i , or at least one word of its history, is an unknown word (for example, an OOV word or an absent substitution error ) and so does not belong to the vocabulary associated with the LMs. The probability of aword w i is then given by Eq. 12 : with V being the vocabulary associated with the lan-guage models, P w w i | w i i  X  n + 1 the probability given by the n -gram LM, and P c w i | w i i  X  n + 1 the probability given by the n -class LM.

To create the word-to-class backoff language model, the n -gram and the n -class LMs are built separately, using the classic maximum likelihood estimation (MLE). When build-ing the n -class LM, classes are estimated for in-vocabulary words and for OOV words. However, the classes associated with absent substitution errors are not known. So, we con-sider that the set of classes associated with absent substitution errors is the whole set of classes of the LM; indeed, an absent substitution error can correspond to any word and thus to any class. 7.3.2 Use of the word-to-class backoff language model to The aim of the correction step was to correct recognition errors on the top-list phrase, using a language model and competing recognition results in the word graph (for the sub-stitution errors and segmentation errors ; however, for the lat-ter, the correct word is not certain to be one of the competing results). The language model used for that step is the word-to-class backoff LM, presented in the previous sub-section: the n -gram part of the backoff LM will be used on most of the words, except when the considered word (or one word in its history) is an OOV word or has been characterized as an absent substitution error : in that case, the n -class part of the LM is used instead. Figure 6 shows a pruned word graph, where the part of the word-to-class backoff LM that is used to compute the linguistic probability of a word is given for each word of the top-list phrase. It can be seen that the n -class part of the LM is used to compute the linguistic probability for the word  X  X ar X  (because it was detected as an absent substi-tution error) and for the word  X  X ot X  (because the word in its history is an absent substitution error so it does not belong to the vocabulary). For the other words, the n -gram part of the word-to-class backoff LM can be used.

The corrected phrase finally corresponds to the best path in the pruned word graph, using Eq. 8 to combine the prob-ability of words from the word-to-class backoff language model with their recognition probabilities previously given by the recognition system ( i.e. provided in the N -best lists or computed as an approximation, as presented in Sect. 5.2 ). 8 Experiments and results In this section, we report the experiments that were conducted to evaluate the whole error handling framework we have pro-posed. First, we describe the experimental setup, including a description of the handwritten and the linguistic data, as well as the two on-line handwritten phrase recognition sys-tems we use to generate the N -best lists of phrases given as inputs to our framework. Then, we discuss the optimization of the parameters of the various parts of the framework (using validation sets). Finally, the overall results on the test set are presented. 8.1 Experimental setup In this sub-section, we describe the linguistic data (LMs and associated vocabularies) and the handwritten databases we use in our experiments. 8.1.1 Handwritten data The experiments were performed on two on-line handwritten phrase databases (Fig. 7 shows examples of phrases from the two databases). These databases are divided into different sets (at least one training set and one test set, and possibly validation sets) with no writer appearing in more than one dataset (writer-independent tasks are thus considered). The two following paragraphs give greater details on these databases.

ImadocSen-OnDB 1 is an in-house database containing sentences acquired from a TabletPC. The written sentences have been extracted from part of the Brown corpus [ 16 ]. This is a simplified database since the sentences only con-tain lowercase letters ( i.e. 26 different characters). Because it is of medium size, the database is divided into only two datasets: the training set, which is used both to train the clas-sifiers (SVMs for the characterization step) and to optimize the parameters, and the test set, which is used to measure the final recognition results. Table 1 sums up the character-istics of the database. In this table, a token corresponds to a sequence of letters, a sequence of digits or a symbol ( e.g. a punctuation mark).

IAM-OnDB [ 25 ] is a large database containing on-line handwritten texts acquired from a whiteboard. The written texts have been extracted from part of the LOB corpus [ 22 ] and contain 81 distinct characters (all lowercase and capi-tal letters, punctuation marks, digits, a character for garbage symbols, and a character for the space). The IAM-OnDB-t2 task is considered here, which handles the recognition of handwritten lines. This database is divided into four data-sets (as given in Table 2 ): the training set is used to train the classifiers, the two validation sets are used to optimize the other parameters, and the test set is used to measure the final recognition results using the whole framework. 8.1.2 Linguistic data The language models used in the different parts of the frame-work are built on the tagged Brown corpus [ 16 ]usingthe SRILM toolkit [ 38 ]. This corpus contains 52,954 sentences (1,002,675 words), where 46,836 sentences (900,108 words) were used to train the LMs (we call this part of the corpus the Brown training corpus ). The remaining sentences are not considered, because they are used in the Imadoc database (see Sect. 8.1.1 ). This ensures that the training set of the LMs and the handwritten test sets are, and remain, indepen-dent. We restricted the vocabulary associated with the LMs to the 20,000 words of the corpus that occur most frequently (other words in the corpus are considered as OOV words and are mapped to the tag &lt;unk&gt; ). To train the probabilities of the LMs, the words of the corpus are divided into tokens (corresponding to the tokens previously defined).

The same LMs are used in the experiments on both data-bases. A bigram LM is used in the various steps of the frame-work: in the error detection part (used in the computation of posterior probabilities), in the error characterization part (to compute different word features), and in the error correction step (to be used as the main component of the word-to-class backoff LM). A 4-class LM is also used in the correction step (this time to be used as the backoff component of the word-to-class backoff LM). For this n -class LM, we consider 145 classes, which correspond to the POS classes in the tagged version of the Brown corpus. 8.2 Evaluation metrics To evaluate the different steps of the error handling frame-work, we use different metrics. We present them in this sub-section.

To evaluate the performance of the task, from a recogni-tion point of view, we use the common word accuracy (WA) and word recognition rate (WRR), which are defined by WA = and WRR = with # w or d s being the number of all the words of the phrases to be recognized in the set considered and # subst , # ins , and # del the number of substitutions, insertions, and deletions in the resulting recognized phrases, respectively.

To evaluate the posterior probabilities used as confidence indices, we compute the normalized cross entropy (NCE), which is commonly used to measure the quality of confi-dence measures (the higher its value, the better the confidence measure). The NCE is defined by: NCE = with H and H with p c = N c N being the average probability that a word is correct ( N c is the number of correct words in the set consid-ered, and N is the total number of words), p i the predicted confidence that the word w i is correct (given by the confi-dence measure), and W corr and W err the sets of correct words and error words, respectively.

The Classification Error Rate (CER) is used to measure the performance of the error detection step (and to chose the error threshold  X  err ) and is defined as follows: CER = with # corr ect err being the number of correct words identi-fied as error hypotheses, and # errors corr the number of errors identified as correct words. Finally, the precision (Prec), the recall (Rec), and the F-Measure (F) are classical measures used in the field of information retrieval. Here, they are used to assess the per-formance of identifying errors. They are defined as follows: Prec = Rec = and F = 2 with # errors err being the number of error hypotheses cor-rectly identified as error hypotheses. Prec gives the percent-age of word classified as errors and that are indeed errors, whereas Rec gives the percentage of errors to identify that are indeed classified as errors. The F-measure is then a single-valued metric that reflects the trade-off between the precision and the recall. 8.3 Baseline phrase recognition systems We used two on-line handwriting recognizers in our exper-iments. In this sub-section, we present both, as well as the baseline recognition results on the two datasets ( i.e. the rec-ognition results before any error handling approach is used). 8.3.1 ResifPhrase sentence recognition system The first recognizer we used is the on-line handwritten sen-tence recognition system RESIFPhrase [ 33 ]. Given an input handwritten sentence, a graph containing handwritten word segmentation hypotheses is built. To identify these hypothe-ses, a radial basis function network (RBFN) is used to classify each inter-stroke gap. A confidence index is associated with each of these classification results and is used to create addi-tional segmentation hypotheses. A MAP decoding is then performed on the word graph to find the likeliest sentence, using graphical and linguistic information as given by Eq. 7 . In this case, P ( S | W k ) is, in fact, the accumulated score given by the word recognition system: it combines graphic and lexical scores given by the word recognition system RESIF-Mot [ 6 ]. The graphic score includes adequation measures between each character and its corresponding model, as well as spatial and statistical information between characters; the lexical score depends on edit operations performed during the lexical post-processing step. P ( W k ) is given by a bigram LM trained on the same part of the Brown corpus as the LMs used in the error handling framework. Nevertheless, it is different from these LMs because its vocabulary contains 13,748 words made up of lowercase letters only (the same vocabulary is used in the RESIFMot recognizer).

The limitation of this recognizer is that it can only recog-nize lowercase letters which restricts its use to ImadocSen-OnDB. For this reason, we consider another recognizer to evaluate our error handling framework on the IAM-OnDB, which is a more realistic database. 8.3.2 Microsoft text recognizer To obtain N -best lists of phrases from on-line handwritten texts (as given in IAM-OnDB), we used the recognizer pro-vided by Microsoft in their TabletPC sdk [ 31 ]. This enables us to evaluate our whole approach with a real  X  X lack box X  recognizer, since the N -best lists only contain phrase hypoth-eses (ordered according to a recognition score, but one that is not given). Furthermore, neither the vocabulary used in the recognizer nor the LMs involved are known. 8.3.3 Baseline recognition results In the following experiments, we consider three error han-dling tasks, depending on the handwritten database consid-ered and the recognition system used to provide the input N -best list of phrase hypotheses. The tasks are:  X  Sen/RP: RESIFPhrase recognizer on ImadocSen-OnDB;  X  Sen/MS: Microsoft recognizer on ImadocSen-OnDB;  X  Txt/MS: Microsoft recognizer on IAM-OnDB.
 Table 3 gives the baseline WA and WRR (defined in Sect. 8.2 ), for each task.

The WA and the WRR are first given, considering only the top-1 phrase on the list: these are the baseline rates against which those obtained with our error handling approaches will be compared. The recognition rate when considering the top-N phrases of the list is also given (here, N is set to 150): it gives an upper bound for our approach. Indeed, it can only be achieved if all the word error hypotheses are perfectly detected and perfectly corrected (the correction step only uses words of the competing phrases to correct the errors). It also implies a perfect characterization step, since the cor-rection step is performed on the graph pruned according to the characterized error types. Thus, we can see that the WRR for the first and the third tasks could be improved (due to about a 6% difference between the top-1 and top-N rates), whereas it might be more difficult to improve the WRR for the second task (only a 1% difference between the top-1 and top-N rates).

The rates obtained with the Microsoft recognizer are higher than those obtained with the RESIFPhrase recognizer. However, these results cannot be compared directly, since the Microsoft recognizer uses a larger vocabulary, more sophis-ticated LMs, and is also trained on a much larger training set. Furthermore, the rates with the Microsoft recognizer are higher than those obtained in [ 20 ] (using also the Microsoft recognizer on IAM-OnDB), which is due to the fact that, here, we compute rates on the tokens and not on the words. Thus, a handwritten compound word corresponding to 3 tokens (the first word, the dash, and the second word) will only be recognized for [ 20 ] if all its tokens are recognized, whereas we will focus here on how many of its tokens are recognized. 8.4 Comparison of confidence measures to detect error In the experiments presented in this sub-section, we com-pare the confidence measures used to detect word error hypotheses. Table 4 gives the NCE for the word posterior probabilities, and for the two word posterior probability approximations presented in Sect. 5.2 (the NCE is computed on the training set for ImadocSen-OnDB and on the first validation set for IAM-OnDB).

We can see that the posterior probability obtains the best NCE value, but the approximated posterior proba-bilities based on the relative frequency and on the rank frequency also give good values. The rank frequency gives better results than just using the frequency because it gives a lower score to a word that appears only in the last phrases of the list than to a word that appears the same number of times in the first phrases. So, for the Sen/MS and Txt/MS tasks, we choose to use the rank-weighted frequency of words to approximate the word recogni-tion probabilities, so that we can compute approximated posterior probabilities for the words (because no word rec-ognition probability is provided by the Microsoft recog-nizer), while we use the actual recognition probabilities for Sen/RP.
 8.5 Setting of the error detection threshold In this sub-section, we present the experiments on the choice of the error detection threshold, now that we have chosen the word posterior probability approximations (see Sect. 8.4 ). In Fig. 8 , ROC curves are given, when using the word posterior probabilities as confidence indices (see Sect. 5.3 ): each point of these curves corresponds to a chosen threshold  X  err and shows the compromise between the correct words, the con-fidence score of which is above  X  err (TAR, for True Accep-tance Rate ), and the error words, the confidence score of which is above  X  err (FAR, for False Acceptance Rate ). To plot these curves, we used the same training and validation sets as previously, for the three tasks.

For the tasks Sen/MS and Txt/MS, it can be seen that ROC curves are almost lines from (0%, 0%) to (100%, 100%), which means that error words cannot really be separated from correct words, at least using only their posterior probabil-ities. This may be because of the absence of a word rec-ognition score given by the recognition system: the rank frequency score we use may not be good enough to com-pute the approximated posterior probabilities. In order not to detect too many false errors ( i.e. that are indeed correct words), we set the threshold to 0.1. For the Sen/RP task, the ROC curve obtained has a better shape and allows us to set the error detection threshold to 0.25 (corresponding to a 88.5% TAR and a 79.8% FAR). 8.6 Choice of the feature set to characterize error In the experiments described in this sub-section, we compare the various feature sets used in a SVM to characterize word error hypotheses into the three predefined error types (see Sect. 6 ).
A SVM is trained for each of the three tasks using parame-ter optimization with a 10-fold cross-validation, thanks to the LIBSVM library [ 7 ]. The training sets of the two databases are used to train the SVMs; these training sets are limited to the actual error words (the correct words are discarded from the sets).

Table 5 gives the accuracy, that corresponds to the word recognition rate WRR (here, computed only on the error words), for each feature set considered and each task (it is the metric used in LIBSVM to optimize the parameters of the SVMs trained).

The WRR obtained using the wSet are already good and are only slightly improved when using the next context, lSet . The improvement is more significant when using the larger context, nSet , which improves the WRR by 2 to 3%.

In order to study the impact of the different features, we used a tool from LIBSVM to perform feature selection: it computes the contribution of each feature, in terms of the F-measure Fand then computes the WRR for classifiers trained with different subsets of the features. The better WRR is thus obtained when using the whole set of fea-tures, nSet . Table 6 also shows the importance of each feature, considering the three previous tasks (1 stands for the most important feature and 16 for the less important one).

It can be seen that the most important information to char-acterize error hypotheses into the predefined types are:  X  features from wSet (minus the unigram probability  X  the mean of posterior probabilities of concurrent words  X  information on the previous word of the top-list phrase This explains the good results obtained when using only wSet and the improvement brought by information on the previous word of the top-list phrase. 8.7 Evaluation of the overall error handling approach In this sub-section, we present the final results on the test sets of both databases, using the parameters optimized in the previous sub-sections, for the three error handling tasks. Table 7 gives the word recognition rate WRR, and the classification error rate CER, for the three different tasks, using the whole error handling approach (see Sect. 8.2 for their definitions). Moreover, to measure more precisely the performance of the error handling process, the precision (Prec), the recall (Rec), and the F-Measure (F) are given in Table 8 .

The WRR is decreased for the three tasks, when using only the error detection step (by 9 X 10%). Consequently, the CER is increased by  X  10% for the Sen/MS and Txt/MS tasks but only by  X  5% for the Sen/RP task. When adding the cor-rection step, the WRR is improved, for the three tasks, when compared to the error detection step alone, but it is still below the baseline rates (by  X  5%). Likewise, the CER is decreased (and is  X  3 X 5% over the baseline CER). This behavior is unavoidable because correct words are selected during the error detection step, and not all of the so-detected errors can be corrected during the correction step.

As this work is not placed in the context of a recog-nition task, we evaluate more precisely the error handling contribution, using classic methods from the field of infor-mation retrieval (see Table 8 ). It can be seen that the recall is decreased during the correction process because of the attempt to correct detected error hypotheses, but the correction step also improves the precision and the F-measure. More particularly, better results are obtained for the Sen/RP task, where the actual recognition scores of the words are used during the correction step, to retrieve the final phrase. 9 Conclusion We have proposed a framework to handle recognition errors on phrase transcripts, from handwriting recognition systems, that are meant to be used as inputs to a higher-level system ( e.g. information retrieval systems or text categorization sys-tems). The framework takes a N -best best list of phrases (given by the recognizer) as input and outputs a phrase on which errors are detected, characterized, and even corrected. This approach is decomposed into four steps: an alignment step (where a word graph is built by aligning the N -best phrases of the list), a detection step (where word posterior probabilities are used as confidence measures to detect error hypotheses on the words in the top-list phrase), a character-ization step (where the previously detected error hypotheses are characterized into predefined error types, which are also used to prune the word graph accordingly), and a correction step (where a word-to-class backoff LM is defined and used to retrieve the final phrase on the pruned graph and thus to correct initial recognition errors). Experiments on two hand-written phrase databases were performed, using two recog-nition systems to provide the initial N -best lists: thus, three tasks were considered (Sen/RP, Sen/MS, and Microsoft). The results of this first implementation are mitigated. Indeed, even though the features chosen were proven to allow the characterization of error hypotheses into predefined error types and the correction step was shown to be an added-value to the error detection step alone, the overall results (in terms of WRR) are below the baseline WRR. Starting from the results obtained in terms of precision and recall, further investigations will be needed to improve the different steps of the proposed error handling framework.

Future works will investigate approaches to align mul-tiple segmentations to better correct segmentation errors. To do so, we could align multiple segmentation hypotheses when creating the word graph, as proposed in [ 43 ], where an edge can be align with two other edges, when the graph is created. We will study the bias of the poster-ior probabilities, as it was shown in [ 21 ] to impact the NCE and thus the quality of confidence measures based on these probabilities. We will investigate using other fea-tures for the characterization step and/or the detection step, especially probabilities given by n -class LMs using POS classes (as the n -class part of our word-to-class backoff LM) that were proven to be efficient, in recent works in speech recognition [ 13 , 36 ]. It would also be of interest to investigate using other measures to optimize the error detection threshold, like the F-measure, and to study how it impacts the correction step and thus the overall error handling process. We will also investigate how to cor-rect words, not only using the words that appear in the input phrase list. Indeed, it would be interesting to han-dle words characterized as absent substitution errors ,for which the correct recognition result does not appear in the word graph. In that case, we could use their classes, as identified by the word-to-class backoff LM, during the error correction step. The class of a word may then be used to select a specific vocabulary, containing only words of that class. This specific vocabulary could be given to a word recognition system, which could be used to fur-ther recognize the word. Finally, it would be of inter-est to measure how the error handling approach impacts the performance of higher-level applications that deal with handwritten transcripts (for information retrieval or catego-rization tasks, for example), as in [ 26 ], or to study how to present all these error characterization and correction results to an end-user (in a freeform note-taking application, for example). References
