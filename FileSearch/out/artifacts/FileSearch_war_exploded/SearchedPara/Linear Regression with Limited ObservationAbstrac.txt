 Elad Hazan ehazan@ie.technion.ac.il Tomer Koren tomerk@cs.technion.ac.il In regression analysis the statistician attempts to learn from examples the underlying variables affecting a given phenomenon. For example, in medical diagnosis a certain combination of conditions reflects whether a patient is afflicted with a certain disease.
 In certain common regression cases various limitations are placed on the information available from the exam-ples. In the medical example, not all parameters of a certain patient can be measured due to cost, time and patient reluctance.
 In this paper we study the problem of regression in which only a small subset of the attributes per exam-ple can be observed. In this setting, we have access to all attributes and we are required to choose which of them to observe. Recently, Cesa-Bianchi et al. (2010) studied this problem and asked the following interest-ing question: can we efficiently learn the optimal re-gressor in the attribute efficient setting with the same total number of attributes as in the unrestricted regres-sion setting? In other words, the question amounts to whether the information limitation hinders our ability to learn efficiently at all. Ideally, one would hope that instead of observing all attributes of every example, one could compensate for fewer attributes by analyz-ing more examples, but retain the same overall sample and computational complexity.
 Indeed, we answer this question on the affirmative for the main variants of regression: Ridge and Lasso. For support-vector regression we make significant advance-ment, reducing the parameter dependence by an expo-nential factor. Our results are summarized in the table below 1 , which gives bounds for the number of exam-ples needed to attain an error of  X  , such that at most k attributes 2 are viewable per example. We denote by d the dimension of the attribute space.
 Our bounds imply that for reaching a certain accuracy, our algorithms need the same number of attributes as their full information counterparts. In particular, when k =  X ( d ) our bounds coincide with those of full information regression, up to constants (cf. Kakade et al. 2008).
 We complement these upper bounds and prove that accurate Ridge regressor. For Lasso regression, Cesa-Bianchi et al. (2010) proved that  X ( d  X  ) attributes are necessary, and asked what is the correct dependence on the problem dimension. Our bounds imply that the number of attributes necessary for regression learning grows linearly with the problem dimensions.
 The algorithms themselves are very simple to imple-ment, and run in linear time. As we show in later sec-tions, these theoretical improvements are clearly visi-ble in experiments on standard datasets. 1.1. Related work The setting of learning with limited attribute obser-vation (LAO) was first put forth in (Ben-David &amp; Dichterman, 1998), who coined the term  X  X earning with restricted focus of attention X . Cesa-Biachi et al. (2010) were the first to discuss linear prediction in the LAO setting, and gave an efficient algorithm (as well as lower bounds) for linear regression, which is the primary focus of this paper. 2.1. Linear regression In the linear regression problem, each instance is a pair ( x , y ) of an attributes vector x  X  R d and a target variable y  X  R . We assume the standard framework of statistical learning (Haussler, 1992), in which the pairs ( x , y ) follow a joint probability distribution D over R d  X  R . The goal of the learner is to find a vector w for which the linear rule  X  y  X  w &gt; x provides a good prediction of the target y . To measure the performance of the prediction, we use a convex loss function ` ( X  y,y ) : R 2  X  R . The most common choice is the squared loss squares regression. Hence, in terms of the distribution D , the learner would like to find a regressor w  X  R d with low expected loss, defined as The standard paradigm for learning such regressor is seeking a vector w  X  R d that minimizes a trade-off between the expected loss and an additional regular-ization term, which is usually a norm of w . An equiv-alent form of this optimization problem is obtained by replacing the regularization term with a proper con-straint, giving rise to the problem where B &gt; 0 is a regularization parameter and p &gt; 1. The main variants of regression differ on the type of ` p norm constraint as well as the loss functions in the above definition:  X  Ridge regression: p = 2 and squared loss,  X  Lasso regression: p = 1 and squared loss.  X  Support-vector regression: p = 2 and the  X  -Since the distribution D is unknown, we learn by re-that are assumed to be sampled independently from D . the loss function induced by the instance ( x t ,y t ). We distinguish between two learning scenarios. In the full information setup, the learner has unrestricted access to the entire data set. In the limited attribute observation (LAO) setting, for any given example pair ( x ,y ), the learner can observe y , but only k at-tributes of x (where k &gt; 1 is a parameter of the prob-lem). The learner can actively choose which attributes to observe. 2.2. Limitations on LAO regression Cesa-Biachi et al. (2010) proved the following sample complexity lower bound on any LAO Lasso regression algorithm.
 Theorem 2.1. Let 0 &lt;  X  &lt; 1 16 , k &gt; 1 and d &gt; 4 k . For any regression algorithm accessing at most k at-tributes per training example, there exist a distribution D over { x : k x k  X  6 1 } X { X  1 } and a regressor w ? with k w ? k 1 6 1 such that the algorithm must see (in expectation) at least  X ( d k X  ) examples in order to learn a linear regressor w with L D ( w )  X  L D ( w ? ) &lt;  X  . We complement this lower bound, by providing a stronger lower bound on the sample complexity of any Ridge regression algorithm, using information-theoretic arguments.
 Theorem 2.2. Let  X  =  X (1 / algorithm accessing at most k attributes per training example, there exist a distribution D over { x : k x k 2 6 1 } X { X  1 } and a regressor w ? with k w ? k 2 6 1 such that the algorithm must see (in expectation) at least  X ( d k X  2 ) examples in order to learn a linear regressor w , k w k 2 6 1 with L D ( w )  X  L D ( w ? ) 6  X  .
 Our algorithm for LAO Ridge regression (see section 3) imply this lower bound to be tight up to constants. Note, however, that the bound applies only to a par-ticular regime of the problem parameters 3 . 2.3. Our algorithmic results We give efficient regression algorithms that attain the following risk bounds. For our Ridge regression algo-rithm, we prove the risk bound while for our Lasso regression algorithm we establish the bound
E [ L D (  X  w )] 6 min Here we use  X  w to denote the output of each algorithm on a training set of m examples, and the expectations are taken with respect to the randomization of the algorithms. For Support-vector regression we obtain a risk bound that depends on the desired accuracy  X  . Our bound implies that examples are needed (in expectation) for obtaining an  X  -accurate regressor. In this section we present and analyze our algorithms for Ridge and Lasso regression in the LAO setting. The loss function under consideration here is the squared loss, that is, ` t ( w ) = 1 2 ( w &gt; x t  X  y t venience, we show algorithms that use k + 1 attributes of each instance, for k &gt; 1 4 .
 Our algorithms are iterative and maintain a regressor w t along the iterations. The update of the regressor at iteration t is based on gradient information, and specifically on g t :=  X  ` t ( w t ) that equals ( w &gt; x t for the squared loss. In the LAO setting, however, we do not have the access to this information, thus we build upon unbiased estimators of the gradients. Algorithm 1 AERR Parameters: B, X  &gt; 0 Input: training set S = { ( x t ,y t ) } t  X  [ m ] and k &gt; 0 Output: regressor  X  w with k  X  w k 2 6 B 1: Initialize w 1 6 = 0 , k w 1 k 2 6 B arbitrarily 2: for t = 1 to m do 3: for r = 1 to k do 4: Pick i t,r  X  [ d ] uniformly and observe x t [ i t,r ] 6: end for 8: Choose j t  X  [ d ] with probability w t [ j ] 2 / k w t 9:  X   X  t  X  X  w t k 2 2 x t [ j t ] / w t [ j t ]  X  y t 10:  X  g t  X   X   X  t  X   X  x t 11: v t  X  w t  X   X   X  g t 12: w t +1  X  v t  X  B/ max {k v t k 2 ,B } 13: end for 3.1. Ridge regression Recall that in Ridge regression, we are interested in the linear regressor that is the solution to the optimization problem (2) with p = 2, given explicitly as Our algorithm for the LAO setting is based on a randomized Online Gradient Descent (OGD) strategy (Zinkevich, 2003). More specifically, at each iteration t we use a randomized estimator  X  g t of the gradient g t to update the regressor w t via an additive rule. Our gra-dient estimators make use of an importance-sampling method inspired by (Clarkson et al., 2010).
 The pseudo-code of our Attribute Efficient Ridge Re-gression (AERR) algorithm is given in Algorithm 1. In the following theorem, we show that the regressor learned by our algorithm is competitive with the opti-mal linear regressor having 2-norm bounded by B . Theorem 3.1. Assume the distribution D is such that k x k 2 6 1 and | y | 6 B with probability 1 . Let  X  w be the output of AERR, when run with  X  = p k/ 2 dm. Then, k  X  w k 2 6 B and for any w ?  X  R d with k w ? k 2 6 B , 3.1.1. Analysis Theorem 3.1 is a consequence of the following two lem-mas. The first lemma is obtained as a result of a stan-dard regret bound for the OGD algorithm (see Zinke-vich 2003), applied to the vectors  X  g 1 ,...,  X  g m . Lemma 3.2. For any k w ? k 2 6 B we have The second lemma shows that the vector  X  g t is an un-biased estimator of the gradient g t :=  X  ` t ( w t ) at it-eration t , and establishes a  X  X ariance X  bound for this estimator. To simplify notations, here and in the rest of the paper we use E t [  X  ] to denote the conditional ex-pectation with respect to all randomness up to time t . Lemma 3.3. The vector  X  g t is an unbiased estimator of the gradient g t :=  X  ` t ( w t ) , that is E t [  X  g addition, for all t we have E t [ k  X  g t k 2 2 ] 6 8 B 2 For a proof of the lemma, see (Hazan &amp; Koren, 2011). We now turn to prove Theorem 3.1.
 Proof (of Theorem 3.1). First note that as k w t k 2 6 B , we clearly have k  X  w k 2 6 B . Taking the expectation of (4) with respect to the randomization of the algo-On the other hand, the convexity of ` t gives ` t ( w t )  X  ` ( w ? ) 6 g &gt; t ( w t  X  w ? ) . Together with the above this implies that for  X  = 2 B/G Taking the expectation of both sides with respect to the random choice of the training set, and using G 6 2 B p 2 d/k (according to Lemma 3.3), we get Finally, recalling the convexity of L D and using Jensen X  X  inequality, the Theorem follows. 3.2. Lasso regression We now turn to describe our algorithm for Lasso re-gression in the LAO setting, in which we would like to solve the problem The algorithm we provide for this problem is based on a stochastic variant of the EG algorithm (Kivinen &amp; Warmuth, 1997), that employs multiplicative updates Algorithm 2 AELR Parameters: B, X  &gt; 0 Input: training set S = { ( x t ,y t ) } t  X  [ m ] and k &gt; 0 Output: regressor  X  w with k  X  w k 1 6 B 1: Initialize z + 1  X  1 d , z  X  1  X  1 d 2: for t = 1 to m do 3: w t  X  ( z + t  X  z  X  t )  X  B/ ( k z + t k 1 + k z  X  t k 4: for r = 1 to k do 5: Pick i t,r  X  [ d ] uniformly and observe x t [ i t,r ] 7: end for 9: Choose j t  X  [ d ] with probability | w [ j ] | / k w k 10:  X   X  t  X  X  w t k 1 sign( w t [ j t ]) x t [ j t ]  X  y t 11:  X  g t  X   X   X  t  X   X  x t 12: for i = 1 to d do 13:  X  g t [ i ]  X  clip(  X  g t [ i ] , 1 / X  ) 14: z + t +1 [ i ]  X  z + t [ i ]  X  exp(  X   X   X  g t [ i ]) 15: z  X  t +1 [ i ]  X  z  X  t [ i ]  X  exp(+  X   X  g t [ i ]) 16: end for 17: end for based on an estimation of the gradients  X  ` t . The mul-tiplicative nature of the algorithm, however, makes it highly sensitive to the magnitude of the updates. To make the updates more robust, we  X  X lip X  the entries of the gradient estimator so as to prevent them from getting too large. Formally, this is accomplished via the following  X  X lip X  operation: for x  X  R and c &gt; 0. This clipping has an even stronger effect in the more general setting we consider in Sec-tion 4.
 We give our Attribute Efficient Lasso Regression (AELR) algorithm in Algorithm 2, and establish a cor-responding risk bound in the following theorem. Theorem 3.4. Assume the distribution D is such that k x k  X  6 1 and | y | 6 B with probability 1 . Let  X  w be Then, k  X  w k 1 6 B and for any w ?  X  R d with k w ? k 1 B we have provided that m &gt; log 2 d .
 3.2.1. Analysis In the rest of the section, for a vector v we let v denote the vector for which v 2 [ i ] = ( v [ i ]) 2 for all i . In order to prove Theorem 3.4, we first consider the augmented vectors z 0 t := ( z + t , z  X  t )  X  R 2 d and  X  g (  X  g t ,  X   X  g t )  X  R 2 d , and let p t := z 0 t / k z 0 t k vectors, we have the following.
 Lemma 3.5.
X The lemma is a consequence of a second-order regret bound for the Multiplicative-Weights algorithm, essen-tially due to (Clarkson et al., 2010). By means of this lemma, we establish a risk bound with respect to the Lemma 3.6. Assume that k E t [  X  g 2 t ] k  X  6 G 2 for all t , for some G &gt; 0 . Then, for any k w ? k 1 6 B , we have E Our next step is to relate the risk generated by the lin-ear functions  X  g &gt; t w , to that generated by the  X  X lipped X  Lemma 3.7. Assume that k E t [  X  g 2 t ] k  X  6 G 2 for all t , for some G &gt; 0 . Then, for 0 &lt;  X  6 1 / 2 G we have The final component of the proof is a  X  X ariance X  bound, similar to that of Lemma 3.3.
 Lemma 3.8. The vector  X  g t is an unbiased estimator of the gradient g t :=  X  ` t ( w t ) , that is E t [  X  g addition, for all t we have k E t [  X  g t ] 2 k  X  6 8 B 2 For the complete proofs, refer to (Hazan &amp; Koren, 2011). We are now ready to prove Theorem 3.4.
 Proof (of Theorem 3.4). Since k w t k 1 6 B for all t , we obtain k  X  w k 2 6 B . Next, note that as E t [  X  g t ] = g mas 3.6 and 3.7 together, we get for  X  6 1 / 2 G that Proceeding as in the proof of Theorem 3.1, and choos-ing  X  = 1 G q log 2 d 5 m , we obtain the bound Note that for this choice of  X  we indeed have  X  6 1 / 2 G , as we originally assumed that m &gt; log 2 d . Finally, putting G = 2 B p 2 d/k as implied by Lemma 3.8, we obtain the bound in the statement of the theorem. In this section we show how our approach can be extended to deal with loss functions other than the squared loss, of the form (with f real and convex) and most importantly, with the  X  -insensitive absolute loss function of SVR, for which f ( x ) = | x |  X  := max {| x | X   X , 0 } for some fixed 0 6  X  6 B (recall that in our results we assume the labels y t have | y t | 6 B ). For concreteness, we consider only the 2-norm variant of the problem (as in the standard formulation of SVR) X  X he results we obtain can be easily adjusted to the 1-norm setting. We overload notation, and keep using the shorthand ` duced by the instance ( x t ,y t ).
 It should be highlighted that our techniques can be adapted to deal with many other common loss func-tions, including  X  X lassification X  losses (i.e., of the form popularity, we chose to describe our method in the context of SVR.
 Unfortunately, there are strong indications that SVR learning (more generally, learning with non-smooth loss function) in the LAO setting is impossible via our approach of unbiased gradient estimations (see Cesa-Bianchi et al. 2011 and the references therein). For that reason, we make two modifications to the learn-ing setting: first, we shall henceforth relax the budget constraint to allow k observed attributes per instance in expectation ; and second, we shall aim for biased gradient estimators, instead of unbiased as before. To obtain such biased estimators, we uniformly  X  -approximate the function f by an analytic func-tion f  X  and learn with the approximate loss func-suboptimal regressor of the approximate problem is an 2  X  -suboptimal regressor of the original problem. For learning the approximate problem we use a novel tech-nique, inspired by (Cesa-Bianchi et al., 2011), for esti-mating gradients of analytic loss functions. Our esti-mators for  X  `  X  t can then be viewed as biased estimators of  X  ` t (we note, however, that the resulting bias might be quite large).
 Procedure 3 GenEst Parameters: { a n }  X  n =0  X  Taylor coefficients of f 0 Input: regressor w , instance ( x ,y ) Output:  X   X  with E [  X   X  ] = f 0 ( w &gt; x  X  y ) 1: Let N = d 4 B 2 e . 2: Choose n &gt; 0 with probability Pr[ n ] = ( 1 2 ) n +1 3: if n 6 2 log 2 N then 4: for r = 1 ,...,n do 5: Choose j  X  [ d ] with probability w [ j ] 2 / k w k 2 2 6:  X   X  r  X  X  w k 2 2 x [ j ] / w [ j ]  X  y 7: end for 8: else 9: for r = 1 ,...,n do 10: Choose j 1 ,...,j N  X  [ d ] w.p. w [ j ] 2 / k w k 2 2 12: end for 13: end if 14:  X   X   X  2 n +1 a n  X   X   X  1  X   X  2  X  X  X   X   X  n 4.1. Estimators for analytic loss functions Let f : R  X  R be a real, analytic function (on the entire real line). The derivative f 0 is thus also analytic and can be expressed as f 0 ( x ) = P  X  n =0 a n x n { a n } are the Taylor expansion coefficients of f 0 . In Procedure 3 we give an unbiased estimator of f ( w &gt; x  X  y ) in the LAO setting, defined in terms of the coefficients { a n } of f 0 . For this estimator, we have the following (proof is omitted).
 Lemma 4.1. The estimator  X   X  is an unbiased esti-mator of f 0 ( w &gt; x  X  y ) . Also, assuming k x k k w k 2 6 B and | y | 6 B , the second-moment E [  X   X  2 is upper bounded by exp( O (log 2 B )) , provided that the Taylor series of f 0 ( x ) converges absolutely for | x | 6 1 . Finally, the expected number of attributes of x used by this estimator is no more than 3 . 4.2. Approximating SVR In order to approximate the  X  -insensitive absolute loss function, we define where  X  is expressed in terms of the error function erf, and consider the approximate loss functions `  X  t ( w ) = f ( w &gt; x t  X  y t ) . Indeed, we have the following. Algorithm 4 AESVR Parameters: B, X , X  &gt; 0 and accuracy  X  &gt; 0 Input: training set S = { ( x t ,y t ) } t  X  [ m ] and k &gt; 0 Output: regressor  X  w with k  X  w k 2 6 B 1: Let a 2 n = 0 for n &gt; 0, and 2: Execute algorithm 1 with lines 8 X 9 replaced by: 3: Return the output  X  w of the algorithm Claim 4.2. For any  X  &gt; 0 , f  X  is convex, analytic on the entire real line and The claim follows easily from the identity | x |  X  = 1 2 | x  X   X  | + 1 2 | x +  X  | X   X . In addition, for using Procedure 3 we need the following simple observation, that follows immediately from the series expansion of erf( x ). We now give the main result of this section, which is a sample complexity bound for the Attribute Efficient SVR (AESVR) algorithm, given in Algorithm 4.
 Theorem 4.4. Assume the distribution D is such that k x k 2 6 1 and | y | 6 B with probability 1 . Then, for any w ?  X  R d with k w ? k 2 6 B , we have E [ L D (  X  w )] 6 L
D ( w ? ) +  X  where  X  w is the output of AESVR (with  X  properly tuned) on a training set of size The algorithm queries at most k + 6 attributes of each instance in expectation.
 Proof. First, note that for the approximate loss func-tions `  X  t we have Hence, Lemma 4.1 and Claim 4.3 above imply that  X  g t in Algorithm 4 is an unbiased estimator of  X  `  X  t ( w t ). Furthermore, since k x 0 t k 2 6 1  X  and | y  X  t | 6 2 B according to the same lemma we have E t [  X   X  2 t ] = exp( O (log 2 B  X  )). Repeating the proof of Lemma 3.3, we then have
E t [ k  X  g t k 2 2 ] = E t [ Replacing G 2 in the proof of theorem 3.1 with the above bound, we get for the output of Algorithm 4, which imply that for obtaining an  X  -accurate regressor  X  w of the approximate problem, it is enough to take m as given in (8). However, claim 4.2 now gives that  X  w itself is an 2  X  -accurate regressor of the original prob-lem, and the proof is complete. In this section we give experimental evidence that sup-port our theoretical bounds, and demonstrate the su-perior performance of our algorithms compared to the state of the art. Naturally, we chose to compare our AERR and AELR algorithms 5 with the AER algo-rithm of (Cesa-Bianchi et al., 2010). We note that AER is in fact a hybrid algorithm that combines 1-norm and 2-norm regularizations, thus we use it for benchmarking in both the Ridge and Lasso settings. We essentially repeated the experiments of (Cesa-Bianchi et al., 2010) and used the popular MNIST digit recognition dataset (LeCun et al., 1998). Each instance in this dataset is a 28  X  28 image of a hand-written digit 0  X  9. We focused on the  X 3 vs. 5 X  task, on a subset of the dataset that consists of the  X 3 X  dig-its (labeled  X  1) and the  X 5 X  digits (labeled +1). We applied the regression algorithms to this task by re-gressing to the labels.
 In all our experiments, we randomly split the data to training and test sets, and used 10-fold cross-validation for tuning the parameters of each algorithm. Then, we ran each algorithm on increasingly longer prefixes of the dataset and tracked the obtained squared-error on the test set. For faithfully comparing partial-and full-information algorithms, we also recorded the total number of attributes used by each algorithm.
 In our first experiment, we executed AELR, AER and (offline) Lasso on the  X 3 vs. 5 X  task. We allowed both AELR and AER to use only k = 4 pixels of each training image, while giving Lasso unrestricted access to the entire set of attributes (total of 784) of each instance. The results, averaged over 10 runs on random train/test splits, are presented in Figure 1. Note that the x -axis represents the cumulative num-ber of attributes used for training. The graph ends at roughly 48500 attributes, which is the total number of attributes allowed for the partial-information algo-rithms. Lasso, however, completes this budget after seeing merely 62 examples.
 As we see from the results, AELR keeps its test er-ror significantly lower than that of AER along the en-tire execution, almost bridging the gap with the full-information Lasso. Note that the latter has the clear advantage of being an offline algorithm, while both AELR and AER are online in nature. Indeed, when we compared AELR with an online Lasso solver, our algorithm obtained test error almost 10 times better. In the second experiment, we evaluated AERR, AER and Ridge regression on the same task, but now allow-ing the partial-information algorithms to use as much as k = 56 pixels (which amounts to 2 rows) of each instance. The results of this experiment are given in Figure 2. We see that even if we allow the algorithms to view a considerable number of attributes, the gap between AERR and AER is large. We have considered the fundamental problem of statis-tical regression analysis, and in particular Lasso and Ridge regression, in a setting where the observation upon each training instance is limited to a few at-tributes, and gave algorithms that improve over the state of the art by a leading order term with respect to the sample complexity. This resolves an open ques-tion of (Cesa-Bianchi et al., 2010). The algorithms are efficient, and give a clear experimental advantage in previously-considered benchmarks.
 For the challenging case of regression with general con-vex loss functions, we describe exponential improve-ment in sample complexity, which apply in particular to support-vector regression.
 It is interesting to resolve the sample complexity gap of 1  X  which still remains for Lasso regression, and to improve upon the pseudo-polynomial factor in  X  for support-vector regression. In addition, establishing analogous bounds for our algorithms that hold with high probability (other than in expectation) appears to be non-trivial, and is left for future work. Another possible direction for future research is adapt-ing our results to the setting of learning with (ran-domly) missing data, that was recently investigated X  see e.g. (Rostamizadeh et al., 2011; Loh &amp; Wainwright, 2011). The sample complexity bounds our algorithms obtain in this setting are slightly worse than those pre-sented in the current paper, and it is interesting to check if one can do better.
 We thank Shai Shalev-Shwartz for several useful dis-cussions, and the anonymous referees for their detailed comments.
 Ben-David, S. and Dichterman, E. Learning with re-stricted focus of attention. Journal of Computer and System Sciences , 56(3):277 X 298, 1998.
 Cesa-Bianchi, N., Shalev-Shwartz, S., and Shamir, O. Efficient learning with partially observed attributes.
In Proceedings of the 27th international conference on Machine learning , 2010.
 Cesa-Bianchi, N., Shalev-Shwartz, S., and Shamir, O. Online learning of noisy data. IEEE Transactions on Information Theory , 57(12):7907  X 7931, dec. 2011. ISSN 0018-9448. doi: 10.1109/TIT.2011.2164053. Clarkson, K.L., Hazan, E., and Woodruff, D.P. Sub-linear optimization for machine learning. In 2010 IEEE 51st Annual Symposium on Foundations of Computer Science , pp. 449 X 457. IEEE, 2010.
 Haussler, D. Decision theoretic generalizations of the
PAC model for neural net and other learning appli-cations. Information and computation , 100(1):78 X  150, 1992.
 Hazan, E. and Koren, T. Optimal algorithms for ridge and lasso regression with partially observed attributes. Arxiv preprint arXiv:1108.4559 , 2011. Hazan, E., Agarwal, A., and Kale, S. Logarithmic re-gret algorithms for online convex optimization. Ma-chine Learning , 69(2):169 X 192, 2007.
 Kakade, S.M., Sridharan, K., and Tewari, A. On the complexity of linear prediction: Risk bounds, margin bounds, and regularization. In Advances in
Neural Information Processing Systems , volume 22, 2008.
 Kivinen, J. and Warmuth, M.K. Exponentiated gra-dient versus gradient descent for linear predictors. Information and Computation , 132(1):1 X 63, 1997. LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.
Gradient-based learning applied to document recog-nition. Proceedings of the IEEE , 86(11):2278 X 2324, 1998.
 Loh, P.L. and Wainwright, M.J. High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity. In Advances in Neu-ral Information Processing Systems , 2011.
 Rostamizadeh, A., Agarwal, A., and Bartlett, P.
Learning with missing features. In The 27th Confer-ence on Uncertainty in Artificial Intelligence , 2011. Vapnik, V.N. The nature of statistical learning theory . Springer-Verlag, 1995.
 Zinkevich, M. Online convex programming and gen-eralized infinitesimal gradient ascent. In Proceed-ings of the 20th international conference on Machine
