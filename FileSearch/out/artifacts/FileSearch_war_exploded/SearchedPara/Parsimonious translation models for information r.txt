 1. Introduction
Recently, the language modeling approach has become a popular IR model based on its sound theoretical &amp; Jones, 1976 ), it estimates individual document models that use unique probability distribution of words rather than explicitly inferring the relevance information. From the perspective of information retrieval, the language modeling approach provides a very flexible framework to deal with the complex relationship among terms. For example, two adjacent terms in a query has a dependency in the bi-term language model ( Srikanth &amp; Srihari, 2002 ), whereas a term in a query can have a syntactic dependency with any term within a query in the dependency language model ( Gao et al., 2004 ), and a term in a query is dependent on parent term in a sentence tree ( Nallapati &amp; Allen, 2002 ). The language modeling approach can be incorporated with some problem.

This paper focuses on improving initial retrieval performance on language modeling framework by using automatic query expansion. Here, the initial retrieval means the retrieval process without using top retrieved back is highly dependent on the initial retrieval performance. Thus, the automatic query expansion based on term co-occurrence statistics has been explored by two approaches in the language modeling approach: the Zhai, 2001 ).

The statistical translation model (STM) is based on a document X  X uery translation process by employing a operation of this model is similar to the document expansion rather than the query expansion because the expansion process is performed not on query terms but on document terms. The statistical translation model has difficulties with two aspects: requirement of relevance judgment data and time complexity at retrieval.
Although the title X  X anguage model ( Jin et al., 2002 ) eases the former problem, which regards the title of each document as a query and constructs pseudo relevance judgment data from these title X  X ocument pairs, the lat-ter problem is left unsolved. On the other hand, Markov chain translation model (MCTM) has no trouble ment X  X uery translation for estimating the query model. Since the process only expands the query terms, the relationship between the term and the documents in Markov chain partially decomposes the relationship of the query and the documents in relevance judgment.

However, previous approaches raise problems of computational complexity and retrieval effectiveness for constructing the translation model. For the translation model, the naive constructing method of term co-occurrence statistics requires a prohibitive time-consuming process as well as a large space overhead. In addi-tion, expansion terms can reduce the retrieval performance because common terms or stop words can be included in term co-occurrence statistics.

To resolve these two problems, an additional process is necessary to extract only the topical terms in doc-terms is performed based not only on language model but also any retrieval model. For example, in vector space models, one can consider traditional term weighting schemes such as tf
As a starting point, this paper pursues a well-founded framework in language modeling approach to extract topical terms and applies them in query expansion.

Fortunately, the parsimonious language model is proposed to eliminate stop words automatically and to reduce indexing space ( Hiemstra et al., 2004 ). Unlike the simple MLE document model, the parsimonious doc-ument model can eliminate the global common part and leave the topically related part. The parsimonious language model can be applied to eliminate stop words and to extract topical terms as well. After extracting topical terms, a new translation model will be obtained from term X  X erm translation model on these parsimo-nious language models. This paper calls this translation model parsimonious translation model .
This paper is a further exploration of Hiemstra X  X  work ( Hiemstra et al., 2004 ). Hiemstra revolves three main processes for using parsimony: indexing time, retrieval time and feedback time. However, he did not address the usage of it with a translation model to expand an original query model without feedback process. This paper presents an empirical evidence that the parsimonious language model is useful in reducing time and space complexity, with increased retrieval performance over baseline and a non-parsimonious model.
The rest of the article is organized as follows. In Section 2 , KL divergence framework of the language mod-eling approaches, MCTM, and some related works for the query model estimation problem are briefly reviewed. In Section 3 the problems inherent to the traditional translation model are examined by presenting formal analysis and motivate using the parsimonious model. Section 4 explains the parsimonious translation model in detail and Section 5 shows various evaluations about the parsimonious translation model with dis-cussion. Finally, conclusions and outline directions for future work are given. 2. Background 2.1. Language modeling approach to information retrieval
The language modeling approach to information retrieval ranks documents according to likelihood of a query from document language models ( Ponte &amp; Croft, 1998 ). For a query Q = q h , the likelihood of query is denoted by P ( Q j h D ).

Eq. (1) is further derived by using assumption of term dependency. Unigram language model is derived by adopting the fact that terms in a query are independently generated given a document model as follows.
This naive assumption is elaborated by assuming that each term in a query is dependent on a previously located term ( Song &amp; Croft, 1999 ).

More natural derivation is presented in a dependency language model by assuming that each term in a query is dependent on its syntactic governor ( Gao et al., 2004 ). Gao et al. (2004) reported that the dependency lan-guage model improves the baseline significantly, and bi-gram substantially.

Lafferty and Zhai (2001) suggested KL divergence retrieval framework to incorporate query expansion and relevance feedback by introducing the concept of a query model ( Lafferty &amp; Zhai, 2001 ), where the query model is a probabilistic model for user X  X  information need. In KL divergence retrieval framework, the score of document is the negative KL divergence between query model h
Eq. (2) summarizes the two challenging problems: estimation of document model and estimation of query model.

There have been several studies done on the estimation of document language models. A fundamental issue in estimating a document language model is smoothing. The most popular smoothing method in language modeling is Jelinek Mercer smoothing, with interpolation of the MLE (maximum likelihood estimation) of the document language model with a background collection language model P ( w j h ies of several smoothing methods.

The second problem is the query model estimation. If h Q is assigned by an empirical distribution of the given query sample Q = q 1 q m , then Kullback X  X eiber divergence framework is identical with the original language modeling approach. The query model estimation also has trouble with the data sparseness problem since most users X  query samples are constructed ad hoc and do not perfectly describe all related query terms. Smoothing query model is called semantic smoothing ( Berger &amp; Lafferty, 1999; Lafferty &amp; Zhai, 2001;
Lavrenko &amp; Croft, 2001 ), which means that additional probabilities are given into semantically or topically related terms to the query. Semantic smoothing of query corresponds to automatic query expansion in tradi-tional IR methods. 2.2. Markov chain translation model 2.2.1. Translation model
MCTM (Markov Chain translation model) models a random walk process on Markov chain between terms and documents ( Lafferty &amp; Zhai, 2001 ). MCTM defines a translation model between terms, which is used for semantic smoothing of the initial query model. The translation model basically forms a stochastic Markov chain transition matrix where each state corresponds to a term. The ( w , q )-entry of the matrix is filled with ument models containing w . where mixture weights are given posterior probability P ( h t ( q j w ), and a random walk process is defined where term (state) is randomly moved on the Markov chain.
Random walks can hold several sequential term transitions. As each state transition is performed, the stop event can be occurred with the probability of (1 a ). If the stop event occurs, then there is no further state transition processes . For simplification and reliability, we allow only the case that the number of random walks is maximally 1. Then, the state translation probability is generalized as follows. where d ( w , q ) is one if w equals to q and zero otherwise. In other words, t events, where the random walk process starts at word q and immediately stops, or stops at word q after one step when starting at other term w (since all terms stop after one random walk, we ignore the stop prob-ability). As a result, we obtain another Markov chain by using state translation probability t single state translation process is performed instead of the random walk process. 2.2.2. Query model estimation
The translation model allows a global query expansion process based on their co-occurrence-like character-istics. Lafferty and Zhai (2001) estimates the query model P ( w j h walk process starting at w is stopped at any term of a given query. This estimation is formulated as follows. where P ( w j / ) is a prior model for user X  X  preference of w . This leads to where ^ h Q is MLE of given query terms. In this paper, we call h probability toward backward direction from expansion terms to query terms.
 Unlike the method above, we can consider a new query model h the random walk process starting at an arbitrary term of a given query is stopped at term w . This new esti-mation method is formulated as follows.
 It is rewritten by using MLE of query model.

We call h F Q forward query model (our proposal) to mean the translation probability toward the forward direction from query terms to expansion terms. The forward query model is used in language modeling approach for cross-lingual retrieval which models query translation process from source language terms lation based on forward query model is well-performed and comparative to the document translation-based method in their test collections. Query expansion in our problem is a kind of query translation which models translation process from query terms into document terms. His work indirectly provides an evidence that the forward query model is reasonable in the query expansion problem. Thus, it is natural to use the forward translation model instead of the backward translation model for query expansion. The backward translation model is more adequate to the document translation process.

To clearly see the difference between Eqs. (5) and (6) , we re-consider two estimation methods by simple modification based on the Markov chain theory. First, from definition of transition probability, we can derive the following equality where P  X  w  X  X  t ( w j q ) corresponds to the transition probability, the forward Markov chain and the reversed Markov chain respectively. P ( w ) corresponds to stationary probabilities of two Markov chains (forward and reversed chains).

Then, the forward and the backward query model are modified as follows (for simplicity, we assume that and q . From these final formulas, we can review some interesting characteristics of the forward and backward query model. In the backward query model, the expansion terms are weighted by their topicality, while in the forward query model expansion terms are weighted on topicality of query terms co-occurred. However, the expansion terms of backward query model can have biased weights, since if the terms are just highly  X  X opical X  then they are likely to have high probabilities regardless of their relevancy to query terms. On the other hand, in the forward query model, more reasonable probabilities are assigned where terms co-occurred with  X  X nfor-mative X  query terms have high probabilities. In other words, the forward query model tends to reflect the query relevancy than the term topicality, whereas the backward query model tends to reflect the opposite. In addi-in a separate retrieval process even though they have no additional high weights. Thus, the forward query model is more reasonable than the backward query model.

As demonstrated in the experimentations in Section 5 , the forward query model improves the baseline lan-guage model, while the backward query model does not improve the baseline method and sometimes results in a poorer performance. Motivated from this result, we selected the forward query model as our reference method.

According to the assumption of the uniform distribution for the prior model P ( w j / ), the forward query model is simplified as follows.
 This paper calls polation of the expansion query model with the MLE query model.
 2.3. Related works
The relevance language model provides an alternative estimation of the query model ( Lavrenko &amp; Croft, 2001 ). The relevance language model indicates the langauge model of relevant documents, where the relevance concept of probabilistic retrieval model is revived in the language modeling framework. Since there are no relevant documents for given queries, the relevance language model h of generating other terms from documents containing query terms as follows ( Lavrenko &amp; Croft, 2001 ): where P ( w j Q ) indicates the probability model for generating w from a document model given the query Q . By using Bayesian theorem, P ( w j Q ) is rewritten as follows: assumes that query Q and w are sampled independently from a document model as follows:
In conditional sampling, it is assumed that w and query terms q given a document model. Interestingly this assumption can be reviewed in the perspective of translation model, and the resulting model is related to MCTM. According to conditional assumption, P ( w , Q ) is written by
Since P ( q i j w ) corresponds to the translation model t notation as follows: Now, the approximated relevance model P ( w j Q ):
In fact, Eq. (9) reflects an inner log term of mutual information measure of w and q where Co a ( w , q )/ P ( w ) P ( q i ) corresponds to inner log term of MI measure.

Either i.i.d. sampling or conditional sampling is accepted in only top retrieved documents to reduce the risk of the assumption on the whole document models. As a result, the summation part of the above formula is restricted to only top retrieved documents instead of all documents in the collection.

Local feedback methods are proposed to effectively estimate the original query model ( Zhai &amp; Lafferty, expansion. The global expansion enables detecting either linguistically related collocation or topically related terms on whole document collections. In addition, it is well-known that the local feedback is highly dependent retrieved documents. This issue will be further discussed with experimentation in this paper.
Basically, MCTM is a variant of a statistical translation model that resolves several inherent limitations of the models. From the perspective of a language modeling framework, MCTM provides several advantages for automatic query expansion. The first reason is that it provides another formulation of traditional measure such as co-occurrence statistics for automatic query expansion. Second, it contains restricted query expansion such as pseudo relevance feedback or cluster-dependent query expansion, where top retrieved documents and cluster-based retrieval are used for restriction, respectively. Third, MCTM provides a learning mechanism, since the translation model in MCTM can be learned by using relevance judgment. Fourth, the translation model in MCTM is general, since it is used to formulate other similar methods such as relevance language model and statistical translation model for resolving term-mismatch problem.

Instead of smoothing query model, smoothing of document can be an alterative method which will has sim-ilar effects. The statistical translation model is an example of such semantic smoothing of document where doc-ument terms are expanded instead of the query terms ( Berger &amp; Lafferty, 1999 ). document corresponds to document expansion technique of traditional retrieval ( Singhal &amp; Pereira, 1999 ), and document translation of the cross-lingual retrieval. Cluster-based smoothing that has been recently pro-posed in the language modeling approach can be reviewed in regard of the semantic smoothing of document ( Liu &amp; Croft, 2004 ). 3. Analysis of Markov chain translation model 3.1. Computational complexity
However, the calculation process becomes a time-consuming work as the number of document increases. To observe the time complexity, first we rewrite t ( w j q ) as follows: where P ( q )is to co-occurrence statistics of q and w among whole documents ( Co for constructing co-occurrence statistics for all translation probabilities (i.e. translation matrix). Algorithm 1. Construction of co-occurrence statistics Input : N documents in collection C Output : Co-occurrence sparse matrix Co ( w , v )
In the above algorithm, co-occurrence statistics t ( w j v ) are accumulated as the number of processed docu-ments increase. Let the average number of unique terms in documents be K and the number of documents be N . The expected time complexity (TC) is obtained as follows.

The important fact is that the time complexity is proportional to the number of documents by linear time. Its time complexity should be seriously considered, since recent retrieval environments require a large number of documents. For a simple example, when K is 100, atomic operations of 5000 (the operation marked with the above algorithm box) are required for each document. If N is only 200,000, the total number of atomic operations reaches 1G. This quantity is not ignorable, since these operations cannot be processed fully on memory and many part of them would be processed on secondary storage. 3.2. Retrieval risk
There is a negative effect on retrieval performance in expansion terms from MCTM. To access the retrieval effectiveness, we first revisit the Eq. (2) .
 where k is Jelinek X  X ercer smoothing parameter for the document language model and h guage model. When we estimate h Q by using the query language model using MCTM, there are common terms with high positive probabilities in h Q , because MCTM uses an MLE document model for h document model will assign high probability to common terms, translation probability t ( w j q ) which corre-sponds to co-occurrence statistics to common terms. Intuitively, if the query model contains common terms with high weight, the risk of retrieval will significantly increase.

To formalize the risk of highly weighted common terms, we will start from defining retrieval risk. Assume that two terms w and v are given where w is a common term and v is a topically relevant term, and P ( w j h Q )= j P ( v j h Q ) where j P 1, and both common term w and topical word v appears in document D . Let the retrieval influence of w be the increment of negative KL divergence in Eq. (12) .
 of w is minor compared with that of v . The following definition gives us reasonable concept of retrieval risk.
Definition 1. Assume that document D contains both common term w and topical term v . (2) Retrieval risk of w for v is resolved when Risk( w , v ) &lt; 0 (or prevent retrieval risk of w for v ). (3) Retrieval risk of w for v is un-resolved when Risk( w , v ) P 0.
 and derive two lemmas.

Definition 2. Topicality and non-topicality is defined as follows. (2) Non-topicality of term : Non-topicality of w is defined by P ( C j w ) as follows.
 Following lemma describes necessary and sufficient condition of topicality for resolving retrieval risk. Lemma 1. If common term w and topical term v in query model are related with P(w j h the condition for resolving retrieval risk of w for v is equivalent to the following condition: In Eq. ( 14 ) , the right term is called topicality bound of v for resolving retrieval risk of w.
Proof. From definition, to resolve retrieval risk of common term w for topically related term v , the condition that Risk( w , v ) &lt; 0 should be satisfied.
 From Definition 2 , this inequality is rewritten by From our assumption of term w and v which is P ( w j h Q )= j P ( v j h By simple calculation, the above inequality is further derived by Finally we obtain the condition Eq. (14) . h From Lemma 1 ,at j = 1, we obtain the constraint P ( h D j v )&gt; P ( h topicality of topical term is larger than the topicality of common word. However as j increases, the bound of bound goes to 1 with fast convergence rate. This implies that this bound of topicality of v can fail easily for some stop words w and j . To understand the un-satisfiability of this bound, let us derive the following theorem by assuming special situation of document representation. Although the following theorem is induced in a very simple binary representation of documents which may be un-realistic, we will present the result of theo-rem since the meaning of the acquired bound is more clear than that of Eq. (14) in familiar terms of df( v ).
Lemma 2. Given j , assume that each document has the same length, and occurrence of term is binary (0 or 1). If
Proof. Let the average number of unique terms in documents be K and the number of documents be N . From assumption of binary document representation, P ( w j h D P ( w j h C ) to Eq. (15) , we would obtain
Now since df( v ) 6 N , the bound of df( v ) would be obtain By combining the above final two inequalities, we can derive condition Eq. (16) . h Eq. (16) describes the bound of df( v ) to resolve the retrieval risk in the binary representation of documents.
Eq. (16) is less tight bound than Eq. (14) since this condition is only sufficient but not necessary. According stop words or common terms with non-ignorable high weights, then they must be eliminated since their effects on expansion terms are very dangerous. 4. Parsimonious translation model 4.1. Motivation
In Section 3 , we discussed the computational complexity and the retrieval risk of MCTM. To reduce this computational complexity, one possible strategy is to use only feasible terms instead of all the terms in the of term pairs within the local context: small windows such as few terms or phrase level or sentence level ( Song disambiguation) topical context and local context play different roles. Co-occurrence only from local context cannot completely substitute for co-occurrence from global context, and its effect will be different from the effect of using topical context. Especially in the query expansion problem, topically related terms should be selected and expanded. Co-occurrence statistics on topical context would be a primal resource for our prob-lem, rather than those on local context.

Fortunately, in a language modeling approaches, the parsimonious language model ( Hiemstra et al., 2004 ) provides a device to handle topical terms in a document. The parsimonious language model enables us to build models that are significantly smaller than standard models within its model. In this model, it is assumed that highly probable k terms are topical in the document and non-zero probabilities are only assigned to them.
Other terms have zero probabilities. By applying a Markov chain on this parsimonious document model, a translation model can be constructed. We called this translation model the parsimonious translation model , dis-criminating it from the original translation model. In other words, the parsimonious translation model is the estimated translation model using the parsimonious document models instead of MLE document models in
Eq. (3) . 4.2. Parsimonious document model
As noted in Section 2 , document language models are constructed by mixing the MLE document language model and global collection language model. MLE for document model is far from a document specific model because it contains global common terms with high probabilities. To construct document specific topic model, we assume that documents are generated from a mixture model of the document specific model (which we want to obtain) with global collection model. For given document D , the likelihood of document is as follows: where P ( D ) is the likelihood of document. Note that l indicates another smoothing parameter that is different from k . Document specific model is defined as ~ h D such that maximizes the likelihood. To maximize the doc-ument likelihood, the EM algorithm is applied ( Dempster, Laird, &amp; Rubin, 1977 ).
 E-step: M-step: where P  X  w j ~ h D  X  i is a document specific model obtained after i th EM iteration.

After EM iteration is converged to ~ h D , the selection process is performed, where only highly topical terms are selected and non-topical terms are discarded. For non-topical terms w , its probability P  X  w j
Discarded probability is re-distributed uniformly on selected topical terms. Let D for given document D . Then, the parsimonious document language model P  X  w j h where Z D is a normalization factor  X  1 =
Several possible methods can be used for term selection. In fact, some selection methods have been used in pseudo relevance feedbacks. We summarize them as follows. (2) Select _ topratio ( P t ): Top terms are selected in proportional to the number of distinct terms in a docu-(3) Select _ top ( K ): Top K terms are selected. This method has been used in classical pseudo relevance feed-(4) Select _ cut ( ): Terms are discarded when the probability of it is less than . Recently, this method has
Now, parsimonious translation model is obtained by just substituting this parsimonious language model into Eq. (3) instead of using MLE document model. where t s is parsimonious translation model.

Remark that there can be some variations on the translation model by using prior probability P  X  h example, when applying select_topratio( P t ), the normalization factor Z since the summation of probabilities of selected terms is not the same (remind that Z this case, P ( h D ) can be assigned in proportional to this normalization factor. This assignment is reasonable concerning that higher Z D indicates higher topicality of a document. In this normalization, documents with higher topicality have more effects on estimation than the translation model. Note that even when than P but somewhat different among documents). 5. Experimentation 5.1. Experimental setting
Our experimental database consists of TREC4 and TREC8 data collections. We decompose TREC4 into five sub-collections ( Gao et al., 2004 ). Table 1 summarizes the information of the seven data collections. The  X  X # Doc X  X  is the total number of documents,  X  X # D.T. X  X  indicates the average number of unique terms of doc-uments,  X  X # Q X  X  is the number of topics,  X  X # Term X  X  is the number of indexing terms and  X  X # R X  X  is the number of relevant documents in each test set. In TREC4, there are 50 queries in TREC4. However, selected queries are used which have the number of relevant documents more than 3 (including 3). The reason is to reduce the bias effect of queries which have only 1 or 2 relevant documents. As a result, TREC4-FR and TREC4-ZIFF col-lection contains a small number of queries. For indexing terms, we perform the typical preprocessing step by removing stop words and then applying Porter stemming.

There are several parameters in our experimentation, including smoothing parameter k , mixing parameter a and parsimonious level P t ( P ) and so on. To focus on evalution according to parsimonous level, we assume that k and a were pre-given as follows. First, for the baseline language modeling approach, Jelinek X  X ercer smoothing are used. k is setted by 0.7 in TREC4-FR and 0.3 in other test collections, respectively. The reason for this different setting is that TREC4-FR tends to draw different baseline performance curve according to k , differently from other collections. Second, the interpolation parameter a is fixed to 0.4. This value was robust in all our test collections.

For evaluation measure, we use MAP (mean average precision). 5.2. Effects of parsimonious translation model
Fig. 1 describes how average precision performance changes according to different values of P (from 0.1 to 0.99) when a query model is estimated from OTM and PTM-l respectively. l indicates the smoothing param-eter used to estimate parsimonious document model which is explained in Section 4.1 . For l , five values such Section 4.2 . The prior probability of document model P ( h
From Fig. 1 , MAPs are sensitive to the parsimonious factor P in all test collections. MAP curves tend to be formance for each PTM-l on test collection. As P is far away from P * MAP smoothly decreases. This smooth-ness is important since it causes the process to find highly performed P feasible.

Remark that PTM shows the performance improvements over baseline and PTM at many P around P * as well as P * for most collections. For some test collections such as TREC4-SJM, TREC4-WSJ and TREC4-D, their performance differences are high. However, TREC4-FR collection is an exception, where PTM is not better than OTM except for when P is 0.5 and 0.9, even its performance curve is not convex but monotonic according to P .

The performance difference according to l is quite interesting. In TREC-4, as l gets small, the best P tends to be large, shifted into a positive direction of P . Such tendency is found in TREC4-AP and TREC4-WSJ more clearly. This result is interpretable if we assume that there is a reasonable number of terms for improving per-formance when constructing term co-occurrence. The interpretation is as follows. First, l determines the skew-ness of the estimated parsimonious document model. The skewness means that only a small number of terms have high probability. A smaller l makes the estimated parsimonious document model more skewed, where the estimate model is entirely determined by topicalities of terms (such as idf ) rather than their frequency of the given document. The skewness occurs since the variance on topicalities is much larger than the variance of their frequency of the document. On the other hand, as l is larger, it makes the estimated model more flatter for the similar reason. Given P , a skewed distribution makes the number of selected terms fewer, which is likely to be fewer than the reasonable number of terms. In this case, the number of terms should be increased by enlarging
P .If P makes the selected terms to cover the reasonable number, then the performance will be increased. This interpretation is re-confirmed when discussing the number of selected terms in the next subsection. Remark that smaller l shows much better performances than other l s in the case of TREC4-SJM and
TREC4-WSJ. It implies that the weighting of selected terms is an important part even if the selected number of terms are similar. In TREC4, smaller l seems to assign good weights for terms from its high performance. We will further discuss the importance of weighting in next subsection.
 The result of TREC-8 seems to be different from TREC4. While TREC4 shows the best when l ( l * ) is 0.01,
TREC8 does not show a good performance in such case. In particular, in TREC8-T, PTM-0.01 has completely failed. In fact, if we ignore l of 0.01, then the tendency is similar to one of TREC-4. Even in TREC8, smaller l tends to show better performance although the difference is marginal. In this regard, we will expect that the performance curve is convex according to l . In other words, in TREC-4, the performance may not be better when we use much more smaller l s which is less than 0.01 (e.g. 0.001 or 0.005). In TREC-8, we can infer that the best l is between 0.01 and 0.1. Shifting effects shown in TREC4 are not prominent in TREC8.
Why is the best l different? Is it related to characteristics of TREC4 and TREC8 collection? As an example, consider the number of total terms in collection. In Table 1 , TREC8 collection contains 810,400 that are rela-tively many compared with other. The average of collection probability will be 1/810,400, thus one can think that higher l is adequate for a collection having such small probability. However, it is potentially not evident because TREC4-ZIFF contains many number of terms, which is comparable the value of TREC8, showing different shape from TREC8. Currently, there is no clear explanation for this question. Remark that high sen-sitivity on l is clashed with the result of generative model on local feedback by Zhai and Lafferty (2002) .
Essentially, in the perspective of parsimonious model, the setting of their generative model on local feedback is the same as our global expansion. The only difference is that they apply the mixture model on whole feed-back documents, whereas we use a single document. From the experiments according to different values of mixture parameter ( l ), they observe the insensitivity of retrieval performance.

Table 2 summarizes the best performance of the parsimonious models (PTM * ) and OTM in the seven test collections. The last column with the symbol  X  X %chg X  X  indicates improvement ratio of PTM
OTM and PTM * , either  X  *  X  X r X  **  X  is marked if Wilcoxon sign ranked test comparing with the baseline is baseline, whereas OTM has not. Only in TREC4-FR, OTM has significant improvement over the baseline. nificance level 1%. For TREC4 test collections, when combining results of all sub-collections of TREC4, Wil-coxon test was passed within 2% level. Although PTM * does not significantly improve OTM in TREC4, PTM * is valuable since it is a convincing method to improve the baseline, whereas OTM is not.
 Tables 3 and 4 show a more detailed comparison between the language model and the query model on recall levels. 5.2.1. Comparison of term selection methods
To compare term selection, we choose select_topratio and select_ratio for evaluation methods. Since for select_topratio the feasible number of selected terms showing a reliable performance is different from selec-t_ratio, we perform four additional evaluations across P t all test collections.

Fig. 2 shows the retrieval performance of two different selection methods on TREC4 X  X  four collections, where the same marker is used for the same collection. The solid line and the dotted line indicate select_ratio and select_topratio, respectively. In this graph, P t of select_topratio is given by the parsimonious factor on x -axis. At the best point, two methods shows similar performance. However, the performance is better at select_ratio in TREC4-ZIFF and select_topratio in TREC4-FR. In select_topratio, the best P than 0.1, whereas, in select_ratio the best performance is obtained as P is more than 0.1. As discussed in the previous section, this difference of the best P is highly related to a reasonable number of selected terms.
The performance curve of select_topratio clearly shows how spending time has reduced. For example, if the performance is reliable when P t is less than 0.1 in a collection, then it means that the computation complexity for constructing term co-occurrence is reduced more than 0.01. In most collections, the best performance is achieved at 0.005 of P t , thus the reduction of time complexity is about 0.0025% which is remarkable. As shown in Table 1 , the average number of terms does not exceed 200 for a document, which implies that the average
Again, TREC4-FR collection, however, is an exception, where the performance is the best when P 5.2.2. Effects of reduction of storage overhead
This section discusses how storage overhead is reduced when PTM is applied for constructing P ( w j q ). To make analysis possible, the translation model is constructed only for queries which q appeared in the given query set. Although we divide TREC4 test collections into sub-collections for performance evaluation, the storage analysis uses all 50 description queries to construct the translation model without dividing queries according to their sub-collections. For TREC8 collection, all 50 queries from title and description field are used.

Fig. 3 (a) describes the storage size of PTM-l using select_ratio, and PTM-0.75 using select_topratio according to parsimonious factor in TREC4-AP. The storage size is physical value which is obtained to store size when select_topratio of PTM-0.75 shows the best performance. The vertical line indicates the parsimoni-ous factor ( P ) which is expected to perform best when using select_ratio at PTM * .

It is obtained by matching the curve of storage size in select_ratio at the best PTM-l (PTM * ) to the curve in
PTM-0.01. select_topratio in PTM-0.75 is the best when P t
We observe that according to P t , the size has almost linearly increased. On the other hand, according to P , the size increases very slowly as l is smaller. It provides strong evidence for skewness of the parsimonious model as l is smaller. As discussed in the previous section, when P the best. In this graph, a dash-dot line indicates the curve of storage size on select_topratio when P which is parallel to x -axis. Based on this curve, if we find P corresponding to the same size when P such P becomes smaller as l gets smaller. It explains exactly of the shifting tendency of performance curve according to l in the previous section. Additionally, we can explain the sensitivity of PTM-l according to
P in Fig. 1 , by observing changing tendency of the storage size. When l is smaller, the storage size increases confirmed in Fig. 3 (b), which plots the storage size of TREC4-FR. When l is 0.01, model generated in P around 0.9 is similar to the size when P t is 0.1. From this result, again we can understand why the best P is so much larger in TREC4-FR. Although this paper does not draw curves of other collection, we found that overall phenomenon is similar in other collections (see Fig. 4 for TREC4-ZIFF and TREC8-T).
Table 5 summarizes P * and interpolated points ~ P on select_ratio X  X  curve which meets line of the storage size ally in TREC4-ZIFF and TREC4-FR, the difference is big between them.

From this result, it seems that the number of selected terms is a highly important factor. Although l gen-erates different sorting methods for selection, the number of terms in the best performance tends to be regular.
Table 6 shows the storage size of select_topratio using PTM-0.75 according to the various parsimony levels ( P is from 0.01 to 0.2). TREC8-T and TREC8-D are represented in the single entry at TREC8. There are two numbers in one cell. The number on the top is storage size (Unit: M (Mega) byte) and the number on the bot-tom is the ratio of the storage size of P t to one when all terms are used (Unit: %). Bold face on these numbers indicates when P t is P t .
 The best performance was achieved by only using a very small number of terms, which is clearly shown in TREC4-SJM, TREC4-WSJ, TREC4-ZIFF and TREC8, where the ratio of storage is less than 3%. Even
TREC4-AP and TREC4-FR are not poor, where ratios are relatively large from 7% to 10%. These small ratios mean that PTM is highly effective for storage reduction as well as retrieval performance. 5.2.3. Influence of selected terms weighting
After the terms are selected by using either select_ratio or select_topratio, their weighting is important for reliable retrieval effectiveness. To see the importance of weighting, we compare two different weighting meth-ods based on document model. The model is MLE, and the second one is the parsimonious document model, which were used for OTM and PTM, respectively.

The result is shown in Fig. 5 on TREC4-SJM and TREC8-D. Clearly, MLE model does not show substan-tial improvement as the parsimonious level is more greater ( P when P is 0.8 and 0.9 in TREC8-D. Overall, as P t decreases, MLE model gets worse. On the other hand, there are high effectiveness on the parsimonious document model when P shown in other collections, although all of graphs are not be plotted. 5.2.4. Forward query model vs. backward query model
As discussed in Section 2 , this section shows the empirical results between the forward query model and the backward query model. Table 7 shows the comparison of the forward query model and the backward query model on OTM. The baseline language model is the non-expanded language modeling approach using Jeli-nek X  X ercer smoothing. We found the suitability of the forward query model over the backward query model.
The forward query model improves the baseline language model, while the backward query model does not improve the baseline method and sometimes results in a worse performance. The reason was discussed in
Section 2 . 5.2.5. Example of estimated query model using translation model
As an example of query model, let us compare the query models using OTM and PTM for the query topic 216 in TREC4. We estimated the query model in AP88 + 89 test collections. Description of query topic 216 is occurring in those unafflicted at this.  X  X  The extracted terms for query topic 216 are given as follows:
Table 8 shows the estimated query model using OTM and PTM. In OTM, common terms such as said and ing to our analysis on retrieval risk of MCTM. On the other hand, PTM will cause the query model to sig-nificantly reduce the number of common terms compared with one of OTM. In addition, it is notable that good relevant topical terms such as drug and hospit are extracted in this model. 5.3. Application: pseudo relevance feedback on parsimonious translation model
In fact, it seems that global query expansion and local feedback (pseudo relevance feedback) have similar effects on term-mismatch problem. Compared to the local feedback, the global expansion based on a whole document collection provides a discriminative resource such as collocations or tightly coupled pragmatic units. We believe that it has the advantages to perform the local feedback from results of the global query expansion, or to combine the approach between them. The main reasons are given as follows. 1. Putting various aspects in the top ranked documents: Basically, it is well-known that the local feedback is
Kurland and Lee (2005) perform the local feedback based on the cluster-based retrieval to improve the aspect recall. In the same regard, the global query expansion enables to put various aspects in top retrieved docu-ments. Expansion terms with the global expansion will discover aspects which are difficult to be detected from original query only. Sometimes, relevant documents which are poorly matched with the original query will be highly ranked. 2. Enabling reliable feedback with high precision: Local feedback has trouble with performance degrada-tion when the performance of initial retrieval is low. Selective query expansion methods recognize such diffi-cult queries and give a decision whether local feedback should be applied or not ( Cronen-Townsend, Zhou, &amp; sion by improving the performance of initial retrieval. As described in the previous sections, PTM not only provides high precision, but also high recall. Thus, local feedback could be reliable as the number of difficult queries are reduced.

In this experimentation, two methods are considered as the combination of global query expansion and local feedback. 1. Local feedback from PTM results (PQM): This method performs local feedback using the top documents retrieved by PTM instead of the documents retrieved by baseline language model. We simply call this method PQM. 2. Interpolation of global query expansion of local feedback (PQM2): This method first causes the local query model to use the top retrieved documents and then combines it with PTM. The combined query model is used in the second retrieval. We call this method PQM2.

The query model in these two methods are formulated into the mixture model which consists of the global query model obtained from PTM and the feedback query model which is obtained from either the baseline result or the result of PTM. The mixture model is given as follows: where b is the interpolation parameter and h F is the feedback query model, which is obtained by applying model-based feedback using generative model of feedback documents Zhai and Lafferty (2002) . Previous local feedback is estimated from Eq. (19) by using h F Q as MLE query model the local feedback method based on the baseline result.

Fig. 6 shows the retrieval performance of QM, PQM-R and PQM2-R according to b where R is the number of top retrieved documents used for feedback. Table 9 shows the best performance of them. From this exper-iment, we observe several characteristics. First, at the best performance, QM tends to have smaller b than when the combining method is applied. As b is smaller, the feedback query model has more higher effects on the performance. Thus, the tendency implies that the feedback query model dominantly affects the perfor-mance of QM compared to the original query model, whereas the both global query model and the feedback query model affect the performances of combining methods (PQM and PQM2) on even grounds. An exception is found in TREC4-FR collection where the feedback query model has weak effect on two combining methods compared to the global query model.
 Second, the combining feedback approach is superior over the local feedback method in most collections.
However, there are two exceptional collections. In TREC4-WSJ collection, QM tends to be more effective than combining methods although PTM is highly effective at initial retrieval. In addition, the result of TREC4-AP collection shows that PQM has failed on improving. This result is quite interesting since performances of
PTM * in these collections are substantial as shown in Table 2 . It implies that even when the result of initial retrieval is significantly improved, the local feedback does not always guarantee the performance improvement at least these two test sets. This remains as an open problem.

Third, PQM is better than PQM2 when combining methods are highly effective. Particularly, TREC4-ZIFF collection clearly shows such effectiveness of PQM compared to PQM2. On the other hand, in TREC4-AP and
TREC4-WSJ collections where combining methods are not effective compared to QM, PQM2 is more reliable than PQM. This result is related to the fact that top retrieved documents are the same for QM and PQM2, causing that h F is the same for both methods. Thus, the performance of PQM2 tends to be similar to QM when combining methods are not effective.
 this case, PQM and PQM2 significantly improve QM. However, in other collections where QM is much better is possible to use performance predictors such as query clarity to check the effectiveness of PTM * ( Cronen-
Townsend et al., 2004 ). 6. Conclusion
We proposed an effective construction method for co-occurrence statistics using a parsimonious translation model. The parsimonious translation model involves an elegant method for selecting highly topical terms in doc-guage modeling approaches for information retrieval. From experimentation on seven different collections, we showed that a query model based on the parsimonious translation model preserves the effectiveness of traditional translation models, and remarkably reduces the time and space complexity of traditional translation models.
We have made interesting observations on the parsimony factors in language of test collections. In our test collections, the performance was significantly improved over baseline by using a few number of selected terms. Going on this research, several experimental issues remain as follows.

Automatic inferring optimal parsimonious factor P : It is an important issue since the performance of PTM is highly sensitive to P .

Comparison with other term selection methods: Currently, we only compare select_ratio and select_topra-tio methods. However select_cut and select_top methods deserve to be evaluated.
 Extending into other smoothing methods such as Dirichlet and adding-one smoothing.

Discovering more reliable combination method of global query expansion and local feedback: In spite of a substantial improvement over the initial retrieval, combining method sometimes has failed to improve. This reason should be more clearly identified.

Extending parsimonious method into other retrieval models such as vector space model and probabilistic model: Generally, our method belongs to the parsimonious global query expansion which can also be applied in other retrieval models such as vector space model and probabilistic retrieval model.
The parsimonious translation model can be applied to cross-lingual information retrieval for resolving translation term ambiguities. In the future, we will extend our method into cross-lingual information retrieval, and perform experimentation on several other collections for translation term weighting.

The translation model can be generalized into a topic-based translation model. This problem is based on the fact that term co-occurrence may depend on a specific topic. In one topic, two terms do not co-occur fre-quently, but in other topics, two terms will frequently co-occur. This issue may be related to the extension of the cluster-based language model ( Liu &amp; Croft, 2004; Kurland &amp; Lee, 2004 ).

Effective construction of co-occurrence information is possible using restricted context by referring only a local window such as a phrase or a sentence instead of the whole document window. It is an interesting exper-imental issue to combine the translation model based on the local window with the global translation model based on the document window. We hope that such combination is more robust than a single translation model, since local context and global document context have different effects on the query expansion problem. Acknowledgement This work was supported by the Korea Science and Engineering Foundation (KOSEF) through the Advanced Information Technology Research Center (AITrc).
 References
