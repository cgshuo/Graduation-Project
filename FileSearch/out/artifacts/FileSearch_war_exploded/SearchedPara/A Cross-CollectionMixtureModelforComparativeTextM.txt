 In this pap er, we de ne and study a novel text mining problem, whic h we refer to as Comparativ e Text Mining (CTM). Giv en a set of comparable text collections, the task of comparativ e text mining is to disco ver any laten t com-mon themes across all collections as well as summarize the similarit y and di erences of these collections along eac h com-mon theme. This general problem subsumes man y interest-ing applications, including business intelligence and opinion summarization. We prop ose a generativ e probabilistic mix-ture mo del for comparativ e text mining. The mo del sim ul-taneously performs cross-collection clustering and within-collection clustering, and can be applied to an arbitrary set of comparable text collections. The mo del can be estimated ecien tly using the Exp ectation-Maximization (EM) algo-rithm. We evaluate the mo del on two di eren t text data sets (i.e., a news article data set and a laptop review data set), and compare it with a baseline clustering metho d also based on a mixture mo del. Exp erimen t results sho w that the mo del is quite e ectiv e in disco vering the laten t common themes across collections and performs signi can tly better than our baseline mixture mo del.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Searc h and Retriev al]: Text Mining General Terms: Algorithms Keyw ords: Comparativ e text mining, mixture mo dels, clus-tering
Text mining is concerned with extracting kno wledge and patterns from text [5, 6]. While there has been much re-searc h in text mining, most existing researc h is focused on one single collection of text. The goals are often to extract basic seman tic units suc h as named entities, to extract rela-tions between information units, or to extract topic themes. In this pap er, we study a novel problem of text mining re-ferred to as Comp arative Text Mining (CTM). Giv en a set of comparable text collections, the task of comparativ e text mining is to disco ver any laten t common themes across all collections as well as summarize the similarit y and di er-ences of these collections along eac h common theme. Specif-ically , the task involves: (1) disco vering the di eren t com-mon themes across all the collections; (2) for eac h disco vered theme, characterize what is in common among all the col-lections and what is unique to eac h collection. The need for comparativ e text mining exists in man y di eren t applica-tions, including business intelligence, summarizing reviews of similar pro ducts, and comparing di eren t opinions about a common topic in general.

In this pap er, we study the CTM problem and prop ose a generativ e probabilistic mixture mo del for CTM. The mo del sim ultaneously performs cross-collection clustering and within-collection clustering, and can be applied to an arbitrary set of comparable text collections. The mixture mo del is based on comp onen t multinomial distribution mo dels, eac h characterizing a di eren t theme. The common themes and collection-sp eci c themes are explicitly mo deled. The pro-posed mo del can be estimated ecien tly using the Exp ectation-Maximization (EM) algorithm.

We evaluate the mo del on two di eren t text data sets (i.e., a news article data set and a laptop review data set), and compare it with a baseline clustering metho d also based on a mixture mo del. Exp erimen t results sho w that the mo del is quite e ectiv e in disco vering the laten t common themes across collections and performs signi can tly better than our baseline mixture mo del.

The rest of the pap er is organized as follo ws. In Section 2, we brie y introduce the problem of CTM. We then presen t a baseline simple mixture mo del and a new cross-collection mixture mo del in Section 3 and Section 4. We discuss the exp erimen t results in Section 5.
With the popularit y of e-commerce, online customer eval-uations are becoming widely pro vided by online stores and third-part y websites. Pioneers like amazon.com and epin-ions.com have accum ulated large amoun ts of customer input including reviews, commen ts, recommendations and advice, etc. For example, the num ber of reviews in epinions.com is more than one million[4]. Giv en a pro duct, there could be up to hundreds of reviews, whic h is imp ossible for the readers to go through. It is thus desirable to summarize a collection of reviews for a certain type of pro ducts in order to pro vide the readers the most salien t feedbac ks from the peers. For review summarization, the most imp ortan t task is to iden tify di eren t seman tic asp ects of a pro duct that the review ers men tioned and to group the opinions accord-ing to these asp ects to sho w similarities and di erences in the opinions.

For example, supp ose we have reviews of three di eren t brands of laptops (Dell, IBM, and Apple), and we want to summarize the reviews. A useful summary would be a tab-ular represen tation of the opinions as sho wn in Table 1, in whic h eac h row represen ts one asp ect (subtopic) and di er-ent columns corresp ond to di eren t opinions.

It is, of course, very dicult, if not imp ossible to pro-duce suc h a table completely automatically . However, we can achiev e a less ambitious goal { iden tifying the seman tic asp ects and iden tifying the common and speci c character-istics of eac h pro duct in an unsup ervise d way. This is a concrete example of comparativ e text mining.
The example above is only one of the man y possible appli-cations of comparativ e text mining. In general, the task of comparativ e text mining involves: (1) disco vering the com-mon themes across all the collections; (2) for eac h disco vered theme, characterize what is in common among all the col-lections and what is unique to eac h collection. It is very hard to precisely de ne what a theme is, but it corresp onds roughly to a topic or subtopic. The gran ularit y of themes is application-sp eci c. CTM is a fundamen tal task in ex-ploratory text analysis. In addition to opinion comparison and summarization, it has man y other applications, suc h as business intelligence (comparing di eren t companies), cus-tomer relationship managemen t (comparing di eren t groups of customers), and seman tic integration of text (comparing comp onen t text collections).

CTM is challenging in sev eral ways: (1) It is a completely unsup ervised learning task; no training data is available. (It is for the same reason that CTM can be very useful for man y di eren t purp oses { it mak es minim um assumptions about the collections and in principle we can compare any arbitrary partition of text.) (2) We need to iden tify themes across di eren t collections, whic h is more challenging than iden tifying topic themes in one single collection. (3) The task involves a discrimination comp onen t { for eac h disco v-ered theme, we also want to iden tify the unique information speci c to eac h collection. Suc h a discrimination task is dif-cult given that we do not have training data. In a way, CTM goes beyond the regular one-collection text mining by requiring an \alignmen t" of multiple collections based on common themes.

Since no training data is available, in general, we must rely on unsup ervised learning metho ds, suc h as clustering, to perform CTM. In this pap er, we study how to use prob-abilistic mixture mo dels to perform CTM. Belo w we rst describ e a simple mixture mo del for clustering, whic h repre-sen ts a straigh tforw ard application of an existing text min-ing metho d, and then presen t a more sophisticated mixture mo del speci cally designed for CTM.
A naiv e solution to CTM is to treat the multiple collec-tions as one single collection and perform clustering. Our hop e is that some clusters would represen t the common themes across the collections, while some others would rep-resen t themes speci c to one collection (see Figure 1). We now presen t a simple multinomial mixture mo del for clus-tering an arbitrary collection of documen ts, in whic h we assume there are k laten t common themes in all collections, and eac h is characterized by a multinomial word distribu-tion (also called a unigram language mo del). A documen t is regarded as a sample of a mixture mo del with these theme mo dels as comp onen ts. We t suc h a mixture mo del to the union of all the text collections we have, and the obtained comp onen t multinomial mo dels can be used to analyze the common themes and di erences among the collections.
Formally , let C = f C 1 ; C 2 ; :::; C m g be m comparable col-lections of documen ts. Let 1 ; :::; k be k theme unigram language mo dels and B be the bac kground mo del for all the collections. A documen t d is regarded as a sample of the follo wing mixture mo del (based on word generation). where w is a word, d;j is a documen t-sp eci c mixing weigh t for the j -th asp ect theme, and ing weigh t of the bac kground mo del B . The log-lik eliho od of all the collections C is where V is the set of all the words (i.e., vocabulary), c ( w; d ) is the coun t of word w in documen t d , and = ( f j ; d;j is the set of all the theme mo del parameters. The purp ose of using a bac kground mo del is to \force" clustering to be done based on more discriminativ e words, leading to more informativ e and more discriminativ e comp onen t mo dels. We con trol this e ect through B .

The mo del can be estimated using any estimator. For example, the Exp ectation-Maximization (EM) algorithm [3] can be used to compute a maxim um likeliho od estimate with the follo wing updating form ulas:
This mixture mo del is closely related to the probabilis-tic laten t seman tic indexing mo del (PLSI) prop osed in [7] and treats CTM as a single-collection text mining problem. However, suc h a simple mo del is inadequate for CTM for two reasons: (1) We have completely ignored the structure of collections. As a result, we may have clusters that repre-sen t only some , not all of the collections. (2) There is no easy way to iden tify whic h theme cluster represen ts the common information across collections and whic h represen ts speci c information to a particular collection. Belo w we presen t a more sophisticated coordinated mixture mo del, whic h is speci cally designed for CTM and addresses these two de -ciencies.
Figure 2: The Cross-Collection Mixture Model
Our main idea for impro ving the simple mixture mo del for comparativ e text mining is to explicitly distinguish com-mon theme clusters that characterize common information across all collections from special theme clusters that char-acterize collection-sp eci c information. Thus we now con-sider k laten t common themes as well as a poten tially dif-feren t set of k collection-sp eci c themes for eac h collection (illustrated in Figure 2). These comp onen t mo dels directly corresp ond to all the information we are interested in disco v-ering. The sampling distribution of a word in documen t d (from collection C i ) is now collection-sp eci c. Speci cally , it involves the bac kground mo del ( B ), k common theme mo dels ( 1 ; :::; k ), and k collection-sp eci c theme mo dels ( 1 ;i ; :::; k;i ), whic h are to capture the unique information about the k themes in collection C i . That is, where B is the weigh t on the bac kground mo del B and C is the weigh t on the common theme mo del j (as opp osed to the collection-sp eci c theme mo del j;i ). Intuitiv ely, when we \generate" a word, we rst decide whether to use the bac kground mo del B according to B ; the larger B is, the more likely we will use B . If we decide not to use B , then we need to decide whic h theme to use; this is con trolled by d;j , the probabilit y of using theme j when generating words in d . Finally , once we decide whic h theme to use, we still need to decide whether we should use the common theme mo del or the collection-sp eci c theme mo del, and this is con-trolled by C , the probabilit y of using the common mo del. The weigh ting parameters B and C are inten tionally to be set by the user, and their interpretation is as follo ws. re ects our kno wledge about how noisy the collections are. If we believ e the text is verb ose, then B should be set to a larger value. In our exp erimen ts, a value of 0 : 9 0 : 95 often works well. C indicates our emphasis on the commonalit y, as opp osed to the specialit y in comparativ e text mining. A larger C would allo w us to learn a richer common theme mo del, whereas a smaller one would learn a weak er com-mon theme mo del, but stronger special mo dels. The optimal value dep ends on the speci c applications.

According to this generativ e mo del, the log-lik eliho od of the whole set of collections is
We estimate the bac kground mo del B using all the avail-able text in the m text collections. That is,
Since B and C are set man ually , this leaves us with the follo wing parameters to estimate: (1) the common theme mo dels, = f 1 ; :::; k g ; (2) the special theme mo dels for eac h collection C i , C i = f 1 ;i ; :::; k;i g ; and (3) the theme mixing weigh ts for eac h documen t d : d = f d; 1 ; :::; As in the simple mixture mo del, we can also use the EM algorithm to compute a maxim um likeliho od estimate. The updating form ulas are sho wn in Figure 3. Eac h EM iteration involves scanning all the text once, so the algorithm is quite scalable.
Once the mo del is estimated, we will have k collection-speci c mo dels for eac h of the m collections and k common theme mo dels across all collections. Eac h of these mo d-els is a word distribution or unigram language mo del. The high probabilit y words can characterize the theme/cluster extracted. Suc h words can often be used directly as a sum-mary or indirectly (e.g., through a hidden Mark ov mo del) to extract relev ant sen tences to form a summary of the cor-resp onding theme. The extracted word distributions can also be used in man y other ways, e.g., to classify other text documen ts or to link the related passages in the text collec-tions so that a user can navigate the information space for comparativ e analysis.

We can input our bias for CTM through setting B and C man ually . Speci cally , B allo ws us to input our kno wledge about the noise (stop words) in the data { if we kno w the text data is verb ose, then we should set B to a high value, whereas if the data is concise and mostly con ten t-bearing keyw ords, then we need to set B to a smaller value. Sim-ilarly , C allo ws us to input a trade-o between extracting common theme mo dels (setting C to a higher value) vs. ex-tracting collection-sp eci c mo dels (setting C to a smaller value). Suc h biases cannot be learned by the maxim um like-liho od estimator. Indeed, maximizing the data likeliho od is only a means to achiev e our ultimate goal, whic h is why we want to regularize our mo del in a meaningful way so that we can imp ose certain preferences while maximizing the data likeliho od. The exibilit y and con trol pro vided by B and
C mak e it possible for a user to con trol the focus of the results of comparativ e text mining.
We evaluated the Simple Mixture mo del (SimpMix) and the Cross-Collection Mixture mo del (CCMix) on two do-mains { war news and laptop reviews.
The War news data consists of news excerpts on two com-parable events: (1) Iraq war and (2) Afghanistan war, both of whic h occurred in the last two years. The Iraq war news excerpts were a com bination of 30 articles from the CNN and BBC web sites over the last one year span. The Afghanistan war data consists of 26 news articles downloaded from the CNN and BBC web sites for one year starting from Nov. 2001. Our goal is to compare these two wars and nd out their common and speci c characteristics.

The results of using either the simple mixture mo del or the cross-collection mixture mo del are sho wn in Table 2, where the top words of eac h theme mo del are listed along with their probabilities. We set B = 0 : 95 for SimpMix and set b = 0 : 9, C = 0 : 25 for CCMix; in both cases, the num ber of clusters is xed to 5. Variations of these parameters are discussed later.

We see that although there are some interesting themes in the results of SimpMix (e.g., cluster3 and cluster4 app ear to be about American and British inquiry into the pres-ence of weap ons in Iraq, resp ectiv ely, while cluster2 suggests the presence of British soldier in Basra, a town in southern Iraq), they are all about Iraq war. We do not see any obvi-ous theme common to both Iraq war and Afghanistan war. This is exp ected given that SimpMix pools all documen ts together without exploiting the collection structure.
In con trast, the results of CCMix explicitly suggest the common themes and the corresp onding collection-sp eci c themes. For example, cluster3 clearly suggests that in both wars, there has been loss of lives. Furthermore, the top words in the corresp onding Iraq theme include names of some key defense people that are involved in the Iraq war (e.g., \Ho on" is the last name of the british defense secre-tary and \Sanc hez" is the last name of the U.S General in Iraq). In comparison, the top words in the corresp onding Afghanistan theme includes the name of the U.S Defense secretary who had an imp ortan t role in the Afghan war.
Cluster4 and cluster5 are also meaningful themes. The common theme captured in Cluster4 is the Monda y brie ngs by an ocial spokesman of a political administration during both wars; the corresp onding special themes indicate the dif-ference in the topics discussed in the brie ngs (e.g., weap on inquiry for Iraq war and Bin Laden for Afghanistan war). The common theme of Cluster5 is about the diplomatic role played by the United Nations (UN). The corresp onding spe-cial themes again suggest the di erence between the two wars. The Iraq theme indicates the role of UN in sending weap on insp ectors to Iraq; the Afghanistan theme refers to Northern Alliance that receiv ed aid from the UN and came to power in Afghanistan after the defeat of Taliban.
This data set was constructed to test our mo dels for com-paring opinions of customers on di eren t laptops. We man-ually downloaded the follo wing 3 review sets from epin-ions.com [4], ltering out the misplaced ones: Apple iBo ok (M8598LL/A) Mac Noteb ook (34 reviews), Dell Inspiron 8200 (8TW ORH) PC Noteb ook (22 reviews), IBM ThinkP ad T20 2647 (264744U) PC Noteb ook (42 reviews).

The results on this data set are generally similar to those on war news. Due to the limit of space, we only sho w the CCMix results in Table 3, whic h are obtained by setting =.7 and B =.96 and xing the num ber of clusters to 8. Here we again see man y very interesting common themes; in-deed, the top two words in the common themes can pro vide a very good summary of the themes (e.g., \sound and speak-ers" for cluster1, \battery hours" for cluster5, and "Mi-crosoft Oce" for cluster8). However, the special themes, although suggesting some di erences among the three lap-tops, are much harder to interpret. This may be because there is a great deal of variation in pro duct-sp eci c opin-ions in the data, whic h mak es the data extremely sparse for learning a coheren t collection-sp eci c theme for eac h of the eigh t themes.
When we vary B and C in CCMix, the results are gen-erally di eren t. Speci cally , when B is set to a small value, non-informativ e stop words tend to sho w up in common themes. A reasonable value for B is generally higher than 0.9 { in that case, the mo del automatic ally eliminates the non-informativ e words from the theme clusters, allo wing for more discriminativ e clustering. Indeed, in all our exp eri-men ts, we have inten tionally retained all the stop words, and the mo del is clearly able to lter out non-informativ e words, though in some cases, they still sho w up as top words in the common themes of the news data. They can be \eliminated" by using an even higher B , but then we may end up having insucien t information to learn a common theme reliably . C a ects the vocabulary allo cation between the common and collection-sp eci c themes. In the news data exp erimen ts, when we change C to a value above 0.4, the collection-sp eci c terms would dominate the common theme mo dels. In the laptop data exp erimen ts, when C is less than 0.7, we lose man y con ten t keyw ords of the com-mon themes to the corresp onding collection-sp eci c themes. Both B and C are inten tionally left for a user to tune so that we can incorp orate application-sp eci c bias into the mo del.
The most related work to our work is the coupled clus-tering metho d presen ted in [8], whic h app ears to be one of the very few studies considering the clustering problem in multiple collections. They extend the information bottle-nec k approac h to disco ver common clusters across di eren t collections. Comparativ e text mining goes beyond this by analyzing both the similarities and collection-sp eci c di er-ences. We also use a completely di eren t approac h based on probabilistic mixture mo dels. Another related work is [10], where cross-training is used for learning classi ers from mul-tiple documen t sets. Our work di ers from it in that we per-form unsup ervised learning. The asp ect mo dels studied in [7, 2] are also related to our work but they are closer to our baseline mo del and are not designed for comparing multiple collections. There are man y studies in documen t clustering [1]. Again, the di erence lies in that they consider only one collection and thus are similar to the baseline mo del.
Our work is also related to documen t summarization, es-pecially multiple documen t summarization (e.g.,[9, 12]). In-deed, we can the results of CTM as a special form of sum-mary of multiple text collections. However, an imp ortan t di erence is that while a summary intends to retain the ex-plicit information in text (to main tain delit y), CTM aims at extracting non-ob vious implicit patterns.
In this pap er, we de ne and study a novel text mining problem referred to as comparativ e text mining. It is con-cerned with disco vering any laten t common themes across a set of comparable collections of text as well as summariz-ing the similarities and di erences of these collections along eac h theme.

We prop ose a generativ e cross-collection mixture mo del for performing comparativ e text mining. The mo del sim ul-taneously performs cross-collection clustering and within-collection clustering, and can be applied to an arbitrary set of comparable text collections. We de ne the mo del and presen t the EM algorithm that can estimate the mo del ef-cien tly. We evaluate the mo del on two di eren t text data sets (i.e., a news article data set and a laptop review data set), and compare it with a baseline clustering metho d based on a simple mixture mo del. Exp erimen t results sho w that the cross-collection mixture mo del is quite e ectiv e in dis-covering the laten t common themes across collections and performs signi can tly better than the baseline simple mix-ture mo del. The prop osed mo del has man y obvious applica-tions in opinion summarization and business intelligence. It also has man y other less obvious applications in the general area of text mining and seman tic integration of text. For example, our mo del can be used to compare the course web pages from the ma jor computer science departmen t web sites to disco ver core computer science topics. It can also be used to compare literature collections in di eren t comm unities to supp ort concept switc hing [11].

The work rep orted in this pap er is just an initial step toward a promising new direction. There are man y interest-ing future researc h directions. First, it may be interesting to explore how we can further impro ve the CCMix mo del and its estimation. One interesting direction is to explore the Maxim um A Posterior (MAP) estimator, whic h would allo w us to incorp orate more prior kno wledge in a princi-pled way. For example, a user may already have certain thematic asp ects in mind. With MAP estimation, we can easily add that bias to the comp onen t mo dels. Second, we can generalize our mo del to mo del semi-structured data to perform more general comparativ e data mining. One way to achiev e this goal is to introduce additional random variables in eac h comp onen t mo del so that we can mo del any struc-tured data. Finally , it would be very interesting to explore how we could exploit the learned theme mo dels to pro vide additional help to a user who wants to perform comparativ e analysis. For example, the learned common theme mo dels can be used to construct a hidden Mark ov mo del (HMM) to iden tify the parts in the text collections about the common themes, and to connect them through automatically gener-ated hyperlinks. This would allo w a user to easily navigate through the common themes. [1] D. Bak er and A. McCallum. Distributional clustering [2] D. Blei, A. Ng, and M. Jordan. Laten t Diric hlet [3] A. P. Dempster, N. M. Laird, and D. B. Rubin. [4] epinions.com, 2003. http://www.epinions.com/. [5] R. Feldman and I. Dagan. Kno wledge disco very in [6] M. A. Hearst. Untangling text data mining. In [7] T. Hofmann. Probabilistic laten t seman tic indexing. [8] Z. Marx, I. Dagan, J. Buhmann, and E. Shamir. [9] K. McKeo wn, J. L. Kla vans, V. Hatziv assiloglou, [10] S. Sara wagi, S. Chakrabarti, and S. Godbole. [11] B. R. Schatz. The interspace: Concept navigation [12] H. Zha. Generic summarization and keyphrase
