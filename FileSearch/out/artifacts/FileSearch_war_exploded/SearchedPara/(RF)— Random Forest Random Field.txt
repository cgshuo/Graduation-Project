 which provides a principled way to jointly reason about mult iple, statistically dependent random its utility on a challenging task of conjoint object recogni tion and segmentation. Identifying subimage ownership among occurrences of disti nct object classes in an image is a fun-damental, and one of the most actively pursued problem in com puter vision, machine learning, and each image pixel. Our approach builds on the following commo n recognition strategies: (i) Labels of neighboring image parts are likely to be correlated  X  one o f the main principles of perceptual organization; and (ii) Recognized objects dictate which ot her objects to expect in the scene, and and context by a graphical model aimed at capturing statisti cal dependencies among random vari-unified framework for combined object recognition and segme ntation, as a graph-structured predic-tion of all random variables in a single, consistent model of the scene.
 The graphical model we use is Conditional Random Field (CRF) [12] X  X ne of the most popular recognition and segmentation. CRF defines a posterior distr ibution of hidden random variables Y (labels), given observed image features X , in a factored form: p ( Y | X ;  X  )= 1 Each potential  X  The potentials are often defined as linear functions of param eters,  X  as a log-linear function, which is not adequate when the dete ctor outputs do not provide a linear MAP assignment that minimizes the energy P graphs. The intractability of CRF learning and inference of ten motivates prior work to resort to approximate algorithms, e.g., graph-cuts, and loopy belief propagatio n (LBP). The effect of these approximations on the original semantics of CRF is poorly un derstood. For example, an approximate Motivation: Some of the aforementioned shortcomings can be addressed wh en CRF inference is conducted using the Metropolis-Hastings (MH) algorithm. M H draws samples Y ( t ) from the CRF X  X  the proposal and posterior distributions. Consequently, t he bottleneck of every CRF learning and inference  X  namely, computing the partition function Z  X  is eliminated in MH. of computing each individual distribution for conducting M H jumps. Previous work on MH for amples, RF grows many decision trees. We view the trees as a wa y of discriminatively structuring of a stacked-classifier architecture which replaces the sta ndard majority voting of RF. shown that RF that cuts down to pure leaves uses a weighted, la yered, nearest neighbor rule [16]. Contributions: We combine RF and CRF into a new, principled and elegant compu tational frame-examples in leaf nodes of each decision tree. This evidence i s then used for the non-parametric of which is then used to derive the theoretical performance bou nds of a two-class ( RF ) 2 . Paper Organization: Sections 2 X 4 specify the CRF model, its MH-based inference, and RF-based We formulate multiclass object recognition and segmentati on as the MAP inference of a CRF, de-boundaries using the domain knowledge. Since the right scal e at which objects occur is unknown, we use all regions from all scales.
 The extracted regions are organized in a graph, G = ( V,E ) , with V and E are sets of nodes and edges. The nodes i =1 ,...,N correspond to multiscale segments, and edges ( i,j )  X  E capture their spatial relations. Each node i is characterized by a descriptor vector, x i is descendent of region j , if i is fully embedded as subregion within ancestor j . Two regions i Each edge ( i,j ) is associated with a tag, e CRF is defined as the graphical model over G . Let Y = { y with the nodes, indicating the class label of the correspond ing region, y K p of nodes. Then, we define CRF as Multi-coloring of CRF is defined as the joint MAP assignment Y  X  = arg max following section, we explain how to conduct this inference . For CRF inference, we use the Swendsen-Wang cut algorithm (S W-cut), presented in [18]. SW-cut iterates the Metropolis-Hastings (MH) reversible jump s through the following two steps. (1) p MCMC methods that consider one node at a time (e.g., Gibbs sam pler), SW-cut operates on a num-ber of nodes at once. Consequently, SW-cut converges faster and enables inference on relatively large graphs. Below, we review steps (1) and (2) of SW-cut, fo r completeness. components CC . If two nodes i and j have different labels, they cannot be in the same CC , so according to posterior distribution p a connected component CC from step (1), and randomly reassign a new color to all nodes i n that CC . To separate the re-colored CC from the rest of the graph, we cut existing edges that connect The acceptance rate,  X  ( A  X  B ) , of the move from A to B is defined as distributions characterizing states A and B can be specified as where Cut neighbors of node i , N ( i ) = { j : j  X  V, ( i,j )  X  E } .
 initialize all nodes in the CRF with label 0. Next, we show how to compute the ratios in Eq. (3). since RF provides near Bayesian optimal decisions, as theor etically shown by Breiman [13]. In the following, we describe how to build RF, and use it for computi ng the ratios in Eq. (3). labels it covers. Each region i is characterized by a d-dimensional descriptor vector, x which encodes the photometric and geometric properties of i . The training dataset { ( x 1 ,...,M } is used to learn an ensemble of T decision trees representing RF. reaches a leaf node. Each leaf l records a class histogram,  X   X  of training examples in l is then k  X  histogram,  X  also have the relationship type e  X  namely, ascendent/descendent, touching, or far relation ship. Given  X  defined in (3), which control the Metropolis-Hastings jumps in the SW-cut. Suppose two regions, represented by their descriptors x state B of one iteration of the SW-cut. Also, after passing x learned RF, suppose they reached leaves l t ual probability p  X  X ut X  edges, Cut performance bounds of a simple, two-class ( RF ) 2 . ( First, the MSRC dataset consists of 591 images showing objec ts from 21 categories [3]. We use the of 3547 images of urban environments, and has manually annot ated regions [6, 19]. As in [6], one labels of bounding boxes around object occurrences as groun d truth.
 ceptual significance of a region boundary, P descriptor vector consisting of the following properties: (i) 30-bin color histogram in the CIELAB Gaussian filters computed at each pixel, and mapped to 250 cod ewords whose dictionary is obtained by K-means over all training images; (iii) 128-dimensional region boundary descriptor measuring random splits of training data to train 100 decision trees of RF, constructed in the top-down way. connected CRF graph from the extracted regions (Sec. 2), and run the SW-cut inference (Sec. 4). e , are not accounted for when computing p recorded by leaf nodes of the learned decision trees, but it d oes not increase complexity. occur in images more frequently. Tab. 1 and Tab. 2 show our pix el-wise classification accuracy on MSRC and Street-Scene images. Table. 2 also compares the t hree variants of ( RF ) 2 on MSRC segmentation. As can be seen, ( RF ) 2 outperforms the latest CRF models on both datasets. Our segmentation results on example MSRC and Street-Scene i mages are shown in Fig. 5. Labels performance for all object classes except one. and changes in illumination and scale. (best viewed in color )
Method MSRC StreetScene Test time ( RF ) 2 -1 69.5%  X  13.7% 78.2%  X  0.5% 45s ( RF ) 2 -2 80.2%  X  14.4% 86.7%  X  0.5% 31s ( RF ) 2 -3 82.9%  X  15.8% 89.8%  X  0.6% 31s Table 2: The average pixel-wise classification accuracy and average computation times on the MSRC and Street-Scene datasets of the three variants of our approach with those of the state-of-the-art CRF-based methods. guided SW-cut inference of CRF takes 10s X 30s on a 2.40GHz PC w ith 3.48GB RAM for MSRC and Street-Scene images. Table 2 shows that our average runn ing times are comparable to those of the other CRF methods that use approximate inference [3, 6, 2 0, 21]. MH jump between states A and B is controlled by the acceptance rate  X  ( A  X  B ) which depends on bounds of RF is based on the theoretical analysis of evidence trees, presented in [15]. 6.1 An Upper Error Bound of ( RF ) 2 An error occurs along MH jumps when a balanced reversible jump is encountered, i.e., when there is no preference between jumping from state A to state B and reverse, q ( B  X  A ) predicts that the posterior distribution of state B is larger than that of A , p ( Y = B | G ) probability of this error, specified as From Eq. (6), P (  X  ) can be computed using the probability density function of a p roduct of ran-dom variables Z sequel, all random variables Z Also, we will prove that all random variables W f f the distribution f within CC is the same as the number of nodes in CC , as a result of the probabilistic  X  X utting X  of graph edges by the SW-cut algorithm. Given f following theorem.
 Theorem 1. The probability that ( RF ) 2 makes a wrong prediction is P (  X  )= P ( Z W  X  1)=  X K tions f  X  , and where K Proof. Define H = Z W . Then, f K 1  X  R 1 0 f H ( h ) dh =  X K 1 (  X  ) .
 As we will show in the following section, the parameter  X  is directly proportional to a measure of accuracy of RF predictions, referred to as probabilistic margin. Since K decreases as the probabilistic margin of RF increases. 6.2 A Mathematical Model of RF Performance In this section, we derive that the RF estimates of the ratios of posteriors Z to show that when a positive instance is dropped through one o f the decision trees in RF, it will states that the probability that evidence voting misclassi fies an instance, P (  X  Proposition 1. The probability that RF with T trees, where every leaf stores C training instances, incorrectly classifies an instance is upper bounded, P (  X  Proof. Evidence voting for labeling an instance can be formalized a s drawing a total of C T inde-pendent Bernoulli random variables, with the success rate p these Bernoulli random variables. Thus, a positive instanc e is incorrectly labeled if S ative instance is misclassified if S Chernoff bound, we obtain P (  X  ciated Bernoulli random variable, is p misclassifies a pair of test instances, P (  X  Proposition 2. Given RF as in Proposition 1, the probability that RF incorre ctly labels a pair of instances having a certain relationship is upper bounded, P (  X  Proof. Evidence voting for labeling a pair of instances can be forma lized as drawing a total of C where +1 is received for correct, and  X  1 for incorrect labeling of the instance pair. Let S a sum of these Bernoulli random variables. Then, P (  X  pair of instances when dropping the pair through a decision t ree, the success rate p as fraction of pairs of instances that have the same type of rela tionship.
 terior ratio of an instance is upper bounded, P ( Z gives the probability density function f terior ratio of a pair of instances is upper bounded, P ( W  X  i  X  CC and j  X  N ( i ) . This gives the probability density function f W where  X  fixed to their typical values: C = 20 , T = 100 , and  X  = 0 . 1 . We have presented ( RF ) 2  X  a framework that uses the random forest (RF) for the MCMC-ba sed inference of a conditional random field (CRF). Our key idea is to employ RF to directly compute gence rate and accuracy of the CRF inference. Such a non-para metric formulation of CRF and its parametric CRF models on the task of multiclass object recog nition and segmentation. We have also trees, the number of training examples stored in every leaf n ode, and the probabilistic margin.
