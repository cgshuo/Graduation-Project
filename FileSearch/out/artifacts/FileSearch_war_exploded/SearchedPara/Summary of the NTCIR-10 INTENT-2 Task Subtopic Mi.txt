 The NTCIR INTENT task comprises two subtasks: Subtopic Min-ing , where systems are required to return a ranked list of subtopic strings for each given query; and Document Ranking , where sys-tems are required to return a diversified web search result for each given query. This paper summarises the novel features of the Sec-ond INTENT task at NTCIR-10 and its main findings, and poses some questions for future diversified search evaluation. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval diversity, evaluation, intents, subtopics, test collections NTCIR (NII Testbeds and Community for Information access Research) 1 is a sesquiannual evaluation forum that focusses pri-marily on Asian language information access. The INTENT task launched at NTCIR-9 [16], is closely related to the TREC Web Di-versity Task [4]. The Second INTENT task (INTENT-2) was con-cluded at the NTCIR-10 conference in June 2013 [12]. This paper summarises the novel features of the task and its main findings, and poses some questions for future diversified search evaluation.
Figure 1 outlines the INTENT task. In the Subtopic Mining (SM) subtask, participants are asked to return a ranked list of subtopic strings for each query from the topic set (Arrows 1 and 2), where a subtopic string is a query that specialises and/or disambiguates the search intent of the original query . The organisers create a pool of these strings for each query, and ask the assessors to manually cluster them, and to provide a label for each cluster. Then the or-ganisers determine a set of important search intents for each query, http://research.nii.ac.jp/ntcir/ http://research.microsoft.com/INTENT/ Table 1: Number of INTENT-2 runs (teams). E: English; C: Chinese; J: Japanese.
 where each intent is represented by a cluster label with its clus-ter of subtopics (Arrows 3 and 4). Then they ask ten assessors to vote whether each intent is important or not for a given query; and based on the votes compute the intent probabilities (Arrows 5 and 6) [16]. The SM runs are then evaluated using the intents with their associated probabilities and subtopic strings. This subtask can be regarded as a component of a search result diversification system, but other applications such as query suggestion and completion are also possible.
 The black arrows in Figure 1 show the flow of the Document Ranking (DR) subtask, which is similar to the TREC Diversity Task [4]. Participants are asked to return a diversified ranked list of URLs for each query from the aforementioned topic set (Arrows 7 and 8). The organisers create a pool of the URLs for each query, ask the assessors to conduct graded relevance assessments for each intent of each query, and consolidate the relevance assessments to form the final graded relevance data (Arrows 9, 10 and 11) [16]. The DR runs are evaluated using the intents, their probabilities and the relevance data. The aim of search result diversification is to maximise both the relevance and diversity of the first search engine result page, given a query that is ambiguous or underspecified .
Table 1 shows the number of runs submitted to and the number of teams that participated in the INTENT-2 task. The English SM subtask was introduced at INTENT-2, by using the TREC 2012 Web topics kindly provided by the Web Track coordinators.
For the general task design of INTENT, we refer the reader to the INTENT-1 overview paper [16]. New features of INTENT-2 are as follows. (I) As we have mentioned earlier, we introduced an English SM subtask using the 50 TREC 2012 Web Track topics. While the TREC  X  X ubtopics X  were created at NIST [4] 3 , we inde-pendently created a set of intents for each topic at INTENT-2 from subtopic pools, as shown in Figure 1 4 . (II) We provided an  X  X fficial X  set of search engine query sugges-tions for each query to participants, to improve the repro-ducibility and fairness of experiments. (At INTENT-1, dif-ferent teams scraped their own versions of query suggestions from different search engines.) (III) For the Chinese and Japanese topic sets, we provided a base-line non-diversified run and the corresponding web page con-tents to participants 5 . This enables researchers to isolate the problem of diversifying a given search result from that of producing an effective initial search result. (IV) We included single-intent navigational topics in the Chinese and Japanese topic sets. A navigational topic should require one answer or one website, and therefore may not require diversification. We thereby encouraged participants to ex-plore selective diversification [15]: instead of uniformly ap-plying a diversification algorithm to all topics, determine in advance which topics will (not) benefit from diversification. Moreover, we tagged each intent with either informational or navigational based on five assessors X  votes: each assessor judged each intent independently by referring to a specific guideline we provided, and an intent was tagged with nav-igational only if four or five assessors judged it to be navi-gational. This enables us to conduct intent type-sensitive di-versification [8], whose rationale is that returning redundant results to navigational intents should not be rewarded, and that more space should be allocated to informational intents. (V) All participants were asked to produce results not only for the INTENT-2 topics but also for the INTENT-1 topics. More-over, participants who also participated in INTENT-1 were encouraged to submit  X  X evived Runs X  to INTENT-2, using their systems from INTENT-1. This is to monitor progress across NTCIR rounds. Figure 2 outlines this effort. By com-paring the new INTENT-2 runs with the Revived Runs (ar-rows (a) vs. (d)), we can discuss progress across the two rounds; by comparing the performance over the INTENT-1 topic set and that over the INTENT-2 topic set for each of the Revived Runs (arrows (c) vs. (d)), we can discuss the comparability of the two test collections [7] 6 . Unfortunately, no Revived Run was submitted to the SM subtask, so we can only conduct this analysis for the DR subtask. http://trec.nist.gov/data/web/12/ full-topics.xml We will report elsewhere [10] on an analysis across TREC and NTCIR using the common topic set.
We did not do this for English as we had no English DR subtask. Arrow (b) in Figure 2 means evaluating new runs by reusing the INTENT-1 test collection, but it is known that diversified search test collections are highly unlikely to be reusable mainly due to shallow pooling [9, 11].

Tables 2 and 3 provide some statistics of the INTENT-2 test collections that we have constructed. Our test collections contain per-intent graded relevance assessments that reflect two assessors X  judgments: for example, L 4 is the highest relevance level, which means that two assessors each assigned a score of two to the doc-ument [16]. As for the INTENT-2 document collections, they are the same as the ones used at INTENT-1 [16]: the SogouT corpus and the Japanese portion of ClueWeb09 8 .
The primary evaluation metrics we use (for both SM and DR subtasks) are intent recall ( X  X -rec X ) and D( )-nDCG . I-rec (a.k.a. subtopic recall [17]) is simply the proportion of intents covered by a system output. D-nDCG is a diversity version of the well-known normalised discounted cumulative gain [5], where a gain value of a relevant document is computed as the  X  X lobal Gain X  (GG) across all known intents. At INTENT-2, the  X  X ocal X  (i.e. per-intent) gain values are set to 4, 3, 2 and 1 for each L 4 -, L 3 -, L 2 relevant document, respectively.

For each participating run, we plot D-nDCG (overall relevance) against I-rec (diversity), and also compute D -nDCG as a sim-ple linear combination of I-rec and D-nDCG. Some advantages of the D-measure framework over  X  -nDCG [3] and intent-aware met-rics [1, 2] have been demonstrated elsewhere [13, 14]. http://www.sogou.com/labs/dl/t-e.html http://lemurproject.org/clueweb09/
The D-measure framework was designed for diversified search, but we used it also for evaluating the ranked list of subtopic strings in the SM subtask. In the case of the SM task, by construction, each subtopic string belongs to exactly one intent and only binary relevance is available. Thus, D-nDCG reduces to standard nDCG where each gain value is exactly the probability of the intent to which the subtopic string belongs.

In the INTENT-2 DR subtask, we additionally used intent type-sensitive metrics called DIN-nDCG and P+Q [8], by leveraging the informational and navigational intent tags. As was mentioned earlier, the rationale is that, since a navigational intent requires exactly one relevant document by definition, evaluation metrics should not reward systems for returning redundant information for such a topic; instead, systems should aim to allocate more space to informational intents.

DIN-nDCG is a simple variant of D-nDCG: for each navigational intent, it treats only the first retrieved relevant document as relevant, and ignores the rest. P+Q is a combination of metrics called P Q-measure : Q is a graded-relevance version of Average Precision and is suitable for informational intents; P + is a similar metric but it assumes that no user will go beyond the first most highly relevant document found in the ranked list. It is therefore suitable for nav-igational intents. P+Q computes a Q value for each informational intent and a P + value for each navigational intent, and finally com-bines the values using the intent-aware approach [1, 2]. In short, compared to the TREC Web Track Diversity Task, the INTENT-2 evaluation framework has the following unique features: (a) we utilise intent probabilities, to encourage systems to allocate more space in the search engine result page to popular intents than to minor ones; (b) we utilise per-intent graded relevance, to encour-age retrieval of highly relevant documents over marginally relevant ones; (c) we encourage selective diversification (decide whether or not to diversify per topic) and intent type-aware diversification (do not reward systems for returning redundant information for naviga-tional intents) 9 .
The official I-rec/D-nDCG graphs, which show the balance be-tween overall relevance and diversity, are shown in Figures 3-6. The Japanese DR results are omitted due to lack of space: we had only two participating teams for this subtask (see Table 1). Below, we summarise our main findings.
 English Subtopic Mining (Figure 3) THUIR-S-E-4A outperformed all other runs in terms of Mean D -nDCG, but hultech , KLE , ORG (organisers X  team), SEM12 and THCIB all have at least one run that is statistically indistinguishable from this top run. Whereas, all runs from LIA and TUTA1 significantly underperformed THUIR-S-E-4A .
 Chinese Subtopic Mining (Figure 4) TUTA1-S-C-1A outperformed all other runs in terms of Mean D -nDCG, but the six par-ticipating teams are statistically indistinguishable from one another.
The TREC 2011 and 2012 diversity test collections have graded relevance assessments; all TREC diversity test collections (2009-2012) have the informational and navigational subtopic tags. How-ever, they have not been utilised for evaluating the runs.
Figure 3: I-rec/D-nDCG graph for English Subtopic Mining.
Figure 4: I-rec/D-nDCG graph for Chinese Subtopic Mining. Figure 5: I-rec/D-nDCG graph for Japanese Subtopic Mining. Japanese Subtopic Mining (Figure 5) ORG-S-J-3A outperformed Chinese Document Ranking (Figure 6) THUIR-D-C-1A outper-Japanese Document Ranking (figure not shown) MSINT-D-J-4B Navigational Topics The D -nDCG values for navigational topics Figure 6: I-rec/D-nDCG graph for Chinese Document Rank-ing. Figure 7: Per-topic D-nDCG/D IN-nDCG/P+Q performances for THUIR-D-C-1A. Figure 8: Correlation between D-nDCG and DIN-nDCG/P+Q for Chinese Document Ranking. ues: for these topics that have one navigational intent and no informational intents, P+Q reduces to P + , which reaches one if an L 4 -relevant document is retrieved at rank 1. Figure 7 shows per-topic performances for our Chinese DR top per-former THUIR-D-C-1A : This run achieves a P+Q of one for the six navigational topics indicated with baloons.
 Navigational Intents Intent type-sensitive metrics that leverage the informational and navigational intent tags produce system rankings that are somewhat different from that produced by the intent type-agnostic D-nDCG, although, by definition, DIN-nDCG approaches D-nDCG as the fraction of naviga-tional subtopics decreases. Figure 8 visualises the correla-tions for the Chinese DR subtask: it can be observed that the correlation between D-nDCG and DIN-nDCG is much higher than that between D-nDCG and P+Q.
 We also investigated the comparability of the INTENT-1 and INTENT-2 topic sets using the Revived Runs submitted to the Chi-nese and Japanese DR subtasks (one Chinese run and two Japanese runs). According to two-sample bootstrap hypothesis tests [6], there were no significant performance differences at  X  =0 . 05 for any of our primary metrics (I-rec and D( )-nDCG), which suggests that the topics sets are more or less comparable (see Figure 1).
While it may be too demanding to expect a substantial progress between just two rounds of NTCIR, the progress monitoring prac-tice depicted in Figure 2 is probably worth continuing and to apply it to other tasks.

The TREC Web Track has discontinued the diversity task; it is not clear if there will be an INTENT-3 task at NTCIR-11. However, it should be noted that diversity test collections are highly unlikely to be reusable [9, 11]: thus, if researchers want to continue improv-ing diversified search 10 , we do require a new diversity test collec-tion. Note also that now a new corpus, ClueWeb12, is available [4]. Do we want a new diversity test collection based on this corpus? Do we need a new approach to evaluating diversified search? We would like to discuss these questions at the SIGIR poster session.
We note that there is a full paper session on Diversity at SIGIR 2013: diversified search is still a popular topic.
