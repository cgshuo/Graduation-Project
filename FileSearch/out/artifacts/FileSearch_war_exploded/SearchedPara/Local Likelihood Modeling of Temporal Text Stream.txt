 Guy Lebanon lebanon@stat.purdue.edu Yang Zhao zhao18@stat.purdue.edu Time stamped documents such as news stories often cannot be accurately modeled by a single time invari-ant distribution. An alternative is to assume that the concepts underlying the distribution generating the data drift with time. In other words, the data is gen-erated by a time dependent process z ( t )  X  p t ( z ) , t  X  I  X  R whose approximation {  X  p t : t  X  I } becomes the main objective of the learning task. We assume that the time t is a continuous quantity, even in cases where the realized time points form a discrete sample. For example, assuming that the time stamps repre-sent the days of the year when the documents were authored, we assume that the set { 1 , . . . , 365 } is a discrete sample from a underlying continuous interval [1 , 365]. We further assume that the data samples z ( t ) sampled from p t , correspond to pairs z ( t ) = ( x, y ) con-stituting a document x and a categorial-valued label y . Such pairs ( x, y ) appear often in practice, for example with y corresponding to the document topic (Lewis et al., 2004), sentiment (Pang &amp; Lee, 2005), author (Mosteller &amp; Wallace, 1964) or Email spam/no-spam (Mulligan, 1999).
 Assuming that our data is a set of time stamped doc-uments and labels ( t, ( x, y )), the drift p t ( x, y ) can be characterized by considering the temporal transition of the joint distribution p t ( x, y ), the conditionals p t p which of the distributions above to model depends on the application at hand. For example, modeling p ( y | x ) is usually sufficient for document classification purposes while modeling p t ( x | y ) is necessary for lan-guage modeling which is an important component in speech recognition, machine translation, and IR. We demonstrate the presence of concept drift in prac-tice by considering the Reuters RCV1 dataset (Lewis et al., 2004) which contains over 800,000 news sto-ries gathered in a period spanning 365 consecutive days and categorized according to topic. Figure 1 dis-plays the temporal change in the relative frequency (number of appearance in a document divided by document length) of three words: million , common , and Handelsgesellschaft (German trade unions) for documents in the most popular RCV1 category titled CCAT . It is obvious from these plots that the relative frequency of these words vary substantially in time. For example, the word Handelsgesellschaft appear in 8 distinct time regions, representing time points in which German trade unions were featured in the Reuters news archive.
 The temporal variation in relative frequencies illus-trated by Figure 1 corresponds to a drift in the dis-tribution generating the data. Since the drift is rather pronounced, standard estimation methods based on maximum likelihood are not likely to accurately model the data. In this paper, we consider instead estimat-ing { p t ( x, y ) : t  X  I } based on the the local likelihood principle. Local likelihood is a locally weighted ver-sion of the loglikelihood with the weights determined by the difference between the time points associated with the sampled data and a the time at which the inference takes place.
 After presenting a more formal discussion of concept drift in Section 3 and the definition of local likelihood in Section 4 we turn to examine in detail the case of modeling p t ( x | y ) with local likelihood for n -grams and modeling p t ( y | x ) with local likelihood for logistic re-gression. In the case of 1-grams or the naive Bayes model, we provide a precise as well as asymptotic de-scription of the bias and variance which illuminates certain facts concerning the selection of weights and the difference between the online and offline scenarios. Experiments conducted on the RCV1 dataset demon-strates the local likelihood estimation in practice and contrasts it with more standard non-local alternatives. Concept drift or similar phenomena under different names have been studied in a number of communi-ties. It has recently gained interest primarily due to an increase in the need to model large scale temporal data streams.
 Early machine learning literature on the concept drift problem involved mostly computational learning the-ory tools (Helmbold &amp; Long, 1994; Kuh et al., 1990). Hulten et al. (2001) studied the problem in the context of datamining large scale streams whose distribution change in time. More recently, Forman (2006) studied the concept drift phenomenon in the context of infor-mation retrieval in large textual databases. Sharan and Neville (2007) consider the modeling of temporal changes in relational databases and its application to text classification.
 Overall, the prevailing techniques have been to train standard methods on examples obtained by filtering the data through a sliding window. Tibshirani and Hastie (1987) developed the local likelihood idea in the statistics community within the context of non-parametric smoothing and regression. More details on local likelihood can be found in (Loader, 1999). Formally, the concept drift phenomenon may be thought of as a smooth flow or transition of the joint distribution of a random vector. We will focus on the case of a joint distribution of a random vector X and a random variable Y representing predictor and re-sponse variables. We will also restrict our attention to temporal or one dimensional drifts.
 Definition 1. Let X and Y be two discrete random vectors taking values in X and Y . A smooth temporal drift of X, Y is a smooth mapping from I  X  R to a family of joint distributions By restricting ourselves to discrete random variables we can obtain a simple geometrical interpretation of concept drift. Denoting the simplex of all distributions over the set S by we have that Definition 1 is equivalent to a smooth parameterized curve in the simplex P X X Y .
 The drift in the joint distribution can be decomposed in several ways. The first decomposition p t ( x, y ) = p ( x | y ) p t ( y ) is useful for generative modeling and the for conditional modeling. In the generative case we will cally an easier problem due to its lower dimensionality (in most cases involving text documents |Y|  X  |X| ). In the case of conditional modeling, we focus on mod-eling p t ( y | x ) and we ignore the drift in the marginal p ( x ) since it is irrelevant for discriminative tasks. In both cases we assume that our data is a set of time-stamped labeled documents sampled from p t ( x, y ) where the time points t are sampled from a distribu-tion g ( t ). If g is a continuous density, the number of samples at time t , denoted by N t , is no greater than 1 with probability 1. In practice, however, we allow N t to be larger than 1 in order to account for the dis-cretization of time. We thus have the data where the time points are sampled from g ( t ) and ( x tj , y tj )  X  p t ( x, y ).
 To illustrate these concepts in the context of the RCV1 dataset, we display in Figure 2 the total number of words per day (left) and the total number of docu-ments per day (right) corresponding to the most pop-ular category in RCV1. As is evident from the right panel, g ( t ) is a highly non-uniform density correspond-ing to varying amount of news content in different dates.
 It is easy to come up with two simple solutions to the problem of concept drift modeling. The first so-lution, called the extreme global model, is to simply ignore the temporal drift and use all of the samples in D regardless of their time stamp. This approach results in a single global model  X  p which serves as an estimate for the entire flow { p t , t  X  I } effectively mod-eling the concept drift as a degenerate curve equiva-lent to a stationary point in the simplex. The second simple alternative, called the extreme local model, is to model p t using only data sampled from time t i.e. poses the concept drift estimation into a sequence of disconnected estimation problems.
 The extreme local model has the benefit that if the individual estimation problems are unbiased, the esti-mation of the concept drift is unbiased as well. The main drawback of this method is the high estimation variance resulting from the relatively small number of daily samples N t used to estimate the individual mod-els. Furthermore, assuming D is finite we can only estimate the drift in the finite number of time points appearing in the dataset D (since we have no train-ing data for the remaining time points). On the other hand, the extreme global model enjoys low variance since it uses all data points to estimate p t . Its main drawback is that it is almost always heavily biased due to the fact that samples from one distribution p t used to estimate a different distribution p t It is a well known fact that the optimal solution in terms of minimizing the mean squared estimation er-ror usually lies between the extreme local and extreme global models. An intermediate solution can trade-off increased bias for reduced variance and can signif-icantly improve the estimation accuracy. Motivated by this principle, we employ local smoothing in form-ing a local version of the maximum likelihood principle which includes as special cases the two extreme models mentioned above. The intuition behind local smooth-ing in the present context is that due to the similar-ity between p t and p t +  X  , it makes sense to estimate p t using samples from neighboring time points t +  X  . However, in contrast to the global model the contribu-tion of points sampled from p t +  X  towards estimating p should decrease as  X  increases. The local likelihood principle extends the ideas of non-parametric regression smoothing and density estima-tion to likelihood-based inference. We concentrate on using the local likelihood principle for estimating p ( x | y ) and p t ( y | x ) which are described next. 4.1. Local Likelihood for n -Gram Estimation We apply local likelihood to the problem of estimating p ( x | y ) by assuming the naive Bayes assumption i.e. that x | y is generated by a multinomial distribution or its n -gram extensions. Assuming documents contain words belonging to a finite dictionary of size V , the naive Bayes assumption may be stated as where c ( w, x ) represents the number of times word w appears in document x . Similarly, the n -gram model extends naive Bayes (3) by considering n -order Markov dependency. The naive Bayes and n -gram are a main-stay of statistical text processing (Manning &amp; Schutze, 1999) and usually lead to accurate language modeling, especially when appropriate smoothing is used (Chen &amp; Goodman, 1998). For notational simplicity we con-sider the problem of estimating p t ( x ) rather than the equivalent p t ( x | y ) and we concentrate on naive Bayes i.e. 1-gram. Extending the discussion to n -grams with n &gt; 1 is relatively straightforward and is omitted due to lack of space.
 Applied to the concept drift problem, the local log-likelihood at time t is a smoothed or weighted ver-sion of the loglikelihood of the data D in (2) with the amount of smoothing determined by a non-negative smoothing kernel K h : R  X  R We assume that the kernel function is a normalized density concentrated around 0 and parameterized by a scale parameter h &gt; 0 reflecting its spread and satisfying the relation K h ( r ) = h  X  1 K ( r/h ) for some K : R  X  R referred to as the base kernel form. We further assume that K has bounded support and R u r K ( u ) du &lt;  X  for r  X  2. Wand and Jones (1995) provide more details on the formal requirements of a smoothing kernel.
 Three popular kernel choices are the tricube, tri-angular and uniform kernels, defined as K h ( r ) = h  X  1 K ( r/h ) where the K ( ) functions are respectively The uniform kernel is the simplest choice and leads to a local likelihood (4) equivalent to filtering the data by a sliding window i.e.  X   X  t is computed based on data from adjacent time points with uniform weights. Un-fortunately, it can be shown that the uniform kernel is suboptimal in terms of its statistical efficiency or rate of convergence to the underlying distribution (Wand &amp; Jones, 1995). Surprisingly, the triangular kernel has a higher statistical efficiency than the Gaussian kernel and is the focus of our experiments in this subsection. We use the tricube kernel in the next subsection. The scale parameter h is central to the bias-variance tradeoff. Large h represents more uniform kernels achieving higher bias and lower variance. Small h rep-resents a higher degree of locality or lower bias but higher variance. Since lim h  X  0 K h approaches Dirac X  X  delta function and lim h  X  X  X  K h approaches a constant function the local log-likelihood (4) interpolates be-tween the loglikelihoods of the extreme local model and the extreme global model mentioned in Section 3 as h ranges from 0 to +  X  .
 Solving the maximum local likelihood problem for each t provides an estimation of the entire drift {  X   X  t : t  X  R } with  X   X  t = arg max  X   X   X   X  t (  X  | D ). In the case of the naive Bayes or n -gram model we obtain a closed form expression for the local likelihood maximizer  X   X  t as well as convenient expressions for its bias and variance. In general, however, there is no closed form maximizer and iterative optimization algorithms are needed in order to obtain  X   X  t = arg max  X   X   X   X  t (  X  | D ) for all t . We denote the length of a document in (2) by | x words in day t in (2) by | x t | def = P N t j =1 | x P documents x tj is independent of t and is drawn from a distribution with expectation  X  .
 Under the above assumptions, the local likelihood (4) of the naive Bayes model becomes where  X   X  P V . The local likelihood has a single global maximum whose closed form is obtained by setting to 0 the gradient of the Lagrangian to obtain The estimator  X   X  t is a normalized linear combination of word counts where the combination coefficients are determined by the kernel function and normalized by the number of words in different days. We note that  X   X  in (8) is different from a weighted averaging of the relative frequencies c ( w, x  X  j ) / P w  X  c ( w  X  , x  X  j We distinguish between two fundamental scenarios for predicting the drift  X  t .
 Offline scenario: The goal is to estimate the drift Online scenario: The goal is estimate a model for As with other statistical estimators, the accuracy of  X   X  may be measured in terms of its mean squared error (  X   X  t  X   X  t ) 2 which decomposes as the sum of the squared bias and variance of  X   X  t . Examining these quantities allow us to study the convergence rate of  X   X  t  X   X  and its leading coefficient .
 Proposition 1. The bias vector bias (  X   X  t ) def = E  X   X  and variance matrix of  X   X  t in (8) are bias (  X   X  t ) = where diag ( z ) is the diagonal matrix [ diag ( z )]  X  Proof. The random variable (RV) c ( w, x  X  j ) is dis-tributed as a sum of multivariate Bernoulli RVs, or sin-gle draws from multinomial distribution. The expec-tation and variance of the estimator are that of a lin-ear combination of iid multinomial RVs. To conclude the proof we note that for Y  X  Mult(1 ,  X  ), E Y =  X  , Var (  X  ) = diag(  X  )  X   X  X   X  .
 Examining Equations (9)-(10) reveals the expected de-pendency of the bias on h and  X  t . The contribution to the bias of the terms (  X   X   X   X  t ), for large |  X   X  t | , will decrease as h decreases since the kernel becomes more localized and will reduce to 0 as h  X  0. Similarly, for slower drifts, k  X   X   X   X  t k , t  X   X  will decrease and reduce the bias.
 Despite the relative simplicity of Equations (9)-(10), it is difficult to quantitatively capture the relationship between the bias and variance, the sample size, h,  X  , and the smoothness of  X  t , g . Towards this goal we de-rive the following asymptotic expansions.
 Proposition 2. Assuming (i)  X , g are smooth in t , (ii) h  X  0 , hn  X   X  , (iii) g &gt; 0 in a neighborhood of t , and (iv) document lengths do not depend on t and have expectation  X  , we have in the offline case bias (  X   X  t | I ) = h 2 21 ( K )  X   X  t Var (  X   X  t | I ) = 02 bias (  X   X  t | I ) = h 11 ( K )  X   X  t + o P ( h ) (12) Var (  X   X  t | I ) = 02 where kl ( K ) def = R u k K l ( u ) du is assumed to be finite and  X   X  t is the vector [  X   X  t ] i = d dt [  X  t ] i . The proof is somewhat similar to the derivation of the asymptotic bias and variance of the Nadaraya-Watson local regression (Wand &amp; Jones, 1995) and is omit-ted due to space limitations. The notation g n p  X  f represents convergence in probability of g n to f i.e.  X   X  &gt; 0 , P ( | g n  X  f | &gt;  X  )  X  0, and g n = o P ( f sents g n /f n p  X  0.
 Corollary 1. Under the assumptions in Proposi-tion 2, and in particular h  X  0 , nh  X   X  , the esti-mator  X   X  t is consistent i.e.  X   X  t p  X   X  t in both the offline and online settings.
 Proposition 2 specifies the conditions for consistency as well as the rate of convergence. In particular, the bias of online kernels converges at a linear rather than quadratic rate. In either cases, the estimator is biased and inconsistent unless h  X  0 , n  X   X  and nh  X  1  X   X  . Expressions (11)-(12) reveal the performance gain associated with a slower drift and sampling density g indicated by  X   X  t and g  X  ( t ) and with more (represented by n ) and longer (represented by  X  ) documents. Figure 3 displays the RCV1 per-word test set loglikeli-hood for the online and offline scenarios as a function of the (triangular) kernel X  X  bandwidth. As expected, offline kernels performs better than online kernels with both achieving the best performance for a bandwidth approximately 25 which corresponds to a support of 25 days in the online scenario and 50 days in the offline scenario. Note that in addition to obtaining higher ac-curacy than the global model corresponding to h  X  X  X  , the local model enjoys computational efficiency as it ignores a large portion of the training data. A central issue in local likelihood modeling is select-ing the appropriate bandwidth h . A practical solu-tion is to use cross validation or some other automatic bandwidth selection mechanism. On RCV1 data, the performance of such cross validation schemes is very good and the estimated bandwidth possesses test set loglikelihood that is almost identical to the optimal bandwidth (see Figure 4, left).
 Allowing the kernel scale to vary over time results in a higher modeling accuracy than using fixed bandwidth for all dates (see Figure 4, right). A time-dependent cross validation procedure may be used to approx-imate the time-dependent optimal bandwidth which performs slightly better than the fixed-date cross val-idation estimator. Note that the accuracy with which the cross validation estimator approximates the opti-mal bandwidth is lower in the time-dependent or vary-ing bandwidth situation due the fact that much less data is available in each of the daily cross validation problems.
 From a theoretical perspective, the asymptotic bias and variance can be used to characterize the optimal bandwidth and study its properties. Minimizing the (offline) leading term of sum of component-wise MSE with respect to h we obtain the bandwidth estimator  X  h t = (13) 4 n X  2 21 ( K ) P j [  X   X  t ] j g  X  ( t ) / p g ( t ) + p As expected, the optimal bandwidth decreases as since in these cases the variance decreases and bias either increases or stays constant. In practice,  X   X  t , may vary significantly with time which leads to the conclusion that a single bandwidth selection for all t may not perform adequately. These changes are illus-trated in Figure 5 (left) which demonstrates the tem-poral change in the gradient norm.
 Perhaps more interesting than the dependency of the optimal bandwidth on n,  X ,  X   X  t ,  X   X  t is its dependency on the time sampling distribution g ( t ). Equation (13) reveals an un-expected non-monotonic dependency of the optimal bandwidth in g ( t ). The dependency, ex-pressed by  X  h t  X  ( P V j =1 ( c 1 j / p g ( t )+ c 2 j p illustrated in Figure 6 (left) where we assume for sim-plicity that c 1 j , c 2 j do not change with j resulting in (  X  h standing this relationship is the increased asymptotic bias due to the presence of the term g  X  ( t ) /g ( t ) in Equa-tion (11). Intuitively, the variations in g ( t ) expressed by g  X  ( t ) introduce a bias component which alters the otherwise monotonic role of the optimal bandwidth and bias-variance tradeoff. Since g ( t ) is highly non-uniform (as illustrated in Figure 2), this dependency of  X  h t on g ( t ) is likely to play a significant role. We finally point out that different words w have dif-ferent parameters [  X  t ] w and parameter derivatives [  X  which indicates that it is unlikely that a single band-width will work best for all words. Frequent words are likely to benefit more from narrow kernel smoothing than rare words which almost never appear. As a re-sult, a lower bandwidth should be used for frequent words while a high bandwidth should be used for rare words. A systematic investigation of these topics is beyond the scope of this paper. 4.2. Local Likelihood for Logistic Regression Often, the primary goal behind modeling the drift is conditional modeling i.e. predicting the value of y given x . In this case, drift modeling should focus on estimating the conditional p t ( y | x ) since modeling the marginal p t ( x ) becomes irrelevant. In contrast to the modeling of the conditional by Bayes rule p t ( y | x )  X  p ( x | y ) p t ( y ) described in the previous section, we ex-plore here direct modeling of { p t ( y | x ) : t  X  I } using local likelihood for logistic regression.
 By direct analogy to Equation (4) the conditional local likelihood estimator p t ( y | x ) is the maximizer of the locally weighted conditional loglikelihood  X  (  X  | D ) = As in the generative case, the kernel parameter h bal-ances the degree of the kernel X  X  locality and controls the bias-variance tradeoff.
 Denoting by f ( x ) the vector of relative frequencies in the document x , the logistic regression model lowing local conditional likelihood  X  (  X  | D ) =  X  In contrast to the naive Bayes model in the previous section, the local likelihood does not have a close form maximizer. However, it can be shown that under mild conditions it is a concave problem exhibiting a single global maximum (for each t ) (Loader, 1999). Most of the standard iterative algorithms for training logis-tic regression can be modified to account for the local weighting introduced by the smoothing kernel. More-over, recently popularized regularization techniques such as the penalty c k  X  k q , q = 1 , 2 may be added to the local likelihood to obtain a local regularized ver-sion equivalent to maximum posterior estimation. Figure 5 (right) displays classification error rate over a held-out test set for local logistic regression as a func-tion of the train set size. The classification task was predicting the most popular class vs the second most popular class in RCV1. The plots in the figure con-trast the performance of the online and offline tricube kernels with optimal and infinite bandwidths, using L 2 regularization. The local model achieved a relative re-duction of error rate over the global model by about 8%. As expected, the online kernel generally achieve worse error rates than the offline kernels. In all the experiments mentioned above we averaged over mul-tiple random samplings of the training set to remove sampling noise. A large number of textual datasets such as emails, webpages, news stories, etc. contain time stamped documents. For such datasets, considering a drifting rather than a stationary distribution is often appropri-ate. The local likelihood framework provides a natu-ral extension for many standard likelihood models to the concept drift scenario. As the drift becomes more noticeable and the data size increases the potential benefits of local likelihood methods over their extreme global or local counterparts increase.
 In this paper we illustrate the drift phenomenon and examine the properties of the local likelihood estima-tor including the asymptotic bias and variance tradeoff and optimal bandwidth. Experiments conducted on the RCV1 dataset demonstrate the validity of the lo-cal likelihood estimators in practice and contrast them with more standard non-local alternatives.
