 1. Introduction
In recent years, there has been a growing use of data mining and machine learning outside the computer science com-munity. Such methods are being integrated into decision support systems for fields such as finance, marketing, insurance, and medicine. Another partial motivation for our work is the need to support medical decisions, more precisely breast cancer recurrence prediction. We will use this example as a short introduction to data mining, modeling, and, finally, the explana-tion of models, which is the main topic of this paper.

A typical decision support scenario usually starts with a domain and a problem. In our medical example, the problem is whether or not the patient X  X  cancer will recur. In general, the true concepts behind the domain are unknown, but oncol-ogists have a certain degree of expert knowledge. They have also been gathering patients X  data at the time of surgery and recording recurrence through patient follow up, all in an effort to gain some additional insight into the concepts that drive breast cancer recurrence. Each patient is therefore an instance from our domain and is described by several features (for example age, tumor size, etc.) and a class value or class label. In our case we have only two possible class values: recur-rence, no recurrence. The set of all recorded instances is often referred to as the data set , which is where data mining and machine learning begin. The first phase is typically data preprocessing, where we address missing values, remove useless instances, possibly discretize continuous features, and address other similar issues. Once the data set has been prepro-cessed, it can be used to train, test, and choose the best performing prediction model. This can be combined with feature selection [8,17] to reduce the features to those relevant for the problem. Given that our problem is a classification prob-lem, we can select from a variety of different classifiers, some of which will be more suitable and some less suitable for the problem. From these models, we pick the one that gives the best results and is therefore the most suitable model for our task. This model can be given to oncologists, who can use it as a black-box that can predict the probability of breast cancer recurrence for new patients. However, the task does not end here, because physicians are reluctant to use or even refuse to use these models, despite the fact that models outperform physicians in several medical prediction tasks [13].
Their reluctance to make a decision based only on a single probabilistic output is understandable, given their huge respon-sibility and the importance of the decision. Similarly, users from other fields may also be reluctant or may require addi-tional insight into the model X  X  decision. That is why there is a growing need to enhance the predictions with additional information  X  an explanation . 1.1. Existing explanation methods
Some models already have a transparent decision-making process from which it is easy to extract an explanation. For that contains the instance. Bayesian networks are also an example of a highly interpretable model and their interpretability can be further improved [7]. For most other, less transparent models, model-dependent approaches have been developed.
For example, Breiman provided additional tools for explaining the decisions of his Random forests algorithm [4], and a lot of work has been done on explaining the decisions of artificial neural networks [24], which are one of the least transparent models. The ExplainD [22] framework provides explanations for additive classifiers. Nomograms [18] are also used for visu-alization and explanation of decisions and were applied to naive Bayes [19] and, in a limited way, to SVM (Support Vector
Machines) [11]. The interpretability of some models can also be improved by systematically reducing the number of training instances (see, for example [6]). Considering that for almost any given model, we already have a model-dependent explana-tion method, why do we even need a model-independent explanation method? The main reason is consistency of explanation .
The problem of model-dependent methods is that once the model is replaced with a model of a different type, the explana-tional time and effort to get used to a new explanation method. Also, it is quite common to have several different models performing different tasks in the same decision support system. With a model-independent explanation method, we can ex-plain their decisions to the user in a uniform way.
 As far as we know, there are two existing model-independent methods for explaining a model X  X  decision for an instance.
One is proposed by Robnik- X ikonja and Kononenko [20] (we will refer to this method as EXPLAIN ) and the other by Lemaire et al. [16]. While there are several differences between these two methods, they both have the same approach. Both methods compute the influence of a feature value by changing that value and observing the changes in the model X  X  output. Bigger changes in the output imply that the feature value was more important for the prediction, and vice versa. However, because both methods observe only a single feature at a time, they have a huge flaw, which can be illustrated by simple disjunction.
Let A 1 and A 2 be binary features from the domain with a binary class and the following concept: class  X  A explain the instance A 1  X  1 ; A 2  X  1. Obviously the decision is 1 _ 1 equals 1. By changing A equals 1. Because the prediction did not change, both methods would conclude that A
A will be marked as irrelevant as well. This flaw is not reserved just for disjunction but is present whenever there is redun-dancy among the features or their values. As a result, the explanations generated by existing methods are often inaccurate and misleading, which, in turn, makes them more difficult to trust. 1.2. A new instance explanation method
In this paper, we propose a new model-independent explanation method IME (Interactions-based Method for Expla-nation) that is designed to deal with the flaws of existing model-independent methods. As IME is a method for explain-ing classifier decisions for an instance, we assume that we have an arbitrary classifier trained on a data set, a new instance from the given domain, and a class value. The classifier makes a prediction, in other words an assessment of the class value X  X  probability, knowing the feature values of the instance. There is usually a difference between this assessment and the prior probability of the class value (i.e., if no feature values are known). The question is: Which fea-tures influenced the classifier X  X  decision and caused this difference in prediction? IME provides the answer by assigning to each feature value a real number  X  a contribution . The sum of these contributions equals the difference in prediction, and the size of each contribution is proportional to the feature value X  X  influence on the decision. A positive contribution indicates that the value contributes to the class value, and a negative contribution indicates that the value speaks against the class value.

It is obvious from the disjunction example that we may overlook an important part of the model X  X  decision-making pro-cess, unless we look at every possible combination of feature values. Therefore, to avoid missing something, we observe how the prediction changes for each subset of the power set of feature values. These changes are then combined to form a con-tribution for each feature value. Dealing with the whole power set results in an exponential time complexity, but, there are several reasons why the method is already a valuable contribution. IME is able to deal with various concepts (even where existing model-independent methods would fail) and generate useful and intuitive explanations for a variety of different and a precise explanation is needed. Because the generated contributions are implicitly normalized, it makes it easy to com-pare them across instances and even across different models. A real-life breast cancer recurrence prediction problem is used as an example to illustrate the method being successfully applied in practice. Finally, we discuss how the exponential time complexity of IME can be overcome.

The paper is divided into six sections. In Section 2, we formally define the IME method. In Section 3, we introduce several usability of the method X  X  explanations. In Section 5, we discuss the method X  X  high time complexity and point out possible solutions. With Section 6, we conclude the paper and offer some ideas for further work. 2. The definition of the method finite set of class values for this domain, C . Let X  X  R j h : b
X ! X  0 ; 1 m maps an instance x Q 2 X Q to a set of m probabilities, one for each class value ( m  X j C j ). Given that we are interested in explaining only a single class value at a time, we can write h : ways of approximating these marginal predictions will be discussed later on.
 ference that we are trying to explain:
Note that D -terms are instance-dependent but we omit it in the notation to simplify the presentation. We can generalize (1) to an arbitrary instance subspace:
The difference D Q is not only a result of the influence of individual feature values but also a result of how feature values interact. In fact, observing a set Q of feature values together might contribute something to the decision that could not be seen when observing any true subset of Q . We refer to such contributions as interaction contributions . Let D of all such interaction contributions:
With (3), we decomposed the difference in prediction into 2 we derive our definition of an interaction contribution I bution of an empty-set interaction is zero, which results in the following recursive definition:
The 2 n interaction contributions that we get for our instance x already provide an explanation. However, this explanation each feature value. This is done by dividing each interaction contribution into as many parts as there are feature values in-tribution of the i th feature X  X  value:
One might argue that less probable values play a more important role in an interaction and thus deserve a larger share of the interaction, so we may obtain more informative results by dividing the interaction based on the feature value X  X  prior probabilities. However, by using additional information such as feature value probabilities, we bypass the model that we are explaining and compromise the explanation by forcing our external view onto it. Without using any prior knowledge, dividing into parts of equal size is the only symmetrical way of completely assigning an interaction contribution to the contributions of involved feature values. The other argument against this division is that it assumes that feature values are either independent or equiprobable. However, as we will show in three illustrative examples, the interpretation of dependencies is left to the model that we are explaining, which is arguably what an explanation method should do. In other words, if the model has successfully learned a dependency among feature values, then this will also be reflected in the explanation. 2.1. Illustrative examples
Our first illustrative example will be a domain with the disjunction concept. Let A
C  X f 0 ; 1 g a binary class, and X  X f 0 ; 1 gf 0 ; 1 gf 0 ; 1 g our instance space. The concept is C  X  A h : b
X ! X  0 ; 1 be an (ideal) Bayesian classifier which was trained on a data set where all feature value combinations are equi-value 1. In other words, how did the three feature values contribute to the model X  X  prediction P  X  C  X  1 j a 0  X  X  1? as we take into account at least one of the first two feature values, the predicted probability will be 1. In other words, h  X  a 0
W  X  X  1 whenever at least one of the first two features is included in W . Therefore, according to (2), the following D -terms dicted probability is now equal to the probability of at least one of the remaining values being one. In other words, if we observe only the third column and only those rows where the value of the third feature is 0, we narrow the data set down to four instances, three of which have class value 1. Therefore, h  X  a 0
We can now use (4) to calculate the interaction contributions. Single-feature interaction contributions are equal to the corresponding D -terms:
Finally, the contributions of feature values are:
The sum of all contributions equals the total difference between the predicted probability and the prior class probability  X  p 2  X  p 3  X  3 24  X  1 8  X  1 prior probability). Therefore, for instance a 0 both 1 X  X  speak favorably towards the class being instance. As we can see, IME does not have difficulties dealing with disjunctive concepts.

Using the same domain, but explaining the instance a 00  X f 0 ; 0 ; 0 g , we get the following contributions: f 7 to now we have assumed that the feature values are both independent and equiprobable, but how would the contributions change if they were not? Let the probabilities of A 1  X  1, A 1 is assigned the largest negative contribution for being 0. The larger the probability of the value being 0, the smaller the negative contribution when the value is actually 0.

Finally, what if feature values are not even independent? Let A dency between two features ( A 1  X  1 ) A 2  X  1). For example, on a 000  X f 1 ; 1 ; 1 g the explanation method generates the contributions f 22 72 ; 4 72 ; 28 72 g . The feature value A contribution to reflect that. The remaining two feature values contribute roughly the same amount, although A slightly larger contribution due to the fact that the other two feature values bring nothing new when observed together. 2.2. Approximating marginal predictions
Not all models have the ability to make marginal predictions (i.e., predict instances with omitted feature values), so we have to approximate these predictions. The approach used in this paper is similar to the wrapper approach [12] used in fea-ture-selection. We approximate a model X  X  marginal prediction h  X  x set and retraining the model. This results in a new model h instance x Q has no missing features, so we can use it to approximate the marginal prediction: h  X  x for each of the 2 n subsets of features.

Two other approaches for marginal prediction approximation are worthy of mention, but are not used in our method. The first approach involves assigning special unknown values or NA  X  X  to feature values that we want to omit from the instance.
This approach is limited to models that support the use of unknown values, which is the reason we can not use this approach in our generalized explanation method. The second approach approximates the omission of a feature value with an average expected prediction across all perturbations of the feature value, where each perturbation is weighted by its probability.
Both Robnik- X ikonja and Kononenko [20] and Lemaire et al. [16] use variations of this approach in their model-independent explanation methods. Although it was used only for approximating the omission of a single feature, it can be generalized to an arbitrary number of features, with an exponentially growing number of perturbations. As the number of omitted feature values increases, so does the approximation error. The advantage of this approach is that the model does not have to be re-trained. However, continuous features have to be discretized because we can only try a finite number of feature value com-3. Does the explanation reflect the model?
While the task of a model is to describe the domain, the explanation method X  X  task is to describe the model X  X  interpretation of the domain. Ideally, the better the model is at recognizing the domain concepts, the higher the quality of the explanation should be, and vice versa. In other words, the explanation of a poor-performing model may not reflect the concepts behind the domain, and the explanations of a well-performing model should reflect at least some concepts behind the domain.
The quality of the model X  X  interpretation can be described by its prediction quality and the closeness of the explanation to the class value, p , and the actual outcome p a : b  X  X  p p measures the classifier X  X  ability to make precise probabilistic assessments. Our method uses the classifiers X  predicted prob-abilities, which makes the Brier score more appropriate than other well-known prediction quality measures such as predic-tion accuracy (measures the classifier X  X  ability to correctly classify an instance) or Area Under the ROC Curve (measures the classifier X  X  ability to distinguish between instances with different class values).

We define explanation quality by first defining the optimal explanation of an instance. The optimal explanation of an in-are therefore optimal explanations for those instances and those data sets. This is based on the assumption that an expla-nation method best describes the concepts behind the domain when explaining a model that optimally learns the concepts.
Note that the optimal explanation is method-dependent, so different explanation methods would produce different optimal explanations for the same instance. We measure the explanation quality of an explanation by measuring how much it differs from the optimal explanation. For this purpose we use a Euclidean distance, d , between the generated contributions and the contributions from the optimal explanation.

We will use several artificial data sets and different types of classifiers to empirically show that IME explanation quality correlates with the model X  X  prediction quality . Note that the use of artificial data sets, where all concepts are known, is ability to handle extreme examples of concepts that are uncommon in real-world data sets.
 3.1. Models and artificial data sets
We tested the method using the following models: naive Bayes ( NB ), decision tree ( DT ), k-nearest neighbors ( kNN ), sup-port vector machine with a polynomial kernel ( SVM ) and a multilayered feed-forward artificial neural network ( ANN ). See, for example, [14] for detailed descriptions of these learning algorithms.

The first five artificial data sets described in this section were introduced by Robnik- X ikonja and Kononenko [20] to test disjunctive concepts and completely random features, respectively. Each data set consists of 2000 examples, half of which are used for training the model and half for testing it. All features are either binary or continuous with values from the  X  0 ; 1 interval, and, unless otherwise noted, 0 and 1 are the only two possible class values. On all data sets we assume we are explaining class value 1. We now briefly describe the artificial data sets: condInd: This data set has eight binary features. The class value is also binary and both classes are equally probable. The which we omit due to space limitations. Because the features are conditionally independent given the class, NB is considered to be the most suitable for this data set. of the remaining three important features. Noise was added to the class by reverting it in 10% of the cases so for each in-for this data set as it can split the data on each feature and each leaf can then correspond to a different rule. group: This data set has four continuous features. The three class values are scattered around group centers that are placed so that both important feature values have to be known to gain any knowledge about the class value. The remaining when the class is 1 and 1 6 when it is either 0 or 2. The kNN model is considered most suitable for this data set because in-stances with the same class value are clustered together. cross: This data set has four continuous features, two of which are unrelated to the class. The class value is 1 when  X  I and 1 4 when it is 0. SVM is considered best when dealing with such a data set because instances with different class values can be linearly separated in the SVM  X  X  transformed feature space. chess: This data set has four continuous features and represents a 4 4 chessboard where instances have either class va-is considered most suitable for this data set, because the concept behind the data set is more complex and the remaining classifiers will have more difficulty due to the assumptions they make. Only kNN should have some success in predicting the class values of instances close to the center of each field, but not close to the edge. dius of 0.5. The three important features serve as three-dimensional coordinates, and the class value is 1 if and only if the instance lies within the sphere. The optimal explanation of an instance from this data set is more complex since it is a con-terms for this data set. of the three important features is 1. The true explanation of an instance is a bit more complicated for this data set and de-random: This data set has four continuous features, all of which are unrelated to the class. The optimal explanation of an instance would therefore assign 0 to every feature value. 3.2. Results
The complete test results are shown in Table 3 , and the models that were most suitable for each of the first five data sets do in fact produce the best predictions (underlined). The best explanations are also generated on these models (underlined), with the exception of the groups domain, where ANN produces a slightly better explanation despite being slightly out-pre-formed prediction-wise by the kNN model.

In Fig. 1 , we plot the Brier score and d pairs from Table 3 for four data sets. Note that d is the mean explanation quality almost linear connection between prediction and explanation quality. This implies that with increasing prediction quality the explanations near the optimal explanations. In practice this means that the explanation of a high-quality model would not only reflect how the model works, but also the concepts behind domain itself. Note that an anomaly appears in the dis-junct data set results, where models with a perfect Brier score generate sub-optimal explanations. This is possible, because we do not evaluate the Brier score of the sub-models that we build to obtain marginal predictions, yet we use their outputs.
To remedy this, we could combine the Brier scores of all the sub-models built and get a more precise indicator of the model X  X  quality. 4. Do the explanations make sense?
In the previous section, we have shown that the explanations reflect the model. However, the explanations would be of no use if they were not also intuitive and informative. We will use several examples to emphasize the usefulness of the expla-the name of the data set, the name of the model (the decision of an artificial neural network is being explained), the pre-dicted class probability for this instance, and the actual class value. On the left-hand side of the visualization, we have the names of the features, and on the right-hand side, we have the feature values for this instance. The bars represent the contributions of individual feature values. The concept behind the Monk1 data set is that the class value equals one if the probability of 1. The explanation correctly reflects that features A
A  X  1 is the most important contributor to the classifier X  X  decision. The remaining feature values are assigned insignificant contributions, which reflects the fact that they are irrelevant for the decision. Note that the contributions of A entirely equal due to the fact that their values are not represented in the training data set with the same frequency and al-Table 1 then the generated contributions would be equal as well.

In Fig. 2 b we have another explanation for the same instance, but, this time, we are explaining the classification of the naive Bayes classifier. Similar to the artificial neural network, the naive Bayes correctly classifies the instance. However, incorrect, it actually correctly reflects how the naive Bayes classifier works. The naive Bayes classifier is based on the assumption that the features are conditionally independent given the class value; this assumption leaves it unable to learn properties of models and enable comparison among different types of models. attributes
Fig. 3 a is an explanation of an instance from the sphere data set, using the SVM classifier. The contributions can be either positive (reaching right) or negative (reaching left), depending on whether the feature value speaks for or against the class value. For example, the value 0.031 of the important feature I and has a negative contribution. The value 0.939 of the random feature R sphere, any two of the values would provide sufficient knowledge to classify the instance outside of the sphere (class value method, which incorrectly assigns a zero contribution to each feature value.

Finally, we may combine the contributions of individual feature values across several instances to get an overview of how of individual feature values across all test instances. The darker bars represent the mean positive and mean negative contri-4.1. Enhanced explanations
Up to this point, our visualizations contained only feature value contributions, which tell us how much each feature value feature value X  X  contribution but also in possible dependencies and interactions between feature values. In such cases, the user can use an enhanced instance explanation.
 In Fig. 5 a, we have an enhanced instance explanation for the Monk1 instance that we discussed in the previous section. contribution on its own (or I f A i g ) without any interactions with other feature values. Immediately, we notice that A itself would result in approximately the same certainty of class value 1 and that it has a smaller contribution when observed with other feature values. On the other hand, A 1  X  1 and A served on its own. Therefore, the vertical lines inform the user that there must be some interaction between these three fea-contributions would suggest. Now, we can easily come to the conclusion that A attributes
Our second example on Fig. 5 b is an instance from the xor data set. We are explaining the decision of a decision tree, which performs very well on this data set and correctly classifies this instance. The explanation also correctly assigns an equal negative contribution to each important feature value. Because the concept behind this data set is the parity problem, feature value has a significant contribution on its own. Only when the values of the three important features are observed the three important features is equal to the difference between the prior probability and the predicted probability. Only a between I 1  X  1, I 2  X  0, I 3  X  1, is the sole important contributor to the model X  X  decision. 4.2. Applying the method to a real-life oncology data set
Breast cancer recurrence prediction is an important aspect of oncology. It helps to identify patients with the most critical prognoses and reduces the number of unnecessary therapies. Oncologists from the Institute of Oncology, Ljubljana, Slovenia have provided us with a breast cancer data set with 949 instances, which serves as an excellent test and an example of the usefulness of our explanation method. Each instance is an individual breast cancer patient and is described with 13 features recorded at the time of breast cancer surgery. The class is either 1 (recurrence within 5 years) or 2 (no recurrence within 5 years). A more detailed description of the features can be found in Table 4 .

In Fig. 6 , we can see the model explanation for the oncological data set using the IME method, and in Fig. 7 we can see the model explanation for the same data set generated using the Robnik- X ikonja and Kononenko method [20]. Both were gen-erated for the Random forests algorithm [4], which was the best performing model on the data set. The model achieved an accuracy of 0 : 73 across 100 test instances, which was slightly better than the oncologists X  accuracy on the same test in-stances. Note that both oncologists and the Random forests model are significantly better predictors than random predic-tions and the default predictor (i.e., always predicting the majority class value). The Random forests generated model uses more features, even features of minor significance, to produce a prediction. Subsequently, there are many small inter-actions between feature values, and the EXPLAIN method produces a model explanation that is difficult to interpret. On the other hand, IME computes all the interactions and divides them among the feature values that are part of the interactions. expressed. Indeed, oncologists have confirmed that the model explanations reflect their expert medical knowledge regarding the importance of individual feature values in breast cancer recurrence prediction. An oncologist can use the model expla-nation to get an overview of how the model functions. For example, when observing Fig. 6 the expert would first look at the dark bars which reveal the overall influence of the feature on the model X  X  decision. Features LVI, nLymph, and posRatio on to individual feature values which reveal that LVI = 1 has a distinctly positive contribution (speaks in favor of a recur-rence), low values of nLymph have a negative contribution, while high values of nLymph have a distinctly positive contri-bution, and so on. Because these conclusions are in agreement with current medical knowledge the expert oncologist can conclude that the model has learned knowledge that is relevant for breast cancer recurrence prediction.
To evaluate IME instance explanations, we generated 20 instance explanations and gave them to an expert oncologist for evaluation. The 20 instances were chosen from 100 test instances which were not included in the training set. To ensure diversity, the instances were selected in a semi-random way so that half of the instances had no recurrence and half had come for a patient is not known in advance so it is important to include misclassifications because it enables us to observe with the explanation and would make the same incorrect prediction for that instance.

For each instance, we used the following evaluation procedure: For each feature value, the oncologist had to either agree day medical practice. Therefore, the contributions of 4 features (histType, famHis, maxNode and posRatio) could not be eval-uated, and our evaluation produced a total of 180 agreements/disagreements. The results are shown in Table 5 , and we can see that there were a total of 21 disagreements, i.e., 12 % of all contributions.

Eleven of the disagreements are on the features age and menop , which is also an age-related feature. Oncologists have ogists explained, a higher than usual number of young patients in our data were treated with therapies, because youth was, and still is, considered a negative factor. Consequently, young age sometimes moves the model X  X  decision towards non-recurrence, because it implies a higher probability of therapy, which, in turn, reduces the chance of recurrence. This leaves us with 10 actual disagreements, which results in an encouraging 94 % agreement rate with the contributions generated by our explanation.

Finally, in Fig. 8 we have two explanations for two different patients. We can use these explanations to examine how an expert oncologist or even a non-expert user can interpret the model X  X  decision. On the left-hand side is an instance expla-nation for a breast cancer recurrence prediction using the Random forests model. The large number of positive lymph nodes (nLymph) and the large ratio between positive lymph nodes and total lymph nodes (posRatio) both speak heavily towards recurrence for this patient. The remaining feature values speak against recurrence but do not outweigh the two most tumor) is the only value that notably speaks towards recurrence. However, three values have a considerable contribution against recurrence, so the model predicts a low probability of recurrence (0.06). These three values are: small tumor size (stage = 0), absence of positive lymph nodes (nLymph = 0), and subsequently lowest possible ratio of positive lymph nodes (posRatio = 0). 5. Discussion of time complexity
Admittedly, the exponential time complexity is the biggest obstacle barring wider applicability of IME . However, we have shown that the method can already be successfully applied to real-world problems, as is. Furthermore, the explanation method can be combined with feature selection methods, thus reducing the number of features and enabling us to use IME on more data sets. While both feature selection and explanation methods investigate the relevance of features, there are sev-eral important differences between the two. First, the main goal of feature selection is to select a feature subset that max-selection and explanation are applied separately, at different stages of modeling. Second, feature selection deals with fea-tures at the model level, while IME deals with feature values on the instance level. And third, feature selection optimizes hand, the goal of explanation methods is to reveal as many details about the influence and interactions of features as pos-sible, which requires a higher time complexity. A brief analysis of the UCI machine learning repository [2] and existing fea-repository is 174), most data sets either already have a small number of features (15 or less) or can be reduced to a small number of relevant features without compromising the prediction quality. This implies that for most data sets the optimal Table 6 shows the running times on artificial data sets used in this paper. The explanations for 432 test instances in the
Monk1 data set were generated in under one second for each model. The explanations for 100 test cases from the real life oncology data set were generated in 35 s for the NB model and 241 s for the Random forests model. Note that a desktop com-puter (2.4 GHz CPU, 2 GB RAM, running Windows XP) was used for generating the explanations. The application code was a straightforward, non-optimized implementation of the method in Java (v1.6.0), and is available via email request. All the measurements are mean running times across 10 runs.

The purpose of the work described in this paper was to develop an explanation method that can handle all the possible concepts, regardless of the data set or classification model used. This is what distinguishes IME from existing model-inde-pendent methods, and this will provide a solid foundation with which to compare approximation methods. The tradeoff is a high time complexity. While the development of an approximation method is beyond the scope of this paper, we want to provide some ideas to illustrate that developing such a method is indeed possible. Note that the contribution of the i th feature value (5) can be expressed using only D -terms (see Appendix for a detailed proof). With this equation, we can avoid computing individual interactions, if we are only interested in the final contribution:
While these changes might not be normally distributed, they do have a finite variance and, following the central limit the-orem, the sum of a sample of these changes would be normally distributed. This would justify using a sampling method to approximate a feature value X  X  contribution. Approximating an individual interaction contribution is more challenging be-action contributions from the bottom up, only up to a certain size, would have a polynomial time complexity. This approach to comprehend. 6. Conclusion
In this paper, we have proposed a new explanation method, IME , for explaining classifier decisions. The method ex-plains a model X  X  decision for an instance, and it can be used for any classifier. The instance explanation is provided in the form of feature value contributions, which describe how individual feature values contribute to the decision. Addition-ally, interaction contributions are provided, which describe how interactions between subsets of features contribute to the model X  X  decision. Both feature value contributions and interaction contributions are expressed as a difference in probabil-ity, and their sum is implicitly normalized. This makes it easier to interpret the explanations and to compare how different models classify the same instance. Results on several artificial data sets and classification models show that IME explana-tions closely follow the quality of the model. An analysis of the explanations generated by IME showed that the contribu-tions reveal the influence of the feature values and that the explanations provide insight into how the model learns from the data set. The application of IME to an oncological data set has shown not only that the method can already be applied in practice but also that expert oncologists agree with a vast majority of the generated explanations. We conclude that IME is a significant improvement over existing model-independent explanation methods and is especially useful when detailed explanations are needed.

The most important part of future work is developing an efficient approximation method. There are also some minor is-sues that need to be resolved, such as improving the visualization and providing a more detailed model explanation to the user. The use of IME for feature selection is also a possibility worth exploring. While the method X  X  time complexity makes other feature selection methods more appropriate for optimizing prediction quality, the explanations generated by IME work on this topic, we also want to explore the possibility of extending the method to regression models. Appendix A. Proof of the contribution expression
The recursive definition of an interaction contribution (4) can be transformed into a non-recursive form. The non-recur-the M X bius inversion [21] of the set of all subsets, partially ordered by inclusion):
This equation is similar to multivariate mutual information in information theory, the difference being that marginal pre-to our work. Several approaches to multivariate mutual information have been taken by Bell [3], Han [9], Yeung [25] and
Jakulin [10]. The last of these also includes further references to interaction-related publications and is most suitable for those who want to become familiar with the subject of interactions from an information-theoretic view.

The non-recursive definition of an interaction contribution can be used to express the contribution of an arbitrary ele-ment of the set. We will start by using the definition of a feature value X  X  contribution (5) and writing the contribution of a single feature from the set of n features. We can safely assume that the feature in question is labeled with 1: Let us examine the number of appearances of D Q on the right-hand side of (8) after we expand each interaction using (4). N , where 1 2 Q . The term D Q appears only in interactions I smallest such set W is, of course, Q , where D Q appears with a positive sign. In interactions I with a negative sign and there are exactly k 1 such interactions in the sum in (8), because we can choose the additional ele-ment from the remaining k elements that are not already in the set j Q j . If we write all such terms up to I and take into account that each interaction I W is divided by j W j , we get the series: N D Q is similar for 1 R Q . The only difference is, that the smallest such set W where D odd, and the first term in the series starts with a negative sign: function B  X  p ; q  X  : Using the known relation between beta and gamma function B  X  p ; q  X  X  C  X  a  X  X  X  a 1  X  ! , we can further derive our series:
Let S  X f 1 ; 2 ; ... ; n g . Now we are ready to derive the desired contribution: Appendix B. The D -terms of the true explanation of the sphere data set
Let an instance A 1  X  x 1 , A 2  X  x 2 , A 3  X  x 3 , A 4  X  x coordinate system and scale the sphere radius to 1. Now the true contribution of an feature value depends only on it X  X  abso-lute value and we can focus on a single quadrant. Each instance X  X  X  x
By dividing an eight of the sphere X  X  volume by the volume of the unit cube, we derive that the prior probability of class if one coordinate is known) minus the prior probability:
When two coordinates are known, the remaining coordinate defines a line. The length of the intersection of that line and the sphere quadrant defines the two-feature D -terms. Two extreme coordinates are enough to put the entire instance out of the sphere and in that case the D -term is automatically 0 minus the prior probability: When all three coordinates are known, we know exactly where the instance lies and whether it is inside the sphere:
References
