 Graphs are widely used to model complicated structures in many scientific and commer-cial applications. Frequent graphs, those o ccurring frequently in a collection of graphs, are especially useful in characterizing gra ph sets, detecting network motifs [2], dis-criminating different groups of graphs [3], classifying and clustering graphs [4,5,6], and building graph indices [7]. For example, Huan et al. [5] successfully applied the frequent graph mining technique to extract coherent structures and used them to iden-tify the family to which a protein belongs. Yan et al. [7] chose discriminative patterns from frequent graphs and applied them as indexing features to achieve fast graph search.
Unfortunately, general-purpose graph mining algorithms cannot fully meet users X  demands for mining patterns with their own constraints. For example, in computational biology, a highly connected subgraph could represent a set of genes within the same functional module [8]. In chem-informatics, scientists are often interested in frequent graphs that contain a specific functional fragment, e.g. , a benzine ring. In all these applications, it is critical for users to have control on certain properties of the mining results for them to be meaningful. However, previous studies have left open the problem of pushing sophisticated structural constraints to expedite the mining process. This gap between user demand and the capability of current known mining strategies calls for a constraint-based mining framework that in corporates these structural constraints. Related Work. A number of efficient algorithms for frequent graph mining are available in data mining community, e.g. , AGM [14], FSG [15], the path-join algo-rithm [16], gSpan [17], MoFa [3], FFSM [18], SPIN[19] and Gaston [20]. Few of them considered the necessary changes of the mining framework if structural constraints are present. Constraint-based frequent pattern mining has been studied in the context of association rule mining by Ng et al. [9], which identifies three important classes of constraints: monotonicity, antimonotonicity and succinctness and develops efficient constraint-based frequent itemset mining algorithms for a single constraint. Ng et al. also pointed out the importance of exploratory mining of constrained association rules so that a user can be involved in the mining pro cess. Other complicated constraints, such as gradients [26], block constraints [27], constraints on sequences [28], and connectiv-ity constraints [23], are proposed for different applications. Pei et al. discovers another class of constraint convertible constraints and its pushing methods. Constrained pattern mining for graphs has been looked into by Wang et al. , [29], although only constraints with monotonicity/antimonotonicity/succinctness are discussed. Bucila et al. [10] intro-duced a DualMiner framework that simultaneously takes advantage of both monotonic-ity and antimonotonicity to increase mining efficiency. The general framework of these mining methods is to push constraints deep in order to prune pattern search space. Al-though this is effective in many cases, the greatest power of constraint-based frequent pattern mining is achieved only when considering together the reduction on both the pattern search space and the data search space. Bonchi et al. have taken successful steps in this direction by proposing ExAnte, [11,12,13], a pre-processor to achieve data re-duction in constrained itemset mining. ExAnte overcame the difficulty of combining the pruning power of both anti-monotone and monotone constraints, the latter of which had been considered hard to exploit without compromising the anti-monotone constraints. Boulicaut and De Raedt [1] have explored constraint-based mining as a step towards inductive databases.
 Our Contributions. In this paper we show that the data reduction technique can in fact be extended beyond the preprocessing stage as in ExAnte, and pushed deeper into the mining algorithm such that the data search space is shrunk recursively each time it is projected for a pattern newly grown, through the entire mining process. More impor-tantly, our study of graph constraints shows that for sophisticated constraints in data sets whose structures are more complicated than itemsets, data space pruning could be effective only when the structural relationship between the embedded pattern and the data is taken into account. This new constraint property, which we term as Pattern-inseparable D-antimonotonicity , is unique in the context of graphs and, to our best knowledge, has not been explored before in lite rature. It distinguishes itself from other pruning properties in that most sophisticated structural constraints, e.g. , diameter, den-sity, and connectivity, exhibit neither antimonotonicity nor monotonicity. Without ex-ploiting Pattern-inseparab le D-antimonotonicity, current mining algorithms would have to enumerate all frequent graphs in the first place and then check constraints on them one by one. The paper makes the following contributions: First, we present the first sys-tematic study of the pruning properties for complicated structural constraints in graph mining which achieves pruning on both pa ttern and data spaces. The full spectrum of pruning power is covered by (1) extending the known antimonotonicities for itemsets to easier cases and (2) discovering novel patte rn-separable and pa ttern-inseparable D-antimonotonicities to handle structural cons traints when pattern embeddings have to be considered. Secondly, a general mining framework is proposed that incorporates these pruning properties in graph pattern mining. In particular, data space pruning is cou-pled tightly with other constraint-based pruning such that data reduction is exploited throughout the entire mining process. Thirdly, discussion is given on mining strategy selection when a trade-off has to be made be tween the naive enumerating-and-checking approach and our pruning-property-driven approach. As a convention, the vertex set of a graph P is denoted by V ( P ) and the edge set by E ( P ) . For two graphs P and P , P is a subgraph of P if there exists a subgraph isomorphism from P to P , denoted by P  X  P . P is called a supergraph of P .In graph mining, a pattern is itself a graph, and will also be denoted as P . Given a set of graphs D = { G 1 ,G 2 ,...,G n } , for any pattern P ,the support database of P is denoted as D P , and is defined as D P = { G i | P  X  G i , 1  X  i  X  n } . D P is also referred to as the data search space of P ,or data space for short. A graph pattern P is frequent if and only if | D P | | D |  X   X  for a support threshold  X  .

A constraint C is a boolean predicate on the pattern space U .Define f C : U  X  X  0 , 1 } as the corresponding boolean function of C such that f C ( P )=1 ,P  X  U if and only if P satisfies the constraint C . For example, let C be the constraint Max Degree ( P )  X  10 for a graph pattern P .Then f C ( P )=1 if and only if the maximum degree of all the vertices of P is greater than 10. We formulate the constraint-based frequent graph pattern mining problem as the following: Definition 1. (Constraint-based Frequent Graph Pattern Mining) Given a set of graphs D = { G 1 ,G 2 ,...G n } , a support threshold  X  , and a constraint C , constraint-Here are some graph constraints used in this paper: (1) The density ratio of a graph P , of a graph P is the maximum length of the shortest path between any two vertices of P .(4) EdgeConnectivity ( P ) ( V ertexConnectivity ( P ) ) is the minimum number of edges(vertices) whose deletion from P disconnects P . gPrune can be applied to both Apriori-based model and the pattern-growth model. In this paper, we take the pattern-growth model as an example to illustrate the prun-ing optimizations. Nevertheless, the techniques proposed here can also be applied to Apriori-based methods. The pattern-growth graph mining approach is composed of two stages (1) pattern seed generation (2) pattern growth. The mining process is conducted by iterating these two stages until all frequent patterns are found.
 Pattern Seed Generation: We u s e gSpan [17] to enumerate all the seeds with size of increasing order. One pattern seed is generated every time and proceeds to the pattern growth stage. A pattern seed could be a vertex, an edge, or a small structure. Pattern Growth: As outlined in Algorithm 1, PatternGrowth keeps a set S of pattern seeds and a set F of frequent patterns already mined. Each iteration of P atternGrowth might generate new seeds (added to S ), and identify new frequent patterns (added to F ). Line 1 initializes the data structures. Initially, S contains only the pattern seed. Line 2 to 11 grow every pattern seed in S until S is exhausted. For each pattern seed Q , which is taken from the set S in Line 3, Q is checked through its data search space and augmented incrementally by adding a new edge or vertex (Lines 4 and 5). Each augmented pattern is checked for pattern pruning in Line 6 and dropped whenever it satisfies the pattern pruning requirements. Al l the augmented patterns that survive the checking are recorded in S t . Then in Line 8, for each surviving pattern, we construct its own support data space from that of Q  X  X . Line 9 checks data pruning for each G in the support space. Since each thus augmented pattern is a frequent pattern, we add them to F in Line 10. Finally, these patterns are added to S , so that they will be used to grow new patterns. When S is exhausted, a new pattern seed will be generated until it is clear that all frequent patterns are discovered. The algorithm input: A frequent pattern seed P , graph database D = { G 1 ,G 2 ,...,G n } and the existing frequent pattern set F .The algorithm output: New frequent pattern set F .
 Algorithm 1. PatternGrowth PatternGrowth checks for pattern pruning in Line 6 and data pruning in Line 9. Pattern pruning is performed whenever a new augmented pattern is generated. This means any unpromising pattern will be prune d before constructing its data search space. Notice that data pruning is performed whenever infrequent edges are dropped after a new data search space is constructed and offers chance to drop new target graphs. As such, the search space for a pattern keeps shrinking as the pattern grows. A pruning property is a property of the constraint that helps prune either the pattern search space or the data search space. Pruning properties which enable us to prune patterns are called P-antimonotonicity , and those that enable us to prune data are called D-antimonotonicity . 4.1 Pruning Patterns (1) Strong P-antimonotonicity Definition 2. A constraint C is strong P-antimonotone if f C ( P )=1  X  f C ( P )=1 for all P  X  P .
 Strong P-antimonotonicity is simply the antimonotone property which has been known long since [9]. We call it strong P-antimonotonicity only to distinguish it from the other P-antimonotonicity introduced below. An example of strong P-antimonotone constraint for graph is acyclicity. (2) Weak P-antimonotonicity Constraints like  X  Density Ratio ( G )  X  0 . 1  X  is not strong P-antimonotone. Growing agraph G could make Density Ratio ( G ) go either up or down. However, they have weak P-antimonotonicity , which is based on the following intuition. If a constraint C is not strong P-antimonotone, then there must exist a pattern P violating C and a su-pergraph of P ,say Q , that satisfies C . In this case, we cannot prune graph P even if P violates C because Q might be missed if Q can only be grown out of P .However,if we can guarantee that Q can always be grown from some other subgraph P such that P satisfies C , we can then safely prune P .
 Definition 3. A constraint C is weak P-antimonotone if for a graph P where | V ( P ) |  X  k for some constant k , f |
V ( P ) | = | V ( P ) | X  1 . k is the size of the minimum instance to satisfy the constraint. When mining for weak P-antimonotone constraints, since we are sure that, for any constraint-satisfying pat-tern Q , there is a chain of substructures such that g 1  X  g 2  X  ...  X  g n = Q and g i satisfies the constraint for all 1  X  i  X  n , we can drop a current pattern P if it vi-olates the constraint, even if some supergraph of P might satisfy the constraint. Weak P-antimonotonicity allows us to prune patterns without compromising the completeness of the mining result. A similar property on itemsets,  X  X oose antimonotonicity X , has been discussed by Bonchi et al. in [13]. Notice that if a constraint is strong P-antimonotone, it is automatically weak P-antimonotone; but not vice versa. Also note that we can have similar definition of weak P-an imonotonicity with the chain of substructure decreasing in number of edges.

We us the graph density ratio example to illustrate the pruning. The proof of the following theorem is omitted due to space limit. Theorem 1. Givenagraph G ,if Density Ratio ( G ) &gt; X  , then there exists a sequence of subgraphs g 3 ,g 4 ,...,g n = G , | V ( g i ) | = i (3  X  i  X  n ) such that g 3  X  g 4  X  ...g n and Density Ratio ( g i ) &gt; X  .
 Theorem 1 shows a densely connected graph can always be grown from a smaller densely connected graph with one vertex less. As shown in this example of graph den-sity ratio, even for constraints that are not strong P-antimonotone, there is still pruning power to tap if weak P-antimonotonicity is available. 4.2 Pruning Data (1) Pattern-separable D-antimonotonicity Definition 4. A constraint C is pattern-separable D-antimonotone if for a pattern P and a graph G  X  D P , f C ( G )=0  X  f C ( P )=0 for all P  X  P  X  G .
 For constraints with pattern-separable D-antimonotonicity, the exact embeddings of the pattern are irrelevant. Therefore, we only need to check the constraint on the entire graphs in the pattern X  X  data search space, and s afely drop a graph if it fails the constraint.
Consider the constraint  X  X he number of edges in a pattern is greater than 10  X  .The observation is that every time a new data search space is constructed for the current pattern P , we can scan the graphs in the support space and prune those with less than 11 edges.

It is important to recognize that this data reduction technique can be applied repeat-edly in the entire mining process, instead of applying in an initial scan of the database as a preprocessing procedure. It is true that we will not benefit much if this data pruning is effective only once for the original data set, i.e. , if any graph surviving the initial scanning will always survive in the later pruning. The key is that in our framework, data pruning is checked on every graph in the data search space each time the space is updated for the current pa ttern. As such, a graph surviving the initial scan could still be pruned later. This is because when updating the search space for the current pattern P , edges which were frequent at last step could now become infrequent, and are thus dropped. This would potentially change each graph in the data search space, and offer chance to find new graphs with less than 11 edges which become eligible for prun-ing only at this step. Other examples of pattern-separable D-antimonotonic constraints include path/feature containment , e.g. , pattern contains three benzol rings . (2) Pattern-inseparable D-antimonotonicity Unfortunately, many constraints in practice are not pattern-separable D-antimonotone. V ertexConnectivity ( P ) &gt; 10 is a case in point. The exact embedding of the pattern is critical in deciding whether it is safe to d rop a graph in the data search space. These constraints are thus pattern-inseparable. In these cases, if we  X  put the pattern P back to G  X , i.e. , considering P together with G , we may still be able to prune the data search space.
 Definition 5. A constraint C is pattern-inseparable D-antimonotone if for a pattern P and a graph G  X  D P , there exists a measure function M : { P } X { G } X  X  0 , 1 } such that M ( P, G )=0  X  f C ( P )=0 for all P  X  P  X  G . The idea of using pattern-inseparable D-antimonotone constraints to prune data is the following. After embedding the current pattern P into each G  X  D P , we compute by a measure function, for all supergraphs P such that P  X  P  X  G , an upper/lower bound of the graph property to be computed in the constraint. This bound serves as a necessary condition for the existence of a constraint-satisfying supergragh P .We discard G if this necessary condition is violated. For example, suppose the constraint is V ertexConnectivity ( P ) &gt; 10 . If after embedding P in G , we find that the maximum vertex connectivity of all the supergraphs of P is smaller than 10, then no future pattern growing out of P in G will ever satisfy the constraint. As such G can be safely dropped. The measure function used to compute the bounds depends on the particular constraint. For some constraints, the computational cost might be prohibitively high and such a computation will not be performed. Another cost issue associated with pruning based on pattern-inseparable D-antimonotonicity is the maintenance of the pattern growth tree to track pattern embeddings. The Mining algorithm has to make a choice based on the cost of the pruning and the potential benefit. More discussion on the trade-off in these cases is given in Section 5. We use the vertex connectivity as an example to show how to perform data pruning. The time cost is linear in the pattern size for this constraint.

Let Neighbor ( P ) be the set of vertices adjacent to pattern P . For the vertex connec-tivity constraint, the following lemma gives a necessary condition for the existence of a P such that V ertexConnectivity ( P )  X   X  .
 Lemma 1. If | Neighbor ( P ) | &lt; X  , then there exists no P such that P  X  P  X  G and V ertexConnectivity ( P ) &gt; X  .
 Therefore, for each pair of pattern P and G  X  D P , the measure function M ( P, G ) could first embed P in G , and then identify Neighbor ( P ) .If | Neighbor ( P ) | is smaller than 10, returns 0. This pruning check is comput ationally cheap and only takes time linear in | V ( G  X  P ) | .

We summarize our study on the most useful constraints for graphs in Figure 1. Proofs are omitted due to space limit. The checking steps for pruning patterns and data are both associated with a computa-tional cost. Alternatively, one can first mine all frequent patterns by a known mining algorithm, gSpan [17] for example, then check constraints on every frequent pattern output, and discard those that do not satisfy the constraint. We call this method the enumerate-and-check approach in the following discussion. Which approach is better depends on the total mining cost in each case. Th e best strategy therefore is to estimate the cost and potential benefit for each approach at every pruning step, and adopt the one that would give better expected efficiency.

The growth of a pattern forms a partial order  X  defined by subgraph containment, i.e. , P  X  Q if and only if P  X  Q . The partial order can be represented by a pattern tree model in which a node P is an ancestor of a node Q if and only if P  X  Q .

Each internal node represents a frequent pattern, which is associated with its own data search space { T i } . The execution of a pattern mining algorithm can be viewed as growing such a pattern tree. Every initial pattern seed is the root of a new tree. Every time it augments a frequent pattern P , it generates all P  X  X  children in the tree. As such, each leaf corresponds to an infrequent pattern , or, in our mining model, a pattern that does not satisfy the constraints, since it is not further grown. Accordingly, the running time of a pattern mining algorithm can be bounded by the total number of nodes it generates in such a tree. This total sum is composed of two parts: (1) the set of all internal nodes, which corresponds to all the frequent patterns and is denoted as F ;and (2) the set of all leaves, denoted by L , which corresponds to the infrequent patterns or constraint-violating patterns.

Let X  X  look at the running time of the enumerate-and-check approach. Let the mini-mum support threshold be  X  , i.e. a frequent pattern has to appear in at least  X  | D | graphs, where D is the entire graph database. Let T c ( P ) be the cost to check a constraint C on a graph P . The running time of the enumerate-and-check approach can be lower-bounded as follows: 1. Internal Nodes 2. Leaf Nodes
Then the total cost, T P , for mining from an initial pattern seed P by the enumerate-and-check approach is lower-bounded as T P  X  P =  X  | D || F the set of frequent patterns grown from P . Essentially, the time cost of the enumerate-and-check approach is proportional to | F P | . To bound | F P | means to bound the number of frequent patterns that would be generated from a pattern P . It is very hard to analyti-cally give an accurate estimation of this heavily-data-dependent quantity. Our empirical studies show that in general, | F P | is very large for small patterns. An upper-bound of | F constraint.

Now we show how we should choose between the enumerate-and-check approach and our mining framework in both pattern pruning and data pruning cases. 1. Pruning Patterns: If we can prune a frequent pattern P after checking constraints 2. Pruning Data: If we can prune a graph G from the data search space of P after In this section, we are going to demonstrate the pruning power provided by the the new antimonotonicities introduced in our framework, i.e. , weak pattern-antimonotonicity and data-antimonotonicity. Among all of structural constraints described in Figure 1, minimum density ratio and minimum degree ar e selected as representatives. All of our experiments are performed on a 3.2GHZ, 1GB-memory, Intel PC running Windows XP.
We explored a series of synthetic datasets and two real datasets. The synthetic data generator 1 is provided by Yan et al. [23], which includes a set of parameters that allow a user to test the performance under different conditions. There are a set of parameters for users to specify: the number of target graphs( N ), the number of objects ( O ), the number of seed graphs ( S ), the average size of seed graphs ( I ), the average number of seed graphs in each target graph ( T ), the average density of seed graphs ( D ), and the average density of noise edges in target graphs.
 The detailed description about this synthetic data generator is referred to [23]. For a dataset which has 60 relational graphs of 1,000 distinct objects, 20 seed graphs (each seed graph has 10 vertices and an average density 0 . 5 ), 10 seed graphs per re-lational graph, and 20 noise edges per object ( 0 . 01  X  1 , 000  X  2 ), we represent it as N60O1kS20T10I10 D0.5d0.01. The first experiment is about the minimum density ratio constraint. As proved in Section 4, minimum density ratio has weak pa ttern-antimonotonicity property. For each graph whose density ratio is greater than  X  ( 0 &lt; X   X  1 . 0 ), we can find a subgraph with one vertex less whose density is greater than  X  . That means we can stop growing any frequent pattern with one more vertex if its density is less than  X  .

Figure 2 shows the pruning performance with various minimum density ratios. The data set used here is N60O1kS20T10I10 D0.5d0.01. The Y axis depicts the intermediate frequent patterns that are accessed during the mi ning process. The fewer the intermedi-ate patterns, the better the performance, given the cost of checking the pattern X  X  density ratio is negligible. The two curves show the performance comparison between methods with and without weak P-antimonotone pruning. As the figure shows, with the inte-gration of the minimum density ratio constraint, we only need to examine much fewer frequent patterns, which proves the effectiveness of weak pattern-antimonotonicity. In the next experiment, we fix the density ratio threshold at 0 . 5 and change the average density ratio of seed graphs ( D ) in the above synthetic dataset. The denser the seed graphs, the more the dense subgraph patterns. It could take longer to find these patterns. Figure 3 depicts the performance comparison between methods with or without weak P-antimonotone pruning. We found that when D is greater than 0 . 6 , the program with-out P-antimonotone pruning cannot finish in hours, while the one with P-antimonotone pruning can finish in 200 seconds.

Besides the weak P-antimonotone pruning, we also examined pattern inseparable D-antimonotonicity to pruning the data search space for the density ratio constraint. Given a frequent subgraph P and a graph G in the database ( P  X  G ), we need a measure to quickly check the maximum de nsity ratio for each graph Q ,where P  X  Q  X  G .For this purpose, we developed an algorithm for fast maximum density ratio checking. Let P be the image of P in G . Our algorithm has three steps: (1) transform G to G by merging all of the nodes in P . (2) apply Goldberg X  X  maximum density ratio subgraph finding algorithm to find a maximum density ratio subgraph in G (time complexity O ( n 3 logn ) ,where n = | V ( G ) | ) [24]. (3) for graph G , calculate a maximum density ratio subgraph that contains P ; if this density ratio is below the density ratio threshold, we can safely drop G from the data search space of P ( i.e. , G does not contain any sub-graph Q that contains P and whose density ratio is greater than the threshold). For each discovered subgraph, we perform this checking to prune the data search space as much as possible. Although this checking is much faster than enumerating all subgraphs in G , we find it runs slower than a method without pruning, due to the high computational cost. That is, the cost model discussed in Section 5 does not favor this approach. Through this exercise, it was learned that, in order to deploy D-antimonotone pruning, the corresponding measure function in Defintion 5 has to be fast enough.

We applied the above frequent dense subgraph mining algorithm to the real data that consists of 32 microarray expression sets measuring yeast genome-wide expression pro-files under different types of perturbations, e.g., cell cycle, amino acid starvation, heat shock, and osmotic pressure. Each dataset includes the expression values of 6661 yeast genes over multiple conditions. We model each dataset as a relational graph, where nodes represent genes, and we connect two genes with an edge if they have high cor-relation in their expression profiles. The patterns mined by our methods exhibit strong biological meanings. The mining result was published in our previous work [23]. Although we did not find a good D-antimonotonicity for the density ratio constraint, D-antimonotonicity is still app licable for other constraint s, e.g., the minimum degree constraint. Neither pattern antimonotonicity nor weak pattern antimonotonicity is avail-able for the minimum degree constraint. Thus, we develop a pruning technique us-ing pattern-inseparable data antimonotonicity, which checks the minimum degree of a pattern embedded in each graph. If the degree is below threshold  X  ,wedropthecor-responding graph from the data search space. The dropping will also decrease the fre-quency of each pattern and its superpatterns, which may make them infrequent as a result.

Figure 4 shows the comparison of pruning performance between data-antimonotonicity and a one-scan pruning method that drops vertices with less than  X  edges before running PatternGrowth . When the minimum degree constraint is weak, e.g. , minimum degree threshold is low, these two methods have similar performance. However, when the constraint becomes strong, the pruning based on data-antimonotonicity performs much better.

Figure 5 shows the number of subgraph isomorphisms performed for these two algo-rithms. It is clear that, using data antimonotonicity, a lot of graphs are pruned in the early stage so that the number of subgraph isomorphisms done in the later stage can be signifi-cantly reduced. We now check one constraint with pattern separable D-antimonotonicity  X  the minimum size constraint. The minimum size constraint on frequent itemset min-ing and sequential pattern mining has been explored before, e.g., SLPMiner developed by Seno and Karypis [25]. Suppose our task is to find frequent graph patterns with minimum length  X  . One approach is to check the graphs in the data search space of each discovered pattern P and prune the graphs that are not going to generate patterns whose size is no less than  X  . We developed several heuristics and applied our algorithm to mine the AIDS antiviral screen compound dataset from Developmental Theroapeu-tics Program in NCI/NIH [30]. The dataset contains 423 chemical compounds that are proved active to HIV virus.
 Figure 6 shows the runtime of the two algorithms with and without pattern separable D-antimonotone pruning, with different support thresholds. The size constraint is set in a way such that less than 10 largest patterns are output. It is a surprise that for this dataset, pattern separable D-antimonotone pruning is not effective at all. Closer exami-nation of this dataset reveals that most o f the graphs can not be pruned because the sizes of frequent patterns are relatively small in comparison with the graphs in the database. This once again demonstrates, as also shown in the density ratio constraint, that the effectiveness of the integration of a cons traint with the mining process is affected by many factors, e.g., the dataset and the pruning cost. In this paper, we investigated the problem of incorporating sophisticated structural con-straints in mining frequent graph patterns over a collection of graphs. We studied the nature of search space pruning for both patterns and data, and discovered novel an-timonotonicities that can significantly boost p runing power for graph mining in each case: (1) weak pattern-antimonotonicity for patte rns; (2) pattern-separable and pattern-inseparable data-antimonotonicities for da ta. We showed how these properties can be exploited to prune potentially enormous s earch space. An analysis of the trade-off between the enumerating-and-checking approach and the antimonotonicity-based ap-proach was also given in this study.

