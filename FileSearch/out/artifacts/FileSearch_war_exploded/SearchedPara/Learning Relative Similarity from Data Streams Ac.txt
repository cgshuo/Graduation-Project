 Relative similarity learning, as an important learning scheme for information retrieval, aims to learn a bi-linear similarity function from a collection of labeled instance-pairs, and the learned func-tion would assign a high similarity value for a similar instance-pair and a low value for a dissimilar pair. Existing algorithms ways made available for learning. However, this is not always re-alistic in practice since the number of possible pairs is quadratic to the number of instances in the database, and manually label-ing the pairs could be very costly and time consuming. To over-come the limitation, we propose a novel framework of active on-line similarity learning. Specifically, we propose two new algo-rithms: (i) PAAS: Passive-Aggressive Active Similarity learning; (ii) CWAS: Confidence-Weighted Active Similarity learning, and we will prove their mistake bounds in theory. We have conducted extensive experiments on a variety of real-world data sets, and we find encouraging results that validate the empirical effectiveness of the proposed algorithms.
 H.4 [ Information Systems Applications ]: Miscellaneous; D.2.8 [ Information Retrieval ]: Online Learning X  Active Learning Image Retrieval, Active Learning, Online Learning
Similarity learning, also known as distance learning, has been widely studied in machine learning and data mining communi-cations, such as ranking[4], recommendation system[1], image re-trieval [21, 26, 2, 30, 17] and text information retrieval [9], etc. In previous research, a variety of machine learning approaches have been proposed for learning distance or similarity functions from training data [31, 29, 14]. One of the most notable schemes is the c  X  Distance Metric Learning (DML) approach, which aims to learn a Mahalanobis distance [31] and requires the distance metric sat-isfying the Positive Semi-Definiteness (PSD) property. However, imposing the PSD requirement often makes the DML task compu-tationally challenging, particularly when handling large-scale train-ing data in high dimensional spaces, although various approxi-mate techniques have been proposed to improve computational ef-ficiency. Another strong assumption of DML approaches is that the exact distances of all pairs of instances should be obtained, which are usually costly and time consuming.

Instead of learning the Mahalanobis distance, another technique is to explore the relative similarity learning [4], which aims to learn a bilinear similarity model for measuring the similarity of any in-stance pair (two instances) by eliminating the PSD requirement, which is thus much more computationally efficient and scalable. More importantly, only relative similarities among any two pairs of instances are needed, which greatly alleviates the requirement of exact distances among all instances. In literature, several online learning algorithms have been proposed for learning relative sim-ilarity from data streams. Most existing studies generally assume that the data streams of instance pairs are fully labeled. However, we argue this may not be a realistic setting since the number of in-stance pairs is quadratic with respect to the number of instances, and labeling such a massive pool of instance pairs could be ex-tremely costly and time consuming for large-scale applications, es-pecially when the labeling task has to explicitly involve human in the loop in an early stage of deploying a new system.

Unlike the existing relative similarity learning approaches, in this paper, we investigate a novel scheme of active online learning of relative similarity from unlabeled data streams without requiring labeling every instance pair. Specifically, we propose two efficient and scalable online learning algorithms to tackle the new problem: (i) PAAS: Passive-Aggressive Active Similarity learning and (ii) CWAS: Confidence-Weighted Active Similarity learning. We then analyze the theoretical bounds of the proposed online learning al-gorithms, and further validate their empirical performances via an extensive set of experiments on various real datasets. The encour-aging results show that the proposed online active relative similar-ity learning algorithms can achieve highly competitive performance while significantly reducing the amount of labeling costs.
The rest of this paper is organized as follows. We first review related work, and then present the formulation of the proposed two algorithms as well as their theoretical analysis. Further, we empiri-cally validate the proposed algorithms in terms of effectiveness and efficiency, and finally conclude this paper and discuss the future work.
This paper is mainly related to two major categories of studies in literature: similarity learning, and active learning. We briefly review each of them below.

Similarity learning has been extensively studied in machine learning and data mining communities. Most existing works have been focused on DML [31] for learning a Mahalanobis distance through a PSD matrix [16, 22, 24]. Weinberger et al. [28] proposed a large margin nearest-neighbor classifier to address the DML prob-lem in the context of ranking. Globerson and Roweis [11] proposed to address the positive constraints in a supervised setting. Based on LogDet-regularization with different loss functions, Davis et al. [8] proposed online metric learning algorithms. All these al-gorithms aim to learn a PSD matrix M , based on which the dis-similarity of two images x 1 and x 2 is measured by computing ( x 1  X  x 2 ) &gt; M ( x 1  X  x 2 ) . However, it is computationally costly to impose the PSD constraint on M , which makes these algorithms in-appropriate for large-scale problems. To overcome this limitation, the bilinear form similarity function x &gt; 1 Mx 2 is proposed, where M is not required to be PSD. Along this direction, Chechik et al. [4] proposed the first-order relative similarity learning algorithm  X  X A-SIS" based on the first-order based online learning algorithm [6, 35, 15]. Recently, Crammer and Chechik [5] proposed a second-order relative similarity learning algorithm  X  X ROMA" based on second-order based online learning algorithm [7, 33]. However, all these algorithms require a large set of labeled data to train the similarity models, which is not always realistic in many real-world applica-tions due to the expensive labeling cost.

Active learning is a learning technique for reducing labeling cost by querying the most informative examples for labeling, which has been extensively studied in machine learning literature [3]. Ex-isting active learning algorithms could be generally classified into four categories: uncertainty-based [18, 27, 34], searching through the hypothesis space [10], minimizing the expected error and vari-ance on the pool of unlabeled instances [20, 13], and exploiting the structure information [25] among the instances. More can be found in the comprehensive survey [23]. Our strategy is more re-lated to the work in the first two categories. Specifically, we adopt the typical active learning strategy in the work [3, 32, 19], where the algorithm maintains a Bernoulli random variable Z t  X  { 0 , 1 } function on the t -th triplet. The algorithm will query the label and update the model only when Z t = 1 .

Despite being extensively studied [23], most of the active strate-gies are proposed for the classification or regression tasks. To our knowledge, there is no previous work exploring the active strategy on the relative similarity learning problem. To alleviate the cost in labeling, we explore the idea of active learning algorithms for overcoming the limitation of conventional relative similarity learn-ing approaches.
In this section, we first introduce the problem setting for active online learning of relative similarity from data streams, and then present the details of the proposed Active Online Similarity Learn-ing algorithms.
Following [4], we would study the problem of online learning a relative similarity function S ( x , x 0 ) ,  X  x , x 0  X  R the similarity between x and x 0 . Formally, let be a sequence of triplets (where [ T ] = { 1 ,...,T } ) . For the t -th triplet, y t = 1 indicates x t instance is more similar with x x , while y t =  X  1 implies x t is less similar with x 1 t than x goal is to learn a similarity function S ( x , x 0 ) that assigns a higher similarity score to the similar pair and a lower similarity score to the non-similar pair. Formally, given a triplet ( x t , x learned similar function should satisfy:
For the similarity function, we specifically adopt a linear simi-larity function that has a bi-linear form, where M  X  R d  X  d . To note, it is possible to learn a M  X  where d 6 = d 0 , as similarity function between two different spaces, however we will assume d = d 0 for simplicity.

To learn the parameter M , we could introduce some loss func-tions to measure its performance on the t -th triplet, for example, the hinge loss is defined as ` ( M ; ( x t , x 1 t , x 2 t ; y t )) = h 1  X  y t [ S ( x where the hinge loss [  X  ] + = max(0 ,  X  ) encourages y t e.g. logistic loss. In this article, we adopt the hinge loss for sim-plicity.

To solve this online relative similarity learning task, we can adopt those existing methods including Perceptron, Online Gradi-ent Descent, Online Passive Aggressive, etc. Although these online learning algorithms can achieve a cumulative loss comparable with the one attained by any fixed hypothesis, one major limitation is that they require all the labels of the instances while labeling in-stances might be very expensive or time consuming. To solve this problem, we would study active online similarity learning, which will try to only query the labels of a few informative instances so that the number of queried labels is reduced while the performance dose not degrade too much.
To solve Active Online Similarity Learning, we propose to adopt the selective sampling technique used in [3]. Specifically we will use a stochastic sampling scheme to decide whether it is necessary to query the label of the current instance. This scheme maintains a Bernoulli random variable Z t  X  { 0 , 1 } , where Z t = 1 indicates the label should be queried at the t -th step. More specifically, this scheme employs the following sampling probability where  X  &gt; 0 is a parameter to tune the number of queried labels and is the margin of similarities between x &gt; t Mx 1 t and x tuitively, the smaller the value of | p t | , the lower the confidence of the model on the current prediction; as a consequence, we should query the label with a higher probability of Pr( Z t = 1) . Once Algorithm 1 PAAS : The proposed algorithm of Passive-Aggressive Active Similarity learning Input: smooth parameters  X  &gt; 0 , C &gt; 0
Initialize: M 1 = 0 for t = 1 , 2 ,...,T do end for the label is queried, the algorithm will update the model using the online Passive Aggressive strategy, i.e., between minimizing the adjustment of the model and minimizing the loss of the new model on the current example. This will produce a first-order update method [4, 6], i.e., nally, the proposed Passive-Aggressive Active Similarity learning (PAAS) algorithm is summarized in Algorithm 1.
To improve the learning performance, a second order similarity learning algorithm AROMA is proposed [5], which does not only use the first order information but also the second-order informa-tion, i.e., the covariance matrix for all the features, to update the model. Compared to the first-order algorithm OASIS, the effec-tiveness of AROMA has been theoretically and empirically veri-fied. However, directly combining AROMA with query strategy in Equation (3) makes the theoretical analysis challenging.
Following the similar idea of AROMA, we assume the model maintains a Gaussian distribution, N ( M ,  X  ) , where M  X  and  X   X  R d 2  X  d 2 encode the model X  X  knowledge of and confidence of the model. At the t -th round, when receiving ( x t , x firstly decide whether it is necessary to query the true label based on Equation (3). If Z t = 1 , we query the label and update the distribution by minimizing the following objective function where is the Kullback-Leibler divergence of two distributions, G  X  X  t ( M t ) =  X  y t X t , where X t = x t ( x 1 t  X  x 2 t ) Algorithm 2 CWAS : The proposed algorithm of Confidence-Weighted Active Similarity learning Input: learning rate  X  ; regularization parameter  X  for t = 1 , 2 ,...,T do end for [ X new distribution not far away from the old one. The second term is a first order approximation of the current loss function at the cur-rent model, which is used to minimize the loss of the new model on the current example. The final term is to update the confidence of the model, since a new triplet is observed.  X  and  X  are two positive parameters to trade off these objectives.

To solve this optimization problem, we can set the derivatives  X  the following updating rules: where mat (  X  ) is the inverse function of vec (  X  ) .
 Finally, we summarize the proposed CWAS in Algorithm 2.

Remark One key drawback of this algorithm is that the dimen-sion of  X  t is d 2  X  d 2 , which will result in very high memory and computational complexities. In practice, we can use diagonal  X  reduce these complexities. Or equivalently, we can store a matrix  X  t  X  R d  X  d , and change the Equations (5) and (6) into where is element-wise product.
Denote m t = I ( y t 6 =  X  y t ) , we would analyze the performance of the proposed two algorithms in terms of expected mistake bounds E pendix. Firstly, we have the following theorem for the first-order algorithm PAAS.

T HEOREM 1. If the PAAS algorithm is run with a sequence of triplets ( x t , x 1 t , x 2 t ; y t ) , t  X  [ T ] , with D for any T &gt; 0 , and M  X  R d  X  d , we have where  X  = max(1 /C,D 2 X ) , and L T ( M ) = P T t =1 ` t In addition, the expected number of labels queried equals to P
Remark: The above bound depends on the choice of param-eter  X  . Generally,  X  could be viewed as a parameter to rule the extent to which the learning model fits the present data [3]. Min-imizing the right hand side over  X  , we can observe that setting  X  = p 1 + 4 CL T ( M ) / k M k 2 F in the above theorem gives the fol-lowing upper bound for E [ P T t =1 m t ]
Under the same assumptions as in the above theorem, we have the following theoretical guarantee for the second-order algorithm CWAS.

T HEOREM 2. If the algorithm CWAS is run with a sequence M  X  R d  X  d , we have Setting  X  = ( D M + | 1  X   X  |k M k ) q 2  X  Tr (  X   X  1 T +1 following bound holds for CWAS where D M = max t k M t  X  M k 2 F .
 Remark By optimizing over  X  , and setting  X  = 1 , we will have
In this section, we conduct experiments to evaluate the efficacy of the proposed algorithms for online similarity learning from data streams on several benchmark datasets.
To examine the performance, we conduct extensive experiments on a variety of benchmark datasets from web machine learning repositories. Table 1 shows the details of five datasets used in our experiments. Caltech256 [12] is a standard dataset for the image classification and ranking problem 1 . We use the same classes with [4] to generate the caltech50 and caltech249 . The other datasets are standard machine learning datasets publicly available at LIBSVM and UCI Machine Learning Repository 3 .
We evaluated the performance of all algorithms using precision-at-top-k, a standard ranking precision measure based on nearest instances were ranked according to their similarity to the query in-stance based on Equation (1), and the number of same-class in-stances among the top k instances (the k nearest neighbors) is com-puted, and then averaged across test instances. This measure is short named as AP. We also calculated the mean Average Precision (mAP), a measure that is widely used in the information retrieval community. For both of the AP and mAP measures, we set k = 10 .
On all datasets, we used standard 5 -fold cross validation and re-port both the AP and the mAP on test set. In Table 1, # Triplets the training set in each fold.
To examine the efficacy, we compare the proposed algorithms with the state-of-the-art relative similarity learning algorithms, where they have to query every triplet X  X  label during training stage.
To our knowledge, there is no work on exploring active learning on the relative similarity learning, so we compare our proposed algorithms with their random versions, where they randomly decide whether to query the label of t -th triplet. Alg. Query(%) AP@10 mAP@10 Time(s) Query(%) AP@10 mAP@10 Time(s) Alg. Query(%) AP@10 mAP@10 Time(s) Query(%) AP@10 mAP@10 Time(s) Alg. Query(%) AP@10 mAP@10 Time(s) Query(%) AP@10 mAP@10 Time(s)
For all algorithms, the parameters are searched within the space 10 [  X  5:1:5] using cross validation. For both PAAS and CWAS al-gorithms, we evaluate their performances on 10 different query ratios which are achieved by setting the sampling threshold  X  = 2 [  X  10:2:10] . The query ratio represents the ratio of queried labels over the total number of triplets in the data stream. PARS and CWRS are also evaluated on 10 query ratios by setting the ran-dom sampling parameters according to the query ratios in PAAS and CWAS, respectively.
In this experiment, we evaluate the performance of the proposed algorithms with some fixed query ratios. Table 2 shows the experi-mental results on different datasets.

First of all, with the same query ratio, the proposed active sim-ilarity learning algorithms (CWAS and PAAS) consistently out-perform their random versions (CWRS and PARS), respectively. Besides, CWAS greatly outperforms PAAS over all the datasets. Moreover, both CWAS and PAAS also achieve smaller variances as compared to their random algorithms (PARS and CWRS), respec-tively. These observations further confirm the effectiveness and ro-bustness of the proposed algorithms.

Secondly, on all the datasets, the proposed second-order active online similarity learning algorithm CWAS always achieves the best performance and the smallest variance. This is consistent with the previous results as shown in Figure 1 and 3, which further con-firms the effectiveness of exploiting the second-order information.
Thirdly, by examining the running time cost, PAAS and CWAS spent slightly more time cost as compared to their random variants, respectively, mainly due to the cost of computing the query strate-gies.

In addition, CWAS in general spends more time than PAAS due to the computation of the second order information. However, the extra time cost could almost be ignored considering the high effi-ciency of the online learning scheme. In practice, for the higher dimensional datasets, we could adopt the diagonal version algo-rithm  X  X WAS-d" to further reduce the running time cost, as shown in subsequent experiments.
In this section, we evaluate the performance of the proposed al-gorithms with varied query ratios as shown in Figure 1. Several observations could be drawn from the results.

Firstly, we can observe that the proposed active online learning algorithms can consistently outperform their random versions, re-spectively. This observation is consistent with the one in Table 2 and confirms the effectiveness of the proposed algorithms on se-lecting informative instances. More importantly, with around 30% query ratio, the proposed algorithm CWAS could achieve the simi-lar performance as CWS which queries all. This means that the pro-posed second-order active algorithm CWAS could save us around 70% effort compared to the traditional passive algorithms. For the proposed first-order active algorithm, PAAS could save us around 50% effort compared to the passive algorithm OASIS.

Secondly, the proposed second-order algorithm CWAS can out-perform the first-order algorithm PAAS. From Figure1, we can ob-serve CWAS could outperform PAAS on most of the datasets from the very small query ratio. The same result also could be observed on the CWRS compared to the PARS. These findings confirmed the effectiveness of introducing the second-order information. From the figure, we also could observe that the baseline second-order al-gorithm AROMA could achieve a little higher performance than our proposed algorithm CWS, on some datasets. One possible rea-son may be that the learning rate  X  is fixed in our algorithm in the Equation (5), while it is adaptive in the algorithm AROMA [7]. However, CWS in general is comparable with AROMA, while its active version is much easier to be theoretically analyzed.
In this section, we evaluate the sensitivity of the parameters both in Algorithm 1 and 2, respectively. However, it is difficult to eval-uate the algorithms with active query strategies. Thus, we evalu-ate their passive versions OASIS and CWS, respectively. Figure 2 shows the varied performances corresponding to different parame-ters on four of the datasets. For each dataset, the left figure shows the performance (Y-axis) corresponding to the parameter C (X-axis) in OASIS, and the right figure shows the performance (dif-ferent color) corresponding to parameter  X  (X-axis) and  X  (Y-axis) denotes mean Average Precision at 10. in CWS, where bright color corresponds to higher performance in terms of Average Precision at 10 than the dark color.
 For the parameter C , we can see it greatly affects the results of OASIS, on all of the datasets, which could be explained using our mistake bound in Theorem 1. Specifically, the mistake bound could be divided into two terms, and the parameter C divides the first term and multiplies the second term. When C is too small, the first term will dominate the mistake bound, and the second term will dominate the mistake bound when C is too large. This is consistent with the results shown in Figure 2. For most of the datasets, C = 1 would present a promising result.

For algorithm CWS, the relationship between the performance and the parameters is relatively complex. From Figure 2, we can observe that the learning rate parameter  X  should be not too small in general. And the regularization parameter  X  should be search around 1.
Although the second-order algorithm CWAS greatly outper-forms the first-order algorithm PAAS, it is costly to maintain the covariance matrix when the number of features is large. To reduce the complexity, we can use their diagonal versions by Equations (7) and (8). To test the diagonal algorithm CWAS-d, we carried out ex-periments on large-scale datasets caltech50 and caltech249 shown in Figure 3, where we can observe similar results as the one in Fig-ure 1. In addition, the increase of performance of CWAS-d over PAAS is not as large as CWAS in Figure 1 as expected, since only part of the second-order information is used. Figure 4: Time cost corresponding to varied querying ratio on large-scale datasets
In Figure 4, we evaluate the relationship between the querying ratio and the time-cost of our proposed algorithms. From the fig-lineally increasing with respect to the increasing querying ratio. Besides, CWAS-d cost a little extra time than its random version CWRS-d due to the computation of the query strategy, and PAAS costs almost the same as its random version PARS. What X  X  more, the second-order algorithm CWAS-d cost more than the first-order algorithm PAAS due to the computation of the second-order infor-mation, this is consistent with the finding in Table 2. More im-portantly, in Figure 3, we can observe that with around 30% query ratio, the proposed algorithm PAAS and CWAS-d could obtain sim-ilar performances as their passive versions OASIS and CWS-d, re-spectively. Meanwhile, when the query ratio is around 30%, in Figure 4, PAAS and CWAS-d only spends around 30% of the time spent by OASIS and CWS-d , respectively. These observations il-lustrate that our proposed algorithms can save a lot effort in labeling and training the model without sacrificing performance compared to the passive algorithms which query the labels of all the triplets.
To overcome the critical limitation of traditional passive online similarity learning from data streams, in this paper, we proposed a novel framework of active online learning for relative similar-ity learning. Specifically, we proposed two active online similarity learning algorithms for reducing the number of queried labels in the learning process. We theoretically analyzed the bounds of the pro-posed algorithms and conducted extensive experiments to examine the effectiveness of their empirical performance. The encourag-ing empirical results validate the effectiveness and efficiency of the proposed algorithms.
 There are several aspects we are interested to explore in future. Firstly, it would be more interesting to design an automatic method to assign the parameter  X  to control the query ratio, currently, it is manually assigned.
 Secondly, for the proposed second-order based algorithm CWAS, it would be more effective to consider the second-order information when designing the query strategy, such as the covari-ance information contained in  X  .

Thirdly, we are interested to design a real online active relative similarity learning where the labels of instance come from several noisy online workers, such as from the crowdsourcing platforms. This research is supported by the National Research Foundation, Prime Minister X  X  Office, Singapore under its IDM Futures Fund-ing Initiative and administered by the Interactive and Digital Me-dia Programme Office. This research is also partially supported by Singapore MOE tier 1 research grant (C220/MSS14C003).
 In this appendix, we provide the proofs of the theorems in the sec-tion 3.4.
 To prove the Theorem 1, we need the following lemma.
  X  be the step size parameter for PAAS as given in the algorithm. Then the following bound holds for any M  X  R d  X  d : where l t = I ( ` t ( M t ) &gt; 0 and sign(p t ) = y t I (sign( p t ) 6 = y t ) , I is the indicator function,  X  &gt; 0 and X x Lemma 1 can be proved using similar techniques in [3].Given Lemma 1, Theorem 1 can be proven as follows:
P ROOF . According to Lemma 1, we have
Plugging  X  =  X  +1 2 ,  X   X  1 into the above inequality will result in Since  X  t  X  min( C, 1 /D 2 X ) , the above inequality implies: Taking expectation with the above inequality, plugging the equal-ity E Z t =  X / (  X  + | p t ) and re-arranging the result conclude this theorem.
 In this subsection we will abuse M t , G t , X t to denote vec ( M vec ( G t ) , and vec ( X t ) , respectively. To prove Theorem 2, we need the following lemma.
 CWAS is run on this sequence of triplets, then the following bound holds for any M  X  R d  X  d , where l t = I ( ` t ( M t ) &gt; 0 and sign(p t ) = y t I (sign( p t ) 6 = y t ) , I is the indicator function,  X  &gt; 0 and k M
P ROOF . When Z t = 0 , it is easy to verify the inequality in the theorem.
 When Z t = 1 , it is easy to observe that where Because f t is convex, we have the following inequality  X  M , Re-arranging the above inequality will result in Now, we would provide a lower bound for G &gt; t ( M t +1 where the second inequality used the facts G t m )(  X  y t X t ) and  X  X  t ( M t +1 ) = 0 , i.e., Combining the above equality with the facts and we get the following bound for G &gt; t ( M t +1  X  M ) , Combining the previous inequalities, will give the following im-portant inequality Replacing M with  X  M concludes the proof.
 Given Lemma 2, the theorem 2 can be proven as follows: P ROOF . Firstly, according to the update rule we can derive the following equality where, we used the fact A = B + xx &gt; implies x &gt; A  X  1 Plugging the above equality into the inequality in the Lemma 2, and re-arranging it will gives Summing the above inequality over t = 1 , 2 ,...,T can give
X Now, we would like to bound the right hand side of the above in-equality. Firstly, we bound the first term as
X  X k M 1  X   X  M k 2 Tr (  X   X  1 2 ) + = max  X  2( D M + | 1  X   X  |k M k ) 2 Tr (  X   X  1 T +1 ) , where D M = max t  X  T k M t  X  M k 2 . Combining the above two inequalities, result in
X + 1  X   X  for all x &gt; 0 .
 Taking expectation with the above inequality and using the fact E
Dividing the above inequality with  X  concludes the proof. [1] G. Adomavicius and A. Tuzhilin. Toward the next generation [2] Q. Cao, Y. Ying, and P. Li. Similarity metric learning for face [3] N. Cesa-Bianchi, C. Gentile, and L. Zaniboni. Worst-case [4] G. Chechik, V. Sharma, U. Shalit, and S. Bengio. Large scale [5] K. Crammer and G. Chechik. Adaptive regularization for [6] K. Crammer, O. Dekel, J. Keshet, and S. Shalev-shwartz. [7] K. Crammer, A. Kulesza, and M. Dredze. Adaptive [8] J. V. Davis, B. Kulis, P. Jain, S. Sra, and I. S. Dhillon. [9] W. B. Frakes and R. Baeza-Yates, editors. Information [10] Y. Freund and Y. Mansour. Learning under persistent drift. In [11] A. Globerson and S. T. Roweis. Metric learning by [12] G. Griffin, A. Holub, and P. Perona. Caltech-256 object [13] Y. Guo and D. Schuurmans. Discriminative batch mode [14] S. C. Hoi, W. Liu, M. R. Lyu, and W.-Y. Ma. Learning [15] S. C. Hoi, J. Wang, and P. Zhao. Libol: A library for online [16] R. Jin, S. Wang, and Y. Zhou. Regularized distance metric [17] D. D. Lewis. Learning in intelligent information retrieval. In [18] D. D. Lewis and J. Catlett. Heterogenous uncertainty [19] J. Lu, P. Zhao, and S. C. Hoi. Online passive aggressive [20] N. Roy, A. Mccallum, and M. W. Com. Toward optimal [21] Y. Rui, T. S. Huang, and S.-F. Chang. Image retrieval: [22] M. Schultz and T. Joachims. Learning a distance metric from [23] B. Settles. Active learning literature survey. University of [24] S. Shalev-Shwartz, Y. Singer, and A. Y. Ng. Online and batch [25] V. S. Sheng, F. Provost, and P. G. Ipeirotis. Get another [26] L. Si, R. Jin, S. C. Hoi, and M. R. Lyu. Collaborative image [27] S. Tong and D. Koller. Support vector machine active [28] K. Q. Weinberger, J. Blitzer, and L. K. Saul. Distance metric [29] L. Wu, R. Jin, S. C. Hoi, J. Zhu, and N. Yu. Learning [30] W. Wu, H. Li, and J. Xu. Learning query and document [31] L. Yang and R. Jin. Distance metric learning: A [32] P. Zhao and S. C. Hoi. Cost-sensitive online active learning [33] P. Zhao, S. C. Hoi, and R. Jin. Double updating online [34] P. Zhao, S. C. Hoi, and J. Zhuang. Active learning with [35] P. Zhao, R. Jin, T. Yang, and S. C. Hoi. Online auc
