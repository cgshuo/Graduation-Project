 People often face a dilemma when analyzing high dimensional data. On one hand, more features imply more information available for the learning task. On the other hand, irrelevant/contradicting features introduce noise and may mislead the learning algorithms. This difficulty has been studied extensively in the literature from differen t perspectives including dimension reduction, feature selection, model ensembling, etc.

Among them, multiple subspace learning is a promising paradigm to address the high dimensional difficulty. In this approach, we construct multiple simple yet informative subspaces of the original high dimensional data. For example, principle component analysis (PCA) chooses the subspaces that best preserve the variance of the data. Then we can either build learning models in the ag-gregated space, or build models collabo ratively in each of the subspaces. This paradigm brings several desirable advantages. First, we can construct the sub-spaces by grouping related features together and separating contradicting fea-tures simultaneously. This is superior to simple feature reduction which may lose information carried by contradict ing features. Second, such collaborative learning mode in the aggregated space is superior to separately learning one submodel at a time and finally combining them. In fact, this mode share some spirit with multi-source learning [3] in the literature. In the language of multi-source learning, directly learning the original high dimensional data is actually the early-source-combination based approach, which might be too difficult for a single model. At the other extreme, directly assembling the separately learned submodels is actually the l ate-source-combination based approach, which might make very limited or even no information to be shared among different submod-els. The aggregated/collaborative learning mode is actually the intermediate-source-combination approach, which can balance between the learning difficulty of too many features for individual models and the ensemble difficulty of many too isolated and non-cooperative models.

Along this line, in this paper, we focus on the task of subspace learning for clustering high dimensional data. Specifically, we first construct numerical one-dimensional subspaces consisting of highly related features. In theory, such sub-spaces can substantially alleviate the unstable difficulties often encountered by clustering algorithms such as K -means. In practice, we show such subspaces can be efficiently constructed by leveragi ng correlation coefficients. Next, by fur-ther exploiting the one-dimension nature, we propose strategies to aggregate the representatives from the numerical one-dimensional subspaces into the final projected space. Finally, we use real-w orld document data sets to compare our approach with several competing methods in terms of performance lift and clus-tering separability. The experimental results demonstrate that our approach can find more clusterable subspaces which align better with the true class labels.
The rest of the paper is organized as follows. Section 2 summarizes recent works related to subspace learning. In Section 3, we show the numerical one-dimensional subspaces can be constructed by controlling the correlation struc-ture. In Section 4, we propose strategies to build the final projected subspace by aggregating the representatives from the numerical one-dimensional subspaces. Section 5 validates the effectiveness o f our idea on real-world document data sets. Section 6 concludes this paper with some remarks on future work. Our work can be categorized as dimension reduction for clustering. Although there have been extensive studies of dimension reduction techniques in the lit-erature, few of them are designed specia lly for the general clustering problems. In [10], the idea of grouping correlated features was exploited for the regression of the DNA microarray data. Specifically, the authors defined the  X  X upergenes X  by averaging the genes within the correlate d feature subspaces and then used them to fit the regression models. In our case of unsupervised clustering, however, we do not have response for learning, which was used in [10] to analyze the accuracy improvement of the regression with the a veraged features. Instead, we show that the subspaces of correlated features are ac tually of numerical one-dimension, which speaks to the improved clustering stability. Furthermore, empirical stud-ies on real-world data sets suggest that they enjoy higher clustering separability which aligns better with the true class labels. In [1], another approach of dimen-sion reduction, random projection, was exploited for the clustering problems. It is shown that any set of N points in D dimensions can be projected into O ( K/ X  2 ) dimensions, for  X   X  (0 , 1 / 3), where optimal K -means can be preserved. In the later experiments, we will compare our methods with this baseline approach.
Another category of related work includes the validation measures of the clus-tering results. [17] gave an organized study of the external validation measures. Normalization solutions and major properties of several measures were provided. Later, [9] investigated more widely used internal clustering validation measures. Recently, [5] studied the effectiveness o f the validation measures with respect to different distance metrics. It is shown that the validation measures might bias-edly prefer some distance metrics. Thus, we should be careful with the choice of validation measures involving distance computation. In this section, we first use the simpleness of 1-dimensional clustering to intro-duce the motivation of our work. Then we show how to construct numerical 1-dimensional subspaces by controlling the correlation structure of the features. 3.1 1-Dimensional Clustering The clustering problem canbeformulatedas: Problem 1. Given a set of observations X , and the number of clusters K ,the optimal clustering solution C = { C 1 ,  X  X  X  ,C K } minimizes the so-called within-cluster sum of squares (WCSS): where  X  k is the centroid of cluster C k .
 The most common solver for this problem, K -means [18], can only achieve local optima, which are not stable. Indeed, we might have more than one solutions, which are often inconsistent with one another. However, there is a special place where K -means yields more stable clustering results: 1-dimensional space. Proposition 1. For any two K -means clustering solutions on a 1 -dimensional C i where c that x 1  X  C 1 1 ,x 2  X  C 1 2 but x 1  X  C 2 2 ,x 2  X  C 2 1 .
 The proof is straightforward and is omitted due to space limit. In other words, K -means clustering is very simple in 1-dimensional space, which is equivalent to finding the cut points. This can also be intuitively visualized in the clustergram [12], as we will see later in Figure 1. In short, the clustergram examines how data points in each cluster are assigned to new clusters in the next round as the number of clusters increase. When Pr oposition 1 holds, it is expected that there are few cross lines connecting the co nsecutive solutions. However, few data are so perfectly  X 1-dimensional X  in reality. Hence, in the following, we seek 1-dimension-like subspaces, where Proposition 1 can be preserved approximately. 3.2 Numerical 1-Dimensional Subspace In 1-dimension-like subspaces (subset of fe atures), it is observed that, if most of the variation of the data can be captured by the first principle component, then K -means is roughly equivalent to clustering in 1-dimensional space (along the first principle direction). In this case, Proposition 1 will still hold under the mild assumption that all cluster centers can b e roughly connected by a line parallel to the first principle direction. Specifically, note that, if data point x is closer to cluster center c , its projection x, v is also closer to c on the axis of the first principle direction v . Formally, this notion is captured by the numerical 1-dimensional space define below [11, 7]: Definition 1. A data set X is numerical 1 -dimensional with error  X  ,ifandonly if  X  2  X   X  X  1 ,where  X  1  X   X  2  X  X  X  X  are singular values of X (standardized to be of zero-mean and unit-variance along each feature).

At first glance, we need to perform singular value decomposition many times to find such subspaces, which is expensive in high dimensional space. Nevertheless, as we will show below, the error  X  is bounded with a term of correlation among features, which can be leveraged to const ruct the desired subspaces efficiently. Theorem 1. If the average correlation of different features in the d -dimensional data set X is  X &gt; 0 ,then X is numerical 1 -dimensional with error Proof. Suppose matrix X  X  R N  X  d is already standardized to be of zero-mean and unit-variance along each feature (column). Then the feature correlations of X can be expressed by C = 1 N X X where the diagonal coefficients are all 1. With the singular value decomposition (SVD) X = U X V where U, V are unitary matrices and the diagonal coefficients of  X  are  X  1 , X  2 ,  X  X  X  ,wehave C = 1 N V X   X V where  X   X  = diag(  X  2 1 , X  2 2 ,  X  X  X  ). It follows that Let J be the column vector with 1 as all coefficients, then on one hand we have On the other hand, with the average of non-diagonal coefficients in C ,  X  ,we have J C J = i,j c ij  X  ( d 2  X  d )  X  + d . Hence, it follows that and this concludes our proof.

Theorem 1 suggests that, with a proper threshold of average correlation, the agglomerative hierarchical clustering over the feature set with average linkage can unambiguously group the original space into numerical 1-dimensional sub-spaces with error lower than the desired level. The standard Euclidean distance between features can be used as the linkage when the data matrix is of zero-mean and unit-variance along each feature. In the general case, the computational com-plexity of the agglomerative average linkage algorithm for D -dimensional data is O ( D 3 ), which is not efficient for big data a pplications. However, we note that Theorem 1 still holds if we denote  X  as the minimal correlation between fea-tures. This leads to the complete linkage clustering for which the computational complexity can be reduced to roughly O ( D 2 ). We will use this procedure in our experiments and denote it by F = N 1 dSpaces ( X , X  ) in the following discussions, where X is the data matrix,  X  is the maximal error of numerical 1-dimensional subspaces, and F is the constructed subspaces.
 The effectiveness of the subspace cons truction algorithm can be visualized in Figure 1, as mentioned earlier. Specifically, for a given high dimensional data set X , we can produce a clusgtergram by directly applying a clustering algorithm, such as K -means with increasing number of clusters. Then we can construct the numerical 1-dimensional subspaces F , and produce the same clusgergram in each subspace S in F . The results show that, in the subspaces, there are few cross lines connecting the consecutive solutions. Now we have constructed subspaces where the clustering problem can be ap-proached stably. However, clustering algorithms directly applied to the isolated subspaces might produce degenerated solu tions, since no information is shared between the subspaces. On the other hand, since each subspace S is numer-ically only of 1 dimension, it can be approximated by a few observation fea-tures. A natural way to this end is to investigate the SVD S = U  X  V ,where  X  = diag(  X  1 ,  X  X  X  , X  s ) is a diagonal matrix consisting of s positive singular values of S :  X  1  X  X  X  X  X  X   X  s . In general, we can transform S to SV by the principal direc-tions in V . Then, guaranteed by Theorem 1, we can use only the first principal component Sv where v is the first principal direction in V corresponding to  X  1 . Note that, this is often computationa lly more efficient, since we only need the first singular vector and it is not necessary to fully decompose S . Also, when the number of features are small in S , the computation can be further boosted by decomposing S S as in Theorem 1. This collaborative strategy of subspace en-semble is detailed in Algorithm 1, where mSpace ( F ) denotes the combination of the projected components of the multiple subspaces in F ,and mCluster denotes the clustering problem solver applied to mSpace ( F ).

In addition to the above strategy of aggregating projected components, we can also progressively approximate the subspaces in the light of [8]. Specifically, sup-pose we have the approximation 1 S d for the first d subspaces S 1 , S 2 ,  X  X  X  , S d .To approximate the next new subspace S d +1 , we compute the SVD S = U  X  V , where S =( 1 S d , S d +1 ) is concatenation of 1 S d and S d +1 . Then the new ap-proximation 1 S d +1 = SP where P are the top d + 1 principal directions in V . Algorithm 1. The multiple subspaces clustering algorithm The details are given in Algorithm 2, where pSpace ( F ) denotes the approxima-tion described above for the subspaces in F ,and pCluster denotes the clustering problem solver applied to pSpace ( F ). 5.1 Experimental Data Sets For evaluation, we used six real data sets from different domains, all of which are available at the website of CLUTO [4]. Some characteristics of these data sets are shown in Table 1. One can see diverse characteristics in terms of size (#doc), dimension (#term), number of clusters (#class) and cluster balance are covered by the investigated data sets. The cluster balance is measured by the ratio MinClass/MaxClass, where MinClass and MaxClass are the sizes of the smallest class and the largest class, respectively. 5.2 Comparison of Performance Lift To see how much improvements can be ach ieved by the subspaces, regardless which solver of Problem 1 is used, we compute the performance lift [14, 5] in Algorithm 2. The progressive subspaces clustering algorithm the approximated subspaces. Specifically, the performance lift can be defined assignments for the data set X and Y is the true class labels. The performance lift actually represents the difference be tween the ground truth of the clustering structure and the random clustering solution. The higher the lift is, the easier it will be for the solver of Problem 1 to find the optimal solutions. Thus, we can use this lift to see which subspaces help most. To estimate the lift( X | Y ), we can generate T (e.g., 10) random clustering assignments { C 1 ,  X  X  X  ,C T } , and compute different approximated subspaces for all the data sets.

Specifically, we generate T = 10 random clustering assignments to estimate the performance lift. By controlling the error  X  used in F = N 1 dSpaces ( X , X  ), we can construct approximation mSpace ( F )and pSpace ( F ) with different di-mensions, e.g., d = 100 , 200 ,  X  X  X  , 1000. For comparison, we also compute the per-formance lifts with top d principal components constructed by simple PCA, as denoted by  X  PC  X  in Figure 2. The line denoted by  X  RP  X  stands for Random Pro-jection [1], which constructs the low dimensional approximation of X  X  R N  X  D by X X  where  X   X  R D  X  d is random matrix with entries +1 / equal probability. We can see that mSpace , pSpace ,and PC are all effective to boost the performance lift. Also, while mSpace and pSpace outperform others consistently, mSpace achieves significantly higher lift of performance. 5.3 Comparison of Clustering Separability Adopted in [4, 5], one can investigate the data separability for the unsuper-vised clustering problem. Specifically, for each cluster C i in the clustering solu-distance, EDis ( C i ), over the average internal distance, IDis ( C i ). The average internal distance IDis ( C i ) is the average distance between the instances in C i , and the average external distance EDis ( C i ) is the average distance between the instances in C i and the instances in the rest of the clusters C j where j = i .The higher the ratio is for a cluster, the more compact and isolated the cluster will be, which, in turn, makes it easier for a clustering solver to identify the cluster. The ratio results of data set  X  X a1 X  are listed in Table 2, which clearly indicates that mSpace and pSpace provide better clustering separability. The last row also reports the average separability for the 6 clusters, where mSpace performs best. Besides, Figure 3 shows the average cluster separability for all of the six data sets. One can see that mSpace performs best on all data sets with few exceptions where pSpace performs better. 5.4 Analysis of Computational Cost To reduce the dimensionality of the data matrix X  X  R N  X  D to d ,ourmeth-ods first construct the numerical 1-dimensional spaces. In the general case, the complexity of this step is O ( D 2 ), as we discussed in Section 3. To construct mSpace , we need to perform SVD further d times in the subspaces, each of O ( N ) time, since most of the subspaces are of very low dimensions and we only need the first principal component. Thus the total computational cost for mSpace is O ( D 2 + dN ). For pSpace , the computational complexity of the progressive SVD is costly O ( d 2 N ) and the total cost is O ( D 2 + d 3 N ). For PCA, we have the com-putational cost of O ( N 2 D )when N  X  D or O ( ND 2 )when D  X  N .Inourexperi-ments, since most of the data sets are very high dimensional, we have the order of computational cost for the evaluated methods: RP&lt;PC&lt;mSpace&lt;pSpace , which aligns with the order of performance. In this paper, we proposed a numerical one-dimension approach to high di-mensional data clustering. An efficient c orrelation-based method was provided to construct the numerical one-dimensional subspace, which is well-clusterable and thus makes the clustering stable. Al so, we discussed two strategies to col-laboratively aggregate them into the final projected space. The experiments on real-world data sets demonstrated that such transformed data aligns better with the true class labels with respect to clustering.

This paper focused on the collaborative ensembling of the one-dimensional subspaces. For the future work, we plan to investigate collaboratively building clustering submodels directly in each of these one-dimensional subspaces. In the literature, this is related to the areas of multiple clustering [19, 6], clustering kernel [16] and clustering ensembles [1 3, 2, 15], where there are still many open problems to be answered.

