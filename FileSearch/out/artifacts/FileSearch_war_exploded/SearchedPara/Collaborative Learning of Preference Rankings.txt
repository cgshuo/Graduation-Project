 We propose a model for learning user preference rankings for the purpose of making product recommendations. The model allows us to learn from pairwise preference statements or from (incomplete) rankings over more than two items. We present two algorithms for performing inference in this model, both with excellent scaling in the number of users and items. The superior predictive performance of the new method is demonstrated on the well-known sushi preference data set. In addition, we show how the model can be used effectively in an active learning setting where we select only a small number of informative items for learning.
 I.2.6 [ Artificial Intelligence ]: Learning Recommendation, Collaborative Learning, Preferences, Rank-ing, Bayes, Active Learning, Approximate Inference
Collaborative recommendation has mostly been studied based on explicit feedback in the form of ratings, or based on implicit binary feedback such as observed purchases or clicks. Often real world data sources lie in between these ex-tremes. Explicit ratings of items are rare and hard to obtain, but often the information is richer than a simple binary sig-nal such as click/non-click. For example, users may express relative value judgments in comparing two different prod-ucts, or they may provide a partial preference ranking over available items. Such rankings can be explicit such as lists of favorite songs, or inferred from implicit information such as play counts for songs. To make efficient use of such informa-tion we propose a new bilinear factor model that maps latent user preferences to observed pairwise comparisons or rank-ings over items. Since feedback is relative to other items, this modeling approach is more robust than models of user preferences on an absolute scale. Yet it makes more efficient use of available data compared to methods that only allow for binary feedback. Research [2] also shows that people find it easier to formulate their preferences in such a relative way. An additional advantage is that modeling preference rank-ings directly leads to a ranking of items to be recommended to users, which is the end goal of many recommendation systems.

We present the new user preference model in Section 2, and develop two methods of performing inference in this model in Section 3. In Section 4 we apply the new model to learn correspondents X  preferences over sushi items and show that the new method compares favourably to other existing methods. In Section 5 we discuss the potential of the model to guide a more active learning strategy, where we actively and selectively ask the user for relative feedback on different items. Finally, Section 6 concludes.
Each of the N users and M items are represented with low-rank factors: user i with a K  X  1 parameter vector u i , and item j with a K  X  1 parameter vector v j . As some items are predominantly more popular than others, a univariate bias parameter b j is added to each. One might also add similar user-specific offsets to the model.

The user and item features are combined into a latent score s i,j , which represents how much user i likes item j . This latent score is generated by a bilinear model similar to the one used in Matchbox [7] and many other papers in the collaborative filtering literature.

The relative ordering of a set of scores determines a user X  X  preference of one item over the next. If user i prefers items j 1 j 2 ( meaning  X  X s preferred to X ), we require that s i,j s i,j 2 . We then observe a number of pairwise comparisons between the different latent scores s i,j . For each user, we denote these by C i , a sparsely filled matrix of dimension M  X  M , with elements These observed preferences can be explicitly provided by the user in the form of a ranking or a number of pairwise preference statements, or they can be inferred from the be-havior of the user, for example by ordering the time spent interacting with different items. Section 4 gives an example application, where we use the model to learn stated prefer-ences over sushi items. Importantly, if the preferences are expressed as a ranking of items, one might always find a set { s i,j } that is consistent over the ranked items j , and hence consistent with user i  X  X  observations C i . The data likelihood is therefore where I [true] = 1 and I [false] = 0. This is similar to the TrueSkill model of [1].

We assign independent normal priors to the user and item vectors, with The two hyperparameters  X  and  X  are set manually, but can also be inferred from the data, as explained in [6]. For brevity we do not consider this here. The full Bayesian net-work of our model is given in Figure 1. Figure 1: The proposed Bayesian factor model for learning preference rankings
The next section discusses how to infer the posterior dis-tribution of the parameters U , V , b conditional on the ob-served preferences.
The model proposed in the last section does not admit a closed form posterior distribution for the parameters U , V , b that we need in order to make recommendations. We there-fore propose two strategies for approximating this posterior distribution: a Gibbs sampling algorithm to generate sam-ples from the posterior distribution, and a hybrid message passing algorithm to minimize local divergence measures be-tween the posterior distribution and a factored approxima-tion. The performance and scaling of these two algorithms is evaluated on real world data in Section 4.
We can generate correlated samples from the posterior dis-tribution using a Gibbs sampling algorithm that iteratively samples from the conditional distributions p ( u i | V , b , S ), p ( v j ,b j | U , S ) and p ( s i,j | s i,/j , u i , v j where s i,/j denotes the vector of all scores for user i exclud-ing the j -th. The conditional distributions p ( u i | V , b , S ) and p ( v j ,b j | U , S ) are Gaussian and have been used by several authors before. See [6] for their precise form. The full condi-tional distributions p ( s i,j | s i,/j , u i , v j ,b j , C truncated normal, which follows from the Gaussian condi-tional prior (1) and the truncating likelihood (3).
The s i,j are most efficiently updated by first performing a forward pass over all scores for a given user i , sampling the scores s i,j in the order of the observed preference rank-ing, followed by a backward pass sampling in the reversed order. (Observe that we do not have to sample those s for which we have no feedback.) We find that this updating schedule does a good job of sampling the relative differences between the scores, but that it is slow in changing the over-all level of the scores. To further improve the mixing of the Gibbs sampling algorithm we therefore follow the for-ward and backward pass by an additional Monte Carlo step that simultaneously shifts all scores for a given user, while leaving the stationary distribution of the Markov chain in-variant. The update equation for this step is given as with L i the number of items for which user i has provided feedback,  X  f i the mean predicted score for those items, and  X  s the mean sampled score.

Since sampling the scores using the steps outlined here is relatively quick compared to sampling U , V and b , we find that the most efficient implementation of Gibbs sampling resamples S multiple times per iteration.
The Gibbs sampling algorithm outlined in the last section is relatively fast and can be applied at quite a large scale, however for very large data sets a deterministic approxima-tion of the posterior distribution may provide a better trade-off between accuracy and computational cost. An additional advantage of such a deterministic approximation is that it converges to a single mode of the posterior distributions and that it can be represented more compactly than the Gibbs sampling approximation, which reduces the computational cost of generating new recommendations for users given the posterior approximation. We develop a new algorithm to construct such a deterministic approximation, making use of Expectation Propagation (EP) [5] for the ranking likelihood (3) and Variational Bayes for the latent factor model. EP provides an excellent approximation for the uni-modal pos-terior resulting from the truncated Gaussian in (3), whereas Variational Bayes picks and locally approximates the poste-rior mode resulting from the product factor in (1).
We approximate the posterior distribution p ( U , V , b | C ) with a fully factorized Gaussian although our inference algorithm can also be used with a Gaussian approximation that preserves some of these de-pendencies, e.g. q ( U , V , b ) = Q j q ( v j ,b j ) Q i der to optimize this approximate posterior distribution we first approximate the likelihood term p ( C | S ) by a product of univariate Gaussian density functions in s i,j , i.e. with  X  (  X  ) a Gaussian pdf, which we initialize to have infinite variance. The parameters of the likelihood approximation,  X  i,j and  X  2 i,j are then set using EP. This EP step starts with the construction of a  X  X seudo prior X  on the s i,j , Using this pseudo prior, the algorithm for determining the that used by the TrueSkill rating system [1], with the scores s i,j taking the place of the  X  X layer skills X  in that system. We refer the reader to [1] for the specifics on the EP step.
After the EP step, we optimize the posterior approxima-tion using Variational Bayes, i.e., we choose our posterior approximation to solve
This step can be implemented efficiently using the Varia-tional Bayes Expectation Maximization (VBEM) algorithm. The resulting update equations can be found in [6]. How-ever, note that for our application the expectations with re-spect to s i,j in (9) follow from q ( U , V , b ) rather than from a separate posterior approximation on s i,j , as is more com-monly used (e.g. [6]). By avoiding this explicit approxi-mation of p ( S | C ), the posterior approximation q ( U , V , b ) gains in accuracy without increasing computational cost. The VBEM and EP steps are repeated until convergence.
For many real world applications of recommendation al-gorithms, both the number of users as well as the number of items is very large, necessitating the use of parallel com-putation to speed up inference. Both algorithms described above can be completely parallelized over users when up-dating S and U , and over items when updating V and b , which is an important advantage over the message passing algorithm used in [7]. Since in our application the number of items is quite small compared to the number of users, we found it most efficient to distribute the users and their feed-back over multiple threads. Within each thread, S and U can then be updated without requiring any communication across threads. Every update of V and b then requires each thread to submit the sufficient statistics for the update of these variables and to receive the updated values. Since the number of items is relatively low this adds very little over-head and it allows us to speed up inference almost linearly with the number of available computation nodes.
In order to compare the new algorithms to existing meth-ods we evaluate them on the sushi preference data of [3]. This data set was generated by asking 5,000 survey corre-spondents to order a subset of 100 sushi types according to their preferences. Each correspondent provided two such ordered lists containing 10 different sushi types. [3] evalu-ate their collaborative ranking approach by training on list  X  X  X  and using the model to predict the order of list  X  X  X . They measure the performance of their method by the aver-age Spearman correlation between the predicted and realized ranking. We use this measure to compare the performance of the new method to the  X  X antonac X  algorithm of [3], and also to compare our two inference algorithms against each other. Using this measure, we found that the maximum pre-dictive accuracy was reached after about 1000 draws of the Gibbs sampler after a burn-in period of 100 draws, or after 50 iterations of the VB/EP algorithm. The corresponding results are shown in Table 1 below.
 Table 1: Prediction accuracy of different methods on Sushi preference data
The results in Table 1 show that the new method com-pares favorably to that of [3]: The Gibbs sampling version of the new algorithm improves the Spearman correlation of the predictions with the test set by 0.07 in comparison with the Nantonac method, while the deterministic posterior ap-proximation gives an improvement of 0.05. The relatively small performance difference between the Gibbs sampling inference algorithm and the deterministic posterior approx-imation suggests that the latter is the more practical choice for real world applications, taking into account its benefits discussed in Section 3.2.
In order to improve our recommendations we may actively ask users to provide explicit feedback on certain items. This is most commonly done on an absolute rating scale, i.e. by asking the users to rate items. However, some studies in-dicate that people are better able to formulate their prefer-ences in a relative way, by ranking multiple items, see e.g. [2]. Such relative preference statements can be used directly by the model presented in Section 2.

Asking the user for feedback is costly as it will take time for the user to think about his or her preferences. In ad-dition, users may find it difficult to provide a full ranking of a very large list of items, so the the number of items we can enquire about is limited. When selecting this limited number of items we should take into account that not ev-ery item will be equally informative. A popular measure of the amount of information contained in a data point is the entropy reduction in our posterior distribution that we can expect upon conditioning on that data point [4]. By max-imizing the expected entropy reduction in our posterior we can select the most informative items to present to the user for feedback.

Since the posterior distribution of the model given in Sec-tion 2 is not available in closed form, we cannot maximize the expected entropy distribution exactly. However, we can get an estimate of the amount of information in each possi-ble observation by making use of posterior approximations. In doing so we will focus on the entropy reduction in the posterior distribution of the user parameters q ( u i  X  due to their greater number  X  are generally much more uncertain than the parameters of the items. To derive an expression for the approximate entropy reduction after ob-taining a new observation, we assume a factorized posterior approximation over U , V , d and S , optimized using Varia-tional Bayes. Note that this is not exactly the same as the approximation presented in Section 3.2, where we integrated out S in updating U , V and d . The Variational Bayes EM algorithm then uses the following update equation for the approximate posterior distribution on the user parameters:  X  = The entropy of this approximate posterior distribution is given by
After adding a new item l to the ranking of the user we can update the approximate posterior distribution q ( u i q entropy of q 0 ( u i ) is then given by
In order to maximize the information gain, or entropy re-duction, we should thus ask the user to rank that item for which the parameter vector v l has the highest expected Ma-halanobis norm k v l k  X  with respect to the covariance matrix of the current posterior approximation. This has the effect of selecting items that are most informative for exactly those elements of the user vector u i of which we are most uncer-tain. Note that for the approximate entropy (12) it does not matter what other item we compare the new item l to, or even whether we have a complete ranking with the new item or just a partial ranking. While this is obviously a very crude approximation, it still gives us a useful rule for actively selecting training examples as shown below.
We evaluate this active selection strategy using the sushi preference data, and we compare the resulting prediction accuracy with that obtained under random selection of the training examples. For each user the data set contains a training set ranking of 10 items of sushi. We actively select a subset of these items for each user by starting out with an empty selection set and subsequently adding that sushi item that minimizes the expected entropy in Equation (12). We then use the resulting selection of training examples to predict the ranking of the test set. For comparison, we do the same while selecting randomly from the remaining sushi items at each iteration. We display the accuracy of the re-sulting predictions for different numbers of selected items from a minimum of 3 to the maximum of 10. As can be seen from Figure 2 the active selection method leads to faster learning of the correct preferences than random selection of training examples.

Note that the performance measures of the two selection methods in Figure 2 converge as the number of training ex-amples increases because both methods select from the same limited set of 10 potential examples. For small numbers of examples the performance of the active selection method im-proves much faster than under random selection, indicating the practical value of such an active learning strategy for real life applications, where the user typically only provides feedback on a relatively small fraction of items. Figure 2: Prediction accuracy obtained using active versus random selection of training examples
We have proposed a Bayesian factor model to learn users X  preference rankings for the purpose of product recommen-dation. Learning preference rankings with this model can be done quickly and efficiently at large scale, using the two inference algorithms we have developed. The accuracy of our model was demonstrated on a real world data set and was shown to improve upon existing methods. In addition, we have shown that the model can also be used effectively for active preference elicitation. By actively selecting prod-uct comparisons to present to the user, we can uncover the user X  X  preferences without requiring large amounts of feed-back. This makes the process of preference elicitation much less burdensome on the user, and it can dramatically im-prove prediction accuracy for real life applications. [1] P. Dangauthier, R. Herbrich, T. Minka, and [2] S. R. Jaeger, A. S. J X rgensen, M. D. Aaslyng, and [3] T. Kamishima and S. Akaho. Nantonac collaborative [4] D. J. MacKay. Information-based objective functions [5] T. P. Minka. Expectation propagation for approximate [6] U. Paquet, B. Thomson, and O. Winther. A [7] D. H. Stern, R. Herbrich, and T. Graepel. Matchbox:
