 widom@ db.stanford.edu Univ~ Figure 1: A small Web graph G and simplified node-pairs graph G 2. SimRank scores using parameter C = 0.8 are shown for nodes in G 2. document corpus with cross-reference information. In the case of recommender systems, a user's preference for an item constitutes a relationship between the user and the item. Such domains are natu-rally modeled as graphs, with nodes representing objects and edges representing relationships. We present an algorithm for analyzing the (logical) graphs derived from such data sets to compute similar-ity scores between nodes (objects) based on the structural context in which they appear, a concept to be made clear shortly. The intuition behind our algorithm is that, in many domains, similar objects are related to similar" objects. More precisely, objects a and b are sim-are themselves similar. The base case is that objects are similar to themselves. 
As an example, consider the tiny Web graph G shown in Figure l(a), representing the Web pages of two professors ProfA and ProfB, their students StudentA and StudentB, and the home page of their university Univ. Edges between nodes represent hypeflinks from one page to another. From the fact that both are referenced (linked to) by Univ, we may infer that ProfA and ProfB are similar, and some  X  previous algorithms are based on this co-citation [10] information. We generalize this idea by observing that once we have concluded similarity between ProfA and ProfB, and considering that ProfA and ProfB reference StudentA and StudentB respectively, we can also conclude that StudentA and StudentB are similar. Continuing forth, we can infer some similarity between Univ and ProfB, ProfA and StudentB, etc. 538 
E represent relationships between objects. In Web pages or sci-entific papers, which are homogeneous domains, nodes represent documents, and a directed edge (p, q) from p to q corresponds to a reference (hyperlink or citation) from document p to document q. In a user-item domain, which is bipartite, we represent both users and items by nodes in V. A dkected edge ~p, q) corresponds to a purchase (or other expression of preference) of item q by person p. The result in this case is a bipartite graph, with users and items on either side. Note that edge weights may be used to represent varying degrees of preference, but currently they are not considered in our work. of in-neighbors and out-neighbors of v, respectively. Individual in-neighbors are denoted as h(v), for 1 &lt; i &lt; out-neighbors are denoted as Oi(v), for 1 &lt; i &lt; 
Recall that the basic recursive intuition behind our approach is "two objects are similar if they are referenced by similar objects." As the base case, we consider an object maximally similar to itself, to which we can assign a similarity score of 1. (If other objects are known to be similar a-priori, such as from human input or text matching, their similarities can be preassigned as well.) Referring back to Figure 1, ProfA and ProfB are similar because they are both referenced by Univ (i.e., they are co-cited by Univ), and Univ is (maximally) similar to itself. Note in Figure l(b) the similarity score of 1 on the node {Univ, Univ}, and the score of 0.414 on the node {ProfA, ProfB}. (How we obtained 0.414 will be described later.) 
StudentA and StudentB are similar because they are referenced by similar nodes ProfA and ProfB; notice the similarity score of 0.331 on the node for {StudentA, StudentB} in Figure l(b). izes SimRank as motivated above. Section 3.3 modifies the equation for bipartite graphs, such as graphs for recommender systems as dis-cussed in Section 2. The actual computation of SimRank values is discussed in Section 3.4, including pruning techniques to make the algorithm more efficient. benefits of SimRank in scenarios where information is limited. 3.2 Basic ShnRank Equation 
Let us denote the similarity between objects a and b by s(a, b) E [0, 1]. Following our earlier motivation, we write a recursive equa-where C is a constant between 0 and 1. A slight technicality here is that either a or b may not have any in-neighbors. Since we have no way to infer any similarity between a and b in this case, we should set s(a, b) = 0, so we define the summation in equation (1) to be 0 when I(a) = 0 or I(b) = 0. dered) pair of objects a and b, resulting in a set of n 2 SimRank 
Figure 2: Shopping graph G and a simplified version of the derived node-pairs graph G 2. Bipartite SimRank scores are shown for G 2 using C1 = C2 = 0.8. 3.3 Bipartite SimRank 
Next we extend the basic SimRank equation (1) to bipartite domains consisting of two types of objects. We continue to use recommender systems as motivation. Suppose persons A and B purchased item-sets {eggs, frosting, sugar} and {eggs, frosting, flour} respectively. 
A graph of these relationships is shown in Figure 2(a). Clearly, the two buyers are similar: both are baking a cake, say, and so a good recommendation to person A might be flour. One reason we can con-clude that A and B are similar is that they both purchased eggs and frosting. But moreover, A purchased sugar while B purchased flour, and these are sin~lar items, in the sense that they are purchased by similar people: cake-bakers like A and B. Here, similarity of items and similarity of people are mutually-reinforcing notions:  X  People are similar if they purchase similar items.  X  Items are similar if they are purchased by similar 
The mutually-recursive equations that formalize these notions are analogous to equation (1). Let s(A, B) denote the similarity be-tween persons A and B, and let s(c, d) denote the similarity between items c and d. Since, as discussed in Section 2, directed edges go from people to items, for A # B we write the equation and for c  X  d we write If A = B, s(A, B) = 1, and analogously for s(c, d). Neglecting 
C1 and C2, equation (2) says that the similarity between persons A and B is the average similarity between the items they purchased, and equation (3) says that the similarity between items c and d is the average similarity between the people who purchased them. The constants C1, C2 have the same semantics as C in equation (1). 540 as in (4) k according to equation (1). The values Rk(*, *) are nondecreas-ing as k increases. We show in [4] that they converge to limits satisfying (1), the SimRank scores s(% ,), i.e., for all a, b E V, lim~oo Rk(a, b) ~-s(a, b). In all of our experiments we have seen rapid convergence, with relative rankings stabilizing within 5 itera-tions (details are in Section [4]), so we may choose to fix a number K ~ 5 of iterations to perform. 
Let us analyze the time and space requirements for this method of computing SimRank. The space required is simply pairs (a, b). The time required is O(Kn2d2), since on each iter-ation, the score of every node-pair (n 2 of these) is updated with values from its in-neighbor pairs (d2 of these on average). As it cor-responds roughly to the square of the average in-degree, d2 is likely to be a constant with respect to n for many domains. The resource requirements for bipartite versions are similar. 
We mentioned that typically K ~ 5, and in most cases we also expect the average in-degree to be relatively small. However, n 2 can be prohibitively large in some applications, such as the Web, where it exceeds the size of main memory. Specialized disk layout and indexing techniques may be needed in this case; such techniques are beyond the scope of this paper. However, in the next subsec-tion we do briefly consider pruning techniques that reduce both the time and space requirements. Pruning has allowed us to run our ex-periments entirely in main memory, without the need for disk-based techniques. One way to reduce the resource requirements is to prune the logical considered, and a similarity score is computed for every node-pair. When n is significantly large, it is very likely that the neighborhood small percentage (&lt; 1%) of the entire domain. Nodes far from a node v, whose neighborhood has little overlap with that of v, will tend to have lower similarity scores with v than nodes near v, an ef-fect that will become intuitive in Section 4. Thus one pruning tech-consider node-pairs only for nodes which are near each other. If we consider only node-pairs within a radius of r from each other in the underlying undirected graph (other criteria are possible), and there are on average d~ such neighbors for a node, then there will be node-pairs. The time and space complexities become and O(ndr) respectively, where d2 is the average of pages a, b close enough to each other. Since d~ is likely to be much less than n and constant with respect to n for many types of data, we can think of the approximate algorithm as being linear with a possibly large constant factor. experimentally for the actual data sets. For the case of scientific papers, our empirical results suggest that this is a good approxima-tion strategy, and allows the computation to be carried out entirely in main memory for a corpus of r~ = 278,626 objects. More details can be found in Section [4]. 
As discussed in Section 3.2, it is important to have an intuition for the similarity scores produced by the algorithm. For this we provide an intuitive model based on "random surfers". We will show that the SimRank score s(a, b) measures how soon two random surfers are expected to meet at the same node if they started at nodes a and b and randomly walked the graph backwards. The details involve some complexity, and are developed in the remainder of this section. The model is presented in the context of general directed graphs; variations for bipartite SimRank (Section 3.3) are easy to derive and we leave them to the interested reader. Let H be any strongly connected graph (in which a path exists be-tween every two nodes). Let u, v be any two nodes in H. We define the expected distance 2 d(u, v) from u to v as The summation is taken over all tours t (paths that may have cycles) which start at u and end at v, and do not touch v except at the end. 1 if l(t) = 0. Note that the case where u = v, for which d(u, v) = has length 0. Because of the presence of cycles, there are infinitely many tours from u to v, and (5) is an (convergent) infinite sum. The expected distance from u to v is exactly the expected number of steps a random surfer, who at each step follows a random out-edge, would take before he first reaches v, starting from u. For our model, we extend the concept of expected distance to ex-pected meeting distance (EMD). Intuitively, the expected meeting distance re(a, b) between a and b is the expected number of steps re-quired before two surfers, one starting at a and the other at b, would meet if they walked (randomly) in lock-step. The EMD is symmetric by definition. Before formalizing EMD, let us consider a few exam-ples. The EMD between any two distinct nodes in Figure 3(a) is (informally) ~, since two surfers walking the loop in lock-step will follow each other forever. In Figure 3(b), ra(u, v) = re(u, w) = cx~ (surfers will never meet) and ra(v, w) = 1 (surfers meet on the next step), suggesting that v and w are much more similar to each other than u is to v or w. Between two distinct nodes of 3(c), the EMD is 3, suggesting a lower similarity than between v and vJ in 3(b), but higher than between u and v (or u and w). 21n the literature this quantity, in undirected graphs, is known as the hit-ting time [7], but we will develop the idea differently and so choose to use another name for our presentation. 
To define EMD formally in G, we use the derived graph G 2 of node-pairs. Each node (a, b) of V 2 can be thought of as the present in G 2 says that in the original graph G, one surfer can move from a to c while the other moves from b to d. A tour in G 2 of length n represents a pair of tours in G also having length n. 
The EMD re(a, b) is simply the expected distance in G 2 from (a, b) to any singleton node (x, x) E V 2, since singleton nodes in G 2 represent state, s where both surfers are at the same node. More precisely, The sum is taken over all tours t starting from (a, b) which touch a singleton node at the end and only at the end. Unfortunately, G 2 may not always be strongly connected (even if G is), and in such cases above. However, this definition would cause problems in defining distances for nodes from which some tours lead to singleton nodes while others lead to (a, b). We discuss a solution to this problem in the next section. There are various ways to circumvent the "infinite EMD" problem discussed in the previous section. For example, we can make each surfer "teleport" with a small probability to a random node in the graph (the soluticm suggested for PageRank in [8]). Our approach, which as we will see yields equations equivalent to the SimRank equations, is to map all distances to a finite interval: instead of com-puting expected length l(t) of a tour, we can compute the expected f(l(~)), for a nonnegative, monotonic function f which is bounded on the domain [0, ~). With this replacement we get the expected-f meeting distance. For our purposes, we choose the exponential function f(z) = c z, where c E (0, 1) is a constant. The benefits of this choice of f, which has values in the range (0, 1] over domain ~), are:  X  Equations generated are simple and easy to solve.  X  Closer nodes have a lower score (meeting distances of 0 go to 1 We define s'(a, b), the similarity between a and b in G based on expected-f meeting distance, as where e is a constant in (0, 1). The summation is taken to be 0 if there is no tour :from (a, b) to any singleton nodes. Note from (7) 
Let us consider these similarity scores on Figure 3 using C = 0.8 as an example. Between any two distinct nodes a, b in Figure 3(a), s'(a,b) = 0. In Figure 3(b), s'(v,w) = 0.8 while s'(u,v) = s'(u, w) = 0. Fbr any two distinct nodes in the complete graph of Figure 3(c), s'(a, b) ~ 0.47, a lower score than between v and w in Figure 3(b). We now show that s'(., ,) exactly models our original definition of SimRank scores by showing that s'(,, ,) satisfies the SirnRank 542 
There are a number of avenues for future work. Foremost, we must address efficiency and scalability issues, including additional prun-ing heuristics and disk-based algorithms, One possible approxima-tion that differs from the neighborhood-based pruning heuristic in 
Section 3.4.2 is to divide a corpus into chunks, computing accurate similarity scores separately for each chunk and then combining them r into a global solution. A second area of future work is to consider ternary (or more) relationships in computing structural-context sim-ilarity. For example, in the student-course domain we might also include the professors who taught the courses and the grades re-ceived by the students. Extending our entire framework to encom-pass such relationships should be possible, but it is not straightfor-ward. Finally, we believe that structural-context similarity is only one component of similarity in most domains, so we plan to explore the combination of SimRank with other domain-specific similarity measures. 
References [I] http://www.google.com. [2] Ricardo Baeza-Yates and Berthier Ribeiro-Neto. Modem [3] David Goldberg, David Nichols, Brian M. Oki, and Douglas [4] Glen Jeh and Jennifer Widom. SimRank: A measure [5] Jon M. Kleinberg. Authoritative sources in a hyperlinked en-[6] Joseph A. Konstan, Bradley N. Miller, David Maltz, [7] L~szl6 Lovfisz. Random Walks on Graphs: A Survey, vol-[8] Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry [9] Upendra Shardanand and Pane Maes. Social information fil-[lO] 
