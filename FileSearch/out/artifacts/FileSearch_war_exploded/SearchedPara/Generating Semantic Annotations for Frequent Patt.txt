 As a fundamental data mining task, frequent pattern mining has widespread applications in many different domains. Re-search in frequent pattern mining has so far mostly focused on developing efficient algorithms to discover various kinds of frequent patterns, but little attention has been paid to the important next step  X  interpreting the discovered frequent patterns. Although some recent work has studied the com-pression and summarization of frequent patterns, the pro-posed techniques can only annotate a frequent pattern with non-semantical information (e.g. support), which provides only limited help for a user to understand the patterns.
In this paper, we propose the novel problem of generat-ing semantic annotations for frequent patterns. The goal is to annotate a frequent pattern with in-depth, concise, and structured information that can better indicate the hidden meanings of the pattern. We propose a general approach to generate such an annotation for a frequent pattern by con-structing its context model, selecting informative context indicators, and extracting representative transactions and semantically similar patterns. This general approach has po-tentially many applications such as generating a dictionary-like description for a pattern, finding synonym patterns, discovering semantic relations, and summarizing semantic classes of a set of frequent patterns. Experiments on differ-ent datasets show that our approach is effective in generating semantic pattern annotations.
 Categories and Subject Descriptors: H.2.8 [Database Management]: Database Applications -Data Mining General Terms: Algorithms Keywords: frequent pattern, pattern annotation, pattern context, pattern semantic analysis
With its broad applications such as association rule min-ing [2], correlation analysis [4], classification [6], and cluster-ing [19], discovering frequent patterns from large databases has been a central research topic in data mining for years. Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. Various techniques have been developed for mining frequent item sets [2, 8], sequential patterns [3], graph patterns [22], etc. These techniques can usua lly output a large, complete set of frequent patterns efficiently, and provide basic statis-tic information such as support for each pattern. However, the excessive volume of the output pattern set and the lack of context information has made it difficult to interpret and explore the patterns. In most cases, a user only wants to explore a small set of most interesting patterns, and before exploring them, to have a rough idea about their hidden meanings or why they are interesting. This is analogous to literature reviewing. Before deciding whether to read through a paper, a reader often wants to first look at a short summary of the main ideas of the paper. Similarly, it is also highly desirable to have such a summary for a fre-quent pattern to explain or indicate the potential meanings of the pattern and to help a user decide whether and how to explore the pattern. Therefore, a new major challenge in frequent pattern mining has been raised by researchers, which is how to present and interpret the patterns discov-ered, in order to support the exploration and analysis of individual patterns. To meet this challenge and facilitate pattern interpretation, we need to annotate each frequent pattern with semantically enriched, in-depth descriptions of the pattern and its associated context.

Researchers have employed concepts like closed frequent pattern [15], and maximum frequent pattern [16] to shrink the size of output patterns and provide more information beyond  X  support  X . Recently, novel methods have been pro-posed either to mine a compressed set of frequent patterns [20] or to summarize a large set of patterns with the most representative ones [21]. Both of them employ extra infor-mation of frequent patterns beyond the simple information of support , which is either transaction coverage [20] or pat-tern profiles [21]. These methods successfully reduce the number of output patterns and present only the most inter-esting ones to the user. However, the information that these methods use to annotate a frequent patten is restricted to morphological information or simple statistics (e.g. support, transaction coverage, profile). From such an annotation, users could not infer the semantics of a pattern, thus still have to look through all the data transactions containing the pattern in order to figure out whether it is worth exploring.
In this paper, we study the problem of automatically gen-erating semantic annotations for frequent patterns, by which we mean to extract and provide concise and in-depth infor-mation for a frequent pattern, which indicates the semantics, or hidden meanings, of the pattern.

What is an appropriate semantic annotation for a frequent pattern? Generally, the hidden meaning of a pattern can be inferred from the patterns with similar meanings, the data objects co-occurring with it, and the transactions in which the pattern appears. In principle, we expect such an annotation to be compact, well structured, and indicative of the meanings of the pattern. This criterion is analogous to dictionary entries, which annotate each term with structured semantic information.
 Example 1: An example of a dictionary entry 1 In Example 1, we see that in a typical dictionary entry, the annotation for a term is structured as follows. First, some basic non-semantic information is presented. Second, a group of definitions are given, which suggests the semantics of the term, followed by several example sentences, which show the usage of this term in context. Besides, a set of synonyms, thesaurus or semantically similar terms are pre-sented, which have similar definitions with this term.
Analogically, if we can extract similar types of semantic in-formation for a frequent patte rn and provide such structured annotations to a user, it will be very helpful for him/her to interpret the meanings of the pattern and further ex-plore it. Given a frequent pattern, it is trivial to generate non-semantic information such as basic statistics and mor-phological information, so the main challenge is to generate the semantic descriptions of a pattern, which is the goal of our work. First, we should ideally provide precise semantic definitions for a pattern like those in a dictionary. Unfortu-nately, this is not practical without expertise of the domain. Thus we opt to look for information that can indicate the semantics of a frequent pattern, which presumably can help a user infer the precise semantics. Our idea is inspired from natural language processing, where the semantics of a word can be inferred from its context, and words sharing similar contexts tend to be semantically similar [13]. Specifically, by defining and analyzing the context of a pattern, we can find strong context indicators and use them to represent the meanings of a pattern. Second, we also want to extract the data transactions that best represent the meanings of the pattern, which is analogical to the example sentences. Finally, semantically similar patterns (SSPs) of the given pattern, i.e., patterns with similar contexts as the original pattern, can be extracted and presented. This is similar to the synonyms or thesauri of a term in dictionary. There-fore, an example of semantic pattern annotation (SPA) can be shown as follows:
Example 2: An example of annotating a frequent pattern The example is selected from Merriam-Webster X  X  Collegiate Dictionary &amp; Thesaurus The term  X  X requent pattern X  in this example is itself a fre-quent itemset, or a frequent sequential pattern in text. This dictionary-like annotation provides semantic information re-lated to  X  X requent pattern X , consisting of its strongest con-text indicators, the most representative data transactions, and the most semantically similar patterns.

Despite its importance, to the best of our knowledge, the semantic annotation of frequent patterns has not been well addressed in existing work. In this work, we define the novel problem of generating semantic annotations for fre-quent patterns. We propose a general approach to auto-matically generate structured annotations as shown in Ex-ample 2, by: 1) formally defining and modeling the context of a pattern; 2) weighting context indicators based on their strength to indicate pattern semantics; and 3) ranking trans-actions and semantically similar patterns based on context similarity analysis. Empirical experiments on three different datasets show that our algorithm is effective for generating semantic pattern annotations and can be applied to various real world tasks.

The semantic annotations generated by our algorithm have potentially many other applications, such as ranking pat-terns, categorizing and clustering patterns with semantics, and summarizing databases. Applications of the proposed pattern context model and semantical analysis method are also not limited to pattern annotation; other example appli-cations include pattern compression, transaction clustering, pattern relations discovery, a nd pattern synonym discovery.
The rest of the paper is organized as follows. In Section 2, we formally define the problem of semantic pattern annota-tion and a series of its associated problems. In Section 3, we introduce how the pattern context is modeled and instanti-ated. Pattern semantic analysis and annotation generation is presented in Section 4. We discuss our experiments and results in Section 5, the related work in Section 6, and our conclusions in Section 7, respectively.
In this section, we formally define the problem of semantic pattern annotation (SPA).

Let D = { t 1 ,t 2 , ..., t n } be a database containing a set of transactions t i , which can be itemsets, sequences, or graphs, etc. Let p  X  be a pattern (e.g., an itemset, a subsequence, or a subgraph) in D and P D = { p 1 , ..., p l } be the set of all such patterns. We denote the set of transactions in which p  X  appears as D  X  = { t i | p  X   X  t i ,t i  X  D } .
Definition 1 (Frequent Pattern): A pattern p  X  is fre-threshold and | D  X  | | D | is called the support of p  X  , usually de-noted as s (  X  ).
 Definition 2 (Context Unit): Given a dataset D and the set of frequent patterns P D ,a context unit is a basic ob-ject in D which carries semantic information and co-occurs with at least one p  X   X  P D in at least one transaction t The set of all such context units satisfying this definition is denoted as U D .

With this general definition, a context unit can be an item, a pattern, or a transaction in practice, depending on the specific task and data.

Definition 3 (Pattern Context): Given a dataset D and a frequent pattern p  X   X  P D ,the context of p  X  , denoted as c (  X  ), is represented by a selected set of context units U U
D such that every u  X  U  X  co-occurs with p  X  . Each selected context unit u is also called a context indicator of p associated with a strength weight w ( u,  X  ), which measures how well it indicates the semantics of p  X  .

The following is an example of the context for an item-set pattern in a small dataset with only two transactions. The possible context units for this dataset are single items, itemsets and transactions, and the context indicators of the itemset pattern are selected from the context units appear-ing with it in the same transactions.
 Example 3: An example of pattern context
With the definitions above, we now define the concept of semantic annotation for a frequent pattern and the related 3 subproblems.

Definition 4 (Semantic Annotation): Let p  X  be a frequent pattern in a dataset D , U  X  be the set of context indicators of p  X  ,and P be a set of patterns in D .A semantic annotation of p  X  consists of: 1) a set of context indicators of p  X  , I  X   X  U  X  ,s.t.  X  u  X  I  X  and  X  u  X  U  X   X  I  X  , w ( u , X  ) w ( u,  X  ); 2) a set of transactions T  X   X  D  X  ,s.t.  X  t  X   X  t  X  D  X   X  T  X  , t is more similar to c (  X  )than t under some similarity measure; and 3) a set of patterns P  X  P s.t.  X  p  X  P and  X  p  X  P  X  P , c ( p )iscloserto c (  X  )than c ( p ). Definition 5 (Context Modeling): Given a dataset D and a set of possible context units U , the problem of Context Modeling is to select a subset of context units U , define a strength measure w (  X  , X  ) for context indicators, and construct a model of c (  X  ) for each given pattern p  X  . Definition 6 (Transaction Extraction): Given a dataset D , the problem of Transaction Extraction is to define a sim-ilarity measure sim (  X  ,c (  X  )) between a transaction and a pat-tern context, and to extract a set of k transactions T  X   X  for frequent pattern p  X  ,s.t.  X  t  X  T  X  and  X  t  X  D  X   X  T sim ( t ,c (  X  ))  X  sim ( t, c (  X  )).
 Definition 7 (Semantically Similar Pattern (SSP) Extraction): Given a dataset D and a set of candidate pat-terns P c , the problem of Semantically Similar Pattern (SSP) Extraction is to define a similarity measure sim ( c (  X  ) ,c ( tween the contexts of two patterns, and to extract a set of k patterns P  X  P c for any frequent pattern p  X  ,s.t.  X  p  X  c (  X  )isthecontextof p  X  .
 With the definitions above, we may define the task of Semantic Pattern Annotation (SPA) as to: 1) select context units and design a strength weight for each unit to model the contexts of frequent patterns; 2) design similarity measures for the contexts of two pat-terns, and for a transaction and a pattern context; 3) for a given frequent pattern, extract the most significant context indicators, representative transactions and semanti-cally similar patterns to construct a structured annotation.
This problem is challenging in various aspects. First, we do not have prior knowledge on how to model the context of a pattern or select context units when the complete set of possible context units is huge. Second, it is not immediately clear how to analyze pattern semantics, thus the design of the strength weighting function and similarity measure is nontrivial. Finally, since no training data is available, the annotation must be generated in a completely unsupervised way. These challenges, however, also indicate a great advan-tage of the SPA techniques we will propose  X  they do not depend on any domain knowledge about the dataset or the patterns.

In the following two sections, we present our approaches for modeling the context of a frequent pattern and annotat-ing patterns through semantic context analysis.
In this section, we discuss how to model pattern contexts through selecting appropriate context units and defining ap-propriate strength weights. Given a dataset D and a set of frequent patterns P D , our goal is to select a set of context units which carry semantic information and can discriminate the meanings of the frequent patterns. The discriminating power of each context unit will be captured by its strength weights.

Vector Space Model (VSM) [17] is commonly used in nat-ural language processing and information retrieval to model the content of a text. For example, in information retrieval, a document and a query are both represented as term vec-tors, where each term is a basic concept (i.e., word, phrase), and each element of the vector corresponds to a term weight reflecting the importance of the term. The similarity be-tween documents and queries can thus be measured by the distance between the two vectors in the vector space. For the purpose of semantic modeling, we represent a transac-tion and the context of a frequent pattern both as vectors of context units. We select VSM because it makes no assump-tion on the vector dimensions and gives the most flexibility to the selection of dimensions and weights. Formally, the context of a frequent pattern is modeled as follows.
Context Modeling: Given a dataset D , a selected set of context units { u 1 , ..., u m } , we represent the context c (  X  )ofa frequent pattern p  X  as a vector w 1 ,w 2 , ..., w m ,where w w ( u i , X  )and w (  X  , X  ) is a weighting function. A transaction t is represented as a vector v 1 ,v 2 , ..., v m ,where v u  X  t ,otherwise v i =0.

The two key issues in a VSM are to select the vector dimensions and to assign weights for each dimension [17]. Specifically, the effectiveness of context modeling is highly dependent on how to select context units and design the strength weights. Actually, due to the generality of VSM, the proposed vector-space pattern context model is quite general and covers different strategies for context unit selec-tion and weighing functions. In the following subsections, we first discuss the generality of the context model, and then discuss specific solutions for the two issues respectively.
Some existing work has explored non-morphological infor-mation of frequent patterns with some concepts related to the  X  X attern context X  defined above. We now show that the notion of  X  X attern context X  is more general and can cover those concepts as special cases.

In [21], Yan et al. introduced the profile of an itemset for summarizing itemset patte rns, which is represented as a Bernoulli Distribution Vector. In fact, this  X  X rofile X  of a fre-quent itemset  X  can be written as a vector w ( o 1 , X  ) ,w ( o ..., w ( o d , X  ) over all the single items { o i } in D .Here erwise. This shows that this  X  X rofile X  is actually a special instance of the context model as we defined, where single items are selected as context units.

Xin and others proposed a distance measure for mining compressed frequent-pattern sets, which is computed based on the transaction coverage of two patterns [20]. Inter-estingly, the  X  X ransaction coverage X  is also a specific in-stance of  X  X attern context X . Given a frequent pattern p  X  the transaction coverage of p  X  can be written as a vector in D , where each transaction is selected as a context unit, and w ( t i , X  )=1if p  X   X  t i and 0 otherwise.

Covering the concepts in existing work as specific instances, the pattern context model we proposed is general and has quite a few benefits. First, it does not assume pattern types. The pattern profile proposed in [21] assumes that both trans-actions and patterns are itemsets, thus does not work for other patterns such as sequential patterns and graph pat-terns. Second, the pattern context modeling allows different granularity of context units and different weighting strate-gies. In many cases, single items are not informative in terms of carrying semantic information (e.g., single nucleotides in DNA sequences), and the semantic information carried by a full transaction is too complex and noisy (e.g., a text document). The context modeling we introduced bridges this gap by allowing various granularity of semantic units, and allows the user to explore the pattern semantics at the level that corresponds to their beliefs. Furthermore, this model is adaptive to different strength weighting strategies for context units, where the user X  X  prior knowledge about the dataset and patterns can be easily plugged in.
With the general definition presented in Section 2, the selection of context units is quite flexible. In principle, any object in the database that carries semantic information or serves to discriminate patterns semantically can be a context unit, thus context units can be single items, transactions, patterns, or any group of items/patterns, depending on the characteristics of the task and data.

Without losing generality, in our work we assume a pat-tern is the minimal units which carries semantic information in a dataset, and thus select the context units as patterns . All kinds of units can be considered as patterns with a spe-cific granularity. For example, in a sequence database, every single item can be viewed as a sequential pattern of length 1, and every transaction can be viewed as a sequential pattern which is identical to the transactional sequence. The choos-ing of patterns as context units is task dependent, and can usually be optimized with prior knowledge about the task and the data. For example, we can use words as context units in a text database, and in a graph database, we pre-fer subgraph patterns to be context units, since single items (i.e., vertices and edges) are noninformative.

This general strategy gives much freedom to select context units. However, selecting patterns of various granularity may cause the redundancy of context because these patterns are highly redundant. As discussed in previous sections, we expect the context units not only to carry semantic informa-tion but also to be as discriminative as possible to indicate the meanings of a pattern. However, when various granu-larity of patterns are selected as context units, some units will become less discriminative, and more severely, some be-comes redundant. For example, when the pattern  X  X ining subgraph X  is added as a context unit, the discriminating power of other units like  X  X ining frequent subgraph X  and  X  X ubgraph X  would be weakened. This is because the trans-actions containing the pattern  X  X ining subgraph X  always contain  X  X ubgraph X , and likely also contain  X  X ining frequent subgraph X , which means that these patterns are highly de-pendent and not discriminative to indicate the semantics of the frequent patterns co-occurring with them. This re-dundancy also brings a lot of unnecessary dimensions into the context vector space where the dimensionality is already very high. This redundancy in dimensions will affect both the efficiency and accuracy of distance computation between two vectors, which is essential for SPA. In our work, we ex-amine different techniques to remove the redundancy of con-text units without losing the semantic discriminating power.
One may first think of using existing techniques such as pattern summarization and dimension reduction to remove the redundancy of context units.

While the context units can be any patterns in principle, we are practically not interested in those with very low fre-quency in the databases. Therefore, the context units we initially include are frequent patterns . There exist meth-ods for summarizing frequent patterns with k representative patterns [21], but they only work for itemset patterns and are not general enough for our purpose.

Some techniques such as LSI [5] have been developed to reduce the dimensionality in high dimensional spaces, espe-cially for text data. However, these techniques aim to mit-igate the sparseness of data vectors by reducing the dimen-sionality, and are not tuned for removing the  X  X edundant X  dimensions. This is because all these dimensionality reduc-tion techniques consider that each dimension is  X  X mportant X  and the information it carries will always be preserved, or propagated into the new space. This is, however, different from our goal of re dundancy remov al. For example, let d and d 2 correspond to the patterns  X  X B X  and  X  X BC X . If we consider d 2 to be redundant w.r.t d 1 ,wedonotexpectthe information of d 2 to be preserved after the removal of d Since neither the pattern summarization nor the dimen-sionality reduction technique is directly applicable to our problem, we examine alternative strategies. Noticing that the redundancy of context units is likely to be caused by the inclusion of both a frequent pattern and its sub patterns, we explore closed frequent patterns [15] and maximum frequent patterns [16] to solve this problem.

A maximal frequent pattern is a frequent pattern which does not have a frequent super-pattern. It is easy to show that maximum frequent pattern is not appropriate for this problem since it may lose important discriminative units. For example, the frequent pattern  X  X ata cube X , although not a maximum frequent pattern, indicates different semantics from the frequent pattern  X  X rediction data cube X , and thus should not be removed.

Definition 8 (Closed Frequent Pattern): Afrequent pattern p  X  is closed if and only if there exists no super-pattern p  X  of p  X  ,s.t. D  X  = D  X  .

We assume that a context unit is not redundant only if it is a closed pattern. This assumption is reasonable because  X  p  X   X  P D ,if p  X  is not closed, there is always another fre-quent pattern p  X   X  P D ,where p  X   X  p  X  and  X  t i  X  D ,we have p  X   X  t i  X  p  X   X  t i . This indicates that we can use p  X  as a representative of p  X  and p  X  without losing any se-mantic discriminating power. Therefore, in our work we use closed frequent patterns as our initial set of context units. The algorithms for mining different kinds of closed frequent patterns can be found in [15, 23].
However, as stated in [21], a small disturbance within the transactions may result in hundreds of subpatterns that could have different supports, which cannot be pruned by closed frequent pattern mining. Those subpatterns are usu-ally with supports only slightly different from that of the master pattern. Therefore, their discriminating power for the semantics of the frequent patterns is very weak when their master patterns are also included as a context unit. We present clustering methods to further remove redundancy from the closed frequent patterns.

Microclustering is usually employed as a preprocessing step to group data points from presumably the same clus-ter to reduce the number of data points. In our work, we first introduce a distance measure between two frequent pat-terns and then introduce two microclustering algorithms to further group the close frequent patterns.

Definition 9 (Jaccard Distance): Let p  X  and p  X  as two frequent patterns. The Jaccard Distance between p  X  and p  X  is computed as:
Jaccard Distance [10] is commonly applied to cluster data based on their co-occurrence in transactions. Our need is to group the patterns that tend to appear in the same transac-tions, which is well captured by Jaccard Distance. Jaccard Distance has also been applied to pattern clustering in [20].
With Jaccard Distance, we expect to extract clusters such that the distances between inner-cluster units are bounded. We present two microclustering algorithms as follows: In the Hierarchical M icroclustering method presented as Algorithm 1, we iteratively group two clusters of patterns with the smallest distance, where the distance between two clusters are defined as the Jaccard distance between the far-Algorithm 1 Hierarchical Microclustering Input: Transaction dataset D, Output: A set of patterns, P = { p 1 , ..., p k } 1: initialize n clusters C i , each as a closed frequent pattern; 2: compute the Jaccard Distance d ij among { p 1 , ..., p 3: set the current minimal distance d = min ( d ij ); 4: while ( d&lt; X  ) 5: select d st where ( s, t ) = argmin i,j d ij ; 6: merge clusters C s and C t intoanewcluster C u ; 7: foreach C v = C u 8: compute d uv = max ( d  X  X  )where p  X   X  C u ,p  X   X  C v ; 9: foreach C u ; 10: foreach p  X   X  C u ; 11: compute  X  d  X  = avg ( d  X  X  )where p  X   X  C u ; 12: add p  X  into P ,where  X  =argmin i (  X  d i ); 13: return Algorithm 2 One-pass Microclustering Input: Transaction dataset D, Output: A set of patterns, P = { p 1 , ..., p k } 1: initialize 0 clusters; 2: compute the Jaccard Distance d ij among { p 1 , ..., p 3: foreach ( p  X   X  X  ) 4: foreach cluster C u 5:  X  d  X ,u = max ( d  X  X  )where p  X   X  C u ; 6: v =argmin u (  X  d  X ,u ); 7: if (  X  d  X ,v &lt; X  ) 8: assign p  X  to C v 9: else 10: initialize a new cluster C = { p  X  } 11: foreach C u ; 12: foreach p  X   X  C u ; 13: compute  X  d  X  = avg ( d  X  X  )where p  X   X  C u ; 14: add p  X  into P ,where  X  =argmin i (  X  d i ); 15: return thest patterns in the two clusters. The algorithm termi-nates when the minimal distance between clusters becomes larger than a user-specified threshold  X  . The second algo-rithm, which we call One-Pass M icroclustering, iteratively assigns a closed frequent pattern p  X  to its nearest cluster if the distance is below  X  , where the distance between p and a cluster C is defined as the Jaccard distance between p  X  and its farthest pattern in C . Both algorithms give us a set of microclusters of closed frequent patterns. They both guarantee that the distance between any pair of patterns in the same cluster is below  X  . Only the medoid of each cluster is selected as a context unit. By varying  X  ,auser can select context units with various levels of discriminating power of pattern semantics. It is clear that Algorithm 2 only passes the pattern set once and thus is more efficient than the hierarchical algorithm, at the expense that the quality of clusters depends on the order of patterns. The performance of these two methods are compared in Section 5.

Once the context units are selected, the remaining task is to assign a weight to each dimension of the context model, which represents how strong the context unit corresponding to this dimension indicates the meaning of a given pattern. Intuitively, the strongest context indicators for a pattern p should be those units that frequently co-occur with p  X  but infrequently co-occur with others.

Practically, many types of weighting functions can be used to measure the strength of a context indicator. For example, we can assign the weight for a context indicator u for p  X  the number of transactions with both u and p  X  . However, in principle, a good weighting function is expected to satisfy several constraints :
Given a set of context indicator U and a frequent pattern p , a strength weighting function w (  X  ,p  X  ) is good if  X  1. w ( u i ,p  X  )  X  w ( p  X  ,p  X  ): the best semantic indicator of 2. w ( u i ,p  X  )= w ( p  X  ,u i ): two patterns are equally strong 3. w ( u i ,p  X  ) = 0 if the appearance of u i and p  X  is inde-
An obvious choice is co-occurrences, which however, may not be a good measure. On one hand, it does not satisfy constraints 3. On the other hand, we want to penalize the context units that are globally common patterns in the col-lection. Which means, although they may co-occur many times with p  X  , it may still not be a good context indica-tor for p  X  because it also co-occurs frequently with others. In general, the context units that are strongly correlated to p  X  should be weighted higher. In our work, we introduce a more principled measure.

Mutual Information (MI) is widely used to measure the mutual independency of two random variables in informa-tion theory, which intuitively measures how much informa-tion a random variable tells about the other. The definition of mutual information is given as
Definition 10: (Mutual Information). Given two fre-quent patterns p  X  and p  X  ,let X = { 0 , 1 } and Y = { 0 , 1 be two random variables for the appearance of p  X  and p  X  respectively. Mutual information I( X ; Y ) is computed as: where P ( x =1 ,y =1)= | D  X   X  D  X  | | D | , P ( x =0 ,y =1)= dard Laplace smoothing to avoid zero probability.
It can be easily proved that Mutual Information satisfies all the three constraints and f avors the strongly correlated units. In our work, we use mutual information to model the indicative strength of the context units selected.
Given a set of patterns as candidate context units, we ap-ply closeness testing and microclustering to remove redun-dant units from this initial set. We then use mutual infor-mation as the weighting function for each indicator selected. Given a frequent pattern, we apply semantic analysis with its context model and generate annotations for this pattern, as discussed in the following section.
Let U = { u 1 ,u 2 , ..., u k } be a selected set of k context units and w (  X  ,p  X  ) be the unit weighting function w.r.t. any frequent pattern p  X  , i.e. I (  X  ; p  X  ). The context model, or con-text vector c (  X  )for p  X  is w ( u 1 ,p  X  ) ,w ( u 2 ,p  X 
As introduced in Section 1, we make the assumption that the frequent patterns are semantically similar if their con-texts are similar to each other. In our work, we analyze the semantics of frequent patterns by comparing their context models. Formally,
Definition 11 (Semantical Similarity): Let p  X  , p  X  , p  X  be three frequent patterns in P and c (  X  ), c (  X  ), c (  X  ) their context models. Let sim ( c (  X  ) ,c (  X  )) : V k  X  &gt;sim ( c (  X  ) ,c (  X  )), we say that p  X  is semantically more sim-ilar to p  X  than p  X  w.r.t. sim ( c (  X  ) ,c (  X  )).
Cosine is widely used to compute the similarity between two vectors, and is well explored in information retrieval to measure the relevance between a document and a query if both are represented with a vector space model [17]. In our work, we use cosine similarity of two context vectors to mea-sure the semantic similarity of two corresponding frequent patterns. Formally, the cosine similarity of two context vec-tors is computed as where c (  X  )= a 1 ,a 2 , ..., a k and c (  X  )= b 1 ,b 2 , ..., b
With the context model and the semantical similarity measure, we now discuss how to generate semantic anno-tations for frequent patterns.
Let p  X  be a frequent pattern and c (  X  )beitscontext model, which is defined in this work as a context vector w 1 ,w 2 , ..., w k over a set of context units U = { u 1 ,u As defined in Section 2, w i is a weight for u i which tells how well u i indicates the semantics of p  X  . Therefore, the goal of extracting strongest context indicators is to extract a sub-set of k context units U  X   X  U such that  X  u i  X  U  X  and  X  u j  X  U  X  U  X  ,wehave w i  X  w j .

With a strength weighting function w (  X  ,p  X  ), e.g., mutual information as introduced in Section 3, we compute w i = w ( u i ,p  X  ), rank u i  X  U with w i in descending order and select the top k u i  X  X .
Let p  X  be a frequent pattern, c (  X  ) be its context model, and D = { t 1 , ...t l } be a set of transactions, our goal is to select k t transactions T  X   X  D with a similarity function s (  X  ,p  X  ), s.t.  X  t  X  T  X  and  X  t  X  D  X  T  X  , s ( t, p  X  )  X 
To achieve this, we first represent a transaction as a vec-tor in the same vector space as the context model of the frequent pattern p  X  , i.e., over { u 1 ,u 2 , ..., u k use the cosine similarity presented in Section 3 to compute the similarity between a transaction t and the context of p . The rest is again a ranking problem. Formally, let c ( t )= w 1 ,w 2 , ..., w k where w i =1if u i  X  t and w otherwise. We compute sim ( c ( t ) ,c (  X  )) for each t  X  them in descending order and select the top k t t  X  X .
Let p  X  be a frequent pattern, c (  X  ) be its context model, and P c = { p 1 , ..., p c } be a set of frequent patterns which are believed to be good candidates for annotating the se-mantics of p  X  , i.e., as synonyms, thesauri, or more generally as SSPs. Our goal is to extract a subset of k c patterns P let { c ( p 1 ) , ..., c ( p c ) } be the context vectors for We compute sim ( c ( p i ) ,c (  X  )) for each p i  X  P c ,rankthemin descending order, and select the top k c p i  X  X .

Note that the candidate SSP set for annotation is quite flexible. It can be the whole set of frequent patterns in D ,or a user-specified set of patterns based on his prior knowledge. It can be a set of homogenous patterns with p  X  ,orasetof heterogenous patterns. For example, it can be a set of pat-terns or terminology from the domain that a user is familiar with, and is used to annotate patterns from an unfamiliar domain. This brings great flexibility to apply the general SPA techniques to different tasks. By exploring different types of candidate SSPs, we can find quite a few interest-ing applications of semantic pattern annotation, which are discussed in Section 5.
In this section, we present experiment results on three different datasets to show the effectiveness of the semantic pattern annotation technique for various real-world tasks.
The first dataset we use is a subset of the DBLP dataset 2 It contains papers from the proceedings of 12 major con-ferences in Database and Data Mining. Each transaction consists of two parts, the authors and the title of the cor-responding paper. We consider two types of patterns: (1) frequent co-authorship, each of which is a frequent itemset of authors and (2) frequent title terms, each of which is a frequent sequential pattern of the title words. The goal of experiments on this dataset is to show the effectiveness of the SPA to generate a dictionary-like annotation for frequent patterns. Our experiments are designed as follows: 1) Given a set of authors/co-authors, annotate each of them with their strongest context indicators, the most repre-sentative titles from their publications, and the co-authors or title patterns which are most semantically similar to them. Note that the most representative titles do not necessarily mean their most influential work, but rather the titles which best distinguish their work from others X  work. 2) Given a set of title terms (sequential patterns), anno-tate each of them with their strongest context indicators, the most representative titles, the most similar terms, and the most representative author/co-authors. Note again that the most representative author/co-authors are not necessar-ily the most well-known ones, but rather the authors who are most strongly correlated to the topics (terms). In both experiments, we use the tools FP-Close [7] and CloSpan [23] to generate closed frequent itemsets of co-authors and closed sequential patterns of title terms respec-tively. The title words are stemmed by Krovertz stemmer [12], which converts the morphological variations of each English word to its root form. We set the minimum sup-port for frequent itemset as 10 and sequential patterns as 4, http://www.informatik.uni-trier.de/  X  ley/db/ which outputs 9926 closed sequential patterns. We use the One-Pass microclustering algorithm discussed in Section 3 to remove redundancy from those sequential patterns and get a smaller set of 3443 patterns, with  X  =0 . 9 (the average Jaccard distance between these patterns is &gt; 0 . 95).
Table 1 shows the medoids and cluster members of three microclusters generated by the One-Pass microclustering al-gorithm discussed in Section 3, all of which begin with the term  X  X ine X . We see that different variations of the same concept are grouped into the same cluster, although all of them are closed patterns. This successfully reduces the pat-tern redundancy. It is interesting to see that the pattern  X  X ata mine X  and  X  X ine data X  are assigned to different clus-ters, which cannot be achieved by the existing pattern sum-marization techniques such as [21]. The results generated by hierarchical microclustering are similar.

In Table 2, we selectively show the results of semantic pattern annotations. We see that the SPA system can auto-matically generate dictionary-like annotations for different kinds of frequent patterns. For frequent itemsets like co-authorship or single authors, the strongest context indica-tors are usually their other co-authors and discriminative ti-tle terms that appear in their work. The semantically similar patterns extracted also reflect the authors and terms related to their work. However, these SSPs may not even co-occur with the given pattern in a paper. For example, the pattern  X  X iayong wang X ,  X  X iong yang&amp;philip s yu&amp;wei wang X  actu-ally do not co-occur with the pattern  X  X ifeng yan&amp;jiawei han X , but are extracted because their contexts are similar. For a single author, whose context is usually more diverse, the SSPs are more likely to be title terms instead of authors.
We also present the annotations generated for title terms, which are frequent sequential patterns. Their strongest con-text indicators are usually the authors who tend to write them in the titles of their papers, or the terms that tend to co-appear with them. Their SSPs usually provide inter-esting concepts or descriptive terms which are close to their meanings, e.g.  X  X nformation retrieval  X  information filter X ,  X  X query  X  complex language, function query language X .
In both scenarios, the representative transactions extracted give us the titles of papers that well capture the meaning of the given patterns. We only show the title words in Table 2 for each transaction.

These experiments show that the SPA can generate dic-tionary like annotations for frequent patterns effectively. In the following two experiments, we quantitatively evaluate the performance of SPA, by applying it to two interesting tasks.
A challenging and promising research topic in computa-tional biology is to predict the functions for newly discovered proteinmotifs,whichareconservedaminoacidsequence patterns characterizing the function of proteins. To solve this problem, researchers have studied how to match Gene Ontology(GO) terms with motifs [18]. Usually, each protein sequence, which contains a number of motifs, is assigned a set of GO terms that annotate its functions. The goal of the problem is to automatically match each individual mo-tif with GO terms which best represent its functions. In this experiment, we formalize the problem as: Given a set of transactions D (protein sequences with motifs tagged and GO terms assigned), a set P of frequent patterns in D to be annotated (motifs), and a set of candidate patterns P c with explicit semantics (GO terms), our goal is for  X  p  X   X  P , find P
We used the same data set and judgments (i.e., gold stan-dard) as used in [18]. The data has 12181 sequences, 1097 motifs, and 3761 GO terms. We also use the same perfor-mance measure as in [18] (i.e., a variant of Mean recip-rocal rank (MRR) [11], notated as MRR in the following sections for convenience) to evaluate the effectiveness of the SPA technique on the Motif -GO term matching problem.
Let G = { g 1 ,g 2 , ..., g c } be a set of GO terms. Given a motif pattern p  X  , G = { g 1 ,g 2 , ..., g k } X  G is a set of  X  X or-rect X  GO terms for p  X  in our judgement data. We rank G with the SPA system and pick the top ranked terms, where G is treated as either context units or semantically similar patterns to p  X  . This will give us a rank for each g i  X  r ( g i ). MRR (w.r.t. p  X  ) is then computed as where r ( g i )isthe i th correct GO term for p  X  .If g i in the top ranked list, we set 1 /r ( g i ) = 0. We take the average over all the motifs, MRR =1 /m P  X   X  P MRR  X  to measure the overall performance, where m is the number of motifs in our judgement file. Clearly, 0  X  MRR  X  1. Ahigher MRR value indicates a higher precision, and the top-ranked GO terms have the highest influence on MRR , which is intuitively desirable.

If we are ranking the full candidate GO set for annotation, a  X  X azy X  system may either just give them the same rank or rank them randomly. It is easy to show that the expected MRR score for these two cases are the same, which is where | G | is the number of GO terms in G . E [ MRR ]drops monotonously when | G | increases, which indicates the larger the candidate set is, the more difficult is the ranking task. We use this value as the baseline to compare our results. We employ all the motifs and GO terms as context units. Since these patterns are not overlapping with each other, we do not use microclustering to preprocess the context units. We compare the ranking of GO terms either as context in-dicators or as SSPs. We also compare the use of Mutual Information and co-occurrence as strength weight for con-text units. These strategies are compared in Table 3: We see that SPA is quite effective in matching motifs with GO terms, consistently outperforming the baseline. Rank-ing GO terms as context units achieves better results than ranking them as SSPs, which is reasonable because a GO term usually describes only one aspect of a motif X  X  function and is shared by a number of motifs, thus its context is likely quite different from that of a motif.

Interestingly, we notice that although Mutual Information is a better measure in principle, in this specific problem, us-ing MI as the strength weight for context units is not as good as using simple co-occurrence. This may be because there are hardly many GO terms that are globally very common in this dataset, and therefore MI over penalizes the frequent patterns. A detailed discussion on why co-occurrence mea-sure outperforms MI on Motif-GO matching is given in [18].
As discussed in Section 4.3, the algorithm for extracting semantically similar patterns aims at finding patterns whose meaning is very close to the pattern to be annotated. Ideally, they would be synonyms, or thesauri of the given pattern. These patterns may not ever co-occur with the given pattern but tend to have similar contexts, thus cannot be extracted as strong context indicators. We do another experiment to test the performance of SPA on extracting SSPs.

In biomedical literature, it is common that different terms or aliases are used in different studies to denote the same gene, which are known as gene synonyms (see e.g., Table 4). These synonyms generally do not appear together but are  X  X eplaceable X  with each other. Detecting them can help many literature mining tasks. In this experiment, we test the application of SPA to matching gene synonyms.
We construct the synonym list for 100 fly genes, which are randomly selected from the data provided by BioCre-AtIvE Task 1B 3 . Ling et al. collected 22092 abstracts from MEDLINE 4 which contain the keyword  X  X rosophila X  [14]. We extract the sentences from those abstracts which con-tain at least one synonym in the synonym list. Only the synonyms with support  X  3 are kept, which gives us a small set of 41 synonyms. We then mix those synonyms which belong to different genes and use the algorithm of extract-ing SSPs to recover the matching of synonyms. Specifically, given a synonym from the mixed list, we rank all synonyms with the SSP extraction algorithm. The performance of the system is evaluated by comparing the ranked list with the correct synonyms for the same gene. We also use MRR as the evaluation measure. The results are shown as follows.
Table 5: MRR of SPA on gene synonym matching
From Table 5, we see that the SPA algorithm is also ef-fective for matching gene synonyms, which significantly out-performs the random baseline. When using closed sequential patterns as context units, we always achieve better results than using single words (items) as context units, where a higher minimum support (minsup) usually yields better re-sults. When closed sequential patterns are used, further microclustering indeed improves the performance of the sys-tem. However, when the minsup is higher, this improvement http://www.pdg.cnb.uam.es/BioLINK/BioCreative.eval.html http://www.ncbi.nlm.nih.gov/entrez/query.fcgi is decaying. This is reasonable because when the minsup is higher, there is less redundancy among the output closed patterns. Using hierarchical microclustering is slightly bet-ter than using the one-pass algorithm, but not always.
Finally, we discuss the performance of microclustering in removing redundant context units. The effectiveness and ef-ficiency are shown in Figure 1. Both microclustering meth-ods improve the precision (MRR score) when more redun-dant patterns are grouped into clusters. However, when  X  is set too large, the precision decreases. This indicates that we may have over penalized the redundancy and lost useful context units. A good  X  for this task is around 0.8.
Although the cluster quality may not be optimized, the performance of one-pass microclustering is comparable to hi-erarchical microclustering on this task. While in principle, the hierarchical clustering is not efficient, the early termi-nation by using a small  X  saves a lot of time. The one-pass algorithm is more efficient than the hierarchical clustering, and is not affected by  X  . The overhead that both algorithms suffer is the computation of Jaccard distances for all pairs of patterns, i.e., O ( n 2 ) where n is the number of patterns. However, this computation can be coupled in frequent pat-tern mining, as discussed in [20].
To the best of our knowledge, the problem of semantic pat-tern annotation has not been well studied in existing work.
Most frequent pattern mining work [2, 8, 3, 22] focuses on discovering frequent patterns efficiently from the database, and does not address the problem of pattern postprocess-ing. To solve the problem of high redundancy in patterns discovered, closed frequent pattern [15], maximum frequent pattern [16] and top-k closed pattern [9] are proposed to shrink the size of output patterns while keeping the impor-tant ones. However, none of this work provides additional information other than simple statistics to help users inter-pret the frequent patterns. The context information for a pattern tends to be ignored.

Recently, researchers develop new techniques to approx-imate, summarize a frequent pattern set [1, 21], or mine compressed frequent pattern sets [20]. Although they ex-plored some kind of context information, none of the work can provide in-depth semantic annotations for frequent pat-terns as we do in our work. The context model proposed in our work covers both the pattern profile in [21] and trans-action coverage in [20] as special cases.

Context and semantic analysis are quite common in natu-ral language and text processing (see e.g., [17, 5, 13]). Most work, however, deals with non-redundant word-based con-texts, which are quite different from pattern contexts.
In specific domains, people have explored the context of specific data patterns to solve specific problems [18, 14]. Although not optimally tuned, the general techniques pro-posedinourworkcanbewellappliedtothosetasks.
Existing frequent pattern mining work usually generates a huge amount of frequent patterns without providing enough information to interpret the meanings of the patterns. Some recent work introduced postprocessing techniques to sum-marize and compress the pattern set, which shrinks the size of the output set of frequent patterns but does not provide semantic information for patterns.

We propose the novel problem of semantic pattern an-notation (SPA)  X  generating semantic annotations for fre-quent patterns. A semantic annotation consists of a set of strongest context indicators, a set of representative transac-tions, and a set of semantically similar patterns (SSPs) to a given frequent pattern. We define a general vector-space context for a frequent pattern. We propose algorithms to exploit context modeling and semantic analysis to generate semantic annotations automatically. The context modeling and semantic analysis method we presented is quite gen-eral and can deal with any types of frequent patterns with context information. The method can be coupled with any frequent pattern mining techniques as a postprocessing step to facilitate interpretation of the discovered patterns.
We evaluated our approach on three different dataset and tasks. The results show that our methods can generate se-mantic pattern annotations effectively. As shown in our ex-periments, our method can be potentially applied to many interesting real world tasks through selecting different con-text units and focusing on candidate patterns for SSPs.
Although the proposed SPA framework is quite general, in this paper, we only studied some specific instantiation of the framework based on mutual information weighting and cosine similarity measure. A major goal for future research is to fully develop the potential of the proposed framework by studying alternative instantiations. For example, we may explore other options for context unit weighting and seman-tic similarity measurement, the two key components in our framework.
We thank Tao Tao and Xu Ling for providing the datasets of Motif-GO matching and gene synonym matching, respec-tively. This work was in part supported by the National Science Founda tion under award numbers 0425852, 0308215, and 0513678. [1] F. Afrati, A. Gionis, and H. Mannila. Approximating [2] R. Agrawal, T. Imieliski, and A. Swami. Mining [3] R. Agrawal and R. Srikant. Mining sequential [4] S. Brin, R. Motwani, and C. Silverstein. Beyond [5] S.C.Deerwester,S.T.Dumais,T.K.Landauer, [6] M. Deshpande, M. Kuramochi, and G. Karypis.
 [7] G. Grahne and J. Zhu. Efficiently using prefix-trees in [8] J.Han,J.Pei,Y.Yin,andR.Mao.Miningfrequent [9] J.Han,J.Wang,Y.Lu,andP.Tzvetkov.Mining [10] P. Jaccard. Nouvelles recherches sur la distribution [11] P. Kantor and E. Voorhees. The TREC-5 confusion [12] R. Krovetz. Viewing morphology as an inference [13] D. Lin and P. Pantel. Induction of semantic classes [14] X.Ling,J.Jiang,X.He,Q.Mei,C.Zhai,and [15] N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal. [16] J. Roberto J. Bayardo. Efficiently mining long [17] G. Salton, A. Wong, and C. S. Yang. A vector space [18] T. Tao, C. Zhai, X. Lu, and H. Fang. A study of [19] K. Wang, C. Xu, and B. Liu. Clustering transactions [20] D. Xin, J. Han, X. Yan, and H. Cheng. Mining [21] X. Yan, H. Cheng, J. Han, and D. Xin. Summarizing [22] X. Yan and J. Han. gspan: Graph-based substructure [23] X. Yan, J. Han, and R. Afshar. Clospan: Mining
