 Recent research on Recommender Systems, specifically Col-laborative Filtering, has focussed on Matrix Factorization (MF) methods, which have been shown to provide good so-lutions to the cold start problem. However, typically the same settings are used for Matrix factorization regardless of the density of the matrix. In our experiments, we found that for MF, Root Mean Square Error (RMSE) for recom-mendations increases (i.e. performance drops) for sparse matrices. We propose a Two Stage MF approach so MF is run twice over the whole matrix; the first stage uses MF to generate a small percentage of pseudotransactions that are added to the original matrix to increase its density, and the second stage re-runs MF over this denser matrix to predict the user-item transactions in the testing set. We show using data from Movielens that such methods can improve on the performance of MF for sparse matrices.
 Matrix Factorization; Sparse Matrices; Two Stage Matrix Factorization; Pseudotransactions;
Recommender systems has been developed by many re-searchers using different approaches [2], such as content ba-sed [14, 19], collaborative [5], model based [19], and hybrid method [19, 3]. Many open data in various domains (movie: MovieLens[7], Netflix [10]; music: Yahoo Music [9]; joke: Jaster [6], etc.) has been published to accelerate recom-mender systems research. The first winner [10] of the Netflix prize 1 reported that Matrix factorization has many benefits for overcoming problems in recommender systems such as data sparsity and cold start [21].  X 
Computing Science PhD Student at University of Ab-erdeen, UK, Lecturer at Telkom University, Indonesia, email : agungtoto@telkomunivesity.ac.id https://en.wikipedia.org/wiki/Netflix Prize methods can decrease RMSE compared to the standard sin-gle stage MF (  X  4), before finishing with a discussion of how our method relates to previous work, and potential direc-tions for future work (  X  5).
To investigate the performance of MF as a function of matrix density, we ran experiments using a subset of the MovieLens 20M dataset [7] that involved the most active users (interactions frequency  X  1,750) and most active items (interactions frequency  X  5,000). Using those criteria, we obtained a matrix of 361 users, 1,005 items, and 362,805 transactions. 10% of the populated cells were held back as a test dataset. The remaining training matrix was subsampled to provide 19 smaller training sets by with different levels of density. We applied MF (gradient descent [20] 2 with 100 iterations) with different values of k (10, 15, 20) and the results are plotted in Figure 1.
 Figure 1: RMSE performance by matrix density.

In general, Figure 1 shows that the RMSE on unseen test data increases (i.e. performance decreases) when the matrix gets sparcer. The increase in RMSE begins to get steeper when density is below around 30%. This observation leads us to the hypothesis that we can attempt to reduce the RMSE by modifying the MF algorithm to operate in two stages. The first stage is geared towards adding pseudotransactions to increase the density of the matrix to around 30%. The second stage applies MF to the known matrix with added pseudotransactions. We explore one method of generating the pseudotransactions in this paper; there are potentially others, and the strength of the paper is that this idea can be generalised to incorporate numerous other methods, as discussed in  X  5.
Many Recommender Systems operate on sparse matrices (for instance, the density of the transaction matrix for Net-flix Prize is 1.18% dense [10]; MovieLens 10M is 1.31% dense [7]; MovieLens 20M is 0.53% dense [7]; EachMovie is 2.37% dense). From  X  2.1, we can see MF makes better test set predictions when operating on a more dense matrix. The http://www.quuxlabs.com/blog/2010/09/matrix-factorization-a-simple-tutorial-and-implementation-in-python/ tables are the Testing set RMSE using the standard one stage MF. These tables show that the two stage MF outper-forms the single stage MF for a wide variety of parameter settings (marked as yellow cells), but only on more sparse training sets (TS-1, TS-2, and TS-3). The Best RMSE im-provement around 0.01 is achieved in TS-1 (most sparse ma-trix) with 25% additional pseudo transactions. It appears from the tables that lower values of p perform better.
Our results show that the proposed two stage MF works better for most combinations of m and n , and that the in-crease in performance is greater for sparser matrices. Our experiments have only sampled matrices with density of 3% to 15%. In the recommendation system domain, matrices are often more sparse (Netflix Prize is 1.18% dense [10]; MovieLens 10M is 1.31% dense [7]; MovieLens 20M is 0.53% dense [7]; EachMovie is 2.37% dense), and the two stage methods should be explored for even sparcer matrices than used in this study. In this report we used MF using the Gra-dient Descent algorithm. We have no reason to believe the two-stage method will not work for other algorithms (such as Multiplicative [11, 12], Alternating Least Square [4, 1], and more), but this needs to be verified in future work.
In this paper we have explored only one method of gen-erating pseudotransactions. There are related methods that have been reported in the literature, that for instance use hy-brid methods by for example using memory based collabora-tive filtering to add pseudotransactions. Sharaifi et. al. [18] report an RMSE of 1.9346 over the Movielens 100K dataset for MF with a cost funcion over all cells and report lower RMSE for the method by replacing all zero-cells with item median (RMSE 1.1292), user median (RMSE 1.0234) and to-tal median rating (RMSE 0.9973) before applying MF. This cannot be directly compared to ours because they use MF methods that calculate a cost function over all cells (not just known (i.e. non-zero) cells in the data, as we do, which gives better RMSE results for MF). Others explore inputting the missing value using combinations of attribute selection and local learning to optimize the recommendation process [16] or using external resources to identify similarities between users or items (e.g. Facebook pages [17] or demographic data [13]).

There are other multi-stage methods we are keen to ex-plore that restrict themselves to using MF. One example is to use a divide and conquer strategy. If we sort the rows (users) and columns (items) of the matrix by the transac-tion frequency, the top left of the matrix is most dense, and the bottom right least dense. Various divide and conquer approaches can be applied to leverage the denser part of the matrix. Figures 2 shows one of many methods of apply-ing MF iteratively on sub-matrices of different sizes starting from denser matrices at the top or the left, before finally applying MF to the full matrix. Yellow boxes are the tar-get matrices to populate in the current iteration, red boxes are already completed matrices, while thick boxes are the submatrices that we apply MF to in the current iteration. Divide and conquer strategies have in the past been used to speed up MF [15] for the purpose of improving time perfor-mance and should be investigated further for possible RMSE reduction.

We also wish to investigate combining MF with clustering or content based filtering to generate hybrid methods. When clustering or content based filtering is used as a first stage, we can create many small subsets of users and items to create pseudotransactions matrices, before then applying MF in the second stage.

Figure 2: Possible Divide and Conquer Strategy.
I would like to thank Lembaga Pengelola Dana Pendidikan (LPDP), Departemen Keuangan Indonesia in awarding a scholarship to support my financial studies at the Univer-sity of Aberdeen. I also would like to thank my supervisors, Advaith Siddharthan, Chenghua Lin, and Judith Masthoff, for their patient guidance, encouragement and advice to en-hance my works. [1] M. W. Berry, M. Browne, A. N. Langville, V. P. [2] J. Bobadilla, F. Ortega, A. Hernando, and [3] R. Burke. Hybrid recommender systems: Survey and [4] M. Chu, F. Diele, R. Plemmons, and S. Ragni.
 [5] C. Desrosiers and G. Karypis. A comprehensive survey [6] D. Gupta, M. Digiovanni, H. Narita, and K. Goldberg. [7] F. M. Harper and J. A. Konstan. The movielens [8] H. Kim and H. Park. Nonnegative matrix factorization
