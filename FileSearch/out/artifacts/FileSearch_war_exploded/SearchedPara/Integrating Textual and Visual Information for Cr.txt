 Multimedia data has an explosive growth nowadays. Internet, for example, contains millions of images, videos and music. Finding the requesting information from large amount of multimedia data is challenging. Two types of approaches, i.e., con-tent-based and text-based approaches, are usually adopted in image retrieval [8]. Content-based image retrieval (CBIR) uses low-level visual features such as color, texture and shape to represent images. Users can employ example images as que-ries, or directly specify and weight low-level visual features to retrieve images. Im-ages that are visually similar to an example image or contain the specified visual features are returned. In text-based approaches, text is used to describe images and formulate queries. Because images and image representations are in different types of media, media transformation is required. The medium of data collection is transformed from image into text and a text retrieval system is used to index and retrieve images. Textual fea-tures can be derived from the text accompanying with an image such as caption or surrounding text. Text-based approach encounters the following problems. 
Since images are produced by people familiar with their own languages, they can be annotated in different languages. In this way, text-based image retrieval has multi-lingual nature. Besides, images are neutral to different language users. They can re-documents. In such a situation, cross-language image retrieval has attracted research-ers X  attentions recently and is organized as one of evaluation tasks in Cross-Language Evaluation Forum (CLEF) [6]. In addition to media transformation, language transla-tion is also necessary to unify the language usages in queries and documents in cross-language image retrieval. 
Textual and low-level visual features have different semantic levels. Textual fea-ture is highly semantic, while low-level visual feature is less semantic and is rela-tive to human perception. These two types of features are complementary and pro-vide different aspects of information about image. In this paper, we explore the integration of textual and visual information in cross-language image retrieval. An approach that automatically transforms textual queries into visual representations is proposed. The generated visual representation is treated as a visual query to retrieve images. The results using textual and visual queries are combined to generate the final result. 
The rest of this paper is organized as follows. Section 2 introduces the proposed model. The integration of textual and visual information is illustrated. Section 3 mod-els the relationships between text and images. How to generate visual representation of textual query is introduced. Section 4 shows the experiment designs. The selection of suitable textual query terms to construct visual queries is the major concern. Be-sides, three types of experiments, incl uding monolingual image retrieval, cross-language image retrieval and ideal visual queries, are made. Finally, we conclude our work in Section 5. Several hybrid approaches that integrate visual and textual information have been proposed. A simple approach is: conducting text-and content-based retrieval sepa-rately and merging the retrieval results of the two runs [1, 10, 12]. In contrast to the parallel approach, a pipeline approach employs textual or visual information to per-form initial retrieval, and then uses the other features to filter out the irrelevant images [16]. In the above two approaches, users have to issue two types of queries, i.e., tex-specify low-level visual features. 
Figure 1 shows the flow of a cross-language image retrieval system. This system automatically transforms textual queries into visual representations. The generated visual representation is treated as a visual query and is used to retrieve images. First, the relationships between text and images are mined from a set of images annotated with text descriptions. A transmedia dictionary which is similar to bilingual dictionary is set up. When a user issues a textual query, the system automatically transforms the textual query into a visual one using the transmedia dictionary. In this way, we have both textual and visual queries. Given an image collection, two kinds of indices are generated for image retrieval. Comparatively, a visual representation of the textual query retrieves images using the visual index. The retrieval results of textual and generated visual queries are merged together. 
The proposed approach can be applied to monolingual and cross-language image retrieval. In cross-language information retrieval, translation ambiguity and target polysemy problems have to be tackled in the translation process. If a word is not translated correctly, we cannot capture the correct meaning of the word in its context. If the translation is polysemous, the undesired documents that contain the translation could be helpful to reduce these problems. images and text. For an image, each word in its description may relate to a portion of we could link the words to the corresponding parts. This is analogous to word align-image as a language, the textual description and visual parts of an image is an aligned sentence. The correlations between the vocabularies of two languages can be learned from the aligned sentences. Given a picture of sunset, for example, we can link textual feature  X  X unset X  to visual feature  X  X ed circle X 
In automatic annotation task, several approaches have been proposed to model the correlations between text and visual representation, and generate text descriptions from images. Mori, Takahashi and Oka [14] divided images into grids, and then the grids of all images are clustered. Co-occurrence information is used to estimate the probability of each word for each cluster. Duygulu, et al. [7] used blobs to represent images. First, images are segmented into regions using a segmentation algorithm. All regions are clustered and each cluster is assigned a unique label (blob token). EM al-Lavrenko, and Manmatha [9] proposed a cross-media relevance model (CMRM) to learn the joint distribution of blobs and words. They further proposed continuous-space relevance model (CRM) that learned the joint probability of words and regions, rather than blobs [11]. 
This paper considers blobs as a visual representation of images, and adopts Blob-world [3] to segment an image into regions. Blobworld groups pixels in an image into regions which are coherent in low-level properties such as color and texture, and which roughly correspond to objects or part of objects. For each region, a set of fea-tures such as color, texture, shape, position and size are extracted. The regions of all images are clustered into 2,000 clusters by the K-means clustering algorithm accord-ing the extracted features. Each cluster is assigned a unique number, i.e., blob token, and each image is represented by the blob tokens of clusters that its regions belong to. We treat blobs as a language in which each blob token is a word. In this way, we can use text retrieval system to index and retrieve images using blob language. 
Given the textual descriptions and blob tokens of images, we mine the correlation between textual and visual information. Mutual Information (MI) is adopted to meas-ure the strength of correlation between an image blob and a word. Let x be a word and y be an image blob. The Mutual Information of x and y is defined as follows. 
Where p(x) is the occurrence probability of word x in text descriptions, In Formula 1, the probabilities are estimated by maximum likelihood estimation. After the MIs between words and blobs are computed, we can generate related blobs for a given word w i . The blobs whose MI values with w i exceed a threshold are asso-ciated to w i . The generated blobs can be regarded as the visual representation of w i . In this way, a transmedia (word-blob) dictionary is established. 4.1 Test Data and Indexing St. Andrews image collection which was used in ImageCLEF 2004 ad hoc task [6] is adopted to evaluate our system. The image collection consists of 28,133 photographs from St. Andrews University Library X  X  photographic collection, which is one of the largest and most important collections of historic photography in Scotland. The majority of images (82%) in the St. Andrews image collection are in black and white. Sample images are shown in Figure 2. All images are accompanied by a textual description writ-ten in English by librarians working at St. Andrews Library. Figure 3 shows an example of image and its caption in the St. Andrews image collection. The captions are semi-structured and consist of several fields including document number, headline, record id, description text, category, and file names of images. The test set contains 25 topics, and each topic has text description and an example image. The English version of the topics has two fields, i.e., title and narrative. The titles of each topic are translated into several languages. Figure 4 shows topic 2 as an example, which is in Chinese. [15] is used to build both the textual and visual indices. For the textual index, the caption text, &lt;HEADLINE&gt; and &lt;CATEGORIES&gt; fields of English captions are used for index-ing. All words in these fields are stemmed and stopwords are removed. For visual index, the blob tokens of each image are indexed. The weighting function used is BM25. 4.2 Monolingual Image Retrieval We evaluate our approach in monolingual image retrieval at first. The correlations be-tween text and images are learned from St. Andrews image collection. The title field of a topic is used as a query to retrieve images. For each textual query, a visual query is generated from the query terms according to the mined relationships. The first issue to be considered is which query terms are adopted to generate the visual query. Intui-tively, we can generate visual representation for each query term. However, not all query terms are relative to the visual content of images. Here, we employ part-of-speech (POS) to select suitable query terms to generate visual representations. Brill tagger [2] is used to tag English topics. Different types of POSes are explored to tell which types of query terms are useful. Nouns only (without named entities), nouns with named entities, verbs only, adjectives only, or nouns, verbs, and adjectives to-gether are experimented. 
For each selected query term, the top n blobs of MI values with it exceed a thresh-old t are regarded as its visual representation. The values of parameter n from 10 to 40 and t from 0.1 to 0.4 are experimented. The blobs corresponding to the selected query terms form a visual query. It is used to retrieve images using visual index. The results of textual and generated visual queries are merged into the final result. For each im-combined using weights 0.9 and 0.1 for the textual and visual runs, respectively. The top 1000 images of the highest combined scores are reported. 
The performance of the proposed approach is shown in Figure 5. Mean average precision measures the retrieval performances. The approach of nouns only, higher threshold and more blobs has better performance than that of using verbs and adjec-tives. The performances of using verbs or adjectives only in different setting of n and t are similar. This is because there are only a few verbs and adjectives in the topic set, e.g., only 4 adjectives in 4 topics and 9 verbs in 8 topics, and the MI values of blobs with verbs and adjectives tend to be low. When using nouns, verbs and adjectives, the performance is slightly worse than using nouns only. The performance is dropped when name entities are added. It is even worse than using all words with stopword removal (ALL-SW). 
The best performance is 0.6591 when using nouns only, n &gt;20, and t =0.4. Compar-ing to using textual query only, the mean average precision is increased. The perform-show that the proposed approach increases retrieval performance. Although the gener-ated visual queries are not so good enough, the integration of them is useful to improve Andrews image collection are in black and white, that makes image segmentation more difficult. Second, clustering affects the performance of the blobs-based approach. If image regions that are not similar enough are clustered together, the cluster (blob) may have several different meanings. That is analogous to the polysemy problem. 4.3 Cross-Language Image Retrieval In the experiments of cross-language image retrieval, Chinese queries are used as source queries and translated into English to retrieve English captions of images. First, the Chinese queries are segmented by a word recognition system and tagged by a POS tagger. Named entities are identified by Chinese NER tools [4]. For each Chi-nese query term, we find its translation equivalents using a Chinese-English bilingual with the highest frequency of occurrence in the English image captions are considered as the target language query terms. 
For those named entities that are not included in the dictionary, a similarity-based backward transliteration scheme [13] is adopted. First, transformation rules [5] tell out the name and the keyword parts of a named entity. The keyword parts are general nouns, and are translated by dictionary lookup as described above. The name parts, which are transliterations of foreign names, are transliterated into English using simi-larity-based backward transliteration. Total 3,599 English names from the image cap-tions are extracted. Given a transliterated name, 300 candidate names are selected from the 3,599 names using an IR-based candidate filter [12]. We transform the trans-literated name and candidate names to International Phonetic Alphabet (IPA), and compute the similarities between IPA representations of the transliterated name and candidate names. The top 6 candidate names with the highest similarity are chosen as the original names. 
Visual queries are generated from Chinese queries. In order to learn the correla-tions between Chinese words and blob tokens, image captions are translated into Chi-nese by SYSTRAN (http://www.systransoft.com/) system. Similarly, POS selects and t from 0.01 to 0.04 are experimented. Figure 6 shows that the performances of term selection strategies are similar to that of monolingual image retrieval. Using nouns only to generate visual query has better performance than using verbs and ad-jectives only. When n&gt;30, using nouns, verbs and adjectives together performs better than using nouns only. The best performance is 0.4441 when using nouns, verbs and adjectives, n&gt;30, and t=0.02. The performances of textual query and generated visual query are shown in Table 2. In cross-language experiment, the improvement of re-trieval performance is not as well as monolingual experiment. One of the reasons is that the quality of training data is not good. We use a famous machine translation sys-affect the correctness of learned correlations. 
The performance of generated visual query is not so good enough. One of the rea-sons is that we use only a part of query terms to generate visual query, thus some in-formation is lost. In some topics, the retrieved images are not relevant to the topics, while they are relevant to the query terms that are used to generate visual query. Take Topic 13, i.e., 1939 (The Open Championship golf tournament, St. Andrews 1939), as an example.  X   X  (St),  X  (golf) and  X  (Open Championship) are tagged as nouns, thus are selected to gen-erate visual query. In the top 10 images shown in Figure 7, 9 images are about the Open Championship golf tournament, but are not the one held in 1939. It shows that using visual information only is not enough, integrating textual information is needed. 4.4 Ideal Visual Query Since the performance of generated visual query depends on image segmentations, blob clustering, and so on, we create an ideal query from relevant images to test if a visual query can help increase the performance of image retrieval. A useful visual query will exist if the relevant images for a query share some image features. The common image features can help us retrieve the relevant images well. We use x 2 score to select blobs from relevant images. For each query we generate 10 blobs whose x 2 scores are larger than 7.88 ( v =1, p =0.005). The selected blobs form a visual query to retrieve images. The retrieval result is combined with that of a textual query. The per-formances are shown in Table 3. The results show that a good visual query can im-prove performance of image retrieval. This paper explores the uses of both textual information and visual features to cross-language image retrieval. We conduct English monolingual and Chinese-English cross-language retrieval experiments to evaluate our approach. Experimental results show that combining retrieval results of textual and generated visual query improves retrieval performance. The generated visual query has little impact in the cross-lingual experiments. One of the reasons is that using machine translation system to translate English captions into Chinese introduces many translation errors that affect the cor-rectness of learned correlations. We also construct an ideal visual query from relevant images. Using the ideal visual query increases retrieval performance about 12.3% and 8.8% in monolingual and cross-language image retrieval, respectively. The results show that a good visual query can improve performance of image retrieval. 
We use POS to select query terms for constructing a visual query. Experiments show that nouns are appropriate to generate visual queries, while using named entities is helpless. Nouns usually indicate the objects in images, which is the kernel of an im-age, thus it is reasonable to link nouns to the image regions which correspond to ob-jects. Named entities, such as person name, location name and date, do not have strong relations with image regions, and cannot be represented well by visual repre-sentations. In this way, the visual representations of named entities introduce noise and decrease the retrieval performance. Similarly, verbs that indicate actions are hardly represented by visual features. Thus, verbs are not feasible for visual query generation. Some adjectives that are relative to visual features could be used to gener-ate visual queries. For example, red is relative to color, a low-level visual feature. In the experiments, we use syntactic information to select query terms. Semantic infor-mation which may provide more clues for term selection is not used. We will investi-gate query term selection on semantic level in the future. Acknowledgement. Research of this paper was partially supported by National Sci-ence Council, Taiwan, under the contracts NSC93-2752-E-001-001-PAE and NSC94-2752-E-001-001-PAE. 
