 Almost all popular media players, like Windows Media Player and RealPlayer, have beautiful convenient user interfaces and they provide slider bars for users to locate to any time point of the video clip that is playing. However, users couldn X  X  manage to find the episodes they are interested in quickly, for the conventional slider bar does not provide any information about the episodes or events except the time. 
There are all kinds of annotations applie d to various researching areas: Qian, etc [1] integrated photo annotating tasks into instant messaging applications; Abowd, etc [2] provided a means for archiving and annota ting large collections of informal family movies; Ramos, etc [3] explore a variety of interaction and visualization techniques for fluid navigation, segmentation, linking, and annotation of digital videos; Costa, etc [4] implemented a VAnnotator which allowed user to annotate audiovisual content using a timeline model. People use annotations. 
Emotion is usually associated with the behaviors of human being. People convey and change it along with spoken languages, manners or facial expressions. The con-tent of various kinds of video materials has tight association with the emotion of roles. Hugo Liu, etc [5] established a textual affect sensing engine and annotated the sen-tences in a text document with emotions. Th ey used colors to represent emotions and sequenced them into a color bar, which represented the progression of affect through a text document. Experiments were set up to evaluate their method. The results dem-onstrated that it facilitated the within-document information foraging activity. 
In this paper, we propose an affective annotation method and implemented it in an affectively annotated media player. In addition, two experiments were set up to evalu-ate the proposed method. The first experiment tries to figure out whether there is any improvement in the speed while locating to an important scene with the help of the affective annotation method. The other one tries to examine how well the user would interpret the scenario of the movie clip after watching it in restricted time. There is such a large vocabulary we use to describe emotion but no one can tell that how many categories all emotions can be divided into. The metrics about emotion can be instrumentally measured from speeches, face expressions or any other way people convey emotion, but we still describe emotion and interpret it in impressionistic terms, which may be highly subjective [7]. Different methodologies yield different classifications of emotional states, researchers and experts keep exploring and have proposed many different schemes [8, 9]. As no clear standard has been set, in this paper, we group human affect into five categories: happiness, sadness, fear, anger and neutral affect. 
Just like affective classification, there is a large body of literature on the psychol-ogy of color [6], and results of researches vary largely from different situations. Though no color encoding can claim to be completely intuitive [5], color has its pre-dominance for presentation on user interface: simple and clear. We use a color bar along with a slider bar to annotate roles X  affect at different time points in a movie (or other media). Fig. 1a shows an example of the color bar with affective annotation. Color blocks on it represent various emotions. And they span the time the particular emotions occurs. Yellow blocks on it represents happiness, while red, blue, green and brown stand for anger, sadness, fear and neutral affect, referencing [10], in the im-plementation of color bar. In addition, gaps between those color blocks represent absence of corresponding role. On the other hand, Fig.1b shows another color bar which only identifies the role X  X  presence (represented with brown). These two condi-tions are to be compared in the experiments to evaluate the effect of affective annota-tion. The thumb of the slider bar slides beneath the color bar. Like the conventional slider bar, it selects and tells time position of the movie (or other media). What X  X  time and any time! 
EmoPlayer is an experimental platform which implements the color bar based af-fective annotation method. As shown in Fig. 3, it has the combo box select color bars between different roles. The affective information is saved in an XML file and it X  X  organized in the format as Fig. 2. In order to evaluate the color bar based affective annotation method for video clips, two experiments were set up. The first experiment tries to figure out whether there was any improvement in the speed while locat ing to an important scene with the help of the affective annotation. The other one tries to examine how well the user would interpret the story of the movie clip after watching it in restricted time. 
Three movie clips were hunted for the experiments. The general principle for choosing movie clips was that they should be various, changeful and expressed straightforwardly in emotion. One of the three clips was used for practicing before the experiments while the other two were used for experiment 1 and 2, respectively. The latter two for formal experiments were dubbed in Chinese. In order to annotate the video clips selected for the experiment, a three-person group was set up. The mem-bers are sophisticated with emotion expression. They did the annotating job and ac-cording to the subjective evaluation rated by the subjects after experiments, it was considered accurate. 
Before the beginning of the experiments, the interface was introduced and every operation was explained explicitly to each subject. After that, they were given suffi-cient time to get familiar with EmoPlayer, including how the colors represented the emotions, which color corresponded to which emotion, how to use the slider bar and get the current emotional information and so on. The subjects were told to work as quickly as possible. 3.1 Experiment 1 In this experiment, a 2 (using affective annotation or not)  X  2 (familiar or not) be-tween group design has been employed. Five target events were selected from the movie clip before the experiment. A region lasting about 7 seconds around an event was marked as acceptable region. When the thumb of the slider bar got into this re-gion, the subject would be notified that he/she had reached the target event. The mean TCTs (Task Completion Time) were studied in this experiment. 24 subjects were recruited and each of them was presented a movie clip and asked to locate to five target events, one after another. Before each event, he/she was given lovers start to dance X . When he/she locat ed to a time point within the acceptable re-gion of the event, he/she would be notified and asked to prepare for the next event at the same time, until all of the five events were found out. The subjects were asked to operate as quickly as possible at every event. 
Fig. 4 shows the mean TCT over various conditions. The results of ANOVA show significant effect of affective annotation, F(1, 20) = 17.313, p&lt;0.001; significant ef-fect of familiarity, F(1, 20) = 66.138, p&lt;0.001. From above we conclude that the color bar based affective annotation method significantly improves the efficiency while event-locating was improved by 18.4% for the subjects who were familiar with the material, while it was improved by 35.3% for the unfamiliar subjects. We saw that the on their knowledge of the order of events of the story. 3.2 Experiment 2 In this experiment we were trying to figure out how well the subjects could interpret the movie clip in restricted time and if it could yield better result under the help of the color bar based affective annotation. A 2 (using affective annotation or not)  X  3 (time limit) between group design has been employed in this experiment. Like experiment 1, half of the subjects had the affective annotation in the interface while half not. None of the subjects were familiar with the movie clip selected for the experiment. movie clip time to finish watching it. We name them as  X  X hort watching time X ,  X  X e-dium watching time X ,  X  X ong watching time X  users, respectively. 24 subjects were recruited, they were asked to watch the movie clip in certain time limit. They were told to pay attention to the scenarios and details and make their own strategy to drag the thumb of the slider bar to make full use of time. After that, every-one was presented a questionnaire with 10 questions on it which were all about the scenario of the movie clip. Most of the questions were simple and subjects could answer them without any doubt if they had watched the part. After that, subjects were asked to give subjective evaluations to three 5-point scale questions. 
Fig. 5 shows the mean scores the subjects got in this experiment under different conditions. It can be seen from the chart that the mean scores of  X  X edium watching time X  are much higher than that of  X  X hort watching time X . Under the  X  X hort watching time X  condition, the mean score of subjects using the affective annotation is even a bit less than the ones not using it, which implies that no improvement or even negative effect of using the affective annotation would be yielded if the watching time assigned annotation is expected, the watching time assigned should be at least more than 1/3 of 2/3 of full time and without affective annotation got only an average of around 6.0 points. This is a relatively low score, lower than the time ratio: 2/3. Meanwhile, with affective annotation, the mean score of  X  X ong watching time X  subjects ascends to around 7.5, which is almost one point above the time ratio. We recorded the procedure of the experiments with a screen capturing software. In ex-slider bar within the segments of one certain color, which is always related with the event, until the scene in the playing window matched the target event they search for. Then they made several tiny adjustments to reach the acceptable region. Concluding from this, they has. In this way, they could cut the searching area into much smaller pieces and find the target much quicker. As for the subjects without affective annotation, they had to drag the thumb of the slider bar back and forth among the brown segments. 
In experiment 2, we found that under the condition without the affective annota-tion, many subjects tended to keep draggi ng the thumb of the slider bar forward in some certain offset. The offset was sometimes adjusted depending on whether they were interested in the current scene. They seldom drag the thumb back during this procedure. Generally, subjects just paid much attention on the screen and not seemed to care about the color bar. Some of them did use the combo box to switch roles, but they seemed not quite follow the color bar. As mentioned above, subjects without affective annotation didn X  X  have a clue to organize the time to watch the movie clip. When dragging to forward offsets, they were not quite sure what to occur next. Be-sides, how to set the offset was such a headache. What X  X  worse, when a subject watched to the end of the clip but there was still some time remaining, he/she proba-bly had to drag the thumb back and continue watching it for the second time, during which there must be reduplicative watch and still bad time-organizing. This might be an explanation of why the mean scores were so close to each other among the three different watching time conditions, demonstrated in Fig. 5, without affective annota-tion. On the other hand, when it was affectively annotated, we saw many subjects go through the affectively annotated segments of one role, and then switched to another role. They told us after experiments that they preferred this way to make each role X  X  story clear. From above we noticed that th e behaviors varied from different condi-tions. The affective annotation really offered a new means for the users to watch a movie, of which the efficiency was evaluated by the experiment we just discussed. In this paper, we proposed a color bar based affective annotation method and imple-mented it in EmoPlayer. In addition, two experiments were set up to evaluate the method, and the results indicated that the subjects could find certain events faster and understand the scenarios better under the he lp of affective annotation than the case affective annotation was not used. 
