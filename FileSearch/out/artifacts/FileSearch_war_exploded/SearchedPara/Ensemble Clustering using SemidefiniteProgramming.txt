 In the so-called Ensemble Clustering problem, the target is to  X  X ombine X  multiple clustering solu-tions or partitions of a set into a single consolidated clustering that maximizes the information shared (or  X  X greement X ) among all available clustering solutions. The need for this form of clustering arises in many applications, especially real world scenarios with a high degree of uncertainty such as image segmentation with poor signal to noise ratio and computer assisted disease diagnosis. It is quite com-mon that a single clustering algorithm may not yield satisfactory results, while multiple algorithms may individually make imperfect choices, assigning some elements to wrong clusters. Usually, by considering the results of several different clustering algorithms together , one may be able to miti-gate degeneracies in individual solutions and consequently obtain better solutions. The idea has been employed successfully for microarray data classification analysis [1], computer assisted diagnosis of diseases [2] and in a number of other applications [3].
 Formally, given a data set D = { d 1 , d 2 , . . . , d n } , a set of clustering solutions C = { C 1 , C 2 , . . . , C m } obtained from m different clustering algorithms is called a cluster ensemble . Each solution, C i , is the partition of the data into at most k different clusters. The Ensemble Clus-tering problem requires one to use the individual solutions in C to partition D into k clusters such that information shared ( X  X greement X ) among the solutions of C is maximized. 1.1 Previous works The Ensemble Clustering problem was recently introduced by Strehl and Ghosh [3], in [4] a related notion of correlation clustering was independently proposed by Bansal, Blum, and Chawla. The problem has attracted a fair amount of attention and a number of interesting techniques have been proposed [3, 2, 5, 6], also see [7, 4]. Formulations primarily differ in how the objective of shared information maximization or agreement is chosen, we review some of the popular techniques next. The Instance Based Graph Formulation (IBGF) [2, 5] first constructs a fully connected graph G = The edge weight w ij for the pair ( d i , d j ) is defined as the number of algorithms in C that assign the nodes d i and d j to the same cluster (i.e., w ij measures the togetherness frequency of d i and d ). Then, standard graph partitioning techniques are used to obtain a final clustering solution. In Cluster Based Graph Formulation (CBGF), a given cluster ensemble is represented as C = Like IBGF, this approach also constructs a graph, G = ( V, W ) , to model the correspondence (or  X  X imilarity X ) relationship among the mk clusters, where the similarity matrix W reflects the Jaccard X  X  similarity measure between clusters  X  C i and  X  C j . The graph is then partitioned so that the clusters of the same group are similar to one another. Variants of the problem have also received considerable attention in the theoretical computer science and machine learning communities. A recent paper by Ailon, Charikar, and Newman [7] demonstrated connections to other well known problems such as Rank Aggregation, their algorithm is simple and obtains an expected constant approximation guarantee (via linear programming duality). In addition to [7], other results include [4, 8]. A commonality of existing algorithms for Ensemble Clustering [3, 2, 9] is that they employ a graph construction, as a first step. Element pairs (cluster pairs or item pairs) are then evaluated and their edges are assigned a weight that reflects their similarity. A natural question relates to whether we can find a better representation of the available information. This will be the focus of the next section. Consider an example where one is  X  X ggregating X  recommendations made by a group of family and friends for dinner table seating assignments at a wedding. The hosts would like each  X  X able X  to be able to find a common topic of dinner conversation. Now, consider three persons, Tom, Dick, and Harry invited to this reception. Tom and Dick share a common interest in Shakespeare, Dick and Harry are both surfboard enthusiasts, and Harry and Tom attended college together. Because they had strong pairwise similarities, they were seated together but had a rather dull evening. A simple analysis shows that the three guests had strong common interests when considered two at a time, but there was weak communion as a group. The connection of this example to the ensemble clustering problem is clear. Existing algorithms represent the similarity between elements in D as a scalar value assigned to the edge joining their corresponding nodes in the graph. This weight is essentially a  X  X ote X  reflecting the number of algorithms that assigned those two elements to the same cluster. The mechanism seems perfect until we ask if strong pairwise coupling necessarily implies coupling for a larger group as well. The weight metric considering two elements does not retain information about which algorithms assigned them together. When expanding the group to include more elements, one is not sure if a common feature exists under which the larger group is similar. It seems natural to assign a higher priority to triples or larger groups of people that were recommended to be seated together (must be similar under at least one feature) compared to groups that were never assigned to the same table by any person in the recommendation group (clustering algorithm), notwithstanding pairwise evaluations, for an illustrative example see [10]. While this problem seems to be a distinctive disadvantage for only the IBGF approach; it also affects the CBGF approach. This can be seen by looking at clusters as items and the Jaccard X  X  similarity measure as the vote (weight) on the edges. To model the intuition above, we generalize the similarity metric to maximize similarity or  X  X gree-ment X  by an appropriate encoding of the solutions obtained from individual clustering algorithms. More precisely, in our generalization the similarity is no longer just a scalar value but a 2 D string. The ensemble clustering problem thus reduces to a form of string clustering problem where our objective is to assign similar strings to the same cluster.
 The encoding into a string is done as follows. The data item set is given as D with | D | = n . Let m be the number of clustering algorithms with each solution having no more than k clusters. We represent all input information (ensemble) as a single 3 D matrix, A  X &lt; n  X  m  X  k . For every data element d l  X  D , A l  X &lt; m  X  k is a matrix whose elements are defined by
It is easy to see that the summation of every row of A l equals 1 . We call each A l an A -string. Our goal is to cluster the elements D = { d 1 , d 2 , . . . , d n } based on the similarity of their A -strings. We now consider how to compute the clusters based on the similarity (or dissimilarity) of strings. We note that the paper [11] by Gasieniec et al., discussed the so-called Hamming radius p -clustering and Hamming diameter p -clustering problems on strings. Though their results shed considerable light on the hardness of string clustering with the selected distance measures, those techniques cannot be directly applied to the problem at hand because the objective here is fairly different from the one in [11]. Fortunately, our analysis reveals that a simpler objective is sufficient to capture the essence of similarity maximization in clusters using certain special properties of the A -strings.
 Our approach is partly inspired by the classical k -means clustering where all data points are assigned to the cluster based on the shortest distance to the cluster center. Imagine an ideal input instance for the ensemble clustering problem (all clustering algorithms behave similarly)  X  one with only k unique members among n A -strings. The partitioning simply assigns similar strings to the same partition. The representative for each cluster will then be exactly like its members, is a valid A -string, and can be viewed as a center in a geometric sense. General input instances will obviously be non-ideal and are likely to contain far more than k unique members. Naturally, the centers of the clusters will vary from its members. This variation can be thought of as noise or disagreement within the clusters, our objective is to find a set of clusters (and centers) such that the noise is minimized and we move very close to the ideal case. To model this, we consider the centers to be in the same high dimensional space as the A -strings in D (though it may not belong to D ). Consider an example where a cluster i in this optimal solution contains items ( d 1 , d 2 , . . . , d 7 ) . A certain algorithm C j would C j behave if evaluating the center of cluster i as a data item? The probability it assigns the center to cluster s is 4 / 7 and the probability it assigns the center to cluster p is 3 / 7 . If we emulate this logic  X  we must pick the choice with the higher probability and assign the center to such a cluster. It can be verified that this choice minimizes the dissent of all items in cluster i to the center. The A -string for the center of cluster i will have a  X  1  X  at position ( j, s ) . The assignment of A -string (items) to clusters is unknown; however, if it were somehow known, we could find the centers for all other clusters i  X  [1 , k ] by computing the average value at every cell of the A matrices corresponding to the members of the cluster and rounding the largest value in every row to 1 (rest to 0 ) and assigning this as the cluster center. Hence, the dissent within a cluster can be quantified simply by averaging the matrices of elements that belong to the cluster and computing the difference to the center. Our goal is to find such an assignment and group the A -strings so that the sum of the absolute differences of the averages of clusters to their centers (dissent) is minimized. In the subsequent sections, we will introduce our optimization framework for ensemble clustering based on these ideas. We start with a discussion of an Integer Program (IP, for short) formulation for ensemble clustering. For convenience, we denote the final clustering solution by C  X  = { C  X  1 , . . . , C  X  k } and C ij denotes the cluster i by the algorithm j . The variables that constitute the IP are as follows.
 We mention that the above definition implies that for a fixed index i 0 , its center, s iji 0 also provides an indicator to the cluster most similar to C  X  i 0 in the set of clusters produced by the clustering algorithm C . We are now ready to introduce the following IP. (4) minimizes the sum of the difference between s iji 0 (the center for cluster C  X  i 0 ) and the average of most similar cluster to C  X  i 0 among all the clusters produced by algorithm C j . Hence, if s iji 0 = 0 in C  X  i 0 that do not consent with the majority of the other elements in the group w.r.t. the clustering solution provided by C j . In other words, we are trying to minimize the dissent and maximize the consent simultaneously. The remaining constraints are relatively simple  X  (5) enforces the condition that a data element should belong to precisely one cluster in the final solution and that every cluster must have size at least 1 ; (6) ensures that s iji 0 is an appropriate A -string for every cluster center. The formulation given by (4)-(6) is a mixed integer program (MIP, for short) with a nonlinear ob-jective function in (4). Solving this model optimally, however, is extremely challenging  X  (a) the constraints in (5)-(6) are discrete; (b) the objective is nonlinear and nonconvex. One possible way of attacking the problem is to  X  X elax X  it to some polynomially solvable problems such as SDP (the prob-lem of minimizing a linear function over the intersection of a polyhedron and the cone of symmetric and positive semidefinite matrices, see [12] for an introduction). Our effort would be to convert the nonlinear form in (4) into a 0 -1 SDP form. By introducing artificial variables, we rewrite (4) as where the term c iji 0 represents the second term in (4) defined by Since both A lij and X li 0 are binary, (9) can be rewritten as Let us introduce a matrix variable y i 0  X &lt; n whose l th column is defined by Let A ij  X &lt; n be a vector whose l th element has value A l ( i, j ) . This allows us to represent (10) as where B ij = diag( A ij ) is a diagonal matrix with ( B ij ) ll = A l ( i, j ) , the second and third properties follow from Z i 0 = y i 0 y T i 0 being a positive semidefinite matrix. Now, we rewrite the constraints for X in terms of Z . (5) is automatically satisfied by the following constraints on the elements of Z i 0 . where Z ( uv ) i 0 refers to the ( u, v ) entry of matrix Z i 0 . Since Z 0 i is a symmetric projection matrix by construction, (7)-(13) constitute a precisely defined 0 -1 SDP that can be expressed in trace form as where Q i 0 ( i, j ) = c iji 0 = tr( B ij Z i 0 ) , and e n  X &lt; n is a vector of all 1 s . The experimental results for this model indicate that it performs very well in practice (see [10]). However, because we must solve the model while maintaining the requirement that S i 0 be binary (otherwise, the problem becomes ill-posed), a branch and bound type method is needed. Such approaches are widely used in many application areas, but its worst case complexity is exponential in the input data size. In the subsequent sections, we will make several changes to this framework based on additional observations in order to obtain a polynomial algorithm for the problem. Recall the definition of the variables c iji 0 , which can be interpreted as the size of the overlap between Let us also define vector variables q ji 0 whose i th element is s iji 0  X  c iji 0 . In the IP model 1 , we try to minimize the sum of all the L 1 -norms of q ji 0 . The main difficulty in the previous formulation stems from the fact that c iji 0 is a fractional function w.r.t the assignment matrix X . Fortunately, we of squares is maximized when its largest entry is as high as possible. Thus, minimizing the function 1  X  P k i =1 ( c iji 0 ) 2 is a reasonable substitute to minimizing the sum of the L 1 -norms in the IP model 1. The primary advantage of this observation is that we do not need to know the  X  X ndex X  ( i  X  ) of the maximal element c i  X  ji 0 . As before, X denotes the assignment matrix. We no longer need the variable s , as it can be easily determined from the solution. This yields the following IP. We next discuss how to transform the above problem to a 0 -1 SDP. For this, we first note that the objective function (18) can be expressed as follows. which can be equivalently stated as The numerator of the second term above can be rewritten as where X 0 i is the i 0 th column vector of X . Therefore, the second term of (21) can be written as Since each matrix Z i 0 is a symmetric projection matrix and X i 0 when i 0 1 6 = i 0 2 , Z is a projection matrix of the form X ( X T X )  X  1 X . The last fact also used in [13] is originally attributed to an anonymous referee in [14]. Finally, we derive the 0 -1 SDP formulation for the problem (18)-(19) as follows. Relaxing and Solving the 0 -1 SDP: The relaxation to (24)-(26) exploits the fact that Z is a projec-tion matrix satisfying Z 2 = Z . This allows replacing the last three constraints in (26) as I Z 0 . By establishing the result that any feasible solution to the second formulation of 0 -1 SDP, Z feas is a rank k matrix, we first solve the relaxed SDP using SeDuMi [15], take the rank k projection of Z  X  and then adopt a rounding based on a variant of the winner-takes-all approach to obtain a solution in polynomial time. For the technical details and their proofs, please refer to [10]. Our experiments included evaluations on several classification datasets, segmentation databases and simulations. Due to space limitations, we provide a brief summary here. Our first set of exper-iments illustrates an application to several datasets from the UCI Machine Learning Repository: (1) Iris dataset, (2) Soybean dataset and (3) Wine dataset; these include ground truth data, see http://www.ics.uci.edu/ mlearn/MLRepository.html. To create the ensemble, we used a set of [4 , 10] clustering schemes (by varying the clustering criterion and/or algorithm) from the CLUTO clustering toolkit. The multiple solutions comprised the input ensemble, our model was then used to determine a agreement maximizing solution. The ground-truth data was used at this stage to evaluate accuracy of the ensemble (and individual schemes). The results are shown in Figure 1(a)-(c). For each case, we can see that the ensemble clustering solution is at least as good as the best clustering algorithm. Observe, however, that while such results are expected for this and many other datasets (see [3]), the consensus solution may not always be superior to the  X  X est X  clustering solution. For instance, in Fig. 1(c) (for m = 7 ) the best solution has a marginally lower error rate than the ensemble. An ensemble solution is useful because we do not know a priori that which algorithm will perform the best (especially if ground truth is unavailable). Figure 1: Synergy. The fraction of mislabeled cases ( [0 , 1] ) in a consensus solution (  X  ) is com-pared to the number of mislabelled cases (  X  ) in individual clustering algorithms. We illustrate the ensemble effect for the Iris dataset in (a), the Soybean dataset in (b), and the Wine dataset in (c). Our second set of experiments focuses on a novel application of ensembles to the problem of image segmentation. Even sophisticated segmentation algorithms may yield  X  X ifferent X  results on the same image, when multiple segmentations are available, it seems reasonable to  X  X ombine X  segmentations to reduce degeneracies. Our experimental results indicate that in many cases, we can obtain a better overall segmentation that captures (more) details in the images more accurately with fewer outlying clusters. In Fig. 2, we illustrate the results on an image from the Berkeley dataset. The segmenta-tions were generated using several powerful algorithms including (a) Normalized Cuts, (b) Energy Minimization by Graph Cuts and (c) X (d) Curve Evolution. Notice that each algorithm performs well but misses out on some details. For instance, (a) and (d) do not segment the eyes; (b) does well in segmenting the shirt collar region but can only recognize one of the eyes and (c) creates an additional cut across the forehead. The ensemble (extreme right) is able to segment these details (eyes, shirt collar and cap) nicely by combining (a) X (d). For implementation details of the algorithm including settings, preprocessing and additional evaluations, please refer to [10]. Figure 2: A segmentation ensemble on an image from the Berkeley Segmentation dataset. (a) X (d) show the individual segmentations overlaid on the input image, the right-most image shows the segmentation generated from ensemble clustering.
 The final set of our experiments were performed on 500 runs of artificially generated cluster ensem-bles. We first constructed an initial set segmentation, this was then repeatedly permuted (up to 15% ) yielding a set of clustering solutions. The solutions from our model and [3] were compared w.r.t. our objective functions and Normalized Mutual Information used in [3]. In Figure 3(a), we see that our algorithm (Model 1 ) outperforms [3] on all instances. In the average case, the ratio is slightly more than 1 . 5 . We must note the time-quality trade-off because solving Model 1 requires a branch-and-bound approach. In Fig. 3(b), we compare the results of [3] with solutions from the relaxed SDP Model 2 on (24). We can see that our model performs better in  X  95% cases. Finally, Figure 1(b) shows a comparison of relaxed SDP Model 2 with [3] on the objective function optimized in [3] (using best among two techniques). We observed that our solutions achieve superior results in 80% of the cases. The results show that even empirically our objective functions model similarity rather well, and that Normalized Mutual Information may be implicitly optimized within this framework. Remarks. We note that the graph partitioning methods used in [3] are typically much faster than the time needed by SDP solvers (e.g., SeDuMi [15] and SDPT3) for comparable problem sizes. How-ever, given the increasing interest in SDP in the last few years, we may expect the development of new algorithms, and faster/more efficient software tools. We have proposed a new algorithm for ensemble clustering based on a SDP formulation. Among the important contributions of this paper is, we believe, the observation that the notion of agreement in an ensemble can be captured better using string encoding rather than a voting strategy. While a partition problem defined directly on such strings yields a non-linear optimization problem, we illustrate a transformation into a strict 0 -1 SDP via novel convexification techniques. The last result of this paper is the design of a modified model of the SDP based on additional observations on the structure of the underlying matrices. We discuss extensive experimental evaluations on simulations and real datasets, in addition, we illustrate application of the algorithm for segmentation ensembles. We feel that the latter application is of independent research interest; to the best of our knowledge, this is the first algorithmic treatment of generating segmentation ensembles for improving accuracy. Figure 3: A comparison of [3] with SDP Model 1 in (a), and with SDP Model 2 on (24) in (b). The solution from [3] was used as the numerator. In (c), comparisons (difference in normalized values) between our solution and the best among IBGF and CBGF based on the Normalized Mutual Information (NMI) objective function used in [3].
 Acknowledgments. This work was supported in part by NSF grants CCF-0546509 and IIS-0713489. The first author was also supported by start-up funds from the Dept. of Biostatistics and Medical Informatics, UW  X  Madison. We thank D. Sivakumar for useful discussions, Johan L  X  ofberg for a thorough explanation of the salient features of Yalmip [16], and the reviewers for sug-gestions regarding the presentation of the paper. One of the reviewers also pointed out a typo in the derivations in  X  6.

