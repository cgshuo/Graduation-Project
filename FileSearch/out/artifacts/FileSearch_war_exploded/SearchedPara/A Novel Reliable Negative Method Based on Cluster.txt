 manageable and easy to understand. Text categorization or classification aims to negative examples are mandatory for machine learning. The main bottleneck of build-numerous unlabeled examples are easily available. 
Recently, semi-supervised learning algorithms from a small set of labeled data with the help of unlabeled data have been defined. These techniques alleviate some labor-intensive effort. Semi-supervised learning includes two main paradigms: (1) learning learning from positive examples and unlabeled examples (with no labeled negative examples). Many researchers have studied lear ning in the first paradigm [1]. In learn-ing from positive and unlabeled examples, some theoretical studies and practical algorithms have been reported in [2-9]. 
In this paper, we study learning from positive data with the help of unlabeled data,  X  X nteresting X  for a specific user. Documents pointed by the user X  X  bookmarks defined easily available on the World Wide Web. is sometimes sufficient to consider unlabeled data as negative ones [2-3]. Recently, a few algorithms were proposed to solve the problem. One class of algorithms is based on a two-step strategy as follow. These algorithms include Roc-SVM [7], S-EM [8], PEBL (Positive Examples Based Learning) [9]. 
Step 1: Identifying a set of reliable negative documents from the unlabeled set. In this step, S-EM uses a Spy technique, PEBL uses a technique called 1-DNF, and Roc-SVM uses the Rocchio algorithm. Expectation Maximization (EM) algorithm wi th a NB (Naive Bayesian) classifier, while PEBL and Roc-SVM use SVM (Support Vector Machine). Both S-EM and Roc-SVM have some methods for selecting the final classifier. PEBL simply uses the last classifier at convergence. 
The underlying idea of these two-step strategies is to iteratively increase the num-ber of unlabeled examples that are classified as negative while maintaining the posi-tive examples correctly classified. This idea ha s been justified to be effective for this problem [8]. positive and unlabeled examples to identify the reliable negative document, and evaluate our method with other two methods (PEBL, and Roc-SVM). clustering based approach in section 3; and comparative experiments have been made in section 4; finally make conclusion in section 5. strategy. The techniques of the Roc-SVM, the S-EM and the PEBL have been re-ported in [7], [8], [9] respectively. 
In this paper, we use P to denote the positive examples set, U for unlabeled exam-ples set, and RN for reliable negative examples set that produced from the unlabeled examples set U . 
Li, X.L. et al. report the Spy technique in the S-EM [7]. It first randomly selects a set S of positive documents from P and put them in U . Documents in S act as  X  X py X  documents. The spies behave similarly to the unknown positive documents in U . amples set is large, it is worse than others. 
The Roc-SVM algorithm uses the Rocchio method to identify a set RN of reliable method, each document is represented as a vector, Let D be the whole set of training classifier is achieved by constructing a prototype vector j C more similar to td is assigned to td . 
When use this method, the amount of RN is so big that biased the classifier of step 2 and poor performance, especially when the P set is small. 
The PEBL uses the 1-DNF method, first builds a positive feature set PF which con-tains words that occur in the positive examples set P more frequently than in the unla-beled examples set U . Then it tries to filter out possible positive documents from U . A negative document. In this algorithm, the amount of RN set is always small and some-times is short text examples. Its performance is poor when the number of positive ex-amples set is small. When the positive examples set is large, it becomes more stable. and positive examples set P . foundation for various data mining and analysis techniques. 
The standard clustering algorithms can be categorized into partitioning algorithms such as k-means and hierarchical algorith ms such as Single-Link or Average-Link. k-means called bisecting k-means [10] is introduced, which yields even better per-Then it keeps partitioning the currently largest cluster into two clusters, again using k-means, until a total number of k clusters has been discovered. examples set and unlabeled examples set with parameter k . Last, if proportion of posi-tive examples in each cluster is lower than the threshold that given, and then add this cluster to RN . Algorithm: Exploiting Reliable Negative by Clustering Input: P positive examples set Output: RN (reliable negatives set) 
Steps: 1. RN ={}; 2.Clustering set E = P  X  U; 3.run bisecting k-means with parameter k on E , and divide into E 1 , E 2 , ... E k , in each E i ( i = 1, 2, ..., k ), the positive examples in it is P i ; 4.for each E i ( i = 1, 2, ... , k ) if | P i |/| E i |&lt; T then RN = RN  X  E i . 
We use the CLUTO toolkit package [12] for clustering, which use bisecting k-means algorithm. The parameter T generally is small, usually set to zero, i.e. the cluster that has no positive examples can be used as reliable negative examples set. Yang, Y. scores are almost negligible [11]. From our experiments in section 4, we also observed small. So we set k as 20. Algorithm: Iterative SVM Input: P positive examples set Output: The final classifier S ; 
Steps: 1. Assigned the label 1 to each document in P; 2. Assigned the label -1 to each document in RN ; 3. While(true) 4. Training a new SVM classifier S i with P and 
RN ; 5. Classify Q using S ; 6. Let the set of documents in Q that are classified as negative be W; 7. If W ={} then break; 8. Else Q = Q -W ; RN = RN  X  W; 9. End if 10. End while 
For step 2, we run SVM iteratively as shown in fig. 2. This method is similar to the step 2 of PEBL technique and Roc-SVM technique except that we do not use an addi-more possible negative examples from Q ( U  X  RN ) and put them in RN . The iteration select a good classifier from a set of classifiers built by SVM, and use the last SVM there is a danger in running SVM repetitively, since SVM is sensitive to noise. How-ever, it is hard to catch the best classifier [6]. We now evaluate our proposed method with the Roc-SVM technique [7] and the PEBL technique [9]. We do not compare with the S-EM technique [8], because it uses the Na X ve Bayesian method, which is a weaker classifier than the SVM, and our pro-same setting as [6] in order to allow comparison on the square. 4.1 Experiments Setup and Data Preprocess which has 21578 documents collected from the Reuters newswire. Among 135 cate-gories, only the most populous 10 are used. 9980 documents are selected to use in our experiment, as shown in Table 1. Each category is employed as the positive examples class, and the rest as the negative examples class. This gives us 10 datasets. but no feature selection or stemming was done. The tf-idf value is used in the feature vectors. For each dataset, 30% of the documents are randomly selected as test docu-documents from the positive examples class is first selected as the positive examples set P . The rest of the positive and negative documents are used as unlabeled examples set U . We range  X  from 10%-90% to create a wide range of scenarios. 4.2 Evaluation Measures In our experiments, we use the popular F 1 score on the positive examples class as the sion, recall and F 1 defined as: 
For evaluating performance average across categories, there are two conventional methods, namely macro-average and micro-average. Macro-averaged performance scores are determined by first computing the performance measures per category and then averaging those to compute the gl obal means. We use macro-averaging. 4.3 Experiment Results In our experiments, we implemented the 1-DNF method used in PEBL and the Roc-chio method in the Roc-SVM. We use the CLUTO toolkit package [12] for clustering, not tune the parameters. For Roc-SVM, we use  X  =16 and  X  =4 in formula (1). 
We first compare the quantity of reliable negative examples produced by three positive examples set P . For the PEBL, the quantity of initial negative examples is so small; by browsing the initial negative examples, we found these examples sometimes are short paper, and the quality is poor too. For the Rocchio method, the quantity of reliable negative examples is so big that near the two third of training data. For clus-tering method, the quantity is moderate, and sometimes balanced the training set. 
Then we compare the F 1 score of our method with other two methods. The results of the PEBL method and the Rocchio method are extract from the experiment of Bing Liu et al. [6]. Fig.3 shows the macro-averaged F 1 score on the 10 Reuters datasets for When  X  is bigger, our method is as good as other methods. But there is still room for further improvement. 
The poor quality and quantity of the reliable negative examples by PEBL increase the number of the iterations of SVM, which ends up longer training time. The quan-tity of negative examples of Rocchio method is so big that biased the training set. Our proposed method produces the moderate quantity reliable negative examples. In this paper, we discussed the two-step strategies for learning a classifier from posi-tive examples and unlabeled examples data. The clustering method was added to the existing techniques. A comprehensive evaluation was conducted to compare their performances. Our method produces the moderate quantity reliable negative examples outperforms than other two; when  X  is bigger, our method is as good as other methods. The work was supported by the National Natural Science Foundation of China under Grant No. 60373099, the Project of the Jilin Province Science and Technology De-velopment Plan under the Grant No.20070533, and the Science Foundation for Young Teachers of Northeast Normal University (No.20070602). We would like to thank the anonymous reviewers for their comments and suggestions. 
