 The aim of our project is to predict the real world characteristics of players of massively-multiplayer online games, such as Second Life (SL). We sought to predict actual player attributes like age or educa-tion levels, and personality traits including leader-ship or conformity. Our task was to do so using only the behaviors, communication, and interaction among the players produced during game play. To do so, we logged all players X  avatar movements,  X  X ouch events X  (putting on or taking off clothing items, for example), and their public chat messages (i.e., messages that can be seen by all players in the group). Given the complex nature of interpreting chat in an online game environment, we required a tool that would allow annotators to have a synchro-nized view of both the event action as well as the chat utterances. This would allow our annotators to correlate the events and the chat by marking them simultaneously. More impor tantly, being able to view game events enables more accurate chat anno-tation; and conversely, viewing chat utterances helps to interpret the signi ficance of certain events in the game, e.g., one avatar following another. For example , an exclamation of:  X  X  can X  X  do it! X  could be simply a response (rejection) to a request from another player; however, when the game action is viewed and the speaker is seen attempting to enter a building without success, another interpretation may arise (an assertion, a call for help, etc.). players (and other on-line games) may be inferred to varying degrees from the appearance of their avatars, the behaviors they engage in, as well as from their on-line chat communications. For exam-ple, the avatar gender generally matches the gender of the owner; on the other hand, vocabulary choices in chat are rather poor predictors of a player X  X  age, even though such correlation is generally seen in real life conversation. of the ease of creating objects, controlling the play environment, and collecting players X  movement, chat, and other behaviors. We generated a corpus of chat and movement data from 48 quests comprised of 206 participants w ho generated over 30,000 messages and approximately 140 hours of recorded action. We required an annotation tool to help us efficiently annotate dialogue acts and communica-tion links in chat utterances as well as avatar movements from such a large corpus. Moreover, we required correlation be tween these two dimen-sions of chat and movement since movement and other actions may be both causes and effects of verbal communication. We developed a multi-modal event and chat annot ation tool (called RAT, the Relational Annotation Tool), which will simul-taneously display a 2D re ndering of all movement activity recorded during our Second Life studies, synchronized with the chat utterances. In this way both chat and movements can be annotated simul-taneously: the avatar movement actions can be re-viewed while making dialogue act annotations. This has the added advantage of allowing the anno-tator to see the relationships between chat, behav-ior, and location/movement. This paper will describe our annotation process and the RAT tool. Annotation tools have been built for a variety of purposes. The CSLU Toolkit (Sutton et al., 1998) is a suite of tools used for annotating spoken lan-guage. Similarly, the EMU System (Cassidy and Harrington, 2001) is a speech database management system that supports multi-level annotations. Sys-tems have been created that allow users to readily build their own tools such as AGTK (Bird et al., 2001). The multi-modal tool DAT (Core and Al-len, 1997) was developed to assist testing of the DAMSL annotation scheme. With DAT, annota-tors were able to listen to the actual dialogues as well as view the transcripts. While these tools are all highly effective for their respective tasks, ours is unique in its synchronized view of both event ac-tion and chat utterances. cation use either off-the shelf qualitative data anal-ysis programs like Atlas.ti or NVivo, a few studies have annotated chat using custom-built tools. One approach uses computer-mediated discourse analy-sis approaches and the Dynamic Topic Analysis tool (Herring, 2003; Herring &amp; Nix; 1997; Stromer-Galley &amp; Martison, 2009), which allows annotators to track a specific phenomenon of online interaction in chat: topic shifts during an interaction. The Virtual Math Teams project (Stahl, 2009) created a ated a tool that allowed for the simultaneous play-back of messages posted to a quasi-synchronous discussion forum with whiteboard drawings that student math team members used to illustrate their ideas or visualize the math problem they were try-ing to solve ( X akir, 2009). human interaction is found in the AMI Meeting Corpus (Carletta, 2007). It captures participants X  head movement information from individual head-mounted cameras, which allows for annotation of nodding (consent, agreement) or shaking (dis-agreement), as well as participants X  locations within the room; however, no complex events involving series of movements or participant proximity are considered. We are unaware of any other tools that facilitate the simultaneous playback of multi-modes of communication and behavior. 3 Second Life Experiments To generate player data, we rented an island in Second Life and developed an approximately two hour quest, the Case of the Missing Moonstone. In this quest, small groups of 4 to 5 players, who were previously unacquainted, work their way together through the clues and puzzles to solve a murder mystery. We recruited Second Life players in-game through advertising and setting up a shop that inter-ested players could browse. We also used Facebook ads, which were remarkably effective. started after they arrived in a starting area of the island (the quest was open only to players who were made temporary members of our island) where they met other players, browsed quest-appropriate clothing to adorn their avatars, and re-ceived information from one of the researchers. Once all players arrived, the main quest began, progressing through five geographic areas in the island. Players were accompanied by a  X  X raining sergeant X , a researcher us ing a robot avatar, that followed players through the quest and provided hints when groups became stymied along their in-vestigation but otherwise had little interaction with the group. obstacles that required coordinated action, such as all players standing on special buttons to activate a door, or the sharing of information between players, such as solutions to a word puzzle, in order to ad-vance to the next area of the quest (Figure 1). Slimy Roastbeef :  X  X ho X  X  got the square gear? X  Kenny Superstar:  X  X  do, but I X  X  stuck X  Slimy Roastbeef:  X  X an you hand it to me? X  Kenny Superstar :  X  X  don X  X  know how X 
Slimy Roastbeef :  X  X pen your inventory, click Quest activities requiring coordination among the players were common and also necessary to ensure a sufficient degree of movement and message traf-fic to provide enough material to test our predic-tions, and to allow us to observe particular social characteristics of players. Players answered a sur-vey before and then again after the quest, providing demographic and trait information and evaluating other members of their group on the characteristics of interest. 3.1 Data Collection We recorded all players X  avatar movements as they purposefully moved avatars through the virtual spaces of the game environment, their public chat, and their  X  X ouch events X , which are the actions that bring objects out of player inventories, pick up ob-jects to put in their inventories, or to put objects, such as hats or clothes, onto the avatars, and the like. We followed Yee and Bailenson X  X  (2008) technical approach for logging player behavior. To get a sense of the volume of data generated, 206 players generated over 30,000 messages into the group X  X  public chat from the 48 sessions. We com-piled approximately 140 hours of recorded action. The avatar logger was implemented to record each avatar X  X  location through their (x,y,z) coordinates, recorded at two second intervals. This information was later used to render the avatar X  X  position on our 2D representation of the action (section 4.1). The Relational Annotation Tool (RAT) was built to assist in annotating the massive collection of data collected during the Second Life experiments. A tool was needed that would allow annotators to see the textual transcripts of the chat while at the same time view a 2D representation of the action. Addi-tionally, we had a textual transcript for a select set of events: touch an object, stand on an object, at-tach an object, etc., that we needed to make avail-able to the annotator for review. several reasons. First, in order to fully understand the communication and interaction occurring be-tween players in the game environment and accu-rately annotate those messages, we needed annotators to have as much information about the context as possible. The 2D map coupled with the events information made it easier to understand. For example, in the quest, players in a specific zone, encounter a dead, maimed body. As annota-tors assigned codes to the chat, they would some-times encounter exclamations, such as  X  X w X  or  X  X ross X . Annotators would use the 2D map and the location of the exclaiming avatar to determine if the exclamation was a result of their location (in the zone with the dead body) or because of something said or done by another player. Location of avatars on the 2D map synchronized with chat was also helpful for annotators when attempting to disam-biguate communicative links. For example, in one subzone, mad scribblings are written on a wall. If player A says  X  X ou see that scribbling on the wall? X  the annotator needs to use the 2D map to see who the player is speaking to. If player A and player C are both standing in that subzone, then the annotator can make a reasonable assumption that player A is directing the question to player C, and not player B who is located in a different subzone. Second, we annotated coordinated avatar move-ment actions (such as following each other into a building or into a room), and the only way to read-ily identify such complex events was through the 2D map of avatar movements. the annotator to simultaneously view all modes of representation. There are three distinct panels in this interface. The left hand panel is the 2D repre-sentation of the action (section 4.1). The upper right hand panel displays the chat and event tran-scripts (section 4.2), while the lower right hand por-tion is reserved for the three annotator sub-panels (section 4.3). 4.1 The 2D Game Representation The 2D representation was the most challenging of the panels to implement. We needed to find the proper level of abstraction for the action, while maintaining its usefulness for the annotator. Too complex a representation would cause cognitive overload for the annotator, thus potentially deterio-rating the speed and quality of the annotations. Conversely, an overly abstract representation would not be of significant value in the annotation proc-ess. Second Life Island: Starting Area, Mansion, Town Center, Factory and Apartments . An overview of the area in Second Life is displayed in Figure 3. We decided to represent each area separately as each group moves between the areas together, and it was therefore never necessary to display more than one area at a time. The 2D representation of the Man-sion Area is displayed in Figure 4 below. Figure 5 is an exterior view of the actual Mansion in Second Life. Each area X  X  fixed representation was rendered using Java Graphics, reading in the Second Life (x,y,z) coordinates from an XML data file. We rep-resented the walls of the buildings as connected solid black lines with openings left for doorways. Key item locations were marked and labeled, e.g. Kitten, maid, the Idol , etc. Even though annotators visited the island to familiarize themselves with the layout, many mansion rooms were labeled to help the annotator recall the layout of the building, and minimize error of annotation based on flawed re-call. Finally, the exact time of the action that is cur-rently being represented is displayed in the lower left hand corner. 
Figure 4: 2D representation of Second Life action Figure 5: Second Life view of Mansion exterior Avatar location was recorded in our log files as an (x,y,z) coordinate at a two second interval. Avatars were represented in our 2D panel as moving solid color circles, using the x and y coordinates. A color coded avatar key was displayed below the 2D rep-resentation. This key related the full name of every avatar to its colored circle representation. The z coordinate was used to determine if the avatar was on the second floor of a building. If the z value indicated an avatar was on a second floor, their icon was modified to include the number  X 2 X  for the du-ration of their time on the second floor. Also logged was the avatar X  X  degree of rotation. Using this we were able to represent which direction the avatar was looking by a small black dot on their colored circle. event annotation, the action would move forward, in synchronized step in the 2D map. In this way at any given time the annotator could see the avatar action corresponding to the chat and event tran-scripts appearing in the right panels. The annotator had the option to step forward or backward through the data at any step inte rval, where each step corre-sponded to a two second increment or decrement, to provide maximum flexibility to the annotator in viewing and reviewing the actions and communica-tions to be annotated. Additionally,  X  X lay X  and  X  X top X  buttons were added to the tool so the anno-tator may simply watch the action play forward ra-ther than manually stepping through. 4.2 The Chat &amp; Event Panel Avatar utterances along with logged Second Life events were displayed in the Chat and Event Panel (Figure 6). Utterances and events were each dis-played in their own column. Time was recorded for every utterance and event, and this was displayed in the first column of the Chat and Event Panel. All avatar names in the utterances and events were color coded, where the colors corresponded to the avatar color used in the 2D panel. This panel was synchronized with the 2D Representation panel and as the annotator stepped through the game action on the 2D display, the associat ed utterances and events populated the Chat and Event panel. 4.3 The Annotator Panels The Annotator Panels (Figures 7 and 10) contains all features needed for the annotator to quickly annotate the events and dialogue. Annotators could choose from a number of categories to label each dialogue utterance. Coding categories included communicative links, dialogue acts, and selected multi-avatar actions . In the following we briefly outline each of these. A more detailed description of the chat annotation scheme is available in (Shaikh et al., 2010). 4.3.1 Communicative Links One of the challenges in multi-party dialogue is to establish which user an utterance is directed to-wards. Users do not t ypically add addressing in-formation in their utterances, which leads to ambiguity while creating a communication link be-tween users. With this annotation level, we asked the annotators to determine whether each utterance was addressed to some user, in which case they were asked to mark which specific user it was ad-dressed to; was in response to another prior utter-ance by a different user, which required marking the specific utterance responded to; or a continua-tion of the user X  X  own prior utterance. rate mapping of dialogue dynamics in the multi-party setting, and is a critical component of tracking such social phenomena as disagreements and lead-ership. 4.3.2 Dialogue Acts We developed a hierarchy of 19 dialogue acts for annotating the functional aspect of the utterance in the discussion. The tagset we adopted is loosely based on DAMSL (Allen &amp; Core, 1997) and SWBD (Jurafsky et al., 1997), but greatly reduced and also tuned significantly towards dialogue pragmatics and away from more surface character-istics of utterances. In particular, we ask our anno-tators what is the pragmatic function of each utterance within the dialogue, a decision that often depends upon how earlier u tterances were classi-fied. Thus augmented, DA tags become an impor-tant source of evidence for detecting language uses and such social phenomena as conformity. Exam-ples of dialogue act tags include Assertion-Opinion, Acknowledge, Information-Request, and Confirma-tion-Request. fairly challenging task to our annotators, who need to be trained for many hours before an acceptable rate of inter-annotator agreement is achieved. For this reason, we consider our current DA tagging as a work in progress. 4.3.3 Zone coding ing set of subzones. A subzone is a building, a room within a building, or any other identifiable area within the playable sp aces of the quest, e.g. the Mansion has the subzones: Hall, Dining Room, Kitchen, Outside, Ghost Room , etc. The subzone was determined based on the avatar(s) (x,y,z) coor-dinates and the known subzone boundaries. This additional piece of data allowed for statistical analysis at different levels: avatar, dialogue unit, and subzone. 4.3.4 Multi-avatar events As mentioned, in addition to chat we also were in-terested in having the annotators record composite events involving multiple avatars over a span of time and space. While the design of the RAT tool will support annotation of any event of interest with only slight modifications, for our purposes, we were interested in annotating two types of events that we considered significant for our research hy-potheses. The first type of event was the multi-avatar entry (or exit) into a sub-zone, including the order in which the avatars moved. Subzone X  annotation as displayed in the Chat &amp; Event Panel. Figure 9 shows the corresponding se-ries of progressive moments in time portraying en-try into the Bank subzone as represented in RAT. In the annotation, each avatar name is recorded in or-der of its entry into the subzone (here, the Bank). Additionally, we record the subzone name and the time the event is completed 3 .  X  X ollow X X  event, i.e., wh en one or more avatars appeared to be following one another within a sub-zone. These two types of events were of particular interest because we hypothesized that players who are leaders are likely to enter first into a subzone and be followed around once inside. of composite events can be added as needed; for example, group forming and splitting, or certain joint activities involving objects, etc. were fairly common in quests and may be significant for some analyses (although not for our hypotheses). created to facilitate speedy markup while minimiz-ing opportunities for error (Figure 10). A  X  X oves Into Subzone X  event is annotated by recording the ordinal (1, 2, 3, etc.) for each avatar. Similarly, a  X  X ollows X  event is coded as avatar group  X  X  X  fol-lows group  X  X  X , where each group will contain one or more avatars. 
Figure 8: The corresponding annotation for Figure 9 event, as displayed in the Chat &amp; Event Panel 5 The Annotation Process To annotate the large volume of data generated from the Second Life quests, we developed an an-notation guide that defined and described the anno-tation categories and decision rules annotators were to follow in categorizing the data units (following previous projects (Shaikh et al., 2010). Two stu-dents were hired and trained for approximately 60 hours, during which time they learned how to use the annotation tool and the categories and rules for the annotation process. After establishing a satisfac-tory level of interrater re liability (average Krippen-Krippendorff X  X  alpha accounts for the probability of chance agreement and is therefore a conservative measure of agreement), the two students then anno-tated the 48 groups over a four-month period. It took approximately 230 hours to annotate the ses-sions, and they assigned over 39,000 dialogue act tags. Annotators spent roughly 7 hours marking up the movements and chat messages per 2.5 hour quest session. 5.1 The Annotated Corpus The current version of the annotated corpus consists of thousands of tagged messages including: 4,294 action-directives, 17,129 assertion-opinions, 4,116 information requests, 471 confirmation requests, 394 offer-commits, 3,075 responses to information requests, 1,317 agree-accep ts, 215 disagree-rejects, and 2,502 acknowledgements, from 30,535 pre-split utterances (31,801 post-split). We also as-signed 4,546 following events. 6 Conclusion In this paper we described the successful imple-mentation and use of our multi-modal annotation tool, RAT. Our tool was used to accurately and simultaneously annotate over 30,000 messages and approximately 140 hours of action. For each hour spent annotating, our annotators were able to tag approximately 170 utterances as well as 36 minutes of action. functional and very efficient at helping them easily assign categories to the relevant data units, and that they could assign those categories without produc-ing too many errors, such as accidentally assigning the wrong category or selecting the wrong avatar. The function allowing for the synchronized play-back of the chat and movement data coupled with the 2D map increased comprehension of utterances and behavior of the players during the quest, im-proving validity and reliability of the results. Acknowledgements This research is part of an Air Force Research Laboratory sponsored study conducted by Colorado State University, Ohio Univ ersity, the University at Albany, SUNY, and Lockheed Martin. 
