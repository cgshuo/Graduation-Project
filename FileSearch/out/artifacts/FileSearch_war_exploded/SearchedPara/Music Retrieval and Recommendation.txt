 In this tutorial, we give an introduction to the field of and state of the art in music information retrieval (MIR). The tutorial particularly spotlights the question of music sim-ilarity, which is an essential aspect in music retrieval and recommendation. Three factors play a central role in MIR research: (1) the music content, i.e., the audio signal itself, (2) the music context, i.e., metadata in the widest sense, and (3) the listeners and their contexts, manifested in user-music interaction traces. We review approaches that extract fea-tures from all three data sources and combinations thereof and show how these features can be used for (large-scale) music indexing, music description, music similarity mea-surement, and recommendation. These methods are further showcased in a number of popular music applications, such as automatic playlist generation and personalized radio sta-tioning, location-aware music recommendation, music search engines, and intelligent browsing interfaces. Additionally, related topics such as music identification, automatic music accompaniment and score following, and search and retrieval in the music production domain are discussed.
 H.5.5 [ Information Interfaces and Presentation ]: Sound and Music Computing X  Methodologies and techniques ; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Music Information Retrieval; Music Recommendation; Con-tent; Context; Listener
As the amount of music available via streaming services, online stores, platforms like YouTube, and other web sources has skyrocketed over the last couple of years. Retrieving rel-evant music that matches the user X  X  taste is a challenging, Figure 1: Four different categories of factors that influence music perception. albeit important task to make accessible the ever-growing digital music repositories in an intelligent manner. Given the current rise of social media and user-generated contents, retrieving information about music as well as retrieving mu-sic itself heavily relies on text-based IR techniques, as text is still the widest used means of communication on the web. On the other hand, multimodal retrieval schemes for multi-media content demand for acoustic features and make hybrid (signal-and text-based) approaches attractive.

Music information retrieval (MIR) is a research field that aims  X  among other things  X  at automatically extracting semantically meaningful information from various represen-tations of music entities, such as a digital audio file, a band X  X  web page, a song X  X  lyrics, or a tweet about a microblogger X  X  current listening activity.

A key approach in MIR is to describe music via computa-tional features, which can be broadly categorized into music content , music context , user properties , and user context , cf. Figure 1. While music content-based features are derived di-rectly from the audio signal of the music file, music context refers to pieces of information that are not encoded in the actual audio file, nevertheless play an important role in hu-man perception of music. Such aspects include the meaning of song lyrics, the background of an artist, the cover of an al-bum, the sequence of songs selected by a DJ to constitute a playlist, or collaborative tags describing a release. Extract-Figure 2: The different levels of feature abstraction and the  X  X emantic gap X  between them. ing music content features requires access to the actual audio file; in contrast, contextual feature extractors require as in-put only editorial metadata (e.g., name of artist and song) to harvest music-related information from the web and con-sequently approximate similarities between music items. On the other hand, music content features are in general more objective than music context features as the underlying data source, i.e., the audio itself, does not change dynamically, in contrast to user-generated content or other kinds of con-textual data sources. Both types of features (content and context), however, share a susceptibility to noise of different kinds. User properties refer to the listener X  X  demographic information, musical education and experience, preferences and taste, as well as personality traits. The user context in-cludes environmental aspects as well as physical and mental activities of the music listener. Particularly aspects of the latter two categories are often difficult to derive. Methods aiming at modeling these dimensions thus typically exploit traces of user-music interaction, such as listening histories or geo-location data, in order to learn a representation of (possibly latent) user state and context.

Depending on the chosen source, features might be closer to a concept as humans understand it ( high-level ) or closer to a strictly machine-interpretable representation ( low-level ). For instance, features derived from the music content or the sensor data captured with a user X  X  personal device will be mostly statistical descriptions of signals that are difficult to interpret, i.e., low-level features, whereas information ex-tracted from web sources, such as occurrences of words in web page texts, is typically easier to interpret. For all types of features in order to be processable, we need a numeric ab-straction. Between these low-level numeric representations and the factors of musical perception that we want to model there is a discrepancy  X  commonly known as  X  X emantic gap, X  cf. Figure 2. Particularly, the hybrid and user-centric tech-niques discussed strive to narrow this gap.
MIR research has been seeing a paradigm shift over the last couple of years, as an increasing number of recently pub-lished approaches focus on the contextual feature categories, or at least combine audio-based techniques with data mined from web sources or the user X  X  signals. This is reflected in the structure of the tutorial.

First, we introduce selected existing applications that rely on MIR technology to motivate the presented contents and relate them to real-world scenarios and applications, such as automated music playlist generation, personalized web radio, music recommendation systems, and intelligent user interfaces to music. Then, we summarize the ideas behind and discuss advantages and disadvantages of computational features extracted from music content and music context, as well as user-centric information. Each of these discussions is substantiated in a dedicated segment.

Regarding music content analysis, we give a brief intro-duction to signal processing methods (PCM, A/D Conver-sion, FFT, DCT, etc.) to lay the foundation for elaborated methods of music processing (e.g., [9]). We review some standard approaches to audio feature extraction on frame and block level as well as state-of-the-art similarity measures using features such as MFCCs [17], block-level features[29], and pitch class profiles [20]. We also briefly address features for related MIR tasks such as beat detection [4], melody ex-traction [21], or score following [1]. In addition, aspects of large-scale indexing [27] and the problem of hubness for re-trieval in high-dimensional feature spaces are addressed [28]. Further attention is given to evaluating MIR systems beyond the traditional IR-related measures and the difficulties en-tailed by the need for objective quantification, e.g., [5, 31].
As for aspects of the music context , we focus on data ac-cessible through web technology. To this end, we introduce the field of web-based MIR and give a detailed description and comparison of contextual data sources on music (e.g., web pages and blogs [10], micro-blogs [23], user tags [16, 14], and lyrics [18]) and discuss related methods to obtain this data (web mining, games with a purpose [15], etc.). These sources can be exploited in order to
Regarding the user-centric aspects (user properties and context) and their applications in music recommendation and other personalized systems, we discuss sources of music interaction traces (e.g., playlists [19], ratings [6], postings and micro-blogs [24], peer-to-peer networks [13, 30], and social networks [7]) and possibilities to mine the context directly from sensor data using smart devices [8]. Meth-ods that use this data can then be applied for tasks such as playlist generation, tag prediction, and location-aware music recommendation. We further address methods that include information from both context data and content in-formation, either by learning hybrid similarity measures or by optimizing audio-based or hybrid similarity functions in order to reflect preference of users [26]. Additionally, user requirements such as need for novelty, diversity, or serendip-ity are addressed [3, 33]. This last segment concludes with an outlook to the next years of MIR and the biggest chal-lenges the field is facing.

The following outlines the structure of the tutorial: 1. Introduction to Music Similarity and Retrieval
Dr. Peter Knees is an assistant professor at the Depart-ment of Computational Perception at the Johannes Kepler University Linz . He holds a Master X  X  degree in Computer Science from the Vienna University of Technology and a Ph.D. degree from the Johannes Kepler University Linz .
Since 2004, he co-authored over 60 peer-reviewed confer-ence and journal publications, served as program committee member for several conferences relevant to the fields of mu-sic, multimedia, and text IR, including ISMIR, ACM Mul-timedia, ECIR Tutorials, and the Adaptive Multimedia Re-trieval workshop and was an organizer of the International Workshop on Advances in Music Information Research se-ries and the SIGIR 2014 Workshop on Social Media Retrieval and Analysis . He is teaching grad-level courses on Multi-media Search and Retrieval , Learning from User-generated Data , Multimedia Data Mining , and Intelligent Information Systems and has given tutorials and lectures on music IR at ECIR, SIGIR, and RuSSIR. In addition to music and web information retrieval, his research interests include multime-dia systems, user interfaces, and recommender systems. Dr. Markus Schedl is an associate professor at the Johannes Kepler University Linz / Department of Com-putational Perception . He graduated in Computer Science from the Vienna University of Technology and earned his Ph.D. in Computer Science from the Johannes Kepler Uni-versity Linz . Markus further holds a Master X  X  degree in In-ternational Business Administration from the Vienna Uni-versity of Economics and Business . He (co-)authored more than 100 refereed conference/workshop papers and journal articles (published, among others, in SIGIR, ECIR, ACM Multimedia; Journal of Machine Learning Research, ACM Transactions on Information Systems, Springer Information Retrieval, IEEE Multimedia). He is an associate editor of the Springer International Journal of Multimedia Informa-tion Retrieval, serves on various program committees, and as reviewer (among others, for ACM Multimedia, ECIR, IJCAI, ICASSP, IEEE Visualization; Transactions of Mul-timedia, Transactions on Intelligent Systems and Technol-ogy, Information Sciences, Pattern Recognition Letters). He is co-founder of the International Workshop on Advances in Music Information Research (AdMIRe) and the Inter-national Workshop on Social Media Retrieval and Analysis (SoMeRA), the latter held in conjunction with SIGIR 2014. He recently co-authored an article titled  X  X usic Informa-tion Retrieval: Recent Developments and Applications X  in Foundations and Trends in Information Retrieval [25].
This tutorial is supported by the European Union Sev-enth Framework Programme FP7/2007-2013 through grant agreements no. 601166 (PHENICX) and no. 610591 (Giant-Steps), as well as by the Austrian Science Funds (FWF): P25655. [1] A. Arzt and G. Widmer. Towards effective  X  X ny-time X  [2] L. Barrington, D. Turnbull, M. Yazdani, and [3] O. Celma and P. Herrera. A new approach to [4] S. Dixon, F. Gouyon, and G. Widmer. Towards [5] J. S. Downie, J. Futrelle, and D. Tcheng. The [6] G. Dror, N. Koenigstein, and Y. Koren. Yahoo! music [7] B. Fields, M. Casey, K. Jacobson, and M. Sandler. Do [8] M. Gillhofer and M. Schedl. Iron Maiden while [9] B. Gold and N. Morgan. Speech and Audio Signal [10] P. Knees, E. Pampalk, and G. Widmer. Artist [11] P. Knees, T. Pohle, M. Schedl, and G. Widmer. A [12] P. Knees and M. Schedl. Towards Semantic Music [13] N. Koenigstein, Y. Shavitt, and T. Tankel. Spotting [14] P. Lamere. Social Tagging and Music Information [15] E. Law, L. von Ahn, R. Dannenberg, and [16] M. Levy and M. Sandler. A semantic space for music [17] B. Logan. Mel Frequency Cepstral Coefficients for [18] R. Mayer, R. Neumayer, and A. Rauber. Rhyme and [19] B. McFee and G. Lanckriet. Hypergraph Models of [20] H. Purwins. Profiles of Pitch Classes: Circularity of [21] J. Salamon and E. G  X omez. Melody extraction from [22] M. Schedl. Automatically Extracting, Analyzing, and [23] M. Schedl. #nowplaying Madonna: A Large-Scale [24] M. Schedl. Leveraging Microblogs for Spatiotemporal [25] M. Schedl, E. G  X omez, and J. Urbano. Evaluation in [26] M. Schedl, S. Stober, E. G  X omez, N. Orio, and C. C. [27] D. Schnitzer. Indexing Content-Based Music [28] D. Schnitzer, A. Flexer, M. Schedl, and G. Widmer. [29] K. Seyerlehner, G. Widmer, M. Schedl, and P. Knees. [30] Y. Shavitt, E. Weinsberg, and U. Weinsberg. Mining [31] J. Urbano and M. Schedl. Minimal test collections for [32] B. Whitman and S. Lawrence. Inferring Descriptions [33] Yuan Cao Zhang, Diarmuid O Seaghdha, Daniele
