 Users express their personal preferences through ratings, adoptions, and other consumption behaviors. We seek to learn latent representations for user preferences from such behavioral data. One representation learning model that has been shown to be effective for large preference datasets is Restricted Boltzmann Machine (RBM). While homophily, or the tendency of friends to share their preferences at some level, is an established notion in sociology, thus far it has not yet been clearly demonstrated on RBM-based preference models. The question lies in how to appropriately incorpo-rate social network into the architecture of RBM-based mod-els for learning representations of preferences. In this paper, we propose two potential architectures: one that models so-cial network among users as additional observations, and another that incorporates social network into the sharing of hidden units among related users. We study the efficacies of these proposed architectures on publicly available, real-life preference datasets with social networks, yielding useful insights.
 user preferences; homophily; representation learning; social recommendation; restricted Boltzmann machine
Representation learning [5] deals with deriving useful la-tent representations from a large amount of data, so as to enable better learning or prediction. It is an area of ac-tive research in diverse fields, including speech recognition [13], computer vision [19], natural language processing, etc., where approaches based on neural networks and deep learn-ing are currently generating a lot of interest.

In this work, we are particularly interested in representa-tion learning for preference data. Users express their prefer-ences in various ways, e.g., when they assign ratings to items, when they tag or bookmark contents they like, when they purchase or re-purchase products, when they watch videos or listen to music. Such behaviors generate a large amount of data that carry a lot of information about what users like and dislike. Our aim is to learn a latent representation for each user X  X  preferences from such behavioral data.

One pioneering work based on undirected graphical model [31] used Restricted Boltzmann Machine (RBM) to model user preferences from ratings. [31] showed that RBM could learn representations for user preferences from large-scale datasets such as Netflix [4], with collaborative filtering per-formance competitive with matrix factorization X  X . More-over, being a basic building block of many deep learning models, RBM could potentially benefit from further devel-opment in the currently very active deep learning research.
Many online social media platforms today capture not just users X  preferences through their behaviors, but also their so-cial connections with one another through their friendship links. This highlights an important aspect of user prefer-ences that has not yet been factored in by previous RBM-based models, i.e., homophily , or the tendency of people with social connections to have shared preferences at some level. Homophily is an established notion in sociology [26], and as we will survey in Section 2, factoring in social network information often helps collaborative filtering algorithms.
Given the preponderance of evidence from the literature on homophily, we hypothesize that social network infor-mation could also potentially improve the performance of RBM-based models at learning representations for user pref-erences. The research questions lie in how to do so appropri-ately. In this paper, we investigate two primary structures.
The first is to model social network links as observations that are generated by RBM, in addition to behaviors such as ratings or adoptions. This yields our first proposed model, SocialRBM -Wing described in Section 4, which features a dual-wing structure, i.e., two groups of visible units (cor-responding to item adoptions and social links respectively) sharing a common layer of hidden units. This has the advan-tage of simplicity, by learning the social effects from data.
Rather than modeling social links as observations, the sec-ond model, SocialRBM -Deep described in Section 5, fea-tures a deep structure, i.e., a higher layer of hidden units that are partially shared among friends, while still maintain-ing some global sharing across all users. This has the ad-vantage of incorporating the homophily assumption directly into the higher layer of hidden units, so as to allow the origi-nal lower layer of hidden units to focus on capturing patterns of item adoptions among all users. Moreover, deeper struc-tures tend to produce more robust features, which are less likely to suffer from noise than shallower structures.
Contributions. In this work, we make the following con-tributions: 1. As far as we know, this is the first systematic study of 2. Towards this objective, we propose two models, namely: 3. To verify the efficacy of the models, we conduct an em-
A Boltzmann Machine (BM) [1] is a network of stochastic binary units, with one hidden layer and one visible layer. A more popular form with simpler inference is Restricted Boltzmann Machine (RBM) [31], which allows only connec-tions between hidden and visible layers, but not within each layer. Variants of RBM include replacing binary units with Gaussian hidden units [31] or visible units [30] for continuous values, or learning from inequality constraints [35]. RBM can also be extended to have multiple hidden layers, form-ing a Deep Boltzmann Machine (DBM) [30].

The use of RBM for collaborative filtering was pioneered by [31]. This has been extended in several directions. [8] ex-panded into two sets of hidden layers: for modeling correla-tions among users and items respectively. [32] used autoen-coder in place of RBM. [7] used autoencoder on ratings to learn representation for initializing an existing matrix factor-ization [22]. In this work, we build upon the well-established RBM model [31] to further incorporate social networks.
By incorporating social networks into modeling ratings or adoptions, our work is related to multi-modal representation learning. For instance, [17, 39] modeled text and images, [33] modeled audio and video, while [38] modeled text and ratings. We are distinct in two ways. For one, we are model-ing a different set of modalities. For another, most of these works treat modalities as observations, whereas we also ex-plore another means for incorporating one modality (social network) into the configuration of shared hidden units.
Previous works in collaborative filtering were mostly based on matrix factorization [2, 16, 18]. It thus follows that past works on incorporating social networks were modifications of matrix factorization [10, 40], e.g., generating social network links [23], regularizing friends X  latent vectors [24], or express-ing one X  X  ratings [21] or latent vector [15] as a function of those of friends. These and our work essentially follow two different forks of collaborative filtering paradigms: matrix factorization and RBM respectively. The two paradigms are effectively complementary and co-existent. Previous studies showed that ensembles of collaborative filtering paradigms could yield better performance than any one paradigm on its own [3, 4, 14]. In this work, we focus on investigating the effects of homophily on RBM-based preference models.
There are also efforts to factor in social network into other types of models, such as topic modeling [28, 37]. Such works rely heavily on the availability of rich features, such as words. Modeling features is an orthogonal direction to our focus here on the effects of social networks on adoptions.
Restricted Boltzmann Machine (RBM). RBM is a form of Markov Random Field, with the structure of a bipar-tite graph, connecting two types of binary stochastic units: visible units x  X  X  0 , 1 } N and hidden units h  X  X  0 , 1 }
As a member of the family of energy-based models, its energy function is defined as follows: where a and b are bias vectors for visible and hidden units respectively, and W is the N  X  K matrix of weights associ-ated with the connections between visible and hidden units.
Based on the energy function, the likelihood P ( x ) of an observed boolean vector x is as follows: where Z = P x 0 , h 0 exp(  X  E ( x 0 , h 0 )) is the partition function for normalization, while  X  is the set of model parameters.
The two conditional probabilities P ( h | x ) and P ( x | h ) can be expressed as follows: where  X  ( t ) = 1 / 1 + exp (  X  t ) is the logistic function. The conditional distributions indicate that units in one layer are activated independently given activations from the other.
The model can be trained by maximizing the log-likelihood log P ( x ;  X ) using an approximation gradient ascent  X  X on-trastive divergence X  (CD) introduced by [12].

RBM for Preference Data. Let us denote U to be the number of users, and N to be the number of items. For each user, we observe a visible vector x  X  X  0 , 1 } N , where  X 1 X  indicates the user X  X  adoption of an item, and  X 0 X  otherwise. Here, for simplicity, we model binary adoptions. For multi-scale ratings, we can use softmax units instead, as in [31].
These observations can be modeled as one RBM instance for each user, with the same number of K hidden units, and all the instances share the same a , b and W weights. This way, we can derive a K  X  dimensional latent representation vector h for each user, within a huge space of combinations resulting from the activations of various binary hidden units.
Incorporating Social Networks. We assume that we are also given a social network graph G , where each edge is a symmetric connection between two users. Our objective is to integrate G into an RBM model for user preferences, so as to arrive at a better latent representation h for each user. In the RBM above, there is no user-specific parameter. Therefore, unlike some non-RBM approaches outlined in Section 2, we cannot simply tie user-specific parameters of friends. Directly employing regularization among hidden layers of RBM would not be applicable either, because the shared weights would lead to optimization on the whole net-work globally at the same time, rather than just locally among friends. This motivates our approach of placing social constraints to express homophily through the model struc-ture so as to learn personalized representation using the hid-den layers. In the next two sections, we describe two RBM-based structures that we find effective for this problem. Figure 1: SocialRBM -Wing : both social connections and ratings/adoptions play a role as observations encoded jointly through a shared hidden layer.
Our first model, SocialRBM -Wing , models item adop-tions and social connections as two sets of observations, which are encoded through a shared hidden layer. As shown in Figure 1, the two sets of visible units resemble two  X  X ings X  attached to a  X  X ody X  (the shared hidden layer), thus the name of the model. Our intuition is that if the model can learn co-occurrence patterns across both users X  item adop-tions and social connections, we would then be able to pre-dict item adoptions given one X  X  social connections.
Each user is associated with two sets of visible units. The first set, for item adoptions, is x  X  { 0 , 1 } N , as described in Section 3. The second, for social connections, is y  X  X  0 , 1 } i.e., a binary vector of U dimensions, where each element y in the vector is activated if the user has a connection to user u in the social network G . In the following, for simplicity, we describe the model and the learning for an individual user. For the collection of all users, the gradients with respect to the shared weight parameters are averaged over all users.
Figure 1 shows an illustration for one example user user-1 or u 1 , with a social connection to herself as well as to her friends ( u 3 and u 4 ), thus the visible units y 1 , y 4 activated (shaded in blue). The user is also observed to have two item adoptions (the second and fourth items), thus the visible units x 2 and x 4 are activated (shaded in green). The conditional distribution of x is as shown in Eq. (4). In turn, the conditional distribution of y is shown in Eq. (5) below, where V jk is the weight parameter (shared among all users) associated with the connection between visible unit y and hidden unit h k .

The conditional distribution over hidden units for encod-ing both visible layers is given in Eq. (6), while the energy function is given in Eq. (7).
 Learning. From Eq. (7), we can derive the log-likelihood L ( x , y ;  X ) of visible inputs x and y for each user: where Z ( X ) = P x 0 , y 0 , h 0 exp (  X  E ( x 0 , y 0 , h function to normalize the probability sum to 1.

Model parameters are learned by contrastive divergence [12] (CD) with n -step sampling (CD-n ), as shown in Eq. (10) where P 0 ,P n are respectively data distributions at step 0 and n after sampling from conditional distributions. In prac-tice, we use one-step (CD-1) to approximate the gradients, which is commonly used for training RBM [34].  X  L ( x , y )
Regularization. Due to the strong imbalance in data between seen and unseen (missing) items, we find that CD tends to lead to hidden biases increasing over time. If most of the hidden units were activated during the training pro-cess, then the model would not differentiate the represen-tations among users. Hence, we incorporate regularization of the expected hidden activations around a desired level of activation  X  into the objective function [20]. As shown in Eq. (11), the aggregated gradients are updated at the same time with the direction found in Eq. (10), where  X  P is the ex-pected activation of hidden units.  X  is a tunable coefficient. L ( x , y ;  X ) = L ( x , y ;  X )  X   X 
After learning the model parameters, we estimate the la-tent representation, and predict unseen adoptions by recon-structing the visible layer from the hidden layer, by perform-ing one step of sampling to reconstruct the data as below:
Compared to the RBM model for adoptions only, incorpo-rating social network puts more constraints on deciding the user preferences, which will be affected by both global (all users) and local (their friends) patterns. In terms of learn-ing, one more benefit is the potential to reduce overfitting due to cross-modality patterns produced from two sides of observations. In addition, to deal with cold-start users with few or no observed adoptions, the model can make use of the observations from their social connections to infer item adoptions. This is related to the notion of cross-modality in [39] with similar structure, but with different types of stochastic units, targeted for modeling text and images. Figure 2: SocialRBM -Deep : The top layer h 2 has U hidden units, corresponding to U users. Each user is represented by a single hidden unit on the top layer with weights shared with their friends. For example, user u 1 has connections to u 3 and u 4 , thus the hidden units h 2 1 , h 2 3 and h 2 4 are available for encoding u adoptions. The other hidden units h 2 2 and h 2 5 will be unavailable (dashed). All K units in the middle layer h 1 will always be available to all users.
In this section, we propose an alternative approach that adapts the model architecture to the social network struc-ture. The intuition of SocialRBM -Deep is that instead of letting social connections bring users and their friends closer through shared observations, we allow friends to affect each other X  X  representations through sharing hidden units.
In terms of its structure, this model SocialRBM -Deep is a  X  X lipped X  version of SocialRBM -Wing , with the social network layer now stacked up on top of the item adoption layer. Its role also changes from visible units for observation to hidden units, as illustrated in Figure 2.

The structure of SocialRBM -Deep is reminiscent of a two-layer Deep Boltzmann Machine (DBM), thus its name. There are two layers of hidden units h 1 (middle layer) and h 2 (top layer). The middle layer is shared across all train-ing instances (users). However, one critical difference from DBM is that our top layer is not shared across all instances. Our structure is such that each user is represented at the top layer by a group of hidden units. When learning the representation of a user, only the groups of hidden units cor-responding to her own, as well as those of her friends X  could be activated. This induces sharing particularly among users with social connections. Without losing generality, in this work we use a group size of one, to keep the number of pa-rameters of SocialRBM -Deep the same with SocialRBM -Wing , thus establishing parity for comparison later. In other words, to encode the item adoptions of a user, SocialRBM -Deep makes use of ( F u + 1) hidden units, where F u is the number of friends of u . In Figure 2, for user u 1 with two friends, three hidden units are available at the top layer. Figure 3: A visual for the sequence of forward steps in the mean-field inference, in which at the first step, hidden units h 2 in the top layer will be randomized for initialization. The errors from both  X  1 vs.  X  2 in the regularization are backpropagated through this sequence for the gradient updates.

The energy function with all connections in the model is shown below: E ( x , h 1 , h 2 ) =  X  where h 1 and h 2 are the vectors of hidden units at the mid-dle and top layers respectively.

Learning. The parameter updates are similar to Eq. (10), except that P 0 = P ( h 1 , h 2 | x ) is now approximated by a variational distribution Q ( h 1 , h 2 ) [30] as described in Sec-tion 5.2. For the full distribution, we run n -step Gibbs sam-pler through following conditional distributions:
Our model is based on social network to allow sharing in the training process of the top layer. Due to the connec-tivity across layers, the effect of sharing permeates through all levels of the deep structure. The model is a combination of two kinds of sharing: one is  X  X lobal X  across all users (the first layer); the other is  X  X ocal X  based on the graph struc-ture, whereby users are co-trained only with direct friends or through mutual friends (via overlapping hidden groups). In contrast, the original RBM only explores global patterns.
To better initialize the parameters for mean-field steps, we apply pretraining stage for each layer in the deep network. As discussed in [30], mid-layer hidden units can be activated from both higher and lower layers; thus, the aggregation of posterior over these units could be contributed by halving the weights after learning, or duplicating them in training and keeping its value in the testing process.

Regularization. As in Section 4, we apply regulariza-tion to hidden activations. However, due to the use of vari-ational inference for posterior approximation (as discussed in Section 5.2), the gradients could be computed via back-propagation algorithm (illustrated in Figure 3) through n steps of mean-field update as discussed in [29]. The aggre-gate objective function is as follows: where  X  1 , X  2 are the desired levels of hidden unit activations in each of the two respective layers.
We apply the mean-field inference [30] to approximate the true posterior by a fully factorized distribution Q . Learning is conducted via minimizing the KL ( Q ( h 1 , h 2 ) || P ( h or maximizing the lower-bound of likelihood with respect to variational parameters  X  k = Q ( h 1 k = 1) , X  j = Q ( h 2 shown below: ln P ( x ;  X )  X  The fixed-point equations are produced below :
For experiments, we set 15 iterations for updating alter-natingly from Eq. (16) and Eq. (17) until convergence. Fi-nally, the probabilities for unseen items are computed via Eq. (18), and ranked in descending order for prediction.
The experimental objective is primarily to investigate the effects of homophily on RBM-based models for learning the representation of user preferences. We pursue this objective by evaluating the performance of comparable RBM-based models on two publicly-available 1 real-life datasets [6].
Datasets. The first dataset, Delicious , originally came from an online bookmarking site, whereby a set of users bookmark a set of URL X  X . These are essentially binary ob-servations of item adoptions. In addition, it is also an on-line social network, which allows users to indicate friendship links. The second dataset, LastFM , originated from an on-line radio site, where users can tag their favourite artists http://grouplens.org/datasets/hetrec-2011 No. of users 1,867 1,892 No. of items 69,226 17,632 No. of adoptions &lt; user, item &gt; 104,220 92,834 No. of social links &lt; user, user &gt; 15,328 25,434 Adoption density 0.08% 0.27% Social network density 0.44% 0.71% (items). These are also modeled as binary observations of adoptions. Similarly, it has a social network among users. The statistics for these two datasets are shown in Table 1. While the number of users are similar, Delicious is the larger and sparser dataset, with many more items and significantly lower adoption density. This sparsity also implies that it is the more difficult dataset for prediction. Other than their sizes, the two datasets are characteristically quite different. Delicious , driven by bookmarks, tends to have a greater level of personalization and expected homophily, whereas LastFM , driven by music artists, tends to have greater uni-formity due to the presence of popular artists with broad appeal. Experimentation with these two contrasting natures would allow us to derive greater insights.

We focus on binary adoptions, because our main concern here is on the effects of social networks. With some modifi-cation, the models could apply to multi-scale ratings, which we will consider for future work. Moreover, we do not use tag information from the datasets, for parity with the origi-nal RBM model [31] that does not use such features either.
Comparisons. Because of our objective of studying ho-mophily on RBM-based models, we can demonstrate this most clearly and directly by comparing the two proposed models SocialRBM -Wing and SocialRBM -Deep (with so-cial network) to the original RBM model [31] (without so-cial network). To establish parity among the models, we use the same dimensionality for the latent representations, i.e., K = 100 hidden units. This setting was also used in in [14, 31]. We will conduct this comparison in Section 6.1.
Although the dimensionality of the representation is the same, the models do not all have the same number of param-eters. The two models with social networks SocialRBM -Wing and SocialRBM -Deep have an identical number of parameters. Compared to RBM , both have an additional number of K  X U weight parameters, which are required to connect the social layer and the hidden units. To ensure that the observed effects are not due to these additional pa-rameters alone, in Section 6.2, we conduct another set of ex-periments comparing the same models ( SocialRBM -Wing or
SocialRBM -Deep respectively), but replacing the social networks with random networks of the same structure.
Finally, in Section 6.3, we briefly explore whether the learnt representations of friends tend to exhibit greater sim-ilarity after incorporating social networks during learning.
In this section, we conduct a comparison between the two proposed models and the baseline RBM model.

Task. Since one of the main applications of learning rep-resentations from preference data is for recommendation, here we evaluate the models on the task of predicting users X  item adoptions. The adoption data for each user is ran-
Figure 4: Recall@M for Delicious at various M  X  X . domly split into 80% for training vs. 20% for testing. For each dataset, we create ten such training/testing splits. For each user, we seek to predict the held-out item adoptions in the testing set. This is done by getting each model to learn from the training set, and output a ranked list of the top M items whose adoptions were not seen in the training set.
In the training stage, each sample data is divided into small batches of 100 instances (users) for each iteration. All models are trained with a learning rate of 0.003 in 1000 iter-ations util convergence. We also apply a momentum of 0.8 to speed up and a weight-decay of 0.001. For the pretrain-ing stage in SocialRBM -Deep , we train each layer in 500 iterations with the similar process discussed in [30].
Metric. There are various ways to evaluate recommen-dations [11, 27]. For sparse preference datasets such as ours, unseen item adoptions may not necessarily mean that a user dislikes the items; the user might simply have been unaware of them. This makes it difficult to compute precision accu-rately. However, since the observed item adoptions in the testing sets are known to be true positives, following [28, 36], we focus on measuring recall. The recall of a model X  X  prediction of top M items for a user is defined as follows: recall @ M  X  number of correctly predicted items in top M For comparison across models, we average the recall@M across all the users. We vary the value of M in the range of [20 ... 200]. Higher recall at lower M indicates a better re-sult, implying that the correct items tend to be among the top-ranked predictions. We average the results across the ten training/testing splits described above. Where appro-priate, we present not only the mean, but also the standard deviation, as well as statistical significance test results.
Analysis. First, we look into the Delicious dataset. Fig-ure 4 shows the cumulative recall up to top 200. As ex-pected, as M increases, recall generally increases, because the numerator of the recall equation above would increase, while the denominator is stable. Importantly, based on the trend lines in Figure 4, it is evident that both SocialRBM Wing and SocialRBM -Deep have higher recall results than the baseline RBM . Between the two models with social net-works, SocialRBM -Deep has a higher performance than SocialRBM -Wing . Interestingly, the former tends to in-crease faster in recall than the latter as M increases, indi-cating SocialRBM -Deep  X  X  greater effectiveness at placing the ground truth items higher in the prediction ranked lists.
To look into the differences between various models in greater detail, Table 2 shows not just the mean recall values, Table 2: Comparison of Recall@M (mean  X  stan-dard deviation) for Delicious. Best results are in bold.  X  (0.05 level) and  X  X  (0.01 level) indicate statis-tically significant improvement over RBM .  X  X  (0.01 level) indicates statistically significant improvement over SocialRBM -Wing .

Table 3: Comparison of Recall@M for LastFM. but also the standard deviations. We see that SocialRBM -Deep is the best (in bold). The standard deviations are also relatively small, implying that the mean values are quite reflective of the relative performances across models. We also conduct paired samples Student X  X  t -test for statisti-cally significant differences, indicating that the outperfor-mance by SocialRBM -Wing over RBM , as well as that by SocialRBM -Deep over the other two models, are indeed statistically significant.

We reiterate that the key result here is the relative outper-formance by the models with social network over the base-line RBM , of which the results here are strongly indicative. Compared to RBM , SocialRBM -Deep shows an increase by a factor of 1.5X to 2.5X. The increase by SocialRBM -Wing is smaller, but still significant. The absolute values of recall in Delicious are low, because it represents a very chal-lenging dataset, especially without using any tag feature. A random predictor would attain a recall@200 of merely 0.0029. Thus, the performance of our models represents an increase by an order of magnitude over a random baseline.
Turning to LastFM , we show the corresponding table of results in Table 3. Similar observations as made above for Delicious on the relative outperformance by the models in-corporating social networks can also be made for LastFM .
Comparing the two datasets, in terms of the absolute val-ues, the results on Delicious are lower than those on LastFM . This can be explained by the disparity in the adoption den-sities shown in Table 1, i.e., 0.08% for Delicious vs. 0.27% for LastFM . The lower density indicates greater uncertainty in prediction, thus higher error rate. However, the relative improvements among models on Delicious are more signifi-cant than on LastFM . We hypothesize that this comes from the different characteristics of two datasets. For popular items, such as music artists, even unrelated people may still prefer similar music artists. Conversely, bookmarks are less frequent, and may be more prone to social influence, ex-plaining the greater effects of homophily seen in Delicious . Figure 5: Relative Comparison of Social Network vs. Random Network for Delicious.
 Figure 6: Relative Comparison of Social Network vs. Random Network for LastFM.
We try to keep the models comparable by using the same dimensionality of latent representation across models. To investigate the effects of the social network itself, in this sec-tion we conduct another set of experiments, which attempt to control the effects of the model structure, and isolate the effects of homophily alone. The best way to do so is to com-pare within the same model, but swap the social network graph with a random graph of a similar structure.

Randomization. Ascertaining data mining results via randomization was advocated by [9]. Here, we follow the way they generate random graphs, so as to maintain the structure of the original graph in terms of the degree of each node. Essentially, a random graph is obtained by randomiz-ing the adjacency matrix of the original graph, while keeping the same row and column sums. For each dataset, we gen-erate ten random networks from the original social network, and compare the model run with these random networks with the same model run on the social network.

Analysis. For this study, we still use the recall@M met-ric described above. However, our focus here is not on the absolute performance, but on whether the same model will produce different levels of performance when run with social network or random network. To present these results clearly, we peg the performance by the model with social network at 100% (the absolute values can be seen in Table 2 and Table 3), and present the relative results by the same model but with random network as a percentage of the former.
For Delicious , Figure 5(a) shows the relative compari-son of recall by SocialRBM -Wing , when social network is replaced by random network. Figure 5(b) shows that of SocialRBM -Deep . For both, we witness significant drops Table 4: Comparison of MAP for Similarity Ranking of Social Links.  X  X  (0.01 level) indicates statistically significant improvement over RBM . in recall for random networks, e.g., recall@20 drops to 55% for SocialRBM -Wing , and 65% for SocialRBM -Deep . Figure 6(a) and Figure 6(b) show similar experiments on LastFM . They tell a similar story, though the drop is modest, with random networks attaining 98% of the performance of social network. This is another evidence of the relatively weaker effect of homophily on LastFM than on Delicious , which we trace to the different natures of the items. If the patterns of adopting popular artists are similar among users, changing the network structure would not lead to significant changes in adoptions, thus explaining the smaller drop rates.
For both Delicious and LastFM , the drops due to random networks are statistically significant at 0.01 level in all cases. This supports the hypothesis of the measurable effects of homophily on the RBM-based models for item adoptions.
So far we learn that improvements arise from the right model architecture, as well as the right network structure. This comes from the intuition built into the models to bring together the representations of socially connected users, ei-ther via hidden units ( SocialRBM -Deep ) or visible units (
SocialRBM -Wing ). In this final experiment, we briefly explore this intuition, by using the similarities among the learnt representations of users to rank one X  X  friends.
Task. The user representation is a vector of hidden units inferred from Sections 4 and 5. For each user, we rank other users based on the cosine similarity to their learnt represen-tations. The aim is to see whether the user X  X  friends would be ranked highly in this list, i.e., friends have similar rep-resentations. This study is not meant to be predictive, but rather reflective , whether the learnt representations may be correlated to the social connections in the training set. Metric. We borrow a metric from information retrieval: Mean Average Precision or MAP [25]. This is computed as the mean of the average precision across all users, as follows. AvgPrecision j is computed by averaging the precisions at each of u j  X  X  recall points (the ranks of u j  X  X  friends).
Analysis. The main expectation is that if the proposed models do indeed absorb the social network information well during the learning phase, they would perform better at this task than the baseline RBM . Table 4 shows that on both datasets, SocialRBM -Wing and SocialRBM -Deep have significantly higher MAP than RBM . For LastFM , they increase by a factor of 1.3X to 1.7X. For Delicious , they in-crease by an even larger factor of 2.5X to 3X, supporting the hypothesis of a stronger homophily effect on Delicious . Overall, these results support that the learnt representa-tions of friends become more correlated as a result of the homophily assumptions built into our proposed models.
We study the homophily effect on RBM-based models for preference datasets, proposing two models for incorpo-rating social network. The first, SocialRBM -Wing , oper-ates by fitting two sets of observations: item adoptions and social network. The second, SocialRBM -Deep , uses so-cial network as a form of sharing hidden units at the top layer. These models are verified on two publicly available real-life item adoption datasets. The main conclusions are two-fold. First is the importance of the right architecture, as evidenced by SocialRBM -Deep  X  X  outperformance over SocialRBM -Wing and RBM . Second is the importance of the right network information, as evidenced by the outper-formance by social network over random network for each model. For future work, we plan to investigate enrichments to the proposed models, such as additional modalities. This research is supported by the National Research Foun-dation, Prime Minister X  X  Office, Singapore under its NRF Fellowship Programme (Award No. NRF-NRFF2016-07).
