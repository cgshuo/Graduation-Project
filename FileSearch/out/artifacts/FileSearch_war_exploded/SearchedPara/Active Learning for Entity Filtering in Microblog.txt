 Monitoring the reputation of entities such as companies or brands in microblog streams (e.g., Twitter) starts by selecting mentions that are related to the entity of interest. Entities are often ambigu-ous (e.g.,  X  X aguar X  or  X  X ord X ) and effective methods for selectively removing non-relevant mentions often use background knowledge obtained from domain experts. Manual annotations by experts, however, are costly. We therefore approach the problem of entity filtering with active learning, thereby reducing the annotation load for experts. To this end, we use a strong passive baseline and ana-lyze different sampling methods for selecting samples for annota-tion. We find that margin sampling X  X n informative type of sam-pling that considers the distance to the hyperplane used for class separation X  X an effectively be used for entity filtering and can sig-nificantly reduce the cost of annotating initial training data. H.3.3 [ Information Search and Retrieval ]: Information filtering Text classification; Entity filtering; Active learning; Twitter
With increasing volumes of social media data, monitoring and analyzing this data is a vital part of the marketing strategy of busi-nesses. The extraction of topics, conversations, and trends around an entity (such as a company, organization, celebrity) allows ana-lysts to understand and manage the entity X  X  reputation. It is infea-sible to manually process every single tweet or blogpost that may have been written about an entity. Since entity names are often ambiguous, filtering social media for relevant information X  X hat is, Entity Filtering (EF) X  X aves tedious work and is a vital pre-processing step for further automation of Online Reputation Moni-toring (ORM) [8, 12, 13]. If the performance of the EF module de-creases, the performance of all subsequent modules is harmed [13]. EF on social media is therefore an active field of research and has  X  Now at Google Switzerland.
 previously been considered in various settings: at the WePS-3 eval-uation effort [1] and as part of the RepLab 2012 and 2013 chal-lenges [2, 3]. Missing important tweets and news items about an entity of interest can be disastrous and expensive [9]. An entity may need to react immediately to avoid long-lasting harmful pub-licity. The ORM industry therefore seeks to find a balance between manual and automatic filtering.

Our approach to EF is based on active learning [11], a semi-automatic machine learning process interacting with the user for updating the classification model. It selects instances that are meant to maximize the classification performance with minimal anno-tation effort. Active learning is especially attractive in the set-ting of EF for ORM as it promises to (a) use the analysts X  back-ground knowledge and understanding to improve performance, and (b) capture new topics and problems without exhaustive annotation effort. Active learning has been widely used in information access tasks [10, 16] and text categorization [7]. Below, we present an active learning framework for EF. 1 We start from simple but com-petitive passive baselines and then examine alternative strategies for sampling examples for learning and their applicability for EF. In this section we elaborate our active learning approach to EF. Briefly, (1) instances are represented as feature vectors; (2) in-stances from the training dataset are used for building the initial classification model; (3) test instances are automatically classified using the initial model; (4) we sample candidates for additional labeling; this step is performed by margin sampling: the instance closest to the class separation is selected; and (5) the user manu-ally inspects the instance and labels it; the labeled instance is then considered when updating the model. The active learning process is repeated until a termination condition is satisfied.

We use a Support Vector Machine 2 (SVM) classifier. Our active learning approach can be split into the selection of candidates for active annotations, annotation of the candidates and updating the model . Therefore, one iteration of our learning model follows the following three steps: (1) select the best candidate x from the test set T ; (2) annotate the candidate x ; and (3) update the model.
If the resources are available, the training data used to initialize the model can be a large manually annotated (bulk) set of tweets published before the test set. Below we detail the candidate selec-tion, candidate annotation, and model updating steps.
Candidate selection is the process of sampling candidates that are used for annotation. A successful selection approach selects candi-
The code is available at http://damiano.github.io/al-ef http://scikit-learn.org/stable/modules/svm.html dates that, when annotated, improve the model. Standard baseline approaches are: passive learning without sampling, which is iden-tical to non-active learning, random sampling , which samples ran-domly from the pool, and margin sampling , which samples close to the margin of the classification boundary. We also propose two further approaches to improve margin sampling. For reranking , we rerank the list of samples based on density.
 Passive learning. Passive learning does not use any active learning at all. We only initialize the model without retraining it, therefore skipping the candidate selection, training, and updating phase. Random sampling. Here, the candidate instance is sampled with-out replacement from the training set. There is no informed prior on the instances. Random sampling has proven to be effective for other tasks, e.g., building dependency treebanks [5], or clinical text classification [6].
 Margin sampling. The most commonly used sampling method in binary classification problems is uncertainty sampling [11]. We consider a specific uncertainty sampling method especially suitable for support vector machines [15]: margin sampling . We measure the uncertainty of a candidate x based on the distance to the margin: where P ( C 1 | F x ) and P ( C 2 | F x ) are the probabilities that the candidate x , as represented by the feature vector F x , generates the classes C 1 and C 2 , respectively.

Candidates are sampled based on the classification difficulty, there-by selecting candidates where the classifier is less confident. Fol-lowing this, the candidate x to be annotated from the test set T is selected as follows: This candidate x is then annotated and used to update the model. For a linear kernel of the SVM this means: instances (tweets here) that are closest to the class separation are selected.
 Margin*Density. Following [17], we incorporate the density of a candidate into the maximization criterion of a candidate ranker. In-tuitively, while outliers may be difficult to predict, they are also not very helpful in improving the classifier: where KNN ( x, T, K ) are the K most similar instances to x in the test set T . The similarity sim ( x, x i ) between two instances x, x based on the Jaccard similarity. We say that the K-Density( x ) is the Density( x ) , if K is dynamically set to | T | .

In the Margin*Density setting, the candidate x to be annotated from the test set T is selected as follows: where D ( x i ) is a placeholder for K-Density( x ) or Density( x ) .
Once the candidates are selected, the algorithm collects annota-tions from the user. Section 3.2 details how we simulate the user input.
The training of the model is fast. 3 We therefore decided to re-train the model with every freshly annotated instance. The instance
In our experiments, a few dozen seconds on a workstation with 16 cores, 2.6GHz, and 96GB RAM workstation. In the ORM scenario, the training set is entity-oriented and would typically not include more than a thousand tweets. and its annotation are added to the training set and the model is re-trained. As commonly done, the weights for both training and new instances are uniform. We aim to analyze the effectiveness of active learning for EF. Below, we describe the dataset, the feedback scenario, and how we evaluate.
We use the RepLab2013 [3] dataset, which is, to our knowledge, the largest dataset available for the EF task in microblog posts. The dataset comprises a total of 142 , 527 tweets in two languages: English and Spanish. The dataset consists of 61 entities in four domains: automotive, banking, universities and music. For every company, 750 ( 1 , 500 ) tweets were used as training (test) set, on average, with the beginning of the training and test set being six months apart. Crawling was performed from June 1, 2012 to De-cember 31, 2012 using each entity X  X  canonical name as query (e.g.,  X  X tanford X  for Stanford University).

Tweets are represented as set-of-words: bag-of-words with bi-nary occurrence ( 1 if the word is present in the tweet, 0 if not). We removed punctuation, lowercasing, tokenizing by white spaces, reducing multiple repetitions of characters (from n to 2 ), and stop-words.
Without direct users, the usual approach to model an active learn-ing setting is to take the annotations from the test set. This simu-lates the user feedback; this is also what we do. We therefore train on the dedicated training set and sample from the entire test set.
For our experiments we use Support Vector Machines, using a linear kernel. 5 The penalty parameter C is automatically adjusted by weights inversely proportional to class frequencies. We use the default values for the rest of parameters.

We compare the effectiveness using different N test of sampled tweets with the effectiveness of two passive supervised learning ap-proaches: the initial model and the best approach at RepLab2013. We compare random sampling, margin sampling, and the diverse instantiations of density sampling methods listed in the previous section. Table 1 provides an overview over the acronyms used for the runs that we consider. The passive run is the underlying base-line for active learning; it is based on the training set. The best run Table 1: Runs used in our experiments. MSD and MS-RRD can be combined with a K for K -Density.
 is the score for the best performing system at RepLab2013. RS and MS are active learning runs, using random and margin sampling, http://nlp.uned.es/replab2013
We tested different algorithms (Na X ve Bayes, Decision Trees) and this is the one that obtained the best results in terms of the initial (passive learning) model. respectively. MSD combines margin with density sampling, based on the candidate set. Finally, MS-RRD reranks margin sampling based on density.
Unless stated otherwise, we use the official evaluation metrics from the RepLab2013 Filtering Subtask: accuracy and the har-monic mean of reliability and sensitivity F 1 ( R, S ) [4]. We use the Student X  X  t-test to evaluate the significance of observed differ-ences, using Bonferroni normalization where appropriate. We de-note significant improvements with N and M ( p &lt; 0 . 01 and p &lt; 0 . 05 , respectively). Likewise, H and O denote declines.
We analyze the passive baseline in Section 4.1. We then dis-cuss the performance of margin sampling and random sampling in Section 4.2 and the impact of training in Section 4.3. Section 4.4 analyses the performance of density sampling. In order to establish our passive baseline as state of the art for EF, we compare it with the best performing system at RepLab. Table 2: Performance of passive baseline and best RepLab2013 system.

Table 2 compares the effectiveness of best and passive runs in terms of accuracy and F 1 ( R, S ) . A number of observations are worth making. First, in terms of accuracy, the effectiveness of all the runs is above 0 . 9 , leaving little room for improvement. The passive run outperforms the best run, but the difference is not statis-tically significant. In contrast, F 1 ( R, S ) reveals more differences between the runs. Taking into account that our passive approach that relies on sets-of-words is more efficient than the approach used in best  X  X hich makes use of external knowledge bases X , we con-clude that passive is a suitable starting point to use for active learn-ing of EF.
We compare the effectiveness of two candidate selection meth-ods for active learning, random and margin sampling. We also com-pare our active learning approach to EF against the state-of-the-art in the EF task and against our passive learning baseline. Figure 1: Accuracy (1a) and F 1 ( R , S ) (1b) vs. percentage of manually annotated tweets N test .

Fig. 1 compares the MS and RS runs with the passive runs in terms of accuracy (1a) and F 1 ( R, S ) (1b). MS outperforms passive and best after inspecting only 2% of the test data (which, on aver-age, corresponds to 30 tweets per entity), obtaining a F score of 0 . 52 (vs. 0 . 49 ). Using N test = 5% , MS significantly outperforms best , obtaining an F 1 ( R, S ) -score of 0 . 63 other hand, RS needs more feedback to be able to reach best . Us-ing N test = 5% it achieves an F 1 ( R, S ) -score of 0 . 48 , while using 10% of sampled tweets achieves a score of 0 . 52 . The graphs also show MS outperforming RS consistently. Here, differences begin to be statistically significant from 3%  X  5% , with F 1 ( R, S ) -scores of 0 . 57 M , 0 . 63 N . Interestingly, while RS shows a linear behav-ior, MS starts with an exponential gain of effectiveness in terms of F 1 ( R, S ) . In terms of F 1 ( R, S ) , the effectiveness reached by RS after inspecting 10% of the test data can be achieved by MS considering only 2% . This amounts to an 80% reduction in cost.
In sum, our active learning approach requires small amounts of feedback to outperform state-of-the-art passive EF systems. Addi-tionally, margin sampling significantly outperforms random sam-pling.
We examine to which degree active learning can reduce the cost of the initial training phase. Initializing any supervised approach X  whether passive or active X  X o EF has a cost derived from annotating the initial training data. We look at different percentages of training data used to initialize the model. Figure 2: F 1 ( R , S ) -scores with different percentages of train-ing data for the initial model ( x -axis) and different percent-ages of test data for manually inspection during the active learning process ( y -axis), using margin (2a) or random (2b) sampling. Red/dark and yellow/light correspond to lower and higher F 1 ( R , S ) values, respectively.

Fig. 2 shows heat maps representing the evolution of F 1 ( R, S ) for different percentages of training data ( x -axis) and sampled test data ( y -axis), for MS (2a) and RS (2b). Red/dark and yellow/-light correspond to lower and higher F 1 ( R, S ) values, respectively. MS needs less training data to obtain competitive F 1 ( R, S ) -scores than RS . For instance, initializing the model with 10% and in-specting 10% of test data using MS achieves an F 1 ( R, S ) -score of 0 . 55 , while RS achieves only 0 . 44 . Considering only 10% (i.e., 75 tweets) as the initial training set, the effectiveness of best can be reached after 100 tweets ( N test &lt; 7% ) using MS . In terms of annotation cost, this corresponds to a 75% reduction.

In sum, the cost of training the initial model can be substantially reduced by using active learning, especially with margin sampling.
We compare the effectiveness of favoring samples that are close to the margin and similar to other, unknown, samples in the can-didate set. Fig. 3a compares margin*density sampling ( MSD ) for tweets over different K and K , for MSD and MS-RRD , respectively. K = T and the best performing MSD reranking ( MS-RRD , T = 3 ) in terms of F 1 ( R, S ) with margin sampling ( MS ). Both approaches to MSD performs worse than MS . Experiments that consider differ-ent numbers of instances K to compute the density (Fig. 3b) show that the performance is quite constant over K .

The performance of reranking based on density highly depends on the quantity of items of the initial ranking that are being con-sidered (Fig. 3c). The performance drops significantly with the number of candidates used to rerank. While the performance of MS-RRD is closer to MS than MSD , vanilla MS performs signifi-cantly better than all density approaches.

One entity, however, stands out: Chrysler reaches an F 1 ( R, S ) of 1 using MSD and an F 1 ( R, S ) of 0 for normal margin sam-pling. 6 This entity has very clear topical clusters, about winning prizes , chrysler 300 , chrysler building , and union rights . Manually classifying one of the elements in a cluster automatically classifies all the other elements. The other entity, Bankia , where MSD per-forms much better than MS , features a similar topical distribution.
We can conclude that unlike what was found in previous work, for EF density usually does not provide complementary informa-tion to margin sampling on this dataset.
We have examined the feasibility of using active learning for en-tity filtering on tweets. We have shown that much less annotation is needed when annotation is done on the fly, i.e., using active learn-ing with 10% of the initial training set can lead to a 75% reduc-tion of costs. We have contrasted several state-of-the-art sampling methods and have shown that margin sampling works best.

Future work should focus on entity filtering specific sampling algorithms and exploring the task in streaming scenarios. Since entity filtering is a daily task, an active learning streaming scenario simulates the ORM process even better.

Note that F 1 ( R, S ) tends to zero when the system displays a non-informative behavior [4, 14].
