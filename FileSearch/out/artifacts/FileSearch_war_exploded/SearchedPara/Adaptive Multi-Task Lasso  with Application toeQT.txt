 association SNPs from high dimensional eQTL datasets when p  X  N , where p is the number of knowledge to improve the performance of detecting eQTLs. mapping problems, which can yield a sparse and easily interp retable solution via an  X  widely used Lasso is limited to treating all SNPs equally. incorporate prior knowledge of genomic locations.
 advantages of adaptive multi-task Lasso over many other com petitors. Let X setting includes one design matrix X and multiple tasks (genes) for k = 1 , . . . , K , where  X  is a standard Gaussian noise. We further assume that X P 2.1 Lasso and Multi-task Lasso for association mapping problems. Mathematically, it solv es the  X  where  X  determines the degree of regularization of nonzero  X  large. Due to the singularity at the origin, the  X  are posterior mode estimates under a multivariate independ ent Laplace prior for  X  [2]. which solves the problem, where k  X  related tasks via the  X  estimate under appropriate priors with fixed scaling parameters. useful prior knowledge for effective association analysis . 2.2 Adaptive Multi-task Lasso where  X  and  X  are the scaling parameters for the  X  parameters  X  First, unlike the multi-task Lasso, which contains the  X  sity, the  X  Finally, as to be extended, unlike Lasso and multi-task Lass o which treat  X  scaling parameter, we can adaptively penalize each  X  knowledge on SNPs).
 where f j standardized, i.e., P features, we further add the constraints that P interpreted as a regularization on the feature weights  X   X  0 and  X   X  0 . flexible and easily extensible. Recall that the Lasso estimates can be interpreted as MAP estimates under Laplace priors. Similarly, to achieve a framework that enjoys an elegant Bayesian interpretation, we define a Bayesian network and treat the adaptive multi-task learning problem as finding its MAP estimate. Specifically, we build a Bayesian network as shown in Fig. 2 in order to compute the MAP estimate of  X  under adaptive scaling parameters, {  X ,  X  } . We define the conditional probability of  X  given scaling parameters as, Extension to a fully Bayesian approach is our future work. hard to compute, we use its upper bound, as given by, method and a coordinate descent method, respectively. For the first step of optimizing L over  X  and  X  , the sub-problem is to solve where P problem (4) using a modified version of the algorithm propose d for the sparse group Lasso. check the group sparsity condition that  X  we check whether  X  k we optimize problem (4) over  X  k continued until a convergence condition is met.
 dient of its objective function with respect to  X  k where g and h are sub-gradients of the  X  Then, we check the group sparsity that  X  According to subgradient conditions, we need to have a g k g the minimal square  X  k g where c k update is needed; otherwise, we continue to the next step of c hecking whether  X  k Again, we start by assuming  X  k According to the definition of the subgradient h k otherwise,  X  k j 6 = 0 R). We used majorize-minimize algorithm with gradient desc ent [15]. weights until convergence. Note that the parameters  X  levels, are determined by cross or hold-out validation. ( f SML (sparse multi-task Lasso which is AML without adaptive w eights), A+  X  Lasso penalty), Single SNP [17], Lasso and  X  parameters (e.g.  X  SML, A+  X  A+  X  To further validate the effectiveness of Lasso penalty, we r un AML and A+  X  however, A+  X  Adaptive weights help not only reduce false positives but al so increase true positives. Fig. 4 shows the learned feature weights of  X  (  X  is al-most identical to  X  and not shown here). The results are based on 100 simulations for each association strength 0.3, 0.5, 0.8 and 1, and half of error bar represents one standard deviation from the mean. We could observe that discrete features f est weights are assigned to f reasonable because f large standard deviation (STD: 1) compared to that of fea-tures f most important since they discriminate true association SNPs with a high probability 0.8. is small (i.e., 0.3), AML and A+  X  in terms of test errors is: A +  X  continuous feature (conservation score). For a discrete fe ature, we set its value as f j feature is found on the j -th SNP, f j All the features are then standardized. Fig. 6 represents  X  learned from the yeast eQTL dataset (  X  is almost identical to  X  ). The features are ncRNA ( f noncoding exon ( f binding site ( f son ( f gene ( f tures turn out to be important including ncRNA, snRNA, binding site, 5 X  UTR intron and snoRNA as well as one continuous feature, i.e., conservation score. These re-sults agree with biological insights. For example, ncRNA, snRNA and snoRNA are potentially important for gene regulation since they are functional RNA molecules hav-levels of many genes. They were not reported as eQTL hotspots in Yvert et al. [20]. outperforms other methods via  X  SNPs are associated with a large number of genes.
 Acknowledgments This work was done under a support from NIH 1 R01 GM087694-01, NIH 1RC2HL101487-01 (ARRA), AFOSR FA9550010247, ONR N0001140910758, NSF Caree r DBI-0546594, NSF IIS-0713379 and Alfred P. Sloan Fellowship awarded to E.X.
