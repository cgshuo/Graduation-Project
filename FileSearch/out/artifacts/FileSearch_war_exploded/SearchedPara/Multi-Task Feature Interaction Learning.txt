 Linear models are widely used in various data mining and machine learning algorithms. One major limitation of such models is the lack of capability to capture predictive infor-mation from interactions between features. While introduc-ing high-order feature interaction terms can overcome this limitation, this approach dramatically increases the model complexity and imposes significant challenges in the learn-ing against overfitting. When there are multiple related learning tasks, feature interactions from these tasks are usu-ally related and modeling such relatedness is the key to im-prove their generalization. In this paper, we propose a novel Multi-Task feature Interaction Learning (MTIL) framework to exploit the task relatedness from high-order feature in-teractions. Specifically, we collectively represent the fea-ture interactions from multiple tasks as a tensor, and prior knowledge of task relatedness can be incorporated into dif-ferent structured regularizations on this tensor. We formu-late two concrete approaches under this framework, namely the shared interaction approach and the embedded interac-tion approach. The former assumes tasks share the same set of interactions, and the latter assumes feature interac-tions from multiple tasks share a common subspace. We have provided efficient algorithms for solving the two for-mulations. Extensive empirical studies on both synthetic and real datasets have demonstrated the effectiveness of the proposed framework.
  X 
Computing methodologies  X  Multi-task learning;  X  Information systems  X  Data mining; muti-task learning; feature interaction; structured regular-ization; tensor norm
Linear models are simple yet powerful machine learning and data mining models that are widely used in many ap-plications. Due to the additive nature of the linear models, it can fully unleash the power of feature engineering, allow-ing crafted features to be easily integrated into the learning system. This is a desired property in many practical appli-cations, in which high-quality features are the key to pre-dictive performance. Moreover, efficient parallel algorithms are readily available to learn linear models from large-scale datasets. Despite its attractive properties, one apparent lim-itation of such models is that they can only learn a set of individual effects of features contributing to the response, due to its linear additive property. Thus when a part of the response is derived from interactions between features, such models would not be able to detect such non-linear predictive information, thereby leading to poor predictive performance.

In practice, high-order feature interactions are common in many domains. For example, in genetics studies, envi-ronmental effects and genetic-environmental interaction are found to have strong relationship with the variability in adoptee aggressivity, conduct disorder and adult antisocial behavior [7]. Similarly, the interaction effects between con-tinuance commitment and affective commitment was found in predicting annexed absences [28]. Also, a recent study of depression found that genotype, sex, environmental risk and their interaction have combined influence on depression symptoms [12]. It is also reported that the interaction of brain-derived neurotrophic factor and early life stress expo-sure are identified in predicting syndromal depression and anxiety, and associated alterations in cognition [16]. In biomedical studies, many human diseases are a result of complicated interactions among genetic variants and envi-ronmental factors [19]. One intuitive solution to overcome this limitation is to augment interaction terms into linear models, explicitly modeling the effects from the interactions. However, this will dramatically increase the model complex-ity and lead to poor generalization performance when there is limited amount of data [9, 11, 23, 26, 35].

On the other hand, when there are multiple related learn-ing tasks, the multi-task learning (MTL) paradigm [1, 4, 8] has offered a principled way to improve the generalization performance of such learning tasks by leveraging the related-ness among tasks and performing inductive transfer among them. The past decade has witnessed a great amount of success in applying MTL to tackle problems where large amount of labeled data are not available or creating such datasets incurs prohibitive cost. Such problems are espe-cially prevalent in biological and medical domains, where MTL has achieved significant success, including data analy-sis on genotype and gene expression [21], breast cancer di-agnosis [37] and progression modeling of Alzheimer X  X  Dis-ease [18], etc. The MTL improves generalization perfor-mance by learning a shared representation from all tasks, which serves as the agent for knowledge transfer. Structured regularization has provided an effective means of modeling such shared representation and encoding various types of domain knowledge on tasks [1, 20, 24, 33]. The attractive benefits provided by MTL make it an ideal scheme when learning problems involve multiple related tasks with fea-ture interactions, because tasks may be related with each other by shared structures on feature interactions. For ex-ample, predicting various cognitive functions may involve a shared set of interactions among brain regions.

However, many existing MTL frameworks are based on linear models [1] in the original input space. Thus they can-not be directly applied to explore task relatedness in the form of high-order feature interactions. On the other hand, although traditional nonlinear MTL methods based on neu-ral networks (e.g., [2]) can exploit non-linear feature interac-tions to some extends, it is generally difficult to encode prior knowledge on task relatedness to such models. In this paper, we propose a novel multi-task feature interaction learning framework, which learns a set of related tasks by exploit-ing task relatedness in the form of shared representations in both the original input space and the interaction space among features. We study two concrete approaches under this framework, according to different prior knowledge about the relatedness via feature interactions. The shared interac-tion approach assumes that there are only a small number of interactions that are relevant to the predictions, and all tasks share the same set of interactions; the embedded inter-action approach assumes that, for each task, the feature in-teractions are derived from a low-dimensional subspace that is shared across different tasks. We have provided formula-tions and efficient algorithms for both approaches. We con-duct empirical studies on both synthetic and real datasets to demonstrate the effectiveness of the proposed framework on leveraging feature interactions from tasks. The contribu-tions of this paper are three folds:
The remainder of this paper is organized as follows: Sec-tion 2 reviews related work of MTL and models involving feature interactions. Section 3 introduces the framework for MTIL. The two approaches under MTIL have been given in 4. Section 5 presents the experimental results on both synthetic and real datasets. Section 6 concludes the paper. The proposed research is related to existing work on MTL and feature interaction learning. In this section, we briefly summarize the these related work and show how our work advances these areas. MTL has been extensive studied over the last two decades. In the center of most MTL algorithms is how task relation-ships are assumed and encoded into the learning formula-tions. The concept of learning multiple related tasks in par-allel was first introduced in [8]. It was demonstrated in multiple real-world applications that adding a shared repre-sentation in neural network tasks can help others get better models. Such discovery had inspired many subsequent re-search efforts in the community and applications in diverse application domains. Among these studies, the regularized MTL framework has been pioneered by [13]. The regular-ization scheme can easily integrate various task relationship into existing learning formulations to couple MTL, thus pro-viding a flexible multi-task extension to existing algorithms. It is well adopted and is soon generalized to a rich family of MTL algorithms.
 MTL via Regularization. Among the work in the regu-larization based MTL scheme, there are many different as-sumptions about how tasks are related, leading to different regularization terms in the formulation. For example, one common assumption is that the tasks share a subset of fea-tures, and the task relatedness can be captured by imposing a group sparsity penalty on the models to achieve simultane-ous feature selection across tasks [33, 24]. Another common assumption is that the models of tasks come from the same subspace, leading to a low-rank structure within the model matrix. Directly penalizing the rank function leads to NP-hard problems, and one convex alternative is to penalize the convex envelop of the rank function, i.e., trace norm. This encourages low-rank by introducing sparsity to the singular values of the model matrix [20]. In [1], the authors studied a MTL formulation that learns a common feature mapping for the tasks and assumed all tasks share the same features after the mapping. The authors have shown that this as-sumption can also be equivalently expressed by a low-rank regularization on the model. There are many more formu-lations that fall into this category of formulation to capture task relatedness by designing different shared representa-tion and regularization terms, such as cluster structures [38], tree/graph structures [21, 10], etc. However, to the best of our knowledge, all of these formulations do not consider fea-ture interactions in the model, and extensions to consider interactions are not straightforward. In this work, we will extend the MTL framework to enable knowledge transfer not only in the original input space, but also in higher-or-der feature interaction space.
 Multilinear MTL. The use of tensor in MTL has shown to be very effective in representing structural information un-derlying in MTL problems. In [27], Romera-Paredes et al. proposed a multilinear multitask (MTMTL) framework that arranges parameters of linear effects from all tasks into a ten-sor W , by which they are able to represent the multi-modal relationships among tasks. In a dataset containing multi-modal relationships, tasks can be referenced by multiple in-dices. In MTMTL, the authors employed a regularizer on W to induce a low-rank structure to transfer knowledge among Figure 1: Illustration of MTL with feature inter-actions. (a) the feature interactions from multiple tasks can be collectively represented as a tensor Q ; group sparse structures (c) and low-rank structures (b) in feature interactions can be used to facilitate multi-task models. tasks. The optimization problem contains the minimization of tensor X  X  rank, which leads to solving a non-convex prob-lem. Thus the authors develop an alternating algorithm, employing the Tucker decomposition and convex relaxation using tensor trace norm. Although the authors also used a tensor representation in MTL, the learning formulations, implications, as well as the meaning of such the tensor is fun-damentally different from those in our work. The proposed MTIL framework utilizes tensor to capture the relatedness among tasks and transfer knowledge through high-order fea-ture interactions, which cannot be achieved by any existing MTL formulations. Note that the tensor in MTMTL is in-dexed by multi-modal tasks. In MTIL, the tensor is indexed by features and tasks, which is clearly different from the aforementioned work. In the proposed embedded interaction approach for MTIL, however, we face a similar challenge in MTMTL to seek a solution involving a low-rank tensor.
In many machine learning tasks, we are interested in learn-ing a linear predictive model. Given the input feature vector of a sample, the response is given by a linear combination of these features, i.e., a weighted sum of the features. Be-cause of this reason we call them linear effects. There are strong evidences found in many complex applications that, in addition to the linear effects, there are also effects from high-order interactions between such features. As a result, there are considerable efforts from both academia and in-dustry aiming at addressing this limitation by removing the additive assumption and including interaction effects.
To overcome the dimensionality issues introduced by in-teraction effects, two types of heredity constraints have been studied [5]; namely strong hierarchy in which an interac-tion effect can be selected into the model only if both of its corresponding linear effects have been selected, and weak hierarchy, in which an interaction effect can be selected if at least one of its corresponding linear effects has been se-lected. In [11], the authors proposed an approach known as SHIM to identify the important interaction effects. SHIM extends the classical Lasso [29] and enforces a strong hier-archy. An iterative algorithm was proposed based on Lasso, which may not scale to problems with high dimensional fea-ture space. Radchenko et. al proposed the VANISH method to address the problem [26]. They developed a convex for-mulation with a refined penalty that can not only learn the sparse solution, but also treat the linear and interaction ef-fects using different weights. This way, the main effect could have more influence on the prediction. In [5], a hierarchical lasso was proposed to search for interactions with large main effects instead of considering all possible interactions. The authors proposed an algorithm based on ADMM for strong hierarchy lasso and a generalized gradient descent for weak hierarchical lasso. More recently, Liu et al. [23] proposed an efficient algorithm for solving the non-convex weak hier-archical Lasso directly, based on the framework of general iterative shrinkage and thresholding (GIST) [17]. The au-thors proposed a closed form solution of proximal operator and further improved the efficiency of solving the subprob-lem of proximal operator from quadratic to linearithmic time complexity.

In many real work applications there are multiple related tasks. When those these tasks involve interaction effects, the tasks could be related via the high order feature interactions. In our paper, we propose to address the model complexity issue from interaction effects using a new perspective, by leveraging such relatedness.
In this section, we present the framework of Multi-Task feature Interaction Learning (MTIL). For completeness, we give a self-contained introduction of our work. We will de-rive concrete learning algorithms under this framework in Section 4.
 Linear and Interaction Effects. Consider the traditional linear models. For an input feature vector x  X  R d and a scalar response y , we have assumed the following underlying linear generative model: where w  X  R d is the weight vector for linear effects, and N (0 , X  2 ) is a Gaussian noise. A linear model f ( x ; w )= x can be a quite effective prediction function. However, if the underlying generative model includes effects from feature interactions, i.e., where x i x j Q i,j is the joint effect between the i th feature and the j th feature, and Q i,j is the weight for this joint effect. This type of feature interactions have been commonly found in many applications. If the training data follow this distri-bution then the linear model is not enough to capture the relationship between input features and output responses. One of the approaches is to introduce non-linear feature in-teraction terms into the linear model. That is, we can denote it as a quadratic function: where w  X  R d and Q  X  R d  X  d collectively represent the parameters for linear effects and interaction effects, respec-tively. We note that Q is typically symmetric because this representation includes two terms involving feature i and j : x x j ( Q i,j + Q j,i ) and it also includes second-order feature transformations of the original features x 2 i Q i,i . Discussions on Feature Interactions. In supervised learning, we seek a predictive function that maps an in-put vector x  X  R d to a corresponding output y  X  R .Let ( X , y )= { ( x 1 ,y 1 ) , ( x 2 ,y 2 ) , ... ( x n ,y n in which each data point is drawn from certain i.i.d. distri-bution  X  . The goal of learning is to find the best predictor  X  f  X  X  so that the predicted value  X  y i for the input data x as close as possible to the ground truth y i ,  X  ( x i ,y given a loss function L ( ., . ). We hope that the predictor f learned in this way is close to the optimal model that mini-mizes the expected loss according to the  X  : Such predictor is given by the minimum of the empirical risk: The error caused by learning the best predictor in the train-ing dataset is called the estimation error. The error caused by using a restricted H is called the approximation error. For a fixed data size, the smaller the hypothesis space H the larger the approximation error, and vice versa. The trade-off between approximation error and estimation error is controlled by selecting the size of H . By including feature interactions we would enlarge the hypothesis space, and we may be able to dramatically minimize the approximation error compared to the traditional hypothesis space for lin-ear models. On the other hand, we note that given a limited amount of data, a large hypothesis space may result in mod-els with poor generalization performance. We will need to either increase our training data, or provide effective regu-larizations to narrow down the hypothesis space.
 Multi-task Feature Interactions. We consider the set-ting that there are multiple learning tasks which are related not only in the original feature space, but also in terms of feature interactions. The propose framework simultaneously learns all related tasks and provides an effective regulariza-tion on the hypothesis space using relatedness on the inter-actions.

Let D =( X 1 , y 1 ) ,..., ( X T , y T ) be the training data for the T learning tasks, and the i.i.d. training samples for task t is drawn from (  X  t ) m t , where m t is the number of data points available for task t . We collectively denote the distribution as D X   X  = T t =1 (  X  t ) m t . All tasks have a d -dimensional feature space (i.e., x i  X  R d ). The correspond-ing features are homogeneous and have the same semantic meaning. The total training data points are: ( X t , y t )= { ( x 1 t ,y 1 t ) , ( x 2 t ,y 2 t ) ,..., ( x The goal of MTL is to learn T functions for the tasks such that f t ( x it )= y it , based on the assumption that all task functions are related to some extent.

In order to consider interactions for each task, we use the quadratic predictive function in Eq. 1 for all tasks. We col-lectively represent the linear effects from all tasks as a matrix W =[ w 1 ,..., w T ]  X  R d  X  T , w i  X  R d and the interaction ef-fects as a tensor Q X  R d  X  d  X  T , in which the t -th frontal slice Q t  X  R d  X  d represents the interaction effects for task t .We illustrate this interaction tensor in Figure 1(a).
Given specific loss functions  X  for samples from one task, (e.g., square loss for regression and logistic loss for clas-sification, see Table 1), the loss function for each task is ( f, w , Q ; X , y )= m t i =1  X  ( f ( x i ; w , Q ) ,y i feature interaction loss function is given by: Note that it is not necessary for all tasks to have the same loss function. In MTL, the learning of each task benefits from the knowledge from other tasks, which effectively re-duces the hypothesis space for all tasks. In order to achieve knowledge transfer among tasks, we would like to impose shared representations via designing regularization terms on both W and Q , which specify how tasks are related in the original feature space and features interactions, respectively. The MTIL Framework. The proposed Multi-Task fea-ture Interaction Learning (MTIL) framework is then given by the following learning objective: where R F ( W ) is the regularization providing task related-ness in the original feature space, R I ( Q ) is the regularization encoding our knowledge about how feature interactions are related among tasks,  X  R and  X  I are the corresponding regu-larization coefficients. For  X  I  X  X  X  , the problem reduces to traditional MTL, when R I is chosen properly. In this paper, we formulate two concrete approaches to capture the feature interaction patterns:
We note that even though we only provided two specific approaches in this paper, the proposed MTIL framework could offer broader class of formulations. The proposed framework allows many other possible ways to define task relatedness on feature interactions, leading to a brand-new research area of MTL.
In this section, we will study how the formulations and al-gorithms of the shared interaction approach and embedded interaction approach under the proposed MITL framework. We note that extension of multi-task learning to feature in-teractions is not trivial because of the involvement of ten-sors. We start with formulating the shared interaction ap-proach by incorporating a group Lasso penalty to introduce structured sparsity on the tensor, which would select only a set of common feature interactions across different tasks that are relevant to the prediction. For the embedded inter-action approach, we propose both a convex formulation and a non-convex formulation. While the convex formulation leads to efficient optimization algorithms and global solu-tions, the non-convex formulation provides reduced storage complexity for large-scale problems. In this paper, we use the following basic definition of tensor: Mode-n fiber is a vector defined by fixing every index but one. We may see it as the higher order analogue of matrix rows (mode-2 fibers) and columns (mode-1 fibers). For ex-ample, in a three-way tensor Q X  R n 1  X  n 2  X  n 3 , the mode-3 Mode-n unfolding is the process of reordering the ele-trix. The mode-k unfolding of tensor Q is denoted by Q ( k concatenating all mode-k fibers of the tensor.
 Rank-n in our paper denotes the rank of tensor X  X  mode-n unfolding. It X  X  actually the dimension of the space spanned by the mode-n fibers of tensor. Specifically, rank n ( Q rank( Q ( n ) ). When Q is a matrix (i.e. 2-way tensor), this becomes the regular definition of rank, since rank 1 ( Q )= rank 2 ( Q ) = rank( Q ).
The goal of the shared interaction approach is to identify a set of common and relevant feature interactions across dif-ferent tasks. The interaction tensor Q in our framework has provided a convenient representation to encode such in-formation, and we are able to incorporating a group Lasso penalty [14] to induce a special type of structured sparsity on the tensor, coupling the same interactions for all tasks. Recall that the sparsity implies that only the significant in-teraction effects are captured in the model. For the purpose of shared interaction, a sparse tensor norm is defined as: Note that this norm enforces a symmetric sparsity by over the tensor, so that the one group is defined to include co-efficients of one interaction between feature i and feature j , from all tasks. Penalizing the tensor sparse norm leads to the following formulation: min where the parameter  X  I control the sparsity of tensor Q ,a larger  X  willendupwithamoresparse Q .Thesolution to formulation delivers a tensor such that the mode-3 fibers are either all zeros vectors or non zero vectors, i.e., inter-action effects between 2 features x i ,x j either exists on all tasks, or irrelevant for all tasks. Note that even the sparsity patterns is same for all tasks, their interactions may have different weights. It is easy to see that, this approach sub-sumes the traditional multi-task learning as a special case: when  X  I  X  X  X  by setting regularization parameter on tensor Q to infinity, all the elements in of Q in the solution will be zeros, and the model only considers linear effects.
When the loss function L chosen is convex and continu-ously differentiable with Lipschitz continuous gradient [26], then we can use proximal based gradient methods, such as first order FISTA [3], SpaRSA [34] or second order Proximal Newton [22] to solve it efficiently. Because that the linear effects and interaction effects are decoupled in the predictive function, a major class of loss functions belong to this cate-gory, and we give a few examples of common loss functions in Table 1. Note that even when L is non-convex, a local optimal solution can be efficiently obtained using the GIST framework [17]. The key to apply these algorithms is to ef-ficiently compute the proximal operator that associates to the problem (refer to [25] for more details about proximal): min where  X  W and  X  Q are intermediate solutions at each step,  X  and  X  2 are regularization parameters augmented with step size. Note that we have extend the Forbenius norm from matrix to tensor. We see that the problem is decoupled for W and Q . And the tensor proximal: canbesolvedinthesamewayasthegroupLassoproximal operator [36]. Moreover, we find that when the gradient is symmetric, we don X  X  need to enforce a symmetric tensor predictive function given in Eq. (1).
 sparse norm, and we could simply use a simple alternative: and initialize the algorithm with a symmetric tensor as the starting point. The reason that symmetry holds can be ex-plained by two parts. First, the gradient of Q is symmetric, therefore the gradient descent step won X  X  change the symme-try of tensor Q . Second, the proximal operator associated to sparse tensor norm won X  X  change the symmetry of matrix. To see this, the proximal operation is performed by vector-izing the matrix into a vector and shrink each element of the vector with respect to a input vector, which is obtained by the last gradient descent step. Since the input vector repre-sents an symmetric matrix, the element and its symmetric element will always shrink to the same new value. There-fore, the symmetry of Q holds. The sparse tensor norm is equivalent to perform the l 1 projection of vectors where each element is the l 2 norm of mode-3 fiber in tensor Q .
The share interaction approach has enforced a very re-strictive form of how tasks are supposed to relate to each other. In many applications, the prediction may be a re-sult of complicated feature interactions, instead only in-volves a few interactions. Even though the prediction may involve all feature interactions, it is usually a reasonable as-sumption that there are patterns among these interactions. Numerically, existence of patterns imply a low-dimensional subspace, which is reflected by a low-rank structure in the matrix. When there are multiple related learning tasks, one way for these tasks relate to others via a shared low-dimensional subspace, which gives us a low-rank tensor. As such, we may design a structured regularization to encour-age the matrix Q to be a low-rank tensor. In this paper we describe one convex formulation that encourages low-rank structure by penalizing a tensor norm and one non-convex formulation that directly learns a low-rank representation.
One way to obtain a low-rank tensor is to augment our formulation with a rank penalty. One problem associates to tensor is that there is no consistent way to define the rank of a tensor. One way is to use the average rank of unfolding on different mode [15]: where N is the total number of mode of the tensor ( N =3 when only pair-wise interactions), and Q ( n ) is unfold on n mode. Since minimizing the rank function is proven to be NP-hard, we could penalize the trace norm instead, which is the convex envelope of the rank function. The trace norm is defined as the sum of singular values of the matrix vari-able [20]. We then obtain the following convex formulation: min where .  X  denotes the trace norm. However, this convex formulation penalizes every mode of tensor Q to be jointly low rank, which may be too restricted in practice, which may lead to suboptimal performance. Moreover, the practi-cal way to solve the formulation in Eq. (7) is to use the alter-nating direction methods of multipliers (ADMM) [6], which introduces auxiliary variables and equality constraints, in or-der to decouple the three tensor trace norm terms. However, ADMM algorithm in practice is shown to have a slow con-vergence rate, and less preferred when composite proximal methods such as FISTA can be applied.

One alternative way to address these issues is to use the latent trace norm [30, 31], which is defined as following for a N  X  way tensor: where Q (1) ... Q ( N ) are a set of low-rank auxiliary tensors, which states that the original tensor can be decomposed into the sum of a set of tensors that are low-rank in different modes. Finally, we proposed to drop the equality constraint that each auxiliary tensor equal to the original one, but we directly use the mixture of tensors to represent the original tensor, so the problem becomes a unconstrained optimiza-tion problem. The predictive function of task t with such mixture is given by: where Q ( j )  X  R d  X  d  X  K ,  X  j =1 , 2 , 3 are the auxiliary tensors for replacing the original tensor Q , matrix Q ( j ) ( j ) is the mode j unfolding of tensor Q ( j ) , Q ( j ) t  X  R d  X  d frontal slice of tensor Q ( j ) . Finally, our convex formulation under embedded interaction approach is given by: The convexity of this formulation holds since both the loss function and the penalty are convex. We note that this formulation can be solved in the same way as the formulation in Eq. (7), and the model is much more flexible to model the complicated interactions among the features, leveraging the advantages of such auxiliary tensors.
Although using proximal gradient methods we are able to secure an optimal solution for the convex formulation, the time complexity and storage cost are unacceptable in prac-tice as the dimension of data increase. To see this, we note that the proximal operator associated to a trace norm reg-ularized objective requires singular projections [20], which requires cubic-complexity singular value decomposition. Re-call in each iteration of the gradient methods could involve more than one computation of proximal operator [3], and thus the computation may be prohibitive when dimension grows larger. On the other hand, we have to maintain 3 dense tensors of size d  X  d  X  T which means the storage cost is at O ( d 2 ), where T isthenumberoftasksandtypically we have T d . Also the mixture of three low-rank aux-iliary tensors may lead to some difficulty when it comes to analyzing the predictive model itself.

To this end, we propose to use a tensor with a explicit low-rank structure. Consider the interaction effects matrix Q R d  X  d for one task, we assume the low-rank decomposition Q = B  X  QB T , where B  X  R d  X  r is a basis matrix,  X  Q is a small matrix, capturing the information of the original tensor under the set of bases (columns) in B .Toseethis,we can expand Q = r i,j =1  X  Q ( i,j ) B i B T j , meaning the matrix Q is a result of interactions among bases in B and also spanned by the columns of B . We thus can use a predictive function that explicitly considers this low-rank structure: When there are multiple tasks, our assumption for embed-ded interaction approach is the shared basis, meaning B is restricted to be same as all other tasks. The multi-task loss function is thus given by: L ( W , { B } ,  X  Q ; f nvc , X , Y )= where  X  Q X  R r  X  r  X  T collective denotes the set of matrices from all tasks. This loss function is not convex because of the multiplication of variables in x T B  X  QB T x . This loss function leads to our final non-convex formulation for embedded: where the regularization R I ( { B } ,  X  Q ) can be Forbenius norm or other structural information (e.g. 1 norm). The dimen-sion r of B can be chosen according to the need of specific ap-plication demands, and can be selected by cross-validation. In general, we choose r d . We note that the storage com-plexity for the feature interaction effects (e.g., tensor reduce from O ( d 2 K )to O ( dr + r 2 K ), which is dramatically smaller than the full tensor, especially in the high dimen-sional settings. We could use the family of block coordinate descent algorithms [32] to alternatively solve the variables W , { B } ,and  X  Q , to get a local optimal solution.
In this section, we perform experiments on both synthetic datasets and two real world datasets to evaluate the effec-tiveness of our proposed MTIL framework.
 Figure 2: RMSE comparison between RR and STIL on two synthetic datasets with sample size of 1k and 5k, respectively.
In order to justify the effectiveness of modeling the feature interactions and MTIL framework, we test our methods on synthetic datasets.
In this subsection, we test whether the interactions be-tween features can be properly handled by adding the inter-action term Q . To do so, we create a single task synthetic dataset by assuming: where X  X  R n  X  d is the feature matrix, y  X  R n  X  1 is the responses, w  X  R d  X  1 is the weight vector, Q  X  R d  X  d is a symmetric, low-rank sparse matrix, which represents the feature interactions in the dataset, and  X  X  (0 , 0 . 01 I the additive noise term. We generate 20 synthetic datasets with different sizes (1000 or 1k and 5000 or 5k) and differ-ent feature dimensions (varying from 10 to 100, stepped by 10) by randomly selecting X , w ,and Q and computing y according to Eq.(8).

We use single task feature interaction learning model (STIL) to evaluate the effectiveness of the interaction term Q : min where w  X  R d  X  1 is the weight vector, Q  X  R d  X  d is the fea-ture interaction matrix, and Q 1 , 1 = i j | Q i,j | denotes the 1 , 1 norm.

We compared the Root Mean Square Error (RMSE) be-tween the Ridge Regression(RR) and STIL on both of the synthetic datasets. As the results show in Figure 2, STIL outperforms RR on both of the datasets, which shows the effectiveness of modeling the feature interaction in the data. Besides, STIL-5k (RR-5k) performs better than STIL-1k (RR-1k), which demonstrates that the learning models will capture the underlining models of the data better with larger training size. Also note that with the number of dimensions increases, STIL will gradually overfit the data, because of the dramatic increase of the interactions between features. Figure 3: Synthetic dataset (Multi-task): Root Mean Square Error (RMSE) comparisons among all the methods. The Y-axis is RMSE, X-axis is dimen-sion of features.
In order to test the effectiveness of MTIL, we generate a multi-task synthetic data by assuming: where X t  X  R n  X  d is the feature matrix of task t , y t is the responses of task t , W  X  R d  X  T =[ w 1 , w 2 , w is the weights for tasks. As described in Section 4.3, we gen-erate feature interaction matrix Q t = Bq t B T and project it into a sparse, symmetric space.

In this experiment, we generate 5 datasets with different feature dimensions from 10 to 50, stepped by 10, by ran-domly selecting X t , w t , B and q t .

The predictive performance of the methods outlined below are examined on the synthetic multi-task datasets:
Figure 3 compares the RMSE of the above methods on the 5 synthetic datasets. We can see that MTIL-L-Ln and MTIL-S-Ln are not that sensitive to the change of feature di-mensions, thanks to the low-rank assumption on the feature interaction. Also, RR and MTL-L share a similar perfor-mance, which is consistent with the fact that we did not as-sume any low-rank structure in this synthetic dataset. Note that although STIL performs almost the best on low dimen-sional data, its performance deteriorates rapidly compared with other MTIL methods, due to the incapability of learn-ing the feature interactions across tasks.
This dataset contains the examination records of 15362 students with 28 features from 139 schools in years of 1985, 1986 and 1987, provided by the Inner London Education Au-thority(ILEA). In this dataset, each task is to predict exam scores for students in one out of the 139 schools. We per-form 4 sets of experiments by varying the amount of train-ing size, from 20% to 50% of the total sample size. We test the approaches summarized in section 5.1.2 and tune the parameters on  X  R in set [10  X  1 , 10 0 , ..., 10 9 , 10 MTIL-L-Ln and MTIL-S-Ln methods, the rank of matrix r foreachtaskaretunedin[2 , 3 , ..., 19 , 20]. For MTIL-L-S and MTIL-L-Lc, we tune the regularization parameters  X  I in [10  X  1 , 10 0 , ..., 10 9 , 10 10 ].

The experimental results are shown in Table 2. First, for most of the methods, RMSE will decrease when the train-ing size increases. This means that providing more data in the training set will help overcome the overfitting prob-lem. Also, we found that the performance of embedded feature approaches (i.e. MTIL-L-Lc, MTIL-L-Ln, MTIL-S-Ln) are worse than the single task learning approach. The reason behind this is that embedded feature approaches do not have sparse constraints on the interaction term, which will severely overfit the data when there is not sufficient training samples. Additionally, the MTL-L and MTIL-L-S obtain better performance than single task learning, which indicates that the low-rank structure shared by tasks are effectively captured by the low-rank assumption in these two methods. Moreover, MTIL-L-S method outperforms all other methods, which empirically proves the effectiveness of learning the shared interactions with sparse constraints.
The Alzheimer X  X  Disease Neuroimaging Initiative (ADNI) database(adni.loni.ucla.edu), which was launched in 2003 as a 5-year public-private partnership, is aimed to test whether the positron emission tomography (PET), serial magnetic resonance imaging (MRI), other biological markers, and clin-ical and neuropsychological assessments can be combined to measure the progression of mild cognitive impairment (MCI) and early Alzheimer X  X  disease (AD). We follow the procedure of preprocessing mentioned in [39] and obtain 648 subjects and 305 MRI features. The parameters are tuned in the same way as we described in 5.2. Table 3: Performance comparison of different meth-ods on the ADNI dataset in terms of RMSE. All of the MTLs outperform the single task learning ap-proaches (RR and STIL) and MTIL-S-Lc method outperforms all other methods, which demonstrates the effectiveness of embedded feature interactions.
The RMSE comparison result is shown in Table 3. First, we found that all of the MTLs outperform the single task learning approaches (RR and STIL), which demonstrates the effectiveness of learning multiple tasks jointly by explor-ing the relatedness between tasks, as well as the existence of the underlying relatedness between tasks in the ADNI dataset. Second, the RMSE results of MTIL-L-S and MTL-L are comparable with each other, which indicates that the multiple tasks in this dataset do not share the same fea-ture interaction structure. Finally, the result of MTIL-S-Lc method outperforms all other methods, which shows superi-ority of our feature interaction framework. Through a mix-ture of 3 low-rank tensor, we are able to learn the feature interaction pattern in this dataset.
The proposed multi-task feature interaction learning frame-work has provided us a way to bridge related tasks using interaction effects. By employing different types of regular-izations on the interaction effects tensor, the formulations under this framework have very different characteristics. For the shared interaction approach: we utilize Group Lassoontheinteractiontensortocontrolthemodelcom-plexity. The proximal operator admits a closed form solu-tion, and thus the overall computational cost is very low. We are able to obtain interpretable results from the model, showing what are important interactions that are relevant to the prediction tasks. The main drawback is that we assume all tasks share the same set of interaction effects, which may not be the case for many data sets. One way to further improve the formulation is by extending the strong or weak heredity properties [5, 23] to the proposed MTIL framework.
For the embedded interaction approach: we can easily ob-tain the global optimal for the convex formulation. Though we are able to tune the regularization parameter on the trace norms to control the rank of the interaction tensor, it is usu-ally very hard to decide the value unless cross-validation is used. A rank larger than necessary may lead to over-fitting when training samples are insufficient. On the other hand, the obtained mixture of 3 tensor is hard to interpret. The non-convex formulation provides a better model decompo-sition, from which we can see the combination of basis for different tasks and identify embedded bases that are shared among the set of tasks. The drawback of this formulation is that we may easily trapped in a bad local optimal unless we carefully choose the initial value (e.g., using the solution from the convex formulation).

In general, this framework can be generalized into many other possible relatedness on feature interactions by incorpo-rating different regularization terms. Different approaches of this framework should be carefully chosen according to the application domain. In the future work we plan to study the statistical properties of the proposed model, which may lead to deeper understanding of these interaction models. One major limitation of linear models is the lack of capa-bility to capture predictive information from interactions between features. While introducing high-order feature in-teraction terms can overcome this limitation, this approach tremendously increases the model complexity and imposes significant challenges in the learning against overfitting. In this paper, we proposed a novel Multi-Task feature Inter-action Learning (MTIL) framework to exploit the task re-latedness from high-order feature interactions, which pro-vides better generalization performance by inductive trans-fer among tasks via shared representations of feature inter-actions. We formulate two concrete approaches under this framework and provide efficient algorithms: the shared in-teraction approach and the embedded interaction approach. The former assumes tasks share the same set of interactions, and the latter assumes feature interactions from multiple tasks come from a shared subspace. We have provided effi-cient algorithms for solving the two approaches. Extensive empirical studies on both synthetic and real datasets have demonstrated the effectiveness of the proposed framework. This material is based in part upon work supported by the National Science Foundation under Grant Numbers IIS-1565596 and Office of Naval Research N00014-14-1-0631.
