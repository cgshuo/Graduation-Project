 YONG CHEN
University of Hong Kong, The 54th Research Institute of CTE, China and Fudan University and KWOK-PING CHAN University of Hong Kong 1. INTRODUCTION
Postprocessing is crucial to recognition systems such as text recognition sys-tems and speech recognition systems. Postprocessing can significantly improve the recognition rate of recognition systems by correcting errors in the initial recognition results based on language models. For Chinese character recogni-tion systems, postprocessing is typically implemented as follows. In the recog-nition phase of a Chinese sentence, recognizers generate n candidates for each character in the sentence sorted by similarity score. When n only one recognition candidate for each character, the recognition rate, that is, the primitive recognition rate, is usually low since some correct candidates may not appear in first place on the candidate list. Postprocessing can improve the primitive recognition rate by replacing the wrong first-place characters with correct p th-place (1 &lt; p  X  n ) characters.

However, the top n candidate recognition rate forms an upper bound for postprocessing results. In this study, we achieve a breakthrough by using a new postprocesing strategy, namely, word suggestion. With the word sugges-tion strategy, correct character candidates out of the top n candidate list have the chance to replace wrong first-place characters. The recognition rate can be improved further. With the word suggestion strategy, correct character can-didates have the chance to replace wrong first-place characters regardless of whether they are in the top n candidate list.

To prevent introducing wrong words in the process of word suggestion, rough set theory is used to discover negatively correlated relationships between words.
The negatively correlated relationships are then used to preclude wrongly sug-gested words. 2. THE TRIGGER-PAIR LANGUAGE MODEL AND THE CHINESE LANGUAGE
The idea of trigger pair [Lau 1993] is actually based on the reading behavior of human beings. When people read text, especially handwritten scripts, they may encounter misspelled and blurred words. However, people can usually in-fer the correct words by referring to the neighboring, highly-associated words.
A trigger pair can be denoted as A 0  X  B , where A 0 represents the trigger, and B represents the triggered word. Usually the trigger and the triggered word are a few words apart. This is significant since long-distance dependency relationships can be captured. Trigger-pair language models can be used as complement to N -gram models which are able to capture short-distance de-pendency relationships. The degree of association between the trigger and the triggered word is measured by mutual information (MI) and can be computed according to Equation (1). The MI values will be used to evaluate paths through a word lattice in the stage of postprocessing.

In the trigger-pair model, the trigger occurs first, the triggered word occurs later. We can predict the current word (the triggered word) based on the knowl-edge of what word (the trigger) occurred in the past. The value of the mutual information indicates how likely B will occur if A 0 has already occurred. The prediction of the current word is performed based on one single word (the trig-ger) in traditional trigger-pair models. We propose a multiple word trigger-pair model in which each trigger pair consists of more than two words. In a multiple word trigger pair, one word acts as the triggered word, whereas the remaining words act as the trigger. That is, the trigger consists of more than one word, A
A better than that based on one single word. This advantage of the multiple word trigger-pair model over traditional trigger-pair models leads to better perfor-mance in postprocessing application. Another advantage of the multiple word trigger-pair model over traditional trigger-pair models is that multiple word trigger-pairs can be used to correct errors remaining in the postprocessing re-sult. That is, with the multiple word trigger-pair model, a two-stage postpro-cessing can be implemented as shown in Figure 1. In the first stage, it is the
MI values of multiple word trigger pairs that are used for postprocessing; in the second stage, it is the stable patterns represented by multiple word trigger pairs that are used for postprocessing. Next, we introduce how to correct errors in the second stage of postprocessing.

In practice, it is much likely for Chinese character recognizers to only partially recognize a Chinese word, that is, some constituent characters are correctly recognized, but others are not. Those correctly recognized charac-ters can be good hints for correcting the word. For example, let the word sequence  X   X  be the recognition result of the sentence  X   X . There is an erroneous character  X   X . It should be  X   X . However, the character  X   X  is recognized correctly. In this case, one can easily correct the error by referring to  X   X  and surrounding words. Our purpose is to make computers correct this kind of error based on multiple word trigger pairs.

Multiple word trigger pairs represent stable patterns. The occurrence of some words in a multiple word trigger pair may indicate the probable presences of other words in the same multiple word trigger pair. For example, three-word trigger pair  X   X  may suggest a word  X   X  for a recognized single character  X   X  when  X   X  and  X   X  appear in the context. If the suggested word  X   X  is evaluated as consistent with other neighboring words such as  X   X  and  X   X , then  X   X  is accepted as the correct word.

It is worth noting that the word suggestion strategy is especially valuable when the character  X   X  is not in the top n candidate list. The absence of  X   X  X n the list means that the error cannot be corrected through postprocessing based on traditional trigger pair models. The ability of multiple word trigger pairs to introduce correct characters out of the top n candidate list into the final result makes recognizers more likely to achieve a recognition rate greater than that of the top n candidates.

To formalize this, let w 1 w 2 w 3 w 4 be a sentence of four words; let C
C
C 3 C 4  X  C 5  X  C 6  X  C 7  X  C 8 C 9 be the recognition result. Let X  X  assume that all words in the sentence are correctly recognized except the word w a three-character word, is incorrectly recognized as 3 consecutive characters,
C
C 6 , and C 7 . We focus on C 5 , the sixth character. In our system, the recog-nizer generates 9 (top n , n = 9) candidates for each character in the sentence.
These character candidates form a character lattice and are sorted in descend-ing order according to their recognition score. We select the top k (1 character candidates from the sixth column of the character lattice. We examine the multiple word trigger-pair model to see whether there are multiple word trigger pairs satisfying the following requirements: (a) the triggered word is a two-character or three-character word; (b) the first character of the triggered word is C 5 or matched with one of the k character candidates; (c) the first and second trigger words of trigger pairs satisfying (a) and (b) can be found in the leading words of w 3 . The triggered words of trigger pairs satisfying these re-quirements are considered to be possible correct words corresponding to the position C 0 5 . The purpose of using the top k (more than 1) character candidates for word suggestion is to suggest more correct words based on more matches.
In the previous description, words are suggested depending on matches on the first character (requirement (b)). Actually, words can also be suggested depending on matches on other constituent characters. Three-word trigger pairs can be used for error correction in three ways: (1) based on the first two words, correcting errors in the last word; (2) based on the last two words, correcting errors in the first word; (3) based on the first and last word, correcting errors in the middle word. 3. CONSTRUCTION OF THE MULTIPLE WORD TRIGGER-PAIR LANGUAGE MODEL
Multiple word trigger pairs are derived from ordered word sequences in which words are highly associated. Discovery of the most interesting word sequences is computationally intensive since the training text is very large. For example, in our training text, there are 25 million Chinese characters. To alleviate this problem, we think of finding multiple word trigger pairs from a large text as mining frequent patterns from text. Hence, a variety of text mining algorithms can be utilized. In this study, the Apriori algorithm is used to mine the most interesting multiple word trigger pairs.

The Apriori algorithm, originally proposed by Agrawal et al. [1993], is mainly used for discovering association rules from market transaction records. For example, researchers may find an association rule like  X  X read, milk X , indicating that customers tend to buy bread and milk at the same time in a supermarket, and that bread and milk are highly associated. Therefore, shop managers can put bread and milk in the same shelf to facilitate customer needs. 3.1 The Apriori Algorithm
We start the discussion of the Apriori algorithm by introducing some definitions.  X  Itemset. A set of items.
  X  Frequent itemset. An itemset is frequent if the number of its occurrences in the transaction database exceeds a predefined value (support count).  X  k -itemset. An itemset containing k members.  X  C k . A set of itemsets containing k members.  X  L k . A set of frequent itemsets containing k members, L
The pseudocode for the Apriori algorithm is as follows.  X  Pass 1 (1) Generate the itemset candidates for C 1 (2) Scan the database to count the occurrences of itemsets in C (3) Put the frequent itemsets in L 1  X  Pass k ( k &gt; 1) (1) Generate the k-itemset candidates for C k from the frequent itemsets in (2) Scan the database to count the occurrences of itemsets in C (3) Put the frequent itemsets in L k
The Apriori algorithm reduces the search space by setting support count and by pruning impossible itemset candidates. Once all frequent itemsets are generated, another criterion, confidence, is used to find the most interesting association rules where support ( A and B ) is the number of transactions containing both A and B , and support ( A ) is the number of transactions containing A .

Let l be a frequent itemset and s be one of its subsets. Every nonempty subset
In sum, the process of discovering association rules is accomplished in two steps.  X  Use the Apriori algorithm to find the frequent itemsets.  X  Based on the frequent itemsets, derive association rules. 3.2 Discover Interesting Word Sequences
The Apriori algorithm is designed for mining frequent itemsets from transac-tion data. In our case, we would like to discover interesting word sequences from a large training text. One major difference between word sequences and commodity itemsets is that word sequences are order-sensitive, but commodity itemsets are not. Item A and item B are likely to be purchased together by cus-tomers is equivalent to item B and item A are likely to be purchased together by customers. However, word sequences are order-sensitive.

Word sequences A 1 0 , A 2 0 , B , A 1 0 , B , A 2 0 and A another. Hence, it is necessary to adapt the Apriori algorithm. The pseudocode for the adapted Apriori algorithm [Chen et al. 2003] is as follows.  X  Pass 1 (1) Generate the candidate words for C 1 (2) Scan the text to count the occurrences of words in C (3) Put the frequent words in L 1  X  Pass k ( k &gt; 1) (1) Generate the candidate k -word sequences for C k from the frequent word (2) Scan the text to count the occurrences of the word sequences in C (3) Put the frequent word sequences in L k p . word 1 k  X  1  X  X  X  q . word k  X  1 is provided by a support language model L be introduced later.

To illustrate, we introduce how to discover three-word word sequences from a training text. A three-word word sequence is denoted as A , B , C , indicating that
A appears in a context first, B follows, and C comes last. In order to capture both short-and long-distance dependency relationships, two kinds of three-word word sequences are mined. One kind of three-word word sequence has A in the second-to-the-last sentence, B in the last sentence, and C in the current sentence. The other kind of sequence has A , B , and C in the same sentence ap-pearing sequentially. The former aims at capturing long-distance dependency relationships between words; the latter aims at capturing short-distance de-pendency relationships between words. We refer to the former as intersentence word sequence, and the latter as intrasentence word sequence.

In the following example, we describe how to discover interesting intersen-tence three-word word sequences. This procedure is also applicable to discov-ering intrasentence three-word word sequences.
Actually, the previous algorithm reflects the fact that sequences
W conditions for { W i tions to reduce the search space.

Once frequent three-word sequences have been discovered, a confidence threshold is set to select the most interesting word sequences. 3.3 The Multiple Word Trigger-Pair Language Model
Given a word sequence, several trigger pairs can be derived. These trigger pairs are called multiple word trigger pairs. In particular, three trigger pairs can be derived from one three-word word sequence, depending on which words act as the trigger. Take { W 1 , W 2 , W 5 } for an example. The derived multiple word trigger pairs are listed in Table VII.

We refer to the trigger pairs derived from intersentence word sequences as intersentence multiple word trigger pairs, and the trigger pairs derived from intrasentence word sequences as intrasentence multiple word trigger pairs. 4. THE VARIABLE PRECISION ROUGH SET THEORY
Apparently, suggesting as many correct words as possible is the key to im-proving the recognition rate to the most extent. As shown in experiments in Chen et al. [2003], if k = 9, and all suggested correct words can ap-pear in the final results, then the recognition rate will increase from about 90% to about 97%, a fairly significant improvement. However, the signifi-cant improvement cannot be achieved in practice due to suggested wrong words. It is necessary to prevent wrong words from getting into the word lattice.

We observed that multiple-character words generated in the first stage of postprocessing are correct in most cases, and thus reliable. Wong [1998] had the same conclusion in his research. Therefore, these words can be used as landmark words to filter suggested wrong words. The basic idea is that it is less likely for a word to appear together with its negatively correlated words in a context. Therefore, if a suggested word is negatively correlated with land-mark words, then it will be rejected. Therefore, we need to find negatively cor-related relationships between words. The variable precision Bayesian rough set (VPBRS) theory [Slezak 2003], developed from the rough set theory [Kostek 1982], is used for this purpose. The rough set theory has been used in many fields such as machine learning [Wei et al. 2002], knowledge discovery [Mohabey et al. 2000], pattern recognition [Kostek 1993], etc. The main advantage of the rough set theory is that it does not need any preliminary or additional information about data.

In practice, most datasets are inconsistent. In other words, it is quite common that records in a table characterized by the same set of attribute-value pairs may have different values for the decision attribute. For example, the damage to different car parts may have the same malfunction symptom. To identify a particular damaged car part, a car mechanic has to analyze the problem and draw a conclusion from his experiences. However, it is difficult for the car mechanic to exactly identify the damaged car part solely by his experiences since the definitions for malfunctions are nondeterministic. Hence, it is not feasible to represent the knowledge deterministically.

The rough set theory does not provide the probability of occurrence of each rule. This makes the rough set theory impractical for some applications where the probability of occurrence of each rule is necessary. To deal with this problem,
Ziarko [1993] proposed a variable precision extension of the rough set theory, called Variable Precision Rough Set Model (VPRSM). In VPRSM, the two ap-proximations are described by probability, thus making the rough set theory applicable to the analysis of nondeterministic data. VPRSM improves the util-ity of the rough set theory.

We use the example shown in Table VIII to illustrate how to discover knowl-edge from data using VPRSM. Assuming that Table VIII is derived from a database of an automobile insurance company, the conditional probability P ( X | S i ) represents the probability of the event that an object belonging to the C -elementary set S i belongs to the set X .

In VPRSM, the approximations are defined by conditional probabilities rather than by relations of set inclusion and overlap. Two parameters are used in the definition. The first parameter l , called the lower limit, represents the highest acceptable degree of the conditional probability P ( X elementary set S i in the negative region of the set X . In other words, the l -negative region of the set X , NEG l ( X ) is defined as
The l -negative region of the set X is a collection of objects, which cannot be classified as included in the set with the probability higher than l , based on the available information.

The second parameter u , called upper limit, represents the least acceptable degree of the conditional probability P ( X | S i ) to include the elementary set S in the positive region.

The positive region of the set X is a collection of objects, which can be classified as included in the set with the probability not less than u , based on the available information.

When defining the approximations, it is assumed that 0  X  l itively, l specifies those objects that unlikely to belong to X , whereas u specifies objects that are likely to belong to the set X .

The objects that are not classified as being in the positive region nor in the negative region belong to the boundary region, BNR l , u likely to belong to X and also insufficiently unlikely to not belong to X .
In some applications, the objective is to achieve some improvement in the certainty of prediction based on available information rather than to produce rules satisfying preset certainty requirements. Therefore, it appears to be ap-propriate to use no any parameters to control model derivation. Based on the previous analysis, Slezak et al. [2003] presented the Bayesian rough set (BRS) theory by modifying VPRSM to allow for derivation of parameter-free predictive models from data while preserving the essential notions and methods of rough set theory. The Bayesian rough set positive, negative, and boundary regions are defined, respectively, by
POS BRS ( X ) and NEG BRS ( X ) are, respectively, the areas of certainty improve-ment and certainty loss with respect to predicting the occurrence of X based on the available information. Information defined in BND BRS to X .
 In this study, P ( X ) is estimated by the following equation.

Our goal is to investigate the negatively correlated relationships between words so that some suggested words that turn out to be negatively correlated with landmark words can be removed. Essentially, a textual document is an information source including inconsistent, uncertain, and incomplete data. We treat investigating negatively correlated relations between words as discover-ing rules from a database. In a training text, each sentence can be viewed as a record. Given a word, the goal is to find those words which are negatively correlated with it. The obvious attributes that can be used are the identities of neighboring words in a sentence and their positions relative to the word of interest. All neighboring words and their positions can be used as attributes to describe the relation. However, the rules defined by identities and positions will be too specific to be practical. Furthermore, there will be few multiple-character words in the recognition result that can be used as landmark words.
Hence, we take the identity of one neighboring word and its position as at-tributes and use them to partition the universe U and get elementary sets,
U/IND ( { Identity of word, Position } ). Table IX is an illustration of the partition of the universe U with respect to the word  X   X  (life). By using BRS, we can find those rules falling in the negative region which reflect the negatively cor-related relations between words. Those rules will look like (occurrence of word X , position)  X  (  X  occurrence of word Y ).
 The probability of the word  X   X  is 0.002. Accordingly, each elementary set
S i can be assigned to a rough region as shown in Table X.

The positive region will be the union of S 1  X  S 2  X  S 4; the negative region will be the union of S 3  X  S 5; the boundary region will be S 6. The rules in the positive region describe the positively correlated relationships between words; the rules in the boundary region are unrelated to the decision; the rules in the negative region describe the negatively correlated relations and will be used to filter wrong words. 5. CONSTRUCTION OF THE NEGATIVELY-CORRELATED RELATIONSHIP MODEL
We generate the negatively-correlated relationship model on a training text of 25 million Chinese characters, which contains news documents on econ-omy, finance, law, politics, etc. The news documents were downloaded from
China Infobank ( , www.chinainfobank.com). Each news document con-tains around 1500 Chinese characters. All news documents are processed by SLEX (a word segmentation and tagging tool, developed by Beijing University).
Given a word, many words in the vocabulary may never occur with it in any sentence in the training text. These words have zero probability conditioning on the word and should be collected in the negatively correlated word (NCW) list. However, the size of the NCW list would be excessive if all of them were collected. To include only the most interesting NCWs in the list, we only col-lect those words whose occurrence frequencies are greater than a predefined threshold. We find that, as the threshold decreases, more NCWs will be in-cluded in the list, and the performance of the model will be better. However, the change in performance improvement is small when the average size of the NCW lists exceeds 120 (when the occurrence frequency threshold is 0.001).
Table XI shows the average size of the NCW lists for different word-occurrence probability thresholds. We empirically determined the threshold is 0.001 in this study. The NCW lists of all words form the negatively-correlated relationship model.
 6. POSTPROCESSING BASED ON LANGUAGE MODELS
In this section, we introduce how to perform postprocessing based on language models. The conditional probabilities of n -gram models and MI values of trigger-pair models are used to evaluate paths through word lattices.

First of all, we introduce character lattice and word lattice. Given a sentence, recognizers generate several candidates for each character in the sentence. Can-didates of all characters form a character lattice. The character lattice is then converted to a word lattice by identifying all potential words in the character lattice. All paths through the word lattice are evaluated based on language models, and the best one is output.

In Figure 2, w 2 1 w 2 2  X  X  X  w 2 M result of the second preceding sentence; w 1 1 w 1 2  X  X  X  w represents the recognition result of the immediately preceding sentence; the table represents the word lattice L , containing word candidates of the current sentence. The current sentence consists of M 0 words. w 0 word (node) at the j th column; the word sequence enclosed by the dotted square denotes a partial path P ending at w 0 x appending w 0 ij . The extended partial path is evaluated according to Equation (2).
Here, w 2 represents the second preceding sentence; w 1 represents the im-mediately preceding sentence; and w 0 represents the current sentence. w and w 1 y represent two words: one in the second preceding sentence, and one in the immediately preceding sentence. They act as the trigger of a matched intersentence trigger pair. w 0 x and w 0 y represent two words in the partial path, which act as the trigger of a matched intrasentence trigger pair. MI( sents the MI value of a trigger pair. P ( w 0 ij | w 0 x
SM(  X  ) represents the recognition score of w 0 ij produced by recognizers; are parameters. The values of  X  and  X  were determined empirically. The best result was achieved when  X  = 0 . 518 and  X  = 0 . 472. 7. EXPERIMENTAL RESULTS
We conducted experiments on a Dell PC (Optiplex GX620, Pentium 4, 3GHz, 1G main memory, Linux OS, written in C) with two test texts, containing 100,000
Chinese characters each. The two test texts, TextA and TextB, were generated by collecting articles from the Internet. They cover topics on economy, finance, law, technology, stocks, etc.

We applied the Apriori algorithm to the training text of 25 million Chinese characters to generate both intrasentence and intersentence three-word trigger pairs. By setting the support count to 10 and confidence threshold to 6 percent, 600,000 intrasentence three-word trigger pairs were generated; by setting the support count to 16 and confidence threshold to 7 percent, 400,000 intersentence three-word trigger pairs were generated. We combined them to get a hybrid three-word trigger pair model of 1,000,000 entries. Our experiment showed that the model achieves the best performance when the ratio of between intrasen-tence to intersentence three-word trigger pair is about 6:4. The sizes of sets gen-erated in the process of performing the Apriori algorithm are listed in Table XII. The mutual information for three-word trigger pairs is computed in Equation (3).

The Chinese character recognizer used in the experiments is a Bayes clas-sifier. Peripheral features, for example, ET1 and ET2, and wavelet technique-based multiresolution features [Mallat 1989] are used to characterize Chinese character images. Each Chinese character image is characterized by a 64-D fea-ture vector. Each character had 800 training samples. 780 samples were used for training, while the remaining 20 samples were used for testing. A detailed introduction can be found in Chen et al. [2003].

The primitive recognition rate (without postprocessing) was 81.3% (TextA) and 81.6% (TextB). When using traditional trigger-pair models plus a trigram model for postprocessing, the recognition rate was 89.3% (TextA) and 89.8% (TextB). When using the multiple word trigger-pair model plus trigram model for postprocessing, the recognition rate was 90.8% (TextA) and 91.2 (TextB). Im-provements of 1.5% and 1.4% over traditional trigger-pair models were achieved on the two test texts, respectively. The improvements can be attributed to two factors. First, the multiple word trigger-pair model is able to provide more ac-curate information (represented by mutual information) on the effect of preced-ing words on the current word than traditional trigger-pair models. Second, the multiple word trigger-pair model supports automatic word correction. Partially recognized words were corrected by this mechanism. When the word correc-tion strategy was not used, the recognition rate was 89.9% (TextA) and 90.5% (TextB). When the word correction strategy was used, additional improvements of 0.9% and 0.7%, respectively, were achieved based purely on the second factor.
It is worth noting that with the automatic word-correction mechanism, the recognition rates of 90.8% (TextA) and 91.2% (TextB) exceed the top-9 candi-date recognition rates of 90.6% (TextA) and 90.8% (TextB). This is due to the following facts. Based on multiple word-trigger pairs, some correct words were suggested into word lattices in the second stage of postprocessing. Those words might contain correct characters out of the top-9 character candidate lists. All of the previous discussions are illustrated in Figure 3 (test on TextA) and Figure 4 (test on TextB).

We also tried to use the word suggestion strategy based on N -gram mod-els. However, the result was unsatisfactory. The recognition rate became even worse when we used the word suggestion strategy based on bigram models.
We used the first word of a bigram as the trigger, the second word as the trig-gered word. Given a trigger, the number of corresponding trigged words was very large. Consequently, an excessive number of wrong words were suggested, and it worsened the recognition rate. The result was positive when we used the word suggestion strategy based on trigram models. However, the word sug-gestion strategy contributes only a 0.2 percent improvement. One underlying constraint for trigrams is that three words in each trigram must be contiguous in a context. However, very often highly associated words in a context are not contiguous. They may scatter in one single sentence or in several sentences.
Furthermore, unlike data mining techniques, the algorithms for generating trigram models do not have the ability of selecting the most interesting entries from a vast volume of candidates. Hence, the word suggestion strategy appears to be less effective with trigram models.

If the wrongly-suggested word prevention technique is used, then the con-straint on the parameter k can be relaxed slightly by changing k from 2 to 3.
This leads to more correct words being suggested and thus results in 0.7% ad-ditional improvement on TextA and 0.8% additional improvement on TextB as shown in Figure 5 and Figure 6.

There are 11,200 sentences in TextA. Without using the wrongly-suggested word prevention technique, 5,270 correct words and 13,845 wrong words are suggested. By using the wrongly-suggested word prevention technique, the number of suggested wrong words reduces to 11,293; the number of sug-gested correct words reduces to 5,258. There are 12,100 sentences in TextB.
Without using the wrongly-suggested word prevention technique, 5,350 cor-rect words and 13,905 wrong words are suggested. By using the wrongly-suggested word prevention technique, the number of suggested wrong words reduces to 11,733; the number of suggested correct words reduces to 5,339.
These results are shown in Table XIII. The decrease in the number of the wrongly-suggested words significantly reduces the chance of introducing wrong words into the final results. It can be argued tat the technique can fur-ther improve the performance, though a few suggested correct words are rejected.

For TextA, 4,532 correct out-of-candidate-list characters were suggested into word lattice by using the word suggestion technique, and 1,656 of them ap-peared in the final result. 10,069 erroneous out-of-candidate-list characters were introduced, and 98 of them appear in the final result. For TextB, 4,598 correct out-of-candidate-list characters were suggested into word lattice by us-ing the word suggestion technique, and 1,589 of them appeared in the final result. 9,887 erroneous out-of-candidate-list characters were introduced, and 84 of them appear in the final result. Generally, the effect of the word sugges-tion strategy is positive.

We also conducted experiments with pure intrasentence and pure intersen-tence multiple word trigger-pair models of 1,000,000 entries. The results are shown in Table XIV. 8. CONCLUSION
In this study, we used data mining techniques to generate the multiple trigger-pair model. The multiple word trigger-pair model is able to provide more ac-curate information on the dependent relationships between words. More im-portantly, based on multiple word trigger pairs, recognizers can-perform wrong word corrections to achieve a recognition rate even greater than the top-n can-didate recognition rate. To prevent suggesting wrong words, rough set theory is used to explore negatively correlated relationships between words. If a sug-gested word is negatively correlated with landmark words in context, then it will be rejected. Our future effort will focus on finding other potential ap-proaches to filter suggested wrong words more effectively.

The authors would like to thank the associate editor and the anonymous review-ers. Their valuable comments and suggestions greatly improved the quality of this article.

