 In this pap er, we describ e the dev elopmen t of a elded appli-cation for detecting malicious executables in the wild. We gathered 1971 benign and 1651 malicious executables and enco ded eac h as a training example using n -grams of byte codes as features. Suc h pro cessing resulted in more than 255 million distinct n -grams. After selecting the most relev ant n -grams for prediction, we evaluated a variet y of inductiv e metho ds, including naiv e Bayes, decision trees, supp ort vec-tor mac hines, and boosting. Ultimately , boosted decision trees outp erformed other metho ds with an area under the roc curv e of 0.996. Results also suggest that our metho d-ology will scale to larger collections of executables. To the best of our kno wledge, ours is the only elded application for this task dev elop ed using techniques from mac hine learning and data mining.
 H.2.8 [ Database Managemen t ]: Database Applications| Data Mining ; I.2.6 [ Arti cial Intelligence ]: Learning| Conc ept Learning ; K.6.5 [ Managemen t of Computing and Information Systems ]: Securit y and Protection| Invasive Softwar e General Terms: Algorithms, Exp erimen tation, Securit y Keyw ords: Data Mining, Concept Learning, Securit y, Malicious Soft ware
Malicious code is \an y code added, changed, or remo ved from a soft ware system to inten tionally cause harm or sub-vert the system's intended function" [27, p. 33]. Suc h soft-ware has been used to compromise computer systems, to destro y their information, and to render them useless. It has also been used to gather information, suc h as passw ords and credit card num bers, and to distribute information, suc h as pornograph y, all without the kno wledge of the system's users. As more novice users obtain sophisticated computers Cop yright 2004 ACM 1 X 58113 X 888 X 1/04/0008 ... $ 5.00. with high-sp eed connections to the Internet, the poten tial for further abuse is great.

Malicious executables generally fall into three categories based on their transp ort mec hanism: viruses, worms, and Trojan horses. Viruses inject malicious code into existing programs, whic h become \infected" and, in turn, propagate the virus to other programs when executed. Viruses come in two forms, either as an infected executable or as a virus loader, a small program that only inserts viral code. Worms, in con trast, are self-con tained programs that spread over a net work, usually by exploiting vulnerabilities in the soft-ware running on the net work ed computers. Finally , Trojan horses masquerade as benign programs, but perform ma-licious functions. Malicious executables do not alw ays t neatly into these categories and can exhibit com binations of beha viors.

Excellen t technology exists for detecting kno wn malicious executables. Soft ware for virus detection has been quite successful, and programs suc h as McAfee Virus Scan and Norton AntiVirus are ubiquitous. Indeed, Dell recommends Norton AntiVirus for all of its new systems. Although these pro ducts use the word virus in their names, they also detect worms and Trojan horses.

These programs searc h executable code for kno wn pat-terns, and this metho d is problematic. One shortcoming is that we must obtain a cop y of a malicious program before extracting the pattern necessary for its detection. Obtain-ing copies of new or unkno wn malicious programs usually entails them infecting or attac king a computer system.
To complicate matters, writing malicious programs has become easier: There are virus kits freely available on the Internet. Individuals who write viruses have become more sophisticated, often using mec hanisms to change or obfus-cate their code to pro duce so-called polymorphic viruses [3, p. 339]. Indeed, researc hers have recen tly disco vered that simple obfuscation techniques foil commercial programs for virus detection [7]. These challenges have prompted some re-searc hers to investigate learning metho ds for detecting new or unkno wn viruses, and more generally , malicious code.
Our e orts to address this problem have resulted in a elded application, built using techniques from mac hine learning [30] and data mining [17]. The Malicious Exe-cutable Classi cation System ( mecs ) curren tly detects un-kno wn malicious executables \in the wild", that is, without remo ving any obfuscation. To date, we have gathered 1971 system and non-system executables, whic h we will refer to as \benign" executables, and 1651 malicious executables with a variet y of transp ort mec hanisms and payloads (e.g., key-loggers and bac kdo ors). Although all were for the Windo ws operating system, it is imp ortan t to note that our approac h is not restricted to this operating system.

We extracted byte sequences from the executables, con-verted these into n -grams, and constructed sev eral classi-ers: ib k , tfidf , naiv e Bayes, supp ort vector mac hines ( svm s), decision trees, boosted naiv e Bayes, boosted svm s, and boosted decision trees. In this domain, there is an is-sue of unequal but unkno wn costs of misclassi cation er-ror, so we evaluated the metho ds using receiv er operating characteristic ( roc ) analysis [40], using area under the roc curv e as the performance metric. Ultimately , boosted de-cision trees outp erformed all other metho ds with an area under the curv e of 0.996.

We deliv ered mecs to the mitre Corp oration, the spon-sors of this pro ject, as a researc h protot ype. Users interact with mecs through a command line. They can add new executables to the collection, update learned mo dels, dis-play roc curv es, and pro duce a single classi er at a speci c operating point on a selected roc curv e.

With this pap er, we mak e three main con tributions. We sho w how established metho ds for text classi cation apply to executables. We presen t empirical results from an ex-tensiv e study of inductiv e metho ds for detecting malicious executables in the wild. We rep ort on a elded application dev elop ed using mac hine learning and data mining.
In the three sections that follo w, we describ e related work, our data collection, and the metho ds we applied. Then, in Section 6, we presen t empirical results, and in Section 7, we discuss these results and other approac hes.
There have been few attempts to use mac hine learning and data mining for the purp ose of iden tifying new or un-kno wn malicious code. These have concen trated mostly on pc viruses, thereb y limiting the utilit y of suc h approac hes to a particular type of malicious code and to computer sys-tems running Microsoft's Windo ws operating system. Suc h e orts are of little direct use for computers running the unix operating system, for whic h viruses pose little threat. How-ever, the metho ds prop osed are general, meaning that they could be applied to malicious code for any platform, and presen tly, malicious code for the Windo ws operating system poses the greatest threat.

In an early attempt, Lo et al. [25] conducted an analy-sis of sev eral programs|eviden tly by hand|and iden ti ed tell-tale signs , whic h they subsequen tly used to lter new programs. While we appreciate their attempt to extract patterns or signatures for iden tifying any class of malicious code, they presen ted no exp erimen tal results suggesting how general or extensible their approac h migh t be. Researc hers at ibm 's T.J. Watson Researc h Cen ter have investigated neu-ral net works for virus detection [21] and have incorp orated a similar approac h for detecting boot-sector viruses into ibm 's Anti-Virus soft ware [41].
 More recen tly, instead of focusing on boot-sector viruses, Schultz et al. [37] used data mining metho ds, suc h as naiv e Bayes, to detect malicious code. The authors collected 4,301 programs for the Windo ws operating system and used McAfee Virus Scan to lab el eac h as either malicious or be-nign. There were 3,301 programs in the former category and 1,000 in the latter. Of the malicious programs, 95% were viruses and 5% were Trojan horses. Furthermore, 38 of the malicious programs and 206 of the benign programs were in the Windo ws Portable Executable ( pe ) format.
For feature extraction, the authors used three metho ds: binary pro ling, string sequences, and so-called hex dumps . The authors applied the rst metho d to the smaller col-lection of 244 executables in the Windo ws pe format and applied the second and third metho ds to the full collection.
The rst metho d extracted three types of resource infor-mation from the Windo ws executables: (1) a list of Dy-namically Link ed Libraries ( dll s), (2) functions calls from the dll s, and (3) the num ber of di eren t system calls from within eac h dll . For eac h resource type, the authors con-structed binary feature vectors based on the presence or ab-sence of eac h in the executable. For example, if the collection of executables used ten dll s, then they would characterize eac h as a binary vector of size ten. If a given executable used a dll , then they would set the entry in the executable's vector corresp onding to that dll to one. This pro cessing resulted in 2,229 binary features, and in a similar manner, they enco ded function calls and their num ber, resulting in 30 integer features.

The second metho d of feature extraction used the unix strings command, whic h sho ws the prin table strings in an object or binary le. The authors formed training examples by treating the strings as binary attributes that were either presen t in or absen t from a given executable.

The third metho d used the hexdump utilit y [29], whic h is similar to the unix octal dump ( od -x ) command. This prin ted the con ten ts of the executable le as a sequence of hexadecimal num bers. As with the prin table strings, the authors used two-b yte words as binary attributes that were either presen t or absen t.

After pro cessing the executables using these three meth-ods, the authors paired eac h extraction metho d with a sin-gle learning algorithm. Using ve-fold cross-v alidation, they used ripper [8] to learn rules from the training set pro-duced by binary pro ling. They used naiv e Bayes to es-timate probabilities from the training set pro duced by the strings command. Finally , they used an ensem ble of six naiv e-Ba yesian classi ers on the hexdump data by training eac h on one-sixth of the lines in the output le. The rst learned from lines 1, 6, 12, . . . ; the second, from lines 2, 7, 13, . . . ; and so on. As a baseline metho d, the authors imple-men ted a signature-based scanner by using byte sequences unique to the malicious executables.

The authors concluded, based on true-p ositiv e ( tp ) rates, that the voting naiv e Bayesian classi er outp erformed all other metho ds, whic h app ear with false-p ositiv e ( fp ) rates and accuracies in Table 1. The authors also presen ted roc curv es [40], but did not rep ort the areas under these curv es. Nonetheless, the curv e for the single naiv e Bayesian classi-er app ears to dominate that of the voting naiv e Bayesian classi er in most of the roc space, suggesting that the best performing metho d was actually naiv e Bayes trained with strings.

However, as the authors discuss, one must question the stabilit y of dll names, function names, and string features. For instance, one may be able to compile a source program using another compiler to pro duce an executable di eren t enough to avoid detection. Programmers often use metho ds to obfuscate their code, so a list of dll s or function names may not be available.
 The authors paired eac h feature extraction metho d with a learning metho d, and as a result, ripper was trained on a much smaller collection of executables than were naiv e Bayes and the ensem ble of naiv e-Ba yesian classi ers. Although results were generally good, it would have been interesting to kno w how the learning metho ds performed on all data sets. It would have also been interesting to kno w if com bining all features (i.e., strings, bytes, functions) into a single training example and then selecting the most relev ant would have impro ved the performance of the metho ds.

There are other metho ds of guarding against malicious code, suc h as obje ct reconciliation [3, p. 370], whic h involves comparing curren t les and directories to past copies; one can also compare cryptographic hashes. One can also au-dit running programs [38] and statically analyze executables using pre-de ned malicious patterns [7]. These approac hes are not based on data mining, although one could imagine the role suc h techniques migh t play.

Researc hers have also investigated classi cation metho ds for the determination of soft ware authorship. Most noto-rious in the eld of authorship are the e orts to determine whether Sir Frances Bacon wrote works attributed to Shak e-speare [13], or who wrote the twelve disputed Federalist Pa-pers, Hamilton or Madison [22]. Recen tly, similar techniques have been used in the relativ ely new eld of softwar e foren-sics to determine program authorship [39]. Gra y et al. [15] wrote a position pap er on the sub ject of authorship, whereas Krsul [23] conducted an empirical study by gathering code from programmers of varying skill, extracting soft ware met-rics, and determining authorship using discriminan t analy-sis. There are also relev ant results published in the literature pertaining to the plagiarism of programs [2, 19], whic h we will not surv ey here.

Krsul [23] collected 88 programs written in the c program-ming language from 29 programmers at the undergraduate, graduate, and facult y levels. He then extracted 18 layout metrics (e.g., inden tation of closing curly brac kets), 15 style metrics (e.g., mean line length), and 19 structure metrics (e.g., percen tage of int function de nitions). On average, Krsul determined correct authorship 73% of the time. In-terestingly , of the 17 most exp erienced programmers, he was able to determine authorship 100% of the time. The least exp erienced programmers were the most dicult to classify , presumably because they had not settled into a consisten t style. Indeed, they \were surprised to nd that one [pro-grammer] had varied his programming style considerably from program to program in a perio d of only two mon ths" [24, x 5.1].

While interesting, it is unclear how much con dence we should have in these results. Krsul [23] used 52 features and only one or two examples for eac h of the 20 classes (i.e., the authors). This seems underconstrained, esp ecially when rules of thum b suggest that one needs ten times more examples than features [18]. On the other hand, it may also suggest that one simply needs to be clev er about what constitutes an example. For instance, one could presumably use functions as examples rather than programs, but for the task of determining authorship of malicious programs, it is unclear whether suc h data would be possible to collect or if it even exists. Fortunately , as we discuss in the next section, a lack of data was not a problem for our pro ject.
As stated previously , the data for our study consisted of 1971 benign executables and 1651 malicious executables. All were in the Windo ws pe format. We obtained benign exe-cutables from all folders of mac hines running the Windo ws 2000 and xp operating systems. We gathered additional ap-plications from SourceF orge ( http://sourceforge.net ). We obtained viruses, worms, and Trojan horses from the Web site VX Hea vens ( http://vx.netlux.org ) and from computer-forensic exp erts at the mitre Corp oration, the sponsors of this pro ject. Some executables were obfuscated with compression, encryption, or both; some were not, but we were not informed whic h were and whic h were not. For one collection, a commercial pro duct for detecting viruses failed to iden tify 18 of the 114 malicious executables. Note that for viruses, we examined only the loader programs; we did not include infected executables in our study .
We used the hexdump utilit y [29] to con vert eac h exe-cutable to hexadecimal codes in an asci i format. We then pro duced n -grams, by com bining eac h four-b yte sequence into a single term. For instance, for the byte sequence ff 00 ab 3e 12 b3 , the corresp onding n -grams would be ff00ab3e , 00ab3e12 , and ab3e12b3 . This pro cessing re-sulted in 255,904,403 distinct n -grams. One could also com-pute n -grams from words, something we explored and dis-cuss further in Section 6.1. Using the n -grams from all of the executables, we applied techniques from information re-triev al and text classi cation, whic h we discuss further in the next section.
Our overall approac h drew techniques from information retriev al (e.g., [16]) and from text classi cation (e.g., [12, 36]). We used the n -grams extracted from the executa-bles to form training examples by viewing eac h n -gram as a binary attribute that is either presen t in (i.e., 1) or absen t from (i.e., 0) the executable. We selected the most relev ant attributes (i.e., n -grams) by computing the information gain ( IG ) for eac h: where C is the class, v j is the value of the j th attribute, P ( v j ; C ) is the prop ortion that the j th attribute has the value v j in the class C i , P ( v j ) is the prop ortion that the j th n -gram tak es the value v j in the training data, P ( C ) is the prop ortion of the training data belonging to the class C . This measure is also called aver age mutual information [43].
We then selected the top 500 n -grams, a quan tity we de-termined through pilot studies (see Section 6.1), and applied sev eral learning metho ds, most of whic h are implemen ted in weka [42]: ib k , tfidf , naiv e Bayes, a supp ort vector ma-chine ( svm ), and a decision tree. We also \boosted" the last three of these learners, and we discuss eac h of these metho ds in the follo wing sections.
One of the simplest learning metho ds is the instance-based ( ib ) learner [1]. Its concept description is a collection of training examples or instances. Learning, therefore, is the addition of new examples to the collection. To classify an unkno wn instance, the performance elemen t nds the ex-ample in the collection most similar to the unkno wn and returns the example's class lab el as its prediction for the unkno wn. For binary attributes, suc h as ours, a con venien t measure of similarit y is the num ber of values two instances have in common. Varian ts of this metho d, suc h as ib k , nd the k most similar instances and return the ma jorit y vote of their class lab els as the prediction. Values for k are typically odd to prev ent ties. Suc h metho ds are also kno wn as nearest neighb or and k-nearest neighb ors .
For the tfidf classi er, we follo wed a classical approac h from information retriev al [16]. We used the vector space model , whic h entails assigning to eac h executable (i.e., doc-umen t) a vector of size equal to the total num ber of distinct n -grams (i.e., terms) in the collection. The comp onen ts of eac h vector were weigh ts of the top n -grams presen t in the executable. For the j th n -gram of the i th executable, the metho d computes the weigh t w ij , de ned as where tf ij (i.e., term frequency) is the num ber of times the i th n -gram app ears in the j th executable and idf j = log d (i.e., the inverse documen t frequency), where d is the total num ber of executables and df j is the num ber of executables that con tain the j th n -gram. It is imp ortan t to note that this classi er was the only one that used con tinuous attribute values; all others used binary attribute values.

To classify an unkno wn instance, the metho d uses the top n -grams from the executable, as describ ed previously , to form a vector, ~u , the comp onen ts of whic h are eac h n -gram's inverse documen t frequency (i.e., u j = idf j ).

Once formed, the classi er computes a similarit y coe-cien t ( SC ) between the vector for the unkno wn executable and eac h vector for the executables in the collection using the cosine similarity measur e : where ~u is the vector for the unkno wn executable, ~ w i vector for the i th executable, and k is the num ber of distinct n -grams in the collection.

After selecting the top ve closest matc hes to the un-kno wn, the metho d tak es a weigh ted ma jorit y vote of the executable lab els, and returns the class with the least weigh t as the prediction. It uses the cosine measure as the weigh t. Since we evaluated the metho ds using roc analysis [40], whic h requires case ratings , we summed the cosine mea-sures of the negativ e executables in the top ve, subtracted the sum of the cosine measures of the positiv e executables, and used the resulting value as the rating. In the follo w-ing discussion, we will refer to this metho d as the tfidf classi er.
Naiv e Bayes is a probabilistic metho d that has a long history in information retriev al and text classi cation [26]. It stores as its concept description the prior probabilit y of eac h class, P ( C i ), and the conditional probabilit y of eac h attribute value given the class, P ( v j j C i ). It estimates these quan tities by coun ting in training data the frequency of oc-currence of the classes and of the attribute values for eac h class. Then, assuming conditional indep endence of the at-tributes, it uses Bayes' rule to compute the posterior proba-bilit y of eac h class given an unkno wn instance, returning as its prediction the class with the highest suc h value: For roc analysis, we used the posterior probabilit y of the negativ e class as the case rating.
Supp ort vector mac hines ( svm s) [5] have performed well on traditional text classi cation tasks [12, 20, 36], and per-formed well on ours. The metho d pro duces a linear classi er, so its concept description is a vector of weigh ts, ~ w , and an intercept or a threshold, b . However, unlik e other linear clas-si ers, suc h as Fisher's, svm s use a kernel function to map training data into a higher dimensioned space so that the problem is linearly separable. It then uses quadratic pro-gramming to set ~ w and b suc h that the hyperplane's margin is optimal, meaning that the distance is maximal from the hyperplane to the closest examples of the positiv e and neg-ativ e classes. During performance, the metho d predicts the positiv e class if h ~ w ~x i b &gt; 0 and predicts the negativ e class otherwise. Quadratic programming can be exp ensiv e for large problems, but sequen tial minimal optimization ( smo ) is a fast, ecien t algorithm for training svm s [32] and is the one implemen ted in weka [42]. During performance, this implemen tation computes the probabilit y of eac h class [33], and for roc analysis, we used probabilit y of the negativ e class as the rating.
A decision tree is a tree with internal nodes corresp ond-ing to attributes and leaf nodes corresp onding to class lab els. For sym bolic attributes, branc hes leading to children corre-spond to the attribute's values. The performance elemen t uses the attributes and their values of an instance to traverse the tree from the root to a leaf. It predicts the class lab el of the leaf node. The learning elemen t builds suc h a tree by selecting the attribute that best splits the training exam-ples into their prop er classes. It creates a node, branc hes, and children for the attribute and its values, remo ves the attribute from further consideration, and distributes the ex-amples to the appropriate child node. This pro cess rep eats recursiv ely until a node con tains examples of the same class, at whic h point, it stores the class lab el. Most implemen ta-tions use the gain ratio for attribute selection [35], a mea-sure based on the information gain. In an e ort to reduce overtraining, most implemen tations also prune induced de-cision trees by remo ving subtrees that are likely to perform poorly on test data. weka 's j48 [42] is an implemen tation of the ubiquitous c4.5 [35]. During performance, j48 as-signs weigh ts to eac h class, and we used the weigh t of the negativ e class as the case rating.
Boosting [14] is a metho d for com bining multiple classi-ers. Researc hers have sho wn that ensemble metho ds often impro ve performance over single classi ers [9, 31]. Boost-ing pro duces a set of weigh ted mo dels by iterativ ely learn-ing a mo del from a weigh ted data set, evaluating it, and rew eigh ting the data set based on the mo del's performance. During performance, the metho d uses the set of mo dels and their weigh ts to predict the class with the highest weigh t. We used the AdaBo ost.M1 algorithm [14] implemen ted in weka [42] to boost svm s, j48 , and naiv e Bayes. As the case rating, we used the weigh t of the negativ e class. Note that we did not apply AdaBo ost.M1 to ib k because of the high computational exp ense.
To evaluate the approac hes and metho ds, we used strat-i ed ten-fold cross-v alidation. That is, we randomly parti-tioned the executables into ten disjoin t sets of equal size, selected one as a testing set, and com bined the remaining nine to form a training set. We conducted ten suc h runs using eac h partition as the testing set.

For eac h run, we extracted n -grams from the executa-bles in the training and testing sets. We selected the most relev ant features from the training data, applied eac h classi-cation metho d, and used the resulting classi er to rate the examples in the test set.

To conduct roc analysis [40], for eac h metho d, we pooled the ratings from the iterations of cross-v alidation, and used labroc4 [28] to pro duce an empirical roc curv e and to com-pute its area and the standard error of the area. With the standard error, we computed 95% con dence interv als [40]. We presen t and discuss these results in the next section.
We conducted three exp erimen tal studies using our data collection and exp erimen tal metho dology , describ ed previ-ously . We rst conducted pilot studies to determine the size of words and n -grams, and the num ber of n -grams relev ant for prediction. Once determined, we applied all of the clas-si cation metho ds to a small collection of executables. We then applied the metho dology to a larger collection of exe-cutables, all of whic h we describ e in the next three sections.
We conducted pilot studies to determine three quan tities: the size of n -grams, the size of words, and the num ber of se-lected features. Unfortunately , due to computational over-head, we were unable to evaluate exhaustiv ely all metho ds for all settings of these parameters, so we assumed that the num ber of features would most a ect performance, and be-gan our investigation accordingly .

Using the exp erimen tal metho dology describ ed previously , we extracted bytes from 476 malicious executables and 561 benign executables and pro duced n -grams, for n = 4. (This smaller set of executables constituted our initial collection, whic h we later supplemen ted.) We then selected the best 10, 20, . . . , 100, 200, . . . , 1000, 2000, . . . , 10,000 n -grams, and evaluated the performance of a svm , boosted svm s, naiv e Bayes, j48 , and boosted j48 . Selecting 500 n -grams pro-duced the best results.

We xed the num ber of n -grams at 500, and varied n , the size of the n -grams. We evaluated the same metho ds for n = 1 ; 2 ; : : : ; 10, and n = 4 pro duced the best results. We also varied the size of the words (one byte, two bytes, etc.), and results suggested that single bytes pro duced better results than did multiple bytes.

And so by selecting the top 500 n -grams of size four pro-duced from single bytes, we evaluated all of the classi cation metho ds on this small collection of executables. We describ e the results of this exp erimen t in the next section.
Pro cessing the small collection of executables pro duced 68,744,909 distinct n -grams. Follo wing our exp erimen tal metho dology , we used ten-fold cross-v alidation, selected the 500 best n -grams, and applied all of the classi cation meth-ods. The roc curv es for these metho ds are in Figure 1, while the areas under these curv es with 95% con dence interv als are in Table 2.

As one can see, the boosted metho ds performed well, as did the instance-based learner and the supp ort vector ma-chine. Naiv e Bayes did not perform as well, and we discuss this further in Section 7.
With success on a small collection, we turned our atten-tion to evaluating the text-classi cation metho ds on a larger collection of executables. As men tioned previously , this col-lection consisted of 1971 benign executables and 1651 mali-cious executables, while pro cessing resulted in over 255 mil-lion distinct n -grams of size four. We follo wed the same exp erimen tal metho dology|selecting the 500 top n -grams for eac h run of ten-fold cross-v alidation, applying the clas-si cation metho ds, and plotting roc curv es.

Figure 2 sho ws the roc curv es for the various metho ds, while Table 3 presen ts the areas under these curv es ( auc ) with 95% con dence interv als. As one can see, boosted j48 outp erformed all other metho ds. Other metho ds, suc h as ib k and boosted svm s, performed comparably , but the roc curv e for boosted j48 dominated all others.
To date, our results suggest that metho ds of text classi-cation are appropriate for detecting malicious executables in the wild. Boosted classi ers, ib k , and a supp ort vec-tor mac hine performed exceptionally well given our curren t data collection. That the boosted classi ers generally out-performed single classi ers echos the conclusion of sev eral empirical studies of boosting [4, 6, 9, 14], whic h suggest that boosting impro ves the performance of unstable clas-si ers, suc h as j48 , by reducing their bias and variance Figure 1: roc curv es for detecting malicious exe-cutables in the small collection. Top: The entire roc graph. Bottom: A magni cation.
 Table 2: Results for detecting malicious executables in the small collection. Areas under the roc curv e ( auc ) with 95% con dence interv als.
 Figure 2: roc curv es for detecting malicious exe-cutables in the larger collection. Top: The entire roc graph. Bottom: A magni cation.
 Table 3: Results for detecting malicious executables in the larger collection. Areas under the roc curv e ( auc ) with 95% con dence interv als.
 [4, 6]. Boosting can adv ersely a ect stable classi ers [4], suc h as naiv e Bayes, although in our study , boosting naiv e Bayes impro ved performance. Stabilit y may also explain why the bene t of boosting svm s was inconclusiv e in our study [6].

Our exp erimen tal results suggest that the metho dology will scale to larger collections of executables. The larger collection in our study con tained more than three times the num ber of executables in the smaller collection. Yet, as one can see in Tables 2 and 3, the absolute performance of all of the metho ds was better for the larger collection than for the smaller. The relativ e performance of the metho ds changed somewhat. For example, the svm moved from fourth to second, displacing the boosted svm s and ib k .

Visual insp ection of the concept descriptions yielded in-teresting insigh ts, but further work is required before these descriptions will be directly useful for computer-forensic ex-perts. For instance, one short branc h of a decision tree in-dicated that any executable with two pe headers is mali-cious. After analysis of our collection of malicious executa-bles, we disco vered two executables that con tained another executable. While this was an interesting nd, it represen ted an insigni can tly small portion of the malicious programs.
Leaf nodes covering man y executables were often at the end of long branc hes where one set of n -grams (i.e., byte codes) had to be presen t and another set had to be absen t. Understanding why the absence of byte codes was imp ortan t for an executable being malicious pro ved to be a dicult and often imp ossible task. It was fairly easy to establish that some n -grams in the decision tree were from string sequences and that some were from code sequences, but some were incomprehensible. For example, one n -gram app eared in 75% of the malicious executables, but it was not part of the executable format, it was not a string sequence, and it was not a code sequence. We have yet to determine its purp ose.
Nonetheless, for the large collection of executables, the size of the decision trees averaged over 10 runs was about 90 nodes. No tree exceeded 103 nodes. The heigh ts of the trees nev er exceeded 13 nodes, and subtrees of heigh ts of 9 or less covered roughly 99.3% of the training examples. While these trees did not supp ort a thorough forensic analysis, they did compactly enco de a large num ber of benign and malicious executables.

To place our results in con text with the study of Schultz et al. [37], they rep orted that the best performing approac hes were naiv e Bayes trained on the prin table strings from the program and an ensem ble of naiv e-Ba yesian classi ers trained on byte sequences. They did not rep ort areas under their roc curv es, but visual insp ection of these curv es sug-gests that with the exception of naiv e Bayes, all of our meth-ods outp erformed their ensem ble of naiv e-Ba yesian classi-ers. It also app ears that our best performing metho ds, suc h as boosted j48 , outp erformed their naiv e Bayesian classi er trained with strings.

These di erences in performance could be due to sev eral factors. We analyzed di eren t types of executables: Their collection consisted mostly of viruses, whereas ours con-tained viruses, worms, and Trojan horses. Ours consisted of executables in the Windo ws pe format; about 5.6% of theirs was in this format.

Our better results could be due to how we pro cessed byte sequences. Schultz et al. [37] used non-o verlapping two-b yte sequences, whereas we used overlapping sequences of four bytes. With their approac h it is possible that a useful feature (i.e., a predictiv e sequence of bytes) would be split across a boundary . This could explain why in their study string features app eared to be better than byte sequences, since extracted strings would not be brok en apart. Their approac h pro duced much less training data than did ours, but our application of feature selection reduced the original set of more than 255 million n -grams to a manageable 500.
Our results for naiv e Bayes were poor in comparison to theirs. We again attribute this to the di erences in data ex-traction metho ds. Naiv e Bayes is well kno wn to be sensitiv e to conditionally dep enden t attributes [10]. We used over-lapping byte sequences as attributes, so there were man y that were conditionally dep enden t. Indeed, after analyzing decision trees pro duced by j48 , we found evidence that over-lapping sequences were imp ortan t for detection. Speci cally , some subpaths of these decision trees consisted of sequen-tially overlapping terms that together formed byte sequences relev ant for prediction. Schultz et al.'s [37] extraction meth-ods would not have pro duced conditionally dep enden t at-tributes to the same degree, if at all, since they used strings and non-o verlapping byte sequences.

Regarding our exp erimen tal design, we decided to pool a metho d's ratings and pro duce a single roc curv e (see Sec-tion 5) because labroc4 [28] occasionally could not t an roc curv e to a metho d's ratings from a single fold of cross-validation (i.e., the ratings were degenerate). We also con-sidered pro ducing roc con vex hulls [34] and cost curv es [11], but determined that traditional roc analysis was appropri-ate for our results (e.g., the curv e for boosted j48 dominated all other curv es).

In our study , there was an issue of high computational overhead. Selecting features was exp ensiv e, and we had to resort to a disk-based implemen tation for computing infor-mation gain, whic h required a great deal of time and space to execute. However, once selected, weka 's [42] Java imple-men tations executed quic kly on the training examples with their 500 binary attributes.

In terms of our approac h, it is imp ortan t to note that we have investigated other metho ds of data extraction. For instance, we examined whether prin table strings from the executable migh t be useful, but reasoned that subsets of n -grams would capture the same information. Indeed, after insp ecting some of the decision trees that j48 pro duced, we found evidence suggesting that n -grams formed from strings were being used for detection. Nonetheless, if we later de-termine that explicitly represen ting prin table strings is im-portan t, we can easily extend our represen tation to enco de their presence or absence. On the other hand, as we stated previously , one must question the use of prin table strings or dll information since compression and other forms of obfuscation can mask this information.

We also considered using disassem bled code as training data. For malicious executables using compression, being able to obtain a disassem bly of critical sections of code may be a questionable assumption. Moreo ver, in pilot studies, a commercial pro duct failed to disassem ble some of our mali-cious executables.

We considered an approac h that runs malicious executa-bles in a sandb ox and pro duces an audit of the mac hine instructions. Naturally , we would not be able to completely execute the program, but 10,000 instructions may be suf-cien t to di eren tiate benign and malicious beha vior. We have not pursued this idea because of a lack of auditing tools, the dicult y of handling large num bers of interactiv e programs, and the inabilit y of detecting malicious beha vior occurring near the end of sucien tly long programs.
There are at least two immediate commercial applications of our work. The rst is a system, similar to mecs , for de-tecting malicious executables. Serv er soft ware would need to store all kno wn malicious executables and a comparably large set of benign executables. Due to the computational overhead of pro ducing classi ers from suc h data, algorithms for computing information gain and for evaluating classi ca-tion metho ds would have to be executed in parallel. Clien t soft ware would need to extract only the top n -grams from a given executable, apply a classi er, and predict. Up dates to the classi er could be made remotely over the Internet. Since the best performing metho d may change with new training data, it will be critical for the serv er to evaluate a variet y of metho ds and for the clien t to accommo date any of the poten tial classi ers. Used in conjunction with stan-dard signature metho ds, these metho ds could pro vide better detection of malicious executables than is curren tly possible.
The second is a system orien ted more toward computer-forensic exp erts. Even though work remains before decision trees could be used to analyze malicious executables, one could use ib k or the tfidf classi er to retriev e kno wn ma-licious executables similar to a newly disco vered malicious executable. Based on the prop erties of the retriev ed ex-ecutables, suc h a system could give investigators insigh ts into the new executable's function. However, it remains an open issue whether an executable's statistical prop erties are predictiv e of its functional characteristics, an issue we are curren tly investigating and one we discuss brie y in the con-cluding section.
We considered the application of techniques from infor-mation retriev al and text classi cation to the problem of detecting unkno wn malicious executables in the wild. After evaluating a variet y of classi cation metho ds, results suggest that boosted j48 pro duced the best classi er with an area under the roc curv e of 0.996. Our metho dology resulted in a elded application called mecs , the Malicious Executable Classi cation System, whic h we have deliv ered to the mitre Corp oration.

In future work, we plan to investigate a classi cation task in whic h metho ds determine the functional char acteristics of malicious executables. Detecting malicious executables is imp ortan t, but after detection, computer-forensic exp erts must determine the program's functional characteristics: Does it mass-mail? Does it mo dify system les? Does it open a bac kdo or? This will entail remo ving obfuscation, suc h as compression, if possible. Furthermore, most mali-cious executables perform multiple functions, so eac h train-ing example will have multiple class lab els, a problem that arises in bioinformatics and in documen t classi cation.
We anticipate that mecs , the Malicious Executable Clas-si cation System, is but one step in an overall scheme for detecting and classifying \malw are." When com bined with approac hes that searc h for kno wn signatures, we hop e that suc h a strategy for detecting and classifying malicious exe-cutables will impro ve the securit y of computers. Indeed, the deliv ery of mecs to mitre has pro vided computer-forensic exp erts with a valuable tool. We anticipate that pursuing the classi cation of executables into functional categories will pro vide another.
The authors rst and foremost thank William Asmond and Thomas Ervin of the mitre Corp oration for pro vid-ing their exp ertise, advice, and collection of malicious ex-ecutables. The authors also thank the anon ymous review-ers for their time and useful commen ts, Ophir Frieder of iit for help with the vector space mo del, Ab dur Cho wd-hury of aol for advice on the scalabilit y of the vector space mo del, Bob Wagner of the fda for assistance with roc analysis, Eric Blo edorn of mitre for general guidance on our approac h, and Matthew Krause of Georgeto wn for help-ful commen ts on an earlier draft of the pap er. Finally , we thank Ric hard Squier of Georgeto wn for supplying much of the additional computational resources needed for this study through Gran t No. DAAD19-00-1-0165 from the U.S. Arm y Researc h Oce. This researc h was conducted in the De-partmen t of Computer Science at Georgeto wn Univ ersit y. Our work was supp orted by the mitre Corp oration under con tract 53271. [1] D. Aha, D. Kibler, and M. Alb ert. Instance-based [2] A. Aik en. MOSS: A system for detecting soft ware [3] Anon ymous. Maximum Security . Sams Publishing, [4] B. Bauer and R. Koha vi. An empirical comparison of [5] B. Boser, I. Guy on, and V. Vapnik. A training [6] L. Breiman. Arcing classi ers. The Annals of [7] M. Christo dorescu and S. Jha. Static analysis of [8] W. Cohen. Fast e ectiv e rule induction. In Proceedings [9] T. Dietteric h. An exp erimen tal comparison of three [10] P. Domingos and M. Pazzani. On the optimalit y of the [11] C. Drummond and R. Holte. Explicitly represen ting [12] S. Dumais, J. Platt, D. Hec kerman, and M. Sahami. [13] E. Durning-La wrence. Bac on is Shake-sp eare . The [14] Y. Freund and R. Schapire. Exp erimen ts with a new [15] A. Gra y, P. Sallis, and S. MacDonell. Soft ware [16] D. Grossman and O. Frieder. Information retrieval: [17] D. Hand, H. Mannila, and P. Sm yth. Principles of [18] A. Jain, R. Duin, and J. Mao. Statistical pattern [19] H. Jank owitz. Detecting plagiarism in studen t Pascal [20] T. Joac hims. Text categorization with supp ort vector [21] J. Kephart, G. Sorkin, W. Arnold, D. Chess, [22] B. Kjell, W. Woods., and O. Frieder. Discrimination [23] I. Krsul. Authorship analysis: Iden tifying the author [24] I. Krsul and E. Spa ord. Authorship analysis: [25] R. Lo, K. Levitt, and R. Olsson. MCF: A malicious [26] M. Maron and J. Kuhns. On relev ance, probabilistic [27] G. McGra w and G. Morisett. Attac king malicious [28] C. Metz, Y. Jiang, H. MacMahon, R. Nishik awa, and [29] P. Miller. hexdump 1.4. Soft ware, http://gd. [30] T. Mitc hell. Machine Learning . McGra w-Hill, New [31] D. Opitz and R. Maclin. Popular ensem ble metho ds: [32] J. Platt. Fast training of supp ort vector mac hines [33] J. Platt. Probabilities for SV mac hines. In P. Bartlett, [34] F. Pro vost and T. Fawcett. Robust classi cation for [35] J. Quinlan. C4.5: Programs for machine learning . [36] M. Sahami, S. Dumais, D. Hec kerman, and [37] M. Schultz, E. Eskin, E. Zadok, and S. Stolfo. Data [38] S. Soman, C. Krin tz, and G. Vigna. Detecting [39] E. Spa ord and S. Weeb er. Soft ware forensics: Can we [40] J. Swets and R. Pic kett. Evaluation of diagnostic [41] G. Tesauro, J. Kephart, and G. Sorkin. Neural [42] I. Witten and E. Frank. Data mining: Practic al [43] Y. Yang and J. Pederson. A comparativ e study on
