 Integrating AI with Human Computer Interaction has received significant attention in recent years [8, 11, 13, 3, 2]. In most applications, e.g. travel scheduling, information retrieval, or computer desktop navigation, the relevant state of the computer is fully observable, but the goal of the user is not, which poses a difficult problem to the computer assistant. The assistant needs to correctly reason about the relative merits of taking different actions in the presence of significant uncertainty about the goals of the human agent. It might consider taking actions that directly reveal the goal of the agent, e.g. by asking questions to the user. However, direct communication is often difficult due to the language mismatch between the human and the computer. Another strategy is to take actions that help achieve the most likely goals. Yet another strategy is to take actions that help with a large number of possible goals. In this paper, we formulate and study several classes of interactive assistant problems from the points of view of decision theory and computational complexity. Building on the framework of decision-theoretic assistance (DTA) [5], we analyze the inherent computational complexity of optimal assistance in a variety of settings and the sources of that complexity. Positively, we analyze a simple myopic heuristic and show that it performs nearly optimally in a reasonably pervasive assistance problem, thus explaining some of the positive empirical results of [5].
 We formulate the problem of optimal assistance as solving a hidden-goal MDP (HGMDP), which is a special case of a POMDP [6]. In a HGMDP, a (human) agent and a (computer) assistant take actions in turns. The agent X  X  goal is the only unobservable part of the state of the system and does not change throughout the episode. The objective for the assistant is to find a history-dependent policy that maximizes the expected reward of the agent given the agent X  X  goal-based policy and its goal distribution. Despite the restricted nature of HGMDPs, the complexity of determining if an HGMDP has a finite-horizon policy of a given value is PSPACE-complete even in deterministic environments. This motivates a more restricted model called Helper Action MDP (HAMDP), where the assistant executes a helper action at each step. The agent is obliged to accept the helper action if it is helpful for its goal and receives a reward bonus (or cost reduction) for doing so. Otherwise, the agent can continue with its own preferred action without any reward or penalty to the assistant. We show classes of this problem that are complete for PSPACE and NP. We also show that for the class of HAMDPs with deterministic agents there are polynomial time algorithms for minimizing the expected and worst-case regret relative to an omniscient assistant. Further, we show that the optimal worst case regret can be characterized by a graph-theoretic property called the tree rank of the corresponding all-goals policy tree and can be computed in linear time.
 The main positive result of the paper is to give a simple myopic policy for general stochastic HAMDPs that has a regret which is upper bounded by the entropy of the goal distribution. Fur-thermore we give a variant of this policy that is able to achieve worst-case and expected regret that is logarithmic in the number of goals without any prior knowledge of the goal distribution. To the best of our knowledge, this is the first formal study of the computational hardness of the prob-lem of decision-theoretically optimal assistance and the performance of myopic heuristics. While the current HAMDP results are confined to unobtrusively assisting a competent agent, they provide a strong foundation for analyzing more complex classes of assistant problems, possibly including direct communication, coordination, partial observability, and irrationality of users. Throughout the paper we will refer to the entity that we are attempting to assist as the agent and the assisting entity as the assistant . Our objective is to select actions for the assistant in order to help the agent maximize its reward. The key complication is that the agent X  X  goal is not directly observable to the assistant, so reasoning about the likelihood of possible goals and how to help maximize reward given those goals is required. In order to support this type of reasoning we will model the agent-assistant process via hidden goal MDPs (HGMDPs) .
 General Model. An HGMDP describes the dynamics and reward structure of the environment via a first-order Markov model, where it is assumed that the state is fully observable to both the agent and assistant. In addition, an HGMDP describes the possible goals of the agent and the behavior of the agent when pursuing those goals. More formally, an HGMDP is a tuple  X  S,G,A,A 0 ,T,R, X ,I S ,I G  X  where S is a set of states, G is a finite set of possible agent goals, A is the set of agent actions, A 0 is the set of assistant actions, T is the transition function such that T ( s,g,a,s 0 ) is the probability of a transition to state s 0 from s after taking action a  X  A  X  A 0 when the agent goal is g , R is the reward function which maps S  X  G  X  ( A  X  A 0 ) to real valued rewards,  X  is the agent X  X  policy that maps S  X  G to distributions over A and need not be optimal in any sense, and I S ( I G ) is an initial state (goal) distribution. The dependence of the reward and policy on the goal allows the model to capture the agent X  X  desires and behavior under each goal. The dependence of T on the goal is less intuitive and in many cases there will be no dependence when T is used only to model the dynamics of the environment. However, we allow goal dependence of T for generality of modeling. For example, it can be convenient to model basic communication actions of the agent as changing aspects of the state, and the result of such actions will often be goal dependent. We consider a finite-horizon episodic problem setting where the agent begins each episode in a state drawn from I S with a goal drawn from I G . The goal, for example, might correspond to a physical location, a dish that the agent wants to cook, or a destination folder on a computer desktop. The process then alternates between the agent and assistant executing actions (including noops) in the environment until the horizon is reached. The agent is assumed to select actions according to  X  . In many domains, a terminal goal state will be reached within the horizon, though in general, goals can have arbitrary impact on the reward function. The reward for the episode is equal to the sum of the rewards of the actions executed by the agent and assistant during the episode. The objective of the assistant is to reason about the HGMDP and observed state-action history in order to select actions that maximize the expected (or worst-case) total reward of an episode.
 An example HGMDP from previous work [5] is the doorman domain, where an agent navigates a grid world in order to arrive at certain goal locations. To move from one location to another the agent must open a door and then walk through the door. The assistant can reduce the effort for the agent by opening the relevant doors for the agent. Another example from [1] involves a computer desktop where the agent wishes to navigate to certain folders using a mouse. The assistant can select actions that offer the agent a small number of shortcuts through the folder structure.
 Given knowledge of the agent X  X  goal g in an HGMDP, the assistant X  X  problem reduces to solving an MDP over assistant actions. The MDP transition function captures both the state change due to the assistant action and also the ensuing state change due to the agent action selected according to the policy  X  given g . Likewise the reward function on a transition captures the reward due to the assistant action and the ensuing agent action conditioned on g . The optimal policy for this MDP corresponds to an optimal assistant policy for g . However, since the real assistant will often have uncertainty about the agent X  X  goal, it is unlikely that this optimal performance will be achieved. Computational Complexity. We can view an HGMDP as a collection of | G | MDPs that share the same state space, where the assistant is placed in one of the MDPs at the beginning of each episode, but cannot observe which one. Each MDP is the result of fixing the goal component of the HG-MDP definition to one of the goals. This collection can be easily modeled as a restricted type of partially observable MDP (POMDP) with a state space S  X  G . The S component is completely ob-servable, while the G component is unobservable but only changes at the beginning of each episode (according to I G ) and remains constant throughout an episode. Furthermore, each POMDP tran-sition provides observations of the agent action, which gives direct evidence about the unchanging G component. From this perspective HGMDPs appear to be a significant restriction over general POMDPs. However, our first result shows that despite this restriction the worst-case complexity is not reduced even for deterministic dynamics.
 Given an HGMDP M , a horizon m = O ( | M | ) where | M | is the size of the encoding of M , and a reward target r  X  , the short-term reward maximization problem asks whether there exists a history-dependent assistant policy that achieves an expected finite horizon reward of at least r  X  . For general POMDPs this problem is PSPACE-complete [12, 10], and for POMDPs with deterministic dynam-ics, it is NP-complete [9]. However, we have the following result.
 Theorem 1. Short-term reward maximization for HGMDPs with deterministic dynamics is PSPACE-complete.
 The proof is in the appendix. This result shows that any POMDP can be encoded as an HGMDP with deterministic dynamics, where the stochastic dynamics of the POMDP are captured via the stochastic agent policy in the HGMDP. However, the HGMDPs resulting from the PSPACE-hardness reduction are quite pathological compared to those that are likely to arise in practice. Most impor-tantly, the agent X  X  actions provide practically no information about the agent X  X  goal until the end of an episode, when it is too late to exploit this knowledge. This suggests that we search for restricted classes of HGMDPs that will allow for efficient solutions with performance guarantees. The motivation for HAMDPs is to place restrictions on the agent and assistant that avoid the fol-lowing three complexities that arise in general HGMDPs: 1) the agent can behave arbitrarily poorly if left unassisted and as such the agent actions may not provide significant evidence about the goal; 2) the agent is free to effectively  X  X gnore X  the assistant X  X  help and not exploit the results of assistive action, even when doing so would be beneficial; and 3) the assistant actions have the possibility of negatively impacting the agent compared to not having an assistant. HAMDPs will address the first issue by assuming that the agent is competent at (approximately) maximizing reward without the assistant. The last two issues will be addressed by assuming that the agent will always  X  X etect and exploit X  helpful actions and that the assistant actions do not hurt the agent.
 Informally, the HAMDP provides the assistant with a helper action for each of the agent X  X  actions. Whenever a helper action h is executed directly before the corresponding agent action a , the agent receives a bonus reward of 1. However, the agent will only accept the helper action h (by taking a ) and hence receive the bonus, if a is an action that the agent considers to be good for achieving the goal without the assistant. Thus, the primary objective of the assistant in an HAMDP is to maximize the number of helper actions that get accepted by the agent. While simple, this model captures much of the essence of assistance domains where assistant actions cause minimal harm and the agent is able to detect and accept good assistance when it arises.
 An HAMDP is an HGMDP  X  S,G,A,A 0 ,T,R, X ,I S ,I G  X  with the following constraints: In a HAMDP, the primary impact of an assistant action is to influence the reward of the following agent action. The only rewards in HAMDPS are the bonuses received whenever the agent accepts a helper action. Any additional environmental reward is assumed to be already captured by the agent policy via  X ( s,g ) that contains actions that approximately optimize this reward.
 The HAMDP model can be adapted to both the doorman domain in [5] and the folder prediction domain from [1]. In the doorman domain, the helper actions correspond to opening doors for the agent, which reduce the cost of navigating from one room to another. Importantly opening an incorrect door has a fixed reward loss compared to an optimal assistant, which is a key property of HAMDPs. In the folder prediction domain, the system proposes multiple folders to save a file, potentially saving the user a few clicks every time the proposal is accepted.
 Despite the apparent simplification of HAMDPs over HGMDPs, somewhat surprisingly the worst case computational complexity is not reduced.
 Theorem 2. Short-term reward maximization for HAMDPs is PSPACE-complete.
 The proof is in the appendix. Unlike the case of HGMDPs, we will see that the stochastic dynamics are essential for PSPACE-hardness. Despite this negative result, the following sections show the utility of the HAMDP restriction by giving performance guarantees for simple policies and improved complexity results in special cases. So far, there are no analogous results for HGMDPs. Given an assistant policy  X  0 , the regret of a particular episode is the extra reward that an omniscient assistant with knowledge of the goal would achieve over  X  0 . For HAMDPs the omniscient assistant can always achieve a reward equal to the finite horizon m , because it can always select a helper action that will be accepted by the agent. Thus, the regret of an execution of  X  0 in a HAMDP is equal to the number of helper actions that are not accepted by the agent, which we will call mispredictions . From above we know that optimizing regret is PSPACE-hard and thus here we focus on bounding the expected and worst-case regret of the assistant. We now show that a simple myopic policy is able to achieve regret bounds that are logarithmic in the number of goals.
 Myopic Policy. Intuitively, our myopic assistant policy  X   X  will select an action that has the highest probability of being accepted with respect to a  X  X oarsened X  version of the posterior distribution over goals. The myopic policy in state s given history H is based on the consistent goal set C ( H ) , which is the set of goals that have non-zero probability with respect to history H . It is straightforward to maintain C ( H ) after each observation. The myopic policy is defined as: where G ( s,a ) = { g | a  X   X ( s,g ) } is the set of goals for which the agent considers a to be an acceptable action in state s . The expression I G ( C ( H )  X  G ( s,a )) can be viewed as the probability mass of G ( s,a ) under a coarsened goal posterior which assigns goals outside of C ( H ) probability zero and otherwise weighs them proportional to the prior.
 Theorem 3. For any HAMDP the expected regret of the myopic policy is bounded above by the entropy of the goal distribution H ( I G ) .
 Proof. The main idea of the proof is to show that after each misprediction of the myopic policy (i.e. the selected helper action is not accepted by the agent) the uncertainty about the goal is reduced by a constant factor, which will allow us to bound the total number of mispredictions on any trajectory. Consider a misprediction step where the myopic policy selects helper action h i in state s given his-tory H , but the agent does not accept the action and instead selects a  X  6 = a i . By the definition of the myopic policy we know that I G ( C ( H )  X  G ( s,a i ))  X  I G ( C ( H )  X  G ( s,a  X  )) , since otherwise the assistant would not have chosen h i . From this fact we now argue that I G ( C ( H 0 ))  X  I G ( C ( H )) / 2 where H 0 is the history after the misprediction. That is, the probability mass under I G of the con-sistent goal set after the misprediction is less than half that of the consistent goal set before the misprediction. To show this we will consider two cases: 1) I G ( C ( H )  X  G ( s,a i )) &lt; I G ( C ( H )) / 2 , and 2) I G ( C ( H )  X  G ( s,a i ))  X  I G ( C ( H )) / 2 . In the first case, we immediately get that I we get the desired result that I G ( C ( H 0 ))  X  I G ( C ( H )) / 2 . In the second case, note that Combining this with our assumption for the second case implies that I G ( C ( H 0 ))  X  I G ( C ( H )) / 2 . This implies that for any episode, after n mispredictions resulting in a history H n , I G ( C ( H n ))  X  2  X  n . Now consider an arbitrary episode where the true goal is g . We know that I G ( g ) is a lower bound on I G ( C ( H n )) , which implies that I G ( g )  X  2  X  n or equivalently that n  X   X  log( I G ( g )) . Thus for any episode with goal g the maximum number of mistakes is bounded by  X  log( I G ( g )) . Using this fact we get that the expected number of mispredictions during an episode with respect to I G is bounded above by  X  P g I G ( g ) log( I G ( g )) = H ( I G ) , which completes the proof. Since H ( I G )  X  log( | G | ) , this result implies that for HAMDPs the expected regret of the myopic policy is no more than logarithmic in the number of goals. Furthermore, as the uncertainty about the goal decreases (decreasing H ( I G ) ) the regret bound improves until we get a regret of 0 when I G puts all mass on a single goal. This logarithmic bound is asymptotically tight in the worst case. Theorem 4. There exists a HAMDP such that for any assistant policy the expected regret is at least log( | G | ) / 2 .
 Proof. Consider a deterministic HAMDP such that the environment is structured as a binary tree of depth log( | G | ) , where each leaf corresponds to one of the | G | goals. By considering a uniform goal distribution it is easy to verify that at any node in the tree there is an equal chance that the true goal is in the left or right sub-tree during any episode. Thus, any policy will have a 0.5 chance of committing a misprediction at each step of an episode. Since each episode is of length log( | G | ) , the expected regret of an episode for any policy is log( | G | ) / 2 .
 Resolving the gap between the myopic policy bound and this regret lower bound is an open problem. Approximate Goal Distributions. Suppose that the assistant uses an approximate goal distribution I
G instead of the true underlying goal distribution I G when computing the myopic policy. That is, the assistant selects actions that maximize I 0 G ( C ( H )  X  G ( s,a )) , which we will refer to as the myopic policy relative to I 0 G . The extra regret for using I 0 G instead of I G can be bounded in terms of the KL-divergence between these distributions KL ( I G k I 0 G ) , which is zero when I 0 G equals I G . Theorem 5. For any HAMDP with goal distribution I G , the expected regret of the myopic policy with respect to distribution I 0 G is bounded above by H ( I G ) + KL ( I G k I 0 G ) .
 The proof is in the appendix. Deriving similar results for other approximations is an open problem. A consequence of Theorem 5 is that the myopic policy with respect to the uniform goal distribution has expected regret bounded by log( | G | ) for any HAMDP, showing that logarithmic regret can be achieved without knowledge of I G . This can be strengthened to hold for worst case regret. Theorem 6. For any HAMDP, the worst case and hence expected regret of the myopic policy with respect to the uniform goal distribution is bounded above by log( | G | ) .
 Proof. The proof of Theorem 5 shows that the number of mispredictions on any episode is bounded above by  X  log( I 0 G ) . In our case I 0 G = 1 / | G | which shows a worst case regret bound of log( | G | ) , which also bounds the expected regret of the uniform myopic policy. We now consider several special cases of HAMDPs. First, we restrict the agent X  X  policy to be deterministic for each goal, i.e.  X ( s,g ) has at most a single action for each state-goal pair ( s,g ) . Theorem 7. The myopic policy achieves the optimal expected reward for HAMDPs with determin-istic agent policies.
 The proof is given in the appendix. We now consider the case where both the agent policy and the environment are deterministic, and attempt to minimize the worst possible regret compared to an omniscient assistant who knows the agent X  X  goal. As it happens, this  X  X inimax policy X  can be captured by a graph-theoretic notion of tree rank that generalizes the rank of decision trees [4]. then rank(node) = 0 , else if a node has at least two distinct children c 1 and c 2 with equal highest ranks among all children, then rank(node) = 1+ rank ( c 1 ) . Otherwise rank(node) = rank of the highest ranked child.
 The optimal trajectory tree (OTT) of a HAMDP in deterministic environments is a tree where the nodes represent the states of the HAMDP reached by the prefixes of optimal action sequences for different goals starting from the initial state. 1 Each node in the tree represents a state and a set of goals for which it is on the optimal path from the initial state.
 Since the agent policy and the environment are both deterministic, there is at most one trajectory per goal in the tree. Hence the size of the optimal trajectory tree is bounded by the number of goals times the maximum length of any trajectory, which is at most the size of the state space in deterministic domains. The following Lemma follows by induction on the depth of the optimal trajectory tree. Lemma 1. The minimum worst-case regret of any policy for an HAMDP for deterministic envi-ronments and deterministic agent policies is equal to the tree rank of its optimal trajectory tree. Theorem 8. If the agent policy is deterministic, the problem of minimizing the maximum regret in HAMDPs in deterministic environments is in P.
 Proof. We first construct the optimal trajectory tree. We then compute its rank and the optimal minimax policy using the recursive definition of tree rank in linear time.
 The assumption of deterministic agent policy may be too restrictive in many domains. We now consider HAMDPs in which the agent policies have a constant bound on the number of possible actions in  X ( s,g ) for each state-goal pair. We call them bounded choice HAMDPs.
 Definition 2. The branching factor of a HAMDP is the largest number of possible actions in  X ( s,g ) by the agent in any state for any goal and any assistant X  X  action.
 The doorman domain of [5] has a branching factor of 2 since there are at most two optimal actions to reach any goal from any state.
 Theorem 9. Minimizing the worst-case regret in finite horizon bounded choice HAMDPS of a con-stant branching factor k  X  2 in deterministic environments is NP-complete.
 The proof is in the appendix. We can also show that minimizing the expected regret for a bounded k is NP-hard. We conjecture that this problem is also in NP, but this question remains open. In this paper, we formulated the problem of optimal assistance and analyzed its complexity in mul-tiple settings. We showed that the general problem of HGMDP is PSPACE-complete due to the lack of constraints on the user, who can behave stochastically or even adversarially with respect to the assistant, which makes the assistant X  X  task very difficult. By suitably constraining the user X  X  actions through HAMDPs, we are able to reduce the complexity to NP-complete, but only in deterministic environments with bounded choice agents. More encouragingly, we are able to show that HAMDPs are amenable to a simple myopic heuristic which has a regret bounded by the entropy of the goal distribution when compared to the omniscient assistant. This is a satisfying result since optimal communication of the goal requires as much information to pass from the agent to the assistant. Im-portantly, this result applies to stochastic as well as deterministic environments and with no bound on the number of agent X  X  action choices.
 Although HAMDPs are somewhat restricted compared to possible assistantship scenarios one could imagine, they in fact fit naturally to many domains where the user is on-line, knows which helper actions are acceptable, and accepts help when it is appropriate to the goal. Indeed, in many domains, it is reasonable to constrain the assistant so that the agent has the final say on approving the actions proposed by the assistant. These scenarios range from the ubiquitous auto-complete functions and Microsoft X  X  infamous Paperclip to more sophisticated adaptive programs such as SmartEdit [7] and TaskTracer [3] that learn assistant policies from users X  long-term behaviors. By analyzing the complexity of these tasks in a more general framework than what is usually done, we shed light on some of the sources of complexity such as the stochasticity of the environment and the agent X  X  policy. Many open problems remain including generalization of these and other results to more general assistant frameworks, including partially observable and adversarial settings, learning assistants, and multi-agent assistance. Proof of Theorem 1. Membership in PSPACE follows from the fact that any HGMDP can be poly-nomially encoded as a POMDP for which policy existence is in PSPACE. To show PSPACE-hardness, we reduce the QSAT problem to the problem of the existence of a history-dependent assistant policy of expected reward  X  r .
 Let  X  be a quantified Boolean formula  X  x 1  X  x 2  X  x 3 ...  X  x n { C 1 ( x 1 ,...,x n )  X  ...  X  C m ( x 1 ,...,x n ) } , where each C i is a disjunctive clause. For us, each goal g i is a quantified clause,  X  x 1  X  x 2  X  x 3 ...  X  x n { C i ( x 1 ,...,x n ) } . The agent chooses a goal uniformly randomly from the set of goals formed from  X  and hides it from the assistant. The states consist of pairs of the form ( v,i ) , where v  X  { 0 , 1 } is the current value of the goal clause, and i is the next variable to set. The ac-tions of the assistant are to set the existentially quantified variables. The agent simulates setting the universally quantified variables by choosing actions from the set { 0 , 1 } with equal probability. The episode terminates when all the variables are set, and the assistant gets a reward of 1 if the value of the clause is 1 at the end and a reward of 0 otherwise.
 Note that the assistant does not get any useful feedback from the agent until it is too late and it either makes a mistake or solves the goal. The best the assistant can do is to find an optimal history-dependent policy that maximizes the expected reward over the goals in  X  . If  X  is satisfiable, then there is an assistant policy that leads to a reward of 1 over all goals and all agent actions, and hence has an expected value of 1 over the goal distribution. If not, then at least one of the goals will not be satisfied for some setting of the universal quantifiers, leading to an expected value &lt; 1. Proof of Theorem 2. Membership in PSPACE follows easily since HAMDP is a specialization of HGMDP. The proof of PSPACE-hardness is identical to that of 1 except that here, instead of the agent X  X  actions, the stochastic environment models the universal quantifiers. The agent accepts all actions until the last one and sets the variable as suggested by the assistant. After each of the assistant X  X  actions, the environment chooses a value for the universally quantified variable with equal probability. The last action is accepted by the agent if the goal clause evaluates to 1, otherwise not. There is a history-dependent policy whose expected reward  X  the number of existential variables if and only if the quantified Boolean formula is satisfiable. Proof of Theorem 5. The proof is similar to that of Theorem 3, except that since the myopic policy is with respect to I 0 G rather than I G , on any episode, the maximum number of mispredictions n is bounded above by  X  log( I 0 G ( g )) . Hence, the average number of mispredictions is given by: Proof of Theorem 7. According to the theory of POMDPs, the optimal action in a POMDP maxi-mizes the sum of the immediate expected reward and the value of the resulting belief state (of the assistant) [6]. When the agent policy is deterministic, the initial goal distribution I G and the history of agent actions and states H fully capture the belief state of the agent. Let V ( I G ,H ) represent the optimal value of the current belief state. It satisfies the following Bellman equation, where H 0 stands for the history after the assistant X  X  action h i and the agent X  X  action a j .

Since there is only one agent X  X  action a  X  ( s,g ) in  X ( s,g ) , the subsequent state s 0 in H 0 , and its value do not depend on h i . Hence the best helper action h  X  of the assistant is given by: h  X  ( I G ,H ) = arg max where C ( H ) is the set of goals consistent with the current history H , and G ( s,a i ) is the set of goals g for which a i  X   X ( s,g ) . I ( a i  X   X ( s,g )) is an indicator function which is = 1 if a i  X   X ( s,g ) . Note that h  X  is exactly the myopic policy.
 Proof of Theorem 9. We first show that the problem is in NP. We build a tree representation of an optimal history-dependent policy for each initial state which acts as a polynomial-size certificate. Every node in the tree is represented by a pair ( s i ,G i ) , where s i is a state and G i is a set of goals for which the node is on a good path from the root node. We let h i be the helper action selected in node i . The children of a node in the tree represent possible successor nodes ( s j ,G j ) reached by the agent X  X  response to h i . Note that multiple children can result from the same action because the dynamics is a function of the agent X  X  goal.
 To verify that the optimal policy tree is of polynomial size we note that the number of leaf nodes is upper bounded by | G | X  max g N ( g ) , where N ( g ) is the number of leaf nodes generated by the goal g and G is the set of all goals. To estimate N ( g ) , we note that by our protocol, for any node ( s i ,G i ) where g  X  G i and the assistant X  X  action is h i , if a i  X   X ( s,g ) , it will have a single successor that contains g . Otherwise, there is a misprediction, which leads to at most k successors for g . Hence, the number of nodes reached for g grows geometrically with the number of mispredictions. Since there Hence the total number of all leaf nodes of the tree is bounded by | G | 1+log k , and the total number of nodes in the tree is bounded by m | G | 1+log k , where m is the number of steps to the horizon. Since this is polynomial in the problem parameters, the problem is in NP.
 To show NP-hardness, we reduce 3-SAT to the given problem. We consider each 3-literal clause C i of a propositional formula  X  as a possible goal. The rest of the proof is identical to that of Theorem 1 except that all variables are set by the assistant. The agent accepts every setting, except possibly the last one which he reverses if the clause evaluates to 0 . Since the assistant does not get any useful information until it makes the clause true or fails to do so, its optimal policy is to choose the assignment that maximizes the number of satisfied clauses so that the mistakes are minimized. The assistant makes a single prediction mistake on the last literal of each clause that is not satisfied by the assignment. Hence, the worst regret on any goal is 0 iff the 3-SAT problem is satisfiable. The authors gratefully acknowledge the support of NSF under grants IIS-0905678 and IIS-0964705.
