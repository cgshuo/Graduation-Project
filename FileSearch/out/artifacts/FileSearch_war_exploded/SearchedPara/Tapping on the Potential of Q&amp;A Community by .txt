 The rapidly increasing popularity of community-based Question Answering (cQA) services, e.g. Yahoo! Answers, Baidu Zhidao, etc. have attracted great atte ntion from both academia and indus-try. Besides the basic problems, like question searching and an-swer finding, it should be noted that the low participation rate of users in cQA service is the crucial problem which limits its de-velopment potential. In this pape r, we focus on addressing this problem by recommending answer providers, in which a question is given as a query and a ranked list of users is returned according to the likelihood of answering th e question. Based on the intuitive idea for recommendation, we try to introduce topic-level model to improve heuristic term-level me thods, which are treated as the baselines. The proposed approach consists of two steps: (1) dis-covering latent topics in the content of questions and answers as well as latent interests of users to build user profiles; (2) recom-mending question answerers for new arrival questions based on latent topics and term-level m odel. Specifically, we develop a general generative model for qu estions and answers in cQA, which is then altered to obtain a novel computationally tractable Bayesian network model. Experiments are carried out on a real-world data crawled from Yahoo! Answers during Jun 12 2007 to Aug 04 2007, which consists of 118510 questions, 772962 answers and 150324 users. The experimental results reveal signif-icant improvements over the baseline methods and validate the positive influence of topic-level information. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  search process ; H.3.5 [ Information Systems and Applications ]: On-line information Services  X  web-based services; G.3 [ Probability and Statistics ]. Algorithms, Experimentation, Human Factors Community-based Question Answering, Question Answerer Recommendation, Latent Topi c Modeling, Gibbs Sampling As Web 2.0 applications become more and more popular, an enormous human knowledge shari ng and exchanging activities are occurring explicitly or implicitly everyday online. Inspired by the idea that the wisdom of crowds is larger than few intelligent indi-viduals [4, 5], much work has been proposed to leverage crowds X  knowledge contained in many diffe rent types of social media, such as Wikipedia [34], query log [35, 36], social annotation [32], blogosphere [31] and community-based QA [33]. Research on these social media has been mainly focused on discovering hidden semantic information from structured content [34] or unstructured content [32, 35, 36], and identifying influential authorities in community [31, 33]. In this paper, we focus on the communi-ty-based QA service in large part motivated by its popularity and the diversity of knowledge covered by it. Community-based Question Answering (cQA) service is a partic-ular form of online service for le veraging user-generated content, which is gaining increasing audiences in recent years, such as Yahoo! Answers 1 , Baidu Zhidao 2 and Live QnA 3 . In cQA, users exchange and share their knowle dge explicitly by asking ques-tions or answering others X  questions in all service-predefined cat-egories. Additionally, users ca n give positive or negative judg-ment to answers provided by other users. Intuitively, the questions and answers in QA communities can be viewed as a large base which stores all users X  knowledge. Benefitting from the know-ledge diversity of cQA portals [21], people are supposed to find the answer of any question once all people fully participate into community and share their knowledge. However, the main prob-lem faced by cQA is the low participation rate of most users. It means that most answers or knowledge in the community comes from minority users. Therefore, the primary question is: can we tap on the potential of majority users in cQA? Actually, the low participation rate in cQA is caused by two main reasons. (1) Solving a new question is not an attractive job to most users, so they typically are not willing to spend their time on it. (2) Many new users, even experienced users, do not know about the arrival of new questions which may interest them and can be solved by them. In nowadays X  cQA services, many incentive me-chanisms have been adopted to stimulate the participation enthu-siasm of the users, such as awarding score to the question answer-ers. However, there is no signif icant improvement after taking such steps. As illustrated in Figure 1, we can see that the number http://answers.yahoo.com http://zhidao.baidu.com/ http://qna.live.com/ Figure 1. User participation in our crawled Yahoo! Answers corpus, where AnsQ denotes the number of questions ans-wered by a user. of users who answered more than 5 questions is only 20% of the total number of users. The discussion in [21] also concluded heavy tail phenomena in all the three subcategorie s (Programming, Cancer and Wrestling). It is worthy to note that all incentive mechanisms taken by cQA service providers are focused on tackle with the first cause of low participation rate, low-enthusia sm. Additionally, in today X  X  QA community, users have to browse the question category hierarchy to find interesting and solvable questions among millions of open questions to answer. It is r eally time-consuming and enthu-siasm-dispelling even with the help of organized question catego-ries. This kind of open question finding also keeps users from fully participating in QA community. In this paper, we focus on recommending possible and reasonable question answerers for new arrival questions in order to tackle with the second cause of low participation rate in cQA. To the best of our knowledge, it is the fi rst work on solving this problem in the context of cQA. Two intuitive ideas for handling this prob-lem is either to find similar solved questions for new arrival ques-tions first, and then recommend the answerers of these similar questions as the answerers for the new question, or construct user profiles first according to their hi story activities, and then suggest question answerers based on user-profiles. In the above two intui-tive ideas, the similarities betw een question-question and between question-user are calculated on the term-level. However, the con-tent quality variance in cQA caused by the informality of natural language and the data sparseness problem caused by low partici-pation brings trouble to term-level question answerer recommen-dation. Therefore, we try to combine topic-level information about questions and users with term-level question answerer recom-mendation in our approach. First, we seek to discover latent topics in the content of questions as well as the associated answers, and latent topic interests of users. We propose a generative model to simulate user behaviors in cQA, for both question asking and answering, and then simultaneously obtain topic analysis of ques-tions/answers and users. Second, we recommend answer providers for new questions according to discovered topics as well as term-level information of questions and users. In the experiment part, we first discuss the topic number selection followed by an analysis about the topics discover ed by the proposed model. And then we compare the performance of several methods for question answerer recommendation on real-world cQA data which is crawled from Yahoo! Answers. The experimental results demon-strate that the combination of topic-level and term-level model improves the performance of question answerer recommendation. The remainder of this paper is organized as follows: Section 2 introduces some prior work related to our approach. Section 3 discusses a genera l generative model for questions and answers in cQA followed by a simplified tractable model for user behavior. Experimental results are presented in Section 4. At last, we con-clude the paper and discuss about future work in Section 5. In this section, we review three lines of work which are closely related to our approach: resear ch on Question Answering, topic analysis using generative models, and expert search. There is a large body of work conducted on QA domain. The main purpose of these work is to avoid the lag time involved by waiting for a response from other users. Typically, question searching and answer finding for new questions are the two primary applica-tions. In question search, given a question as query, the task is to find similar solved questions in QA community. Jeon et al. in [8] pro-posed an approach to estimate th e question semantic similarity based on their answers. Additionally, Jeon et al. proposed a trans-late model to find similar questions in large question-answer arc-hives in [7]. Besides searchin g for similar questions, recommend-ing related questions in cQA, proposed by Y. Cao in [10] recently, is an alternative of similar question search. In answer finding, given a question as query, the task is to find the right answer to it in QA community. Initially, many research on answer retrieval have been done on FAQ data. In [11], Berger et al. used only statistic techniques to tackle with question answering. FAQ Finder [12] combines statis tics and semantic techniques in question answers finding. WordNet [1] is used as a semantic knowledge base to calculate the semantic similarity. Methods using question templates and artificial rules to answer questions automatically are proposed in [14] by Sneiders. Other extensive research on question answering has been done on TREC data [13, 15]. Additionally, [6] proposed a framework using non-textual features in QA community to predict the quality of answers. Recently, E. Agichtein et al. proposed method in [33] to distin-guish high-quality content, both questions and answers, from the rest. Different from questions and answers finding problems above, the focus of this paper is to recommend possible and rea-sonable answer providers to ne w questions. Although P. Jurczyk and E. Agichtein [37] exploited li nk analysis to identify authority users in QA community, it is quite different from the problem addressed in this paper too. Vector Space Model [2] is the mo st famous and successful model for text retrieval because of its simplicity. However, the  X  X ag-of-words X  assumption brings the problems of high dimen-sion document representation and non-semantic relationship be-tween words. Eliminating stop words in documents can be viewed as a simple attempt to reduce the dimension of document space. The first model dealing with spac e reduction is La tent Semantic Indexing (LSI) [22], which uses Singular Value Decomposition to represent document in a low-dimension space. Although LSI in-cludes  X  X emantic X  in its name, it still encounters the problem of lacking for semantic explanation. In order to leverage semantic between words in documents, pLSI [23] introduces latent topics to represent documents, which also reduces document representation space dimension, and model the data generation process as a Bayesian network. Further, David M. Blei et al. in [24] proposed Latent Dirichlet Allocation (LDA) to address the overfitting prob-lem faced by pLSI by introducing a Dirichlet prior over topics and words. Much work has been done based on the above models or their extensions. In [25], Wu et al. exploited pLSI for discovering latent semantic between social annotations in order to benefit Semantic Web. Zhou et al. in [28] extends LDA to model the generation of both Web document and associat ed annotations for improving information retrieval. M. Rosen-Zvi et al. [26] modeled author factor into document producing for document content characteri-zation as well as author interest representation. Besides, X. Wang and A. McCallum [30] combined time information with latent topic discovering for modeling topic variance over time. Like the previous approaches, we treat interests of users as a mul-tinomial distribution over latent topics, and each topic is a multi-nomial distribution over words too. Different from LDA [24] and the above extensions, we introduce the category information of questions and answers, which is predefined by cQA services, into the process of discovering latent t opics. It is proven to be positive information for question answer ing recommendation in our expe-riments. Different from question and answ er finding problem which uses existing community knowledge direct ly to resolve the new ques-tions, the task of question answer er recommendation is to predict the possible answerers for new que stions based on the activity history of users. Similarly, expert search is to find a ranked list of domain experts for the given query . Nowadays, the main approach for expert search is candidate-cen tered, in which user profiles are constructed by aggregating inform ation from documents related to corresponding users. When a query comes, the users are returned as a ranked list according to the similarity between the query and user-profiles. In [20], Mockus et al. proposed an expert finding system by using log data to facilita te software engineering. [17, 19] finding experts by mining experts and expertise from E-mail communications. [9, 18] both propos ed profile-based models for expert finding on general documents. In [16], Balog et al. com-pared two generative models for expert search resulted in empha-sizing document topicality. Although expert search is similar to our problem, there are some particular aspects that make question answerer recommendation different from expert search. Ob viously, experts could be reason-able answerers in categories like Mathematics and Programming. However, according to the discussion in [21], there are some cat-egories in cQA where questions are asking for neither expertise nor support, but rather opinion and conversation. In such catego-ries, expert users might not be suitable question answerers, at least not the only possible answerers. Moreover, the interests of users in cQA are not as focus as expertise. Additionally, the motivation of expert search is focus on finding experts who can solve the need of users. But the objective of question answerer recommen-dation is to invite many possible answerers to provide answer in order to improve the user particip ation in cQA. Furthermore, the query type in question answerer recommendation is different from expert search. In traditional expert search, query often consists of several keywords. However, in question answerer recommenda-tion, the query is a new question, which is a multi-field (question category, question title and question detail) natural language document. From this point of view, question answerer recom-mendation can be viewed as a general case of expert search. Let  X  X  X  X  X   X   X ,  X   X ,...,  X   X  and  X  X  X  X  X   X   X ,  X   X ,...,  X   X  denote all users and questions in the QA community respectively.  X  X  X  X  and  X  X  X  X   X  asked by user  X   X  . Every question  X   X  consists of three parts, question category , question title and question detail . Question category  X  X  X   X   X  is the category the ques tion belongs to, which is specified by question asker. Question title  X  X  X   X  description of the question. Op tionally, the question asker can provide a more concrete explanat ion about the question with more words in question detail  X  X  X   X   X  . Every question has a list of asso-ciated answers to it. Additionally,  X   X  asker and the  X  th answerer for question  X   X  respectively. Question Answerer Recommendation. For a new arrival ques-tion  X  , the question answerer recomme ndation task is to suggest a ranked list of users  X   X  where the higher ranked users are more possible to answer it. To tackle with the answerer recommendation problem, we need to resolve the following three sub-problems. z Question representation z User expertise and interest representation z Ranking o c endation candidates f re omm In the following sections, our approaches to handle the above three problems will be discussed in detail. We propose a probabilistic generative model for QA community. The model defines the generation process of questions and an-swers posted by users in all cate gories. The motivation of model-ing the data generation process in cQA is to simultaneously dis-cover the latent topics contained by terms, categories and users. After topic analysis, we will exploit the topic distributions of terms and categories to analyze th e content of new questions. The topic distributions of questions and users are the basis of topic-level answerer recommendation. In this section, we first introduce a general model in which answer content is influenced by question content. After that, we simplify it to obtain a tractable Bayesian network model, namely User-Q uestion-Answer (UQA) Model. Finally, we will discuss the pa rameter estimation method for the UQA model. Before introducing general generativ e model, we first give a brief review about the basic Latent Dirichlet Allocation (LDA) model. Our notations used in this paper are summarized in Table 1, and the graphic model representations of LDA model, general genera-tive model and UQA model are shown in Figure 2, Figure 3 and Figure 4 respectively. LDA models the generation of document content as two independent stoc hastic processes by introducing latent topic space. For an arbitrary word  X   X  X  X  in document  X  , (1) a topic  X   X  X  X  is first sampled from the multinomial distribution  X  which is generated from the Dirichlet prior parameterized by  X  , (2) and then the word  X   X  X  X  is generated from multinomial distri-bution  X   X  terized by  X  . The two Dirichlet priors for document-topic distri-butions  X   X  and topic-word distributions  X   X  reduce the probability of overfitting training documents and enhance the ability of inferring topic distribution for new documents. As a background introduction, we here describe our understanding about the question asking and answering process in cQA. For question askers, they first have a question to be asked in mind, and then choose a category for the question and at last post the question in that category. For qu estion answerers, they choose the question they would like to answer in one category and then post their answers according to the c ontent of question. Based on this understanding and inspired by the discussion in [21], the characte-ristics of question asking and answering are quite divergent in different categories. Therefore, we intend to introduce category information about questions and answers into the general genera-tive model. The plate notation of general ge nerative model is presented in Figure 3. Inspired by related work on topic analysis [24, 27, 28, 30], we make the following assu mptions about the probabilistic structure of general ge nerative model. First, each question or an-swer is modeled as a multinomial distribution over latent topics, and each topic is modeled as a multinomial distribution over words and a multinomial distributi on over categories. Second, the prior distributions for topics, words and categories follow differ-ent parameterized Dirichlet distri bution, which is conjugate prior for multinomial distribution. On the question asking side  X  , a topic  X  is first drawn from the multinomial distribution  X  and then a word is sampled from the multinomial distribution  X  and a category  X  is also sampled from the multinomial distribu-tion  X   X  for the word. Repeating this process  X  the content and category for one question. We obtain the whole question set by repeating the above process  X  times. On the question answering side (right-hand side), for each word  X  answer  X  provided by user  X  , a topic  X  is first drawn from mul-tinomial distribution  X   X  and conditioned on the content and cat-egory of the question, and then a word is sampled from the multi-nomial distribution  X   X  . Repeating this process  X  the content of the answer. We obtain the whole answer set by repeating the process  X  times. Since the topi c set for questions and answers can be very simila r but different, we model them differently in general model,  X   X  and  X   X  respectively. Addition-ally, we can acquire the topic analysis for users by combining the topics of questions they aske d and answers provided by them. From the discussion above, we find that the general generative model illustrated in Figure 3 is not quite applicable in practice. Because we have to estimate a lot of parameters: (1)  X  X  X  document-topic multinomia l distribution; (2)  X   X   X  X   X  multinomial distribution; (3)  X  X  X   X  conditional probabilities to express the correlation between wo rds of question and the topics of answer; (4)  X  X  X   X  conditional probabilities to represent the correlation between category of que stion and the topics of answer. Moreover, there are some additi onal parameters which are diffi-cult to tune in practice (  X   X   X ,  X   X ,  X   X ,  X   X ,  X  and  X  will simplify the general model to a computationally tractable structure in next section. In order to reduce the general ge nerative model to a computation-ally tractable one, we first make four assumptions. First, we as-sume the topic space of question content is as same as that of an-swer content. This assumption means that we can treat the content of questions and answers in the same way. In fact, the variance of content is noted as a crucial problem in cQA [33], which indicates that the topics of many low quality answers might be far away from the topic of corresponding questi on. However, this is out of the scope of this paper so that we consider this assumption rea-sonable in our problem setting. Second, we assume the users have the same prior distribution type over topics for asking and ans-wering. Third, the parameters fo r topics prior distribution are identical in asking an d answering for the same user. Although the users might have different parame ters for topic distribution in question asking and answering, or even different type of topic prior distribution, we here make this two assumptions for the sake of low computational complexity. Fourth, like other candi-date-centric approaches for expert search, we assume that the user interests can be captured by the questions they asked, and answers they provided. According to the above assumptions, we arrive at a simplified computable model, namely Use r-Question-Answer (UQA) Model. The plate notation of UQA is illustrated in Figure 4. From the graphic representation of UQA, we can see that the main charac-teristic of UQA is that it is user-centric instead of ques-tion/answer-centric in general gene rative model. Each user is con-sidered as a pseudo-document which is a combination of all ques-tions he/she asked and answers he /she answered. After this trans-formation, the number of multinomial distribution over topics in UQA reduces to  X  , which is much smaller than the number in general model,  X  X  X  . And the topic prior distribution follows a symmetric Dirichlet parameterized by  X  . In addition, the topic space of question asking and answering are merged into a single one, and the word distribution  X  is chosen from a symmetric Dirichlet (  X  ) prior. It is interesting to note the difference between our user-centric UQA model and the autho r-topic model in [2 6]. In author-topic model, the authorship of an arbitrary word in the multi-author document is not known so that author-topic model assumes a uni-form contribution of all document authors. However, in our prob-lem setting, the author of every question and answer in cQA is explicitly presented. Actually, the authorship information of word is important to precisely identify user interests and expertise. Therefore, we model the word authorship information into UQA model. As shown in the above process, the parameters of UQA model can be summarized as follows. We could estimate  X , X  and  X  from data directly, however, for the simplicity, we use fixed Dirichlet prior (  X  X  50  X   X   X   X   X 0.05 and  X , 50  X   X  ) in our experiments like many previous work [26, 28, 30]. Now, the UQA model leaves three sets of parameters for us to estimate, namely us-er-topic distributions  X   X  , topic-word distribution  X  ic-category distribution  X   X  . But, similar to LDA model, infe rence can not be done exactly in UQA model. Expectation-Maximi zation (EM) algorithm is a possible choice for estimating the parameters of models with la-tent variables. However, EM suffers from the possibility of run-ning into local maxima and the high computational burden. Therefore, we employ an alternative approach, Gibbs sampling [29, 38], which is gaining popularity in recent work on latent topic analysis [26, 27, 28, 30]. In Gibbs sampling, we evaluate th e posterior distributions instead of estimating parameters directly. Note that we use the Dirichlet distribution which is the conjugat e prior of multinomial distribu-tion, thus we can integrate out  X  ,  X  and  X  easily and capture the uncertainty associated with them. We only need to estimate which indicates the pr obability of topic  X   X  X  X   X   X  X  X  ,  X   X  X  X  and word, category and topic assignments to all other words in user-profile  X  , where  X   X  X  X  X   X ,  X  X  X  X   X  and  X  X  X  X  word, category and topic assignments to all words except the word  X   X  X  X  w eva ate e joint probab then use the chain rule to get the desired where  X   X  X  is the number of times of word  X  are assigned to the are assigned to topic  X  . The concrete derivation for equation (1) is presented in Appendix A. In equation (1), we have provided the conditional probability for asymmetric Dirichlet priors. However, we use symmetric Dirichlet priors in our experiments for simplic-are used for  X   X  and  X   X  in our experiments too. Algorithm 1 demonstrates the Gibbs sampling algorithm for UQA model training, where  X   X  X  X   X  X  X  represents the current topic assignment of the  X  th word in user-profile  X  . We crawled 118510 resolved questions posted between Jun 12 2007 and Aug 04 2007 from Yahoo! Answers. In this question set  X  , there are 150324 users, denoted by  X  , involved in both asking and answering and 772962 answers. Additionally, questions in this dataset cover 20 different to p categories of Yahoo! Answers. We divide the whole question set and user set  X  X , X  X  into six subsets according to the user participation degree and question category respectively. Additionally, we separate each subset into each subset, there are about 910  X  of all questions in training set, which posted before Aug 03 2007, a nd the rest are in testing set. For all training sets and testing sets, we remove the stop words and perform stemming for all word s before further experiments. The description and detail inform ation of each subset are pre-sented below and in Table 2. Each dataset contains a question set  X  X  and an user set  X  X  . Every question in  X  X  contains three parts, respectively the title, the detail and the category. Additionally, every question has a list of associated answers to it. The user set  X  X  contains all the answer providers for every associated answer and question askers. Table 2. Details of 6 datasets. In each cell the total number is followed by the numbers in testing set and training set respec-tively in parentheses separated by a slash. USER-20 : Two subsets  X   X  X   X  X  and  X   X  X   X  X  , in which every user in  X   X  X  have asked and answered not less than 20 times in  X   X  X  . The asker and at least one answerer of each question in  X  are in  X   X  X  . The associated answers to each question in  X  the ones provided by users in  X   X  X  . USER-15 and USER-10 : Similar to subset USER-20 , except the number of questions asked and answ ered by users in these subsets is not less than 15 and 10 respectively. PETS :  X   X  X  X  X  X  contains all questions in the top category Pets . The associated answers to each question in  X   X  X  X  X  X  are the same as the associated answers to it in  X  . ELEC and FINA : Similar to subset PETS , except the questions in these subsets are in the top category Consumer Electronics and Business &amp; Finance respectively. In the top three subsets, they contain questions from all 20 top categories. We call these three su bsets user-partitioned sets. Addi-tionally, we call the other threes subsets category-partitioned sets. In term-level search process, we treat the questions as three-field documents, question titl e, question detail a nd question category. We regard user-profiles as eight-field documents too, respectively the title, detail and category of the questions asked answered by them, the answers provided by them and the associated answers to the questions asked by them. In this section, we will concentr ate on how to select proper topic numbers to obtain generative mode l with acceptable performance on testing sets and enough itera tion number in Algorithm 1 to prevent overfitting on training sets. In order to achieve this objec-tive, we use the standard measure for probability model, perplexi-ty, to estimate the performance of our model. We calculate the perplexity value for each subset on a hold-out testing set, which is a seque  X  X   X  , a s nce of tuples  X ,  X   X ,  X   X  X  X   X  X  X  X  X  s follow Here the probability  X  X  X   X   X ,  X   X |  X   X  is calculated according to the parameters trained from training set by a straightforward calcula-tion as: We test perplexity value for each subset on different settings of topic number and iteration number s. The results are shown in Figure 5 and Figure 6. Note that the lower perplexity value indi-cates better generalization ability on the hold-out testing set. In the first set of experiments, we demonstrate the influence of iteration number of Gibbs sampling on the model generalization ability. We fix the topic number as 100 and change the iteration number from 1 to 1000 in experiment. From Figure 5, we can see the perplexity values decreases dramatically for the first 10 itera-tions on 6 datasets. Additionally, while the iteration number keeps increasing, the perplexity starts to grow after 50 iterations, which indicates an overfitting on the training set. It is worthwhile men-tioning that the absolute perplexity value of user-partitioned sets is larger than that of category-partitioned sets due to the larger con-tent diversity in us er-partitioned sets. In Figure 6, the perplexity values for different settings of topic number are plotted. For each to pic number, we stop model train-ing when the perplexity decrement between two consecutive itera-tions is less than 1. The perplexity decreases when the number of topics starts to increase. However, after a certain point, the per-plexity values begin to increase on all 6 subsets. For example, the perplexity value reaches its minimum for USER-20 at the point of 40 topics. And for PETS, the variance of perplexity on the change of topic number is very small, wh ich demonstrate that the topic in PETS is quite concentrated. Based on the above experiments, we train the UQA model for 50 topics and set the stop condition in Algorithm 1 as the perplexity decrement between two consecutive iterations is less than 1. In this section, we illustrate some top words and categories for the topics discovered by UQA model in the subset USER-10 and ELEC to judge the quality. Some top words and categories for sample topics are illustrated in Table 3. The example topics dem-onstrate two kinds of phenomena, general topic cross different categories and sub-topics within same category. USER-10 : From the sample topics shown in Table 3, we can see that the same topic may appear in different categories. Like Topic 20, it concentrates on taking pho tos and appears in three different categories, which have strong sema ntic correlation, namely DIY, Camera and Photography. ELEC : Different from the topics di scovered in user-partitioned sets, the sample topics demonstr ate sub-topics in the same cate-gory. For example, topic 12 and topic 17 are both focused in cat-egory Consumer Electroni cs&gt;Home Theater , but they focus on two different aspects, vide o and audio respectively. In this section, we propose our approach for question answerer recommendation which incorporat es topics discovered by UQA model with term-level matching me thods. As described in Section 3, every question in cQA consists of three distinct parts, question title, question detail and question category. Based on this T able 3. Top words and categories for some discovered topic observation, we treat the questio ns as multi-field documents and choose BM25F [3] as the foundation of our solution to question answerer recommendation problem since BM25F is the classical and formal model for multi-field doc uments retrieval. Similarly, we treat user-profiles of the users for BM25F as multi-field doc-uments too, in which there are 8 fields, respectively the title/detail/category of questions answered by them, answers pro-vided by them and associated an swers to the questions asked by them. Next, we will compare the performance of following me-thods on question answerer recommendation task: z Question-based search on content (QST-BM25) : We find z User-profile-based search on content (USER-BM25) : In In the above two approaches, we heuristically train the field weighting in BM25 by 2-fold cross-validation on training sets of 6 datasets by trying each field wei ght from 1 to 10 at step length 1. z Question-based search on topic (QST-TOPIC) : Questions Then we match a question  X  to new question  X  with probability by assuming the questions and topi cs have the same prior proba-bility. After that, for each similar question  X  , we assign its probability equally to its answerers. Finally, we recommend answerers ac-cording to the probability that they will answer the new question. z User-profile-based search on topic (USER-TOPIC ): Us-z QST-TOPIC + USER-TOPIC (QST-USER-TOPIC) : we z QST-BM25 + QST-USER-TOPIC (QST-BM25-TOPIC) : Evaluation : Instead of labeling recommended answerers manual-ly, we evaluate the above 6 appro aches by treating the actual ans-werers for questions in testing da ta as the ground truth, which are the users who provide answers in real-world data. We calculate the prediction precision for the top 1, 2, 5, 10 and  X  suggested answerers, where  X  is the real number of answers for test ques-tions. The evaluation results are shown in Table 4. The experi-mental results show that the combination of topic-level and term-level matching benefits the performance of question answer-er recommendation. According to the t-test between QST-BM25 and QST-BM25-TOPIC , we can see that the improvement is greater on category-partitioned sets than it is on user-partitioned sets. Star marks in Table 4 indicate statistically significant differences in performance with a 95% confidence. The relatively small im-provement on user-partitioned data may be caused by the rela-tively more topics in user-partitioned sets since they contain more categories. And the relieved data sparseness problem in us-er-partitioned sets makes the term-level methods effective enough. Additionally, the best performance is achieved when  X  QST-BM25-TOPIC . It shows that the topic and user weighting should not be very large. Figure 7 illustrates the influence of  X  and  X   X  on the performance of QST-USER-TOPIC and QST-BM25-TOPIC in USER-20 dataset. When tuning parameter  X  , we fix  X   X   X 0.5 . 
Figure 7. The influence of different settings of  X   X  and  X 
P@5 in USER-20 for QST-USER-TOPIC and QST-BM25-TOPIC Note that in real cQA data, many questions only have few answer-ers , like only one or two answerers. For these questions, the preci-sion is very low even the methods suggest correct question ans-werers. For instance, if a questi on only has one answer (which is very common in cQA data), the P@10 is only 0.1 even the top 10 suggested users are all reasonabl e and contain the correct one. Actually, the average answer number of questions in the whole set is about 6.7. Therefore, we choose P@1, P@2 and P@5 as the main metric to evaluate the performance of question answerer recommendation. Another important thing is that the data sparse-ness in training dataset also leads to the low prediction precision, which means the categories or word s a user used in testing sets may never appear in training sets. In this paper, we observe the low participation problem in cQA caused by inefficiency of finding questions which interests and is solvable by users. We try to relieve the burden of searching ques-tions for users by recommending possible question answerers to new questions, which may raise the overall participation rate in QA community. In order to suggest reasonable answerers to new questions, we first try to discover latent topics in cQA, which can simultaneously discover topic distribution for words, categories and users in QA community. A nove l generative model, namely UQA model, is proposed for the generation of content in cQA. The main contributions of this paper are: (1) The proposal and formulation of question answerer recom-mendation problem, which might tap on the user participation in cQA effectively. (2) The proposal of a computable probabilistic generative mod-el for user behavior. Based on which, we try several ways to rec-ommend possible answerers for questions. (3) The study of several methods X  performance on question answerer recommendation. The extensive experimental results on the real world cQA data show that combining topic-level informa-tion with term-level similarity significantly improves the perfor-mance of term-level only methods. For future work, we may discover some other applications or ex-tensions for UQA model. In cQA, some answers may be spam that they never help to solve the question. Thus, detecting low quality answers in cQA is very important. We could measure the topic variances between the topic of questions and the corresponding answers by using the topics di scovered by UQA model to identify the quality of answers. UQA is a ve ry general model in which no service-specific feature is included. Additionally, inspired by [30], we could take the temporal dime nsion of questions and answers into consideration. After this ex tension for UQA, we may find the interest change of users over time, and then we can suggest ans-werers for questions by estimating the current interests of users, which is more reasonable. Besi des ranking combination between topic-level and term-level mode l, we may apply Language Model as the term-level model and combine them with probability in future experiments. The authors would like to thank IBM China Research Lab for its continuous support to and cooperat ion with Shanghai Jiao Tong University. We would also like to express our gratitude to the data preparation work by Bohai Yang. Be sides, we also appreciate the valuable suggestions from Yuanjie Liu, Dingyi Han and Jason Liu. In the end, we would like to thank the anonymous reviewers for their elaborate and helpful comments We first derivate the joint probability  X   X   X , X , X  |  X , X , X   X  for the whole user set. By leveraging that Dirichlet distribution is the conjugate prior of multinomial dist ribution, we can simply inte-grate out the parameters  X  ,  X  and  X  . All the symbols have the Using the chain rule, we can obtain the conditional probability for topic  X   X  X  X  conditioned on all other word, category and topic as-
