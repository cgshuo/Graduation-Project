 Traditionally, statistical machine translation (SMT) models have assumed that the word should be the ba-sic token-unit of translation, thus ignoring any word-internal morphological structure. This assumption can be traced back to the first word-based models of IBM (Brown et al., 1993), which were initially pro-posed for two languages with limited morphology: French and English. While several significantly im-proved models have been developed since then, in-cluding phrase-based (Koehn et al., 2003), hierarchi-cal (Chiang, 2005), treelet (Quirk et al., 2005), and syntactic (Galley et al., 2004) models, they all pre-served the assumption that words should be atomic.
Ignoring morphology was fine as long as the main research interest remained focused on languages with limited (e.g., English, French, Spanish) or min-imal (e.g., Chinese) morphology. Since the attention shifted to languages like Arabic, however, the im-portance of morphology became obvious and sev-eral approaches to handle it have been proposed. Depending on the particular language of interest, researchers have paid attention to word inflections and clitics , e.g., for Arabic, Finnish, and Turkish, or to noun compounds , e.g., for German. However, derivational morphology has not been specifically targeted so far.

In this paper, we propose a paraphrase-based ap-proach to translating from a morphologically com-plex language. Unlike previous research, we focus on the pairwise relationship between morphologi-cally related wordforms, which we treat as poten-tial paraphrases , and which we handle using para-phrasing techniques at various levels: word, phrase, and sentence level. An important advantage of this framework is that it can cope with various kinds of morphological wordforms, including derivational ones. We demonstrate its potential on Malay, whose morphology is mostly derivational.

The remainder of the paper is organized as fol-lows: Section 2 gives an overview of Malay mor-phology, Section 3 introduces our paraphrase-based approach to translating from morphologically com-plex languages, Section 4 describes our dataset and our experimental setup, Section 5 presents and anal-yses the results, and Section 6 compares our work to previous research. Finally, Section 7 concludes the paper and suggests directions for future work. Malay is an Astronesian language, spoken by about 180 million people. It is official in Malaysia, In-donesia, Singapore, and Brunei, and has two major dialects, sometimes regarded as separate languages, which are mutually intelligible, but occasionally dif-fer in orthography/pronunciation and vocabulary: Bahasa Malaysia ( lit.  X  X anguage of Malaysia X ) and Bahasa Indonesia ( lit.  X  X anguage of Indonesia X ).
Malay is an agglutinative language with very rich morphology. Unlike other agglutinative languages such as Finnish, Hungarian, and Turkish, which are rich in both inflectional and derivational forms, Malay morphology is mostly derivational. Inflec-no grammatical gender, number, or tense, verbs are not marked for person, etc.

In Malay, new words can be formed by the fol-lowing three morphological processes:  X  Affixation , i.e., attaching affixes, which are not  X  Compounding , i.e., forming a new word by  X  Reduplication , i.e., word repetition. In
Malay has very little inflectional morphology, It also has some clitics 2 , which are not very frequent and are typically spelled concatenated to the preced-ing word. For example, the politeness marker lah can be added to the command duduk / X  X it down X  to yield duduk lah / X  X lease, sit down X , and the pronoun nya can attach to kereta to form kereta nya / X  X is car X . Note that clitics are not affixes, and clitic attachment is not a word derivation or a word inflection process.
Taken together, affixation, compounding, redu-plication, and clitic attachment yield a rich vari-ety of wordforms, which cause data sparseness is-sues. Moreover, the predominantly derivational na-ture of Malay morphology limits the applicabil-ity of standard techniques such as (1) removing some/all of the source-language inflections, (2) seg-menting affixes from the root, and (3) clustering words with the same target translation. For example, if pel ajar / X  X tudent X  is an unknown word and lemma-tization/stemming reduces it to ajar / X  X each X , would this enable a good translation? Similarly, would seg-menting 3 pel ajar as peN+ ajar , i.e., as  X  X erson do-ing the action X  +  X  X each X , make it possible to gener-ate  X  X tudent X  (e.g., as opposed to  X  X eacher X )? Finally, if affixes tend to change semantics so much, how likely are we to find morphologically related word-forms that share the same translation? Still, there are many good reasons to believe that morphologi-cal processing should help SMT for Malay.

Consider affixation , which can yield words with similar semantics that can use each other X  X  trans-lation options, e.g., di ajar / X  X e taught (intransitive) X  and di ajarkan / X  X e taught (transitive) X . However, this cannot be predicted from the affix, e.g., compare minum / X  X rink (verb) X   X  minum an / X  X rink (noun) X  and makan / X  X at X   X  makan an / X  X ood X .
Looking at compounding , it is often the case that the semantics of a compound is a specialization of the semantics of its head, and thus the target lan-guage translations available for the head could be us-able to translate the whole compound, e.g., compare kerjasama / X  X ollaboration X  and kerja / X  X ork X . Alter-natively, it might be useful to consider a segmented version of the compound, e.g., kerja sama .

Reduplication , among other functions, expresses plural, e.g., pelajar-pelajar / X  X tudents X . Note, how-ever, that it is not used when a quantity or a num-ber word is present, e.g., dua pelajar / X  X wo students X  and banyak pelajar / X  X any students X . Thus, if we do not know how to translate pelajar-pelajar , it would be reasonable to consider the translation options for pelajar since it could potentially contain among its translation options the plural  X  X tudents X .

Finally, consider clitics . In some cases, a clitic could express a fine-grained distinction such as po-liteness, which might not be expressible in the target language; thus, it might be feasible to simply remove it. In other cases, e.g., when it is a pronoun, it might be better to segment it out as a separate word. We propose a paraphrase-based approach to Malay morphology, where we use paraphrases at three dif-ferent levels: word, phrase, and sentence level. First, we transform each development/testing Malay sentence into a word lattice , where we add simplified word-level paraphrasing alternatives for each morphologically complex word. In the lattice, each alternative w  X  of an original word w is assigned the weight of Pr( w  X  | w ) , which is estimated using pivoting over the English side of the training bi-text. Then, we generate sentence-level paraphrases of the training Malay sentences, in which exactly one morphologically complex word is substituted by a simpler alternative. Finally, we extract additional Malay phrases from these sentences, which we use to augment the phrase table with additional transla-tion options to match the alternative wordforms in the lattice. We assign each such additional phrase p  X  a probability max p Pr( p  X  | p ) , where p is a Malay phrase that is found in the original training Malay text. The probability is calculated using phrase-level pivoting over the English side of the training bi-text. 3.1 Morphological Analysis Given a Malay word, we build a list of morpholog-ically simpler words that could be derived from it; we also generate alternative word segmentations: (a) words obtainable by affix stripping (b) words that are part of a compound word (c) words appearing on either side of a dash (d) words without clitics (e) clitic-segmented word sequences (f) dash-segmented wordforms (g) combinations of the above.

The list is built by reversing the basic morpho-logical processes in Malay: (a) addresses affixation, (b) handles compounding, (c) takes care of redu-plication, and (d) and (e) deal with clitics. Strictly speaking, (f) does not necessarily model a morpho-logical process: it proposes an alternative tokeniza-tion, but this could make morphological sense too.
Note that (g) could cause potential problems when interacting with (f), e.g., adik-beradik would be-come adik -beradik and then by (a) it would turn into adik -adik , which could cause the SMT sys-tem to generate two separate translations for the two instances of adik . To prevent this, we forbid the application of (f) to reduplications. Taking into ac-count that reduplications can be partial, we only al-strings to the left and to the right of the dash, re-spectively, LCS ( x, y ) is the longest common char-acter subsequence, not necessarily consecutive, of the strings x and y , and | x | is the length of the string x . For example, LCS ( adik , beradik )= adik , and thus, the ratio is 1 (  X  0 . 5 ) for adik-beradik . Similarly, LCS ( gunung , ganang )= gnng , and thus, the ratio is 4/6=0.67 (  X  0 . 5 ) for gunung-ganang . However, for aceh-nias , it is 1/4=0.25, and thus (f) is applicable.
As an illustration, here are the wordforms we generate for adik-beradiknya / X  X is siblings X : adik , adik-beradiknya , adik-beradik nya , adik-beradik , beradiknya , beradik nya , adik nya , and beradik . And for berpelajaran / X  X s educated X , we build the list: berpelajaran , pelajaran , pelajar , ajaran , and ajar . Note that the lists do include the original word.
To generate the above wordforms, we used two morphological analyzers: a freely available Malay lemmatizer (Baldwin and Awab, 2006), and an in-house re-implementation of the Indonesian stemmer described in (Adriani et al., 2007). Note that these tools X  objective is to return a single lemma/stem, e.g., they would return adik for adik-beradiknya , and ajar for berpelajaran . However, it was straightfor-ward to modify them to also output the above in-termediary wordforms, which the tools were gener-ating internally anyway when looking for the final lemma/stem. Finally, since the two modified ana-lyzers had different strengths and weaknesses, we combined their outputs to increase recall. 3.2 Word-Level Paraphrasing We perform word-level paraphrasing of the Malay sides of the development and the testing bi-texts.
First, for each Malay word, we generate the above-described list of morphologically simpler words and alternative word segmentations; we think of the words in this list as word-level paraphrases . Then, for each development/testing Malay sentence, we generate a lattice encoding all possible para-phrasing options for each individual word.

We further specify a weight for each arc. We as-sign 1 to the original Malay word w , and Pr( w  X  | w ) to each paraphrase w  X  of w , where Pr( w  X  | w ) is the probability that w  X  is a good paraphrase of w . Note that multi-word paraphrases, e.g., resulting from clitic segmentation, are encoded using a sequence of arcs; in such cases, we assign Pr( w  X  | w ) to the first arc, and 1 to each subsequent arc.

We calculate the probability Pr( w  X  | w ) using the training Malay-English bi-text, which we align at the word level using IBM model 4 (Brown et al., 1993), and we observe which English words w and w  X  are aligned to. More precisely, we use pivoting to estimate the probability Pr( w  X  | w ) as follows:
Then, following (Callison-Burch et al., 2006; Wu and Wang, 2007), we make the simplifying assump-e , thus obtaining the following expression:
We estimate the probability Pr( e from the word-aligned training bi-text as follows: where #( x, e ) is the number of times the Malay word x is aligned to the English word e .

Estimating Pr( w  X  | e the training bi-text, e.g., because it is a multi-token sequence generated by clitic segmentation. Thus, we of all Malay words in the training bi-text that are re-dure. So, we estimate Pr( w  X  | e where f orms ( x ) is the set of the word-level para-
Since the training bi-text occurrences of the words that are reducible to w  X  are distinct, we can rewrite the above as follows:
Finally, the probability Pr( v | e using maximum likelihood: 3.3 Sentence-Level Paraphrasing In order for the word-level paraphrases to work, there should be phrases in the phrase table that could potentially match them. For some of the words, e.g., the lemmata, there could already be such phrases, but for other transformations, e.g., clitic segmenta-tion, this is unlikely. Thus, we need to augment the phrase table with additional translation options.
One approach would be to modify the phrase ta-ble directly, e.g., by adding additional entries, where one or more Malay words are replaced by their para-phrases. This would be problematic since the phrase translation probabilities associated with these new entries would be hard to estimate. For example, the clitics, and even many of the intermediate morpho-logical forms, would not exist as individual words in the training bi-text, which means that there would be no word alignments or lexical probabilities available for them.

Another option would be to generate separate word alignments for the original training bi-text and for a version of it where the source (Malay) side has been paraphrased. Then, the two bi-texts and their word alignments would be concatenated and used to build a phrase table (Dyer, 2007; Dyer et al., 2008; Dyer, 2009). This would solve the prob-lems with the word alignments and the phrase pair probabilities estimations in a principled manner, but it would require choosing for each word only one of the paraphrases available to it, while we would pre-fer to have a way to allow all options. Moreover, the paraphrased and the original versions of the corpus would be given equal weights, which might not be desirable. Finally, since the two versions of the bi-text would be word-aligned separately, there would be no interaction between them, which might lead to missed opportunities for improved alignments in both parts of the bi-text (Nakov and Ng, 2009).
We avoid the above issues by adopting a sentence-level paraphrasing approach. Following the gen-eral framework proposed in (Nakov, 2008), we first create multiple paraphrased versions of the source-side sentences of the training bi-text. Then, each paraphrased source sentence is paired with its orig-inal translation. This augmented bi-text is word-aligned and a phrase table T  X  is built from it, which is merged with a phrase table T for the original bi-text. The merged table contains all phrase entries from T , and the entries for the phrase pairs from T  X  that are not in T . Following Nakov and Ng (2009), we add up to three additional indicator features (tak-ing the values 0.5 and 1) to each entry in the merged phrase table, showing whether the entry came from (1) T only, (2) T  X  only, or (3) both T and T  X  . We also try using the first one or two features only. We set all feature weights using minimum error rate train-ing (Och, 2003), and we optimize their number (one,
Each of our paraphrased sentences differs from its original sentence by a single word, which prevents combinatorial explosions: on average, we generate 14 paraphrased versions per input sentence. It fur-ther ensures that the paraphrased parts of the sen-tences will not dominate the word alignments or the phrase pairs, and that there would be sufficient inter-action at word alignment time between the original sentences and their paraphrased versions. 3.4 Phrase-Level Paraphrasing While our sentence-level paraphrasing informs the decoder about the origin of each phrase pair (orig-inal or paraphrased bi-text), it provides no indica-tion about how good the phrase pairs from the para-phrased bi-text are likely to be.

Following Callison-Burch et al. (2006), we fur-ther augment the phrase table with one additional feature whose value is 1 for the phrase pairs com-ing from the original bi-text, and max the phrase pairs extracted from the paraphrased bi-text. Here p is a Malay phrase from T , and p  X  is a Malay phrase from T  X  that does not exist in T but is obtainable from p by substituting one or more words in p with their derivationally related forms generated by morphological analysis. The probability Pr( p  X  | p ) is calculated using phrase-level pivoting through En-glish in the original phrase table T as follows (unlike word-level pivoting, here e
We estimate the probabilities Pr( e Pr( p  X  | e i ) as we did for word-level pivoting, except that this time we use the list of the phrase pairs ex-tracted from the original training bi-text, while be-fore we used IBM model 4 word alignments. When calculating Pr( p  X  | e possible Malay phrases q in T that are reducible to p tain. This can be rewritten as follows: where par ( q ) is the set of all possible phrase-level paraphrases for the Malay phrase q .

The probability Pr( q | e mum likelihood from the list of phrase pairs. There is no combinatorial explosion here, since the phrases are short and contain very few paraphrasable words. 4.1 Data We created our Malay-English training and develop-ment datasets from data that we downloaded from the Web and then sentence-aligned using various heuristics. Thus, we ended up with 350,003 training sentence pairs, including 10.4M English and 9.7M Malay word tokens. We further downloaded 49.8M word tokens of monolingual English text, which we used for language modeling .
 For testing , we used 1,420 sentences with 28.8K Malay word tokens, which were translated by three human translators, yielding translations of 32.8K, 32.4K, and 32.9K English word tokens, respectively. For development , we used 2,000 sentence pairs of 63.4K English and 58.5K Malay word tokens. 4.2 General Experimental Setup First, we tokenized and lowercased all datasets: training, development, and testing. We then built directed word-level alignments for the training bi-text for English  X  Malay and for Malay  X  English using IBM model 4 (Brown et al., 1993), which we symmetrized using the intersect+grow heuristic (Och and Ney, 2003). Next, we extracted phrase-level translation pairs of maximum length seven, which we scored and used to build a phrase table where each phrase pair is associated with the fol-lowing five standard feature functions: forward and reverse phrase translation probabilities, forward and reverse lexicalized phrase translation probabilities, and phrase penalty.

We trained a log-linear model using the following standard SMT feature functions: trigram language model probability, word penalty, distance-based dis-tortion cost, and the five feature functions from the phrase table. We set all weights on the development dataset by optimizing BLEU (Papineni et al., 2002) using minimum error rate training (Och, 2003), and we plugged them in a beam search decoder (Koehn et al., 2007) to translate the Malay test sentences to English. Finally, we detokenized the output, and we evaluated it against the three reference translations. 4.3 Systems Using the above general experimental setup, we im-plemented the following baseline systems:  X  baseline . This is the default system, which uses  X  lemmatize all . This is the second baseline that  X   X  X oisier X  channel model . 6 This is the model of
Our full morphological paraphrasing system is lattice + sent-par + word-par + phrase-par . We also experimented with some of its components turned off. lattice + sent-par + word-par excludes the additional feature from phrase-level paraphras-ing. lattice + sent-par has all the morphologically simpler derived forms in the lattice during decod-ing, but their weights are uniformly set to 0 rather than obtained using pivoting from word alignments. Finally, in order to compare closely to the  X  X oisier X  channel model, we further limited the morpholog-ical variants of lattice + sent-par in the lattice to lemmata only in lattice + sent-par (orig+lemma) . The experimental results are shown in Table 1.
First, we can see that lemmatize all has a consis-tently disastrous effect on BLEU, which shows that Malay morphology does indeed contain information that is important when translating to English.
Second, Dyer (2007) X  X   X  X oisier X  channel model helps for small datasets only. It performs worse than lattice + sent-par (orig+lemma) , from which it dif-fers in the phrase table only; this confirms the im-portance of our sentence-level paraphrasing.
Moving down to lattice + sent-par , we can see that using multiple morphological wordforms in-stead of just lemmata has a consistently positive im-pact on BLEU for datasets of all sizes.

Adding weights obtained using word-level piv-oting in lattice + sent-par + word-par helps a bit more, and also using phrase-level paraphrasing weights yields even bigger further improvements for lattice + sent-par + word-par + phrase-par .
Overall, our morphological paraphrases yield sta-tistically significant improvements ( p &lt; 0 . 01 ) in BLEU, according to Collins et al. (2005) X  X  sign test, for bi-texts as large as 320,000 sentence pairs.
A closer look at BLEU. Table 2 shows detailed n -gram BLEU precision scores for n =1,2,3,4. Our system outperforms the baseline on all precision scores and for all numbers of training sentences.
Other evaluation measures. Table 3 reports the results for five evaluation measures: BLEU and NIST 11b, TER 0.7.25 (Snover et al., 2006), METEOR 1.0 (Lavie and Denkowski, 2009), and TESLA (Liu et al., 2010). Our system consistently outperforms the baseline for all measures.

Example translations. Table 4 shows two trans-lation examples. In the first example, the redupli-cation bekalan-bekalan ( X  X upplies X ) is an unknown word, and was left untranslated by the baseline sys-tem. It was not a problem for our system though, which first paraphrased it as bekalan and then trans-lated it as supply . Even though this is still wrong (we need the plural supplies ), it is arguably preferable to passing the word untranslated; it also allowed for a better translation of the surrounding context.
In the second example, the baseline system trans-lated menjalani kehidupan (lit.  X  X o through life X ) as undergo training , because of a bad phrase pair, which was extracted from wrong word alignments. Note that the words menjalani ( X  X o through X ) and kehidupan ( X  X ife/existence X ) are derivational forms of jalan ( X  X o X ) and hidup ( X  X ife/living X ), respectively. Thus, in the paraphrasing system, they were in-volved in sentence-level paraphrasing, where the alignments were improved. While the wrong phrase pair was still available, the system chose a better one from the paraphrased training bi-text. Most research in SMT for a morphologically rich source language has focused on inflected forms of the same word. The assumption is that they would have similar semantics and thus could have the same translation. Researchers have used stemming (Yang and Kirchhoff, 2006), lemmatization (Al-Onaizan et al., 1999; Goldwater and McClosky, 2005; Dyer, 2007), or direct clustering (Talbot and Osborne, 2006) to identify such groups of words and use them as equivalence classes or as possible alternatives in translation. Frameworks for the simultaneous use of different word-level representations have been pro-posed as well (Koehn and Hoang, 2007).

A second important line of research has focused on word segmentation , which is useful for languages like German, which are rich in compound words that are spelled concatenated (Koehn and Knight, 2003; Yang and Kirchhoff, 2006), or like Arabic, Turk-ish, Finnish, and, to a lesser extent, Spanish and Italian, where clitics often attach to the preceding word (Habash and Sadat, 2006). For languages with more or less regular inflectional morphology like Arabic or Turkish, another good idea is to segment words into morpheme sequences , e.g., prefix(es)-stem-suffix(es), which can be used instead of the original words (Lee, 2004) or in addition to them. This can be achieved using a lattice input to the translation system (Dyer et al., 2008; Dyer, 2009).
Unfortunately, none of these general lines of re-search suits Malay well, whose compounds are rarely concatenated, clitics are not so frequent, and morphology is mostly derivational, and thus likely to generate words whose semantics substantially dif-fers from the semantics of the original word. There-fore, we cannot expect the existence of equivalence classes: it is only occasionally that two derivation-ally related wordforms would share the same tar-get language translation. Thus, instead of look-ing for equivalence classes, we have focused on the pairwise relationship between derivationally related wordforms, which we treat as potential paraphrases .
Our approach is an extension of the  X  X oisier X  channel model of Dyer (2007). He starts by generat-ing separate word alignments for the original train-ing bi-text and for a version of it where the source side has been lemmatized. Then, the two bi-texts and their word alignments are concatenated and used to build a phrase table. Finally, the source sides of the development and the test datasets are converted into confusion networks where additional arcs are added for word lemmata. The arc weights are set to 1 for the original wordforms and to 0 for the lem-mata. In contrast, we provide multiple paraphras-ing alternatives for each morphologically complex word, including derivational forms that occupy in-termediary positions between the original wordform and its lemma. Note that some of those paraphrasing alternatives are multi-word , and thus we use a lattice instead of a confusion network. Moreover, we give different weights to the different alternatives rather then assigning them all 0.

Second, our work is related to that of Dyer et al. (2008), who use a lattice to add a single alter-native clitic-segmented version of the original word for Arabic. However, we provide multiple alterna-tives. We also include derivational forms in addi-tion to clitic-segmented ones, and we give different weights to the different alternatives (instead of 0).
Third, our work is also related to that of Dyer (2009), who uses a lattice to add multiple alterna-tive segmented versions of the original word for Ger-man, Hungarian, and Turkish. However, we focus on derivational morphology rather than on clitics and inflections, add derivational forms in addition to clitic-segmented ones, and use cross-lingual word pivoting to estimate paraphrase probabilities. Finally, our work is related to that of Callison-Burch et al. (2006), who use cross-lingual pivot-ing to generate phrase-level paraphrases with corre-sponding probabilities. However, our paraphrases are derived through morphological analysis ; thus, we do not need corpora in additional languages. We have presented a novel approach to trans-lating from a morphologically complex language, which uses paraphrases and paraphrasing tech-niques at three different levels of translation: word-level, phrase-level, and sentence-level. Our experi-ments translating from Malay, whose morphology is mostly derivational, into English have shown signif-icant improvements over rivaling approaches based on several automatic evaluation measures.

In future work, we want to improve the proba-bility estimations for our paraphrasing models. We also want to experiment with other morphologically complex languages and other SMT models.
 This work was supported by research grant POD0713875. We would like to thank the anony-mous reviewers for their detailed and constructive comments, which have helped us improve the paper.
