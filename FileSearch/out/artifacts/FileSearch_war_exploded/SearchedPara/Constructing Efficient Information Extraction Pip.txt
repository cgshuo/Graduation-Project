 Information Extraction (IE) pipelines analyze text through several stages. The pipeline X  X  algorithms determine both its effectiveness and its run-time efficiency. In real-world tasks, however, IE pipelines often fail acceptable run-times because they analyze too much task-irrelevant text. This raises two interesting questions: 1) How much  X  X fficiency potential X  de-pends on the scheduling of a pipeline X  X  algorithms? 2) Is it possible to devise a reliable method to construct efficient IE pipelines? Both questions are addressed in this paper. In particular, we show how to optimize the run-time efficiency of IE pipelines under a given set of algorithms. We evaluate pipelines for three algorithm sets on an industrially relevant task: the extraction of market forecasts from news articles. Using a system-independent measure, we demonstrate that efficiency gains of up to one order of magnitude are possible without compromising a pipeline X  X  original effectiveness. Categories and Subject Descriptors : I.2.7 [AI]: Nat-ural Language Processing X  Text analysis H.3.4 [Informa-tion Storage and Retrieval]: Systems and Software X  Performance evaluation (efficiency and effectiveness) General Terms : Algorithms, Performance Keywords : information extraction, run-time efficiency
Information Extraction (IE) will improve today X  X  possi-bilities of knowledge and information acquisition. Yet, the number of industrial applications is still limited. Typical IE tasks require several analysis stages ranging from prepro-cessing over the extraction of entities, attributes, relations, and events to reference resolution and normalization.  X  X on-ventional X  pipelines for these tasks execute all analyses on the whole input and tend to run into efficiency problems, i.e., they fail to meet some required response time. We use the term efficiency here only to address such run-time concerns. Accordingly, we use effectiveness to describe the quality of an algorithm, such as its accuracy. Existing approaches to increase efficiency focus on faster algorithms for single anal-yses within a pipeline. However, more effective algorithm sets normally lead to a decrease in efficiency as illustrated in Figure 1a. In contrast, we aim to increase efficiency without Figure 1: (a) Typical growth curve of the run-time of an losing effectiveness by optimizing the pipeline itself: instead of changing the pipeline X  X  algorithms we introduce filtering stages and rearrange the pipeline X  X  schedule (Figure 1b).
Within an IE task, different analyses A 1 , ..., A k take place, e.g. tokenization or person entity recognition, whereas each analysis A i can be operationalized by alternative IE algo-rithms. We call an ordered subset  X  =( A 1 , ..., A k )ofanal-yses an analysis schedule and denote the employed set of algorithms as A . The tuple  X  = A , X  constitutes an In-formation Extraction pipeline . Each algorithm A  X  A needs information of certain input types in order to work properly, and A provides information of certain output types. Conse-quently, only a subset of the k ! possible analysis schedules fulfill the input constraints for all algorithms in A . While some algorithms will fail completely if their requirements are not met, others may degrade significantly in their effec-tiveness. We hence consider only pipelines whose schedules comply with all input constraints imposed by the employed algorithms. We call such pipelines admissible . If admissibil-ity is maintained, the exchange of a pipeline X  X  schedule does not impair the pipeline X  X  effectiveness.
 Contributions In this paper, we present a generic method to construct efficient IE pipelines based on four paradigms: 1) a maximum decomposition of the given IE task into single analyses, 2) early filtering of relevant information, 3) lazy evaluation of the applied algorithms, and 4) an optimized scheduling of the resulting filtering stages. We have evalu-ated our method on a subtask of the market forecast extrac-tion task [13]. To make the efficiency of pipelines comparable beyond system bounds, we use a new measure of efficiency that relies on an internal comparison of the algorithms in a pipeline. The results indicate that our method can speed up conventional IE pipelines up to an order of magnitude, while maintaining effectiveness. Related Work The limited industrial usage of IE has recently been discussed in [14]. The authors identify the lack of reusability as one major problem and introduce a component-based IE approach. We also exploit a decompo-sition into components, but we address another major prob-lem of IE: efficiency. Run-time efficiency has always been a main aspect of algorithm research, but we observe that most rewarded research in IE and in Natural Language Processing (NLP) focuses on effectiveness as do the leading evaluation tracks, namely, MUC, ACE, and the CoNLL Shared Task. While effectiveness is the key to making information pro-cessable, we argue that the success of IE systems in the wild is decided by their response times.
 Efficiency is by far not an unknown topic in IE and NLP. For instance, greedy decoding has been shown to be compet-itive with exact inference in entity recognition [10] and ma-chine translation [7]. Other research uses hashing or heuris-tic search for feature extraction [6] and parsing [4, 8], while Petrov [9] studies efficiency under an iterative refinement of the complexity of NLP models. Like in these examples most research concentrates on single analyses . In contrast, we aim to develop efficient analysis pipelines . Pipelines that oper-ate on large sources first need to efficiently filter candidate texts, which has been done with fast text categorization [12] or querying techniques trained on text with the relations of interest [1]. The KnowItNow system builds specialized index structures to speed up the IE process [5], and a sched-ule designed with efficiency in mind is given in [2]. In both cases, the authors focus on retrieval aspects. Instead, we investigate the core IE algorithms for text analysis.
For the purpose of this paper, we first introduce two def-initions in order to quantify the efficiency of IE algorithms and pipelines. On this basis, we then present the essential steps to optimize the efficiency of IE pipelines.

Definition 1. Let A be an IE algorithm and D atextcol-lection with n sentences. If A needs time T D ( A )toprocess all sentences of D on a system  X , then the average sentence response time of A on  X  is t ( A )= T D ( A ) /n . Definition 1 can be adopted to t ( X ) for an IE pipeline  X . However, response times are not meaningful by themselves: usually, run-time complexities are defined in a relative man-ner, by measuring the run-time in unit cost steps that are accepted over system bounds, programming environments, and the like. In this regard we employ t ( A 0 )ofatokeniza-tion algorithm A 0 on the given system  X  as unit cost step. Thereby, we normalize t ( A ) with respect to IE tasks and cancel out many system-specific side effects.

Definition 2. Let  X  be an IE task,  X  a pipeline to tackle  X  ,
E the effectiveness of  X  on  X  ,and A 0 a tokenization al-gorithm. Then, the averaged efficiency of  X  to tackle  X  at effectiveness E is F @ E = t ( X ) /t ( A 0 ) .

F @ E allows to estimate t ( X ) system-independently. In general, t ( X ) follows from the response times of the al-gorithms and the number of processed sentences. While t ( A ) is inherent to an algorithm A , the number of pro-cessed sentences depends on a pipeline X  X  schedule. To con-struct a preferably efficient pipeline, we now introduce four paradigms that yield a tailored scheduling of its algorithms, and that can be seen as operational steps of a method. Paradigm 1: Maximum Decomposition The deeper atask  X  is decomposed into single analyses, the better a pipeline for  X  can be optimized. Some analyses have a predefined order, though. Also, some algorithms may be bundled in off-the-shelf tools, which can only be used in a black-box manner. In preprocessing, decomposition means to separate the analyses for sentence splitting, tokenization, the decoding of sequence information (e.g. part-of-speech-tagging), and the creation of syntactic tree structures (e.g. dependency parsing). Similarly, decomposing the recogni-tion of different entity, attribute, and relation types supports optimization. Some of these are required , whereas others are optional . Finally, IE pipelines extract information on spe-cific events, which relate to statements in the text, i.e., text windows of w&gt; 0 sentences. Such a statement is only rele-vant if it comprises an anchor for an event of interest. Paradigm 2: Early Filtering It is reasonable to focus only on information that probably satisfies a given infor-mation need. Thus, filters are inserted after each core IE analyses. In particular, statements without instances of a required entity, attribute, or relation type are discarded as well as statements that are classified not to refer to an event of interest. Also, statements are only kept if the required entities and attributes can be normalized. We call the com-bination of preprocessing, IE analysis, and filter a filtering stage . While filtering can drastically reduce the amount of data to be processed, we thereby miss statements where en-tities and attributes are spread across the text. This tradeoff can be influenced by adjusting the statement size w . Paradigm 3: Lazy Evaluation Based on the filtering stages, a simple but effective paradigm can be applied: each analysis is delayed until its results is needed, and it is ex-ecuted only on possibly relevant text windows. Of course, no analysis is performed more than once. The rationale is that, the more filtering takes place before the execution of an algorithm A , the less data A will process.
 Paradigm 4: Optimized Scheduling Finally, the filter-ing stages are arranged in the most efficient way. Assume that we have implementations of two stages F 1 and F 2 with t ( F 1 )and t ( F 2 ). If F 1 is applied to n sentences, it filters a number m ( F 1 )ofthe n sentences on average, while F 2 filters m ( F 2 ) sentences. In order to minimize the response time of both stages in sequence, F 1 is then applied before F 2 iff t ( F 1 )+ m ( F 1 ) /n  X  t ( F 2 ) &lt;t ( F 2 )+ m ( F 2
While the filter ratios ,suchas m ( F 1 ) /n , can be estimated based on a training set, they may change after each applied filter. For simplicity, we approximate the optimized schedule in Section 3 by only pairwise computing the most efficient schedule of two filters using their initial filter ratios. The Task We evaluated the following subtask  X  of the mar-ket forecast extraction task [13]: Extract all related time and money entities that refer to a revenue forecast. An example for such a relation is:  X  X n 2009, market analysts expected touch screen revenues to reach [$9B] money [by 2015] While  X  is quite new and non-standard, the implications of the evaluation can be transferred to other tasks as well. The Data We used the Revenue Corpus introduced in [13] and processed its training set to develop and train al-gorithms, to measure their response times, and to estimate the initial filter ratios of the filtering stages. 1 On the test set, we evaluated the efficiency and the effectiveness of all constructed pipelines. In this set, all 347 of the 6,038 sen-tences, in which both a time and a money entity relate to revenue, are manually annotated as statements on revenue, among them 104 forecasts. Hence, we set the statement size w (cf. Section 2) of all evaluated algorithms to 1. Analyses and Algorithms Following paradigm 1, we de-composed  X  into the following nine analyses (cf. Figure 2a): se Split the input text into sentences. tk Tokenize the sentences. lp Lemmatize the tokens and tag their part-of-speech. dp Parse dependencies of the tokens in the sentences. tr Recognize all time entities in the sentences. mr Recognize all money entities in the sentences. re Extract relations between time and money entities. si Identify statements on revenue. fc Classify whether a sentence is a forecast or not.
As we only needed lemmas and tags together, we did not decompose lp . Table 1 lists the algorithms that we evaluated for the nine analyses. We used the TreeTagger 2 [11] and the mate-tools 3 [3, 4] for lemmatization, tagging, and parsing. Where no reference is given, we measured effectiveness on the test set. Since we had more than one algorithm for lp , dp , re ,and si , we investigated three algorithm sets:
Differences are marked in bold. Because of a very simple relation extractor, A 1 needs no dependency parsing at all. Filtering Stages For paradigm 2, we inserted a filter after each IE analysis (cf. Figure 2b). The results of dp are used for re ,whereas dp itself requires lemmas and tags from lp as input. So, for lazy evaluation, we delayed lp and dp to execute them right before re (cf. Figure 2c).
 Schedules and Pipelines Thelaststepistoderivean optimized schedule, which we sketch for A 2 . As follows from the dependencies in Table 1 the filtering stages F t =( tr, tf ) and F m =( mr, mf )havetoprecede F r =( lp,dp,re,rf ) and F s =( si, sf ) for admissibility, and F t also has to pre-cede F f =( fc,ff ). We pairwise computed the most efficient schedule of two stages using inequation 1. As a result, we moved F f before F r and inserted lp before fc .Also,wepost-poned F r after F s as illustrated in Figure 2d. Altogether, we finished with the optimized schedule  X   X  2 for A 2 :
Revenue Corpus, http://infexba.upb.de
TreeTagger, http://code.google.com/p/tt4j mate-tools, http://code.google.com/p/mate-tools Table 1: The evaluated algorithms A for analysis A , the analyses each A depends on, its average sen-tence response time t ( A ) in milliseconds, and its ef-fectiveness E ( A ) if applied in isolation (accuracy Acc , labeled attachment score LAS ,precision P ,recall R ). Table 2: Precision (P), recall (R), and F 1 -score (F 1 ) of all admissible pipelines  X  i,j = A i , X  j .  X   X  2 =( se,tk,tr,tf,lp,fc,ff,mr,mf,si,sf,dp,re,rf ) For A 1 and A 3 , we accordingly obtained  X   X  1 and  X   X  3  X   X  1 =( se,tk,si,sf,tr,tf,mr,mf,lp,fc,ff,re,rf )  X   X  3 =( se,tk,tr,tf,mr,mf,si,sf,lp,fc,ff,dp,re,rf )
As baselines, we used all pipelines with one of the sched-ules  X  a ,  X  b ,and  X  c from Figure 2a X  X . In fact,  X  c resem-bles the schedule from [2]. Except for  X   X  1 , we implemented a pipeline  X  i,j in Java for each A i and  X  j .  X   X  1 applies si before tr and mr . Thus, a pipeline with  X   X  1 is only admissible for A 1 , which contains si 1 for si . We ran all pipelines five times on the test set using a 2 GHz Intel Core 2 Duo MacBook with 4GBRAM and averaged over the response times.
 Effectiveness Results The effectiveness of each pipeline is given in Table 2. Precision and recall increase from A 1 to A significantly. Only in case of A 2 , the optimization impaired the effectiveness: the F 1 -scores of both  X  2 , 2 = A 2 , X   X  2 , 3 = A 2 , X   X  3 are one point lower than for the baselines. However, this is noise from the TreeTagger, which operates Table 3: The average sentence response time t ( X  i,j ) with standard deviation  X  i,j of each admissible pipeline  X  i,j = A i , X  j measured in milliseconds. on token-level. For such algorithms, filtering may influence the feature values of tokens near sentence boundaries. Efficiency Results Table 3 lists t ( X  i,j )ofeach X  i,j .For A 1 , our method led to a decrease from 3.23 ms to 2.44 ms. This improvement seems small, but note that se 1 and tk 1 already need over 1 ms (cf. Table 1). In case of A 2 ,the response time was reduced to less than 1/10 of t ( X  2 ,a )and to less than 1/3 of t ( X  2 ,c ). For A 3 , the pipeline  X  t ( X  3 , 3 )=10 . 19 ms is over 4 times as fast as the most efficient baseline  X  3 ,c .Moreover, X  3 , 3 clearly outperforms  X  3 , 2 offers evidence for the benefit of paradigm 4, while especially the scheduling of A 1 profits most from paradigm 1 and 2. Accordingly, looking at the standard deviations, we see that  X  b is significantly faster than  X  a only for A 1 .
 Efficiency at Effectiveness Using the F 1 -score as effec-tiveness E , we plotted F @ E for the three algorithm sets and the six schedules in Figure 3. The interpolated curves of the schedules have the shape introduced in Section 1, and we observe only a moderate slope for the optimized sched-ules. Also, F @ E cancelled out effects of high memory load from applying complex algorithms, such as dp 2 ,tomanysen-tences. For instance, this yielded F ( X  3 ,a ) / F ( X  3 , 3 as opposed to t ( X  3 ,a ) /t ( X  3 , 3 ) = 16.5.
 Discussion The three algorithm sets mainly differ in the uniformity of the algorithms X  response times (cf. Table 1), which gives rise to the potential of lazy evaluation and op-timized scheduling. In general, the room for improvement depends on the density and distribution of task-relevant in-formation in input texts. The more dense relevant informa-tion occurs, the less improvement through filtering can be expected, and, the more spread event-related information is across the text, the larger the statement size w must be in order to achieve high recall. Still, our method is generic in that it allows to integrate arbitrary text analysis algorithms and in that it can, in principle, be applied to any IE task.
Existing approaches to run-time efficiency in IE mostly rely on the development of fast algorithms . In contrast, we presented a method to increase the efficiency of IE pipelines . Using a system-independent measure, we showed for a com-plex IE task and three algorithm sets that a pipeline can be significantly sped up without impairing its effectiveness. The improvement is especially high if the response times of the employed algorithms differ largely. Also, the impact de-pends on the density and the distribution of task-relevant information. These observations X  X long with the four op-timization paradigms of our method X  X re a step towards a theory of efficiency in IE, which we plan to extend and un-derpin within forthcoming research. Figure 3: Averaged efficiency at effectiveness F @ E of all admissible pipelines  X  i,j = A i , X  j at the three effectiveness levels represented by A 1 ,A 2 ,andA 3 . [1] E. Agichtein and L. Gravano. Querying Text Databases for
Efficient Information Extraction. In ICDE , pp. 113 X 124, 2003. [2] E. Agichtein. Scaling Information Extraction to Large Document Collections. Bulletin of IEEE-CS Technical
Committee on Data Engineering , 28:3 X 10, 2005. [3] A. Bj  X  orkelund, B. Bohnet, L. Hafdell, and P. Nugues. A High-Performance Syntactic and Semantic Dependency
Parser. In COLING: Demonstrations , pp. 33 X 36, 2010. [4] B. Bohnet. Very High Accuracy and Fast Dependency
Parsing is not a Contradiction. In COLING , pp. 89 X 97, 2010. [5] M.J. Cafarella, D. Downey, S. Soderland, and O. Etzioni.
KnowItNow: Fast, Scalable Information Extraction from the Web. In HLT and EMNLP , pp. 563 X 570, 2005. [6] G. Forman and E. Kirshenbaum. Extremely Fast Text Feature Extraction for Classification and Indexing. In
CIKM , pp. 1221 X 1230, 2008. [7] U. Germann, M. Jahr, K. Knight, D. Marcu, and Y. Yamada. Fast Decoding and Optimal Decoding for Machine
Translation. In ACL , pp. 228-235, 2001. [8] A. Pauls and D. Klein. k-best A  X  Parsing. In ACL and
IJCNLP , pp. 958 X 966, 2009. [9] S. Petrov. Coarse-to-Fine Natural Language Processing.
PhD Thesis, University of California at Berkeley, 2009. [10] L. Ratinov and D. Roth. Design Challenges and
Misconceptions in Named Entity Recognition. In CoNLL , pp. 147 X 155, 2009. [11] H. Schmid. 1995. Improvements in Part-of-Speech Tagging with an Application to German. In ACL
SIGDAT-Workshop , pp. 47 X 50. [12] B. Stein, S. Meyer zu Eissen, G. Gr  X  afe, and F. Wissbrock. Automating Market Forecast Summarization from Internet
Data. In WWW/Internet , pp. 395 X 402, 2005. [13] H. Wachsmuth, P. Prettenhofer, and B. Stein. Efficient Statement Identification for Automatic Market Forecasting.
In COLING , pp. 1128 X 1136, 2010. [14] D.C. Wimalasuriya and D. Dou. Components for Information Extraction: Ontology-Based Information
Extractors and Generic Platform. In CIKM , pp. 9 X 18, 2010.
