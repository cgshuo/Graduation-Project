 Max Vladymyrov mvladymyrov@ucmerced.edu Miguel  X  A. Carreira-Perpi  X n  X an mcarreira-perpinan@ucmerced.edu Many machine learning algorithms rely on the choice of meta-parameters that govern their per-formance. These parameters depend on the data and good values are often hard to find. One such meta-parameter is the bandwidth  X  that is used in the construction of affinities in many machine learning problems. These include dimensionality reduction methods such as LLE ( Roweis &amp; Saul , 2000 ), Laplacian eigenmaps ( Belkin &amp; Niyogi , 2003 ), ISOMAP ( de Silva &amp; Tenenbaum , 2003 ), SNE ( Hinton &amp; Roweis , 2003 ), and the elastic embedding ( Carreira-Perpi  X n  X an , 2010 ); clustering methods such as spectral clustering ( Ng et al. , 2002 ) and mean-shift algorithms ( Carreira-Perpi  X n  X an , 2006 ); semi-supervised learning ( Zhou et al. , 2004 ; Belkin et al. , 2006 ); and many others. In some of those algorithms  X  is the only parameter to tune and a user has to try several values until the desired quality of the algorithm is achieved. When the dataset is large, such a process is not interactive and can lead to frustration and ultimately to the user refusing to use a potentially good algorithm. On top of that, the best results of the algorithm may not be achieved for a single value of  X  for all the points, but rather for a separate bandwidth for every datapoint, in which case the existence of automatic procedure is vital.
 In their spectral clustering algorithm, Ng et al. ( 2002 ) suggest to set  X  to the value giving least distorted clus-ters, but this requires running the algorithm, which is expensive. In a supervised setting, Er et al. ( 2002 ) se-lect the bandwidth per cluster of data as the one that captures the variation between points in each clus-ter, but minimizes the overlapping of nearest neigh-bors in different classes. The method requires tuning some parameters that depend on the mean and vari-ance of the clusters. Bchir &amp; Frigui ( 2010 ) estimate one  X  per cluster in an unsupervised manner using a fuzzy logic framework. The objective function of this method maximizes the scaling parameter per cluster up until the clusters start to overlap. There also ex-ist classic rules of thumb, such as setting  X  separately for each point to the distance d k to the k th nearest neighbor of that point, where k is a user parameter (set to 7 in Zelnik-Manor &amp; Perona , 2005 ). This has the odd behavior that  X  would change proportionally to changes in d k , but would ignore any changes to the rest of the distances, no matter how large, as long as d k remained the k th distance; or else it would change discontinuously.
 In this paper, we study a previously proposed way to set per-point bandwidths that takes into account the whole distribution of distances and is a continuous, differentiable function of them. For a given point x  X  K R kernel density estimator of width  X  defined on a finite set of points x 1 , . . . , x N  X  R D . Thus we have a discrete distribution p ( x ;  X  ) with probabilities for n = 1 , . . . , N p ( x ;  X  ) = where d n = k x  X  x n k . We focus on the case where K ( k ( x  X  x n ) / X  k 2 ) is the Gaussian kernel. We set  X  in-dividually for point x to a value such that the entropy of the distribution p ( x ;  X  ), considered as a function of  X  for fixed d 1 , . . . , d N , equals log K , where K is a user-set perplexity parameter. The perplexity, widely used in natural language processing ( Manning &amp; Sch  X utze , 1999 ), has an intuitive interpretation. A perplexity of K in a distribution p over N neighbors means p pro-vides the same surprise as if we were to choose among K equiprobable neighbors. Having set  X  in this way, the resulting value p n ( x ;  X  ) can be used as an affinity between x and x n . We call them entropic affinities . These affinities were introduced by Hinton &amp; Roweis ( 2003 ) as a better way to define the local scaling of the Gaussian distribution in their stochastic neighbor em-bedding (SNE) method. Their definition of p ( x ;  X  ) was particularized to x being one of the data points, but our generalization simplifies things later. The affinity p nm between points x n and x m is then p n ( x m ;  X  ). If we consider an affinity matrix W with entries K ( x n , x m ) and degree matrix D = diag ( P N n =1 w nm ), then p de-fines the random-walk matrix P = D  X  1 W , where each row is our distribution p ( x n ;  X  ). Thus, the entropic affinities seek a matrix P (  X  1 , . . . ,  X  N ) as a function of the kernel widths for each data point so that each row of P has perplexity K . To compute each  X  n , Hinton &amp; Roweis ( 2003 ) performed a search to find a bracket for the solution, initialized at [0 , 1], and then used bisections. This becomes noticeably slow with large datasets.
 Fig. 1 illustrates how the entropic affinities indeed im-prove over using a single  X  or simple rule-of-thumb adaptive  X  n (the distance to the 7th nearest neigh-bor; Zelnik-Manor &amp; Perona , 2005 ). We applied a nonlinear dimensionality reduction algorithm, the elas-tic embedding ( Carreira-Perpi  X n  X an , 2010 ), to the COIL dataset (rotation sequences of 10 physical objects ev-ery 5 degrees, each a grayscale image of 128  X  128 pixels, total N = 720 points in 16 384 dimensions; Nene et al. , 1996 ). The left plot clearly shows the sep-aration between the manifolds as well as the sequential structure of each manifold. The embedding resulting from a single  X  or  X  n from the 7th neighbor does not show such a structure. The bottom plots show the  X  values in the latter two cases result in a wide range of perplexity values.
 This paper improves our understanding of entropic affinities and their numerical computation. Section 1 proves useful properties, in particular that the function  X  ( x ) is well defined and continuously differentiable, and give simple bounds for it. Based on this, section 2 describes fast, scalable algorithms that compute  X  and the entropic affinities themselves in very few iterations to almost machine precision, by processing points in a certain order. Section 3 shows experimental results with image and text datasets. The entropy of the distribution ( 1 ) is defined as
H ( x ,  X  ) =  X  P N n =1 p n ( x ,  X  ) log( p n ( x ,  X  )) In particular, for the Gaussian kernel it becomes H ( x ,  X  ) =  X  where we will work with the precision parameter  X  = 1 / 2  X  2 . We can express ( 3 ) and its derivatives wrt  X  using the partition function Z (  X  ) = P N n =1 exp(  X  d 2 and moments m k (  X  ) = P N n =1 p n d 2 k n as follows:
H ( x ,  X  ) =  X m 1 + log Z H  X   X  ( x ,  X  ) =  X   X  ( m 2  X  m
H  X  X   X  ( x ,  X  ) =  X  ( m 3  X  3 m 2 m 1 + 2 m 3 1 ) + m 2 1 m k can be expressed as a function of Z and its deriva-tives using the following recursive definition: We now consider the problem of searching for  X  (or  X  ), which is implicit given the perplexity K : This is a 1D root-finding problem or an inversion prob-lem if H ( x ,  X  ) is invertible over  X  .
 The first derivative of F ( x ,  X , K ) is equal to the one from H ( x ,  X  ) and is always negative for  X  &gt; 0 given that the neighboring points are not equidistant from x . This means that ( 3 ) is a monotonically decreasing function of  X  that decreases from log N for  X  = 0 to 0 for  X   X   X  . Thus, the problem ( 6 ) is well defined for any value of  X  &gt; 0 and has a unique root  X  ( x ) for any K  X  (0 , N ) in the same interval.
 Next, notice that H ( x ,  X  ) is continuously differentiable in an open neighborhood of ( x 0 ,  X  0 ) for some fixed  X  0 &gt; 0 and x 0  X  R D and that H  X   X  ( x ,  X  0 ) 6 = 0. Thus, we can apply the implicit function theorem to show the existence of a uniquely defined local continuously differentiable function  X  ( x ) that satisfies  X  ( x 0 ) =  X  Moreover, since H ( x ,  X  ) is invertible, the function  X  ( x ) is also a global function defined for all x . The same argument can be applied to F ( x ,  X , K ), leading to the existence of a continuously differentiable global func-tion  X  ( x , K ) defined for all K  X  (0 , log N ) and x  X  R D . Finally, we give bounds [  X  L ,  X  U ] for  X  ( x , K ), i.e., sat-K , that are easy to compute and reasonably tight. As-sume w.l.o.g. the squared distances are sorted increas-ingly: d 2 1 &lt; d 2 2 &lt; &lt; d 2 N . Define  X  2 N = d  X  2 = d results hold taking  X  2 2 as the first nonzero d 2 n  X  d 2 Theorem 1.1. The following give lower and upper bounds for the root of ( 6 ) : where p 1 is the unique solution in the interval [3 / 4 , 1] of the equation: 2(1  X  p 1 ) log The proof is given in the supplementary material. Sol-ving ( 9 ) can be done with Newton X  X  method and needs be done only once for all the points in the dataset, since p 1 depends only on K and N . The computation of the bounds is thus O (1) for each data point, since they only need d 1 , d 2 and d N . Tighter bounds can be obtained by using all distances d 1 , . . . , d N , but at a cost O ( N ), which defeats the purpose.
 Rescaling the data (or the distances) rescales  X  as well, i.e., H  X  1 ( K ;  X d ) =  X H ( K ; d ) for any  X  &gt; 0. This suggests rescaling the data should rescale the bounds correspondingly, which indeed happens for our bounds. Our results carry over, suitably modified, to some vari-ations of our problem. The formulation ( 1 ) implies p nn 6 = 0. It is also possible to set self-affinities p nn (as is sometimes done) by defining p over the distances d 2 to d N instead. One can also use sparse affinities if defining p ( x ;  X  ) on the k nearest neighbors of x rather than all N points. This means setting N = k with points sorted in increasing distance to x . To compute the entropic affinities we need to solve the root-finding problem ( 6 ) as efficiently as possi-ble for every point in the dataset. There exist many one-dimensional root-finding algorithms with differ-ent convergence orders. Some of the most popu-lar derivative-free methods are the bisection method, Brent X  X  method ( 1973 ) and Ridders X  method ( 1979 ). These methods have universal convergence guaran-tees and take as an input an interval bracketing the root, which they iteratively shrink. Derivative-based methods such as Newton X  X , Halley X  X  and Euler X  X  meth-ods ( Traub , 1982 ) construct a sequence of iterates in-stead. The next iterate is found based on the value of the function and its derivatives at the current iterate. These methods usually do not have global convergence guarantees unless the function has some very specific form ( Melman , 1997 ). However, their convergence or-der is usually higher than that from derivative-free methods using the same amount of information. No-tice that although the cost of ( 6 ) as well as its deriva-tives scales as O ( N ), the computation of the function takes about three times as long. For F we need to com-pute Z (exponentiation and summation over N terms) and m 1 (summation over N ), but the derivatives can be computed sequentially and require calculation of just one of the m k per derivative. Thus, it is benefi-cial to have as little function evaluations as possible. We will use derivative-based root-finding methods with a simple modification so they achieve global con-vergence (from any starting point). We initialize the algorithm with an interval bracketing the root (ob-tained from the bounds ( 1.1 )) and an initial point. The algorithm consists of two nested loops: an outer loop with the bisection method, which is slow but guar-antees global convergence, and an inner loop with a derivative-based method, which is fast but only locally convergent. For each iteration of the inner loop, the algorithm evaluates the function, updates the bounds based on a new function value, computes the necessary Algorithm 1 Root-finding framework
Input: initial  X  , perplexity K , distances d 2 1 , . . . , d compute bounds B using Theorem 1.1 . while true do end while derivatives and applies a derivative-based method. If the output of the method falls outside of the current brackets or the number of iterations exceeds a certain constant maxit , the inner loop terminates and the next point is computed using the outer bisection loop. Thus the sequence of iterates contains a subsequence of bi-section steps (every maxit steps at most), which neces-sarily converges. Practically, we use a rather big value of maxit = 50, since our good initialization (see be-low) makes it very infrequent for the derivative-based method step to fail. Algorithm 1 shows the framework. Finally, to avoid dealing with negative values of  X  and to make the function more well-behaved, we find roots over log  X  =  X  2 log  X  rather than  X  or  X  . This modifies the expressions for the derivatives of the function ( 6 ) slightly. The final formulae are available in the sup-plementary material. 2.1. Choice of the root-finding algorithm It appears that locally, close to the root, it is not essen-tial which exact derivative-based method is used, but how many derivatives are used. Gander ( 1985 ) shows that many of the third-order methods can be described jointly using simple framework. He proves that the order convergence if H (0) = 1, H  X  (0) = 1 / 2. Halley X  X  method is recovered with H ( t ) = (1  X  1 2 t )  X  1 , Euler X  X  method with H ( t ) = 2(1 + gives Newton X  X  method. Traub ( 1982 ) showed that, for any p &gt; 1, given p  X  1 derivatives of the function there exists no method with convergence order higher than p . Thus, if we use only one derivative, we cannot do better than second order convergence and locally, close to the root, the behavior of Newton X  X  method is optimal. Similarly, for two derivatives, both Euler X  X  and Halley X  X  methods are optimal for third-order con-vergence methods. The differences between methods arise mostly when the iterations are far from the root. Among the many root-finding methods that we tried, we focus here mostly on the following three: New-ton X  X , Euler X  X  and Halley X  X  method. Newton X  X  method is a second-order method that approximates the func-tion with a line (i.e., up to a first derivative) and the next iteration is found by the intersection of the tan-gent of the current point with the x -axis. Euler X  X  and Halley X  X  methods are third-order methods that approx-imate the function with a parabola and a hyperbola re-spectively ( Scavo &amp; Thoo , 1995 ). Those curves agree with the current iterate up to the first two derivatives. Fig. 2 shows a typical case of search for log  X  for a given log K . We initialized the three algorithms in different places inside the interval (  X  4 , 3) and computed how many and what kind of iterations they need to find the root to an accuracy tol = 10  X  10 . Notice that close to the root, Halley X  X  and Euler X  method behave almost identically to each other, while for Newton X  X  method the region where the number of iterations equal to 1 is a lot smaller. This is caused by a higher convergence order of the former methods compared to the latter. However, in the region far from the root, the methods behave quite differently. The initial steps of Newton X  X  and Euler X  X  methods are too big and send the next iterate out of the bounds, causing our algorithm to use a bisection. The region where bisection iterations occur is smaller for Euler X  X  method compared to that of Newton X  X  method because the parabolic approxima-tion leads to smaller steps than the linear one. Halley X  X  method never has to use bisection steps because the hyperbolic approximation is too conservative and uses steps that are too small to get out of the flat region. 2.2. Bounds and initialization For the bounds, our goal is to find a region around the root that is as tight as possible and is efficient to compute from the distances. Theorem 1.1 can be applied for this. It guarantees to contain the root and it takes a constant amount of time to compute. Fig. 3 shows an example of the bounds and the entropy for a typical point in the Lena image dataset. We computed both bounds for different values of K . Notice that the bounds are quite tight and, except for the small region near the upper bound, do not include the flat, numerically challenging region of the function. As for the initialization, we need to find a good initial iterate for the root-finding algorithm, i.e., as close as possible to the root. One way to do it is to provide precomputed initialization values directly to the algo-rithm. For example, we can initialize the algorithm from the middle of the bounds from Theorem 1.1 , or initialize  X  from the distance to the k th neighbor. But log
K these initializations ignore the distances to most of the points, which do affect the entropy and so the root. We also do not want to include more information in the initialization if its computational cost becomes com-mensurate to evaluating the entropy function itself. Instead, we propose to capitalize on the correlation that exists between  X  and the structure of the dataset. We can then link the points to each other based on some criterion and initialize the algorithm from the solution of the points for which  X  was already found. This order can be sequential or, more generally, based on a tree. In the sequential order each new point is initialized from the solution to the previous one. In the tree order, the order is not linear, but forms a di-rected tree (or forest in general) with each point being a node. The points are then processed in an order (such as breadth-first search) which ensures that the root of a parent node is visited before the root of its children (which are initialized from the parent). For both sequential and tree orders, each root point can be initialized, for example, from the middle of the bounds. We now describe two different strategies for choosing the order and show how they are correlated with  X  . The first, local strategy is based on the existence and continuity of the function  X  ( x ) defined in section 1 . Continuous changes in x lead to continuous changes in  X  , so expect nearby points to have similar  X  values (except where  X  ( x ) changes quickly). Therefore, we can use a local ordering of the points in the dataset. Among various orders we have explored, the one de-rived from a minimum spanning tree (or forest) of the dataset works well and is efficient to compute. We used Kruskal X  X  algorithm, which takes O ( N  X  log N ) time to construct an MST given a  X  -nearest-neighbor graph. The MST is faster to compute if using a small  X  , but it should not be too small that it loses too much con-nectivity information. Empirically we found out that  X  = 10 gives a good tradeoff and we use it for all our experiments. We observed that the choice of the root point(s) does not critically affect the results. Our second strategy takes into account the density around the points. The closer the points are to x , the smaller the distances d k are and the bigger the entropy H ( x ,  X  ) is. Therefore, for the entropy to remain con-stant, the resulting  X  must be larger in dense regions and smaller in sparser ones. Indeed,  X  (or  X  ) is related to a nonparametric density estimate of the dataset. Estimating the density in the first place is difficult, but we can use a simple estimate given by the distance from the query point x to its k th neighbor. Then, we can sort the points x 1 , . . . , x N in increasing distance of its k th nearest neighbor, which gives a sequential or-der. As for the choice of k , we find a correlation with the desired perplexity value K : we observe empirically that the best k (which gives the best initializations) is usually approximately equal to K . We call this the D
K order. Note this is different from the old rule-of-thumb of setting  X  directly to the distance to k th neighbor ( k = 7 in Zelnik-Manor &amp; Perona , 2005 ). We use this only to initialize the root finding. Fig. 4 shows how  X  changes from point to point and the resulting D K and MST orders for a dataset of 1 000 randomly generated uniformly distributed points.  X  changes according to our predictions above: the changes are gradual in any local neighborhood and For each of the datasets, we present three statistics in fig. 6 : the total runtime for the different root-finding algorithms with the different initializations; the aver-age number of iterations per point required to achieve the tolerance; and the number of the points that con-verged after a certain iteration count.
 First of all, notice that Halley X  X  and Euler X  X  meth-ods have very similar performance for all the datasets. Both methods require only two iterations for most of the points. However, there is a small difference, in particular for the bounds order of the MNIST dataset and for the MST , bounds and random orders in the Grolier dataset. The reason is the initialization in the flat region for many of the points (note that the result of those initializations is not good compared to e.g. the D
K order). Similar to what we see in the flat region of fig. 2 , Halley X  X  method is more conservative and moves slowly towards the solution, whereas Euler X  X  method uses steps that are too big and retreats to bisection, which moves away from the flat region in one iteration. Compared to the other derivative-based root-finding methods, Newton X  X  method is a second-order method and requires slightly more iterations than Halley X  X  or Euler X  X  methods. However, its runtime is lower be-cause each iteration does not need to compute the second derivative of the entropy, which costs O ( N ). For the derivative-free methods, Ridder X  X  method is fastest, but is still approximately twice as slow as New-ton X  X  method. Brent X  X  method and the bisection are approximately 5  X  and 10  X  20  X  as slow as Newton X  X  method, respectively.
 For different initializations, MST and D K have very similar results for the MNIST and Lena datasets, with D
K being only slightly better (e.g. for Euler X  X  method in the Lena dataset it takes 2 . 09 iterations on average for D K compared to 2 . 22 for MST ). However, for the Grolier dataset the MST order does almost as badly as the random order. This is due to the spatial empti-ness that we described above. This does not seem to affect the D K order, which is only 22% slower than the oracle order. The bounds and random orders perform almost identical to each other and not terribly bad, only about 1 X 2 iterations more than the other initial-izations. However, for 50% of the points, the extra iterations are the bisection iterations during the first steps of the algorithm when the initial region is flat and the root-finding methods send us away from the bounds. This also indicates that the bounds are quite tight and one or two extra iterations are able to move very close to the root no matter where the initializa-tion is. The raster order for Lena dataset does almost as good as the MST order, suggesting it as an fast alternative local order for image pixels. Finally, the oracle order achieves nearly 1 iteration per point for Euler X  X  and Halley X  X  method on Lena and MNIST. For example, for Euler X  X  method only 0 . 1% of all the points needed 2 iterations to converge. However, in terms of the runtime the MST and D K orders achieve a speed that is only twice as slow as the optimal one. For the Grolier dataset the average number of iterations per point is more than one even for the oracle order. This is because, even in the best case, the  X  s are not as close to each other as in the Lena and MNIST datasets. The entropic affinities can give high-quality results with many different machine learning algorithms that are based on graphs (as illustrated in fig. 1 ), and only require the user to set the perplexity K and possi-bly the sparsity level of the affinity matrix. How-ever, up to now they have not been in widespread use outside nonlinear embedding methods such as SNE ( Hinton &amp; Roweis , 2003 ) or EE ( Carreira-Perpi  X n  X an , 2010 ). Reasons for this could be the lack of a closed-form expression for the bandwidth of each point given the perplexity, and the (up to now) computational cost involved in solving for it. With our numerical algorithms, there is now very little difference between applying a closed-form formula and solving for the im-plicit bandwidths almost exactly. This is for two rea-sons. First, even if a user is able to compute band-widths very efficiently (e.g. with a rule-of-thumb for-mula), computing the elements of the affinity matrix themselves is still O ( N 2 ) or O ( N  X  ) in the full and sparse case, respectively. Each iteration of our root-finding method has this same cost, but (1) we require just a few such iterations, and (2) the affinities are produced for free in our last iteration. Thus, the cost of applying the rule-of-thumb formula to compute the affinity values given the bandwidths is compara-ble to that of computing entropic affinities and their bandwidths. Second, we can achieve near-machine-precision at nearly no extra cost because of the high order of convergence of the root-finding methods. The bisection algorithm used by Hinton &amp; Roweis ( 2003 ), although slow, was not much of a prob-lem up to now because the optimization in meth-ods such as SNE was so costly that the number of points N was limited to a few thousands, for which the bisection time was acceptable. How-ever, recent improvements in embedding optimization ( Vladymyrov &amp; Carreira-Perpi  X n  X an , 2012 ) have signif-icantly increased the values of N that are practical: the embedding optimization takes 15 min for 20 000 MNIST images in a workstation, while the bisection-based computation of the entropic affinities takes over 20 min and becomes a bottleneck. With our algorithm, this time is reduced to 55 seconds.
 Some work has used the fast Gauss transform to compute a single bandwidth parameter for ker-nel density estimation (KDE) ( Raykar et al. , 2010 ; Raykar &amp; Duraiswami , 2007 ). These algorithms use an approximate decomposition of the computations to achieve linear runtime with N -body problems, such as KDE. As mentioned before, one needs to add the cost of computing the affinities given the bandwidth, so by Amdahl X  X  law this reduces the speedup. The pa-rameters of the fast Gauss transform are also hard to tune ( Raykar &amp; Duraiswami , 2007 ) and a bad choice can make the runtime even slower that the exact com-putation. More importantly, as mentioned before, in many cases a single bandwidth parameter is just not good enough. Based on our knowledge there is no work that estimates per-point  X  fast, except for rules-of-thumb and cross-validation methods that are slow ( Sheather , 2004 ; Duong &amp; Hazelton , 2005 ). By extending the entropic affinity function to the en-tire Euclidean space, we have been able to characterize its behavior, show that it is a well-defined function and give explicit bounds for its implicitly defined value. Based on these properties, we have analyzed different algorithms for the computational problems involved: root-finding and ordering points for best initialization. One of the best and simplest choices is a Newton-based iteration, robustified with bisection steps, using a tree-or density-based order. This achieves just above one iteration per data point on average, which is the opti-mally achievable performance.
 Entropic affinities work better than using a single bandwidth or multiple bandwidths set with a rule of thumb, provide a random-walk matrix for a dataset, and only require a user to set the global number of neighbors. The fact that they define the scale implic-itly and require an iterative computation may have prevented their widespread application, but our algo-rithm makes the computation scale up almost as if they were given in explicit form. Matlab code is available from the authors X  web page.
 Partly funded by NSF CAREER award IIS X 0754089. Bchir, Ouiem and Frigui, Hichem. Fuzzy relational kernel clustering with local scaling parameter learn-ing. In Proc. of the 2010 IEEE Int. Workshop on
Machine Learning for Signal Processing (MLSP10) , pp. 289 X 294, Kittil  X a, Finland, Aug. 29  X  X ep. 1 2010. Belkin, Mikhail and Niyogi, Partha. Laplacian eigen-maps for dimensionality reduction and data repre-sentation. Neural Computation , 15(6):1373 X 1396, Jun. 2003.
 Belkin, Mikhail, Niyogi, Partha, and Sindhwani,
Vikas. Manifold regularization: A geometric frame-work for learning from labeled and unlabeled ex-amples. Journal of Machine Learning Research , 7: 2399 X 2434, Nov. 2006.
 Brent, Richard P. Algorithms for Minimization with-out Derivatives . Prentice-Hall, Englewood Cliffs, N.J., 1973.
 Carreira-Perpi  X n  X an, Miguel  X  A. Fast nonparamet-ric clustering with Gaussian blurring mean-shift.
In Proc. of the 23rd Int. Conf. Machine Learn-ing (ICML 2006) , pp. 153 X 160, Pittsburgh, PA, Jun. 25 X 29 2006.
 Carreira-Perpi  X n  X an, Miguel  X  A. The elastic embedding algorithm for dimensionality reduction. In Proc. of the 27th Int. Conf. Machine Learning (ICML 2010) , pp. 167 X 174, Haifa, Israel, Jun. 21 X 25 2010. de Silva, V. and Tenenbaum, Joshua B. Global versus local approaches to nonlinear dimensionality reduc-tion. In NIPS , volume 15, pp. 721 X 728, MIT Press, Cambridge, MA, 2003.
 Duong, Tarn and Hazelton, Martin L. Convergence rates for unconstrained bandwidth matrix selectors in multivariate kernel density estimation. J. Multi-variate Analysis , 93(2):417 X 433, Apr. 2005.
 Er, Meng Joo, Wu, Shiqian, Lu, Juwei, and Toh,
Hock Lye. Face recognition with radial basis func-tion (RBF) neural networks. IEEE Trans. Neural Networks , 13(3):697 X 710, May 2002.
 Gander, Walter. On Halley X  X  iteration method. Amer. Math. Monthly , 92(2):131 X 134, Feb. 1985.
 Hinton, Geoffrey and Roweis, Sam T. Stochastic neighbor embedding. In NIPS , volume 15, pp. 857 X  864. MIT Press, Cambridge, MA, 2003.
 Manning, Christopher D. and Sch  X utze, Hinrich. Foun-dations of Statistical Natural Language Processing . MIT Press, Cambridge, MA, 1999.
 Melman, A. Geometry and convergence of Euler X  X  and Halley X  X  methods. SIAM Review , 39(4):728 X 735, Dec. 1997.
 Nene, S. A., Nayar, S. K., and Murase H. Columbia object image library (COIL-20). Technical Re-port CUCS X 005 X 96, Dept. of Computer Science, Columbia University, Feb. 1996.
 Ng, A. Y., Jordan, M. I., and Weiss, Y. On spectral clustering: Analysis and an algorithm. In NIPS , vol-ume 14, pp. 849 X 856. MIT Press, Cambridge, MA, 2002.
 Raykar, Vikas C. and Duraiswami, Ramani. The im-proved fast Gauss transform with applications to machine learning. In Large Scale Kernel Machines , Neural Information Processing Series, pp. 175 X 202. MIT Press, 2007.
 Raykar, Vikas C., Duraiswami, Ramani, and Zhao, Linda H. Fast computation of kernel estimators.
Journal of Computational and Graphical Statistics , 19(1):205 X 220, 2010.
 Ridders, C. J. F. A new algorithm for computing a sin-gle root of a real continuous function. IEEE Trans. Circuits and Systems , 26(11):979 X 980, Nov. 1979. Roweis, Sam T. and Saul, Lawrence K. Nonlinear di-mensionality reduction by locally linear embedding. Science , 290(5500):2323 X 2326, Dec. 22 2000.
 Scavo, T. R. and Thoo, J. B. On the geometry of
Halley X  X  method. Amer. Math. Monthly , 102(5):417 X  426, May 1995.
 Sheather, Simon J. Density estimation. Statistical Sci-ence , 19(4):588 X 597, Nov. 2004.
 Traub, J. F. Iterative Methods for the Solution of Equations . Prentice-Hall, second edition, 1982. Vladymyrov, Max and Carreira-Perpi  X n  X an, Miguel  X  A.
Partial-Hessian strategies for fast learning of non-linear embeddings. In Proc. of the 29th Int. Conf.
Machine Learning (ICML 2012) , pp. 345 X 352, Ed-inburgh, Scotland, Jun. 26  X  July 1 2012.
 Zelnik-Manor, Lihi and Perona, Pietro. Self-tuning spectral clustering. In NIPS , volume 17, pp. 1601 X  1608. MIT Press, Cambridge, MA, 2005.
 Zhou, Dengyong, Bousquet, Olivier, Lal, Thomas N.,
Weston, Jason, and Sch  X olkopf, Bernhard. Learn-ing with local and global consistency. In NIPS , vol-ume 16, pp. 321 X 328. MIT Press, Cambridge, MA,
