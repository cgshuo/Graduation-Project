 Distributed word representations represent word with a real valued vector, which is also referred to as word embedding. Well learned distributed word representations have been shown capable of captur-ing semantic and syntactic regularities (Pennington et al., 2014a; Mikolov et al., 2013c) and enhanc-ing neural network model by being used as features (Collobert and Weston, 2008; Bengio and Heigold, 2014; Wang et al., 2015).

Sequence tagging is a basic structure learning task for natural language processing. Many primary pro-cessing tasks over sentence such as word segmen-tation, named entity recognition and part-of-speech tagging can be formalized as a tagging task (Zhao et al., 2006; Huang and Zhao, 2007; Zhao and Kit, 2008b; Zhao and Kit, 2008a; Zhao et al., 2010; Zhao and Kit, 2011). Recently, many state-of-the-art sys-tems of tagging related tasks are implemented with bidirectional long short-term memory (BLSTM) re-current neural network (RNN), for example, slot filling (Mesnil et al., 2013), part-of-speech tagging (Huang et al., 2015), and dependency parsing (Dyer et al., 2015) etc. All of these systems use distributed representation of words to involve word level infor-mation. Better trained word representations would further improve the state-of-the-art performance of these tasks which makes it worthy to research the training methods of word representations.

The existing training methods of word represen-tation can generally be divided into two classes: 1) Matrix factorization methods. These methods uti-lize low-rank approximation to decompose a large matrix that contains corpus statistics. One typical work is the latent semantic analysis (LSA) (Deer-wester et al., 1990) in which the matrix records  X  X erm-document X  information, i.e., the rows cor-respond to words, and the columns correspond to different documents in the corpus. Another work is hyperspace analogue to language (HAL) (Lund and Burgess, 1996) which decomposes the matrix recording  X  X erm-term X  information, i.e., the rows correspond to words and columns correspond to the number of times that a word occurs in the given context. 2) Window-based methods. This type of methods learn representations by training a neural network model to make prediction within local con-text windows. For example, (Bengio et al., 2003) learns word representation through a feedforward neural network language model which predicts a word given its previous several words. (Collobert et al., 2011) trains a neural network to judge the va-lidity of a given context. (Mikolov et al., 2013a) proposes skip-gram and continuous bag-of-words (CBOW) models based on a single-layer network ar-chitecture. The objective of skip-gram model is to predict the context given the word itself, while the objective of CBOW is to predict a word given its context. Aside from these two sets of methods, dis-tributed representation can also be obtained by train-ing recurrent neural network (RNN) language model proposed by (Mikolov et al., 2010) or GloVe model proposed by (Pennington et al., 2014a) which trains a log-bilinear model on word-word co-occurrence counts.

All of these methods suffer from shortcomings that might limit the quality of trained word distri-butions. The matrix factorization family only uses the statistics of co-occurrence counts, disregarding of the position of word in sentence and word or-der. The window-based methods only consider local context, which is incapable of involving information outside the context window. While RNN language model theoretically considers all information of the previous sequence, but fails to involve the informa-tion of the posterior sequence. The word-word co-occurence counts that GloVe model relies on also only include information within a limited sized con-text window.

In this paper, we propose a novel method to ob-tain word representation by training BLSTM-RNN model on a specifically designed tagging task. Since BLSTM-RNN theoretically involves all information of input sentence, our approach avoids those short-ages suffered by most current methods.
 We firstly introduce the structure of BLSTM-RNN used to learn word representations in section 2. Then the tagging task for training BLSTM-RNN is described in section 3. Experiments are presented in section 4, followed by conclusion. The structure of BLSTM-RNN to train word rep-resentation is illustrated in Figure 1. Each input is composed of a word identity x 1 i and additional real-valued features x 2 i . x 1 i is represented with one-hot representation which is a binary vector with dimen-sion | V | where V is the vocabulary. The input vector I of the network is computed as: where W 1 and W 2 are weight matrixes connecting two layers and are updated with the neural network during training. W 1 x 1 i is also known as the dis-tributed representation of word or word embedding which is a real-valued vector usually with a much smaller dimension than x 1 i . Distributed represen-tation trained in other tasks can be easily incorpo-rated by initializing W 1 with these external repre-sentations. According to the structure shown in Figure 1, W 1 is a matrix of weights that is updated during train-ing, thus the distributed representations contained in W 1 are learned simultaneously with the train-ing of BLSTM-RNN on any supervised learning tasks. However, all such tasks for BLSTM-RNN, to the best of our knowledge, require labeled data which is usually too small in size and hard to ob-tain. In this section, we propose a tagging task spe-cially for BLSTM-RNN to train distributed repre-sentations with unlabeled data.

In this method, BLSTM-RNN is applied to per-form a tagging task with only two types of tags to predict: incorrect/correct. The input is a sequence of words which is a normal sentence with several words replaced by words randomly chosen from vo-cabulary. The words to be replaced are chosen ran-domly from the sentence. In practice, we generate a random number for each word, and a word is chosen to be replaced if the number is lower than a given threshold. For those replaced words, their tags are 0 (incorrect) and for those that are not replaced, their tags are 1 (correct). A simple sample of constructed corpus is shown in Figure 2. Although it is possible that some replaced words are also reasonable in the sentence, they are still considered  X  X ncorrect X . Then BLSTM-RNN is trained to judge which words have been replaced by minimizing the binary classifica-tion error on the training corpus. When the network is trained, W 1 contains all trained word representa-tions. In our experiments, to reduce the vocabulary V , each letter of input word is transferred to its low-ercase. The upper case information is kept in an additional features x 2 i which in practice is a three-dimensional binary vector to indicate if x 1 i is full lowercase, full uppercase or leading with a capital letter.

Our approach is similar to (Collobert and We-ston, 2008) and (Gutmann and Hyv  X  arinen, 2012; Mnih and Teh, 2012; Vaswani et al., 2013). All of these works introduce randomly sampled words and train a neural network on a binary classification task, while (Collobert and Weston, 2008) learns represen-tations for a feedforward network and (Gutmann and Hyv  X  arinen, 2012; Mnih and Teh, 2012; Vaswani et al., 2013) learns normalization parameters instead of representations. 4.1 Experimental setup To construct corpus for training word representa-tions, we use North American news (Graff, 2008) which contains about 536 million words as un-labeled data. The North American news data is first tokenized with the Penn Treebank tokenizer are replaced with the symbol  X  #  X  . For example, both words  X  tel92  X  and  X  tel6  X  are converted into  X  tel#  X . The vocabulary is limited to the most fre-quent 100,000 words in North American news cor-pus (Graff, 2008), plus one single  X  X NK X  sym-bol for replacing all out of vocabulary words. The threshold to determine whether a word is replaced is 0.2, which means about 20% tokens in corpus are re-placed with tokens randomly selected from vocabu-lary. BLSTM-RNN is implemented based on CUR-RENNT (Weninger et al., 2014), an open source GPU-based toolkit of BLSTM-RNN. The dimension of word representation as well as input layer size of BLSTM-RNN is 100 and hidden layer size is 128.
Three published methods for training word repre-sentations are compared: Skip-gram (Mikolov et al., 2013a), CBOW (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014a). They are reported su-perior in capturing meaningful latent structures than other previous works in (Mikolov et al., 2013a; Pen-nington et al., 2014a), thus are regarded as the state-of-the-art approach of training word representations. We train the Skip-gram and CBOW model using the word2vec tool (Mikolov et al., 2013b) with a con-text window size of 10 and 10 negative samples. For training GloVe , we use the GloVe tool (Pennington et al., 2014b) with a context window size 10. These configurations are set by following (Pennington et al., 2014a). Training corpus, vocabulary and dimen-sion of word representations are set the same as that in experiment for training word representations with are replaced with words randomly chosen from vocabulary. 4.2 Evaluation The quality of trained distributed representation is evaluated by the performance of BLSTM-RNN which uses the trained representations on practical tasks. The representations which lead to better per-formance are considered containing more useful la-tent information and are judged better. The struc-ture of BLSTM-RNN to test word representations is the same as that in Figure1. To use trained repre-sentation, we initialize the weight matrix W 1 with these external representations. For words without corresponding external representations, their repre-sentations are initialized with uniformly distributed random values, ranging from -0.1 to 0.1. Three typ-ical tagging tasks are used for the evaluation: part-of-speech tagging (POS), chunking (CHUNK) and named entity recognition (NER).  X  The POS tagging experiment is conducted on  X  CHUNK experiment is conducted on the data  X  NER experiment is conducted on the CoNLL-
To focus on the effect of word representation, for all tasks, we use the network with the same hid-den structure and input features. The size of input layer is 100, size of BLSTM hidden layer is 128 and output layer size is set as the number of tag types according to the specific tagging task. Input features are composed of word identity and three-dimensional binary vector to indicate if the word is full lowercase, full uppercase or leading with a cap-ital letter.
 Table 1 presents the performance of BLSTM-RNN with different distributed representations on these three tasks. BLSTM is the baseline sys-tem that does not involve external word representa-tions. Among all representations, BLSTMWE which is trained by our approach gets the best performance on all three tasks. It shows our approach is more helpful for BLSTM-RNN. Besides, all of the three published word representations also significantly en-hance BLSTM RNN. It confirms the commonly ac-cepted notion that word representation is a useful feature. 4.3 Analysis Table 2 shows the performance of word rep-resentations trained on corpora with different size. BLSTMWE (10M), BLSTMWE (100M) and BLSTMWE (536M) are word representations respec-tively trained by BLSTM-RNN on the first 10 mil-lion words, first 100 million words and all 536 mil-lion words of the North American news corpus. As expected, there is a monotonic increase in perfor-mance as the corpus size increases. This observation suggests that the result might be further improved by using even bigger unlabeled data.

Table 3 presents running time with different meth-ods to train word representations on 536 million words corpus. BLSTM-RNN is trained on one NVIDIA Tesla M2090 GPU. The other three meth-ods are trained on a 12 core, 2.53GHz Intel Xeon E5649 machine, using 12 threads. Though with the help of GPU, BLSTM-RNN is still slower than the other methods. However, it should be noted that the speed of our approach is acceptable compared with previous neural network language model based methods, including (Bengio et al., 2003; Mikolov et al., 2010; Mnih and Hinton, 2007), as our model uses a much simpler output layer which only has two nodes, avoiding the time consuming computation of the big softmax output layer in language model. In this paper, we propose a novel approach to learn distributed word representations with BLSTM-RNN. Word representations are implemented as the layer weights and are obtained as a byproduct of training BLSTM-RNN on a specially designed task, thus theoretically involve information of the whole sentence. The quality of word representations are evaluated by the performance of BLSTM-RNN which uses these representations on three tagging tasks: part-of-speech tagging, chunking and named entity recognition. In experiments, word representa-tions trained by our approach outperform the word representations trained by other published methods. Our work demonstrates an alternative way to im-prove BLSTM-RNN X  X  performance by learning use-ful word representations.

