
In order to benefit from the continuous improvements in digital data collection capabilities, efficient data mining and analysis tools are required. One important tool in this context, which has numerous applications is clustering [1]. Following seminal work by Cheng and Church [2] in the area of gene expression using microarray technology, substantial focus has been placed in recent years on co-clustering. Co-clustering extends clustering by allowing simultaneous clustering of the rows and columns of a data matrix, aiming to identify a subset of rows which exhibit similar behavior across a subset of columns, or vice versa [3], [4]. While many co-clustering techniques have been proposed over the years (kernel based [5], [6], exhaustive enumeration [7], spectral analysis [8], greedy [2], CTWC [9] and others, surveyed in [4] and [3]), few have considered the problem of finding co-clusters involving lagged correlations between the behavior of a subset of rows (objects) over a subset of columns [10]. Yet this latter case, which may reveal an underlying regulatory mechanism governing the value of the participating objects, is quite common in real life settings. For example, consider the problem of identifying a group of people coordinating their movements in a crowd (e.g., trying to get from point A to point B). If the group keeps its original formation, then the trajectories of the members X  spatial positions over time form a lagged pattern . Similarly, consider the application of oil and gas exploration based on reflection seismology [11]. Here, seismometers are placed on the surface, recording seismic waves. A single initiated explosion creates a wave which is reflected from each underground layer with varying time differences (depending on the depth and structure of that layer). Therefore, an appropriate time lagged analysis of the reflections received by different seismometers placed in different locations on the ground may reveal the structures and dimensions of the layers [11].

We denote this problem of extending co-clusters to cap-ture lagged correlations between a subset of rows over a subset of columns as a  X  X agged pattern X  (see Figure 1, based on [10]). While the idea of finding lagged patterns between different streams of data is not new, existing methods are inherently limited to comparing pairs of objects [12], [13] or mining clusters with contiguous columns [14] X  X 16] and thus cannot be successfully applied to the general lagged co-clustering problem.

As in most clustering problems, there are various mea-sures for the quality of clusters found. Given the fact that co-clustering is a specific case of lagged co-clustering, with a zero lag, the latter problem is NP-complete for any measure for which the non-lagged co-clustering problem is NP-complete. In particular, we base our model on [2], [15] adding to it a lag aspect, which is also proved to be NP-complete. Therefore, the main contribution of the paper is a polynomial-time Monte-Carlo algorithm for lagged co-clustering denoted LC , which is the first attempt, to the best of our knowledge, to develop a polynomial approximation to the problem. The LC algorithm takes as input a real number matrix and a maximum error value and outputs a set of lagged co-clusters whose errors do not exceed the pre-specified value. As part of the analysis we prove that the output includes, with fixed probability, a lagged co-cluster which is optimal according to any monotonically increasing objective function of the cluster dimension. The algorithm handles many of the inherent properties common to non-lagged data [4]. For example, it overcomes noise (erroneous reading due to local noise, equipment accuracy, experi-mental or human error), anti-correlations (down-regulated, adapting gene expression terminology), missing values (e.g., due to equipment malfunction) and overlapping patterns. Furthermore, the lagged co-clusters are mined even if the amplitude of the reflected values fades along columns (as in the seismometers example).

The algorithm and its properties are extensively evaluated using artificial data and real-world data from two differ-ent domains (topographic data and river flow data). The artificial data is used mainly to demonstrate the efficiency of the algorithm in mining relevant and coherent lagged co-clusters, to verify the theoretical bounds and to show actual performance. The data from the two other domains are used to demonstrate the ability of the algorithm to produce relevant and valid clusters based on overlapping, partially missing and noisy real data.

The remainder of the paper is organized as follows: in the following section we review related work. In Section III we define the model and show that for most interesting variants of the problem it is an NP-Complete problem. Section IV presents the algorithm while Section V gives proof of the probabilistic guarantee to efficiently mine relevant lagged co-clusters. Section VI analyzes the running time and Sec-tion VII presents the experiments conducted and results. We conclude with a discussion and directions for future research in Section VIII.

A wealth of research has been undertaken studying clus-tering (see [1] for a survey). The research has emerged from a variety of fields: biology, physics, economics, computer science and more. Typically, in clustering problems, clusters are extracted from a matrix dataset where the rows repre-sent objects and the columns represent the features of the object [1], [3].

Simple mining techniques look for fully dimensional clusters: subsets of rows over all columns, or subsets of columns over all rows [12, inter alia]. These techniques have several inherent vulnerabilities, e.g., difficulty in handling irrelevant, noisy or missing features, which often exist, inaccuracy due to the  X  X urse of dimensionality X  [17] and even counter-productivity as they increase the background noise [3], [4].

To overcome these obstacles, the data analysis has to find the relevant subspace for a particular pattern and ignore the rest, i.e., mining clusters contained in subset of rows over a subset of columns. This type of clustering is known as bi-clustering, co-clustering, co-regulation or simply clustering. The different approaches for co-clustering (see [2], [5] X  X 10], [14], [15], [18] X  X 20], surveyed in [4] and [3]), are based on different models (additive vs. multiplicity, axis alignment, rows over columns preferment, cluster scoring function, overlapping, etc.) and algorithmic strategies (greedy, divide and conquer, kernel based, bayesian networks, etc.). Sub-stantial effort has been directed at non-lagged co-clustering of datasets with temporal nature (surveyed in [21]).

The lagged co-clustering model generalizes the co-clustering model by introducing lags (shifts) between the dataset X  X  objects. Most algorithms trying to mine lagged co-clusters do so by working on pairs of rows. They differ in the correlation techniques being used: cross-correlation, normalized, Granger, Pearson, partial and others [12], [13, inter alia]. Extending these algorithms to mine clusters of more than two rows demands some combinatorial solution (e.g., merging), which is both time consuming and heavily dependent on the closeness merit function. In addition, correlated pairs do not necessarily have the transitive prop-erty [12].

Among the few studies that have considered a lagged co-clustering model involving clusters of more than two rows, most were focused on a decadent variant in which the goal was finding a subset of rows over a contiguous subset of columns [14] X  X 16], [18].

One algorithmic approach for this variant is to discretize the real number input matrix by transforming onto a finite alphabet-matrix,  X  m  X  n , enabling the use of fast string matching techniques that run in a polynomial time (see q-cluster algorithm [14], [16] and the CCC-Biclustering algorithm [15]). The main drawback of this approach is the alphabet size. Since it requires data discretization, a coarse abstraction using a small alphabet may lead to greater errors and finer clusters being missed. Using large |  X  | will have a dramatic influence on the run-time as it is exponentially dependent on |  X  | .

Another approach suggested for this variant uses the dynamic programming method. It first searches for small coherent clusters to serve as building blocks. Then, it hierar-chically merges them, while activating pruning methods (see S 2 D 3 algorithm [18]). The main drawback of this approach is an exponential run-time.

The work most relevant to our research is the ts-Cluster algorithm proposed by Yin et al. [10]. The algorithm uses dynamic-programming and a hierarchical-merge approach in order to mine lagged co-clusters. The main drawbacks of the algorithm are the reduction to a small alphabet,  X  = { up, non, down } , looking for trend-like clusters rather than a more subtle model. Furthermore, and most importantly, the running time of the ts-Cluster algorithm is exponential.
In order to present the lagged co-clustering model, we augment the legacy co-cluster definition [2] to include the lagging aspects. A lagged co-cluster of an m  X  n real number matrix X , is a submatrix determined by a subset I of the rows and their corresponding T lags ( | T | = | I | ) over a subset J of the columns, aligned to some extent to a lagged mechanism (see Figure 1). A lagged regulatory mechanism holds if for every two rows i 1 ,i 2  X  I and their corresponding lag T i 1 ,T i 2 , the proportion between the entries over all j  X  J is constant independent of j : X variable G i indicating object i  X  X  regulation strength; a latent variable T i indicating the influencing-lag of object i and a latent variable H j indicating the regulatory intensity in sample j (see Figure 1b). 1 Therefore, in a lagged co-cluster, we expect the submatrix elements to comply with the relation: X i,j  X  G i H j + T i for all ( i,j )  X  ( I,J ) . A particular measure for the deviation in X i,j from the approximation G
H j + T i is the modification of the relative error criteria used for non-lagged co-clusters [19]: G i H j + T i /X goal is to mine large submatrices, with a relative error below a certain pre-defined threshold. The optimal size submatrix depends on the merit function, f ( | I | , | J | ) , used for evaluating the submatrix size rank. We can rank a submatrix by its perimeter, | I | + | J | , area, | I |  X  | J | , or any other trade-off between the number of rows and the number of columns. Previous work (e.g., [6]) mainly handled biological datasets characterized by thousands of rows over tens of columns [3]. Therefore, it is reasonable to consider case of m n the inclusion of an additional column is worth the exclusion of a relatively large number of rows. In contrast, lagged co-cluster datasets can be characterized by time readings. This results in hundreds or thousands of columns, or, in an on-line version, an infinite stream of columns. Therefore, any assumption regarding the relation between the number of rows and the number of columns is futile. Consequently, we allow the use of any monotonically growing objective function  X  ( | I | , | J | ) . Our problem thus turns into finding an optimal size submatrix with a relative error below a certain threshold:
To facilitate analysis, we switch from a multiplicative model to an additive model by applying logarithm trans-formation, setting A i,j = log X i,j , R i = log G i , C log H j + T i and  X  = log  X  . Therefore, our problem translates to finding R i , T i and C j such that for all i,j , Notice that for lagged-anti -correlations, i.e., X G We note that other models, such as derivative or power-law, can be easily incorporated in the above formulation.

Definition 1: The sleeve-width of a submatrix A , defined by a subset J of columns, a subset I of rows and their corresponding lag T , is: The notion of sleeve-width reflects the extent to which an entry i,j in the lagged co-cluster is allowed to deviate from being considered as the summation of R i + C j + T i .
At this point, we have all we need in order to formally define a lagged co-cluster. However, we extend the model to include two additional parameters,  X  and  X  , that allow the user to specify the minimum dimensions of the mined cluster:  X  -the minimum number of the rows expressed as a fraction of m ;  X  -the minimum number of the columns expressed as a fraction of n . If the user wishes not to constrain the cluster X  X  dimensions, trivial defaults can be used, i.e., | I | , | J | X  2 .

Definition 2: Let 0 &lt;  X , X  &lt; 1 . A lagged co-cluster of matrix A with a sleeve-width w &gt; 0 is a triple ( I,T,J ) , with J a subset of the columns, I a subset of the rows and T their corresponding lag, that satisfies the following:  X  Size: The number of the rows is 2  X   X m  X  | I | = | T |  X  Sleeve-width: sw T ( I,J )  X  w . i.e., for all i  X  I and Therefore, lagging and shifting row i by T i and R i , respec-tively, will place each column j  X  J within a sleeve-width of w surrounding the row profile. For the specific case where T i = 0 , we obtain a definition equivalent to the one used for non-lagged co-clustering [19].

Our goal is thus to mine an optimal size submatrix with a sleeve-width not exceeding a pre-defined sleeve width w . As we prove in the following theorem, this problem is NP-complete.

Theorem 1: The problem of finding an optimal | I | = | J | or | I | X | J | lagged co-cluster is NP-complete.
 ( I,T,J ) to a non-lagged co-clustering problem ( I,J ) , which is proven to be NP-complete [2], [5]. We apply a polynomial time reduction by converting the lagged-matrix, A , into a non-lagged one, A 0 , as follows. First, we randomly choose a row p  X  A . From every other row i  X  A , we create 2 n new entries in A 0 , each with a different lag in comparison to p (i.e.,  X  n  X  lag  X  n ). Null entries resulting from such alignments are marked as missing values. The resulting matrix A 0 is a non-lagged co-clustering problem of size 2 nm  X  3 n .

In this section, we present the LC algorithm, a polyno-mial time Monte-Carlo algorithm. Naturally, the design of LC is mostly influenced by solution concepts introduced for non-lagged co-clustering problems [5], [19]. The LC algorithm guarantees with fixed probability the mining of optimal dimension lagged co-clusters with a sleeve-width not exceeding a pre-specified value. The algorithm X  X  input is a matrix of real numbers and its output is a list of lagged co-clusters. It makes use of random projection, which is a common technique for mining co-clusters [5], [6]. The algorithm inherently handles noise and overlapping: noise by allowing the lagged co-cluster to be of some maximal pre-specified sleeve-width; overlapping by utilizing the inde-pendent random projection to reveal sub-dimensions relevant only to a specific cluster. Missing values are overcome by calculating the coherence of a lagged co-cluster on the non-missing values of the submatrix [19], [20].

Figure 2 presents the LC algorithm. Generally, the al-gorithm can be divided into the following phases: (1) Initialization: randomly choose a discriminating row and a discriminating set of columns as seeds; (2) Row addition: go over all rows and lags and add those that comply with the sleeve-width criteria; (3) Column addition: go over all the columns and add the ones that comply with the sleeve-width criteria. The inclusion of a row or a column only after complying with the sleeve-width criteria, guarantees that only relevant rows and columns are added to the lagged co-cluster.
 LC Algorithm
Input: X , an m  X  n matrix of real numbers, and w  X  0 , the maximum acceptable sleeve-width.

Output: List of ( I,T,J ) , a list of lagged co-clusters that are submatrices of X with columns subsets J , rows subsets I and their corresponding lag T , which have a sleeve-width score that does not exceed w .

Initialization: Setting of ` p , ` S and | S | is thoroughly discussed in the following section. 1: loop ` p times 2: randomly choose row p : 1  X  p  X  m ; 3: loop ` S times 4: randomly choose a set of columns S ; 5: set ( I,T )  X  ( p, 0) ; 6: for each ( i,t ) : 1  X  i  X  m ,  X  n  X  t  X  n do 7: if sw T ( I  X  X  i } ,S )  X  w then 8: J  X  S ; 9: for each j : 1  X  j  X  n do 10: if sw T ( I,J  X  X  j } )  X  w then 11: if | I | &lt;  X m or | J | &lt;  X n then discard ( I,T,J ); 12: return a list of the ( I,T,J ) ;
The calculation of sw T ( I,J ) (line 7, 10) is done by computing sw ( I,J ) [19] on the non-lagged submatrix. Such a non-lagged submatrix is obtained by lagging each row i  X  I relative to p by T i ( T p = 0 ). Null entries caused by the lagging process are marked as missing-values.

Next we show some guarantees of the LC algorithm X  X  ability to efficiently mine relevant and coherent lagged co-clusters. These guarantees are demonstrated experimentally in Section VII.

We prove that the LC algorithm guarantees finding an optimal lagged co-cluster of sleeve-width w with a fixed probability in a polynomial number of iterations. The struc-ture of the proof is inspired by [19] and consists of two major stages. First, we show that an optimal lagged co-cluster can be mined using a log-size discriminating set with probability 0.5. Based on this capability, we then show that when running the LC algorithm in a polynomial number of iterations, it mines the optimal lagged co-cluster with a probability of at least 0.5.

The proof relies on the important insight that a sufficient size for a discriminating set is logarithmic in the size of the set [5], [6]. This latter result enables the use of a small subset, of O (log mn ) size, randomly chosen from the columns, which discriminates the participating rows and their lags.

The definition of a discriminating set for the lagged model is given as follows.

Definition 3: Let ( I,T,J ) be a lagged co-cluster of sleeve-width w , and p  X  I . S  X  J is a discriminating set for ( I,T,J ) with respect to p if it satisfies: 1. sw T ( { i,p } ,S )  X  w for all i  X  I . 2. sw T ( { i,p } ,S ) &gt; w for all i /  X  I .
 We next show that for an optimal lagged co-cluster ( I  X  ,T  X  ,J  X  ) , there are many small sets of size O (log mn ) , each of which is a discriminating set with a probability of 0.5. This latter result is important since upon selecting p  X  I  X  and S  X  J  X  , we can deduce I  X  , T  X  and J  X  . Furthermore, the capability of finding a discriminating set with probability 0.5, is later used by Theorem 3 to mine an optimal lagged co-cluster, in a polynomial number of iterations.

Theorem 2: Let ( I  X  ,T  X  ,J  X  ) be an optimal lagged co-cluster of sleeve-width w , with  X   X  ( | J  X  | /n ) &lt;  X  let p  X  I  X  . Any randomly chosen subset S of J  X  , of size | S |  X  log(4 mn ) / log(1 / 3  X  0 ) , is a discriminating set for ( I  X  ,T  X  ,J  X  ) , with respect to p with probability at least 0 . 5 . condition (1) of Definition 3 always holds and the probability that condition (2) does not hold is less than 0 . 5 .
Let R  X  i , i  X  I  X  , be a column profile, T  X  i , i  X  I lagged column profile, and C  X  j , j  X  J  X  , a row profile for ( I  X  ,T  X  ,J  X  ) . Condition (1) of definition 3 is always satisfied, since { i,p } X  I  X  and S  X  J  X  , so sw T ( { i,p } ,S )  X  sw
Moving to condition (2) of Definition 3 we note that S fails to be a discriminating set for I  X  with respect to p , only if there exists a row i /  X  I  X  such that sw T ( { i,p } ,S )  X  w . We next show that the probability of this for a particular row i and lag t is at most (3 | J  X  | /n ) | S | &lt; (3  X 
According to Definition 2, sw T ( { i,p } ,S )  X  w means that there are R i ,T i ,R p ,T p (= 0) , and C j , j  X  S , such that: | A Shifting each row i  X  I (in the first inequality) by T and subtracting the second inequality (of row p ) we obtain | A i,j  X  A p,j  X  R | X  w for all j  X  S , and some R (= R i  X  R Due to the lagged co-cluster optimality, we show that there are no more than 3 | J  X  | columns j that satisfy this inequality.
If | A i,j  X  A p,j  X  R |  X  w then: ( A p,j  X  C  X  j  X  R  X  p w  X  A i,j  X  C  X  j  X  R  X  p  X  R  X  ( A p,j  X  C  X  j  X  R  X  p ) + w . Since | A p,j  X  C  X  j  X  R  X  p |  X  1 2 w for all j  X  J  X  , it follows that  X 
Lemma 1: Let J  X  J  X  , and let i /  X  I  X  . If | A i,j  X  C w/ 2 for some r and all j  X  J , then | J | &lt; | J  X  | . a lagged co-cluster of sleeve-width w satisfying  X  ( I,J ) &gt;  X  ( I  X  ,J  X  ) , contradicting the optimality of ( I  X  ,T
Therefore, for a row i /  X  I  X  , lag t and for each of the intervals: [  X  3 2 w,  X  1 2 w ] , [  X  1 2 w, 1 2 w ] and [ at most | J  X  | columns j such that A i,j  X  C  X  j  X  R  X  p that interval, summing to at most 3 | J  X  | columns satisfying | A
Therefore, the occurrence probability for some lag t (  X  n  X  t  X  n ) and some row i ( 1  X  i  X  m ) is bounded (after substituting | S | X  log(4 mn ) / log(1 / 3  X  2 mn (3  X  0 ) | S |  X  0 . 5 .

According to Theorem 2 any randomly selected set of size | S |  X  log(4 mn ) / log(1 / 3  X  0 ) is a discriminating set with probability of at least 0.5. The bound is very hard to set as  X  is unknown. In Section VII we show experimentally that a random subset of size 0 . 4 log(4 mn ) + 2 will do, freeing the user from specifying the  X  0 -trade-off.

Theorem 3: Let S be a discriminating set for an optimal lagged co-cluster ( I  X  ,T  X  ,J  X  ) of sleeve-width w . Provided ` p  X  ln 4 / X  and ` S  X  2 ln 4 / X  | S | , the LC algorithm will mine the optimal lagged co-cluster, with probability of at least 0.5.
 in the outer loop that satisfies p  X  I  X  is at least  X  . Thus, the probability of the outer loop failing in all ` p tries is bounded by (1  X   X  ) ` p  X  1 / 4 . In the inner loop, the probability of satisfying the columns discriminating set S  X  J  X  is at least  X  | S | , since | J  X  |  X   X n . Following Theorem 2, any given S  X  J  X  is a discriminating set with probability of at least 0 . 5 with respect to p . Therefore, the probability that all ` inner loops iterations fail to find a discriminating set does not exceed (1  X  1 2  X  | S | ) ` S  X  1 / 4 . It follows that LC chances of mining the optimal lagged co-cluster upon a p  X  I  X  and S  X  J  X  is at least 3 / 4  X  3 / 4 &gt; 1 / 2 . When it does, from the discriminating property of S we get that I = I  X  T = T  X  . The set J satisfies J  X  J  X  as for any column j  X  J , I  X  and T  X  : From the optimality of ( I  X  ,T  X  ,J  X  ) , we obtain J = J
The total number of iterations is bounded by Theorem 3 as N = ` p ` S = O (1 / (  X  X  | S | )) . The inner for-loops running time is O (  X  ( m | S | + n ) mn ) , where | S | = O (log mn ) (see Theorem 2). In all, the running time is polynomial and independent of  X  : O (( m | S | + n ) mn/ X  | S | ) , for some 0 &lt;  X  &lt; 1 . Experiments reported in Section VII show that | S | can be taken as 0 . 4 log(4 mn ) + 2 and that the number of iterations is always significantly less than the above bound.
The LC algorithm is extensively evaluated using both artificial and real-world data. The use of artificial data, which naturally enables tighter control, is important as it facilitates the examination of specific, isolated properties of the algorithm. Complementary experiments with real-world data include river flow and topographic data. The latter experiments demonstrate the LC algorithm X  X  capability in mining both temporal, i.e., time reading data, and non-temporal datasets.
 A. Experiments with Artificial Data
The advantage of using artificial data is that we have the maximum control in verifying the validity of clusters found (in comparison to real-world data). Specifically, the contri-butions of the experimentation used for the LC algorithm with artificial data are threefold. First, they establish a  X  X est practice X  for the setting of parameters. Secondly, they enable the verification of theoretical bounds and show that these bounds are coarse while better performance is achieved in practice. Finally, through the experiments, the actual run-time of the algorithm is demonstrated. 2 1) Sleeve-width default: An artifact cluster is a submatrix that was not formed due to some hidden regulatory mech-anism but as a mere aggregation of noise. Such artifacts are undesirable as they add irrelevant output. We wish to examine the lagged model from the aspect of finding artifact lagged co-clusters, i.e., whether it is common to mine such artifacts. In order to answer the question, one must specify the desired sleeve-width and the required cluster dimensions. Intuitively, the larger the sleeve-width and the smaller the dimensions, the greater the chance of mining artifact clusters. A sleeve-width of 5% of the matrix range, has been shown to be a good trade-off for a non-lagged model, levelling between mining relevant co-clusters and not having artifact, falsified co-clusters, due to noise [2], [19].
To examine the efficiency of a 5% sleeve-width for mining lagged co-clusters we conducted the following experiment. Random matrices, with values in the range of [100 , 1100] , of various sizes: [100  X  100] , [1000  X  100] and [10000  X  100] were created. For each random matrix, one million random lagged submatrices were created using different dimensions over all possible lags. The sleeve width for each random lagged submatrix was then computed. Table I presents the result of the experiment. The rows and columns rubrics specify the dimension of the random submatrix. The tail rubric specifies the average percentage of cases of which the random lagged submatrix had a sleeve-width of at most 5%. As evident from the table, the larger the submatrix dimensions, the lower the chance of mining an artifact. Even when using very small dimensions (e.g., 5  X  4 ), the probability of mining lagged co-clusters with a sleeve-width of 5% or less is insignificant. Therefore, ordinary mining using practical dimensions, has a non-significant probability of mining artifacts.

It is notable that sleeve-width is highly dependent on the  X  X ature X  of the dataset being mined. Nevertheless, in the absence of any prior knowledge, a sleeve-width of 5% is a good default value to use. 2) Discriminating set size: The discriminating set size, | S | , directly affects the run-time of the LC algorithm. Theorem 2 provides us with the following bound: | S |  X  log(4 mn ) / log(1 / 3  X  0 ) , where  X  0 specifies the ratio between the number of columns in an optimal lagged co-cluster and the number of matrix columns. The bound undesirably depends on  X  0 , a parameter of which the user has no knowledge. In order to get a sense of what the value of | S | is in practice, we conducted the following experiment. We first created random matrices of various sizes: from small ones of [10  X  10] to large ones of [100000  X  100] . We set the dimensions of the cluster size to  X , X   X  { 0 . 1 , 0 . 4 , 0 . 6 , 0 . 8 } . Then we generated a random lagged co-cluster within the specified dimensions and put it at a random location in the matrix, overriding the existing values. Then, a subset of the lagged co-cluster columns was chosen at random 100 , 000 times, and checked whether it was a discriminating set according to Definition 3. We consider a set of size | S | to be discriminating, if it can successfully discriminate in all of the 100 , 000 times.

Figure 3 depicts the relationship between | S | and log(4 mn ) . We observe the reconstruction of the linear relationship derived from Theorem 2. In addition, we obtain from Figure 3 an easy-to-use,  X  0 free, formula for setting | S | : | S | = 0 . 36 log 2 (4 mn ) + 2 . 33  X  0 . 4 log 3) Run-time, Number of Iterations and Hit Rate: The-orem 3 states that for N = ` p ` S  X  2ln 2 4 / (  X  X  | S | we are guaranteed to find an optimal lagged co-cluster, with a probability of at least 0 . 5 . The following experiment was conducted in order to test the practical behavior of the following boundaries: (1) The 0.5 probability boundary; (2) The number of iterations N ; (3) The actual run-time it takes to find a lagged co-cluster (in ms).

For these purposes we generated a random matrix of size m  X  n , m = 1000 , n = 1000 , with values in the range of [100 , 1100] . Inside the matrix, a random lagged co-cluster was randomly placed, overriding the original values. The lagged co-cluster was of a random size  X   X  [0 . 05  X  0 . 9] ,  X   X  X  0 . 3 , 0 . 5 , 0 . 8 } and a sleeve width of 5% . | S | was set to 10, using the previous experimental result (see VII-A2), for a matrix of size 1000  X  1000 . Setting N to the limit given in Theorem 3, and repeating the execution of the algorithm 100 times for each cluster size, we counted: (1) Hit rate: how many times out of the 100 repetition the algorithm managed to precisely mine the planted cluster; (2) Iterations: how many iterations it took in practice to mine the cluster; (3) Run-time: how long (in ms) it took to mine the cluster. The experiments were conducted using the platform: Intel core i7 (920) @ 2.67GHz CPU with 6GB RAM, Windows 7 64 bit. The algorithm was programmed in Java 1.7.

The results obtained are as follows.  X  Hit Rate: While the theoretical bound is set for 50%  X  Number of Iterations: Figure 4 presents the actual  X  Run-time: The boundary specified in Section VI is: To summarize, the LC algorithm managed to mine optimal lagged co-clusters with a probability of 91% which is substantially better than the 50% theoretical guarantee, and did so in a feasible running time, requiring only 30%-60% of the theoretical bound for the number of iterations. B. Experiments on Topographic Data
A topographic map is a 3D representation of a surface (see Figure 5). Such a map is often represented as a Digital Elevation Map (DEM) which is a grid-based sample of the surface elevation. Detailed DEM maps are large datasets, e.g., consider a 100  X  100 km 2 map with 1 meter grid sample. Such a map has 10 10 elevation samples -or in other words such a map has 10 Giga pixels. Manipulating and querying such datasets often requires sophisticated algorithms, and in many cases due to the nature of the problem or the size of the dataset, efficient heuristics rather than exact ones [22].
In this experiment, we wish to examine the capability of the LC algorithm to cluster random viewpoints, i.e., skylines seen from different angles . The experiment consisted of 17 different high-resolution elevation maps representing various types of terrain such as plains, hills, mountains, lakes and dunes. Each map represents a rectangular area of 25  X  25 km 2 , and includes 1 . 625  X  10 6 grid samples. For each terrain, 100-3000 random locations were chosen within it using two steps: (1) Locating 10-50 random points as centers; (2) For each location, ascribing a random center from which the distance is within a random range of [0.5-4] km. In order to overcome minor obstacles within close range of the observer, a random height value in the range of [10-30] meters above ground was assigned to each location. In order to create a viewpoint , each location was assigned a random angle value a 0 in the range of [0,60] degrees representing the skyline starting angle. Each skyline consisted of 300 samples representing the angle range of [ a 0 ,a 0 + 300] degrees. For each angle the maximal z  X  blocking angle was computed, as shown in Figure 6.

The results obtained indicate that the algorithm can mine precise and valid clusters. Figures (7a), (7b) and (7c) present lagged co-cluster results for various terrain maps. Each black dot represents a viewpoint, while red lines represent a lagged co-cluster, binding those viewpoints.

It would seem only natural for this dataset that, in every mined lagged co-cluster, the angle between any two viewpoints will be equal to the lag between them. Figure 8 presents the probability of occurrences of ( Angle  X  Lag ) ( 0 means Angle  X  Lag ) corresponding to the above terrain maps. The graph shows that in all the different terrain maps, there is a very high chance of the mined lag being very close to the angle of the viewpoint.

To strengthen our belief that clusters in such datasets can only be found using the lagged model, we ran a non-lagged co-cluster algorithm [19] on the same datasets, with the same clustering requirements. As expected, in this latter experiment no co-clusters were found.

In conclusion, we have shown that, given a topographic map and a skyline view, it is possible to derive the location from which the skyline was seen using the mining of lagged co-clusters. The LC algorithm performed well in this case, managing to successfully mine lagged co-clusters from different terrain types, with a small number of artifact clusters. The algorithm can thus be used as a classifier in this domain and even as a means of navigation if used continuously [23]. In addition to the derived spatial location, we can also infer the angle of the viewpoint. The failure of the non-lagged algorithm and the success of the LC algorithm implies that the lagged co-clusters were mined due to their lagged nature and not as a result of terrain properties. However, terrains that are very flat (i.e., no reference points) or highly noisy (a slight location change may lead to a significant skyline change due to the effect of hiding and distortions) can be a challenge.
 C. Experiments on River Flow Data
The final dataset used for our experiments was real-time water data obtained from the U.S. Geological Survey (USGS) [24]. We compiled a dataset containing flow read-ings of rivers in the states of New Mexico, Colorado and Nevada. There are 539 rows (objects) each representing a gauge. The columns show the gauge X  X  discharge (ft 3 /s) read-ings for March 2010, sampled every 15 minutes, resulting in 2877 columns.

A relevant lagged co-cluster will naturally be formed by readings from gauges located along the same river, as water flowing downstream will present a correlated flow between different measuring stations with a lag of time.
 The flow of a stream depends on multiple parameters. Many of them change dramatically over time and space: nature (local and global weather conditions, joining and forking rivers, water evaporation, water loss through the river bed, etc.) and human influence (dams, factories, irrigation pools, settlements, pumping stations, sewage systems, etc.). In addition, the data are very noisy due to human and equip-ment inaccuracy and are characterized by a high missing data ratio ( 23% ) caused by various reasons: equipment malfunc-tion, river conditions (effect of ice, flood damage, zero flow), station only recording seasonally, etc. Therefore, mining such datasets for lagged co-clusters is highly complicated.
We consider a cluster to be accurate if all the participating gauges are located within the same basin. We note that it is unlikely to mine a cluster containing all the gauges in a basin. For example, gauges located at the exit of a dam or those which are malfunctioning would not be included.
Using the LC algorithm, we mined 488 lagged co-clusters (see example in Figure 9). Of those, 461 clusters (94%) were in the same state. Manually checking the 27 inter-state clus-ters, we found that at least one gauge is located at the exit of a dam thus changing the flow of the river. 405 clusters (83%) were in the same basin. Manually checking the 56 inter-basin clusters reveals the following reasons for the mismatch: (1) Technical administrative division of basins into upper middle and lower part (12 clusters); (2) Fork in a river: streams merging from different basins (36 clusters); (3) Gauges are located in swampy areas (8 clusters). Therefore, we achieved an accuracy of 94% on a state granularity and an accuracy of 93% on a basin granularity (considering (1) and (2) above as valid), while providing relevant explanations for the artifact lagged co-clusters (dams and swampy areas) enabling future pre-processing exclusion.

As water flows downstream, we expect the lag difference between any two stations to be positive if their altitude difference is also positive. Out of the 488 clusters found, 92% followed the above logic. The other 8% had the following characteristics: (1) Human intervention, e.g., a dam or an irrigation area (35 clusters); (2) Environment factors, e.g., a high lake feeding two streams (6 clusters). Therefore, the lag proved to be a good indication of the direction of water flow.

As with the topographic data, we ran a non-lagged clus-tering algorithm [19] on the above dataset finding only 4 clusters (in comparison to 488 found by the LC algorithm). All clusters were trivial and caused by: (1) Irrigation area; (2) Station position at the exit of a dam; (3) Short distance between stations (i.e., river sampling of 15 minutes is a gross granularity).

The importance of co-clustering is unquestionable and has been thoroughly discussed and demonstrated in cited prior work. The lagged co-clustering model generalizes the co-clustering model, enabling the inclusion of an additional im-portant dimension, a lag aspect , in the regulatory paradigm. The real-life datasets used in the former section were large, highly noisy, contained many missing values and were rich in overlapping clusters. While the LC algorithm managed to find precise, coherent and relevant lagged co-clusters in a practicable time and with almost no artifacts, the use of a non-lagged co-clustering method did not result in any relevant clusters. This encouraging result is important in the sense of model validation and suggests a great potential for mining lagged co-clusters in many other fields of science, technology and medicine (e.g., gene expression data [8], [9], MRI data [1]). It is notable that not only datasets with a time aspect can benefit from such use of the algorithm, and the lagged aspect can have various interpretations (e.g., in the topographic dataset used in our experiment, the lagged aspect is the point of view).

As a generalization of the co-clustering problem, the lagged problem is NP-complete for most interesting optimal-ity measure. The LC algorithm presented in this paper relies on a strong theoretical base, enabling a probability promise on mining an optimal lagged co-cluster and a theoretical bound to the number of iterations it will take. The exper-iments using artificial environment, reported in the former section, reveal a substantially better actual performance in terms of accuracy and efficiency (in comparison to the theoretical bounds). Unlike other algorithms, LC does not assume any specific scoring merit on the mined clusters. Furthermore, the optimal cluster is mined with no additional rows or columns, i.e., ( I,T,J ) = ( I  X  ,T  X  ,J  X  ) . Since the LC algorithm iterations are independent, it can be reconstructed as a polynomial-time approximation scheme (PTAS) in order to increase precision. The use of parallel computing or special hardware can boost the performance even further.
The algorithm has several configurable parameters, for which this paper presents default values. As in non-lagged co-clustering models, one of the key parameters that needs to be set carefully in order to mine meaningful clusters is the sleeve-width. Setting it too high might result in many artifact clusters, while setting it too low might preclude valid clusters. In order to choose an appropriate value for this parameter, one can adopt any of the methods suggested for non-lagged co-clustering (e.g., gradual increase, starting from a relatively small sleeve).

As in non-lagged co-clustering, the ability to mine lagged co-clusters offers important functionalities, e.g., using the tool as a classifier. Nevertheless, unlike non-lagged co-clustering, the ability to mine lagged co-clusters encapsu-lates also a forecasting functionality, which can be highly useful in numerous applications ranging from meteorology to stock markets. While the current results supply some basic forecasting functionality (following the lagged-pattern found), we believe there is far more that can be developed in this aspect in terms of future research.

