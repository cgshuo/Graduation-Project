
Feng Zhang 1 , 2 ,TiGong 1 ,VictorE.Lee 3 ,GansenZhao 4 , and Guangzhi Qu 5 In the real world, there are many scientific and engineering fields handling high dimension data. Typical fields include graphical modeling, video surveillance, face recognition, latent semantic indexing (LSI) and collaborative filtering (CF) recommendation.

These fields share the fact that thei r data can be represented as a matrix in which each row represents an object, and each column denotes a dimension (feature) of the object. However, the number of dimensions may reach to millions while the matrix is very sparse in general. In the fields with sparse data matrix representation, finding a lower rank approximation to the higher rank data is very important. In other words, it is significant to compute summarized dimensions (features) to explore the latent relati onship between objects and features. The benefits brought by the approximation include predicting unobserved data values in data matrices, exploring latent structures underlying data matrices, smoothing errors of observed data, and finally obtaining recommendations with a higher computation performance.

Matrix factorization (MF) tries to fulfill the work of approximation. It de-composes a matrix into a product of matrices. There are many ways to perform MF. For example, QR-factorization, LU-factorization, and Singular Value De-composition (SVD) are the most famous ones in linear algebra [5]; Non-negative factorization (NNF) is another popular one which has been used in many fields in recent years [10,17].

In the field of CF recommendation, MF has been proven to be a very successful method since the Netflix Prize competition [1,8,7,9,18,4]. The key challenge in recommendation based on MF is that the factorization process is very time-consuming. It is unrealistic to adopt such techniques to scenarios where users, items, and ratings grow constantly. In order to address this issue, the incremental version of MF seems to be a promising solution.

Inspired by the incremental nature of stochastic gradient descent (SGD) and the underlying techniques to compute incremental SVD, we introduce comprehensive methodologies to perform online CF recommendation based on incremental MF. The methodologies leverage only newcomers to incrementally update the training model, without having to rebuild the whole model. Extensive experiments have been carried out to show that the proposed methodologies perform well based on two results: the comparable recommendation accuracy an d the excellent incremen-tal properties. 1.1 Contributions 1. To our knowledge, this is the first work to apply the Moore-Penrose pseu-doinverse technique [15] to implement incremental SGD-based MF and show its superior performance. 2. We propose comprehensive solutions to perform online CF from the aspects of new-user, new-item and new-rating. MF is a popular method to implement model-based CF [12,19]. In the Netflix Prize competition, Koren et al.[1,8] adopted MF methods to win one million dollars provided by the organizers. In addition to the winner, many participants there used similar MF methods to improv e the predictive a ccuracy of recom-mender systems. Since then a new wave of recommender system research based on MF has taken off [7,9,18,4].

There are already some studies fulfilling the incremental training. Sarwar et al. do an early work for this [14]. They used the folding-in [3,2] method to incrementally achieve SVD-based CF . However, SVD is not considered as a practical technique to implement CF due to the sparsity problem and its poor scalability property. MF techniques can be implemented even under a very sparse data scenario and have good scalability performance, which are the major reasons why they are so popular and are utilized to be a replacement of SVD to perform CF.

Two early works in the information retrieval community [3,2] demonstrate how to use folding-in techniques to update a document index while new documents or new terms are added into the document-term space. However, the folding-in techniques are used in the incremental SVD updates in these studies and they do not consider the new-rating case. Our paper is the first one to apply the folding-in techniques to the SGD-driven MF and also consider the new-rating case.

Rendle and Schmidt-Thieme [13] make a contribution to online recommen-dations based on MF models. They simply and directly applied SGD to learn regularized kernel MF models. Recently, Lu o et al. [11] report superior results outperforming Rendle and Schmidt-Thiemes X .

Luo et al. [11] transformed the gradient descent rules into new ones suitable to perform incremental updates. They designed two incremental regularized matrix factorization (RMF) models: the Increm ental RMF (IRMF) and the Incremental RMF with linear biases (IRMF-B). Their e xperiments on two large, real datasets suggest positive results, which proves t he efficiency of their strategy. However, their studies are based on an assumption that training examples are learned simultaneously, which may affect the r ecommendation accuracy, as shown in the implementation of ourselves. 3.1 Regularized Matrix Factorization Regularized Matrix Factorization is first introduced by Webb 1 andisanim-plementation of latent factor models. The core idea in regularization is using some parameters such as learning rate and regularization factor to reduce the complexity of learning procedure and avoid overfitting.
 Definition 1 (Regularized Matrix Factorization). Given a rating matrix R  X  R m  X  n , where each row vector denotes a user, each column vector denotes a column, and each entry r ui  X  R denotes user u  X  X  preference on item i . All known is to construct a low rank approximation of R .Let f denote the dimension of the feature space, P  X  R m  X  f denote the user feature matrix where each row is a user rating vector p u ,and Q  X  R f  X  n denote the item feature matrix where each column represents an item rating vector q i . The approximation of the rating by user u on item i can be confined to the inner product of the corresponding user-item feature vector pair, shown as Equation (1):
The values of p u and q i can be learnt by solving a regularized least squares problem, shown as Formula (2): function to evaluate the parameters p u and q i in order to  X  X est X  fit the original overfitting by penalizing the magnitudes of the parameters.

There are studies suggesting that incorporating three ratings,  X  ,theoverallav-bias of item i from the average, could contribute to improving the prediction accu-popularity of items. Then the prediction rule shown as Equation (1) turns to the following:
Correspondingly, the regularized least squares problem associated with Equa-tion (3) becomes to:
Let e ui = r ui  X   X  r ui , then the optimization problem denoted by (4) can be easily solved by SGD techniques. 3.2 Incremental MF The essence of the above method is to employ MF to approximate the original matrix.

Deduced from the above analysis, the approximation is equivalent to an opti-mization problem in the Euclidean s pace, which can be formulated as: where || . || F denotes the Frobenius norm.
 The process of MF shown in (6) can be explained as a space projection process. There is a dual view of the projection. 1. If R is viewed as a set of row vectors (user vectors), then Q ,whichis also a set of row vectors, should be view ed as base vectors in a new space. The goal of MF is to project R onto the new base of Q . In the new space, P is the coefficient matrix, in which each row cons ists of coefficients corresponding to the base vectors of Q . 2. If R is viewed as a set of column vectors ( item vectors), then it is projected onto a new subspace of P , in which a set of column vectors form the base. In this case, Q is the coefficient matrix, in which ea ch column consists of coefficients corresponding to the subbase vectors of P .

If the first view is taken, and a new user u signs on, then the row vector for u needs to be projected onto the new subspace with a base of Q . The challenge is to figure out the coefficients under the new subspace. where x is the column vector of coefficients. X + is the Moore-Penrose pseudoin-verse of matrix X .

If the second view is taken, and a new item i is added, then the column vector for i needs to be projected onto the new space with a base of P . The challenge is to figure out the coefficients under the new space as well.
Let e = || R  X  PQ || 2 F , a measure of the quality of MF. It is obvious that the lower value of e , the higher quality of the factorization, and vice versa. 3.3 Incremental MF with Biases Section 3.2 considers the cases where only MF is involved. This section discusses the online CF cases where biases, together with MF, are involved.

If MF incorporates the bias parameters of Equation (3), then Equation (6) canberewrittenasEquation(10): where M =[ a ij ] is a constant matrix denoting the global average, i.e., a ij =  X  ; U b i is a constant row vector storing user biases; storing the item biases and each b i 1 stores an item bias.
Note that in U each row is a constant row vector, and in I each column is a constant column vector.

Now we consider how to perform incremental predictions based on Equation (10). To achieve this, methods to perform incremental computation of all the variables in the equation have to be found out. The problem could be separated into two parts: the first is dealing with the parameters of M , U ,and I ;andthe second is dealing with the MF part PQ .

In order to calculate M , U ,and I , formulas similar to those in [11] are adopted to compute  X  , b u and b i . n and
Once a new rating r u,i is issued,  X  can be updated by: b u can be updated by: and b i can be updated by:
When a new user u signs in, then  X  can be updated by:
In this case, for each i  X  R ( u ), b i can be updated by Equation (16); and b u can be computed by: In order to calculate the MF , three scenarios are considered.
 1. New-User signing in When new users sign in, then based on Equation (10), Equation (8) turns to: 2. New-Item Added
When new items are added (incremental column growth), then Equation (9) turns into: 3. New-Rating issued
In this case, the user and item involved are both the existing ones but the user now issues a new rating for the item. This case is different from the new-user and the new-item ones. Since it is observed that SGD inherently is an incremental method, this property is simply utilized to perform the incremental updates. When a new rating arrives, the SGD iterative procedure is used to achieve the Then they are compared with the previous set of parameters,  X  , b u , b i , p u and q . The set of parameters leading to high er accuracy is returned as the answer. Four major experiments were carried out: 1. Experiments to judge the influence of latent number of features on the perfo rmance; 2. Experiments to determine the basis size of starting matrix; 3. Experiments to evaluate the performance of online CF: user incremental (new-user) perfor mance, item incremen tal (new-rating) performance and rating incremental (ne w-item) performance; 4. Experiments to compare with the state-of-the-art results for the incremental recommendation precision.
 Experiments have been carried out using two popular benchmark datasets: MovieLens 100K and MovieLens 1M 2 , whose characteristics are described in Table 1.

All the experiments were conducted on a laptop with an Intel Core i3 CPU and 4.00GB RAM, running WIN7 32 OS. And the programming language is Python 2.6.6 Python.
 The experiments involve some paramet ers, whose values were set as shown in Table 2. 4.1 Dimension of the Feature Space The goal of the first experiment is to determine the dimension of the feature space, which is a key parameter to decide the quality of the MF. To decide such parameter, batch training and incremental training experiments were performed based on different dimension values. The results are consistent with those in [8]: increasing f benefits the accuracy of predic tion in most cases. The reason may be with f increased, a better matrix appr oximation is performed. In our settings, it seems that for both the MovieLens 100K dataset and the MovieLens 1M dataset, 100 is a suitable f value to conduct the MF. 4.2 Basis Size In order to perform incremental MF and online CF, the basis size of starting matrix has to be determined. The basis size may also affect the CF performance as well. Experiments were carried out to decide the basis size. For MovieLens 100K, the results show when user basis size reaches to 500, RMSE measure remains stable and changes little, so 500 was chosen as a suitable user basis size for this dataset. For the similar reason, 800 was chosen as the item basis size and 50,000 the rating basis size. For MovieLens 1M, with quite the similar reason, 2,500 was chosen as the user basis size, 1,000 the item basis size and 500,000 the rating basis size. Due to the space limitation, the above two detailed results are not shown here. 4.3 Performance of Incremental Properties In this part of experiments, the incremental performance of MF based CF was evaluated. The performance includes the prediction precision RMSE and the elapsed time. The experiments consider th e three incremental c ases of new-user, new-item, and new-rating.

To compare them with the batch model of matrix factorization CF, while evaluating the incremental properties, the matrix factorization CF based on batch model training using the current size of data is conducted. 1. New-User Case
For the MovieLens 100K dataset, the experiments were carried out starting with a model size of 500 users and rising up to 900 users with an increment of 50 users. And for the dataset of MovieL ens 1M, the experiments were started with 2,500 users and risen up to 6,000 users with an increment of 500 users. 2. New-Item Case
For the MovieLens 100K dataset, the experiments were carried out starting with a model size of 800 items and rising up to 1,600 items with an increment of 100 items. And for the dataset of MovieLens 1M, the experiments were started with 1,000 items and risen up to 3,900 items with an increment of 500 items. 3. New-Rating Case
For the MovieLens 100K dataset, the experiments were carried out starting with a model size of 50,000 ratings and rising up to 100,000 ratings with an increment of 5000 ratings. And for the dataset of MovieLens 1M, the experiments were started with 500,000 ratings and risen up to 1,000,000 ratings with an increment of 50,000 ratings.

For the cases of new-user, new-item an d new-rating, the results are respec-tively plotted in Fig. 1, Fig. 2 and Fig. 3. With the data size increased, better incremental performance is obtained: a slightly better RMSE and a significantly less elapsed time. The reason for the imp rovement in runtime is: incremental computation definitely costs much less time than batch computation. However, it seems surprising to see that the incremental version of MF works better than MF with re-computation. Overfitting could explain such results. Since the datasets contain some noises thus letting all data involve in the batch computation may not always ensure a better performance over the computation in an incremental model also outperforms the batch mode l in recommendation accuracy. However, our method is simpler and has a better per formance, as the experimental results of the following section show. 4.4 Comparison with State-of-the-Art Results of this part of experiments are compared with those generated by the incremental methods prop osed in [11]. It seems that [11] does not differentiate the cases into new-user, new-item and new-rating since their methods take all the three cases as rating increments. To compare their methods with ours, we reimplemented their methods under the three cases with our same controlled parameters.

The results are shown in Fig. 4 and Fig. 5, which demonstrate that the meth-ods proposed in this paper slightly outperform the optimized one in [11], IRMF-B. We have stated the reason in the sect ion of related work. IRMF-B assumes that each training examples are learned simultaneously, which may destroy the learning regularities and lea d to less optimized results.

Also, as stated in the section of related work, the results reported in [11] are better than those in [13], so it doesn X  X  need to replicate the experiments of [13] and compare our results with theirs. Comparing with [11] gives enough evidence to show the superiority of our methods. Some previous studies solve the increm ental CF problem only from the aspects of new-user and new-item using the classic fold-in method. For solving the new-rating problem, it seems that a different mechanism has to be invented. This paper proposes new methods to solve the problem in an integrated manner. 1. They do not need to rebuild the existing model and do true incremental recommendation. 2. The experimental resu lts demonstrate their superior perfor-mance compared with the benchmark and state-of-the-art results. The methods proposed seem simpler than the previo us ones, but they are effective and effi-cient, which proves an idiom: simple is beautiful.
 Acknowledgments. The study is partially supported by the Natural Sci-ence Foundation of Hubei Province under Grant No. 2015CFB450, the enterprise-funded latitudinal resea rch project under G rant No. 2014196221, and the Guangzhou Research Infrastructure Development Fund under Grant No. 2012224-12.

