
Standard k -nearest neighbor (k-NN) is a widely a pplicable clustering, outlier detec-tion and classification technique that demons trates high recall accuracy; see Dasarathy (1991), Wettschereck (1994), and Hodge and Austin (2004) for an overview of k -nearest neighbour techniques. k-NN uses sim ilar procedures for clustering, outlier detection and classification: examining the distances to the nearest neighbours. To cluster data, the k-NN determines the distance to a record X  X  neighbours using some suitable distance metric, such as Euclidean distance, so the cluster bounds may be identified. During outlier d etection (Knorr and Ng 1998), the k-NN calculates the distance between a record and its nearest n eighbours. Any points with a high distance to its nearest neighbours is designated an ou tlier. For classification, k-NN examines those points in a particular data space lying nearest to a query point. K-NN then uses the respective classifications of these nearest neighbours to determine the class of the query point. There are various approaches described in Dasarathy (1991) and
Wettschereck (1994) for determining the class of a point from the set of classes of its nearest neighbours, such as majority vote or weighted majority vote. and Ng 1998) with respect to the number of records n in the data set as the ap-proach calculates the distance to each reco rd for every record in the data set. The computational complexity is also direc tly proportional to the dimensionality of the data m so k-NN has O ( n 2 m ) runtime. As a result, there is a practical upper limit to both the number of records and the data dimensionality that may be processed even on modern high-speed computers dependent on the processor time available. maintaining the recall accuracy of a sta ndard k-NN procedure to allow extremely large data sets to be processed. Two methods for speeding k-NN are prototype se-lection and attribute selection (Skalak 1994). A large data set can be stored as a few lower dimensional prototype vectors, thus decreasing the k-NN processing time ex-ponentially (Skalak 1994). However, prototyping must be applied carefully and selec-tively, as it will increase the sparsity of the distribution and the density of the nearest neighbours. Attribute selection demonstr ates high accuracy when coupled with k-NN (Aha and Bankert 1994) but is computationally expensive due to the combinatorial problem of attribute subset selection. In this paper, we focus on speeding the under-lying k-NN process and we note that both prototyping and attribute selection will speed our k-NN further.
 sification accuracy of k-NN, as that has been well documented elsewhere (for ex-ample, Wettschereck (1994)) but to show the speedup achieved using the Advanced
Uncertain Reasoning Architecture (AURA (Austin 1995)) to implement k-NN. The approach described in this paper is an enhancement of the binary neural network k-NN classifier described in Zhou et al. (1999), Hodge et al. (2003) and Weeks et al. (2003), which is generically applicable to any pattern classification or clustering task. It quantises numeric values to form binary vectors, which are matched in the bi-nary neural network. Zhou et al. (1999) demonstrated that the approach outperformed
Multi-Layer Perceptron (Bishop 1995) and Radial Basis Function (Bishop 1995) networks with respect to speed and recall accuracy on a classification task. We demonstrated that a previous version of the technique (Hodge et al. 2003) was faster than standard k-NN with much lower computational growth while achieving com-parable accuracy. We showed in Weeks et al. (2003) over 99% accuracy for our
AURA k-NN compared with standard k-NN. We used the AURA k-NN to select a set of candidate matches and then postpro cessed the candidate matches using stan-dard k-NN. This paper augments the techni que to further speed the recall by a fac-tor of 1.8 using a more accurate squared E uclidean distance approximation within the binary neural network so that no postp rocessing using standard k-NN is neces-sary.
 quantisation preserves the Euclidean distances between the attribute values and thus whether our binary neural approximation recalls the correct nearest neighbours as identified by a conventional Euclidean distance approach. The second aim is to demonstrate the speedup achieved by using AURA compared with processing the data set using standard Euclidean distance k-NN. We also identify the ideal config-uration for the two data sets in this paper and provide heuristics for determining the ideal configuration for other data sets.
In the remainder of this paper, we provide a detailed overview of binary neural networks in Sect. 2; AURA and our numeric-to-binary quantisation method in Sect. 3; a description of the evaluation methodology and the results in Sect. 4; a detailed analysis and comparison of the methods evaluated in Sect. 5; and the conclusions we have drawn from our analyses in Sect. 6.
The AURA C++ library provides a range of classes and methods for rapid par-tial matching of large data sets (Austin 1995). AURA belongs to a class of neu-ral networks called Random Access Memory (RAM-based) networks. RAM-based networks were first developed by Bledsoe and Browning (1959) and Aleksander and Albrow (1968) for pattern recognition and led to the WISARD pattern recog-nition machine (Aleksander et al. 1984). The y include Hopfield associative mem-ories (Hopfield 1982). See also Austin (1998) for a detailed compilation of RAM methods.

RAMs are founded on the twin principles of ma trices (usually called correlation matrix memories (CMMs)) and n -tupling to store associations between inputs I outputs O j as shown in Fig. 1. Each matrix accepts m inputs as a vector or tuple addressing m rows and n outputs as a vector addressing n columns of the matrix.
During the training phase, the matrix weights M lk are incremented if both the input row I jl and output column O jk are set. During recall, the presentation of vector I elicits the recall of vector O j as vector I j contains all of the addressing information necessary to access and retrieve vector O j from the matrix.

In RAM-based networks, training is thus a single-epoch process with one training step for each input X  X utput association prese rving their high speed. This also makes associative memories computationally si mple and transparent with well-understood properties. In contrast, in most conventional neural networks used for classification such as MLP or RBF, Bishop (1995), training takes time and the resultant network is effectively a black box . RAM-based networks are able to partially match records during retrieval. Therefore, they can rapidly match records that are close to the input but do not match exactly. This partial matching is a central concept for our binary k-NN described in the following paragraphs.
The AURA methodology has introduced a thresholding technique, which we describe later, that can retrieve the top n matches, unlike the other RAM-based networks. Dur-ing recall, AURA correlates the inputs to the stored matrix weights, sums the matrix columns and thresholds the summed column totals to retrieve sets of the best match-ing records. We have coupled this with a qua ntisation technique to map numeric data onto the binary inputs needed by the CMM. This rapid training, computational sim-plicity, network transparency, partial m atch capability and thresholding coupled with our quantisation technique make AURA ideal to use as the basis of an efficient k-NN implementation. A more formal defin ition of AURA, its components and methods now follows.
 uses binary and integer-valued input I and output O vectors to train records into the CMM and recall sets of matching records from the CMM. For the methodology described in this paper, we  X  Train the data set into the CMM, which indexes all records in the data set and  X  Apply query records to the CMM in turn and retrieve a set of the best matching
In our k-NN implementation, input vectors represent quantised records during CMM training and output vectors uniquely identif y each record in the data set. The training process is given in (1).

Training is a single-epoch process with one training step for each input X  X utput as-sociation (each I j  X  O T j in (1)), which equates to one step for each record in the data set.
The CMMs in AURA require binary input vectors for training so we need to quantise (bin) and encode any numeric attributes. Our approach is to map the attribute values onto bins, each of which indexes a specific row in the CMM. Each individual bin thus maps onto an integer as in (2) and (3), which identifies the bit to set within the CMM input vector and thus corresponds to a row in the CMM. Equation (2) represents the quantisation algorithm, where a set of input values for attribute f map onto each bin, which in turn maps to a unique integer to index the CMM row. We then set the appropriate bit in the input vector as in (3). The range of attribute values mapping to each bin is equal.
 where i  X  AttributeValue f , cardinality ( Z f )  X  cardinality tive integer offset within the binary vector I j for each attribute f ,where offset offset f + numberOfBins ( f ) and  X  is a many-to-one mapping and a one-to-one mapping and  X  sets the appropriate bit in the vector.

If a new record is evaluated with an attribute value that lies beyond those pre-viously seen, it is placed in an extreme-va lue bin at the appropriate end of the data range for values either larger than or smaller than those previously processed. Once sufficient records have values mapping to e ither of these bins, we recalculate all bin boundaries for that attribute to ensure that the set of bins covers the range of values. We do not show the extreme-value bins in the figures in this paper for simplicity.
The data sets used in our evaluation are both unsupervised (no class attributes) and comprise solely numeric attributes so we require an unsupervised binning method (Dougherty et al. 1995; Witten and Frank 1999). There are two unsupervised bin-ning methods: equiwidth binni ng and equifrequency binning and we use the former.
Equiwidth binning aims to subdivide the a ttributes uniformly across the range of each attribute. The range of values is divided into b bins such that each bin is of equal width and the number of records E mapping to a particular bin is proportional to the number of records n and inversely proportional to the number of bins b . The even widths of the bins prevents distortion of the approximation of the squared
Euclidean distances in the CMM compared with, for example, equifrequency binning used previously in AURA (Zhou et al. 1999), which aligns the bin boundaries so each bin contains an approximately equal number of records. This then prevents regions of CMM saturation and evens the recall speed as all rows have an equivalent number of set bits. However, variable-width binning distorts the Euclidean distance approximation, as there are a larger number of bins where the attribute values are clustered and relatively few bins representing the outlying values, so the spread of attribute values represented by the bin varies markedly. This is particularly important for normally distributed attribute values where there will be many bins near the mode value and very few near the extremes. This distortion of distances is particularly germane for distance-based machine-learning techniques such as k-NN. It lowers the recall accuracy of the AURA k-NN com pared with fixed-width binning when compared with the results from a standard k-NN by up to 3% from an evaluation we performed previously (Hodge et al. unpublished). See Dougherty et al. (1995) for an overview of supervised-and unsupervised-binning techniques.

We vary the number of bins ( b ) in our evaluations to pinpoint the ideal config-uration with sufficiently high recall yet acceptable run time, as more bins slows the recall as more CMM rows have to be processed. Choosing a suitable value for b is more difficult for unsupervised data compared with supervised data, so a heuris-tic approach is necessitated, Witten and Frank (1999) recommend a cross-validation approach. We provide heuristics for selecting the number of bins in Sects. 5 and 6.
Once the bins and integer mappings ha ve been determined, we need to map each record D onto a binary input vector I j . Each attribute D section of bits in the binary vector as in (5), (6) where D f in record D .

Each concatenated binary vector repres ents a record from the data set and forms an input I j to the CMM. The CMM associates the input with a unique output vector O j during training, see (1). Each output v ector is orthogonal with a single bit set corresponding to the records position in the data set, the first record has the first bit set in the output vector, the second and so on.
To recall the nearest matches for a query record, we first produce an input vector by quantising the target values for each attrib ute to identify the bins and thus CMM rows to activate as in (5) and (6) and Subsect. 3.1. One problem with this quantisation is the boundary effect. The bins have hard boundaries, so records lie within one bin only. Hence, for a particular value, the distance to other points in the same bin may be greater than the distance to a point in a neighbouring bin. For example, if the binning boundaries lie on whole numbers and the query value is 4.99, then 4.01 will lie in the same bin yet 5.01 will be in th e adjacent bin. However, 5.01 is much closer to 4.99 than 4.01.
 unpublished) enhanced (Zhou et al. 1999) to set the bit representing the bin for the retrieve any values that lie just across the bin boundary and hence may be closer. We improved this previously by exploiting AUR A X  X  ability to handle integer-valued input vectors (Hodge et al. 2003; Weeks et al. 2003). The technique called integer pyramid (or triangular) uses one triangular kernel per attribute. Each kernel is superimposed onto the CMM input vector to form a concaten ated kernel input vector for recall from the CMM. During recall, the summed intersection of the kernels contains discrete concentric patterns of equivalent CMM score. These scores represent the quantised city block distance (see (7)) and are at a maximum where the target values for all F attributes coincide and decrease with distance from the target values.
The integer pyramid achieved 99.49% accuracy and 100% accuracy for the two data sets used in this paper (Weeks et al. 2003) compared with 17.01% and 63.51% re-spectively for the 3-bits set. In this paper, we improve the technique further using a parabolic Kernel, which is analogous to quantised squared Euclidean distance (see (8)).
 rather than city block distance, as in Fi gs. 2 and 3. It has been shown to be more accurate than integer pyramid (Weeks et al. 2003), where the parabolic kernel (called
SemiCircle in the paper) achieved 99.57% accuracy and 100% accuracy compared with 99.49% accuracy and 100% accuracy, respectively, with the integer pyramid (triangular) kernel for the two-data sets used in this paper.

The parabolic kernel value for each bin ( bins fk ) in attribute f is given in (9), where the target value X  X  bin is bins ft , max ( n ) is the maximum number of bins across all attributes and n f is the number of bins for attribute f . All kernels have the same using  X  f to spread the kernel across the range of each attribute in turn within the
CMM input vector. The parabolic kernel is then superimposed onto the input vector as in (10).

We move the kernels to match the input values unlike, RBF (Bishop 1995), where the kernels are fixed. The bin containing th e query (target) value effectively receives the highest score, with the score decreas ing monotonically as the distance between the median bin, then the parabola is offset and truncated at one, end as in attribute of Fig. 4, where the parabola is centred near the top and truncated at the top. If all attributes have an equivalent number of bins, then the superimposed parabolas will be identical. However, if the number of bins varies across the attributes, then the width of the parabolas varies accordingly due to  X  across the attribute width as shown in Fig. 4.
To retrieve the best matching records for a particular query record (represented by integer-valued input I k ) using parabolic kernels, the AURA k-NN effectively calcu-lates the dot product of the input vector I k and the CMM, computing a positive integer-valued output vector O k (the summed output vector) as in (11) and Fig. 4. The summed output O k is thresholded to produce a binary output vector as in Fig. 4.
We use the L -max threshold (Austin 1995). L -max thresholding essentially retrieves at least L top matches, i.e. at least L nearest neighbours. L -max thresholding sets a bit in the thresholded output vector for every location in the summed output vector that has a value higher than a threshold value. The threshold value is set to the highest integer value that will retrieve at least L matches. For k-NN, L is set to the value of k ,where k is the number of nearest neighbours required.
AURA can identify the k -nearest matching records by the bits set in the thresholded output vector. In the work here, bit 0 in the output vector corresponds to the first record in the data, bit 1 to the second record and so on. Therefore, if bit the thresholded output vector, then the first record is a match.
Comparing the conventional k-NN and the AURA-based approach, the nearest neigh-bour in the standard k-NN is the record with the lowest squared Euclidean distance.
Paradoxically, in the AURA k-NN, the best matching record (nearest neighbour) is the record with the highest score in the summed output vector. Also, the CMM-based approach calculates the k -nearest neighbours by traversing rows in the matrix, it is row based. The standard k-NN is similar to traversing columns in the same CMM with floating-point matrix entries rather than the binary entries of the AURA
CMMs, so it may be considered column based. The nested loop for standard k-NN for a single query record is For all records (columns) For all attributes (rows) In contrast, the loop for the CMM for a single query record is For all attributes (rows)
For all records (columns) In this section, we analyse the processing time of the standard k-NN versus the
AURA k-NN, the recall accuracy achieve d by AURA and the scalability with respect to data size of the two techniques.
 All techniques use C++ algorithms compiled with GNU g++ v2.95.3 using the Solaris8 OS and run as command-line applications on a 750 MHz SPARC-based Sun
Blade1000 with 4 GB RAM. The AURA methodology uses the AURA C++ class library (AURA Web Page 2003), which provides classes and methods for CMMs and thresholding.
We use two data sets to assess the techniques, chosen to contain large numbers of records with numeric attributes. The RE AL data set contains 200,000 records with 14 continuous-valued attributes 0 . 0  X  x  X  1 . 0 generated using a Java random-number generator. The IBM data set contains 20,000 records with 9 integer-valued attributes, where the attribute ranges vary from 0  X  x  X  4to50 , 000  X  using the IBM Data Generator (2003). The first data set analyses the recall accuracy and recall precision of our quantisation and indexing due to the fine-grained differ-ences between attribute values. The second data set analyses the recall accuracy of our quantisation and binning compared with the standard squared Euclidean distance when the data ranges vary widely.
 every pair of records) storing each record X  X  k nearest neighbours in an ordered linked list k elements in length. The list holds the indices of the k nearest records and their respective distances from x , sorted in ascending distance order. It has computational growth O ( n 2 ) with respect to the number of records. The algorithm is For each record x For each record y
We use range normalisation in our squared Euclidean distance calculation to ensure that all attributes are in the range 0 to 1 and, hence, each attribute produces an equal weight in the squared Euclidean distance calculation.

We provide two timings for the standard k-NN data structure generation. We recorded the time to read in the entire data set from a file on disk, generate the k-NN data structure with n 2 distance calculations, where each v ector is compared with all others except itself to calculate its nearest neighbours and output the 100 nearest neighbours to a file on disk. We then implemented a speed up by exploiting the commutativity of distance ( SquaredEuclidDist ( x , y )  X  SquaredEuclidDist the training time for this approach. Once we have calculated the distance between records x and y ,( SquaredEuclidDist ( x , y ) where x &lt; neighbour list of x if it is closer than the most distant neighbour in the current list, as previously, but we also add x to the nearest neighbour list of y if it falls in the top k neighbours. Essentially, we only need calculate the half distance matrix, where x &lt; y ,using n 2 from a file on disk and the time to train the data into the CMM. This includes, for each record in the data set in turn, the time to convert the record to a CMM input vector and superimpose the parabolic kernels onto the vectors, the time to sum and threshold all columns of the CMM, the time to sort the matches into order and the time to output the 100 nearest neighbours to a file on disk.
 and 999 bins. The number of bins alters the specificity of the matching by varying the number of records with equivalent scores. The number of bins is inversely pro-portional to the number of values in each bi n from (4) and, hence, the number of records that will map to each bin per attrib ute. We aim to identify the optimum num-ber of bins across various data sets, which is a trade off between retrieval time and granularity. Too many bins will take longer to process but too few bins will mean too many equivalently scored matches during recall, thus slowing recall as all matches are processed and also lowering accuracy by reducing the scoring granularity.
Table 1 lists the processing time (tr aining time plus recall time) for the n standard techniques and the AUR A k-NN with 49, 99, 149, 249, 499 and 999 bins for the REAL (200,000 records with 14 continuous-valued attributes) and IBM (20,000 records with 9 integer-valued attrib utes) data sets. We ensured the process was as similar as possible for all algorithms under investigation to allow an unbiased comparison.
We saved the two nearest-neighbour lists produced by the standard k-NN (with k = 100) to a file, one list for REAL and one list for IBM. These lists provide a benchmark to compare the nearest neighbour list for the AURA technique. We recalled and listed the 100 nearest neighbours identified by the AURA technique for the first 20,000 records from the REAL and all 20,000 records from the IBM data set for the various numbers of bins listed above. The REAL data set is randomly ordered so the 20,000 records provide a statistically sound evaluation. From these 20,000 lists of 100 nearest neighbours, we counted the number of nearest neigh-bours common to this list and the corresponding standard k-NN list for that record.
We calculated the average number of common records for each of the 20,000 com-parisons to give the average recall percen tage for each of the six configurations of
AURA. We repeated this evaluation for the first 50 nearest neighbours and then the first 25 nearest neighbours in the lists.

Table 2 lists the percentage recall accu racy when the nearest neighbour lists re-trieved by the AURA k-NN with 49, 99, 149, 249, 499 and 999 bins are compared with the corresponding list for the standard k-NN using both the REAL and IBM data sets for 100 nearest neighbours, 50 nearest neighbours and for 25 nearest neigh-bours.
Using subsets of the REAL data set with 40 K, 80 K, 120 K, 160 K and 200 K records, we noted the processing time for the n 2 and n 2 2 k-NN techniques and the AURA k-NN with 149 bins and calculating 100 nearest neighbours. We elected to use 149 bins for the AURA k-NN, as this performed well for both the REAL and IBM data sets (see Tables 1 and 2). We note that, in the scalability test, we fix the number of bins to note the increase in processing time. In a real-world system, the user may elect to increase the number of bins proportionally as the size of the data set grows to maintain separability of the data values.
 the AURA k-NN with 149 bins when the REAL data set size increases from 40,000 to 200,000 records. Figure 5 plots the results from table 3 and extrapolates the trend line of the plots to allow the reader to view the computational growth (plot gradient).
For the REAL data set, the AURA k-NN ranges from 4.0 times faster with 49 bins to 3.6 faster with 999 bins compared with n 2 standard k-NN and ranges from 2.5 to 2.3 faster compared with n 2 2 standard k-NN. For the IBM data set, the AURA k-NN ranges from 4.0 times faster with 49 bins to 3.3 faster with 999 bins compared with n 2 standard k-NN and ranges from 2.7 to 2.3 faster compared with k-NN.

Weeks et al. (2003), which takes 156 seconds to process the IBM data set using 149 bins and 23,249 seconds for the REAL data set with 149 bins compared with 88 seconds and 13,332 seconds, respectively, for the method in this paper.
The recall accuracy for the AURA k-NN is 97% for the REAL data set and 99% for the IBM data set when the first 25 nearest neighbours are compared. It is 91% for the REAL and 99% for the IBM when the first 50 nearest neighbours are compared and 84% for the REAL data set and 97% for the IBM data set when the 100 nearest neighbours are compared. This compares with 99.59% for the REAL and 100% for the IBM with the AURA k-NN in (Weeks et al. 2003) when 100 nearest neighbours are compared. We note that the largest percentage of the nearest neighbours omitted by the AURA k-NN are toward the end of the nearest neighbour list (i.e. the 90th X 100th nearest neighbours) from Fig. 6.

Even though we are using the kernels to overcome the bin-boundary problem alluded to in section 3.2, for the continuous-valued REAL data, the bin boundaries are more of an issue than for the integer-valued IBM data set. The integer values are discrete so the minimum distance between any two integers is 1. As we increase the number of bins, we are tending toward one integer value per bin and hence no bin-boundary problem. The real values are accurate to 16 decimal places, so there may be only 0.0000000000000001 between two data points separated by a bin boundary, but from equation 4, the maximum intrabin distance is width the recall accuracy achieved for the REAL d ata set is just below the recall accuracy achieved for the IBM data set for all configurations. We can see from Table 2 that the recall accuracy increases with the numbe r of bins for REAL-25 but conversely decreases for REAL-100. For REAL-50, it increases, then decreases with respect to the number of bins. The recall accuracy incr eases consistently as the number of bins increases for all configurations of the IBM data set. For a real-valued data set, we posit the rule-of-thumb more bins for fewer neighbours but conversely fewer bins for more neighbours. For a discrete data set, we posit the rule-of-thumb more bins is better. However, we note that there is a trade off between the number of bins and the processing time. Even though increasing the number of bins for REAL-25 and all IBM configurations increases the accura cy, it also increases the processing time.
We would recommend 149 bins as the optimum value for these two data sets with respect to this trade off, although there is little difference between the recall accuracy between 99 bins and 999 bins within each configuration. We would recommend a cross-validation to ensure maximum accuracy if time permits starting with 149 bins.
 times faster than the standard techniques with 200,000 records. Although growth is still near O ( n 2 ) , due to the nature of k-NN, the faster initial processing time means we can process up to approximately 540,000 records in 100,000 seconds, whereas n 2 k-NN can only process 270,000 records in 100,000 seconds, at which point the computational growth becomes steep for n 2 . If we increase the number of records by 20,000, we increase the processing time for n 2 classification or outlier detection as described in this paper is inherently O to the following loop embedded in the algorithm: For all n records
Calculate distance to all other ( n  X  1) records .
This paper has demonstrated that AURA k-NN has faster recall speed than standard techniques even when optimisations that effectively halve the number of calculations are included within the standard technique. AURA scales better than the standard technique and the difference between the recall times (AURA time time) increases as the data size increases.
 viously postprocessed with a standard k-NN (Hodge et al. 2003; Weeks et al. 2003) a set of candidate matches extracted from the CMM (usually of size 10 effectively use AURA to minimise the s earch space for the k-NN and then use the more accurate but slower Euclidean technique to process the candidate matches. This improves recall accuracy with 100 nearest neighbours to 99.59% and 100% for the
REAL and IBM data sets, respectively (Weeks et al. 2003), to the slight detriment of the recall speed but (Weeks et al. 2003) is still faster than standard k-NN. Alterna-tively, if speed is the issue, the kernel appr oximation detailed in this paper emulates the distances sufficiently accurately fo r 50 nearest neighbours or very accurately with 25 nearest neighbours while speeding recall by a factor of 1.8 compared with the
AURA k-NN in Weeks et al. (2003). For large data sets, we posit the method de-tailed in this paper due to the faster speed, preferably with 25 nearest neighbours, and recommend a k value of fewer than 50. From the evaluation here, we would rec-ommend 149 bins, as this provi des the best trade-off between recall accuracy versus recall speed for both data sets. We would r ecommend a cross-validation to determine the optimum number of bins if time permits starting with 149 bins. For a real-valued data set, we posit the rule-of-thumb more bins for fewer neighbours but conversely fewer bins for more neighbours. For a discrete data set, we posit the rule-of-thumb more bins is better. For smaller data sets, we posit the method detailed in Weeks et al. (2003) due to the higher accuracy.

The recommended AURA k-NN configurations emulate the standard k-NN by 99%+ with respect to neighbours retrieved. For a classification task, the class counts of the neighbours retrieved for each query record would be 99%+ similar for the recommended configurations compared with the standard technique.

For the AURA k-NN technique, there are two user-specified parameters: the num-ber of neighbours and the number of bins per attribute. Thus, the use of AURA to speed the k-NN has introduced only one additional parameter compared with stan-dard k-NN, the number of bins per attribute. There are four variables that affect the execution speed in the AURA k-NN: the number of neighbours, the number of attributes, the number of records and the number of bins per attribute. The use of
AURA to speed the k-NN has introduced only one additional variable compared with standard k-NN, the number of bins per attribute.

We propose to use the AURA k-NN developed here, within a fraud-detection system and within an outlier-detection system, where the data sets are large with approximately 400,000 records and 360 attributes.

