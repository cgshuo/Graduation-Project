 In this demo, we present Cleanix, a prototype system for cleaning relational Big Data. Cleanix takes data integrated from multiple data sources and cleans them on a shared-nothing machine cluster. The backend system is built on-top-of an extensible and flexible data-parallel substrate X  the Hyracks framework. Cleanix supports various data cleaning tasks such as abnormal value detection and correction, incomplete data filling, de-duplication, and conflict res-olution. We demonstrate that Cleanix is a practical tool that sup-ports effective and efficient data cleaning at the large scale.
Recent popular Big Data analytics applications are motivating both industry and academia to design and implement highly scal-able data management tools. However, the value of data not only depends on the quantity but also relies on the quality. On one side, due of the high volume and the high variation, those Big Data ap-plications suffer way more data quality issues than traditional ap-plications. On the other side, efficiently cleaning a huge amount of data in a shared-nothing architecture has not been well studied yet. Therefore, to improve the data quality is an important yet challeng-ing task
Many data cleaning tools [6] have been proposed to help users to detect and repair errors in the data. Although these systems could clean data effectively for many datasets, they are not suitable for cleaning Big Data due to the following three reasons. First, none of the existing systems can scale out to hundreds and thousands of machines in a shared-nothing manner. Second, various error types such as incompleteness, inconsistency, duplication, and value conflicting may co-exist in the Big Data while most existing sys-tems are ad-hoc and only focus on a specific error type. As exam-ples, CerFix [4] focus on inconsistency while AJAX [5] is for de-ing systems often require users to have specific data cleaning exper-tise. For example, CerFix [4] require users to understand the con-cept of conditional functional dependency (CFD), while AJAX [5] lets users express data cleaning tasks with a declarative language. However, many real-world users do not have a solid data cleaning background nor understand the semantics of a specific data clean-ing language.

In order to address the fundamental issues in existing systems and support data cleaning at a very large scale, we design and im-plement a new system called Cleanix. We list the key features of Cleanix as follows.  X 
Scalability . Cleanix performs data quality reporting tasks and data cleaning tasks in parallel on a shared-nothing com-midity machine cluster. The backend system is built on-top-of Hyracks [1], an extensible, flexible, scalable and general-purpose data parallel execution engine, with our user-defined data cleaning second-order operators and first-order functions.  X 
Unification . Cleanix unifies various automated data repair-ing tasks for errors by integrating them into a single parallel dataflow. New cleaning functionalities for newly discovered data quality issues could be easily added to the Cleanix dataflow as ei-ther user-defined second-order operators or first-order functions.  X 
Usability . Cleanix does not require users to be data cleaning ex-perts. It provides a simple and friendly graphical user interface for users to select rules with intuitive meanings and high-level descriptions. Cleanix also provides a bunch of visualization util-ities for users to better understa nd error statistics, easily locate the errors and fix them.
 The main goal of this demonstration is to present the Cleanix sys-tem architecture and execution process by performing a series of data integration and cleaning tasks. We show how the data clean-ing operators are used to clean the data integrated from multiple data sources.
We give a system overview in this section. First, we discuss the data cleaning tasks in Section 2.1. Then, Section 2.2 briefly introduces the Hyracks execution engine and illustrates why Hyracks is chosen as the Cleanix backend. Finally, we discuss the Cleanix architecture of the system in Section 2.3.
Cleanix aims to handle four types of data quality issues in a uni-fied way:  X 
Abnormal value detection and correcting is to find the anomalies according to the users X  options of rules and modify them to a near value that coincides with the rules.  X 
Incomplete data filling is to find the empty attributes in the data and fill them with proper values.  X 
De-duplication is to merge and remove duplicated data.  X 
Conflict resolution is to find conflicting attributes in the tuples referring to the same real-world entity and find the true values for these attributes.
 We believe that these four data cleaning tasks cover most data qual-ity issues. Note that even though some data errors could not be pro-cessed directly such as non-concurrency and inconsistency, one can take care of them by dynamically deploying new first-order user-defined functions into our system. For example, non-concurrency can be processed as conflict resolutions among the data referring to the same real-world entity.
We use Hyracks as the Cleanix backend to accomplish the above tasks efficiently at large scales Hyracks is a data-parallel execution engine for Big Data computations on shared-nothing commodity machine clusters. Compared to MapReduce [3], Hyracks has the following advantages:  X 
Extensibility . It allows users to add data processing operators and connectors, and orchestrate them into whatever DAGs. How-ever, in the MapReduce world, we need to cast the data cleaning semantics into a scan (map) X  X roup-by(reduce) framework.  X 
Flexibility . Hyracks supports a variety of materialization poli-cies for repartitioning connectors, while MapReduce only has the local file system blocking-materialization policy and the HDFS materialization policy. This allows Hyracks to be elastic to dif-ferent cluster configurations.  X 
Efficiency . The extensibility and flexibility together lead to sig-nificant efficiency potentials.
 Several cloud computing vendors are developing non-MapReduce parallel SQL enginesto support fast Big Data analytics. However, these systems are like  X  X nions" [2] X  X ne cannot directly use their internal Hyracks-like engines under the SQL skin for data cleaning. However, the Hyracks software stack is like a layered  X  X arfait" [2] and Cleanix is yet-another parfait layer on-top-of the core Hyracks layer.
Cleanix provides web interfaces for users to input the informa-tion of data sources, parameters and rule selections. Data from mul-tiple data sources are preprocessed and loaded into a distributed file system X  X DFS 1 . Then each slave machine reads part of the data to start the cleaning. The data cleaning dataflow containing second-order operators and connectors is executed on slaves according to the user specified parameters and rules (e.g., first-order functions). At the end of the dataflow, the cleaned data are written to HDFS. Finally, the cleaned data are extracted from HDFS and loaded into the desired target database.
In this section, we discuss the details of the Cleanix data cleaning pipeline, the algorithmic operators and the profiling mechanism.
To make the discussion brief, we use A, I and D and C to rep-resent the modules of the process of abnormal value detection and correcting, incomplete data filling, de-duplication and conflict res-olution, respectively. The order of four tasks of data cleaning in Cleanix is determined with the consideration of effectiveness and efficiency. These four modules could be divided into two groups. http://en.wikipedia.org/wiki/Apache_Hadoop Module A and I are in the same group (Group 1) sharing the same detection phase since the detection of abnormal values and empty attributes can be accomplished in a single scan of the data. Module D and C are in the same group (Group 2) since the identifications of entities with the entity resolution operator are required for both de-duplication and conflict resolution. De-duplication merges tu-ples with the same entity identification while conflict resolution is referring the same entity identification. The reason why Group 1 is executed before Group 2 is that the repairation of abnormal values and empty attributed will increase the accuracy of entity resolu-tion. In Group 1, Module A is before I since abnormal values in-In Group 2, Module D is before C since only when different tuples referring to the same entity are found and grouped, the true values of conflicting attributes could be found.
The dataflow graph is shown in Figure 1. The dataflow has 8 algorithmic operators and 4 stages, where the computation of each stage is  X  X ocal" to each single machine and the data exchange (e.g., broadcast or hash repartitioning) happens at the stage boundaries. In the following part, we illustrate the algorithmic operators and the rules for each stage in the topological order in Figure 1.
Stage 1 . This stage is performed on each slave machine.  X 
DataRead . It scans incoming file splits from the HDFS. The data are parsed and translated into the Cleanix internal format.  X 
Correct . This is blocking operator X  X ata are checked accord-ing to the rules selected by users to detect the abnormal values and incomplete tuples. When an abnormal value is detected, it is corrected according to corresponding revision rules (first-order functions). When an incomplete tuple is encountered, it is iden-tified for further processing.  X 
BuildNullGram . This operator builds an inverted list for all in-complete tuples for the imputation based on similar tuples. The inverted list is called the gram table . It is a hash table in which the k -gram is the key and the id set of tuples containing such a k -gram is the value.
 Stage 2 . The incoming broadcast connector to this stage broadcasts the gram tables such that all slaves share the same global gram table.  X 
Fill . For each tuple with incomplete attribute, similar tuples are found according to the gram table. The incomplete attribute is filled with the aggregated value of the corresponding attribute in similar tuples according to the imputation rules (first-order functions) selected by users such as average, max or the most frequent.  X 
BuildGram . A local gram table is built for the local data for the attributes potentially containing duplications or conflicts, which are chosen by users. Since a local gram table has been built with BuildNullGram operator, only the newly filled values of corresponding attributes are scanned in this step.
 Stage 3 . The local gram tables are broadcast to make all slaves share the same global gram table. Note that in this stage, only the updated values in local gram tables are broadcast.  X 
ComputeSimilarity . The similarities between each local tuple and other tuples are computed according to the global gram ta-ble. When the similarity between two tuples is larger than a threshold, they are added to the same group. After local data are scanned, many groups are obtained. Stage 4 . The groups are partitioned acco rding to the hashing value of bloom filter of the union of gram sets in this group.  X 
De-duplication . A weighted graph G is built to describe the sim-ilarity between tuples in each group. Similar vertices are merged iteratively in G until no pairs of vertices can be merged [7]. This step is executed iteratively until the ratio between the number of shared connected vertices and the number of the adjacent ver-tices of each vertex is smaller than a threshold. The tuples corre-sponding to all merged vertices are considered as duplications.  X 
Conflict Resolution . Tuples corresponding to the merged ver-tices are merged. During the merging, when an attribute with conflicting values is detected, it is resolved with voting accord-ing to the selected rules chosen by users. The options (first-order functions) include max, min, average and the most frequent. Each stage sends a corresponding profiling report back to the Hyracks master machine by using the Hyracks management events. When the master machine receives the profiling reports, it redirects them to the Cleanix graphical user interface such that users can see the error exploration report, the data quality report and the data cleaning result review.
In this section we describe the user interfaces of Cleanix in de-tails and explain the aims of our demonstration.
 Specifying Parameters. The first step in using Cleanix is to load data from data sources into the system. Users simply need to provide the name, port, username and password of the data sources. Our system also supports databases on the web with a reachable IP address.

The interface for user inputting parameters is generated accord-ing to the schema of databases. Our system requires users to input three kinds of required information for each attribute: (1) whether the attribute should be checked; (2) whether the attribute is allowed to be null; (3) the data type of an attribute.
 Error Exploration. Our system shows the error in the data ex-ploration interface for users to review data errors. In this demon-stration we show the following features.  X  How the users can explore the data with error identifications.
The errors in data are distinguished with different colors in this interface. The user can then further select a tuple in a table. Then the details of the data errors are shown. In this way, users can identify the reasons why the tuple is marked as an error tuple.  X 
How the users explore the data by means of data errors. When the user selects an data error, its corresponding tuples will be displayed in a table. A user could further select a data error de-tection or correction rule and the tuples with attributes violating the rules are shown with the desired attribute highlighted. Data Quality Report. Statistics of data quality information are summarized and shown to users to check the data quality in high level. In the demonstration we show:  X 
The data quality problem in table and attribute level. In partic-ular, this component computes various data quality problem in quantity by means of data sources, tuples and attributes shown in histogram. A similar categorization exists at both the attribute and tuple level.  X 
How the violations are distributed among the data. Cleanix com-putes various statistical measures and reports statistics regarding to the selected rules. The user can choose to retrieve this infor-mation at different levels.
 demonstration, we illustrate the exploration of data cleaning re-sults and interaction of user and the system. More specifically, we compare the repaired data with the original ones. The original and modified data are distinguished in different colors. When the user selects a modified value, the modifications are shown. Additionally, the user could modify the data. The modifications are merged when the cleaned data is transmitted from HDFS to the target database If the input value has some errors, they will be identified and sug-gested correct modifications close to the input value are shown for are highlighted.
 Acknowledgements. This paper was partially supported by NGFR 973 grant 2012CB316200 and NSFC grant 61472099.
