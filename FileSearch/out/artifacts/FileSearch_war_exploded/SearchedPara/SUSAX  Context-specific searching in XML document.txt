 1. Introduction
Over the past several years  X  X  X oogle X  X  has become part of the common phraseology used by people of all ages. Coined after the popular Google [9] search engine, the phrase is more generally representative of the key-word searching capabilities offered by search engines such as Google [9] , Yahoo [12] , HotBot [10] , and MSN [11] . In all cases the user X  X  query, a single keyword or a set of keywords and X  X d or or X  X d, is matched exactly against an inverted index of keywords, and a ranked list of documents (by PageRank [9] for instance) contain-ing the specified keyword are returned to the user.
 However, while keyword searching has indeed been very successful in narrowing down the contents of the
Web to the pertaining subset of information, it has two primary drawbacks. First, the accuracy of the search is tightly coupled to the choice of keywords. For example, to get all the publications of Michael Stonebraker we must choose the right keywords for conducting the search. Fig. 1 shows the results obtained by using the keywords  X  X  X ublished work Stonebraker X  X ,  X  X  X tonebraker X  X ,  X  X  X rticles Stonebraker X  X  and  X  X  X ublications Stoneb-raker X  X . Searches with these keywords yielded different sets of results  X  in fact non-overlapping results in the first page. Only the keywords  X  X  X ublications Stonebraker X  X  yielded the desired hits, shown in Fig. 2 ,in the first page of results. Second, keywords are limited in their expressibility. In particular, they fail to ade-quately capture the contextual information implicit in searches typically conducted by the users. For example, the query all publications by Stonebraker in 1997 is at best represented by keywords  X  X  X tonebraker publications 1997 X  X   X  a search whose first hit is completely irrelevant.

These drawbacks of keyword searching while formidable to address in the general Web context, are how-ever tractable for a significant chunk of the Web that comprises of semi-structured XML documents. In this paper, we present an extension to schema matching techniques [3,2,4,8,17,6] to efficiently address the draw-backs of keyword searching over XML documents. In particular, we propose SUSAX  X  a system for approx-imate contextual querying over XML documents wherein queries are represented as simple XPaths of the form/ publications / Stonebraker /1997.

A key contribution of our work is the novel algorithm used to match the XPath-like query with similar paths in the repository. The algorithm is based on sequence alignment algorithms prevalent in life sciences domain for discovering the similarity between genome and protein sequences. We show how a commonly used sequence alignment algorithm, the Needleman X  X unsch algorithm [22] , can be adapted for discovering and cataloging the similarity between two paths  X  providing a ranked set of results that takes both approximate matching and contextual information into account. Our results show that approximate, path-augmented con-textual searching over XML documents yields better, more relevant results than exact keyword searching alone.

Roadmap : The rest of the paper is organized as follows. In Section 2 we provide some necessary background on the different alignment algorithms including the base algorithm for our work  X  the Needleman X  X unsch algorithm. Section 3 presents the core of our approach and sets the context for the remainder of the paper. Section 4 describes the linguistic algorithm that we employ as part of our overall SUSAX algorithm, while Section 5 outlines the adaptation of the Needleman X  X unsch algorithm for searching and aligning simple
XPaths. Section 6 gives an architectural overview of the SUSAX system. In Section 7 , we present our hypoth-esis and describe our experimental setup and methodology, while in Section 8 we analyse the results of our experiments. Section 9 reviews some relevant related work and we conclude in Section 10 . 2. Background  X  pairwise alignment algorithms Pairwise sequence alignments are fundamental to similarity searches conducted in the life sciences domain.
Two basic classes of alignment algorithms have been developed  X  global and local. Global alignment algo-rithms attempt to align sequences over their entire length, while local alignment algorithms concentrate on discovering and aligning only the conserved motifs. Wunsch [22] and the optimal local alignment algorithm proposed by Smith and Waterman [25] are the foundations for many of the algorithms proposed in later works such as repeat matches and overlap matches. In this section, we give a brief overview of these algorithms.
In general, both Needleman X  X unsch (NW) and Smith X  X aterman (SW) take two input sequences and gen-erate an alignment together with a score as an output. The input sequences are represented as FASTA [13] sequences X  X  popular format for sequence representation. A sequence in the FASTA format begins with a one line description that includes an identifier and a description for the sequence, followed by multiple lines of the actual sequence data. The sequence data itself is represented using the IUB/IUPAC codes for amino acids. The output of the algorithm is the aligned sequences  X  sequences that have potentially been modified by the introduction of gaps  X  together with an overall score for the alignment. Figs. 3 and 4 show the global and local alignment of two input sequences computed using the Needleman X  X unsch and Smith X  X aterman algorithms respectively. The figures show the overall score returned by the algorithms, 1 and 28 respectively for global and local alignment, the input sequences, and the output aligned sequences. Both Needleman X 
Wunsch and the Smith X  X aterman algorithms have a computational complexity of O( nm ), where n and m are the lengths of the two sequences. The Smith X  X aterman algorithm is generally considered to be the more sensitive algorithm.

In general, the total score of an alignment is calculated based on the sum of terms for each pair of residues plus the term for each gap introduced by the algorithm. Typically, each exact match gets a positive score, each mismatch gets a penalty of s , where s is the score retrieved from the substitution matrix, representing an insertion or deletion in one of the two sequences, gets a penalty of d . The BLOSUM50 and PAM matrices are two commonly used substitution matrices. Table 1 shows a fragment of the BLO-
SUM50 matrix. 2.1. The Needleman X  X unsch algorithm
The Needleman X  X unsch algorithm [22] finds the best global alignment of two sequences under a given sub-stitution matrix. In this algorithm, the similarity score is maximized, and the largest number of residues of one sequence that can be matched with a second sequence allowing for all possible gaps is discovered. This algorithm
The Needleman X  X unsch algorithm can be divided into three phases: the initialization phase, the fill phase, and the trace back phase.

In the initialization phase, a matrix F indexed by two sequences X for the matrix is given as where d represents the gap penalty.

In the next phase, the fill phase, the algorithm proceeds to fill the matrix, starting at the top left corner and x , y j is given as where the value F ( i 1, j 1) + s ( x i , y j ) is obtained if x to a gap; and F ( i , j 1) d if y i is aligned to a gap.
 As the matrix F is filled, a back pointer is kept in each cell specifying the link to the cell from which the for the global alignment between the sequences X n and Y m matrix for the example sequences in Fig. 3 [5] with the backpointers for each cell value.

In the traceback phase, the actual alignment between the sequences X at x i if the backtrace is to ( i 1, j ); and  X  X - X  X  is added at position y cell of the matrix is reached, the traceback step terminates. The output of this step is an alignment between the two given sequences. Fig. 3 shows the final aligned sequences obtained after the traceback phase, while the value of the last cell F ( n , m )= 1 indicates the overall score. 3. SUSAX  X  an overview In this section, we give a brief overview of the three key steps that represent the core functionality of SUSAX, and using which we can provide approximate contextual searching of XML documents. In
SUSAX, a query Q is represented as a simple path as shown in Fig. 5 . Here, the leaf node, publisher , represents the keyword for the search, while the path from the root to the leaf, Sigmod / paper represents the context for the keyword search. XML documents are broken down into similarly represented set of source paths.

The given query Q is compared against all sources paths P in the repository, and a ranked list of documents highlighted with matching source paths is returned to the user. The approximate contextual comparison of the query Q with a source path P is conducted via the three primary steps outlined in Fig. 6 and briefly described below.

Step 1  X  Tokenization: In the first step, a given query Q is converted into a set of tokens, where each token is separated from the next by a X / X . For example, the tokenization step generates the tokens { Sigmod , paper , publisher } for the query Q given in Fig. 5 . A similar tokenization step is carried out for the source paths, one at a time, resulting for example in the tokens { SigmodRecord , issue , articles , article , authors } for the source path shown in Fig. 5 .

Step 2  X  Approximate matching: Once tokenized, a similarity measure is computed for each possible query X  source token pair. The similarity between the query and source tokens provides a measure of the approx-imate match between the two tokens X  1.0 if the two tokens match exactly, and a value between 0.0 and 0.9 for approximate matches. The output of this step is a similarity matrix that provides the similarity score for every token pair in the query and source paths. Fig. 7 depicts a similarity matrix generated for the query and source paths given in Fig. 5 . The similarity measure and the corresponding similarity matrix are com-puted using the Label Match algorithm detailed in Section 4 .

Step 3  X  Contextual alignment: The similarity matrix generated in Step 2 is used in the final step to compute the optimal alignment between the query and the source paths. The contextual alignment is the adaptation of the Needleman X  X unsch pairwise global alignment algorithm, and aligns, as much as possible, each token in the query path to its best match in the source path. The output of this step is the aligned sequence together with a total normalized score for the alignment. The normalized score for the alignment provides an overall ranking of the query path to all relevant source paths in the repository. Fig. 8 shows the align-ment of the query and the source path given in Fig. 5, together with the alignment score. Note here that two gaps (aligning with the tokens issue and articles in the source path) are introduced in the query path to provide the best match for each query token ( Sigmod , paper and publisher ). Details on the con-textual alignment step are presented in Section 5 .
 4. Approximate matching  X  the label similarity algorithm
Approximate matching computes the similarity for every query X  X ource token pair for a given query and source path, and dynamically generates as output a similarity matrix (shown in Fig. 7 ) with similarity scores for all possible query X  X ource token pairs. To compute the similarity scores, and hence the similarity matrix, we employ a label match [26] algorithm that semantically compares two labels using a dictionary and other aux-iliary information.

A token, typically representative of natural language, can be classified as either (i) an atomic token  X  com-posed of a single word; or (ii) a composite token  X  composed of multiple words, where the start of each word is distinguished generally by punctuations (for example, purchase-order ), case distinction (for example, purchase- X  they can be a fully defined dictionary word, an abbreviation, an acronym, or a substring. For example, qty is an abbreviation of quantity ; uom an acronym of unitOfMeasure ; and addr a substring of address .
The similarity measure between two tokens, T q and T s , is determined as follows. The two given tokens are first parsed into subtokens. The similarity between the query and source subtokens is then measured using two tokens T q and T s is computed as the average of the best similarity measure of each query subtoken with a source subtoken. Formally, the similarity measure, SM ( T where or wup [27] ) for a pair of tokens, j T q j is the number of query subtokens for the query token. The variables t
T s and j T s j are defined similarly for the source token.

Fig. 9 gives the pseudo-code for the algorithm. For each token pair, we first check if there is an identity, acronym, abbreviation, or substring match between the two tokens using a domain-specific, local dictionary that defines the common set of abbreviations, acronyms, and commonly used substring/short hand notations for a given domain. If such a match cannot be determined, we invoke the linguistic similarity algorithm to determine the similarity distance between the two tokens. We use the path linguistic similarity measure  X  a similarity measure based on the path lengths between concepts, and equal to the inverse of the shortest path length between two concepts. To determine the path similarity of two words, we use Wordnet::Similarity [23] , a freely available tool that measures the semantic similarity and the relatedness between a pair of concepts.
The tool provides six measures of similarity including the path measure, and three measures of relatedness, all of which are based on the WordNet [18] lexical database. We ran an independent set of experiments [26] and found that the path similarity measure had the highest precision and recall for the domains tested.
Fig. 7 shows the similarity matrix generated by the label similarity algorithm for the token sets { SigmodRe-cord , issue , articles , article , authors } and { Sigmod , paper , publisher }. 5. Contextual alignment: adaptation of the Needleman Wunsch algorithm
The tokens generated in the Tokenization step together with the similarity matrix produced by the Approx-imate Matching step (see Section 3 ) are the main inputs for the last and final step of Contextual Alignment that produces the aligned paths. In this section, we describe the Contextual Alignment algorithm  X  an adap-tation of the Needleman X  X unsch global alignment algorithm.

Based on the Needleman X  X unsch algorithm, we split our Contextual Alignment algorithm into three phases: the initialization phase, the fill phase, and the traceback phase.

In the initialization phase, a matrix F indexed by the two paths, query path P structed. The initial value for the matrix is set as where d represents the gap penalty  X  that is the penalty incurred if there is no possible match between the two tokens. 2 Typically, the gap penalty is a tunable parameter and can be adjusted automatically by the system or set by the user. Here i and j represent the indices for the matrix F .

In the next phase, the fill phase, the algorithm proceeds to fill the matrix, starting at the top left corner and
T , T s is given as where the value F ( i 1, j 1) + SM ( T q , T s ) is obtained if T aligned to a gap; and F ( i , j 1) d if T s is aligned to a gap. Here, SM ( T tained from the similarity matrix generated in Step 2 of the overall SUSAX process. This term corresponds to the s term in the original Needleman X  X unsch algorithm described in Section 2 . Fig. 10 depicts the matrix F as computed by Eq. (4) for the query and source paths given in Fig. 5 .
 As the matrix F is filled, a back pointer is kept in each cell specifying the link to the cell from which the for the global alignment between the query path P q and the source path P matrix F with the backpointers for the query P q and the source paths P
In the traceback phase, the actual alignment between the paths P was derived. Based on the backtracking, a pair of tokens is added onto the front of the current alignment. The tokens t q and t s are added if the backtrace is to ( i 1, j 1); a gap character  X  X - X  X  is added at t reached, the traceback step terminates. The output of this step is an alignment between the two given sequences. Fig. 8 illustrates the final alignment produced by the traceback phase for the example query and source paths shown in Fig. 5 . 6. Architectural overview of SUSAX
Fig. 12 gives an architectural overview of the SUSAX system used for the experimental results presented in this paper. The Local Repository is the data store of XML documents collected from various sources on the
Web. We do not consider, for this version of SUSAX, issues pertaining to web crawling and refreshing of the documents in the repository in event of updates. The PathGen component generates all unique paths from the
XML documents in the Local Repository. The PathGen unit, executed only when the Local Repository is updated, also embeds useful information, such as the source document for each path. The XML paths gen-erated by the PathGen are stored in the XML Path Repository. While currently the XML Path Repository is represented by a single XML document, we plan on adding an XML indexing facility to speed up the approximate contextual search outlined in this paper.

Users input simple path queries into the system via the Query Interface. The Query Interface both cap-tures the user query as well as pre-processes the query removing empty tokens and making the queries case-independent. The user query is passed on to the Match unit, the core component of SUSAX. The Match unit performs three primary tasks. First, the Match unit retrieves the set of paths from the
XML Path Repository against which the query path must be compared. Second, for every query and source path pair it follows the three key steps outlined in Fig. 6  X  namely the Tokenization, Approximate
Matching and Contextual Alignment. Tokenization generates a set of tokens corresponding to the query and source paths as outlined in Section 3 . The Approximate Matching unit uses the Label Match Algo-rithm outlined in Section 4 to produce the similarity matrix. Finally, the Contextual Alignment unit uses the similarity matrix as input and produces the final alignment between the query and source paths. Last, the Match unit performs book-keeping and updates a Local Similarity Cache of similarity values for a token pair. The Local Similarity Cache maintains the similarity values between two given tokens, and is used primarily as a faster alternative to the Label Match algorithm. Thus, if a pair of tokens exists in the Local Similarity Cache, the similarity value is used for the similarity matrix generation and the Label Match algorithm is not invoked.

The last unit of the SUSAX is the Sorting and Ranking component. The Sorting and Ranking unit takes the pairs of all query and source paths, their alignments and alignment scores as input. As a first step, it nor-malizes the alignment scores by dividing the alignment score with the total number of query tokens. The source (aligned) paths are then sorted based on the normalized score. Documents corresponding to the ranked list of source (aligned) paths are displayed in decreasing order of scores to the user. 7. Experimental evaluation
A series of experiments were conducted to evaluate the potential benefits of SUSAX system. In particular, the experiments were designed to test the following hypotheses:  X  Approximate keyword searching provides higher recall of results, albeit with poorer precision, than exact keyword searching.  X  Context-specific searching can improve both the precision and recall of exact keyword searching.  X  Approximate keyword and context-specific searching together provide better precision and recall than exact keyword searching.
 Additionally, the performance of SUSAX was compared with keyword-based searching provided by a local Google search running against the same XML repository.
 7.1. Experimental setup and methodology The SUSAX system was implemented in Java J2SE Runtime Environment Version 5.0 and evaluated on a
Dell 2.8 GHz Pentium 4 workstation with 512 Mb of RAM. The Local Repository for this set of experiments was setup using XML documents obtained from the University of Washington X  X  XML Repository [24] , linux docs, and rss feeds by news channels like CNN, and BBC. Documents from UW Repository [24] included XML documents from 321gone , ebay , book , customer , SigmodRecord , ubid , and yahoo . Unique
XML paths were generated from these documents resulting in a total of 1228 paths in the XML Path Repos-itory. Queries, while targeted towards these domains, were variants, in length and token labels, on the paths in the repository. The Label Match algorithm used the Perl interface to the WordNet-Similarity, version 0.15, that internally utilized WordNet 2.1. 7.2. Gap penalty and threshold
To evaluate the quality of our approach, we compared the manually determined real matches ( R ) for a given match task with the matches P returned by the match algorithm. We determined the true positives, that the SUSAX algorithm was computed. and taking into account the post-match effort needed for both removing false matches and adding the missed matches. 7.2.1. Computing optimal gap penalty
The gap penalty d introduced in Section 5 plays an important role in determining the quality measured in terms of the precision and recall of the matched source paths returned to the user. Recall that d is the cost of introducing a gap in the source or the query path to indicate the lack of a match for a given query token T q with respect to the source tokens T s and vice versa. A high gap penalty biases the overall algo-rithm to disregard semantic similarity, while a low gap penalty (tending to 0) over-emphasizes the semantic similarity between the tokens. An optimal gap penalty value ensures a balance between the semantic and structural information encapsulated in the path, and hence influences the overall precision and recall of the algorithm.

We ran experiments to determine the optimal range of the d value for high precision and recall. For this experiment, the three different queries shown in Table 3 were run. Each query was run against the Local
XML Path Repository, and the precision and recall of the SUSAX algorithm for varying values of the gap penalty d were computed. For this set of experiments the match threshold was set to 0.6 , that is only those matches with a normalized score greater than 0.6 were considered. To compute the precision and recall for each query, a domain expert manually determine the matches she would expect from the Local
XML Path Repository prior to the start of the experiment. These results were used to compare the results of the SUSAX algorithm. Table 3 shows the number of pre-determined matches in the repository for each query.

Table 4 shows the matches returned by the SUSAX algorithm for the query Sigmod / article / author ized precision and recall value and ranges from 0 to 1 . For the queries tested a gap penalty d = 0.15 was considered optimal as it provided high precision and recall. For all subsequent experiments gap penalty d = 0.15 . 7.2.2. Match threshold
The match threshold is the watermark above which a result returned by the SUSAX algorithm is considered to be a match. A low threshold value increases the false positives thereby decreasing the recall of the algo-rithm, while a high threshold value can potentially miss some real matches lowering the precision of the algo-rithm. A set of experiments were run to determine the optimal threshold for the SUSAX algorithm. For this experiment the gap penalty was set to d = 0.15 , and the precision and recall of the SUSAX algorithm for a varying number of match threshold values was computed. The experiments were run using four independent queries shown in Table 5 . Table 5 shows the expected number of matches determined manually by a domain expert.

Table 6 shows the matches returned by SUSAX for the query item / HardDrive for threshold values of 0.4 and 0.6 . Fig. 14 depicts the average precision and recall measured for the four queries. The x -axis shows the varying match threshold values and ranges between 0 and 1 . The y -axis depicts the precision and recall values normalized between 0 and 1 . A match threshold in the range 0.50  X  0.60 provides optimal precision and recall for the four queries tested. For the remainder of the experiments, the match threshold for the
SUSAX algorithm is set to 0.6 . 8. Results and analysis
To test our hypothesis, and evaluate the SUSAX approach, we conducted a set of experiments. For these experiments, the local XML repository was setup as discussed in Section 7.1 . The six queries used for the experiments are shown in Table 7 together with the total number of manual matches as determined by two independent domain experts.

The experiments conducted were:  X  Exact keyword searching: In this experiment the query was specified as a single keyword and an exact label match algorithm was used to conduct the search.  X  Approximate keyword searching: In this experiment the query was specified as a keyword. However, an approximate label match algorithm was used to conduct the search. The match threshold was set to 0.6 for this experiment.  X  Exact keyword searching with exact context: In this experiment, the query was specified in the format shown in Fig. 5 , that is with both keyword and context. An exact label match algorithm was utilized for matching both the keyword and the context.  X  Approximate keyword searching with approximate context: In this experiment the query was specified in the format shown in Fig. 5 . However, an approximate label match was utilized to match both keyword and context. This combination reflected the full capabilities of the SUSAX system. The gap penalty d was set to 0.15 and the match threshold was set to 0.6 for the purpose of this experiment. 8.1. Keyword queries: exact versus approximate searching
The first set of experiments were designed to measure the impact of exact and approximate searching on the precision and recall of the result sets. For this experiment, the queries were limited to keyword-only queries, and exact and approximate match algorithms were used for the search. The queries for this experiment were formulated using the keywords specified for the queries in Table 7 , and the expected matches in the repository were determined apriori by a domain expert. Table 8 shows a sample set of matches that were obtained for the query Q = volume using exact and approximate searching together with the manually determined matches.
Fig. 15 compares the precision of the results obtained using exact and approximate keyword searching. The x -cision of results obtained using exact keyword searching was on average higher than the precision of the approx-imate keyword searching. This was an expected result as in general exact search produces more reliable results.
Fig. 16 compares the recall of the result sets obtained using exact and approximate searching. Recall is the share of real matches returned by the algorithm. The x -axis shows the different queries used in the experiment (see Table 7 ) and the y -axis shows the recall of the results as a percentage value. The recall of the results obtained using approximate keyword searching was distinctively high (100% for all queries), while exact keyword searching on average provided a recall of only about 60%. These precision and recall results validate our first hypothesis  X  exact keyword provides better precision, while approximate keyword matching plays a significant role in raising the recall. 8.2. Keyword + context: exact searching
The second set of experiments were designed to measure the impact of the context on the precision and recall of the obtained results. For this experiment, two types of queries were used, keyword queries and key-word with context queries. Exact label match algorithm was used for the search, and the precision and recall of the output results was measured. The keyword only queries for this experiment were formulated using the key-words specified for the queries in Table 7 . The keyword with context queries used in the experiments are given in Table 7 . The expected set of results for both keyword and keyword with context queries were determined apriori by a domain expert. Table 9 shows a sample set of matches that were obtained for the keyword only query Q k = volume and keyword with context query Q c = SigmodRecord / volume using exact searching, together with the manually determined matches.

Fig. 17 compares the precision of the results obtained using an exact match algorithm for keyword only and keyword with context queries. The x -axis shows the different queries used in the experiment (see Table 7 for the queries), and the y -axis shows the precision of the result as a percentage value, with 100% indicating the highest achievable precision. The additional information supplied by the context of a query can drastically improve the precision of the obtained results, providing for example 100% precision for the queries tested. This higher precision can be directly attributed to the additional context information.

Fig. 18 compares the recall of the results obtained using exact searching for keyword and keyword with context queries. The x -axis shows the different queries used in the experiment, and y -axis shows the recall as a percentage value. The recall for the results obtained using keyword with context queries was significantly lower than the recall obtained using keyword queries. This can be attributed to the fact that the additional context information caused, in some cases, relevant results to be filtered out. The results of this experiment affirm the second hypothesis  X  context information has the potential of providing more precise results but can degrade the recall of the results.
 8.3. Keyword + context: exact vs approximate searching
The third set of experiments was designed to examine compounded effects of approximate searching on key-word with context queries. For this experiment, two types of queries were used keyword queries and keyword with context queries. Approximate label match algorithm was used for keyword + context queries, while exact label match was used for keyword only queries. The precision and recall of the output results was measured. The keyword only queries for this experiment were formulated using the keywords specified for the queries in
Table 7 . The keyword with context queries used in this experiment were as given in Table 7 , and the expected set of results for both keyword and keyword with context queries were determined apriori by a domain expert.
Table 10 shows a sample set of matches for the exact keyword only query Q word with context query Q c = SigmodRecord/volume , together with the manually determined matches.
Fig. 19 compares the precision of the results obtained using exact keyword searches and approximate searches for keyword with context queries. The x -axis shows the different queries used in the experiment (see Table 7 for the queries), and the y -axis shows the precision of the result as a percentage value, with 100% indicating the highest achievable precision. On average approximate searching for keyword + context queries provides the same precision as exact matching of keyword queries. This improvement of precision over approximate keyword ( Fig. 15 ) searching can be attributed to the additional information encapsulated in the context of the query. Note however, that the precision is in general lower than the precision of results obtained by exact searching of keyword + context queries.

Fig. 20 compares the recall of the results obtained by exact keyword searching and approximate key-word + context queries. The x -axis depicts the different queries used while the y -axis shows the recall in percentage. The recall for the results obtained using approximate searching on keyword + context queries was 100% for all the queries, significantly higher than the recall obtained using exact keyword searching.
In general, the experiments found that approximate searching with keyword + context queries provided higher precision and recall when compared with exact keyword searching thereby confirming our third hypothesis. The experiments also indicate that a combination approach that utilizes approximate searching for the context and exact searching for the keyword has the potential to provide the best of all considered approaches. 8.4. SUSAX versus local Google
The last experiment compared the precision and recall of a local Google search with the full-fledged approximate keyword and context searching of SUSAX. The queries in Table 7 were used for the experiments.
For Google Desktop, each path was converted into multiple keywords. Table 11 shows a sample set of results for the query SigmodRecord / volume obtained using approximate searching on keyword + context queries, the results of the query SigmodRecord / volume obtained using Google Desktop, together with the manu-ally expected set of matches.
 Fig. 22 shows the precision of the results obtained using approximate keyword with approximate context SUSAX and Google Desktop searching. The x -axis shows the different queries used in the experiment (refer
Table 7 for queries used), and the y -axis shows the precision as a percentage value. The precision of results obtained using Google desktop on average provides similar precision to that obtained using SUSAX X  X  approx-imate keyword with approximate context searching.

Fig. 21 shows the recall for the results. The x -axis shows the different queries used and the y -axis shows the recall in percentage values. The recall of the results obtained using approximate searching on key-word + context queries was 100% for all the queries, much higher than the recall obtained using Google
Desktop searching. This result further validates the utility of approximate searching as well as the benefit of providing contextual information. Both techniques can be used to improve the precision and recall for searching.
 9. Related work
There is relatively little work that explores the use of context to enhance keyword searching. Zapper tech-nologies [7] have developed a system that allows clients to search for a keyword in the context of the document in which the keyword occurs. Typically, information is retrieved from the document, involving semantic key-word extraction and clustering, which is then sent to generate new augmented queries targeted to already exist-ing search engines. The returned results are semantically reranked using the context. However, searching is quite restrictive, in that it does not allow the user to specify the context by himself, instead relying entirely on the extracted context. Lowe [16] has developed a prototype of a search engine that takes context related information such as the local space defined by the navigational structure of the document to improve the pre-cision of searching. This is in keeping with some of the results presented in thesis.

Many match algorithms [3,2,4,8,17,6] have been proposed in literature to address the problem of schema integration. Many of the algorithms developed thus far rely on two primary factors to detect similarities between the schema entities: the label and the structure of the involved entities. For example, Madhavan et al. have proposed CUPID [17] that discovers mapping between schema elements based on their names, data-tural matching to establish correspondences between the schema entities. Matching in CUPID is carried out in three different phases. First phase is of Linguistic Matching that proceeds in three steps. First, Normalization of the element labels such as if it is an abbreviation, acronyms, or punctuations of some labels. Second step is that of Categorization that categorizes schema elements based on data types, schema hierarchy, and linguistic content. The third, and the last step is of Comparison which computes the linguistic similarity coefficients of schema element names. Second phase is Structure Matching that calculates the structure similarity of schema elements (a measure of similarity of the contexts in which the elements occur in the two schemas) in a bottom-up fashion. Lastly, in the third phase, the weighted match based on the linguistic and structural similarity of the pairs of elements is provided.

TranScm [19] , on the other hand, performs schema matching at the granularity of elements in a top X  X own fashion using a set of predefined rules to match node by node (referred to as vertices in the paper) on the graph representation of the input schemas. Matching at a level typically requires matching of the labels of the ver-tices as well as its descendants. Its performance deteriorates when top-level structures are not quite similar.
SKAT [20] focuses on identifying the articulation over two ontologies by comparing input schemas based on their ontological knowledge sources and user defined rules which are utilized to increase precision. Two nodes are once again matched based on their labels. Structural similarity is established based on the similarity of their hierarchical structure. Neirman et al. [21] , on the other hand, have proposed a structure-based simi-larity algorithm for XML documents, based on measuring the edit distance for rooted trees. It differs from
SUSAX in that no linguistic match algorithms are employed as part of their match algorithm, nor is there any provision for context-specific match.

Other systems such as LSD [1] and SemInt [14] use machine learning and neural networks frameworks respectively as an approach for combining different match techniques and user feedback to ultimately improve the accuracy of schema matching. LSD employs and extends current machine learning techniques to semi-automatically find mappings between input schemas. LSD uses user-supplied semantic mappings for a small tion in the input schemas. Once they are trained, LSD finds semantic mappings for a new schema by applying the learners and then combining their predictions using a meta-learner. SemInt trains neural networks to find matches between two input schemas. SemInt provides a match procedure using a classifier to categorize attri-butes according to their field specifications (15 constraint-based, and five content-based criteria) data values and, then trains a neural network to recognize similar attributes. It does not support any linguistic matching, nor does it employ structural matching. Our work now investigates the possibility of applying schema match-ing techniques to provide approximate and context-specific keyword searching. 10. Conclusions and future work
As the Web moves progressively towards more semi-structured content and as the number of XML documents on the Web increases, we have at our disposal more information that can be harnessed to provide more effective and accurate searching (than pure keyword searching), for these documents. We present in this paper SUSAX  X  an algorithm for approximate , context -specific searching over XML documents. The SUSAX algorithm is a unique blend of linguistic similarity techniques prevalent in information retrieval and schema matching domains with global sequence alignment techniques, in particular the Needleman X  X unsch global alignment algorithm, that are indispensable in the life sciences domain. Our experimental results have shown that contextual informa-tion has the potential to dramatically increase the precision of the matches obtained by a linguistic match algo-rithm. However, contextual information alone degrades the overall recall of results. Combining approximate searching together with contextual information, however, yields the best overall accuracy of the result sets. Future work
In this paper our concentration has been on the development of the approximate, context-specific search algorithm, SUSAX. There are, however, two components to the overall SUSAX approach. The first, is the pairwise comparison of the query Q with a source path P . This pairwise comparison is potentially repeated for all possible pairings of the query Q with source path P of all the source paths with the highest scoring source path ranked as the top hit and the lowest scoring source path as the lowest hit. While we had anticipated using the normalized score of the algorithm to produce the ranked list of source paths, we found that this basic assumption did not hold. Many paths with more accurate keyword matches but a lower context match were ranked lower than source paths with lower, in some cases no, keyword match but higher contextual match. For example, for the query sigmod / paper / publisher , the source path / SigmodRecord / issue / articles / article / authors was ranked lower (based on nor-malized score) than the source path / SigmodRecord / issue / articles / article . This result was counter-intuitive to what we expected. As part of the future work, our goal is to explore heuristics for the ranking of the source paths and hence the XML documents containing the relevant information.

References
