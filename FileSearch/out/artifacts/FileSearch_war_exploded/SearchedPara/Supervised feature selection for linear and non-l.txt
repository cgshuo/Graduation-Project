 1. Introduction
Monitoring the quality of meat products is a signi fi cant concern in the food industry. Supplying a consistent high quality product requires a continuous assessment in the meat industry. This requires a development of on-line inspection methods for auto-mation of the inspection process ( Sharifzadeh et al., 2012 ). Conventional assessment methods in this case are based on sub-jective visual judgment and laboratory tests which are time-con-suming, destructive and inconsistent in terms of human accuracy.
The visual appearances such as the texture pattern and the color of the meat are the main criteria for both the manufacturer and customer. These parameters are linked to the chemical properties such as the water-holding capacity, intra-muscular (marbling) and protein content ( Sun, 2010 ). As a result, surface color is an important parameter for quality measurement in the meat industry.
 One ef fi cient color space for quanti fi cation of food items is the CIELab or L n a n b n color space, due to its precise characteristics color space de fi ned by the International Commission on Illumination  X  abbreviated as CIE in 1976. L n a n b n has a perceptually equal space. This means that the Euclidean distance between two colors in the CIELab color space is strongly correlated with the human visual perception ( Tkal  X  i  X  and Tasi  X  ,2003 ). The L n is the luminance component and the a n and b n are chromatic components.
Colorimeters and spectrophotometers are traditional instru-ments for measurements of colors such as L n a n b n in the food industry. They provide a quantitative measurement in a similar way to the human eye ( Wu and Sun, 2013 ; Balaban and Odabasi, 2006 ). Colorimeters, such as the Minolta chromameter or the Hunter Lab, are used to measure the color of primary radiation sources that emit light and secondary radiation sources that re or transmit external light ( Le X n et al., 2006 ). Therefore, color values are obtained optically but not mathematically. Before doing the measurements, the instrument is usually calibrated.
Traditional instrumental measurements can only measure the surface of a sample that is uniform and rather small ( Balaban and Odabasi, 2006 ). Hence, they cannot completely represent the surface characteristics especially when it is non-uniform and highly textured as is the case for meat. In order to have a global representation of the target surface, computer vision techniques can be used to quantify the color ( Wu and Sun, 2013 ). This leads to the formationofa3DmapofL n a n b n color values. Such a map represents
Color space conversion techniques can be employed to transfer an image into the L n a n b n space with the desired numerical and visual speci fi cations. Thereby, the images o f the meat samples from other color spaces such as RGB or CMYK can be transferred into L space. In this way, it is possible to convert each image pixel into
L a b n and therefore, generalize the representation.

Reviewing the literature shows that, conversion to L n a n mainly performed using RGB images. In Larrain et al. (2008) and Mendoza et al. (2006) standard sequential transformation into
XYZ color space and then from XYZ to L n a n b n was used for RGB images of beef and vegetables respectively. In Fdhal et al. (2009) , conversion for the RGB images of the standard color patches into
L a b n was performed using BPANN. 1 In Cao and Jun (2011) and
Cao and Jun (2008) , RBFNN 2 and GRNN 3 were used for conversion from CMYK color space to CIELab respectively.

The use of RGB images has some drawbacks. An RGB image, captured by a digital camera, is formed by fi ltering the incoming photons into three broad primary channels representing the color variables; Red, Green and Blue (RGB). These three variables are enough to describe a color sensation. However, the intensity recorded in each channel is an integration over a large range of wavelengths and therefore, two objects with different spectral radiant power distribu-tion may seem to have similar colors in an RGB image. This is called metameric failure, which means matching colorimetrically under one illumination, but differ under anot her. It occurs when the spectral radiant power distribution of two objects are different, but the rough splitting of photons fails to observe this Dissing et al. (2010) .In addition, RGB is a device dependent color space and the color of an object may be slightly different in two different camera records.
Multispectral imaging is an alternative for solving these limita-tions. In a multispectral imaging system, the sampling frequency of the electromagnetic spectrum is high and images are formed in very narrow bands compared to the three broad intervals used in standard RGB imaging. Therefore, the distribution of incoming photons for each pixel is approximated correctly. Besides the visual bands that characterize the color information, the higher wavelengths such as NIR are related to the chemical character-istics. Therefore, spectral imaging has been widely used for food quality control applications ( Gamal et al., 2009 ; Dissing et al., 2009 ; Sharifzadeh et al., 2013 ).

So far, multispectral imaging has never been used in color conversion of food items. Color conversion using the spectral images can be done based on statistical predictive models. The advantage of such methods over the standard matrix transforma-tion was investigated in Le X n et al. (2006) . In that work, a sequential transformation was used for conversion of the RGB images of color samples into L n a n b n . In addition, OLS regression and ANN 5 with early stopping generalization were employed and their results showed that the ANN model obtained the best performance. In Dissing et al. (2010) , the multispectral images of the standard color patches were transformed into the CIE-XYZ using linear regression models.

This paper focuses on conversion of multispectral images (430  X  970 nm) of different types of raw meat into L n a n b n the following, we explain the main points investigated in this paper:
Since the food items can have variation, it is important to create and validate the prediction models on food products. Therefore, the use of real meat samples instead of the color patches for building the prediction models was investigated. Uncooked meat is translucent and transparent. Therefore the light re fl it, not only comes from its surface but part of it comes from below the surface. Meat also has structure due to fi bers with orientation.
The color patches do not have structure and the light is re directly from the surface. Therefore, a model built on color patches do not work well on raw meat samples.

Due to the fact that the vision systems with their spectra are costly and not feasible to implement in the industry for online food productions, the sparsity is impor tant and performing predictions using a minimum number of wavelengths would make the required vision system more cost ef fi cient. Therefore, we propose a new supervised feature selection strategy based on EN and lasso sion as a pre-processing step. The selected features were compared with PCA using three different regression strategies. A complete comparison between linear, non-linear and kernel-based regression methods was performed, which we did not see in the previous works. In order to have a general and fair judgment about the methods, the original data set was divided randomly into 25 training and test sets and the regression methods were tested on all of them and the average results were considered.

Finally, the results of the spectral images were compared with the RGB images.

The rest of the paper is organized as follows; Section 2 is about color description and Section 3 describes the data preparation.
In Section 4 , we describe linear, non-linear and kernel-based regression methods respectively. Section 5 is about the proposed supervised linear feature selection algorithm. Experimental results are presented in Section 6 . Finally, there is a conclusion for this paper in Section 7 .
 2. Color description
In principle, there are two methods for describing color; the spectral and the tristimulus data description ( X-Rite, 2004 ). Spectral data, describes the surface properties of the colored object. It demonstrates how the surface affects (re fl ects, absorbs, transmits, or emits) light. Conditions such as lighting changes, the uniqueness of each human viewer, and different rendering meth-ods have no effect on these surface properties. In this paper, the multispectral images of meat are the input images.

The tristimulus data which is a 3D color space, describes the color of an object, as it appears to human eye or sensor, and as it would be reproduced on a device such as a monitor or printer. A CIELab color could be considered as a point in a 3D coordinate color space as shown in Fig. 1 . On the other hand, RGB and CMYK color representation describe a color as three values that can be mixed to generate the color. In contrast to these color spaces, CIELab is device-independent, meaning that the range of colors in this color space does not depend on the characteristics of lightening condition. In addition, the RGB and CMYK color spaces are much smaller than the range of colors that is visible to the human eye.

In this paper, the output color is CIELab which is a uniform and and ranges from 0 to 100; a n denotes the red/green value; and b the yellow/blue value. The range of both chromatic components is between 128 and 128. This Color space resembles a three-dimensional space and uses rectangular coordinates based on the perpendicular yellow-blue, green-red and illumination axes as shown in Fig. 1 . 3. Data preparation The meat data for this work was provided by the Danish Meat Research Institute. Fig. 2 shows six different samples of meat from the used data set. In this data set, there were images of different types of turkey, chicken, beef, veal and pork.
 In order to prepare the reference L n a n b n measure, two Minolta Chroma Meters CR300 and CR400 were used. Each Minolta data was acquired at eight locations on each meat sample and the average and standard deviations of these readings were recorded. Then, the two Minolta results were averaged. The mean values were used as the reference L n ,a n and b n for each sample. The average standard deviations will be used as a reference for evaluation of the accuracy of the prediction models.

Totally, we used 52 meat samples which were divided ran-domly into training and test sets 25 times. In each data set, the number of training samples were 38 which were used for building the models and the remaining 14 samples were kept as unseen data for the test step.

For each meat sample, multispectral images were acquired at 20 different wavelengths ranging from 430 to 970 nm using a VideometerLab. VideometerLab is a multispectral imaging device. A sample is placed inside an integrating sphere. On top of the sphere, there is a camera which achieves a uniform and repro-ducible illumination. The illuminating diodes achieve the same level of intensity in all bands. They were calibrated radiometrically as well as geometrically to obtain the optimal dynamic range for each LED as well as to minimize distortions in the lens and thereby pixel-correspondence across the spectral bands. The optimal light condition avoids shadows and specular re fl ections ( Dissing et al., 2009 ).

To form the feature vectors from the multispectral images, a Region of Interest (ROI) of size 200 200 pixel was selected from each sample image. In the next step, the pixel gray levels in each ROI were averaged at each wavelength. Therefore, we fi nally have 20 features per meat sample. The feature matrix is X N P , where N denotes the number of samples and P is the number of wavelengths. The three output components are L N 1 ; A N 1
For ease of notation, we consider each of them as Y in the following sections.

One important point about the data set is that, we did not know the regions, where the measurements were performed.
This means that, there is a deviation or mismatch between the regions from which, X is formed and the regions that Y values were measured.

In order to conduct the comparative experiment with the color checker, the standard X-Rite color checker was used. As shown in
Fig. 3 , it has 24 squares of colors in an 4 6 array. The multi-spectral images of this color checker were prepared in exactly the same wavelengths and light settings as the meat samples and the data set was formed in the same way. It has 24 samples in 20 color checker is known. 4. Methods
In this section three regression strategies namely linear, non-linear and kernel-based methods that were used in this paper are explained. Due to the limited number of samples, a fi vefold CV was applied on the training data for the optimal choice of model parameters in all the methods. 4.1. Linear regression
To convert the pixel intensities in the multispectral images into L n a n b n units, we can simply use the unbiased OLS model ^
Y  X  X ^ probable that some wavelengths have higher correlation to some of the output components (L n a n b n ), selection and shrinking stra-tegies can be useful.

One simple regularization method is ridge regression which uses the L 2 norm penalty to shrink some of the regression coef fi cients. This decreases the variance of the outputs. Another ef fi cient regularization method is PLS which selects directions or components based on both the variance in the co-variates and lot of variation in X that has no connection to the variation of outputs and instead, the response is highly sensitive to the low variations of input, PLS can be a good solution. Therefore, we apply the PLS regression to improve the result, in the case such scenario exists in our data.

There are not necessarily prominent changes between images of all sequences of wavelengths and some of them are highly correlated. In this case a sparse solution such as lasso which uses the L 1 norm penalty can be employed: ^  X  argmin  X  1 2  X  N Here,  X  j is the j th coef fi cient and  X  controls the shrinkage rate.
Another sparse regression method is EN. EN is in fact a compro-mise between lasso and ridge. Each regression coef fi cient is calculated as a weighted combination of ridge and lasso. EN selects variables like lasso, and shrinks together the coef fi cients of the correlated predictors like ridge ( Hastie et al., 2008 ). The EN regression coef fi cients are computed by minimization of the following function: ^  X  where there are both L 1 and L 2 penalty terms. The sparse regres-sion methods result in the use of less wavelengths. As mentioned before, this is important regarding the economical concerns. 4.2. Non-linear regression
ANN can be used as a non-linear regression solution. Fig. 4 shows the architecture of a simple ANN for regression with one hidden layer. First, M linear combinations of the input variables are built and then each combination is transformed using an activa-tion function h  X  :  X  : where  X  ij is the weight parameter and  X  0 j is the bias. Then, the output ^ Y is constructed as a linearly weighted combination of the non-linear basis functions  X  j  X  X  X  : ^
Y  X  X ;  X   X  X  f  X  M f  X  :
 X  is an activation function which is usually, the identity function in the case of regression ( Bishop, 2006 ).

Although this non-linear model is more complex and dif fi to interpret, it may probably be more accurate for some types of data. Therefore, when there is no need for a detailed inter-pretation of the model, ANN may be a good solution which is the case for color conversion. The choice of basis function and the solution strategy for the weight parameters vary in different ANNs. In addition, the architecture of an ANN is also based on the number of hidden layers and neurons. As mentioned in Section 1 , in many previous color conversion works, different types of ANN were used. Therefore, in this work, their application was investi-gated and compared with the linear methods. 4.2.1. ANN modeling and parametrization One widely used ANN is the single hidden layer feed-forward ANN which uses a sigmoid basis function:  X  X  X  X  s j  X  X  X  X  1 1  X  exp  X  S
Here, S j is the scale parameter which controls the activation rate. A large scale may cause hard activation around 0.
Another type is the RBFNN that uses a non-linear RBF 8 based on the Euclidean distance or Mahalanobis distance (like a Gaussian kernel function):  X  X  X  X   X  j  X  J X  X  j J  X  X  6  X  where  X  j is the center vector of the j th hidden node and distance function. The RBFNN also has one hidden layer.
The parameters of the ANN models are commonly estimated by minimization of the sum of square function shown in Eq. (7) , using the BP procedure ( Hastie et al., 2008 ). This is a gradient descent process.
 E  X   X   X  X  Min  X  N
BPANN is a well-known and widely used network and it has been used for color conversion problem as mentioned in Section 1 . Although it is a powerful algorithm, it has some drawbacks. One important problem with the error function minimization for complex and fl exible models is the over-fi tting on training data and poor generalization. Because a complex model is more fl in capturing the training data behavior. Other problems are slow convergence and the possibility that the network converges to a local minimum. The ANN algorithms are also sensitive to the initial points and it is recommended to restart the algorithm several times for this reason. We applied the simple BPANN as well as the generalized BPANN with early stopping on our data set. They were also used in Le X n et al. (2006) and Fdhal et al. (2009) , for conversion from RGB into L n a n b n units. Although they worked in some of the 25 random sets, the results were poor for most of them and the average results were not satisfactory. This is because of the above mentioned problems. Due to this oscillating and unstable behavior of BP, we employed other types of BPANN.
In the literature, there are ANNs that employ different strate-gies to overcome these problems ( Bishop, 2006 ; Bishop and Tipping, 2003 ; Hagan et al., 1996 ). In this paper we applied some of these strategies and compared their results; The ANN with Adaptive learning rate and momentum term was tested to accel-erate the convergence. In addition, different regularized ANNs were used to constrain the parameters. In the following, the tested ANNs will be explained in detail. 4.2.2. ANN with adaptive learning rate and momentum term
Considering the error minimization in Eq. (7) , the gradient  X 
E  X   X   X  can be obtained by means of back-propagation of errors through the layers. This gradient is used in the family of gradient training algorithms which iteratively form: where  X  k is the current weight,  X  k is the learning rate and k is the step number and  X  k  X  E  X   X  k  X  shows the search direction. The BP gradient-based training algorithms minimize the error function using the above gradient decent or steepest descent method with constant, heuristically chosen, learning rate.

The learning rate determines how fast a network will learn the relationships between input and output patterns. A smaller value of the learning rate means a slower learning process. In fact, the optimal learning rate changes during the training process, as the algorithm moves across the performance surface. Therefore, the performance of the steepest descent algorithm would improve, if the learning rate changes during the training process. An adaptive learning rate attempts to keep the learning step size as large as possible while keeping learning stable ( Hagan et al., 1996 ).
The idea about using a momentum BP is to stabilize the weight change and smooth the osculation in the trajectory. Therefore, a fraction of the previous weight change  X   X  k is considered in updating of the current weights  X  k  X  1 . Acting like a low-pass fi lter, momentum allows the network to ignore small local minima in the error surface and slide through them. It also speeds the convergence because, when all weight changes are in the same direction, the momentum ampli fi es the learning rate.  X   X  k  X  1  X   X   X   X  k  X  1  X   X   X  where  X  is the momentum coef fi cient and should be between 0 and 1. This gives the system a certain amount of inertia since the weight vector will tend to continue moving in the same direction unless opposed by the gradient term.

Both the BP with adaptive learning rate and BP with momen-tum term were applied on the 25 data sets. 4.2.3. Regularization of ANN
Feed-Forward ANN regularization : The simplest regularizer is the quadratic in which, a penalty term is added to the error function and penalizes the sum of weights toward zero similar to the regularization of the linear methods. The results of this method were acceptable on the validation sets and some of the test sets. However, the average test results were not satisfactory, showing very unstable and oscillating response on the different sets. This may happen due to the convergence in a local minimum.
These poor results will not be presented in this paper. Instead, the Bayesian regularization was used. It is an interesting approach which estimates the ANN parameters by a probabilistic approach ( Bishop, 2006 ). Both the model output targets Y and parameters are characterized as random variables with normal distributions. Then, the Bayesian rule is applied, to calculate their prior and posterior probabilities. Consequently, the predictive distribution of the output is obtained, using the sum and product rules for probabilities as shown in Eq. (10) . For more details we refer to ( Bishop, 2006 ; Bishop and Tipping, 2003 ).
 P  X  ^ Y j X ; Y tr  X  X  where, Y tr denotes the data used for training the model. The averaging nature of the Bayesian method over many different possible solutions solves the over-fi tting problem.
Another regularized ANN that was tested is the Nr_quadratic neural regressor with a quadratic cost function from DTU:toolbox a hyperbolic tangent non-linear functions for the hidden layer and linear output layer. The weights of the ANN are optimized with a
MAP 9 approach and the quadratic error function is augmented with a Gaussian prior over the weights. An adaptive regularization is used to prevent the over fi tting. For more information, we refer to the documents provided in Kolenda et al. (2002) .

BPANN are sensitive to the number of neurons in their hidden layers. Too few neurons can lead to under fi tting and too many neurons can cause over fi tting. For this reason, for training of all the ANN algorithms described in Sections 4.2.2 and 4.2.3 , loops are used for the best choice of the number of hidden nodes. Algorithm 1 shows the procedures used to train the ANN model. In each CV iteration, there is a loop on hidden nodes size. There is also another loop which repeats the training for each fold and each hidden node size several times. This will restart the network, training from different initial points and also helps to avoid falling in a local minimum. The output network from this algorithm will be used for the test data.

Algorithm 1. Training algorithm for ANNs described in Sections 4.2.2 and 4.2.3 .
 Inputs: Training data ( X tr ; Y tr ) Initialization:
Algorithm: 1. For cv  X  1, ... ,5 repeat: 2. For nhd  X  1, ... , HD repeat: 3. For rp  X  1, ... , Rep repeat: End loops 2 and 3 End loop 1 Output: Best trained ANN and validation error RBFNN Regularization : For generalization of the RBFNN, the
GRNN is used ( Specht, 1991 ). In GRNN, the best prediction with minimum variance is obtained as the conditional mean value of Y given X . ^ Y  X  X  X  X  E  X  Y tr j X  X   X 
This could be calculated using the joint probability. GRNN uses a nonparametric approach to calculate the joint probability
P  X  X ;
Y tr  X  by a Gaussian isotropic kernel (Parzen window). The resulting probabilistic output is shown in Eq. (13) . The numerator is the sum of the weighted training targets which contribute according to their joint probabilities with the input test sample, to form the output target. The denominator normalizes the solution. ^ Y  X  X  X  X  ^
Y  X  X  X  X  where D i  X  X  X X i tr  X  T  X  X X i tr  X  and Y i tr ; X i tr values. s is the standard deviation of the Gaussian kernel and is called the smoothing parameter. As can be seen from this equation, the contribution weights are in fact the Mahalanobis distance of the test input from the training samples. This means that the closer training samples will contribute more in the prediction of the output target. The smoothing parameter has great effect on the output prediction. With larger s , more training data will contribute in the target output than with a small
In each CV iteration, we loop over different s values and repeated the training like in Algorithm 1 , for the proper choice of 4.3. Kernel-based regression
SVM 10 was used as a kernel-based method for regression. SVM is characterized based on a maximum margin algorithm. Given the set of training data f X  x 1 ; y 1  X  ; ... ;  X  x N ; y N  X g , SVM that has at most  X  deviation from the actual target y . For this aim, the features are mapped to an M-dimensional feature space using non-linear basis functions  X  h  X  x  X  X  . Then, a linear model is con-structed in this feature space: f  X  x ;
 X  X   X  M
To estimate  X  m and  X  0 , a new type of loss function called  X  sensitive loss function is used:
V  X  r  X  X 
The objective function to be minimized is as follows: min
The second term in Eq. (16) controls the complexity level of the model. This optimization leads to a kernel based solution: ^ f  X  x  X  X  h  X  x  X  T ^  X   X   X  N to Hastie et al. (2008) . 5. The proposed supervised linear feature selection
Feature selection can be used as a pre-processing step before all the explained methods. It helps to avoid over fi tting by reducing the number of trainable parameters as much as possible.
Since the sparse linear regression methods perform both feature selection and regression together, it is not expected that a feature selection step improve their results. But, for non-sparse regression methods, it can be effective.

In the case of a feed-forward ANN, with a fl exible number of hidden nodes, it is well known that the hidden layer can be regarded as taking the role of feature selection and dimension reduction. In in each CV iteration of Algorithm 1 , the loop over the number of hidden nodes performs this selection properly. It has been demonstrated that CV is a successful model selection method ( Shi and Xu, 2006 ). In addition, for ANN models, feature selection can be applied on the input variables X N P as a pre-processing step before the regression. It can be combined with any type of neural network.

One common dimension reduction method is PCA. It projects the variables orthogonally in to a new space in which, they are sorted according to their variances. Therefore, it is possible to exclude features with low variance from the model. But, PCA is an unsupervised feature selection algorithm. This means that it does not consider the important information in the target values Y tr and their dependencies to the training spectra X addition, PCA is not a sparse feature reduction method. Because each principal component is a linear combination of all the variables.

However, according to the reasons described in Section 1 , we are interested in using a minimum number of wavelengths. Although the sparse linear regression methods such as EN and lasso perform this, to improve the prediction results, we propose to use them for supervised linear feature selection. As described in Section 4.1 , these methods will remove the redundant and irrelevant variables from the model, even with low or high variance. Algorithm 2 shows the different steps of our proposed supervised feature selection algorithm to form the reduced feature sets from EN.

In Algorithm 2 ,thevector Freq wasusedtorecordthe number of times each wavelength had non-zero regression coef fi cients and w was the vector of wavelengths. EN regression wasrepeated4timesoneachofthe25inputtrainingsets.The 4 repetitions were done to cancel the effect of randomness in CV loops. At the fi nal iteration, the frequency of being non-zero for each of the 20 coef fi cients were obtained. The sorted Freq vector shows the top frequent non-zero coef fi cients. Their correspond-ing wavelengths could be found in the re-ordered version of the w vector according to the sorted Freq . At this step, the number of wavelengths, to be used as the fi nal selected features were determined. For this aim, another iteration over all possible candidate numbers (1  X  20)weretested.Inthecaseofhigher number of wavelengths (when N 5 P ) this can be reduced to a limited candidate list. The average RMSE 11 was considered as a criterion for the fi nal decision. The best number of features among the 20 candidates corresponds to the one with the minimum RMSE ( n ). Finally, the selected wavelengths were used to form the new training and test feature matrices. The same algorithm was used for feature selection by lasso. These two method were compared with PCA.
 Algorithm 2. The proposed algorithm for feature selection using EN.
 Inputs: 25 sets of ( X tr ; X ts ; Y tr ) Initialization:
Algorithm: 1. For all the 25 sets and for rep  X  1, ... ,4 repeat: End loop 1 2. For i  X  1, ... , 20 repeat: 3. For all the 25 sets and for rep  X  1, ... ,4 repeat: End loops 2 and 3
Output: 25 sets of ( Xtr EN ; Xts EN ) 6. Experimental results
In this section, fi rst the evaluation criteria for prediction models will be introduced. Then, we will show the results from the experiments on the X-Rite color checker. In the next step, the results of applying linear , non-linear and kernel-based models on all the spectral data will be presented. Then, we will show the results of the same models on the selected features from both our proposed method and PCA. Since there were many tables of results, only the box plots are illustrated here and the complete tables are presented in the appendix. The RGB images exper imental results will be shown next and also an L n a n b n image will be formed. Finally, there will be a discussion. 6.1. Evaluation measures for prediction models
R -square ( R 2 ), RMSE and  X  E measures are used for evaluation of the models.

R 2 is a statistical measure that shows the amount of data variation explained by a regression model. In order to calculate the R , RSS, 12 TSS 13 and ESS 14 are de fi ned as follows: RSS  X   X  N
The most general de fi nition of the  X  R 2  X  or coef fi cient of determination is R  X  1 RSS TSS 100  X  19  X 
In this de fi nition,  X  R 2  X  is calculated based on the unexplained variance by the model or in other words the variance of the model's error.

RMSE shows the estimated standard deviation of the error and is calculated as follows: RMSE  X 
As mentioned in Section 3 , the average standard deviation of the Minolta measurements can be used as a reference for evaluation of the prediction models. Table 1 shows the overall average of standard deviations for all the 14 samples in the25testsets.TheestimatedRMSEasthestandarddeviation of the prediction model, can be compared with these measured values.
The delta error  X  E shows the color difference. A  X  E of 1 or less is not perceptible by human eye. A  X  E between 3 and 6 is typically considered as an acceptable match in commercial applications. Since the  X  E calculations are illuminant-dependent, calculations from colors viewed or measured under different illuminants are not comparable ( Upton, 2006 ).

E  X  6.2. Color checker test results
As described in Section 1 , due to the transparency and texture structure of the raw meat, the use of multispectral images of meat may probably work better than the standard color checkers for color prediction. This was investigated by performing two experi-ments on the color checker data and meat samples.

In the fi rst experiment, the color checker data was used for training a prediction model and in the second one, the 25 meat training sets were used. Then, they were applied for prediction on the respecting training sets as well as the 25 test sets. The average results were considered. The linear sparse EN regression was used to form the prediction model for L n a n b n color components. Since the color checker data had limited number of samples ( X 24 20 LOOCV 15 was used for both experiments on the color checker and meat data. This helps to have good generalization while fi the optimal model parameters. The results are presented in Table 2 .

As can be seen, both models were capable of predicting on their own training data. But, the color checker failed to predict the color components for the meat data as expected regarding the physical characteristics of the raw meat. The negative R 2 shows the high
RSS in Eq. (19) . These results motivate us to use the multispectral images of meat to build the prediction models.

The errors in the case of the color checker training data was higher than expected. The reason for this was investigated by calculation of 0.95% con fi dence interval of the mean values of the color patches. First, the standard error of the regions of interests in the 24 patches of color was calculated from which, we computed x 24 20 for the 95% con fi dence interval of the mean values. Then, it was used for calculation of the con fi dence intervals for the three components and the averaged results were considered.

L a b
These results explain the reason for high RMSE tr for the color checker. In addition, The average values of the ROIs for the 24 color patches plus/minus the standard deviation within each region, that, although the color patches seem to be uniform, there is still variation in the spectral images of each color patch. 6.3. Linear model results
In this section the results of applying the linear regression methods described in Section 4.1 are presented. As stated before, the tables of average results on the 25 training and test sets are shown in the appendix. In Fig. 6 , the box plots of the R results over the 25 different sets are shown. The R 2 results for the
L and a n components were better than b n component. The test
RMSEs (see the appendix), show higher prediction error compared to the measurements errors shown in Table 1 . The training set results was better than the test set. The best  X  E ts was 3.12 obtained from the ridge regression. Since the 25 sets were generated randomly, possibly some of the training sets did not include the existing variation inside the original data set. Con-sidering the fact that the original data set consists of a few samples of different types of meat, the above mentioned issue, may explain some far data points from the median in the box plots.
Since we are interested in sparse solutions, the number of times that the EN and lasso regression coef fi cients were non-zero in the 25 sets are illustrated for the three components in Fig. 7 .We call this a frequency map because, it shows the frequency of having non-zero coef fi cients for each wavelength. Comparing the wavelengths with the spectrum of colors shown in the bottom of the plots, helps to fi nd which wavelengths are mostly selected by
EN and lasso. As can be seen, some near infra-red wavelengths in all cases were among the top most frequent bands. 6.4. ANN results
In this section, the results of applying the non-linear regression methods described in Section 4.2 are presented. Since in this paper different ANNs are compared, their names are contracted for the ease of notation. For feed-forward ANN, a simple one hidden layer architecture similar to the Fig. 4 was considered. The algorithm shown in Algorithm 1 was used for training the generalized feed-forward ANN with adaptive learning rate (CVHA), momentum BP (CVHM), Bayesian regularization (CVHB) and Neural regressor with quadratic cost function (CVHQ). The range of hidden neurons for training the GRNN (CVSG). However, a loop for the best choice of the smoothing value s was used instead of the hidden neurons loop. The regularized RBFNN model is a 2 layer network. For the smoothing value s , 100 different values were generated logarith-mically between 0.01 to 10.

Fig. 8 shows the box plots of R 2 test results. We can see that, there are some very far outliers from the median which may affect example, for the CVHB prediction for b n component. This may happen in ANN due to the inappropriate initial point or a convergence to a local minimum. Among the tested ANNs, the GRNN (CVSG) shows the lowest performance. Like linear models, the non-linear models work fi ne on the training data. The best training results are obtained from the CVHQ, CVHB and CVHA and for the test data, the best two models are the CVHQ and CVHB. The best  X  E ts was 3.85 obtained by CVHQ. The average training results are satisfactory however, the test results are not better than the linear models using all the 20 wavelengths (see the appendix). One reason can be the high number of input variables. Regarding the higher complexity of the ANNs than the linear models, reducing their input variables may improve the results. 6.5. SVM results
Fig. 9 shows the box plots of R 2 test results for the three components using SVM. The results of the SVM regression model does not show a signi fi cant improvement compared to the previous methods. During training the model, a linear kernel obtained the best result and was used in the fi nal model. In contrast to the previous models, there are no outliers in the output results. 6.6. Feature selection results The proposed supervised feature selection strategy based on
EN and lasso in Section 5 as well as PCA were used to reduce the number of wavelengths. Then the resulting reduced spectral data was employed in training the models.

First, a PCA analysis was performed on each of the 25 data sets. The 97% of the variation was explained just by the fi
PC components in all cases, which was a very signi fi cant reduction in data dimension. Fig. 10 shows the average of the selected PCs in the 25 data sets with respect to the wavelengths.
As can be seen, both PCs enhance the higher part of the wavelengths corresponding to the NIR wavelengths. The fi rst
PC which describes more than 90% of the variations has another peak around the red color area, that corresponds to the different color ranges of the meat samples and can explain the correlation with the a n component. However, the second component shows a negative correlation peak in the red color area. It also has two small peaks in blue and yellow ranges which explains the b color component.

The second and third sets of reduced features were formed using the Algorithm 2 in a supervised approach. Fig. 11 shows the frequency map of the 20 wavelengths by EN and lasso. Similar to the PC components, the near infra-red wavelengths have high frequencies in all cases specially, for the a n component. In addition, some visible bands were among the high frequent wavelengths.
These frequencies were sorted in a descending order and their corresponding 20 wavelengths were also re-ordered. Then, for each of the 25 training sets, a candidate subset of the top wavelengths were considered and an EN regression was applied for 4 iterations. The candidate subset length was varied from 1 to 20. The average RMSE results of these 20 candidate subsets are illustrated in Fig. 12 . The minimum RMSE corresponds to the best number of top wavelengths. Table 3 , shows the fi nal number of selected bands for each component.

The reduced sets of features obtained from the PCA and the proposed method were used to build the prediction models. Fig. 13 shows the box plots of the R 2 test results for linear, non-linear and SVM regression methods. This fi gure just shows the results of the EN-based feature selection. In the case of linear models, we can see that by using less bands, the results are better than Fig. 6 , except for the two sparse methods, EN and lasso, as we expected. Comparison of Fig. 8 with this fi gure shows that, the use of less wavelengths did not made considerable changes in the median for non-linear models. Many outliers can be seen in the both box plots of the ANN methods. The lowest median among all methods was for CVSG in all the three components. Comparing Fig. 9 for SVM with this fi gure does not show important differences.
The complete results are presented in the appendix. The PCA did not improve the results in almost all cases. Comparing the results for the ANN models show some improvements in the maximum averages obtained on the test sets. This does not mean that all the non-linear models results were improved by the proposed features. In the case of SVM results, the most prominent improvement obtained for the a n component. Comparing all the results, the best  X  E ts was 2.87 obtained from the ridge regression using the EN-based feature selection. 6.7. Comparison with RGB images
In order to investigate the effect of the number of wavelengths in the accuracy of the regression models, we have extracted the RGB components from the 20 original bands. Then, these pseudo RGB features were used to perform L n a n b n prediction using the best linear and non-linear models from the previous experiments as well as the SVM method. The average results over the 25 data sets are presented in Table 4 . The prediction result in the case of L component, is good, showing that for brightness component, the use of three RGB bands may be enough. The results for the chromatic components are worse than the multispectral bands specially in the case of the b n component. We can see that the complex non-linear methods can do signi fi cantly better predic-tions on the features from the limited RGB bands for the chromatic components, compared to the linear and kernel-based models. All the  X  E ts values are above 4.

Although a real RGB image captured by a CCD camera may not be exactly the same as the images we formed by band extraction over the multispectral images, the poor prediction results for the color components compared to the results using multispectral bands, can demonstrate the superiority of the multispectral imaging. 6.8. Displaying L n a n b n components made a prediction for all the pixe ls of a meat sample. To form these images, one of the trained ridge models on the EN-based feature selection method was used for each of the three components. Fig. 14 illustrates the pseudo RGB image and the corresponding images of the L n a n b n components. In the L n image, the main structure of the marbled meat is distinguishable. In the a n and b n image, we can observe the color variation in different parts of the meat. 6.9. Discussion We investigated the use of multispectral images of raw meat for L a n b n color prediction. Considering t hevariationintheresultsofthe same methods on the 25 random sets, the important role of an appropriate training set, covering the existing variation of the population, in success of the prediction model becomes clear. Another point is that, comparison of the best results of different models show that, the use of a sub-set of features can improve the results. In our work, the proposed sup ervised linear feature selection algorithm outperformed the PCA for all tested methods. However, the best results were obtained by applying a non-sparse linear regression method like ridge on these features. SVM was the next best method for the selected features. Although the non-linear methods are more complex and more time-consuming in training, they did not obtain higher results in average, compared to the two other methods. Their box plots show that, an inappropriate initial point or a convergence to a local minimum may affect the fi model dramatically and their average results may not improve due to these few poor outliers. On the other hand, the results show that more complex models work better on limited number of features.
The L n a n b n predictions from pseudo RGB features support this.
In addition, we found that for prediction of the L n component, simple RGB bands give good average result. But, they fails to gain acceptable results for the chromatic components.

Another important point in terms of the reduction in wave-lengths is that, for each of the three components, the reduced number of wavelengths by the proposed method can perform an acceptable prediction. The best average test results of the all three strategies and their combination with the pre-processing methods are compared in Fig. 15 . In addition, the comparison of the best
E of these four approaches are presented in Fig. 16 .

The selected features in Fig. 11 , showed high frequencies in selection of the NIR wavelengths together with some visible bands in all cases. This shows the importance of the spectral imaging. In Cao and Jun (2008) , the GRNN (CVSG) was suggested for CMYK color conversion into the L n a n b n . Considering the tested non-linear models, we can see that in the case of multispectral images of meat, this model shows the lowest performance compared to the other models. However, there was no comparison in Cao and Jun (2008) between different ANN models. In Larrain et al. (2008) , the regression of colorimeter measurements on RGB images of only beef samples gained the highest R 2 for a n nent (96%), while for the two other components it was less than 60%. In our work, the best R 2 was also obtained for a n component and the R 2 of the two other components was higher. However, the best  X  E ts in our work was less than that work (2.87 and 1.57 respectively). The main reasons are the random division and averaging over 25 test sets and also the use of different meat types (veal, beef, chicken, pork, etc.) than one item, makes the fi tting task with the prediction models more dif fi cult.
We believe that, the mismatch between the regions where measurements were performed and the ROI regions are likely one main source of error in our models. In addition, as stated before, the random division of the original data set,with limited samples of many varieties, into training and test sets can be another source of error. Because it raises the possibility that some of the training sets do not cover the existing variability inside the original data set and therefore, the average results be decreased. 7. Conclusion
In this paper, multispectral images of different kinds of raw meat were used for prediction of the L n a n b n color components, which is useful for food quality inspection. The use of meat images was preferred over the use of standard color checkers due to the special characteristics of raw meat such as transparency and structure. Results from the experiments supports this. Three regression strategies, linear, non-linear and kernel-based (SVM) were compared for color conversion. In addition, fi nding a sparse solution with a minimum number of wavelengths is important, since they are economically more effective for industrial vision systems. Therefore, a supervised linear feature selection algorithm was proposed. This method was compared with PCA using all three strategies. In order to generalize the results and make a reliable comparison between different methods, the original data set was randomly divided 25 times into training and test sets. Comparison of the results showed that the proposed feature selection strategy with non-sparse linear regression gained the best average results for all the color components. Finally, comparison with the pseudo RGB data showed the super-iority of the multispectral data for prediction of the chromatic components.
 Acknowledgments This work was (in part) fi nanced by the Center for Imaging Food Quality project which is funded by the Danish Council for Strategic Research (contract no 09-067039) within the Program Commission on Health, Food and Welfare.
 Appendix A See Tables A1  X  A10.
 References
