 Online detection of video clips that present previo usly unseen events in a video stream is still an open challenge to date. For this online new event detection (ONED) task, existing st udies mainly focus on optimizing the detection accuracy instead of the detection new events in real time, especially for large-scale video collections such as the video content available on the Web. In this paper, we propose several scalable techniques to improve the video processing speed of a baseline ONED system by orders of magnit ude without sacrificing much detection accuracy. First, we use text features alone to filter out most of the non-new-event clips and to skip those expensive but unnecessary steps including image fea ture extraction and image similarity computation. Second, we use a combination of indexing and compression methods to speed up text p rocessing. We implemented a prototype of our optimized ONED syste m on top of IBM X  X  System S. The effectiveness of our techniques is evaluated on the standard TRECVID 2005 benchmark, which demon strates that our techniques can achieve a 480-fold speedup with detection accuracy degraded less than 5%. H.5.1 [Multimedia Information Systems]: video, H.3. 3 [Information Search and Retrieval]: information fil tering, H.3.4 [Systems and Software]: performance evaluation (eff iciency and effectiveness) Algorithms, Experimentation, Performance Online new event detection, large-scale video strea ming, real-time filtering, efficiency 
For streaming video, new event detection (NED) is the task of capturing the first video clips that present previo usly unseen events. This task has practical applications in a number of domains such as intelligence gathering, financial market analysis, and news analysis, where useful information is always buried in a larg e amount of data that grows rapidly with time. Since these applicati ons are often time-critical and require fast turn-around, it is h ighly desirable to develop an online new event detection (ONED) system in practice. For instance, the US government is building a massi ve computer system that can monitor television broadcasts for a nti-terrorism purposes [8, 17], and ONED is one of the most essen tial components for this system. Moreover, with the rapi dly-increasing popularity of user generated content in multimedia sharing Web sites such as YouTube [32], ONED will provide a use ful information filtering and retrieval platform for ge neral users to automatically follow interesting stories or to disc over new events from one or more large video sources. Figure 1. Events in a video stream. Different shape s correspond to different events. Filled shapes represent the cl ips that need to be captured. 
About a decade ago, ONED on document streams starte d to gain more and more interest in the text processing commu nity [2, 3, 4, 7, 14, 16, 18, 19, 22, 25, 29, 30]. As an extension of its text counterpart, ONED on video streams has also attract ed a growing attention in the video processing community by leve raging both text and visual information [10, 11, 15, 28, 31]. The ba sic idea of video ONED systems is to compare a new clip with all the clips that arrived in the past. If their similarity values bas ed on text and visual features are all below a certain threshold, the new clip will be predicted as presenting a new event. Previous work [11] has shown that additional image information plays an importan t role in identifying the relevant video clips and achieving better topic tracking results. However, to our best knowledge, a ll these efforts on video ONED mainly focus on optimizing the detect ion accuracy instead of the detection efficiency. Actually, thes e methods yield a quadratic time complexity with respect to the numbe r of clips. Thus, they are not efficient enough to detect new video e vents in a real-time environment, especially for large-scale video collections. For example, in the intelligence gathering system being developed by the US government [8, 17], tens of thousands of tel evision channels are required to be monitored simultaneously. For Yo uTube [32], hundreds of thousands of video clips are uploaded e very day. In this case, it is very difficult for existing ONED system s to handle such an aggregated and extremely high-bandwidth video st ream in real time. 
In this paper, we propose several techniques to add ress the aforementioned efficiency problem and improve the v ideo processing rate of an ONED system by orders of magn itude without sacrificing much detection accuracy. Since the comp utation on image features is rather time-consuming, we maximiz e the efficiency of our ONED system by delaying the proce ssing of image features as much as possible. More specifical ly, we propose the following three optimization steps. First, we u se text features alone to filter out most of the non-new-event clips , so that the expensive image feature extraction step of these cl ips is waived. Then, when comparing the new clip with an old clip, we first compute their text similarity and skip the costly i mage similarity personal or classroom use is granted without fee pr ovided that copies are not made or distributed for profit or commercial advant age and that copies bea r permission and/or a fee. CIKM X 08, October 26 X 30, 2008, Napa Valley, California, USA. Copyright 2008 ACM 978-1-59593-991-3/08/10...$5.00. computation if their texts are sufficiently dissimi lar. Finally, we use a combination of indexing and compression methods t o speed up text processing. During image similarity computatio n, we also remove the anchor images to improve the detection a ccuracy of the ONED system. 
We implemented a prototype of our proposed ONED sys tem on top of IBM X  X  System S. As described in Wu et al. [1 3], System S is a stream processing middleware that provides an app lication execution environment for processing elements (or a pplications) developed by users to filter and analyze data strea ms. Our evaluation on the standard TRECVID 2005 benchmark [ 26] shows that our techniques can improve the video processin g rate by two orders of magnitude (reducing the processing time f or the entire video collection from two hours to 15 seconds) with out sacrificing much detection accuracy. 
The rest of this paper is organized as follows. Sec tion 2 introduces a baseline system. Section 3 analyzes th e overall dissimilarity value computation formula of a clip p air. Section 4 presents our techniques for improving efficiency. S ection 5 investigates the performance of our techniques. We conclude in Section 6. 
Before discussing the proposed techniques in detail , we first describe our baseline ONED system in this section. This baseline system combines the two most influential informatio n sources suggested in the state-of-the-art ONED system repor ted in Hsu and Chang [11], including TF-IDF text features and bina ry image duplicate features. Our improvements presented in t he rest of this paper are built on this baseline system. 
Figure 2 shows the architecture of the baseline ONE D system, where video streams can come from one or more multi -lingual video channels. These streams are then partitioned into shots. Each shot is about several (e.g., three) seconds long an d defined as a single continuous camera operation without an edito r's cut, fade, or dissolve. For each shot, the feature extraction mod ule both extracts image features from its keyframe, and obtains the E nglish text features by using automatic speech recognition foll owed by machine translation, so that the original video cli ps in different languages become comparable. Then the ONED componen t uses the text and image features to identify the new-event shots that present previously unseen events, and sends these s hots to a consumer, who can be either a person or a computer program that does deeper analysis. (Note that, although we use v ideo shots as the basic NED unit in this work, our following analysis is not relying on this choice and thus they are universally applicabl e to other units such as news story and so on.) 
The baseline system uses the traditional tf  X  idf term weights as the text features [23]. Since each shot S is too short to contain enough text for computing meaningful text similarity (see Section 2.3), we the same channel.) Following the convention of info rmation retrieval [23], we define a term as a unique word and vocabulary as and a shot S in a shot set E , the baseline system uses the following formulas to compute the term weight: (f1) term frequency (tf) weight )1 ln( + = tf w tf , (f2) inverse document frequency (idf) weight (f3) term (tf  X  idf) weight idf tf t w w w  X  = . text of S , N is the total number of shots in E , and df is the number of shots in E whose texts contain t . 
In practice, there are many different ways to extra ct image features that are (almost equally) suitable for det ecting near-duplicate images. Our current baseline system uses the color moment feature described in Campbell et al. [6], wh ere the localized color statistics are extracted from a 3  X  keyframe image, and the first three moments for eac h grid in Lab color space are used to construct the n=81 image features f ( n i  X   X  1 ) of S [6]. The IBM TALES (Translingual Automatic Language Exploitation) system [12, 20] can use computer clus ters to perform both image and text feature extraction on v ideo streams from thousands of channels simultaneously with a de lay of about paper, we focus on the ONED components that existin g systems cannot complete in real time. 
To detect new-event shots in a video ONED system, w e need to compute the dissimilarity between two shots S 1 and S text and image features. The smaller the dissimilar ity is, the more likely S 1 and S 2 are to present the same event. We show the dissimilarity computation method as follows. First, the text dissimilarity value is obtained using (f4) and (f5) : (f4) normalized text dot product value (f5) text dissimilarity value where best detection accuracy in the latest TDT5 competit ion [25] for ONED on document streams. Next, we obtain the image dissimilarity value using (f6) and (f7) [11]: (f6) normalized image dissimilarity value (f7) binarized image dissimilarity value where for binarizing the image dissimilarity, and I is the indicator function. That is, the binarized image dissimilarity is 1 if the normalized combination of the text dissimilarity value and the binarized image dissimilarity value according to (f8): (f8) where w image is the linear weight for the visual modality. As mentioned in Hsu and Chang [11], such a linear fusi on model is one of the most effective approaches to fuse visual and text modalities in video ONED systems. 
In this section, we present the details of the base line system. We follow the typical pre-processing operations in inf ormation retrieval standard Porter stemmer [21], and (2) stopwords are removed by using the standard SMART stopword list [24]. Note t hat, the shot set E keeps changing as new shots continue to arrive in a video streaming environment. As mentioned in Braun and Ka neshiro [5], for ONED purpose, the computation of the tf and idf weights can be based on a static shot set E' that has characteristics similar to E . For a term that does not exist in the text of E' , its df is assumed as one. Compared to the method that incrementally updates t he statistics N and df , this static method has a much lower overhead, whi le the detection accuracy remains roughly the same [5]. 
When a shot S arrives, S is first pre-processed and its features are saved in memory. Then S is compared with all the old shots that arrived in the past except for the L=50 shots that just arrived from same news story segment as S . If all the overall dissimilarity values between S and the old shots are above a threshold T , S is predicted to be a new-event shot. Otherwise if the overall di ssimilarity value between S and an old shot S old is below T , S is predicted to present the same event as S old . 
The experiments presented by Hsu and Chang [11] hav e shown that although text features are the most effective component in detecting new events, visual near-duplicates can st ill consistently enhance the detection accuracy of the text baseline . To be more specific, using both text and image features can im prove the detection accuracy of the text baseline by up to 25 %. This can be explained by the fact that similar images in two sh ots often provide evidence that they present the same event, even if their associated speech transcript may not be sufficientl y similar due to paraphrasing or speech recognition/translation erro rs [10]. example, which is about a Korean violinist X  X  perfor mance in example, which is about Chinese president Jintao Hu  X  X  visit of 
To illustrate this issue, we provide two examples i n Figure 3 and Figure 4. Both examples come from the TRECVID 2 005 (a) Keyframe image of the first shot, which was broadcasted at 12am on Nov. 18, 2004. (a) Keyframe image of the first shot, which was bro adcasted at 7pm on Nov. 17, 2004. video collection [26], each of which contains two s hots that present the same event but were broadcasted at diff erent times. The first example is about a Korean violinist X  X  per formance in Toronto. The second example is about Chinese presid ent Jintao Hu X  X  visit to Argentina. In both examples, when we use the default parameter settings in the baseline system, text features by themselves cannot correctly detect that these two s hots are presenting the same event. (The threshold for the n ormalized text dot product value is T w image  X  + 1 when only text features are used, as described in Section 3.) However, by consi dering the additional evidence from image features, the system can produce the correct predictions. 
To provide more insight on the overall dissimilarit y value, we rewrite the original dissimilarity formula (f8) int o an equivalent form that treats text and image features asymmetric ally. We further analyze this alternative form to show how the NED p rocess can be more efficient. To begin, we substitute the formula s (f5) and (f7) into (f8) and rewrite the overall dissimilarity of S 1 and S (f9) We analyze (f9) by considering two possible cases, while either case has two sub-cases: (1) When the keyframes of S 1 and S 2 are near-duplicate images, (2) When the keyframes of S 1 and S 2 are not near-duplicate images, Figure 5 illustrates the above four sub-cases. 
For any two shots S 1 and S 2 , it seems that we must use both their text/image features and check all of the above four sub-cases to determine whether they present the same event. Howe ver, this turns to be overkill in many cases. By treating text and image asymmetrically, we can greatly simplify the NED ope ration by rewriting the above four sub-cases into the followi ng equivalent three cases (see Figure 5), among which only Case 2 has two sub-cases: (1) Case 1 : (2) Case 2 : T w dotprod text T (3) Case 3 : 
In the above cases, both Case 1 and Case 3 only req uire the text features of shots S 1 and S 2 . Hence, for ONED purpose, text features and image features can be treated asymmetrically, i .e., we can use text features as a pre-filter to filter out most of the unnecessary operations on image features. This can bring a huge benefit to the detection efficiency, because the text similarities of most shot pairs are low [18], and hence Case 1 is the most frequent ly occurring case. On the other hand, it is undesirable to process ima ge features before text features because using image features alone ca nnot determine whether S 1 and S 2 present the same event [11]. 
In this section, we describe our techniques for imp roving the efficiency of the ONED system based on the analysis of Section 3. We first give a high-level overview of our optimize d ONED system, and then elaborate on the individual techni ques. Figure 6. Our optimized online new event detection system. Figure 6 shows the architecture of our optimized ON ED system. Video streams from one or more channels are divided into shots. For each shot S , the text features are extracted by using speech recognition as well as machine translation techniqu es. The text features are used to identify and remove the non-ne ws shots. The remaining news shots are fed to the ONED component, where new-event shots are identified and sent to the cons umer. During the ONED process, we extract the image features of S only when text feature extraction module it is necessary to determine whether the keyframe o f S is an anchor image and to compute the image similarities between S and the old shots. Figure 7 shows the high-level de scription of the algorithm used in the ONED component. The details o f this algorithm are explained in Sections 4.3-4.6. Image_feature_extraction_flag=FALSE; /* whether ima ge First_story_shot_flag=TRUE; // whether S is a new-event Use the pre-filtering method described in Section 4 .6 to identify the old shots that need to be compared wit h S . For each such old shot S old { 
Compute 
If (
Else if ( } 
Else if ( T w dotprod text T } /* end of Case 2 */ } /* end of the for loop */ If (First_story_shot_flag) { If (!Image_feature_extraction_flag) { } Save S  X  X  information in memory; 
Send S to the consumer of the ONED system; } 
Figure 7. High-level description of the algorithm u sed in the In broadcast videos, non-news video segments (e.g., commercials, TV shows) are usually mixed with news stories. For ONED purpose, non-news shots should not be treated as new-event s hots, even if no similar shots have appeared before. Removing these shots can not only reduce the number of shots that need to be pro cessed by the ONED component, but also improve the efficiency and the detection accuracy of the ONED system. 
To this end, a simple method is to manually specify the regular time periods when news videos are broadcasted. Howe ver, such a method is not scalable to tens of thousands of chan nels, as is the typical case that an ONED system needs to handle [8 , 17]. Moreover, our purpose here is to remove all the non -news shots rather than commercials only [1, 9, 10]. As an alte rnative, we apply a simple text-based method to remove the non-news s hots. Its basic idea is that non-news shots (e.g., commercials) oft en have larger background noise than news shots, which makes it di fficult for the speech recognizer to recognize the text in the non-news video. Also, non-news shots (e.g., TV shows). Based on these two properties, we predict that a shot S is not news if the recognized text of S contains fewer than J distinct terms where J is a predetermined constant. Our experiments in Section 5.1 show that a reasonable c hoice for J is usually between 50 and 80. Although this method is rather simple, it is highly accurate and has a low overhead that help s to improve the efficiency of the ONED system. Also, the expensive image feature extraction step is no longer needed for the dropped non-news shots. 
As mentioned in Section 3, it is desirable to delay the processing of image features as much as possible. As shown in Figure 6 and Figure 7, when processing a new shot S , we first extract its text features but not its image features. When comparing S with an old shot S old , we first compute their normalized text dot produc t instead of their image dissimilarity. If of Section 3), we predict that S and S old present different events. If and S old present the same event. In both Case 1 and Case 3, we skip the costly but unnecessary image dissimilarity comp utation step. Only in Case 2 (when T w dotprod text T need to compute the image dissimilarity. Since the text dot products of most pairs of shots are low [18], Case 2 usually occurs much less frequently than Case 1 and Case 3. Consequently, mo st image dissimilarity computations can be saved. 
Moreover, when we make the prediction that a new sh ot is not a new event, if all the compared old shots belong to either Case 1 or Case 3, we can skip the expensive image feature ext raction step. In other words, we only need to extract image features for a new shot S when either we predict that S is a new-event shot or we have in the presence of a large number of channels, most shots will be presenting existing events due to the repeated ment ion of the same event both across different channels and within the same channel [25, 18]. Also, Case 1 and Case 3 occur much more f requently than Case 2. Thus, we can skip the expensive image featu re extraction step for a large fraction of the shots. 
In news videos, news stories are typically broadcas ted by anchor persons. Figure 8 shows an image example of an anchor person from the CNN news. Two news shots from the s ame channel often have keyframes with the same anchor p erson, but present different events. However, in this case, th e similar keyframes should not be treated as a hint that thes e two shots present the same event. To take this factor into ac count, we use the method described in Campbell et al. [6] to dete ct which keyframes are anchor images based on Support Vector Machines and low-level color correlogram features. When comp aring two shots, we set the binarized image dissimilarity to be 1 if the keyframe of either shot is an anchor image. That is to say, we treat their keyframes to be dissimilar if either of them is an anchor shot. This can reduce the effect of the false evidence of anchor shots on the detection accuracy of the ONED system. 
Typically, the discussion of an event only lasts fo r a finite amount of time in news videos, and a new shot is un likely to present memory the information of those old shots that are within a sliding window of the last W days. Here W is a predetermined constant. The image features (see Section 4.6 for details) but no t its video images, as only these features are needed for comparing S with future shots. Once an old shot expires from the sliding window, i ts information is thrown away immediately. 
Typically, an event is presented by a large number of shots. Only the same event tend to be similar to each other. Th erefore, it is overkill to compare a new shot with all the old sho ts that present the same event. Instead, we only keep the information o f the new-event shots. When a new shot S arrives, S is compared with the old new-event shots. If S is predicted to be a new-event shot that presents a new event, S  X  X  information is saved in memory. Otherwise S is discarded. 
All the terms in the text of a shot can be sorted in descending order of their term weights. In general, those term s with larger weights are more important for NED. Hence, for each saved shot, we keep only the top-K terms with the largest weights rather than all the terms. Here K is a predetermined constant. Only the top-K terms are used to compute the text dot product. 
To reduce the overhead of computing dissimilarity v alues, a pre-filtering technique is developed by using a low-ove rhead method to quickly filter out most of the shots that present d ifferent events from the new shot. In this way, we can substantially red uce the number of dissimilarity values that need to be computed. Cons ider two shots S appear in the top terms of both S 1  X  X  text and S top terms can be used to quickly filter out unneces sary computations. More specifically, we have a predeter mined constant M ( M  X  K ). Before computing the text dot product of S first check whether the top-M terms of S 1 and S 2 intersect. If so, we continue to compute the text dot product of S 1 and S we predict that S 1 and S 2 present different events and do not compute their text dot product. 
We build indices to avoid unnecessary processing of the shots that have been pre-filtered out. Each term in the v ocabulary has a Two indices are kept for all the saved shots: a for ward index and an inverted index. The forward index has an entry for each saved shot. These entries are sorted in descending order of sho ts X  arrival time. This allows us to quickly identify and drop the inf ormation of those shots that have expired from the sliding window of the last W days (see Section 4.5). For each saved shot, the corresp onding entry keeps both the image features and the top-K terms associated with their term weights. These terms are sorted in ascen ding order of their term ids. Consequently, the text dot product of two shots can be computed through an efficient  X  X erge X  of their t erm lists. 
For each saved shot, only its top-M terms are tracked by the inverted index. The inverted index has an entry for each term in the vocabulary. The entry for term t is a posting (linked) list of the shot ids of all the shots whose top-M terms contain t . These shot ids are sorted in descending order so that merging posting lists can be done efficiently. When a new shot S arrives, we only scan the M posting merged together to find the shot ids of the candida te shots that may present the same event as S . This is the pre-filtering technique described above. Then for each such candidate shot S c , the forward index is used to compute the text dot product and t he image dissimilarity (if needed) of S and S c . This computation is performed at the same time that candidate shot ids are genera ted. In this way, if processing for S stops immediately. Otherwise if S is predicted to be a new-event shot, S  X  X  information can be easily added into the inverted index, as S  X  X  shot id is larger than the shot ids of the saved shots. 
We implemented a prototype of our optimized ONED sy stem on top of IBM X  X  System S [13]. Our implementation uses two processing elements (PEs) that consume and produce streams of data through input and output ports, respectively. One PE produces the video stream and sends it to another PE that im plements ONED. 
To evaluate the performance of the proposed system, we use the largest available video retrieval benchmark, TR ECVID 2005 [26]. This benchmark includes 171 hours of videos f rom six channels in three languages (Arabic, English, and C hinese). The time span is from October 30 to December 1, 2004. O ur measurements were performed on two computers, each with one 1.6GHz processor, 1GB main memory, one 75GB disk, a nd running Linux. The default parameters in our ONED system are as fo llows: T=0.9 (the threshold for the overall dissimilarity value ), w (the weight for the visual modality), T image =0.2 (the threshold value for image dissimilarity), J=70 (the threshold of the number of distinct terms for determining whether a shot is ne ws), W=29 (the sliding window size in days), K=250 (the number of top terms kept in each saved shot), and M=10 (the number of top terms used for pre-filtering purpose). 
For ONED on text document streams, Luo et al. [18] has shown that those techniques proposed in Sections 4.5 and 4.6 can significantly improve the processing rate of the ON ED system without sacrificing much detection accuracy. In our experiments on the TRECVID 2005 collection, we also found that the output results of the ONED system differ by only 5% when t hose techniques are used and when they are not used. Thi s also confirms that those techniques do not have much imp act on the detection accuracy of the ONED system. Moreover, Lu o et al. [18] showed that the default values of the parameters W , K , and M are reasonable in detecting new events for news streams . Therefore, in this section, we focus on evaluating the techniq ues described in Sections 4.2~4.4 by varying the values of the param eters T , W T image , and J . Among these four parameters, T , W are also needed in the baseline system. At present, there is no publicly available benchmar k for video ONED with official annotation. It is difficult to l abel the ground truth for ONED on the TRECVID 2005 video collection by ourselves, as the labeling procedure used by the or ganizations responsible for creating benchmarks typically invol ves two steps [27]: (1) running multiple different systems on the video set to generate the candidate results; (2) hiring professi onal analysts to make the judgment. Nevertheless, this issue should not become a problem in the evaluation because we mainly focus o n improving the detection efficiency. Moreover, our technique o f delaying the processing of image features (Section 4.3) does not affect the detection accuracy of the ONED system. It is also e asy to see that our techniques of removing non-news shots (Section 4.2) and handling anchor images (Section 4.4) can improve th e detection accuracy of the ONED system, as it is widely known that similar techniques can improve the accuracy of video retrie val [6]. Therefore, in our experiments, we focus on the proc essing speed of the ONED system rather than on the detection acc uracy. 
In this section, we provide some justification for the default parameters T=0.9 , w image =0.1 , and T image =0.2 . Our experiments in Section 5.2 show that the effectiveness of our tech niques is insensitive to these exact values within a fairly l arge range. 
We first consider the default parameter of T manually verify that by choosing this parameter and using the n=81 color moments described in Section 2.2 as image fea tures, the following two conditions are satisfied. First, for any two shots S and S 2 , if their keyframe images are judged to be similar according to formula (f7) in Section 2.3, these images are in deed similar and provide clue that S 1 and S 2 present the same event, as shown in Figure 4 and Figure 5. This ensures that T image is not too large. Second, we have a sufficient number of reasonably s imilar keyframe image pairs among all the shots. Note that if T small, we are basically only allowing identical ima ges to be judged as similar images and hence the image features have almost no effect in the overall dissimilarity value computati on formula (f8), which is undesirable [11]. In our experiments, we f ind that within the range [0.15, 0.25], the exact value of T image does not have much impact on the performance of the ONED system. 
Next, we discuss the other two default parameters T=0.9 and w image =0.1 . Consider a special case where the keyframe images of going back to the traditional case of ONED on docum ent streams [18], where image features are unavailable and we c an only use text features. Then according to formula (f9) in Section 3, for any two shots, image w T +  X  1 is the threshold value for using their normalized text dot product value to determine whet her they present the same event. As has been shown in Braun and Kane shiro [5] for ONED on document streams, the optimal threshold val ue for the text dot product value is around 0.2. Thus, we shou ld have contribution w image be half of that threshold value, we have 
In this section, we report the efficiency of the pr oposed algorithm and carry out a series of sensitivity ana lysis to evaluate the impact of parameters on the performance of the ONED system. The TRECVID 2005 collection was originally used as the benchmark for video retrieval rather than ONED. We found that its size is too small to evaluate ONED systems appr opriately and results in certain undesirable effects. So we make the following adjustments to compensate for these undesirable eff ects. 
First, when we measure the system throughput and th e total video set processing time, we do not include the ti me spent on extracting text and image features. Instead, we mea sure the feature extraction time separately. This is because the time spent on extracting text and image features is proportion al to the number of shots processed, while the time spent on the ONED component is quadratic with respect to the number o f shots processed, which is much more expensive in a typica l ONED situation where tens of thousands of channels are h andled simultaneously [8, 17]. 
Second, due to the small size of the TRECVID 2005 v ideo set, few shots there have the chance of getting repeated . Consequently, most news shots there (a rough estimate is about 65 %) are predicted as new-event shots. However, in a large scale ONED environment, we would expect most news shots to be non-new-event ones. For example, TDT5 [25] is the standard benchmark for ON ED on text document streams. It has one hundred times more (vo ice) text than the TRECVID 2005 video set, and a rough estimate is that about 85% of all the documents in that benchmark are non-new-event documents [18]. To compensate this effect, we only consider non-new-event shots when measuring the percentage of ne ws shots whose image feature extraction steps are saved. If this percentage is large, we would expect the corresponding percentage for all the news shots (i.e., both new-event shots and non-new-event shots) to be large in a typical video ONED scenario. 
Our main results are as follows. Using the proposed techniques for improving efficiency, it takes 15 seconds to pr ocess all the shots in the TRECVID 2005 video set. In contrast, t he baseline system described in Section 2 uses 7,203 seconds (t wo hours) to process the same shots. Compared to the baseline, o ur techniques improve the efficiency by two orders of magnitude ( 480 times). In the following experiments, we varied the value of e ach parameter while keeping the other parameters fixed in order t o analyze the sensitivity of their value settings. J (the threshold of the number of distinct terms for determining whether a shot is news) 
The first experiment concerns J , the threshold of the number of distinct terms for determining whether a shot is ne ws. We varied J from 40 to 100. To test the identification accuracy of our method described in Section 4.2, we randomly selected one hour X  X  video sequence in the entire TRECVID 2005 video set and m anually labeled the non-news shots and the news shots. (We also tested a few other video sequences in the TRECVID 2005 video set and the results are similar.) Figures 9 and 10 show the imp act of J on both the precision and recall of identifying non-news sh ots and news recall drops significantly. A good value for J is between 50 and 80, where we can obtain both good precision and good re call. 
Figure 11 shows the impact of J on the percentage of identified non-news shots in the entire TRECVID 2005 video set . Figure 12 shows the impact of J on the throughput of our ONED system. The larger the J , the more shots are identified and dropped as non-news shots and hence the higher the processing rate of t he ONED system. In the default case that J=70 , 75% of all the shots are identified and dropped as non-news shots. T (threshold for the overall dissimilarity value) The second experiment concerns T , the threshold for the overall dissimilarity value. We varied T from 0.85 to 0.95. As mentioned in Section 4.3, delaying the processing of image featu res can save many unnecessary image dissimilarity computations. Figure 13 shows the impact of T on the percentage of saved image dissimilarity computations. Most shot pairs present different events and have small normalized text dot product values. That is, most fall into Case 1. Thus, the percentage of saved ima ge dissimilarity computations decreases as T increases. In the default case that T=0.9 and J=70 , 77% of all the image dissimilarity computations are saved. w image (weight for the visual modality) The third experiment concerns w image , the weight for the visual modality. We varied w image from 0.05 to 0.15. As mentioned in Section 4.3, delaying the processing of image featu res can waive the expensive image feature extraction step for a large number of shots. Figure 14 shows the impact of w image on the percentage of non-new-event news shots whose image feature extraction ste ps are saved. The larger the w image , the fewer shot pairs fall into Case 3 of Section 3 ( feature extraction steps can be saved. Consequently , the percentage of non-new-event news shots whose image feature ext raction steps are saved decreases as w image increases. In the default case that w image =0.1 and J=70 , 45% of all the image feature extraction steps are saved for the non-new-event news shots. 
On our computer, for each 3-second shot, it takes a bout 2 seconds to extract the text features by performing speech recognition followed by machine translation, and it takes another 2 seconds to extract the image features. Hence, in the case that 75% of all the shots are dropped as non-news shots, 85% of all the news shots are identified as non-new-event shots, a nd 45% of the image feature extraction steps are skipped for thes e non-new-event shots, we can save about 85% (75%+25%  X  85%  X  45%) of all the image feature extraction steps, or equivalently 40% of the total overhead on text and image feature extraction for a ll the shots. T image (threshold value for image dissimilarity) 
The fourth experiment concerns T image , the threshold value for image dissimilarity. We varied T image from 0.1 to 0.3. Figure 15 shows the impact of T image on the throughput of our ONED system. For each keyframe image of a shot, very few (if any ) keyframe images of the other shots are similar to it. As lon g as T a reasonable range, these images can pass the image similarity filtering condition of the concrete value of T image . Consequently, T effect on the throughput of our ONED system. 
This paper proposes several techniques for improvin g the efficiency of online new event detection on video s treams so that video ONED becomes real-time. We implemented a prot otype of our framework on top of a stream processing middlew are. Our experiments with the standard TRECVID 2005 benchmar k show that the proposed techniques can improve the video processing rate by two orders of magnitude without sacrificing much detection accuracy (less than 5%). Also, the effectiveness of our techniques is insensitive to the choice of the exact parameter va lues. We thank Jerome L. Quinn for answering our question s on the IBM TALES system. [1] L. Agnihotri, N. Dimitrova, and T. McGee et al . Envolvable [2] J. Allan, V. Lavrenko, and H. Jin. First Story Detection in TDT [3] J. Allan, R. Papka, and V. Lavrenko. On-Line N ew Event [4] T. Brants, F. Chen. A System for New Event Det ection. SIGIR [5] R. Braun, R. Kaneshiro. Exploiting Topic Pragm atics for New [6] M. Campbell, S. Ebadollahi, and D. Joshi et al . IBM Research [7] F. Chen, A. Farahat, and T. Brants. Story Link Detection and [8] M. Clayton. US Plans Massive Data Sweep. The C hristian [9] P. Duygulu, M. Chen, and A.G. Hauptmann. Compa rison and [10] P. Duygulu, J. Pan, and D.A. Forsyth. Towards Auto-[11] W. Hsu, S. Chang. Topic Tracking across Broadc ast News [12] IBM Technology Translates Arabic Media Broadca sts to [13] K. Wu, P.S. Yu, and B. Gedik et al. Challenges and [14] G. Kumaran, J. Allan. Text Classification and Named Entities [15] J.R. Kender, M.R. Naphade. Visual Concepts for News Story [16] X. Li, B.W. Croft. Novelty Detection Based on Sentence Level [17] E. Lipton. Software to Monitor Overseas Opinio ns of U.S. The [18] G. Luo, C. Tang, and P.S. Yu. Resource-Adaptiv e Real-Time [19] Z. Li, B. Wang, and M. Li et al. A Probabilist ic Model for [20] R. Peterson. IBM Strives for Super Human Speec h. [21] M.F. Porter. An Algorithm for Suffix Stripping . Program 14(3): [22] N. Stokes, J. Carthy. Combining Semantic and S yntactic [23] A. Singhal. Modern Information Retrieval: A Br ief Overview. [24] SMART Stopword List. [25] TDT Homepage. http://www.nist.gov/speech/tests /tdt. [26] TREC Video Retrieval Evaluation. http://www-[27] E.M. Voorhees. Overview of TREC 2005. TREC 200 5: 1-15. [28] X. Wu, C. Ngo, and Q. Li. Threading and Autodo cumenting [29] Y. Yang, T. Pierce, and J.G. Carbonell. A Stud y of [30] Y. Yang, J. Zhang, and J.G. Carbonell et al. T opic-conditioned [31] Y. Zhai, M. Shah. Tracking News Stories across Different [32] YouTube Homepage. http://www.youtube.com. 
