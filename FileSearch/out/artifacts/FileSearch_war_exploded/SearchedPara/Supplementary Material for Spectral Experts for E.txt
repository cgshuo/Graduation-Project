 Arun Tejasvi Chaganty chaganty@cs.stanford.edu Percy Liang pliang@cs.stanford.edu Let [ n ] = { 1 ,...,n } denote the first n positive integers. x  X  R d ; i.e. x  X  p i between two p -th order tensors:  X  X,Y  X  = P i if for all i,j  X  [ d ] p which are permutations of each other, X i index is equal to j .
 average operator norm over all p unfoldings.
 P index vectors k with that count profile. assume we are given data ( x i ,y i )  X  X  p generated by the following process,  X  1 ( x ) =  X   X  h  X  M 1 ,x  X  + (2)  X  2 ( x ) =  X   X   X  2 h  X  M 2 ,x  X  2  X  + 2  X   X  h ,x  X  + ( 2  X  E [ 2 ]) (3) We assume that k x i k X  R , k  X  h k X  L and | | X  S .
 which let us succinctly represent the low-rank regression problem as follows, where we have used  X  p to represent the vector [  X  p ( x )] x  X  X  convexity constant  X  ( X p ) such that Then the error of  X  M p is bounded as follows: k  X  M p  X  M  X  p k F  X   X  n If then, with probability at least 1  X   X  , Proof Recall the definition of  X  ( X p ), Expanding the definition of the observation operator: Unfolding the tensors, letting  X   X  p def = 1 n P ( x,y )  X  X  k vec( X ) k 2  X k cvec( X ) k 2  X  p ! k vec( X ) k 2 . Then, we have By Weyl X  X  theorem, trates in Frobenius norm. Applying Lemma 5, with probability at least 1  X   X  , Now we seek to control k  X  p k F . Since k x k 2  X  R , we can use the bound lowing holds, for each p  X  X  1 , 2 , 3 } .
 (recall the D p  X  X  are independent to simplify the analysis). By definition, completing the proof.
 to bound each  X  p ( x ), We have used inequality k M 1  X   X  h k 2  X  2 L above.
 Bounding  X  E [  X  p ( x ) x  X  p ] k  X  p ( x ) x holds with probability at least 1  X   X  1 ,  X  E 3 [  X  3 ( x ) x  X  3 ]  X  E [  X  3 ( x ) x  X  3 | x ] for p  X  X  1 , 2 , 3 } , we get our result. lemma, M and, for some such that least 1  X   X  , for all h  X  [ k ] .
 estimate,  X  X , of X in the operator norm.
 Also define W  X  and  X  W  X  to be their pseudo-inverses.
 get, Consequently, T has orthogonal eigenvectors, with eigenvalues 1 / We can relate k  X  W k and  X  W to  X  M that  X  M Thus, tensor power method; for all h  X  [ k ], where (  X  W ) min is the smallest eigenvalue of T . the error in  X  and  X  to the error in  X  W and  X  .
 k  X   X  h  X   X  h k 2 = k  X  W  X   X   X   X  W  X   X  k 2 get, This allows us to simplify the above expression as follows, and k  X   X  h  X   X  h k 2  X  , i.e. as well as, probability at least 1  X   X  , Proof Define Z i = X i  X  E [ X ].
 by the bounded assumption of X i and the triangle inequality. Re-arranging: bound E [ f 2 ]: where the cross terms are zero by independence of the Z i  X  X . M because k M k F = k vec( M ) k 2 .
 Proposition 6 (Perturbation Bounds on Whitening Matrices) Let A be a rank-k W and W  X  , International Conference on Machine Learning , 2013.
 137, 2011.
