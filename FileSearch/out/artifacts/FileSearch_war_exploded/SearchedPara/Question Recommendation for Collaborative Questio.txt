 Collaborative question answering (CQA) communities rely on user participation for their success. This paper presents a supervised Bayesian approach to model expertise in on-line CQA communities with application to question recom-mendation, aimed at reducing waiting times for responses and avoiding question starvation. We propose a novel algo-rithm called RankSLDA which extends the supervised La-tent Dirichlet Allocation model by considering a learning-to-rank paradigm. This allows us to exploit the inherent collaborative effects that are present in CQA communities where users tend to answer questions in their topics of exper-tise. Users can thus be modeled on the basis of the topics in which they demonstrate expertise. In the supervised stage of the method we model the pairwise order of expertise of users on a given question. We compare RankSLDA against several alternative methods on data from the Cross Validate com-munity, part of the Stack Exchange network. RankSLDA outperforms all alternative methods by a significant margin. H.4 [ Information Systems Applications ]: Miscellaneous Learning to Rank; Supervised Latent Dirichlet Allocation; Latent Topics Model; Question Recommendation; Commu-nity Question Answering; Expert Modeling
Many users resort to the Internet for finding answers to their questions and solving their information needs [22]. Com-munity Question Answering (CQA) websites, such as Yahoo! Answers or Stack Overflow, allow users to formulate ques-tions to leverage the expertise of other members participat-ing in the community. In the past years, CQA websites have rapidly grown in size and popularity. For instance, Yahoo! Answers includes over 300 million questions posted since it launched in 2005; only during 2012, it received an average of 7 , 000 questions and 21 , 000 answers per hour 1 .
CQA websites are heavily dependent on community par-ticipation. While most provide passive question discovery tools, such as search or unanswered question lists, they still require significant effort from expert users to find the ques-tions to which they can provide responses. This additional load on the community can potentially reduce participation and degrade the overall quality of the content.

A solution to this problem studied in previous work con-siders the task of automatically finding potential responders to questions [24, 10, 13, 18, 19, 11, 6, 20, 8]. These question recommendation systems aim at increasing participation by proactively warning users about the presence of questions suitable to their interests and expertise. The task of match-ing experts to questions given their previous answers to sim-ilar questions can be posed as a Recommendation problem. The expected result of implementing such a system is that less questions are left unanswered and the elapsed time for answers is significantly decreased. Note that such a sys-tem can be effectively used along with other participation encouragement tools, such as rewards, karma points, etc.
One common feature of most CQA systems is the presence of community feedback tools, which serve as a crowd-sourced and distributed curation mechanism. Users can easily vote, positively or negatively, for questions (if they consider them interesting for future reference) or answers (if they correctly solve the problem stated in the associated question). All votes casted for a post, and hence for the user that posted it, get aggregated into a single score, which serves as a proxy for question/answer quality. This rich source of information can be used for modeling user expertise.

State-of-the-art Recommender Systems are often based on factor models e.g. [9, 17, 12, 14]. Factor-based collaborative filtering methods represent both  X  X tems X  and  X  X sers X  with a vector of latent features. In our problem setting, the pres-ence of text (questions and answers) allows us to use a more interpretable basis for modeling the experts and the ques-tions, namely topic modeling [3]. The main idea behind the model we introduce is that users with expertise in similar topics are likely to answer similar questions, we use this fact both to recommend questions to experts but also to reinforce the learning of the corpus topics.
 In this paper, we present RankSLDA , Rank Supervised Latent Dirichlet Allocation, a supervised probabilistic topic model with direct application to question recommendation. RankSLDA is based on a Bayesian inference framework that
Source: http://searchengineland.com extends the LDA model to account for the authorship of questions and answers as well as for community feedback. The proposed model combines the semantic content mod-eling benefits of LDA with supervised ranking learning to model the observed community scores based on the latent topics assigned to each question.

The contribution of this paper is two-fold. First, we pro-pose a novel learning-to-rank extension to supervised LDA, and provide the derivation of a Gibbs sampler to perform inference. Second, we apply this model to the question rec-ommendation task and provide experimental results of its performance compared to several state-of-the art methods.
The paper is organized as follows. In Section 2 we review previous literature on related topics. In Section 3 we provide an in-depth description of the generative model proposed, and the procedure to train and tune its parameters. In Sec-tion 4 we describe the setup used to study the proposed method, and in Section 5 we present the results obtained. We conclude with Section 6.
A body of literature exists around expertise modeling in the CQA context. Two main approaches have been used to this end: network and content-based. Network-based meth-ods are specifically targeted to CQA communities with a narrow topic focus. Networks built from the interaction of users in question threads are analyzed to infer their rela-tive expertise order. Network centrality has been proven to be a good indicator of expertise on them [24]. Different net-work creation approaches have been proposed for comparing expertise between users to establish a global rank. For in-stance, competition-based expertise networks are formalized by establishing directed links between the best answerer and all the other contributors [10, 2].

Content-based methods consider an Information Retrieval centric vision, where users are profiled according to their contributions in the website, and are ranked with respect to an expertise query (e.g. a new question). Methods based on TF-IDF [13], pLSA [18], and probabilistic topic mod-els [19] have been proposed using this general framework. A related line of work considers the use of a classification-based approach, where the user-question relationships are represented in a common feature space to find their recipro-cal relevance [6]. Tensor factorization approaches, capturing known relationships between askers, question and answerers, have also been proposed to predict best answerers [20].
Several bayesian generative models have been used in this setting. Guo et al. introduced the User-Question-Answer model, which considers user profiles as topic mixtures, and uses question categories to improve the recommendation per-formance [8]. Ni et al. proposed the Topic-based User Inter-est model, which leverages community selected best answers to promote users that contribute high quality posts [11].
Beside best answers , which can be biased and unreliable [5], none of these works make explicit use of the rich community feedback available from most CQA communities. One pre-vious work that uses aggregated voting scores is [21]. In it, observed votes are drawn as part of the generative process from a Gaussian Mixture Model. In contrast, we consider a generative model with a supervised stage, where observed votes are used for building optimized user profiles based on the latent topics of the questions and answers contributed. Also, the fLDA algorighm [1] predicts ratings of users for documents, expressed as topic mixtures, by using matrix factorization to model the affinity between users and topics. While it can be applied to question recommendation, the model is optimized for rating prediction instead of ranking.
In this section we provide the formalization of the ques-tion recommendation task that we use in the rest of the paper. The main element of this formalization is the ques-tion thread, which comprises the following components: one unique question (which originates the thread), one or more answers (that attempt to solve the problem stated in the question), community feedback (aggregated scores computed from the community votes to questions and answers), and user information (user identifier of each question and an-swer). For the sake of brevity, we will refer to the individual textual components of the thread, both questions and an-swers, as posts . In this scenario, each post has been authored by a single user and has an associated quality score given by the community. We can obtain an absolute ranking of experts for each question by sorting answers in decreasing order of aggregated voting scores. The question recommen-dation task is to predict this rank for new questions.
RankSLDA builds on supervised latent Dirichlet alloca-tion (sLDA [3]), an approach that combines LDA topic mod-eling, where document topic mixtures are drawn from a Dirichlet distribution, with a response variable associated to individual documents. The goal is to find the latent top-ics that best explain the observed responses. The key in-novation of this paper is the extension of sLDA, originally restricted to a single response value per document, to a more flexible multitask scenario able to model the multiple pair-wise preferential relationships observed for each document.
As noted, we pose question recommendation as a pairwise ranking problem. We denote as s ( d,u ) the aggregated score for user u received for his/her contribution to question d . To optimize for the latent topics that best explain the observed preferential relationship of user pairs we consider r d as the ordering for a given question thread d , so that user pairs ( u i ,u j )  X  r d when s ( d,u i ) &gt; s ( d,u j ). The model observed responses are then defined by:
Given  X ( d,u ), a feature representation of the matching between a question thread d and a user u , the learning pro-cedure finds the model  X  that maximizes the number of pairs ( u i ,u j )  X  r d where A binary classification approach on pairwise differences of vectors ( X ( d,u i )  X   X ( d,u j )) can be used to model this prob-lem, which requires inverting some of the duplets ( u to balance the training dataset. The plate diagram of the RankSLDA model is shown in Figure 1.
Our model, depicted in Fig 1, can be expressed as a gen-erative process that generates question threads and assigns Figure 1: Graphic model representing RankSLDA . Shaded nodes represent observed variables and edges probabilistic dependencies. pairwise preference scores, y ( i,j ) d . The observed scores, y which encode the relative rank order between users, are as-sumed to come from a Bernouilli distribution parametrized by: 1) the ranking model (  X  ) and 2) the question-user fea-tures ( X ( d,u i ) and  X ( d,u j )): Given the topic mixture of the current question,  X  its matching to user profiles,  X ( d,u ), the ranking model de-termines pairwise preference scores, where their sign deter-mines the predicted ranking order. The sigmoid function maps these scores in the [0 , 1] interval, which serves as the parameter p ( i,j ) d of the Bernouilli distribution from which the observed variable is drawn. Depending on the discrepancies of predicted and observed values, the model adapts both the ranking coefficients and the topic mixtures to maximize the likelihood of the observed data.

The influence of question topics and users X  expertise on the observation is included in the feature vector  X ( d,u ). As later described, we infer user X  X  expertise during the su-pervised step using multitask regression over the actual ob-served scores, s ( d,u ). The regression coefficients,  X  be interpreted as the relevance of each topic for a given user u , effectively encoding their topical knowledge. With these user profiles, we can obtain the final feature vector as a func-tion  X ( d,u ) = f (  X  d ,  X  u ) (point-wise vector multiplication in our case). The generative model proceeds as follows: 1. Draw topic distributions  X  k  X  Dir (  X  ) with i = 1 ...K . 2. For each question thread d = 1 ...D :
Note that we use the empirical topic distribution to de-fine the documents X  mixtures  X z d . Statistical inference is then
Symbol Description d,u,k,n index for documents, users, topics and words U Number of users in the community D Number of documents (i.e. question threads)
N d Number of words in document d z d,n , w d,n n-th topic assignment and term of document d y d Response for user pair ( u i ,u j ) and document d r d Binary ordering between users for document d z d Topic mixture proportion of document d z d Same as z d excluding the n-th word of d  X  d Multinomial topic distribution of document d  X  k Multinomial word distribution of topic k  X  u User topical expertise model  X  Pair-wise ranking model used to find the distribution over latent variables and the pa-rameter values that best explain the observed data. We fol-low the approach described in [7] using stochastic EM, where the E-step is performed using collapsed Gibbs sampling to infer topic assignments for the terms in the documents. The process is started by randomly initializing the topics, and then alternates between sampling topics of words z d,n and optimizing the regression parameters  X  u and ranking model  X  for the given topic assignments and observations. Because there is a dependency between topic assignments and ob-served responses, the inferred topic distribution favors topic assignments that minimize the difference between the pre-dicted and the observed responses.

In the remaining of the paper we used the notation sum-marized in Table 1 to refer to the variables and parameters of the model.
We use a collapsed Gibbs sampler to collect samples from the posterior distribution of the model. In this section, we derive the equations of the Gibbs sampler used, which presents interesting differences to LDA and sLDA due to its multitask nature. The collapsed Gibbs sampler needs to compute the probability distribution of P ( z d,n = k ) condi-tioned on the rest of the variables: P ( z d,n | z  X  n , w , y ;  X  ,  X  ,  X  ,  X  ) = P ( z d,n , z
For Gibbs sampling purposes, we can compute a propor-tional expression and then normalize: P ( z d,n = k | z  X  n , w , y ;  X  ,  X  ,  X  ,  X  ) = where the last equality is the joint probability distribution of the model. The first two terms of this expression are the standard factors of a collapsed LDA Gibbs sampler (ex-panded in equation 3). The term P ( y | z ,  X  ,  X  ) of this con-ditional probability distribution includes information on the likelihood that new topic assignments z d explain the obser-vations, shifting the distribution towards topics that gener-ate responses in consonance to their observed value y ( i , j )
Note that the current topic assignment can only affect the predicted response for the current document d , hence: where proportionality is kept with respect to z d , n . Given a set of topic assignments to document d , the probability of observing the response value for a given user pair ( u i ,u
Assuming independence, we can express the joint proba-bility of observed user responses for a given document as Note that the expression considers only user pairs ( u i ,u r . The probability of topics assignments is therefore shifted towards those that maximize the correct number of ranking predictions from  X  . We can now completely specify the con-ditional distribution for our Gibbs sampler: where n w,k refers to the number of times word w has been assigned to topic k , and n d,k refers to the frequency of topic k in document d . We use the standard notation  X  n to refer to the counts where the term being sampled is excluded.
Computing pairwise preference scores of users on new questions is equivalent to marginalizing over y d and sam-pling topics using eq. 3, as shown by [7].
After sampling topics for each document, we find new re-gression parameters for all users,  X  u , as well as the ranking model,  X  , that maximize the likelihood of the response vari-ables conditioned on the current state of topic assignments.
The main issue for learning parameters  X  is that most users have relatively few documents (questions) so that find-ing an accurate representation might be difficult. To allevi-ate this issue we resort to multitask learning which allows us to jointly learn all the  X  u vectors. Multitask learning [4] can be seen as a form of transfer learning in which similar learning  X  X asks X  share information contained in their train-ing signals. This is particularly well suited for our setting as each user can represent a task and we can easily assume that groups of users will be experts in a set of common topics thus allowing for transfer of information among them. Moreover, we use group lasso [23] to select the relevant topics for all the users. We thus optimize the following multitask lasso objective using an l 1 /l 2 regularization norm: where s the matrix of scores of all documents d and user u combinations, z is the matrix of document d topic k repre-sentations,  X  the matrix of all user u , topic k representations and  X  the regularization parameter.

Regarding the ranking model  X  , the proposed framework is flexible enough to accomodate different binary classifica-tion methods. A metric notion of the decision function is required to properly assess the variation experienced by the predicted responses for different topic assignments, which gets transferred into the the Gibbs samping distribution (eq. 4). We experimented with two linear models, Logistic Re-gression and Linear SVM, because the linear formulation allows for several optimizations in the Gibbs sampling im-plementation. While both performed similarly in terms of accuracy, we chose logistic regression with l 2 regularization in our experimental setting as it empirically showed to be more stable w.r.t. the choice of hyperparameters.
The model counts with a number of hyperparameters that need to be properly tuned, in particular the Dirichlet priors  X  and  X  , the regression regularizer  X  , and the classification regularizer C . These are treated as constants to be esti-mated instead of random variables of the model.

To describe our estimation strategy let us first define how the stochastic EM procedure is carried out. Firstly, we al-low for a number burn-in iterations of the Gibbs sampler, which we set empirically to M b = 40. During these itera-tions, we remove from the sampling distribution the factor corresponding to P ( y | z ,  X  ,  X  ) as we have not yet enough information to initialize both  X  and  X  . From this point on-wards, the procedure follows these steps: 1. Smooth topic assignments: we compute a smoothed 2. Build regression and ranking models: We find new val-3. Draw topic assignments using CGS: We perform M g This process is repeated until the total number of Gibbs sampling iterations reaches M = 500.

Regarding the estimation of the concentration parameters,  X  and  X  , we estimate them directly from data. Wallach et al. provided empirical evidence of the performance gains ob-tained by using asymmetric priors [16], which are unfeasible to set using grid-search approaches. In particular, we used the Digamma Recurrence Relation proposed by Wallach [15] to optimize both concentration parameters. This process is performed every 5 iterations of the Gibbs sampler. Regard-ing the regularization parameters  X  and C , we chose them using grid-search with a 3-fold cross validation strategy.
We consider the task of question recommendation, which comprises the prediction of the best experts for previously unseen questions. To this end, we consider a set of ques-tions that has a known set of experts, generate a rank of possible experts using our model, and compare both. We use community feedback as ground truth for the quality of responses, so the the ideal rank to predict is obtained by sorting users for each question in decreasing order of score (i.e. aggregated number of votes).
The study presented in this paper considers data from the Cross Validate 2 community of the Stack Exchange net-http://stats.stackexchange.com N. Replies Figure 2: Number of replies for Cross Validate users, in decreasing order of participation (log-log scale) work, focusing on Q&amp;A about statistics and machine learn-ing. Stack Exchange releases Creative Commons-licensed data dumps quarterly, which include the textual content of posts and other metadata (e.g. date, scores, authors). Throughout the experiments of this paper, we used the Cross Validate data dump corresponding to June 2013, which comprises 3 years, 21 , 819 questions, 28 , 429 answers and 11 , 281 unique users. Only 15 , 894 of these questions have at least 1 answer, which represents 72 . 84% of the total. The median time to receive the first answer is 2 . 3 hours. Hence, there is room for improvement in terms of answer coverage and average waiting time.

Regarding participation, only 3 , 334 users ( &lt; 30%) an-swered at least 1 question. Low participation levels are or-ganic to CQA communities, where most of the answers are generated by a minority of contributors. In this case, 80% of the answers have been contributed by the top 430 most active users. The median number of questions replied per user is 1, with a mean of 8 . 527. In Figure 2 we show this long-tail distribution of answering participation in our set.
In terms of community feedback, users make active use of the rating mechanisms. Both questions and answers can be voted as positive or negative; votes get aggregated into a score value, which we used as ground truth for the supervised stage of our model. In total, 23 , 986 posts were voted by the community, representing almost 85% of all the posts.
Section 5 evaluates the effectiveness of the presented model by comparing it to several other methods that we use as baselines. For every question in the test set we compare to:  X  Popularity Ranking ( PR ): ranks users according to their answering frequency in decreasing order. This naive baseline approach does not entail profiling users from their history of previous contributions.  X  TF-IDF ( BOW ): considers user profiles based on a tf-idf representation of the contributions of users to the sys-tem. In our setup, user profiles encompass all textual con-tent (questions and answers) posted by the user and, ad-ditionally, the questions they have replied. Ranking is es-tablished in increasing order of cosine distance between the tf-idf representation of new questions with all user profiles.  X  LSI ( LSI ): considers latent semantic indexing (LSI) for representing user profiles and questions. We consider ques-tion threads (including all contributed answers) as the doc-uments for finding the latent topics of the corpus. User pro-files are built by aggregating all their contributions as well as the questions they have provided answers to. These are then used as input documents to the learned LSI model to obtain users X  topic mixtures. Ranking is established using the cosine distance between user profiles and questions.  X  LDA+Ranking ( LDA-R ): considers a RankSLDA model where the supervised factor of the Gibbs sampler (eq. 4) is omitted. This corresponds to an unsupervised LDA model where topics are learned from the corpus and then used to train a pairwise ranking model using the observed responses s ( d,u ) as ground truth. Similarly to RankSLDA , question threads act as the documents for finding the latent topics of the corpus z d , and user profiles  X  u are learned from the ob-served responses via regression. These two pieces of informa-tion are used to build feature vectors  X ( d,u ) as explained in section 3.2, which serve to train the pairwise ranking model. Comparing with this baseline enables us to observe the ef-fect of including the observed scores as an integral part of the Bayesian inference process for the RandSLDA model.
We used ranking evaluation metrics to assess the precision of our method and the different baseline approaches con-sidered. Note that our target rank to predict is obtained by sorting users for each question in decreasing order of community-provided score.  X  P@k: Precision at cut-off k measures the number of users that provided a response in the top k positions of the predicted rank, normalized by the cut-off value. To binarize our ground truth we consider any user with an aggregated score over 0 as relevant. We evaluate precision at cut-off levels K = { 1 , 5 , 10 } .  X  nDCG@k: Discounted Cumulative Gain (DCG) ex-tends P @ k to allow for multiple relevance values. It is de-fined by the expression: where s i stands for the relevance score of the i-th author. The normalized DCG (nDCG) normalizes this score to allow comparison across different queries. The normalizing factor is obtained by computing the DCG@k for the ideal rank. We evaluate nDCG at cut-off levels K = { 1 , 5 , 10 } .  X  MAP: Mean average precision for a set of queries is where AP ( q ) denotes the average precision for query q , com-puted as where I (cond) is an indicator function with value 1 if cond is true, and 0 otherwise.  X  MRR: Mean Reciprocal Rank is the average of the Re-ciprocal Rank for a set of queries. The reciprocal rank for a query q is defined as with r q being the rank of the first relevant user for question q , using the same notion of relevance as for P@k.
In this section we present the results obtained from the application of the RankSLDA model, as well as the baseline methods, to the question recommendation scenario using the dataset introduced in Section 4. We split the original dataset into two partitions for training and test purposes following a strictly chronological criterion. Our training set comprises 75% of the question threads, from July 2010 to February 2013. The rest of the dataset, from February 2013 onwards, was used as the test collection.

The number of active users in the training set was 8 , 805, from which 2 , 930 contributed replies. We decided to set a minimum threshold of activity by discarding from the train-ing set all users with 5 or less contributed replies, which led to a total of 967 users. Regarding the test set, we consider the same collection of 967 users as we do not have informa-tion to model new answerers. After applying this criterion for user selection, we removed the contributions of  X  X nac-tive X  users from the dataset. We then proceeded to elimi-nate question threads with less than two answers, leading to a total of 6 , 108 question threads for training and 2 , 092 for test. The dataset information is gathered in Table 2.
We show the overall results for all performance metrics in Table 3. We can observe that the RankSLDA method systematically outperforms the baselines for all metrics con-sidered. The best performance is achieved by the RankSLDA method for K = 50 topics, as the higher number of topics enables the model to generate finer-grained models of user expertise, boosting the question-user matching accuracy.
Despite its simplicity, popularity ranking achieves com-parable results to more sophisticated approaches (LSI with up to 500 topics) and not far from the tf-idf approach, es-pecially if we factor in the high dimensionality of the lat-ter, over 45 , 000 features in our corpus. The supervised approaches, LDA-R and RankSLDA , provide significant im-provements over more simple baselines. Even with a low dimensionality, K = 5 topics, RankSLDA achieves a MAP of over 0 . 10, meaning that in average at least 1 of the top-10 users ranked is relevant for the question. None of the unsupervised methods was able to obtain this score. Collection Type N.Question threads N.Users Training Unfiltered 16 , 893 2 , 930 Filtered 6 , 108 967
The MAP evolution across iterations of the Gibbs sampler for different number of topics, K is depicted in Figure 3. After the initial burn-in period of the first 40 iterations, the full conditional sampling distribution starts being taken into account. This is followed by a steady increase in MAP, that is particularly noticeable for K = { 5 , 10 , 20 } .
For K = 50, we observe an initial growing trend that plateaus after the first 100 iterations. This evidences a ten-dency of the RankSLDA model to overfit the training data when increasing the number of topics. An early termination strategy could be used to avoid this artifact and get closer to the maximum test ranking performance, obtained in this case at iteration i = 280 with an MAP = 0 . 1339.
 In Figure 4 we compare the evolution of MAP between RankSLDA and LDA-R across iterations of the Gibbs sam-pler for different number of topics. LDA-R shows a similar pattern in all cases, with a rapid increase in MAP after the burn-in period, and a plateau effect once the Gibbs sam-pler converges to the posterior distribution. RankSLDA , on the other hand, continuously improves MAP by alternating the EM steps described in Section 3. These plots show the impact of equation 4 in the assignment of document top-ics for the RankSLDA model, which effectively shifts topic assignments towards mixtures that better help explain the observed rankings during training.

Figures 5 and 6 show similar plots comparing RankSLDA and LDA-R for P@k and nDCG@k respectively. The results depict a similar scenario to the one described previously for Figure 4, with LDA-R plateauing after a small number of iterations, and RankSLDA improving in time thanks to the model capacity to adapt to observed scores when sampling topic assignments. We can observe the most notable dif-ference between LDA-R and RankSLDA in the P@1 case, which highlights the better ability of RankSLDA to choose a relevant user at the top of the rank.
In this paper, we have proposed RankSLDA , a Bayesian framework that combines supervised ranking with topic mod-eling. It can be applied to question recommendation, where both community feedback and text content topics are jointly modeled for ranking users according to their relevance for new questions. Our experiments using data from the Cross Validate community show empirical evidence of the ability of the model to influence topic assignments during training to better explain the observed community scores. CQA com-munities could benefit from question recommendation for decreasing the rate of unanswered questions and reducing the average waiting time for answers to new questions.
We plan to explore how this model could be used to en-courage participation in the community by promoting the rank of less active users in the long-tail of participation. We are also interested in exploring alternative ranking models in this framework, e.g. list-wise ranking approaches. The best score for each evaluation metric is highlighted in boldface.
The work leading to these results has received partial funding from the EU 7th Framework Programme (FP7/2007-2013) under grant agreement n o 610594 (CrowdRec).
