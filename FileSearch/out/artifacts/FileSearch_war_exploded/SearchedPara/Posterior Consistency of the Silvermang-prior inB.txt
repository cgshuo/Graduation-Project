 kernels, we seek to find a predictive function f ( x ) from the training data. estimation of f ( x ) is then formulated as a regularization problem of the form where L ( y, f ( x )) is a loss function, k h k 2 H parameter. By the representer theorem [7], the solution for (1) is of the form where K (  X  ,  X  ) is the kernel function. Noticing that k h k 2 H (2) into (1), we obtain the minimization problem with respect to (w.r.t.) the  X  i as coefficients.
 prior  X   X  N n a generalized Silverman g-prior .
 n we let  X   X   X  N n of K  X  for which  X  j = 1 .
 Our analysis is based on the following regression model M  X  : model choice problem via the hypotheses H 0 :  X  = 0 and H  X  :  X   X   X  R n  X  . Let e K  X  = [ 1 n , K  X  ] . The following condition is also assumed: formalize the problem of consistency for model choice as follows [1]: tion under the true model M  X  . 2.1 A Noninformative Prior for ( u,  X  2 ) regarded as a common parameter for both M  X  and M 0 .
 After some calculations the marginal likelihood is found to be where  X  y = 1 n Let RSS  X  = (1  X  R 2  X  ) k y  X   X  y 1 n k 2 be the residual sum of squares. Here, It is easily proven that for fixed n , plim g immediate to obtain the marginal distribution of the null model as Then the Bayes factor for M  X  versus M 0 is Zellner g -prior [4], Bartlett X  X  paradox arises for the Silverman g -prior. The Bayes factor for M  X  versus M  X  is given by property (6) is equivalent to Assume that under any model M  X  that does not contain M  X  , i.e, M  X  + M  X  , condition (10) reduces to Sec. 3.
 probabilities are consistent for model choice.
 where w 1 ( n  X  ) = 1 and w 2 ( n ) = n satisfies condition (11). difference between the BICs of these two models is given by We thus obtain the following asymptotic relationship (the proof is given in Sec. 3): Theorem 2 Under the regression model and the conditions in Theorem 1, we have Furthermore, if M  X  is not nested within M  X  , then plim n  X  X  X  ln BF  X  X  S limits are taken w.r.t. the model M  X  . 2.2 A Natural Conjugate Prior for ( u,  X  2 ) the standard conjugate prior: where Ga ( u | a, b ) is the Gamma distribution: We further assume that u and  X   X  are independent. Then The marginal likelihood of model M  X  is thus  X  probabilities are consistent for model choice.
 conditions. Similarly with Theorem 2, we also have Theorem 4 Under the regression model and the conditions in Theorem 3, we have Furthermore, if M  X  is not nested within M  X  , then plim n  X  X  X  ln BF  X  X  S limits are taken w.r.t. the model M  X  . In order to prove these theorems, we first give the following lemmas. Lemma 1 Let A = have the same size as A . Then A  X  1  X  B is positive semidefinite. Proof The proof follows readily once we express A  X  1 and B as The following two lemmas were presented by [1].
 M  X  j M  X  , then and (ii) for any model M  X  that does not contain M  X  , if (10) satisfies, then then n ln Proof It is easy to compute and an n  X   X  n  X  positive diagonal matrix  X  n Letting z =  X   X  1 ( n  X  n and n X  condition (5) that independent of n . Hence, We thus have plim n  X  X  X  f ( z ) = 0 . The proof is completed. now write  X   X  = Because 0 &lt; g  X   X  g  X  , e K 0  X  e K  X  +  X   X   X  ( e K 0  X  e K  X  +  X  11  X  ) = Lemma 1 that  X   X  1  X   X  3.1 Proof of Theorem 1 We now prove Theorem 1. Consider that Because we have ln Because it is easily proven that Now consider the following two cases: 3.2 Proof of Theorem 2 Using the same notations as those in Theorem 1, we have 3.3 Proof of Theorem 3 relationship We thus have From this result the proof follows readily. methods.
 [7] G. Wahba. Spline Models for Observational Data . SIAM, Philadelphia, 1990.
