 This article deals with the notion of reduction in uncertainty when the probability mass is distributed over similar values than dissimilar values. Shannon X  X  entropy is a frequently used information theoretic measure of the uncertainty as-sociated with random variables, but it depends solely on the set of values the probability mass function assumes, and does not take into consideration whether the mass is dis-tributed among extreme values or not. A similarity struc-ture, possibly obtained through domain knowledge, on the values assumed by the random variable may reduce the asso-ciated uncertainty. More the similarity, less the uncertainty. A novel measure named Similarity Adjusted Entropy (or Sim-adjusted Entropy for short), that generalizes Shannon X  X  entropy, is then proposed to capture the effects of this sim-ilarity structure. Sim-adjusted entropy provides a mecha-nism for incorporating the domain expertise into an entropy based framework for solving various data mining tasks. Ap-plications highlighted in this manuscript include clustering of categorical data and measuring audience diversity. Exper-iments performed on Yahoo! Answers data set demonstrate the ability of the proposed method to obtain more cohesive clusters. Another set of experiments confirm the utility of the proposed measure for measuring audience diversity. H.1.1 [ Systems and Information Theory ]: Information Theory Entropy, Similarity, Audience Diversity, Categorical Clus-tering, Information Theory
The present article introduces the notion of measuring uncertainty based on not just the probabilities involved but also on the states to which those probabilities were assigned. Additional information, referred to as similarity between the various states, is integrated into the standard definitions of uncertainty measures. This proposed measure would take into consideration given information like  X  X ain X  and  X  X riz-zle X  are more similar than  X  X ain X  and  X  X unny X , and suggest that the case of  X  X qually likely to Rain or Drizzle X  involves less uncertainty than the case of  X  X qually likely to Rain or be Sunny X . This novel measure, named Similarity Adjusted Entropy (or Sim-adjusted Entropy), provides a mechanism for measuring uncertainty in situations where the categories cannot be considered in isolation, and yet, are not similar enough to be merged together. This generalizes the existing uncertainty measure, in the sense that when the knowledge regarding the relation between various conditions is com-pletely absent, it is identical to Shannon X  X  Entropy.
Two applications of the proposed measure, namely cate-gorical clustering and measuring diversity, are highlighted. These are areas where Shannon X  X  Entropy has been widely used, and our experimental results demonstrate how pow-erful a tool sim-adjusted entropy can be in the hands of a domain expert. Employing sim-adjusted entropy as the ob-jective function to be minimized for categorical clustering, one may guide the clustering algorithm towards grouping points with similar features together. Measuring user di-versity by using the proposed measure instead of Shannon X  X  entropy, one tends to identify situations where more users are from dissimilar categories as more diverse.

The proposed measure is introduced, along with some mo-tivational examples, in Section 2. Section 3 deals with appli-cations of sim-adjusted entropy, experiments on which are described in Section 4. A brief discussion on further exten-sions of this work appears in Section 5.
Before we propose a generalization of the classical entropy measure H ( p 1 ,p 2 ,...,p k ) =  X  P k i =1 p i log p i some motivational examples to demonstrate what the gen-eralization tries to achieve. The following are various cases where just considering the probability distribution does not suffice to quantify the uncertainty involved.
We now put forward a measure that generalizes Shan-non X  X  entropy [5] by incorporating the notion that informa-tion gained through x i is reflected on x j proportional to the similarity between them. Let A = (( a ij )) 1  X  i,j  X  k ilarity matrix, possibly based on a similarity function [8], with a ij = 1 signifying that x i and x j may be considered identical. We observe that is one generalization of H ( p criterion, and propose  X  log P k j =1 a ij p j as the unit of in-formation given the similarity matrix A . In other words, this is the amount of information gained by knowing that X has taken the value x i , which indicates that not only did we get to know completely about x i , but also partially about other values, too. G is the expectation of this unit of information. We shall refer to G ( p 1 ,p 2 ,...,p k ; A ) by G ( p G , whenever clear from the context.
The proposed measure G being a generalization of en-tropy, may replace it in situations where additional infor-mation is available about the categories. In particular, sim-adjusted entropy may be used as a measure of diversity. Two diversity-related applications, namely, categorical clustering and measuring user diversity, are presented here.
Categorical (or nominal) data clustering needs special treat-ment as distances between points are not always computable. There have been many attempts at clustering categorical data [4], and a common approach is to assume that all the features are equally important and that there is no similar-ity between the features. That way, one just needs to pay attention to only the feature counts of individual points, as well as, the clusters, for obtaining a criterion for determin-ing a good clustering. Entropy of the feature proportions in each cluster is a commonly used criterion to segregate the points based on the feature counts [2, 6].

In practice, however, some categorical attributes and their values may be similar to one another. Existing methods deal with this by explicitly merging these features. How-ever, merging features introduces other problems that need special attention. First, one needs to perform some thresh-olding in order to decide which features are similar enough to deserve being treated as identical, and this now requires additionally addressing the problem of how these thresholds are to be chosen. Also, one may encounter the chaining effect, whereby even if merging the feature pair (1 , 3) does not make sense, it might happen because the pairs (1 , 2) and (2 , 3) are merged together.

Replacing entropy by sim-adjusted entropy seems quite natural in this situation as one does not need to create en-tirely dissimilar features artificially. The features, and their counts, remain unchanged (i.e., without thresholding) while the similarity information is incorporated into the cluster-ing criterion directly. This allows one to guide the clustering algorithm in a systematic manner towards including similar features in the same cluster.

As in [6], the clustering is performed by minimizing the total system sim-adjusted entropy using a Markov Chain Monte Carlo approach  X  iteratively picking a point at ran-dom from one cluster and moving it to a randomly picked cluster if it reduces the system entropy. We improvise on this by moving all identical points at the same time, thereby reaching the global optimum in fewer iterations.
Audience or user diversity is an important metric for mea-suring the health of an online network (e.g., Yahoo!) and various properties under it (e.g., Yahoo! Finance, Yahoo! Sports, etc.), and it may be measured along various user dimensions, such as demographics (age, gender, geo, etc .), behavior (page views and clicks on various properties, etc. ), and the monetary value of a user to the network. Observing a lack of diversity may prompt actions of two kinds from property owners: (a) tune content towards catering to the dominant category, and (b) devise strategies for attracting the user groups currently in a minority.

Traditionally, Shannon entropy was used to measure how varied the audience is, but it is not always adequate. For ex-ample, when measuring diversity by the age of users, whether the audience is equally split between teenagers and middle-aged users, or equally split between teenagers and baby boomers, the diversity value is computed as the same. Intu-itively, however, the second case appears to be more diverse. The following is a real-life example of this situation.
Suppose, diversity is computed based on the distribution across three age categories, namely, 0 X 29, 30 X 44, and 45+, denoted by A 1 , A 2 , and A 3 , respectively. The proportions of users of Property A are 35%, 31%, and 34%, respectively, whereas, the corresponding numbers for Property B 1 are
Properties A and B are real websites, and are being referred to in an anonymized manner only due to Yahoo! X  X  corporate policies regarding revealing demographic information 30%, 35%, and 35%. It may be noted that the proportions for B are nearly a permuted version of those for A , and so, diversity as computed by Shannon X  X  entropy is about the same for both properties. This diversity measure does not consider the meaning of the classes, however, and would come up with the same value, say, even if the third cate-gory is for a different age group. We argue that, given that the classes are age groups, and since users with similar age are expected to behave similarly, the diversity is higher for A where more mass is in the disparate categories (young and old) than for B . Similar arguments hold along other dimensions, as it would be very difficult to have unrelated categories purely by design. This provides the motivation for incorporating additional information into the diversity computation.

Sim-adjusted entropy, which discounts the diversity value if the user base is split among similar categories rather than dissimilar categories, may be used instead to measure di-versity. In [3], entropy was used for categorical variables while variance was preferred for numeric attributes. Sim-adjusted entropy is suited for nominal variables with an asso-ciated similarity structure. Of course, some effort is required for obtaining the similarity values themselves, but this is a tradeoff that a property owner would need to make in order to be able to measure diversity more accurately and avoid reaching wrong conclusions about the health of the prop-erty. After all, the perceived similarity between various age groups might vary from one property to another. Similar arguments apply to other dimensions (e.g., geo location).
Experimental results are now presented to demonstrate the utility of sim-adjusted entropy. Each application listed earlier is dealt with in a separate subsection.
We now provide details on the application of sim-adjusted entropy on clustering categorical data, and demonstrate the efficacy of our approach in enhancing the semantic richness of the clusters obtained. We experiment with datasets from Yahoo! Answers to illustrate the application of sim-adjusted entropy for clustering. Another experiment on the Plants database yielded similar results, but is not included here for want of space.

First, we consider a snapshot of users X  activities from Ya-hoo! Answers that range from May 2006 to Aug 2009. We chose four factoid categories, namely, 1) Maintenance and Repairs, 2) Motorcycles, 3) Cameras, 4) Cam-corders, as these had high quality answers compared to high activity categories like Dating or Polls &amp; Surveys. Note that cate-gories 1 and 2 are similar and so are 3 and 4. We selected the resolved questions from these categories, where the best answers were chosen by the asker. We preferred not to con-sider the best answers that were chosen by community votes as these could be from spammers. Each of the best answers was rated by the asker from 1 to 5. Aggregated data per answerer had 4( categories )  X  5( ratings ) = 20 features, and a total of 306354 answers from 93984 answerers.

Next, we describe an experiment that leverages domain expertise using similarity among the features to enhance the cluster diversity.

We tested the robustness of our approach on the Mainte-nance category from Yahoo! Answers [1], which had 151849 answers with 4938, 4612, 22288, 43836, and 76175 answers of ratings 1 to 5, respectively from 48272 users. We computed the proportional reduction in system entropy ( S 0  X  S ) /S (where S 0 is the initial system entropy without any cluster-ing) with varied number of clusters for different similarity matrices. The matrices, symmetric and with all the diago-nal entries equal to 1, were input in sparse matrix format (off-diagonal entries not mentioned are 0) are as follows: The reduction in entropy increased with both the number of clusters and the similarity values.

While clustering of Yahoo! Answers dataset without con-sidering any side information does lead to category specific users in each cluster, the authority of the users is not cap-tured in the clusters. This is mainly due to all the ratings being treated as independent and unrelated. For example, consider two users from the category Cameras, with one hav-ing ratings ranging between 3 and 5, while the other X  X  range from 1 to 3. The clustering process with unrelated ratings may put these two users in the same cluster, hence diluting the authority levels. However if we treat ratings 3, 4, and 5 as being similar to one another but not to 1 or 2, the author-itative users in each category may be discovered separately. By imposing similarity on categories along with ratings, we can bias or guide the clustering to get authoritative set of users in similar categories.

In this experiment, we use the notion of Sim-adjusted en-tropy to group together equally authoritative users of simi-lar categories. We assigned a similarity of 0.5 to ratings in Cameras and Cam-Corders, and a similarity of 0.6 to those in Motorcycles and Maintenance. Within each category, we assigned similarity values of 0.9, 0.6, and 0.9 to the rat-ing pairs (1,2), (2,3), and (4,5), respectively. The number of clusters was fixed at 8, accounting for high and low ex-pertise in each of the four categories. With no similarity, the clustering procedure was able to find clusters of users in the individual categories. This may be seen in Fig. 1 where we plot the overall fraction of the features (in bub-ble size) for each of the clusters obtained. It may be noted that, clusters 3, 4 and 8 had users who answered in Cam-eras, Cam-Corders and Maintenance category, respectively. Users in Motorcycles category were clustered into high and low ratings in clusters 7 and 5, respectively. On the other hand, using sim-adjusted entropy , users are naturally clus-tered in related categories. In Fig. 1, clusters 3 and 4 had users with low quality answers (ratings of 1,2,3) across simi-lar categories. Users from Cam-corders category are divided based on high (cluster 5) and low (cluster 4) quality ratings, as opposed to all of them being grouped together in the no similarity case. Similar observations may be made for other clusters/categories as well.
This section presents results that demonstrate the utility of the proposed diversity measure. As in Section 3.2, the age groups A 1 , A 2 , and A 3 are as defined as 0 X 29, 30 X 44, and 45+, respectively. Age distributions over these age groups were obtained from an internal reporting system at Yahoo! for the top 200 Yahoo! properties, both in terms of unique Table 1: Examples of age distributions for properties where H and G 0 . 5 differ vastly users and page views. For the sake of simplicity, we assume in this set of experiments that (a) Sim ( A 1 ,A 3 ) = 0, and (b) Sim ( A 1 ,A 2 ) = Sim ( A 2 ,A 3 ) = a . In this assumption, the value of a signifies how much similarity exists between users of similar age. The diversity measure shall be denoted G a (with G 0  X  H ). For want of space, we only present experiments where a = 0 . 5.

Kendall X  X   X  between H and G 0 . 5 is 0.78, that is, the two measures do not agree about 22% of the time on which prop-erty has more age diversity. Some examples are also pro-vided in Table 1 2 to depict how G 0 . 5 better matches our intuition about diversity than G 0 . One may note that G shows a 19% change between Property C and Property D although they have the same entropy. This is a result of the latter having 72% of the mass in the extremes as opposed to just 45% for the former. Similar is the case for Properties E and F . In contrast, while entropy changes significantly due to the change in distributions of Properties G and H , G 0 . 5 does not change much because the distributional changes are mostly restricted between similar age groups.
A novel measure based on the premise that additional in-formation about the relationship between various categories helps reduce uncertainty has been proposed in this work. A simple generalization of the entropy measure, called simi-larity adjusted entropy, has been put forward, and its basic characteristics have been studied. Two applications, namely, categorical data clustering and user diversity measurement, names of the properties anonymized as mentioned in Sec-tion 3.2 were described in detail. Experimental results convincingly demonstrate the utility of the proposed approach.

One may observe that, throughout this article, the focus was always on how to incorporate a given similarity struc-ture into the definition of entropy, rather than determining the similarity itself. So, this approach provides a simple al-ternative for measuring diversity to someone already armed with additional information over what is used by the clas-sical Shannon X  X  entropy. Another point to be noted is that the proposed measure is just one of several possible gener-alizations of entropy of the desired form.

In future, once the proposed measure is established as a measure of diversity, we would like to present it as a mea-sure of information. That would give rise to notions of sim-adjusted information gain and maximum sim-adjusted en-tropy, and may have other machine learning applications like classification, decision trees, etc. [1] L. A. Adamic, J. Zhang, E. Bakshy, and M. S.
 [2] D. Barbara, J. Couto, and Y. Li. COOLCAT: an [3] M. A. Bedau, M. Zwick, and A. Baham. Variance and [4] P. Berkhin. Survey of clustering data mining [5] T. M. Cover and J. A. Thomas. Elements of [6] T. Li, S. Ma, and M. Ogihara. Entropy-based criterion [7] R. Likert. A technique for the measurement of [8] S. Santini and R. Jain. Similarity measures. IEEE
