 An organization makes a new release as new information become available, releases a tailored view for each data request, releases sensitive information and identifying information separately. The availability of related releases sharpens the identification of indi-viduals by a global quasi-identifier consisting of attributes from re-lated releases. Since it is not an option to anonymize previously released data, the current release must be anonymized to ensure that a global quasi-identifier is not effective for identification. In this paper, we study the sequential anonymization problem under this assumption. A key question is how to anonymize the current release so that it cannot be linked to previous releases yet remains useful for its own release purpose. We introduce the lossy join ,a negative property in relational database design, as a way to hide the join relationship among releases, and propose a scalable and practical solution.
 H.2.7 [ Database Administration ]: [Security, integrity, and protec-tion]; H.2.8 [ Database Applications ]: [Data mining] Algorithms, Performance, Security k -anonymity, privacy, sequential release, classification, generaliza-tion
The work on k -anonymity [16][17] addresses the problem of re-ducing the risk of identifying individuals in a person-specific table.  X 
Research was supported in part by a research grant and a PGS scholarship from the Natural Sciences and Engineering Research Council of Canada. The work was done while the first author is visiting Nanyang Technological University, Singapore.
 Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00.
 Typically, a set of identifying attributes in a table, called the quasi-identifier or QID , is generalized to a less precise representation so that each partition grouped by QID contains at least k records (i.e., persons). Hence, if some record is linked to an external source by a QID value, so are at least k  X  1 other records having the same QID value, making it difficult to distinguish a particular individ-ual. In this notion, the QID is restricted to the current table, and the database is made anonymous to itself . In most scenarios, how-ever, related data were released previously: an organization makes a new release as new information becomes available, releases a separate view for each data sharing purpose (such as classifying a different target variable [6][23][5]), or makes separate releases for personally-identifiable data (e.g., names) and sensitive data (e.g., DNA sequences) [11]. In such scenarios, the QID can be a combi-nation of attributes from several releases, and the database must be made anonymous to the combination of all releases thus far .The example below illustrates this scenario.
Pid Name Job Class 1 Alice Banker c1 2 Alice Banker c1 3 Bob Clerk c2 4 Bob Driver c3 5 Cathy Engineer c4
E XAMPLE 1. Consider the data in Table 1. Pid is the person identifier and is included only for discussion, not for release. Sup-pose the data holder has previously released T 2 and now wants to release T 1 for classification analysis of the Class column. Essen-tially T 1 and T 2 are are two projection views of the patient records. The data holder does not want Name to be linked to Disease in the join of the two releases; in other words, the join should be k -anonymous on { N ame, Disease } . Below are several observa-tions that motivate our approach.
 (1) Join sharpens identification : after the join, the attacker can uniquely identify the individuals in the { Bob, HIV } group through the combination { N ame, Disease } because this group has size 1. When T 1 and T 2 are examined separately, both Bob group and HIV group have size 2. (2) Join weakens identification : after the join, the { Alice, Cancer } group has size 4 because the records for different persons are matched (i.e., the last two records in the join table). When T 1 and T 2 are examined separately, both Alice group and Cancer group have smaller size. In the database termi-nology, the join is lossy . Since the join attack depends on matching the records for the same person, a lossy join can be used to combat the join attack. (3) Join enables inferences across tables : the join reveals the inference Alice  X  Cancer with 100% confidence for the individuals in the Alice group.

This example illustrates a scenario of sequential release : T unknown when T 2 was released, and T 2 , once released, cannot be modified when T 1 is considered for release. This scenario is differ-ent from the view release in the literature [13][26][7] where both T and T 1 are a part of a view and can be modified before the release, which means more  X  X ooms X  to satisfy a privacy and information requirement. In the sequential release, each release has its own in-formation need and the join that enables a global identifier should be prevented. In the view release, however, all tables in the view serve the information need collectively, possibly through the join of all tables.

One solution, suggested in [17], is to k -anonymize the current release T 1 on QID that is the set of all join attributes. Since a fu-ture release may contain any attribute in T 1 , QID essentially needs to contain all attributes in T 1 . Another solution, suggested in [19], is generalizing T 1 based on the previous T 2 to ensure that no value more specific than it appears in T 2 would be released in T solutions suffer from monotonically distorting the data in a later re-lease. The third solution is releasing a  X  X omplete X  cohort where all potential releases are anonymized at one time, after which no ad-ditional mechanism is required. This solution requires predicting future releases. The  X  X nder-prediction X  means no room for addi-tional releases and the  X  X ver-prediction X  means unnecessary data distortion. Also, this solution does not accommodate the new data added at a later time. We consider the sequential anonymization of the current release T 1 in the presence of a previous release T 2 , assuming that T T 2 are projections of the same underlying table. This assumption holds in all the scenarios that motivate this work: release new at-tributes, release a separate set of columns for each data request, or make separate releases for personally-identifiable columns and sen-sitive columns. The release of T 1 must satisfy a given information requirement and privacy requirement. The information require-ment could include such criteria as minimum classification error [2][5][6][23] and minimum data distortion [16][17]. The privacy requirement states that, even if the attacker joins T 1 with T will not succeed in linking individuals to sensitive properties. We formalize this requirement into limiting the linking between two attribute sets X and Y over the join of T 1 and T 2 . This privacy no-tion, called ( X, Y ) -privacy , generalizes k -anonymity [16][17] and sensitive inferences [3][21][22]. A formal definition will be given in Section 3.

Our basic idea is generalizing the current release T 1 so that the join with the previous release T 2 becomes lossy enough to disorient the attacker. Essentially, a lossy join hides the true join relationship to cripple a global quasi-identifier. We first show that the sequential anonymization subsumes the k -anonymization, thus the optimal so-lution is NP-hard. We present a greedy method for finding a min-imally generalized T 1 . To ensure the minimal generalization, the lossy join responds dynamically to each generalization step. There-fore, one challenge is checking the privacy violation over such dy-namic join because a lossy join can be extremely large. Another challenge is pruning, as early as possible, unpromising generaliza-tion steps that lead to privacy violation. To address these chal-lenges, we present a top-down approach to progressively specialize T 1 starting from the most generalized state. It checks the privacy violation without executing the join and prunes unpromising spe-cialization based on a proven monotonicity of ( X, Y ) -privacy. We demonstrate the usefulness of this approach on real life data sets. Finally, we discuss the extension to more than one previous release.
Our major difference from previous works is that we consider se-quential releases and a global quasi-identifier formed by attributes from several releases. Previous works primarily considered a single release. [1] [12] showed that the optimal k -anonymization is NP-hard. Algorithms for k -anonymization include [8][16][17] for min-imum distortion, and [2][5][6][23] for classification. Variations and alternatives of k -anonymity were also studied. [9] proposed the no-tion of multidimensional k -anonymity where generalization is over multi-dimension-at-a-time. [10] proposed the l -diversity to address the attacks based on the lack of diversity of sensitive properties. [21][22] proposed to limit the confidence of inferring a sensitive property for a group of individuals. [24] proposed some generaliza-tion methods to simultaneously achieve k -anonymity and limit the confidence. [25] proposed the notion of personalized anonymity. All the above works considered a single release.

Several recent works measured information disclosure arising from linking two or more tables. [13] suggested a measure on in-formation disclosure by a set of views with respect to a secret view. [4] studied whether a new view disclosed more information than the existing views with respect to a secret view. Both works employed a probability model to measure information disclosure, which is different from the k -anonymity model. [7][26] presented a method of detecting privacy violation by a view set over a base table. Since both works only detect, but do not remove, a violation, whether the tables are released sequentially or not is not an issue. [20] consid-ered k -anonymization of the data owned by multiple parties under the assumption that a record is identified by a common key shared by all parties. In the sequential release scenario, this common key assumption does not hold and the join attributes can be generalized as part of the global quasi-identifier.
For a table T ,  X ( T ) and  X  ( T ) denote the projection and selec-tion over T , att ( T ) denotes the set of attributes in T ,and notes the number of distinct records in T .
We assume that X and Y are disjoint sets of attributes that de-scribe individuals and sensitive properties in any order. An example is X = { Name,Job } and Y = { Disease } . There are two ways to limit the linking between X and Y .
 D EFINITION 3.1 ( ( X, Y ) -ANONYMITY ). Let x be a value on X .The anonymity of x wrt Y , denoted a Y ( x ) , is the number of distinct values on Y that co-occur with x , i.e., |  X  Y  X  of records containing x .Let A Y ( X )= min { a Y ( x ) | We say that T satisfies the ( X, Y ) -anonymity for some specified integer k if A Y ( X )  X  k .

In words, ( X, Y ) -anonymity states that each value on X is linked to at least k distinct values on Y . The existing k -anonymity is the special case where X serves QID and Y is a key in T . The next ex-ample shows the usefulness of ( X, Y ) -anonymity where Y is not a key in T and k -anonymity fails to provide the required anonymity. E XAMPLE 2. Consider the table A record in the table represents that a patient identified by Pid has Job , Zip , PoB (place of birth), and T est . In general, a pa-tient can have several tests, thus several records. Since QID { Job, Zip, P oB } is not a key in the table, the k -anonymity on QID fails to ensure that each value on QID is linked to at least k is possible that the k records matching a value on QID may involve no more than k/ 3 patients. With ( X, Y ) -anonymity, we can spec-ify the anonymity wrt patients by letting X = { Job, Zip, P oB and Y = Pid , that is, each X group must be linked to at least k distinct values on Pid .If X = { Job, Zip, P oB } and Y = each X group is required to be linked to at least k distinct tests.
Being linked to k persons or tests does not imply that the prob-ability of being linked to any of them is 1 /k if some person or test occurs more frequently than others. Thus a large k does not nec-essarily limit the linking probability. The ( X, Y ) -linkability below addresses this issue.

D EFINITION 3.2 ( ( X, Y ) -LINKABILITY ). Let x beavalue on X and y be a value on Y .The linkability of x to y , denoted l ( x ) , is the percentage of the records that contain both x and y among those that contain x , i.e., a ( y, x ) /a ( x ) .Let L max { l y ( x ) | x  X  X } and L Y ( X )= max { L y ( X ) | We say that T satisfies the ( X, Y ) -linkability for some specified real 0 &lt;k  X  1 if L Y ( X )  X  k .

In words, ( X, Y ) -linkability limits the confidence of inferring a value on Y from a value on X . With X and Y describing in-dividuals and sensitive properties, any such inference with a high confidence is a privacy breach. Often, not all but some values y on Y are sensitive, in which case Y can be replaced with a sub-set of y i values on Y , written Y = { y 1 ,  X  X  X  ,y p } , and a different threshold k can be specified for each y i . More generally, we can allow multiple Y i , each representing a subset of values on a differ-ent set of attributes, with Y being the union of all Y i . For example, Y 1 = { HIV } on T est and Y 2 = { Banker } on Job .Sucha  X  X alue-level X  specification provides a great flexibility essential for minimizing the data distortion.

E XAMPLE 3. Suppose that ( j, z, p ) on X = { Job, Zip, P oB occurs with the HIV test in 9 records and occurs with the Diabetes test in 1 record. The confidence of ( j, z, p )  X  HIV is 90%. With Y = T est ,the ( X, Y ) -linkability states that no test can be in-ferred from a value on X with a confidence higher than a given threshold.

When no distinction is necessary, we use the term  X  ( X, Y to refer to either ( X, Y ) -anonymity or ( X, Y ) -linkability. The fol-lowing corollary can be easily verified.

C OROLLARY 3.1. Assume that X  X  X and Y  X  Y . For the same threshold k ,if ( X ,Y ) -privacy is satisfied, ( X, Y is satisfied.
One way to look at a ( X, Y ) -privacy is that Y serves the  X  X efer-ence point X  with respect to which the privacy is measured. For ex-ample, with Y = T est each test in Y serves a reference point, and A
Y ( X ) measures the minimum number of tests associated with X , and L Y ( X ) measures the maximum confidence of inferring a test from X . To satisfy a ( X, Y ) -privacy, our approach is generalizing X while fixing the reference point Y . We assume that, for each categorical attribute in X , there is a pre-determined taxonomy tree of values where leaf nodes represent domain values and a parent node is a generalization of child nodes. The root is the most gen-eralized value of the attribute, denoted AN Y . Each generalization replaces all child values with the parent value. We consider only the generalization that forms a  X  X ut X  in a taxonomy tree, where a cut contains exactly one value on every root-to-leaf path. The val-ues in a cut can be on different levels of the taxonomy tree. Such generalization is more general than the full-domain generalization [8][16][17] where all generalized values must be on the same level of the taxonomy tree.

A generalized table can be obtained by a sequence of specializa-tions starting from the most generalized table . Each specialization is denoted by v  X  X  v 1 ,  X  X  X  ,v c } ,where v is the parent value and v ,  X  X  X  ,v c are the child values of v . It replaces the value v in ev-ery record containing v with the child value v i that is consistent with the original domain value in the record. A specialization for a continuous attribute has the form v  X  X  v 1 ,v 2 } ,where v are two sub-intervals of the larger interval v . Instead of being pre-determined, the splitting point of the two sub-intervals is chosen on-the-fly to maximize information utility. More details on infor-mation utility will be discussed in Section 5.2.
Consider a previously released table T 2 and the current table T where T 2 and T 1 are projections of the same underlying table and contain some common attributes. T 2 may have been generalized. We want to generalize T 1 to satisfy a given ( X, Y ) -privacy. To pre-serve information, T 1  X  X  generalization is not necessarily based on T ,thatis, T 1 may contain values more specific than in T 2 T 1 and T 2 , the attacker may apply prior knowledge to match the records in T 1 and T 2 . Entity matching has been studied in database, data mining, AI and Web communities for information integration, natural language processing and Semantic Web. We cannot con-sider a priori every possible way of matching. Our work primar-ily considers the matching based on the following prior knowledge available to both the data holder and the attacker: the schema in-formation of T 1 and T 2 , the taxonomies for categorical attributes, and the following inclusion-exclusion principle for matching the records. Assume that t 1  X  T 1 and t 2  X  T 2 .  X 
Consistency Predicate : for every common categorial attribute A , t 1 .A matches t 2 .A if they are on the same generalization path in the taxonomy tree for A . Intuitively, this says that t t 2 .A can possibly be generalized from the same domain value.
For example, Male matches Single Male . This predicate is implicit in the taxonomies for categorical attributes.  X  Inconsistency Predicate : for two distinct categorical attributes
T 1 .A and T 2 .B , t 1 .A matches t 2 .B only if t 1 .A and t not semantically inconsistent according to the  X  X ommon sense X .
This predicate excludes impossible matches. If not specified,  X  X ot semantically inconsistent X  is assumed. If two values are semantically inconsistent, so are their specialized values. For example, Male and P regnant are semantically inconsistent, so are Married Male and 6 M onth P regnant .

We do not consider continuous attributes because their taxonomies may be generated differently for T 1 and T 2 . Both the data holder and the attacker use these predicates to match records from T T . The data holder can  X  X atch up with X  the attacker by incorpo-rating the attacker X  X  knowledge into such  X  X ommon sense X . We as-sume that a match function tests whether ( t 1 ,t 2 ) is a match. is a match if both predicates hold. The join of T 1 and T ble on att ( T 1 )  X  att ( T 2 ) that contains all matches join attributes refer to all attributes that occur in either predicates. Note that every common attribute A has two columns T 1 .A and T .A in the join. The following observation says that generalizing the join attributes produces more matches, thereby making the join more lossy. Our approach exploits this property to hide the original matches.

Observation 1. ( Join preserving )If ( t 1 ,t 2 ) is a match and if t is a generalization of t 1 , ( t 1 ,t 2 ) is a match. ( Join relaxing )If ( t ,t 2 ) is not a match and if t 1 is a generalization of t join attribute A , ( t 1 ,t 2 ) is a match if and only if t are on the same generalization path and t 1 .A is not semantically inconsistent with any value in t 2 .
 Consider a ( X, Y ) -privacy. We generalize T 1 on the attributes X  X  att ( T 1 ) , called the generalization attributes . Corollary 3.1 im-plies that including more attributes in X makes the privacy require-ment stronger. Observation 1 implies that including more join at-tributes in X (for generalization) makes the join more lossy. There-fore, from the privacy point of view it is a good practice to include all join attributes in X for generalization. Moreover, if X contains a common attribute A from T 1 and T 2 , under our matching predi-cate, one of T 1 .A and T 2 .A could be more specific (so reveal more information) than the other. To ensure privacy, X should contain both T 1 .A and T 2 .A in the ( X, Y ) -privacy specification. holder has previously released a table T 2 and wants to release the next table T 1 ,where T 2 and T 1 are projections of the same under-lying table and contain some common attributes. The data holder wants to ensure a ( X, Y ) -privacy on the join of T 1 and T sequential anonymization is to generalize T 1 on X  X  att ( that the join of T 1 and T 2 satisfies the privacy requirement and T remains as useful as possible.

T HEOREM 3.1. The sequential anonymization is at least as hard as the k -anonymization problem.

Proof: The k -anonymization of T 1 on QID is the special case of sequential anonymization with ( X, Y ) -anonymity, where X is QID and Y is a common key of T 1 and T 2 and the only join attribute. In this case, the join trivially appends the attributes of T according to the common key, after which the appended attributes are ignored.
To generalize T 1 , we will specialize T 1 starting from the most generalized state. A main reason for this approach is the following anti-monotonicity of ( X, Y ) -privacy with respect to specialization: if (
X, Y ) -privacy is violated, it remains violated after a specializa-tion. Therefore, we can stop further specialization whenever the (
X, Y ) -privacy is violated for the first time. This is a highly de-sirable property for pruning unpromising specialization. We first show this property for a single table.

T HEOREM 4.1. On a single table, the ( X, Y ) -privacy is anti-monotone wrt specialization on X .

Proof: For ( X, Y ) -anonymity, it suffices to observe that a spe-cialization on X always reduces the set of records that contain a X value, therefore, reduces the set of Y values that co-occur with a X value. For ( X, Y ) -linkability, suppose that a special-ization v  X  X  v 1 ,  X  X  X  ,v c } transforms a value x on X to the spe-cialized values x 1 ,  X  X  X  ,x c on X . Following an idea in [21], if l ( x i ) &lt;l y ( x ) for some x i , there must exist some x l ( tion does not reduce L Y ( X ) .

On the join of T 1 and T 2 , in general, ( X, Y ) -anonymity is not anti-monotone wrt a specialization on X  X  att ( T 1 ) . To see this, let T ( C, D )= { c 1 d 3 ,c 2 d } and T 2 ( D, Y )= { d 3 y 3 ,d where c i ,d i ,y i are domain values and d is a generalized value of d and d 2 . The join based on D contains 3 matches ( c 1 d 3 ( c d 3 ,d 3 y 3 ) , ( c 2 d, d 1 y 1 ) ,and A Y ( X )= A Y ( X = { C, T 1 .D, T 2 .D } . After specializing the record c into c 2 d 2 , the join contains only two matches ( c 1 d ( c d 3 ,d 3 y 3 ) ,and A Y ( X )= a Y ( c 1 d 3 d 3 )=2 . Thus, A increases after the specialization.

The above situation arises because the specialized record c matches no record in T 2 or becomes dangling. However, this situa-tion does not arise for the T 1 and T 2 encountered in our sequential anonymization. We say that two tables are population-related if ev-ery record in each table has at least one matching record in the other table. Essentially, this property says that T 1 and T 2 are about the same  X  X opulation X  and there is no dangling record. Clearly, if T and T 2 are projections of the same underlying table, as assumed in our problem setting, T 1 and T 2 are population-related. Observation 1 implies that generalizing T 1 preserves the population-relatedness.
Observation 2. If T 1 and T 2 are population-related, so are they after generalizing T 1 .

L EMMA 4.1. If T 1 and T 2 are population-related, A Y ( X not increase after a specialization of T 1 on X  X  att ( T Proof : As in the first part of Theorem 4.1, a specialization always reduces the set of Y values that co-occur with X values. From Observation 2, X values are specialized but not dropped in the spe-cialized join. Therefore, the minimization for A Y ( X ) is over a set of values in which each value is only reduced, but not dropped.
Now, we consider ( X, Y ) -linkability on the join of T 1 is not immediately clear how a specialization on X  X  att ( affect L Y ( X ) because the specialization will reduce the matches, therefore, both a ( y, x ) and a ( x ) in l y ( x )= a ( y, x next lemma shows that L Y ( X ) does not decrease after a special-ization on X  X  att ( T 1 ) .

L EMMA 4.2. If Y contains attributes from T 1 or T 2 , but not from both, L Y ( X ) does not decrease after a specialization of T on the attributes X  X  att ( T 1 ) .

Proof : Theorem 4.1 has covered the specialization on a non-join attribute. So we assume that the specialization is on a join attribute in X 1 = X  X  att ( T 1 ) , in particular, it specializes a value x X 1 into x 11 ,  X  X  X  ,x 1 c .Let R i be the set of T 1 records containing x 1 i after the specialization, 1  X  i  X  c . We consider only non-empty R i  X  X . From Observation 2, some records in T 2 will match the records in R i .Let x 2 i be a value on X 2 = X  X  att in these matching records and let S i be the set of records in T containing x 2 i . Note that | R i | =0 and | S i | =0 .Let R R 1  X  X  X  X  X  R c . | R | = j | R j | . Without loss of generality, assume that l y ( x 11 x 21 )  X  l y ( x 1 i x 2 i ) ,where 1  X  i cialization does not decrease L y ( X ) , therefore, L Y ( ition is that of Theorem 4.1 and the insight that the join preserves the relative frequency of y in all matching records. Let us consider two cases, depending on whether y is in T 1 or T 2 .

Case 1 : y is in T 1 .Let  X  i be the percentage of the records con-taining y in R i . Since all records in R i match all records in S From the join preserving property in Observation 1, all records in R match all records in S i .Sowehave
Case 2 : y is in T 2 .Let  X  i be the percentage of records containing y in S i . Exactly as in Case 1, we can show l y ( x 1 i x  X  1  X   X  i ,where 1 &lt;i  X  c , all records in R match all records in S .Now,
C OROLLARY 4.1. The ( X, Y ) -anonymity on the join of T 1 T is anti-monotone wrt a specialization of T 1 on X  X  att ( sume that Y contains attributes from either T 1 or T 2 , but not both. The ( X, Y ) -linkability on the join of T 1 and T 2 is anti-monotone wrt a specialization of T 1 on X  X  att ( T 1 ) .

C OROLLARY 4.2. Let T 1 , T 2 and ( X, Y ) -privacy be as in Corol-lary 4.1. There exists a generalized T 1 that satisfies the privacy if and only if the most generalized T 1 does.

Remarks. Lemma 4.1 and Lemma 4.2 can be extended to several previous releases T 2 ,  X  X  X  ,T p after the join is so extended. Thus, the anti-monotonicity of ( X, Y ) -privacy holds for one or more pre-vious releases. Our extension in Section 7 makes use of this obser-vation.
We present the algorithm for generalizing T 1 to satisfy the given (
X, Y ) -privacy on the join of T 1 and T 2 . We can first apply Corol-lary 4.2 to test if this is possible, and below we assume it is. Let X denote X  X  att ( T i ) , Y i denote Y  X  att ( T i ) ,and J attributes in T i ,where i =1 , 2 . The algorithm, called Top-Down Specialization for Sequential Anonymization (TDS4SA) , is given in Algorithm 1. The input con-sists of T 1 , T 2 ,the ( X, Y ) -privacy requirement, and the taxonomy tree for each categorical attribute in X 1 . Starting from the most generalized T 1 , the algorithm iteratively specializes the attributes A j in X 1 . T 1 contains the current set of generalized records and Cut j contains the current set of generalized values for A iteration, if some Cut j contains a  X  X alid X  candidate for specializa-tion, it chooses the winner w that maximizes Score . A candidate is valid if the join specialized by the candidate does not violate the privacy requirement. The algorithm then updates Score ( v status for the candidates v in  X  Cut j . This process is repeated un-til there is no more valid candidate. On termination, Corollary 4.1 implies that a further specialization produces no solution, so T a maximally specialized state satisfying the given privacy require-ment.
 Below, we focus on the three key steps in Lines 3 to 5. Algorithm 1 Top-Down Specialization for Sequential Anonymiza-tion Input : T 1 , T 2 ,a ( X, Y ) -privacy requirement, a taxonomy tree for each categorical attribute in X 1 .
 Output : a generalized T 1 satisfying the privacy requirement. 1: generalize every value of A j to AN Y j where A j  X  X 1 2: while there is a valid candidate in  X  Cut j do 3: find the winner w of highest Score ( w ) from  X  Cut j 4: specialize w on T 1 and remove w from  X  Cut j ; 5: update Score ( v ) and the valid status for all v in 6: end while 7: output the generalized T 1 and  X  Cut j ;
Score ( v ) evaluates the  X  X oodness X  of a specialization v for pre-serving privacy and information. Each specialization gains some  X  X nformation X , Inf oGain ( v ) , and loses some  X  X rivacy X , PrivLoss We choose the specialization that maximizes the trade-off between the gain of information gain and the loss of privacy, proposed in [5]: Inf oGain ( v ) is measured on T 1 whereas PrivLoss ( v ) sured on the join of T 1 and T 2 .

Consider a specialization v  X  X  v 1 ,  X  X  X  ,v c } . For a continuous attribute, c =2 ,and v 1 and v 2 represent the binary split of the in-terval v that maximizes Inf oGain ( v ) . Before the specialization, T [ v ] denotes the set of generalized records in T 1 that contain v . After the specialization, T 1 [ v i ] denotes the set of records in T contain v i , 1  X  i  X  c .

The choice of Inf oGain ( v ) and P rivLoss ( v ) depends on the information requirement and privacy requirement. If T 1 is released for classification on a specified class column, Inf oGain be the reduction of the class entropy [15], defined by
Inf oGain ( v )= Ent ( T 1 [ v ])  X  Ent ( R ) is the class entropy of a set of records R following from Shannon X  X  information theory [18]. The more dominating the ma-jority class in R is, the smaller Ent ( R ) is and the smaller the clas-sification error is. The computation depends only on the class fre-quency and some count statistics of v and v i in T 1 [ v ]  X  X  X  X  T 1 [ v c ] . Another choice of Inf oGain ( v ) could be the no-tion of distortion [17]. If generalizing a child value v i value v costs one unit of distortion, the information gained by the specialization v  X  X  v 1 ,  X  X  X  ,v c } is The third choice can be the discernibility [2].

For ( X, Y ) -privacy, P rivLoss ( v ) is measured by the decrease of A Y ( X ) or the increase of L Y ( X ) due to the specialization of v : A Y ( X )  X  A Y ( X v ) for ( X, Y ) -anonymity, and L L
Y ( X ) for ( X, Y ) -linkability, where X and X v represent the at-tributes before and after specializing v respectively. Computing P rivLoss ( v ) involves the count statistics about X and Y over the join of T 1 and T 2 , before and after the specialization of v ,which can be expensive.

Challenges. Though Algorithm 1 has a simple high level struc-ture, several computational challenges must be resolved for an ef-ficient implementation. First, each specialization of the winner w affects the matching of join, hence, the checking of the privacy re-quirement (i.e., the status on Line 5). It is extremely expensive to rejoin the two tables for each specialization performed. Second, it is inefficient to  X  X erform X  every candidate specialization v just to update Score ( v ) on Line 5 (note that A Y ( X v ) and L defined for the join assuming the specialization of v is performed). Moreover, materializing the join is impractical because a lossy join can be very large. A key contribution of this work is an efficient solution that incrementally maintains some count statistics without executing the join. We consider the two types of privacy separately.
Two expensive operations on performing the winner specializa-tion w are accessing the records in T 1 containing w and matching the records in T 1 with the records in T 2 . To support these opera-tions efficiently, we organize the records in T 1 and T 2 structures. Recall that X 1 = X  X  att ( T 1 ) and X 2 = X and J 1 and J 2 denote the join attributes in T 1 and T 2
Tree1 and Tree2. In Tree1 , we partition the T 1 records by the attributes X 1 and J 1  X  X 1 in that order, one level per attribute. Each root-to-leaf path represents a generalized record on X with the partition of the original records generalized being stored at the leaf node. For each generalized value v in Cut j , Link up all nodes for v at the attribute level of v . Therefore, Link provides a direct access to all T 1 partitions generalized to v . Tree1 is updated upon performing the winner specialization w in each iteration. In Tree2 , we partition the T 2 records by the attributes J and X 2  X  J 2 in that order. No specialization is performed on T Tree2 is static. Some  X  X ount statistics X , described below, are stored for each partition in Tree1 and Tree2.

Specialize w (Line 4) . This step performs the winner specializa-tion w  X  X  w 1 ,  X  X  X  ,w c } , similar to the TDS algorithm for a single release in [5]. It follows Link [ w ] , and for each partition P link,  X 
Step 1: refine P 1 into the specialized partitions for w i into Link [ w i ] . The specialized partitions remain on the other links of P 1 . This step will scan the raw records in P 1 same scan, we also collect the following count statistics for each (new) partition P on Link [ w i ] , which will be used later to up-date Score ( v ) .Let P [ u ] denote the subset of P containing the value u and | P | denote the size of P :  X  is a class label in the class column, y is a value on Y ,and w is a child value of w i . These count statistics are stored together with the partition P .  X 
Step 2: probe the matching partitions in Tree2. Match the last |
J 1 | attributes in P 1 with the first | J 2 | attributes in Tree2. For each matching node at the level | J 2 | in Tree2, scan all partitions
P 2 below the node. If x is the value on X represented by the pair ( P 1 ,P 2 ) , increment a ( x ) by | P 1 | X | P 2 | , increment a by | P 1 [ y ] | X | P 2 | if Y is in T 1 ,orby | P 1 | X | P
T 2 ,where y is a value on Y . We employ an  X  X -tree X  to keep a ( x ) and a ( x, y ) for the values x on X .Inthe X -tree ,the x values are partitioned by the attributes X , one level per attribute, and are represented by leaf nodes. a ( x ) and a ( x, y ) at the leaf node for x . Note that l y ( x )= a ( x, y ) /a L y ( X )= max { l y ( x ) } over all the leaf nodes x in the X -tree.
Remarks. This step (Line 4) is the only time that raw records are accessed in our algorithm.

Update Score(v) (Line 5) . This step updates Score ( v ) for the candidates v in  X  Cut j using the count statistics collected at the partitions in Tree1 and a ( x ) and a ( x, y ) in the X -tree. The idea is the same as [5], so we omit the details. An important point is that this operation does not scan raw records, therefore, is efficient. This step also updates the  X  X alid X  status: If L Y ( X v )  X  as  X  X alid X .

Analysis. (1) The records in T 1 and T 2 are stored only once in Tree1 and Tree2. For the static Tree2, once it is created, data records can be discarded. (2) On specializing the winner w , Link provides a direct access to the records involved in T 1 and Tree2 provides a direct access to the matching partitions in T 2 matching is performed at the partition level, not the record level, it scales up with the size of tables. (3) The cost of each iteration has two parts. The first part involves scanning the affected parti-tions on Link [ w ] for specializing w in Tree1 and maintaining the count statistics. This is the only operation that accesses records. The second part involves using the count statistics to update the score and status of candidates. (4) In the whole computation, each record in T 1 is accessed at most | X  X  att ( T 1 ) | X  h times because a record is accessed only if it is specialized on some attribute from X  X  att ( T 1 ) ,where h is the maximum height of the taxonomies for the attributes in X  X  att ( T 1 ) .
Like for ( X, Y ) -linkability, we use Tree1 and Tree2 to find the matching partitions ( P 1 ,P 2 ) , and performing the winner special-ization and updating Score ( v ) is similar to Section 5.3. But now, we use the X -tree to update a Y ( x ) for the values x on X ,and there is one important difference in the update of a Y ( x that a Y ( x ) is the number of distinct values y on Y associated with the value x . Since the same ( x, y ) value may be found in more than one matching ( P 1 ,P 2 ) pair, we cannot simply sum up the count ex-tracted from all pairs. Instead, we need to keep track of distinct Y values for each x value to update a Y ( x ) . In general, this is a time-consuming operation, e.g., requiring sorting/hashing/scanning. Be-low, we identify several special but important cases in which a can be updated efficiently.

Case 1: X contains all join attributes. In this case, J 1 and J 2  X  X 2 , and the partitioning in Tree1 and Tree2 is based on X 1 and X 2 . Hence, each x value is contributed by exactly one matching ( P 1 ,P 2 ) pair and is inserted into the X -tree only once . Therefore, there is no duplicate Y value for each x value. The computation is as follows: for each matching ( P 1 ,P 2 ) pute a Y ( x 1 x 2 ) by a Y 1 ( x 1 )  X  a Y 2 ( x 2 ) ,where x for x i in Treei. a Y i ( x i )=1 if Y i =  X  . a Y 1 ( x 1 ) and a Y 2 ( x 2 ) are computed as follows. At the root of Tree1, we sort all records in the partition according to Y step if Y 1 =  X  ). For the value x 1 represented by the root, a is equal to the number of distinct Y 1 values in the sorted list. On performing the winner specialization w , as we follow Link Tree1 to specialize each partition P 1 on the link, we create the sorted list of records for the specialized partitions of P ues x 11 ,  X  X  X  ,x 1 c . Note that these lists are automatically sorted because their  X  X arent X  list is sorted. For the static Tree2, we can collect a Y 2 ( x 2 ) at each leaf node representing a value x an initialization and subsequently never need to modify it.
Case 2: Y 2 is a key in T 2 . In this case, the matching pairs (
P 1 ,P 2 ) for the same value x do not share any common Y values; therefore, there is no duplicate Y value for x . To see this, let P air be the set of all matching pairs ( P 1 ,P 2 ) representing x . Since all Taxation Age (Ag) Cont. 17 -90 Common Martial-status (M) Cat. 7 4
Immigra-Native-country (Nc) Cat. 40 5 P  X  X  in P air x have the same X value (i.e., x ), they must have different join values on J 1 (otherwise they should not be different partitions). This means that each P 2 occurs in at most one pair (
P 1 ,P 2 ) in P air x .Since P 2  X  X  are disjoint and Y 2 is a key of T the pairs ( P 1 ,P 2 ) in P air x involve disjoint sets of Y fore, disjoint sets of Y values. This property ensures that, for each matching ( P 1 ,P 2 ) , a Y ( x ) can be computed by a Y 1 where a Y 1 ( x 1 ) and a Y 2 ( x 2 ) are stored with P 1 Tree2, as in Case 1. Note that a Y 2 ( x 2 ) is equal to | is a key of T 2 .

Case 3: Y 1 is a key of T 1 and Y 2 =  X  . In this case, each P in Tree1 involves | P 1 | distinct Y 1 values and shares no common Y values with other partitions. To update the X -tree, for each P and all pairs ( P 1 ,P 2 ) representing the same value x on X ,weset a ( x ) to | P 1 | only once . Note that Y 2 =  X  is required; otherwise we have to check for duplicates of Y values.

Case 4: Y is a key of the join of T 1 and T 2 . For example, if Y = { K 1 ,K 2 } ,where K i is a key in T i . In this case, a is equal to the number of records containing x in the join. Since each pair ( P 1 ,P 2 ) involves a disjoint set of records in the join, we increment a Y ( x ) by | P 1 | X | P 2 | for the value x represented by (
P All experiments were conducted on an Intel Pentium IV 2.4GHz PC with 1GB RAM. The data set is the publicly available Adult data set from [14], previously used in [2][5][6][8]. There were 30,162 and 15,060 records without missing values for the pre-split training set and testing set respectively. We combined them into one set for generalization. Table 2 describes the attributes (Cat. for categori-cal and Cont. for continuous) and the binary Class corresponding to income levels  X  50K or &gt; 50K. We adopted the taxonomy trees in [5]. The data is released to two users. Taxation Department ( T is interested in the first 12 attributes and the Class attribute. Immi-gration Department ( T 2 ) is interested in the last 5 attributes. Both are interested in the 3 common attributes in the middle, M , Re , S . We created two versions of the data set ( T 1 ,T 2 ) ,SetAandSetB.
Set A (categorical attributes only): This data set contains only categorical attributes. T 1 contains the Class attribute, the 3 cate-gorical attributes for Taxation Department and the 3 common at-tributes. T 2 contains the 2 categorical attributes for Immigration Department and the 3 common attributes. The top 6 ranked at-tributes in T 1 are M , Re , S , E , O , W in that order, ranked by discriminative power on the Class attribute. The join attributes are the common attributes M, Re, S . The rationale is that if join at-tributes are not important, they should be removed first.
Set B (categorical and continuous attributes): In addition to the categorical attributes as in Set A, T 1 contains the additional 6 continuous attributes from Taxation Department. T 2 is the same as in Set A. The top 7 attributes in T 1 are Cg , Ag , M , En , Re , H , S in that order.

We consider two cost metrics. The  X  X lassification metric X  is the classification error on the generalized testing set of T 1 classifier for Class is built from the generalized training set of T The  X  X istortion metric X  was proposed in [17]. Each time a cate-gorical value is generalized to the parent value in a record in T there is one unit of distortion. For a continuous attribute, if a value v is generalized to an interval [ a, b ) ,thereis ( b  X  a unit of distortion for a record containing v ,where [ f 1 full range of the continuous attribute. The distortion is separately computed for categorical attributes and continuous attributes. The total distortion is normalized by the number of records.
We choose X so that (1) X contains the N top ranked attributes in T 1 for a specified N (to ensure that the generalization is per-formed on important attributes), (2) X contains all join attributes (thus Case 1 in Section 5.4), and (3) X contains all attributes in T TopN refers to the ( X, Y ) -anonymity so chosen. Below, K key in T i , i =1 , 2 . We compare the following error or distortion:  X 
XY E : the error produced by our method with Y = K 1 .  X  XY E ( row ) : the error produced by our method with Y = {  X 
BLE : the error produced by the unmodified data.  X 
KAE : the error produced by k -anonymity on T 1 with QID = att ( T 1 ) .  X  RJE : the error produced by removing all join attributes from  X 
XY D : the distortion produced by our method with Y = K 1  X 
KAD : the distortion produced by k -anonymity on T 1 with QID = att ( T 1 ) .
 The  X  X enefit X  and  X  X oss X  refer to the error/distortion reduction and increase by our method in comparison with another method.
Results for Set A. Figure 1 depicts KAD and XY D averaged over the thresholds k =40 , 80 , 120 , 160 , 200 , with KAD being the benefit compared to k -anonymization. For Top3 to Top6 , this benefit ranges from 1 to 7.16, which is significant consider-ing KAD = 9.23. Figure 2 depicts the classification error av-eraged over the thresholds k =40 , 80 , 120 , 160 , 200 . BLE 17 . 5% ,RJE =22 . 3% ,KAE =18 . 4% . The main results are summarized as follows.

XY E  X  BLE : this is the loss of our method compared to the unmodified data. In all the cases tested, XY E  X  BLE is at most 0.9%, with the error on the unmodified data being BLE =17 This small error increase, for a wide range of privacy requirements, suggests that the information utility is preserved while anonymiz-ing the database in the presence of previous releases.

XY E  X  XY E ( row ) : this is the loss due to providing anonymiza-tion wrt Y = { K 1 } compared to anonymization wrt Y = { K For the same threshold k ,since a K 1 ( x )  X  a K 1 ,K 2 ( mer requires more generalization than the latter. However, this ex-periment shows that the loss is no more than 0.2%. On the other hand, the anonymization with Y = { K 1 ,K 2 } failed to provide the anonymity wrt K 1 . For example, for Top6 and k = 200 , 5.5% of the X values linked to more than 200 values on { K 1 ,K 2 tually linked to less than 200 distinct values on K 1 . This problem cannot be easily addressed by a larger threshold k on the number of values for { K 1 ,K 2 } because the number of K 1 values involved can be arbitrarily low.

RJE  X  XY E : this is the benefit over the removal of join at-tributes. It ranges from 3.9% to 4.9%, which is significant consid-ering the base line error BLE =17 . 5% . The benefit could be more significant if there are more join attributes. Since the attacker typi-cally uses as many attributes as possible for join, simply removing join attributes is not a good solution.
 KAE  X  XY E : this is the benefit over the k -anonymization on T . For Set A, this benefit is not very significant. The reason is that T 1 contains only 6 attributes, many of which are included in X to ensure that the generalization is not on trivial attributes. As a result, the privacy requirement becomes somehow similar to the standard k -anonymization on all attributes in T 1 . However, Set B where T 1 contains more attributes, a more significant benefit was demonstrated.

Results for Set B. Figure 3 shows the distortion reduction com-paredtothe k -anonymization of T 1 , KAD ( cat )  X  XY D ( categorical attributes, and KAD ( cont )  X  XY D ( cont ) for contin-uous attributes. For both types of attributes, the reduction is very significant. This strongly supports that the lossy join achieves pri-vacy with less data distortion. Figure 4 depicts the classification error. BLE =14 . 7% , RJE =17 . 3% , and averaged KAE = 18 . 2% . The main results are summarized as follows.

XY E  X  BLE : this loss is averaged at 0.75%, a slight increase of error compared to the unmodified data.

XY E  X  XY E ( row ) : We observed no loss for achieving the more restrictive anonymization wrt Y = { K 1 } compared to wrt Y = { K 1 ,K 2 } . We noted that both methods bias toward continu-ous attributes and all join (categorical) attributes are fully general-ized to the top value AN Y . In this case, every record in T every record in T 2 , which makes a K 1 ( x ) and a K 1 ,K RJE  X  XY E : this benefit is smaller than in Set A. For Set B, join attributes are less critical due to the inclusion of continuous attributes, and the removal of join attributes results in a more gentle loss.
 KAE  X  XY E : this benefit is more significant than in Set A. The k -anonymization of T 1 suffers from a more drastic general-ization on QID that now contains both continuous and categorical attributes in T 1 . As a result, our benefit of not generalizing all at-tributes in T 1 is more evident in this data set.
Scalability. For all the above experiments, our algorithm took less than 30 seconds, including disk I/O operations. To further eval-uate its scalability, we enlarged Set A as follows. Originally, both T 1 and T 2 contain 45,222 records. For each original record r in a table T i , we created  X   X  1  X  X ariations X  of r ,where  X &gt; pansion scale. For each variation of r in T i , we assigned a unique identifier for K i , randomly and uniformly selected q attributes from X , i =1 , 2 , randomly selected some values for these q attributes, and inherited the other values from the original r . The rationale of variations is to increase the number of partitions in Tree1 and Tree2. The enlarged data set has  X   X  45 , 222 records in each ta-ble. We employed the Top6 ( X, Y ) -anonymity requirement with Y = K 1 and k =40 in Set A. Other choices require less runtime.
Figure 5 depicts the runtime distribution in different phases of our method for 200K to 1M data records in each table. Our method spent 885 seconds in total to transform 1M records in T 1 imately 80% of the time was spent on the preprocessing phase, i.e., sorting records in T 1 by K 1 and building Tree2. Generalizing T satisfy the ( X, Y ) -anonymity took less than 4% of the total time.
In this experiment, we focused on the classification error be-cause the distortion due to ( X, Y ) -Linkability is not comparable with the distortion due to k -anonymity. For Set A, we specified four ( X, Y ) -linkability requirements, denoted Top1 , Top2 , Top3 and Top4 , such that Y contains the top 1, 2, 3 and 4 categorical attributes in T 1 . The rationale is simple: if Y does not contain important attributes, removing all attributes in Y from T provide an immediate solution. We specified the 50% least fre-quent (therefore, most vulnerable) values of each attribute in Y as the sensitive properties y . X contains all the attributes in T in Y , except T 2 .Ra and T 2 .N c because otherwise no privacy re-quirement can be satisfied. For Set B, T 1 and X contain the 6 continuous attributes, in addition to the categorical attributes in Set A. Besides XY E , BLE and RJE in Section 6.1, RSE denotes the error produced by removing all attributes in Y from T
Results for Set A. Figure 6 shows the averaged error over thresh-olds k = 10% , 30% , 50% , 70% , 90% . BLE =17 . 5% and RJE 22 . 3% . XY E  X  BLE is no more than 0.7%, a small loss for a wide range of ( X, Y ) -linkability requirement compared to the un-modified data. RSE  X  XY E is the benefit of our method over the removal of Y from T 1 . It varies from -0.2% to 5.6% and increases as more attributes are included in Y . RJE  X  XY E spans from 4.1% to 4.5%, showing that our method better preserves informa-tion than the removal of join attributes.
 Results for Set B. Figure 7 depicts the averaged XY E and RSE . BLE =14 . 7% and RJE =17 . 3% . XY E is 15.8%, 1.1% above BLE . RSE  X  XY E spans from 0.1% to 1.9%, and RJE  X  XY E spans from 0.7% to 2.6%. These benefits are smaller than in Set A because continuous attributes in Set B took away clas-sification from categorical attributes. In other words, the removal of join attributes or attributes in Y , all being categorical attributes, causes less error. However, XY E consistently stayed below RSE and RJE .

Scalability. Our algorithm took less than 20 seconds in Set A and less than 450 seconds in Set B, including disk I/O operations. The longest time was spent on Set B for ( X, Y ) -linkability because the interval for a continuous attribute is typically split many times before the maximum linkability is violated. For scalability eval-uation, we used the Top1 requirement described above for Set A and k = 90% . We enlarged Set A as described in Section 6.1, but the values for Y are inherited from the original r instead of being assigned unique identifiers. Figure 8 depicts the runtime distribu-tion of our method with 200K to 1M data records in each table. Our method spent 83 seconds to transform 1M records in T 1 preprocessing phase, i.e., building Tree2, took less than 1 second. Generalizing T 1 to satisfy the ( X, Y ) -linkability took 25 seconds.
The proposed method pays a small data penalty to achieve a wide range of ( X, Y ) -privacy in the scenario of sequential releases. The method is superior to several obvious candidates, k -anonymization, removal of join attributes, and removal of sensitive attributes, which do not respond dynamically to the ( X, Y ) -privacy specification and the generalization of join. The experiments showed that the dynam-ical response to the generalization of join helps achieve the speci-fied privacy with less data distortion. The proposed index structure is highly scalable for anonymizing large data sets.
We now extend this approach to the general case that more than one previously released tables T 2 ,  X  X  X  ,T p . One solution is first joining all previous releases T 2 ,  X  X  X  ,T p into one  X  X istory table X  and then applying the proposed method for two releases. This his-tory table is likely extremely large because all T 2 ,  X  X  X  generalized versions and there may be no join attributes between them. A preferred solution should deal with all releases at their original size. Our insight is that, as remarked at the end of Sec-tion 4, Lemma 4.1-4.2 can be extended to this general case. Below, we extend some definitions and modification required for the top-down specialization in Section 5.

Let t i be a record in T i .The Consistency Predicate states that, for all releases T i that have a common attribute A , t i the same generalization path in the taxonomy tree for A .The In-consistency Predicate states that for distinct attributes T T .B , t i .A and t j .A are not semantically inconsistent according predicates. The join of T 1 ,T 2 ,  X  X  X  ,T p is a table that contains all matches ( t 1 ,t 2 ,  X  X  X  ,t p ) .Fora ( X, Y ) -privacy on the join, X and Y are disjoint subsets of att ( T 1 )  X  att ( T 2 )  X  X  X  X  X  X contains a common attribute A , X contains all T i .A such that T contains A .
 that tables T 2 ,  X  X  X  ,T p were previously released. The data holder wants to release a table T 1 , but wants to ensure a ( X, Y on the join of T 1 ,T 2 ,  X  X  X  ,T p .The sequential anonymization is to generalize T 1 on the attributes in X  X  att ( T 1 ) such that the join satisfies the privacy requirement and T 1 remains as useful as possi-ble.

We consider only ( X, Y ) -linkability for the top-down special-ization; the extension for ( X, Y ) -anonymity can be similarly con-sidered. For simplicity, we assume that previous releases T have a star join with T 1 :every T i ( i&gt; 1 ) joins with T forming the winner specialization w , we use Treei, i =1 , probe matching partitions in T i .Let J i ( j ) denote the set of join at-tributes in T i with T j .Let X i = X  X  att ( T i ) and Y Tree1 is partitioned by X 1  X  J 1 (2)  X  X  X  X  X  J 1 ( p )) .For i Treei is partitioned by J i (1) and X i  X  J i (1) . AsinSection5, we identify the partitions on Link [ w ] in Tree1. For each parti-tion P 1 on the link, we probe the matching partitions P i by matching J i (1) and J 1 ( i ) , 1 &lt;i  X  p .Let ( P 1 such that P 1 matches P i , 2  X  i  X  p .If ( P 1 ,  X  X  X  ,P p both predicates, we update the X -tree for the value x represented by ( P 1 ,  X  X  X  ,P p ) : increment a ( x, y ) by s 1  X  X  X  X  X  s p ment a ( x ) by | P 1 | X  X  X  X  X | P p | ,where s i = | P i | if Y s = | P i [ y i ] | if Y i =  X  .
Previous works on k -anonymization focused on a single release of data. In reality, data is not released in one-shot, but released con-tinuously to serve various information purposes. The availability of related releases enables sharper identification attacks through a global quasi-identifier made up of the attributes across releases. In this paper, we studied the anonymization problem of the current re-lease under this assumption, called sequential anonymization .We extended the privacy notion to this case. We introduced the notion of lossy join as a way to hide the join relationship among releases. We addressed several computational challenges raised by the dy-namic response to the generalization of join, and we presented a scalable solution to the sequential anonymization problem. [1] G. Aggarwal, T. Feder, K. Kenthapadi, R. Motwani, [2] R. Bayardo and R. Agrawal. Data privacy through optimal [3] C. Clifton. Using sample size to limit exposure to data [4] A. Deutsch and Y. Papakonstantinou. Privacy in database [5] B. C. M. Fung, K. Wang, and P. S. Yu. Top-down [6] V. S. Iyengar. Transforming data to satisfy privacy [7] D. Kifer and J. Gehrke. Injecting utility into anonymized [8] K. LeFevre, D. J. DeWitt, and R. Ramakrishnan. Incognito: [9] K. LeFevre, D. J. DeWitt, and R. Ramakrishnan. Mondrian [10] A. Machanavajjhala, J. Gehrke, and D. Kifer. l -diversity: [11] B. Malin and L. Sweeney. How to protect genomic data [12] A. Meyerson and R. Williams. On the complexity of optimal [13] G. Miklau and D. Suciu. A formal analysis of information [14] D. J. Newman, S. Hettich, C. L. Blake, and C. J. Merz. UCI [15] J. R. Quinlan. C4.5: Programs for Machine Learning . [16] P. Samarati. Protecting respondents X  identities in microdata [17] P. Samarati and L. Sweeney. Protecting privacy when [18] C. E. Shannon. A mathematical theory of communication. [19] L. Sweeney. k -Anonymity: a model for protecting privacy. In [20] K. Wang, B. C. M. Fung, and G. Dong. Integrating private [21] K. Wang, B. C. M. Fung, and P. S. Yu. Template-based [22] K. Wang, B. C. M. Fung, and P. S. Yu. Handicapping [23] K. Wang, P. S. Yu, and S. Chakraborty. Bottom-up [24] R. C. W. Wong, J. Li., A. W. C. Fu, and K. Wang. [25] X. Xiao and Y. Tao. Personalized privacy preservation. In [26] C. Yao, X. S. Wang, and S. Jajodia. Checking for
