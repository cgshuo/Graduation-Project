 Transition-based parsing algorithms construct out-put syntax trees using a sequence of shift-reduce actions. They are attractive in computational ef-ficiency, allowing linear time decoding with de-terministic (Nivre, 2008) or beam-search (Zhang and Clark, 2008) algorithms. Using rich non-local features, transition-based parsers achieve state-of-the-art accuracies for dependency parsing (Zhang and Nivre, 2011; Zhang and Nivre, 2012; Bohnet and Nivre, 2012; Choi and McCallum, 2013; Zhang et al., 2014).

Deterministic transition-based parsers works by making a sequence of greedy local deci-sions (Nivre et al., 2004; Honnibal et al., 2013; Goldberg et al., 2014; G  X  omez-Rodr  X   X guez and Fern  X  andez-Gonz  X  alez, 2015). They are attractive by very fast speeds. Traditionally, a linear model has been used for the local action classifier. Re-cently, Chen and Manning (2014) use a neural net-work ( NN ) to replace linear models, and report im-proved accuracies.

A contrast between a neural network model and a linear model is shown in Figure 1 (a) and (b). Figure 1: Five deterministic transition-based parsers with discrete and continuous features. A neural network model takes continuous vector representations of words as inputs, which can be pre-trained using large amounts of unlabeled data, thus containing more information. In addition, us-ing an extra hidden layer, a neural network is ca-pable of learning non-linear relations between au-tomatic features, achieving feature combinations automatically.

Discrete manual features and continuous fea-tures complement each other. A natural question that arises from the contrast is whether traditional discrete features and continuous neural features can be integrated for better accuracies. We study this problem by constructing the neural network shown in Figure 1 (e), which incorporates the dis-crete input layer of the linear model (Figure 1 (a)) into the NN model (Figure 1 (b)) by conjoining it with the hidden layer. This architecture is con-nected with previous work on incorporating word embeddings into a linear model.

In particular, Turian et al. (2010) incorporate word embeddings as real-valued features into a CRF model. The architecture is shown in Figure 1(c), which can be regarded as Figure 1(e) with-out the hidden layer. Guo et al. (2014) find that the accuracies of Turian et al can be enhanced by discretizing the embedding features before com-bining them with the traditional features. They use simple binarization and clustering to this end, find-ing that the latter works better. The architecture is shown in Figure 1(d). In contrast, Figure 1(e) di-rectly combines discrete and continuous features, replacing the hard-coded transformation function of Guo et al. (2014) with a hidden layer, which
We correlate and compare all the five systems in Figure 1 empirically, using the SANCL 2012 data (Petrov and McDonald, 2012) and the stan-dard Penn Treebank data. Results show that the method of this paper gives higher accura-cies than the other methods. In addition, the method of Guo et al. (2014) gives slightly better accuracies compared to the method of Turian et al. (2010) for parsing task, consistent with Guo et al X  X  observation on named entity recognition (NER). We make our C++ code publicly avail-able under GPL at https://github.com/ SUTDNLP/NNTransitionParser . We take Chen and Manning (2014), which uses the arc-standard transition system (Nivre, 2008). Given an POS-tagged input sentence, it builds a projective output y by performing a sequence of state transition actions using greedy search. Chen and Manning (2014) can be viewed as a neutral alternative of MaltParser (Nivre, 2008).

Although not giving state-of-the-art accuracies, deterministic parsing is attractive for its high pars-ing speed (1000+ sentences per second). Our in-corporation of discrete features does not harm the overall speed significantly. In addition, determin-istic parsers use standard neural classifiers, which allows isolated study of feature influences. Following Chen and Manning (2014), training of all the models using a cross-entropy loss objective with a L2-regularization, and mini-batched Ada-Grad (Duchi et al., 2011). We unify below the five deterministic parsing models in Figure 1. 3.1 Baseline linear ( L ) We build a baseline linear model using logistic re-gression (Figure 1(a)). Given a parsing state x , a vector of discrete features  X  d ( x ) is extracted ac-cording to the arc-standard feature templates of Ma et al. (2014a), which is based on the arc-eager templates of Zhang and Nivre (2011). The score of an action a is defined by where  X  represents the sigmoid activation func-tion, the feature weights with respect to actions, a can be SHIFT, LEFT( l ) and RIGHT( l ). 3.2 Baseline Neural ( NN ) We take the Neural model of Chen and Manning (2014) as another baseline (Figure 1(b)). Given a parsing state x , the words are first mapped into continuous vectors by using a set of pre-trained word embeddings. Denote the mapping as  X  e ( x ) . In addition, denote the hidden layer as  X  h , and the i th node in the hidden as  X  h,i ( 0  X  i  X |  X  h | ). The hidden layer is defined as where put and hidden layers. The score of an action a is defined as where hidden and output layers. We use the arc-standard features  X  e as Chen and Manning (2014), which is also based on the arc-eager templates of Zhang and Nivre (2011), similar to those of the baseline model L . 3.3 Linear model with real-valued We apply the method of Turian et al. (2010), com-bining real-valued embeddings with discrete fea-tures in the linear baseline (Figure 1(c)). Given a state x , the score of an action a is defined as where  X  is the vector concatenation operator. 3.4 Linear model with transformed We apply the method of Guo et al. (2014), com-bining embeddings into the linear baseline by first transforming into discrete values. Given a state x , the score of an action is defined as where d is a transformation function from real-value to binary features. We use clustering of em-beddings for d as it gives better performances ac-cording to Guo et al. (2014). Following Guo et al. (2014), we use compounded clusters learnt by K-means algorithm of different granularities. 3.5 Directly combining linear and neural We directly combine linear and neural features (Figure 1(e)). Given a state x , the score of an ac-tion is defined as where  X  h is the same as the NN baseline. Note that like d in Guo,  X  h is also a function that trans-forms embeddings  X  e . The main difference is that it can be tuned in supervised training. 4.1 Setting We perform experiments on the SANCL 2012 web data (Petrov and McDonald, 2012), using the Wall Street Journal (WSJ) training corpus to train the models and the WSJ development corpus to tune parameters. We clean the web domain texts fol-lowing the method of Ma et al. (2014b). Au-tomatic POS tags are produced by using a CRF model trained on the WSJ training corpus. The POS tags are assigned automatically on the train-ing corpus by ten-fold jackknifing. Table 1 shows the corpus details.
 Table 1: Corpus statistics of our experiments, where TA denotes POS tagging accuracy.

Following Chen and Manning (2014), we use the pre-trained word embedding released by Col-lobert et al. (2011), and set h = 200 for the hidden  X  = 0 . 01 for the initial learning rate of Adagrad. 4.2 Development Results Fine-tuning of embeddings. Chen and Man-ning (2014) fine-tune word embeddings in su-pervised training, consistent with Socher et al. (2013). Intuitively, fine-tuning embeddings allows in-vocabulary words to join the parameter space, thereby giving better fitting to in-domain data. However, it also forfeits the benefit of large-scale pre-training, because out-of-vocabulary (OOV) words do not have their embeddings fine-tuned. In this sense, the method of Chen and Manning resembles a traditional supervised sparse linear model, which can be weak on OOV.

On the other hand, the semi-supervised learning methods such as Turian et al. (2010) and Guo et al. (2014), do not fine-tune the word embeddings. Embeddings are taken as inputs rather than model parameters. Therefore, such methods can expect better cross-domain accuracies.

We empirically compare the models NN , Turian and This by fine-tuning (+T) and not fine-tuning (-T) word embeddings, and the results are shown in Figure 2. As expected, the baseline NN model gives better accuracies on WSJ with fine-tuning, but worse cross-domain accuracies. Interestingly, our combined model gives consistently better ac-curacies with fine-tuning. We attribute this to the use of sparse discrete features, which allows the model to benefit from large-scale pre-trained embeddings without sacrificing in-domain perfor-mance. The observation on Turian is similar. For the final experiments, we apply fine-tuning on the NN model, but not to the Turian and This . Note also tat for all experiments, the POS and label em-bedding features of Chen and Manning (2014) are fine-tuned, consistent with their original method. Dropout rate. We test the effect of dropout (Hin-ton et al., 2012) during training, using a default ra-tio of 0.5 according to Chen and Manning (2014). In our experiments, we find that the dense NN model and our combined model achieve better per-formances by using dropout, but the other models do not benefit from dropout. 4.3 Final Results The final results across web domains are shown in Table 2. Our logistic regression linear parser and re-implementation of Chen and Manning (2014) give comparable accuracies to the percep-
It can be seen from the table that both Turian and Guo 4 outperform L by incorporating embed-ding features. Guo gives overall higher improve-ments, consistent with the observation of Guo et al. (2014) on NER. Our methods gives signifi-cantly 5 better results compared with Turian and Guo , thanks to the extra hidden layer.

Our OOV performance is higher than NN , because the embeddings of OOV words are not tuned, and hence the model can handle them effectively. Interestingly, NN gives higher accuracies on web domain out-of-embedding-vocabulary (OOE) words, out of which 54% are in-vocabulary.

Note that the accuracies of our parsers are lower than the best systems in the SANCL shared task, which use ensemble models. Our parser enjoys the fast speed of deterministic parsers, and in partic-ular the baseline NN parser (Chen and Manning, 2014). For comparison with related work, we conduct ex-periments on Penn Treebank corpus also. We use the WSJ sections 2-21 for training, section 22 for development and section 23 for testing. WSJ con-stituent trees are converted to dependency trees us-with previous work. The ZPar POS-tagger is used to assign POS tags. Ten-fold jackknifing is per-formed on the training data to assign POS auto-matically. For this set of experiments, the parser hyper-parameters are taken directly from the best settings in the Web Domain experiments.

The results are shown in Table 3, together with some state-of-the-art deterministic parsers. Com-paring the L , NN and This models, the observa-tions are consistent with the web domain. Table 3: Main results on WSJ. All systems are de-terministic.

Our combined parser gives accuracies competi-tive to state-of-the-art deterministic parsers in the literature. In particular, the method of Chen and Manning (2014) is the same as our NN baseline. Note that Zhou et al. (2015) reports a UAS of 91.47% by this parser, which is higher than the results we obtained. The main results include the use of different batch size during, while Zhou et al. (2015) used a batch size of 100,000, we used a batch size of 10,000 in all experiments. Honnibal et al. (2013) applies dynamic oracle to the deter-ministic transition-based parsing, giving a UAS of 91.30%. Ma et al. (2014a) is similar to ZPar local, except that they use the arc-standard transitions, while ZPar-local is based on arc-eager transitions. punctuations, leading to about 1% UAS improve-ments over the vanilla system.

Recently, Dyer et al. (2015) proposed a de-terministic transition-based parser using LSTM, which gives a UAS of 93.1% on Stanford conver-sion of the Penn Treebank. Their work shows that more sophisticated neural network structures with long term memories can significantly improve the accuracy over local classifiers. Their work is or-thogonal to ours. As discussed in the introduction, our work is re-lated to previous work on integrating word embed-dings into discrete models (Turian et al., 2010; Yu et al., 2013; Guo et al., 2014). Along this line, there has also been work that uses a neural net-work to automatically vectorize the structures of a sentence, and then taking the resulting vector as features in a linear NLP model (Socher et al., 2012; Tang et al., 2014; Yu et al., 2015). Our re-sults show that the use of a hidden neural layer gives superior results compared with both direct integration and integration via a hard-coded trans-formation function (e.g binarization or clustering).
There has been recent work integrating contin-uous and discrete features for the task of POS tagging (Ma et al., 2014b; Tsuboi, 2014). Both models have essentially the same structure as our model. In contrast to their work, we systemati-cally compare various ways to integrate discrete and continuous features, for the dependency pars-ing task. Our model is also different from Ma et al. (2014b) in the hidden layer. While they use a form of restricted Boltzmann machine to pre-train the embeddings and hidden layer from large-scale ngrams, we fully rely on supervised learning to train complex feature combinations.

Wang and Manning (2013) consider integrat-ing embeddings and discrete features into a neu-ral CRF. They show that combined neural and dis-crete features work better without a hidden layer (i.e. Turian et al. (2010)). They argue that non-linear structures do not work well with high di-mensional features. We find that using a hid-den layer specifically for embedding features gives better results compared with using no hidden lay-ers. We studied the combination of discrete and con-tinuous features for deterministic transition-based dependency parsing, comparing several methods to incorporate word embeddings and traditional sparse features in the same model. Experiments on both in-domain and cross-domain parsing show that directly adding sparse features into a neural network gives higher accuracies compared with all previous methods to incorporate word embeddings into a traditional sparse linear model.
 We thank the anonymous reviewers for their con-structive comments, which helped to improve the paper. This work is supported by the Singapore Ministry of Education (MOE) AcRF Tier 2 grant T2MOE201301 and SRG ISTD 2012 038 from Singapore University of Technology and Design.
