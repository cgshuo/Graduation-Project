 This paper describes a simple clustering approach to person name disambiguation of retrieved documents. The methods are based on standard IR concepts and do not require any task-specific features. We compare different term-weighting and indexing methods and evaluate their performance against the Web People Search task (WePS). Despite their simplicity these approaches achieve very competitive performance. H.3 [ Information Storage and Retrieval ]: H.3.1 Con-tent Analysis and Indexing; H.3.3 Information Search and Retrieval; H.3.4 Systems and Software; H.4 [ Information Systems Applications ]: H.4.2 Types of Systems General Terms: Algorithms, Measurement, Performance, Experimentation Keywords: Clustering, Person name disambiguation Searching for people is one of the most common web search tasks. An increasing number of people now have a web pres-ence; be it directly through their home page or indirectly, .e.g., through their employer X  X  or sports club X  X  web page. This also means that there is a growing number of web pages that are associated with different persons sharing the same name. From a user X  X  perspective it can be a tedious task to discriminate among the returned search results between the different people. A much preferred solution to this is a system that automatically groups the results in a way that disambiguates between the different people sharing a name.
The Web People Search Evaluation workshops (WePS) [2] focus on the problem of people disambiguation in search and provide data sets to evaluate different approaches.
Our approach described here has been developed and eval-uated in the context of the WePS challenge. While most previous approaches tried to integrate person-specific fea-tures into their disambiguation approach, our approach is fairly general, requiring hardly any specific adaptations to the person disambiguation task. Yet, at the same time our approach is not only robust but also achieved the second highest performance out of 78 submitted runs in the latest WePS evaluation [3].
 There are a number of well-established clustering approaches that have been used in various machine learning tasks, with K-Means clustering and agglomerative hierarchical cluster-ing being the most prominent ones. Our experiments con-firmed the findings of Balog et al. [4] that agglomerative hi-erarchical clustering performs best in the context of person disambiguation and we focus our attention on the various strategies within agglomerative hierarchical clustering.
Agglomerative hierarchical clustering is an unsupervised, greedy machine learning approach that iteratively groups items together. Starting with documents being their own cluster (i.e. a singleton cluster), the two clusters with the highest similarity are clustered together, replacing the orig-inal two clusters. This step is applied iteratively until only one cluster remains, or the stopping criterion is fulfilled, which can be a fixed number of iterations, or X  X s we do here X  X  minimum similarity threshold [7].

At the core of agglomerative hierarchical clustering lies the definition of similarity. The approaches we investigate here are centered around cosine similarity, derivations of which are commonly used in IR. While several of the previous top-performing approaches exploit richer features, includ-ing named entity recognition [5, 6], base phrase structure [5], and document structure information [5, 6], we are par-ticularly interested in finding out how far standard retrieval-based measures can take us. The approaches discussed here are evaluated against the dif-ferent WePS corpora: WePS-1 dev-test (49 names), WePS-1 test (30 names), and WePS-2 test (30 names). WePS-1 dev-test was distributed before WePS-1 to allow participants to develop their systems, while the latter two were the test sets of the respective WePS evaluations.

The quality of a clustering is typically measured with re-spect to purity and inverse purity, but recently Amig  X o et al. [1] have shown that B-cubed precision and recall have most of the desirable properties of a clustering evaluation metric. As B-cubed precision and recall are also the main metrics for the WePS-2 evaluation we use them for comparing our different approaches. B-cubed precision (B 3 -P) and recall (B 3 -R) are defined as [1]: where c ( d ) is the set of documents put by the system into the same cluster as d and t ( d ) is the ground truth, i.e. the docu-The metrics used are B-cubed precision (B 3 -P) and recall (B -R B 3 -F B 3 -P B 3 -R B 3 -F B 3 -P B 3 -R B 3 -F ments with which d should be clustered together. [  X  ] returns 1 if the argument statement is true and 0 otherwise. We also combine both metrics in the macro-averaged F-Score (with  X  = 0 . 5).

Below we describe the dimensions along which we com-pared the different approaches. Names between parentheses refer to the runs in Table 1.
 can be based on the similarity between the two closest/farthest documents (min/max) or the centroid of all documents within the clusters (centr). Overall, the min strategy (c) outper-forms the other two ((a) and (b)).
 ments can be based on the surface words (surface) or the stemmed terms (stm). Additionally, the document repre-sentation can be expanded by WordNet hypernyms (hyper). Stemming improves performance ((d) outperforms (c)), but adding hypernyms does hurt; see (e) vs. (d).
 term within-document frequencies (tf nor), where the tf-score is computed as: Using the absolute frequencies instead (abs) again hurts per-formance substantially; see (f) vs. (d).
 some approaches use local (loc) document frequencies, con-sidering only the documents retrieved for the name to be dis-ambiguated. Using the document frequencies for entire col-lections, i.e. all names, does lead to more reliable counts and better performance; see (g) vs. (d). On the other hand, in-cluding documents frequencies from additional background collections such as web crawls (crawl) and Wikipedia (wiki) do again hurt performance; see (h) and (i) vs. (d). frequencies show that clustering is very sensitive to the set of terms that mainly represents a document. Continuing along this line we experimented with varying window sizes (win) and indexed only terms that occurred within n words from a mention of the search name (here n = 50). This approach lead to further improvements ((j) vs. (d)), and to the best overall system.
 focused on indexing named entities only, we include this approach as well (names). For the WePS-1 test collection the performance of (k) comes close to the best simple term-based measure (j), but overall, it falls clearly behind. The approach described here shows that simple methods, mainly using frequency-based statistics can lead to high per-formance in the person disambiguation task. At the same time, our comparisons show that small variations in the def-inition of the similarity function can have a substantial im-pact on performance, warranting careful evaluation.
