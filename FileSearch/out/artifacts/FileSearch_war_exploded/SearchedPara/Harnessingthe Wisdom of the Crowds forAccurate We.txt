 Clipping Web pages, namely extracting the informative clips (ar-eas) from Web pages, has many applications, such as Web printing and e-reading on small handheld devices. Although many exist-ing methods attempt to address this task, most of them can either work only on certain types of Web pages (e.g., news-and blog-like web pages), or perform semi-automatically where extra user efforts are required in adjusting the outputs. The problem of clip-ping any types of Web pages accurately in a totally automatic way remains pretty much open. To this end in this study we harness the wisdom of the crowds to provide accurate recommendation of informative clips on any given Web pages. Specifically, we lever-age the knowledge on how previous users clip similar Web pages, and this knowledge repository can be represented as a transaction database where each transaction contains the clips selected by a us-er on a certain Web page. Then, we formulate a new pattern mining problem, mining top-1 qualified pattern , on transaction database for this recommendation. Here, the recommendation considers not on-ly the pattern support but also the pattern occupancy (proposed in this work). High support requires that patterns appear frequently in the database, while high occupancy requires that patterns occupy a large portion of the transactions they appear in. Thus, it leads to both precise and complete recommendation. Additionally, we ex-plore the properties on occupancy to further prune the search space for high-efficient pattern mining. Finally, we show the effective-ness of the proposed algorithm on a human-labeled ground truth dataset consisting of 2000 web pages from 100 major Web sites, and demonstrate its efficiency on large synthetic datasets. H.2.8 [ Database Application ]: Data Mining Algorithms, Design, Experimentation  X 
The work was done when Lei Zhang and Linpeng Tang were vis-iting HP Labs China.
 Occupancy, Frequent and Dominant Pattern, Web Page Clipping, Wisdom of the Crowds
Many people use the World Wide Web as the primary informa-tion source in their daily lives. However, Web pages encoded by the HTML language are designed for viewing on PC screen and may not be suitable for other purposes, such as Web printing and e-reading on small handheld devices. Thus, extracting informa-tive clips (referring to content areas in Web pages) attracts many research works [12, 14, 3, 10, 9]. However, most of these pre-vious works focus on extracting article Web pages, such as news stories, encyclopedia entries, and blog posts. In these article pages, the informative contents include the title, text-body, content-related images and image captions and so on. These content items usually appear in the similar layouts and formats in the article Web pages. For example, the article body usually consists of contiguous para-graph blocks, which occupy the main area of the Web page. The article title is usually placed above the text-body in a more standout font. Thus, these commonly-shared visual features can be used to extract these informative items with high accuracy [3]. Besides article pages, there are a large number of non-article Web pages (e.g., online shopping pages, job recruiting pages and recipe pages). For these non-article pages the informative clips are scattered irregularly, thus it is hard to summarize the visual fea-tures, which makes the automatic extraction of these informative clips impossible. For example, in a product Web page users are usually interested in the product name, brief description, product image, and price. However, these informative items are formatted in diverse manners in different Web sites. Thus, an expedient solu-tion to clipping non-article Web pages is a semi-automatic process, where users can manually select the informative areas in an interac-tive interface. This is exactly what the tool of Smart Print for Web page printing [9].

Let us see how Smart Print works using the example in Figure 1. When a user opens the recipe Web page 2 and clicks Smart Print button, it recommends a valuable area, which is highlighted in a white rectangle (Figure 1(a)). The user can also preview the recom-mended print content (Figure 1(b)) at this time. Then, the user can interactively add or remove any content blocks on this page. For example, as shown in Figure 1(c) the recipe title "sausage-strata" is added and some uninformative clips, i.e. the six black rectangles
An extension of Web browser, download from http:www.hp.com/go/smartprint http://www.diabeticlifestyle.com/recipes/breakfast/sausage-strata inside the big white rectangle, are removed. Finally, you can see that we get exactly what we want by the current print preview as shown in Figure 1(d)). Through this whole process, we see clearly that, many tedious user efforts are still required in Smart Print . And what is more, when we want to print similar Web pages, e.g. print another recipe from the same Web site, we have to do the tedious selection again. It means that in current Smart Print users X  efforts are utilized only once and then discarded. But in fact these users X  selections are very valuable to help us to understand users X  interests and printing intentions.

Therefore, a more intelligent solution to clipping Web pages is in urgent need, where the user efforts in selecting the content areas are eliminated or greatly alleviated. Ideally, only one click is required in Clipping any Web page. To this end we leverage the wisdom of the crowds for more accurate recommendation. Here, the wisdom of the crowds comes from the log data on how previous users clip the Web pages. The print logs can be collected by Smart Print with users X  permission. At a high level, we want to learn from the print log what other users were printing for a particular Web page and then recommend the learned selection to the current user. For ex-ample, assume that the selection on the "sausage-strata" Web page (Figure 1(c)) is shared to our knowledge repository. Then, if an-other user wants to print another recipe page about "quick-lemon-bread" 3 , by leveraging the print logs our method can directly rec-ommend the perfect informative clips (as shown in Figure 1(e)(f)) without manual selection. Interested readers can also watch a demo video 4 of this work to taste its effectiveness.
 Table 1: The transaction database for Figure 1(c) and 1(e)
In fact, these log data can be represented as a transaction database where each transaction contains the content clips selected by a user on a Web page. For example, Table 1 shows a transaction database for the content clips selected in Figure 1(c) and 1(e), where item-s A and B represent the two white rectangles (clips) and items C,D,E,F,G,H represent the six black rectangles. Then, with this http://www .diabeticlifestyle.com/recipes/bread/quick-lemon-bread http://goo.gl/VixPl transaction database we formulate a novel pattern mining problem, mining top-1 qualified pattern , for accurate informative clips rec-ommendation on any given Web pages. It considers both the pat-tern support and the pattern occupancy . Previous studies widely consider support as the primary measure of pattern interestingness. In this study we introduce an another measure, occupancy , which requires that patterns should occupy a large portion of the transac-tions they appear in. Thus, intuitively the patterns with both high support and high occupancy may lead to both precise and complete recommendation. We summarize our contributions in this study as follows:  X  We harness the wisdom of the crowds for more accurate Web page clipping. We formulate this task as a pattern mining problem on transaction databases. Besides the widely-studied pattern sup-port, we propose a new concept, pattern occupancy , which mea-sures the occupancy degree of a pattern inside the transactions it appears in. Thus, pattern support together with occupancy may lead to both precise and complete recommendation.  X  We propose an efficient algorithm for this pattern mining prob-lem with the constraints of both support and occupancy. Specifical-ly, we explore the properties on occupancy, which can further prune the search space compared with the search process of frequent pat-tern mining.  X  We demonstrate the effectiveness of the proposed algorithm on a human-labeled ground truth dataset consisting of 2000 Web pages from 100 major print-worthy Web sites. The average precision and recall of the proposed method reach more than 90 % . Compared with the baseline method without the support of the wisdom of the crowds the precision increases 23.8 % and the recall increases 5.1 % . In addition, we also show the efficiency of the proposed algorith-m on large synthetic datasets. Our algorithm runs faster than the baseline method by several orders of magnitude.

The rest of the paper is organized as follows. We describe the related work in Section 2. We formulate the task of clipping We-b pages as a pattern mining problem in Section 3 and propose the efficient algorithm to solve it in Section 4. We address the issues on how to identify and represent informative clips in Web pages in Section 5 and summarize the whole solution in Section 6. We re-port an empirical study in Section 7 and finally conclude the paper in Section 8.
Web page clipping has received substantial interests in litera-ture [12, 14, 3, 10, 8, 16, 9]. Most existing methods only attemp-t to focus on extracting article Web pages. For example, Paster-nack and Roth [12] proposed a method based on the maximum subsequence segmentation to generate scores for words using fea-tures of tri-grams and tags. Wang et al.[14] proposed a SVM-based template-independent wrapper with the content and spatial features to identify two minimum sub-trees containing title and ar-ticle body. Luo et al. [10] utilized the visual and DOM tree based features to identify paragraphs as basic elements and then find the region for the main content by the maximum subsequence algo-rithm. Furthermore, some other heuristics are used to filter out the non-informative clips within the main content region. Fan et al. proposed a full-fledged system to extract title, text-body, content-related images and image captions from any article Web page [3] and achieved a high extraction accuracy.

However, such methods make use of the some prior assumptions on the layouts and formats of article Web pages and only work well for text-dominant Web pages such as news and blogs. They may not work well on non-article Web pages such as shopping, recipes or other types of Web pages where the informative clips are scattered irregularly in different formats. For non-article pages Lim et al. [9] proposed a semi-automatic tool Smart Print which can automatically select the main content of the web page in the first round and then allows the users to make adjustments. All of these previous works leverage the features of DOM tree, text or visual information on the given Web page. To our best knowledge, this work is the first study to show the log data on how previous users clip the Web pages can greatly improve the accuracy in clipping any Web page.

Pattern mining algorithms are adopted for the recommendation of informative clips. Frequent pattern mining [1] has been well rec-ognized to be fundamental to many important data mining tasks. There is a great amount of work that studies efficient mining of fre-quent patterns [5, 6, 7, 2, 11]. These algorithms can be classified into mining frequent patterns [5, 6, 7], frequent maximal pattern-s [2], and frequent closed patterns [11]. To reduce the number of frequent patterns some interestingness measures and constraints are proposed [13, 4]. The concept of occupancy proposed in this paper can be viewed as a new interestingness measure. However, it is not anti-monotonic , monotonic , convertible , and succinct [4], thus, no previous methods can be leveraged. In this study we explored the properties on occupancy and show how it can be used to greatly prune the search space. The experimental result shows its superior-ity in efficiency over the other methods.
In this section we will detail how to transform our task of Web page clipping based on the print logs into a pattern mining problem. First, we will briefly describe this task as follows.
The task of Web page clipping is actually to recommend a set of informative clips for the given Web page. In this application we assume that a log database is given for the Web pages from a Web site, where each piece of log records all informative clips a previous user selected on one of these Web pages. If we consider each clip as an item , then any piece of log becomes a transaction (set) of items and the log database becomes a transaction database [1]. The details on how to identify a clip in a Web page will be addressed later in Section 5.

Although the selections from different users may be different due to their own use habits and interests, generally their selections on similar Web pages may reach certain degree of consensus since the user intensions in clipping certain Web pages may be similar. Then, our task is to mine users X  interests and intensions in clipping these Web pages and then provide more accurate recommendation.
The key question to this task is how to measure the quality of a clipping pattern (a set of items). On one hand, the more frequently a pattern appears in the database (it means a large number of users select this set of informative clips), the better it is. Thus, the sup-port of a pattern is a key factor to its quality. On the other hand, we prefer the pattern which occupies a large portion of the transac-tions it appears in. Note that besides the items in a given pattern a transaction may include some other ones. If we apply the pattern onto the Web page corresponding to this transaction, all these other items will be missed. Intuitively, the patterns with high occupancy may lead to more complete recommendations. Thus, it is anoth-er factor of its quality. Altogether, patterns with bigger values of support and occupancy are preferred. With this motivation we give some definitions and notations as follows, and then formulate the problem.
A transaction database 5 is a set of transactions, where each trans-action is a set of items. Let I be the complete set of distinct items and T be the complete set of transactions. Any non-empty set of items is called an itemset and any set of transactions is called a transaction set . The transactions that contain all the items in an itemset X are the supporting transactions of X , denoted as The frequency of an itemset X (denoted as f req ( X ) ) is the num-ber of transactions in T X .
 The following two definitions are adopted from [1].
 Definition 1 (Support): The support of X is defined as Definition 2 (Frequent Itemset): For a given minimum support threshold  X  ( 0 &lt;  X   X  1 ), X is said to be frequent if  X  ( X )
In this application, the itemset we intend to find should also oc-cupy a large portion of the transactions in which it appears. We can calculate the occupancy degree as follows. For an itemset X we identify all its supporting transactions T X . For each transaction t  X  T X we calculate the ratio of | X | the number of items in X and t , respectively. We then aggregate these ratios to compute a single value of occupancy for X . In this paper we focus on the harmonic average of these ratios while other aggregate functions such as quantile or min may also be considered. The definition of occupancy is given as follows.
 Definition 3 (Occupancy): Formally, the occupancy of an itemset X is defined as where t is any support transaction of X .

In other words, the occupancy of an itemset X is the ratio of the occurrences of the items in X to the total number of the items in
T X (the supporting transactions of X ). The high value of the
This term is commonly used by the frequent pattern mining com-munity, such as [1]. occupanc y indicates that besides the items in X there are only a small number of items left inside the supporting transactions of X . Thus, the itemset with high occupancy is preferred.

Take the transaction database in Figure 2 as an example. Con-sider the itemset { BC } . Its supporting transactions are the support  X  ( { BC } ) = 4 6 , and the occupancy  X  ( { BC =
In some sense, occupancy describes the relative size of an item-set to its supporting transactions. One may compare it to the ab-solute size of an itemset, i.e., the number of items it contains. For example, in [15], Wang et al. formulated the problem of finding the top-k frequent itemsets that have absolute size larger than min _ l , so the absolute size of a itemset was used as measure on its quality. However, in many cases where transactions in the database varies in sizes, occupancy provides a more accurate and normalized score on the quality of the itemset. Later we will also empirically prove the superiority of occupancy over absolute size of an itemset on the application of web-print recommendation.

One may think that the itemset with a larger absolute size lead-s to a bigger value of occupancy. However, it is not always true. Here, we generate another transaction database based on the one in Figure 2. There are also 6 transactions in it, where the first 5 ones are the same with the ones in Figure 2 and the last one changes to { BCDEF GHI } . In this new transaction database  X  ( { BC  X  ( { BCD } ) &lt;  X  ( { BC } ) . The reason is that BCD only appears in large transactions where it only occupies a small fraction, while BC appears in many smaller transactions where it occupies a large fraction. Thus, occupancy does not always increase monotonically when we add more items to an itemset. Similarly, we can show that occupancy does not always decrease monotonically when we add more items to an itemset either. This non-monotonic property of occupancy is in contrast to that of support in frequent pattern mining.
 Definition 4 (Dominant Itemset): For a given minimum occupan-cy threshold  X  ( 0 &lt;  X   X  1 ), X is said to be dominant if  X  ( X )  X  .

W ith the definition of support and occupancy we can measure the quality of an itemset by combining these two factors. Definition 5 (Quality): The quality of an itemset X is defined as q ( X ) =  X  ( X ) +  X  X  ( X ) , where occupancy weight 0  X  is a user defined parameter to capture the relative importance of support and occupancy.
 Definition 6 (Qualified Itemset): For a given minimum support threshold  X  and a minimum occupancy threshold  X  ( 0 &lt;  X ,  X  1 ), X is said to be qualified if  X  ( X )  X   X  and  X  ( X )
Assume that we have the log database which records how pre-vious users clipped the Web pages from a Web site. Given a Web page from the same Web site, we aim to recommend the informa-tive clips for this Web page. More specifically, let I be the com-plete set of distinct clips in the database. For a given Web page we can get a set Q  X  I of clips which are included in this Web page. It is meaningless to recommend the informative clips which are not in the given Web page (how to determine Q will be addressed in Section 5). Thus, our task is to select a subset of Q for the clip recommendation.

In this application it is required that interesting patterns should be both frequent and dominant. On one hand, if a pattern F is fre-quent it means that there are enough cases such a pattern appears in the transaction database. Thus, it may improve the recommenda-tion precision. On the other hand, if X is dominant it indicates that the recommendation of X is complete enough. Therefore, with the definition of support and occupancy, we formulate the problem of mining top-1 qualified patterns as follows.
Mining Top-1 Qualified Pattern . With the definition of sup-port, occupancy and quality, we describe our recommendation method as follows. The recommended itemset is F  X  Q which has the maximal quality value among the qualified itemsets (for a given support threshold  X  and a occupancy threshold  X  ). Formally, it is
This problem formulation is illustrated in Figure 3. Actually, we aim to find the top-1 qualified itemset among the intersection area of frequent and dominant itemsets.

There are three parameters in the definition of the top-1 qualified pattern, namely  X ,  X ,  X  . If there is no itemset that is both frequent and dominant with respect to  X ,  X  , the top-1 qualified pattern does not exist and it will not output any result since no non-empty pat-tern exists that meets the quality requirements. Parameter  X  , the occupancy weight, is a user defined parameter to capture the rela-tive importance of support and occupancy.

One may think that dominant patterns with high occupancy usu-ally contain a large number of items and thus methods based on Maximal Frequent Itemset (an itemset X is a maximal frequent itemset if X is frequent and no superset of X is frequent [2]) min-ing may also work in this application. Specifically, given a support threshold we can get multiple maximal frequent itemsets. Among them we can select the one with the largest number of items for the recommendation. Is this a valid method for our task? The answer is "no" for the following reasons. First, in methods based on min-ing maximal frequent itemsets, the number of items in a pattern is used as a measure in pattern selection. Compared with the concept of occupancy, this is actually the absolute size of an itemset while occupancy is the relative size of an itemset to the number of items in its supporting transactions. Among all the frequent itemsets, the maximal frequent itemset selected usually has very low support, thus leading to a low precision in recommendation. Secondly, in mining top qualified pattern a weighted sum of both support and occupancy is used as the interestingness measure, which may lead to better recommendation performance compared to the pattern-s selected by methods based on maximal frequent itemsets. The experimental results in Section 7 further validate our analysis here.
As mentioned before, Wang et al. [15] formulated a similar pat-tern mining task, i.e. the top-k frequent itemset with sizes larger than a minimum threshold. Here we can immediately see the differ-ences of our problem to that problem: first, we use occupancy, the relative size, rather the absolute size of an itemset as an interesting-ness measure; second, we use the the weighted sum of support and occupancy, rather than just the support as the quality of an itemset.
Next we will propose an efficient algorithm to this new pattern mining problem.
The straightforward solution to the pattern mining problem is to first generate all the frequent itemsets, calculate the occupancy value for each frequent itemset, and then select the qualified itemset with maximum quality value. In this section we will show how the properties on occupancy and quality measures can be injected deeply into the search process and greatly prune the search space.
Pattern mining algorithms usually adopt the lexicographic subset tree [2, 11] to guide the search process. See Figure 2 as an exam-ple for four items A, B, C, D . The top element in the lattice is the empty set and each lower level l contains all the l -itemsets (itemsets with exactly l items). The l -itemsets are ordered lexicographically on each level. Generating children in this manner enumerates all the distinct itemsets to be considered without redundancy. As there are 4 items, there are in total 2 4 = 16 itemsets for consideration. Thus, in the lexicographic subset tree there are 16 nodes, each of which corresponds to an itemset. Frequent pattern mining usual-ly leverages the monotonic decreasing property of support values when adding more items to a given itemset. That is, if an itemset X is not frequent then all the supersets of X are not frequent either. Thus, the traversal in the tree is to find a cut (the red line in Fig-ure 2) such that all the nodes (itemsets) above the cut are frequent, and all the nodes below the line are infrequent.

Note in our task of clip recommendation we only consider the items in Q , namely the clips identified in the given Web page (here, I may contain the distinct clips in the logs from the same Web site). Thus, the lexicographic subset tree only includes the items in Q . The search space becomes much smaller compared with the one on the complete set of distinct items.
With the proposed occupancy and quality measure we can fur-ther prune the search space. Specifically, we can give the upper bounds of the occupancy and quality values for all the nodes in a subtree. In other words, the occupancy or quality of any node in a given subtree will be no bigger than its upper bound. If the upper bound on the occupancy is smaller than the occupancy threshold  X  , the corresponding subtree will be pruned. Also, we can maintain the current biggest quality value in the search process, denoted by q  X  . Then, the subtrees with the upper bounds less than q  X  will be pruned.
 Take the subtree with the root { C } in Figure 2 as an example. We can give an upper bound of the quality for all the nodes in the subtree, including { C } , { CD } . Currently, this upper bound is less than the quality of node { BC } . Thus, this subtree is trimmed. The blue line (which is above the red one) in Figure 2 is the cut line for our problem. Clearly, it further reduces the search space.
Next, we will propose the properties to show how the upper bounds on occupancy and quality are computed. .

First, we give the notations which will be widely used in the following properties. For any subtree, let X be the itemset for the subtree root, Y be the itemset including all the new items which will be extended in all the descendants of this subtree. For example, for the subtree root X = { A } there are three new items B, C, D which will appears in the descendants. Thus, Y = { BCD } . Then, we have the following properties.
 Lemma 1: For any itemset W in the subtree of X let u be the frequency of W . Then, the occupancy of W satisfies that where t l i is any supporting transaction for X and I is the complete set of distinct items.
 Pr oof: Remember that Y is the itemset including all the new items which will be extended in the descendants, X is the root of the subtree. Now we consider the occupancy of W . where t l 1 ,  X  X  X  , t l u be any supporting transactions of X .
Inequality (5) comes from Z  X  X  X  Y . Inequality (6) comes from the inequality that if b &gt; a &gt; 0 ,  X   X  0 , then Note that the right hand side of Equation (7) is only dependant on u, X, Y . Thus, it can be denoted as F ( u, X, Y ) . Then, we also have Pr oof: Let  X  k be the k -th largest value in the vector  X  X  T
W  X  , and  X  k be the k -th smallest value in the vector  X  X  Y ) | : t  X  T W  X  . Then, by this definition we have  X  k +1  X   X 
F or any k ( 1  X  k  X  u ) we have Then, So
Inequality (10) comes from the property that if 1  X  a i b an y i , then Inequality (11) comes from the property that if 1  X  a b  X  c It is required that itemset W be frequent. Thus, the frequency of W is not smaller than  X   X |T| . As shown in Lemma 2, F ( u, X, Y ) will not decrease along the decrease of u . Then, we have Theorem 1: In other words, u  X  is the minimal frequency value which satisfies the support threshold  X  . Theorem 1 actually gives an upper bound on the occupancy values of the itemsets in the subtree. Theorem 2: Pr oof: Let u be the frequency of W . Then,
It is required that itemset W be frequent. Thus,  X   X |T|  X  |T
X | . Then, It follows the conclusion.
 Theorem 2 actually gives an upper bound on the quality values of the itemsets in the subtree.
So far we assume that the transaction database of print logs is provided in advance. In this section we will detail how to identify the clips in a Web page. We first give some preliminaries on Web page analysis before we address these details.
Here, we provide some preliminaries for DOM (Document Ob-ject Model) tree, DOM tree path and Web page rendering, which are the key concepts in Web page analysis.

DOM Tree . A DOM tree for any HTML or XML document is defined as a tuple T = { V, E, L V , F V } , where V is the set of ver-tices, E  X  V  X  V is the set of edges, L V is the set of vertex labels, and F V : V  X  L V maps the vertices to their labels. Figure 4(a) shows an example of DOM tree.
 Figur e 4: (a) An example DOM tree; (b) The corresponding Web page for Figure 1(a); (c) Another example DOM tree with one more adv; (d) The corresponding Web page for Figure 1(c).
DOM Tree Path . A DOM Tree Path (DTPath for short) for a node v in a DOM tree T is a sequence of label-position pairs, i.e. P v = (( l 1 , k 1 ) ,  X  X  X  , ( l n , k n )) , where l HTML tag and attributes of node i and k i is the relative position among its sibling nodes. It records the path from the root of T to node v . For example, the DTPath of the node v 5 and v 6 in Figure 4(a) is ((html,1),(body,1),(div,1)) and ((html,1),(body,1),(div,2)) re-spectively. Note that in this example the attributes on each node are omitted from the DTPath just for description convenience. S-ince the node attributes contain much format and style information especially when CSS (Cascading Style Sheets) is more and more popular in Web page design, practically they help to identify the informative clips more precisely.

Matching . The DTPath P is matched in a DOM tree T , de-noted by P &lt; T , if and only if there exists a path from the root node to an internal node in T such that the sequence along this path is exactly P . Consider Figure 4(a) again. We can say the DTPath (( html, 1) , ( body, 1) , ( h 1 , 1)) can be matched in this tree since along this path we reach node v 4 . However, the DTPath (( html, 1) , ( body, 1) , ( div, 3)) can not be matched in this tree.
Web Page Rendering . The whole HTML document appears in a Web browser after rendering . The rendering process actually as-signs each DOM tree node a bounding rectangle , identifying where the content inside the node is displayed in the browser. Thus, a n-ode v also corresponds to a unique bounding rectangle. Figure 4(b) shows the Web page corresponding to the DOM tree in Figure 4(a). As you can see, v 4 , v 5 , v 6 corresponds to the three rectangles of article title , adv1 , and article body respectively.
 The tool of Smart Print provides the interactive interface in a Web browser, where users can select the informative clips in a Web page. Actually, each clip is identified by a bounding rectangle, which then corresponds to a node in the DOM tree of the Web page. Thus, any clip can be identified by the DTPath of the selected node in the DOM tree. Then, a piece of clipping log actually contains a set of DTPaths.
To recommend informative clips we need to get all the clips which are included in a given Web page. Recommending the infor-mative clips which are not in the given Web page is meaningless. As each clip is represented by its DTPath, this task is to match a set of DTPaths to a given DOM tree. More specifically, let complete set of distinct DTPaths in the database. We aim to find all the DTPaths in I , which can be matched in the given DOM tree T . Formally, this set Q is { P | P  X  I  X  P &lt; T } . These clips are the recommendation candidates in the given Web pages.
 A straightforward method to this task is to consider each DT-Path separately. Then, its complexity to n DTPaths is o ( where N i is the number of nodes in the i -th DTPath.
So far the DTPath considers the HTML tag and attributes of a node, and also its relative position among its siblings. However, the match of such DTPaths is not robust to the change of DOM tree structures. See the examples in Figure 4. The Web page in Figure 4(b) contains one advertisement, and the DTPath for article body is
P : (( html, 1) , ( body, 1) , ( div { id = \ article _ body " The Web page in Figure 4(d) has the same Web page template with the one in Figure 4(b). However, one more advertisement is insert-ed into the page. Thus, the DTPath for article body in Figure 4(d) changes to
P  X  : (( html, 1) , ( body, 1) , ( div { id = \ article _ body " P and P  X  are two distinct items in the transaction database even though they refer to the same type of content clips. It indicates that the transaction database generated by such DTPathes will be very sparse. Additionally, the clip for article body in Figure 4(d) will be missed if we apply P to match it. Thus, such DTPathes are vulnerable to the change of DOM tree structures.

After some empirical studies we find that this situation happen-s quite often, and is worth careful consideration. We address this issue by proposing generalized DTPaths by which we do not con-sider the relative positions for some nodes during the matching pro-cess. Specifically, we require that two distinct DTPathes should be combined as a generalized one when they are different only at the position indexes for some nodes in the pathes. For example, except the position indexes of the third nodes the other parts of the two paths P and P  X  are the same. Thus, they should be combined as a generalized DTPath
P  X  : (( html, 1) , ( body, 1) , ( div { id = \ article _ body " where  X  can be any integer. It indicates that we will not consider the position index of the third node in this path when matching. Using the generalized DTPath P  X  the clips of article body in Figures 4(b) and (d) can be identified successfully. Readers may also worry that some more clips will be matched by the generalized DTPaths since a generalized DTPath can match multiple clips. We argue that this situation is acceptable since the identified clips by a generalized DTPath appear in the same format and style (remember that the attributes of the HTML nodes are used here).
 After this combination the number of distinct generalized DT-Paths is much smaller than that of the original DTPaths. Thus, the item size of the corresponding transaction database becomes much smaller, leading to more efficient pattern mining. Additionally, the database becomes more dense, which definitely helps to find the patterns in clipping the Web pages by users. We summarize the solution framework in Figure 5.

Step 1. After a clipping request is fired, the client first sends the url and DOM tree of the current Web page to the server. Then, the server looks up in the log database to collect the print logs from the same Web site of the give URL. Based on these logs, it construct the corresponding transaction database T and all the distinct DTPaths P in these logs are also identified.

Step 2. Given the complete set P of distinct DTPaths we com-bine them to a set P  X  of generalized DTPaths. Accordingly, we generate the transaction database based on the generalized DTPath-s. See the details in Section 5.3.

Step 3. Identify the generalized DTPaths which is matched in the current DOM tree T . The set of the matched generalized DTPaths is denoted by Q . See the details in Section 5.2.

Step 4. Generate the pattern F in Equation 1 using the efficient algorithm in Section 4. Finally, the clips identified by the general-ized DTPaths in F are recommended to the given Web page.
In this section we present an empirical evaluation of our pro-posed method. Specifically, we show the effectiveness of the pro-posed pattern mining algorithm on a human-labeled ground-truth dataset including 2000 Web pages, discuss the impact from the method parameters and show the efficiency of our method on the synthetic datasets.
A human-labeled ground truth dataset 6 consisting of 2000 Web pages was collected from 100 major print-worthy Web sites. These Web sites were selected from the 10 Web site types, namely DIY, Recipe, Shopping, News, Jobs, Science, Article, Leaning, Travel and Sports . They are top Web site types, which attract large num-bers of Web page printing. For each Web site type we selected 10 Web sites, and for each Web site we selected 20 Web pages. The http://home.ustc.edu.cn/ stone/files/Publications/Data.zip Web pages from the same Web site may have the different Web templates. On each Web page we manually select the informative clips as the ground truth.

We compare the proposed solution Clipping via Crowd Intelli-gence ( CIC for short) with two baseline methods. The first one is the method used in the tool of Smart Print [9] ( SP for short). This tool analyzes the visual features of a given Web page and aims to select one clip for the main content as recommendation. This rec-ommendation usually covers a big area, inside which it may contain un-informative junks. Note that this is a method without the support of the wisdom of the crowds. The other baseline method is based on Maximal Frequent Itemset mining ( MFI for short) over the trans-action database for the clipping log data. Specifically, we can first generate all the maximal frequent itemsets and among them select the one with the largest number of items for recommendation.
We use leave-one-out cross validation to evaluate the accuracy of the methods on the data-sets. For the 20 Web pages from a Web site, we iteratively select one page as query and the log data on the remaining 19 Web pages are used to generate the transaction database for recommendation. The result is then compared against the ground truth of the query page.

A recommendation result actually refers to a set of content clips on the given Web page. Thus, we can evaluate its effectiveness by calculating the overlap area between the recommended clips and the ground truth. Note that each clip corresponds a bounding rect-angular (area) with four coordinates (i.e. x, y, height and width) after rendering in the browser and we can easily get those coor-dinates. See the evaluation example in Figure 6. The two blue rectangles refers to the two clips selected by a user, and the two green ones refers to the two clips recommended by our method-s. Thus, the Area O is their overlap area. Then, with the overlap area we can calculate the precision P , recall R and F 1 score of the recommendation in terms of area size. Specifically,
Where A G is the clipping region of ground truth, A R is the clip-ping region of the recommendation result and | . . . | denotes the region size. If the precision is less than 1, it means that we need to remove some areas from the recommendation. If the recall is less than 1, it indicates that we need to add some contents to get the exact clipping areas. Effectiveness . Table 2 records the average precision, recall and F1 score of the three methods on each Web site type. Specifically, we average the performance values over the 200 Web pages from each Web site type. In the recommendation we set  X  = 0 . 01 ,  X  = 0 . 1 and  X  = 2 . The average precision and recall over all the 2000 Web pages of CIC reach 92.46 % and 94.93 % respectively. Compared with the SP method, CIC increases 23.81 % in precision and 5.14 % in recall. This clearly shows that leveraging the wisdom of the crowds greatly improves the recommendation performance. Compared with the best MFI result (  X  = 0 . 1 ) in Table 3, CIC in-creases 1.34 % in recall and 0.42 % in precision. It indicates that CIC helps to find better patterns for recommendation. The reason why CIC can find better pattern has been detailed discussed in Sec-tion 3.3.
 Table 2: The average recommendation performance (%) of CI-C and SP on each Web site type (  X  = 0 . 01 ,  X  = 0 . 1 and  X  = 2 ) T able 3: The average recommendation performance (%) of M-FI and CIC with different  X  on the 2000 Web pages T able 4: Results of CIC ( % ) with different  X  (  X  = 0 . 01 and  X  = 0 . 1 )
Impact from  X  . The parameter  X  decides the trade-off between support and occupancy in the quality measure. Here, we also evalu-ate its impact on the recommendation performance. Table 4 shows the average performance over 2000 Web pages when  X  changes from 0.1 to +  X  .  X  = +  X  means that we only consider occupancy in the quality measure. Clearly, we can see when  X  increases from 0.1 to 2 both the precision and recall increase. When  X  continues to increase from 2, both the precision and the recall drop. Usually, too much emphasis on frequency will typically recommend infor-mative but uncomplete itemset, in other word, the recommend area R in Figure 6 is small and almost overlap with the overlap area O (Note that the ground truth area G is fixed). Thus, the precision is high and the recall is low when  X  is small. With the increase Figure 7: The effects of the number of transactions on MAFIA and our method of  X  from 0.1 to 2, both recommend area R and the overlap area O grow, leading to the large improvement on recall and the slight increase on precision. In contrast, too much emphasis on occupan-cy will recommend an over-complete itemset with too many items. However, in our application of Smart Print there are actually two kinds of items in the database X  X he positive items (the clips that users select) and the negative items (the clips that users de-select). When  X  is increasing from 2 to +  X  , the recommendation will in-clude many such negative items, leading to the decrease of overlap area O . Thus, both the precision and the recall drop. For our 2000 Web page data set,  X  = 2 achieves the best result. In practice,  X  can be chosen by cross validation to achieve the best performance in similar manners.

Efficiency . To show the efficiency of the proposed pruning meth-ods in Section 4 we compare its running time with the straightfor-ward method to our problem on the synthetic transaction databases with different numbers of transactions. The data sets used in the experiment are generated by the IBM synthetic data generator itemset patterns. Here, the straightforward method is simply fre-quent pattern mining with the check on occupancy. The implemen-tation of MAFIA [2], a highly efficient method for finding frequent patterns, is adopted in this comparison. Our method leverages the properties in Section 4 to further prune the search space, thus may achieve better efficiency.

Here, we generate a database of N = 10000 transactions and s-cale it up by vertical concatenation of the database. The results are shown in Figure 7, including the running time and the number of nodes in the subset tree searched by the baseline and our method. Note that in this experiments we only duplicate the database to in-crease its size. Thus, the number of nodes visited in the subset tree is expected to stay unchanged. As can be seen from Figure 7(a), our method only searches about 1 . 4% nodes of those searched by the baseline. With respect to running time, our algorithm X  X  run-ning time grows linearly, from 2 . 45 seconds for 10000 transactions to 10 . 34 seconds for 50000 transactions. What is interesting is that the baseline X  X  running time remains stable when the number of transactions grows. After careful investigation, we think it is due to the extreme efficiency of bit operators used extensively by MAFIA. However, since it takes the baseline about 195 seconds to run the experiment for each data set, our algorithm is still much faster. In this work we harness the wisdom of the crowds for accurate Web page clipping. The crowd intelligence comes from the log data on how previous users clip Web pages. We formulate this problem as a new pattern mining task, mining top-1 qualified pattern , which considers both the support and occupancy in pattern recommen-dation. Along this line, we propose the properties on occupancy which helps to greatly prune the search space for high efficiency. In addition, we propose the generalized DOM tree paths to identify http://www .cs.rpi.edu/ zaki/www-new/pmwiki.php/Software content clips in Web pages, and show its robustness to the change of DOM tree structures. Compared with previous works our method significantly improves the accuracy in Web page clipping, which is shown by the experimental study on the 2000 Web pages.
 It is worth mentioning that the proposed problem of mining top-K qualified pattern can be useful to other pattern mining applica-tions where each transaction corresponds to a set of actions for a task. Also, we can extend this concept of occupancy in sequential mining.
The authors Lei Zhang, Enhong Chen and Guiquan Liu are sup-ported by grants from Natural Science Foundation of China (Grant No. 60775037), The National Major Special Science &amp; Technol-ogy Projects (Grant No. 2011ZX04016-071), The HeGaoJi Na-tional Major Special Science &amp; Technology Projects (Grant No. 2012ZX01029001-002) and Research Fund for the Doctoral Pro-gram of Higher Education of China (20093402110017).
