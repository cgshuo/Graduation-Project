 We introduce and explore the concept of an individual X  X  rel-evance threshold as a way of reconciling differences in out-comes between batch and user experiments.
 H.4 [ Information Storage and Retrieval ]: Miscellaneous Performance, Design, Experimentation, Human Factors Search engines, information retrieval evaluation, user st udy
Information retrieval (IR) experiments based on the Cran-field methodology measure system performance using a batch of queries and a test collection that has a subset of the doc-uments judged as relevant or irrelevant by human judges for each query. The utility of a system is then computed using a metric that aggregates the relevance judgements for docu-ments in ranked lists returned by the system for each query. For example, many papers report IR system comparisons us-ing the TREC document collections, topics and judgements, using Mean Average Precision (MAP) as the metric [9].
An alternate way to evaluate systems is to take a group of human users and ask them to perform search tasks with the various systems, comparing outcome measures such as the time to complete a task, success or failure on a task, or sub-jective measures like user satisfaction. Previous studies [1, 2, 5, 6, 7, 8] have shown that attempting to transfer results from batch experiments into laboratory based user studies is difficult. That is, the systems rated as superior in batch experiments are unlikely to assist users in performing thei r tasks more quickly or more accurately than the systems that are rated poorly in the batch experiments.

There are many possible causes for this seeming mismatch between batch and user-based experimental outcomes. In this paper we introduce and test the idea of a mismatch in relevance threshold between the judges used to gather the batch data, and the users on which the systems are trialled.  X  Supported in part by the Australian Research Council.
Participants were recruited from our university, and ex-periments were carried out in accordance with the guideline s of the RMIT University Human Research Ethics Committee. Using topics and documents from the TREC GOV2 collec-tion [3], relevance assessments were made for each document by two subjects on a four-point categorical scale: complete ly relevant(3), highly relevant(2), marginally relevant(1) or not relevant(0), giving two batch judgements for each document .
Using a similar framework to our previous studies [8], we constructed ranked lists using the known relevance levels o f documents to achieve a given level of P@1. This resulted in eight search systems (sets of lists) as summarized in Ta-ble 1. Subjects were presented with information needs based on the TREC topics, and asked to find documents that help to resolve the need. We then measured the amount of time that a user needs to find a relevant document for an infor-mation need. For each document that a user viewed in a search results list, they could choose to save the document (indicating that it is relevant), or not save it (not relevan t).
A relevance threshold is the point on the relevance scale where a user saves half of the documents that they have read. We attempted to measure the relevance threshold of individual users while they undertook the search task by ex-amining the number of documents of each relevance level that each user read and then did, or did not, save. In this part of the experiment, we assumed that the true relevance level of a document was the ceiling of the average of the two batch judgments that had been made on that document. Using techniques from psychophysics, we fit a Weibull psy-chometric function to the data of each user, and used the 50% point as the threshold from these curves [4].
The aim of our experiment is to explore the effects of a mismatch in relevance threshold between judges and users by altering the relevance thresholds used in the batch ex-periments. In particular, we evaluate systems U x , where category one (marginally relevant) documents are consid-ered relevant (this is the default assumption in most TREC experiments); systems V x , where category one documents are considered irrelevant; and systems W x where only cat-egory three (completely relevant) documents are considere d relevant. If users are not using the same relevance criteria as were used in the batch judgements, then we would expect differences in systems that appear in the batch experiments to not be reflected in the user experiments.

Differences in time to find relevant documents using dif-ferent systems is shown by the circles in Figure 1. Using System P@1 Determination of relevance (4-pt scale) Table 1: Systems used (mapping of 4-pt relevance scale to P@1): U x has a strict irrelevance criterion, W x a strict relevance criterion, and V x a mix of the two.
 Figure 1: Mean time taken to save the first doc-ument using pairs of inferior and superior systems. Circles include all users; squares are a subset of users decided by relevance threshold (see text). batch judgements where category one (marginally relevant) documents are considered relevant (U x ) shows a significant difference in the time that users need to find a relevant doc-ument when using a system with different levels of P@1 ( t -test, p = 0 . 0101). When category one documents are con-sidered irrelevant (V x ), the gap between the systems from a user perspective is reduced (circles are closer together i n the V x panel than in the U x panel), but the difference is still significant ( p = 0 . 0140). When the batch judgments insist that only completely relevant documents are consid-ered as relevant (systems W x ), then users do not notice a difference between the two systems ( p &gt; 0 . 05). Therefore, we expect that there may be a mismatch between the rele-vance threshold of judges for the W x systems and our user population.
A user X  X  relevance threshold should be less than one if their behaviour is to match that used in the batch experi-ments that assessed systems U0 and U1 as the inferior and superior systems. That is, if a user read a category one doc-ument (marginally relevant), there should be a better than even (50%) chance that the user would save that document as relevant, since category one documents were considered relevant in the batch experiments. Our users have different relevance thresholds; if we were to exclude those users from the data that have a threshold lower than one, and reanalyze the time taken until the first document is saved, we would expect the system U0 to perform more poorly (time to save User 1 2 3 4 5 6 7 8 9 10 11 Thresh. 0.0 0.1 0.1 0.2 1.2 1.4 1.5 1.8 1.8 1.9 2.1 increases), and the time taken to save a document with sys-tem U1 to decrease. Similarly, for the batch experiments that evaluated system V1 as superior to system V0, it was assumed category one (marginally relevant) documents were irrelevant, and so users should have a threshold between 1 and 2 if they are to match the judges.
 Table 2 shows the individual user relevance thresholds. The first four users all have a threshold below one; that is, there is a more than even chance that they would categorize a level one document (marginally relevant) as relevant. The remaining seven users, however, all have a threshold greate r than one, indicating that there is less than a 50% chance that they would save a category one document.

If we exclude those seven users (5 to 11) who have a rel-evance threshold mismatch, and re-evaluate the time taken to save documents using systems U0 and U1, then we get the mean time shown by the square in the U1 section of Figure 1. It is clear that the mean time to save with U1 went down due to the exclusion of users with a threshold greater than one. Unfortunately, there was not enough data to conclude that mean time with U0 went up.

Re-evaluating Systems V0 and V1 using users with thresh-olds between 1.5 and 2.5, thus choosing the users whose relevance thresholds match the judges used in the batch ex-periment that says V1 is better than V0, we see that the gap between V0 and V1 widens (squares compared to circles in the V x panel), as expected. However, the difference between time is no longer statistically significant ( p &gt; 0 . 05), probably due to the small number of observations. Thus, when rele-vance thresholds match, batch differences are more clearly reflected in the user experience.

There were other sources of mismatch that were explored in this study, but space prohibits their discussion in this abstract.
 We thank Justin Zobel and Steve Garcia for valuable dis-cussions.
