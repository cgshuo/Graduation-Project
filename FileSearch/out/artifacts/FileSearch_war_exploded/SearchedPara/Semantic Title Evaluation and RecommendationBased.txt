 Text mining techniques have been sought after in order to make informed de-cisions efficiently based on tremendous textural information [1]. For a lot of documents, a good title, which gives a con cise and semantic representation of contents in main text, often provides a shortcut for readers to digest docu-ments. However, due to various reasons like lexical ambiguity (say, polysemy), eye-catching intention or being prepar ed by an inexperienced writer, a lot of documents come with titles whose semantics are away from their main texts. For example,  X  X earning to fly X  can be a title of a book for flight training or an autobiography for Victoria Beckham. These motivate us to develop automatic techniques to evaluate to what degree a title captures the main contents of its associated document. As a byproduct, our title evaluation techniques can be used to recommend a title-worthy sentence from which a quality title could be generated.

Two issues hamper adjusting traditional document summarisation evaluation techniques including ROUGE for title evaluation. To evaluate a title, these tech-niques require a, normally human generat ed, reference summary [12]. In addition, the evaluation is mainly based on whether words in a title appear or not in the reference summary. It is often not enough, especially considering polysemy and synonymy. To overcome these two issues, we will propose to compute semantic similarity in a space spanned by latent topics learnt by topic models, and then to use a statistical hypothesis test to c heck or classify whether a title is poor.
Our title evaluation and recommendation techniques are relevant to ex-tractive text summarisation. Extractive summarisation only chooses informa-tion (words/sentences) from documents to compose concise representation for them [13]. There are mainly two types of approaches [13]. One type of ap-proaches first derive an intermediate representation like topic words, TF*IDF, Latent Semantic Analysis (LSA), and Bay esian topic models, for documents that captures the contents in main text. Se ntences are then scored for importance. Because of their modelling generalisability to unseen documents and short tex-tual units [2, 8, 9], we choose topic models to learn a topic representation of a document and its sentences/title. In this way, not only can one handle polysemy and synonymy [5], but also make a title, sentences, a document directly com-parable. We will develop and compare two semantic title evaluation techniques based on either a recent Segmented To pic Model (STM) [9] or the standard Latent Dirichlet Allocation (LDA) [2].

In the second type of summarisation approaches, indicator representation ap-proaches, the text is represented by a diverse set of possible importance in-dicators that do not aim at discovering topicality [13]. These indicators are combined, using graph-based ranking methods, say PageRank [10] or machine learning techniques, say classification [6], to score the importance of each sen-tence. Different from Bayesian topic models, these approaches normally require extra information [10, 13, 14, 15], such as costly training data [6, 14], Word-Net [6], Wikipedia [14, 15], or search query logs [14]. Our experiments show our title recommendation techniques can g ive very short summaries with quality comparable with the winners of DUC X 04, including [6, 10].

The basic procedure of our Semantic Title Evaluation and Recommendation (STER) techniques is as follows. (1) We use topic models to generate latent top-ics, each of which is a probability distribution over words, from documents as well as sentences/titles in them. Based on two topic models STM and LDA, we have STERSTM and STERLDA techniques respectively. (2) Each docu-ment/sentence/title is represented as a mixture of latent topics. (3) Semantic similarity values are calculated between documents and its sentences/title based on their topic distributions. (4) Being compared with other sentences in a doc-ument, a title with a statistically significantly low similarity is regarded as un-favourable. The top similar sentence is recommended as a title worthy sentence.
In Section 2, we first brief Bayesian topic modelling techniques. Section 3 presents STERSTM and STERLDA. Experimental results of STERSTM, and comparison with STERLDA and the methods participated in DUC X 04 are re-ported in Section 4, followed by c oncluding comments in Section 5. In order to estimate semantic coverage of a title, we need to compute semantic similarity between a title and its whole document in the same latent topic space. Because of better generalisation capability to unseen documents and short tex-tual units than LSA and its variants [9], we learn this topic space using Bayesian topic models that specify a probabilistic process by which text documents can be generated.

The canonical topic model, LDA [2], is a latent variable model of documents, where a document is regarded as a mixture of K latent topics, each of which is a probability distribution over words. Following [9], documents are indexed by i ( i =1 ,  X  X  X  ,I ), and words w are observed data, each is indexed by l (( l = 1 ,  X  X  X  ,L )). The latent variables are  X  for a document) and z (the topic assignments for observed words), and the model parameter of  X  k  X  X  ( per-topic word distributions ). This generative model, as illustrated in Fig. 1(a), is as follows:  X  z i,l  X  Multinomial K (  X  i )  X  i,l ; w i,l  X  Multinomial W  X  z i,l  X  i,l. Dirichlet K (  X  )isa K -dimensional Dirichlet distribution, and W is the number of different words. The h yper-parameters  X  and  X  are Dirichlet priors for word and topic distributions respectively.

Since LDA was introduced, topic models have been widely extended in the text mining community (see [5, 8] and references therein). Topic models have been successfully used in document summarisation [13], opinion mining [16], sequential topic evolution [8, 7], etc. Via l everaging hierarchic al structure within a document, such as a document consist ing of sentences (Fig. 1(b)), STM can generate much more accurate topics than LDA and its variants [9]. In addition, it models a document and its sentences in the same topic space, which is required by our semantic title evaluation. In fact, in STM, topic proportions of sentences distribute around the topic proportion of the whole document, as it is described by a Poisson-Dirichlet Process (PDP). Conditioned on the model parameters  X  ,  X  ,  X  and PDP parameters a,b (called discount and strength respectively, 0  X  a&lt; 1, b&gt;  X  a ), STM that we used in this paper assumes the following generative process (graphical view see Fig. 1(c)): 1. For each document documents D i ( i  X  X  1 ,  X  X  X  ,I } ), draw a document topic 2. For each sentence S i,j ( j  X  X  1 ,  X  X  X  ,J i } ) The procedure of our semantic title evaluation methods is given as follows. It first represents a document, its sentences and title using the same set of latent topics learned by a topic model. The semantic similarity between a title/sentence and the document is computed based on their topic proportion (i.e., distributions) vectors. Via comparing the title X  X  similarity value with those of sentences in main text, we use a hypothesis test to compute p-Value to check how semantically good a title is. As a byproduct, p-Values for those sentences can also be used to recommend a top one for a title candidate , from which a title can be generated quickly.

Algorithm 1 outlines our Semantic Title Evaluation and Recommendation method based on STM (STERSTM). In the preprocessing step (Step 1), a doc-ument is first split into its constituent sentences by a Perl programme (Lin-gua:en:sentence package) [3] based on a regular expression and a list of abbre-viations. Hereinafter, a title is treated as a separate sentence for the sake of Algorithm 1 STERSTM simplicity. Sentences are then split into words. After that, all stop-words, ex-tremely frequent ( e.g. , top 30 in our experiments) words, and least frequent ( e.g. , less than 5 times) words are removed. We do not stem words in order to keep post-processed senten ces with an acceptable length.

After having the word list w i,j,l for each sentence S i,j in document D i ,we run the efficient collapsed Gibbs sampling a lgorithm [9] to estimate parameters in STM (Step 2). In Step 3, with a sufficient number of samples being drawn from the converged Markov chain for STM, topic distributions of documents and sentences can be estimated by a fixe d point estimation with inverting the generative process in Section 2.

Step 5 calculates the semantic similarity between a document and its sentences using their topic proportion vectors  X  i and  X  i,j . The widely used cosine similarity measures similarity between two vectors by calculating the cosine of the angle between them: Because a topic proportion vector also indicates a multinomial distribution, we can also use the Hellinger distance or Kullback-Leibler divergence, which quan-tify the similarity between two probability distributions [13, 7]. As our prelim-inary title evaluation experimental results show there is little difference among these similarity metrics, we will only present results for the cosine similarity. Ex-amples of cosine similarities of senten ces within two patents and one conference paper could be found in Figs. 2(a) and 3.

Before introducing Steps 6-8, we show that it is not easy to specify a constant threshold for semantic similarities for determining a favourable title through ex-amples in Fig. 3. Similarities for differen t documents have different value ranges. Comparing with other sentences in a document, 0.95 is reasonably good for the patent X  X  title while just average for the paper X  X  title in Fig. 3(a). Similarly, because the numbers of sentences in a document can range from a few dozens to several thousands, it is difficult to specify a threshold for rank based on similarity or rel-ative rank (e.g., rank of title divided by the total number of sentences within a document). Rank 34th is possibly favourable for a title within a very long docu-ment, but doubtable for a short one in Fig. 3(a). A lot of sentences arguably have high semantic similarity with a document. A small change on the title X  X  similarity value may lead to a big change on its rank as well as its relative rank. We give a statistical mechanism to speci fy document-specific  X  X hresholds X , as Generalised Extreme Value (GEV) distribution is able to fit well these similarity values in Figs. 2(a) and 3. In the extreme value theorem, the GEV distribution is a limited distribution of properly normalized minima of a sequence of inde-pendent and identically distributed random variables [4]. It is a family of contin-uous probability distributions, and it is a general distribution family, including Weibull and Gumbel distribution families. The GEV distribution we used has a cumulative distribution function: scale parameter and  X  3  X  R the shape parameter.

In Step 6, parameter  X  of the GEV distribution are estimated via maximising likelihood of all the similarity values within the same document. The parameter estimation can be visually validated via such as probability plot, quantile (Q-Q) plot, density plot or return level plot [4]. Diagnosis plots for a fitted GEV distribution for similarity values in Fig. 2(a) are exemplified in Fig. 2(b).
Step 7 in Algorithm 1 computes p-Values of all the sentences within a docu-ment. The p-Values for a sentence/title here can be used for fulfilling a statistical hypothesis test. The p-Value is the probability of the similarity observation un-der the null hypothesis (H0) which hypothesises that its similarity value based on topics is not extreme in comparison wi th counterpart sentences. We can reject the null hypothesis if and only if the p-Value is less than the significance level threshold. We will use a conservative threshold, say, 10% in this work. Therefore, if the p-Value for a title is less than the threshold, we reject the null hypothe-sis and draw a statistically sound conclusion that the title is not semantically good enough (in comparison with other sentences in the associated document). In other words, we can categorise such a title as  X  X nfavourable X . Our experiment results, some presented in Section 4.2, s how that the sentences with large seman-tic similarity values can summarise the whole document excellently. As a matter of factor, STERSTM is very close to the runner-up of Task 1 (summarising an English document into a very short summary) in the Document Understanding Conference (DUC) in 2004 [6]. Thus, we may categorise a title as  X  X xcellent X  if its p-Value is larger than 90%. Other titles, with moderate p-Value ranging from 0.10 to 0.90, will be categorised as  X  X verage. X  Step 8 conducts this categorisation. It also sorts sentences of a document based on their p-Values (equivalently, their semantic similarities). Finally, the p-Values of titles generated by STERSTM can evaluate titles in a statistically sound way without a reference summary. The top sentences with highest p-Value from STERSTM can be recommended as the title-worthy sentence or a title candidate for the document.

Steps of Algorithm 1 could be independently replaced with other techniques to develop new methods. For example, Ste p 5 can be replaced with other sentence scoring techniques [10, 13]. Steps 2 and 3 can be replaced with a modelling technique as soon as it can represent documents and sentences in the same semantic space. When steps 2 and 3 ar e replaced with LDA, we call the new method STERLDA. LDA does not consider document structure as STM does. In order to derive topic distributions for both documents and their sentences, we need to run LDA twice, one on the document level, another on the sentence level. However, these two LDAs will come up with two different sets of latent topics due to unsupervised learning. To tackle this problem, the topics generated on the document level are used and fixed in training LDA on the sentence level. STERSTM can run on a single document, while STERLDA cannot. To facilitate a fair comparison, we ran both of them on a set of documents. We set the number of topics K = 50, and priors  X  =0 . 05 and  X  =0 . 01 for both STM and LDA, and a =0 . 02 and b = 10 for STM in our experiments in this paper. 4.1 Semantic Title Evaluation Experiments We used two sets of documents for title ev aluation experiments. One is Patents-99, where 99 U.S. patents were randomly selected from 5000 U.S. patents 1 granted between Jan. and Mar. 2009 under t he class  X  X omputing; calculating; counting X  with international patent classification (IPC) code G06. After prepro-cessing, the numbers of post-processed s entences in these patents range from 60 to 2163. The second data set is NIPS-100, in which 100 papers were randomly selected from NIPS conference papers in 2004. These papers contain a lot of equations, which make the preprocessing step harder. The numbers of sentences range from 68 to 207.

As we discussed in Section 3, p-Value from a GEV distribution can give us more informative evaluation than ranks etc. When the similarity value of a title has a high rank, it often has a low p-Value. Though the rank and the p-Value are negatively correlated, p-Value takes into account of similarity values of other sen-tences within the same document, and becomes more informative. For example, for NIPS paper 579, its title  X  Validity estimates for loopy Belief Propagation on binary real-world networks  X  has the semantic similarity of 0.921, and is ranked only 116th in comparison with the 131 sentences from the paper, and looks really unfavourable. However, p-Value of 0.417 does not provide evidence statistically significantly to claim this title is unfavourable. Another similar example could be found in Fig. 3(b).

Fig. 3 illustrates semantic similarities between sentences/title and a whole document based on topics learned by STM. For Paper 642 from NIPS X 04, the title  X  Integrating Topics and Syntax  X  has the similarity value of 0.9993, and it is ranked 34th in comparison with 155 sentences from the paper. Its p-Value from the GEV distribution is 0.970. That means this is an excellent title. From Fig. 3(b), we can see the title  X  Web page performance scoring  X  has the semantic similarity of 0.9247. It is ranked as 388th in comparison with other 650 sentences from the patent. Its p-Value is 0.319, which says the title is not excellent from the viewpoint of covering the whole patent semantically. From its abstract 2 ,we can see it could be improved if some word related with  X  tool  X  X r X  browser-based tool  X  is appended to the title. As another evidence, the top semantically similar sentence chosen by STERSTM is  X  More particularly, the invention relates to a tool which analyses the content and structure of Web pages in real time and produces statistics and a performance score . X  Fig. 4(a) illustrates the semantic sim ilarity values of titles from NIPS-100. The 100 similarity values of these titles generated by STERSTM range from 0.86 to almost 1. They are normally quite high. The similarities by STERLDA range from almost 0 to 0.996 and have a broader value range. It seems that STERLDA generates less reliable evaluation than STERSTM in terms of sim-ilarity values. For this document set, according to STERSTM, 45 out of 100 papers have excellent titles, including the one in Fig. 3(a). STERSTM doesn X  X  find any unfavourable title, which is not surprised as all the papers were pre-pared by experienced researchers. STERLDA surprisingly finds 11 unfavourable titles, and only 23 excellent ones as summarised in Table 1. For example, the title  X  Methods Towards Invasive Human Brain Computer Interfaces  X  X fPaper 443 in NIPS X 04 has the p-Value of 0.058 and is inappropriately regarded as unfavourable by STERLDA.
 Fig. 4(b) gives the p-Values of these titles within the document set Patents-99. The p-Values of the 99 titles based on STERSTM range from 0.22 to very close to 1. STERSTM finds 45 excellent titles, and it does not find any unfavourable patent titles, as we would expect. In comparison, p-Values from STERLDA range from 0 to close to 1. It finds only 23 excellent titles and 11 unfavourable titles. Thus, STERSTM can evaluate titles more reliably than STERLDA based on the two document sets. 4.2 Semantic Title Recommendation Experimental Results In this section, we empirically check w hether our proposed techniques can rec-ommend a title worthy sentence from the viewpoint of capturing the main idea of a document [11]. Due to limitation of space, we only report results on one set of documents, DUC-2004. DUC-2004 is the benchmarks used for Task 1 (generating a very short summary from a document) in NIST X  X  DUC X 04 3 . The corpus con-sists of 50 sets of documents each contai ns 10 same topic documents on average. The documents came from the AP newspap ers and New York Times newspapers. The short summary generated is peer summary and it is automatically evacu-ated by one of widely used document su mmarisation metrics, Recall-Oriented Understudy for Gisting Evaluation (ROUGE) [12]. ROUGE essentially calcu-lates n-gram overlaps between given summaries and previously-written human summaries. A high level of overlap should indicate a high level of shared concepts between the two summaries. There are f our reference summary (or model sum-mary) per document in DUC-2004. ROUGE can evaluate a short given summary by comparing it with up to four reference summaries.

We report evaluation results based on ROUGE-1, i.e., checking unigram over-lap between a given summary and a refere nce summary, partially because both STM and LDA are trained with unigrams. In particular, we use F-measure, which is a weighted harmonic mean of recall and precision.
 where the recall is the proportion of wor ds in the reference summary appearing in the given sentence, and precision is the p roportion of words in the given sentence appearing in the reference summary. Both precision and recall are based on an understanding and measure of relevance. An F-measure score reaches its best value at 1 and worst score at 0.

As we mentioned in Section 3, to facilitate fair comparison, a sentence was trimmed (removing duplicate words, frequent words, and semantically less impor-tant words which are not in top 100 word lists of in topic-word distributions) as ROUGE truncates summaries longer than the target length of 75 bytes (alphanu-merics, whitespace, and punctuation included) before evaluation for DUC-2004.
For this corpus, the average recall, precision and F-measure of STERSTM are 0.218, 0.250, and 0.232, respectively. For STERLDA, they are 0.182, 0.160, and 0.169, respectively. STERSTM obviously outperforms STERLDA. In com-parison with 40 participation methods in the DUC X 04 conference, STERSTM did quite well in terms of all the three measures. It is ranked as 7th, 9th and 5th in terms of average recall (Fig. 5(a)), precision, and F-measure (Fig. 5(b)). One DUC X 04 participation method [6] that requires training data and WordNet, has F-measure of 0.234, which is the runne r-up in Fig. 5(b). Its average recall is 0.217, quite close to that of STERSTM. The another graph-based document summarisation technique, the winner of several tasks in DUC X 04, LexRank [10] has F-measure of 0.208 for this task and is 13th in Fig. 5(b). Thus, in terms of quality of very short summaries generated for DUC-2004, STERSTM is compa-rable with the top methods participated in the DUC X 04 conference. Based on a recent topic modelling technique, Segmented Topic Model (STM), this work has presented one Semantic Title Evaluation and Recommendation (STER) technique, STERSTM. Through comparing title/sentences with the whole document in the topic space cr eated by STM, STERSTM computes the semantic similarity of title/sentences, w hich can estimate the semantic coverage of a title/sentence. Via fitting a Gener alised Extreme Value (GEV) distribu-tion over the similarity values of sent ences and a title within a document and calculating p-Value under the distribution, STERSTM is able to identify excel-lent and unfavourable titles without extra information like a human generated reference summary. The sentence with top p-Value is recommended as a title candidate. Experimental results on seve ral different document sets have shown STERSTM can pick up some improvable titles, statistically significantly outper-form STERLDA, a counterpart based on the canonical topic model LDA, and generate very short summaries with quality comparable with various document summarisation techniques.

There are several possible extensions of this work. Better trimming techniques to shorten a sentence to a concise and rea dable title could improve title recom-mendation [11]. It is appealing to explore more reliable statistical distributions for semantic similarity values, especially for those for small documents. We are also going to extend the proposed techniques for multiple relevant documents, embedding key words or other meta data.
 Acknowledgements. Huidong Jin was financially supported by the Environ-mental and Agricultural Informatics program, CSIRO Mathematics, Informatics and Statistics. Lan Du was supported under the Australian Research Coun-cil X  X  Discovery Projects funding sc heme (project numbers DP110102506 and DP110102593). The authors are grateful to the anonymous reviewers and Dr Peter Caley for their constructive comments.

