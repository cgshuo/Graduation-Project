 Recently it has been realized that one strengt h of the kernel-based learning paradigm is its ability to support non-vectorial input spaces, [2, 3, 4, 5]. This is mainly due to the fact that the proper definition of a kernel function enables the structured data to be embedded in some linear feature space without the explicit computation of the feature map. As a result any propositional algorith m which is based on inner products can be applied on the structured data.

In [6] we made one step in the direction of bringing kernel methods and learning from structured data together and we proposed a novel and general framework based on concepts of relational algebra for kernel-based learning over relational schema. We defined kernel functions over relati onal schema which are instances of -Convolution kernels and use them as a basis for a relational instance-based learning algorithm. One of the main limitations of relational algebra representation as described in [6], is that, although it is ideal for modeling sets, it can not naturally model lists. To tackle this problem we recently proposed in [1] an exten sion to this representation language in such a way that it allows for modeling of lists of complex objects (relational instances). This new representation was used within th e framework of distance-based learning and for the task of classification of protein fingerprints promising results were reported.
In this paper we propose new kernels over extended relational algebra language which operate directly on the structures defined in [1]. This amounts to defining new kernels on lists of relational instances. We report experimental results on a problem of classification of protein fingerprints for which we were able to improve the best accu-racy reported in the literature. Consider a general relational schema that consists of a set of relations R = { R 1 ,...,R n } . The schema of a relation R i is the set of attributes of R i andwedenoteitas R i ( A 1 ,..., A ( v j 1 ,v j 2 , ...,v jz i ) and v jl the value of the A l attribute in the R i j tuple. An attribute A k is called a potential key of relation R i if it assumes a unique value for each in-stance of the relation. An attribute A l of relation R j is a foreign key if it references a potential key A k of relation R i and takes values in the domain of attribute A k in which case we will also call the A k a referenced key .A set link is a quadruple of the form sl ( R i ,A k ,R j ,A l ) where either A l is a foreign key of R j referencing a potential key A k of R i or vice versa. To be able to represent lists we define a list link as a quin-tuple ll ( R i ,A k ,R j ,A l ,LIST ( A l )) where R i , A k , R j , A l are defined as before and LIST ( A l ) is a list of values from D ( A l ) defining the order of the elements of the list. The association between A k and A l encoded in sl and ll models one-to-many relations: for sl one element of R i can be associated with a set of elements of R j whereas for ll one element of R i is connected with a list of elements from R j .

We will call the set of attributes of a relation R i that are not keys (i.e. referenced keys, foreign keys or attributes defined as keys but not referenced) standard attributes and denote it with I A,R i . The notion of links is critical for our relational learner since it will provide the basis for the new types of attributes, i.e. set and list.
For a given referenced key A k of relation R i we denote by SL ( R i ,A k ) the set of links sl ( R i ,A k ,R j ,A l ) in which A k is referenced by foreign key A l of R j . By analogy  X  SL ( R i ,A k ) we denote the set of all set links in which one of the potential keys of R i is referenced as a foreign key by an attribute of another relation. By LL ( R i )=  X  LL ( R i ,A k ) we denote the set of all list links in which one of the potential keys of R i is referenced as a foreign key by an attribute of another relation.

Similarly for a given foreign key A l of R j , SL  X  1 ( R j ,A l ) will return the standard link sl ( R i ,A k ,R j ,A l ) where A k is a potential key of R i referenced by the foreign key A l of R j . By analogy we define LL  X  1 ( R j ,A l ) .If R j has more than one foreign keys then by SL  X  1 ( R j )=  X  l SL  X  1 ( R j ,A l ) we denote the set of all set links of R j defined by the foreign keys of R j .By LL  X  1 ( R j )=  X  l LL  X  1 ( R j ,A l ) we define the set of all list links of R j defined by the foreign keys of R j .

To define a classification problem one of the relations in R should be defined as the main relation , M . , i.e. the relation on which the classification problem will be defined. Each instance, M i j ,ofthe M relation will give rise to one relational instance , M + i j , i.e. an instance that spans the different relations in R . More precisely M + i j will have the same set of standard attributes I A,M and the same values for these attributes as M i j has and each link sl  X  SL ( M )  X  SL  X  1 ( M ) ( ll  X  LL ( M )  X  LL  X  1 ( M ) ) adds in M i j one attribute of type set (list). The value of an attribute of type set (list) is defined basedonthelink sl (or ll ) and it will be the set (list) of instances with which M i j is associated in some relation R j when we follow the link sl ( ll ). We will denote I sl,R i and I ll,R i the set of attributes of type set and list, respectively. By recursive application of this procedure we obtain the complete description of the relational instance M + i j . More details on relational algebra representation can be found in [6, 1]. In this Section we will introduce a new class of kernels over extended relational alge-bra structures which are instances of the -Convolution kernels, [2], and are based on kernels introduced in [6].

In order to define these kernels we recal l from Section 2 that a given relation R i is divided into three parts: I A,R i , I sl,R i , I ll,R i , which denote set of standard attributes, set of attributes of type set and set of attributes of type list, respectively. A relational in-is the vector of standard attributes and v a I sl,R ( v l 1 ,...,v l |I ll,R i | ) are vectors of attributes of type set and list.

Given this formalism we defined the Direct Sum Kernel on the set X (if |I A,R i | =0 ) as I
A,R i of the standard attributes of R i and K set ( ., . ) and K list ( ., . ) are kernels between sets or lists, respectively. Here we use a normalized version of k  X  (if |I A,R i | =0 ): K to relational instances can be found in [6].

In the next Section we will define kernels over sets ( K set ( ., . ) ) and kernel over lists (
K Kernels over Lists and Sets. Before defining kernels over lists we first introduce some helpful notation. Lets denote by i a sequence i 1  X  i 2  X  X  X  X  X  X  i n of indices; we say that i  X  i if i is one of the sequence indices. We denote with l ( i ) the length of a sequence i . For a given attribute, v al of type list v al [ i k ] is its k element.
 Kernel ,[5],andwhere v al and v bl are attributes of type list, the subsequences i and j are assumed to be contiguous and 0 &lt; X &lt; 1 is a parameter penalizing longer subse-quences. A slightly more general kernel is proved in [5] to be a valid kernel, which is computable in O ( mn ) where m and n are the lengths of the lists v al and v bl respec-tively.

The other kernel on lists we experimented with is a specialized version of the kernel over basic terms from [3] which we will call the Longest Common Sublist Kernel .This the same length and 0 otherwise. This kernel in more general settings is proved to be a valid kernel in [3].

The above kernels are normalized such that the examples in the corresponding fea-ture space have a unit norm, i.e. K list ( x, y ):= K list ( x,y )  X  K
The Contiguous Subtree Kernel and Longest Common Sublist Kernel are related to each other in the sense that they apply to the same kind of data. However the underlying notion of similarity of the Contiguous List Kernel and the kernel from [3] is different. In the former the overall similarity is measured by sum of the mutual similarities of all the (consecutive) sublists of the same length. The similarity between the sublists are computed by means of other kernels. On th e other hand the kernel from [3] only takes the longest common contiguous sublist at the start of the two sublists into account. In that sense the former takes a more  X  X lobal X  view.

For attributes of type set we use the Averaged Cross Product Kernel ,[6]. We checked the performance of the SVM algor ithm, [7], on a protein fingerprint clas-sification problem. Protein fingerprints are groups of conserved motifs (regions) drawn from multiple sequences alignment that can be used as diagnostic signatures to identify and characterize collections of protein sequences, [8]. Broadly speaking, fingerprints may be diagnostic for a gene family or superfamily (united by a common function), or a domain family (united by a common structural motif). Fingerprints can be de-scribed by its component motifs and protein sequences, we are therefore confronted with a multirelational learning problem. Our approach will be different from the one presented in [8] since there the task represe ntation is propositionalized by aggregating protein and motif character istic over a fingerprint.

We modeled this data in the following way: the  X  X ingerprints X  (main) relation with global characteristics of the instances is associated through an one-to-many relation with the  X  X otifs X  relation. Additionally there is a number of relations with aggregated information about proteins (actually proteins IDs) associated with the main relation using one-to-one relations. Fingerprints are globally characterized by (among others) number of proteins and proportion of protein sequences that match all or only a part of the motifs in a fingerprint. Individual motifs are characterized by a number of amino acids and protein sequences, coverage (the fraction of protein sequences in the finger-print that match the motif) and a number of features measuring motif X  X  conservation. The last source of information are protei n sequences and more precisely their SWISS-PROT/TrEMBL labels. Features computed on the bas is of these labels can be considered as statistics computed on the set of proteins and hence they can be stored in the  X  X inger-prints X  relation. However, keeping them in s eparate relations provides us a way to treat missing values which is the case here since not all proteins have a SWISS-PROT entry. More information about different attributes in different relations can be found in [8].
We also defined three different data representation based on combinations of the two different types of link associations defined in Section 2. In the first two approaches we assumed that each instance from the  X  X inge rprints X  relation is associated with a set and list of instances from the  X  X otifs X  table, resp ectively. The latter approach assumes that the order in which motifs appear along the sequence of amino acids is important. with a number of conserved regions so there exists an intrinsic notion of order (along protein sequences). In the third approach we combine the two previous representations: an instance from the  X  X ingerprints X  relatio n is associated both with the set and list of instances from the  X  X otifs X  table.

We follow the experimental procedure reported in [8] where 1842 fingerprints records from version 37 of the PRINTS database were split into the design and testing set. In all experiments we limited ourselves to normalized polynomial k P p,a ( ., . ) (where p =2 ,a =1 ) and Gaussian RBF k G attributes of type lists we use the Contiguous Sublist Kernel (  X  =0 . 5 )and Longest Com-mon Sublist Kernel defined in Section 3. We also explored the behavior the SVM with the C =1 complexity parameter. We estimate accu racy using ten-fold cross-validation and control for the statistical significance of observed differences using McNemar X  X  test (sig. level=0.05). We also establish a ranking schema of different relational kernels as follows: in a given dataset if kernel a is significantly better than b then a is credited with one point and b with zero points; if there is no significant difference then both are credited with half point. Results are presented in table 1.

To compare the different data representations we fix a submodel and average the ranks of K list (or K set ), k P and k G , ignoring their parameter settings. The average ranks for the models where motifs are represented as sets or lists are 2.75 and 3.125, respectively, whereas for the third model (in stances from the  X  X ingerprints X  relation are associated both with the set and list of motifs) the average rank is 6.75. We can see that there is a clear advantage in terms of predic tive performance of the third model over the others.

Here the best results are obtained for the k P elementary kernel ( p =2 ,a =1 ) together with the Contiguous Sublist Kernel and where the third representation of the data is used. The estimated cross-valida tion accuracy is 87.63 % whereas the holdout accuracy is 87.35 %. This represents a statis tically significant improvement over the best accuracy previously reported in [8] by 1.72 % for cross-validation and 1.43 for the holdout test set. In this paper we proposed a new class of kernels which extends our previous work presented in [6, 1] in the sense that these ker nels are defined over a richer representa-tion language. Our kernels can be considered as instances of the -Convolution kernel where the subkernels are elementary ker nels and kernels over sets and lists.
Although many kernels have been recently proposed for sequences over a finite al-phabet (e.g. [4]), not much work has been done in defining kernels over lists of complex objects. The exceptions are kernels described in [5, 3], variants of which are proposed in Section 3. The main difference between the Contiguous Sublist Kernel and the Contigu-ous Subtree Kernel from [5] is that the latter uses an additional user defined matching function whereas for the objects we consider, the relation determines whether two re-lational instances are matchable or not. The other difference is that kernels from [5] are highly specialized whereas our kernels can be used for any classification problem. The Longest Common Sublist Kernel is a direct application of a kernel over basic terms defined in [3]. The main difference is that the kernel proposed in [3] was applied only for sequences over a finite alphabet (with matching kernel for sequences X  elements) whereas we extended it to more complex stru ctures where only a kernel over elements of the lists is needed. Comparison to other kernels over (general) complex structures can be found in [6].

Experiments on the protei n fingerprints show that in terms of accuracy there is an advantage of relational SVM over propositi onal one. On the other hand using the re-lational approach makes it easer to preprocess the data since it has a clear relational representation. In the future work we will concentrate on designing more refined ker-nels for sets and lists. The other remaining challenge here is that of bringing more discriminatory information (e.g. biological literature) to bear on the classification of protein fingerprints.

