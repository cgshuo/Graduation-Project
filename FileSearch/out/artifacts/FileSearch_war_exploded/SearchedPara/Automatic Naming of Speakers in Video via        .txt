
With the rapid development of information technology, more and more multimedia data appear s on the Internet, and there has been an explosive growth of audio -visual content . C ontent -based multimedia retrieval research develops rapidly. However, there are many limit ation s in multimedia retrieval , such as most retrieval manners are based on a singl e type of multimedia data, or the other modal data only play s a secondary role. To solve this problem, some researcher s are interested in cross -media retrieval, namely matching multimodal information by a certain correlation to realize flexibly cross ing di fferent medium for retrieval. However, this often requires expensive manual annotations , especially for video contents.

Manual annotation of each new video source is expensive and impossible. An inter-esting alternative is using unsupervised approaches to n ame people in multimedia doc-ument s. In most previous works, the researcher s first automatically classif ied each speech with an anonymous label and then use d other methods to find the name of each class. Most previous works concern ed the naming of people in video, and essentially use the same framework: a) face clustering ; b) extracting names for each person ; and c) names/faces mapping. Such method s are different in how to cluster faces, how to extract names and how to match names wit h faces.

However, extracting names for each person in video is a very difficult problem, as subtitles usually do not directly describe the faces in video. Moreover, even if the name of a face is mentioned in subtitles, the alignment may be wrong . There are many other problems, such as there are often many faces in the same frame, and many names in the transcripts, or many unnamed faces or names that are mentioned but not displayed. A nother difficult problem is that the identification of the perso n can be very hard due to the changes in pose, lighting conditions, facial expressions and partial occlusion. Recently, there has been a surge of interest in neural networks. In particular, deep and large networks have exhibited impressive results. However , most previous work s on the recognition of characters in video did not use the novel technology, they still use d the traditional method of clustering and traditional features in the face clustering stage. Another pr oblem is that many previous work s need ed a lot of man ual annotation infor-mation or did not make full use of the information in the video frame.

Based on these observations above, a novel scheme is proposed in this paper for facilitating more effective people news annotation via name -face Mapping by integrat-ing multimodal information involved in video news. Our proposed scheme differs from other earlier work in multiple a spects, as shown in Fig . 1 . a) In the face clustering stage, we mainly use the popularity technology of neural network to train a generic face clas-sifier, and extract deep descriptors of the human face by the classifier. b) In the name extraction stage, not only the text information such as video introduction, subtitles are used, but we also use the OCR technology to extract the text information in each frame of video. All the information will be recorded on the video track, and will also be pre-liminary matched with the face in the first stage. The i nitial matching information can be found in this way . c) In the matching stage, an efficient optimization algorithm based on the fuzzy clustering is particularly established to verify the feasibility of our auto-matic name -face Mapping algorithm.

Previous works on the naming of people in video, use d essentially the same frame-work: a) Face clustering; b) Extracting names for each person; c) Names/ Faces map-ping. However, previous work s on the recognition of ch aracters in video ha d often ig-nored the availability of textual information . Many researcher s obtained the names by manually annotation. A semi -automatic method wa s proposed in [ 1 ] to name f ace im-ages in BBC news. It needed to manually name some faces and then use d the iterative label propagation in a graph of connected faces or name -face pairs. In order to reduce the manual work, some researcher s d u g the information in the subtitles or audi o. Ever-ingham et al . in [2 ] obtai n ed the high precision by combining multiple sources of in-formation including subtitles, transcripts and visual information. Poignant et al . in [ 3 ] even tr ied to use the audio information to extract names . Besides extracting names, it wa s a challenging tas k to obtain clusters for per character without merging multiple characters into a single cluster. Zhou et al . in [ 4 ] ma d e use of must -link and cannot -link constraints to cluster per character. However, it need ed to know the number of cluster s in advance an d wa s difficult to achieve good result s for video s .

Usually, we cannot obtain the high precision in face clustering by using the tradi-tional representation of facial features. Recently, there has been a surge of interest in neural networks. In particular, deep and large networks have exhibited impressive re-sults [5] . However, most previous work s on the recognition of characters in video do not use such novel technology . Many other previous work s need ed a large number of manual annotation information or d id not make full use of the information in the video frame. Many clustering methods neglect ed the particularity of video news.

We can obtain the high precision by combining multiple sources of information, both visual and textual. The principal novelties that we introduce are: (i) extracting face fea-tures in video by using the neural network; (ii) strengthening the mapping between names and faces by analyzing the co -occurrence of names and faces; (iii) automatically and efficiently labelling the appearances of main characters with their names.
This section describes how we extract faces and cluster persons. It aim s to extract the faces in the video and extract the descriptors of their appearances. The descriptors can be used to m atch the same person and improve the final experimental results. 3.1 Face D etection
Many tools and algorithms can extract the face in the picture easily and quickly. In a 30 -minutes video, we can extract tens of thousands of faces which belong to dozens of individuals.

Firstly, we need to extract the faces in the video. A video is composed of man y coherent pictures and the technology of extracting the face region from the static image is very mature at present. In this paper, we use the V -J video face recognition method for the extraction work. If we just simply extract the faces from each frame in video, we will get very l arge and complex human faces, and it will aggravate the burden of clustering. F ortunately, the frames in a video are not independent. There is strong con-tinuity between frame s . We can use this peculiarity to get a preliminary clustering of persons in video , which will greatly reduce the initial complexity of faces, and will improve the efficiency and accuracy of clustering results.

Face tracking is similar to object tracking. There are many ways to achieve good results at present. In this paper, we mainly u se the optical flow method (Kanade -Lucas  X  Tomasi , KLT ) [2] to track faces. The instantiation for the algorithm of face tracking is shown in the Fig. 2.

The KLT algorithm mainly tracks multiple points to track objects . In terms of face tracking, we can identify the feature points in the face region, then use t he KLT algo-rithm to track such feature points in order to achieve the goal of tracking faces. The KLT algorithm can only track objects with small changes. Thus it can only be used in the continuous subset of video. We need to divide the video into smaller pieces accord-ing to the continuity. Considering the accuracy and efficiency, the segmentation criteria is the color histogram between two adjacent frames. We cannot achieve absolutely ac-curate identification of video segments by using the color histogram, but small mistakes just increase t he burden of initial clustering and cannot lead to too much influenc e on the final result. 3.2 Feature E xtraction
After the treatment in the previous section, we have already extract the faces of video, and carry on the preliminary clustering. However, be cause of the particularity of video, the same person could still be divi ded into dozens to hundreds of different classes after the preliminary clustering. I t is an almost impossible task to name all the classes, thus we need further clustering results.

In the previous work, most people use d the traditional method to extract th e features of each face, and design ed a certain characteristic to cluster. T he traditional methods generally are difficult to adapt to the complexity of faces in video. There is a huge bottleneck in traditional methods. Fortunately, we have a more accurate method of clus-tering with the emergence of neural network. In this paper, we use VGG Face De-scriptor [6] to extract the features of faces. The CNN architecture A is given in full detail in Table 1. It can achieve the high precision in LFW and Youtube Face s Dataset.
We align the faces before extracting the features and we use the approach in [7] to align the faces in this paper. 3.3 Face C lustering
We can obtain good clustering results by the excellent face features extracted from neural network a nd the inhere n t constraint in vi deo : faces in same frame cannot be linked and faces in same track must be linked . In this paper, we cluster the faces by the distance proposed in [8] and the inherent constraint in video.
In this section, we will mainly introduce how to deal wi th another important infor-mation in the cross -modal manner . The source of modern videos is variety, and a video is not often with enough text introduction, especially lack of the introduction of im-portant characters in video. Finding the personal informati on of video is a difficult tas k requirement of video data , especially the neural network technology used in OCR [9] . 4.1 OCR T ext E xtraction
T he script of OCR technology r esearch has made remarkable achievements. We can say that the current print OCR recognition technolo gy has reached a higher level. The OCR techn ology can be used to fully extract the textual information in video. By using OCR, we can extract the text infor mation in each frame of video. 4.2 Name Identification
Although, w e can easily get a lot of the text information, most of the test information is unserviceable . O nly the person  X  s name is useful . In this paper, we use the stanford-NER tool to extract the name in the text information. In order to cooperate with the mapping work, each name in the frame will be converted to the track of the name. W e need to determine the name extract ed by OCR not only the appear ance on which frames, but also the position in each frame. Such information will e nha nce the mapping results . 4.3 Integration of T ext
The p re vious processing fully extracts the names in video, but it also produces a lot of noise s , such as geographica l name , c ompany n ame and the name in the scroll bar . W e can see in the Fig. 3. that a l ot of te x t information could be recognized as the name for the person in the same frame. W e first eliminate the region with moving text and then eliminate the geographical name s and c ompany n ame s . At last, we elimin ate the very rare name .

Now, w e can easily get the correct text information by the above process . However, there are often many persons in the same frame. We need to find the most possible corresponding relation ship between the name and the face to enhance the mapping re-sults. We propose the following formula to determin e the initial matching distance for the co -occurrence name and face : where Name i denotes the track of the i th name; Face j denot e s the track of the j th face; min( dist ( Name i , Face j )) denotes the min distance between Name i and Face j ; F rame_cente r denotes the center coordinate of the frame; area( Face j ) denot e s the sum area of the faces in the i th face track. For most of news video s , the name of a fac e often appears with the face at the same time and the face locates in the center of the frame. We propose the formula to reflect the regular pattern , as shown in the Fig. 4 .
We have the movement track of names and faces by the above processing . We will obtain eventually matching results through the analysis f or the movement track of names and faces. We get the final match results through a matrix of names and faces .
Name -face mapping aims at finding the optimal one -to -one name -face matching in video. S ome probability -statistical models have been used to solve this task. However, most of the strategies are just the process of clustering witho ut mapping and need a lot of manual work [10] . Thus an improved Fuzzy C -Means (FCM) clustering algorithm, which introduces the consideration of salient name information and integrates the lim-itation of name -face co -occurrence, is established to make a better solution for name -face mapping . Compared to the general FCM clustering, the improved one can better de scribe the multimodal features in multimodal videos, and make s more reasonable solution for such a specific mapping issue.
 We will use the following symbo ls: FS : faces in the given video set ; NS : names in the given video set ; FN : the number of face feature vectors in the face set FS ; NC : name cluster s in the name set NS ; NCN : the number of name clusters in the name set NS ; Face i : the face feature vector for the i th face in FS and 1 X  i  X  FN ; NC _ Center j : the prototype of the center of face cluster for the j th NC in NS , 1 X  j  X  NCN ;
U ij : the membership degree of Face i in NC _ Center j ; m : the weighting exponent on each fuzzy membership deg ree , which is set empiri-cally (usually set as 2) .

Our improved FCM clustering algorithm mainly aims at optimizing the following objective function J m for every video. where Dist ( Face i , NC _ Center j ) is an Euclidean distance measure between Face i and NC _ Center j ; and P ij denotes that if the i th face and the j th name co -occur in the same time, P ij is set as 1, otherwise as 0. This function focuses on making the optimization for the distance among the face clusters associated with different NC s, so that each cluster has both the higher intra -cluster cohesion and the farther inter -cluster distance. 5.1 In itial ization
We get the final matching results through optimizing a correlation matrix of names and faces. The initial value of the matrix has great influence on the final results. We need good initial values through the track of names and faces.

The track of faces is obtained by face detection, and the track of names is obtained by OCR . T he track of names obtained by OCR is very important. The names in frames generally appear only once, and are often associated with the locat ion of the relevant people. Thus w e determine the initial value of the matrix by analyzing the matching degree of the track of names and faces in frames, which follow th e following steps:
Firstly, for every trajectory of name, find all the trajectories of faces around the track of names. Secondly, for each finding trajectories of faces, calculate the minimum dis-tance between the trajectories of faces and names.

Finally, t he results of the divisor between the minimum distance of the face and the total trajectory distance is the initial value of Formula (1) .

If the name extract ed by the subtitle appear ing in a frame, we will not consider the effec t of the name. When the subt itle -name appears, the related face may appear at the same time, and also is likely to appear before or later. Subtitle -nam e tends to appear many times, thus we initial ize the matching matrix by analyzing the co -occurrence of subtitle -name and the faces ar ound it.

The initialization for U by using the name salience can be defined as: where RS ( Name j ) denotes the important degree of Name j that co -occurs with Face i in the same video; and because P ij  X  0, it can be sure that Face i and Name j exactly co -occur in the video. Meanwhile, for the different names in a video, the sum of all their salience values is 1. The center of face cluster for each NC in NS is initialized according to Formula ( 4 ) with the parameter P ij . 5.2 Matrix Iteration
There will be a lot of noise s in the matching matrix because of the complexity and particularity of the video , and there will also be many correct initial values in the matrix matching. W e should imp l ement the process of iteration. This process aims at con-stantly amending the center of face cluster for each NC in NS according to Formulae ( 4 ) and ( 5 ), and also int roduces the parameter P ij that plays an important role for con-trolling the iteration.
In every iteration process, the center of face cluster for each NC in NS and the mem-bership degree of each face in the face cluster for each NC are both recalculated and updated. After such a process, the center of each face cluster and the membership de-gree of each face will become more precise. 5.3 Mapping The above iteration pr ocess will stop until the center of face cluster for each NC in NS no longer has offsets, or the number of iteration times reaches the preset maximal value. With the iteration to a convergence state, the fuzzy partition matrix U and each center of face clu ster are both output and taken as the final name -face association map-ping results, as shown in Formula ( 6 ). 6.1 Data set and Evaluation Metrics
Our dataset is established based on the video website of YouTube ! News Data con-structed by ourselves, in which there are 1 1 news videos with around 200 names and about 4 0,000 faces. The proposed method is applied to the dataset ( in total around 8 hours of video ) . The g round -truth names for e very person are produced by manual an-notation . To evaluate the effectiveness of our algorithm, Correct Mapping Rate (C M R) is define d as the percentage of correct mapping s generated in the ground -truth. 6.2 Experimental R esults
To prove the correctness of the parame ters we choose, we analysis the experimental results under different parameters . Table 3 shows the experimental results . The co -oc-currence means the co -occurrence between the name s and faces. The d ist ( F , N ) means the min distance between the names and faces . The d ist ( F , center ) means the average distance between faces and the center of the frame. The area ( F ) means the whole area of the faces.

Our name -face mapping model is created by identifying s alient n ame s, constraint f ace i dentification, and i mproved FCM c lustering a lgorithm . To give full exhibition to the superiority of our mapping model, we have also performed a comparison between our unsupervised method and s ome other methods. Two approaches developed by Poignant et al . [11] and Bendris et al . [12] are analogous with ours to some extent, and w e have accomplish ed the se method s on the same dataset. We test the CNN feature and SIFT feature on every method and the C M R is also divided into three parts. The first C M R is the C M R for the faces whose names appear in the video, the second C M R is the C M R for the faces whose names do not appear in the video and the last C M R is the C M R for the whole faces. The experimental results are presented in Table 4 . The instantiation of s ome name -face mapping example s are shown in the Fig. 5 . 6.3 Analysis and Discussion
We can see that facial features have great influence on the result and we know that the facial features mainly affect the result of the face clustering . Although the features extracted by CNN technology is better than traditional features , we cannot obtain per-fect results of the face clustering. The face clustering is di fficult due to the huge varia-tion in the appearance of each character. The extractor of the facial features is mainly trained by the static images and they are different from the faces extracted from the video. If we can train the extractor by the faces extracted from the video, we shall obtain better descriptors of the face.

It can be found from Table 4 t hat we can obtain the best C M R values. Poignant et al . X  X  approach tries to match all the names with their fa ces and Bendris et al . X  X  approach only matches the name with the speakers. Because of this, Poignant et al . X  X  C M R for the faces with names is better than Bendris et al . X  X  and Bendris et al . X  X  C M R for the faces without names is better than Poignant et al . X  X  . Our approach focuses on distin-guishing the people that appear at the same time so we can obtain better C M R. Many faces often appear in the same time in videos and we can obtain better mapping results by distinguish the important person from the whole fac es.
In this paper, a new framework is introduced to automatic annotate the person in the news video. A novel algorithm is developed by integrating identifying salient names, constraint face identification, and the improved FCM clustering algorithm. Our future work will focus on fusing the audio information in the video and further improve th e OCR accuracy.
 Acknowledgments. This work is supported by National Natural Science Fund of China (61572140), Shanghai Municipal R&amp;D Foundation (16511 105402&amp;16511104704), Shanghai Philosophy Social Sciences Planning Project (2014BYY009), and Zhuoxue Program of Fudan University . Yuejie Zhang is the corresponding author.
