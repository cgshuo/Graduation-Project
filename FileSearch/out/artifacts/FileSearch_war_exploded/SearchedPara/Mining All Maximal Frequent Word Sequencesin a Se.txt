 We present an efficient algorithm for finding all maximal frequent word sequences in a set of sentences. A word se-quence s is considered frequent, if all its words occur in at least  X  sentences and the words occur in each of these sen-tences in the same order as in s , given a frequency threshold  X  . Hence, the words of a sequence s do not have to occur consecutively in the sentences.
 H.3 [ Information Storage and Retrieval ]: Content Anal-ysis and Indexing Algorithms, Theory sequential patterns, text mining, collocations
Several application fields utilize word sequences of some kind. Computational linguistics attempts to find colloca-tions for machine translation, foreign language learning, and lexicography. Information retrieval, particularly question answering, systems could complement their document de-scriptors with phrases, in addition to single terms. Recog-nition of multi-word terms are important in information ex-traction and text summarization, as well.

In most cases, discovery of word sequences has been highly application-dependent, and in many language technology applications guided by linguistic knowledge and heuristics [1, 3, 2]. We introduce in this paper a general method for find-ing interesting word sequences. Interestingness is defined by the frequency of a sequence. More precisely, the method 1. The Congress subcommittee backed away from man-2. He urged Congress to reject provisions that would 3. Washington charged France West Germany the U.K. finds all maximal frequent sequences in a set of text frag-ments, typically in a set of sentences or paragraphs.
Assume S is a set of sentences, and each sentence consists of a sequence of words.

Definition 1. Asequence p = a 1  X  X  X  a k is a subsequence of a sequence q if all the items a i , 1  X  i  X  k ,occurin q and they occur in the same order as in p . If a sequence p is a subsequence of a sequence q , we also say that p occurs in q .
For instance, the sequence &lt; unfair practices &gt; can be found in all of the three sentences in Figure 1.

Definition 2. Asequence p is frequent in S if p is a subsequence of at least  X  min sentences of S ,where  X  min a given minimum frequency threshold.

If we assume that the frequency threshold is 2, we can find two frequent phrases in our sample set of sentences: and &lt; unfair practices &gt; .

Definition 3. Asequence p is a maximal frequent se-quence in S if there does not exist any sequence p in S such that p is a subsequence of p and p is frequent in S .
In our example, the sequence &lt; unfair practices &gt; is not maximal, since it is a subsequence of the sequence &lt; congress retaliation against foreign unfair trade practices &gt; ,whichis also frequent. The latter sequence is maximal.
In addition to a minimum frequency threshold, we also set a maximum frequency threshold  X  max . If we prune away the very frequent words, we can reduce the search space significantly. The disadvantage is naturally that we cannot discover sequences that contain these common words.
Due to the characteristics of textual data, the discovery process is divided into several phases. As the number of maximal frequent pairs and 3-grams is very large and these sequences may occur very frequently, they are found sepa-rately. First, all the words with a frequency less than  X  and the words with a frequency more than  X  max are re-moved. Second, the words of each sentence are sorted and all pairs of the sorted sentence are generated. This way we can compute a frequency for each unordered pair and select pairs that are frequent. The sentences are processed again and all words that do not occur in any unordered frequent pair are removed. During the same pass over the sentences, the frequency of each ordered pair is computed. After the pass, the frequent pairs can be collected.

Next, the sentences are processed again in order to find all frequent ordered 3-grams. The 3-grams of each sentence are generated, but a 3-gram can be ignored, if it contains a pair that is not frequent. Until now, the process only counts occurrences, but the exact positions of the pairs and 3-grams are not needed globally, and hence not stored. When we process longer sequences, however, we store positions of n -grams in the main memory. Hence, the initial phase, i.e. collecting frequent 2-and 3-grams, is finished by collecting the positions of all the frequent 3-grams.
 Algorithm 1. MineMFS.
 Input: G 3 :thefrequent 3 -grams Output: Max : the set of maximal frequent sequences 1. n := 3 2. Max :=  X  3. While G n is not empty 4. For all n -grams g  X  G n 5. If g is not a subsequence of 6. If g is frequent 7. max := Expand( g ) 8. Max := Max  X  X  max } 9. If max = g 10. Remove g from G n 11. Else 12. Remove g from G n 13. Prune( G n ) 14. Join the n -grams of G n to form G n +1 15. n := n +1 16. Return Max
The discovery of longer maximal frequent sequences is shown in Algorithm 1. The algorithm gets as input the set of frequent 3-grams, as described above. The algorithm takes a3-gramand expands it by adding items to it, in a greedy manner, until the longer sequence is no more frequent. The positions of longer sequences are computed from the posi-tions of 3-grams.

In the same way, we go through all 3-grams, but we only try to expand a 3-gram if it is not already a subsequence of some maximal frequent sequence, which guarantees that the same maximal sequence is not discovered several times. When all the 3-grams have been processed, every 3-gram belongs to some maximal frequent sequence. If some 3-gram cannot be expanded, it is itself a maximal frequent sequence. If we knew that every maximal frequent sequence contains at least one unique 3-gram, which distinguishes the sequence from the other maximal frequent sequences, then one pass through the 3-grams would discover all the maximal frequent sequences. As this cannot be guaranteed, the process must be repeated iteratively with longer n -grams.

In the expansion step (Line 7) of Algorithm 1, all the pos-the new item can be added to the tail, to the front or in the middle of a sequence. If one expansion does not produce a frequent sequence, other alternatives have to be checked. The expansion is greedy, however, since after expanding suc-cessfully it proceeds to continue expansion, rather than con-siders alternatives for the expansion. The choice of items to be inserted is restricted by the set of n -grams, i.e., also af-ter expansion the sequence is constructed from the existing n -grams.

Algorithm 1 removes an n -gram which is found to be in-frequent or which is itself a maximal frequent sequence. Al-though the algorithm can find long sequences in the first rounds, it cannot simply remove the n -grams of these se-quences, as the maximal frequent sequences may overlap and, hence, some new maximal frequent sequence may share n -grams with some sequence earlier found. Pruning of the n -gram set is, however, necessary, since the algorithm should avoid generating all the subsequences of the maximal fre-quent sequences, particularly when they are long. In the pruning phase (Line 13), ( n + 1)-grams are combined from existing n -grams, and for each such ( n + 1)-gram g and each sentence s where g occurs, it is checked, whether g indicates that a new maximal frequent sequence m may occur in s . Theoccurrenceof m is possible, if g combines words from two distinct maximal frequent sequences already found in s and g is frequent in the collection S . If the occurrence of m is possible in s , counters for n -suffix and n -prefix of g are incremented by one. Finally, if the values of these counters are at least the minimum frequency threshold, the respec-tive n -grams are not pruned. When g is found to indicate a potential new maximal frequent sequence, also the counters of some other n -grams are incremented, namely the n -grams that can form a longer sequence with g in s . [1] Y. Choueka, T. Klein, and E. Neuwitz. Automatic [2] G. Dias, S. Guillor  X  e, J.-C. Bassano, and J. G. P. Lopes. [3] F. Smadja. Retrieving collocations from text: Xtract.
