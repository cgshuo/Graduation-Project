 John Lee 1  X  Chak Yan Yeung 1  X  Amir Zeldes 2  X  Marc Reznicek 3  X  Anke Lu  X  deling 4  X  Jonathan Webster 1 Abstract Learner corpora consist of texts produced by non-native speakers. In addition to these texts, some learner corpora also contain error annotations, which can reveal common errors made by language learners, and provide training material for automatic error correction. We present a novel type of error-annotated learner corpus containing sequences of revised essay drafts written by non-native speakers of English. Sentences in these drafts are annotated with comments by language tutors, and are aligned to sentences in subsequent drafts. We describe the compi-lation process of our corpus, present its encoding in TEI XML, and report agreement levels on the error annotations. Further, we demonstrate the potential of the corpus to facilitate research on textual revision in L2 writing, by conducting a case study on verb tenses using ANNIS, a corpus search and visualization platform.
 Keywords Learner corpus Textual revision Feedback English as a second language Multi-layer corpus annotation Corpus search and visualization This article presents a learner corpus that consists of sequences of essay drafts, written by language learners and marked by language tutors. This corpus is designed to facilitate research on how language learners revise their writing, and how feedback influences their revision.

Simply put, learner corpora  X  X  X ave all the characteristics commonly attributed to corpora, the only difference being that the data come from language learners X  X  (Granger 2008 ). 1 Text corpora often not only contain the raw text, but also supply various kinds of linguistic annotation to facilitate research. For example, grammatical annotation, which can include part-of-speech tags and syntactic structures, is common in many text corpora and also available in some learner corpora (e.g., Reznicek et al. 2013 ).

A different kind of annotation, particular to learner corpora, is error annotation. It may simply indicate an error category (e.g., Nagata et al. 2011 ), e.g., marking an inappropriate preposition with the category  X  X  X rong preposition X  X . It may also come in the form of a target hypothesis (e.g., Dahlmeier and Ng 2011 ;Lu  X  deling et al. 2008 ; Nguyen and Miyao, 2013 ) 2  X  X .e. a corrected or reconstructed version of the learner sentence X  X n which case the appropriate preposition would also be supplied. Error annotation can be exploited not only in the research of Second Language Acquisition and Foreign Language Teaching (Granger 2004 ; Nesi et al. 2004 ), but also in automated detection and correction of grammatical errors (Lee and Seneff 2008 ; Dale and Kilgarriff 2011 ).

To date, learner corpora have concentrated solely on the final form of learner texts, i.e., the end result of the learner X  X  language production process. This process, however, is often an iterative one, with cycles of textual revision, either self initiated or guided by various interventions from the teacher (Graham and Perin 2007 ). A corpus containing intermediate versions of learner texts, with feedback, would help us better understand the dynamics of this revision process. It can also potentially provide answers to research questions on many related topics, such as:  X  Feedback effectiveness: How often do learners respond to feedback? Which  X  Revision behavior: How do learners revise their texts, with or without feedback?  X  Language teaching methodology: In view of the above, how can we improve
Some preliminary steps toward data-driven research on these questions have been undertaken. For example, through a web-based EFL writing environment, the XWiLL project offers a searchable database of essays written by students with teachers X  comments (Wible et al. 2001 ); however, the impact of the comments on students X  revisions cannot be directly traced. One corpus that aims to address this question is the Malmo  X  University-Chalmers Corpus of Academic Writing as a Process (Eriksson et al. 2012 ). It is expected to include 450 student texts, ranging from undergraduate to PhD levels, along with peer and teacher feedback. Our corpus is comprised of essays by undergraduate learners, but at a much larger scale, with more than 4000 essays and over 8 million words. These essays are annotated with error categories and comments from language tutors. Further, for each essay, the corpus includes not only its final version, but also its earlier drafts, with sentence and word alignments (see Fig. 2 ). By combining these annotations and alignments, our corpus provides the largest resource to date that facilitates research on language learners X  revision process, and how it is influenced by feedback.

This article discusses the content, construction of and access 3 to the corpus. In the next section, we introduce the compilation, annotation and architecture of the corpus. In Sect. 3 , we report on the conversion process of the corpus material from its original HTML format, as blogs in an e-learning environment, to TEI XML. Although the TEI representation is capable of marking up all the information in the corpus, it still requires considerable programming work to gather non-trivial statistics and create sensible visualizations of the corpus. In Sect. 4 , we discuss how we reduced this technical barrier by importing the corpus to ANNIS, a corpus search and visualization platform (Zeldes et al. 2009 ). Section 5 presents a case study on verb tense errors using ANNIS. Finally, Sect. 6 concludes with suggestions for future research directions. The material in this corpus originated with the Language Companion Course (LCC) project at City University of Hong Kong. The project was implemented over seven consecutive semesters, from 2007 to 2010, involving over 4200 predominantly Chinese students (Webster et al. 2011 ). Essays in the corpus were written by students from across 13 disciplines, representing a wide range of subject areas, including humanities and social sciences, natural sciences and engineering, business, law, and creative media (see Table 1 ); and a variety of essay genres. Science and engineering courses assigned lab reports; 4 business courses often involved case studies; linguistics students presented data analyses; and social science students wrote argumentative essays. Across all disciplines, article summaries were also assigned.
To support this large body of students, the project recruited more than 300 language tutors, including staff members of the university X  X  English Language Center, and students studying TESOL at one of the university X  X  global partner institutions, including undergraduates at Brigham Young University, as well as post-graduates at the University of Sydney and the University of British Columbia. While full details about the tutors are unavailable, it was the case that whereas those from the University of Sydney and the University of British Columbia came from a variety of language backgrounds, those from Brigham Young University were, for the most part, native speakers of English. 2.1 Drafts We collected the learner texts and comments from an e-learning environment, the Blackboard system ( http://www.blackboard.com ). Using a word processor built into this environment, the students composed and submitted drafts for written assign-ments. These drafts were saved as blog entries in HTML format. The tutors then provided feedback on language issues by highlighting problematic words and in-serting comments into the drafts. Subsequently, students made revisions to their texts. This cycle continued until the students uploaded a final version to be graded by the professor.

In the rest of this article, each submission is considered a draft ; a sequence of successive drafts, including the final version, will be referred to as draft #1 , draft #2 , etc. One such sequence will be called an essay . Our corpus contains 4337 essays. With an average of 2.7 drafts per essay, there are a total of 11,489 drafts. The average length of a draft is 750 words, yielding a corpus with 8,046,291 words, among the largest annotated learner corpora ever constructed. Detailed statistics can be found in Table 2 . 2.2 Error categories Error annotations were created during the revision process by the tutors. Tutors inserted comments into the drafts in one of two ways: First, they were allowed to select an error category from a fixed list, called the comment bank . Adopted from the XWiLL project (Wible et al. 2001 ) but considerably expanded, the comment bank contains a total of 78 categories, each given a numeric code. In Fig. 1 , for example, the tutor inserted the code  X  X 38 X  X  in square brackets, which refers to the error  X  X  X elative pronoun X  X issing X  X . We will call this kind of comment an error category . The Appendix provides the final version of the comment bank and gives example sentences.

We classified each error category into one of three levels: essay level, clause level, or word level. Table 3 shows the most frequently used error categories at each level. At the essay level, most categories deal with issues of coherence with a few categories relating to informal language and the essay structure. At the clause level, categories include punctuation errors, incorrect use of conjunctions and incorrect word order. At the word level, most categories deal with grammatical mistakes, including errors concerning word choice and spelling. The most frequent errors, involving articles, noun number, subject-verb agreement and prepositions, are similar to those found in other English learner corpora, particularly those by native speakers of Chinese and Japanese (Lee and Seneff 2008 ; Han et al. 2006 ).
 2.3 Open-ended comments As an alternative to the standard error categories, tutors were also allowed to insert custom comments; these will be referred to as open -ended comments . According to previous research, students found detailed comments, specific to their work, to be the most important and useful form of feedback (Lipnevich and Smith 2009 ). Consistent with this finding, a meta-analysis also concluded that students benefit more from general explanations of a grammatical phenomenon than from identification of specific errors (Biber et al. 2011 ). In our corpus, the open-ended comments may explain why a highlighted text was problematic, provide revision suggestions, or raise a question for clarification. In Fig. 1 , instead of simply using the error category  X  X  X ronoun X  X nclear reference X  X , the tutor chose to insert the comment [Be more explicit ... ] to provide a clearer diagnosis of the problem. While most open-ended comments aim at particular words and phrases, they can also address paragraphs or even the entire essay, such as the comment [Nice report! ... ] in Fig. 1 . Such comments are typically placed either at the beginning or end of a draft.

Tutors were more likely to use error categories than open-ended comments; the former accounts for more than 67 % of all comments in the corpus. Both kinds of comments become less frequent, however, as students progress in the revision cycle (see Table 4 ). For draft #1, more than 2 error categories appear every 100 words; in draft #3 and later, the figure drops to 0.16. A similar trend is observed for open-ended comments. This considerable drop corroborates previous findings that feedback does help students improve the overall quality of their drafts (Paulus 1999 ). When the LCC project was conceived, there was no plan to organize the material into a digital corpus. The essay drafts were simply saved as blog entries in HTML format; the comments were marked inline, and not always consistently, in the blogs. Our first task in building the corpus was to convert these blogs into a structured format. Figure 2 graphically depicts the corpus structure.

The Text Encoding Initiative X  X  TEI XML ( http://www.tei-c.org ) is a widely adopted format for text representation and interchange. We chose to encode the corpus according to the current TEI P5 guidelines as this format facilitates further processing by a variety of tools. In our case, we subsequently converted the data for access using ANNIS (see Sect. 4 ). The next section explains how each blog entry was stored as two TEI XML files, encoding the essay and the comments respectively. Section 3.2 describes how TEI XML files were generated for sentence and word alignments. 3.1 Drafts After downloading the drafts as blogs from the Blackboard system, we took the following three steps to convert them to the TEI format. 3.1.1 Automatic linguistic annotations While paragraph breaks can be unambiguously derived from the HTML format, sentence and word boundaries are not explicitly marked. Using Stanford CoreNLP tools (Toutanova and Manning 2000 ; Toutanova et al. 2003 ), we split the text into sentences, tokenized the text into separate word forms, and added lemmas and POS tags to the words. After these steps, one TEI XML document was generated for each respectively, and the parts-of-speech of the words are stored in the  X  X ype X  attributes of the word tags. Each word and sentence is given a unique ID so that it can be referenced from other files.

In order to prevent loss of information, the original appearance of the draft is preserved as much as possible with TEI tags, even if it may not be pertinent to current research questions. For example, we use &lt;hi&gt; to encode formatting styles such as highlighting, bold, underline, strike-through, superscript and subscript. For other special graphical objects that were difficult to capture in text, e.g. pictures and original graphics. 3.1.2 Mapping comments to text spans Error categories and open-ended comments are enclosed in brackets and embedded within the drafts. Each embedded comment addresses a particular text span. The text span concerned was indicated either by the font color or the background color of the words in the blog X  X  HTML format. When an open-ended comment is attached to the beginning or the end of a draft, it is taken to address the entire draft.
The comments are stored in a separate TEI XML file using the &lt;note&gt; tag. Error categories are stored in the  X  X ype X  attribute while open-ended comments are stored as the text within the tags. The  X  X lace X  attribute indicates whether the comment is beginning or end (usually aimed at the entire draft). The  X  X arget X  attribute stores the text span at which a comment is aimed; the text span can be a word element or a range of word elements in the draft.

After this mapping process, we checked whether the error categories are valid, i.e., applicable to the text span at which they are aimed, 5 concentrating on the four most frequently used categories (see Table 3 ). To be valid, the  X  X  X rticle missing X  X  category must be followed by a noun phrase; the  X  X  X oun countable X  X  category must comment on a noun; the  X  X  X erb-subject agreement X  X  category must comment on a verb in simple present tense; and the  X  X  X reposition wrong use X  X  category must comment on a preposition. 6 These categories were found to be valid 96.9, 99.3, 97.6, and 97.4 % of the time. We still need to check whether the text spans corresponding to these valid error categories indeed contain the specified errors; this will be addressed in Sect. 3.3 . 3.1.3 Title and metadata extraction Most blogs begin with a header, which contains the essay title and metadata such as dates, course codes, grade, assignment and draft numbers, as well as the names and IDs of the student and tutor, which are anonymized. These metadata can facilitate studies on longitudinal improvement, i.e., whether and how a student improved his/ her writing through the semester. We extracted the title from the beginning of the header; for ambiguous cases, we compared the extracted title with its counterparts in other drafts of the same essay. Table 5 shows how the title and metadata are stored in components of the &lt;teiHeader&gt;. 3.2 Sentence and word alignments To study the revision process, it is imperative to examine how an original sentence in an older draft was edited to form new sentence(s) in the next draft. We automatically obtained sentence and word alignments between drafts and included them in the corpus.
 3.2.1 Sentence alignment This task has been studied in the context of translation of revised documents (Shemtov 1993 ). Similar to the micro-alignment step in (Barzilay and Elhadad 2003 ), we used the cosine measure as the lexical similarity metric, and also allowed sentence insertion, deletion, merge (two-to-one), and split (one-to-two) alignment. For each consecutive pair of drafts (e.g., drafts #1 and #2, or #2 and #3), the globally best alignment was determined using dynamic programming.

Sentence alignment can be ambiguous. Suppose two sentences, at similar positions in both drafts, share a considerable number of words. The first sentence might have been edited into the second, in which case they should be aligned; alternatively, the first sentence might have been simply deleted and the second inserted, in which case they should not be aligned. Our principle was to prefer higher recall of alignments at the risk of lower precision X  X .e. to align sentence pairs with relatively low similarity X  X ince it is much easier for the corpus user to discount an alignment than to recover an unidentified alignment. This policy was enforced by setting a relatively high cost for insertion and deletion, merge and split.
We chose to use the XCES recommendation for sentence alignments. XCES is the XML application of the Corpus Encoding Standard, a widely accepted set of standards for encoding document structures and linguistic annotations in corpus-based work (Ide et al. 2000 ). For each consecutive pair of drafts, we encode the sentence alignments in a separate TEI file using &lt;link&gt; tags. Each &lt;link&gt; element has three attributes:  X  X rev X  and  X  X ext X  store the two sentence IDs concerned,  X  X ype X  stores the alignment type, which may be  X  X dentical X ,  X  X dited X ,  X  X plit X , or  X  X erged X . A non-aligned sentence may have the alignment type  X  X eleted X  or  X  X nserted X . There is no  X  X ext X  attribute in the former case and no  X  X rev X  attribute in the latter case.
To evaluate the quality of the automatic sentence alignments, we asked a human judge to manually align the sentences for 14 pairs of drafts. Taking the human alignments as reference, the accuracy of the automatic sentence alignments is 89.8 %, measured from the perspective of sentences in the earlier draft. 3.2.2 Word alignment On the pairs of sentences aligned in the previous step, we further performed word alignment. We obtained word alignments with a tool that calculates the Translation Error Rate, an evaluation metric for machine translation (Snover et al. 2006 ). This tool generates word alignments as a side product as it calculates the number of word insertions, deletions, substitutions, and shifts between two sentences. Since the  X  X  X hift X  X  operation allows crossed word alignments, this tool is more suitable for our purposes than most other alternatives.

The words in the sentences are first shifted, or re-ordered, in such a way as to minimize the number of word insertions, deletions, and substitutions. Identical words in the sentences are then aligned. Two non-identical words are considered a substitution (i.e., aligned) if they have similar spelling or have the same lemma. For example, in Fig. 2 , the aligned pairs  X  X  X nother X  X ther X  X , and  X  X  X sed X  X sing X  X , fall into these categories. Otherwise, the words are not aligned, and may be considered an individual insertion or deletion (e.g.,  X  X  X re X  X  in Fig. 2 ).
 Similar to sentence alignments, the word alignments are also stored in a separate TEI XML file using the same XCES conventions. Each &lt;link&gt; element has three attributes:  X  X rev X  and  X  X ext X  store the IDs of the two words concerned; and  X  X ype X  stores the alignment type, which may be  X  X dentical X ,  X  X dited X , or  X  X hifted X . Similar to sentence alignment, a non-aligned word may have the alignment type  X  X eleted X  or  X  X nserted X . 3.3 Level of agreement In order for the corpus to be useful, the reliability of the error annotations is critical. This is usually measured by the level of agreement, i.e., how often two people agree on their error diagnoses X  X hich may involve only error detection or also error correction X  X n a learner text.

Our evaluation measures how often an expert agrees with error categories annotated in the corpus by the tutors; it is thus an error detection task. Although agreement levels vary depending on error type (Andreu Andre  X  s et al. 2010 ), perfect agreement is almost never attained for a variety of reasons. Firstly, since learner texts can contain grammatical errors, they often support multiple interpretations (Lu  X  deling et al. 2008 ). To judge whether the use of a particular preposition is an error, for example, one must first attempt to reconstruct what the learner  X  X  X eally X  X  meant. The ambiguous nature of this task is illustrated in studies where subjects were asked to guess which prepositions were originally intended in a set of English sentences. The subjects agreed on the intended prepositions only 76 % of the time (Tetreault and Chodorow 2008 ); 7 in a similar study, they agreed on the intended article and number for noun phrases only 72 % of the time (Lee et al. 2009 ). To further complicate the matter, the error detection task also demands a judgment on the  X  X  X cceptability X  X  of a word. Even native speakers often disagree on where to draw the line between a passable word choice and one that ought to be corrected. This difficulty is reflected in a study on the NUCLE corpus, recently used in a shared task for automatic grammar correction (Dahlmeier et al. 2013 ). When comparing three independent annotations of a sample of 96 essays, the average kappa is 0.39 for grammatical error detection, and 0.55 for error type identification, which correspond to  X  X  X oderate X  X  and  X  X  X air X  X  agreement, respectively (Landis &amp; Koch, 1977 ). Rosen et al. ( 2014 ) reported kappa ranging from 0.16 ( X  X  X light X  X  agreement) to 0.88 ( X  X  X lmost perfect X  X  agreement) depending on error type, while Rozovskaya and Roth ( 2010 ) attained agreement levels ranging between 56 and 78 % on whether a sentence is  X  X  X orrect X  X  or  X  X  X ncorrect X  X .
 Our evaluation focused on the five most frequent error categories (see Table 3 ). For each category, we randomly selected 200 sentences that contained a text span annotated with that category. We then asked an expert 8 to decide whether the text span should be revised. Table 6 shows how often the experts agreed with the original annotations by the tutors. 9 The agreement level ranged from 73.9 % for  X  X  X reposition wrong use X  X , the most challenging category, to 87.1 % for  X  X  X rticle missing X  X . These figures corroborate previous studies in showing error diagnosis on learner text to be highly ambiguous; they also suggest that the reliability of the tutor annotations in our corpus is comparable with existing learner corpora. Although encoded as TEI documents, the corpus still demands considerable programming knowledge and effort 10 on the part of the user to collect meaningful statistics. To reduce the technical barrier and to provide a convenient graphical interface to view results, we imported our corpus into the ANNIS system, an open source, browser-based corpus search platform for richly annotated corpora (Zeldes et al. 2009 ). As an example of its capability, Fig. 3 shows a query that returns all sentences with the indefinite article  X  X  X  X  X  that has been annotated with the error category  X  X  X rticle X  X rong use X  X  (error category  X  X 5 X  X ) and revised to  X  X  X he X  X  in the next draft. We describe the corpus architecture in Sect. 4.1 , then summarize the conversion process from TEI XML to ANNIS in Sect. 4.2 . 4.1 Annotation layers Our corpus has various types of annotations that may overlap one another; for example, there can be multiple comments addressing overlapping text spans. Independent annotation layers, encoded in a multilayer architecture, are the most suitable representation, as has been argued before (Lu  X  deling et al. 2005 , Reznicek et al. 2013 ). In such an architecture, any number of types of annotations may be saved in a fashion that prevents one annotation layer from conflicting with another. This allows us to annotate the same category multiple times (e.g. multiple competing part-of-speech annotations), to add different categories to a corpus retroactively without disrupting existing annotations, and to annotate structures that conflict hierarchically (e.g. annotations beginning and ending in the middle of other annotations or discontinuous annotation spans).

As listed in Table 7 , our corpus is represented in ANNIS in nine annotation layers. These layers encode the words in the learner texts, their lemma, POS, formatting style, sentence and paragraph boundaries, and the comments. An example sentence is shown with these layers in Fig. 4 . Sentence and word alignments are encoded as  X  X  X elations X  X  between elements in these layers. As listed in Table 8 , each relation bears a number of attributes, including the draft number and the nature of the revision. The interested reader is referred to (Krause and Zeldes, to 2014 ) for further technical detail.
 4.2 Conversion to ANNIS Many different XML formats can be imported into the data model of ANNIS. The most powerful of these in terms of its ability to express complex graph structures is PAULA XML (Dipper 2005 ), which can represent all annotations in our corpus, including annotation spans, parallel alignment on multiple levels and metadata. We converted our TEI documents into PAULA XML, and subsequently imported these into ANNIS using the multi-format, meta-model based converter framework SaltNPepper (Zipser and Romary 2010 ). To demonstrate the research potential of the corpus, we present a case study on the influence of feedback on learners X  revision behavior regarding verb tense, a common error type in the corpus. Whereas previous studies (e.g., Granger 1999 ) focus only on the nature of the errors, our corpus enables us to report how often and how these errors are revised, and the impact of feedback on the revision.

This case study focuses on present and past tenses, the most common tenses in the corpus; the four error categories 11 concerned are thus  X  X erb X  X resent simple X  (i.e., revision to present simple is suggested),  X  X erb X  X ast simple X ,  X  X erb X  X resent perfect X  and  X  X erb X  X ast perfect X . There are a total of 2482 comments involving these error categories. We first give an overview of the ANNIS Query Language (AQL) in Sect. 5.1 , showing how it can access, query and visualize relevant materials with a handful of simple queries. 12 We then report an evaluation on the quality of the annotations in Sect. 5.2 , and discuss the results in Sect. 5.3 . 5.1 Queries in ANNIS Throughout the study, we rely on ANNIS to generate quantitative data. The ANNIS Query Language (AQL) is designed to search for node elements and the edges between them. Roughly speaking, one first specifies the relevant nodes, then states the constraints that must hold between them, and possibly adds metadata restrictions. A node element can be a word, e.g. the query:
POS=/(VB|VB[PZ])/ uses a regular expression, wrapped in slashes, to find all words tagged as VB, VBP or VBZ, the tags for present-tense verbs for the Stanford tagger, which follows the Penn Treebank tagset (Marcus et al. 1993 ). Attribute-value pairs can also be used, e.g.

CommentBank= X  X 85 X  X  finds the error category 85,  X  X erb X  X ast simple X , i.e. past simple tense is needed. When specifying multiple elements, the relationship between them must be stated, e.g. both annotations applying to the same position, as in: POS=/(VB|VB[PZ])/ &amp; CommentBank= X  X 85 X  X  &amp; #1 _=_ #2 This query searches for present-tense verbs and the error category 85, and further specifies that the former (#1) covers the same text as the latter (#2), using the operator _=_. To add the constraint that the present-tense verb was revised to past simple, we add a third search element to find all words tagged as VBD, the tag for past simple verbs. We then use the arrow operator (-&gt;) and the edge type WordAlign to require this past-tense verb (#3) be aligned to #1:
POS=/(VB|VB[PZ])/ &amp; CommentBank =  X  X 85 X  X  &amp; POS= X  X  X BD X  X  &amp; #1 _=_ #2 &amp; #1 -&gt;WordAlign #3
In summary, this query searches for cases of a verb in present simple tense which is revised to the past simple in response to the error category  X  X erb X  X ast simple X . Some search results are shown in Fig. 5 . 5.2 Parser evaluation and agreement level Since our analysis relies on the output of the automatic Stanford POS tagger (Toutanova and Manning 2000 ; Toutanova et al. 2003 ), we would like to measure its accuracy on recognizing verb tenses in learner text. We randomly selected 100 sentence pairs involving revised verb tenses, and examined the POS tags assigned by the tagger to the verbs. The accuracy of these tags was 97 %. Although automatic syntactic analysis for noisy text is a challenging task (Foster et al., 2008 ), the tagger seemed capable of correctly analyzing verb tenses in most cases.

Our analysis also relies on error annotations by the tutors. Similar to the evaluation in Sect. 3.3 , we used these 100 sentence pairs to measure the level of agreement on verb tense error diagnosis. A human judge annotated these errors, which were then compared with the original annotations of the tutors. The agreement level was 93 %, higher than the other categories reported in Sect. 3.3 . Most disagreements appeared to involve the use of the perfect aspect; for example, whether a past/present perfect tense was more appropriate than the past simple, or vice versa. 5.3 Results As shown in Table 9 , it is much more common for students to use the present simple tense where the past was needed, as compared with the reverse direction. This tendency may be influenced by the students X  L1: since Chinese verbs are not inflected, students preferred by analogy the uninflected form in English, which happens to be the present simple.

As for the students X  revision behavior, our corpus shows the feedback uptake rate to be mixed. When asked to change from other tenses to the present simple, students responded at a rate of over 76 %; in contrast, when asked to change to the past perfect, they responded in less than 43 % of the cases. One explanation could be that they were not as familiar with the past perfect tense as with the present simple. In general, the feedback uptake rate in our corpus is lower than those in the literature. For example, in one study on about 1500 teacher comments, only 14 % of the comments were left unaddressed by the students in out-of-class revision (Ferris 1997 ). A larger study on more than 5700 comments yielded similar conclusions, with only 10 % of the comments left unaddressed (Ferris 2006 ; see also the meta-analysis of a variety of studies in Russell and Spada 2006 ). Our lower uptake rate may be partially attributed to the fact that the feedback came from tutors rather than teachers.

Even when students did respond to the feedback, it is a separate question whether improvement again varies according to the tense. When students responded to a suggested revision to the present simple tense, more than two-thirds of their revisions were executed successfully (compare % of changes and % of correct changes in Table 9 ); when responding to a suggested revision to the past perfect, however, less than a half of their revisions were correct. This discrepancy is consistent with our hypothesis above that the students were unfamiliar with the past perfect. If true, this would point to the need for giving more detailed feedback for error categories involving complex grammatical constructions.

This case study has investigated only a narrow aspect of the larger research topic of feedback utility, which remains an open question (Truscott 1996 ; Russell and Spada 2006 ). When both grammatical feedback and content feedback were given, Fathman and Whalley ( 1990 ) reported that all students improved their grammatical accuracy, while 77 % also improved the content of their writing. Ferris ( 1997 ) found that when ESL students responded to teachers X  feedback and made changes, the changes almost always led to overall improvement in their papers. Similar conclusions were made by Ashwell ( 2000 ), Ferris and Roberts ( 2001 ), Chandler ( 2003 ) and Truscott and Hsu ( 2008 ). In contrast, Polio and Fleck ( 1998 ) found that error correction did not result in any significant improvement in the linguistic accuracy of ESL students. This corpus can potentially contribute further data towards these research questions. We presented a corpus containing drafts of a large number of ESL students X  essays, together with comments made by language tutors. The corpus incorporates lemma and part-of-speech annotations, and aligns sentences and words from successive drafts, thus showing how students responded to the comments and revised their writing. Currently the largest dataset of its kind, we evaluated the quality of its error annotations, and motivated how it can support research on textual revision in L2 writing.

To provide access to the data, we encoded our corpus in TEI XML format, and further ported it to the ANNIS corpus visualization and search platform. Through a case study, we showed how revision statistics can be retrieved using straightforward queries in the ANNIS Query Language (AQL), reducing the technical effort needed to conduct data-driven research on a large-scale corpus. This study investigated how ESL students revise verb tense errors and measured their feedback uptake rate. Our analysis indicates that the uptake rate varies according to the tense in question, suggesting that some tenses are more difficult to revise and might warrant more detailed feedback.

This corpus can facilitate many directions of research. We plan to characterize learners X  behavior in textual revision, for example how often sentences are split or merged and at which draft. We would also like to investigate whether and how students revise differently when given or not given feedback; and in the latter case, whether error categories or open-ended comments, and direct or indirect feedback, are more likely to elicit better responses from students.
 The complete list of the error categories 13 used in our corpus, with example sentences, are shown in Table 10 . The text span addressed by the error category is enclosed in square brackets. For some of the categories, we provide an explanation rather than an example because of space constraints. References
 John Lee 1  X  Chak Yan Yeung 1  X  Amir Zeldes 2  X  Marc Reznicek 3  X  Anke Lu  X  deling 4  X  Jonathan Webster 1 Abstract Learner corpora consist of texts produced by non-native speakers. In addition to these texts, some learner corpora also contain error annotations, which can reveal common errors made by language learners, and provide training material for automatic error correction. We present a novel type of error-annotated learner corpus containing sequences of revised essay drafts written by non-native speakers of English. Sentences in these drafts are annotated with comments by language tutors, and are aligned to sentences in subsequent drafts. We describe the compi-lation process of our corpus, present its encoding in TEI XML, and report agreement levels on the error annotations. Further, we demonstrate the potential of the corpus to facilitate research on textual revision in L2 writing, by conducting a case study on verb tenses using ANNIS, a corpus search and visualization platform.
 Keywords Learner corpus Textual revision Feedback English as a second language Multi-layer corpus annotation Corpus search and visualization This article presents a learner corpus that consists of sequences of essay drafts, written by language learners and marked by language tutors. This corpus is designed to facilitate research on how language learners revise their writing, and how feedback influences their revision.

Simply put, learner corpora  X  X  X ave all the characteristics commonly attributed to corpora, the only difference being that the data come from language learners X  X  (Granger 2008 ). 1 Text corpora often not only contain the raw text, but also supply various kinds of linguistic annotation to facilitate research. For example, grammatical annotation, which can include part-of-speech tags and syntactic structures, is common in many text corpora and also available in some learner corpora (e.g., Reznicek et al. 2013 ).

A different kind of annotation, particular to learner corpora, is error annotation. It may simply indicate an error category (e.g., Nagata et al. 2011 ), e.g., marking an inappropriate preposition with the category  X  X  X rong preposition X  X . It may also come in the form of a target hypothesis (e.g., Dahlmeier and Ng 2011 ;Lu  X  deling et al. 2008 ; Nguyen and Miyao, 2013 ) 2  X  X .e. a corrected or reconstructed version of the learner sentence X  X n which case the appropriate preposition would also be supplied. Error annotation can be exploited not only in the research of Second Language Acquisition and Foreign Language Teaching (Granger 2004 ; Nesi et al. 2004 ), but also in automated detection and correction of grammatical errors (Lee and Seneff 2008 ; Dale and Kilgarriff 2011 ).

To date, learner corpora have concentrated solely on the final form of learner texts, i.e., the end result of the learner X  X  language production process. This process, however, is often an iterative one, with cycles of textual revision, either self initiated or guided by various interventions from the teacher (Graham and Perin 2007 ). A corpus containing intermediate versions of learner texts, with feedback, would help us better understand the dynamics of this revision process. It can also potentially provide answers to research questions on many related topics, such as:  X  Feedback effectiveness: How often do learners respond to feedback? Which  X  Revision behavior: How do learners revise their texts, with or without feedback?  X  Language teaching methodology: In view of the above, how can we improve
Some preliminary steps toward data-driven research on these questions have been undertaken. For example, through a web-based EFL writing environment, the XWiLL project offers a searchable database of essays written by students with teachers X  comments (Wible et al. 2001 ); however, the impact of the comments on students X  revisions cannot be directly traced. One corpus that aims to address this question is the Malmo  X  University-Chalmers Corpus of Academic Writing as a Process (Eriksson et al. 2012 ). It is expected to include 450 student texts, ranging from undergraduate to PhD levels, along with peer and teacher feedback. Our corpus is comprised of essays by undergraduate learners, but at a much larger scale, with more than 4000 essays and over 8 million words. These essays are annotated with error categories and comments from language tutors. Further, for each essay, the corpus includes not only its final version, but also its earlier drafts, with sentence and word alignments (see Fig. 2 ). By combining these annotations and alignments, our corpus provides the largest resource to date that facilitates research on language learners X  revision process, and how it is influenced by feedback.

This article discusses the content, construction of and access 3 to the corpus. In the next section, we introduce the compilation, annotation and architecture of the corpus. In Sect. 3 , we report on the conversion process of the corpus material from its original HTML format, as blogs in an e-learning environment, to TEI XML. Although the TEI representation is capable of marking up all the information in the corpus, it still requires considerable programming work to gather non-trivial statistics and create sensible visualizations of the corpus. In Sect. 4 , we discuss how we reduced this technical barrier by importing the corpus to ANNIS, a corpus search and visualization platform (Zeldes et al. 2009 ). Section 5 presents a case study on verb tense errors using ANNIS. Finally, Sect. 6 concludes with suggestions for future research directions. The material in this corpus originated with the Language Companion Course (LCC) project at City University of Hong Kong. The project was implemented over seven consecutive semesters, from 2007 to 2010, involving over 4200 predominantly Chinese students (Webster et al. 2011 ). Essays in the corpus were written by students from across 13 disciplines, representing a wide range of subject areas, including humanities and social sciences, natural sciences and engineering, business, law, and creative media (see Table 1 ); and a variety of essay genres. Science and engineering courses assigned lab reports; 4 business courses often involved case studies; linguistics students presented data analyses; and social science students wrote argumentative essays. Across all disciplines, article summaries were also assigned.
To support this large body of students, the project recruited more than 300 language tutors, including staff members of the university X  X  English Language Center, and students studying TESOL at one of the university X  X  global partner institutions, including undergraduates at Brigham Young University, as well as post-graduates at the University of Sydney and the University of British Columbia. While full details about the tutors are unavailable, it was the case that whereas those from the University of Sydney and the University of British Columbia came from a variety of language backgrounds, those from Brigham Young University were, for the most part, native speakers of English. 2.1 Drafts We collected the learner texts and comments from an e-learning environment, the Blackboard system ( http://www.blackboard.com ). Using a word processor built into this environment, the students composed and submitted drafts for written assign-ments. These drafts were saved as blog entries in HTML format. The tutors then provided feedback on language issues by highlighting problematic words and in-serting comments into the drafts. Subsequently, students made revisions to their texts. This cycle continued until the students uploaded a final version to be graded by the professor.

In the rest of this article, each submission is considered a draft ; a sequence of successive drafts, including the final version, will be referred to as draft #1 , draft #2 , etc. One such sequence will be called an essay . Our corpus contains 4337 essays. With an average of 2.7 drafts per essay, there are a total of 11,489 drafts. The average length of a draft is 750 words, yielding a corpus with 8,046,291 words, among the largest annotated learner corpora ever constructed. Detailed statistics can be found in Table 2 . 2.2 Error categories Error annotations were created during the revision process by the tutors. Tutors inserted comments into the drafts in one of two ways: First, they were allowed to select an error category from a fixed list, called the comment bank . Adopted from the XWiLL project (Wible et al. 2001 ) but considerably expanded, the comment bank contains a total of 78 categories, each given a numeric code. In Fig. 1 , for example, the tutor inserted the code  X  X 38 X  X  in square brackets, which refers to the error  X  X  X elative pronoun X  X issing X  X . We will call this kind of comment an error category . The Appendix provides the final version of the comment bank and gives example sentences.

We classified each error category into one of three levels: essay level, clause level, or word level. Table 3 shows the most frequently used error categories at each level. At the essay level, most categories deal with issues of coherence with a few categories relating to informal language and the essay structure. At the clause level, categories include punctuation errors, incorrect use of conjunctions and incorrect word order. At the word level, most categories deal with grammatical mistakes, including errors concerning word choice and spelling. The most frequent errors, involving articles, noun number, subject-verb agreement and prepositions, are similar to those found in other English learner corpora, particularly those by native speakers of Chinese and Japanese (Lee and Seneff 2008 ; Han et al. 2006 ).
 2.3 Open-ended comments As an alternative to the standard error categories, tutors were also allowed to insert custom comments; these will be referred to as open -ended comments . According to previous research, students found detailed comments, specific to their work, to be the most important and useful form of feedback (Lipnevich and Smith 2009 ). Consistent with this finding, a meta-analysis also concluded that students benefit more from general explanations of a grammatical phenomenon than from identification of specific errors (Biber et al. 2011 ). In our corpus, the open-ended comments may explain why a highlighted text was problematic, provide revision suggestions, or raise a question for clarification. In Fig. 1 , instead of simply using the error category  X  X  X ronoun X  X nclear reference X  X , the tutor chose to insert the comment [Be more explicit ... ] to provide a clearer diagnosis of the problem. While most open-ended comments aim at particular words and phrases, they can also address paragraphs or even the entire essay, such as the comment [Nice report! ... ] in Fig. 1 . Such comments are typically placed either at the beginning or end of a draft.

Tutors were more likely to use error categories than open-ended comments; the former accounts for more than 67 % of all comments in the corpus. Both kinds of comments become less frequent, however, as students progress in the revision cycle (see Table 4 ). For draft #1, more than 2 error categories appear every 100 words; in draft #3 and later, the figure drops to 0.16. A similar trend is observed for open-ended comments. This considerable drop corroborates previous findings that feedback does help students improve the overall quality of their drafts (Paulus 1999 ). When the LCC project was conceived, there was no plan to organize the material into a digital corpus. The essay drafts were simply saved as blog entries in HTML format; the comments were marked inline, and not always consistently, in the blogs. Our first task in building the corpus was to convert these blogs into a structured format. Figure 2 graphically depicts the corpus structure.

The Text Encoding Initiative X  X  TEI XML ( http://www.tei-c.org ) is a widely adopted format for text representation and interchange. We chose to encode the corpus according to the current TEI P5 guidelines as this format facilitates further processing by a variety of tools. In our case, we subsequently converted the data for access using ANNIS (see Sect. 4 ). The next section explains how each blog entry was stored as two TEI XML files, encoding the essay and the comments respectively. Section 3.2 describes how TEI XML files were generated for sentence and word alignments. 3.1 Drafts After downloading the drafts as blogs from the Blackboard system, we took the following three steps to convert them to the TEI format. 3.1.1 Automatic linguistic annotations While paragraph breaks can be unambiguously derived from the HTML format, sentence and word boundaries are not explicitly marked. Using Stanford CoreNLP tools (Toutanova and Manning 2000 ; Toutanova et al. 2003 ), we split the text into sentences, tokenized the text into separate word forms, and added lemmas and POS tags to the words. After these steps, one TEI XML document was generated for each respectively, and the parts-of-speech of the words are stored in the  X  X ype X  attributes of the word tags. Each word and sentence is given a unique ID so that it can be referenced from other files.

In order to prevent loss of information, the original appearance of the draft is preserved as much as possible with TEI tags, even if it may not be pertinent to current research questions. For example, we use &lt;hi&gt; to encode formatting styles such as highlighting, bold, underline, strike-through, superscript and subscript. For other special graphical objects that were difficult to capture in text, e.g. pictures and original graphics. 3.1.2 Mapping comments to text spans Error categories and open-ended comments are enclosed in brackets and embedded within the drafts. Each embedded comment addresses a particular text span. The text span concerned was indicated either by the font color or the background color of the words in the blog X  X  HTML format. When an open-ended comment is attached to the beginning or the end of a draft, it is taken to address the entire draft.
The comments are stored in a separate TEI XML file using the &lt;note&gt; tag. Error categories are stored in the  X  X ype X  attribute while open-ended comments are stored as the text within the tags. The  X  X lace X  attribute indicates whether the comment is beginning or end (usually aimed at the entire draft). The  X  X arget X  attribute stores the text span at which a comment is aimed; the text span can be a word element or a range of word elements in the draft.

After this mapping process, we checked whether the error categories are valid, i.e., applicable to the text span at which they are aimed, 5 concentrating on the four most frequently used categories (see Table 3 ). To be valid, the  X  X  X rticle missing X  X  category must be followed by a noun phrase; the  X  X  X oun countable X  X  category must comment on a noun; the  X  X  X erb-subject agreement X  X  category must comment on a verb in simple present tense; and the  X  X  X reposition wrong use X  X  category must comment on a preposition. 6 These categories were found to be valid 96.9, 99.3, 97.6, and 97.4 % of the time. We still need to check whether the text spans corresponding to these valid error categories indeed contain the specified errors; this will be addressed in Sect. 3.3 . 3.1.3 Title and metadata extraction Most blogs begin with a header, which contains the essay title and metadata such as dates, course codes, grade, assignment and draft numbers, as well as the names and IDs of the student and tutor, which are anonymized. These metadata can facilitate studies on longitudinal improvement, i.e., whether and how a student improved his/ her writing through the semester. We extracted the title from the beginning of the header; for ambiguous cases, we compared the extracted title with its counterparts in other drafts of the same essay. Table 5 shows how the title and metadata are stored in components of the &lt;teiHeader&gt;. 3.2 Sentence and word alignments To study the revision process, it is imperative to examine how an original sentence in an older draft was edited to form new sentence(s) in the next draft. We automatically obtained sentence and word alignments between drafts and included them in the corpus.
 3.2.1 Sentence alignment This task has been studied in the context of translation of revised documents (Shemtov 1993 ). Similar to the micro-alignment step in (Barzilay and Elhadad 2003 ), we used the cosine measure as the lexical similarity metric, and also allowed sentence insertion, deletion, merge (two-to-one), and split (one-to-two) alignment. For each consecutive pair of drafts (e.g., drafts #1 and #2, or #2 and #3), the globally best alignment was determined using dynamic programming.

Sentence alignment can be ambiguous. Suppose two sentences, at similar positions in both drafts, share a considerable number of words. The first sentence might have been edited into the second, in which case they should be aligned; alternatively, the first sentence might have been simply deleted and the second inserted, in which case they should not be aligned. Our principle was to prefer higher recall of alignments at the risk of lower precision X  X .e. to align sentence pairs with relatively low similarity X  X ince it is much easier for the corpus user to discount an alignment than to recover an unidentified alignment. This policy was enforced by setting a relatively high cost for insertion and deletion, merge and split.
We chose to use the XCES recommendation for sentence alignments. XCES is the XML application of the Corpus Encoding Standard, a widely accepted set of standards for encoding document structures and linguistic annotations in corpus-based work (Ide et al. 2000 ). For each consecutive pair of drafts, we encode the sentence alignments in a separate TEI file using &lt;link&gt; tags. Each &lt;link&gt; element has three attributes:  X  X rev X  and  X  X ext X  store the two sentence IDs concerned,  X  X ype X  stores the alignment type, which may be  X  X dentical X ,  X  X dited X ,  X  X plit X , or  X  X erged X . A non-aligned sentence may have the alignment type  X  X eleted X  or  X  X nserted X . There is no  X  X ext X  attribute in the former case and no  X  X rev X  attribute in the latter case.
To evaluate the quality of the automatic sentence alignments, we asked a human judge to manually align the sentences for 14 pairs of drafts. Taking the human alignments as reference, the accuracy of the automatic sentence alignments is 89.8 %, measured from the perspective of sentences in the earlier draft. 3.2.2 Word alignment On the pairs of sentences aligned in the previous step, we further performed word alignment. We obtained word alignments with a tool that calculates the Translation Error Rate, an evaluation metric for machine translation (Snover et al. 2006 ). This tool generates word alignments as a side product as it calculates the number of word insertions, deletions, substitutions, and shifts between two sentences. Since the  X  X  X hift X  X  operation allows crossed word alignments, this tool is more suitable for our purposes than most other alternatives.

The words in the sentences are first shifted, or re-ordered, in such a way as to minimize the number of word insertions, deletions, and substitutions. Identical words in the sentences are then aligned. Two non-identical words are considered a substitution (i.e., aligned) if they have similar spelling or have the same lemma. For example, in Fig. 2 , the aligned pairs  X  X  X nother X  X ther X  X , and  X  X  X sed X  X sing X  X , fall into these categories. Otherwise, the words are not aligned, and may be considered an individual insertion or deletion (e.g.,  X  X  X re X  X  in Fig. 2 ).
 Similar to sentence alignments, the word alignments are also stored in a separate TEI XML file using the same XCES conventions. Each &lt;link&gt; element has three attributes:  X  X rev X  and  X  X ext X  store the IDs of the two words concerned; and  X  X ype X  stores the alignment type, which may be  X  X dentical X ,  X  X dited X , or  X  X hifted X . Similar to sentence alignment, a non-aligned word may have the alignment type  X  X eleted X  or  X  X nserted X . 3.3 Level of agreement In order for the corpus to be useful, the reliability of the error annotations is critical. This is usually measured by the level of agreement, i.e., how often two people agree on their error diagnoses X  X hich may involve only error detection or also error correction X  X n a learner text.

Our evaluation measures how often an expert agrees with error categories annotated in the corpus by the tutors; it is thus an error detection task. Although agreement levels vary depending on error type (Andreu Andre  X  s et al. 2010 ), perfect agreement is almost never attained for a variety of reasons. Firstly, since learner texts can contain grammatical errors, they often support multiple interpretations (Lu  X  deling et al. 2008 ). To judge whether the use of a particular preposition is an error, for example, one must first attempt to reconstruct what the learner  X  X  X eally X  X  meant. The ambiguous nature of this task is illustrated in studies where subjects were asked to guess which prepositions were originally intended in a set of English sentences. The subjects agreed on the intended prepositions only 76 % of the time (Tetreault and Chodorow 2008 ); 7 in a similar study, they agreed on the intended article and number for noun phrases only 72 % of the time (Lee et al. 2009 ). To further complicate the matter, the error detection task also demands a judgment on the  X  X  X cceptability X  X  of a word. Even native speakers often disagree on where to draw the line between a passable word choice and one that ought to be corrected. This difficulty is reflected in a study on the NUCLE corpus, recently used in a shared task for automatic grammar correction (Dahlmeier et al. 2013 ). When comparing three independent annotations of a sample of 96 essays, the average kappa is 0.39 for grammatical error detection, and 0.55 for error type identification, which correspond to  X  X  X oderate X  X  and  X  X  X air X  X  agreement, respectively (Landis &amp; Koch, 1977 ). Rosen et al. ( 2014 ) reported kappa ranging from 0.16 ( X  X  X light X  X  agreement) to 0.88 ( X  X  X lmost perfect X  X  agreement) depending on error type, while Rozovskaya and Roth ( 2010 ) attained agreement levels ranging between 56 and 78 % on whether a sentence is  X  X  X orrect X  X  or  X  X  X ncorrect X  X .
 Our evaluation focused on the five most frequent error categories (see Table 3 ). For each category, we randomly selected 200 sentences that contained a text span annotated with that category. We then asked an expert 8 to decide whether the text span should be revised. Table 6 shows how often the experts agreed with the original annotations by the tutors. 9 The agreement level ranged from 73.9 % for  X  X  X reposition wrong use X  X , the most challenging category, to 87.1 % for  X  X  X rticle missing X  X . These figures corroborate previous studies in showing error diagnosis on learner text to be highly ambiguous; they also suggest that the reliability of the tutor annotations in our corpus is comparable with existing learner corpora. Although encoded as TEI documents, the corpus still demands considerable programming knowledge and effort 10 on the part of the user to collect meaningful statistics. To reduce the technical barrier and to provide a convenient graphical interface to view results, we imported our corpus into the ANNIS system, an open source, browser-based corpus search platform for richly annotated corpora (Zeldes et al. 2009 ). As an example of its capability, Fig. 3 shows a query that returns all sentences with the indefinite article  X  X  X  X  X  that has been annotated with the error category  X  X  X rticle X  X rong use X  X  (error category  X  X 5 X  X ) and revised to  X  X  X he X  X  in the next draft. We describe the corpus architecture in Sect. 4.1 , then summarize the conversion process from TEI XML to ANNIS in Sect. 4.2 . 4.1 Annotation layers Our corpus has various types of annotations that may overlap one another; for example, there can be multiple comments addressing overlapping text spans. Independent annotation layers, encoded in a multilayer architecture, are the most suitable representation, as has been argued before (Lu  X  deling et al. 2005 , Reznicek et al. 2013 ). In such an architecture, any number of types of annotations may be saved in a fashion that prevents one annotation layer from conflicting with another. This allows us to annotate the same category multiple times (e.g. multiple competing part-of-speech annotations), to add different categories to a corpus retroactively without disrupting existing annotations, and to annotate structures that conflict hierarchically (e.g. annotations beginning and ending in the middle of other annotations or discontinuous annotation spans).

As listed in Table 7 , our corpus is represented in ANNIS in nine annotation layers. These layers encode the words in the learner texts, their lemma, POS, formatting style, sentence and paragraph boundaries, and the comments. An example sentence is shown with these layers in Fig. 4 . Sentence and word alignments are encoded as  X  X  X elations X  X  between elements in these layers. As listed in Table 8 , each relation bears a number of attributes, including the draft number and the nature of the revision. The interested reader is referred to (Krause and Zeldes, to 2014 ) for further technical detail.
 4.2 Conversion to ANNIS Many different XML formats can be imported into the data model of ANNIS. The most powerful of these in terms of its ability to express complex graph structures is PAULA XML (Dipper 2005 ), which can represent all annotations in our corpus, including annotation spans, parallel alignment on multiple levels and metadata. We converted our TEI documents into PAULA XML, and subsequently imported these into ANNIS using the multi-format, meta-model based converter framework SaltNPepper (Zipser and Romary 2010 ). To demonstrate the research potential of the corpus, we present a case study on the influence of feedback on learners X  revision behavior regarding verb tense, a common error type in the corpus. Whereas previous studies (e.g., Granger 1999 ) focus only on the nature of the errors, our corpus enables us to report how often and how these errors are revised, and the impact of feedback on the revision.

This case study focuses on present and past tenses, the most common tenses in the corpus; the four error categories 11 concerned are thus  X  X erb X  X resent simple X  (i.e., revision to present simple is suggested),  X  X erb X  X ast simple X ,  X  X erb X  X resent perfect X  and  X  X erb X  X ast perfect X . There are a total of 2482 comments involving these error categories. We first give an overview of the ANNIS Query Language (AQL) in Sect. 5.1 , showing how it can access, query and visualize relevant materials with a handful of simple queries. 12 We then report an evaluation on the quality of the annotations in Sect. 5.2 , and discuss the results in Sect. 5.3 . 5.1 Queries in ANNIS Throughout the study, we rely on ANNIS to generate quantitative data. The ANNIS Query Language (AQL) is designed to search for node elements and the edges between them. Roughly speaking, one first specifies the relevant nodes, then states the constraints that must hold between them, and possibly adds metadata restrictions. A node element can be a word, e.g. the query:
POS=/(VB|VB[PZ])/ uses a regular expression, wrapped in slashes, to find all words tagged as VB, VBP or VBZ, the tags for present-tense verbs for the Stanford tagger, which follows the Penn Treebank tagset (Marcus et al. 1993 ). Attribute-value pairs can also be used, e.g.

CommentBank= X  X 85 X  X  finds the error category 85,  X  X erb X  X ast simple X , i.e. past simple tense is needed. When specifying multiple elements, the relationship between them must be stated, e.g. both annotations applying to the same position, as in: POS=/(VB|VB[PZ])/ &amp; CommentBank= X  X 85 X  X  &amp; #1 _=_ #2 This query searches for present-tense verbs and the error category 85, and further specifies that the former (#1) covers the same text as the latter (#2), using the operator _=_. To add the constraint that the present-tense verb was revised to past simple, we add a third search element to find all words tagged as VBD, the tag for past simple verbs. We then use the arrow operator (-&gt;) and the edge type WordAlign to require this past-tense verb (#3) be aligned to #1:
POS=/(VB|VB[PZ])/ &amp; CommentBank =  X  X 85 X  X  &amp; POS= X  X  X BD X  X  &amp; #1 _=_ #2 &amp; #1 -&gt;WordAlign #3
In summary, this query searches for cases of a verb in present simple tense which is revised to the past simple in response to the error category  X  X erb X  X ast simple X . Some search results are shown in Fig. 5 . 5.2 Parser evaluation and agreement level Since our analysis relies on the output of the automatic Stanford POS tagger (Toutanova and Manning 2000 ; Toutanova et al. 2003 ), we would like to measure its accuracy on recognizing verb tenses in learner text. We randomly selected 100 sentence pairs involving revised verb tenses, and examined the POS tags assigned by the tagger to the verbs. The accuracy of these tags was 97 %. Although automatic syntactic analysis for noisy text is a challenging task (Foster et al., 2008 ), the tagger seemed capable of correctly analyzing verb tenses in most cases.

Our analysis also relies on error annotations by the tutors. Similar to the evaluation in Sect. 3.3 , we used these 100 sentence pairs to measure the level of agreement on verb tense error diagnosis. A human judge annotated these errors, which were then compared with the original annotations of the tutors. The agreement level was 93 %, higher than the other categories reported in Sect. 3.3 . Most disagreements appeared to involve the use of the perfect aspect; for example, whether a past/present perfect tense was more appropriate than the past simple, or vice versa. 5.3 Results As shown in Table 9 , it is much more common for students to use the present simple tense where the past was needed, as compared with the reverse direction. This tendency may be influenced by the students X  L1: since Chinese verbs are not inflected, students preferred by analogy the uninflected form in English, which happens to be the present simple.

As for the students X  revision behavior, our corpus shows the feedback uptake rate to be mixed. When asked to change from other tenses to the present simple, students responded at a rate of over 76 %; in contrast, when asked to change to the past perfect, they responded in less than 43 % of the cases. One explanation could be that they were not as familiar with the past perfect tense as with the present simple. In general, the feedback uptake rate in our corpus is lower than those in the literature. For example, in one study on about 1500 teacher comments, only 14 % of the comments were left unaddressed by the students in out-of-class revision (Ferris 1997 ). A larger study on more than 5700 comments yielded similar conclusions, with only 10 % of the comments left unaddressed (Ferris 2006 ; see also the meta-analysis of a variety of studies in Russell and Spada 2006 ). Our lower uptake rate may be partially attributed to the fact that the feedback came from tutors rather than teachers.

Even when students did respond to the feedback, it is a separate question whether improvement again varies according to the tense. When students responded to a suggested revision to the present simple tense, more than two-thirds of their revisions were executed successfully (compare % of changes and % of correct changes in Table 9 ); when responding to a suggested revision to the past perfect, however, less than a half of their revisions were correct. This discrepancy is consistent with our hypothesis above that the students were unfamiliar with the past perfect. If true, this would point to the need for giving more detailed feedback for error categories involving complex grammatical constructions.

This case study has investigated only a narrow aspect of the larger research topic of feedback utility, which remains an open question (Truscott 1996 ; Russell and Spada 2006 ). When both grammatical feedback and content feedback were given, Fathman and Whalley ( 1990 ) reported that all students improved their grammatical accuracy, while 77 % also improved the content of their writing. Ferris ( 1997 ) found that when ESL students responded to teachers X  feedback and made changes, the changes almost always led to overall improvement in their papers. Similar conclusions were made by Ashwell ( 2000 ), Ferris and Roberts ( 2001 ), Chandler ( 2003 ) and Truscott and Hsu ( 2008 ). In contrast, Polio and Fleck ( 1998 ) found that error correction did not result in any significant improvement in the linguistic accuracy of ESL students. This corpus can potentially contribute further data towards these research questions. We presented a corpus containing drafts of a large number of ESL students X  essays, together with comments made by language tutors. The corpus incorporates lemma and part-of-speech annotations, and aligns sentences and words from successive drafts, thus showing how students responded to the comments and revised their writing. Currently the largest dataset of its kind, we evaluated the quality of its error annotations, and motivated how it can support research on textual revision in L2 writing.

To provide access to the data, we encoded our corpus in TEI XML format, and further ported it to the ANNIS corpus visualization and search platform. Through a case study, we showed how revision statistics can be retrieved using straightforward queries in the ANNIS Query Language (AQL), reducing the technical effort needed to conduct data-driven research on a large-scale corpus. This study investigated how ESL students revise verb tense errors and measured their feedback uptake rate. Our analysis indicates that the uptake rate varies according to the tense in question, suggesting that some tenses are more difficult to revise and might warrant more detailed feedback.

This corpus can facilitate many directions of research. We plan to characterize learners X  behavior in textual revision, for example how often sentences are split or merged and at which draft. We would also like to investigate whether and how students revise differently when given or not given feedback; and in the latter case, whether error categories or open-ended comments, and direct or indirect feedback, are more likely to elicit better responses from students.
 The complete list of the error categories 13 used in our corpus, with example sentences, are shown in Table 10 . The text span addressed by the error category is enclosed in square brackets. For some of the categories, we provide an explanation rather than an example because of space constraints. References
