 Department of Decision Sciences and Information Management, Katholieke Universiteit Leuven, Leuven, Belgium
School of Management, University of Southampton, Southampton, UK Vlerick Leuven-Gent Management School, Leuven, Belgium 1. Introduction
The number of mobile phone users has increased tremendously during the last decade. At the end of 2010, there will be more than five billion mobile phone users, 1 which is over 70% of the world popula-tion. As a result, telecommunication markets are getting saturated, particularly in developed countries, and mobile phone penetration rates are stagnating. Therefore, operators are shifting their focus from attracting new customers to retaining the existing customer base. Moreover, the literature reports that customer retention is profitable because: (1) acquiring a new client is five to six times more costly than retaining an existing customer [5,7,17,52]; (2) long-term customers generate higher profits, tend to be less sensitive to competitive marketing activities, become less costly to serve, and may provide new referrals through positive word-of-mouth, while dissatisfied customers might spread negative word-of mouth [18,29,46,50,53,58,66]; (3) losing customers leads to opportunity costs because of reduced sales [54]. A small improvement in customer retention can therefore lead to a significant increase in profit [60]. For successful targeted marketing campaigns, it is crucial that operators are able to identify clients with a high probability to churn in the near future. Next to correctly classifying the future churn-ers, it is important to gain insight in the reasons for customers to be classified as churners. Therefore, telecom operators prefer compact and interpretable models, as it allows them to check whether the model is in line with current domain knowledge. Moreover, it enables operators to recognize potential warning signs for customer churn.

Although a myriad of techniques for churn prediction has been examined, there has been little atten-tion to the use of Bayesian Network (BN) classifiers. This paper will investigate the predictive power of a number of Bayesian Network algorithms, ranging from the Naive Bayes classifier, with very strict independence assumptions, to General Bayesian Network classifiers, which allow more flexibility. The performance of the classifiers is evaluated with both the Area under the Receiver Operating Character-istic Curve and the Maximum Profit criterion and is rigorously statistically tested.
 Like most real life data mining problems, also churn prediction involves a large number of attributes. Including irrelevant variables would result in complex and incomprehensible classification models, im-peding the interpretability of these classifiers [45]. Hence, feature selection is commonly applied to withhold only those variables with strong explanatory power. Many feature selection algorithms have been proposed in the literature. However, most of these methods perform the selection from a univariate perspective, i.e. they assess a measure of dependence between the attributes and the target variable for each attribute separately. In this study, a feature selection method based on the concept of the Markov Blanket, which is genuinely related to Bayesian Networks, will be analyzed. This method approaches the input selection problem from a multivariate point of view and relies on the concept of conditional independence. Especially in the context of some of the Bayesian Network classifiers, this form of feature selection proves to be useful, as will be shown in Section 5. The impact of this variable reduction on classification performance and network complexity is investigated, since, basically, one is looking for the most compact Bayesian network with the highest explanatory power.

For measuring classifier performance and selecting the most appropriate classification method, a vari-ety of performance measures has been used [1,45]. In this study, the well-known Area under the Receiver Operating Characteristic Curve (AUC) is employed, as well as the Maximum Profit (MP) criterion, re-cently proposed by Verbeke et al. [61]. Instead of measuring performance over the whole output range, as AUC does, the maximum profit criterion performs an intelligent optimization by targeting this frac-tion of the customer base which would maximize the profit generated by a retention campaign. As such, it is able to indicate the model which maximizes the effectiveness of a retention campaign. The rationale behind this performance measure is that the most important goal for a telecom provider is to optimize its profit.

The remainder of this paper is organized as follows. In Section 2, the general problem of customer churn will be stated. Section 3 will give an overview of the main Bayesian Network classifiers and dis-cuss the algorithms briefly. In Section 4, the experimental setup is described, the data set characteristics described in Section 5. 2. Customer churn prediction Many companies and organizations are confronted with customer churn. For instance, wireless tele-com operators report annual churn rates up to 40% of their customer base [49]. Customer churn is associated with a direct loss of income and a diversity of supplementary costs, such as for instance the investments to acquire new customers to maintain the level of the customer base. Therefore, reducing customer churn by directing specifically designed marketing campaigns to the customers with the high-est probability to attrite, has proven to be profitable to a company [60]. To improve the efficiency of customer retention campaigns, a customer churn prediction model is needed to indicate the customers which are the most likely to churn and should be included in the retention campaign.

Customer churn prediction is a problem for which typically a data mining approach is adopted. Data mining entails the overall process of extracting knowledge from data. Based on historical data a model can be trained to classify customers as future churners or non-churners. Numerous classification tech-niques have been applied to predict churn, including traditional statistical methods such as logistic re-gression [9,43,49], non-parametric statistical models like for instance k-nearest neighbor [20], decision trees [44,64], and neural networks [6,34]. An extensive literature review on customer churn prediction modeling can be found in Verbeke et al. [62].

The process of developing a customer churn prediction model consists of several steps. The first step in this process consists of gathering relevant data and selecting candidate explanatory variables. The resulting data set is then cleaned and preprocessed. In the second step a model is built. A modeling technique is selected based on the requirements of the model and the type of data. Input selection is often applied to reduce the number of variables in order to get a consistent, unbiased, and relevant set of explanatory variables. Depending on the number of observations, which can be small in case of new products, a model is trained by cross validation or by splitting the data set in a separate training and test set. The resulting model is then evaluated, typically by comparing the true values of the target variable with the predicted values, but also, if possible, by interpreting the selected variables and the modeled relation with the target variable. A variety of performance measures to evaluate a classification model have been proposed in the literature [1]. As will be discussed in Section 4, in this study both the statistically based area under the receiver operating characteristic curve [38] and the maximum profit criterion, recently proposed by Verbeke et al. [61], will be applied. The latter estimates the profit a telecom operator would make when optimally exploiting the results of a particular classifier. Next, in a third step the model is assessed by a business expert to check whether the model is intuitively correct and in line with business knowledge which requires the induced model to be interpretable [45]. A prototype of the model is then developed, and deployed in the information and communication technology (ICT) architecture. The final step, once a model is implemented that performs satisfactory, consists of regularly reviewing the model in order to asses whether it still performs well. Surely in a highly technological and volatile environment as the telecommunications sect or, a continuous evaluation on newly gathered data is of crucial importance.

Because of the third step, where the model is assessed by a business expert to confirm whether the model is intuitively correct, the comprehensibility aspect of customer churn prediction models is of cru-cial importance. However, comprehensibility of customer churn models thus far only received limited attention in the literature [44,62]. A specific family of classification techniques, which result in compre-hensible models but have not been tested rigorously in a customer churn prediction setting before, are the Bayesian Network Classifiers. Two simple Bayesian Network Classifiers, i.e. Naive Bayes and stan-dard Bayesian Networks, have been included in an extensive benchmarking study by Verbeke et al. [61], which compares the performance of a variety of state-of-the-art classification techniques applied on an extensive number of data sets. Their results suggest that Bayesian Network Classifiers form a viable alternative modeling approach for customer churn prediction as they are able to produce compact and interpretable models. The next section will discuss a number of Bayesian Network Classification tech-niques into detail, which are applied to five real-life telecom churn data sets in Sections 4 and 5. 3. Bayesian network classifiers 3.1. Bayesian networks
A Bayesian network (BN) represents a joint probability distribution over a set of stochastic variables, either discrete or continuous. It is to be considered as a probabilistic white-box model consisting of a qualitative part specifying the conditional (in)dep endencies between the variables and a quantitative part specifying the conditional probabilities of the data set variables [51]. Formally, a Bayesian network consists of two parts B = G,  X  . The first part G is a directed acyclic graph (DAG) consisting of nodes and arcs. The nodes are the variables X 1 to X n in the data set whereas the arcs indicate direct dependencies between the variables. The graph G then encodes the independence relationships in the domain under investigation. The second part of the network,  X  , represents the conditional probability distributions. It contains a parameter  X  x i |  X  x of X i in G . The network B then represents the following joint probability distribution: The first task when learning a Bayesian network is to find the structure G of the network. Once we know the network structure G , the parameters  X  need to be estimated. In general, these two estimation tasks are performed separately. In this paper, we will use the empirical frequencies from the data D to estimate these parameters: It can be shown that these estimates maximize the log likelihood of the network B given the data D . Note that these estimates might be further improved by a smoothing operation [27].

A Bayesian network is essentially a statistical model that makes it feasible to compute the (joint) pos-the complementary subset are observed. This functionality makes it possible to use a Bayesian network as a statistical classifier by applying the winner-takes-all rule to the posterior probability distribution for the (unobserved) class node. The underlying assumption behind the winner-takes-all rule is that all gains and losses are equal. For a discussion of this aspect see, e.g. [22]. A simple example of a Bayesian network classifier is given in Fig. 1. Suppose that, for a particular customer, all variables except C are known and take the following values: A  X  [20; 100) , B =0 , D  X  [0; 130) and E =1 . The probability that the customer will churn conditional on this information can be calculated as: Thus, reading from Fig. 1 and using Eq. (1) yields: Hence, the conditional probability for churning is: According to the winner-takes-all rule, the customer will be classified as a non-churner. In what fol-lows, several structure learning algorithms for the construction of Bayesian network classifiers will be discussed. 3.2. The Naive Bayes classifier
A simple classifier, which in practice often performs surprisingly well, is the Naive Bayes classi-each variable X i given the class label c l . A new test case ( X 1 = x 1 ,...,X n = x n ) is then classified by using Bayes X  rule to compute the posterior probability of each class c l given the vector of observed variable values: The simplifying assumption behind the Naive Bayes classifier then assumes that the variables are con-ditionally independent given the class label. Hence, This assumption simplifies the estimation of the class-conditional probabilities from the training data. Notice that one does not estimate the denominator in Eq. (3) since it is independent of the class. Instead, Naive Bayes classifiers are easy to construct since the structure is given apriori and no structure learning phase is required. The probabilities P ( X i = x i | C = c l ) are estimated by using the frequency counts for the discrete variables and a normal or kernel density based method for continuous variables [35]. Figure 2(a) provides a graphical representation of a Naive Bayes classifier. 3.3. Augmented Naive Bayes classifiers
The strength of Naive Bayes classifiers inspired several authors to develop Augmented Naive Bayes network classifiers. These are methods based on the Naive Bayes classifier while partially relaxing the in-dependence assumption. The Selective Naive Bayes classifier omits certain variables to deal with strong correlation among attributes [39], whereas the Semi-Naive Bayesian classifier clusters correlated at-tributes into pairwise disjount groups [36]. Friedman et al. developed Tree Augmented Naive Bayes (TAN) classifiers , an algorithm where every attribute has one and only one additional parent next to the class node [27].

In this study, the Augmented Naive Bayes classifiers developed by Sacha [55] are used. This is a family of classifiers where the constraints of the TAN approach are further relaxed: not all attributes need to be dependent on the class node and there does not necessarily need to be an undirected path between two attributes. The algorithms exist of a combination of five basic operators, summarized in Table 1. The measure of dependency between two attributes is defined as follows:
Combining different class dependency operators with augmenting methods from Table 1 yields a number of algorithms, listed below as per increasing complexity:  X  Naive Bayes  X  TAN: Tree augmented Naive Bayes  X  FAN: Forest augmented Naive Bayes  X  STAN: Selective tree augmented Naive Bayes  X  STAND: Selective tree augmente d Naive Bayes with discarding  X  SFAN: Selective forest augmented Naive Bayes  X  SFAND: Selective forest augmente d Naive Bayes with discarding The aim of these classifiers is to find a trade-off between the simplicity of the Naive Bayes classifiers (with a limited number of parameters) and the more realistic and complex case of full dependency between the attributes.

Except for Naive Bayes and TAN, all of the above procedures use a search method that requires a quality measure to assess the fitness of a network given the data. In this study, two quality measures proposed by Sacha will be used. The first is the Standard Bayesian (SB) measure, which is proportional to the posterior probability distribution p ( G,  X  | D ) and contains a penalty for the network size. This penalty term is a function of the dimension of the network, the latter being defined as the number of free parameters needed to fully specify the joint probability distribution. A derivation of the Standard Bayesian measure can be found in [55]. The second quality measures is the Local Leave-One-Out Cross Validation. Let V l be the training set D without instance x l : The quality measure is then defined as: measures were combined with the five algorithms defined above, resulting in ten different classifiers.
It is worthwhile mentioning that the Naive Bayes classifier and the ANB classifiers without node discarding do not have the flexibility to remove variables from the network. Hence, even if a variable is completely independent from the target variable, it will still be used by the classifier. As this is clearly undesirable, a feature selection algorithm is carried out as part of the data preprocessing procedure. Although there are many input selection methods available, in this study the Markov Blanket feature selection will be used, since it tackles the problem from a multivariate perspective (see Section 4 for more information). Essentially, it applies the principle of conditional independence, which plays a pivotal role in the theory of Bayesian Networks. 3.4. General Bayesian network classifiers
All the previously discussed methods restrain the structure of the network in order to limit the com-plexity of the algorithms. Omitting those restrictions extends the search space of allowed networks to all General Bayesian Networks (GBN) . Finding the optimal network in such a solution space is known to be an NP-hard problem since the number of DAGs for n variables is superexponential in n [14]. As described by Kotsiantis [37], there are two broad categories of structure learning algorithms. The first consists of heuristic methods, searching through the space of all possible DAGs and measuring the fit-ness of a DAG with a score metric, whereas the second category comprises constraint based approaches using conditional independence (CI) tests to reveal the structure of the network.

Scoring based methods have been compared with CI-based algorithms by Heckerman et al. [33], leading to the observation that CI based methods perform better on sparse networks. Search-and-score algorithms, on the other hand, work with a broader variety of probabilistic models even though their heuristic nature may inhibit finding the optimal structure. 3.4.1. Search-and-score algorithms Several algorithms have been proposed in the literature, e.g. [15,33] show that selecting a single DAG using greedy search often leads to accurate predictions. In this analysis, the well-known K2 al-gorithm [19] is applied. It seeks for the network with the highest posterior probability given a data set and makes the following four assumptions:  X  All attributes are discrete,  X  The instances occur independently, given one Bayesian network,  X  There are no cases that have variables with missing values,  X  The prior probab ility of the conditional probabilities in the conditional probabi lity tables at each Given these assumptions, a greedy search algorithm will find the network with the highest score, given the database D . For a detailed discussion, one may refer to [19]. 3.4.2. Constraint based algorithms
Constraint based algorithms are also known as Conditional Independence (CI) based methods. They do not use a score but employ conditional independence tests to find the relations between attributes given a data set. In this paper, the Three Phase Dependency Analysis (TPDA) algorithm [13] is used, in which the concept of d-separation plays an essential role. It can be shown that if sets of variables X and Z are d-separated by Y in a directed acyclic graph G ,then X is independent of Z conditional on Y in every distribution compatible with G [30,63]. It is precisely this property that will be exploited in the algorithm of Cheng to learn the Bayesian network structure. The algorithm itself consists of four phases. In a first phase, a draft of the network structure is made based on the mutual information between each pair of nodes. The second and third phase then add and remove arcs based on the concept of d-separation and conditional independence tests. Finally, in the fourth phase, the Bayesian network is pruned and its parameters are estimated. The algorithm is described in detail by Cheng [11,12]. 3.4.3. Hybrid methods
Also hybrid methods have been developed, combining characteristics of both search-and score and constraint based algorithms. Examples of such techniques are the Sparse Candidate (SC) algorithm [28] and the Max-Min Hill-Climbing learning algorithm (MMHC) [59].

This latter method finds the parent-children set ( PC ) of each and every node, and thus determines the skeleton of the Bayesian network. In a first phase, also called the forward phase, nodes selected by a heuristic procedure sequentially enter a candidate PC set ( CPC ). The set may contain false positives, which are removed in phase II of the algorithm, i.e. the backward phase. The algorithm tests whether any variable in CPC is conditionally independent on the target variable, given a blocking set S  X  CPC . If such variables are found, they are removed from CPC . As a measure of conditional (in)dependence, the G 2 measure, as described by Kotsiantis [57], is used. This measure is asymptotically following a  X  2 distribution with appropriate degrees of freedom, which allows to calculate a p -value indicating the probability of falsely rejecting t he null hypothesis. Conditional i ndependence is assumed when the p -value is less than the significance level  X  (0.05 and 0.01 in this study). Once the skeleton is determined, a greedy search method is used to direct the edges between the attributes. This is the second step and the search-and-score part of the algorithm, making it a hybrid method. The BDeu score [32] has been used in this paper. 4. Experimental setup The aim of the study is twofold. Firstly, the differences in terms of performance between the Bayesian Network classifiers, described in Section 3, are investigated. Obviously, the aim is to find the algorithm with the most discriminative power, and to determine whether the differences are significant. Secondly, the experiments need to reveal whether the Markov Blanket feature selection has a deteriorating impact on classification performance. Preferably, the input selection would reduce the number of attributes without affecting the predictive performance substantially, since it is supposed to remove variables which are independent from the target, conditional on the variables which are withheld. The remainder of this section will describe the experimental setup. 4.1. Data sets and preprocessing
Four real life and one synthetic data set will be used to evaluate the performance of the different classification techniques. Table 2 summarizes the most important aspects of these data sets. The first data set was obtained directly from a European telecommunication operator, the next three are available at the website of the Center for Customer Relationship Management at Duke University, 2 and the last one is a synthetic data set, available at the UCI repository. 3
The number of observations or instances in a data set is of great importance. The more observations, the more generally applicable the generated model will be. A large data set also allows to split the data used to evaluate the proposed model. This ensures the performance measure is not biased due to over-fitting the model to the test data. As can be seen from the table, the smallest data set has five thousand observations, and the largest up to almost hundred thousand observations. The data sets also differ sub-stantially regarding the number of candidate explanatory variables, in a range from 12 up to 171 (for our model however. The eventual performance of the model mainly depends on the explanatory power of the variables. Since a large number of attributes heavily increases the computational requirements, and most often only a limited number of variables is effectively valuable to explain the target variable, a feature selection method could be used to limit the available attributes. In the next subsection, a Markov Blan-ket based algorithm for feature selection is described. Another characteristic of the data set is the class distribution of the target variable, which is usually heavily skewed in a churn prediction setting. Three of the data sets approximately have an even class distribution, which is the result of undersampling (i.e. removing non-churners from the data set). To test the classifiers, the test sets have been adjusted in order to resemble a realistic churn data set.

When working with real life data sets, it is essential to preprocess the data before starting the analysis because of two main reasons [24]: (1) problems with the data (e.g. missing values, irrelevant data, etc.), and (2) preparation for data analysis (e.g. discretization of continuous variables, etc.). To ensure that the results are comparable, all data sets have been preprocessed in the same manner:  X  The data set is split into a training set (66% of the instances) and a test set (34% of the instances).  X  The missing values are replaced by the median or the mode for continuous or nominal variables  X  Coarse variables (such as e.g. ZIP code) are clustered into smaller groups, in order to have a more  X  Since most Bayesian network algorithms only work with discrete variables, all continuous variables  X  A large number of attributes demands high computational requirements for the Markov Blanket Once these preprocessing steps are carried out, the data sets are ready for the Markov Blanket feature selection algorithm and the Bayesian Network classifiers, described in the following subsections. 4.2. Markov blanket feature selection
Over the past decades, feature selection has become an essential part in predictive modeling. It aims to deal with three problems [31]: (1) improving the accuracy of predictive algorithms, (2) developing faster and more cost-effective predictors, and (3) gaining insight in the underlying process that generated the data. Several techniques for input selection have been proposed, but many of these rely on univariate testing procedures, i.e. the variables are assessed one by one in terms of dependency on the target vari-able. For this study, the Markov Blanket (MB) feature selection algorithm of Aliferis et al. [3,4] has been applied. The procedure is based on the concept of the Markov blanket, which also plays a crucial role in Bayesian Networks. As opposed to commonly used univariate methods, this algorithm approaches the task from a multivariate perspective and exploits the concept of conditional independence. The Markov Blanket of a node X , is the union of X  X  X  parents, X  X  X  children and the parents of X  X  X  children. It can be shown that when the values of the variables in the Markov Blanket of the classification node are observed, the posterior probability distribution of the classification node is independent of all other vari-ables (nodes) that are not in the Markov Blanket [41]. Hence, all variables outside the Markov Blanket can be safely discarded because they will have no impact on the classification node and thus will not affect the classification accuracy. In this way, the Markov Blanket results in a natural form of variable selection. E.g. in Fig. 3, node A 1 will be part of the Markov Blanket because it is a parent of C , A 3 and A of the Markov Blanket and can be ignored for classification purposes, since it will not influence C for a fixed value of A 3 (which is known in a classification context).

The Markov Blanket feature selection algorithm has been applied to the data sets at a significance level of 1% and 5%, and will be referred to as MB.01 and MB.05. Note that if attribute selection is performed, it is applied prior to training and testing the classifiers. The feature selection algorithm has been implemented in the Causal Explorer package for Matlab [2]. 4.3. Bayesian network construction
For all the techniques discussed in Section 3, freely available software implementations have been used. The Naive Bayes classifier, TAN-classifier and the Augmented Naive Bayes classifiers have been implemented by Sacha, and exist in the form of Weka bindings [56], allowing to run the software from within the Weka Workbench [65]. In total, there are ten Augmented Naive Bayes classifiers, as a result of combining two quality measures (Local Leave One Out Cross Validation (LCV_LO) and Standard Bayesian measure (SB)) with five algorithms (itemized in Section 3). The K2 algorithm is directly avail-able in the Weka Workbench. The constraint based algorithm, also called Three Phase Dependency Analysis, is available in the application Powerpredictor , developed by Cheng [10]. The Max-Min Hill-Climbing algorithm is available for Matlab [2]. For inference in the networks generated by the MMHC algorithm, the Bayesian Net Toolbox for Matlab, developed by Murphy [47], has been used. In Table 3, an overview of all software implementations is given. Note that Logistic Regression and the Naive Bayes Classifier are included in this study as benchmarks for the Bayesian Network classifiers. 4.4. Measuring classifier performance
A variety of performance measures has been used to gauge the strength of different classifiers and to select the appropriate model [1,45]. In this study two measures are reported: the Area Under the Receiver Operating Characteristic Curve (AUROC, or briefly AUC), and the Maximum Profit (MP) criterion, recently introduced by Verbeke et al. [61].
 We tested 16 algorithms on five data sets, in combination with MB.05, MB.01, or without preceding Markov Blanket feature selection. This results in 240 values for AUC and 240 values for MP. Section 4.5 will explain how the statistical significance of differences between methods is analyzed. 4.4.1. Area under the receiver operating characteristic curve
Most classification techniques result in a continuous output. For instance in a customer churn predic-tion setting, a probability estimate between zero a nd one of being a churner is produced by the model. Depending on a threshold probability value, a customer will be classified as a churner or a non-churner. The receiver operating characteristic (ROC) curve displays the fraction of the identified churners by the model on the Y-axis as a function of one minus the fraction of identified non-churners. These fractions are dependent on the threshold probability. ROC curves provide an indication of the correctness of the predictions of classification models. In order to compare ROC curves of different classifiers regardless of the threshold value and misclassification costs, one often calculates the area under the receiver op-erating characteristic curve (AUROC or AUC). Assume that a classifier produces a score s = s ( X ) , with X the vector of attributes. For a BN classifier, the score s is equal to the probability estimate and F l ( s ) the corresponding cumulative distribution function. Then, it can be shown that AUC is defined as follows [38]: An intuitive interpretation of the resulting value is that it provides an estimate of the probability that a randomly chosen instance of class one is correctly rated or ranked higher by the classifier than a ran-domly selected instance of class zero (i.e., the probability that a churner is assigned a higher probability to churn than a non-churner). Note that since a pure random classification model yields an AUC equal to 0.5, a good classifier should result in a value of the AUC much larger than 0.5.
 4.4.2. Maximum profit criterion
A second performance measure that will be applied in this study is the MP criterion. In order to compare the performance of different classification models, this measure calculates the maximum profit that can be generated with a retention campaign using the output of a classification model. The profit generated by a retention campaign is a function of the discriminatory power of a classification model, and can be calculated as [49]: With:  X   X = profit generated by a customer retention campaign,  X  N = the number of customers in the customer base,  X   X  = the fraction of the customer base that is targeted in the retention campaign and offered an  X   X  0 = the fraction of all the operator X  X  customers that will churn,  X   X  = lift, i.e. how much more the fraction of customers included in the retention campaign is likely  X   X  = the cost of the incentive to the firm when a customer accepts the offer and stays  X   X  = the fraction of the targeted would-be churners who decide to remain because of the incentive  X  c = the cost of contacting a customer to offer him or her the incentive,  X  CLV = the customer lifetime value (i.e., the net present value to the firm if the customer is re- X  A = the fixed administrative costs of running the churn management program.
 Both the costs and the profits generated by a retention campaign are a function of the fraction  X  of included customers. Optimizing this fraction leads to the maximum profit that can be determined using the lift curve: Many studies on customer churn prediction modeling calculate the top-decile lift to compare the perfor-mance of classification models, i.e. the lift when including the ten percent of customers with the highest predicted probabilities to attrite. However, setting  X  = 10% is a purely arbitrary choice, and including ten percent of the customers generally leads to suboptimal profits and model selection, as shown by Verbeke et al. [61]. Since the ultimate goal of a company when setting up a customer retention campaign is to minimize the costs associated with customer churn, it is logical to evaluate and select a customer churn prediction model by using the maximum profit that can be generated as a performance measure. Whereas the AUC measures the overall performance of a model, the MP criterion evaluates the predic-tion model at the optimal fraction of clients to include in a retention campaign. To calculate the MP, the values of the parameters CLV ,  X  ,  X  ,and c in Equation 9 are taken equal to respectively e 200, 0.30, e 10 and e 1 based on values reported in the literature [8,49] and information from telecom operators. 4.5. Testing statistical significance
The aim of this study is to investigate how classification performance is affected by two specific factors, the type of Bayesian Network classifier and the use of Markov Blanket feature selection. A procedure described in Dem X ar [21] is followed to statistically test the results of the benchmarking experiments and contrast the levels of the factors. In a first step of this procedure the non-parametric Friedman test [26] is performed to check whether differences in performance are due to chance. The Friedman statistic is defined as: with R j the average rank of algorithm j =1 , 2 ,...,k over N data sets. Under the null hypothesis that no significant differences exist, the Friedman statistic is distributed according to  X  2 F with k  X  1 degrees of freedom, at least when N and k are big enough (e.g. Lehman and D X  X brera [42] give k  X  N&gt; 30 as criterion). This requirement is fulfilled in this study when comparing different classifiers ( N = 5  X  3 = 15 and k = 16 ) and when analyzing the impact of feature selection ( N = 5  X  16 = 80 and k = 3).
If the null hypothesis is rejected by the Friedman test, we proceed by performing the post-hoc Nemenyi average ranks differ by at least the critical difference equal to: with critical values q  X  based on the Studentized range statistic divided by with the best performing classifier the Bonferroni-Dunn test [23] is applied, which is similar to post-hoc Nemenyi but adjusts the confidence level in order to control the family-wise error for making k  X  1 instead of k ( k  X  1) / 2 comparisons. 5. Discussion of results
The results of the experiments are reported in Table 1. The left panel shows the AUC whereas the right panel shows the maximum profit criterion (MP), for each classifier-feature selection combination. The table displays the resulting measure for the five data sets discussed in Section 4. Moreover, the column significance of the results, the following notational convention has been adopted. The score or average rank of the best performing classifier is always underlined. The average rank of a classifier is bold if the performance is not significantly different from the best performing technique at a significance level of 5%. Techniques which are different at a 1% significance level are in italic script, whereas a classifier differing at a 5% but not at a 1% level is in normal script. The Bonferroni-Dunn test (see Section 4.5) has been used to compute the statistical significance indicated in Table 4.

In what follows, the impact of MB feature selection and the performance of the Bayesian Network classifiers will be analyzed. Next to AUC, the Maximum Profit criterion has been used to measure classification performance. The main conclusions will be based on the MP, since it gives a more accurate assessment of classification performance in a practical churn setting. After all, an operator is mostly interested in the fraction of the client base which will maximize his profit, i.e. those customers with a high churn probability. Typically, only a small proportion of all customers will be included in a retention campaign. 5.1. Classification performance
To analyze the impact of feature selection on classification performance, a Friedman test has been employed. When using AUC as performance measure, the p -value is 0.24, for MP it equals 0.85, both unambiguously implying that the feature selection does not significantly affect performance. This is positive, as it gives the opportunity to reduce the number of variables without a significant decrease in the predictive power of the resulting models. The complexity of the resulting models will be discussed in the next subsection.

The Friedman test is also applied to investigate whether the differences among classifiers are sig-nificant. The p -value with regards to the AUC measure is 6.53  X  10  X  13 and for MP it equals 5.77  X  10  X  4 , indicating that the differences among classifiers are statistically significant. Hence, a post-hoc Nemenyi test is performed. The outcome of this test is graphically illustrated for both performance metrics in Figs 4(a) and (b). The left end of the line segments indicate the average rank whereas the length is equal to the critical distance at a 1% level of significance, enabling the reader to compare techniques among each other, not only with the best method. 5 The vertical full line gives the cutoff for the 1% level, the dotted and dashed line for the 5% and 10% level whereas the critical distance is equal to 6.75, 5.96, and 5.56 for the the 1%, 5%, and 10% significance levels respectively. One can observe that AUC and MP lead to different rankings, although most of the techniques are not significantly different. AUC is more discriminative as opposed to MP, following from the fact that AUC is measuring classifier performance over the whole output range, whereas MP only looks at the optimal fraction of clients to include in a retention campaign and assesses prediction performance at this specific point. Thus, when using MP that none of the Bayesian Networks significantly outperforms the others, except for TPDA. This is an interesting conclusion, since it indicates that General Bayesian Network methods (except for TPDA) could be used, which are preferable as they lead to more compact and interpretable networks. It is also remarkable that the BN classifiers are not able to outperform logistic regression, a straightforward and fast technique. 5.2. Complexity and interpretability of the Bayesian networks
The discussion above focussed on the performance of the classifiers, which is only one aspect of a good churn prediction model. Also the complexity and interpretability of the resulting classifiers are key properties, since comprehensibility of the model is important as discussed in Section 2. Bayesian networks are appealing to practitioners, as they give an intuitive insight in the factors driving churn behavior and the dependencies among those factors. This applies less to the Naive Bayes classifier and the Augmented Naive Bayes classifiers, since they include, by nature, all or many variables and prohibit general network structures. General Bayesian networks, on the other hand, are more flexible and typically use less variables. As a result, Markov Blanket feature selection will be more useful in combination with (Augmented) Naive Bayes classifiers, since these do not contain a mechanism to get rid of redundant variables.

Figure 5 illustrates the network complexity by showing the network dimension and the number of nodes and arcs for each algorithm-feature selection combination, averaged over the data sets. The di-mension of a Bayesian Network is defined as the number of free parameters needed to fully specify the joint probability distribution encoded by the network, and is calculated as: with r i being the cardinality of variable X i and: with  X  X i the direct parent set for node X i . For logistic regression, which is included in the study as a benchmark algorithm, the number of nodes is equal to the number of attributes, the dimension (or number of free parameters) equals the number of attributes plus one, and the number of arcs is meaningless and therefore discarded for this algorithm.

The graphs show that feature selection reduces the number of nodes, arcs and dimension, as one would expect. The reduction of nodes and arcs is more substantial for the (Augmented) Naive Bayes networks, as mentioned before. An exception could be noticed for the TPDA algorithm where the complexity again increases for MB.01 after having dropped for MB.05. This could be attributed to the fact that the MB feature selection algorithm excluded a variable which had large explanatory power within the TPDA framework and in order to offset this loss, the algorithm has to include more variables than before. This case illustrates that one should be careful when applying feature selection prior to the use of a GBN algorithm. This category of methods already reduces the number of parameters by itself, so reducing the variables prior to training the classifier might be redundant at best or even worsen the result.
Moreover, one can observe that the Augmented Naive Bayes classifiers, which relaxed the TAN as-sumption, are able to reduce the number of nodes and arcs compared to TAN, without a loss in predictive power. Nevertheless, the networks are still too complex to be easily interpretable. The GBN algorithms reduce the complexity even more and contain around 5 to 7 variables, with the only exception of K2, creating very dense networks. 6 Figure 6 shows the network created for data set D2 by the MMHC al-gorithm (without prior input selection). D2 is a real life data set from an operator and contains a high number of variables. Nonetheless, the algorithm is able to withhold only 5 attributes to predict churn behavior. When looking at the network, it is very important to realize that the arcs do not necessarily imply causality, but they should rather be interpreted as correlation between the variables. For this net-work, one can observe that for instance the age of the current handset is correlated with the number of months in service and with churn behavior. The former relation could be explained by the fact that many operators offer a new mobile when signing a contract, whereas the latter could point to a motive for changing mobile phone operator, i.e. a promotional action at another operator. Such relations could be helpful for a company to identify red flags or warning signs for churn behavior. Moreover, it allows to check whether a churn prediction model is in line with current domain knowledge, increasing the credibility and applicability of those models. 6. Conclusions
Customer churn is becoming an increasingly important business analytics problem for telecom oper-ators. In order to increase the efficiency of customer retention campaigns, operators employ customer churn prediction models based on data mining techniques. These prediction models need to be accurate as well as compact and interpretable. This study investigates whether Bayesian Network techniques are appropriate for customer churn prediction.

In this paper, classification performance is measured with the Area under the Receiver Operating Char-acteristic Curve (AUC) and the Maximum Profit (MP) criterion. The results show that both performance measures lead to a different ranking of classification algorithms, even though not always statistically significant, and that AUC is more discriminative than the MP criterion. Whereas AUC measures perfor-mance over the whole customer base, the MP criterion only focusses on the optimal fraction of customers in order to maximize the effectiveness of a retention campaign. In a real life context, the MP criterion is preferred as it will maximize the profit for a telecom operator and therefore, the main conclusions will be based on this performance measure.

The results of the experiments show that Bayesian Network classifiers are not able to outperform tra-ditional logistic regression. However, their contribution lies in the fact that they offer a very intuitive insight into the dependencies among the explanatory variables. The study indicates that Augmented Naive Bayes methods do not lead to compact networks, whereas General Bayesian Network algorithms result in simple and interpretable networks. Thi s may aid practitioners in understanding the drivers behind churn behavior and in identifying warning signs for customer churn. Moreover, the Max-Min Hill-Climbing (MMHC) al gorithm was able to create a compact and comprehensible model without a statistically significant loss in classification performance, as compared to logistic regression and Aug-mented Naive Bayes techniques.

Furthermore, the impact of Markov Blanket (MB) feature selection was tested. The outcome indicates that a reduction of variables as a result of MB feature selection does not decrease the performance significantly. Especially for Augmented Naive Bayes networks it proves to be useful, as it decreases the number of attributes substantially. For General Bayesian Network classifiers on the other hand, MB feature selection is discouraged as the General Bayesian Network algorithms themselves already limit the network complexity, making a prior input selection redundant. Acknowledgements
We extend our gratitude to the Flemish Research Council for financial support (FWO postdoctoral research grant, Odysseus grant B.0915.09), and the National Bank of Belgium (NBB/10/006). References
