 In the era of big data, it is usually agreed that the more data we have, the better results we can get. However, for some domains that heavily depend on user inputs (such as recommender systems), the performance evaluation metrics are sensitive to the amount of noise introduced by users. Such noise can be from users who only wanted to explore the systems, and thus did not spend efforts to provide ac-curate inputs. Noise can also be introduced by the methods of collecting user ratings. In my dissertation, I study how user data can affect prediction accuracies and performances of recommendation algorithms. To that end, I investigate how the data collection methods and the life cycles of users affect the prediction accuracies and the performance of rec-ommendation algorithms.
 H.1 [ Information Systems ]: Personalization, Recommender systems; H.1 [ Human-centered computing ]: Collabora-tive filtering recommender system; noise; recommendation accuracy
For recommender systems, especially those that use col-laborative filtering algorithms, having sufficient data to build recommendation models is crucial. However, research has shown that noise exists in user ratings as evidenced by the inconsistencies observed when users rerated items [11]. Hence, more data may lead to more noise.

Noise in user rating data can be classified as natural noise and malicious noise [19]. Natural noise is noise generated by the methods recommender systems use to col-lect data about user preference. Malicious noise is noise deliberately injected into recommender systems for certain purposes.
 ACM 978-1-4503-2668-1/14/10 X   X  E 15 . 00 . http : //dx.doi.org/ 10 . 1145 / 2566486 . 2568012 .
To address natural noise in user rating, and thus to improve recommendation accuracies, researchers have pro-posed several frameworks to minimize the effect of natural noise on recommendation accuracies. Amatriain et al. [2, 3] proposed methods to quantify the noise in user ratings, and algorithms to reduce natural noise via user re-ratings. My previous work [18] suggested that we could improve the rating interfaces to reduce the noise introduced by users.
On the other hand, several researchers have investigated the effects of malicious data (thus malicious noise) on rec-ommender algorithms, and how recommender systems can deal with malicious data. Lam et al. [15] found that certain recommendation algorithms (such as item-item) were less affected by malicious data than others (such as user-user). Chirita et al. [5] proposed algorithms to detect and remove malicious data generated by malicious users, maintaining recommendation quality.

In my dissertation, I examine how to improve recom-mendation accuracies from a different angle. First, I look at methods to collect rating data, and the effects of those methods on recommendation quality. Second, I look at the life cycles of users, and their effects on the recommenda-tion quality. Thus, I seek to answer these following research questions: 1. How can we modify rating interfaces to reduce natural 2. Do we lose any information when we use the rating 3. Are the recommendation accuracies reduced when us-
Noise exists in user ratings and causes magic barrier [10], the minimum of prediction errors recommendation algo-rithms can obtain. To address the noise, researchers pro-posed approaches in user interface design, data analyses, and algorithms to detect and reduce noise [13, 18, 3, 19].
From the user interface perspectives, researchers looked into the literature for the process of preference construction and elicitation. They found that external factors such as anchoring effects and manipulated predictions had a strong influence on user preferences [1, 6]. They also found that users found it difficult and demanding to convert their pref-erences to the provided rating scales. This difficulty results in noise in user ratings [19].

From the data analytic and algorithmic approaches, re-searchers proposed several frameworks to estimate and min-imize the amount of natural noise appeared in user ratings. Nguyen et al. [18] proposed a framework to estimate the up-per limit of the amount of the true rating noise in a given set of ratings. They suggested that the upper limit was the con-ditional entropy between two sets of ratings and re-ratings. O X  X ahony et al. [19] proposed that we could estimate the amount of the natural noise in user ratings by computing the probability distributions of the consistency between the actual and predicted ratings. This consistency, in turn, was the normalized Mean Absolute Error between the actual and predicted ratings. Said et al. [21] proposed a framework based on the empirical risk minimization principle in sta-tistical learning to estimate the magic barrier, the optimal RMSE a recommendation algorithm can achieve.

Researchers also investigated the effect of deliberately in-jected malicious noise. O X  X ahony et al. [19] proposed a framework based on signal detection theory to detect users who purposely introduced malicious noise into recommender systems. Lam et al. [15], when studying different ways to inject malicious data, found that the item-item algorithm was more resistant to shilling attacks than the user-user al-gorithm.

Although researchers also investigated the effect of recom-mender systems on users such as [17, 20, 8, 9], there is less research on understanding the life-cycles of users in recom-mender systems, and their effects on prediction accuracies. Hence, in this this thesis, I investigate the effects of rating collecting methods, and the user life cycles on the prediction accuracies in recommender systems.

In the next section, I briefly discuss what I have done. I describe my on going research and future directions about the effect of user life-cycles on prediction accuracies.
To answer the first research question, in the rating sup-port interface project [18], I looked at how the methods of collecting user ratings affected the prediction accuracies. To that end, I developed three novel rating support interfaces: a tag interface, an exemplar interface, a tag + exemplar interface.

The tag interface provided users a personalized form of memory support, helping users remember what the movies being rated are about. The exemplar interface provided an anchoring support (rating support) by showing an exem-plar: the most similar movie (to the movie being rated) a user previously rated at the hovered rating star. The tag + exemplar interface provided both memory support and rating support by combining the features of the above two interfaces. With this interface, users recalled not only what movies were about, but also what movies they previously rated at a particular rating.

I ran a between-subject experiment from March 21st 2013 until April 25th 2013 with a baseline as a rating interface without any support. 38,586 ratings from 386 users were collected for the analyses. I found that the exemplar in-terface helped users rate more consistently. Moreover, the exemplar interface achieved the lowest magic barrier, and lowest actual RMSEs. It also had the lowest upper limit of rating noise. From the qualitative analyses (via survey ques-tions), I found that users liked all the rating interfaces that provided the rating support (as in the exemplar interface), since the users perceived these interfaces were more useful than the baseline interface.
After investigating how I could enhance rating interfaces to reduce the noise in the collected data, I have analyzed the effect of user life-cycles on the prediction accuracies. I observe that there are two types of users: committed users and uncommitted users. The committed users are those who plan to use recommender systems in a long term, and for their own benefits. Thus, their ratings are more stable and valuable than those of the uncommitted users, who perhaps want to just explore the systems. Thus, I pose a research question whether we can use only the rating data provided by committed users as representative data of the population when building prediction model. To help readers follow my proposal, I call the rating data provided by the committed users  X  X artial data X .

For this study, I analyze 57,339 MovieLens users collected from 2007-12-19 to 2013-11-20 1 . All these users had their first MovieLens visit in the said period.
 Before I define committed users and uncommitted users, I first define an activity session as a sequence of activities made by a user where the time difference of any two contin-uous activities is less than or equal to one hour 2 .
I compute the survival probability distribution, a well-known technique in survival analyses [12], as a proxy to de-termine how committed users were. The survival probabil-ity at the n th session is defined as the probability that users continue using the system after the n th session. Mathemati-cally, if P n is the survival probability at the n th session, and C n is the number of users who survive at the n th session, then: Figure 1 shows the survival probability until the 70 tivity sessions. I observe that the survival probability drops significantly in the first 10 th sessions, and asymptotically ap-proaches a stable straight line afterward. I classify users who had at least 10 sessions committed users, and users who did not uncommitted users. Table 1 shows that approximately only 10% of users who registered with MovieLens commit-ted at least 10 sessions. These committed users rated 20,221 distinct movies (30.1% higher than that of the uncommitted users). However, the committed users only provide 1,754,516 ratings (approximately 31% less than that of the uncommit-ted users).

I examine the trade-offs and benefits when using only rat-ing data of the committed users (partial data) to feed into recommendation algorithms. In the next section, I describe my analyses to answer the second and the third research questions mentioned in the introduction. Note that in this study, I only evaluate the prediction accuracies for these 5,748 committed users. I remove users who are our lab-mates and those whom MovieLens recruited for testing purposes.
I discuss why I choose one hour cut-off in an up-coming paper due to the limited space I have here. Table 1: The number of distinct users, distinct rated movies and the number ratings of two groups of users who committed at most 9 sessions (uncommit-ted users) and who committed at least 10 sessions (committed users).
In the section 3, I discussed the results for the first re-search question. In this section, I discuss my approach to address the second research question. First, I compute the Kullback-Leibler (KL) divergence. The KL divergence, de-noted as D KL ( P || Q ), measures how much information is lost when using the Q to approximate P [14]. The amount of in-formation lost can be translated as the similarity of the two distributions. Mathematically, we can express the Kullback-Leibler divergence as: where P and Q are two probability distributions.

The KL divergence is widely used to measure the simi-larity of any two distributions of data on various domains such as in detecting topics [16], building a user recommenda-tion framework in social tagging system [22], and (most rel-evant to my research) predicting recommender system per-formances [4].

In this study, I am interested in how much information is lost when I use the rating distribution of the committed users as an approximation of the rating distribution of all users (as shown in figure 2). Let P be the rating distribution of all users, and Q be the rating distribution of the commit-ted users. I measure the Kullback-Leibler divergence of Q from P. Based on formula 1, I compute the KL divergence, D
KL ( P || Q ), is about 0.008 bit. This 0.008 bit suggests that if I use the rating distribution of the committed users to ap-proximate the rating distribution of all users, it is required to use extra 0.008 bit to encode the distribution due to the
Figure 2: The rating distributions of two datasets. loss of information. However, Kluver et al. [13] suggested that there was approximately 1.7 bits per rating for a 10 rating-scale. Thus 0.008 is about 0.47% of bits per rating for a 10 rating scale.

With this small amount of percentage, I conclude that the amount of information lost when I use only the data of the committed users as representative data for the rating distribution of all users is not significant.
In this section, I discuss my third research question: how the performances of recommendation algorithms are on the different data sets.

To evaluate the performances, I use lenskit [7] to com-pute the RMSE and nDCG per user for 5,747 3 users who committed at least 10 sessions. I keep most of the default configurations of lenskit, except the RMSE and nDCG con-figurations. I compute per-user RMSE and nDCG instead of the aggregated RMSE and nDCG. I focus only on the following three algorithms: ItemItem, SVD, and PersMean. Table 2: The average RMSE per user for each algo-rithm when running different datasets. In the brack-ets are the standard deviation (std).

Tables 2 and 3 show the average RMSE, and average nDCG per user when I evaluate the performances of the three algorithms on the two data sets. I observe that there is no significant difference in the performances of the three algorithms (in both RMSE and nDCG measures). Further-more, as shown in table 4, I observe that the running times for both testing and building of all the algorithms when us-ing partial data are faster than when using all data 4 .
One user is removed from this analysis because he did not rate more than 15 movies, the minimum number of ratings required for the cold start process.
At this moment, I am not able to report the consumed memory since Lenskit does not generate a report for this metric. Table 3: The median nDCG per user for each al-gorithm when running different data sets. In the brackets are the standard deviation (std).
 Algorithms Building Time Testing Time All data &amp; ItemItem 922.29 58.13 All data &amp; SVD 349.19 9.32 All data &amp; PersMean 6.50 10.72 Partial data &amp; ItemItem 493.48 56.41 Partial data &amp; SVD 109.00 8.60 Partial data &amp; PersMean 3.30 10.89 Table 4: Building time and testing time (in second) reported by Lenskit when running with all data and the committed users X  (partial data).
I set out to better understand the effect of rating interfaces and the roles of user life-cycles on prediction accuracies. I find that enhancing a rating interface with rating support helps to reduce the noise in user ratings, and improve the prediction accuracies. I also find that the recommendation algorithms produce the same prediction quality with partial data as with all data. I also observe that using the rating distribution of the committed users to approximate the rat-ing distribution of all users does not lead to significant loss of information.

These findings suggest that we can use a substantially less data to achieve the same accuracy as when we use all available data, and that we can improve the rating interfaces to improve the quality of the rating data. Furthermore, our findings have an implication for practical uses. With the substantially less data, the algorithms can run faster, and consume less memory.

My work is not without limitations. First, due to the space limitation, I am only able to report the results on the subset of users of interests. In the future work, I plan to randomly select users, regardless of how committed they are evaluate the accuracy of the recommendation algorithms on these randomly selected users. Second, I have not explored the maximum amount of data I can ignore while keeping an adequate level of prediction accuracies. To discover this threshold, more data analyses must be done.
I thank my advisor, Professor Joseph A. Konstan, for his advice and guidances.
However, they must provide a sufficient ratings to pass the cold-start problem
