 University of Minnesota Cairo University University of Minnesota University of Minnesota short-term memory. But this kind of memory is known to have a severely constrained storage a model of syntactic processing that operates successfully within these severe constraints, by parsing )and mapping this representation to random variables in a Hierarchical Hidden Markov
Model, a factored time-series model which probabilistically models the contents of a bounded memory store over time. Evaluations of the coverage of this model on a large syntactically annotated corpus of English sentences, and the accuracy of a bounded-memory parsing strategy based on this model, suggest this model may be cognitively plausible. 1. Introduction
It is an interesting possibility that human syntactic processing may occur entirely within a general-purpose short-term memory. Like other short-term memory processes, syn-tactic processing is susceptible to degradation if short-term memory capacity is loaded, for example, when readers are asked to retain lists of words while reading (Just and Carpenter 1992); and memory of words and syntax degrades over time within and across sentences (Sachs 1967; Jarvella 1971), unlike semantics and discourse information about referents from other sentences (Ericsson and Kintsch 1995). But short-term memory is known to have severe capacity limitations of perhaps no more than three to four distinct elements (Miller 1956; Cowan 2001). These limits may seem too austere to process the rich tree-like phrase structure commonly invoked to explain word-order regularities in natural language.
 model, based on a right-corner transform X  X  reversible tree transform related to the left-corner transform of Johnson (1998a) X  X hat associates familiar phrase structure trees with the contents of a memory store of three to four partially completed constituents over time. Coverage results on the large syntactically annotated Penn Treebank corpus show a vast majority of naturally occurring sentences can be recognized using a mem-ory store containing a maximum of only three incomplete constituents, and nearly all sentences can be recognized using four, consistent with estimates of human short-term memory capacity.
 by transforming right-branching constituent structures into left-branching structures, allowing child constituents to be composed with parent constituents before either have been completely recognized. But because this composition identifies an incomplete child as the awaited portion of an incomplete parent, it implicitly predicts that this child constituent will be the rightmost (i.e., last) child of the parent, before this child has been completely recognized. Parsing accuracy results on the Penn Treebank using a Hierarchical Hidden Markov Model (Murphy and Paskin 2001) X  X ssentially a probabilistic pushdown automaton with a bounded pushdown store X  X how that this prediction can be reliably learned from training data.
 related models of human syntactic processing using a bounded memory store; Section 3 describes a Hierarchical Hidden Markov Model (HHMM) framework for statistical parsing using this bounded store of incomplete constituents; Section 4 describes the right-corner transform and how it relates conventional phrase structure to incomplete constituents in a bounded memory store; Section 5 describes an experiment to estimate the level of coverage of the Penn Treebank corpus that can be achieved using this transform with various memory limits, given a linguistically motivated binarization of this corpus; and Section 6 gives accuracy results of this bounded-memory model trained on this corpus, given that some amount of incremental prediction (as described earlier) must be involved. 2. Bounded-Memory Parsing
One of the earliest bounded-memory parsing models is that of Marcus (1980). This model maintains a bounded store of complete but unattached constituents as a buffer, and operates on them using a variety of specialized memory manipulation operations, deferring certain attachment decisions until the contents of this buffer indicate it is safe to do so. (In contrast, the model described in this article maintains a store of incom-plete constituents using ordinary stack-like push and pop operations, defined to allow constituents to be composed before being completely recognized.) The Marcus parser provides a bounded-memory explanation for human difficulties in processing garden path sentences: for example, the horse raced past the barn fell , with intended interpretation [ the horse [ RC (which was )raced past the barn]] fell (Bever 1970), in which raced seems like the main verb of the sentence until the word fell is encountered. But this explanation due to memory exhaustion is not compatible with observations of unproblematic parsing of sentences such as these when contextual information is provided in advance: for example, two men on horseback had a race; one went by the meadow, and the other went by the barn (Crain and Steedman 1985). 2 to reduce storage demands in incremental processing using Combinatorial Catego-rial Grammar (CCG), avoiding the need to maintain large buffers of complete but unattached constituents. The right-corner transform described in this article composes incomplete constituents in very much the same way, but CCG is essentially a compe-tence model, in that it seeks to unify lexical category representations used in processing with learned generalizations about argument structure, whereas the model described herein is exclusively a performance model, allowing generalizations about lexical ar-gument structures to be learned in some other representation, then combined with probabilistic information about parsing strategies to yield a set of derived incomplete constituents. As a result, the model described in this article has a freer hand to satisfy strict working memory bounds, which may not permit some of the alternative compo-sition operations proposed in the CCG account, thought to be associated with available prosody and quantifier scope analyses. 1 account of memory capacity limits in parsing ordinary phrase structure trees. The
Johnson-Laird and Abney and Johnson models adopt a left-corner parsing strategy, of which the right-corner transform introduced in this article is a variant, in order to bring memory usage for most parsable sentences to within seven or so active or awaited phrase structure constituents. This account may be used to explain human processing difficulties in processing triply center-embedded sentences like the rat that the cat that the dog chased killed ate the malt , with intended interpretation [ the dog] chased] killed] ate the malt (Chomsky and Miller 1963). But this explanation does not account for examples of triply center-embedded sentences that typically do not cause processing problems: [ NP that [ NP the food that [ him (Gibson 1991). Moreover, the apparent competition between comprehension of center-embedded object relatives and retention of unrelated words in general-purpose memory (Just and Carpenter 1992) suggests that general-purpose memory is (or at least, can be) used to store incomplete constituents during comprehension. This would predict three or four elements of reliable storage, rather than seven (Cowan 2001).
The transform-based model described in this article exploits a conception of chunking (Miller 1956) to combine pairs of active and awaited constituents from the Abney and Johnson analysis, connected by recognized structure, in order to operate within estimates of human short-term memory bounds.
 path and center-embedding difficulties, recent work has turned to explanations other than memory exhaustion for these phenomena. Lewis and Vasishth (2005) attribute processing errors to activation interference among stored constituents that have sim-ilar syntactic and semantic roles. Hale X  X  surprisal (2001) and entropic model (2006) link human processing difficulties to significant changes in the relative probability of competing hypotheses in incremental parsing, such that if activation is taken to be a mechanism for probability estimation, processing difficulties may be ascribed to the relatively slow speed of activation change within the brain (or to collapsing activation when probabilities grow too small, as in the case of garden path sentences). These models explain many processing difficulties without invoking memory limits, and are compatible with brain imaging evidence of increased cortical activity and recruitment of auxiliary brain areas during periods of increased uncertainty in sentence processing (Just and Varma 2007). But if interference or changing activation is posited as the source of processing difficulty, and delays are not linked to memory exhaustion per se, then these theories do not explain how (or whether) syntactic processing operates within general-purpose short-term memory.
 processing can be performed entirely within general-purpose short-term memory by using this memory to store unassimilated incomplete syntactic constituents, derived through a right-corner transform from basic properties of phrase structure trees. As a probabilistic incremental parser, the model described in this article is compatible with surprisal-based explanations of processing difficulties; it is, however, in some sense orthogonal, because it models a different dimension of resource allocation. The surprisal framework models allocation of processing resources (in this case, activation) among disjunctions of competing hypotheses, which are maintained for some amount of time in parallel, whereas the framework described here can be taken to model the allocation of processing resources (in this case, memory elements) among conjunctions of incompletely recognized constituents within each competing hypothesis. this view, there are two ways to simultaneously activate multiple concepts: disjunctively (sharing activation among competing hypotheses) and conjunctively (sharing activation among unassimilated constituents within a hypothesis). But only the inner conjunctive allocation corresponds to the familiar discretely bounded store of short-term memory as described by Miller (1956); the outer disjunctive allocation treats activation as a continuous resource in which like-valued pockets expand and contract as they are reinforced or contradicted by incoming observations. Indeed, it would be surprising if these two dimensions of resource allocation did not exist: the former, because it would contradict years of observations about the behavior of short-term memory; and the latter, because it would require neural activation spreading to be instantaneous and uniform, contradicting most neuropsychological evidence. Levy (2008) compares the allocation of activation in this kind of framework to the distributed allocation of resources in a particle filter (Gordon, Salmond, and Smith 1993), an approximate inference technique for probabilistic time-series models in which particles in a (typically fixed) reservoir are assigned randomly sampled hypotheses from learned transition probabilities, essentially functioning as units of activation. The model described in this paper qualifies this analogy by positing that each individual particle in this reservoir endorses a coherent hypothesis about the contents of a three-to four-element memory store at any given time, rather than about an entire unbounded phrase structure tree. 4 path effects as exceeding a bound of four complete but unattached constituents, or ex-plaining center embedding difficulties as exceeding a bound of seven active or awaited constituents) have been shown to underestimate human sentence processing capacity when equally complex but unproblematic sentences were examined. The hypothesis advanced in this article, that human sentence processing uses general-purpose short-term memory to store incomplete constituents as defined by a right-corner transform, leaves the explanation of several negative examples of unparsable garden path and cen-ter embedding sentences to orthogonal models of surprisal or interference. But in order to determine whether this right-corner memory hypothesis still underestimates human sentence processing capacity, a corpus study was performed on two complementary corpora of transcribed spontaneous speech and newspaper text, manually annotated with phrase structure trees (Marcus, Santorini, and Marcinkiewicz 1993). These spon-taneous speech and newspaper text corpora contain only attested positive examples of parsable sentences, but they may be considered complementary for this purpose because the complexity of spontaneous speech may somewhat understate human recog-nition capacity (potentially limiting it to the cost of spontaneously generating sentences in an unusual social context), and the complexity of newspaper text may somewhat overstate human recognition capacity (though it is composed and edited to be readable, it is still composed and edited off-line), so results from these corpora may be taken together to suggest generous and conservative upper bounds on human processing capacity. 3. Bounded-Memory Parsing with a Time Series Model
The framework adopted in this article is a factored HMM-like time series model, which maintains a probability distribution over the contents of a bounded set of random variables over time, corresponding to hypothesized stores of memory elements. The random variables in this store may be understood as simultaneous activations in a cog-nitive model (similar to the superimposed roles described by Smolensky and Legendre [2006]), and the probability distribution over these stores may be thought of as compet-ing pockets of activation, as described in the previous section. Some of these variables persist as elements of the short-term memory store, and some are transient as results of hypothesized compositions, which are estimated and immediately discarded or folded into the persistent store according to the dependencies in the model. The variables have values or contents (or fillers ) X  X n this case incomplete constituent categories X  X hat change over time, and although these values may be uncertain, the set of hypothesized contents of this memory store at any given point in time are collectively constrained to form a coherent (but possibly incomplete) syntactic analysis of a sentence. mimics a bounded-memory pushdown automaton (PDA), supporting simple push and pop operations on a bounded stack-like memory store. A time-series model is used here instead of an explicit stack machine, first because the probability model is well defined on a bounded memory store, and second because the plasticity of the random variables that mimic stack behavior in this model makes the model cross-linguistically attractive. By evoking additional random variables and dependencies, the model can be defined (or presumably, trained) to mimic other types of automata, such as extended pushdown automata (EPDAs) recognizing tree-adjoining languages with crossed and nested dependencies, as have been hypothesized for languages like Dutch (Shieber 1985). However, the remainder of this article will only discuss random variables and dependencies necessary to mimic a bounded stack pushdown automaton. 3.1 Hierarchical HMMs Hierarchical Hidden Markov Models (Murphy and Paskin 2001) are essentially Hidden
Markov Models factored into some fixed number of stack-like elements at each time step.
 consist of phones, words, or other hypothesized syntactic or semantic information), and observed states o t at corresponding time steps t (typically short, overlapping frames of an audio signal, or words or characters in a text processing application). A most likely sequence of hidden states  X  q 1 .. T can then be hypothesized given any sequence of observed states o 1 .. T : using Bayes X  Law (Equation 2) and Markov independence assumptions (Equation 3) to define a full P ( q 1 .. T | o 1 .. T ) probability as the product of a Language Model (  X  modeled using synchronized levels of stacked-u pcom ponent HMMs in an HHMM, analogous to a shift-reduce parser or pushdown automaton with a bounded stack.
HHMM transition probabilities are calculated in two phases: a  X  X educe X  phase (result-ing in an intermediate, transient final-state variable f t
HMMs terminate; and a  X  X hift X  phase (resulting in a persistent modeled state q which unterminated HMMs transition and terminated HMMs are re-initialized from their parent HMMs. Variables over intermediate f t and modeled q into sequences of depth-specific variables X  X ne for each of D levels in the HHMM hierarchy:
Transition probabilities are then calculated as a product of transition probabilities at each level, using level-specific  X  X educe X   X  F , d and  X  X hift X   X  6 or summed over all combinations of intermediate variables f store contents q 1 t ... q D t persist across time steps. with three depth levels is shown in Figure 1.
 vated. Independence across time points t (Equation 3) arise naturally from causality:
Any change to a memory store configuration to generate a configuration at time step t + 1 should depend only on the current memory store configuration at time step t ; memory operations should not be able to peek backward or forward in time to consult past or future memory stores. Independence across depth levels d (Equation 7) arise naturally from uncertainty about the structure between incomplete constituent chunks (this property of right-corner transform categories is elaborated in Section 4).
HMMs with probability distributions over recursive expansion, transition, and reduc-tion of states at each depth level. In the version of HHMMs used in this paper, each modeled variable is a syntactic state q d t  X  G  X  G (describing an incomplete constituent consisting of an active grammatical category from domain G and an awaited grammat-ical category from domain G  X  X or example, an incomplete constituent S/NP consisting of an active sentence S awaiting a noun phrase constituent NP); and each intermediate variable is a reduction or non-reduction state f d t  X  G  X  X  reduction of incomplete constituent q d t  X  1 to a complete right child constituent of some grammatical category from domain G , or a non-reduction of q as defined in Section 4). An intermediate variable f d t at depth d may indicate reduction or non-reduction according to  X  F-Reduction , d if there is a reduction at the depth level immediately below d , but must indicate non-reduction ( f there is no reduction below: 6 where f D + 1 t = 1 and q 0 t = ROOT .
 where f D + 1 t = 1 and q 0 t = ROOT . This model is conditioned on final-state intermediate variables f d t and f d + 1 t at and immediately below each HHMM level. If there is no re-duction immediately below a given level (the first case provided), it deterministically copies the current HHMM state forward to the next time step. If there is a reduction immediately below the current level but no reduction at the current level (the second case provided), it transitions the HHMM state at the current level, according to the above), it re-initializes this state given the state at the level above, according to the ing examples, for example (in the experiments described in this article), using relative frequency estimation. The overall effect is that higher-level HMMs are allowed to transition only when lower-level HMMs terminate. An HHMM therefore behaves like a probabilistic implementation of a shift X  X educe parser or pushdown automaton with a bounded stack, where the maximum stack depth is equal to the number of depth levels in the HHMM hierarchy. 4. Right-Corner Transform and Incomplete Constituents
The model described in this article recognizes trees in a right-corner transformed representation to minimize usage of a bounded short-term memory store. This right-corner transform is a variant of the left-corner transform described by Johnson (1998a), but whereas the left-corner transform changes left-branching structure into right-branching structure, the right-corner transform changes right-branching structure into 8 left-branching structure. Recognition using this transformed grammar, extracted from a transformed corpus, is similar to recognition using a left-corner parsing strategy (Aho and Ullman 1972). This kind of strategy was shown to reduce memory requirements for parsing sentences with mainly left-or right-recursive phrase structure to fewer than seven active or awaited constituent categories (Abney and Johnson 1991). This is within
Miller X  X  (1956) estimate of human short-term memory capacity (if memory elements store individual categories), whereas parsing heavily center-embedded sentences (known to be difficult for human readers) would require seven or more elements at the frontier of this capacity.
 as three or four distinct items (Cowan 2001), with longer estimates of seven or more possibly due to the human capacity to chunk remembered items into associated groups (Miller 1956). The right-corner strategy described in this paper therefore assumes constituent categories can similarly be chunked into incomplete constituents A / B formed by pairing an active category A with an awaited category B somewhere along the active category X  X  right progeny (so, for example, a transitive verb may become an incomplete constituent VP/NP consisting of an active verb phrase lacking an awaited noun phrase yet to come). 7 These chunked incomplete constituent categories A and B are naturally related through fixed contiguous phrase structure between them, established during the course of parsing prior to the beginning of B , and these incomplete constituents can be composed with other incomplete constituents B / C to form similarly related category pairs A / C .
 have contiguous string yields, so they correspond to the familiar notion of text chunks used in shallow parsing approaches (Hobbs et al. 1996). For example, a hypothesized memory store may contain incomplete constituents S/NP (a sentence without a noun phrase), followed by NP/NN (a noun phrase lacking a common noun), with cor-responding string yields demand for bonds propped up and the municipal , respectively, forming a complete contiguous segmentation of a sentence at any point in processing.
Although these two chunks could be composed into an incomplete constituent S/NN, doing so at this point would close off the possibility of introducing another constituent between these two, containing the recognized noun phrase as a left child (e.g., demand for bonds propped up [ NP [ NP the municipal bonds] X  X  prices] ).
 trees does not have the power to eliminate the bounds of a memory store, however. In a larger cognitive model, syntactic processing is assumed to occur as part of an interactive semantic interpretation process, in which referents of constituents are calculated as these constituents are recognized, and are used to constrain subsequent processing decisions (Tanenhaus et al. 1995; Brown-Schmidt, Campana, and Tanenhaus 2002).
The chunked category pairs A and B in these incomplete constituents A / B result from successful compositions of other such constituents earlier in the recognition process, which means that the relationshi pbetween the referents of A and B is known and fixed in any hypothesized incomplete constituent. But syntactic and semantic relationships between chunks in a hypothesized memory store are unspecified. Chunking beyond the level of incomplete constituents would therefore involve grouping referents whose interrelations have not necessarily been established by the parser. Because the set of referents is presumably much larger than the set of syntactic categories, one may assume there are real barriers to reliably chunking them in the absence of these fixed relationships.
 to different incomplete constituents) could be grouped together as chunks. But for simplicity, this article will assume a very strict condition that only a single incomplete constituent can be stored in each short-term memory element. Experimental results described in Section 5 suggest that a vast majority of English sentences can be recog-nized within these human-like memory bounds, even with this strict condition on chunking. If parsing can be performed in bounded memory under such strict condi-tions, it can reasonably be assumed to operate at least as well under more permissive circumstances, where some amount of syntactically-unrelated referential chunking is allowed.
 strategy (Roark 2001; Henderson 2004). But these systems generally kee plarge numbers of constituents open for modifier attachment in each hypothesis. This allows modifiers to be attached as right children of any such open constituent. But if any number of open constituents are allowed, then either the assumption that stored elements have fixed syntactic (and semantic) internal structure will be violated, or the assumption that syntax operates within a bounded memory store will be violated, both of which are psy-cholinguistically attractive as simplifying assumptions. The HHMM model described in this article upholds both the fixed-element and bounded-memory assumptions by hypothesizing fixed reductions of right child constituents into incomplete parents in the same memory element, to make room for new constituents that may be introduced at a later time. These in-element reductions are defined naturally on phrase structure trees as the result of aligning right-corner transformed constituent structures to sequences of random variables in a factored time-series model. The success of this predictive strategy in corpus-based coverage and accuracy results described in Sections 5 and 6 suggests that it may be plausible as a cognitive model.
 as possible, by maintaining the option of undoing them when necessary (Stevenson 1998). This seems unattractive in the context of an interactive semantic model, however, where syntactic constituents and semantic referents are composed in tandem, because potentially very rich referential constraints introduced by composing a child constituent into a parent would have to be systematically undone. An interesting possibility might be that the appearance of syntactic restructuring may arise from a collection of hypoth-esized stores of syntactically fixed incomplete constituents, pursued in parallel. The results presented in this article suggest that this mechanism is possible, but these two possibilities might be very difficult to distinguish empirically.
 introduced in parsing only at the point in incremental recognition at which they can be associated with a head word (Gibson 1991: Pritcher 1991: Gorrell 1995). In typically head-initial languages such as English, incomplete constituents derived from these head-driven models resemble those derived from a right-corner transform. But head-driven incomplete constituents do not appear to obey general-purpose memory bounds in head-final languages such as Japanese, and do not appear to obey attachment prefer-10 ences predicted by a head-driven account (Kamide and Mitchell 1999), favoring a pre-head attachment account, as a right-corner transform would predict. 4.1 Tree Transforms Using Rewrite Rules
The incomplete constituents used in the present model are defined in terms of tree transforms, which consist of recursive operations that change tree structures into other tree structures. These transforms are not cognitive processes X  X yntax in this model is learned and used entirely as time-series probabilities over random variable values in the memory store. The role of these transforms is as a means to associate sequences of configurations of incomplete constituents in a memory store with linguistically familiar phrase structure representations, such as those studied in competence models or found in annotated corpora.
 rules applied iteratively to each constituent of a source tree, from leaves to root, and from left to right among siblings, to derive a target tree. These rewrites are ordered; when multiple rewrite rules apply to the same constituent, the later rewrites are applied to the results of the earlier ones. 9 For example, the rewrite could be used to iteratively eliminate all binary-branching nonterminal nodes in a tree, except the root.
 of applying a transform to a tree, then applying the reverse of that transform to the resulting tree, will be the original tree itself. In general, a transform can be reversed if the direction of its rewrite rules is reversed, and if each constituent in a target tree matches a unique rewrite rule in the reversed transform. The fact that not all rewrites can be unambiguously matched to HHMM output means that parse accuracy must be evaluated on partially-binarized gold-standard trees, in order to remove the effect of this ambiguous matching from the evaluation. This will be discussed in greater detail in Section 6. 4.2 Right-Corner Transform Using Rewrite Rules
Rewrite rules for the right-corner transform are shown here, first to flatten out right-recursive structure, then to replace it with left-recursive structure,
Here, the first two rewrite rules are applied iteratively (bottom-up on the tree) to flatten all right recursion, using incomplete constituents to record the original nonterminal ordering. The second rule is then applied to generate left-recursive structure, preserving this ordering. Note that the last rewrite leaves a unary branch at the leftmost child of each flattened node. This preserves the simple category labels of nodes that correspond directly to nodes in the original tree, so the original tree can be reconstructed when the right-corner transform concatenates multiple right-recursive sequences into a single left-recursive sequence.
 property of this transform is that it is reversible. Rewrite rules for reversing a right-corner transform are simply the converse of those shown here. The correctness of this can be demonstrated by dividing a tree into maximal sequences of right-recursive branches (that is, maximal sequences of adjacent right children). The first two  X  X latten-ing X  rewrites of the right-corner transform, applied to any such sequence, will replace the right-branching nonterminal nodes with a flat sequence of nodes labeled with slash categories, which preserves the order of the nonterminal category symbols in the original nodes. Reversing this rewrite will therefore generate the original sequence of nonterminal nodes. The final rewrite similarly preserves the order of these nonterminal symbols while grouping them from the left to the right, so reversing this rewrite will reproduce the flattened tree. 12 4.3 Mapping Trees to HHMM Derivations
Any tree can be mapped to an HHMM derivation by aligning the nonterminals with q categories. First, it is necessary to define rightward depth d , right index position t ,and final (rightmost) child status f d t + 1 , for every nonterminal node A in a tree, where
Any right-corner transformed tree can then be annotated with these values and rewrit-ten to define labels and final-state values for every combination of d and t covered by the tree. This is done using the rewrite rule to replace unary branches with f d t + 1 flags, and to copy stacked-up left child constituents over multiple time steps, while lower-level (right child) constituents are being recognized. The dashed line on the right side of the rewrite rule represents the variable number of time steps for a stacked-up higher-level constituent (as seen, for example, in time steps 4 X 7 at depth 1 in Figure 3). Coordinates d , t  X  D ,and T that have f d the tree are assigned label  X   X   X  X nd f d t + 1 = 1 .
 and f d t for each depth d and time step t of the HHMM (see Figure 3). Probabilities for these values directly. Like the right-corner transform, this mapping is reversible, so q and f d t values can be taken from a hypothesized most likely sequence and mapped back 14 to trees (which can then undergo the reverse of the right-corner transform to become ordinary phrase structure trees). Inspection of this rewrite rule will reveal the reverse of this transform simply involves deleting unary-branching sequences that differ only in the value of t and restoring unary branches when f d t + 1 that the categories on the stack at any given time ste pre present a segmentation of the input up to that time step. For example, in Figure 3 at t = 12 the stack contains a sentence lacking a verb phrase: S/VP ( strong demand for ...b onds ), followed by a verb projection lacking a particle: VBN/PRT ( propped ). 4.4 Comparison with Left-Corner Transform
A right-corner transform is used in this study, rather than a left-corner transform, mainly because the right-corner version coincides with an intuition about how incom-plete constituents might be stored in human memory. Stacked-up constituents in the right-corner form correspond to chunks of words that have been encountered, rather than hypothesized goal constituents. Intuitively, in the right-corner view, after a sen-tence has been recognized, the stack memory contains a complete sentential constituent (and some associated referent). In the left corner view, on the other hand, the stack mem-ory after a sentence has been recognized contains only the lower-rightmost constituent in the corresponding phrase structure tree (see Figure 4). This is because a time-order alignment of a left-corner tree to elements in a bounded memory store corresponds to a top-down traversal of the tree, whereas a time-order alignment of a right-corner tree to elements in a bounded memory store corresponds to a bottom-up traversal of the tree. If referential semantics are assumed to be calculated in tandem (as suggested by the Tanenhaus et al. [1995] results), a top-down traversal through time requires some effort to reconcile with the traditional compositional semantic notion that the meanings of constituents are composed from the meanings of their parts (Frege 1892). 4.5 Comparison with CCG
The incomplete constituent categories generated in the right-corner transform have the same form and much of the same meaning as non-constituent categories in a CCG (Steedman 2000). 10 Both CCG operations of forward function application: and forward function composition: appear in the branching structure of right-corner transformed trees. Nested operations can also occur in CCG derivations: as well as in right-corner transformed trees (using underscore delimiters to denote sequences of constituent categories, described in Section 5.1):
There are also correlates of type-raising (unary branches introduced by the right-corner transform operations described in Section 4): operations of backward function application or composition:
This has two consequences. First, right-corner transform models do not introduce am-biguity between type-raised forward and backward operations, as CCG derivations do.
Second, because leftward dependencies (as between a verb and its subject in English) cannot be incorporated into lexical categories, right-corner transform models cannot be taken to explicitly encode argument structure, as CCGs are. The right-corner transform model described in this article is therefore perhaps better regarded as a performance model of processing, given subcategorizations specified in some other grammar (such as in this case the Treebank grammar), rather than a constraint on grammar itself. 16 4.6 Comparison with Cascaded FSAs in Information Extraction
Hierarchies of weighted finite-state automata (FSA) X  X quivalent HMMs may also be viewed as probabilistic implementations of cascaded FSAs, used for modeling syntax in information extraction systems such as FASTUS (Hobbs et al. 1996). Indeed, the left-branching sequences of transformed constituents recognized by this model (as shown in Figure 3) bear a strong resemblance to the flattened phrase structure representations recognized by cascaded FSA systems, in that most phrases are consolidated to flat sequences at one hierarchy level. This flat structure is desirable in cascaded FSA systems because it allows information to be extracted from noun or verb phrases using straight-forward pattern matching rules, implemented as FSA-equivalent regular expressions. using regular expression pattern-matching rules. It also has a fixed number of levels and linear-time recognition complexity. But unlike FASTUS , the model described here can produce X  X nd can be trained on X  X omplete phrase structure trees (accessible by reversing the transforms described previously). 5. Coverage
The coverage of this model was evaluated on the large Penn Treebank corpus of syntactically annotated sentences from the Switchboard corpus of transcribed speech (Godfrey, Holliman, and McDaniel 1992) and the Wall Street Journal (Marcus, Santorini, and Marcinkiewicz 1993). These sentences were right-corner transformed and mapped to a time-aligned bounded memory store as described in Section 4 to determine the amount of memory each sentence would require. 5.1 Binary Branching Structure
In order to obtain a linguistically plausible right-corner transform representation of incomplete constituents, the corpus is subjected to another pre-process transform to introduce binary-branching nonterminal projections, and fold empty categories into nonterminal symbols in a manner similar to that proposed by Johnson (1998b) and Klein and Manning (2003). This binarization is done in such a way as to preserve linguistic intuitions of head projection, so that the depth requirements of right-corner transformed trees will be reasonable approximations to the working memory requirements of a human reader or listener.
 consisting of one subordinate head projection and one argument or modifier, for example: The selection of head constituents is done using rewrite rules similar to the Magerman-
Black head rules (Magerman 1995). Any new constituent created by this process is assigned the label of the subordinate head projection. The subordinate projection may be the left or complete list of head-projection rewrite rules is provided in Appendix A. terminals appended with a  X -LIST X  suffix:
This right-branching decomposition of conjoined lists is motivated by the general preference in English toward right branching structure, and the distinction of right children as  X -LIST X  categories is motivated by the asymmetry of conjunctions such as and and or generally occurring only between constituents at the end of a list, not at the beginning. (Thus, in decomposing coffee, tea or milk, the words tea or milk form an NP-LIST constituent, whereas the words coffee, tea do not.) may arise from this removal. In the case of empty constituents representing traces, the extracted category label is annotated onto the lowest nonterminal dominating the trace using the suffix  X -extr X , X  where  X  X  X  is the category of the extracted constituent. To preserve grammaticality, this annotation is then passed up the tree and eliminated when a wh-, topicalized, or other moved constituent is encountered, in a manner similar to that used in Head-driven Phrase Structure Grammar (Pollard and Sag 1994), but this does not affect branching structure.
 processed Treebank. All remaining super-binary branches are  X  X ominally X  decomposed into right-branching structures by introducing intermediate nodes, each with a label concatenated from the labels of its children, delimited by underscores:
This decomposition is  X  X ominal X  in that the concatenated labels leave the resulting bi-nary branches just as complex as the original n -ary branches prior to this decomposition.
It is equivalent to leaving super-binary branches intact and using dot rules in parsing 18 (Earley 1970). This decomposition therefore does nothing to reduce sparse data effects in statistical parsing. 5.2 Coverage Results
Sections 2 and 3 (the standard training set) of the Penn Treebank Switchboard corpus were binarized as described in Section 5.1, then right-corner transformed and mapped to elements in a bounded memory store as described in Section 4. Punctuation added by transcribers was removed. Coverage of this corpus, in sentences, for a recognizer using right-corner transform chunking with one to five levels of stack memory, is shown in
Table 1. These results show that a simple syntax-based chunking into incomplete con-stituents, using the right-corner transform defined in Section 4 of this article, allows a vast majority of Switchboard sentences (over 99%) to be recognized using three or fewer elements of memory, with no sentences requiring more than five elements, essentially as predicted by studies of human short-term memory.
 speech or edited text, it is possible that coverage results on these data may under-estimate processing requirements, due to the preponderance of very short sentences and sentence fragments in spontaneous speech (for example, nearly 30% of sentences in the Switchboard corpus are only one word long). It may also be argued that coverage results on this corpus more accurately reflect the complexity of speech planning under somewhat awkward social circumstances (being asked to start a conversation with a stranger), which may be more cognitively demanding than recognition. For these reasons, the right-corner transform chunking was also evaluated on Sections 2 X 21 (the standard training set) of the Penn Treebank Wall Street Journal (WSJ) text corpus (see Table 2, column 1).
 requirements than Switchboard, with only 93% of sentences recognizable using three or fewer memory elements. But much of this increase is due to arguably arbitrary treebank conventions in annotating punctuation (for example, commas between phrases are attached to the leftmost phrase: ( [Pierre Vinken ...[61 years old] ,] joined ... ) which can lead to psycholinguistically implausible analyses in which phrases (in this case 61 years old ) are center-embedded by lone punctuation marks on one side or the other.
In general, branching structure for punctuation can be difficult to motivate on linguistic grounds, because punctuation marks do not have lexical projections or argument structure in most linguistic theories. In spoken language, punctuation corresponds to pauses or patterns of inflection, distributed throughout an utterance. It therefore seems questionable to account for punctuation marks in a psycholinguistic model as explicit composable concepts in a memory store. In order to counter possible undesirable effects of an arbitrary branching analysis of punctuation, a second evaluation of the model was performed on a version of the WSJ corpus with punctuation removed. without punctuation, using the right-corner transformed trees just described, shows that 97.66% of trees can be recognized using three hidden levels, and 99.96% can be recognized using four, and again (similar to the Switchboard results), no sentences require more than five remembered incomplete constituents. Table 2, column 3, shows similar results for a left-corner transformed corpus, using left-right reflections of the rewrite rules presented in Section 4.
 four elements across a wide variety of tasks. It is therefore not surprising to find a similar limit in the memory required to parse the Treebank, assuming elements corresponding to right-corner-transformed incomplete constituents.
 both corpora, suggesting that a three-to four-element limit may be soft, and can be relaxed for short durations. Indeed, all quintuply embedded constituents were only a few words long. Interestingly, many of the most heavily embedded words seemed to strongly co-occur, which may suggest that these words arise from fixed expressions and are not compositional. For example, Figure 5 shows one of the 13 phrase structure trees in the Switchboard corpus which require five stack elements in right-corner parsing. The complete sentence is:
If the construction there  X  X  NP AP in this sentence is parsed non-compositionally as a single expression (and thus is rendered left-branching by the right-corner transform as defined in Section 4), the sentence could be parsed using only four memory elements. confounds explanations of the center-embedding difficulties as directly arising from stack limits in a left-corner (or right-corner) parser (Abney and Johnson 1991). It is also interesting to note that three of the incomplete constituents in this example are recursively nested or self-embedded instances of sentential projections, essentially with 20 the same category, similar to the center-embedded constructions which human readers found difficult to process. This suggests that restrictions on self-embedding of identical constituent categories would also fail to predict readability.

Although the five-element sentences found in the Treebank use mostly common phrase structure rules, problematic center-embedded sentences like the salmon the man the dog chased smoked fell may cause difficulty simply because they are examples of an unusual construction: a nested object relative clause. The fact that this is an unusual construction may in turn be a result of the fact that speakers tend to avoid nesting object relative clauses because they can lead to memory exhaustion, though such constructions may become readable with practice. 6. In-Element Composition Ambiguity and Parsing Accuracy
The right-corner transform described in Section 4 saves memory because it transforms any right-branching sequence with left-child subtrees into a left-branching sequence of incomplete constituents, with the same sequence of subtrees as right children. The left-branching sequences of siblings resulting from this transform can then be composed bottom-u pthrough time by re placing each left child category with the category of the resulting parent, within the same memory element (or depth level). For example, in
Figure 6(a) a left-child category NP/NP at time t = 4 is composed with a noun new of category NP/NNP (a noun phrase lacking a proper noun yet to come), resulting in a new parent category NP/NNP at time t = 5 replacing the left child category NP/NP in the topmost d = 1 memory element.
 use in processing descendants of this composed constituent, yielding the human-like memory demands reported in Section 5. But whenever an in-element composition like this is hypothesized, it isolates an intermediate constituent (in this example, the noun phrase new york city ) from subsequent composition. Allowing access to this intermediate constituent X  X or example, to allow new york city to become a modifier of bonds , which itself becomes an argument of for  X  X equires an analysis in which the intermediate constituent is stored in a separate memory element, shown in Figure 6(b). This creates a local ambiguity in the parser (in this case, from time step t = 4) that may have to be propagated across several words before it can be resolved (in this case, at time step t = 7). This is essentially an ambiguity between arc-eager (in-element) and arc-standard (cross-element) composition strategies, as described by Abney and Johnson (1991). In contrast, an ordinary (purely arc-standard) parser with an unbounded stack would only hypothesize analysis (b), avoiding this ambiguity. 12 statistical model to predict when in-element (arc-eager) compositions will occur, in addition to hypothesizing parse trees. The model encodes a mixed strategy: with some probability arc-eager or arc-standard for each possible expansion. Accuracy results on a right-corner HHMM model trained on the Penn Wall Street Journal Treebank suggest that this kind of optionally arc-eager strategy can be reliably statistically learned. 6.1 Evaluation
In order to determine whether a memory-preserving parsing strategy, like the optionally arc-eager strategy, can be reliably learned, a baseline Cocke-Kasami-Younger (CKY) parser and bounded-memory right-corner HHMM parser were evaluated on the stan-dard Penn Treebank WSJ Section 23 parsing task, using the binarized tree set described in Section 5.2 (WSJ Sections 2 X 21) as training data. Training examples requiring more 22 than four stack elements were excluded from training, in order to avoid generating inconsistent model probabilities (e.g., from expansions that could not be re-composed within the bounded memory store).
 the binarization, right-corner, and time-series mapping transforms described in Sec-tions 4 and 5. But some of the binarization rewrites cannot be completely reversed, because they cannot be unambiguously matched to output trees. Automatically derived lexical projections below the annotated phrase level (e.g., binarizations of base noun phrases) can be completely reversed, because the derived categories are character-istically labeled with terminal symbols. So, too, can the conjunction and  X  X ominal X  binarizations described in Section 5.1, because they can be identified by characteristic  X -LIST X  and underscore delimiters. But automatically derived projections above the annotated phrase level cannot be reliably identified in parser output (for example, an intermediate projection  X  X  PP S X  may or may not be annotated in the corpus). In order to isolate the evaluation from the effects of these ambiguous matchings, the evaluation was performed using trees in a partially binarized format, obtained by reversing only those rewrites that result in unambiguous matches. Evaluating on this partially bina-rized data does not seem to unfairly increase parsing performance compared to other published results X  X uite the contrary: an evaluation using the state-of-the-art Charniak (2000) parser scores about half a point worse on labeled F-score (89.3% vs. 89.9%) when its hypotheses and gold standard trees are converted into this format. (POS) model using relative frequency estimates from the training set, backed off to a discriminative (decision tree) model conditioned on the last five letters of each word, normalized over unigram POS probabilities. The CKY baseline and HHMM results were obtained by training and evaluating on binarized trees, which is a necessary condition for the right-corner transform. The CKY baseline results appear to be better than those for a baseline probabilistic context-free grammar (PCFG) system reported by Klein and
Manning (2003) using no modifications to the corpus, and no parent or sibling condi-tioning (see Table 3, top) because the binarization process allows the parser to avoid some sparse data effects due to large flat branching structures in the Treebank, resulting in improved parsing accuracy. Klein and Manning note that applying linguistically motivated binarization transforms can yield substantial improvements in accuracy X  X s much as nine points, in their study (in comparison, binarization only seems to improve accuracy by about seven points above an unmodified baseline in the present study). But the Klein and Manning results for binarization are provided only for models already augmented with Markov dependencies (that is, conditioning on parent and sibling categories, analogous to HHMM dependencies), so it was not possible to compare to a binarized and un-Markovized benchmark.
 trees (modulo right-corner and variable-mapping transforms) were substantially bet-ter than binarized CKY, most likely due to the expanded HHMM dependencies on previous ( q d t  X  1 ) and parent ( q d  X  1 t ) variables at each q probabilities may be defined in terms of three category symbols A , B ,and C : P ( A
BC | A ); whereas some of the HHMM probabilities are defined in terms of five category labels: P ( A / B | C / D , E ) (transitioning from incomplete constituent C / D to incomplete constituent A / B in the context of an expanding category E ). This increases the number of free parameters (estimated conditional probabilities) in the model, not to the point of sparsity; this is similar to the effect of horizontal Markovization (con-ditioning on the sibling category immediately previous to an expanded category) and vertical Markovization (conditioning on the parent of an expanded category) commonly used in PCFG parsing models (Collins 1999).
 in error) is comparable to that reported by Klein and Manning for parent and sibling dependencies (first-order vertical and horizontal Markovization) over a baseline PCFG without binarization (17.5% reduction in error). However, because it is not possible to run the HHMM parser without binarization, and because Klein and Manning do not report results for binarization transforms in the absence of parent and sibling
Markovization, it is potentially misleading to compare the results directly. For example, it is possible that the binarization transforms described here may have performance-optimizing effects that are latent in the binarized PCFG, but are brought out in HHMM parsing.
 rable to that reported for state-of-the-art cubic-time parsers (with no constant bounds 24 on processing storage) using similar configurations of conditioning information, that is, without lexicalization or smoothing.
 formed grammars, and also reports results for parsing with and without parent and sibling Markovization. Again the performance is comparable under similar conditions (Table 3, bottom).
 was selected in order to compare the performance of the bounded-memory model, which predicts in-element or cross-element composition, with that of conventional broad-coverage parsers, which also maintain large beams. With better modeling and vastly more data from which to learn, it is possible that the human processor may need to maintain far fewer alternative analyses, or perhaps only one, conditioned on a lookahead window of observations (Henderson 2004). 15 sion and transition probabilities for each q d t on only the portion of the parent category following the slash (that is, only A 2 of A 1 / A 2 ), in order to avoid sparse data effects.
Examples requiring more than four stack elements were excluded from training. This is because in the basic relative frequency estimation used here, training examples are depth-specific. Because the (unpunctuated) training set contains only about a dozen sentences requiring more than four depth levels, each occupying that level for only a few words, the data on which the fifth level of this model would be trained are very sparse. Models at greater stack depths, and models depending on complete parent cate-gories (or grandparent categories, etc., as in state-of-the-art parsers) could be developed using smoothing and backoff techniques or feature-based log-linear models, but this is left for later work (see Section 7). 7. Conclusion
This article has described a model of human syntactic processing that recognizes com-plete phrase structure trees using only a small store of memory elements of limited complexity. Sequences of hypothesized contents of this memory store can be mapped to and from conventional phrase structure trees using a reversible right-corner transform.
If this syntactic processing model is combined with a bounded-memory interpreter (Schuler, Wu, and Schwartz 2009), however, allowing the contents of this store to be incrementally interpreted within the same bounded memory, it stands to reason that complete, explicit phrase structure trees would not need to be constructed at any time in processing, in keeping with experimental results showing similar lack of retention of words and syntactic structure during human processing (Sachs 1967; Jarvella 1971). ory elements within this framework provides nearly complete coverage of the Penn
Treebank Switchboard and WSJ corpora, consistent with recent estimates of general-purpose short-term memory capacity. This suggests that, unlike some earlier mod-els, the hypothesis that human sentence processing uses general-purpose short-term memory to store incomplete constituents, as defined by a right-corner transform, does not seem to substantially underestimate human processing capacity. Moreover, despite additional predictions that must take place within this model to manage parsing in such close quarters, preliminary accuracy results for an unlexicalized, un-smoothed version of this model, using only a four-element memory store, show close to 84% recall and precision on the standard parsing evaluation. This result is comparable to that reported for state-of-the-art cubic-time parsers (with no constant bounds on processing storage) using similar configurations of conditioning information, namely, without lexicalization or smoothing.
 following evidence that garden path and center-embedding processing difficulties are caused by interference or local probability estimates rather than encounters with mem-ory capacity limits. But this does not mean that memory store capacity and probabilistic explanations of processing difficulty are completely independent. Probability estima-tion seems likely to be dependent on structural information from the memory store (for example, incomplete object relative clauses seem to be very improbable in the context of other incomplete object relative clauses). As hypotheses use more elements in the memory store, the distribution over these hypotheses will tend to become broader, taxing the reservoir of activation capacity, and making it more likely for low proba-bility hypotheses to disappear, increasing the incidence of garden path errors. Further investigations into how the memory store elements are allocated in various syntactic contexts may allow these apparently disparate dimensions of processing capacity to be unified.
 to achieve competitive performance with unconstrained state-of-the-art parsers will require the development of additional approximation algorithms beyond the scope of this article. This is because most modern parsers are lexicalized, incorporating head-word dependencies into parsing decisions, and employing finely tuned smoothing and backoff techniques to integrate these potentially sparse head-word dependencies with denser unlexicalized models. The bounded-memory right-corner HHMM described in this article can also be lexicalized in this way, but because head word dependencies are most straightforwardly defined in terms of top-down PCFG-like dependency structures, this lexicalization requires the introduction of additional formal machinery to transform PCFG probabilities into right-corner form (Schuler 2009). In other words, rather than transforming a training set of trees and mapping them to a time series model, it is necessary to transform a consistent probabilistically weighted grammar (in some sense, an infinite set of trees) into appropriately weighted and consistent right-corner PCFG and HHMM models. This requires the introduction of an approximate inference algorithm, similar to that used in value iteration (Bellman 1957), which estimates probabilities of infinite left-recursive or right-recursive chains by exploiting the fact that increasingly longer chains of events contribute exponentially decreasing probability mass. On top of this, preserving head-word dependencies in incremental processing also requires the introduction of a framework for storing head words of modifier constituents that precede the head word of a parent constituent; including some mechanism to ensure that probability assignments are fairly distributed among competing hypotheses (e.g., by marginalizing over possible head words) in cases where the calculation of accurate dependency probabilities must be deferred until the head word of the parent constituent is encountered. For these reasons, a complete lexicalized model is considered beyond the scope of this article, and is left for future work. 26 Appendix A: Head Transform Rules
The experiments described in this article used a binarization process that included the following rewrite rules, designed to binarize flat Treebank constituents into linguisti-cally motivated head projections: 1. NP: right-binarize basal NPs as much as possible; then left-binarize NPs 2. VP: left-binarize basal VPs as much as possible; then right-binarize VPs 3. ADJP: right-binarize basal ADJPs as much as possible; then left-binarize 4. ADVP: right-binarize basal ADVPs as much as possible; then left-binarize 5. PP: left-binarize PPs as much as possible; then right-binarize PPs after 6. S: grou psubject NP and predicate VP of a sentence; then grou pmodifiers Acknowledgments References 28
