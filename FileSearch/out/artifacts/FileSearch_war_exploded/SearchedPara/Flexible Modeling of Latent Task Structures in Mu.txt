 Alexandre Passos  X  apassos@cs.umass.edu Piyush Rai  X  piyush@cs.utah.edu School of Computing, University of Utah, Salt Lake City, UT U SA Jacques Wainer wainer@ic.unicamp.br Information Sciences Department, University of Campinas, Brazil Hal Daum  X e III hal@umiacs.umd.edu Learning problems do not exist in a vacuum. Often one is tasked with developing not one, but many clas-sifiers for different tasks. In these cases, there is of-ten not enough data to learn a good model for each task individually X  X eal-world examples are prioritiz-ing email messages across many users X  inboxes (Ab-erdeen et al., 2011) and recommending items to users on web sites (Ning &amp; Karypis, 2010). In these set-tings it is advantageous to transfer or share informa-tion across tasks. Multitask learning (MTL) (Caru-ana, 1997) encompasses a range of techniques to share statistical strength across models for various tasks and allows learning even when the amount of labeled data for each individual task is very small. Most MTL methods achieve this improved performance either by assuming some notion of similarity across tasks X  X or example, that all task parameters are drawn from a shared Gaussian prior (Chelba &amp; Acero, 2006), have a cluster structure (Xue et al., 2007; Jacob &amp; Bach, 2008), live on a low-dimensional subspace (Rai &amp; Daum  X e III, 2010), share feature representations (Ar-gyriou et al., 2007), or by modeling the task covariance matrix (Bonilla et al., 2007; Zhang &amp; Yeung, 2010). Choosing the correct notion of task relatedness is cru-cial to the effectiveness of any MTL method. Incorrect assumptions can hurt performance and it is desirable to have a flexible model that can automatically adapt its assumptions for a given problem.
 Motivated by this, we propose a nonparametric Bayesian MTL model by representing the task param-eters (e.g., the weight vectors for logistic regression models) as being generated from a nonparametric mix-ture of nonparametric factor analyzers. Parameters are shared only between tasks in the same cluster and, within each cluster, across a linear subspace that reg-ularizes what is shared. Moreover, by virtue of this being a nonparametric model, various existing MTL models result as special cases of our model; for exam-ple, the weight vectors are drawn from a single shared Gaussian prior, or form clusters (equivalently, gener-ated from a mixture of Gaussians), or live close to a subspace, etc. Our model can automatically interpo-late between these assumptions as needed, providing the best fit to the given MTL problem.
 In addition to offering a general framework for mul-titask learning, our proposed model also addresses several shortcomings of commonly used MTL mod-els. For example, task clustering (Xue et al., 2007), which fits a full-covariance Gaussian mixture model over the weight vectors, is prone to overfitting on high dimensional problems as the number of learning tasks is usually much smaller than the dimensionality, mak-ing it difficult to estimate the covariance matrix. A model based on mixtures of factor analyzers, like ours, can deal with this issue by adaptively estimating the dimensionality of each component, using less parame-ters than in the full rank case. Likewise, models based on task subspaces (Zhang et al., 2006; Rai &amp; Daum  X e III, 2010; Agarwal et al., 2010) assume that the weight vectors of all the tasks live on or close to a single shared subspace, which is known to lead to negative transfer in the presence of outlier tasks. Our model, based on a mixture of subspaces, circumvents these issues by allowing different groups of weight vectors to live in different subspaces when grouping all together them would not fit the data well. One can also view our model as allowing the sharing of statistical strengths at two levels: (1) by exploiting the cluster structure, and (2) by additionally exploiting the subspace struc-ture within each cluster. In the context of MTL, since the task relatedness structure is usually unknown, the standard solution is to try many different models, covering many simi-larity assumptions, with many settings of complexity for each model, and choose the one according to some model selection criteria. In this paper, we take a non-parametric Bayesian approach to this problem (using the Dirichlet Process and the Indian Buffet Process as building blocks) such that the appropriate MTL model capturing the correct task relatedness structure and the model complexity for that model will be learned in a data-driven manner side-stepping the model se-lection issues. 2.1. The Dirichlet Process The Dirichlet Process (DP) is a prior distribution over discrete distributions (Ferguson, 1973). Discreteness implies that if one draws samples from a distribution drawn from the DP, the samples will cluster: new sam-ples take the same value as older samples with some positive probability. A DP is defined by two parame-ters: a concentration parameter  X  and a base measure G 0 . The sampling process defining the DP draws the first sample from the base measure G 0 . Each subse-quent sample would take on a new value drawn from G 0 with a probability proportional to  X  , or reuse a pre-viously drawn value with probability proportional to the number of samples having that value. This prop-erty makes it suitable as a prior for effectively infi-nite mixture models, where the number of mixtures can grow as new samples are observed. Our mixture of factor analyzers based MTL model uses the DP to model the mixture components so we do not need to specify their number a priori . 2.2. The Indian Buffet Process The Indian Buffet Process (IBP) (Griffiths &amp; Ghahra-mani, 2006) and the closely related Beta Pro-cess (Thibaux &amp; Jordan, 2007) define a distribution on a collection of sparse binary vectors of unbounded size (or, equivalently, on sparse binary matrices with one dimension fixed but the other being unbounded). Such sparse structures are commonly used in applica-tions such as sparse factor analysis (Paisley &amp; Carin, 2009) where we want to decompose a data matrix X such that each observation X n  X  R D is represented as a sparse combination of a set of K  X  D basis vec-tors (or factors ) but K is not specified a priori . The generative story in the finite case is (assuming a linear Gaussian model generation): In the above,  X  is a matrix consisting of K columns (the factors) and the factor combination is defined by the sparse binary vector b n of size K . For the more general case of factor analysis, factor combi-nation weights are sparse real-valued vectors, so the model is of the form X n =  X ( s n  X  b n )+ E , where s n is a real-valued vector of the same size as b n (Paisley &amp; Carin, 2009) and can be given a Gaussian prior, and  X  is the elementwise product. Our mixture of factor analyzers based MTL model uses the IBP/Beta Pro-cess to model each factor analyzer so we do not need to specify the number of factors K a priori . Our proposed model assumes that the parameters (i.e., the weight vector) of each task are sampled from a mix-ture of factor analyzers (Ghahramani &amp; Beal, 2000). Note that our model is defined over latent weight vec-tors whereas the standard mixture of factor analyzers is commonly defined to model observed data . We assume that we are learning T related tasks, where each task is represented by a weight vector  X  t  X  R D that is assumed to be sampled from a mixture of F factor analyzers where each factor analyzer consists of K  X  min { T, D } factors (note: our model also al-lows each factor analyzer to have a different number of factors). Here D denotes the number of features in the data. Each task is a set of X and Y values, and each Y is assumed to be generated from the cor-responding X value and task weight vector. In our model, the weight vector  X  t for task t is generated by first sampling a factor analyzer (defined by a mean task parameter  X  t  X  R D and a factor loading matrix  X  ing that factor analyzer. In equations, this be written as  X  t =  X  t +  X  t f t +  X  t .
 The weight vector  X  t is a sparse linear combination of K basis vectors represented by the columns of  X  t (each column is a  X  X asis task X ). The combination weights are given by f t  X  R K which we represent as s t  X  b t where s t is a real valued vector and b t is a binary valued vector, both of size K . Our model uses a Beta-Bernoulli/IBP prior on b t to determine K , the num-ber of factors in each factor analyzer. The {  X  t ,  X  t } pair for each task is drawn from a DP, also giving the tasks a clustering property, and there will be a finite number F  X  T of distinct factor analyzers. Finally,  X   X  X  or (0 , 1 Figure 1 shows a graphical depiction of our model and Figure 2 shows the generative story for the linear re-gression case . The DP base measure G 0 is a product of two Gaussian priors for  X  t ,  X  t . In our nonparamet-ric Bayesian model, F and K need not be known a priori ; these are inferred from the data.
 For classification, the only change is that the first line in the generative model becomes Y t,i  X  B er ( sig (  X  t X t,i )) , where sig ( x ) = 1 1+exp(  X  x ) is the logistic func-tion and B er is the Bernoulli distribution. A number of existing multitask learning models arise as special cases of our model as it nicely interpolates between some different and useful scenarios, depending on the actual inferred values of F and K , for a given multitask learning dataset:  X  Shared Gaussian Prior ( F =1 , K =0): (Chelba  X  Cluster-based Assumption ( F &gt; 1 , K =0):  X  Linear Subspace Assumption ( F =1 , K &lt;  X  Nonlinear Manifold Assumption : A mixture Our nonparametric Bayesian model can interpolate be-tween these cases as appropriate for a given dataset, without changing the model structure or hyperparam-eters. From a non-probabilistic analogy, our model can be seen as doing dictionary learning/sparse cod-ing (Aharon et al., 2010) over the latent weight vec-tors (albeit, using an undercomplete dictionary set-ting since we assume K  X  min { T, D } ). The model learns M dictionaries of basis tasks (one dictionary per group/cluster of tasks, and M inferred from the data) and tasks within each cluster are expressed as a sparse linear combination of elements from that dictio-nary. Our model can also be generalized further, e.g., by replacing the Gaussian prior on the low-dimensional latent task representations s t  X  R K by a prior of the form P ( s t +1 | s t ), one can even relax the exchangeabil-ity assumption of tasks within each group, and have tasks that are evolving with time. 3.1. Variational inference As this model is infinite and combinatorial in nature, exact inference is intractable and sampling-based in-ference may take too long to converge (Doshi-Velez et al., 2009; Blei &amp; Jordan, 2006). Hence, we employ a variational mean-field algorithm to perform inference in this model. To do so, we lower-bound the marginal log-probability of Y given X using a fully factored ap-proximating distribution Q over the model parameters  X ,  X ,  X  , z, b, s : To do so, we approximate the DP and the IBP with a tractable distribution Q . For the DP we use a fi-nite stick-breaking distribution, based on the infinite stick-breaking representation of the DP (Blei &amp; Jor-dan, 2006). In this representation, we introduce, for each  X  t , a multinomial random variable z t that indexes the infinite set of possible mixture parameters  X  and  X . The z t vector is nonzero on its i -th component with probability  X  i Q j&lt;i (1  X   X  j ), where  X  is an in-finite set of independent B et (1 ,  X  1 ) random variables ( B et is the Beta distribution). A finite approximation to the DP is obtained by setting a given  X  i to 1, which sets the probability of z j for j &gt; i necessarily to 0. While there is a similar stick-breaking construction to the IBP (Teh et al., 2007), it is not in the exponen-tial family and requires complicated approximations, so we represent the IBP by its finite Beta-Bernoulli approximation (Doshi-Velez et al., 2009).
 The distribution we are approximating then (for the linear regression case) is shown in Figure 3 (top). The stick-breaking distribution SBP which is the prior for z is such that P ( z t = i ) =  X  i Q j&lt;i (1  X   X  j ) . In our variational distribution, we set the number of factor analyzers in the truncated stick-breaking rep-resentation to a hyperparameter F and the number of factors in each such analyzer to a truncation level hyperparameter K . After inference, if the truncation levels are set high enough, most factor analyzers (and factors within each factor analyzer) will not be used, effectively approximating the property of the infinite model that only a small finite number of components
Q ( z t = i ) =  X  z is ever used to model a finite data set. It is worthwhile to note that while the solution found by the variational approximation is necessarily finite and with complex-ity bounded by the truncation parameters, it will still implicitly perform model selection. Therefore, more often than not, it will concentrate most of its posterior mass on models with less complexity than the trunca-tion parameters suggest. Ishwaran &amp; James (2001) present two theorems to help choose these truncation levels, as using smaller values of F and K (particularly K , as the update equations are quadratic in K ) can lead to significant savings of computing time (in our experiments, we simply set these to min { D, T } ) which we found to be sufficient).
 Our approximating Q distribution is shown in Figure 3 (bottom). For the linear regression case, we treat P ( Y |  X  ) by lower-bounding it directly, without intro-ducing an approximating distribution for Y . In the case of logistic regression, we use the lower bound by (Jaakkola &amp; Jordan, 1996) that allows us to integrate out the logistic function.
 Apart from approximating the DP with the truncated stick-breaking prior, approximating the IBP with a set of symmetric, finite Beta distributed variables, and lower-bounding the logistic function with a quadratic, all the computations involved in deriving the varia-tional lower bound are straightforward exponential-family computations. Note that for Q we could use more general covariances instead of the identity ma-trices. In practice, we found that this did not improve classification performance, and it would imply on a significantly higher computational cost. Another less expensive option however would be to use the same hyperparameter for each feature, i.e., a spherical (in-stead of diagonal) covariance  X  2 I which would require optimizing w.r.t. a single hyperparameter  X  . The vari-ational parameter updates are:  X   X  )  X   X   X  In the above  X  denotes the digamma function. While it is possible to update  X   X  requires inverting a matrix, and in our experiment this matrix was often ill-conditioned, so we updated  X   X  by optimizing the lower bound with the L-BFGS-B optimizer (Zhu et al., 1997). The optimizer is run until convergence at each iteration, warm-started with the previous value. We note that it could be replaced by any other optimizer, including gradient methods, with no changes in the above equations.
 For regression, the gradient of the lower bound with respect to  X   X   X  L (  X   X  For classification the gradient is similar, the main difference being that there is an extra factor in the X t,i X T t,i  X   X  t term involving the variational parameter for the lower bound of the logistic function. We also optimize the lower bound w.r.t the precision parameter  X  to obtain an empirical Bayes estimate: 1  X  The hyperparameters  X  1 and  X  2 are held fixed and can be optimized by cross-validation. We initialize the in-ference process with  X   X  solution to each task X  X  regression or classification prob-lem. Then we alternate updating all other parameters to convergence and updating  X   X  rameters. The value of  X   X  classification accuracy, usually stabilizes after the first couple of iterations, and the only changes observed are further improvements to the lower bound. This matches behavior observed in Ando &amp; Zhang (2005). All our experiments were run on three iterations. We present results on both synthetic and real-world datasets, and on linear regression and classification settings. As a sanity check to show that our model can learn the underlying latent task structures correctly, we generated a synthetic data consisting of 5 clusters of tasks. Each cluster consists of 10 binary classifi-cation tasks, having 100 examples each. We used a 50/50 split for train/test data. Each task is repre-sented by a weight vector of length D = 20. Figure 4 (left) shows the true correlation structure of the tasks and Figure 4 (right) shows the recovered structure by our model: it correctly infers the correct number (5) of clusters. Our model resulted in a classification ac-curacy of 83.2%, whereas independently learned tasks resulted in an accuracy of 79.2%.
 Our next set of experiments compare our model with a number of baseline methods on several synthetic and real-world multitask regression and multitask classifi-cation problems. Our baselines include:  X  Independently learned tasks -STL : assumes the  X  Multitask Feature Learning -MTFL : assumes  X  Shared Gaussian prior over the weight vectors - X  Single shared subspace -RANK (Zhang et al.,  X  DP mixture model based task clustering -DP- X  L earning with W hom to S hare -LWS (Kang Of these baselines, MTFL and LWS were used for re-gression problems only since the publicly available im-plementations are for regression. In the experiments, we would refer to our model as MFA-MTL ( M ixture of F actor A nalyzers for M ulti T ask L earning). In all our experiments, we set the hyperparameters  X  1 = 1 and  X  2 = 5, as these values performed reasonably in preliminary experiments. The truncation level for the DP can be chosen to be equal to the number of tasks T , and for the IBP, to be the minimum of T and the number of features D in the data. This is often more than necessary and in most of our experiments, much smaller truncation levels were found to be sufficient. For our multitask regression experiments, we com-pared MFA-MTL with STL, MTFL, and LWS (we skip the other baselines as they performed compara-bly or worse than MTFL/LWS). For this experiment, we used three datasets -one synthetic dataset used in (Kang et al., 2011), and two real-world datasets used commonly in the multitask learning literature: (1) School : This dataset consists of the examination scores of 15362 students from 139 schools in London. Each school is a task so there are a total of 139 tasks for this dataset. (2) Computer : This dataset consists of a survey of 190 students about the chances of purchas-ing 20 different personal computers. There are a total of 190 tasks, 20 examples per task, and 13 features per example. For the synthetic data, we followed the similar procedure for train/test split as used by (Kang et al., 2011). For School and Computer datasets, we split the data equally into training and test set and further only used 20% of the training data (training set deliberately kept small as is often the case with multitask learning problems in practice). The average mean squared errors (i.e., across tasks) in predicting the responses by each method are shown in Table 1. As shown in Table 1, MFA-MTL outperforms the other baselines on all the datasets. Moreover, for the syn-thetic data, we found that it also inferred the number of task groups (3) correctly (the LWS baseline needs this number to be specified -we ran it with the ground truth). On the school and computer datasets, MFA-MTL outperforms STL and LWS and does slightly better than MTFL. For LWS on these two datasets, we report the best results as obtained by varying the number of groups from 1 to 20.
 We next experiment with the classification setting. For this, we chose two datasets: (1) Landmine : The landmine detection dataset is a subset of the dataset used in the symmetric multitask learning experiment by (Xue et al., 2007). It contains 19 classification tasks and the tasks are known to be clustered for this data. (2) 20ng: We did the standard training/test split of 20 Newsgroups for multitask learning, following Raina et al. (2006), and used a 50/50 split for the landmine data. The classification accuracies reported by our model and the various baselines on landmine and 20 Newsgroups datasets are shown in Table 2. As shown in Table 2, our method outperforms the various base-lines. We note that 3 of them (PRIOR, RANK, and DP-MTL), which are methods proposed in prior work, are special cases of our model (as discussed in Sec-tion 3). In particular, RANK performs worse than our method, potentially because all weight vectors share the same subspace which may not be desirable if not all the tasks are related with each other. DP-MTL performs worse than our method, potentially because it fits a full-rank Gaussian for each mixture compo-nent and is especially prone to overfit if the number of tasks is smaller than the number of features. Finally, we investigated the behavior of different algo-rithms in the small training data regimes. For this, we varied the amount of training examples per task (for landmine data, we varied the fraction from 20% to 100%; for 20 Newsgroup, we varied the number of ex-amples from 20 to 100). Results are shown in Figure 5. To uncrowd the figure, we compare only with STL and DP-MTL (the best performing baseline). In the small data regimes, our algorithm performs better as com-pared to both STL and DP-MTL. Another important aspect of an MTL algorithm is its asymptotic behavior in the limit of large training data per task. For this experiment, we compared MFA-MTL with STL on the school multitask regression dataset by providing each algorithm the complete training data. MFA-MTL re-sulted in an MSE of 261.4 as compared to STL which gave an MSE of 271.1. Therefore our algorithm tends to do comparably (in fact, marginally better) to in-dependently learned tasks even when the amount of training data per task is sufficiently large. Apart from the prior work on multitask learning dis-cussed in Section 1, our model is based on a somewhat similar motivation as the model proposed in (Argyriou et al., 2008). Their model assumes that tasks can be partitioned into groups and tasks within each group share a kernel. Their assumption is an extension of the earlier work on Multitask Feature Learning (Argyriou et al., 2007) (one of the baselines we used in our ex-periments) that assumes all tasks share the common kernel. In (Kumar &amp; Daum  X e III, 2012), the authors as-sume that there is single set of task basis vectors (i.e., a task dictionary) and each task is a sparse combina-tion of these basis vectors. In their model, the number of basis vectors shared between two tasks (i.e., their  X  X verlap X ) can be seen as the pairwise task similarity. In Kang et al. (2011), the authors proposed a model based on the assumption that the tasks exist in groups and the tasks within each group share features, which is again similar in spirit to our work (this model was one of our baselines in the experiments). In contrast, the generative model we presented in this paper of-fers a number of advantages over these models such as the ability to deal with missing data in a principled manner, doing automatic model complexity control in a nonparametric Bayesian setting, and being flexible enough to subsume these and many other notions as task relatedness used in multitask learning. Among other related work, Canini et al. (2010) pro-pose hierarchical Dirichlet process models as good models for human categorical learning. The idea is that one can model transfer learning by assuming that people unsupervisedly learn subgroups of known classes and use these groups to refine the knowledge of new classes by sharing subgroups via a hierarchical Dirichlet process. Our model can be seen as a discrim-inative analog of their generative model, where aspects of the task parameter X  X nstead of the distribution of the test examples X  X re shared among similar tasks and the sharing structure is discovered automatically. We proposed and evaluated a nonparametric Bayesian multitask learning model that usefully interpolates be-tween many different previously proposed models for estimating task parameters of multiple related learn-ing problems, such as a shared Gaussian prior (Chelba &amp; Acero, 2006), a clustering structure (Xue et al., 2007), reduced dimensionality (Argyriou et al., 2007; Zhang et al., 2006), manifold structure (Ghosn &amp; Ben-gio, 2003; Agarwal et al., 2010), etc. We presented a variational mean-field algorithm for this model that exhibits competitive results on a set of synthetic as well as real-world multitask learning datasets. The proposed model, by using the flexibility afforded by nonparametric Bayesian techniques, requires only min-imal assumptions to be applied to any given multitask learning problem. A possible future work is studying a hierarchical Dirichlet process variant of this model where different tasks are allowed to share exactly the same  X  parameters, which might be beneficial in cases where training data is especially sparse or the tasks are more strongly clustered.

