
In today X  X  society there is an increasing need for automated systems providing per-sonalised recommendations to a user f aced with a large number of choices. For example, an increasing choice of available products is caused by companies shift-ing towards developing customised products that meet specific needs of different groups of customers (Pine 1993). The product customisation trend, coupled with E-commerce, with which customers are not provided with an option to examine the products off-the-shelf in a traditional sense , makes the problem of providing accurate personalised recommendations very importa nt. Increasing the quality of personalised recommendations would increase customer satisfaction and loyalty, and at the same time reduce the costs caused by product return. Another example where personalised recommendations are extremely useful is an information overload situation with an amount of data available through the Internet and other media greatly exceeding the a large number of users with the ability to efficiently locate and retrieve information according to their preferences.

Personalised recommendation systems can be classified into two main categories: content-based and collaborative filtering, although some work has also been done in merging these two approaches to improve the quality of information filtering (Clay-pool et al. 1999; Delgado et al. 1998). In content-based filtering, mostly used for re-trieving relevant textual documents (Maes 1994; Salton and Buckley 1998), a search is performed for items with content most similar to the user X  X  interests. The task of collaborative filtering is to predict the pref erences of an active user given a database of preferences of other users, in which the preferences are typically expressed as nu-merical evaluation scores. Scores can be obtained explicitly by recording votes from behaviour or reactions regarding a given set of items.

Memory-based collaborative filtering al gorithms maintain a database of previous prediction is needed (Breese et al. 1998). The most common representatives are neighbour-based algorithms by which a subset of users most similar to an active user is chosen and a weighted average of their scores is used to estimate the preferences of an active user for other items (Aggrawal et al. 1999; Greening 1997; Konstan et al. 1997; Shardanand and Maes 1995). In contrast, model-based algorithms first develop a description model from a database and use it to make predictions for an al. 1998), Bayesian networks (Pennock et al. 2000), classification-based algorithms (Billsus and Pazzani 1998; Nakamura and Abe 1998), and algorithms for combin-erence patterns from data by using latent class models (Hoffman and Puzicha 1999) or by performing an appropriate clustering of items and users (Ungar and Foster 2002). However, it seems that association rules cannot yet provide a good trade-off between computational time, accuracy , and coverage needed for most practical recommendation systems.

Neighbour-based collaborative filtering algorithms have been shown to be supe-latency in giving predictions for active users can be a serious drawback in systems with a large number of requests that should be processed in real-time. Often, to re-duce computational effort, some form of clustering is used to group users with similar preferences (Aggrawal et al. 1999; Goldberg et al. 2001; Ungar and Foster 1998). previous results (Herlocker et al. 1999) have shown that as the number of items eval-uated by an active user decreases, the prediction accuracy of neighbourhood-based algorithms deteriorates dramatically. (Demanding extensive user effort for successful profiling can discourage a user from using the recommendation system.) Finally, it items (e.g., two users of a movie recommendation system could share opinions on dramas while disagreeing over science fiction).
 to provide preference predictions for an active user. Our approach is similar to pre-viously proposed item-based recommendation algorithms (Karypis 2001; Linden et unseen items. The Top-N algorithm (Karypis 2001) was designed for binary ratings item-to-item similarity mappings approach (Linden et al. 2001) was designed for nu-users, and uses them as weights to predict ratings on unseen items from available rat-ings of an active user. The proposed heuristic strongly weights ratings from popular items and from recently purchased items. Un like these algorithms that use different based approach, we propose statistically based algorithms that are robust to different rapid on-line recommendations.
 lowed by a description of several recommendation algorithms used for comparison with our approach. There, we propose three simple recommendation algorithms: the first two are not personalised, while the third uses active user ratings in a very sim-ple manner. We also outline the neighbour -based algorithm proposed by GroupLens (Herlocker et al. 1999) as a representative of a large class of similar algorithms such as FireFly (Shardanand and Maes 1995) or LikeMinds (Greening 1997). In Sect. 3 we describe and provide the motivation for the regression-based approach. We explain several algorithms with different computati onal requirements, achievable accuracy, and robustness to sparsity. We also propose several adjustments to the proposed al-scenarios. In Sect. 4 we characterise the proposed algorithms on real-life Eachmovie similar to Eachmovie data. We examine t heir properties with respect to accuracy, coverage, and off-line and on-line efficiency in a number of data sparsity scenarios.
Assuming a database of I items partially evaluated by U users, we are given a U matrix D with element r ui representing the evaluation score of item i by user u . vote just for a small subset of available items. By r average score for each user, the average s core for each item, and the overall average score, respectively. Since each user does not vote for each item, by I the subset of items rated by user u . Similarly, U evaluated item i . Given the evaluation scores of an active user a on items I recommendation system task is to estimate scores of user a on the remaining items
I \
I .
We propose three simple recommendation al gorithms to establish accuracy lower bounds on more complex recommendation systems. The first, M average score r  X  X  X  as a prediction of a preference of any user on any item. The second, G
ENERIC , uses the average scores r  X  i of each item i as predictions of a new user X  X  preferences. This algorithm is valid under the assumption that all users have similar preferences. The M EAN and G ENERIC algorithms do not attempt to use information contained in scores provided by an active user.

If users have provided numerical ratings explicitly, it is likely that the subjective nature of ratings will distort overall ratings matrix D . The most obvious consequence correlated users. Therefore, the predictions of A DJUSTED are obtained as predictions from G ENERIC adjusted by the difference the average score of user a and the average generic score over the same set of items, I a , defined as where | I a | is the cardinality of set I a . Hence the prediction of A for an active user preference on item i is calculated as p
Nearest-neighbour algorithms are used in a number of recommendation systems. For benchmark comparison to our proposed alternatives, we have chosen a GropLens al-gorithm (Herlocker et al. 1999) as a representative of the class of nearest-neighbour tween user u and an active user a (Herlocker et al. 1999). Here, where  X  a and  X  u are standard deviations of scores calculated over I p of user a for item i is predicted as a weighted sum of the votes of other users as
If two users have a small number of co-rated items, it is probable that their Pearson bours can significantly deteriorate the accuracy . Significance weighting (Herlocker et al. 1999) was proposed to reduce the weights if the number of co-rated items, n ,is smaller than some predetermined number N . If this is the case, the obtained weight is multiplied by n / N . Neighbourhood selection was introduced to retain only a small subset of the most similar users for prediction (Herlocker et al. 1999). Predicting an item i is done effectively by retaining K of the most similar users from U reported benefits were twofold: the comput ational time needed for each prediction was reduced and slight improvements in accuracy were observed. We implemented both modifications in a neighbour-based algorithm used for comparison with the pro-posed regression-based algorithms. We d enote the neighbour-based algorithm with these modifications as N EIGHBOUR ( N , K ) ,where N and K are adjustable signifi-cance and selection parameters of the algorithm.
 number of items. Reducing to the K most similar users for each item to be predicted and applying (3) still scales as O(UI).
 the on-line prediction process.
We use the coverage, Mean Absolute Error (MAE), and ROC sensitivity to compare examined recommendation algorithms. The coverage is calculated as the percentage
MAE of predicted scores is a statistical accuracy measure used for collaborative filtering algorithms, while the ROC sensitivity is a measure of the diagnostic power of prediction algorithms (Herlocker et al. 1999).
Assuming a database D consisting of a large number of users voting for all items and given unlimited computational and storage resources, a recommendation task could be solved by standard classification (binary ratings) or regression (numerical ratings) methods. In particular, given scores r a of an active user on items from I g on item i given scores on items from I a , and then using g scenarios with | I a |= 0, | I a |= I). Therefore, to be able to provide recommendations for any I a , it is sufficient to learn I k = 0 I k and i /  X  I a . Note that if i  X  I a , there is no need for prediction since p denote such a recommendation algorithm as N AIVE .
 dressed by a N AIVE algorithm. First, learning I k = computationally unfeasible even for small item sets. Second, assuming the fraction p of users rated any of items, the training set for learning the function g data set. Even for a large pool of 10 6 users and for a very dense ratings matrix with p = 0 . 1, training sets with | I a | &gt; 5 for learning g I a a result, practical recommendation algorithms should approximate N their size.

To address the sparseness problem, our regression-based approach to collaborative filtering uses a first-order approximation of nonlinear mappings g approximation to predicting the score p ai of active user a on item i based on its scores on I a can be expressed as where f j , i is a function that predicts the score on item i based on a vote on item j , and w j , i is the corresponding weight. So, assuming a method for choosing the weights w is known, learning I ( I  X  1 ) one-dimensional predictors f is sufficient for solving the approximated recommendation problem.
From a different standpoint, the function f j , i can be considered an expert for items from I a , there are | I a | available experts for predicting the ratings of each item from I \ I a . The recommendation problem can now b e approached as the identification of an optimal combination of experts. To make the solution computationally efficient, we model the experts f j , i as linear functions: where  X  j , i and  X  j , i are the only two parameters to be estimated for each expert. The two parameters could be estimated by using ordinary least squares as minimisation of the mean squared error when using the linear predictors f from (6) and when properly choosing the weights w j , i (Newbold and Granger 1974). (These weights are obtained from (9), as described below.)
Once I(I  X  1) linear experts are learned, they should be combined to give recommen-dations. An integration method based on simple averaging and two statistically based integration methods are proposed in this section.

In simple averaging, all | I a | experts in (4) are given the same weight, w
Since some of the experts can be slightly better than the generic predictor, they can only deteriorate prediction by decreasing the contribution of good experts. Therefore, we propose simple averaging with thresholdi ng where only good experts are retained.
In the A VERAGING (  X  ) algorithm, we reject all experts whose R-squared value defined as is below  X  ,where is the mean squared error (MSE) of expert f j , i , and where the variance of the scores for item i . Note that for large be rejected, which would result in a low coverage of recommendations. In practice, complete coverage. Since A VERAGING (  X  = 0) averages the predictions of all avail-algorithm (Karypis 2001). Thus, it will be useful for making comparisons with the remaining algorithms proposed in this section.

Back in 1969, Bates and Granger (Bates and Granger 1969) suggested that a linear combination of individual predictions can produce results that are superior to any of the individual predictions. For the problem of combining linear experts, the optimal of experts from (4). To predict the score on item i , i  X  (the ones that minimise the MSE) can be found by using the I
C i of prediction errors with elements E that { C i } j , j = MSE j , i .
 using | I a | experts f j , i , j  X  | I a | , one should first select the from elements of the covariance matrix C i as { C i ( 1 , ..., | I weights, such that j  X  I
We call this optimal weighting procedure O PTIMAL _A VERAGING ated with applying (9) include estimating C i , calculating the inverse of C experts are highly correlated, and computing this sufficiently fast. To estimate {
C
C i could be ill-conditioned or even non-invertible.

C ( I a ) could be unstable for highly correlated experts. Since individual experts could often be slightly to moderately better than t he generic predictor, they could be highly correlated. The third problem is computational cost, since the optimal approach re-quires the calculation of an inverse of C i ( I a )
Since | I a | = O(I) and | I \ I a | = O(I), producing predictions for active user with O PTIMAL _A VERAGING would take O(I 4 ) time. In the following, we propose a computationally efficient approximation of this approach that is robust to sparse data.

One of the main consequences of highly corre lated experts is that, when applying (9), assigned to the slightly more accurate one. T o illustrate this, in Table 1 we show the ratio of weights obtained from (9) for two experts with error variances {C and {C i } 2 , 2 = 1 . 1, when their error covariance {C i with error variance {C i } 1 , 1 . Similar effect could be obtained using (9) by decreasing both {C i } 1 , 1 and {C i } 2 , 2 by a constant K &lt; {C be shown that for the scenario with only two experts, using a constant K computationally fast.

The proposed weighting method is based on an estimate of the average error correlation between linear experts over the whole ratings matrix. If we assume that f and f k , i are unbiased predictors for item i , the correlation predictors can be estimated from as small. To address this problem, we approximate (11) by using E valid only under the assumption that the data distribution is identical over U and U j  X  U k , which may not be true.

For an efficient expert weighting procedure, instead of estimating all I ances { C i } j , k , i , j , k = 1, ... I, required by O them to obtain  X  AV . We then calculate a diagonal covariance matrix C where ID is an identity matrix and diag ( C i ) is a diagonal matrix with diagonal elem-ents { C i } j , j , j = 1, ... I. Therefore, the diagonal elements of C as where MSE j , i is calculated from (8). The constructed diagonal matrix C instead of C i to derive suboptimal expert weights from (9). Since C matrix, the corresponding weights are easily calculated as
We denote the proposed algorithm as R OBUST _A VERAGING docode in Table 2.
 serve also that computing the weights w j , i , j  X  I a , requires just O and so giving predictions for all unseen items by an active user would require only O(I 2 ) time.

In Table 3 we summarise the performance of different regression-based algorithms with respect to their speed (off-line and on-line) and robustness to data sparsity (ef-fective data size). The off-line speed of N AIVE is clearly impractical in any realistic scenario, since it is exponential with the number of items: for each of O(I2 examples for learning g I a , i decreases exponentially with is inferior to A VERAGING and R OBUST _A VERAGING , both in terms of speed and sparsity robustness. This is due to the need to examine each triple of items when es-timating the error covariance matrices { C i }, i = 1, ... I, in the training phase, and to invert the corresponding covariance matrix in the prediction phase. The advantage of R
OBUST _A VERAGING comes from the approximation made in (12). Clearly, the sim-ple G ENERIC algorithm is the fastest and most robust to rating sparsity. From these results one should be able to properly decide on the most appropriate recommenda-tion algorithm with respect to given data and available computational resources.
The algorithms A VERAGING and R OBUST _A VERAGING require O( O(UI) time needed by the N EIGHBOUR algorithm, since | smaller than the number of users. While the on-line speed of A
BUST _A VERAGING is clearly superior, they require learning I(I where O(U) time is required for each. However, O(UI since it is done off-line. Additionally, R OBUST _A VERAGING ear parameters and an estimate of the erro r variance for each expert, which requires storing 3I(I  X  1) variables. For most applications this memory requirement compares favourably to memory requirement of neighbour-based algorithms which scales as
O(UI). This notion has been used in A DJUSTED _G ENERIC and N improve the prediction accuracy of the R OBUST _A VERAGING an adjustment similar to the one used in A DJUSTED _G ENERIC to R
OBUST _A VERAGING . The first is the case of an extremely accurate expert f that can predict the score for item i with MSE j , is close to  X  1). This prediction does not need to be adjusted by a linear expert already includes this adjustment. The second is the case of poor ex-perts that are generic predictors with MSE j , i close to close to 0). In this case the adjustment is needed, and prediction. Any realistic combination of experts will be between these two extremes with 0 &lt; MSE j , i &lt; X  2 i . Therefore, some value smaller than adjustment depending on the quality of individual experts.
 active user for item i (last line of Table 2) as
Therefore,  X  r a is first subtracted from each active user X  X  vote, such modified ratings are used for prediction and, finally, predictions are adjusted by adding f i is a linear function, note that the effective level of adjustment in (15) is w j , i  X  j , i ) . This expression is non-negative since w this algorithm as A DJ _R OBUST _A VERAGING .
The practical difficulty when applying A DJUSTED _G ENERIC A items, the statistical si gnificance of an estimate would be advisable to use a pessimistic estimate  X  r a instead of if the number of scores is high.
 a data-generating process: variance matrix  X  of size | I a |  X  | I a | with elements Cov culating  X  involves ratings, while calculating C i from (9) involves prediction errors.
This data-generating proces s is validated by the assumption that the score of an ac-tive user on item i could be approximated by a generi c prediction incremented by an adjustment term and an error term that represents the variance of scores of previous users on item i . The covariance matrix  X  represents the fact that scores on different calculation of  X  r a should be given to a rating with a lower variance. least squares algorithm (Griffiths et al. 1993) as where 1 is a vector of ones, r a is a vector with elements tor with elements { r  X  i , i  X  I a } . From (17), a pessimistic adjustment as where t  X , n is a t-statistic with confidence  X  and with n degrees of freedom. It should be noted that the confidence parameter  X  determines the difference between  X  r . At the extremes, if  X  = 0 . 5then  X  r a =  X  r a , while if also that if covariance matrix  X  is diagonal with constant elements  X  / | I Since a calculation of Var (  X  r a ) involves inverting the matrix complexity, in the performed experiments we simplified the calculation of Var by using a matrix  X  with non-diagonal elements set to zero. We denote the algo-rithms that use the pessimistic adjustment  X  r a with confidence A
The EachMovie data set (McJones 1997) is a publicly available collaborative filtering benchmark database collected during an 18-month period between 1996 X 1997. It contains ratings from 72 916 users on 1 628 movies with a total of 2 456 676 ratings. were collected on a numeric 6-point scal e between 0 and 1, but to make the results more comprehensible we rescaled ratings to integers { line joke recommendation system (http://shadow.ieor.berkeley.edu/humor). The data contain anonymous ratings from 21 800 users on a subset of 100 jokes organised in a matrix of dimensions 21 800  X  100, with the ratings ranging from the so-called gauge set I gauge ={ 5 , 7 , 8 , 13 , 15 ,
We also generated two synthetic data sets to characterise proposed recommendation algorithms in a controlled setting. Both data sets were generated as random samples drawn from a multivariate Gaussian distribution with zero mean, variance one, and a covariance matrix P resembling that of the Eachmovie data. The matrix P was covariance matrix) for an arbitrarily chosen A . By properly choosing A , the matrix
P was generated to resemble the desired second-order statistics. et al. (1998) we performed another set of experiments allowing a smaller number tive user as the observed scores, and then p redicted the remaining scores. The three protocols were named Given2 , Given5 ,and Given10 . Such a small number of scores mendation systems. Finally, Jester and Synthetic data have the feature that all users rated a constant subset of items. By GaugeN we denote a protocol in which a given for prediction.
To perform a number of experiments, and to allow for a fair comparison between the training set. This resulted in 3422 users in the training set and 4790 active users with 503 retained movies. The reduction of the number of users allowed a fairly fast evaluation of the benchmark neighbour-based system, while a lower number of regression-based algorithms (trained in 8 hours on a 700 MHz NT-based computer with 256 MB memory). Finally, to decrease the influence of unreliable experts, we replaced each expert f j , i trained using less than 30 examples (with with a generic predictor for item i .

Due to data sparsity and based on Table 3, N AIVE and O PTIMAL not considered. Out of several considered choices of parameters K and N for N BOUR , we report on the best performing neighbour-based algorithm achieved with
K = 80 and N = 50.
 the AllBut5 protocol. As can be seen, N EIGHBOUR had almost complete coverage, but with an accuracy just slightly better than the A A
VERAGING algorithms showed large sensitivity to the threshold, with offering the best compromise b etween accuracy and coverage.
 while it was lowest for  X  = 0, which corresponds to the Top-N item-based algorithms (Karypis 2001).
 default value for (12). For illustration, we also report the results for  X  = 0 . 99. It can be seen that the values  X  = 0 . while  X  = 0 . 9 was slightly inferior. Since the coverage of R was 100 %, it can be concluded that, overall, it was slightly more successful than the A A
VERAGING was clearly superior to the other algorithms, indicating that the intro-duced adjustment can significantly boost the prediction accuracy. It should be noted that a similar adjustment can be incorporated to the A VERAGING similar accuracy improvements can be expected.

In Table 4 we also used ROC(  X  1 = 4,  X  2 = 3 . 5) to report the classification and each prediction above 3.5 was regarded as a recommendation. ROC(  X  = 3 . 5) is therefore the accuracy of correctly predicting a  X  X ood X  movie based on the given threshold of 3.5. From Table 4 it can be concluded that the MAE and
ROC(4, 3.5) performance measures are almost identical with respect to comparing different algorithms. Therefore, for the remaining analysis we report only the MAE accuracy.

As explained in the previous two sections, regression-based algorithms have su-perior on-line speed to neighbour-based algorithms. Our implementation of these al-gorithms in Matlab on a 700 MHz NT-based computer with 256 MB memory showed that when performing 5  X  4790 predictions, A DJ _R OBUST times faster than N EIGHBOUR ,andA DJ _R OBUST _A VERAGING slower than A DJUSTED _G ENERIC (Table 4). Although we do not claim our imple-mentation is optimal, these results validate the analysis of on-line speed of neighbour-based and the proposed regression-based algorithms from Sect. 3.4. Similar perform-ance results were obtained in the remaining experiments on the Eachmovie and Jester data.
 In Table 5 we show the performance of the MAE of the M A four categories of users depending on the number of provided ratings
But5 protocol. As can be seen, while N EIGHBOUR was very sensitive to the lowest accuracy for the active users giving less than 25 votes, A A
VERAGING was just moderately sensitive, indicating that it can be successfully used even for users providing a relatively small number of votes.

To examine further the robustness of different algorithms to the realistic scenario of small | I a | , a comparison of four different pro tocols is reported in Table 6, where the AllBut5 results are taken from Table 4. As can be seen, the regression-based approach was very robust to the number of scores given by an active user, compar-ing favourably to the N EIGHBOUR algorithm, whose accuracy decreased significantly dent that the performances of A DJUSTED _G ENERIC and A DJ without the pessimistic adjustment (Sect. 3.3) also deteriorated for the Given5 and
Given2 protocols. This was to be expected since estimates of adjustment only two or five scores have extremely low confidence. Applying the pessimistic adjustment allowed a significant accuracy improvement for the Given5 and Given2 protocols. For each protocol, we show t he accuracy achieved with the best choice of  X  confidence levels within an interval [0.5, 1]. As can be seen, the be optimised with respect to the number of available votes; as the number of ratings given by an active user increased, the optimal value of its MAE with the MAE of the G ENERIC algorithm for each of the 503 examined movies with the AllBut5 protocol. The MAE of G ENERIC disagreement in rating a given movie. In the Eachmovie data it ranges from 0.6 to 1.4. By using linear regression, we obtained the relationship indicating the benefits of A DJ _R OBUST _A VERAGING for the recommendation of movies for which there is a large disagr eement among users. For example, from (19) it can be seen that for MAE ( G ENERIC ) = 0 . 6, A DJ _R 7 % more accurate, while for controversial movies with MAE A _R OBUST _A VERAGING is about 17 % more accurate then the generic recom-mendation.
Similar to experiments on the Eachmovie data, we reserved 5000 randomly chosen gauge jokes I gauge allowed direct use of regression algorithms to learn 90 predictors of scores for non-gauge jokes. Therefore, for the Gauge10 protocol, in addition to recommendation algorithms used in the Eachmovie experiments, we were able to use Ordinary Least Squares (OLS) and Neu ral Networks (NN) as predictors, and thus estimate the upper bound on the recommendation accuracy achievable with the
Jester data. 90 neural networks used in the experiments had 10 inputs, 5 hidden nodes and 1 output, and were trained using resilient backpropagation (Riedmiller and Braun 1993). For A DJUSTED _G ENERIC and A DJ _R used pessimistic adjustment with a confidence level of appeared to be the best overall choice in the Eachmovie experiments.
In Table 7 we first show accuracy resu lts of different algorithms with the Gauge10 protocol. The first observation is that the margin between the M best available predictor was similar to the difference observed in the Eachmovie ex-periments. However, the difference between a powerful neural network predictor and A DJUSTED _G ENERIC was almost negligible. A DJ _R OBUST _A rior recommendation algorithm in the Eachmovie experiments, and N had comparable performances with MAE = 0 . 188. Such surprising results could be explained by the fact that the projection of gauge ratings on their two largest princi-an underlying assumption for collaborative filtering.
 We performed additional experiments with the Given2 , Given5 , Given10 ,and
AllBut5 protocols, and we also present the results in Table 7. For all protocols except Given2 ,A DJ _R OBUST _A VERAGING achieved results better than R A
VERAGING ,N EIGHBOUR ,andA DJUSTED _G ENERIC . An interesting result is that the MAE was significantly lower for the Given10 than for the Gauge10 protocol.
This indicates that a better choice for gauge jokes could be possible in order to im-prove the recommendation accuracy of the proposed regression-based algorithms on the Jester data.

While experiments on the real-life Eachmovie and Jester data provided very useful insight into the properties of different recommendation algorithms, we performed additional experiments on synthetic data to pr ovide a more detailed characterisation in a controlled setting.
 0 . 16, where  X  E denotes a correlation between movie ratings in the Eachmovie data.
For the first synthetic data set D 1 , we generated 10 000 training (TR1) and 10 000 test (TS1) examples from a 20-dimensional Gaussian distribution with a mean of zero, variance of one, and covariance matrix P 1 such that E = 0 . 21. Matrix P 1 was generated as P 1 = A 1 T A 1 ,where A with elements randomly taken from a Gaussian distribution with a mean of zero and variance of one.
 than in the Eachmovie data. For the second synthetic data set D 10 000 training (TR2) and 10 000 test (TS2) examples from a 40-dimensional Gaus-sian distribution with a mean of zero, variance of one, and covariance matrix P that E [  X  D2 ]= 0 . 15 and Std [  X  D2 ]= 0 . 16. Matrix P where A 2 was a 40  X  40 matrix with elements randomly taken from a Gaussian dis-tribution with a mean of 0.45 and variance of one. Therefore, D the Eachmovie data with respect to their second-order statistics. data sets D 1 and D 2 . We experimented with GaugeN protocols, with N 10, such that from each generated set we reserved the first N columns as gauge items while the column N + 1 was to be predicted.
 In the first set of experiments (Table 8), we examined the G G
TIMAL _A VERAGING , and Ordinary Least Squares (OLS) algorithms with Gauge2 ,
Gauge5 ,and Gauge10 protocols on 100 different D 1 and D on the MAE and MSE that were averaged over the 100 experiments. Note that the training data in both D 1 and D 2 were without missing values. This allowed the use of OLS predictors to obtai n the upper bound on the accuracy, since OLS is the best unbiased predictor on the generated multid imensional Gaussian data. From Table 8, in the following we summarise the main findings.

The accuracy of R OBUST _A VERAGING increased with the number of ratings for both D 1 and D 2 data. While its accuracy was very close to OLS and O
MAL _A VERAGING for | I a | = 2, the improvement in accuracy with the increase in ( |
I | = 5, 10) was significantly smaller than for OLS and O OLS or O PTIMAL _A VERAGING should be preferred over R
The accuracy of O PTIMAL _A VERAGING was comparable to that of OLS in all experiments. Since using OLS for recommendation assumes building O(2 tors, O PTIMAL _A VERAGING should be preferred over OLS. The accuracy of A
ERAGING (  X  = 0) was poorer than that of R OBUST _A VERAGING (Karypis 2001) could be improved by introducing similar weighted averaging. In our experiments, A DJUSTED _G ENERIC was more accurate than G on D 2 , and was comparable in accuracy to R OBUST tributed to the fact that most correlations between ratings in D sequently, the accuracy of A DJUSTED _G ENERIC on D 1 G ENERIC . Therefore, if ratings are mostly positively correlated, A G
ENERIC could be considered as a rapid and m oderately accurate recommendation algorithm.

The MSE measure indicates much larger di fference in accuracy between differ-ent algorithms than the MAE measure. This result could be another explanation for the seemingly small difference in accuracy between different algorithms obtained on the Eachmovie and Jester data. We adopted the MAE accuracy since it is more rithms.

The main property of data in collaborative filtering is a large proportion of missing robustness. We examined G ENERIC ,R OBUST _A VERAGING ( A
VERAGING , and OLS for the Gauge5 protocol with respect to their robustness to missing data. To perform the experiments, the D 1 data was used to produce ratings retained for each item from D 1 .

In Table 9 we present the dependence of t he algorithm accuracy on the fraction formance analysis provided with Table 3. As can be seen, depending on the level of rating sparsity, different algorithms should be preferred; as the sparsity increases the most appropriate algorithms follow the sequence OLS, O R
OBUST _A VERAGING (  X  = 0 . 95), and, finally, G ENERIC based algorithms, therefore, can be considered appropriate for a relatively large range of rating sparsity scenarios.
 corresponding training data can be chosen only from users that voted for all gauge items and the target item makes it unusable for a higher level of sparsity. Moreover, even if OLS predictors were available, they could be used only with active users that voted for all gauge items. This happens quite infrequently in realistic scenarios with large item sets. O PTIMAL _A VERAGING showed a somewhat smaller sensitivity to sparseness (able to work at p = 0 . 2) while achieving comp arable accuracy to OLS.
Since R OBUST _A VERAGING is trained using only pairs of items, it is very robust to sparseness (worked even for p = 0 . 05) with significant advantages in accuracy over the G ENERIC predictor. Finally, if data sparsity is extremely high (p generic predictions can be provided. It is worth noting that the results in Table 9 were obtained on data sets with 10 000 users, and that Table 3 should be used as a guide in choosing the appropriate recommendation algorithm for a given application. Automated collaborative filtering is a key technique for providing customisation of
E-commerce sites. Various neighbour-based recommendation methods are popular choices for collaborative filtering. Howeve r, their latency can be a serious drawback
In this paper we propose a regression-based approach to collaborative filtering that searches for similarities between items, builds a collection of experts, and combines them in an appropriate way to give predictions for a new user. We examined a num-ber of procedures varying in speed, accuracy, and sparsity robustness. synthetic data suggest that A DJ _R OBUST _A VERAGING magnitude faster than the neighbour-based a lternative. Furthermore, while the accu-racy of neighbour-based algorithms is very sensitive to the number of votes given by an active user, our approach is more accurate than the generic predictor even when this number is very small (two to five votes). However, it is worth noting that a fairly simple A DJUSTED _G ENERIC algorithm appeared to be very robust and difficult to outperform by both neighbour and regression-based algorithms despite its simplicity. ory requirements. We showed that the proposed regression-based algorithms seem to be an appropriate choice for a large range of recommendation scenarios satisfying available for a given item.
To further improve the on-line speed and memory requirements of the regression-based algorithms, deleting experts with poor predicting capabilities can be an ac-ceptable alternative. If there are no available experts to predict on a given item, the maximum coverage could be saved by using simpler models such as A G
ENERIC . With such an approach, the accuracy w ould not significantly deteriorate, since deleted experts are not much better than the mean predictor. The error variance of each expert estimated as part of the regre ssion-based algorithms could be used for guided on-line recommendation systems in which, based on the previous votes, an active user is asked to vote on the items that would maximally decrease the overall prediction error. Deriving an optimal procedure for guided voting is the topic of our future research.

In an attempt to characterise and evaluate the regression-based algorithms, we compared them with the state-of-the-art neighbour-based recommendation algorithm.
In some cases we were also able to make comparisons with standard machine-learning algorithms for regression  X  ordinary least squares and neural networks  X  which served for establishing upper bounds on the achievable recommendation ac-curacy. In addition, we proposed three simple algorithms that served as benchmarks for establishing a lower bound on the recommendation accuracy. A comprehensive evaluation of the regression-based approach with an even larger number of proposed recommendation systems would, without doubt, be highly desirable. However, in number of existing recommendation systems would be extremely important for fur-ther advances in the area of collaborative filtering.

