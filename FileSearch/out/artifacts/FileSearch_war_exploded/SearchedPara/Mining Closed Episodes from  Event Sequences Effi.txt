 applications such as alarm sequence analysis in telecommunication networks [1], financial events and stock trend relationship analysis [4], web access pattern analysis and protein family relationship analysis [5]. An event sequence is a long sequence of events. Each event is described by its type and a time of occurrence. A frequent epi-sode is a frequently recurring short subsequence of the event sequence. 
Previous studies on frequent itemset mining and sequential pattern mining show no super itemset (or super sequence) with the same support. To the best of our knowl-how to find closed episodes efficiently? 
Following the definitions of closed itemset and closed sequence, we can define frequency constraint of 2 (i.e., the minimum support threshold). 
Though the concept of closed episode is similar with that of closed itemset and closed sequence, mining closed episode efficiently is much more challenging. The challenge is caused by two properties in frequent episode mining: (1) there is tempo-ral order among different events in an epis ode; and (2) we take into account repeated occurrences of an episode in the event sequence. 
In this paper, we define a concept of minimal occurrence (a similar concept was defined in [1]) and use minimal occurrence to model the occurrences of an episode in the event sequence. Based on this model, we will develop several pruning techniques mine all closed episodes by exploiting the pruning techniques. 
The remainder of this paper is organized as follows. We give the problem defini-pruning techniques. We demonstrate the performance of our algorithm through exten-sive experiments in Section 4. Section 5 concludes the paper. this paper, we focus on serial episode mining, and we call it episode for simplicity. ( A
N , t N )&gt;, where A i T the ending time. Thus an event sequence can be denoted by ( S , T s , T e ). An episode  X  span t e  X  t s is called the size of the window w . Definition 1 ( Minimal occurrence ) . Given an event sequence S and an episode  X  , a user-defined maximum window size threshold, and t e  X  t s is called the time span of this minimal occurrence. That is, each occurrence of an episode must be within a window no larger than win . t minsup , if sup (  X  )  X  minsup ,  X  is called a frequent episode, or  X  is frequent. of  X  but is neither a forward-extension nor backward-extension super episode of  X  ,  X  is called the middle-extension super episode of  X  .
 Definition 4 (Forward-closed episode). Episode  X  is forward-closed if (1)  X  is fre-quent; and (2) there is no forward-extension super episode of  X  with the same support as  X  . Similarly we can define another two kinds of closed episodes. support as  X  . Definition 6 (Middle-closed episode). Episode  X  is middle-closed if (1)  X  is frequent; and (2) there is no middle-extension super episode of  X  with the same support as  X  . 
A closed episode should be forward-closed, backward-closed and middle-closed simultaneously. Mining Task: Given an event sequence S , a minimum support threshold minsup , and a maximum window size threshold win , the mining task is to find the complete set of closed episodes from S. To enumerate potential closed episodes, we perform a search in a prefix tree as shown 
The mining process follows a breadth-first search order. Starting from the root event to get length  X  ( k +1) episodes at level k +1. For each candidate episode, we com-pute its minimal occurrence set from the minimal occurrence sets of its sub-episodes through a  X  X oin X  operation. Since the episode mining and checking are based on minimal occurrences of episodes, we will first give some properties of minimal occur-rences and show how they can be used for pruning search space.  X 
Based on Property 2, minimal occurrences in MO (  X  ) can be sorted in the ascending order of their starting time. Then the order among an episode X  X  minimal occurrences is strict. If one occurrence starts ahead of another, then it must end earlier.
According to Definition 1, the time span of an episode X  X  minimal occurrence is bounded by the window size threshold win . Therefore, given a minimal occurrence [ t s , t ), if t e  X  t s = win , the episode cannot be extended forward or backward in the time win-fine such minimal occurrences as saturated minimal occurrences. Definition 7 (Saturation and expansion). A minimal occurrence [ t s , t e ) of an episode  X  threshold. Otherwise, it is an unsaturated minimal occurrence. An episode  X   X  X  satura-minimal occurrences. 
Apparently,  X  .saturation+  X  .expansion= sup (  X  ) holds. For a saturated minimal occur-rence of an episode, no additional events can be inserted before the first event or after the last event; otherwise, the maximum window size constraint will be violated. This super episode  X  only in the unsaturated minimal occurrences. The upper bound of  X   X  X  then none of its forward-extension or backward-extension super episodes is frequent. In this case, it is unnecessary to generate and check its super episodes. Therefore, we have following two lemmas.
 extension super episodes of  X  are infrequent.  X  . saturation &gt;0, and (2) there is a saturated minimal occurrence  X   X  MO (  X  ) that cannot be extended in the middle, then  X  must be closed. quence. Its major steps are given in Figure 2. 
The algorithm structure is as follows. It first generates the set of length-1 frequent episodes F 1 (lines 1-4). The event sequence S is scanned and all distinct single events while loop (lines 5-19). which is the set of length  X  k closed episodes. Episodes in F k are processed in a prede-fined order as we explained above. F each candidate episode and checks whether they can be extended forward or back-ward and whether they are closed. Due to space limitation, we omit the details of the two functions. Based on the computed information, each frequent episode  X  is added marked not closed (lines 12-13). For those sub-episodes  X  of  X  that share the first and (line 14). The details of function Clo_prune are given in Figure 3. omit the detail of proof. with MINEPI proposed by Mannila et al. [1] for frequent episode mining. To test the the function Clo_prune and get another algorithm called Clo_episode_NP without the pruning techniques. All of our experiments were performed on a PC with 3Ghz CPU, 2GB RAM, and 300GB hard disk running windows XP. five parameters. The parameter NumOfPatterns is the number of potentially closed episodes in the final sequence, MaxLength and MinLength are the maximum and minimum length of potential episodes respectively, NumOfWindows is used to control the length of the whole sequence, and win is the size of a window. Due to space limi-tation, we omit the detail of the generator. minsup . Thus we create five groups of datasets, each of which is generated by varying one parameter and fixing the other four parameters as shown in Table 2. logarithmic scales in Figures 5, 7, 9, 11 and 13. Clo_episode_NP . This shows that the pruning techniques in Clo_episode are very number of frequent episodes output by MINEPI . longer. For a fixed minsup threshold, the number of frequent episodes by MINEPI increases a lot, but the number of closed episodes does not increase as much. creases significantly with L . Accordingly the running time increases. Figure 8 shows that Clo_episode is much more efficient than MINEPI and Clo_episode_NP. potential closed episodes is fixed at 12 in the sequence. closed episodes decreases. The runn ing time decreases accordingly. Clo_episode performs much better than MINEPI and Clo_episode_NP . Frequent episode mining is an important mining task in data mining. In this paper, we study how to mine closed episodes efficiently. We proposed a novel algorithm Clo_episode , which incorporates several effective pruning strategies and a minimal occurrence-based support counting method. Experiments demonstrate the effective-ness and efficiency of these methods.
 Acknowledgments. This work was supported in part by the National Natural Science Foundation of China under Grant No. 70871068, 70621061 and 70890083. A recent attractive area of research has been detecting statistically relevant se-quences or mining interesting patterns from a given string [1,2]. Given an input string composed of symbols from an alphabet set with a probability distribu-tion defining the chance of occurrence of the symbols we would like to find the portions of the string which deviate from the expected behavior and can thus be potent sources of study for hidden pattern and information. An automated monitoring system such as a cluster of se nsors sensing the temperature of the surrounding environment for fire alert, or a connection server sniffing the net-work for possible intrusion detection provides a few of the applications where such pattern detection is essential. Other applications involve text analysis of e-mails and blogs to predict terrorist activities or judging prevalent public sen-timents and studying trends of the stock market. Similarly, another interesting field of application can be the identification of good and bad career patches of a sports icon. For example, given the runs scored by Sachin Tendulkar in each in-nings of his one-day international cricket career, we may be interested in finding his in-form and off-form patches.

A statistical model is used to determine the relationship of an experimental or observed outcome with the factors affecting the system, or to establish the occurrence as pure chance. An observation is said to be statistically significant if its presence cannot be attributed to randomness alone. The degree of unique-ness of a pattern can be captured by several measures including the p-value and z-score [3,4]. For evaluating the significance of a substring, it has been shown that the p-value provides a more precise conclusion as compared to that by the z-score [1]. However, computing the p-value entails enumerating all the possible outcomes, which can be exponential in nu mber, thus rendering it impractical. So, heuristics based on branch-and-bound techniques have been proposed [5]. The log X  X ikelihood ratio G 2 provides such a measure based on the extent of devi-ation of the substring from its expected nature [6]. For multinomial models, the  X  2 statistic approximates the importance of a string more closely than the G 2 statistic [6,7]. Existing systems for intrusion detection use multivariate process control techniques such as Hotelling X  X  T 2 measure [8], which is again computa-tionally intensive. The chi-square measure, on the other hand, provides an easy way to closely approximate the p-value of a sequence [6]. To simplify compu-tations, the  X  2 measure, unlike Hotelling X  X  method, does not consider multiple variable relationship, but is as effective in identifying  X  X bnormal X  patterns [2]. Thus, in this paper, we use the Pearson X  X   X  2 statistic as a measure of the p-value of a substring [6,7]. The  X  2 distribution is characterized by the degrees of free-dom , which in the case of a string, is the size of the alphabet set minus one. The larger the  X  2 value of a string, the smaller is its p-value, and hence more is its deviation from the expected behavior.

Formally, given a string S composed of symbols from the alphabet set  X  with a given probability distribution P modeling the chance of occurrence of each symbol, the problem is to identify and extract the top-k substrings having the maximum chi-square value or the largest deviation within the framework of p-value measure for the given probability distribution of the symbols. Na  X   X vely we can compute the  X  2 value of all the substrings present in S and determine the top-k substrings in O ( l 2 )timeforastringoflength l . We propose to extract such substrings more efficiently.
 Related Work: The blocking algorithm and its heap variant [9] reduce the practical running time for finding such statistically important substrings, but suffers from a high worst-case running time. The number of blocks found by this strategy increases with the size of the alphabet set and also when the proba-bilities of the occurrence of the symbols are nearly similar. In such scenarios, the number of blocks formed can be almost equal to the length of the given string, thereby degenerating the algorithm to that of the na  X   X ve one. The heap variant requires a high storage space for maintaining the separate max and min heap structures and also manipulates a large number of pointers. Further, the algorithm cannot handle top-k queries. In time-series databases, categorizing a pattern as surprising based on its frequency of occurrence and mining it effi-ciently using suffix trees has been proposed in [10]. However, the  X  2 measure, as discussed earlier, seems to provides a better parameter for judging whether a pattern is indeed interesting.
 In this paper, we propose two algorithms, All-Pair Refined Local Maxima Search (ARLM) and Approximate Greedy Maximum Maxima Search (AGMM) to efficiently search and identify interesting patterns within a string. We show that the running time of the algorithms are better than the existing algorithms with lesser space requirements. The pr ocedures can also be easily extended to work in streaming environments. ARLM, a quadratic algorithm in the number of local maxima found in the input string, and AGMM, a linear time algorithm, both use the presence of local maxima in the string. We show that the approx-imation ratio of the reported results to the optimal is 0.96 or more. Empirical results emphasize that the algorithms work efficiently.

The outline of the paper is as follows: Section 2 formulates the properties and behavior of strings under the  X  2 measure. Section 3 describes the two pro-posed algorithms. Section 4 discusses the experimental results, before Section 5 concludes the paper. Let S = s 1 s 2 ...s l be a given string of length l composed of symbols s i from the alphabet set  X  = {  X  1 , X  2 ,..., X  m } ,where |  X  | = m .Toeachsymbol  X  i  X   X  is associated a p  X  i (henceforth represented as p i ), denoting the probability of occurrence of that symbol, such that m i =1 p i =1.Let  X   X  represented as  X  i,S ) denote the observed number of symbol  X  i in string S .
The chi-square value of a string S  X   X   X  of length l is computed as We now state certain observations and lemmas, the formal proofs of which are in the full version of the paper [11].
 Observation 1. Under string concatenation operation (.), for two arbitrary strings S and T drawn from the same alphabet set and probability distribution of the symbols (henceforth referred to as the same universe ), the  X  2 measure of the concatenated string is commutative, i.e.,  X  2 S.T =  X  2 T.S .
 Lemma 1. The  X  2 value of the concatenation of two strings drawn from the same universe is less than or equal to the sum of the  X  2 values of the individual strings.
 Lemma 2. The  X  2 value of a string composed of only a single type of symbol increases with the length of the string.
 We next define the term local maxima and describe the procedure for finding such a local maxima within a given string.
 Definition 1 (Local maxima). The local maxima is a substring, such that while traversing through it, the inclusion of the next symbol does not decrease the  X  2 value of the resultant sequence. Let s 1 s 2 ...s n be a local maxima of length n . Then the following holds:  X  2 s  X 
The process of finding the local maxima involves a single scan of the entire string. We consider the first local maxima to start at the beginning of the string. We keep appending the next symbol to the current substring until there is a decrease in the chi-square value of the new substring. The present substring is then considered to be a local maxima ending at the previous position. The last symbol appended that decreased the chi-square value signifies the start of the next local maxima. Thus, the running time is O ( l ) time for a string of length l .
 Lemma 3. The expected number of local maxima in a string of length l is O ( l ) . However, practically the number of local maxima is less than l , as all adjacent positions of dissimilar symbols may not correspond to a local maxima boundary. We further optimize the local maxima finding procedure by initially blocking the string S , as described in [9], and then sea rching for the local maxima. A contiguous sequence of the same symbol is considered to be a block, and is replaced by a single instance of that symbol representing the block. If a symbol is selected, the entire block associated with it is considered to be selected. The next lemma shows that a local maxima cannot end in the middle of a block. Lemma 4. If the insertion of a symbol of a block increases the chi-squared value of the current substring, then the chi-squared value will be maximized if the entire block is inserted. Based on the observations, lemmas and local maxima extracting procedure dis-cussed previously, in this section we explain the All-Pair Refined Local Maxima (ARLM) and Approximate Greedy Maximum Maxima (AGMM) search algo-rithms for mining the most significant substring based on the chi-square value. 3.1 All-Pair Refined Local Maxima Search (ARLM) Given a string S of length l and composed of symbols from the alphabet set  X  , we first extract all the local maxima pres ent in it in linear time, as described earlier. With S partitioned into its local maxima, the global maxima can either start from the beginning of a local maxima or from a position within it. Thus, it can contain an entire local maxima, a suffix of it or itself be a substring of a local maxima. It is thus intuitive that the global maxima should begin at a position such that the subsequent sequence of characters offer the maximum chi-square value. Otherwise, we could keep adding to or deleting symbols from the front of such a substring and will still be able to increase its  X  2 value. Based on this, the ARLM heuristic finds within each local maxima the suffix having the maximum chi-square value, and considers the position of the suffix as a potential starting point for the global maxima.

Let xyz be a local maxima, where x is a prefix, y is a single symbol at position  X  ,and z is the remaining suffix. For a local maxima the chi-square value of all its suffices is computed. The starting position of the suffix having the maximum chi-square value provides the position  X  for the component y , i.e, yz will be the suffix of xyz having the maximum chi-square value.

For each local maxima, the position  X  is inserted into a list  X  .Ifnosuchproper suffix exists for the local maxima, the starting position of the local maxima xyz is inserted in the list. After populating  X  with position entries of y for each of the local maxima of the input string, the list contains the prospective positions from where the global maxima may start.

The string S is now reversed and the same algorithm is re-run. This time, the  X  list is similarly filled with positions y relative to the beginning of the string.
For simplicity and efficiency of op erations, we maintain a table C having m rows and l columns, where m is the cardinality of the alphabet set. The rows of the table contain the observed number of each associated symbols present in the length of the string denoted by the column. The observed count of a symbol between two given positions of the string can be easily computed from this table in O (1) time.

Given the two  X  and  X  lists, we now find the chi-square value of substrings from position g  X   X  to h  X   X  ,and g  X  h . The substring having the maximum value is reported as the global maxima. While computing the chi-square values for all the pairs of positions in the two list, the top-k substrings can be main-tained using a heap of k elements.
 Conjecture 1. The starting position of the global maxima is always present in the  X  list.
 Corollary 1. From the above conjecture, it follows that the ending position of the global maxima is also present in the  X  list.
 Finding all the local maxima in the string requires a single pass, which takes O ( l ) time for a string of length l . Let the number of local maxima in the string be d . Finding the maximum valued suffix for each local maxima using the C table, requires another pass of each of the local maxima, and thus also takes O ( l )time. Since, each local maximum contributes one position to the lists, the number of elements in both the lists is d . We then evaluate the substrings formed by each possible pair of start and end positions, which takes O ( d 2 ). So, in total, the time complexity of the algorithm is O ( l + d 2 ). 3.2 Approximate Greedy Maximum Maxima Search (AGMM) In this section, we propose a linear time greedy algorithm for finding the maxi-mum substring, which is linear in the size of the input string S .Weextractall the local maxima of the input string and its reverse, and populate the  X  and  X  lists as discussed previously. We identify the local maxima suffix max hav-ing the maximum chi-square value among all the local maxima present in the string. AGMM assumes this local maxima suffix to be completely present within the global maxima. We then find a position g  X   X  for which the new substring starting at g and ending with max as a suffix has the maximum  X  2 value, for all g . Using this reconstructed substring, we find a position h  X   X  such that the new string starting at the selected position g and ending at position h has the maximum chi-square measure for all positions of h . This new substring is reported by the algorithm as the global maxima.

The intuition here is that the global maxima will contain the maximum of the local maxima to maximize its value. Although this is a heuristic, the assumption is justified by empirical results in Sect ion 4, which shows that we always obtain an approximation ratio of 0.96 or more.

Using the C table, AGMM takes O ( d ) time, where d is the number of local maxima found. The total running time of the algorithm is thus O ( d + l ). Thus, being a linear time algorithm, it provides a order of increase in the runtime as compared to the other algorithms. To assess the performance of the two proposed heuristics ARLM and AGMM, we conduct tests on multiple datasets and compare it with the results of the na  X   X ve algorithm and the blocking algorithm [9]. The heap variant of the blocking algorithm is not efficient as it has a higher running time and uses more memory, and hence has not been reported. We compare the results based on the following parameters: (i) search time for top-k queries, (ii) number of local maxima found, and (iii) accuracy of the result based on the ratio of the optimal  X  2 value to that returned by the algorithms. 1 4.1 Real Datasets We used the innings-by-innings runs scored by Sachin Tendulkar in one-day internationals (ODI) 2 as a real dataset. We quantized the runs scored into 5 symbols as follows: 0-9 is represented by A , 10-24 by B , 25-49 by C , 50-99 by D , and 100+ by E . The actual probability of occurrences of the different symbols were 0 . 28, 0 . 18, 0 . 22, 0 . 22 and 0 . 10 respectively for the five symbols, and the overall average is 45 . 5 runs per innings. Table 1(a) summarizes the results. We find that during his best patch he had scor ed 8 centuries and 3 half-centuries in 20 innings with an average of 84 . 31, while in the off-form period he had an average of 21 . 89 in 9 innings without any score of above 40.

Figure 1(a) show the times taken by the different algorithms for different values of k . The AGMM algorithm requires the least running time as compared to the other procedures, while the ARLM is faster than the na  X   X ve and the blocking ones. The number of local maxima found was 281, which is lesser than 319, the number of blocks constructed by the blocking algorithm. So, the heuristic prunes the search space more efficiently. Figure 1(b) plots the approximation factor for the heuristics. The accuracy of the ARLM heuristic is found to be 100% for the top-1 query, i.e., it provides the correct result validating the conjecture we proposed in Section 3. As the value of k increases we find an increase in the approximation ratio of both the heuristics. 4.2 Synthetic Datasets We now benchmark the ARLM and AGMM heuristics against datasets gener-ated randomly using a uniform distribution. To simulate the deviations from the expected characteristics as observed in r eal applications, we perturb the random data with chunks of data generated from a geometric distribution with param-eter p =0 . 3. The parameters that affect the per formance of the heuristics are: (i) length of the input string, l , (ii) size of the alphabet set, m , and (iii) num-ber of top-k values. For different values of these parameters we compare our algorithms with the existing ones on the basis of (a) time to search, (b) approx-imation ratio of the results, and (c) the number of blocks evaluated in case of blocking algorithm to the number of local maxima found by our algorithm.
Fig 2(a) shows that with the increase in the length of the input string l ,the time taken for searching the top-k queries increases. The number of blocks or local maxima increases with the size of the string and, hence, the time increases. The time increases more or less quadratically for ARLM and the other existing algorithms according to the analysis shown in Section 3.1. ARLM takes less running time than the other techniques, as the number of local maxima found is less than the number of blocks found by the blocking algorithm (see Table 1(b)). Hence, it provides better pruning of the search space and is faster. On the other hand, AGMM being a linear time heuristic, runs an order of time faster than the others. We also find that the accuracy of the top-k results reported by AGMM show an improvement with the increase in the string length (graph not shown). The approximation factor for ARLM is 1 for the top-1 query in all the cases tested, while for other top-k queries and for AGMM it is always above 0 . 96.
Thetimetakenforsearchingthetop-k query as well as the number of blocks formed increases with the size of the alphabet m (Table 1(b) and Fig 2(b)). As m increases, the number of blocks increases as the probability of the same sym-bol occurring contiguously falls off. A local maxima can only end at positions containing adjacent dissimilar symbols. So the number of local maxima found increases as well. There seems to be no appreciable effect of m on the approxi-mation ratio of the results returned by the algorithms. We tested with varying values of m with l =10 4 and k = 2, and found the ratio to be 1 in all cases.
Fig 3(b) shows the effect of varying probability of occurrence of one of the symbols in a string composed of two symbols only. The approximation ratio remained 1 for both heuristics for the top-1 query.

We next show the scalability of our algorithms by conducting experiments for varying values of k for top-k substrings. Fig 3(a) shows that search time increases with the increase in the value of k . This is expected as mo re computations are performed. The accuracy of the results for the heuristics increases with k .For k = 2, it is 0.96, and increases up to 1 when k becomes more than 10. In this paper, we proposed the problem of finding top-k substrings within a string with the maximum chi-square value for mining interesting patterns. The chi-square value represents the deviat ion of the observed from the expected. We used the concept of local maxima and p roposed two efficient heuristics that run in time quadratic and linear in the number of local maxima. Experiments showed that the heuristics are faster than the existing algorithms, are scalable, and return results that have an approximation ratio of more than 0.96.
