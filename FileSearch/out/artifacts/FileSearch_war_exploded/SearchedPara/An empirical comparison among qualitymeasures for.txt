
Instituto Nacional de Astrof X sica,  X ptica y Electr  X nica, Santa Mar X a Tonantzi ntla, Puebla, M X xico
Instituto Superior Polit X cnico Jos X  Ant onio Echeverr X a, Marianao, La Habana, Cuba Centro de Bioplantas, Ciego de  X vila, Cuba 1. Introduction
A supervised classifier predicts the class of a query object based on the relationships it finds in a collection of objects belonging to known classes. Although the prediction accuracy is important, in many domains the classifier and its results should be also easily understandable by experts in the problem domain. Understandable classifiers are frequently based on contrast patterns . A contrast pattern is a pattern appearing frequently in a class and infrequently in the remaining problem classes [1]. Then, if a contrast pattern appears in a query object, it can be taken as an evidence that biases the classification towards the pattern X  X  class.

Two reasons makes the extraction and posterior use of contrast patterns a challenging problem. First, miners usually extract a very large collection of patterns from a database, slowing the classification stage and making the results harder to understand by researchers. Second, it is very hard to distinguish between useful and interesting patterns from random patterns due to chance or sampling procedures. A quality measure (QM) evaluates the discriminative power of a pattern [2], so it can help to solve both problems. Measuring the quality of a contrast pattern is an act ive and important area of pattern recognition and data mining [2 X 4].
 Despite some particular examples, interpreting the result of a quality measure is a complex task [5]. That is why, in many applications, what is important is the order induced by the quality measure. For example, in rule induction systems, mined rules are sorted according to a quality measure, so the query object is first compared with rules with higher quality values. In contrast pattern filtering [6], on the other hand, patterns are sorted before being iteratively selected to be in the reduced pattern subset. Consequently, patterns with higher quality values have higher chances to be selected in the resultant subset.

This paper is another step towards the goal of selecting which quality measure has better behavior in supervised classification. With this intention, we present an empirical comparison of the behavior of 33 quality measures, used for three common supervised classification tasks: aggregation value, pattern filtering, and classification of query objects.

The first contribution of this paper is a correlation study among quality measures, which reveals that many of them have identical behaviors for tasks depending on pattern ranking. Compared with previ-ous published results, we use a synthetic pattern collection formed by all contrast patterns of the posi-tive class with absolute frequency below 100, while previous results use patterns extracted from a real database or randomly generated patterns. We also use Kendall X  X   X  correlation, which is more related to the ranking task than correlations used in other papers like Pearson X  X  or Spearman X  X   X  [7].
The second contribution of this paper is a comparative study of the behavior of quality measures for supervised classification tasks using more quality measures than previous papers. We compare qual-ity measures for supervised classification instead of association rule mining, which is studied in most papers. We also test the behavior of each quality measure when used as an aggregation value. We con-duct an empirical comparison, because theoretical properties are very problem-dependent and frequently contradictory [8].

This paper is an extension of a conference paper [3] with the following differences: it contains a better analysis of previous works, it analyzes more than three times the amount of quality measures, it uses a more meaningful correlation function using synthetic patterns, it contains a section to evaluate quality measures as aggregation values and experimental results are deeper discussed. 2. Quality measures
Measuring the quality of a contrast pattern is a s ubjective and complex task. A large amount of work has been conducted in this area, but there is no widespread agreement on a formal definition of quality or on the proper way to measure it. Although there is some consensus about some important properties that must be fulfilled, some of these properties are contradictory and some others are heavily dependent on the task [8]. The quality measures used in this paper, their symbols, and formal definitions appear in Tables 1 and 2.

In supervised classification, a quality measure q ( P,C,  X  C )  X  R assigns a value to a pattern P ,which is larger when it better discriminates objects between the positive class C and the negative class  X  C (both classes form a partition of the universe U = C  X   X  C , C  X   X  C =  X  ).

For a pattern to be useful for classification, it should not be produced by chance and it should help in deciding the class of query objects. In the first case, we look for patterns that deviate from independence, while in the second case, we look for patterns that deviate from equilibrium [9].

Two random variables are independent if the joint probability is equal to the product of their respective probabilities. This way, a class C and a pattern P are independent if p ( PC )= p ( P ) p ( C ) . Since good from independence. Some quality measures directly test for deviation from independence, like Lift [10] (
A pattern is said to be in equilibrium if it is equally frequent in the two problem classes [9]. A pattern in equilibrium does not bias the classification to any class, so it can be considered useless. Conf = p ( C | P ) [11], for example, assigns 0.5 to patterns in equilibrium, assigning 1 one to pure librium comparing the probabilities of finding the pattern in the positive and negative classes. Disequi-librium can be also estimated by comparing the a-priori class distribution with the pattern distribution, using the  X  2 test [12] or the Pearson correlation coefficient [14].

There are many other quality measures that combine in certain way deviations from equilibrium and independence, and some other based on different properties that seem to work in particular contexts. 3. Quality measure correlations
Due to the large collection of existing quality measures, detecting correlations among them can help researchers to select the proper one for a given task and can alleviate the computational cost of experi-mental comparisons.

Lallich et al. [5] defines two quality measures Q 1 and Q 2 as equivalent if they assign the same order they show that some measures can be represented as affine transformations of other measures, namely Q can be represented as an affine transformation of Conf ,with  X  0 = 0and  X  1 =1 /p ( C ) . Nevertheless, affine transformations do not imply a good correlation. For example, we can express NetConf as an affine p different probabilities can be ordered differently by NetConf and Conf .

A different perspective of quality measure comparison can be obtained from empirical studies. Empir-statistical measure of correlation. There are two strategies to select the contrast patterns for evaluating quality measures: we can mine them from real databases or we can build a set of synthetic patterns.
Mining contrast patterns from real databases is used by Huynh et al. [9], which introduced a tool to correlate quality values from the mined patterns from a given database. Nevertheless, in this case no general conclusions can be extracted. Lenca et al. [17] found some groups of highly correlated quality measures using patterns extracted from 10 real databases. They found that the selected 20 measures can be split into three groups of highly correlated qualities. A similar study with more databases but fewer quality measures obtained similar results [3]. In general, using patterns extracted from real databases can bias results toward properties of patterns with lower support, because they are always much more frequent than patterns with higher supports.

To solve the biasing problem of using real databases, Tan et al. [19] found correlations from a set of randomly generated patterns. They generated 100 000 different patterns, with the only restriction that p ( random patterns does not really solve the biasing problem, it only transforms the known bias into an unknown bias. Second, patterns from different classes are compared among each other, which can hide hide correlations from measures that are equivalent.

After calculating quality measures, we need to co rrelate them. Huynh et al. u sed Pearson correla-tion [3,9], but it is a parametric correlation designed to detect linear relations among values. Therefore, it is not a good choice because quality values are not normally distributed and they can be related by non-linear relations. Tan et al. used the nonparametric correlation Spearman X  X   X  [19]. It has the advan-tage of not depending on the normal distribution, but its interpretation is not quite clear. Finally, Lenca et al. [17] used a variation of the nonparametric correlation Kendall X  X   X  , named  X  1 . Although authors equivalent, like Conf and Zhang .

To overcome the limitations of previous correlation analysis, we performed a correlation analysis with the following characteristics:  X  We created a collection of synthetic patterns formed by all the patterns of the positive class  X  Patterns belong to a fixed and balanced universe: p ( C )= p (  X  C )= 0.5. This way, we found correla- X  We performed correlations using Kendall X  X   X  [7]. Kendall X  X   X  is based on counting the number
After performing the correlation analysis, we found the following clusters, formed by those totally correlated quality measures.  X  Conf , GR , Brins , CConf , Cole , Dep , ExCex , InfGain , Lift , Sebag , Zhang .  X  OddsR , MDisc .  X  SupDif , WRACC , Acc .  X  Pearson ,  X  2 .  X  Cos , Streng .  X  Cover , Sup .

Measures in the same cluster can be freely interchanged for task dependent on pattern sorting, like pattern ranking, classification, and pattern filtering. In the following sections, we use a single repre-sentative of the each cluster. Therefore, all the results and conclusions involving the representative are totally valid for the remaining members of the cluster. 4. Comparing quality measures for classification and filtering
Quality measures are frequently used in supervised classification to rank patterns, to weight patterns, or to extract a small and representative pattern subset. Most papers about comparing quality measures compared their theoretical properties [9,17,19,35], or compared a limited number of quality measures used for some particular task [5,36].

In 2001, An and Cercone [20] presented an empirical evaluation of 10 quality measures used for rule evaluation in decision rule-based classifiers. To classify a query object, their classifier considers three cases:  X  Single match. If all the matched patterns belong to a single class, this class is returned as result.  X  Multiple match. If matching patterns belong to different classes, the class with the highest score is  X  No match. If the query object does not match any pattern, a partial matching is conducted.
Evaluating quality measures with this procedure has two drawbacks. First, the results depend on other parameters different from the quality function, like the partial matching procedure; these parameters might impact the classification accuracy and bias the conclusions. Second, as interpreting the result of a quality measure is a complex task for general measures [5], interpreting the meaning of the scores based on aggregating measured values is a more complex task.

Recently, Garc X a-Borroto et al. [3] introduced an experimental comparison of 10 quality measures used for classification and pattern filtering. They found that top accurate quality measures ( Conf , GR , and OddsR ) have a deceptive behavior for pattern filtering. This paper presents an extension of this research. 4.1. Experimental setup
We selected the following 61 databases from the UCI repository [37], which ranges from two to wine , wpbc ,and zoo . The number of objects per database ranges from 32 to 12960, the number of fea-tures ranges from 3 to 279, and there are numerical databases (22), categorical databases (17), and mixed databases (22).

To mine contrast patterns, we used the LCMine algorithm [6]. LCMine extracts contrast patterns from a collection of decision trees induced using a special procedure that guarantees diversity. Patterns are extracted from paths from the root node to the tree leaves [38], for every tree leaf with higher probability to a single class. That class is assigned as the class of the extracted pattern. Each decision tree is pruned to allow obtaining non-pure leaf nodes, so extracted patterns range from pure contrast patterns to patterns with small difference of support between classes.

Empirical studies about contrast patterns always depend on the characteristics of the patterns that can be extracted with the miner used. Nevertheless, LCMine extracts a high quality subset of the whole pat-tern collection existing in the database, that attains accuracies higher than other pattern based classifiers and comparable to some top-accurate classifiers like SVM [6]. That is why we consider that conclusions about the performance of quality measures using LCMine patterns can be extended to other pattern subsetsaswell.

For comparing classifier accuracies, we performed Friedman tests with all the results [39]. Then, when we found significant differences, we performed the Bergmann-Hommel dynamic post-hoc, because it is more powerful than the classical Nemenyi and Holm procedures [40]. Post-hoc results are shown using CD (critical distance) diagrams, which present the order of the classifier accuracy, the magnitude of differences between them, and the significance of the observed differences in a compact form [39]. In a CD diagram, the rightmost classifier is the best classifier, the classifier position within the segment is the rank value, and two classifiers sharing a thick line means they have statistically similar behavior. We use a 5  X  2 cross validation procedure, as suggested in [39]. 4.2. Comparing quality measures as aggregation function
Classifiers based on contrast patterns classify taking into account the evidence per class that they obtain from the contrast patterns previously mined. A successful scheme for condensing evidence is based on evidence aggregation, where each pattern matching the query object gives a vote. Finally, the query object is assigned to the class with highest aggregated vote. Although the most used vote for aggregation is pattern support [13], in this section, we test as votes the values of the quality measures. This comparison has the following purposes:  X  Determine which quality measures have a more meaningful value. This is based on the hypothesis  X  Avoid a potential bias in the remaining experiments, that could boost accuracies of quality measures
In this experiment, we classify the objects in the training sample aggregating the votes emitted by each quality m easure. The classifier accuracy is then used as an estimation of the quali ty measure behavior. As previous correlation results cannot be used here, because we cannot ignore the quality value, we use all the quality measures, instead of the reduced set we obtained before.

Results, appearing in Fig. 1, show that there is no significant difference among aggregating using the top accurate quality measures. Neverth eless, we select the top thre e measures for the remaining experiments including accuracy evaluations: Lap , Cos ,and Klos . 4.3. Comparing quality measures for classification
As good qualities assign higher values to more discriminant patterns, it is usual to evaluate a quality measure using the accuracy of a classifier that uses it during the classification process. Nevertheless, most classifiers contain parameters to tune that impact the classifier accuracy (like thresholds, aggregation scheme, and normalization procedures). Consequently, using the classifier accuracy as an estimation of the behavior of the quality m easure can be inaccurate.

To minimize the parameter influence in the classification accuracy, we performed the accuracy com-parison using a simple classification algorithm (Algorithm 1). This algorithm assigns the query object O to the class with the maximum aggregated support, calculated with the top-quality patterns from those contained in O . Since this classifier is mostly based on the quality values, we expect that accuracy dif-ferences are mostly due to the quality measure behavior. This way, we can safely estimate the behavior of the quality m easure based on the accuracy. To avoid the b ias of the aggregation procedure, we use different voting values calculated using the topmost accurate measures: Lap , Cos ,and Klos . Algorithm 1 : Classification algorithm used in the experiments Data : PS  X  Set of patterns, QM  X  quality measure, O  X  query object Result : Class assigned to O
Find S = { P  X  PS : P  X  O } ;
MaxQual  X  X  X  argmax s ( QM ( s )) ;
S  X  X  X  { s  X  S : QM ( s )= MaxQual } ; return class with maximum aggregated vote of patterns in S
Accuracy results appear in Figs 3(a) X (c). In the figures, Base refers to the accuracy of the classifier using all the patterns with the traditional aggregation of pattern support. We can extract the following conclusions:  X  Quality measures are almost identically ordered using the three different aggregating functions.  X  Top accurate measures are Conf , OddsR , Lap , Length , Cos , MutInf ,and Klos . Statistical tests prove  X  Quality measures Lever , Spec , RelRisk , NetConf ,and Gain are significantly less accurate than the 4.4. Comparing quality measures for pattern filtering
Pattern filtering is about selecting the smallest pattern subset that does not significantly deteriorate the classifier accuracy [6]. This step helps researchers to understand the classification model and the classification results, because mine rs usually extract a lar ge collection of contrast patterns (frequently, much more than the number of objects in the training sample).

In this section, we use a filtering algorithm that uses a greedy heuristic to find the smallest pattern ation [6]. Consequently, better quality measures are expected to obtain smaller pattern subsets that does not degrade the classification accuracy. The filtering algorithm appears in Algorithm 2. It iteratively adds to the result collection R the pattern with the highest quality from those that cover at least one object from the objects not covered by patterns in R . To alleviate the dependence on the order of objects in T , we repeat each experiment 10 times with different scrambled orders.
 Algorithm 2 : Filtering algorithm used in the experiments Data : PS  X  Set of patterns, QM  X  quality measure, T  X  training sample Result : Selected patterns R
R  X  X  X   X  ; foreach O  X  T do return R Filtering results (Figs 4(a) X (c) and 2) show that the following quality measures did not significantly and Pearson . On the other hand, quality measures Cos , SupDif ,and Pearson return a result with a good tradeoff between accuracy and reduction ratio.

To explain why the top accu rate quality measures, Conf and OddsR , are not good for filtering, we should realize that these qualitie s cannot distinguish among pattern s with zero support in the negative class. This makes two patterns with really different supports in the positive class to be consider as equivalent patterns, which contradicts the Generality/Coverage principle [17]. For example, lets suppose have confidence equal to one, although pattern P 2 appears in all the objects of the positive class while P 1 of Conf to distinguish between pure patterns with different supports, has a significantly better behavior than Conf . 5. Conclusions
Quality measures are important tools for reducing the usually large collection of contrast patterns mined by automatic procedures. They can be also useful to find and remove contrast patterns due to chance or to biases in the data collection. Additionally, a quality measure can help to select the best patterns to classify a query object, if the object matches patterns from different classes.
In this research, we found families of quality measures that assign the same order to any pair of contrast patterns, so they produce the same results for supervised classification tasks like classification and pattern filtering. Using the results of the correlation experiment, we reduced the amount of quality measures to evaluate in further experiments from 33 to 16.
 For obtaining a higher classification accuracy, an empirical comparison with patterns mined by LCMine revealed that the best qualities are Conf , OddsR , Lap , Length , MutInf , Cos ,and Klose ,inthat order. Results obtained with these quality measures allow classifying with similar results than using the whole pattern collection, but using a reduced set of patterns per query object. These results were proved not to be dependent on the aggregation procedure.

Filtering experiments with the same patterns showed that th e most accurate quality measures Conf and OddsR have a significantly worse behavior on pattern filtering, since they can not distinguish among pure patterns.
 Acknowledgments
This work was partly supported by the National Council of Science and Technology of Mexico (CONACyT) through the project grants CB2008-106443 and CB2008-106366; and the scholarship grant 370272.
 References
