 Clustering validation is a long standing challenge in the clus-tering literature. While many validation measures have been developed for evaluating the performance of clustering algo-rithms, these measures often provide inconsistent informa-tion about the clustering performance and the best suitable measures to use in practice remain unknown. This paper thus fills this crucial void by giving an organized study of 16 external validation measures for K-means clustering. Specif-ically, we first introduce the importance of measure normal-ization in the evaluation of the clustering performance on data with imbalanced class distributions. We also provide normalization solutions for several measures. In addition, we summarize the major properties of these external measures. These properties can serve as the guidance for the selection of validation measures in different application scenarios. Fi-nally, we reveal the interrelationships among these external measures. By mathematical transformation, we show that some validation measures are equivalent. Also, some mea-sures have consistent validation performances. Most impor-tantly, we provide a guide line to select the most suitable validation measures for K-means clustering.
 H.2.8 [ Database Management ]: Database Applications X  Data Mining ; I.5.3 [ Pattern Recognition ]: Clustering Measurement, Experimentation Cluster Validation, External Criteria, K-means
Clustering validation has long been recognized as one of the vital issues essential to the success of clustering appli-cations [10]. Despite the vast amount of expert endeavor spent on this problem [7], there is no consistent and conclu-sive solution to cluster validation. The best suitable mea-sures to use in practice remain unknown. Indeed, there are many challenging validation issues which have not been fully addressed in the clustering literature. For instance, the im-portance of normalizing validation measures has not been fully established. Also, the relationship between different validation measures is not clear. Moreover, there are impor-tant properties associated with validation measures which are important to the selection of the use of these measures but have not been well characterized. Finally, given the fact that different validation measures may be appropriate for different clustering algorithms, it is necessary to have a focused study of cluster validation measures on a specified clustering algorithm at one time.

To that end, in this paper, we limit our scope to provide an organized study of external validation measures for K-means clustering [14]. The rationale of this pilot study is as follows. K-means is a well-known, widely used, and successful clus-tering method. Also, external validation measures evaluate the extent to which the clustering structure discovered by a clustering algorithm matches some external structure, e.g., the one specified by the given class labels. From a practi-cal point view, external clustering validation measures are suitable for many application scenarios. For instance, if ex-ternal validation measures show that a document clustering algorithm can lead to the clustering results which can match the categorization performance by human experts, we have a good reason to believe this clustering algorithm has a prac-tical impact on document clustering.

Along the line of adapting validation measures for K-means, we present a detailed analysis of 16 external vali-dation measures, as shown in Table 1. Specifically, we first establish the importance of measure normalization by high-lighting some unnormalized measures which have issues in the evaluation of the clustering performance on data with imbalanced class distributions. In addition, to show the im-portance of measure normalization, we also provide normal-ization solutions for several measures. The key challenge here is to identify the lower and upper bounds of validation measures. Furthermore, we reveal some major properties of these external measures, such as consistency, sensitivity, and symmetry properties. These properties can serve as the guidance for the selection of validation measures in different application scenarios. Finally, we also show the interrela-tionships among these external measures. We show that some validation measures are equivalent and some measures have consistent validation performances.
Most importantly, we provide a guide line to select the most suitable validation measures for K-means clustering. After carefully profiling these validation measures, we be-lieve it is most suitable to use the normalized van Dongen criterion ( V D n ) which has a simple computation form, satis-fies mathematically sound properties, and can measure well on the data with imbalanced class distributions. However, for the case that the clustering performance is hard to dis-tinguish, we may want to use the normalized Variation of Information ( V I n ) instead, since the measure V I n has high sensitivity on detecting the clustering changes.
In this section, we introduce a suite of 16 widely used ex-ternal clustering validation measures. To the best of our knowledge, these measures represent a good coverage of the validation measures available in different fields, such as data mining, information retrieval, machine learning, and statis-tics. A common ground of these measures is that they can be computed by the contingency matrix as follows.
The Contingency Matrix. Given a data set D with n objects, assume that we have a partition P = { P 1 , , P K of D , where S K i =1 P i = D and P i T P j =  X  for 1  X  i 6 = j  X  K , and K is the number of clusters. If we have  X  X rue X  class labels for the data, we can have another partition on D : C = { C 1 , , C K  X  } , where S K  X  i =1 C i = D and C i T C for 1  X  i 6 = j  X  K  X  , where K  X  is the number of classes. Let n ij denote the number of objects in cluster P i from class C j , then the information on the overlap between the two partitions can be written in the form of a contingency matrix, as shown in Table 2. Throughout this paper, we will use the notations in this contingency matrix.

The Measures. Table 1 shows the list of measures to be studied. The  X  X efinition X  column gives the computation forms of the measures by using the notations in the contin-gency matrix. Next, we briefly introduce these measures.
The entropy and purity are frequently used external mea-sures for K-means [20, 26]. They measure the  X  X urity X  of the clusters with respect to the given class labels.
F-measure was originally designed for the evaluation of hi-erarchical clustering [19, 13], but has also been employed for partitional clustering. It combines the precision and recall concepts from the information retrieval community.
The Mutual Information (MI) and Variation of Informa-tion (VI) were developed in the field of information the-ory [3]. MI measures how much information one random variable can tell about another one [21]. VI measures the amount of information that is lost or gained in changing from the class set to the cluster set [16].
 The Rand statistic [18], Jaccard coefficient, Fowlkes and Mallows index [5], and Hubert X  X  two statistics [8, 9] evaluate the clustering quality by the agreements and/or disagree-ments of the pairs of data objects in different partitions.
The Minkowski score [1] measures the difference between the clustering results and a reference clustering (true clus -ters). And the difference is computed by counting the dis-agreements of the pairs of data objects in two partitions.
The classification error takes a classification view on clus-tering [2]. It tries to map each class to a different cluster so as to minimize the total misclassification rate. The  X   X   X  in Table 1 is the mapping of class j to cluster  X  ( j ).
The van Dongen criterion [23] was originally proposed for evaluating graph clustering. It measures the representative-ness of the majority objects in each class and each cluster.
Finally, the micro-average precision, Goodman-Kruskal coefficient [6] and Mirkin metric [17] are also popular mea-sures. However, the former two are equivalent to the purity measure and the Mirkin metric is equivalent to the Rand statistic ( M/ 2 ` n 2  X  + R = 1). As a result, we will not discuss these three measures in the future sections.
 In summary, we have 13 (out of 16) candidate measures. Among them, P , F , M I , R , J , F M ,  X , and  X   X  are posi-tive measures  X  a higher value indicates a better clustering performance. The remainder, however, consists of measures based on the distance notion. Throughout this paper, we will use the acronyms of these measures.
In this section, we present some validation measures which will produce misleading validation results for K-means on data with skewed class distributions.
One of the unique characteristic of K-means clustering is the so-called uniform effect; that is, K-means tends to produce clusters with relatively uniform sizes [25]. To quan-tify the uniform effect, we use the coefficient of variation ( CV ) [4], a statistic which measures the dispersion degree of a random distribution. CV is defined as the ratio of the standard deviation to the mean. Given a sample data ob-jects X = { x 1 , x 2 , . . . , x n } , we have CV = s/  X  x , where  X  x = P n i =1 x i /n and s = p P n i =1 ( x i  X   X  x ) 2 / ( n  X  1). CV is a dimensionless number that allows the comparison of the variations of populations that have significantly differ-ent mean values. In general, the larger the CV value is, the greater the variability in the data.

Example. Let CV 0 denote the CV value of the  X  X rue X  class sizes and CV 1 denote the CV value of the resultant cluster sizes. We use the sports data set [22] to illustrate the uniform effect by K-means. The  X  X rue X  class sizes of sports have CV 0 = 1 . 02. We then use the CLUTO imple-mentation of K-means [11] with default settings to cluster sports into seven clusters. We also compute the CV value of the resultant cluster sizes and get CV 1 = 0 . 42. Therefore, the CV difference is DCV = CV 1  X  CV 0 =  X  0 . 6, which indicates a significant uniform effect in the clustering result.
Indeed, it has been empirically validated that the 95% confidence interval of CV 1 values produced by K-means is in [0.09, 0.85] [24]. In other words, for data sets with CV values greater than 0.85, the uniform effect of K-means can distort the cluster distribution significantly.

Now the question is: Can these widely used validation measures capture the negative uniform effect by K-means clustering? Next, we provides a necessary but not sufficient criterion to testify whether a validation measure can be ef-fectively used to evaluate K-means clustering.
Assume that we have a sample document data containing 50 documents from 5 classes. The class sizes are 30, 2, 6, 10 and 2, respectively. Thus, we have CV 0 = 1 . 166, which implies a skewed class distribution.

For this sample data set, we assume there are two clus-tering results as shown in Table 3. In the table, the first result consists of five clusters with extremely balanced sizes. This is also indicated by CV 1 = 0. In contrast, for the second result, the five clusters have varied cluster sizes with CV 1 = 1 . 125, much closer to the CV value of the X  X rue X  X lass sizes. Therefore, from a data distribution point of view, the second result should be better than the first one.
Indeed, if we take a closer look on contingency Matrix I in Table 3, we can find that the first clustering partitions the objects of the largest class C 1 into three balanced sub-clusters. Meanwhile, the two small classes C 2 and C 5 are totally  X  X isappeared X   X  they are overwhelmed in cluster P by the objects from class C 3 . In contrast, we can easily identify all the classes in the second clustering result, since they have the majority objects in the corresponding clus-ters. Therefore, we can draw the conclusion that the first clustering is indeed much worse than the second one.
As shown in Section 3.1, K-means tends to produce clus-ters with relatively uniform sizes. Thus the first clustering i n Table 3 can be regarded as the negative result of the uniform effect. So we establish the first necessary but not sufficient criterion for selecting the measures for K-means as follows.
Criterion 1. If an external validation measure cannot capture the uniform effect by K-means on data with skewed class distributions, this measure is not suitable for validat ing the results of K-means clustering.
 Next, we proceed to see which existing external cluster val-idation measures can satisfy this criterion.
Table 4 shows the validation results for the two cluster-ings in Table 3 by all 13 external validation measures. We highlighted the better evaluation of each validation measure.
As shown in Table 4, only three measures, E , P and M I , cannot capture the uniform effect by K-means and their vali-dation results can be misleading. In other words, these mea-sures are not suitable for evaluating the K-means clustering. These three measures are defective validation measures. Here, we explore the issues with the defective measures. First, the problem of the entropy measure lies in the fact that it cannot evaluate the integrity of the classes.
We know E =  X  P i p i P j p ij p dom variable view on cluster P and class C , then p ij = n ij /n is the joint probability of the event: { P = P i V C = C } , and p i = n i /n is the marginal probability. There-fore, E = P i p i P j  X  p ( C j | P i ) log p ( C j | P i = H ( C | P ), where H ( ) is the Shannon entropy [3]. The above implies that the entropy measure is nothing but the conditional entropy of C on P . In other words, if the objects in each large partition are mostly from the same class, the entropy value tends to be small (indicating a better cluster-ing quality). This is usually the case for K-means clustering on highly imbalanced data sets, since K-means tends to par-tition a large class into several pure sub-clusters. This leads to the problem that the integrity of the objects from the same class has been damaged. The entropy measure cannot capture this information and penalize it.

The mutual information is strongly related to the en-tropy measure. We illustrate this by the following Lemma.
Lemma 1. The mutual information measure is equivalent to the entropy measure for cluster validation.
 Proof. By information theory, M I = P i P j p ij log p ij H ( C )  X  H ( C | P ) = H ( C )  X  E . Since H ( C ) is a constant for any given data set, M I is essentially equivalent to E . 2
The purity measure works in a similar fashion as the entropy measure. That is, it measures the  X  X urity X  of each cluster by the ratio of the objects from the majority class. Thus, it has the same problem as the entropy measure for evaluating K-means clustering.

In summary, entropy, purity and mutual information are defective measures for validating K-means clustering.
Here, we give the improved versions of the above three de-fective measures: entropy, mutual information, and purity.
Lemma 2. The Variation of Information measure is an improved version of the entropy measure.

Proof. If we view cluster P and class C as two random variables, it has been shown that V I = H ( C ) + H ( P )  X  2 M I = H ( C | P ) + H ( P | C ) [16]. The component H ( C | P ) is nothing but the entropy measure, and the component H ( P | C ) is a valuable supplement to H ( C | P ). That is, H ( P | C ) evaluates the integrity of each class along different clusters. Thus, we complete the proof. 2 By Lemma 1, we know M I is equivalent to E . Therefore, V I is also an improved version of M I .

Lemma 3. The van Dongen criterion is an improved ver-sion of the purity measure.
 of the classes and is a supplement to the purity measure. 2
In this section, we show the importance of measure nor-malization and provide normalization solutions to some mea-sures whose normalized forms are not available.
Generally speaking, normalizing techniques can be divided into two categories. One is based on a statistical view, which formulates a baseline distribution to correct the measure for randomness. A clustering can then be termed  X  X alid X  if it has an unusually high or low value, as measured with respect to the baseline distribution. The other technique uses the minimum and maximum values to normalize the measure into the [0,1] range. We can also take a statistical view on this technique with the assumption that each measure takes a uniform distribution over the value interval.
 The Normalizations of R , F M ,  X  ,  X   X  , J and MS . The normalization scheme can take the form as where max( S ) is the maximum value of the measure S , and E ( S ) is the expected value of S based on the baseline distri-bution. Some measures derived from the statistics commu-nity, such as R , F M ,  X  and  X   X  , usually take this scheme.
Specifically, Hubert and Arabie (1985) [9] suggested to use the multivariate hypergeometric distribution as the baseline distribution in which the row and column sums are fixed in Table 2, but the partitions are randomly selected. This determines the expected value as follows.

Based on this value, we can easily compute the expected values of R , F M ,  X  and  X   X  respectively, since they are the linear functions of P i P j ` n ij 2  X  under the hypergeometric distribution assumption. Furthermore, although the exact maximum values of the measures are computationally pro-hibited under the hypergeometric distribution assumption, we can still reasonably approximate them by 1. Then, ac-cording to Equation (1) and (2), we can finally have the normalized R , F M ,  X  and  X   X  measures, as shown in Table 5.
The normalization of J and M S is a little bit complex, since they are not linear to P i P j ` n ij 2  X  . Nevertheless, we can still normalize the equivalent measures converted from
It is easy to show J  X   X  J and M S  X   X  M S . Then based on the hypergeometric distribution assumption, we have the normalized J  X  and M S  X  as shown in Table 5. Since J  X  and clustering, we normalize them by modifying Equation (1) as S n = ( S  X  min( S )) / ( E ( S )  X  min( S )).

Finally, we would like to point out some interrelationships between these measures as follows.

Proposition 1.
The above proposition indicates that the normalized Hu-bert  X  statistic I ( X  n ) is the same as  X . Also, the normalized Rand statistic ( R n ) is the same as the normalized Hubert  X  statistic II ( X   X  n ). In addition, the normalized Rand statis-Therefore, we have three independent normalized measures including R n , F M n and  X  n for further study. Note that this proposition can be easily proved by mathematical transfor-mation. Due to the space limitation, we omit the proof.
The Normalizations of V I and V D . Another nor-Some measures, such as V I and V D , often take this scheme. However, to know the exact maximum and minimum values is often impossible. So we usually turn to a reasonable ap-proximation, e.g., the upper bound for the maximum, or the lower bound for the minimum.

When the cluster structure matches the class structure perfectly, V I = 0. So, we have min( V I ) = 0. However, finding the exact value of max( V I ) is computationally in-feasible. Meila suggested to use 2 log max( K, K  X  ) to approx-
The V D in Table 1 can be regarded as a normalized mea-sure. In this measure, 2 n has been taken as the upper bound [23], and min( V D ) = 0.
However, we found that the above normalized V I and V D cannot well capture the uniform effect of K-means, because the proposed upper bound for V I or V D is not tight enough. Therefore, we propose new upper bounds as follows.
Lemma 4. Let random variables C and P denote the class and cluster sizes respectively, H( ) be the entropy function, then V I  X  H ( C ) + H ( P )  X  2 log max( K  X  , K ) . Lemma 4 gives a tighter upper bound H ( C ) + H ( P ) than 2 log max( K  X  , K ) which was provided by Meila [16]. With this new upper bound, we can have the normalized V I n as shown in Table 5. Also, we would like to point out that, if we use H ( P ) / 2 + H ( C ) / 2 as the upper bound to normal-ize mutual information, the V I n can be equivalent to the normalized mutual information M I n ( V I n + M I n = 1).
Lemma 5. Let n i , n j and n be the values in Table 2, then V D  X  (2 n  X  max i n i  X  max j n j ) / 2 n  X  1 . Due to the page limit, we omit some proofs. The above two lemmas imply that the tighter upper bounds of V I and V D are the functions of the class and cluster sizes. Using these two new upper bounds, we can derive the normalized V I n and V D n in Table 5.

The Normalization of F and  X  have been seldom dis-cussed in the literature. As we know, max( F ) = 1. Now the goal is to find a tight lower bound. In the following, we propose a procedure to find the lower bound of F .
With the above procedure, we can have the following lemma, which finds a lower bound for F .
 Lemma 6. Given F  X  computed by Procedure 1, F  X  F  X  . Therefore, F n = ( F  X  F  X  ) / (1  X  F  X  ), as listed in Table 5. Finally, as to  X  , we have the following lemma.
 Lemma 7. Given K  X   X  K ,  X   X  1  X  1 /K.

Therefore, we can use 1  X  1 /K as the upper bound of  X  , and the normalized  X  n is shown in Table 5.
Here, we present some experiments to show the impor-tance of DCV ( CV 1  X  CV 0 ) for selecting validation measures.
Experimental Data Sets. Some synthetic data sets were generated as follows. Assume we have a two-dimensional mixture of two Gaussian distributions. The means of the two distributions are [-2,0] and [2,0], respectively. And their covariance matrices are exactly the same as [  X  2 0; 0  X  2
Therefore, given any specific value of  X  2 , we can gener-ate a simulated data set with 6000 instances, n 1 instances from the first distribution, and n 2 instances from the sec-ond one, where n 1 + n 2 = 6000. To produce simulated data sets with imbalanced class sizes, we set a series of n 1 val-ues: { 3000, 2600, 2200, 1800, 1400, 1000, 600, 200 } . If n 1 = 200, n 2 = 5800, we have a highly imbalanced data set with CV 0 = 1 . 320. For each mixture model, we generated Figure 1: A Simulated Data Set ( n 1 = 1000 ,  X  2 = 2 . 5 ). 8 simulated data sets with CV 0 ranging from 0 to 1.320. Further, to produce data sets with different clustering ten-dencies, we set a series of  X  2 values: { 0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5 } . As  X  2 increases, the mixture model tends to be more unidentifiable. Finally, for each pair of  X  2 and n we repeated the sampling 10 times, thus we can have the average performance evaluation. In summary, we produced 8  X  10  X  10 = 800 data sets. Figure 1 shows a sample data set with n 1 = 1000 and  X  2 = 2 . 5.

We also did sampling on a real-world data set hitech to get some sample data sets with imbalanced class dis-tributions. This data set was derived from the San Jose Mercury newspaper articles [22], which contains 2301 doc-uments about computers, electronics, health, medical, re-search and technology. Each document is characterized by 126373 terms, and the class sizes are 485, 116, 429, 603, 481 and 187, respectively. We carefully set the sampling ratio for each class, and get 8 sample data sets with the class-size distributions ( CV 0 ) ranging from 0.490 to 1.862, as shown in Table 6. For each data set, we repeated sampling 10 times, so we can observe the averaged clustering performance. Experimental Tools. We used the MATLAB 7.1 [15] and CLUTO 2.1.1 [11] implementations of K-means. The MAT-LAB version with the squared Euclidean distance is suitable for low-dimensional and dense data sets, while CLUTO with the cosine similarity is used to handle high-dimensional and sparse data sets. Note that the number of clusters, i.e., K , was set to match the number of  X  X rue X  classes.

The Application of Criterion 1. Here, we show how we can apply Criterion 1 for selecting measures. As pointed out in Section 3.1, K-means tends to have the uniform effect on imbalanced data sets. This implies that for data sets with skewed class distributions, the clustering results by K-means tend to be away from  X  X rue X  class distributions.

To further illustrate this, let us take a look at Figure 2(a) of the simulated data sets. As can be seen, for the extreme case of  X  2 = 5, the DCV values decrease as the CV 0 values increase. Note that DCV values are usually negative since K-means tends to produce clustering results with relative uniform cluster sizes ( CV 1 &lt; CV 0 ). This means that, when data become more skewed, the clustering results by K-means tend to be worse. From the above, we know that we can select measures by observing the relationship between the measures and the DCV values. As the DCV values go down, the good measures are expected to show worse clustering performances. Note that, in this experiment, we applied the MATLAB version of K-means.

A similar trend can be found in Figure 2(b) of the sampled data sets. That is, as the CV 0 values go up, the DCV val-ues decrease, which implies worse clustering performances. Indeed, DCV is a good indicator for finding the measures which cannot capture the uniform effect by K-means cluster-ing. Note that, in this experiment, we applied the CLUTO version of K-means clustering.

In the next section, we use the Kendall X  X  rank correlation (  X  ) [12] to measure the relationships between external val-idation measures and DCV. Note that,  X   X  [  X  1 , 1].  X  = 1 indicates a perfect positive rank correlation, whereas  X  =  X  1 indicates an extremely negative rank correlation.
In this subsection, we show the importance of measure normalization. Along this line, we first apply K-means clus-tering on the simulated data sets with  X  2 = 5 and the sam-pled data sets from hitech . Then, both unnormalized and normalized measures are used for cluster validation. Finally, the rank correlation between DCV and the measures are computed and the results are shown in Table 7.

As can be seen in the table, if we use the unnormal-ized measures to do cluster validation, only three measures, namely R ,  X ,  X   X  , have strong consistency with DCV on both groups of data sets. V I , V D and M S even show strong con-Figure 3: Un-normalized and Normalized Measures.
 flict with DCV on the sampled data sets, since their  X  values are all close to -1 on sampled data. In addition, we notice that F ,  X  , J and F M show weak correlation with DCV .
Table 7 shows the rank correlations between DCV and the normalized measures. As can be seen, all the normalized measures show perfect consistency with DCV except for F n and  X  n . This indicates that the normalization is crucial for evaluating K-means clustering. The proposed bounds for the measures are tight enough to capture the uniform effect in the clustering results.

In Table 7, we can observe that both F n and  X  n are not consistent with DCV . This indicates that normalization does not help F and  X  too much. The reason is that the pro-posed lower bound for F and upper bound for  X  are not very tight. Indeed, the normalizations of F and  X  are very chal-lenging. This is due to the fact that they both exploit rel-atively complex optimization schemes in the computations. As a result, we cannot easily compute the expected values from a multivariate hypergeometric distribution perspective, and it is also difficult to find tighter bounds.

Nevertheless, the above experiments show that the nor-malization is very valuable. In addition, Figure 3 shows the cluster validation results of the measures on all the simu-lated data sets with  X  2 ranging from 0.5 to 5. It is clear that the normalized measures have much wider value range than the unnormalized ones along [0,1]. This indicates that the values of normalized measures are more spread in [0, 1].
In summary, to compare cluster validation results across different data sets, we should use normalized measures.
In this section, we investigate measure properties, which can serve as the guidance for the selection of measures.
Here, we define the consistency between a pair of mea-sures in terms of the similarity between their rankings on a series of clustering results. The similarity is measured by the Kendall X  X  rank correlation. And the clustering results are produced by the CLUTO version of K-means clustering on 29 benchmark real-world data sets listed in Table 8. In the experiment, for each data set, the cluster number is set to be the same as the  X  X rue X  class number.

Figure 4(a) and 4(b) show the correlations between the unnormalized and normalized measures, respectively. One interesting observation is that the normalized measures have much stronger consistency than the unnormalized measures. For instance, the correlation between V I and R is merely  X  0 . 21, but it reaches 0 . 74 for the corresponding normalized measures. This observation indeed implies that the normal-ized measures tend to give more robust validation results, which also agrees with our previous analysis.
 Let us take a closer look on the normalized measures in Figure 4(b). According to the colors, we can roughly find that R n ,  X   X  n , J  X  n , M S  X  n , F M n and  X  n are more similar to one another, while V D n , F n , V I n and  X  n show inconsis-tency with others in varying degrees. To gain the precise understanding, we do hierarchical clustering on the mea-sures by using their correlation matrix. The resultant hier-archy can be found in Figure 5 ( X  X  X  means the similarity). As we know before, R n ,  X   X  n , J  X  n and M S  X  n are equivalent, so they have perfect correlation to one another, and form the first group. The second group contains F M n and  X  n . These two measures behave similarly, and have just slightly weaker consistency with the measures in the first group. Finally, V D n , F n ,  X  n and V I n have obviously weaker consistency with other measures in a descending order.

Furthermore, we explore the source of the inconsistency among the measures. To this end, we divide the data sets in Table 8 into two repositories, where R 1 contains data sets with CV 0 &lt; 0 . 8, and R 2 contains the rest. Then we compute the correlation matrices of the measures on the two repositories respectively (denoted by M ( R 1 ) and M ( R and observe their difference ( M ( R 1 )  X  M ( R 2 )) in Table 9. As can be seen, roughly speaking, all the measures except V I n show weaker consistency with one another on data sets in R 2 . In other words, while V I n acts in the opposite way, most measures tend to disagree with one another on data sets with highly imbalanced classes.
In this subsection, we investigate some key properties of external clustering validation measures.

The Sensitivity. The measures have different sensitivity to the clustering results. Let us illustrate this by an exam-ple. For two clustering results in Table 10, the differences between them are the numbers in bold. Then we employ the measures on these two clusterings. Validation results are shown in Table 11. As can be seen, all the measures show different validation results for the two clusterings ex-cept for V D n and F n . This implies that V D n and F n are less sensitive than other measures. This is due to the fact that both V D n and F n use maximum functions, which may loose some information in the contingency matrix. Further-more, V I n is the most sensitive measure, since the difference of V I n values for the two clusterings is the largest.
Impact of the Number of Clusters. We use the data set la2 in Table 8 to show the impact of the number of clus-ters on the validation measures. Here, we change the cluster numbers from 2 to 15. As shown in Figure 6, the measure-ment values for all the measures will change as the increase of the cluster numbers. However, the normalized measures including V I n , V D n and R n can capture the same optimal cluster number 5. Similar results can also be observed for other normalized measures, such as F n , F M n and  X  n .
A Summary of Math Properties. We summarize five math properties of measures as follows. Due to the space limit, we omit the proofs here.

Property 1 (Symmetry). A measure O is symmet-ric, if O ( M T ) = O ( M ) for any contingence matrix M .
The symmetry property treats the pre-defined class struc-ture as one of the partitions. Therefore, the task of cluster validation is the same as the comparison of partitions. This means transposing two partitions in the contingency matrix should not bring any difference to the measure value. This property is not true for F n which is a typical measure in asymmetry. Also,  X  n is symmetric if and only if K = K  X  .
Property 2 (N-invariance). For a contingence ma-trix M and a positive integer  X  , a measure O is n-invariant, if O (  X M ) = O ( M ) , where n is the number of objects.
Intuitively, a mathematically sound validation measure should satisfy the n-invariance property. However, three measures, namely R n , F M n and  X  n cannot fulfill this re-quirement. Nevertheless, we can still treat them as the asymptotically n-invariant measures, since they tend to be n-invariant as the increase of n .
 Property 3 (Convex additivity). Let P = { P 1 , , P
K } be a clustering, P  X  be a refinement of P 1 , and P  X  the partitioning induced by P  X  on P l . Then a measure O is convex additive, if O ( M ( P, P  X  )) = P K l =1 n l n O ( M ( I where n l is the number of data points in P l , I P l represents the partitioning on P l into one cluster, and M ( X, Y ) is the contingency matrix of X and Y .
 The convex additivity property was introduced by Meila [16]. It requires the measures to show additivity along the lattice of partitions. Unnormalized measures including F , V D , V I and  X  hold this property. However, none of the normalized measures studied in this paper holds this property.
Property 4 (Left-domain-completeness). A mea-sure O is left-domain-complete, if, for any contingence ma-trix M with statistically independent rows and columns,
When the rows and columns in the contingency matrix are statistically independent, we should expect to see the poor-est values of the measures, i.e., 0 for positive measures and 1 for negative measures. Among all the measures, however, only V I n and V D n can meet this requirement.

Property 5 (Right-domain-completeness). A mea-sure O is right-domain-complete, if, for any contingence ma-trix M with perfectly matched rows and columns,
This property requires measures to show optimal values when the class structure matches the cluster structure per-fectly. The above normalized measures hold this property.
In a nutshell, among 16 external validation measures shown in Table 1, we first know that Mirkin metric ( M ) is equiv-alent to Rand statistic ( R ), and micro-average precision ( M AP ) and Goodman-Kruskal coefficient ( GK ) are equiva-lent to the purity measure ( P ) by observing their computa-tional forms. Therefore, the scope of our measure selection is reduced from 16 measures to 13 measures. In Section 3, our analysis shows that purity, mutual information ( M I ), and entropy (E) are defective measures for evaluating K-means clustering. Also, we know that variation of information ( V I ) is an improved version of M I and E , and van Dongen cri-terion ( V D ) is an improved version of P . As a result, our selection pool is further reduced to 10 measures.
In addition, as shown in Section 4, it is necessary to use the normalized measures for evaluating K-means clustering, since the normalized measures can capture the uniform ef-fect by K-means and allow to evaluate different clustering results on different data sets. By Proposition 1, we know  X  P  X  be a refinement of P  X  means P  X  is the descendant node of node P in the lattice of partitions. See [16] for details. that the normalized Rand statistic ( R n ) is the same as the Therefore, we only need to further consider R n and can ex-clude J  X  n ,  X   X  n as well as M S  X  n . The results in Section 4 show that the normalized F-measure ( F n ) and classification er-ror (  X  n ) cannot well capture the uniform effect by K-means. Also, these two measures do not satisfy some math proper-ties in Table 12. As a result, we can exclude them. Now, we have five normalized measures: V I n , V D n , R n , F M and  X  n . In Figure 5, we know that the validation perfor-mances of R n , F M n , and  X  n are very similar to each other. Therefore, we only need to consider to use R n .

From the above study, we believe it is most suitable to use the normalized van Dongen criterion ( V D n ), since V D has a simple computation form, satisfies all mathematically sound properties as shown in Table 12, and can measure well on the data with imbalanced class distributions. However, for the case that the clustering performances are hard to distinguish, we may want to use the normalized variation of information ( V I n ) instead 2 , since V I n has high sensitivity on detecting the clustering changes. Finally, R n can also be used as a complementary to the above two measures.
In this paper, we compared and contrasted external val-idation measures for K-means clustering. As our results revealed, it is necessary to normalize validation measures before they can be employed for clustering validation, since unnormalized measures may lead to inconsistent or even mis-leading results. This is particularly true for data with im-balanced class distributions. Along this line, we also provide normalization solutions for the measures whose normalized solutions are not available. Furthermore, we summarized the key properties of these measures. These properties should be considered before deciding what is the right measure to use in practice. Finally, we investigated the relationships among these validation measures. The results showed that some validation measures are mathematically equivalent and some measures have very similar validation performances. This research was partially supported by the National Natural Science Foundation of China (NSFC) (No. 70621061, 70890082, and 70521001), the Rutgers Seed Funding for Col-laborative Computing Research, and the National Science Foundation (NSF) of USA via grant number CNS 0831186.
Note that the normalized variation of information is equiv-alent to the normalized mutual information.
