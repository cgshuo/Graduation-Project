 We consider ordinary least-squares regression using randomly generated feature spaces. Let us first compact subset of R d , and y n  X  R ), assumed to be independently and identically distributed (i.i.d.) with x n  X  X  and noise of variance bounded by  X  2 . We assume that L and  X  are known.
 Now, for a given class of functions F , and f  X  X  , we define the empirical ` 2 -error and the generalization error to optimality.
 In this paper we consider infinite dimensional spaces F that are generated by a denumerable family of functions {  X  i } i  X  1 , called initial features (such as wavelets). We will assume that f  X   X  X  . Since F is an infinite dimensional space, the empirical risk minimizer in F is certainly subject to overfitting. Traditional methods to circumvent this problem have considered penalization, i.e. one searches for a function in F which minimizes the empirical error plus a penalty term, for example b norm are ` 2 (ridge-regression [17]) and ` 1 (LASSO [16]).
 In this paper we follow an alternative approach introduced in [10], called Compressed Least Squares Regression, which considers generating randomly a subspace G P (of finite dimension P ) of F , and then returning the empirical risk minimizer in G P , i.e. arg min g  X  X  dimensional spaces F and provide a characterization of the resulting approximation spaces. Let us briefly recall the method described in [10] and extend it to the case of infinite dimensional Examples of feature spaces satisfying this property include rescaled wavelets and will be described in Section 3.
 The random subspace G P is generated by building a set of P random features (  X  p ) 1  X  p  X  P defined as linear combinations of the initial features {  X  i } 1  X  1 weighted by random coefficients: where the (infinitely many) coefficients A p,i are drawn i.i.d. from a centered distribution with vari-ance 1 /P . Here we explicitly choose a Gaussian distribution N (0 , 1 /P ) . Such a definition of the features  X  p as an infinite sum of random variable is not obvious (this is called an expansion of a Gaussian object) and we refer to [11] for elements of theory about Gaussian objects and for the expansion of a Gaussian object. It is shown that under assumption (1), the random features are well defined. Actually, they are random samples of a centered Gaussian process indexed by the space X with covariance structure given by 1 P  X   X  ( x ) ,  X  ( x 0 )  X  , where we use the notation  X  u, v  X  = two square-summable sequences u and v . Indeed, E A The continuity of the initial features (  X  i ) guarantees that there exists a continuous version of the process  X  p which is thus a Gaussian process.
 Then we define G P  X  X  to be the (random) vector space spanned by those features, i.e. Now, the least-squares estimate g b  X   X  X  P is the function in G P with minimal empirical error, i.e. and is the solution of a least-squares regression problem, i.e. b  X  =  X   X  Y  X  R P , where  X  is the N  X  P -matrix composed of the elements:  X  n,p def =  X  p ( x n ) , and  X   X  is the Moore-Penrose pseudo-b g ( x ) def = T L [ g b  X  ( x )] , where T L ( u ) def = Next, we provide bounds on the approximation error of f  X  in G P and deduce excess risk bounds. 2.1 Approximation error We now extend the result of [10] and derive approximation error bounds both in expectation and in high probability. We restrict the set of target functions to belong to the approximation space K X  X  (also identified to the kernel space associated to the expansion of a Gaussian object): Remark 1. This space may be seen from two equivalent points of view: either as a set of functions that are random linear combinations of the initial features, or a set of functions that are the expec-tation of some random processes (interpretation in terms of kernel space). We will not develop the related theory of Gaussian processes here but we refer the reader interested in the construction of kernel spaces to [11] Let f  X  = and in high probability.
 Theorem 1. For any  X  &gt; 0 , whenever P  X  c 1 log( P X  2 1  X   X  (w.r.t. the choice of the random subspace G P ), in expectation.
 This result relies on the property that inf g  X  X  as a random variable w.r.t. the choice of the random elements A , concentrates around f  X  (in || X || P - X  ( x ) = f  X  ( s ) , since inner-products are approximately preserved through random projections (from a variant of Johnson-Lindenstrauss (JL) Lemma). The proof of Theorem 1 (provided in Appendix of and combining it with a Chernoff-Hoeffding bound for generalizing the result to hold in || X || P -norm. Remark 2. An interesting property of this result is that the bound does not depend on the distribution P . This distribution is used in the definition of the norm || X || P to assess how well a function space G
P can approximate a function f  X  . It is thus surprising that the measure P does not appear in the bound. Actually, the fact that G P is random enables it to be close to f  X  (in high probability or in expectation) whatever the measure P is. This is especially interesting in a regression setting where the distribution P from which the data are generated is not known in advance. 2.2 Excess risk bounds We now combine the approximation error bound from Theorem 1 with usual estimation error bounds for linear spaces (see e.g. [7]). Let us consider a target function f  X  = g  X  (empirical risk minimizer in the random space G P ) defined by (3).
 We now provide upper bounds (both in expectation and in high probability) on the excess risk for the least-squares estimate using random subspaces (the proof is given in [11]).
 Theorem 2. Whenever P  X  c 3 log N , we have the following bound in expectation (w.r.t. all sources of randomness, i.e. input data, noise, and the choice of the random features): Now, for any  X  &gt; 0 , whenever P  X  c 5 log( N/ X  ) , we have the following bound in high probability squares estimate in the random subspace G P has low excess risk. The question we wish to address now is whether we can define spaces for which this is the case. In the next section we provide two examples of feature spaces and characterize the space of functions for which this term is controlled. In the two examples provided below we consider (infinitely many) initial features that are trans-lations and rescaling of a given mother function (which is assumed to be continuous) at all scales. Thus each random feature  X  p is a Gaussian object based on a multi-scale scheme built from an object (the mother function), and will be called a  X  X crambled object X , to refer to the disorderly construction of this multi-resolution random process.
 We thus propose to solve the regression problem by ordinary Least Squares on the (random) approx-imation space defined by the span of P such scrambled objects. In the next sections we provide two examples. The first one considers the case when the mother function is a hat function and we show that the corresponding scrambled objects are Brownian motions. The second example considers wavelets. The proof of bounds (7) and (8) can be found in [11]. 3.1 Brownian motions and Brownian Sheets Dimension 1 : We start with the 1 -dimensional case where X = [0 , 1] . Let us choose as object space of continuous functions C 0 ([0 , 1]) equal to 0 at 0 (introduced by Faber in 1910, and known as the Schauder basis, see [8] for an interesting overview). Those functions are indexed by the scale j and translation index l , but all functions may be equivalently indexed by a unique index i  X  1 . We have the property that the random features  X  p ( x ) , defined as linear combinations of those hat functions weighted by Gaussian i.i.d. random numbers, are Brownian motions (See Example 1 of [11] for the proof). In addition, we can characterize the corresponding kernel space K , which is the product  X  j,l of one-dimensional hat functions (thus j and l are multi-indices). The random fea-tures  X  p ( x ) are Brownian sheets (extensions of Brownian motions to several dimensions) and the corresponding kernel K is the so-called Cameron-Martin space [9], endowed with the norm this space as the set of functions which have a d -th order crossed (weak) derivative  X  d f  X  X  Note that in dimension d &gt; 1 , this space differs from the Sobolev space H 1 .
 Regression with Brownian Sheets: When one uses Brownian sheets for regression with a target function f  X  = as: Thus, from Theorem 2, ordinary least-squares performed on random subspaces spanned by P Brow-nian sheets has an expected excess risk (and a similar bound holds in high probability). 3.2 Scrambled Wavelets in [0 , 1] d the initial features may equivalently be indexed by a unique index i  X  1 . The random features  X  p defined from (2) are called  X  X crambled wavelets X . It can be shown that the resulting approximation space K (i.e. { f  X  = Regression with Scambled Wavelets: Assume that the mother wavelet  X   X  has compact support [0 , 1] d and is bounded by  X  , and assume that the target function f  X  = space H s ([0 , 1] d ) with s &gt; d/ 2 (i.e. such that ||  X   X  || &lt;  X  ). Then, we have, Thus from Theorem 2, ordinary least-squares performed on random subspaces spanned by P scram-bled wavelets has an expected excess risk (and a similar bound holds in high probability).
 In both examples, by choosing P of order 3.3 Remark about randomized spaces Note that the bounds on the excess risk obtained in (7), (8), and (9) do not depend on the distribution P under which the data are generated. This is crucial in our setting since P is usually unknown. It should be noticed that this property does not hold when one considers non-randomized approxima-will approximate functions in a given class using a particular measure P . For example when P =  X  , (with at least d/ 2 vanishing moments), which form an orthonormal basis of L 2 , X  ([0 , 1] d ) , enables to achieve a bound similar to (8). However, this is no more the case when P is not the Lebesgue measure and it seems difficult to modify the features  X  i in order to recover the same bound, even when P is known. This seems to be even harder when P is arbitrary and not known in advance. Randomization enables to define approximation spaces such that the approximation error (either in expectation or in high probability on the choice of the random space) is controlled, whatever the measure P used to assess the performance (even when P is unknown) is.
 For illustration, consider a very peaky (a spot) distribution P in a high-dimensional space X . Reg-ular linear approximation, say with wavelets (see e.g. [6]), will most probably miss the specific characteristics of f  X  at the spot, since the first wavelets have large support. On the contrary, scram-bled wavelets, which are functions that contain (random combinations of) all wavelets, will be able to detect correlations between the data and some high frequency wavelets, and thus discover relevant features of f  X  at the spot. This is illustrated in the numerical experiment below.
 Here P is a very peaky Gaussian distribution and f  X  is a 1-dimensional periodic function. We con-top figures represent a high level view of the whole domain [0 , 1] . No method is able to learn f  X  on the whole space (this is normal since the available data are only generated from a peaky distribu-tion). The bottom figures shows a zoom [0 . 45 , 0 . 51] around the data. Least-squares regression using scrambled objects is able to learn the structure of f  X  in terms of the measure P . Figure 1: LS estimate of f  X  using N = 100 data generated from a peaky distribution P (left plots), using 40 Brownian motions (  X  p ) (middle plots) and 40 hat functions (  X  i ) (right plots). The bottom row shows a zoom around the data. Minimax optimality: Note that although the rate  X  O ( N  X  1 / 2 ) deduced in (9), does not depend on the dimension d of the input data X , it does not contradict the known minimax lower bounds, which are s -times differentiable), see e.g. Chapter 3 of [7]. Indeed, the kernel space K is composed of functions whose order of smoothness may depend on d . For illustration, in the case of scrambled Notice that if one considers wavelets with q vanishing moments, where q &gt; d/ 2 , then one may  X  using scrambled wavelets is minimax optimal (up to logarithmic factors).
 Now, concerning Brownian sheets, we are not aware of minimax lower bounds for Cameron-Martin spaces, thus we do not know whether regression using Brownian sheets is minimax optimal or not. Links with RKHS Theory: There are strong links between the kernel space of Gaussian objects (see eq.(4)) and Reproducing Kernel Hilbert Spaces (RKHS). We now remind two properties that illustrate those links: However it may not be straightforward to compute the eigenvalues and eigenfunctions of the integral operator L k and thus the basis functions  X  i in the general case.
 The approach described in this paper enables to choose explicitly the initial basis functions, and build the corresponding kernel space. For example we have presented examples of expansions using multi-resolution bases (such as hat functions and wavelets), which is not easy to obtain from the Mercer expansion. This is interesting because from the choice of the initial basis, we can characterize the corresponding approximations spaces (e.g. Sobolev space in the case of wavelets). Another more practical benefit is that by using multi-resolution bases (with compact mother function), we can derive efficient numerical implementations, as described in Section 5.
 Related works In [14, 13], the authors consider, for a given parameterized function  X  : X  X   X   X  R bounded by 1 , and a probability measure  X  over  X  , the space F of functions f ( x ) = R of the RKHS with kernel k ( x, y ) = probability over (  X  p ) p  X  P i.i.d  X   X  , there exist coefficients ( c p ) p  X  P such that b f ( x ) = estimates g A X   X  G P of function f  X   X  K in our setting. Indeed we may formally identify  X ( x,  X  p ) with  X  p ( x ) = infinite sequence. However, in our setting we do not require the condition sup x, X   X ( x,  X  )  X  1 to hold and the fact that  X  is a set of infinite sequences makes the identification tedious without the Gaussian random functions theory used here. Anyway, we believe that this link provides a better mutual understanding of both approaches (i.e. [14] and this paper).
 In the work [1], the authors provide excess risk bounds for greedy algorithms (i.e. in a non-linear approximation setting). The bounds derived in their Theorem 3.1 is similar to the result stated in our Theorem 2. The main difference is that their bound makes use of the l 1 norm of the coefficients  X   X  instead of the l 2 norm in our setting. It would be interesting to further investigate whether this difference is a consequence of the non-linear aspect of their approximation or if it results from the coefficients. Due to finite memory and precision of computers, numerical implementations can only handle a which makes use of the random matrix A = ( A p,i ) p  X  P,i  X  F , has a complexity O ( FPN ) . How-ever, in the multi-resolution schemes described here, provided that the mother function has compact support (such as the hat functions or the Daubechie wavelets), we can significantly speed up the computation of the matrix  X  by using a tree-based lazy expansion , i.e. where the expansion of the random features (  X  p ) p  X  P is built only when needed for the evaluation at the points ( x n ) n . Consider the example of the scrambled wavelets. In dimension 1 , using a wavelet dyadic-tree of depth H (i.e. F = 2 H +1 ), the numerical cost for computing  X  is O ( HPN ) (using one tree per random feature). Now, in dimension d the classical extension of one-dimensional wavelets uses a family of 2 d  X  1 wavelets, thus requires 2 d  X  1 trees each one having 2 dH nodes. While the resulting computes all the initial features), one needs to expand at most one path of length H per training point, and the resulting complexity to compute  X  is O (2 d HPN ) .
 Note that one may alternatively use the so-called sparse-grids instead of wavelet trees, which have been introduced by Griebel and Zenger (see [18, 3]). The main result is that one can reduce signif-icantly the total number of features to F = O (2 H H d ) (while preserving a good approximation for sufficiently smooth functions). Similar lazy evaluation techniques can be applied to sparse-grids. Now, using a finite F introduces an additional approximation (squared) error term in the final excess Thus, using P = O ( that has numerical cost O ( P 2 N ) , and then solve the system by inversion, which has numerical cost We analyzed least-squares regression using sub-spaces G P that are generated by P random lin-K = { f  X  , ||  X  || &lt;  X  X  (which is also the kernel space of the related Gaussian object) provides a characterization of the set of target functions f  X  for which this random regression works. We il-lustrated the approach on two examples for which the approximation space is a known functional space, namely a Cameron-Martin space when the random features are Brownian sheets (generated by random combinations at all scales of a hat function), and a Sobolev space in the case of scram-bled wavelets. We derived a general approximation error result from which we deduced excess risk We showed that least-squares regression with scrambled wavelets provides rates that are arbitrarily close to minimax optimality. However in the case of regression with Brownian sheets, we are not aware of minimax lower bounds for Cameron-Martin spaces in dimension d &gt; 1 .
 We discussed a key aspect of randomized approximation spaces which is that the approximation error can be controlled independently of the measure P used to assess the performance. This is essential in a regression setting where P is unknown, and excess risk rates independent of P are obtained.
 We concluded by mentioning a nice property of using multiscale objects like Brownian sheets and scrambled wavelets (with compact mother wavelet) which is the possibility to be efficiently imple-mented. We described a lazy expansion approach for computing the regression function which has A limitation of the current scrambled wavelets is that, so far, we did not consider refined analysis for spaces H s with large smoothness s  X  d/ 2 . Possible directions for better handling such spaces may involve refined covering number bounds which will be the object of future works. This work has been supported by French National Research Agency (ANR) through COSINUS program (project EXPLO-RA number ANR-08-COSI-004). [1] Andrew Barron, Albert Cohen, Wolfgang Dahmen, and Ronald Devore. Approximation and [2] Gerard Bourdaud. Ondelettes et espaces de besov. Rev. Mat. Iberoamericana , 11:3:477 X 512, [3] Hans-Joachim Bungartz and Michael Griebel. Sparse grids. In Arieh Iserles, editor, Acta [4] St  X  ephane Canu, Xavier Mary, and Alain Rakotomamonjy. Functional learning through kernel. [5] D. Coppersmith and S. Winograd. Matrix multiplication via arithmetic progressions. In STOC [6] R. DeVore. Nonlinear Approximation . Acta Numerica, 1997. [7] L. Gy  X  orfi, M. Kohler, A. Krzy  X  zak, and H. Walk. A distribution-free theory of nonparametric [8] St  X  ephane Jaffard. D  X  ecompositions en ondelettes. In Development of mathematics 1950 X 2000 , [9] Svante Janson. Gaussian Hilbert spaces . Cambridge Univerity Press, Cambridge, UK, 1997. [10] Odalric-Ambrym Maillard and R  X  emi Munos. Compressed Least-Squares Regression. In NIPS [11] Odalric-Ambrym Maillard and R  X  emi Munos. Linear regression with random projections. Tech-[12] Stephane Mallat. A Wavelet Tour of Signal Processing . Academic Press, 1999. [13] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In John C. [14] Ali Rahimi and Benjamin Recht. Uniform approximation of functions with random bases. [15] S. Saitoh. Theory of reproducing Kernels and its applications . Longman Scientific &amp; Techni-[16] Robert Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal [17] A. N. Tikhonov. Solution of incorrectly formulated problems and the regularization method. [18] C. Zenger. Sparse grids. In W. Hackbusch, editor, Parallel Algorithms for Partial Differen-
