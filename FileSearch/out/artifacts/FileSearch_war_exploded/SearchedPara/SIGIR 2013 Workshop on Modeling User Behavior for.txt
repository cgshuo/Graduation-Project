 The SIGIR 2013 Workshop on Modeling User Behavior for Information Retrieval Evaluation (MUBE 2013) brings to-gether people to discuss existing and new approaches, ways to collaborate, and other ideas and issues involved in improv-ing information retrieval evaluation through the modeling of user behavior.
 Categories and Subject Descriptors: H.3.4 [Informa-tion Storage and Retrieval]: Systems and Software  X  Per-formance evaluation (efficiency and effectiveness) Keywords: Information retrieval, search evaluation, user models
Information retrieval (IR) evaluation is concerned with producing accurate estimates of the performance of retrieval systems. Excluding in-situ studies of user behavior, IR eval-uation is conducted via simulation of the usage of a retrieval system. Offering the best fidelity to reality, laboratory-based user studies provide simulated search tasks to users and then measure outcomes, such as user satisfaction or success. Un-fortunately, user studies can be costly and difficult to scale to the large number of evaluations required during the devel-opment of new retrieval systems. Cranfield-style evaluation retains the simulated search tasks of laboratory-based user studies, but replaces the user with a model of user behavior and recorded user relevance judgments. After the initial cost of obtaining the relevance judgments, Cranfield-style evalu-ation has a very low cost and scales easily. The quality of the effectiveness estimates produced by Cranfield-style evalua-tion is limited by the user model. The better the user model, the closer Cranfield-style evaluation is to the standard of the laboratory-based user study.

During the past decade, there has been an increasing body of research on ways to incorporate models of user behavior into Cranfield-style effectiveness measures or to understand existing metrics in terms of user models. At the same time, another body of research has focused on the simulation of user interaction with IR user interfaces to predict the value of new user interfaces to retrieval systems. Researchers have also used simulation of user behavior to better understand information retrieval. Space limitations prevent us from cit-ing all applicable work here; for a good starting point to exploring the relevant literature, please see the references in the following recent papers: [1, 2, 3].

These varied approaches to the evaluation of IR systems all rely on the quality of their user models. The better the user models, the better the predictions made by the evalua-tion measures.

While there is some overlap between the researchers pur-suing these different approaches, we believe that it makes sense to bring these groups together to enable fruitful cross-pollination of ideas and methods. This workshop brings to-gether people interested in discussing new approaches and new ways to collaborate and create shared resources for the development and application of user models for information retrieval evaluation.

The workshop solicited two-page poster papers, which were reviewed by a program committee. Accepted papers will be presented during boaster and poster sessions. In addition to the accepted papers, the tentative schedule for the work-shop includes invited speakers, breakout groups, and plenty of time for discussion and presentation of breakout group reports.
 This work was supported in part by the Natural Sciences and Engineering Research Council of Canada (NSERC), in part by GRAND NCE, and in part by the University of Waterloo.

