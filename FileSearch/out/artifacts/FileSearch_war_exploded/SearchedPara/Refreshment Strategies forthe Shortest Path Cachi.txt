 The shortest path query in location based service is now widely used in daily life [1 X 6]. When a user has a shortest path query, a cache in a local server is accessed, and the result, if any, will be directly returned to the user. If there is no off-the-shelf result for the query in the cache, the system has to acces s a global server for the result, which costs communication and computational tim e [7 X 11]. The caching content is therefore, very critical for the efficiency of the whole query system.

In the shortest paths caching problem, g iven a road network and a cache, existing works either select paths with high query frequency in the query log, or those paths that contain the most number of nodes [12, 13]. None of them have talked about the scenario of changing road network. The assumption of an unchanged network, is obviously too strong. In practice, road network actually changes with time. For example, rush hours or traffic congestion change the weight of some edges on the road network, making certain paths in a cache become invalid or the utilization of the cache decreases. When confronting a changing road network, a s traightforward way to refresh a cache is to evaluate the attractiveness of all the present shortest paths in the road network, and reload the cache. Such a method is far more from inefficient, because evaluating all the shortest paths is time-consuming and unnecessary. In this paper, we discuss how to refresh a cache when the weight of a certa in edge changes. This scenario depicts situations in practice where a main road has traffic congestion (increased weight), or traffic congestion on a main road is reduced (decreased weight). Assuming the weight change of only one edge is reasonable, as in the real road network, traffic congestion often happens in only a few roads and these ro ads are not interrelated. All the congested roads can be seen as independent roads and dealt with separately. 1.1 Challenges The refreshment of shortest path caches is challenging in two folds. The first challenge is: when an edge changes its weight, how to detect which paths are affected in a short time. Though some extant works have already proposed methods to monitor affected paths caused by edge weight changes, they have to store extra information. Whereas, in local servers, storage spaces are used t o cache shortest paths as many as possible, leaving no spare space to store extra information. Another challenge is how to devise an efficient refreshment strategy, so as to ensure a high utilization of caches. 1.2 Contributions To the best of our knowledge, our work is the first one to discuss the shortest path caching problem with a changing graph. Th is problem is undoubtedly more close to the real application. To address this problem , we develop an algorithm to detect the shortest paths which are affected by edge changes. In addition, we propose four cache refresh-ment strategies and analyze the performance of proposed strategies by conducting a series of experiments. The published work related with our problem has two streams. One stream is concerned with the monitoring of shortest paths; while the other stream addresses the problem of caching shortest paths.

With respect to the monitoring of shortest paths, there exists a wide spectrum of works. Lee et al. [14] propose an ellipse bound method (EBM), where each shortest path corresponds to an elliptic geographical area and all the updated edges are considered to affect their corresponding paths. The shortcoming of such a method is that they cover a large number of unrelated edges and a lot of computational time is wasted to recompute unchanged paths. Tian et al. [15] develop the notion of query scope to identify affected paths and devise a partial path computation algorithm (PPCA) to quickly recompute the updated paths. Although Lee et al. [14] and Tian et al. [15] effectively monitor affected min-cost paths, their methods wo rk only if auxiliary data structures (elliptic geographical areas and query scope indexes) are available.

Cache is widely used to improve the performance of many support systems. Com-munication cost and query response time are two important indicators in a client-server system, and a cache is therefore employed in the client-side to reduce communication and improve query response time. However, a cache, if located at a client, can only serve queries from the client itself, not other clients. As a consequence, only query-intensive users can be benefited from such a cache. Two caching approaches (dynamic caching and static caching) are proposed by Ma rkatos et al. [11]. The dynamic caching aims to adjust the caching content based on recently received queries, while the static caching exploits the way to effectively initialize system caches. Thomsen et al. [12] and Li et al. [13] carry out static caching techniques for caching shortest paths. They utilize statistics from query logs to estimate the benefit of caching a specific shortest path and employ greedy algorithms to load benefic ial paths in a cache. However, none of the above works considers the shortest path caching problem with a changing graph.
The rest of the paper is organized as follows.In Section 2, we study work related to the shortest path caching problem. In Sec tion 3, we introduce all the concepts and formally define the problem. Then we explore the properties of a changing graph and present our algorithm for detecting affected shortest paths accordingly in Section 4. It is followed by four cache refreshment strategies in Sections 5. In Section 6, we conduct experimental comparison among the proposed methods on real data sets. Finally, we give some closing remarks in Section 7. Definition 1. Graph model. Any transportation network can be modeled as an undi-rected graph G =( V , E )where V = { v 1 , v 2 ,..., v n } is the set of nodes and E is the set of edges. Each edge in this graph is represented by a pair of nodes e ( v i , v j ). Moreover, the weight of each edge e ( v i , v j ) is denoted as w ( v i , v j ).
 Definition 2. Shortest Paths. For any given graph G =( V , E ), the shortest path from node v a to node v b is denoted as P a,b = v x 0 ,v x 1 ,...,v x n here v x 0 = v a , v x n = v b Definition 3. Cache. A cache is denoted by  X  and its capacity is |  X  | .  X  refers to the caching content. The size of a cache and caching content are measured in terms of the number of nodes. It is clearly that the size of  X  is always no larger than the cache capacity, i.e., |  X  | X |  X  | all the time.
 Definition 4. Subpaths set S ( P a,b ) . All the subpaths of P a,b compose set S ( P a,b ) .
For example, for the shortest path P 1 , 8 in Fig. 1, its subpaths set S ( P 1 , 8 ) contains Definition 5. Affected paths. Suppose the shortest path between v a and v b is originally P a,b = v a ,v i ,...,v j ,v b . Due to the change of edge weights, if the shortest path be-tween v a and v b becomes P a,b = v a ,v m ,...,v n ,v b ( P a,b = P a,b ), then P a,b is called an affected path. Lemma 1. Optimal subpath property [16]. A subpath of any shortest path is a shortest path of the ends of that subpath. That is, for a given shortest path P a,b = v a ,v i ,..., v shortest path of P i,j .

Lemma 1 tells us that if we store a certain shortest path in a cache, we can answer all of the shortest path queries whose end nodes are on that path.
 Problem Description. Given a user query log L , a cache  X  , and a computational system which can compute the shortest paths on a graph G =( V , E ), we assume that cache  X  has been fully loaded and the weight of one edge e is changed from w to w .The objective of our problem is to refresh the content of cache  X  . In this Section, we first summarize the properties owned by affected paths and then develop an algorithm to compute affected paths. Given the weight of e can increase as well as decrease, we discuss the two situations respectively. (1) The weight of edge e increases.
 Theorem 1. For graph G , suppose the ends of e is v a and v b and P a,b = e .When w e increases, if P a,b = e , then each path p containing e must be an affected path. In other situations, no shortest paths are affected.
 Proof. When w e increases, it has three cases: 1). If P a,b = e originally but it becomes P a,b = e ,then P a,b is an affected path, and any path containing P a,b is also an affected path. 2). If P a,b = e originally and it remains P a,b = e even if w e increases, then no path is affected. 3). If P a,b = e originally, then the increase of w e does not affect any shortest paths at all. (2) The weight of edge e decreases.
 Theorem 2. For graph G, when the weight of e decreases, if P a,b is an affected path, then P a,b must contain edge e .
 Proof. Assume a shortest path P a,b is an affected path, and P a,b does not contain edge e ,thenithas P a.b = P a,b , indicating that P a,b is not an affected path. There is a conflict with the assumption. Therefore, P a,b must contain edge e .

When w e decreases, we deploy Algorithm 1 to find all the affected paths. Assume the ends of e is v a and v b . Its basic idea is to determine start node v s and end node v and for each pair of start and end node, check whether P s,e is an affected path. As there are many nodes can be start nodes and end node, we use C s and C e to denote the set of candidate start and end nodes, respectively. The determination of C s and C e is performed by a nested loop. First, C s is initialized with v a , the only start node. In the outer loop, pop one candidate start node from C s as current start node v s and all the adjacent nodes with current start node v s become candidate start nodes when the shortest path P s,b contains e , thus, they are added into C s (lines 5 X 7). If a node has been used as a start node before, then do not add it into C s . In the inner loop, the start node v s can be seen as given, and then vary the end node v e . The variation of end nodes is similarly with that of outer variation. v e is obtained by popping a node from C e and the adjacent nodes of current end node v e are added into C e except for those which have entered into C e before (lines 9 X 18). Given v s and v e , Algorithm 1 decides whether P s,e is affected and if yes, put it into the set of affected paths S aff . The pseudo code is shown in Algorithm 1.
 Theorem 3. Our affected shortest paths computing method satisfies both soundness and completeness.
 Proof. We first prove that the algorithm C AL A FFECTED P AT H S is sound, i.e. any path in S aff is an affected path. Accordin g to the algorithm , if a path P s,e exists in S aff , then there must exist that the path P s,e = P s,e ,inwhich P s,e is the new shortest path between v s and v e with weight ( e )= w ,so P s,e is an affected path.

Now we prove that the algorithm C AL A FFECTED P AT H S is complete. Assume that there exist an affected path P s,e ,and v s ,v e is not in S aff . Based on Theorem 2, we know the new shortest path P s,e connecting v s and v e must contain the changed edge e , then according to the algorithm, v s and v e must be added into C s and C e , respectively. So the path P s,e must be considered, and compared with P s,e . Therefore, the algorithm can get the affected path P s,e , the assumption does not hold and the algorithm is complete.

Fig. 1 shows an example of computing affected paths when the weight of e ( v 3 , v 6 ) Afterwards the algorithm computes remaining paths containing e ( v 3 , v 6 ), such as v 2 -v paths v 3 -v 4 -v 6 -v 5 and v 3 -v 4 -v 6 -v 8 are obtained, respectively .
In reality, a cache normally stores a large number of paths, making it time-consuming to directly recompute all the new shortest paths in the cache. Nevertheless, affected paths caused by w e are fewer, because the change of a single edge only affects its surrounding paths. It is efficient to firstly compute the set of affected shortest paths caused by w e , and then check whether the cache contains any affected paths. Though we discuss the problem assuming that only one edge changes its weight, we can still detect affected paths if multiple edges change weights concurrently by deploying Algorithm 1 multiple times. In this section, we introduce four heuristic rule-based path refreshment strategies. 5.1 Through Reload The first refreshment strategy is to empt y the entire cache and re load it. This is the most straightforward idea to refresh the cache. In Li et al. [13], they propose a benefit oriented model to load shortest paths. To make this paper self-contained, we briefly introduce how the benefit oriented model works.

All of us know that the cache utilization, or the cost reduction by using cache, is affected by the query frequency and the computational time of shortest paths. When calculating the cost saved by loading a path P a,b into a cache, we consider: 1) which queries can be answered by the path P a,b ; and 2) the saved computational cost if an-swering a query Q a,b by a cache directly.

For the first consideration, according to Lemma 1, we know that a path can answer all the queries on its subpath set. For the sec ond consideration, it depends on how fre-quently a query arrives and the computational c ost of that query. More specifically, it is the multiplier of the query frequency and the computational cost.

From the above analysis, we have the utilization of storing P a,b in caches as the following form.

Here, the numerator is the total cost reduction saved by caching path P a,b .After normalized by the size of path P a,b , B ( P a,b ) is the unit-size benefit brought by path P a,b if it is loaded into a cache. Due to the limited cache capacity, the unitization can better reflect the value of loading a path after normalization. Readers can understand this as the value of per unit weight in a knapsack problem. Paths with larger B ( P a,b ) have higher priority to be loaded into a cache.

The method wastes a lot of time. Since only an edge changes its weight, its impact on the whole network is limited. The majority of shortest paths do not change at all. As a result, through reload is redundant. 5.2 Affected Paths Update The second strategy is to detect all the affected paths in a cache, and then update these paths in the new network. The advantage of this method is that the computational time is the least; it does not involve comparison of path benefits. The shortcoming of this method is that the ends of affected paths may not be of high benefits after the network changes. It may be better to replace affected paths with other paths. 5.3 Highest Frequency First To overcome the shortcoming of the second method, in the third method, we delete the affected paths in a cache first and then try t o fill the cache until it cannot contain paths any more. Each time, one path is selected. In order to increase the speed of refreshment, we greedily select the path with the highest query frequency. It avoids to compute the benefits of each path which are dynamic val ues based on the current content of the cache. Query frequency, instead, can be easily obtained from the query log and never changes even though the network changes. 5.4 Roulette Wheel Selection At last, the forth strategy is to delete all the affected paths in the cache and load new paths to the cache by the method of roulette wheel . As the historical queries are not the true queries in the future, on one hand, we consider it as a reference for the future queries; on the other hand, we avoid over fitting the cache content to it.
 We use a roulette wheel to determine whi ch paths should be loaded into the cache. One feature of the roulette wheel selection i s that alternatives with low attractiveness can also be selected, although with a relat ively low possibility. We adopt the idea of roulette wheel as the following: for a path p in the query log, its probability of being selected Pr( p ) is proportional to its query frequency f p , as shown in Equation 3.
The summation of the probabilities of all the paths are just 1. The probability of se-lecting each path corresponds to the probability of a variable X with a standard uniform distribution falling into an interval as shown in Equation 4.
 In another word, to realize the probabilities for all the paths is equivalent to generate a random number failing at interval [0,1].

Take Table 1 for an example, it records the path-interval correspondence. When we need to select a path into the cache, we generate a random number according to the standard uniform distribution. Suppose the randomly generated number is 0.6, then P 2 , 11 will be selected because 0.6 belongs to [0 . 5 , 0 . 75] .

In terms of the computational time, the roulette wheel selection is slower than the affected paths update and the highest frequency first, faster than the through reload. As for the performance, it is better than the former two and worse than the through reload. Choosing which refreshment strategy is base d on the scenario requirement. When time is not a concern, then the through reload or the roulette wheel selection may be used; When real time response is a big issue, the affected paths update or the highest fre-quency first may be utilized. There is no absolutely good or absolutely bad. The benchmark data sets used are the commonly known data sets: the road network of Aalborg and Beijing. Both data sets consist of a large number of nodes, edges, and associated weights. Table 2 summarizes the information on the data sets.

We divide each data set into two parts: one pa rt for training and the other part for testing. Training sets act as historical logs while test sets act as future queries. The frequency statistics is extracted from the tr aining set, and the cache is fulfilled based on frequency information. For Aalborg, the training set is 176.2KB and the test set is 177.3KB; for Beijing, the training set is 483.4KB and the test set is 481.7KB.
To simulate the weight change of an edge, we generate 50 instances based on each data set. Out of the 50 instances, 20 times are of weight increase and 30 times are of weight decrease. In each instance, randomly select one edge and change its weight. The increase of a weight ranges from 1% to 100%. To simulate the weight decrease, we first increase the edge weight and load the cach e, then recover the weight to its original value after loading the cache. The experime ntal results in Section 6.1 and Section 6.2 are the average of 50 instances. As for the result in Section 6.3 (the hit-ratios of different refreshment strategies), we sum up results of 50 instances for ease of read, since the hit-ratio differences among different strategies are tiny.

We assume that the query trend on training sets and test sets are similar, and the statistic information from training sets can predict future queries to some extent. All the code were implemented in GNU C++. The experiments were run on a PC with an Intel i7 Quad Core CPU clocked at 3.10 GHz. 6.1 Efficiency of Detecting A ffected Paths in the Cache Detecting affected paths in a cache is an im portant step before refreshing a cache. To demonstrate the performance of our algorithm (Algorithm 1), a basic method named B
ASIC P AT H S is used as a benchmark method. B ASIC P AT H S recomputes the new short-est paths for all the ends pairs in the cache and compare them with the original ones in the cache. If the two paths for a ends pair are different, then an affected path is detected. We test algorithms C AL A FFECTED P AT H S and B ASIC P AT H S on Aalborg and Beijing data set and the running time of the two detection methods is regarded as the performance indicator.

As seen from Fig. 2, algorithm C AL A FFECTED P AT H S has a better performance than algorithm B ASIC P AT H S . For example, in the Aalborg data set, when the cache size is 9MB, the running time of C AL A FFECTED P AT H S is only 267ms; while the running time of B ASIC P AT H S reaches 28.67seconds. In addition, we can see that C AL A F -FECTED P AT H S algorithm uses less time on Aalborg data set than it does on Beijing data set. The reason is that the graph of Beijing data set is more compact. Hence, C
AL A FFECTED P AT H S need check more paths in the graph to yield all the affected paths. 6.2 Overhead of Different Refreshment Strategies In Section 5, we propose four cache refreshment strategies, which are based on different heuristic rules. For ease of illustration, TR is short for the through reload method, APU is short for the affect paths update method, HFF is short for the highest frequency first method and RWS is short for the roulette wheel selection method. We test the running time of these four replacement strategies v.s. various cache sizes as shown in Fig. 3. It reveals that TR spends the most running time and the other three spend similar running time. It is because the overhead reported b y the computer is not that accurate under 1000ms. Meanwhile, It is surprisingly to find that HFF and RWS do not spend much time, just almost at the same s cale with APU. It indirectly indicates that the scale of affected paths is rather small co mpared with the content of the cache. 6.3 Hit Ratio of Different Refreshment Strategies Fig. 4 shows the hit-ratios of four refreshment strategies under different scales of caches. We can observe that the through reload (TR) achieves the highest hit ratio. And the hit ratios of HFF and RWS are similar. At last, APU is the worst. Such a result is consistent with the discussion in Section 5. We address the problem of refreshing cache c ontent in a changing network. To the best of our knowledge, our work is the first one to discuss the cache refreshment problem when the road network changes with time. Road network changes are commonly seen in the real world, for example traffic congestion or road construction. Shortest path caches are necessary to be refreshed periodically, so that the information in them is accurate, valid and the utilization of the cache is maxim ized. In this paper, we first introduce the concept of affected paths, and then exploi t the properties associated with affected paths. In the following, we develop an algorithm to detect affected shortest paths in the cache caused by road network changes. Sequentially, four cache refreshment strategies are illustrated. We give a full explanation o f these strategies, and conduct a series of experiments to compare their performance, i.e., the computational time and the hit ratio on bench mark data sets.
 Acknowledgments. The work is partially supported by the National Natural Science Foundation of China (Nos. 61322208, 61272178), the Joint Research Fund for Overseas Natural Science of China (No. 61129002), the Doctoral Fund of Ministry of Education of China (No. 20110042110028), and the Fundamental Research Funds for the Central Universities ( No. N120504001, N110804002).

