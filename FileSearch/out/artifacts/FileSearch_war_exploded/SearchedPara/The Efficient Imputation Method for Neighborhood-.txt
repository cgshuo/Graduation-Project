 As each user tends to rate a small proportion of available items, the resulted Data Sparsity issue brings significant challenges to the research of recommender systems. This issue becomes even more severe for neighborhood-based col-laborative filtering methods, as there are even lower num-bers of ratings available in the neighborhood of the query item. In this paper, we aim to address the Data Sparsity issue in the context of the neighborhood-based collaborative filtering. Given the ( user,item ) query, a set of key ratings are identified, and an auto-adaptive imputation method is proposed to fill the missing values in the set of key rat-ings. The proposed method can be used with any similarity metrics, such as the Pearson Correlation Coefficient and Cosine -based similarity, and it is theoretically guaranteed to outperform the neighborhood-based collaborative filter-ing approaches. Results from experiments prove that the proposed method could significantly improve the accuracy of recommendations for neighborhood-based Collaborative Filtering algorithms.
 H.4 [ Information Systems Applications ]: Miscellaneous; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Information Filtering Algorithms Imputation, Collaborative Filtering, Recommender Systems
Recommender System attempts to automatically provide personalized recommendations based on the historical records of users X  activities, and it is commonly constructed on the basis of Collaborative Filtering (CF) methods because of their insensitivity to the detailed features of users or items. In general, there are two types of collaborative filtering meth-ods: the neighborhood -based method and the model -based method [1,10]. Although, a lot of research attention has been attracted to the model -based method, a lesson learned from the well-known Netflix Competition tells that neighborhood -based methods and model -based methods can explore very different levels of data patterns in the data set, therefore none of them can always output optimal results solely based on its own [3]. In this paper, we aim to address the data sparsity issue in the context of neighborhood -based method with theoretical analysis.

Neighborhood -based CF methods are generally based on the k nearest neighbor rule. Namely, its performance is highly affected by the selected nearest neighbors. However, there has been serious problems for practitioners trying to use this kind of method in areas where data is sparse. Specif-ically, Data Sparsity refers to insufficient information on a user X  X  rating history, and the corresponding user  X  item rating matrix will thus become extremely sparse. When the data is prevailing with missing values, two like-minded users may not show any similarity. To overcome this prob-lem for recommendation purposes, two major categories of approaches exist: one is to utilize more suitable similarity metrics to identify a better set of neighborhood [1, 5, 19]. The other approach is to design better aggregation meth-ods which integrate the item ratings given by all the neigh-bors [6, 13]. It might be argued that a third category of approaches based on data imputation [5,16,23,25,26,28], to which this work belongs, has begun to emerge over the past couple of years. Among others, data imputation mainly in-volves selecting a set of ( user,item ) pairs whose values are missing or unavailable in the user  X  item rating matrix, and filling them with imputed values before predicting the ratings for the active user. However, despite evidence of im-proved performance in the predictive accuracy, the research of imputation-based CF methods is still in its infancy. Issues such as does all missing data possess the same importance to the recommendation purpose , how to select the most in-formative missing data to impute , and how to trade off the imputation benefit and error , still largely remain unexplored.
In this paper, we propose an Auto-Adaptive Imputation (AutAI) method which can automatically identify a key set of missing data, and adaptively imputate them according to each individual user X  X  rating history. Through this novel method, we can maximize the information contained in the neighborhood of the active user while minimizing the im-putation error brought in. Inspired by Cover X  X  [8] research o n the nearest neighbor rule, which showed that the near-est neighbor contains at least 50% of the information in the infinite training set, we argue that not all missing data pos-sesses equivalent information for a particular prediction, and some missing data is more informative than others. Then, we propose an auto-adaptive imputation method to identify the critical key set of missing data for each active user adap-tively so as to impute the most informative missing data. Therefore, the proposed imputation method can lead to bet-ter recommendations in terms of accuracy, and we provide theoretical analysis on the performance benefit for the pro-posed AutAI method. On this basis, we propose a collabo-rative filtering algorithm by using the AutAI method from both user and item aspects (AutAI-Fusion).

The major contributions of this paper are as follows:
The rest of the paper is organized as follows. We present the background in Section 2, define the Auto Adaptive Impu-tation (AutAI) method in Section 3, together with theoret-ical analysis about imputation benefit and error. Section 4 presents the experiment results, followed by conclusion in Section 5.
In this section, we define the notations used in the paper, and we briefly recap the neighborhood-based method and the model-based method. Let us assume U = { u 1 ,u 2 , ,u m } be a set of m users, T = { t 1 ,t 2 , ,t n } be the set of n items. For m users and n items, the user  X  item rating matrix R is repre-sented as a m  X  n matrix. The user  X  item matrix R can be decomposed into row vectors: R = [ u 1 , u 2 , , u m ] and u i = [ r i 1 ,r i 2 , ,r in ], where T denotes transpose . The row vector u i corresponds to the user u i  X  X  rating informa-tion, and r ij denotes the rating that user u i gave to item t . N k ( u x ) denotes the set of user u x  X  X  k nearest neighbors. T xy = { t i  X  T | r xi 6 =  X  ,r yi 6 =  X  X  denotes the set of items co-rated by both u x and u y . u a denotes the active user for whom the recommendation is processing. t s denotes the ac-tive item on which the recommendation is processing.  X  u and  X  t s denote the average rating of user u a and the average rating on item t s , respectively.
One of the pioneering works in neighborhood-based Collab-orative Filtering was proposed by Paul Resnick [19]. This system, GroupLens utilizes users X  ratings as input, then pre-dicts how much the active user like an un-rated item. In gen-eral, the neighborhood-based Collaborative Filtering meth-od is based on k nearest neighbors (KNN) rules. Its input is usually the entire user  X  item rating matrix R . Two stages are involved in this process: Neighbor Selection and Rating Aggregation .

In the stage of Neighbor Selection , similarity could be evaluated between any two rows or any two columns in the user  X  item rating matrix, corresponding to the user-based approaches [5], or the item-based approaches [22]. It is clear that similarity measurements is one of the founda-tional problems in collaborative filtering methods. When the user  X  item rating matrix is sparse, this problem would be even more difficult. Several methods have been propoesd to address this problem. One way is to design better simi-larity metrics that tolerant the sparsity issue, e.g. the two popular metrics, Pearson Correlation Coefficient [19] and Cosine-based Similarity [1,5]. Another way is to tackle the sparsity issue directly through data imputation, such as De-fault Voting [5], EMDP [16] etc. We are going to discuss this part with more details in the following Section 2.4.
In the stage of Rating Aggregation , for any item t s , all the ratings on t s by users in the N k ( u a ), will be aggregated into the predicted rating value  X  r as by user u a [1]. The weighted majority prediction algorithm predicts the rating on new items based on a weighted sum of the users ratings on those items [11]. Hence the determination of a suitable set of weights becomes an essential problem here. While most con-ventional work relies on the similarity between users or items to determine the weight, Data Sparsity therefore affects this stage too. In contrast to the Neighborhood-based CF methods, the Model-based collaborative filtering algorithms attempt to con-struct a model from the given rating matrix, and utilize the model to predict ratings on new items. A wide range of machine learning techniques have been adopted: super-vised learning , unsupervised learning and matrix decomposi-tion etc. For supervised learning techniques, Su and Khosh-goftaar applied the Belief Nets to treat rating prediction as a multi-class classification problem [24]. For unsuper-vised learning techniques, Lemire and Maclachlan proposed the slope one algorithm, based on the popularity differential principle between items, which means user rating behaviour is influenced by both the user X  X  rating style (e.g. the user X  X  average rating) and items popularity (e.g. the average rat-ing for an item) [14]. For matrix decomposition techniques, Billsus and Pazzani proposed to use the Singular Value De-composition (SVD) to exploit the  X  latent structure  X  in user ratings [4]. This method could utilize information from users whose ratings are not correlated with the active user. Since then, many other SVD-based methods have been proposed, such as the SVD++ method which combines the user X  X  ex-plicit and implicit feedbacks [12].
Data Sparsity is one of the most challenging issues in rec-ommendation techniques. Since users tend to rate only a small fraction of items in a system, the u ser  X  item rating matrix is usually very sparse, with a density of around 1% [22]. Furthermore, Data Sparsity may make the neighbor-hood -based CF methods incapable of finding neighbors and therefore fail to make accurate recommendations [17,22]. To overcome this problem, a number of methods have been pro-posed. One category of methods is to design some similarity measurement which can tolerate the sparsity in the recom-mendation field [9]. Another category is to design better aggregation methods which integrate the item ratings given by all neighbors [6,13]. The third category of these methods is to fill in the missing data by imputation , e.g. the default voting [5], the smoothing method [28], and the missing data prediction [16]. In this paper, we focus on the methods by using imputation.

Default Voting is a straight forward imputation-based me-thod [5] which assumes default values for those missing rat-ings, such as exploiting average ratings by a small group of users as default ratings for other items in order to increase the size of the co-rated item set [7]. Xue et. al. proposed to use some machine learning methods to smooth all miss-ing data in the user  X  item rating matrix [28]. Results from their experiment show that more accurate recommendations are obtained if imputing properly. Recently, Ma et. al. took the imputation confidence into consideration, and only filled in the missing data when it was confident to impute [16]. Specifically, they set a threshold to identify highly confi-dent users, and only make a prediction about the missing data to users that have highly confident neighbors. This idea works well as it prevents poor imputation. However, their proposed EMDP algorithm treats all the missing data equally. Other previous imputation-related works focus on investigating various imputation techniques [23,25,26], e.g. the mean imputation, the linear regression imputation, the multiple imputation [21], and the predictive mean matching imputation [15].

Little work investigates how to impute and which missing data should be imputed? This question is interesting and of importance, since imputed data will bring in some imputa-tion error as well as alleviating the sparsity issue. In this paper, we will investigate these questions, and propose an auto-adaptive imputation method that is presented in the following section.
In this section, we propose a new imputation method to effectively improve the performance of neighborhood-based collaborative filtering. Moreover, a theoretical analysis on the performance benefit of the proposed method is also pro-vided. To our best knowledge, this work is the first one that analyses the imputation methods from a theoretical perspec-tive in the area of collaborative filtering. Finally, by apply-ing the proposed AutAI method, we, furthermore, propose a collaborative filtering framework by using AutAI form both user and item perspectives (AutAI-Fusion).
We first formulate the neighborhood -based CF using the probability theory [20]. The neighborhood -based CF pro-vides recommendations by estimating how much the active user may like un-rated items, which is known as the rating prediction task. Given two variables, the user consuming history u and the available ratings r , the rating prediction task can be formulated as  X  ( u ) = E ( r | u ), which is the expec-tation of dependent variable r given the independent vari-able u . For the recommendation purpose, it is interesting to estimate the value r as on an un-rated item t s for a singular independent variable value u a . The estimator for u a is then: From the perspective of probability theory, the observation values sampled at u a can be used to estimate  X   X  ( u a ).
However, there is no observation values at u a in the con-text of collaborative filtering. To tackle this problem, cer-tain similar users of u a are selected and used to estimate  X   X  ( u a ). This is the assumption of collaborative filtering: sim-ilar users may have similar ratings on items . Specifically, to estimate  X   X  ( u a ), we can first select several neighbors of u then treat the ratings of these neighbors as samples at u Finally, we process the above estimation method as usual. This is the well-known k nearest neighbor estimator [10] (KNN): where k is the number of selected neighbors, u x is one of u  X  X  neighbors, N k ( u a ) is the set of k nearest neighbors, and r xs is the rating of neighbor u x on t s . To further reduce the estimation bias, the KNN estimation usually apply the weighted average [2]: where w ax is the weight of neighbor u x to u a , and  X  = P flect how close the neighbors are to u a .

For neighborhood-based CF, the sparsity issue can lead to the critical problem of unreliable similarity since the simi-larity between two users is actually calculated using a very small set of co-rated items. The user relationship measured by the unreliable similarity cannot capture the overall rela-tionship between two users [17]. Moreover, the similarities of an active user u a to two users ( u x and u y ) are computed on two different sets of co-rated items, which results in in-comparable similarities. Consequently, the performance of neighborhood-based CF is seriously affected by inaccurate similarities due to the sparsity issue.
In this section, we propose a novel Auto-Adaptive Impu-tation (AutAI) method. To make a rating prediction on the active item t s for the active user u a , we argue that not all missing data in the user  X  item rating matrix possesses equivalent information for this rating prediction, and con-versely there is a key set of missing data that are most in-formative for this particular prediction. Accordingly, this key set should be different for every prediction, even for the same user.

The proposed AutAI method can identify which missing data should be imputed automatically, with the imputed set determined adaptively according to a user X  X  own rating history. Specifically, to make rating prediction on item t for user u a , the imputed set is identified by two factors, the users related to u a a nd the items related to t s . We denote the related users as U a , and denote the related items as T u a and u a  X  i  X  U a , and l = |U a | . For example, suppose that the rating histories for two users u a and u a  X  are represented as: where 0 indicates that the corresponding rating is missing. Then T aa  X  = [ t 1 ,t 5 , ], since both user u a and u a  X  rated t 1 , t 5 etc. Therefore, T s is the union of T aa  X  u a  X   X  U a . Furthermore, with respect to item t s , we define the key neighborhood for the active user u a as: where r a  X  i can either be an observing or missing rating. Nor-mally, due to the sparsity of the user  X  item rating matrix, this selected key neighborhood is also sparse. We define all the missing data in this key neighborhood as the key set of missing data for the prediction  X  r as . For each observing rat-ing r a  X  i in N a,s , it plays a key role in the prediction for  X  r as both user u a  X  and item t i are highly related to  X  r for the missing data in N a,s , they have equal importance for the prediction of  X  r as . Please note that N a,s is defined from the user X  X  perspective, so we call this as the user -based Auto-Adaptive Imputation ( user -based AutAI). After N a,s is determined, for each missing data r a  X  i in N a,s , we use the following equation to impute its value: where sim ( u a  X  ,u x ) is the similarity between u a  X  and u defined: sim ( u a  X  ,u x ) = which is the Pearson Correlation Coefficient (PCC) between u a  X  and u x [19]. Then, the imputed ratings in N a,s can be put back to user  X  item rating matrix R , and will be used for the rating prediction on item t s for user u a . It is clear that the missing data are imputed adaptively for user u a on item t s , and the key set of missing data is identified automatically from the user X  X  perspective. The pseudocode of the user -based AutAI method is presented in Algorithm 1.
Similarly, AutAI can also work in an item-based man-ner. In this case, the key neighborhood for the active user u a with respect to the active item t s is defined as N  X  a,s { r U the set of users who rated both t s and t s  X  Consequently, the missing data in N  X  a,s forms the key set of missing data in this item-based manner. Imputing this miss-ing data is called the item -based Auto-Adaptive Imputation ( item -based AutAI).
 The imputed missing data in N a,s contributes in two ways. Firstly, Cover [8] X  X  research on the nearest neighbor rule Algorithm 1 A uto-Adaptive Imputation (AutAI) Require: t he user-item rating matrix R . the active user Ensure: R  X  : the imputed matrix R  X  . 1: for each u x  X  X  do 2: for each u y  X  X  &amp; u y 6 = u x do 3: calculate sim ( u x ,u y ) according to Eq. 8; 4: end for 5: end for 6: R  X   X  X  ; 8: T s  X  X  X  ; 9: for each u a  X   X  X  a do 12: end for 15: if r a  X  i =  X  then 18: end if 19: end for 20: return R  X  ; shows that the nearest neighbor contains at least 50% of the i nformation in the infinite training set. In this sense, the missing data in N a,s contains much more information than the missing data outside of N a,s . So, imputing this missing data will bring in much more benefit compared to imputing other missing data. Moreover, imputation in this way will also bring in less associated imputation error. We provide a theoretical analysis on this point in the following section. Secondly, AutAI makes a further similarity measurement between each neighbor to the active user in the same T s space. Based on the common assumption of the neighbor-hood -based CF algorithms that if two users rated n items similarly, they tend to rate similarly on other items [5, 22], this auto-adaptive imputation method makes the similarity measurement between the active user and any other neigh-bor on the same T s items, rather than on the co-rated items between two users. Formally, we denote the similarity be-tween two users u a and u x on imputed data as sim  X  ( u a which can be measured by any similarity metric, for exam-ple, its PCC-based version is formulated as: sim  X  ( u a ,u x ) = where  X  u x is the average rating of user u x .

On the other hand, the imputed missing data in N  X  a,s also contributes in a similar way to its counterpart in N a,s , so we will not list them separately. The similarity between two items, after applying the item-based AutAI, can be calcu-lated in a similar way, and its PCC-based version is defined: sim  X  ( t s ,t i ) = where  X  t s is the average rating of item t s .
W hy and how can the proposed AutAI method benefit the neighborhood-based CF? In this section, we attempt to answer this question from a theoretical perspective. Specif-ically, we study how the proposed AutAI method can help to select nearest neighbors more accurately. The analysis is conducted in the user-based AutAI, and it can be extended to the item-based AutAI in a straightforward manner.
Neighbor Selection determines the performance of the neigh-borhood based CF, which is based on the measured simi-larities among users [1, 2]. Let us consider a general case: suppose the active user u a and two other users, u x and u have the following rating histories: where  X  r xi is the imputed rating on item t i for u x , and l = |T | . According to Eq. 5, u a contains the real ratings for u but u x contains both real ratings and imputed ratings. So, both u a and u x can be divided into two subsets: T ax and T \ T ax , where T ax denotes the co-rated item set between u a and u x , and T s \ T ax denotes the relative complement of T ax in T s [20]. Therefore, u a and u x can be represented as: where p = | T ax | and l = |T s | .

We measure the similarity between u a and u x by the dis-tance in their item space. The distance on each item can be considered as an estimation for their similarity. The dis-tance on item t i is denoted as d t i . Then, the distance d between u a and u x can be expressed by where  X  i = r xi  X   X  r xi denotes the imputation error on item t for user t x , and  X  r xi denotes the imputed value.
In this study, for two users, we assume that the distance on any item is independent and identically distributed (i.i.d), and we adopt the normal distribution for theoretical analy-sis. Then, the Probability Density Function (PDF) [20, 29] of d t i is: Similarly, we assume the imputation error  X  on any item for any user is also independent and identically distributed (i.i.d), and its PDF is:
Then, the distance d ax between u a and u x is formulated as the expectation over all possible items, d ax = E ( d t ). Practi-cally, the expectation can be estimated by using the average of all observations, so, where n is the number of items taking into account when measuring d ax . In the user-based AutAI method, this set of items is T s , and d aai ax denotes the distance d ax . Considering T s consists of two subsets, T ax and T s \ T ax , d aai ax by items coming from both of them, and can be represented as: where p = | T ax | , q = |T s \ T ax | , and  X  d t j = d t resents the estimation from imputed values as defined in Eq. 11. Due to the existence of the imputation error  X  , the similarity estimation coming from T s \ T ax has to take it into consideration. The cumulative imputation error for the estimation of d aai ax over T s \ T ax is: Consequently, d aai ax in AutAI is represented as the calculation based on real ratings plus the cumulative imputation error  X  where l = |T s | . According to Eq. 13 and Eq. 15,  X  aai is also normal, and its PDF is: where q = |T s \ T ax | . Normally, due to |T s \ T ax |  X  1, q i s much smaller than  X  data set, mean ( |T s \ T ax | ) = 85 . 08, which means the order of magnitude of  X  2  X  q i s around two orders of magnitude less than  X  2  X  . Namely,  X  aai actually varies little around its mean value, and its variance can be ignored comparing with its initial distribution  X  . Consequently, the measurement of d over T s is defined: where l = |T s | . According to Eq. 12, Eq. 13 and Eq. 17, we obtain that: Similarly, the PDF of the distance between u a and u y in AutAI, d aai ay , is:
We define the distance divergence  X  between the distance of two users u x and u y to the active user u a as: which determines their order to the active user. It is well-k nown that the confidence interval (CI) can be used to indi-cate the reliability of an estimation [20], and it has been widely used in the research of neighborhood-based mod-els [2]. Therefore, in this study, we apply CI to measure the reliability of the estimation of  X  . CI is a range of values that quantify the uncertainty of the estimation, and a nar-row CI means high precision [20]. According to Eq. 18 and Eq. 19, the 100(1  X   X  )% confidence interval for  X  in AutAI can be formulated as follows: where z  X / 2 is a standard normal variate which is exceeded with a probability of  X / 2. We define  X   X  aai as the standard error of  X  aai : Please note the width of CI (  X  aai ) is proportional to  X 
Now, let us consider conventional neighborhood-based CF, the measurement of the distance between u a and u x , d knn is estimated over T ax , which is: where p = | T ax | . According to Eq. 12 and Eq. 23, we have where p 1 = | T ax | . Similarly, d knn ay , the distance between u and u y in conventional neighborhood-based CF, also follows the normal distribution, and its PDF is: where p 2 = | T ay | . Therefore, according to Eq. 24 and Eq. 25, the 100(1  X   X  )% confidence interval for  X  knn ,  X  in neighbor-hood based CF approaches, is: where z  X / 2 is a standard normal variate which is exceeded with a probability of  X / 2.  X   X  knn is the standard error of the estimated  X  knn : where p 1 = | T ax | and p 2 = | T ay | . And the width of CI (  X  is proportional to  X   X  knn .

According to Eq. 5, it is clear that l = |T s |  X  | T ax | = p and l = |T s |  X  | T ay | = p 2 . For example, in the benchmark dataset MovieLens , mean ( |T s | ) = 103 . 47, and mean ( | T 18 . 4. Together with Eq. 22 and Eq. 27, we obtain: with equality if and only if |T s | = | T ax | and |T s | = | T which is not valid when facing the data sparsity issue in the field of collaborative filtering. Furthermore, according to Eq. 21 and Eq. 26, one can see that the CI (  X  aai ) is nar-rower than or equal to CI (  X  knn ), since  X   X  aai is smaller than or equal to  X   X  knn . Based on the above theoretical analysis, we can conclude that the proposed AutAI method can effec-tively improve the performance of the neighborhood-based CF through more accurate nearest neighbor selections by using a sparse rating matrix in a novel way.
For the purpose of recommendation, we propose a new rating prediction algorithm, an Auto-Adaptive Imputation algorithm for both users and items (AutAI-Fusion), which can simultaneously take the user activity and the item popu-larity into account. Specifically, we apply the AutAI method for both users and items, and combine the predictions using a linear function.

To take users activity into consideration, we first perform a user-based AutAI imputation as described in Eq. 6, then make prediction as follows. The prediction for r as in this user-based manner is calculated as:  X  r u as =  X  u a + where  X  u a is the average rating of u a , and sim  X  ( u defined in Eq. 9.

Similarly, to take item popularity into consideration, we do an item-based AutAI imputation as described in Sec-tion 3.2, then the prediction for r as in this item-based man-ner is calculated as: where  X  t s is the average rating of t s , and sim  X  ( t in Eq. 10.

After imputing and predicting from both user and item re-spectively, the final prediction for r as in the proposed AutAI-Fusion algorithm is calculated as: A similar fusion strategy is performed in [16, 27]. The pa-rameter  X  determines to what extent the final prediction is based on user-based AutAI or item-based AutAI imputation prediction. When  X  = 1, the prediction is completely gen-erated by taking user activity to perform an AutAI-based prediction. On the other hand, when  X  = 0, the prediction is totally estimated by taking item popularity to perform an AutAI-based prediction. The value for  X  can be determined by doing cross-validation, which we will discuss further in the experiment section.
In this section, we conduct several experiments to answer the following questions: 1. How does the proposed AutAI method perform with 2. How does the proposed AutAI-Fusion algorithm com-3. How does the parameter  X  affect the performance of
The data set we experiment with is the popular bench-mark data set MovieLens 1 , which has been widely used in CF research [16, 18, 27, 28]. MovieLens includes around 1 million ratings collected from 6 , 040 users on 3 , 900 movies.
To evaluate the performance thoroughly, we extract a sub-set of 2000 users from MovieLens who rated at least 30 movies, and further we set up several different experimen-tal configurations. Specifically, we split the selected subset into two sets, the Training set and the Test set. The size of the Training set varies from the first 500, 1000 and 1500 users, and are denoted as M 500 , M 1000 and M 1500 respec-tively. The remaining 500 users are treated as the Test set. For each of the active user within the Test set, we alter the number of rated items provided from 10, 20 to 30, which are represented as Given 10 , Given 20 and Given 30 , respectively. This protocol is widely used in Collaborative Filtering re-search [16,27,28]. Furthermore, we also apply the All-But-One configuration, in which we randomly select one single rating for each user in the data set, then try to predict its value when observing all the other ratings the user has given.
For consistency with other literatures [16, 27, 28], we ap-ply the Mean Absolute Error (MAE) as the measurement h ttp://www.grouplens.org/ Table 1: MAE comparison on different similarity measurement metrics on the Given Data set. (A smaller value means better performance) Table 2: MAE comparison on different similarity m easurement metrics on the All-But-One Data set. (A smaller value means better performance) metric, which is defined as: where r as is the true rating given by user u a on item t  X  r as is the predicted rating, X denotes the test data set, and | X | represents its size. A smaller MAE value means better performance.
In order to evaluate the performance of AutAI, we use the proposed AutAI method on two state-of-the-art simi-larity metrics in Collaborative Filtering [1, 10, 16, 27, 28], the PCC and the COS. We implement 4 algorithms in the user-based manner, namely the user-based PCC algorithm, user-based COS algorithm, and user-based AutAI with PCC and COS, respectively. Then, we compare their prediction performance, in which the number of neighbors is set to 30. We examine AutAI performance on all the experiment con-figurations, including Given and All-But-One data sets.
The results on the Given and All-But-One data sets are shown in Table 1 and Table 2, respectively. The results show that AutAI achieves significant improvements on both similarity metrics in all experiment configurations. This in-dicates that the proposed AutAI method works well and robustly in different sparsity situations. Specifically, on the Given data set, the number of given ratings for the active user is restricted to 10, 20 or 30, which suppresses the effec-tiveness of AutAI, as AutAI determines the key set of miss-ing data to fill in according to the rated items by this user as shown in Eq. 6. However, AutAI can still significantly outperform the user-based CF on all the Given data sets. On the All-But-One data set, AutAI achieves even larger improvements on both PCC and COS metrics as shown in Table 2. For PCC, the MAE is reduced from 0 . 7477 to 0 . 6910 by 7 . 58%, and for COS, it is reduced from 0 . 7652 to 0 . 6932 by 9 . 41%. This is mainly because there is no limitation to users X  rating history, so AutAI can fully work in this natural setting.
Given 10
Given 30 Table 3: Paired-t-test for MAE on the Given Dataset and the All-but-one Dataset
Moreover, in order to examine AutAI X  X  performance thor-o ughly, we vary the number of neighbors from 5 to 40 so as to examine its sensitivity to the neighborhood size. In this paper, we report results on M 500 Given 30 and M 1500 Given for PCC, results on M 500 Given 10 and M 500 Given 30 for COS, and results on All-But-One data sets for both PCC and COS in Fig. 1. We can observe that the neighborhood size does affect the performance. The AutAI method outperforms its counterpart across all neighborhood sizes from 5 to 40 on all data sets. This indicates that the AutAI method iden-tifies the neighborhood relationship more accurately as we demonstrate in Section 3.3, and therefore, achieves better performance across all neighborhood sizes.

The two-tailed, paired t-test with a 95% confidence level has been applied to evaluate the performance of AutAI on both similarity metrics. The results show that the differ-ence of performance with and without AutAI is statistically significant. The detailed paired-t statistics are shown in Ta-ble 3.
In this section, we examine the proposed AutAI-Fusion algorithm by comparing it with other state-of-the-art impu-tation -based algorithms, including the default voting (De-fault Voting) method [5], the EMDP method [16], and the SCBPCC meth-od [28], and two traditional collaborative fil-tering algorithms, the user-based CF (UPCC) and the item-based CF (IPCC), and one model-based algorithm, the Slope Table 4: MAE comparison with other methods on the Given Data set. (A smaller value means better performance) One a lgorithm [14]. For fair comparison, we set  X  and K with the values used in [28], but they may not lead to the best performance. Specifically, the parameters in SCBPCC are set as  X  = 0 . 35, and the cluster number K = 20. All the parameters in EMDP are set as used in [16], namely  X  = 0 . 7 , X  = 30 , X  = 25 , X  =  X  = 0 . 4. Please note that EMDP is an imputation-based algorithm by fusing the user-based CF algorithm and the item-based CF algorithm. The parameter  X  in our algorithm is set to 0 . 4.
 The results on the Given data sets are shown in Table 4. Clearly, it is observed that AutAI-Fusion outperforms all of the other 6 algorithms on all configurations. Specifically,
Given 30 performance) Table 6: Paired-t-test for MAE on the G iven Dataset and the All-but-one Dataset on the M 5 00 Given 10 data set, although all imputation-based methods, AutAI-Fusion, EMDP, SCBPCC and Default Vot-ing, achieve a better performance than UPCC and IPCC, AutAI-Fusion achieves the largest improvement. This trend is obvious on all data sets as shown by the figures in bold in Table 4. This indicates that when the training set is small and the rating history of users is limited, AutAI-Fusion can still identify the most determinant missing values to fill in, in order to identify more appropriate relationships among neighbors. On the All-But-One data set as shown in Ta-ble 5, AutAI-Fusion obtains even better results than all com-pared algorithms by achieving a much smaller MAE 0 . 6864. This is mainly because there is no limitation to the rating history of users, and also because AutAI can work more efficiently in this natural configuration. Table 6 lists the paired-t test statistics (with a 95% confidence level) between AutAI-Fusion and these algorithms, and it is clear that the differences between the performance of AutAI-Fusion and that of other methods are statistically significant.
As we discussed in Section 3.4, we introduced a parameter  X  to balance the prediction from the user-based AutAI and the item-based AutAI, to simultaneously consider both the activity of users and the popularity of items. We conducted several experiments to determine the impact of  X  to the proposed AutAI-Fusion algorithm. Specifically, we vary the  X  value from 0 to 1 with an increasing step of 0 . 05. We report the results on M 500 Given 10 , M 500 Given 30, and All-But-One data sets in Fig. 2.

When  X  = 0, the prediction totally depends on the item-based AutAI imputation; when  X  = 1, the prediction fully depends on the user-based AutAI imputation. Results on the Given data sets show that when there are few train-ing users or few ratings of the active user, item-based Au-tAI can achieve significant improvement compared to user-based AutAI. For example, on data sets M 500 Given 10 and M 500 Given 30 as shown in Fig. 2a and Fig. 2b, the per-formance of AutAI-Fusion with  X  = 0 is better than its performance with  X  = 1. This is because a limited user rating history suppresses the effectiveness of user-based Au-tAI. However, AutAI-Fusion achieves an even better perfor-mance with  X  = 0 . 4. On the other hand, Fig. 2c shows that user-based AutAI and item-based AutAI achieve a similar performance on the All-But-One configuration, which indi-cates that in a natural situation there is no big difference between them. Yet, on all configurations, it is clear that better accuracy can be obtained by combining imputations from users and items.
In this paper, we treat each rating given by a user on an item as an observation for the predictions on this item to other users, which matches the theory of nearest neighbor rules [2, 8]. From this point of view, we define the nota-tion of the key set of missing data for the rating predic-tion, and propose AutAI, a method to automatically and adaptively identify these key missing data for each predic-tion from both the user and the item perspective. Theo-retical and empirical analysis show that imputation based on this key set of missing data leads to a more accurate identification of neighborhood relationships. Therefore, the AutAI-based algorithm can make more accurate predictions than neighborhood-based CF algorithms, including other imputation-based algorithms. To the best of our knowledge, no previous work has studied the theoretical analysis for imputation-based CF methods like this paper has. There-fore, the proposed AutAI method guarantees to work regard-less of whichever similarity metric is applied. Moreover, we also propose an AutAI-Fusion algorithm based on the AutAI method to make predictions by taking user activity and item popularity into consideration. The experiment results show that: (1) the proposed AutAI method can identify neighbors more accurately, and (2) the proposed AutAI-Fusion algo-r ithm can achieve better predictions in terms of accuracy. [1] G. Adomavicius and A. Tuzhilin. Toward the next [2] N. S. Altman. An Introduction to Kernel and [3] R. M. Bell and Y. Koren. Lessons from the Netflix [4] D. Billsus and M. Pazzani. Learning collaborative [5] J. Breese, D. Heckerman, C. Kadie, and Others. [6] L. M. D. Campos, J. M. Fern  X andez-luna, J. F. Huete, [7] S. Chee, J. Han, and K. Wang. Rectree: An efficient [8] T. Cover. Estimation by the nearest neighbor rule. [9] C. Desrosiers and G. Karypis. A Novel Approach to [10] C. Desrosiers and G. Karypis. Chapter 4 A [11] S. a. Goldman and M. K. Warmuth. Learning binary [12] Y. Koren. Factorization meets the neighborhood: a [13] M. Larson and A. Hanjalic. Exploiting User Similarity [14] D. Lemire and A. Maclachlan. Slope one predictors for [15] R. Little. Missing-data adjustments in large surveys. [16] H. Ma, I. King, and M. R. Lyu. Effective missing data [17] M. R. Mclaughlin and J. L. Herlocker. A Collaborative [18] Y. Ren, G. Li, and W. Zhou. Learning Rating [19] P. Resnick, N. Iacovou, M. Suchak, P. Bergstrom, and [20] G. K. B. Richard A. Johnsom. Statistics: Principles [21] D. Rubin. Multiple imputation for nonresponse in [22] B. Sarwar, G. Karypis, J. Konstan, and J. Reidl. [23] X. Su and R. Greiner. A Mixture Imputation-Boosted [24] X. Su and T. Khoshgoftaar. Collaborative Filtering for [25] X. Su, T. Khoshgoftaar, and R. Greiner. Imputed [26] X. Su, T. M. Khoshgoftaar, X. Zhu, and R. Greiner. [27] J. Wang, A. P. de Vries, and M. J. T. Reinders. [28] G.-R. Xue, C. Lin, Q. Yang, W. Xi, H.-J. Zeng, Y. Yu, [29] J. Zhang, Y. Xiang, Y. Wang, W. Zhou, Y. Xiang,
