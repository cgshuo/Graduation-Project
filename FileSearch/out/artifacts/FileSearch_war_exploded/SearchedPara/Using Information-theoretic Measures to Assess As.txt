
Assessing rules with interestingness measures is the cor-nerstone of successful applications of association rule dis-covery. However, there exists no information-theoretic mea-sure which is adapted to the semantics of association rules. In this article, we present the Directed Information Ratio (
DIR ) , a new rule interestingness measure which is ba-sed on information theory. DIR is specially designed for association rules, and in particular it differentiates two opposite rules a  X  b and a  X  b . Moreover, to our knowledge, DIR is the only rule interestingness measure which rejects both independence and (what we call) equili-brium, i.e. it discards both the rules whose antecedent and consequent are negatively correlated, and the rules which have more counter-examples than examples. Experimental studies show that DIR is a very filtering measure, which is useful for association rule post-processing.
Many data mining techniques produce results in the form of rules. These are expressions of the type  X  X f an-tecedent then consequent  X  where the boolean propositions antecedent and consequent are conjunctions of assignment expressions variable=value . Rules have the advantage of being very intelligible for users since they model infor-mation explicitly. They are also a major element of most theories of knowledge representations in cognitive sciences [10], and in particular they underlie many works in artificial intelligence, such as the expert systems. In knowledge dis-covery in databases, the main rule-based paradigms are the classification rules, used in supervised learning to predict a unique class variable as consequent, and the association rules [1], which can have any combination of variables as antecedent and consequent. Classification rules can be ge-nerated by induction algorithms such as CN2 [9] or decision tree algorithms such as C4.5 [18], while association rules are mined by combinatorial algorithms such has Apriori [1].
Due to their unsupervised nature, association rule mi-ning algorithms commonly generate large amounts of rules, with much redundancy [25]. To help the user to find re-levant knowledge in this mass of information, one of the main solutions consists in evaluating and sorting the rules with interestingness measures. There are two kinds of mea-sures: the subjective (user-oriented) ones and the objective (data-oriented) ones. Subjective measures take into account the user X  X  goals and domain knowledge [14] [16], whereas only the data cardinalities appear in the calculation of ob-jective measures (surveys can be found in [22], [11], [24], [2]). In this article, we are interested in the objective mea-sures. We have shown in [4] that there are two different, but complementary, aspects of the rule interestingness: the de-viation from independence and the deviation from what we call equilibrium (maximum uncertainty of the consequent given that the antecedent is true). Thus, the objective mea-sures of interestingness can be classified into two classes:  X  the measures of deviation from independence, which  X  the measures of deviation from equilibrium, which
Among the objective measures of rule interestingness, the information-theoretic measures are particularly intelli-gible and useful since they can be interpreted in terms of information. More precisely, as pointed out by Smyth and Goodman [21], there is an interesting parallel to draw bet-ween the use of information theory [20] in communication systems and the use of information theory to evaluate rules. In communication systems, a channel has a high capacity if it can carry a great deal of information from the source to the receiver. As for a rule, the relation is interesting when the antecedent provides a great deal of information about conditional entropy ( H c ) mutual information ( MI )
Theil uncertainty coefficient ( u )
J-measure ( J ) p ab . log 2 p ab p
Gini index ( G )
Table 1. Information-theoretic measures of in-terestingness for a rule a  X  b the consequent (Smyth and Goodman speak of the infor-mation content of a rule [21]). The information-theoretic measures commonly used to evaluate rule interestingness are the Shannon conditional entropy [9], the average mutual information [12] (often simply called mutual information), the Theil uncertainty coefficient [23] [22], the J-measure [21], and the Gini index [2] [12] (cf. the formulas in table 1). The Shannon conditional entropy measures the average amount of information of the consequent given that the an-tecedent is true (it is used in the CN2 algorithm). The ave-rage mutual information (Shannon entropy decrease) mea-sures the average information shared by the antecedent and the consequent. The Theil uncertainty coefficient measures the entropy decrease rate of the consequent due to the an-tecedent. The J-measure is the part of the average mutual information relative to the truth of the antecedent. Finally, the Gini index is the quadratic entropy decrease.
Even if these measures are commonly used to evaluate association rules (see [11], [22], [2]), they are all better suited to evaluate classification rules. As pointed out by Jaroszewicz and Simovici [12], an association rule should be assessed only on the variable values which are com-prised in the rule 1 , whereas the information-theoretic mea-sures consider the full joint distribution of the antecedent and consequent (this is relevant for classification rules since in supervised learning, the user is interested in all the va-lues of the consequent because it is the class variable). Consequently, the information-theoretic measures do not vary with the permutation of the values of a variable 2 .This invariance is undesirable for association rules since the per-mutation of values definitely transforms an association rule. We say that association rules are not only  X  X ariable-based X  relations but also  X  X alue-based X  relations. If all the same such measures are applied on association rules, then this must be done carefully since it is not possible to distinguish between positive and negative correlations [22].
To be appropriate to association rules, an interestingness measure must respect their value-based semantics by not systematically giving the same value to a rule a  X  b and to its opposite a  X  b . Intuitively, if a  X  b is strong, then a  X  b should be weak. In this article, we propose an inter-estingness measure based on information theory which res-pects the value-based semantics of association rules. This new measure named DIR (for Directed Information Ratio ) allows to reject both the independence and equilibrium si-tuations, i.e. with only one fixed threshold it allows to discard both the rules whose antecedent and consequent are negatively correlated, and the rules which have more counter-examples than examples. To our knowledge, this is a unique feature for a rule interestingness measure. In the next section, we introduce the new measure DIR from earlier works on the assessment of rules using information theory. Section 3 will then review the properties of DIR . Finally, in section 4 we compare DIR to other rule interes-tingness measures within the framework of formal and ex-perimental studies.
We consider a set of objects described by boolean va-riables. In the association rule terminology, the objects are transactions stored in a database, the variables are called items, and the conjunctions of variables are called itemsets. An association rule is a couple ( a, b ) noted a  X  b where and b are two itemsets which have no items in common. The examples of the rule are the objects which verify the ante-cedent a and the consequent b , while the counter-examples are the objects which verify a but not b . A rule is all the bet-ter since it has lots of examples and few counter-examples. In the following, we study two itemsets a and b that we sim-ply call the variables.
 The Shannon entropy of the variable a is: H ( a )=  X  p ( a =1) . log 2 p ( a =1)  X  p ( a =0) . log 2 p The Shannon conditional entropy of the variable b given an event a =1 is defined by:
H ( b/a =1) =  X  p ( b =1 /a =1) . log As can be seen, the entropic functions combine variables and realizations of variables. In order to distinguish them, ,b =1) b ) the realizations of a variable b must be noted b =1 and b =0 in this article, and not b and b as commonly done in the association rule literature. With these explicit notations, an association rule should be written ( a =1)  X  ( b =1) , but we retain the classical notation a  X  b .
Let us consider the amount of information that an event a =  X  gives about a variable b (  X   X  X  0; 1 } ) . We note M ( a =  X , b ) the measures of this amount of information. Blachman [3] studied the M ( a =  X , b ) whose expectation (when averaged over all  X  ) is the average mutual informa-tion between the variables a and b : The two most frequently used measures are the following (see figures 1 and 2): Blachman shows that j is the only non-negative information-theoretic measure satisfying (1), while i is the only antisymmetric 3 information-theoretic measure satisfying (1).

The measure j is the cross-entropy between the apriori and a posteriori distributions of b . It is traditionally ac-cepted as  X  X he X  measure of the amount of information that a =  X  gives about b . In particular, the J-measure (the most commonly used information-theoretic measure within the context of association rules) directly comes from j : J = j  X  P( a =  X  ) [21]. Although the meaning of the measure i is much more obvious (it is the entropy decrease of b due to the event a =  X  ), one prefers j to i because j vanishes only if the variables a and b are independent, while i can vanish outside the independence (see figures 1 and 2). This behavior is due to the symmetrical nature of the entropy H (it does not vary with the permutation of the variable values).
In order to remove the symmetry introduced by the entropy in the measure i , we propose to use a directed entropic function H named reduced entropy [5] (see figure 3).
 Definition 1 The reduced entropy H ( a ) of a variable a is defined by:  X  if p ( a =1)  X  1  X  if p ( a =1)  X  1 One similarly defines the conditional reduced entropy of the variable b given the realization of a :  X  if p ( b =1 /a =1)  X  1  X  if p ( b =1 /a =1)  X  1 The entropy H ( a ) of a variable a can be written as the sum of two reduced entropies: H ( a )= H ( a )+ H ( a )  X  1 , with a being the negation of Contrary to H , H is an asymmetric measure which diffe-rently evaluates an imbalance in favor of a =1 and an im-balance in favor of a =0 : H ( a ) = H ( a ) . More precisely, if a =1 is more frequent than a =0 , then:  X  the reduced entropy H ( a ) measures the entropy of a :  X  the reduced entropy H ( a ) is 1.
 If a =1 is less frequent than a =0 , then the roles are rever-sed. In other words, H measures a  X  X irected uncertainty X  in favor of one of the values, in the sense that if this value is not the more likely, then the uncertainty is considered as maximal.
By introducing the reduced entropy H in the measure i , we have: i ( a =1 ,b )= H ( b )+ H ( b )  X  H ( b/a =1)  X  H ( b/a =1) Hence: So the index i which measures the decrease of the entropy H is the sum of two decreases of reduced entropy H :  X  i ( a =1 ,b ) which is the decrease of reduced entropy  X  i ( a =1 , b ) which is the decrease of reduced entropy
Contrary to the measures i and j , the new index i ( a = 1 ,b ) is absolutely appropriate to evaluate the interestin-gness of an association rule a  X  b : Indeed, i ( a =1 ,b ) increases with the number of examples (probability p ( a =1 ,b =1) ), decreases with the number of counter-examples (probability p ( a =1 ,b =0) , see figure 4), and respects the value-based semantics of association rules by differentiating opposite rules a  X  b and a  X  b . The higher i ( a =1 ,b ) , the more the event a =1 brings information in favor of b =1 , and the more the interestin-gness of the rule a  X  b is guaranteed. If i ( a =1 ,b ) is negative, this means that a =1 brings no information in fa-vor of b =1 , and even that it  X  X emoves X  some information (the uncertainty is lesser by predicting b =1 randomly ra-ther than by predicting b =1 using the rule a  X  b ). In our opinion, i is a measure of what Smyth and Goodman call the information content of rules [21].

Like the directed contribution to  X  2 [13], i allows to dis-tribute the average mutual information of two variables over the rules between them:
MI ( a, b )= p ( a =1) . i ( a  X  b )+ p ( a =1) . i ( a  X  b p ( a =1) . i ( a  X  b ) is the directed contribution of the rule a  X  b to the average mutual information. Each rule takes part in the average mutual information by giving or remo-ving its share of infomation. Like the  X  2 , the average mu-tual information can also be written with the contributions of the four opposite rules.

For all these characteristics, we propose to retain the index i to evaluate the interestingness of association rules. However, a drawback of i is that its maximal value is not fixed but depends on p ( b =1) , making the comparison of rules with different consequents difficult. This maximal value is obtained for logical rules, i.e. rules with no counter-examples ( p ( a =1 ,b =0)=0) . In order to facilitate the filtering of the most informative rules, we normalize i by assigning the maximal value 1 to the logical rules. This amounts to calculating the decrease rate of reduced entropy.
 Definition 2 The Directed Information Ratio ( DIR ) of arule a  X  b is defined by: DIR ( a  X  b )= If p ( b =1)=1 , then H ( b )=0 and DIR is not de-fined. However, such rules are obviously to be discarded since they are completely expected ( i is indeed 0 for these rules). A rule is said to be informative if its DIR is strictly positive.

The main properties of DIR are given in table 2. It must be noticed that DIR satisfies the three properties that de-fine a good interestingness measure according to Piatetsky-Shapiro [17]: it is 0 at independence, it increases with the examples, and it decreases with the sizes of the antecedent and consequent (variations with all other parameters fixed). Furthermore, DIR has no symmetry:  X  it does not assign the same value to a  X  b and to its  X  it does not either assign the same value to a  X  b and
As shown in figure 5, DIR is a convex decreasing func-tion of the number of counter-examples. Among the rule interestingness measures, it belongs to the demanding in-dexes, i.e. the indexes which decrease quickly with the first counter-examples and thus allow to better discriminate the good rules (larger dispersion of values).

Let us consider a rule ( a  X  b ) described by the pro-babilities p ( a =1) , p ( b =1) , and p ( a =1 ,b =0) 4 . The independence is defined by p ( a =1 ,b =0)= p ( a =1) .p ( b =0) , while the equilibrium is defined by p ( a =1 ,b =0)= 1 with fixed p ( a =1) and p ( b =1) , one can distinguish two different cases for DIR :  X  If p ( b =1)  X  1  X  If p ( b =1)  X  1
DIR allows to reject both the independence and equi-librium situations. Indeed, in these situations, DIR is ei-ther negative or worth zero (see table 2). By retaining only strictly positive values of DIR (informative rules), the user discards all the rules whose deviation from indepedence is bad (rules between negatively correlated variables), and also all the rules whose deviation from equilibrium is bad (rules with more counter-examples than examples). So, the measure must be used with a strictly positive threshold to filter the rules. To our knowledge, DIR is the only rule in-terestingness measure which can reject both independence and equilibrium with a fixed threshold. This approach is completely original for rule interestingness assessment.
Figure 6. Information-theoretic measures of deviation from independence
In this section, we compare DIR to the information-theoretic measures traditionally used to evaluate rule inter-estingness (see table 1 for formulas):  X  the Shannon conditional entropy [9], which measures  X  the mutual information [12], the Theil uncertainty [23] As the last four measures have similar behaviors (see figure 6), we only plot one of them in the comparisons below. We choose the J-measure since it is used a lot within the context of association rules (in particular it does not assign the same value to a rule a  X  b and to its converse b  X  a ). As for the conditional entropy, it is not the function H c of the table 1 which is plotted in the comparisons below, but the complementary function 1  X  H c . Indeed, H c assigns its smallest values to the best rules 5 . One generally prefers the opposite behavior for a rule interestingness measure [17].
The figures 7.(A) and 7.(B) compare DIR to the condi-tional entropy and to the J-measure when the probability of counter-examples p ( a =1 ,b =0) increases. The fi-gures clearly illustrate that the conditional entropy and the J-measure do not respect the value-based semantics of asso-ciation rules, since they can increase even though the pro-bability of counter-examples increases. Moreover, one can see that DIR and the conditional entropy have the advan-tage of systematically assigning the value 1 to the logical rules, which are the best rules from an objective point of view. This makes the comparisons among the rules easier, and facilitates the choice of a threshold to filter the rules. On the contrary, for the J-measure and the three other measures of deviation from independence, a value can be assigned to a good rule (lots of examples, few counter-examples), even though on other data the same value would be assigned to a bad rule. In fact, except for the value 0 which always corre-ponds to independence, the values taken by these measures cannot be interpreted in an absolute way, i.e. independently of the data.

The figures 7.(A) and 7.(B) show that the conditional en-tropy detects the equilibrium but not the independence (it could even take high values at independence). On the other hand, the J-measure detects the independence but not the equilibrium. In all cases, filtering the rules on DIR with a strictly positive threshold is enough to reject both indepen-dence and equilibrium. As illustrated in figure 7.(B), DIR is similar to the conditional entropy when p ( b =1)  X  1 2 (the functions are partly identical). This is what enables DIR to detect the equilibrium when p ( b =1)  X  1
We compare the distributions of DIR to the distributions of other interestingness measures on the association rules mined from four datasets (described in table 3). The two first datasets were generated using the IBM synthetic data generator 6 described in [1] which simulates purchases in a supermarket. The two other datasets are a database of lift breakdowns provided by a lift maintenance company, and a database of workers X  psychological profiles used in hu-man resource management. The rules were mined with the Apriori algorithm [1] with a low support threshold to avoid the premature elimination of potentially interesting rules.
As we want here to compare the distributions of mea-sures, we choose measures which, as DIR , take the value 1 for the logical rules. Among the information-theoretic mea-sures, only the conditional entropy satisfies this condition. So, we add to our comparisons two reference measures of rule interestingness which satisfy the condition: the confi-dence [1] ( p ( b =1 /a =1)) and the Loevinger index [15] (1 tion from equilibrium and from independence. As figures 8.(A-D) show, DIR is the most filtering index: for the four datasets, whichever the threshold chosen between 0 and 1, DIR prunes more rules than the others. This is especially useful within the context of association rules where the mi-ning algorithms often generate huge amounts of rules.
Let us explain why DIR is very filtering. In figure 8.(E) in parallel coordinates, each line represents a rule. The fi-gure shows representative rules from T10.I4.D5k that are judged good by confidence but not by the Loevinger index (they have a good deviation from equilibrium but not from independence). On the other hand, figure 8.(F) shows repre-sentative rules from BREAKDOWNS that are judged good by the Loevinger index but not by confidence (they have a good deviation from independence but not from equili-brium). DIR gives bad values to all these rules, since it takes into account both independence and equilibrium.
In this article, we have presented the Directed Infor-mation Ratio ( DIR ) , a new rule interestingness measure which is based on information theory. DIR is specially designed for association rules, and in particular it respects their value-based semantics by differentiating the opposite rules a  X  b and a  X  b . Moreover, to our knowledge, DIR is the only rule interestingness measure which rejects both independence and equilibrium, i.e. it discards both the rules whose antecedent and consequent are negatively cor-related, and the rules which have more counter-examples than examples. Experimental studies have also shown that DIR is a very filtering measure, which is useful for associa-tion rule post-processing. To continue this research work, we will integrate DIR into a data mining platform in order to experiment with this new measure in real applications.
Like all the information-theoretic measures, DIR is a frequential index. This means that it takes into account the size of the data only in an relative way, and not in an ab-solute way (see [4]). More generally, in order to have a complete assessment of the rules, one has to measure not only the deviations from equilibrium and independence, but also the statistical significance of these two deviations. For example,  X  2 [7] or implication intensity [6] allow to mea-sure the statistical significance of the deviation from inde-pendence, while IPEE [4] allows to measure the statistical significance of the deviation from equilibrium. These ap-proaches are complementary to DIR .
 (E and F, in parallel coordinates)
