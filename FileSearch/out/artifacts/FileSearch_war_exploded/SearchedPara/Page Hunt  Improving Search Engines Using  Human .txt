 There has been a lot of work on evaluating and improving the relevance of web search engines. In this paper, we suggest using human computation games to elicit data from players that can be used to improve search. We desc ribe Page Hunt, a single-player game. The data elicited using Page Hunt has several applications including providing metadata for pages, providing query alterations for use in query re finement, and identifying ranking issues. We describe an experime nt with over 340 game players, and highlight some interesting as pects of the data obtained. Categories and Subject Descriptors: H.3.m [Information Storage and Retrieval]: Miscellaneous General Terms: Design, Experimentati on, Human Factors. Keywords: Web Search, Human Computation Games, Query Alterations, Relevance Search engines have become an integral part of our everyday relevance of search engines, and this is typically done using hand-annotated evaluation corpora or measures such as clicks on results. These methods do not help if specific pages never get surfaced due to a variety of reasons; these pages will not get evaluated, nor will they figure in click data. To obviate problems in finding page s relevant to a given query, we look in the other direction: given a web page, what queries will effectively find this web page? To study a large number of pages, we propose to use games to collect relevant data. As reported in [3], more than 200 million hours are spent each day playing computer and vide o games in the U.S. Human computation games seek to harness the time and energy that people spend on games to solve co mputational problems that are otherwise difficult to tackle. Human computati on games were made popular by the ESP game [2], where people tackle an image labeling problem. In these (typically) two-player games, players are matched randomly to prevent collusion. The players collaborate to find  X  X rue X  answers for each task. When there are an odd number of players, these games typically use previously recorded (two-person) games; this raises some issues. In addition, collaborative games tend to elicit very general tags. *This work was done when the authors were on summer internships at Microsoft Research. 1. The player is shown a random web page with URL U. The player can see the web page, but not the URL of the page. 2. The player types in one or more words as a query Q. 3. The system gets the top N search results for this query from a search engine (Live Search, in this case) and displays them. 4. This match is successful if the URL U is in the top N results for the query Q. Players get points based on the rank of U in the result set. The player X  X  score gets updated, and the game advances to the next web page. If query Q does not lead to success, the player changes the query and trie s again. The game continues, page after page, till the player hits a time limit or quits. At each step, we record the player X  X  screen name (or an anonymous id), the web page UR L, the query that was tried, whether it was successful and if so at what rank position, the time, and the points the player got for this query. The player X  X  query behaves as a tag or label for the page. When the player gets it right, this is valuable; but even when it is wrong, the label can be useful. A transparent overlay on the web page prevents players from cutting and pasting long phrases from the page as queries. Players can type in long phrases, but they learn to be discriminating. Page Hunt includes several features to increase fun in game playing, such as using timed re sponses, score keeping, having a top-scorers list, taboo queries, and bonus points. The web pages we use in Page Hunt are those that have been identified by a search engine as requiring better labels, additional metadata, or have ranking issues. Before we released the game to the web, we conducted a pilot study within our company. During this experiment, 341 people played Page Hunt over a peri od of ten days, generating over 14,400 labels on 744 web pages in the system. On average every player contributed 42 labels, and every page has about 19 labels. Using a session limit of 10 minutes of inactivity, we had a total of 681 sessions. Of these, 18% (123 sessions) were by anonymous players. About 47% of the sessi ons were from people who played 2 or more sessions. 16% of the pe ople had 5 or more sessions, and only 240 sessions (35% of the to tal number of sessions) were single session players. From the data collected, we find that some of the pages are easily found (or  X  X unted X  down), while some others are difficult to get in the top search results. A 100% findability level indicates that a page can be easily located sin ce every person shown this web page was able to bring up this page to the top 5 search results. In our experiment, about 27% of th e pages in our database have 100% findability while roughly the same number of pages (26%) has 0% findability. We investigated this further and found that as the length of URL of the page (in characters) increases, the pages are harder to find. See Fig. 2, which shows the findability distribution of the pages as a function of URL length. Findability can be used to compare goodness of search engines, and we intend to work further in this area. 
