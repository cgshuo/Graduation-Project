 Support Vector Machine (SVM), developed by Vapnik based on the Structural Risk Minimization (SRM) principle [1], has performed with a great generalization accu-racy [2]. SVR, a modified version of SVM, was developed to estimate regression functions [3]. Both SVM and SVR are capable of solving non-linear problems. 
For a brief review of SVR, consider a regression function f(x) to be estimated with training patterns { (x i , y i ) } By the SRM principle, the generalization accuracy is optimized by the flatness of the problem. Hence, SVR is trained by minimizing || w || 2 with including training patterns inside the 
It takes O(N 3 ) to solve the optimization problem of Eq. (3), thus the training time training time increases more radically, i.e. in a cubic proportion. 
So far, many algorithms such as Chunking, SMO, SVM light and SOR have been the number of iterations and q is the size of working set. However, their training time complexity is still strongly related to the number of training patterns [4]. Another direction of research efforts focuses on reducing the number of patterns. i.e. the generalization performance of SVM deteriorates. What is desired is to reduce the number of training patterns without accuracy loss. Instead of training SVM with training. Such methods include NPPS (O(N 2 ) ) [5] and Fast NPPS (O(vN) ) [6]. How-ever, NPPS approaches were developed for classification problem, not regression problem. 
In 2004, a pattern reduction method for regression tasks was proposed, which is called HSVM [7]. The training patterns are split into k groups. Then the similarity is calculated between every pattern with the center pattern of each group. The pattern is selected if similarity (i.e. reverse of their euclidean distances) of the pattern is larger than a pre-fixed threshold. Finally, patterns that are far away from each group X  X  cen-ter are rejected from a training pattern set. However, too much accuracy loss occurred. 
The k-NN based pattern selection method was also proposed that employed en-more or less same [8]. get the same regression function with a significantly smaller number of patterns (see estimated from multiple bootstrap samples. Two artificial datasets and two real-world datasets were used for experiments. HSVM and random sampling method were used on benchmark methods. We com-pared the respective results in terms of the training time and mean squared regression error. we summarize the result and conclude the paper with a remark on limitations and fu-ture research directions. tube and then selected patterns stochastically. We made k bootstrap samples of size l (l&lt;n) from original training pattern set ( D ). We trained an SVR with each bootstrap sample and obtained k SVR regression func-
The algorithm is presented in Fig. 4. We used two artificial datasets and two real-world datasets to show the performance function given in Eq. (5) used in [10]. The input variable x was drawn uniformly from consists of 1,000 training patterns and 1,000 testing patterns. terns. of 200 stock prices of Korean stock market. We gathered 2,500 daily patterns between 1995~2004. The first 2,000 patterns were used for training, while the last 500 patterns were used for testing. Both real-world datasets are time series datasets. Hence, we re-Real-world datasets were normalized. based on [13]. The functions of the parameter setting are given in Eq. (7) and Eq. (8). all experiments. 
The parameters of the proposed method were set as follows. The number of boot-strap samples k was set to two values : 10 and 100. The other parameter that controls the number of patterns in a bootstrap sample l was set to 10% of the number of pat-terns in dataset, n . The number of selected patterns s was set to 10%, 30%, 50%, and 70% of n . 
HSVM and random sampling were also implemented to be compared with the pro-posed method. HSVM has a threshold parameter to set. So, we tried various threshold thresholds for each experiment. Mean Squared Error (MSE) was used as a measure of accuracy. Each setting was repeated 10 times and the result is an average of these. time in seconds pairs are plotted that correspond to 70, 50, 30 and 10 percents of pat-shapes of dots. The proposed method was more accurate than HSVM and random sampling given a same amount of training time. The SVRs trained with as low as 30% of the patterns selected by the proposed method resulted in a smaller MSE than SVR trained with 100% of patterns. Many of noisy patterns seemed to be removed. Fig. 8 outperformed HSVM and random sampling. 
The experimental result of Santa Fe E dataset is shown in Fig. 9. The SVRs trained percentages and than HSVM for 70% and 10%. The SVR trained by HSVM did very well for some percentages but not for others. Its results seem rather unstable. The ex-tained. We selected a subset of patterns that are important for training and accomplished re-world datasets, Santa Fe E dataset and KOSPI200 dataset were employed for com-parison. The results showed that the generalization performance of the proposed method was better than HSVM and random sampling. In addition, the proposed method was found quite stable. termining parameters k and l .  X  X easonable X  numbers were set in the experiments. But, hyper-parameters could be set differently after pattern selection. A better generaliza-tion performance could have been obtained. Finally, a more extensive experiment in-volving large scale datasets is due. Pattern selection is most useful when a huge data-complexity. 
