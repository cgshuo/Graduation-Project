 Andrew Cotter cotter@ttic.edu Shai Shalev-Shwartz shais@cs.huji.ac.il John S. Cohen SL in CS, The Hebrew University of Jerusalem, Israel Nathan Srebro nati@ttic.edu An important aspect of kernel SVMs is that, despite being non-parametric, the learned predictor can be expressed in terms of only a subset of the training points, known as  X  X upport vectors X . The number of support vectors determines the memory footprint of the learned predictor, as well as the computational cost of using it. In order for SVMs to be practical in large scale applications, it is therefore important to have only a small number of support vectors. This is particularly true when, as is the case in many appli-cations, the training is done only once, on a powerful compute cluster that can handle large data, but the predictor then needs to be used many times, possibly in real time, perhaps on a small low-powered device. However, when training a SVM in the non-separable setting, all incorrectly classified points will be sup-port vectors X  X .g. with 10% error, the solution of the SVM empirical optimization problem will necessarily have at least 10% of the training points as support vectors. For data sets with many thousands or even millions of points, this results in very large predictors that are expensive to store and use. Even for some separable problems, the number of support vectors in the SVM solution (the margin-maximizing classi-fier) may increase linearly with the training set size (e.g. when all the points are on the margin). And so, even though minimizing the empirical SVM objective might be good in terms of generalization ability (and this is very well studied), as we argue here, it might be bad in terms of obtaining sparse solutions.
 In this paper, we ask how sparse a predictor we can ex-pect, and show how to learn a SVM predictor with an optimal guarantee on its sparsity, without increasing the required sample complexity nor (asymptotically) the training runtime, and for which the worst-case gen-eralization error has the same bound as the best known for (non-sparse) kernel SVM optimization. SVM predictors take the form x 7 X  sign( g w ( x )), where g w ( x ) =  X  w,  X  ( x )  X  is the (real-valued) prediction on example x ,  X ( x ) is a (possibly implicit) mapping into some Hilbert space H , and the predictor is specified by the vector w  X  H . SVM training amounts to finding an  X  X mpirically optimal X  predictor that minimizes a balance between its norm k w k and the training error, measured through the average hinge loss on the train-where ` hinge ( z ) = max { 0 , 1  X  z } = [1  X  z ] + and ( x 1 ,y 1 ) ,..., ( x n ,y n ) is the training set. The Repre-senter Theorem ensures that this predictor can be ex-pressed as w = P n i =1  X  i  X ( x i ). The training vectors x corresponding to non-zero  X  i are the support vectors . Here, we seek predictors w with  X  X parse X  representa-tions, i.e. with a small number of support vectors. We are mostly interested in Kernel SVMs, for which  X  is specified implicitly via a kernel function K ( x,x 0 ) = and analyze our method in terms of  X (  X  ), and then observe that it can be  X  X ernelized X , i.e. implemented in terms of K (  X  ,  X  ). To simplify the presentation, we assume without loss of generality that K ( x,x )  X  1. Using SVMs is theoretically justified as follows: suppose that there exists some (unknown) predic-tor u with low norm and low expected hinge loss L tation is with respect to some unknown distribution ( x,y )  X  D . Then the SVM predictor w , minimiz-ing the appropriate balance between norm and train-ing error, and based on enough training examples drawn i.i.d. from D , will have (with high probability) a small expected misclassification error rate L 0 / 1 ( g w specifically, the following number of examples: are enough to ensure that, with probability of at least 1  X   X  , the SVM predictor will satisfy L 0 / 1 ( g w L hinge ( g u ) + (this follows from Srebro et al. (2010) X  see Appendix E for details 1 ). We will show that one can learn a sparse predictor with essentially the same sample complexity. The question we consider here is whether, given that there exists a reference classifier u as in Section 2, it is possible to efficiently find a w based on a training sample which not only generalizes well, but also has a small support set.
 If the reference classifier u separates the data with a margin, namely L hinge ( g u ) = 0, then one can run the kernelized Perceptron algorithm (see for example Fre-und &amp; Schapire (1999)). The Perceptron processes the training examples one by one and adds a support vec-tor only when it makes a prediction mistake. There-fore, a bound on the number of prediction mistakes (i.e. a mistake bound) translates to a bound on the sparsity of the learned predictor. A classic result shows that if the data is separable with a margin 1 by some vector u , then the Perceptron will make at most k u k 2 prediction mistakes. Combining this with a generaliza-tion bound based on compression of representation (or with an online-to-batch conversion) we can conclude that with n  X   X  O ( k u k 2 / ), the generalization error of w will be at most . The non-separable case is more tricky. If we some-how obtained a vector v which makes a small number of margin violations on the training set, i.e. v = find a w with k v k 2 support vectors which satisfies  X  L 0 / 1 ( g w )  X  v by simply ignoring the examples on which y i  X  v,  X ( x i )  X  &lt; 1 and running the Perceptron on the remainder. Again using a compression bound, we can show that L 0 / 1 ( g w ) is little larger than  X  L 0 / 1 However, we cannot, in general, efficiently find a pre-dictor v with a low margin error rate, even if we know that such a predictor exists. Instead, in learn-ing SVMs, we minimize the empirical hinge loss. It is not clear how to relate the margin error rate to the hinge loss of u . One option is to note that 1 ( z &lt; 1)  X  2[1  X  z/ 2] + , hence v  X  2  X  L hinge g v/ 2 Since 2  X  L hinge g v/ 2 is a convex function of v , we can minimize it as a convex surrogate to v . Unfortu-nately, this approach would lead to a dependence on the quantity 2 L hinge g u/ 2 , which might be signifi-cantly larger than L hinge ( g u ). This is the main issue we address in Section 4, in which we show how to con-struct an efficient sparsification procedure which de-pends on L hinge ( g u ) and has the same error guarantees and sample complexity as the vanilla SVM predictor. Before moving on, let us ask whether we could hope for sparsity less then  X ( k u k 2 ). As the following Lemma establishes, we cannot: Lemma 3.1. Let R, L  X  ,  X  0 be given, with L  X  +  X  1 / 4 and with R 2 being an integer. There exists a data distribution D and a reference vector u such that k u k = R , L hinge ( g u ) = L  X  , and any w which satisfies: must necessarily be supported on at least R 2 / 2 vec-tors. 2 Proof. The idea of the proof is to construct D so that the distribution over the instances is uniform over some orthonormal basis of R R 2 . Then, any w which is supported on less than R 2 / 2 vectors must have g w ( x ) = 0 with probability of at least 1 / 2. For full details see Appendix A. In the previous section we showed that having a good low-norm predictor u often implies there exists also a good sparse predictor, but the existence proof required low margin error . We will now consider the problem of efficiently finding a good sparse predictor, given the existence of low-norm reference predictor u which suf-fers low hinge loss on a finite training sample. Our basic approach will be broadly similar to that of Section 3, but instead of relying on an unknown u , we will start by using any SVM optimization approach to learn a (possibly dense) w . We will then learn a sparse classifier  X  w which mimics w .
 In Section 3 we removed margin violations and dealt with an essentially separable problem. But when one considers not margin error, but hinge loss, the differ-ence between  X  X orrect X  and  X  X rong X  is more nuanced, and we must take into account the numeric value of the loss: 1. If y  X  w,  X ( x )  X   X  0 (i.e. w is wrong), then we can 2. If 0 &lt; y  X  w,  X ( x )  X  &lt; 1 (i.e. w is correct, but there 3. If y  X  w,  X ( x )  X   X  1 (i.e. w is correct and classifies These are equivalent to finding a solution with value at most 1 / 2 to the following optimization problem: We will show that a randomized classifier based on a solution to Problem 4.1 with f (  X  w )  X  1 / 2 has empirical 0/1 error bounded by the empirical hinge loss of w ; that we can efficiently find such solution based on at most 4 k w k 2 support vectors; and that such a sparse solution generalizes as well as w itself. 4.1. The slant-loss The key to our approach is our use of the random-Z  X  Unif [  X  1 / 2 , 1 / 2 ], rather than the standard linear classification rule g w defined in Section 2. The effect of the randomization is to  X  X pread out X  the expected loss of the classifier. We define the loss function: which we call the  X  X lant-loss X  (Figure 4.1), using L and empirical average, analogously to the 0/1 and hinge losses. It is easy to see that E Z ` 0 / 1 (  X  g ` slant-loss of g  X  w . Equally importantly, ` slant ( z  X  1 / ` hinge ( z ), from which the following Lemma follows: Lemma 4.1. For any w , and any  X  w for which Problem 4.1 has value f (  X  w )  X  1 / 2 , we have that Proof. It remains only to establish that  X  L slant ( g  X  w  X  L hinge ( g w ). For every x i ,y i , consider the following three cases: 1. If y i  X  w,  X ( x i )  X   X  0, then ` slant ( y i g  X  w 2. If 0 &lt; y i  X  w,  X ( x i )  X  &lt; 1, then ` slant ( y 3. If y i  X  w,  X ( x i )  X   X  1, then ` slant ( y i g  X  w completes the proof. 4.2. Finding sparse solutions To find a sparse  X  w with value f (  X  w )  X  1 / 2, we apply subgradient descent to Problem 4.1. The algorithm is extremely straightforward to understand and imple-ment. We initialize  X  w (0) = 0, and then proceed itera-tively, performing the following at the t th iteration: 1. Find the training index i t : y i t  X  w,  X ( x i t )  X  &gt; 0 2. Take the subgradient step  X  w ( t )  X   X  w ( t  X  1) The convergence rate of this algorithm is characterized in the following lemma: Lemma 4.2. After T  X  4 k w k 2 iterations of sub-gradient descent, we obtain a solution of the form Proof. First note that f ( w )  X  0. Relying on this pos-sible solution w , the Lemma follows from standard convergence bounds of subgradient descent (see e.g. Section 1.2 of Nesterov (2009)): with the step size  X  = , after performing k w k 2 / 2 iterations, at least one iterate  X  w ( t ) will have an objective function value no greater than . Choosing = 1 / 2 gives the desired result.
 Because each iteration adds at most one new element to the support set, the support size of the solution will likewise be bounded by 4 k w k 2 . 4.3. Generalization guarantee The fact that the optimization procedure outlined in the previous section results in a sparse predictor of a particularly simple structure enables us to bound its generalization error using a compression bound: Lemma 4.3. With probability at least 1  X   X  over the training set, for all  X  w of the form  X  w = training examples, we have: provided that n  X  8 T .
 Proof. This follows directly from compression bounds, e.g. Theorem B.1 (included in Appendix B for com-pleteness), since  X  w is a fixed function (namely a sum) of at most T sample points.
 Instead of using a compression bound, it is also pos-sible to use uniform concentration arguments to ob-tain an almost identical guarantee (up to log factors) which holds for any  X  w (not necessarily sparse) with norm k  X  w k X k w k and value f (  X  w )  X  1 / 3 (see Appendix C). This could be used to justify other optimization approaches to Problem 4.1. However, for the method presented, the compression bound suffices. 4.4. Putting it together Now that all of the pieces are in place, we can state our final procedure, start-to-finish: 1. Train a SVM to obtain w with norm k w k  X  2. Run subgradient descent on Problem 4.1 until we 3. Predict using  X  g  X  w .
 Theorem 4.4. For an (unknown) u , with probability at least 1  X  2  X  over a training set of size: the procedure above finds a predictor  X  w supported on at most O ( k u k 2 ) training vectors and error L 0 / 1 (  X  g L Proof. First, note that with the specified sample size, applying Bernstein X  X  inequality to the fixed predictor u , we have that with probability at least 1  X   X  , Combining Equation 4.3 with the SVM training goal (Step 1) and Lemma 4.1, we have that  X  L slant ( g  X  w )  X  L hinge ( u ) + O ( ). Following Lemma 4.2 we can ap-ply Lemma 4.3 with T = O ( k u k 2 ), and plugging in the specified sample complexity, we have L slant ( g  X  w  X  L slant ( g  X  w ). Combining the two inequalities, and re-calling that the slant-loss of g  X  w is the same as the expected 0 / 1 error of  X  g  X  w , we obtain L 0 / 1 L hinge ( g u ) + O ( ). Lemma 4.2 also establishes the desired bound on the number of support vectors. The procedure is efficient, and aside from initial SVM optimization, requires at most O ( k u k 2 ) iterations. 4.5. Kernelization To this point, we have worked in the explicit kernel Hilbert space H , even though we are interested primar-ily in the kernelized case, where H and  X (  X  ) are spec-ified only implicitly through K ( x,x 0 ) =  X   X ( x ) ,  X ( x We now show how our procedure can be  X  X ernelized X  and implemented using only K (  X  ,  X  ).
 As is typical, we shall rely on the representations ing track of the coefficients  X  and  X   X  . We will also maintain an up-to-date vector of  X  X esponses X   X  c : Notice that the values of these responses provide suf-ficient knowledge to find the update index i at every iteration. We can then perform the subgradient de-scent update  X  w  X   X  w +  X y i  X ( x i ) by adding  X  to  X   X  updating the responses as  X  c j  X   X  c j +  X y i y j K ( x a total cost of n kernel evaluations. These kernel eval-uations dominate the computational cost of our gra-dient descent procedure (all other operations can be performed in O ( n ) per iteration). As is standard for kernel SVM training, we will therefore analyze runtime in terms of the number of kernel evaluations required. With O ( k u k 2 ) iterations, and O ( n ) kernel evaluations per iteration, the overall number of required kernel evaluations for the gradient descent procedure is (ig-noring the  X  dependence): This is less then the best known runtime bound for ker-nel SVM optimization, so we do not expect the sparsi-fication step to be computationally dominant (i.e. it is in a sense  X  X ree X ). In order to complete the picture and understand the entire runtime of our method, we must also consider the runtime of the SVM training (Step 1). The best kernelized SVM optimization guarantee of which we are aware is achieved by the Stochastic Batch Perceptron (SBP, Cotter et al. (2012a)). Us-ing the SBP, we can find w with k w k  X  2 k u k and  X  L kernel evaluations, yielding (with the  X  -dependence): Corollary 1. If using the SBP for Step 1 and the sample size required by Theorem 4.4, the procedure in Section 4.4 can be performed with: kernel evaluations.
 Because the SBP finds a w with k w k X  2 k u k , and our subgradient descent algorithm finds a  X  w supported on 4 k w k 2 training vectors, it follows that the support size of  X  w is bounded by 16 k u k 2 . The runtime of Step 2 (the sparsification procedure) is asymptotically negligible compared to Step 1 (initial SVM training), so the over-all runtime is the same as for stand-alone SBP. Overall, our procedure finds an optimally sparse SVM predic-tor, and at the same time matches the best known sample and runtime complexity guarantees for SVM learning (up to small constant factors). 4.6. Unregularized bias Frequently, SVM problems contain an unregularized bias term X  X ather than the classification function be-g w,b ( x ) = (  X  w,  X ( x )  X  + b ) for a weight vector w and a bias b , where the bias is unconstrained, being permit-ted to take on the value of any real number.
 When optimizing SVMs, the inclusion of an unregu-larized bias introduces some additional complications which typically require special treatment. Our sub-gradient descent procedure, however, is essentially un-changed by the inclusion of a bias (although the SVM solver which we use to find w and b must account for it). Indeed, we need only redefine: in Problem 4.1, and then find  X  w as usual. The resulting sparse classifier is parameterized by  X  w and b , with b being that of the initial SVM solution. The tendency of SVM training algorithms to find solu-tions with large numbers of support vectors has been recognized as a shortcoming of SVMs since their in-troduction, and many approaches for finding solutions with smaller support sizes have been proposed, of vary-ing levels of complexity and effectiveness.
 We group these approaches into two categories: those which, like ours, start with a non-sparse solution to the SVM problem, and then find a sparse approximation; and those which either modify the SVM objective so as to result in sparse solutions, or optimize it using an algorithm specifically designed to maximize sparsity. In this section, we will discuss previous work of both of these types. None of these algorithms have perfor-mance guarantees which can be compared to that of Theorem 4.4, so we will also discuss some algorithms which do not optimize the SVM objective (even ap-proximately), but do find sparse solutions, and for some of which generalization bounds have been proven. In section 7, we also report on empirical comparisons to some of the methods discussed here. 5.1. Post-hoc approximation approaches One of the earliest proposed methods for finding sparse SVM solutions was that of Osuna &amp; Girosi (1998), who suggest that one first solve the kernel SVM op-timization problem to find w , and then, as a post-processing step, find a sparse approximation  X  w using support vector regression (SVR), minimizing the aver-age -insensitive loss, plus a regularization penalty: Optimizing this problem results in a  X  w for which the numerical values of  X  w,  X ( x )  X  and  X   X  w,  X ( x )  X  must be similar, even for examples which w misclassifies. This is an unnecessarily stringent condition X  X ecause the underlying problem is one of classification, we need only find a solution which gives roughly the same classifications as w , without necessarily matching the value of the classification function g w . It is in this re-spect that our objective function, Problem 4.1, differs. Osuna and Girosi X  X  work was later used as a key com-ponent of the work of Zhan &amp; Shen (2005), who first solve the SVM optimization problem, and then exclude a large number of support vectors from the training set based on a  X  X urvature heuristic X . They then retrain the SVM on this new, smaller, training set, and finally apply the technique of Osuna and Girosi to the result. 5.2. Alternative optimization strategies Another early method for finding sparse approximate SVM solutions is RSVM (Lee &amp; Mangasarian, 2001), which randomly samples a subset of the training set, and then searches for a solution supported only on this sample, minimizing the loss on the entire training set. So-called  X  X educed set X  methods (Burges &amp; Sch  X olkopf, 1997; Wu et al., 2005) address the problem of large support sizes by removing the constraint that the SVM solution be supported on the training set. Instead it is now supported on a set of  X  X irtual training vectors X  z ,...,z k with k n , while having the same form as the standard SVM solution: sign P k i =1  X  i y i K ( x,z i One must optimize over not just the coefficients  X  but also the virtual training vectors z i . Because the support set is not a subset of the training set, our lower bound (Lemma 3.1) does not apply. However, the resulting optimization problem is non-convex, and is therefore difficult to optimize.
 More recently, techniques such as those of Joachims &amp; Yu (2009) and Nguyen et al. (2010) have been devel-oped which, rather than explicitly including the search for good virtual training vectors in the objective func-tion, instead find such vectors heuristically during op-timization. These approaches have the significant ad-vantage of not explicitly relying on the optimization of a non-convex problem, although in a sense this dif-ficulty is being  X  X wept under the rug X  through the use of heuristics.
 Another approach is that of Keerthi et al. (2006), who optimize the standard SVM objective function while explicitly keeping track of a support set S . At each it-eration, they perform a greedy search over all training vectors x i /  X  S , finding the x i such that the optimal solution supported on S  X  X  x i } is best. They then add x i to S , and repeat. This is another extremely well-performing algorithm, but while the authors propose a clever method for improving the computational cost of their approach, it appears that it is still too compu-tationally expensive to be used on very large datasets. 5.3. Non-SVM algorithms Collobert et al. (2006) modify the SVM objective to minimize not the convex hinge loss, but rather the non-convex  X  X amp loss X , which differs from our slant-loss only in that the ramp covers the range [  X  1 , 1] instead of [  X  1 / 2 , 1 / 2 ]. Because the resulting objective function is non-convex, it is difficult to find a global optimum, but the experiments of Collobert et al. (2006) show that local optima achieve essentially the same perfor-mance with smaller support sizes than solutions found by  X  X tandard X  SVM optimization.
 Another approach for learning kernel-based classifiers is to use online learning algorithms such as the Percep-tron (e.g. Freund &amp; Schapire (1999)). The Perceptron processes the training example one by one and adds a support vector only when it makes a prediction mis-take. Therefore, a bound on the number of prediction mistakes (i.e. a mistake bound) translates to a bound on the sparsity of the learned predictor.
 As was discussed in Section 3, if the data are separable with margin 1 by some vector u , then the Perceptron can find a very sparse predictor with low error. How-ever, in the non-separable case, the Perceptron might make a number of mistakes that grows linearly with the size of the training sample.
 To address this, online learning algorithms for which the support size is bounded by a budget parameter have been proposed. Notable examples include the Forgetron (Dekel et al., 2005) and the Randomized Budget Perceptron (RBP, Cavallanti et al. (2007)). Such algorithms discard support vectors when their number exceeds the budget parameter X  X or example, the RBP discards an example chosen uniformly at ran-dom from the set of support vectors, whenever needed. Both of these algorithms have been analyzed, but the resulting mistake bounds are inferior to that of the Perceptron, leading to worse generalization bounds than the one we achieve for our proposed procedure, for the same support size. For example, the general-ization bound of the Forgetron is at least 4  X  L hinge ( g The bound of the RBP is more involved, but it is pos-sible to show that in order to obtain a support size of 16 k u k 2 , the generalization bound would depend on obtain only depends on  X  L hinge ( g u ). While the algorithm described in Section 4 gives asymptotically optimal theoretical performance, slight variations of it give better empirical performance. The analysis of Theorem 4.4 bounds the performance of the randomized classifier  X  g  X  w , but we have found that randomization hurts performance in practice, and that one is better off predicting using sign( g  X  w ). Our tech-nique relies on finding an approximate solution  X  w to Problem 4.1 with f (  X  w )  X  1 / 2 , but this 1 / 2 threshold is a relic of our use of randomization. Since randomiza-tion does not help in practice, there is little reason to may achieve a superior sparsity/generalization trade-off at different levels of convergence, and with values of the step-size  X  other than the suggested value of 1 / 2 For this reason, we suggest experimenting with differ-ent values of these parameters, and choosing the best based on cross-validation.
 Another issue is the handling of an unregularized bias. In Section 4.6, we suggest taking the bias associated with  X  w to be the same as that associated with w . However, one could alternatively optimize a version of Problem 4.1 which learns a new bias  X  b along with  X  w . The resulting subgradient descent algorithm (see Appendix D) is slightly more complicated, but its use may result in a small boost in performance. 6.1. Aggressive variant A more substantial deviation from our basic algorithm is to try to be more aggressive about maintaining spar-sity by re-using existing support vectors when optimiz-ing Problem 4.1. This can be done in the the following way: at each iteration, check if there is a support vec-tor (i.e. a training point already added to the support set) for which h i  X  X   X  w,  X ( x i )  X   X  (where is the ter-mination threshold, 1 / 2 in the analysis of Section 4). If there is such a support vector, increase its coefficient  X   X  X nly take a step on index i which is not currently in the support set if all current support vectors sat-isfy the constraint. This yields a potentially sparser solution at the cost of more iterations.
 Basing our experiments on recent comparisons be-tween sparse SVM optimizers (Keerthi et al., 2006; Nguyen et al., 2010), we compare our implementation 3 to the following methods 4 : 1. SpSVM (Keerthi et al., 2006), using Olivier 2. CPSP (Joachims &amp; Yu, 2009), using SVM-Perf. 3. Osuna &amp; Girosi X  X  algorithm (Osuna &amp; Girosi, 4. RSVM (Lee &amp; Mangasarian, 2001), using the LIB-5. CSVM (Nguyen et al., 2010). We did not perform Our comparison was performed on the datasets listed in Table 1. Adult and IJCNN are the  X  X 8a X  and  X  X jcnn1 X  datasets from LIBSVM Tools. Web and For-est are from the LibCVM Toolkit 6 . We also use a multiclass dataset 7 derived from the TIMIT speech corpus, on which we perform one-versus-rest classifica-tion, with class number 3 (the phoneme /k/ ) providing the  X  X ositive X  instances. Both Adult and TIMIT have relatively high error rates, making them more challeng-ing for sparse SVM solvers. Both our algorithm and that of Osuna &amp; Girosi require a reference classifier w , found using GTSVM (Cotter et al., 2011).
 We experimented with two versions of our algorithm, both incorporating the modifications of Section 6, dif-fering only in whether they include the aggressive variation. For the  X  X asic X  version, we tried  X  = { 4  X  4 , 4  X  3 ,..., 4 2 } , keeping track of the progress of the algorithm throughout the course of optimization. For each support size, we chose the best  X  based on a vali-dation set (half of the original test set) reporting errors on an independent test set (the other half). This was then averaged over 100 random test/validation splits. For the aggressive variant (Section 6.1), we experi-mented not only with multiple choices of  X  , but also termination thresholds  X  2  X  4 , 2  X  3 ,..., 1 , running until this threshold was satisfied. Optimization over the parameters was then performed using the same validation approach as for the  X  X asic X  algorithm. In our CPSP experiments, the target numbers of basis functions were taken to be powers of two. For Osuna &amp; Girosi X  X  algorithm, we took the SVR regularization pa-rameter  X  C to be that of Table 1 (i.e.  X  C = C ), and ex-perimented with  X  2  X  16 , 2  X  15 ,..., 2 4 . For RSVM, we tried subset ratios  X   X  2  X  16 , 2  X  15 ,..., 1  X  however, the implementation we used was unable to find a support set of size larger than 200, so many of the larger values of  X  returned duplicate results. The results are summarized in Figure 7. Our aggres-sive variant achieved a test error / support size tradeoff comparable to or better than the best competing al-gorithms, except on the Adult and TIMIT datasets, on the latter of which performance was fairly close to that of CPSP. On the Adult data set, the test errors (reported) are significantly higher then the validation errors, indicating our methods are suffering from pa-rameter overfitting due to too small a validation set (this is also true, to a lesser degree, on TIMIT). Note that SpSVM and CPSP, both of which perform very well, failed to find good solutions on the forest dataset within a reasonable timeframe, illustrating the benefits of the simplicity of our approach.
 To summarize, not only does our proposed method achieve optimal theoretical guarantees (the best possi-ble sparseness guarantee with the best known sample complexity and runtime for kernelized SVM learning), it is also computationally inexpensive, simple to im-plement, and performs well in practice.
 Acknowledgments: S. Shalev-Shwartz is supported Burges, C. and Sch  X olkopf, B. Improving the accuracy and speed of support vector machines. In NIPS X 97 , pp. 375 X 381. MIT Press, 1997.
 Cavallanti, G., Cesa-Bianchi, N., and Gentile, C.
Tracking the best hyperplane with a simple budget perceptron. Machine Learning , 69(2-3), December 2007.
 Chang, C-C. and Lin, C-J. LIBSVM: a library for support vector machines , 2001. Software available at http://www.csie.ntu.edu.tw/ ~ cjlin/libsvm . Collobert, Ronan, Sinz, Fabian, Weston, Jason, and Bottou, L  X eon. Trading convexity for scalability. In ICML X 06 , pp. 201 X 208, 2006.
 Cotter, A., Srebro, N., and Keshet, J. A GPU-tailored approach for training kernelized SVMs. In KDD X 11 , 2011.
 Cotter, A., Shalev-Schwartz, S., and Srebro, N. The kernelized stochastic batch perceptron. In ICML X 12 , 2012a.
 Cotter, A., Shalev-Schwartz, S., and Srebro, N. The kernelized stochastic batch perceptron. http:// arxiv.org/abs/1204.0566 , 2012b.
 Dekel, O., Shalev-Shwartz, S., and Singer, Y. The for-getron: A kernel-based perceptron on a fixed bud-get. In NIPS X 05 , pp. 259 X 266, 2005.
 Freund, Y. and Schapire, R. E. Large margin clas-sification using the perceptron algorithm. Machine Learning , 37(3):277 X 296, 1999.
 Joachims, T. and Yu, Chun-Nam. Sparse kernel svms via cutting-plane training. Machine Learning , 76(2 X  3):179 X 193, 2009. European Conference on Machine Learning (ECML) Special Issue.
 Keerthi, S. Sathiya, Chapelle, Olivier, and DeCoste,
Dennis. Building support vector machines with re-duced classifier complexity. Journal of Machine Learning Research , 7:1493 X 1515, 2006.
 Lee, Y-J. and Mangasarian, O. RSVM: Reduced sup-port vector machines. In Data Mining Institute,
Computer Sciences Department, University of Wis-consin , pp. 00 X 07, 2001.
 Lin, K-M. and Lin, C-J. A study on reduced support vector machines. IEEE Transactions on Neural Net-works , 2003.
 Nesterov, Y. Primal-dual subgradient methods for convex problems. Math. Program. , 120(1):221 X 259, Apr 2009.
 Nguyen, D D., Matsumoto, K., Takishima, Y., and
Hashimoto, K. Condensed vector machines: learn-ing fast machine for large data. Trans. Neur. Netw. , 21(12):1903 X 1914, Dec 2010.
 Osuna, E. and Girosi, F. Reducing the run-time com-plexity of support vector machines, 1998.
 Shalev-Shwartz, S. Introduction to machine learning, lecture notes. Technical report, The Hebrew Univer-sity, 2010. http://www.cs.huji.ac.il/ ~ shais/ Handouts2010.pdf .
 Srebro, N., Sridharan, K., and Tewari, A. Smoothness, low-noise and fast rates. In NIPS X 10 , 2010.
 Wu, M., Sch  X olkopf, B., and Bakir, G. Building sparse large margin classifiers. In ICML X 05 , pp. 996 X  1003, New York, NY, USA, 8 2005. Max-Planck-Gesellschaft, ACM.
 Zhan, Y. and Shen, D. Design efficient support vector machine for fast classification. Pattern Recognition ,
