 With data explosion in recent y ears, timely and cost-effect ive analytics over large scale data has been a hotspot of data management research [1]. Join operation is essential for many data analysis. For example, in log analytic processing, to get some useful statistics, an equal join is necessary between log table and reference tables [2].

However, data skew happens in many practical applications. The well-known 2/8 law demonstrates this phenomenon [3]. On the internet, 20% webpages bring 80% of the page view. Data skew will cause load imbalance and degrade system performance. The well-known hash join is vulnerable to data skew [4]. This paper propose a skew insensitive join algorithm for large-scale data analytics.
Traditional parallel RDBMS can hardly handle massive data [5]. Since first introduction [6], the MapReduce framework has become extremely popular due to its simplicity, shielding parallelization details and fault tolerance. Hadoop, the open-source version of MapReduce is a Magnetism, Agility, Depth(MAD) system [7] popular for big data analytics.

Hadoop provides map-side join and reduce-side join [2]. Map-side join, known as broadcast join in parallel RDBMS, only works when one table is small enough to fit in memory. Reduce-side join [8] works well in most situations, but it X  X  vulnerable to data skew and has high network transmission cost in shuffle phase.
There are two difficulties to solve data skew in joins. One is how to avoid load imbalance. Even range partitioning based join algorithms are difficult to guarantee load balance between reducers. The other is how to dynamically choose the best algorithm since sometimes users are not aware of data skew.
This paper proposes Adaptive Skew Insensitive(ASI) join to dynamically han-dle data skew in large scale data analytics. ASI join won X  X  cause any hot nodes. Based on the cost analysis, ASI join dynamically chooses the best algorithm for different inputs. ASI join works well on current MapReduce framework. Exper-iments compare the performance of state-of-the-art join algorithms. The results show that ASI join outperforms them in terms of data skewness and execution time.
 The rest of the paper is organized as follo ws. Section 2 discusses related work. Section 3 describes ASI join geography and cost model. Section 4 shows repre-sentative experimental results . Section 5 concludes the paper. In traditional parallel RDBMS, there has been in-depth research on skew re-sistant join [9]. However, join is not so well dealt with by MapReduce. Effi-ciently handling join operations has become a hotspot research. Existing work on MapReduce can be grouped into two categories: (1)Runtime monitoring al-gorithms, (2)Range partition based algorithms.

MapReduce handles skew using speculati ve execution [6]. This approach doesn X  X  handle data skew in joins, since the large tasks are not broken up.Some work adopt a runtime monitoring method [10]. Users can set a threshold of maximal data a reducer can process. Once a reducer recei ves more than the threshold, new arriv-ing data will be processed by a new reducer. Unlike them, our method has much lower network communication cost.

To address data skew, some join algorithms use range partitioning instead of hash partitioning to shuffle data from map to reduce phase [9][11][12]. The goal is to eliminate hot reducers. This method can be very time-consuming due to the prodigious network communication cost during shuffle phase. The performance greatly depends on the result of sampling and range partitioning, which can X  X  always guarantee that records of both inputs are evenly partitioned. Considering an equi-join L L.k = R.k R , the intuition behind this skew insensitive join algorithm is to deal with the skewed r ecords and non-skew ed records differ-ently. In this chapter we first state the problem, then illustrate the geography of ASI join, and propose cost analysis and dynamic execution process. 3.1 Problem Statement Considering an equi-join L R using MapReduce, L and R are too large to fit into memory, so only reduce-side join can be used. If L and R are highly skewed, there will be hot reducers or even out of memory errors. The left of Fig.1 is an example of reduce-side join. DataNode 1-3 represent the data distribution, value a is a popular key in L ,and b is a popular key in R . Reduce 1-3 show the workload using hash partition ( h ( k )= k mod 3 + 1). Reducer 2 and 3 become hot nodes and slow down the whole job.
 3.2 ASI Join Description Assume that skew in join datasets is de tected and the skewed value sets of L and R , represented as S L and S R are obtained by sampling, then ASI join will perform map-side join for skewed record s and reduce-side join for non-skewed records. The specific approach is describ ed as following: on each mapper we split R L i loc contains every record of L i with join value in S L and is kept locally. R rep contains the every record of R every record of R i with join value in S R . L i rep contains corresponding record map phase. L i hash and R i hash contain the rest records and are hash partitioned to reducers to be joined.

The right of Fig.1 shows the workload using ASI join under the same data distribution. Records with skewed values are joined in mappers, which greatly reduce the network communication cost between mappers and reducers. Non-skewed records are hash partitioned to r educers and joined there. Apparently there is no hot node and the whole system is more load-balancing.

When a popular value v appears in both S L and S R , we can X  X  broadcast them in both relations. To deal with overlapping skewed values, ASI join plan chooses to include v in only one of S L and S R . When the number of skewed records of L times the record size of L is larger than that of R , v will be included in S L so that the larger skewed records of L are kept locally.

When only L is skewed, S R is an empty set. L i and R i will be split into four while reduce-side join is applied to L i hash and R i hash .If R rep is too large to fit in memory, it will be read into memory several times. Algorithm 1 is the pseudo code of Skew Insensitive join. 3.3 Cost Analysis Generally, the CPU cost of join operation is spent on simple comparison, thus, system I/O and network cost dominate the total execution time. Therefore, we build a model based on the analysis of I/O and network cost.

MapReduce framework involves three stages. C map , C shuffle and C reduce rep-resent the time cost of map, shuffle and reduce. The number of reducers is r ; the number of nodes is n ; x is the percentage of skewed records in L ; y is the percentage of matching records in R ; | L | and | R | represent the size of L and R . The comparison of map join, reduce join and ASI join is listed in table 1. Sampling time will be explained in experiments. Algorithm 1. Skew Insensitive Join
Based on the analysis above, we have a dynamic execution process. If rela-tion L or R is small enough to fit in memory, map join task will be executed. Otherwise, ASI join will sample both inputs to detect data skew. If data skew is detected in one or both relations, Skew Insensitive join will be used to elimi-nate load imbalance. Reduce-side join will be chosen when join key is uniformly distributed.
 4.1 Testbed and Datasets All our experiments were run on a 24-nodes cluster. Each node has six Intel(R) Xeon(R) CPU E5645 2.40GHz processors with 16GB of DRAM(SDRAM) and one 500GB SATA disk. The nodes are connected to an HP ProCurve 2650 at a network bandwidth of 100BaseTx-FD. On each node Ubuntu 12.04 LTS, Hadoop 0.20.2, and Java 1.6 are installed. The block size is the default 64MB. The heap memory size is increased to 1024MB. The TPC-H benchmark [13] and following query are used in our experiments: select * from Customer C, Supplier S where C.Nationkey = S.Nationkey. There are only 25 unique uniform Nationkey in TPC-H. We increased unique Nationkey to 20000 to highlight skew experiments. To control skewness we ran-domly choose a portion of data and change Nationkey to one value. In this way it X  X  easy to understand exactly what experiment is being performed and cap-tures the essence of ZipFian distribution [9]. The following SQL will make the skewness of Customer 10%: update Customer C set C.Nationkey = 1 where random(1,100)  X  10.
 We respectively generate 1G, 10G, 100G records of CUSTOMER and SUP-PLIER and vary the skewness in one or both relations. We compare the perfor-mance of Reduce-Side join(RSJ), Hive X  X  skew join(HSJ), Pig X  X  skew join(PSJ) and our ASI join. 4.2 Experimental Results We first carry out an experiment to show the limitation of map-side join. Fig.3(a) shows the join time is infinite when the small relation gets 100M, which is because the nodes run out of memory while building an in-memory hash table for small relation. So we don X  X  consider map join in experimentation.
We also conduct an experiment to split the sampling time from the whole ASI join time. Fig.3(b) shows the ASI join time when joining two 1G, 10G, 100G datasets with 10% skewness. From which we can see that sampling time only accounts for about five percent of the whole execution time.

We now present the performance comparison of the four algorithms dealing with datasets of different size and skewness. Fig.4 shows the time cost by joining two 1G, 10G, 100G datasets with CUSTOMER having varying skewness. When there is no skew, ASI join takes extra samp ling time so reduce-side join is faster. When the skewness increases, the execution time of reduce-side join grows almost linearly because all skewed records are partitioned to one reducer while the execution time of ASI join nearly stays the same. Reduce-side join will run out of memory when the skewness exceeds 10% in Fig.4(b). ASI join can handle more severe skew, so is HSJ and PSJ.

Fig.5 shows the execution time joining with both datasets having varying skewness. ASI join and reduce-side join show the same performance relation-ship, while PSJ performs worse than Fig.4, which is because Pig X  X  skew join only sample the left input, thus it doesn X  X  handle very well when both inputs are skewed. Hive has optimized the perform ance by reducing the number of MapRe-duce jobs and only requires one job while Pig X  X  skewed join is implemented by three jobs, thus Hive has better performance than Pig. Joining 100G relations, PSJ and RSJ will fail when the skewness of both inputs are 5%, so there are only two starting points represent PSJ and RSJ. The performance of ASI join is not affected very much by larger datasets.
Fig.6 shows the task timelines of RSJ and ASI join on 10G datasets with 10% skewness in both inputs. ASI join spends more time on map phase since the skewed records are joined here. However, RSJ spends much more time on reduce phase due to load imbalance. Overall, ASI join costs much less time than RSJ.
Advantages of ASI join lie mainly in: first, ASI join keeps the large amount of skewed records locally and reduce n etwork communication cost; second, in reduce phase ASI join won X  X  cause any hot reducers. As shown above, these advantages become more obvious as the size and skewness of datasets increase. This paper proposes ASI join algorithm to dynamically handle data skew for large-scale data analytics. We keep the skewed records local and hash join the non-skewed records, which can be applied to any platform to handle skew join. Then we give the cost analysis, based on which ASI join can dynamically chooses the best algorithm for different inputs. ASI join works well on the cur-rent MapReduce framework and is of great reference to other high level query languages. Extensive experiment results have proved that ASI join is much more efficient at handling skewed data t han state-of-the-art methods.

