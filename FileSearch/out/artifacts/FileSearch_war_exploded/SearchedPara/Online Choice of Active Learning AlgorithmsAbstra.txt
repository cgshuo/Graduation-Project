 Yoram Baram baram@cs.technion.ac.il Ran El-Yaniv rani@cs.technion.ac.il Kobi Luz kobil@cs.technion.ac.il The goal in active learning is to design and analyze learning algorithms that can effectively filter or choose the samples for which they ask the teacher for a la-bel. The incentive in using active learning is mainly to expedite the learning process and reduce the label-ing efforts required by the teacher. While there is lack of theoretical understanding of active learning (in par-ticular, the generalization power of computationally practical active learning algorithms is not well under-stood), there are numerous empirical evidences show-ing that active learning can dramatically expedite the learning process.
 We focus on incremental pool-based active learning of classifiers, which can be viewed as the following game consisting of trials . The learner is presented with a fixed pool of unlabeled instances and on each trial the learner chooses one instance from the pool to be la-beled by the teacher. The teacher then provides the learner with the true label of this instance and the learner induces a (new) classifier based on all the la-beled samples seen so far and possibly based on un-labeled instances in the pool. Then a new trial be-gins, etc. Other variants of the active learning problem have been also considered. Two important variants are stream-based active learning (Freund et al., 1997) and learning with membership queries (Angluin, 1988). We do not consider these variants here.
 Numerous active learning algorithms have been pro-posed in the literature. Here we mention two algo-rithms, which appear to be among the best perform-ers, based on empirical studies. The first algorithm relies on kernel machines and was recently indepen-dently proposed by three research groups (Tong &amp; Koller, 2001; Schohn &amp; Cohn, 2000; Campbell et al., 2000). This algorithm, called simple in (Tong &amp; Koller, 2001), uses the current SVM classifier to query the instance closest to the decision hyperplane (in ker-nel space). The second algorithm we discuss, proposed by (Roy &amp; McCallum, 2001), is based on a differ-ent motivation: The algorithm chooses its next ex-ample to be labeled while attempting to reduce fu-ture generalization error probability. Since true future error rates are unknown, the learner attempts to es-timate them using a  X  X elf-confidence X  heuristic that utilized its current classifier for probability measure-ments. Throughout this paper we therefore call this algorithm self-conf .
 Seeking a top performing active learning algorithm we found that neither simple nor self-conf is a con-sistent winner across problems. Moreover, both algo-rithms exhibit a severe pitfall that seems to appear in learning problems with a  X  X OR-like X  structure (see Section 2). While perhaps no single active learning al-gorithm should be expected to consistently perform better than others on all problems, some problems clearly favor particular algorithms. This situation mo-tivates an online learning approach whereby one at-tempts to online utilize an ensemble of algorithms so as to achieve a performance, which is close to the best algorithm in hindsight. This scheme has been extensively studied in computational learning theory, mainly in the context of  X  X nline prediction using ex-pert advice X , see e.g. (Ceza-Bianchi et al., 1997). Our main contribution is an algorithm that actively learns by combining other active learners.
 A reasonable approach for combining an ensemble of active learning algorithms (or  X  X xperts X ) might be to evaluate their individual performance and dynamically switch to best performing expert so far. However, there are two obstacles for successfully implementing this scheme. First, standard classifier evaluation tech-niques such as cross-validation, leave-one-out or boot-strap tend to fail when used to estimate the perfor-mance of an active learner based on the labeled exam-ples chosen by the learner. The reason is that the set of labeled instanced selected by a good active learner tends to be acutely biased towards  X  X ard X  instances that do not reflect the true underlying distribution. In Section 3 we show an example of this phenomenon. Second, even if we overcome the first problem, each time we choose to utilize a certain expert we only get to see the label of the example chosen by this expert and do not get to observe the consequence of choices corresponding to other experts.
 We overcome these two obstacles using the following two ideas: Instead of using standard statistical tech-niques such as cross-validation we use a novel maxi-mum entropy semi-supervised criterion, which utilizes the pool of unlabeled samples and can faithfully evalu-ate the relative progress of the various experts; second, we cast our problem as an instance of the multi-armed bandit problem, where each expert corresponds to one slot machine and on each trial we are allowed to play one machine (i.e. choose one active learning algorithm to generate the next query). We then utilize a known online multi-armed bandit algorithm of (Auer et al., 2002). This algorithm enjoys strong performance guar-antees without any statistical assumptions.
 We present an extensive empirical study of our new active learning meta algorithm and compare its per-formance to its ensemble members consisting of three algorithms: simple , self-conf and a novel active learning heuristic based on  X  X urthest-first traversals X  (Hochbaum &amp; Shmoys, 1985). The resulting meta al-gorithm is shown to consistently perform almost as well as the best algorithm in the ensemble and on some problems it outperforms the best algorithm. In this section we first define more formally  X  X ool-based active learning X  and then briefly describe two known and one novel active learning algorithms. These algo-rithms form the ensemble on which our new  X  X aster X  algorithm is later applied. The known algorithms we selected are simple (Tong &amp; Koller, 2001; Schohn &amp; Cohn, 2000; Campbell et al., 2000) and the algorithm of (Roy &amp; McCallum, 2001), called here self-conf . The reasons we selected these algorithms are that they are both reasonably well motivated and they achieve high performance on real-world data (and beat vari-ous other known algorithms). Nevertheless, both these algorithms are inefficient in learning problems with XOR-like structure. The third algorithm is novel. While this algorithm exhibits quite poor performance on problems with simple structure it particularly ex-cels on XOR-like problems.
 Pool-based active learning. We consider a bi-nary classification problem and are given a pool U = { x 1 , . . . , x m } of unlabeled instances where each x i is a vector in some Euclidean space. Instances are as-sumed to be i.i.d. distributed according to some un-known fixed distribution P ( x ). Each instance x i has a label y i  X  X  (where in our case Y = { X  1 } ) distributed according to some unknown conditional P ( y | x ). At each stage let L be the set of labeled instances al-ready known to the learner. 1 An active learner con-sists of a classifier learning algorithm, which has been trained on L (and possibly on U ) and a querying func-tion Q : L X U  X  U . On each trial the active learner first applies Q to choose one unlabled instance x from U . The label y of x is then revealed and the pair ( x, y ) is added to L and x is removed from U . Then the learner induces a new classifier using L as a training set and a new trial begins, etc.
 Measuring active learning performance. To the best of our knowledge, in the literature there is no consensus on appropriate performance measures for active learning. We propose the following natural per-formance measure, which aims to quantify the  X  X effi-ciency X  of the querying function, while using a particu-lar inductive learning algorithm alg . Fix a particular classification problem. Let U be a random pool of n instances. For each 1  X  t  X  n let Acc t ( alg ) be the true average accuracy achievable by alg using a train-ing set of size t that is randomly and uniformly chosen from U . Let active be an active learning algorithm that uses alg as its inductive learning component. De-fine Acc t ( active ) to be the average accuracy achieved by active after t active learning trials starting with the pool U . Then, the defficiency of active is defined to be Deff n ( active ) = This measure captures the  X  X lobal X  performance of an active learner throughout the learning session. No-tice that the numerator is simply the area between the  X  X aximal X  achievable accuracy Acc n ( alg ) using tive learning algorithm. The denominator is the area between the same maximal accuracy and the learn-ing curve of the  X  X assive X  algorithm. The purpose of the denominator is to normalize the measure so as to be  X  X roblem independent X . Thus, this measure is always non-negative and smaller values in [0 , 1) in-dicate more efficient active learning. Deff n ( active ) has the desired property that if n is sufficiently large so that Acc n ( alg ) (almost) achieves the maximal ac-curacy (for this classifier), then for any n 0 &gt; n , Deff n 0 ( active ) = Deff n ( active ).
 Algorithm simple . This algorithm uses an SVM as its induction component. The querying function of simple at trial t uses the already induced classifier C t  X  1 to choose an unlabled instance, which is closest to the decision boundary of C t  X  1 . Following (Tong &amp; Koller, 2001) in our applications of SVMs through-out this paper we use the  X  X ernel trick X  discussed in (Shaw-Taylor &amp; Christianini, 2002), which guarantees linear separability of the training set in feature space. Algorithm self-conf . At the start of each trial this algorithm already holds a trained probabilistic (soft) classifier given by  X  P ( y | x ). For each x  X  U and y  X  Y the algorithm trains a new classifier  X  P 0 over L 0 ( x, y ) = L X  X  ( x, y ) } and estimates the result-ing  X  X elf-estimated expected log-loss X  defined to be E (  X  Then, for each x  X  U it calculates the self-estimated average expected loss x with the lowest expected loss is then chosen to be queried. The original self-conf algorithm uses Naive Bayes probability estimates. In the experiments described below we implement self-conf using soft (confidence rated) SVMs. Probabilistic estimates are obtained in a standard way using the logistic regression transform. The algorithm as presented is extremely inefficient. Various optimizations and approximations are proposed in (Roy &amp; McCallum, 2001) to make its running time practically feasible. From all these meth-ods we only use random sub-sampling in our imple-mentation of the algorithm: on each trial we estimate E (  X  Algorithm Kernel Farthest-First (KFF) We pro-pose a simple active learning heuristic based on  X  X arthest-first X  traversal sequences in kernel space. Farthest-first (FF) sequences have been previously used for computing provably approximately opti-mal clustering for k -center problems (Hochbaum &amp; Shmoys, 1985). The FF querying function is defined as follows: Given the current set L of labeled instanced, we choose as our next query an instance x  X  X  , which is farthest from L (where the distance of a point from a set is defined to be the minimum distance to a point in the set). Using L X  X  ( x, y ) } as a training set we then induce a classifier. This heuristic has a nice intuitive appeal in our context: The next instance to query is the farthest (and in some sense the most dissimilar) in-stance in the pool from those we have already learned. Unlike simple (and other algorithms) whose querying function is based on the classifier, the above FF query-ing function can be applied with any classifier learning algorithm. We apply it with an SVM and compute dis-tances (for the FF traversals) in kernel space. Example. Figure 1 shows the learning curves of simple , self-conf , kff and of a random sam-pling  X  X ctive learning X  (which we call rand ) 4 for the  X  X OR X  problem of Figure 1(Lower right panel). Notice that in this case, the overall performance of simple and self-conf is significantly worse than that of random sampling (the deficiency of these algorithms is given in Table 2). On the other hand, kff clearly shows that active learning can expedite learning also in this problem. This weakness of both simple and self-conf is typical in many problems with XOR-like structure. However, despite the apparent advantage of kff over simple and self-conf in XOR problems, we later show a number of examples where kff is con-siderably weaker than these algorithms and even than random sampling. The main advantage in considering kff is its use in an ensemble of active learning algo-rithms. Our master algorithm utilizes kff to benefit in XOR-like problems without significant compromises in problems where kff is weaker. In this section we describe an online algorithm for com-bining active learners. The combination algorithm, called here for short comb , is based on a known com-petitive algorithm for the multi-armed bandit problem and on a novel model selection criterion. After describ-ing these two components we present algorithm comb . The Multi-Armed Bandit (MAB) Problem. In this problem a gambler must choose one of n non-identical slot machines to play in a sequence of trials. Each machine can yield rewards whose distribution is unknown to the gambler, and the gambler X  X  goal is to maximize his total reward over a sequence of trials. This classical problem is one of the most basic prob-lems whose essence is the tradeoff between exploration and exploitation : Sticking to any single machine may prevent discovering a better machine. On the other hand, continually seeking a better machine will pre-vent achieving the best possible total reward. We make the following straightforward analogy be-tween the problem of online combining an ensemble of active learners and the MAB problem. The n active learning algorithms in our ensemble are the n slot ma-chines. On each trial, choosing a query generated by one active learning algorithm corresponds to choosing one slot machine. The true (generalization) accuracy achieved (by the combined algorithm) using the aug-mented training set (which includes the newly queried data point) corresponds to the gain achieved by the chosen machine. Of course, this rough analogy does not immediately provide a solution for combining ac-tive learners. Later on we show how to fill in all the missing details.
 The adversarial MAB results of (Auer et al., 2002), provide MAB algorithms that are guaranteed to ex-tract a total gain close to that of the best slot machine (in hindsight) without any statistical assumptions. Two particular MAB algorithms from (Auer et al., 2002) are potentially useful for implementing online choice of active learners. The first algorithm, called exp3.1 , directly matches the above analogy. The sec-ond algorithm, called exp4 , appears to be more suit-able for our purposes. In particular, exp4 is designed to deal with a more sophisticated MAB game than the standard MAB game described above: The goal is to combine and utilize a number k of strategies or experts , each giving an advice on how to play n slot machines. On each trial t , each expert j , j = 1 , . . . , k , provides a weighting b j ( t ) = ( b j 1 ( t ) , . . . , b j n ( t )) with where b j i ( t ) represents the recommended probability of expert j for playing the i th machine, i = 1 , . . . , n , on trial t . Denoting the vector of rewards for the n machines, on trial t by g ( t ) = ( g 1 ( t ) , . . . , g g ( t ) is non-negative and bounded, the expected re-ward of expert j on trial t is b j ( t )  X  g ( t ). In the MAB game only one reward from g ( t ) is revealed to the online player after the player chooses one machine in trial t . For a game consisting of T trials define G ward of the best expert in hindsight. The goal of the online player in this game is to utilize the advice given by the experts so as to achieve reward as close as possi-ble to G max . Algorithm exp4 from (Auer et al., 2002) achieves this goal and this paper proves that the  X  X e-gret X  of exp4 , defined to be G max minus the expected reward of exp4 , is bounded above by O ( This regret bound holds for any number of trials T , provided that one of the experts in the ensemble is the  X  X niform expert X , which always provides the uniform recommendation vector for the n slot machines. To employ exp4 in our context we associate the k ex-perts with the ensemble of k active learning algorithms (note that for the bound to hold we must include rand in our pool of active learners, which corresponds to the  X  X niform expert X ). The slot machines are associated with the unlabeled instances in our pool U . This way, the expert advice vectors are probabilistic recommen-dations on instances of U . It is thus required that each algorithm in the ensemble will provide  X  X atings X  for the entire pool on each trial. In practice, the three algorithms we consider naturally provide such ratings: simple uses the kernel distance from the decision hy-perplane, self-conf by using expected loss and kff using the kernel distance from the training set. The main missing element is the definition of rewards. We deal with this issue in the next subsection. Classification Entropy Maximization. In order to utilize the above MAB algorithm in our context we need to receive after each trial the gain of the in-stance x that was chosen to be queried (corresponding to one slot machine). While the ultimate reward in our context is the expected true accuracy of the classifier resulting from training over L X  X  ( x, y ) } (where y is the label of x ), this quantity is not available to us. At the outset it may appear that standard error (or ac-curacy) estimation methods could be useful. However, this is not the case: Unless L is sufficiently large, stan-dard methods like cross-validation or leave-one-out fail to provide reliable true error estimates for an active learner X  X  performance. This fact, which was already pointed out by (Schohn &amp; Cohn, 2000), is a result of the biased sample acquired by the active learner: In order to progress quickly, the learner must focus on  X  X ard X  or more  X  X nformative X  samples. Consider Figure 2 (left). The figure shows leave-one-out (LOO) estimates of four (active) learning algorithms: simple , self-conf , kff and rand . For each training set size, each point on a curve is generated using the LOO esti-mate based on the currently available labled set L . On Figure 2 (middle) we see the  X  X rue accuracy X  of these algorithms as estimated using a test set. Not only that LOO severely fails to estimate the true accuracy, it even fails to order the algorithms according to their relative success as active learners. This unfortunate behavior is typical to LOO on most datasets and is suffered by other standard estimation techniques in-cluding cross-validation and bootstrap. Thus, these techniques cannot be reliably used for receiving feed-back on the (relative) progress of active learners. Instead we use the following semi-supervised estima-tor, which we call Classification Entropy Maximization (CEM) . We define the CEM score of a classifier with respect to an unlabeled set of points to be the binary entropy of the classification it induces on the unla-beled set. If C is a binary classifier giving values in { X  1 } , let C +1 ( U ) and C  X  1 ( U ) be the positively and negatively classified subsets of some unlabeled set U , respectively. Then, the CEM score of C is the binary entropy H ( | C +1 ( U ) | |U| ). Thus, the CEM score is larger if the classification of the pool is more balanced. Figure 2 (right) provides CEM curves for the three active learn-ers discussed above. Clearly, the CEM measure orders the algorithms in accordance with their true accuracy (middle). This behavior of CEM is typical in most of our empirical examinations and somewhat surpris-ingly, CEM succeeds to correctly evaluate performance even when the positive and negative priors are not bal-anced (see below). While formal connections between CEM and generalization are currently unknown the following informal discussion provides farther insights into CEM and attempts to characterize conditions for its effectiveness.
 A sequence of sets S 1 , S 2 , . . . is called an inclusion se-quence if S 1  X  S 2  X  ,  X  X  X  . Consider the inclusion se-quence of training sets generated by an active learner. Let S = { ( x 1 , y 1 ) , . . . , ( x m , y m ) } be any binary la-beled set of samples where one of the classes (either +1 or  X  1) is a majority and its empirical propor-tion is r (i.e. the size of the majority class over m is r ). Consider any classifier C giving the la-bel C ( x ) for x  X  S . We say that the classification S (with respect to S ) if the majority class in S is also a majority class in S C and its proportion is larger than or equal r . Let I = S 1  X   X  X  X   X  S T be an inclu-sion sequence of labeled samples. We say that a learn-ing algorithm alg is majority-biased (with respect to I ) if the classification of S T by each of the classifiers C , . . . , C T (induced by alg ) is majority-biased, where C i is induced by alg using S i as a training set. Our main (empirical) observation is that whenever the learning algorithm is majority biased with respect to the inclusion sequence of training sets (generated by the learner) CEM X  X  growth rate corresponds to the growth rate of the generalization accuracy in which case the CEM criterion can be used to online compare the performance of active learners. In other words, by comparing the growth of a learner X  X  CEM score (pool classification entropy) we can get a useful indication on the growth of its true accuracy. Consider Figure 3(left) depicting a synthetic  X  X ing X  example in which the posi-tive class (the  X  X ing X ) is significantly larger than that of the negative class (the five small clusters). In partic-ular the proportion of the positive class is r = 0 . 9. When running our three active learners (as well as rand ) on this dataset we observe that all four learners are majority biased. Specifically, undiscovered (nega-tive) small clusters are classified as positive (like the ring). Therefore, for a majority biased learner, bet-ter performance in this example corresponds to more quickly  X  X iscovering X  the 5 small clusters. Clearly by discovering the small clusters such a learner im-proves its true accuracy and simultaneously increases its CEM score.
 Figure 3(middle) compares the true accuracy of the learners as estimated on an independent test set. Fig-ure 3(right) compares the corresponding pool classi-fication entropies (CEM scores). The two graphs ex-hibit a striking correspondence. Clearly, this example shows that the CEM criterion (as measured by the pool proportion) correctly ranks the true accuracy of these four learners. To summarize, the CEM criterion should correctly rank active learners whose learning algorithms are majority biased. We note that SVMs tend to be majority biased (in particular, when applied with RBF kernels).
 Algorithm comb . On Figure 4 we provide a com-mented pseudocode of our master active learning al-gorithm, called comb . The algorithm utilizes an en-semble of active learning algorithms and online tracks the best algorithm in the ensemble. Many of the steps in this code are adapted from the MAB algo-rithm exp4 of (Auer et al., 2002) (in particular, steps 1,3,4,5,10,11). Due to space limitations we refer the reader to (Auer et al., 2002) for a detailed explanation (and the proof guarantee) of exp4 . Here we explain steps 2 and 9, which in our experience are essential for efficient active learning using our scheme (the remain-ing steps, 6, 7 and 8 are self-explanatory). In step 2, af-ter receiving (in step 1) the advice probability vectors from the active learners, we project the pool U over high probability candidate instances. This projection is controlled by the parameter  X  and an instance x in U remains in U e if at least one active learners assigns to x a probability mass greater than  X  . In all the exper-iments described below we used  X  = 0 . 05. 5 In step 9 we apply a utility function to the entropic reward cal-culated on step 8. The contribution of the last queried instance is measured by the change in the entropy. H t is the entropy of the last partition of U generated using the last queried instance in the training set. H t  X  1 is the entropy of the partition of the same pool generated without using the last queried instance. The outcome of the utility function is normalized to be within [0,1]. The reward bound parameter g max , used for setting an optimal value for  X  (step 3) can and is eliminated in all our experiments using a standard  X  X uess and double X  technique. In particular we operate algorithm comb in rounds r = 1 , 2 , . . . , where in round r we set the reward limit to be g r = ( n e ln k/ ( e  X  1))4 r and restart algorithm comb with g max = g r . The round continues until the maximal reward reached by one of the ensemble algorithms exceeds g r  X  n e / X  r . For more details, the reader is referred to algorithm exp3.1 in (Auer et al., 2002). We evaluated the performance of comb on the entire dataset collection of (R  X atsch, 1998), consisting of 13 binary classification problems extracted from the UCI repository. For almost all problems, this collection in-cludes 100 folds each consisting of a fixed 60%/40% training/test partition. The use of this set is partic-ularly useful as it allows for easier experiment repli-cations. We also added our artificial XOR problem characteristics of these datasets appear on Table 1: For each problem we provide the size, the dimension, the bias (proportion of largest class), the maximal accu-racy achieved using the entire pool and the (rounded up) number of instances required by the worst active learner in the ensemble to achieve this maximal accu-racy.
 Table 2 shows the average defficiency of comb and the various active learners in its ensemble for all the datasets. Recall the definition of defficiency of an ac-tive learner as given in Equation (1): Smaller values represent higher active learning efficiency. Each of evident, that none of the ensemble algorithms is con-sistently winning over all sets ( self-conf , simple and kff win in 7, 4 and 3 cases, respectively). Except for one case where kff is significantly better, it is an over-all loser (which is inferior to rand ). In 10 cases out of the 14 presented comb is either the winner (5 cases) or a close runner up (5 cases). In 3 cases ( X  X eart X ,  X  X hyroid X  and  X  X reast-Cancer X ) which are among the smallest datsets, comb is not the winner and not even the runner up, although even in these cases it is not far from the winner. This behavior is reasonable be-cause comb need sufficient  X  X ime X  for both exploration and exploitation. There is only one case, the  X  X ia-betis X  dataset, that demonstrates a problem where our entropy criterion fails in online choosing the best en-semble member. In this case, comb performed signif-icantly worse than both self-conf (the winner) and simple . On the other extreme, of particular interest are cases where comb is the winner. Figure 5 shows such a case and depicts the learning curves of comb and its ensemble members on the  X  X lare-Solar X  dataset. We presented an online algorithm that effectively combines an ensemble of active learners. The algo-rithm successfully utilizes elements from both statisti-cal learning and online (adversarial) learning. Exten-sive empirical results strongly indicate that our algo-rithm can track the best algorithm in the ensemble on real world problems. Quite surprisingly our algorithm can quite often outperform the best ensemble member. Some questions require further investigations. In our experience, the  X  X lassification entropy maximization (CEM) X  semi-supervised criterion for tracking active learning progress is essential and cannot be replaced by standard error estimation techniques. Further studies of this overly simple but effective criterion may be re-vealing. It would also be interesting to examine alter-native (semi-supervised) estimators. Farther improve-ments can be achieved by developing MAB bounds which depend on the game duration. Such bounds can help in controlling the tradeoff between exploration and exploitation when using very small datasets.
