 Restricted Boltzmann Machine (RBM) has shown great ef-fectiveness in document modeling. It utilizes hidden units to discover the latent topics and can learn compact semantic representations for documents which greatly facilitate doc-ument retrieval, clustering and classification. The popular-ity (or frequency) of topics in text corpora usually follow a power-law distribution where a few dominant topics occur very frequently while most topics (in the long-tail region) have low probabilities. Due to this imbalance, RBM tends to learn multiple redundant hidden units to best represent dominant topics and ignore those in the long-tail region, which renders the learned representations to be redundant and non-informative. To solve this problem, we propose Di-versified RBM (DRBM) which diversifies the hidden units, to make them cover not only the dominant topics, but also those in the long-tail region. We define a diversity metric and use it as a regularizer to encourage the hidden units to be diverse. Since the diversity metric is hard to optimize directly, we instead optimize its lower bound and prove that maximizing the lower bound with projected gradient ascent can increase this diversity metric. Experiments on docu-ment retrieval and clustering demonstrate that with diver-sification, the document modeling power of DRBM can be greatly improved.
 H.2.8 [ Database Management ]: Database Applications, Data Mining Algorithms, Experiments Diversified Restricted Boltzmann Machine, Diversity, Power-law Distribution, Document Modeling, Topic Modeling c  X 
Learning lower-dimensional semantic representations [14, 3, 13, 25, 18, 26, 8, 9, 11, 15, 19, 7] for documents is essen-tial for many tasks such as document retrieval, clustering and classification, to name a few. It is assumed that there exist a set of hidden topics underlying the observed words and each document has a representation vector in the la-tent topic space. In general, the latent topic space is able to capture more compact and semantically meaningful char-acteristics of the documents than the observed word space. Performing text mining and natural language understanding tasks on the learned semantic representations can yield bet-ter performance than on the words based representations.
Restricted Boltzmann Machine (RBM) [24, 13, 26] has been successfully used for document representation learn-ing. RBM is a two-layer undirected graphical model where one layer of hidden units are used to capture latent top-ics and one layer of visible units are used to represent ob-served words. The correlation between hidden units and visible units are modeled with undirected weighted edges. Each hidden unit is characterized by a weight vector (to be learned) where each component in the vector corresponds to a word in the vocabulary. Compared with directed topic models such as Probabilistic Latent Semantic Analysis (PLSA) [14] and Latent Dirichlet Allocation (LDA) [3], RBM is very efficient in inferring the latent representations of documents, which is crucial for applications having real-time require-ments such as retrieval. And RBM has been demonstrated to perform more accurately than LDA in document retrieval and classification [13].

In most (if not all) document collections, the popularity of topics usually follows a power-law distribution. A few domi-nant topics appear with very high frequency while most top-ics are of low probabilities. For example, in a news corpus, topics like politics , economics , sports occur a lot while top-ics like garden , furniture , poem are rarely mentioned. Due to this skewness of topic popularity, RBM tends to learn multiple redundant hidden units to cover dominant topics as best as possible and pay little attention to topics in the long-tail region. Failing to capture long-tail topics incurs significant information loss. Though the probability of each long-tail topic is low, the total probability mass of the long-tail region is large because the number of topics in this re-gion is large. These long-tail topics with such a large total probability mass are of equal importance with, if not more importance than, the few dominant topics. Besides, the la-tent representations learned by RBM are redundant in that many dimensions are actually of the same or similar meaning and the resultant high dimensionality incurs computational inefficiency.

In this paper, we aim to address this problem by propos-ing to enhance the diversity of the hidden units in RBM to make them cover as many topics as possible, not only the dominant ones, but also those in the long-tail region. To combat the phenomenon that many redundant hidden units are learned to characterize the dominant topics as best as possible with the price of ignoring long-tail topics, we impose a diversity regularizer over these hidden units to reduce their redundancy and improve their coverage of long-tail topics. We define a diversity metric to formally measure how diverse a set of hidden units are: given the weight vectors A where each column in A corresponds to one hidden unit, the diver-sity metric is defined as  X ( A ) =  X ( A )  X   X ( A ), where  X ( A ) is the mean of the angles between all pairs of weights vectors and  X ( A ) is the variance of these angles. A larger  X ( A ) in-dicates that the weight vectors in A are more diverse. This diversity metric is defined with the rationale that a set of weight vectors possess a larger diversity if the pairwise an-gles between them have a larger mean  X ( A ) and a smaller variance  X ( A ). A larger mean implies that these vectors share larger angles on the whole, hence are more different from each other. A smaller variance indicates that these vectors uniformly spread out to different directions and each vector is evenly different from all other vectors. We employ this diversity metric to regularize the learning of RBM to en-courage the hidden units to be diverse. Considering that the diversity metric is hard to optimize, we seek a lower bound of it which is much more amenable for optimization. We prove that maximizing the lower bound with projected gradient ascent can increase the diversity metric. Experiments on three datasets demonstrate that with diversification, RBM can learn much more powerful latent document representa-tions that boost the performance of document retrieval and clustering greatly.

The major contributions of this paper are:
The rest of the paper is organized as follows. In Section 2, we review related works. In Section 3, we propose the Diversified RBM (DRBM). Section 4 gives experimental re-sults and Section 5 concludes the paper.
Document modeling aims to discover the latent semantics underlying document corpora and learn representations for documents in the latent semantic space. Directed topic mod-els such as Probabilistic Latent Semantic Analysis (PLSA) [14] and Latent Dirichlet Allocation (LDA) [3] represent each topic with a multinomial distribution over the words in the vocabulary and assume each document has a multinomial distribution over topics. The observed words are generated from the latent topics in a hierarchical way. In directed topic models, inferring documents X  distributions over latent topics usually involve iterative optimization or sampling, which is time-consuming and limits the applicability of these models in real-time applications such as document retrieval.
Restricted Boltzmann Machine (RBM) [13, 26] has shown great success in document modeling. [13] proposed a Repli-cated Softmax RBM to model word counts. [26] introduced Deep Boltzmann Machine to extract distributed semantic representations for documents. Another paradigm of works are based on neural network (NN) [2, 25, 18, 20, 8, 9, 11, 15, 19, 7], which aim to learn word/document representa-tions that are able to capture the latent semantics. Com-pared with directed topic models [14, 3], RBM and NN based methods enable fast inference of the latent representations, which is key to real-time applications. Our work is built on top of the existing RBM methods, with a special focus on how to capture low-frequency topics/semantics. It is worth noting that the techniques developed in this paper can be also applied to NN based approaches.

There have been several works [21, 22, 27, 1] attempting to capture the power-law behavior of topic popularity, us-ing priors that encourage the distributions over topics to be long-tailed, such as Pitman-Yor Process prior [21, 22], asym-metric Dirichlet prior [27] and Indian Buffet Process com-pound Dirichlet Process prior [1]. These methods have no regularization over topics X  distributions over words. While these priors encourage new topics to be generated, the newly generated topics may be alike to the existing ones used to model dominant topics. In the end, the learned topics are still redundant and unable to capture the long-tail topics tent Dirichlet Allocation with the goal to learn diverse top-ics, by imposing a Determinantal Point Process (DPP) [16] prior over the topics-word multinomial vectors. The DPP prior allows one to specify a preference for diversity using a positive definite kernel function. In this paper, we study the diversification of RBM and propose an alternative diver-sity measure that has properties complementary to the DPP prior, such as (1) invariant to scaling of the weight vectors; (2) encouraging the weight vectors to evenly spread out. In this section, we propose to diversify the hidden units in RBM to reduce their redundancy and improve the coverage of long-tail topics.
Figure 1 shows the basic Restricted Boltzmann Machine (RBM). RBM [24] is an undirected graphical model consist-ing of a set of hidden units h = { h k } K k =1 and a set of visible units v = { v j } J j =1 . Both h k and v j are assumed to be bi-nary. Each hidden unit is connected with all visible units with undirected edges and there is no connection between two hidden units or two visible units. The energy function defined over h and v is E ( h , v ) =  X  ated with the visible and hidden units respectively. A are the weights on the edges connecting two set of units.  X  ,  X  and A are model parameters.
 To better model word counts, [13] proposed Replicated Softmax RBM. Let V be a D  X  J observed binary matrix of a document containing D tokens. J is the vocabulary size. Row i of V is the 1-of-J coding vector of the i th token in this document. V ij = 1 if the i th token is the j th word in the vocabulary. Under this representation, the energy function E ( h , V ) is defined as Given the observed tokens V , inferring the latent represen-tation h can be done very efficiently where  X  ( x ) = 1 / (1 + exp(  X  x )) is the logistic function. The model parameters can be learned by maximizing the data gence [12] method, which is essentially a gradient ascent approach. The first step towards diversifying Restricted Boltzmann Machine is to measure the diversity of the hidden units. In RBM, each hidden unit possesses a weight vector where the weights are associated with the edges connecting this hid-den unit and the visible units. These weight vectors are the parameters to be learned in RBM. Given the weight vectors A = [ a 1 ,  X  X  X  , a K ] of K hidden units, where each column in matrix A corresponds to one hidden unit, their diversity can be informally described as how different each vector is from others. While there are many ways to measure the difference between vectors a i and a j , we prefer to use the angle between them since the angle is invariant to transla-tion and scaling (with positive factors) of the two vectors. In addition, we do not care about the orientation of vec-tors, thus preferring the angle to be acute or right. If the angle  X  is obtuse, we replace it with  X   X   X  . To sum up, we define the angle  X  ( a i , a j ) between vector a i and a A , we define the diversity metric as  X ( A ) =  X ( A )  X   X ( A ), angles between all pairs of weights vectors and  X ( A ) = angles. A larger  X ( A ) indicates that the weight vectors in A are more diverse. Intuitively, the set of weight vectors pos-sess a larger diversity if the pairwise angles between them have a larger mean and a smaller variance. A larger mean implies that these vectors share larger angles on the whole, hence are more different from each other. A smaller variance indicates that these vectors uniformly spread out to different directions and each vector is evenly different from all other vectors. Encouraging the variance to be small can prevent the phenomenon that the vectors fall into several groups where vectors in the same group have small angles and vec-tors between groups have large angles. Such a phenomenon renders the vectors to be redundant and less diverse, and hence should be prohibited. Throughout the paper, we as-sume the weight vectors are linearly independent. Later we present a justification of the validity of this assumption.
To diversify the hidden units in RBM, we use the diver-sity metric described above to regularize the learning of the weight vectors and define a Diversified RBM (DRBM) prob-lem as where  X  &gt; 0 is a tradeoff parameter between the data likeli-hood and the diversity regularizer. The term  X   X ( A ) in the new objective function encourages the weight vectors in A to be diverse.  X  plays an important role in balancing the fitness of A to the data likelihood and its diversity. Under a small  X  , A is learned to best maximize the data likelihood and its diversity is ignored. As discussed earlier, such a A has high redundancy and may not be able to cover long-tail topics effectively. Under a large  X  , A is learned with high diversity, but may not be well fitted to the data likelihood and hence lose the capability to properly model documents. To sum up, a proper  X  needs to be chosen to achieve the optimal balance.
In this section, we study how to solve the problem ( P1 ) defined in Eq.(4). For the ease of optimization, we first reformulate this problem. Let A = diag( g ) e A , where g is a vector and g i denotes the L 2 norm of the i th column of A , then the L 2 norm of each column vector  X a in e A is 1. Based on the definition of the diversity metric, we have  X ( A ) =  X ( Accordingly, ( P1 ) can be reformulated as ( P2 ) max ( P2 ) can be solved by alternating between g and e A : opti-mizing g with e A fixed and optimizing e A with g fixed. With e A fixed, the problem defined over g is which can be efficiently solved with contrastive divergence (CD) based gradient ascent method. Fixing g , the problem defined over e A is non-smooth and non-convex, which is hard to solve. Now we focus on how to tackle it. As discussed ear-lier, the data likelihood of RBM is usually maximized with gradient ascent (GA) method. To be consistent, it is de-sirable to optimize the diversity regularizer with projected
Projection is needed due to the constraints in ( P2 ). gradient ascent (PGA) as well. Since  X ( e A ) is non-smooth and non-convex, (sub)gradient methods are not applicable. To solve this problem, we derive a smooth lower bound  X ( e of  X ( e A ) and require  X ( e A ) to have the following two traits: 1) the gradient of  X ( e A ) is easy to compute; 2) optimizing  X ( e
A ) with PGA can increase  X ( e A ). The lower bound is given in Lemma 1 2 .
 Lemma 1. Let det( e A T e A ) denote the determinant of the Gram matrix of e A , then 0 &lt; det( e A T e A )  X  1 . Let  X ( arcsin( q det( e A T e A ))  X  (  X  2  X  arcsin( q det( e A T global optimal.

Next we prove that maximizing  X ( e A ) using PGA can in-crease the diversity metric  X ( e A ). The statement is formally described in Theorem 1.
 Theorem 1. Let G ( t ) be the gradient of  X ( e A ) w.r.t e at iteration t .  X   X  &gt; 0 , such that  X   X   X  (0 , X  ) ,  X (  X ( e the projection to the unit sphere.
 Note that  X ( e A ) consists of two parts  X ( e A ) and  X ( e prove Theorem 1, we prove that maximizing  X ( e A ) using PGA can increase the mean  X ( e A ) and reduce the variance  X ( e A ) simultaneously, which are stated in Theorem 2 and 3. Theorem 2. Let G ( t ) be the gradient of  X ( e A ) w.r.t e at iteration t .  X   X  1 &gt; 0 , such that  X   X   X  (0 , X  1 ) ,  X (  X ( e
Theorem 3. Let G ( t ) be the gradient of  X ( e A T ) w.r.t at iteration t .  X   X  2 &gt; 0 , such that  X   X   X  (0 , X  2 ) ,  X (  X ( e
These two theorems immediately imply Theorem 1. To prove Theorem 2, the following lemma is needed.

Lemma 2. Let the weight vector  X a i of hidden unit i be decomposed into  X a i = x i + l i e i , where x i = P K lies in the subspace L spanned by {  X a 1 ,  X  X  X  ,  X a in the orthogonal complement of L , k e i k = 1 , e i l is a scalar. Then the gradient of  X ( e A ) w.r.t a where k i is a positive scalar.
 Given Lemma 2, we can justify why the weight vectors can be always linearly independent during the projected gradient ascent procedure. First, we assume they are initialized to be linearly independent. From Lemma 2, we know that the gradient direction of each weight vector a i is orthogonal to all other weight vectors. So moving along such a direction prevents each vector to fall into the span of all other vectors. And projecting the vectors onto the unit sphere does not change the directions of these vectors. Thereby, these weight vectors would be always linearly independent.
 The proofs of Lemma 2 and Theorem 2 are presented in Appendix B and C respectively. To prove Theorem 3, we need the following lemma:
The proof of Lemma 1 is given in Appendix A.
Lemma 3. Given a nondecreasing sequence b = ( b i ) n and a strictly decreasing function g ( x ) which satisfies 0  X  g ( b i )  X  min { b i +1  X  b i : i = 1 , 2 ,  X  X  X  ,n  X  1 ,b define a sequence c = ( c i ) n i =1 where c i = b i + g ( b b of a sequence. Furthermore, let n 0 = max { j : b j 6 = b we define a sequence b 0 = ( b 0 i ) n i =1 where b 0 i then var ( c )  X  var ( b 0 ) &lt; var ( b ) .
 The proofs of Lemma 3 and Theorem 3 are given in Ap-pendix D and E respectively.
In this section, we present experimental results on two tasks and on three datasets, which demonstrate that with diversification, DRBM can learn much more effective repre-sentations than RBM.
Three datasets were used in the experiments. The first one [5] is a subset of the Nist Topic Detection and Track-ing (TDT) corpus which contains 9394 documents from the largest 30 categories. 70% documents were used for training and 30% were used for testing. The second dataset is the 20 Newsgroups (20-News), which contains 18846 documents from 20 categories. 60% documents were used for training and 40% were used for testing. The third dataset [4] is the Reuters-21578 (Reuters) dataset. Categories with less than 100 documents were removed, which left us 9 categories and 7195 documents. 70% documents were used for training and 30% were used for testing. Each dataset used a vocabulary of 5000 words with the largest document frequency. Table 1 summarizes the statistics of three datasets.
 We used gradient methods to train RBM [13] and DRBM. The mini-batch size was set to 100. The learning rate was set to 1e-4. The number of gradient ascent iterations was set to 1000 and the number of Gibbs sampling iterations in contrastive divergence was fixed to 1. The tradeoff pa-rameter  X  in DRBM was tuned with 5-fold cross validation. We compared with the following baselines methods: (1) bag-of-words (BOW); (2) Latent Dirichlet Allocation (LDA) [3]; (3) LDA regularized with Determinantal Point Process prior (DPP-LDA) [17]; (4) Pitman-Yor Process Topic Model (PYTM) [22]; (5) Latent IBP Compound Dirichlet Allo-cation (LIDA) [1]; (6) Neural Autoregressive Topic Model (DocNADE) [18]; (7) Paragraph Vector (PV) [19]; (8) Re-stricted Boltzmann Machine [13]. The parameters in base-line methods were tuned using 5-fold cross validation.
In this section, we evaluate the effectiveness of the learned representations on retrieval. Precision@100 is used to eval-uate the retrieval performance. For each test document, we retrieve 100 documents from the training set that have the smallest Euclidean distance with the query document. The distance is computed on the learned representations. Precision@100 is defined as n/ 100, where n is the number of retrieved documents that share the same class label with the query document.

Table 2, 3 and 4 show the precision@100 under different number K of hidden units on TDT, 20-News and Reuters dataset respectively. As can be seen from these tables, DRBM with diversity regularization largely outperforms RBM which has no diversity regularization under various choices of K , which demonstrates that diversifying the hidden units can greatly improve the effectiveness of document representation learning. The improvement is especially significant when K is small. For example, on TDT dataset, under K = 25, DRBM improves the precision@100 from 11.2% to 78.4%. Under a small K , RBM allocates most (if not all) hidden units to cover dominant topics, thus long-tail topics have little chance to be modeled effectively. DRBM solves this problem by increasing diversity of these hidden units to en-force them to cover not only the dominant topics, but also the long-tail topics. Thereby, the learned representations are more effective in capturing the long-tail semantics and the retrieval performance is greatly improved. As K increases, the performance of RBM increases. This is because under a larger K , some hidden units can be spared to model topics in the long-tail region. In this case, enforcing diversity still im-proves performance, though the significance of improvement diminishes as K increases.

To further examine whether DRBM can effectively cap-ture the long-tail semantics, we show the precision@100 on each of the 9 categories in Reuters dataset in Table 5. The 2nd row shows the number of documents in each category. The distribution of document frequency is in a power-law fashion, where dominant categories (such as 1 and 2) have a lot of documents while most categories (called long-tail categories) have a small amount of documents. The 3rd and 4th row show the precision@100 achieved by RBM and Table 7: Clustering accuracy (%) on TDT test set DRBM on each category. The 5th row shows the relative im-provement of DRBM over RBM. The relative improvement precision@100 achieved by DRBM and RBM respectively. While DRBM improves RBM over all the categories, the improvements on long-tail categories are much more signif-icant than dominant categories. For example, the relative improvements on category 8 and 9 are 366% and 397% while the improvements are 31% and 81% on category 1 and 2. This indicates that DRBM can effectively capture the long-tail topics, thereby improve the representations learned for long-tail categories significantly.

One great merit of DRBM is that it can achieve notable performance under a small K , which is of key importance for fast retrieval. On the TDT dataset, DRBM can achieve a precision@100 of 78.4% with K = 25, which cannot be achieved by RBM even when K is raised to 500. This in-dicates that with DRBM, one can perform retrieval on low-dimensional representations, which is usually much easier than on high-dimensional representations. For example, in KD tree [10] based nearest neighbor search, while building a KD tree on feature vectors with hundreds of dimensions is extremely hard, feature vectors whose dimension is less than one hundred are much easier to handle.

Table 6 presents the comparison with the state of the art document representation learning methods. As can be seen from this table, our method achieves the best performances on the TDT and 20-News datasets and achieves the sec-ond best performance on the Reuters dataset. The bag-of-word (BOW) representation cannot capture the underly-ing semantics of documents, thus its performance is inferior. LDA, RBM and neural network based approaches including DocNADE [18] and PV [19] can represent documents into the latent topic space where document retrieval can be per-formed more accurately. However, they lack the mechanisms Table 8: Clustering accuracy (%) on 20-News test set Table 9: Clustering accuracy (%) on Reuters test set to cover long-tail topics, hence the resultant representations are less effective. PYTM [22] and LIDA [1] use power-law priors to encourage new topics to be generated, however, the newly generated topics may still be used to model the dominant semantics rather than those in the long-tail re-gion. DPP-LDA [17] uses a correlation kernel Determinantal Point Process (DPP) prior to diversify the topics in LDA. However, it does not improve LDA too much.
Another task we study is to do k-means clustering on the learned representations. In our experiments, the input clus-ter number of k-means was set to the ground truth number of categories in each dataset. In each run, k-means was re-peated 10 times with different random initializations and the solution with lowest loss value was returned. Following [6], we used accuracy to measure the clustering performance. Please refer to [6] for the definition of accuracy. Table 7, 8 and 9 show the clustering accuracy on TDT, 20-News and Reuters test set respectively under different number K of hidden units.
 As can be seen from these tables, with diversification, DRBM achieves significantly better clustering accuracy than the standard RBM. On TDT test data, the best accuracy of RBM is 23.3%. DRBM dramatically improves the ac-curacy to 52.4%. On Reuters test set, the best accuracy achieved by DRBM is 60.9%, which is largely better than the 47.6% accuracy achieved by RBM. The great perfor-mance gain achieved by DRBM attributes to the improved effectiveness of the learned representations. RBM lacks the ability to learn hidden units to cover long-tail topics, which largely inhibits its ability to learn rich and expressive rep-resentations. DRBM uses the diversity metric to regularize the hidden units to enhance their diversity. The learned hidden units under DRBM can not only represent dominant topics effectively, but also cover long-tail topics properly. Accordingly, the resultant representations can cover diverse semantics and dramatically improve the performance of k-means clustering.

DRBM can achieve a high accuracy with a fairly small number of hidden units, which greatly facilitates computa-tional efficiency. For example, on TDT dataset, with 25 hid-den units, DRBM can achieve an accuracy of 52.4%, which cannot be achieved by RBM with even 500 hidden units. The computational complexity of k-means is linear to the feature dimension. Thus, on this dataset, with the latent representations learned by DRBM, k-means can achieve a significant speed up. Similar observations can be seen from the other two datasets.
 Table 10: Clustering accuracy (%) on three datasets
Table 10 presents the comparison of DRBM with the base-line methods on clustering accuracy. As can be seen from this table, our method consistently outperforms the base-lines across all three datasets. The analysis of why DRBM is better than the baseline methods follows that presented in Section 4.2.
Following [13], we computed perplexity on the held out test set to assess the document modeling power of RBM and DRBM. Table 11, 12 and 13 compare the perplexity of RBM and DRBM computed on TDT, 20-News and Reuters dataset respectively. As can be seen from these tables, DRBM can achieve significant lower (better) perplexity than RBM, which corroborates that by diversifying the hidden units, the document modeling power of RBM can be dramatically improved.
We study the sensitivity of DRBM to the tradeoff param-eter  X  . Figure 2 shows how precision@100 in retrieval varies as  X  increases on the TDT, 20-News and Reuters dataset re-spectively. The number of hidden units was fixed to 100. As can be seen from the figures, starting from 0, increasing  X  improves precision@100. That is because a larger  X  induces more diversity of the hidden units, enabling them to better cover long-tail topics. However, further increasing  X  causes the precision to drop. This is because, if  X  is too large, too much emphasis is paid to the diversify regularizer and the data likelihood of RBM is ignored.
Other than evaluating the learned hidden units of DRBM quantitatively, we also visualize and evaluate them in a qual-itative way. We can interpret each hidden unit as a topic and its weight vector as a pseudo distribution over the vo-cabulary. To visualize each hidden unit, we pick up the top 10 representative words which correspond to the ten largest values in the weight vector. Table 14 shows 5 topics learned by RBM and 5 topics learned by DRBM. As can be seen from the table, topics learned by RBM have many near-duplicates and are very redundant. In contrast, the topics learned by DRBM are much more diverse, with a broad cov-dataset erage of various topics including American politics, sports, Iraq war, law and Japanese education. This demonstrates the ability of DRBM to learn diverse topics.
In this paper, we study the problem of diversifying Re-stricted Boltzmann Machine. Due to the skewed distribu-tion of topic popularity, existing RBM tends to learn redun-dant hidden units to represent dominant topics with best long-tail topics. To solve this problem, we propose to di-versify the hidden units to make them to cover not only dominant topics, but also long-tail topics. We define a di-versity metric and use it to regularize the learning of the hid-den units. Considering the diversity metric is not amenable for optimization, we instead optimize its lower bound. We prove that maximizing the lower bound with projected gra-dient ascent can increase the diversity metric. Experiments on document retrieval and clustering show that through di-versification, the representations learned by DRBM can be greatly improved.
The authors would like to thank the anonymous reviewers for the helpful suggestions. This work is supported by the following grants to Eric P. Xing: ASFOR FA95501010247; NSF IIS1111142, IIS447676. [1] C. Archambeau, B. Lakshminarayanan, and [2] Y. Bengio, H. Schwenk, J.-S. Sen  X ecal, F. Morin, and [3] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [4] D. Cai and X. He. Manifold adaptive experimental [5] D. Cai, X. He, and J. Han. Document clustering using [6] D. Cai, X. He, and J. Han. Locally consistent concept [7] Z. Cao, S. Li, Y. Liu, W. Li, and H. Ji. A novel neural [8] Z. Chen and B. Liu. Mining topics in documents: [9] M. Denil, A. Demiraj, N. Kalchbrenner, P. Blunsom, [10] J. H. Friedman, J. L. Bentley, and R. A. Finkel. An [11] R. Guha. Towards a model theory for distributed [12] G. Hinton. Training products of experts by minimizing [13] G. E. Hinton and R. R. Salakhutdinov. Replicated [14] T. Hofmann. Probabilistic latent semantic analysis. In [15] C. Huang, X. Qiu, and X. Huang. Text classification [16] A. Kulesza and B. Taskar. Determinantal point [17] J. T. Kwok and R. P. Adams. Priors for diversity in [18] H. Larochelle and S. Lauly. A neural autoregressive [19] Q. V. Le and T. Mikolov. Distributed representations [20] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and [21] J. Pitman and M. Yor. The two-parameter [22] I. Sato and H. Nakagawa. Topic models with [23] I. R. Shafarevich, A. Remizov, D. P. Kramer, and [24] P. Smolensky. Information processing in dynamical [25] R. Socher, C. C. Lin, C. Manning, and A. Y. Ng. [26] N. Srivastava, R. Salakhutdinov, and G. Hinton. [27] Y. Wang, X. Zhao, Z. Sun, H. Yan, L. Wang, Z. Jin, To prove Lemma 1, the following lemma is needed.

Lemma 4. Let the weight vector  X a i of hidden unit i be decomposed into  X a i = x i + l i e i , where x i = P K lies in the subspace L spanned by {  X a 1 ,  X  X  X  ,  X a in the orthogonal complement of L , k e i k = 1 , e i  X   X a is a scalar. Then det ( e A T e A ) = det ( e A T  X  i e A e A  X  i = [  X a 1 ,  X  X  X  ,  X a i  X  1 ,  X a i +1 ,  X  X  X  ,  X a K ] with  X a
Proof. Part of the proof follows [23]. Let c i denote the i th column of the Gram matrix e A T e ear coefficient in x i , we get det( e A T e A ) = Expanding the determinant according to the i th column, we get det( e A T e A ) = det( e A T  X  i e A  X  i )( l i e i  X   X a Now we proceed to prove Lemma 1.
 Proof. The diversity metric  X ( e A ) comprises of two terms:  X ( e
A ) =  X ( e A )  X   X ( e A ), in which  X ( e A ) and  X ( e A ) measure the mean and variance of the pairwise angles respectively. We bound the two terms separately. We first bound the mean  X ( e A ). Since the weight vectors are assumed to be linearly independent, we have det( e A T e A ) &gt; 0. As l k l e i kk  X a i k X  1 and det( e A T e A ) = det( e A T  X  i e cording to Lemma 4), we have det( e A T e A )  X  det( e A T As  X  j, det(  X a T j  X a j ) = 1, we can eliminate the columns of and apply the inequality repeatedly to draw the conclusion that det( e A T  X  i e A  X  i )  X  1 (and det( e A T e A )  X  1). So l k l between  X a i and  X a j is:  X  (  X a i ,  X a j ) = arccos( |  X a i  X   X a j | ) = arccos( | x  X  arccos( k x i kk  X a j k ) = arccos( k x i k ) = arccos(  X  arccos( q 1  X  det( e A T e A )) = arcsin( q det( e A T e Thus  X ( e A )  X  arcsin( q det( e A T e A )).

Now we bound the variance  X ( e A ). For any i 6 = j , we have proved that  X  (  X a i ,  X a j )  X  arcsin( q det( e the definition of  X  (  X a i ,  X a j ), we also have  X  (  X a  X ( e
A ) is the mean value of all pairwise  X  (  X a i ,  X a arcsin( q det( e A T e A ))  X   X ( e A )  X   X  2 . So |  X  (  X a  X  arcsin( q det( e A T e A )). So  X ( e A )  X  (  X  2  X  arcsin(
Combining the lower bound of  X ( e A ) and upper bound of  X ( e A ), we have  X ( e A )  X   X ( e A ) = arcsin( q det( ( 2  X  arcsin( the optimal value of  X / 2 when the weight vectors in e A are orthogonal to each other. The proof completes.
Proof. According to chain rule, the gradient of  X ( w.r.t  X a i can be written as g 0 (det( e A T e A ))  X  det( g ( x ) = arcsin( have det( e A T e A ) = det( e A T  X  i e A  X  i ) l i e earlier, the weight vectors in e A are linearly independent and hence det( e A T e A ) &gt; 0 and det( e A T  X  i e det( e A T e A ) = det( e A T  X  i e A  X  i ) l i e i  X   X a l as k i e i , where k i = g 0 (det( e A T e A )) det( e A T proof completes.
 To prove Theorem 2, we first introduce some notations. Let V = { ( i,j ) | 1  X  i,j  X  K,i 6 = j,  X a ( t ) i  X   X a { ( i,j ) | 1  X  i,j  X  K,i 6 = j,  X a ( t ) i  X   X a ( t ) j i th column of A ( t ) . Let  X  V = P ( i,j )  X  V (  X  (  X a then  X ( e A ( t +1) )  X   X ( e A ( t ) ) =  X  V + X  N . Let x x ij = |  X a tion  X a i = x i + l i e i in Lemma 4, we have k  X a ( t ) p .
 The following lemmas are needed for proving Theorem 2.
Lemma 5.  X  ( i,j )  X  V , we have  X  (  X a ( t +1) i ,  X a ( t +1) = o (  X  ) , where lim
Proof. For ( i,j )  X  V ,  X a ( t ) i  X   X a ( t ) j = 0, thereby x arccos( x ( t +1) ij )  X   X  2 . Now we prove lim  X  alently, x ( t +1) ij = o (  X  )) and lim the Taylor expansion of arccos( x ) at x = 0, arccos( x ) =  X  x + o ( x ), so lim lim lim = lim
Lemma 6.  X  ( i,j )  X  N ,  X  c ij &gt; 0 , such that  X  (  X a  X   X  (  X a ( t ) i ,  X a ( t ) j ) = c ij  X  + o (  X  ) , where lim
Proof. Using the Taylor expansion of arccos( x ) at x = x ij , we have According to the definition of x ( t +1) ij , we have Using the Taylor expansion of 1  X  1+ x at x = 0, we can obtain that 1  X  k  X  2 ). As  X  2 = o (  X  ) and o (2 l i k i  X  + k 2 i  X  2 can obtain that 1  X  ilarly, 1 q  X  1 + o (  X  ). Substituting the above equations to x ( t +1) o (  X  ))(1  X  l j k j  X  + o (  X  ))  X  1) =  X  x ( t ) ij ( l lim that lim l k j )  X  + o (  X  )) + o (  X  ) = Given these two lemmas, we can prove Theorem 2 now. Proof.
 lim 0. So  X   X  &gt; 0 such that  X   X   X  (0 , X  ) we have  X ( e A ( t +1)
Due to space limit, we present the proof sketch of Lemma 3 here. Please refer to the external supplementary material for the detailed proof.
The proofs are available at http://www.cs.cmu.edu/ ~pengtaox/papers/kdd15_supp.pdf
Proof. First, we construct a sequence of sequences with decreasing variance, in which the variance of the first se-var( c ). We sort the unique values in b in ascending order and denote the resultant sequence as d = ( d j ) m j =1 l ( j ) = max { i : b i = d j } , u ( i ) = { j : d j = b struct a sequence of sequences h ( j ) = ( h ( j ) i ) n i =1 1 , 2 ,  X  X  X  ,m + 1, in the following way:  X  h (1) i = b i , i = 1 ,  X  X  X  ,n ;  X  h (2) i = h (1) i + g ( d m ), 1  X  i  X  l ( m ); b &lt; b n , we have m  X  2. We can prove that var( h ( m +1) var( c ) and  X  j = 1 , 2 ,  X  X  X  ,m, var( h ( j +1) ) &lt; var( h n = max { j : b j 6 = b n } , then  X  i , so var( c )  X  var( b 0 ) &lt; var( b ). The proof completes.
The intuition is when the stepsize  X  is sufficiently small, we can make sure the changes of smaller angles (between consecutive iterations) are larger than the changes of larger angles, then Lemma 3 can be used to prove that the variance decreases. The proof of Theorem 3 utilizes Lemma 5 and 6. decreasing order and denote the resultant sequence as  X  ( t ) (  X   X  then g (  X  ( t ) ij ) is a strictly decreasing function. Let  X  k + c k  X  =  X  sufficiently small, 0  X  g (  X  ( t ) k )  X  min {  X  ( t ) two complementary cases: (1)  X  ( t ) 1 &lt;  X  ( t ) n ; (2)  X   X  1 &lt;  X  g (  X  ( t ) n )) I ( k  X  n 0 ), then var(  X   X  ( t ) )  X  var(  X  can be further written as and  X  &gt; 0. Substituting  X  and  X  into var(  X  0 ( t ) obtain: Note that  X  &lt; 0 and  X  &gt; 0, so  X   X  1 , such that  X  &lt;  X  can draw the conclusion that var(  X   X  ( t ) ) &lt; var(  X  On the other hand, So  X   X  2 &gt; 0 such that  X  &lt;  X  2  X  var(  X  ( t +1) ) &lt; var( Let  X  = min {  X  1 , X  2 } , then
For the second case  X  ( t ) 1 =  X  ( t ) n , i.e.,  X  ( i this case,  X  ( i 1 ,j 1 ) , ( i 2 ,j 2 )  X  N  X  V , (( e A  X a for i 6 = j and p 2 =  X a ( t ) i  X   X a ( t ) j for i = j . As c e
A det(( e A ( t ) ) T e A ( t ) ) and g ( x ) = arcsin( It is clear that  X  ( i 1 ,j 1 ) , ( i 2 ,j 2 )  X  N  X  V , (( write it as c 2 (( p 2  X  p 1 ) I K + p 1 1 K 1 T K )  X  1 identity matrix and 1 K is a vector of 1s whose length is K . Applying Sherman-Morrison formula, we can obtain that (( e implies that  X  ( i 1 ,j 1 ) , ( i 2 ,j 2 )  X  N  X  V , (( e (( e
Putting these two cases together, we conclude that  X   X  2 &gt; 0, such that  X   X   X  (0 , X  2 ),  X ( e A ( t +1) )  X   X ( e A ( t )
