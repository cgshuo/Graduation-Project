 Personal Information Management (PIM) is a rapidly grow-ing area of research concerned with how people store, man-age and re-find information. A feature of PIM research is that many systems have been designed to assist users man-age and re-find information, but very few have been evalu-ated. This has been noted by several scholars and explained by the difficulties involved in performing PIM evaluations. The difficulties include that people re-find information from within unique personal collections; researchers know little about the tasks that cause people to re-find information; and numerous privacy issues concerning personal informa-tion. In this paper we aim to facilitate PIM evaluations by addressing each of these difficulties. In the first part, we present a diary study of information re-finding tasks. The study examines the kind of tasks that require users to re-find information and produces a taxonomy of re-finding tasks for email messages and web pages. In the second part, we propose a task-based evaluation methodology based on our findings and examine the feasibility of the approach using two different methods of task creation.
 H3.3 [ Information Search and Retrieval ]: Measurement,Management,Experimentation, Human Factors Personal Information Management, User Evaluation
Personal Information Management (PIM) is a rapidly grow-ing area of research concerned with how people store, man-age and re-find information. PIM systems -the methods Copyright 2007 ACM 978-1-59593-597-7/07/0007 ... $ 5.00. and procedures by which people handle, categorize, and re-trieve information on a day-to-day basis [18] -are becom-ing increasingly popular. However the evaluation of these PIM systems is problematic. One of the main difficulties is caused by the personal nature of PIM. People collect infor-mation as a natural consequence of completing other tasks. This means that the collections people generate are unique to them alone and the information within a collection is in-trinsically linked with the owner X  X  personal experiences. As personal collections are unique, we cannot create evaluation tasks that are applicable to all participants in an evaluation. Secondly, personal collections may contain information that the participants are uncomfortable sharing within an evalua-tion. The precise nature of this information -what informa-tion individuals would prefer to keep private -varies across individuals making it difficult to base search tasks on the contents of individual collections. Therefore, experimenters face a number of challenges in order to conduct realistic but controlled PIM evaluations.

A particular feature of PIM research is that many sys-tems have been designed to assist users with managing and re-finding their information, but very few have been evalu-ated; a situation noted by several scholars [1, 6, 7]. Recently, however, researchers have started to focus on ways to ad-dress the problem of PIM evaluation. For example, Kelly [16] proposes that numerous methodologies must be taken to examine and understand the many issues involved in PIM, although, she makes explicit reference to the need for labo-ratory based PIM studies and a common set of shared tasks to make this possible. Capra [6] also identifies the need for controlled PIM lab evaluations to complement other evalu-ation techniques, placing specific emphasis on the need to understand PIM behaviour at the task level.

In this paper, we attempt to address the difficulties in-volved to faciliate controlled laboratory PIM evaluations. In the first part of this paper we present a diary study of information re-finding tasks. The study examines the kind of tasks that require users to re-find information and pro-duces a taxonomy of re-finding tasks for email messages and web pages. We also look at the features of the tasks that make re-finding difficult. In the second part, we propose a task-based evaluation methodology based on our findings and examine the feasibility of the approach using different methods of task creation. Thus, this paper offers two con-tributions to the field: an increased understanding of PIM behaviour at the task level and an evaluation method that will facilitate further investigations.

A variety of approaches are available to study PIM. Nat-uralistic approaches study participants performing natu-rally, completing their own tasks as they occur, within fa-miliar environments. These approaches allow researchers to overcome many of the difficulties caused by the personal nature of PIM. As the tasks performed are  X  X eal X  and not simulated, the participants can utilise their own experiences, previous knowledge and information collections to complete the tasks. A benefit of the approach is that data can be captured continuously over extended time periods and mea-surements can be taken at fixed points in time within these [15]. Naturalistic approaches can be applied by conduct-ing fieldwork [17, 8], ethnographic methods as suggested by [15] or via log file analysis [9, 7]. Both ethnographic and fieldwork methods require the presence of an experimenter to assess how PIM is performed, which raises a number of issues. Firstly, evaluation in this way is expensive; taking long time periods to study small numbers of participants and these small samples may not be representative of the behaviour of larger populations. Secondly, because partic-ipants cannot be continually observed, experimenters must choose when to observe and this may affect the findings. An alternative strategy to conducting naturalistic evalua-tions is to utilise log file analysis. This approach makes use of logging software that captures a broad sampling of user activities in the context of natural use of a system. In [9] a novel PIM search tool was deployed to 234 users and the log data provided detailed information about the nature of user queries, interactions with the query interface and about properties of the items retrieved. Log file analysis is a power-ful methodology as it allows the capture of a large quantity of detailed information about how users behave with the system without the expense and distracting influence of an observer. Nevertheless, there are limitations to this strat-egy. Firstly, to attain useful results, the deployed prototype must be something that people would use i.e. it has to be a fully functional piece of software that offers improvement on the systems ordinarily available to participants. Devel-oping a research prototype to this standard is beyond the resources of many researchers. Further, caution must be taken when analysing logs, as the captured data shows noth-ing about the goals and intentions that the user had at the time. It is, therefore, difficult to make any concrete state-ments about the reasons for the behaviour depicted in the logs. This reveals a need to complement naturalistic studies with controlled experiments where the experimenter can re-late the behaviour of study participants to goals associated with known search tasks.

Laboratory-based studies simulate users X  real world environment in the controlled setting of the laboratory, of-fering the ability to study issues that are tightly defined and narrow in scope. One difficulty in performing this kind of evaluation is sourcing collections to evaluate. Kelly [16] pro-poses the introduction of a shared test collection that would provide sharable, reusable data sets, tasks and metrics for those interested in conducting PIM research. This may be useful for testing algorithms in a way similar to TREC in mainstream IR [13]. However, a shared collection would be unsuitable for user studies because it would not be possi-ble to incorporate the personal aspects of PIM while using a common, unfamiliar collection. One alternative approach is to ask users to provide their own information collections to simulate familiar environments within the lab. This ap-proach has been applied to study the re-finding of personal photographs [11], email messages [20], and web-bookmarks [21]. The usefulness of this approach depends on how easy it is to transfer the collection or gain remote access. An-other solution is to use the entire web as a collection when studying web page re-finding [4]. This may be appropriate for studying web page re-finding because previous studies have shown that people often use web search engines for this purpose [5].

A second difficulty in performing PIM laboratory stud-ies is creating tasks for participants to perform that can be solved by searching a shared or personal collection. Tasks relate to the activity that results in a need for information [14] and are acknowledged to be important in determining user behaviour [26]. A large body of work has been carried out to understand the nature of tasks and how the type of task influences user information seeking behaviour. For ex-ample, tasks have been categorised in terms of increasing complexity [3] and task complexity has been suggested to affect how searchers perceive their information needs [25] and how they try to find information [3]. Other previous work has provided methodologies that allow the simulation of tasks when studying information seeking behaviour [2]. However, little is known about the kinds of tasks that cause people to search their personal stores or re-find information that they have seen before. Consequently, it is difficult to devise simulated work task situations for PIM. The excep-tion is the study of personal photograph management, where Rodden X  X  work on categorising personal photograph search tasks has facilitated the creation of simulated work task sit-uations [22]. There have been other suggestions as to how to classify PIM tasks. For example, [5] asked participants to classify tasks based on how frequently they perform the task type in their daily life and how familiar they were with the location of the sought after information and several scholars have classified information objects by the frequency of their use e.g. [24]. While these are interesting properties that may affect how a task will be performed, they do not give experimenters enough scope to devise tasks.

Personal collections are one reason why task creation is so difficult. Rodden X  X  photo task taxonomy provides a solution here because it allows tasks, tailored to private collections to be categorised. Systems can then be compared across task types for different users [11]. Unfortunately, no equiv-alent taxonomy exists for other types of information object. Further, other types of object are more sensitive to privacy than photographs; it is unlikely that participants would be as content to allow researchers to browse their email collec-tions to create tasks as they were with photographs in [11]. This presents a serious problem -how can researchers de-vise tasks that correspond to private collections without an understanding of the kinds of tasks people perform or jeop-ardising the privacy of study participants? A few methods have been proposed. For example, [20] studied email search by asking participants to re-find emails that had been sent to every member in a department; allowing the same tasks to be used for all of the study participants. This approach ensured that privacy issues were avoided and participants could use things that they remember to complete tasks. Nevertheless, the systems were only tested using one type of task -participants were asked to find single emails, each of which shared common properties. In section 4 we show that people perform a wider range of email re-finding tasks than this. In [4], generic search tasks were artificially created by running evaluations over two sessions. In the first session, participants were asked to complete work tasks that involved finding some unknown information. In the second session, participants completed the same tasks again, which natu-rally involved some re-finding behaviour. The limitations of this technique are that it does not allow participants to ex-ploit any personal connections with the information because the information they are looking for may not correspond to any other aspect of their lives. Further, if time is utilised by a system or interface being tested the approach is unsuitable because all of the objects found in the first session will have been accessed within the same time period.

Our review of evaluation approaches motivates a require-ment for controlled laboratory experiments that allow tightly defined aspects of systems or interfaces to be tested. Unfor-tunately, it has also been shown that there are difficulties involved in performing this type of evaluation -it is difficult to source collections and to devise tasks that correspond to private collections, while at the same time protect the pri-vacy of the study participants.

In the following section we present a diary study of re-finding tasks for email and web pages. The outcome is a classification of tasks similar to that devised by Rodden for personal photographs [22]. In section 5 we build on this work by examining methods for creating tasks that do not compromise the privacy of participants and discuss how our work can facilitate task-based PIM user evaluations. We show that by collecting tasks using electronic diaries, not only can we learn about the tasks that cause people to re-find personal information, but we can learn about the contents of private collections without compromising the privacy of the participants. This knowledge can then be used to construct tasks for use in PIM evaluations.
Diary Studies are a naturalistic technique, offering the ability to capture factual data, in a natural setting, with-out the distracting influence of an observer. Limitations of the technique include difficulties in maintaining participant dedication levels and convincing participants that seemingly mundane information is useful and should be reported [19]. [12] suggest that the effects of the negatives can be lim-ited, however, with careful design and good implementa-tion. In our diary study, we followed the suggestions in [12] to achieve the best possible data. To this end, we restricted the recorded tasks to web and email re-finding. By asking users to record fewer tasks it was anticipated that partici-pant apathy would be reduced and dedication levels main-tained. The participants were provided with a personalised web form in which they could record details about their information needs and the contexts in which these needs developed. Web forms were deployed rather than paper-based diaries because to re-find web and email information the user would be at a computer with an Internet connec-tion and there would be no need to search for a paper-based diary and pen.

The diary form solicited the following information: whether the information need related to re-finding a web page or an email message and a description of the task they are per-forming. This description was to contain both the informa-tion that the participant wished to find and the reason that they needed the information. To help with this, the form gave three example task descriptions, which were also ex-plained verbally to each participant during an introductory session. The experimenter ensured that the participants un-derstood that the tasks to be recorded were not limited to the types shown in the examples. The examples were sup-plied purely to get participants thinking about the kinds of things they could record and to show the level of and type of details expected. The form also asked participants to rate each task in terms of difficulty (on a scale from 1-5, where 1 was very easy and 5 was very hard). Finally, they were asked when was the last time they looked at the sought af-ter information. Again, they were able to choose from 5 options (less than a day ago, less than a week ago, less than a month ago, less than a year ago, more than a year ago). Time information was used to examine the frequency with which the participants re-found old and new information, and when combined with difficulty ratings created a pic-ture of whether or not the time period between accessing and re-accessing impacted on how difficult the participants perceived tasks to be. 36 participants, recruited by mass advertisement through departmental communication channels, research group meet-ings and undergraduate lectures, were asked to digitally record details of their information re-finding tasks over a period of approximately 3 weeks. The final population con-sisted of 4 academic staff members, 8 research staff mem-bers, 6 research students and 18 undergraduate students. The ages of participants ranged from 19-59. As both per-sonal and work tasks were recorded, the results collected cover a broad range of re-finding tasks. Several analyses were performed on the captured data. The following sections present the findings. Firstly, we ex-amine the kinds of re-finding tasks that were performed both when searching on email and on the web. Next, we consider the distribution of tasks -which kinds of tasks were per-formed most often by participants. Lastly, we explore the kinds of re-finding tasks that participants perceived as dif-ficult.
During the study 412 tasks were recorded. 150 (36.41%) of these tasks were email based, 262 (63.59%) were web-based. As with most diary studies, the number of tasks recorded varied extensively between particpants. The me-dian number of tasks per participant was 8 (interquartile range (IQR)=9.5). More web tasks (median=5,IQR=7.5) were recorded than email tasks (median=3, IQR=3). This means that on average each participant recorded approxi-mately one task every two days.

From the descriptions supplied by the participants, we found similar features in the recorded tasks for both email and web re-finding. Based on this observation a joint classi-fication scheme was devised, encompassing both email and web tasks. The tasks were classified as one of three types: lookup tasks, item tasks and multi-item tasks. Lookup tasks involve searching for specific information from within a re-source, for example an email or a web page, where the re-source may or may not be known. Some recorded examples of lookup tasks were:
Item tasks involve looking for a particular email or web page, perhaps to pass on to someone else or when the entire contents are needed to complete the task. Some recorded examples of item tasks were:
To clarify, lookup tasks differ from item tasks in two ways -in the quantity of information required and in what the user knows about what they are looking for. Lookup tasks involve a need for a small piece of information e.g. a phone number or an ingredient, and the user may or may not know exactly the resource that contains this information. In item tasks the user knows exactly the resource they are looking for and needs the entire contents of that resource.
Multi-item tasks were tasks that required information that was contained within numerous web pages or email mes-sages. Often these tasks required the user to process or collate the information in order to solve the task. Some recorded examples were: There were a number of tasks that were difficult to classify. For example, consider the following recorded task:
This task actually consists of two sub-tasks: 1 item task(re-find the paper) and 1 lookup task (look for specific informa-tion within the paper). It was decided to treat this as a lookup task because the user X  X  ultimate goal was to access and use the information within the resource.

There were a number of examples of combined tasks, mainly of the form item then lookup, but there were also examples of item then multi-item. For example:
A second source of ambiguity came from tasks such as finding an email containing a URL as a means of re-accessing a web page. It was also decided to categorise these as lookup tasks because in all cases these were logged by participants as email searches and, within this context, what they were looking for was information within an email.

Another problem was that some of the logs lacked the detail required to perform a categorisation e.g.
Such tasks were labelled as U for  X  X nclassifiable X . To verify the consistency of the taxonomy, the tasks were re-categorised by the same researcher after a delay of two weeks. The agreement between the results of the two analyses was largely consistent (96.8%). Further, we asked a researcher with no knowledge of the project or the field to classify a sample of 50 tasks. The second researcher achieved a 90% agreement. We feel that this high agreement on a large num-ber of tasks by more than one researcher provides evidence for the reliability of the classification scheme.
The distribution of task types is shown in table 1. Overall, lookup and item tasks were the most common, with multi-item tasks only representing 8.98% of those recorded. The distribution of the task types was different for web and email re-finding. The majority of email tasks (60%) involved look-ing for information within an email (lookup), in contrast to web tasks where the majority of tasks (52.67%) involved looking for a single web page (item). Another distinction was the number of recorded multi-item tasks for web and email. Multi-item tasks were very rare for email re-finding (only 2.67% of email tasks involved searching for multiple resources), but comparatively common for web re-finding (12.6%).

In addition to the three-way classification described above, the recorded tasks were classified with respect to the tem-perature metaphor proposed by [24], which classifies infor-mation as one of three temperatures: hot, warm and cold. We classified the tasks using the form data. Information that had been seen less than a day or less than a week be-fore the task were defined as hot, information that had been seen less than a month before the task as warm, and infor-mation that had been seen less than a year or more than a year before the task as cold. Unfortunately, a technical diffi-culty with the form only allowed 335(81.3%) of the tasks to be classified. The remainder were defined as U for  X  X nclas-sifiable X . A cross-tabulation of task types and temperatures is shown in table 2.

Most of the tasks that caused people to re-find web pages (42.75%) and email messages (33.33%) involved searching for information that has been accessed in the last week. However there were also a number of re-finding tasks that in-volved searching for older information: 23.30% of the tasks recorded (24.00% for email and 22.90% for web) involved searching for information accessed in the last month and 18.69% of the tasks recorded (24.67% for email and 15.27% for web) were looking for even older information. This is im-portant with respect to evaluation because there is psycho-logical evidence suggesting that people remember less over time e.g. [23]. This means that users may find searching for older information more difficult or perhaps alter their seek-ing strategy when looking for hot, warm or cold information.
We looked for patterns in the recorded data to determine if certain tasks were perceived as more difficult than oth-ers. For example, we examined whether the media type af-fected how difficult the participants perceived the task to be. There was no evidence that participants found either email (median=2 IQR=2) or web (median=2 IQR=2) tasks more difficult. We also investigated whether the type of task or the length of time between accessing and re-accessing made a task more difficult. Figure 1 shows this information graph-ically.

From figure 1, it does not appear that any particular task type was perceived as difficult with respect to the others, although there is a suggestion that lookup tasks were per-ceived more difficult when looking for cold information than hot and item tasks were perceived more difficult for warm information than hot. To assess the relationship between information temperature and the perceived difficulty, we used Mood X  X  median tests to determine whether the rank of difficulty scores was in agreement for the information temperatures being compared (p &lt; 0.05). For the look-up task data, there was evidence that hot tasks were perceived easier than cold (p=0.0001) and that warm tasks were per-ceived easier than cold tasks(p=0.0041), but there was no evidence to distinguish between the difficulty ratings of hot and warm tasks(p=0.593). For the item task data, there was evidence that hot and cold tasks were rated differently (p=0.024), but no evidence to distinguish between hot and warm tasks(p=0.05) or warm and cold tasks(p=0.272).
These tests confirm that the length of time between ac-cessing and re-accessing the sought after information indeed influenced how difficult participants perceived the task to be. Nevertheless, the large number of tasks of all types and temperatures rated by participants as easy i.e. &lt; 3, suggests that there are other factors that influence how difficult a task is perceived to be. To learn about these factors would re-quire the kind of user evaluations proposed by [16, 6] -the kind of evaluations facilitated by our work.
In the first part of this paper, we described a diary study of web and email re-finding tasks. We examined the types of task that caused the participants to search their personal stores and found three main categories of task: tasks where the user requires specific information from within a single re-source, tasks where a single resource is required, and tasks that require information to be recovered from multiple re-sources. It was discovered that look-up and item tasks were recorded with greater frequency than multi-item tasks. Al-though no evidence was found that web or email tasks were more difficult, there was some evidence showing that the time between accessing and re-accessing affected how diffi-cult the participants perceived tasks to be. These findings have implications for evaluating PIM behaviour at the task level. The remainder of this paper concentrates on this, dis-cussing what the findings mean with respect to performing task-based PIM user evaluations.
The findings described in section 4 are useful with re-spect to evaluation because they provide experimenters with enough knowledge to conduct controlled user evaluations in lab conditions. Greco-Latin square experimental designs can be constructed where participants are assigned n tasks of the three types described above to perform on their own collec-tions using x systems. This would allow the performance of the systems or the behaviour of the participants using differ-ent systems to be analysed with respect to the type of task being performed (look-up, item, or multi-item). In the fol-lowing sections we evaluate the feasibility of this approach when employing different methods of task creation.
One method of creating realistic re-finding tasks without compromising the privacy of participants is to use real tasks. Diary-studies, similar to that described above, would allow experimenters to capture a pool of tasks for participants to complete by searching on their own collections. This is extremely advantageous because it would allow experi-menters to evaluate the behaviour of real users, complet-ing real search tasks on real collections while in a con-trolled environment. There is also the additional benefit that the task descriptions would not make any assumptions about what the user would remember in a real life situa-tion because they would only include the information that had been recorded i.e. the information that was available when the user originally performed the task. Nevertheless, to gain these benefits we must, firstly, confirm that the task descriptions recorded are of sufficient quality to enable the task to be re-performed at a later date. Secondly, we must ensure that a diary-study would provide experimenters with enough tasks to construct a balanced experimental design that would satisfy their data needs.

To examine the quality of recorded tasks, 6 weeks after the diary study had completed, we asked 6 of our par-ticipants, selected randomly from the pool of those who recorded enough tasks, to re-perform 5 of their own tasks. The tasks were selected randomly from the pool of those available. The issued tasks consisted of 10 email and 20 web tasks, 9 of which were lookup tasks, 12 were item tasks, and 8 were multi-item tasks. The issued tasks represented a broad-sampling of the complete set of recorded tasks. They also included tasks with vague descriptions e.g.
The usefulness of such tasks would rely on the memories of participants i.e. would the recorder of task LU5 remember which application he referred to and would the recorder of LU6 remember enough about the context in which the task took place to re-perform the task?
Presented with the tasks exactly as they recorded them, the participants were asked to re-perform each task with any system of their choice. Of the 30 tasks issued, 26 (86.67%) were completed without problems, 2 (6.67%) of the tasks were not completed because the description recorded was insufficent to recreate the task, and 2 tasks (6.67%) were not completed because the task was too difficult or the re-quired web page no longer existed. Experimenters are likely to be interested in the final group of tasks because it is im-portant to discover what makes a task difficult and how user behaviour changes in these circumstances. Therefore, from the 30 tasks tested, only 2 tasks were not of sufficient qual-ity to be used in an evaluation situation. Further, there did not seem to be any issue of the type, temperature or diffi-culty ratings affecting the quality of the task descriptions. These findings suggest that the participants who recorded most tasks in the diary study also recorded tasks with suffi-cient quality. However, did the diary study generate enough tasks to satisfy the needs of experimenters? Table 3: The quantities of recorded email tasks
Naturally the exact number of tasks required to perform a user evaluation will depend on the goals of the evaluation, the number of users and the number of systems to be tested etc. However, for illustrative purposes we chose 5 tasks as a cut-off point for our data. From tables 3 and 4, which show the quantities of email and web tasks recorded for each participant, we can see that of the 36 participants, only 13 (36.1%) recorded 5 or more email tasks and 20 (55.6%) recorded 5 or more web tasks. This means that many of the recruited participants could not actually participate in the final evaluation. This is a major limitation of using recorded tasks in evaluations because participant recruitment for user tests is challenging and it may not be possible to recruit enough participants if experimenters lose between half and two-thirds of their populations.

Further, there was some imbalance in the numbers of recorded tasks of different types. Some participants recorded several lookup tasks but very few item tasks and others recorded several item tasks but few lookup tasks. There was also a specific lack of multi-item email tasks. This sit-uation makes it very difficult for experimenters to prepare balanced experimental designs. Therefore, even though our first test suggests that the quality of recorded tasks was suf-ficient for the participants to re-perform the tasks at a later stage, the number of tasks recorded was probably too low to make this a viable option for experimental task creation. However, it may be possible to increase the number of tasks recorded by frequently reminding participants or by making personal visits etc.
Another benefit of diary-studies is that they provide in-formation about the contents and uses of private collec-tions without invading participants X  privacy. This section explores the possibility of using a combination of the knowl-edge gained from diary studies and other attributes known about participants to artificially create re-finding tasks cor-responding to the taxonomy defined in section 4.1. We ex-plain the techniques used and demonstrate the feasibility of creating simulated tasks within the context of a user eval-uation investigating email re-finding behaviour. Space lim-itations prevent us from reporting our findings; instead we concentrate on the methods of task creation.

As preparation for the evaluation, we performed a sec-ond diary-study, where 34 new participants, consisting of 16 post-graduate students and 18 under-graduate students, recorded 150 email tasks over a period of approximately 3 weeks. The collected data revealed several patterns that helped with the creation of artificial tasks. For example, students in both groups recorded tasks relating to classes that they were taking at the time and often different partic-ipants recorded tasks that involved searching for the same information. This was useful because it provided us with a clue that even though some of the participants did not record a particular task, it was possible that the task may still be applicable to their collections. Other patterns revealed in-cluded that students within the same group often searched for emails containing announcements from the same source. For example, several undergraduate students recorded tasks that included re-finding information relating to job vacan-cies. There were also tasks that were recorded by partici-pants in both groups. For example, searching for an email that would re-confirm the pin code required to access the computer labs.

To supplement our knowledge of the participants X  email collections, we asked 2 participants from each group to pro-vide email tours. These consisted of short 5-10 minute ses-sions, where participants were asked to explain why they use email, who sends them email, and their organisational strategies. This approach has been used successfully in the past as a non-intrusive means to learn about how people store and maintain their personal information [17]. Origi-nally, we had planned to ask more participants to provide tours, but we found 2 tours per group was sufficient for our needs. Again, patterns emerged that helped with task creation. We found content overlap within and between groups that confirmed many of our observations from the diary study data. For example, the students who gave tours revealed that they received emails from lecturers for partic-ular class assignments, receipts for completed assignments, and various announcements from systems support and about job vacancies. Importantly, the participants were also able to confirm which other students had received the same infor-mation. This confirmed that many of tasks recorded during the diary study were applicable, not only to the recorder, but to every participant in 1 or both groups.

Based on this initial investigatory work, a set of 15 tasks (5 of each type in our taxonomy) was created for each group of participants. We also created a set of tasks for a third group of participants that consisted of research and aca-demic staff members, based on our knowledge of the emails our colleagues receive. Where possible we used the infor-mation recorded in the diary study descriptions to provide a context for the task i.e. a work task or motivation that would require the task to be performed. When the diary study data did not provide sufficient context information to supply the participants with a robust description of the in-formation need, we created simulated work task situations according to the guidelines of [2]. A further advantage of using simulated tasks in this way, rather than real-tasks, is that some of the users will not have performed the task in the recent past and this allows the examination of tasks that look for information of different temperatures. If only real-tasks had been used all of the participants would have performed the tasks during the period of the diary study.
The created tasks were used in a final evaluation, where we examined the email re-finding behaviour of users with three different email systems. 21 users (7 in each group) performed 9 tasks each (1 task of each type on each system) using their own personal collections in a Greco-Latin square experimental design. Performing a PIM evaluation in this way allowed the examination of re-finding behaviour in a way not possible before -we were able to observe the email re-finding strategies employed by real users, performing re-alistic tasks, on their own collections in a controlled envi-ronment. The study revealed that the participants remem-bered different attributes of emails, demostrated different finding behaviour, and exhibited different levels of perfor-mance when asked to complete tasks of the different types in the taxonomy. The key to both the task creation and the analysis of the results was our taxonomy, which provided the template to create tasks and also a means to compare the behaviour and performance of different users (and systems) performing different tasks of the same type. Some of the findings of the evaluation will be published in [10].
Summarising the approach, to conduct a user experiment using our methodology, researchers would be required to perform the following steps: 1)Conduct a diary study as above 1 . 2)Analyse the recorded tasks looking for overlap between the participants. 3)Supplement the gained knowl-edge about the contents of participants X  collections by asking a selection of the participants to provide a tour of their col-lection. 4)Use the knowledge gained to devise tasks of the three different types defined within the taxonomy. More de-
Information about this and the diary forms required can be found at http://www.cis.strath.ac.uk/  X  dce/PIMevaluations tailed information on how to use the research described in this paper to perform task-based PIM evaluations can be found at our website (see footnote 1).
This paper has focused on overcoming the difficulties in-volved in performing PIM evaluations. The personal nature of PIM means that it is difficult to construct balanced ex-periments because participants each have their own unique collections that are self-generated by completing other tasks. We suggested that to incorporate the personal aspects of PIM in evaluations, the performance of systems or users should be examined when users complete tasks on their own collections. This approach itself has problems because task creation for personal collections is difficult: researchers don X  X  know much about the kinds of re-finding tasks people per-form and they don X  X  know what information is within indi-vidual personal collections. In this paper we described ways of overcoming these challenges to facilitate task based PIM user evaluations.

In the first part of the paper we performed a diary study that examined the tasks that caused people to re-find email messages and web pages. The collected data included a wide range of both work and non-work related tasks, and based on the data we created a taxonomy of web and email re-finding tasks. We discovered that people perform three main types of re-finding task: tasks that require specific information from within a single resource, tasks that require a single complete resource, and tasks that require information to be recovered from multiple resources. In the second part of the paper, we discussed the significance of the taxonomy with respect to PIM evaluation. We demonstrated that balanced experiments could be conducted comparing system or user performance on the task categories within the taxonomy. We also suggested two methods of creating tasks that can be completed on personal collections. These methods do not compromise the privacy of study participants. We ex-amined the techniques suggested, firstly by simulating an ex-perimental situation -participants were asked to re-perform their own tasks as they recorded them, and secondly, in the context of a full evaluation. Performing evaluations in this way will allow systems that have been proposed to improve users X  ability to manage and re-find their information to be tested, so that we can learn about the needs and desires of users. Thus, this paper has offered two contributions to the field: an increased understanding of PIM behaviour at the task level and an evaluation method that will facilitate further investigations.
We would like to thank Dr Mark Baillie for his insightful comments and help analysing the data. [1] R. Boardman, Improving tool support for personal [2] P. Borlund, The iir evaluation model: A framework for [3] K. Bystr  X om and K. J  X arvelin, Task complexity affects [4] R. G. Capra and M. A. Perez-Quinones, Re-finding [5] R. G. Capra and M. A. Perez-Quinones, Using web [6] R. G. Capra and M. A. Perez-Quinones, Factors and [7] E. Cutrell, D.Robbins, S.Dumais, and R.Sarin, Fast, [8] M. Czerwinski, E. Horvitz, and S. Wilhite, A diary [9] S. Dumais, E. Cutrell, J. Cadiz, G. Jancke, R. Sarin, [10] D. Elsweiler and I. Ruthven, Memory and email [11] D. Elsweiler, I. Ruthven, and C. Jones, Dealing with [12] D. Elsweiler, I. Ruthven, and C. Jones, Towards [13] D. Harman, What we have learned, and not learned, [14] P. Ingwersen, Information retrieval interaction , Taylor [15] D. Kelly, B. Bederson, M. Czerwinski, J. Gemmell, [16] D. Kelly and J. Teevan, (to appear in) personal [17] B. H. Kwasnik, How a personal document X  X  intended [18] M.W. Lansdale, The psychology of personal [19] L. Palen and M. Salzman, Voice-mail diary studies for [20] M. Ringel, E. Cutrell, S. Dumais, and E. Horvitz, [21] G. Robertson, M. Czerwinski, K. Larson, D. C. [22] K. Rodden, How do people organise their photographs , [23] D.C. Rubin and A.E. Wenzel, One hundred years of [24] A. J. Sellen and R. H. R. Harper, The myth of the [25] P. Vakkari, Task complexity, problem structure and [26] P. Vakkari, A theory of task-based information
