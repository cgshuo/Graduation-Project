 Users frequently rely on online reviews for decision making. In addition to allowing users to evaluate the quality of indi-vidual products, reviews also support comparison shopping. One key user activity is to compare two (or more) products based on a specific aspect. However, making a comparison across two different reviews, written by different authors, is not always equitable due to the different standards and pref-erences of individual authors. Therefore, we focus instead on comparative sentences, whereby two products are compared directly by a review author within a single sentence. We study the problem of comparative relation mining. Given a set of comparative sentences, each relating a pair of entities, our objective is two-fold: to interpret the com-parative direction in each sentence, and to determine the relative merits of each entity. This requires mining compar-ative relations at two levels of resolution: at the sentence level, as well as at the entity level. Our key observation is that there is significant synergy between the two levels. We therefore propose a generative model for comparative text, which jointly models comparative directions at the sentence level, and ranking at the entity level. This model is tested comprehensively on Amazon reviews dataset with good em-pirical outperformance over the state-of-the-art baselines. H.4 [ Information Systems Applications ]: Miscellaneous; H.2.8 [ Database Applications ]: Data Mining generative model; comparison mining; comparative sentences
With the advent of social information processing, we in-creasingly turn to the wisdom of the crowd in social media  X 
Work done while visiting Singapore Management Univer-sity.
 to aid our decision making. One decision aid is the ability to easily compare various alternatives. Few existing commer-cial systems assist users in making a comparison of entities (products) based on other users X  experiences. Comparison shopping sites such as Google Shopping or PriceGrabber compare prices among different sellers for the same prod-uct. Others, such as DPReview for digital cameras, com-pare different cameras 1 based on structured attributes of the cameras, instead of users X  perception of various aspects.
Our objective is to mine user-generated content to assist users in making well-informed comparisons of entities. Es-sentially, we view it as a crowdsourcing way to determine the relative quality of entities, from the users X  vantage point. In the following discussion, we use online reviews as a repre-sentative example of such a user-generated content. Mining reviews for opinions on various products is an active area of research, including the two distinct, yet complementary areas of opinion mining and comparison mining.

Opinion mining [14] deals with sentences expressing user sentiments about an entity. The following sentence from an Amazon review expresses a positive sentiment on the im-age quality of the camera Canon EOS 50D :  X  X ased on my initial experience with this Camera, I must say that 50D pro-duces amazingly sharp pictures in low light conditions, with kids, and a variety of outdoor scenarios like sports and na-ture. X  . By aggregating sentiments across such sentences, we can arrive at a sense of the quality of a product. While opin-ion mining has its use in revealing the strengths and weak-nesses of an entity, there are inherent disadvantages in bas-ing a comparison of two entities on opinion sentences about individual entities. For one, the respective opinions may be expressed by different users, potentially with different stan-dards or purposes, rendering the comparison inequitable.
Therefore, we focus on comparison mining , whereby the bases for comparison are comparative sentences about two entities. For example, the following sentence expresses a comparison between Canon EOS 50D vs. Canon EOS 40D in terms of image quality:  X  X he 50D is sharper than my 40D and the images are not soft. X  . In this case, the same user (providing a common benchmark) compares two entities within the same context. Table 1 shows several more examples for three pairs of digital cameras. This focus on comparative sentences distinguishes our work from opinion mining, and opens up the potential of other sentences not currently covered by opinion mining. To maintain focus, here we deal with sentences involving two entities, and would explore sentences involving more entities in a future work. http://www.dpreview.com/products/compare/cameras
Problem. Given a corpus of comparative sentences, re-lating pairs of entities in a particular domain (e.g., digital cameras) on a specific aspect (e.g., image quality), we seek to derive the comparative relations among the entities, i.e., between any two comparable entities, which one is better in that aspect. The corpus of comparative sentences may be obtained from user-generated content expressing user pref-erences in social media, such as reviews (see Section 6.1.1).
Naturally, the comparative relations need to be modeled at two levels. First, at the level of each comparative sen-tence. For example, s 1 in Table 1 may be analyzed to deter-mine that it favors Canon EOS 40D . Second, at the level of entity pairs. The sentence-level relations are aggregated to form the comparative relation, e.g., whether Canon EOS 50D is better than Canon EOS 40D . Both levels are impor-tant and useful. The latter provides an aggregate view. The former provides supporting evidence at the sentence level.
Solving this problem is challenging. For one, decipher-ing comparative relation from text is difficult, due to com-plex structures affecting the comparison (e.g., which entity is mentioned first, negation such as  X  X ot soft X ), as well as lin-guistic forms such synonymy (e.g.,  X  X etter X  and  X  X harper X ) and polysemy (e.g.,  X  X oubling X  may connote positively for image quality, but negatively for price). For another, in-trepreting the relation between two entities is fraught with uncertainties due to inconsistencies among different writers.
Approach. Traditionally, these challenges are addressed in a fragmented manner by solving the two levels sepa-rately as a pipeline, for instance by first determining the sentence-level comparisons, and then aggregating them into the entity-level comparisons (see Section 5). This fragmen-tation is unnecessary, and could even be deterimental, when errors from one level propagate unmitigated to the other.
We postulate that an integrated approach would work bet-ter because of the synergy between sentence-level and entity-level comparisons. Intuitively, if one entity is better than another entity, we would expect that many comparative sen-tences would compare the former favorably with respect to the latter. Thus, knowing which entity is better helps to determine the comparison in a sentence, and vice versa.
We illustrate this intuition with an example in Figure 1 involving 6 sentences (right) on 5 entities { A,B,C,D,E } . Supposing the meaning of the first four sentences with the word  X  X etter X  is known, we can confidently rank some pairs, by drawing a bold edge from the worse entity to the better entity, e.g., from B to A , since  X  X  is better than B X . There are a couple of questions that we seek to answer. One question is which of D or E is better, since there is no clue from the bold edges alone. Another question is the Figure 1: Separate treatment of the tasks does not allow meaning of the last two sentences, since we have not yet un-derstood  X  X uperior X . Considering these two questions sep-arately does not offer an answer. Considering them jointly allows us to arrive at an answer to both. Since A  X  B  X  C , by transitivity, we can infer that A  X  C (dotted means in-ferred). In turn, A  X  C allows us to interpret the sentence  X  X  is superior to C X , i.e., that  X  X uperior X  implies the first-mentioned entity is better. This then allows us to parse the last sentence to infer that D  X  E (dotted). We thus can recover the correct rank order A  X  B  X  C  X  D  X  E .
 Contributions. We make the following contributions.
First , we propose an integrated approach for compara-tive relation mining. This is a departure from previous pipelined approaches (see Section 5). It is not necessarily feasible, nor desirable, simply to  X  X titch up X  existing formu-lations of sentence-level and entity-level comparisons. They are disparate frameworks that are not easy to bring together other than as a disjoint pipeline. We therefore build a novel method from the ground up to solve the problem holistically. Our integrated formulation is presented in Section 2.
Second , we design a generative model (see Section 3), called CompareGem , which stands for COMPArative REla-tion GEnerative Model, and propose an inference algorithm based on Gibbs sampling (see Section 4). We turn to genera-tive modeling because it offers several significant advantages. For one, it would be probabilistic , suitable for modeling the uncertainties in text. For another, it would be a joint model, connecting sentence-level and entity-level comparisons seam-lessly. It would also be generative , with greater flexibility in accommodating supervised and unsupervised settings.
Third , through experiments on datasets from Amazon re-views (see Section 6), we validate that CompareGem indeed outperforms the pipelined baselines in both supervised and unsupervised configurations, underlining the utility of inte-grated modeling in comparative relation mining.
As input, we consider a set of entities E (e.g., digital cam-eras). For each pair of entities e i ,e j  X  E , S ij denotes the set of comparative sentences involving e i and e j (in any order of mention) for a specific aspect (e.g., image quality or price). We can equivalently refer to S ij as S ji . For instance, in Table 1, the pair of entities Canon EOS 50D and Canon EOS 40D are associated with a set of four comparative sen-tences { s 1 ,s 2 ,s 3 ,s 4 } on image quality. Some pairs may not have any comparative sentence, if they are never compared by any user, i.e., S ij =  X  . The union of all comparative sen-tences is denoted S = S e S can be obtained from a corpus of reviews in Section 6.1.1.
Our objective is to learn the comparative relation between any two comparable entities e i or e j , in terms of which one is better for the aspect of interest. Using the example of Canon EOS 50D and Canon EOS 40D in Table 1, we see that s 1 favors Canon EOS 40D , whereas s 2 to s 4 favor Canon EOS 50D . The majority consensus favors the con-clusion that Canon EOS 50D is better in image quality.
We first discuss the question of when two entities are com-parable. In layman X  X  terms, we simply do not wish to com-pare apples and oranges. We adopt a data-driven approach as follows. If there is at least one comparative sentence in S about two entities, then some user has deemed them comparable. Comparability is also transitive. If e i has been compared to e 0 , and e 0 has been compared to e j , then we can compare e i and e j using e 0 as a conduit for the comparison.
Definition 1 (Comparability). Two entities e i and e are comparable if and only if one of the following condi-tions holds. First, S ij 6 =  X  . Second, S ij =  X  , but there exist a list of indices i = k 1 ,k 2 ,...,k n  X  1 ,k n = j such that every element of the chain ( e k 1 ,e k 2 ) , ( e k 2 ,e k 3 connecting e i and e j , meets the first condition.
For each comparable pair of entities, we want to determine which one is better. To capture this notion of relative  X  X ual-ity X  among entities, we associate each entity e i with a rank score a i  X  R , where e i is  X  X etter X  than e j if a i &gt; a rank score is latent, and needs to be learnt from the data. As seen in Table 1, the sentences are not always unanimous in terms of which entity is favored. Even when there is a consensus, there may also be some variance (e.g., dissenting opinions). It is important not just to capture the relation at the entity level, but also the comparative direction at the sentence level to provide a full picture of the comparison.
With the notations in place, we are now ready to state our problem statement formally, as follows.

Problem 1 (Comparative Relation Mining). Given a set of entities E and the associated corpus of comparative sentences S , find:
We first discuss the modeling of comparative sentences, before describing our generative model CompareGem .
The convention in modeling text, either for classification [26] or topic modeling [1], is to model a document as a bag of words, due to the assumption of exchangeability of words within a document. In other words, only the frequencies of words, and not the sequence in which they occur, matter.
While we also deal with text, the atomic unit of interest is a sentence. A bag-of-words model is not appropriate for modeling a comparative sentence. Recognizing the favored entity in a comparative sentence is a challenging problem due to the complex sentence structure, whereby word order now becomes important. Let us consider the following ex-ample comparative sentence:  X  X he 50D is sharper than my 40D X  . In terms of the bag-of-words model, the order between 50D and 40D could be swapped interchangeably. However, we know that swapping the order of those two words would change the meaning of the comparison completely.

We observe that, rather than words alone, the clues to the comparison come in the form of features, as follows:
Rather than a bag-of-words model, we model each com-parative sentence s as a bag of features , where each feature w is drawn from a vocabulary of features W (such as men-tioned above). The bag representation also maintains the frequency of each feature within each sentence.
CompareGem is a generative model for comparative sen-tences. Its plate notation is shown in Figure 2. First, for each sentence s  X  S , we generate the comparison outcome (which entity is favored in s ). Thereafter, based on the com-parison outcome, we generate each feature w  X  s .

Generating Comparison Outcome. Each sentence s in the corpus S expresses a comparison outcome involving two entities (say e i and e j ). We associate each sentence s with a binary random variable c s . The event c s = 0 is the outcome when the first-mentioned entity is favored, whereas c = 1 is the outcome when the second-mentioned entity is favored. For simplicity, we do not model a draw, which would not influence the ranking between the two entities.
Since c s is a random variable, its outcome depends on an underlying probability distribution. As mentioned in Sec-tion 2, we associate each entity e i  X  E with a rank score a that reflects the quality of e i . Intuitively, the higher a is than a j , the higher is the probability that a comparative sentence would favor a i . One suitable way to model this probability for binary outcomes is the sigmoid function, as shown in Equation 1, supposing the first-mentioned entity is e i and the second-mentioned entity is e j .

If a i is significantly higher than a j , the probability would tend towards 1, reflecting e i  X  X  much higher quality. If a the probability is 0.5, reflecting the uncertain outcome be-tween two evenly matched entities. Conversely, if a i is sig-nificantly lower than a j , the probability would tend towards 0. The parameter  X  models the sensitivity to the difference between the two rank scores. If  X  is large, small differences in scores would have a high impact on the probabilities.
There is also the question of the appropriate range of a i One option is to model it along a continuous spectrum, with a Gaussian prior for the distribution of a i  X  X , which would encode the prior that most entities are probably of  X  X verage X  rank scores, while some are extremely high or low. However, in some scenarios, the requirement is simply to place each entity along a discrete scale, e.g., 0 to 9, for an easy-to-understand ranking of entities. Another option, which we adopt in this paper, is to have a discretized model, with N number of ranking steps in the scale of 0 to N  X  1. Instead of a Gaussian, the prior can thus be simulated by a binomial distribution Binomial ( N  X  1 ,p 0 ), where p 0 is probability of success in a Bernoulli trial ( p 0 = 0 . 5 for our model). As N  X   X  , we get ever closer to the continuous version. We experiment with different settings of N and  X  in Section 6.
Generating Features. Once the comparison outcome c s for a sentence s is generated, we then generate the features of the sentence. We assume that each outcome c  X  { 0 , 1 } is associated with a parameter  X  c , which is a probability distribution { P( w |  X  c ) } w  X  W over features in the vocabulary W . For instance,  X  0 is a distribution over features for the case when the first-mentioned entity is favored, in which case features involving words such as  X  X etter X ,  X  X harper X  may have higher probabilities. Meanwhile,  X  1 is the same for when the second-mentioned entity is favored, in which case those involving words such as  X  X orse X  may be more likely.
Generative Process. We now describe the full genera-tive process of the model. 1. Both  X  0 and  X  1 are sampled from a Dirichlet distribu-2. For each entity e i  X  E , we sample its rank score a i 3. For every comparative sentence s  X  S comparing two
As shown in Figure 2, the only observed (shaded) vari-ables are the features w  X  X  within each sentence s . All the other random variables are latent and need to be learnt from the data. The likelihood function of an assignment of scores A = { a i } e and latent distributions over features  X  = {  X  0 , X  1 } is shown in Equation 2, where the three main terms correspond to the three main steps in the above generative process.
L ( C,A, X  ) = Y
Once the model parameters have been learned, we would obtain the solution to the Problem 1 defined in Section 2. The rank scores a i  X  X  would provide the ranking among en-tities. On the other hand, the comparison outcome of a sentence s can be obtained from the posterior distribution of c s , taking into account the corresponding entities X  rank scores a i and a j and the features in s , as in Equation 3. P( c s | a i ,a j ,s ) =
As intuited in Section 1, this joint modeling of entity rank-ing ( { a i } e i  X  E ) and sentence-level comparison { c pected to help both tasks. In particular, the effect may be especially significant for the sentence-level comparison, since the ranking may complement the very few features that a sentence usually has. While ranking may be some-what more robust to sentence-level errors due to aggregating multiple sentences, more accurate sentence-level comparison outcomes are still expected to improve ranking inference.
Unsupervised vs. Supervised. In the above genera-tive process, we have assumed a fully unsupervised setting. We would learn the ranking of entities, and the direction of ranking is interpreted by inspecting the feature distribu-tions. This is because, in theory, the direction of larger score indicating  X  X etter X  could be swapped (i.e., smaller score cod-ing for  X  X etter X ) while still having the same likelihood.
Our model also accommodates a  X  X upervised X  setting. To introduce  X  X ight X  supervision, we label a subset of sentences in terms of their comparison outcomes. Where in the unsu-pervised setting, only the w  X  X  are observed, in the supervised setting, we would consider some c s variables (corresponding to a subset of labeled sentences) to also be observed (having known outcomes). This would have the effect of grouping together sentences of the same label, which would then in-fluence the respective feature distributions  X  0 and  X  1 . We will explore both unsupervised and supervised settings in Section 6.

Another possible direction for supervision, which we do not explore in this paper, is to consider the rank score a some entities to be observed. Imposing some ranking order is probably too heavy-handed, because it imposes some form of  X  X pinion X  (as opposed to labeling each sentence X  X  compar-ison outcome, which is more objective). Imposing ranking runs counter to our objective of learning the crowdsourced ranking based on user-generated content.

Relation to Other Learning Models. The modeling of comparison outcomes in Equation 1 has some relation to the family of latent ability models (see Section 5) used to model competitions with known outcomes. In a way, a compara-tive sentence can be interpreted as a  X  X ompetition X  between two entities, where the outcome is one entity becomes the  X  X inner X . The key difference is that in our case the outcomes are latent and unknown, and need to be learned from text. This synergy between competition modeling and generative modeling of text is novel, and we will include a comparison to a baseline of pure competition model in Section 6.
If we were to remove the ranking component altogether, for instance by setting N = 1 so that every entity has exactly the same rank score, the model degenerates into a simpli-fied generative model for text clustering or classification, in a way similar to multinomial Naive Bayes model with un-biased probability over favored entities c s , and with  X  prior of the words parameters. We therefore include Naive Bayes classification as a baseline for comparison in Section 6.
Our feature distributions are also somewhat related to the notion of  X  X opics X  in topic modeling [1], where each topic codes for some concept based on word co-occurences. Sev-eral crucial differences set us apart from topic modeling. In our case, there are always two  X  X opics X  corresponding to the two comparison outcomes (and not an arbitrary num-ber of topics), the distribution is over features (and not over words), and the primary mechanism for learning is the com-parison model in addition to feature co-occurrences (and not word co-occurrences alone as in topic modeling).
Gibbs sampling [11] provides a mechanism to infer hidden variables of a graphical model. It allows drawing samples from a joint probability distribution of two or more random variables, when direct sampling is intractable. It is a special case of Monte Carlo algorithm that defines a Markov chain in the space of possible variable assignments. We sample one variable at a time from the conditional distribution of that variable, conditioned on all the others. The stationary distribution of the Markov chain is the joint distribution over the variables and samples drawn in a such way are guaranteed from the joint distribution.

For CompareGem , we use the collapsed version of Gibbs sampling, by integrating out  X  0 , X  1 . The derivation is pro-vided below.

We separately integrate the expression for  X  0 and  X  1 . For  X  , we have: P( S|  X  ; c ) = where n ( k,c ) denotes the frequency of the feature w k within the sentences with comparison outcome c s = c , and K is the size of the feature vocabulary ( K = | W | ).
 The other components of the likelihood are shown below.
P( a i | N  X  1 ,p 0 ) = ( N  X  1)! The algorithm samples the comparison outcome c s for each sentence s  X  S , and the latent scores a i for each entity e  X  E .

Sampling rank scores A . Fixing the preference assign-ments for sentences C and assuming p 0 = 0 . 5, we sample latent score a i for every entity e i : P( a i = a | A  X  i ,... )  X  P( a | N  X  1 ,p 0 = 0 . 5) Y where A  X  i denotes the set of latent scores for each entity except e i  X  X .

Sampling comparison outcomes C . Fixing the rank-ing scores A , we sample the comparison outcome c s for each sentence s : where  X  n ( k,c ) returns the count of the feature k within all the sentences labeled with c excluding the sampled sentence s ; f s ( c,k ) denotes the frequency of feature k in the sentence s for the assignment c ; f s ( c,  X  ) = P K k =1 f s ( c,k ).
Although Gibbs sampling allows estimating the shape of a probability distribution, one can modify this process to maximize the model likelihood. We used simulated anneal-ing , the technique used in optimization to find global op-timum of a given (non-convex) function. We sample each variable from the modified distribution: where the sequence T = ( t j ) n j =1 defines the cooling sched-ule and a particular value t j is called the temperature. As t  X  0 the distribution becomes sharper (setting t j = 1 for every j recovers standard Gibbs sampling procedure) and the modified distribution concentrates all the mass on the maximal outcome.
 Each iteration of the Gibbs sampler takes O ( | E ||S| ) time. It is easy to see that at each iteration the algorithm samples comparison outcomes C for the sentence set S , which re-quires O ( |S| ) operations. The rank score sampling involves O ( | S ij | ) time for each entity e i , which can be bounded by O ( |S| ). Indeed, if the number of sentences for every entity is evenly distributed, the term | E | can be dropped, then the iteration time is linear in the number of sentences O ( |S| ).
We first review the related work in comparison mining to define our baselines, then compare to other related problems.
Among the earliest task being considered in comparison mining is the identification of comparative sentences from a corpus containing both comparative and non-comparative sentences [17, 9]. Once the comparative sentences have been identified, the next task is to extract the entities being com-pared within each sentence [18, 19], and to resolve mentions of the same entity across sentences [6]. These tasks are or-thogonal, and yet complementary to our problem, and we discuss how we deal with these issues in Section 6.1.1. Our focus in this work is on mining comparative relations . Given a pair of entities, and their relevant comparative sen-tences, which entity is better? This requires an examination of comparative relation at two levels: sentences and entity pairs. As mentioned in Section 1, these two levels have tra-ditionally been studied separately, as follows.

At the sentence level , the objective is to determine which of the two entities being mentioned is considered better. Pre-vious work either uses training labels in supervised classifi-cation [33], or known indicators such as  X  X ros X  and  X  X ons X  within reviews [10]. Since supervised classification is the more recent and more general approach, here we consider two classifiers as baselines: Support Vector Machine ( SVM ) [5] and Naive Bayes ( NB ) [26], as implemented in Weka [12], using the same features as described in Section 3.

At the entity pair level , the objective is to determine which of the two entities is better overall [21, 36, 23]. This is done by aggregating the sentence-level comparisons into an overall ranking of entities. The main approach in previous work is to build a graph of entities, with a directed edge from one entity to another entity, weighted by the number of sentences that claims the latter is better than the former [21]. The ranking is then derived using a network centrality measure such as PageRank ( PR ) [27]. We use this as the first ranking baseline, implemented according to [21].
For the second ranking baseline, we consider latent ability models, used to aggregate pairwise comparisons in education [30], sports [7], and gaming [13]. The input is a set of pair-wise comparisons or matches with known binary outcomes (which entity wins), and the output is the rank score for each entity. As baseline, we will use the Bradley-Terry-Luce model ( BTL ) [3, 25], which shares a similar sigmoid-based probability of winning as our model. This method has not been employed for the task of mining comparisons from text, but we include it as a pseudo-baseline for completeness.
Different from comparison mining, opinion mining focuses on opinions or sentiments on individual products [14]. It is frequently decomposed into two sub-tasks, namely: identi-fying the aspect or feature being described [15], as well as determining the sentiment expressed [24]. These are gen-erally modeled as classification problems, but there are also unsupervised variants based on topic modeling [28, 4]. Other than sentiments, there are also works focusing on modeling the correlation betweeen topics and user ratings [32].
Comparative summarization addresses the problem of sum-marizing two (or more) separate corpora in terms of a com-parison. This is a different setting from ours, where each pair of entities are compared within a sentence. One formu-lation of comparative summarization is sentence alignment , which selects pairs of sentences (one from each corpus), so that each pair of sentences describes the same  X  X spect X  [31]. In some cases, it is desired that sentences within a pair are contrastive [20, 29]. Another formulation of comparative summarization is comparative topic modeling , where the ob-jective is to identify different  X  X iewpoints X  of a topic [35].
Competitor mining deals with identifying the set of com-petitors of a given entity, commonly formulated as finding relationships [16] or similarities among entities [34, 22].
Our objective is to study the effectiveness of CompareGem on two tasks: comparative direction classification, and en-tity ranking. Our focus here is on effectiveness, rather than efficiency. All experiments complete within three minutes on a PC with Intel Core i5 CPU 3.2 GHz and 4GB RAM.
The corpus of comparative sentences S can be obtained from text corpora that contain user evaluation of pairs of products, such as online reviews. We crawled reviews from the Digital Cameras category of Amazon . Not all sentences within the reviews are comparative. It is not our objec-tive to develop a new method to process reviews into com-parative sentences. Below, we describe a methodology that we have used for extracting comparative sentences from re-views, which is followed by manual inspection to ensure a high quality of the dataset. There are three key information that we need to determine: whether a sentence is compara-tive, the entities being compared, and the aspect of interest.
Entity Recognition. Finding the mentions of objects of a particular type (e.g. cameras, laptops) in text is called named entity recognition (NER). There is no ready-made NER system for the domain we are considering (digital cam-
Aspect # sentences #1 is #2 is Functionality 457 38 . 5 61 . 5 Form Factor 78 61 . 3 38 . 7 Image Quality 129 58 . 1 41 . 9 Price 165 52 . 1 47 . 9 Table 2: Dataset Size involving 180 Digital Cameras eras). Therefore, we employ a dictionary matching approach for entity recognition that we find works well in tying the mentions of an object together. We construct the dictionary of entities from product titles, which we employ to perform token-based partial matching search. We then train a deci-sion tree classifier to filter out false positives.
Comparative Sentence &amp; Aspect Identification. Our scope covers sentences that contain two product mentions. We are only concerned with sentences that express a preference for the first mentioned product or for the second one. In addi-tion, we pick the four most frequent aspects mentioned in the reviews, namely: functionality , form factor , image quality , and price . To identify whether a sentence is a comparative, and the aspect of interest, we employ a supervised classi-fication approach. To train the respective SVM classifiers for comparative sentence identification and aspect identifi-cation, we take 1000 randomly selected sentences, and man-ually annotate them. We apply this classifier to the rest of sentences with two product mentions. To reduce false posi-tives, the classifier labels are followed by manual inspection.
Aspect identification could be done as preprocessing step with the use of LDA [1] or any other suitable methods. In-stead, we manually annotated the sentences to guarantee a high quality of annotations and coherence with the specifi-cation benchmark (to be introduced later).

Table 2 shows the dataset sizes. In total, the number of products being compared within extracted sentences is 180. The four aspects respectively have 457, 78, 129, and 165 comparative sentences. Each aspect is a distinct instance of the problem. The distributions between the two classes (whether the first-mentioned (#1) or second-mentioned (#2) entity is favored) are relatively well-balanced. These data sizes are significant, in light of the need to carefully anno-tate the data, not just with labels, but also with ranking benchmarks (see Section 6.1.3). This is much larger than that used in the previous work on entity ranking [21]. We evaluate the performance of CompareGem on two tasks. Every evaluation is conducted in both supervised and unsu-pervised configurations. We begin with the supervised one.
Comparative Direction Classification. In the first task, all the competing algorithms are given a set of labeled (training) and a set of unlabeled (test) data. Each algorithm is required to identify the favored entity for each compara-tive sentence in the test data. One can look at this essen-tially as a binary classification problem. To measure the performance of an algorithm, we calculate its classification accuracy , i.e., the fraction of correctly classified sentences (over the total number of sentences in the test set).
Entity Ranking. In the second task, we want to assess the quality of ranking scores produced by the competing algorithms. It is not always feasible to have a ground truth in the form of a ranking list, because some pairs may not be comparable, or there may not be sufficient evidence for some pairs. Therefore, we assume that the ground truth (see Section 6.1.3) has the form of a set of entity pairs X , where the favored (higher-ranked) entity for each pair in X is known. We thus transform the ranking scores output by each algorithm into a set of ordered pairs Y , which we then compare in terms of its agreement with the ground truth X .
As evaluation metric, we express the ranking accuracy as the agreement between the ground truth X and the output Y in terms of the fraction of concordant pairs over all pairs in the intersection, expressed as a percentage. Given two sets of ordered entity pairs X and Y , where the first element in a pair is favored over the second, two pairs ( a,b )  X  X and ( a ,b 0 )  X  Y are concordant if a = a 0 and b = b 0 .

Ranking accuracy is related to Kendall X  X  tau [8], which can be defined as the number of exchanges needed in a bub-ble sort to convert one rank to the other. Kendall X  X  tau is defined for the totally ordered sets, and, in this case, its normalized value equals the inverse (1  X  ) ranking accuracy. Unlike Kendall X  X  tau, the proposed metric accepts partially ordered sets, and, thus more suitable here, as comparison makes sense only for comparable entities (see Definition 1).
Unsupervised Configuration. In the unsupervised con-figuration, no labeled data is used as input for the model. Therefore, the first task resembles clustering into two clus-ters, rather than classification. We can still use the labels to evaluate this clustering, by computing purity instead. Each cluster is  X  X lassified X  to the majority comparison direction label among sentences in that cluster. We then determine the  X  X lassification accuracy X  as before. Since our model also outputs the ranking scores, we simply determine the ranking accuracy with respect to benchmarks as before. For unsu-pervised, whether higher or lower ranking score represents  X  X etter X  is not known in advance. We check the ranking accuracy in both directions, and take the maximum value.
Because there is no single definitive ranking ground truth, we introduce two ranking benchmarks that together pro-vide a more complete picture of our performance in ranking. The first is specification benchmark , which is based on very objective hard numerical attributes. The second is crowd-sourced benchmark , which is based on the subjective opinions of many users (expressed in the comparative sentences).
Specification Benchmark. The intuition is that users X  preferences can be traced to some specific attribute of the entities. We collect product specification information from dpreview.com 2 and wikipedia.com. For form factor , we say that entity e i is better than e j if both the volume and weight of e i are smaller than the volume and weight of e j . For func-tionality , we consider the entity with the later release date to be better, assuming that the newer model is more func-tional than the older one (comparison is done only within product lines, e.g., EOS 50D and EOS 60D . To ensure that the functionality of products has indeed changed, we only consider differences of more than one year. For price , we consider the entity with the lower price to be better. To be conservative against price fluctuations, we only consider differences of more than 1000USD. The specification bench-marks contain 291 entity pairs for functionality , 5836 pairs for form factor , and 1479 pairs for price .
Digital Photography Review has a large database with de-tailed information about individual digital cameras.
It is not our intention to claim that specification bench-mark alone is sufficient validation, which is why we consider a second unrelated benchmark below. Rather, we use spec-ification benchmark to show that CompareGem is able to recall objective information from texts written by users. In addition, this specification benchmark is independent of the data. Doing well on this benchmark provides some confi-dence that the method may do well for other aspects without suitable specification benchmark, e.g., image quality .
Crowdsourced Benchmark. This benchmark is cre-ated from the complete set of labels used for comparative direction classification (the first task). For each pair of enti-ties, we consider each comparative sentence to vote based on its label. The entity with the majority votes is considered better. Therefore, this benchmark reflects how users in gen-eral rank these entities, which may not necessarily always be consistent with the specifications. The crowdsourced bench-marks contain 175 entity pairs for functionality , 53 pairs for form factor , 102 pairs for price , and 90 pairs for image qual-ity . It is smaller than specification benchmark since it is defined only for pairs that have been compared in the data.
CompareGem has two parameters: 1) the number of rank-ing scores N and 2) the ranking scale  X  (see Section 3). To select the appropriate parameter settings, we conduct a grid search across various settings of N  X  { 10 , 100 , 1000 } with 50:50 random split into training and test data. We ob-serve similar results for various aspects, and will show the results for functionality as a representative.
 Figure 3(a) shows classification accuracies. As we increase N , the general trend is that of increasing classification accu-racy. With more ranking scores, the ranking is more refined, which may help the accuracy of classification. The excep-tion is only when  X  is too high, e.g.,  X  = 3, as the sigmoid function approaches 1 or 0 very rapidly even with small dif-ferences between rank scores. This washes out the effects of latent rank scores, because the prior binomial distribution starts to play a big role and tends to concentrate ranking scores close to its mode. Meanwhile, if  X  is too low, the performance is also worse, because the model becomes in-sensitive to small changes in rank scores.

Similar trends, with similar observations, also apply to the ranking accuracies with respect to the specification bench-mark shown in Figure 3(b). To get parameter settings that balance out both classification accuracy and ranking accu-racy, we combine the two measures with harmonic mean formula ( H ( x,y ) = 2 xy x + y ), shown in Figure 3(c). Evidently, we get the best performance for the setting of N = 1000 and  X  = 1 . 0, which we will use in the following experiments.
The aim is to understand how well CompareGem tackles the classification and ranking tasks in the presence of train-ing data. We repeat every experiment 10 times on different data shuffles (cross-validation), and average all the accuracy values. The training and test data split is 50:50.
Baselines. As discussed in Section 5, for the classification task, we compare to two popular classification models: Naive Bayes ( NB ) and Support Vector Machine ( SVM ). For the ranking task, our baselines are PageRank ( PR ) and BTL . Because PR and BTL assume the comparison outcomes of sentences are known, we use the classification output from the first task, together with the training sentences as in-puts to PR and BTL respectively. For this reason, neither PageRank and BTL is a complete baseline, because they cannot operate independently from a source of comparative directions. Therefore, for ranking, we create four composite baselines from pipelining the two separate steps discussed in this section, namely: SVM+PR , NB+PR , SVM+BTL , and NB+BTL . In contrast, since CompareGem is a generative model, we simply learn the two tasks simultaneously.
For all experiments, we conduct randomization test [2] at 5% statistical significance level for the differences between methods. The best result among methods is in bold. Lower results with statistically insignificant differences are shown in italics. Regular font indicates significantly worse results.
Classification Accuracy. For the classification task, we report the accuracies of all three methods in Table 3. The clear observation is that CompareGem performs signif-icantly better than both SVM and NB for all aspects. This validates our hypothesis that jointly modeling ranking and classification helps the model do better at classifying sen-tences. While NB and SVM classify only one test example T able 4: Supervised: Ranking (Crowdsourced) at a time, CompareGem takes advantage of jointly modeling test examples, and finds label assignments maximizing the a posteriori probability of the entire test collection.
Ranking Accuracy. Table 4 shows the ranking accu-racies for the crowdsourced benchmark. CompareGem has the highest ranking accuracies overall. It is better than the +PR models, which have appeared in previous literature, and the difference is statistically significant. We have also introduced +BTL models as pseudo-baselines, though they have not appeared in previous literature. CompareGem still outperforms SVM+BTL significantly in most aspects. With respect to NB+BTL , CompareGem is a shade better, but not significantly so. We hypothesize that ranking is an  X  X as-ier X  task than classification. Though SVM and NB perform significantly worse in classification at the sentence level (Ta-ble 3), at the level of entity pairs, there could be sufficient number of correctly classified sentences to get the ranking.
Table 5 shows the ranking accuracies for the specification benchmark. Against this benchmark, CompareGem still per-forms well for form factor and price . For functionality , it is slightly worse than SVM+BTL , but not statistically signif-icantly so. Between SVM+BTL and NB+BTL , we now see that the former performs slightly better, which is an oppo-site trend to the crowdsourced benchmark. This is because the two benchmarks are made from two independent sources of information. There are cases when they disagree. For example, based on the specification benchmark, the newer model of Canon Powershot G10 is better than the older model Canon Powershot G2 . However, based on the crowdsourced benchmark, customers actually favor the older model Canon Powershot G2 . Overall, the two bench-marks are quite consistent, as shown by CompareGem  X  X  high ranking accuracies for both benchmarks.
Baselines. In the unsupervised configuration, we do not use any labeled data to classify sentences and induce prod-uct ranking. As baseline, we will use K-means clustering ( K = 2) to perform the first task. For ranking, we deter-mine the comparative direction of each cluster based on the majority label, which we put into PR and BTL , creating two composite baselines: K-means+BTL and K-means+PR .
 Purity. Table 6 shows purity or  X  X lassification accuracy X . Again, it shows that CompareGem is significantly better than the baseline K-means . To give further insight, we also report results for majority baseline ( Majority ) whereby all test examples go into the most frequent class. Interestingly, K-means does not always outperform Majority .
 T able 7: Unsupervised: Ranking (Crowdsourced)
Ranking Accuracy. Table 7 and Table 8 show the rank-ing accuracies for the crowdsourced and specification bench-marks respectively. In both tables, CompareGem tends to have the highest accuracies, except for price in Table 7 (where CompareGem is the second, but not significantly worse). CompareGem  X  X  outperformance is statistically sig-nificant in the specification benchmark for all aspects, and in the crowdsourced benchmark for image quality aspect.
Comparing the results of supervised vs. unsupervised con-figurations, we see that the unsupervised results are indeed lower, as expected. Interestingly, the absolute classification and ranking accuracy values are still relatively good (  X  60%), which is still a reasonable performance in the case where training labels are unavailable or very difficult to obtain.
To gain further insight into the workings of CompareGem , here we investigate the features that play an important role in the supervised model. Since there are two binary classes ( c = 0 indicating the first-mentioned entity #1 in a sentence is favored, as well as c = 1 indicating the second-mentioned entity #2 is favored), we focus on features that are most discriminative between the two classes. A discriminative feature w is one whose conditional probability P( c | w )  X  0 . 8.
In Table 9, we show the top five features most frequent among sentences assigned to each class, for various aspects. For each feature, #1 and #2 refer to the relative positions of the first-and second-mentioned entities, with respect to a word. For functionality , the top feature for c = 0, is  X #1 from #2 X , while that for c = 1 is  X  X rom #1 #2 X . These in-volve the same word  X  X rom X , with different relative positions with respect to the entities. This underlines the importance of the bag-of-features model (see Section 3), as words alone are probably uninformative as features ( X  X rom X  appears in both classes). A similar case exists for image quality , with  X #1 better #2 X  (for c = 0) vs.  X #1 #2 better X  (for c = 1).
Other than their relative positions, the actual words that help make up a feature also matter. Interestingly, for form factor , we see contrasting features such as  X #1 lighter #2 X  (for c = 0) vs.  X #1 heavier #2 X  (for c = 1). For price , we see  X #1 less #2 X  (for c = 0) vs.  X #1 more #2 X  (for c = 1). A spect Co mpareGem K-means+BTL K-means+PR F unctionality 7 6.7 6 7.8 5 8.4 F orm Factor 6 2.4 5 7.3 5 1.9 Pri ce 6 4.3 5 7.3 5 4.4
T able 8: Unsupervised: Ranking (Specification) ( c = 0) ( c = 1) ( c = 0) ( c = 1) ( c = 0) ( c = 1) ( c = 0) ( c = 1)
We study the problem of comparative relation mining, and propose CompareGem as a generative model for comparative sentences. The key insight is jointly modeling two levels of comparative relations: at the level of individual sentences as well as at the level of entity pairs. This holistic treatment of comparative relation mining is novel, and is shown to empirically outperform the previous pipelined approaches.
CompareGem is validated comprehensively on Amazon re-views dataset. Comparison to baselines in both supervised and unsupervised configurations show that CompareGem is especially effective for the comparative direction task at the sentence level, outperforming all the baselines significantly and decisively. For the entity ranking task, CompareGem still produces the highest ranking accuracies, but in some cases the differences to the baselines are relatively close.
This result is revelatory, suggesting that while joint mod-eling of entity ranking and sentence classification is useful for both tasks, the extents of the benefits are asymmetric. Entity ranking helps sentence classification more than the reverse. Nevertheless, these experiments still convincingly show the utility of CompareGem in terms of the two tasks, as well as the flexibility of CompareGem in dealing with both supervised and unsupervised configurations.
