 Xi Chen xichen@cs.cmu.edu Qihang Lin qihangl@andrew.cmu.edu Dengyong Zhou dengyong.zhou@microsoft.com Machine Learning Group, Microsoft Research, Redmond, WA 98052, USA In many real-world machine learning applications, ob-taining sufficient training labels is often the major ob-stacle for good performance. Due to the flourish of many online crowdsourcing services (e.g., Amazon Me-chanical Turk ), an effective way of collecting training labels is to ask a crowd of low-paid nonexpert workers for labeling. The class labels provided by the crowd could be highly noisy, so each instance has to be la-beled several times by different workers such that that we have a large chance to correctly estimate the un-derlying true label from those noisy labels. Each la-bel from the crowd usually has a certain cost (e.g., 10 cents). Given a limited amount of budget, it is impor-tant to wisely allocate the budget among instances and workers so that the overall accuracy is maximized. To tackle this problem, there are the following challenges. We need to estimate the labeling ambiguity for each instance on the fly and avoid spending much budget on fairly easy instances. On the other hand, howev-er, we also need to avoid spending much budget on few highly ambiguous instances. Our goal is to maxi-mize the overall labeling accuracy . Ideally, we should simply put those few highly ambiguous instances aside to save budget for labeling many other relatively easy instances. In addition, we also need to estimate the re-liability of each worker on the fly and allocate as many labeling tasks to reliable workers as possible. To address these challenges in budget-optimal crowd-sourcing, we start from the binary labeling task and assume that: (1) each instance is associated with an unknown probability of being positive; (2) for any giv-en instance and worker pair, the label provided by the worker is drawn from the underlying label distribu-tion of the instance. So it means that each worker is perfectly reliable. Later we will relax (2) to consider inhomogeneous workers. At a first glance, such an as-sumption may seem oversimplified and thus naive. In fact, it turns out that the budget-optimal crowdsourc-ing problem with such an assumption has been highly non-trivial. We can imagine it as a K -coin tossing problem. Each coin has a unknown head probability. We have a budget of T tosses. We sequentially choose a coin to toss according to some policy. We then ob-serve the outcome. A coin may be chosen multiple times. After the tossing budget runs out, we predict for each coin if it is biased to head or tail using all the observed outcomes. Our goal is to find a policy such that the overall prediction accuracy is maximized. To search for the optimal allocation policy, we adopt the Bayesian setting and formulate the problem into a finite-horizon Markov Decision Process (MDP). Here the Bayesian setting is necessary. We shall show that an optimal policy only exists in the Bayesian setting. Using the MDP formulation, the optimal budget al-location policy for any finite budget T can be readily obtained via the dynamic programming (DP). How-ever, DP is computationally intractable for large-scale problems since the size of the state space grows ex-ponentially in T . The existing widely-used approxi-mate policies, such as approximate Gittins index rule (Gittins, 1989) or knowledge gradient (KG) (Gupta &amp; Miescke, 1996; Frazier et al., 2008), either has a high computational cost or poor performance in our prob-lem. In this paper, we propose a new policy, called optimistic knowledge gradient , which combines the KG and the conditional value-at-risk measure (Rockafellar &amp; Uryasev, 2002). The optimistic KG is computation-ally efficient and achieves superior empirical perfor-mance. Theoretically, we prove that it is consistent, that is, when the budget T goes to infinity, the accu-racy converges to 100% almost surely.
 It is easy to extend the MDP formulation to deal with inhomogeneous workers. We introduce one parameter to characterize worker reliability and update the joint distribution of instance labeling difficulty and worker reliability on the fly using the variational approxima-tion. Then our decision process simultaneously selects the next instance to label and the next worker for la-beling the instance. The MDP framework is so flexible that we can further easily extend it to incorporate in-stance contextual information whenever they are avail-able and to handle multi-class labeling.
 In summary, the main contribution of the paper con-sists of the three folds: (1) we formulate the budget allocation in crowdsourcing into a MDP and character-ize the optimal policy using DP; (2) computationally, we propose an efficient approximate policy, optimistic knowledge gradient; (3) the MDP frameowrk can be used as a general framework to address various budget allocation problems in crowdsourcing. To better illustrate our model, we first introduce a sim-plified homogeneous worker setting for binary-labeling task. We note that such a simplification is important for investigating this problem, since the incorporation of workers X  reliability becomes rather straightforward once this simplified problem is correctly modeled (see Section 4).
 Suppose that there are K instances and each one is as-sociated with a true label Z i  X  X  X  1 , 1 } for 1  X  i  X  K . We denote the positive set by H  X  = { i : Z i = 1 } . Moreover, we characterize the labeling difficulty of each instance by  X  i  X  [0 , 1]. More precisely,  X  i can be interpreted as the percentage of the workers labels the i -th instance as positive if a large number of noiseless (perfectly reliable) workers are asked for the labeling task. Let X  X  consider a concrete example of identify-ing whether an individual is an adult (positive) or not (negative) by presenting his/her photo to workers. For an individual above 25 years old, the corresponding  X  i is close to 1; and for one below 15,  X  i is close to 0. For these individuals, it is easy to get consensus labels and thus only a few labels for each photo are enough. On the other hand, for an individual between 15 and 25,  X  will be close to 0.5 and the corresponding labeling task is difficult. We assume that the soft-label  X  consistent with the true label in the sense that Z i = 1 if and only if  X  i  X  0 . 5 and hence H  X  = { i :  X  i  X  0 . 5 } . As described in the introduction, we can model the sequential labeling process as a coin-tossing problem. Given the total budget T , at each stage 0  X  t  X  T  X  1, we choose an instance i t  X  A = { 1 ,...,K } to ac-quire its label from a random noiseless worker. Ac-cording to the meaning of  X  i t , the label we obtained, y with the parameter  X  i t . In fact,  X  i t can be viewed as the head probability of the i t -th coin/instance and the label y i t is the outcome of the coin toss at the stage t . We note that, at this moment, all workers are as-sumed to be identical so that y i t only depends on  X  i t but not on which worker gives the label. When the budget is exhausted, we need to make an inference about the true label and estimate the positive set b H . Since workers are assumed to be identical, the most straightforward way is by the majority vote . Our goal is to determine the optimal allocation sequence (a.k.a. optimal allocation policy) ( i 0 ,...,i T  X  1 ) so that over-all accuracy is maximized. Here, a natural question to ask is whether the optimal allocation policy exists and what assumptions do we need for the existence of the optimal policy. To answer this question, we provide a concrete example and motivates our Bayesian setting. 2.1. Illustration Example Let us check a toy example with 3 instances and 5 col-lected labels (Figure 1). If we only have the budget to get one more label, which instance should be chosen to label? It might be obvious that we do not have to put the remaining budget on the first instance since we are relatively more confident on what its true label should be. Thus, the problem becomes how to choose be-tween the second and third instances. In what follows, we shall show that there is no uniformly optimal poli-cy under the frequentist setting . A uniformly optimal policy can only exist in the Bayesian setting.
 Let us compute the expected improvement in accu-racy in terms of the frequentist risk (Table 1). We assume that  X  i 6 = 0 . 5 and if the number of 1 and  X  1 labels are the same for an instance, the accuracy is 0.5 based on a random guess. From Table 1, we should not label the first instance since the improvement is always 0. This coincides with our intuition. When max(  X  2  X  0 . 5 , 0 . 5  X   X  2 ) &gt; 0 . 5(1  X   X  3 ) or  X  corresponds to the blue region in Figure 1, we should choose to label the second instance. Otherwise, we should ask the label for the third one. Since the true value of  X  2 and  X  3 are unknown, a uniformly optimal policy does not exist. In constrast, if we choose the Bayesian setting with prior distribution on each  X  i we could determine the next instance for labeling by taking another expectation over the distribution of  X  i . Therefore, we adopt the Bayesian setting to formulate the budget allocation problem in crowdsourcing. 2.2. Bayesian Setup We assume that each  X  i is drawn from a known Beta prior distribution Beta( a 0 i ,b 0 i ). This can be interpreted as having a 0 i positive and b 0 i negative pseudo-labels for the i -th instance at the initial stage. In practice when there is no prior knowledge, we can simply assume a i = b Other objective priors (e.g., Jeffreys prior or reference prior) can also be adopted (Robert, 2007).
 At each stage t with Beta( a t i ,b t i ) as the current poste-rior distribution for  X  i , we choose an instance i t and acquire its label y i t  X  Bernoulli(  X  i t ). By the fac-t that Beta is the conjugate prior of the Bernoulli, the posterior of  X  i t in the stage t + 1 will be updated put { a t i ,b t i } K i =1 into a K  X  2 matrix S t , called a state matrix, and let S t i = ( a t i ,b t i ) be the i -th row of S update of the state matrix can be written in a more compact form: where e i t is a K  X  1 vector with 1 at the i t -th entry and 0 at all other entries. As we can see, { S t } Markovian process because S t +1 is completely deter-mined by the current state S t , the action i t and the obtained label y i t . It is easy to calculate the state transition probability Pr( y i t | S t ,i t ), which is the pos-terior probability that we are in the next state S t +1 if we choose i t to be label in the current state S t : and Pr( y i t =  X  1 | S t ,i t ) = 1  X  Pr( y i t = 1 | S t en this labeling process, we further define a filtration {F t } T t =0 , where F t is the  X  -algebra generated by the tion i t , i.e., the instance to label, after we observe the historical labeling results up to the stage t  X  1. Hence, i is F t -measurable. The budget allocation policy is defined as a sequence of decisions:  X  = ( i 0 ,...,i T  X  1 2.3. Accuracy Maximization At the stage T when the budget is exhausted, we need to infer the true label of each instance based on the collected labels. In particular, we need to determine a positive set H T which maximizes the conditional ex-pected accuracy conditioning on F T (i.e., minimizing the posterior risk): where 1 (  X  ) is the indicator function. We first observe that, for 0  X  t  X  T , the conditional distribution  X  |F t is exactly the posterior distribution Beta( a t i ,b which depends on the historical sampling results only through S t i = ( a t i ,b t i ). Hence, we define I ( a,b ) = Pr(  X   X  0 . 5 |  X   X  Beta( a,b )) , (4) i = Pr( i  X  H As shown in (Xie &amp; Frazier, 2012), the final positive set H T can be determined by the Bayes decision rule. Proposition 2.1 H T = { i : P T i  X  0 . 5 } solves (3) and the expected accuracy on RHS of (3) can be written as P According to the next corollary with the proof in Ap-pendix, we show that the construction of H T is based on the majority vote .
 Corollary 2.2 I ( a,b ) &gt; 0 . 5 if and only if a &gt; b and I ( a,b ) = 0 . 5 if and only if a = b . Therefore, H { i : a T i  X  b T i } solves (3) .
 By viewing a 0 i and b 0 i as pseudo-counts of 1s and  X  1s, a i and b estimated positive set H T = { i : a T i  X  b T i } consists of instances with more (or equal) counts of 1s than that of  X  1s. When a 0 i = b 0 i , H T is constructed exactly according to the majority vote rule.
 To find the optimal allocation policy which maximizes the expected accuracy, we need to solve the following optimization problem: where E  X  represents the expectation taken over the policy  X  . The second equality is due to Proposition 2.1 and V ( S 0 ) is called value function at the initial state S 0 . The optimal policy  X   X  is any policy  X  that attains the supremum in (6). 2.4. MDP and Optimal Policy To solve the optimization problem in (6), we formulate it into a Markov Decision Process (MDP). To do so, we use the technique from (Xie &amp; Frazier, 2012) to decompose the final expected accuracy as a sum of stage-wise rewards as shown in the next proposition. Note that the problem in (Xie &amp; Frazier, 2012) is an infinite-horizon one which optimizes the stopping time while our problem is finite-horizon since the labeling procedure must be stopped at the stage T .
 Proposition 2.3 Define the stage-wise expected re-ward as: then the value function (6) becomes: where G 0 ( S 0 ) = P K i =1 h ( P 0 i ) and the optimal policy  X  is any policy  X  that attains the supremum.
 Since the expected reward (7) only depends on S t i ( a use them interchangeably. As a function on R 2 + , R ( a,b ) has an analytical representation. In fact, for any state ( a,b ) of a single instance, the reward of get-ting a label 1 and a label  X  1 are: The expected reward R ( a,b ) = p 1 R 1 + p 2 R 2 with p 1 With Proposition 2.3, the maximization problem (6) is formulated as a T -stage MDP (8), which is associ-ated with a tuple { T, {S t } , A , Pr( y i t | S t ,i t ) ,R ( S Here, the state space at the stage t , S t , is all possible states that can be reached at t . Once we collect a label y , one element in S t (either a t i Therefore, we have The action space is the set of instances that could be labeled next: A = { 1 ,...,K } . The transition proba-bility Pr( y i t | S t ,i t ) is defined in (2) and the expected reward at each stage R ( S t ,i t ) is defined in (7). More-over, due to the Markovian property of { S t } , it is e-nough to consider a Markovian policy (Powell, 2007) where i t is chosen only based on the state S t . With the MDP in place, we can apply dynamic pro-gramming (DP) algorithm (Puterman, 2005; Powell, 2007) (a.k.a. backward induction) to to compute the optimal policy according to the Bellman equation. Al-though DP can identify the optimal policy, its compu-tation is intractable since the size of the state space |S t | grows exponentially in t according to (11). There-fore, we need some computationally efficient approxi-mate policies. In this section, we first review some existing approx-imate policies to better motivate the proposed new policy named optimistic knowledge gradient .
 3.1. Existing Approximate Policies The simplest approximate policy is the uniform sam-pling (a.k.a, pure exploration), i.e., we choose the next instance to label uniformly and independently at ran-dom: i t  X  Uniform(1 ,...,K ). However, this policy does not explore the structure of the problem.
 With the decomposed reward function, our problem is essentially a finite-horizon Bayesian multi-armed bandit (MAB) problem. Gittins (1989) showed that Gittins index is an optimal policy for infinite-horizon MAB with the discounted reward. Since our prob-lem is finite-horizon, Gittins index is no longer opti-mal while it can still provide us a good heuristic index rule. However, the computational cost of Gittins in-dex is very high. To compute finite-horizon Gittins index for our problem, the approximate method (i.e., calibration method (Gittins, 1989; Nino-Mora, 2011)) requires O ( T 3 ) time and space complexity; while the state-of-the-art exact method (Nino-Mora, 2011) re-quires O ( T 6 ) time and space complexity.
 A computationally more attractive policy is the knowl-edge gradient (KG) (Frazier et al., 2008). It is essen-tially a single-step look-ahead policy, which greedily selects the next instance with the largest expected re-ward: As we can see, this policy corresponds to the first step in DP algorithm and hence KG policy is optimal if only one labeling chance is remaining.
 When there is a tie, if we select the one with the s-mallest index, the policy is referred to deterministic KG ; while if we randomly break the tie, the policy is referred to randomized KG . Although KG has been successfully applied to many MDP problems (Powell, 2007), it will fail in our problem as shown in the next proposition with the proof in Appendix.
 Proposition 3.1 Assuming that a 0 i and b 0 i are posi-tive integers and letting E = { i : a 0 i = b 0 i } , then the deterministic KG policy will acquire one label for each item in E and then consistently obtain the label for the first item even if the budget T goes to infinity. According to Proposition 3.1, the deterministic KG is NOT a consistent policy , where the consistent policy refers to the policy that will achieve 100% accuracy almost surely when T goes to infinity. We note that randomized KG policy can address this problem. How-ever, from the proof of Proposition 3.1, randomized KG behaves similar to the uniform sampling policy in many cases and its empirical performance is undesir-Algorithm 1 Optimistic Knowledge Gradient
Input: Parameters of prior distributions for in-stances { a 0 i ,b 0 i } K i =1 and the total budget T . for t = 0 ,...,T  X  1 do end for
Output: The positive set H T = { i : a T i  X  b T i } . able. In the next subsection, we will propose a new approximate allocation policy. 3.2. Optimistic Knowledge Gradient The stage-wise reward R ( a,b ) can be viewed as a ran-dom variable with a two point distribution, i.e., with the probability p 1 = a a + b of being R 1 ( a,b ) and the probability p 2 = b a + b of being R 2 ( a,b ). The KG pol-icy selects the instance with the largest expected re-ward. However, it is not consistent. A simple idea is to select the instance based on the optimistic out-come of the reward, i.e., the instance with the largest R + ( a,b ) = max( R 1 ( a,b ) ,R 2 ( a,b )). The policy, which is named as optimistic knowledge gradient , is presented in Algorithm 1.
 Theoretically, the optimistic KG policy is consistent in our problem as shown in the next theorem with the proof in Appendix.
 Theorem 3.2 Assuming that a 0 i and b 0 i are positive integers, the optimistic KG is a consistent policy, i.e, as T goes to infinity, the accuracy will be 100% (i.e., H T = H  X  ) almost surely.
 Computationally, the optimistic KG has the time com-plexity O ( KT ) and space complexity O ( K ), both of which are smaller in magnitude than that of the ap-proximate Gittins index rule.
 The proposed optimistic KG is motivated by a more general framework, called conditional value-at-risk (CVaR) (Rockafellar &amp; Uryasev, 2002). In particular, for a random variable X with the support X (e.g., our random reward with the two point distribution), let  X  -quantile function be denoted as Q X (  X  ) = inf { x  X  X  :  X   X  F X ( x ) } , where F X (  X  ) is the CDF of X . The value-at-risk is defined as: VaR  X  ( X ) = Q X (1  X   X  ) and the conditional value-at-risk, CVaR  X  ( X ), is the expected reward exceeding (or equal to) VaR  X  ( X ). As shown in (Rockafellar &amp; Uryasev, 2002), CVaR  X  ( X ) can be expressed as:
CVaR  X  ( X ) = max
In our problem, when  X  = 1, CVaR  X  ( X ) = p 1 R 1 + p R 2 , which is the expected reward; when  X   X  0, CVaR  X  ( X ) = max( R 1 ,R 2 ), which is used as the selec-tion criterion in optimistic KG. In fact, a more general policy could be selecting the next instance with the largest CVaR  X  ( X ) with a tuning parameter  X   X  [0 , 1]. We can extend Theorem 3.2 to prove that the policy based on CVaR  X  ( X ) is consistent for any  X  &lt; 1. S-ince our MDP formulation is essentially a multi-armed Bayesian bandit (MAB), the proposed optimistic KG and CVaR based KG could be adopted as general in-dex policies for solving Bayesian MAB. In many crowdsourcing applications, it is important to model worker reliability. Assuming that there are M workers, we can capture the reliability of the j -th worker by introducing an extra parameter  X  j  X  [0 , 1] as in (Dawid &amp; Skene, 1979; Raykar et al., 2010; Karg-er et al., 2012), which is defined as the probability of getting the same label as the one from a random noise-less (perfectly reliable) worker. Let Y i be the label for the i -th instance from a random noiseless worker and Z ij be the label provided by the j -th worker for the i -th instance. Then  X  j = Pr( Z ij = Y i | Y i ) and This model is often called one-coin model. We note that the previous simplified model is a special case of the one-coin model with  X  j = 1 for all j , i.e., assuming that every worker is perfectly reliable .
 We assume that  X  j is also drawn from a Beta prior distribution:  X  j  X  Beta( c 0 j ,d 0 j ). At each stage t , we need to make the decision on both the next instance i to be labeled and the next worker j to label the instance i (we omit t in i,j here for notation simplic-ity). In other words, the action space A = { ( i,j ) : ( i,j )  X  { 1 ,...,K } X { 1 ,...,M }} . Once the decision is made, we observe the label 1 with the probabili-ty Pr( Z ij = 1 |  X  i , X  j ) =  X  i  X  j + (1  X   X  i )(1  X   X   X  1 with Pr( Z ij =  X  1 |  X  i , X  j ) = (1  X   X  i )  X  j +  X  which is the transition probability. Although the likeli-hood Pr( Z ij = z |  X  i , X  j ) ( z  X  X  X  1 , 1 } ) can be explicitly written out, the product of the Beta priors of  X  i and  X  j is no longer the conjugate prior of our likelihood and we need to approximate posterior distribution. In particular, we adopt the variational approximation by assuming the conditional independence of  X  i and  X  j : p (  X  i , X  j | Z ij = z )  X  p (  X  i | Z ij = z ) p (  X  j | Z further approximate p (  X  i | Z ij = z ) and p (  X  j | Z ij by two Beta distributions whose parameters are com-puted using the moment matching. Due to the Be-ta distribution approximation of p (  X  i | Z ij = z ), the reward function takes a similar form as in the previ-ous setting and the corresponding approximate policies (e.g., KG, optimistic KG) can be directly applied. The detailed derivations and the optimistic KG algorithm with worker reliability are provided in Appendix. We can further extend it to a more complex two-coin model (Dawid &amp; Skene, 1979; Raykar et al., 2010) by introducing a pair of parameters (  X  j 1 , X  j 2 ) to model the j -th worker X  X  reliability:  X  j 1 = Pr( Z ij = Y i | Y and  X  j 2 = Pr( Z ij = Y i | Y i =  X  1). Our MDP formulation is a general framework to ad-dress many complex settings of sequential budget allo-cation problems in crowdsourcing. In particular, when the feature information x i for each instance i is avail-able, we could utilize it by assuming  X  i =  X  (  X  w , x i N (  X  0 ,  X  0 ). The posterior  X  t +1 and  X  t +1 can be up-dated using the Laplace method as in Bayesian logistic regression (Bishop, 2007).
 In multi-class labeling problems, the i -th instance is associated with a probability vector  X  i = (  X  i 1 ,... X  iC where  X  ic is the probability that the i -th instance be-longs to the class c and P C i =1  X  ic = 1. By generalizing Beta distribution to the multivariate case, we assume that  X  i has a Dirichlet prior  X  i  X  Dir(  X  0 i ). Then we can formulate the problem into a Bayesian MDP and apply the optimistic KG. We can further use Dirichlet distribution to model worker reliability as in (Liu &amp; Wang, 2012). The detailed derivations of these exten-sions are presented in Appendix. To address new challenges in crowdsourcing problems, many research work has been done. Most of them are solving a static problem, i.e., inferring true la-bels and worker reliability based on a static labeled dataset (Dawid &amp; Skene, 1979; Raykar et al., 2010; Liu &amp; Wang, 2012; Welinder et al., 2010; Whitehill et al., 2009; Bachrach et al., 2012; Zhou et al., 2012; Liu et al., 2012). The first work that incorporates di-versity of worker reliability is (Dawid &amp; Skene, 1979), which uses EM to perform the point estimation on both worker reliability and true class labels. Based on that, Raykar et al. (2010) extended (Dawid &amp; Skene, 1979) into Bayesian framework and Liu &amp; Wang (2012) further introduced Dirichlet prior for modeling work-er reliability in multi-class settings. Our work utilizes the modeling techniques in these two static models as basic building blocks but extends to dynamic budget allocation settings.
 In recent years, there are several works that have been devoted into online learning or budget alloca-tion in crowdsourcing (Karger et al., 2012; Ertekin et al., 2012; Yan et al., 2011; Pfeiffer et al., 2012). The method proposed in (Karger et al., 2012) is based on the one-coin model. In particular, it assigns in-stances to workers according to a random bipartite ( l,r )-regular graph. Although the error rate method is proved to achieve the minimax rate, its analysis is asymptotic and method is not optimal when the bud-get is limited. For other methods, Pfeiffer et al. (2012) failed to model the worker reliability in the allocation process. Yan et al. (2011) required the feature infor-mation for the decision problem. Basically, none of the existing methods has characterized the optimal al-location policy for finite budget.
 We also note that the budget allocation in crowdsourc-ing is fundamentally different from noisy active learn-ing (Settles, 2009; Nowak, 2009). Active learning usu-ally does not model the variability of labeling diffi-culties and assumes a single (noisy) oracle; while in crowdsourcing, we need to model both labeling dif-ficulties for instances and different worker reliability. Secondly, active learning requires the feature vectors for the decision, which could be unavailable for crowd-sourcing. Finally, the goal of the active learning is to label as few instances as possible to learn a good classifier. In contrast, for budget allocation in crowd-sourcing, the goal is to infer the true labels for as many instances as possible. We conduct empirical study to compare our optimistic KG (Opt-KG) policy with several competitors as fol-lows. For all experiments, we start from the uniform prior Beta(1 , 1) = Unif[0 , 1] for each  X  i . 1. Uniform: Uniform sampling. 2. Gittins: Approximate finite-horizon Gittins in-3. Gittins-Inf (Xie): Another policy proposed in (X-4. KG / KG(Random): Deterministic KG or random-5. KOS: The randomized budget allocation algorith-6. BP: Random sampling with the labeling aggrega-We note that both Gittins and Gittins-Inf polices can not be applied when the worker reliability is incorpo-rated as in Section 4. 7.1. Simulated Study We first test the Opt-KG policy for the basic setting where labels are aggregated via majority vote without incorporating worker reliability. In particular, we as-sume K = 50 and generate 20 different sets of {  X  i } K i =1 We vary the total budget T = 2 K, 3 K,..., 10 K , and report the mean and standard deviation of the accura-cy over different sets of {  X  i } in Figure 2(a). The x -axis is the ratio between the budget T and the number of instances K . We note that since each  X  i is generat-ed from uniform prior, the variance of the accuracy is quite large. For better visualization, the deviation in Figure 2 is 0.2 standard deviation. For KG policy, we only plot the accuracy for the randomized policy since we have proved that the deterministic KG will con-sistently sample one instance in Proposition 3.1. As we can see from Figure 2(a), our method and infinite-horizon Gittins perform better than the other three policies as the budget level increases. Although the infinite-horizon Gittins index performs slightly better than our method, it requires solving a linear system with O ( T 2 ) variables at each stage, which could be too expensive for large-scale applications. While our Opt-KG policy has a time complexity linear in KT and space complexity linear in K , which is much more efficient when a quick online decision is required. We also simulate worker reliability  X  j  X  Beta(4 , 1) for j = 1 ,..., 10 and compare different policies in Figure 2(b) over 20 simulations. As we can see, Opt-KG still performs the best. We also point out that, under the one-coin model, the deterministic KG policy will no longer only sample one instance as T goes large and becomes a reasonably good policy. 7.2. Real Data We compare different policies on a standard real dataset for recognizing textual entailment (RTE) (Sec-tion 4.3 in (Snow et al., 2008)). There are 800 in-stances and each instance is a sentence pair. Each sentence pair is presented to 10 different workers to acquire binary choices of whether the second hypoth-esis sentence can be inferred from the first one. There are in total 164 different workers. We first consider our simpler setting without incorporating the diversi-ty of workers. Therefore, once we decide to label an instance, we randomly choose an worker (who provides the label in the full dataset) to acquire the label. Due to this randomness, we run each policy 20 times and report the errorbar of the accuracy in Figure 3(a). As we can see, Opt-KG and infinite-horizon Gittins index policy still perform better than the others. Note that, different from the simulated study, the deviation in er-ror bar here is the standard deviation. We omit the finite-horizon Gittins index rule due to its unaffordable computational complexity.
 When the worker reliability is incorporated, we com-pare different policies in Figure 3(b). We put Beta(4 , 1) prior distribution for each  X  j which indi-cates that we have the prior belief that most workers perform reasonably well and the averaged accuracy is 4 / 5 = 80%. As we can see, the accuracy of Opt-KG is much higher than that of other policies when T is small. It achieves the highest accuracy of 92 . 25% only using 40% of the total budget (i.e., on average, each instance is only labeled by 4 times). Another interest-ing observation is that when the budget is very large (e.g., more than 8K), other policies (e.g., KG, Ran-dom+BP) achieve slightly higher accuracy than Opt-KG. This is mainly due to the restrictiveness of the real experimental setting. In particular, since the ex-periment is conducted on a fixed dataset, the Opt-KG cannot freely choose instance-worker pairs especially when the budget goes up (i.e., the action set is greatly restricted). Comparing Figure 3(b) to 3(a), we also observe that the Opt-KG policy under the one-coin model performs much better than the Opt-KG with the majority vote, which indicates that it is beneficial to incorporate worker reliability.
 The authors would like to thank Qiang Liu for shar-ing the code of KOS and BP; and Jing Xie and Peter Frazier for sharing their code for infinite-horizon Git-tins index. The authors would also like to thank Leon Bottou, Jenn Wortman Vaughan and Chien-Ju Ho for helpful discussions. This work was done when the first two authors were interned at Microsoft Research. Bachrach, Y., Minka, T., Guiver, J., and Graepel, T. How to grade a test without knowing the answers -a
Bayesian graphical model for adaptive crowdsourc-ing and aptitude testing,. In ICML , 2012.
 Bishop, C. M. Pattern Recognition and Machine Learning . Springer, 2007.
 Dawid, A. P. and Skene, A. M. Maximum likelihood estimation of observer error-rates using the em al-gorithm. JRSS-C , 28:20 X 28, 1979.
 Ertekin, S., Hirsh, H., and Rudin, C. Wisely using a budget for crowdsourcing. Technical report, MIT, 2012.
 Frazier, P., Powell, W. B., and Dayanik, S. A knowledge-gradient policy for sequential informa-tion collection. SIAM J. Control Optim. , 47(5): 2410 X 2439, 2008.
 Gittins, J. C. Multi-armed Bandit Allocation Indices. John Wiley &amp; Sons, 1989.
 Gupta, S. S. and Miescke, K.J. Bayesian look a-head one stage sampling allocations for selection the largest normal mean. J. of Stat. Planning and In-ference , 54(2):229 X 244, 1996.
 Karger, D. R., Oh, S., and Shah, D. Budget-optimal task allocation for reliable crowdsourcing systems. arXiv:1110.3564v3, 11 2012.
 Liu, C. and Wang, Y. M. Truelabel + confusions: A spectrum of probabilistic models in analyzing mul-tiple ratings. In ICML , 2012.
 Liu, Q., Peng, J., and Ihler, A. Variational inference for crowdsourcing. In NIPS , 2012.
 Nino-Mora, J. Computing a classic index for finite-horizon bandits. INFORMS Journal on Computing , 23(2):254 X 267, 2011.
 Nowak, R. D. Noisy generalized binary search. In NIPS , 2009.
 Pfeiffer, T., Gao, X. A., Mao, A., Chen, Y., and Rand, D. G. Adaptive polling for information aggregation. In AAAI , 2012.
 Powell, W. B. Approximate Dynamic Programming: solving the curses of dimensionality . John Wiley &amp; Sons, 2007.
 Puterman, M. L. Markov Decision Processes: Discrete Stochastic Dynamic Programming . Wiley, 2005.
 Raykar, V. C., Yu, S., Zhao, L. H., Valadez, G. H.,
Florin, C., Bogoni, L., and Moy, L. Learning from crowds. JMLR , 11:1297 X 1322, 2010.
 Robert, Christian P. The Bayesian Choice: From Decision-Theoretic Foundations to Computational Implementation . Springer Verlag, 2007.
 Rockafellar, R. T. and Uryasev, S. Conditional value-at-risk for general loss distributions. J. of Banking and Finance , 26:1443 X 1471, 2002.
 Settles, B. Active learning literature survey. Technical report, University of Wisconsin X  X adison, 2009.
 Snow, R., Connor, B. O., Jurafsky, D., and Ng., A. Y.
Cheap and fast -but is it good? evaluating non-expert annotations for natural language tasks. In EMNLP , 2008.
 Welinder, P., Branson, S., Belongie, S., and Perona, P.
The multidimensional wisdom of crowds. In NIPS , 2010.
 Whitehill, J., Ruvolo, P., Wu, T., Bergsma, J., and
Movellan, J. R. Whose vote should count more: Op-timal integration of labels from labelers of unknown expertise. In NIPS , 2009.
 Xie, J. and Frazier, P. I. Sequential bayes-optimal poli-cies for multiple comparions with a control. Techni-cal report, Cornell University, 2012.
 Yan, Y., Rosales, R., Fung, G., and Dy, J. Active learning from crowds. In ICML , 2011.
 Zhou, D., Basu, S., Mao, Y., and Platt, J. Learning from the wisdom of crowds by minimax conditional
