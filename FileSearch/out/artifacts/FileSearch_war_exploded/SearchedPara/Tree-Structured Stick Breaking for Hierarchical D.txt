 Dept. of Computer Science Structural aspects of models are often critical to obtaining flexible, expressive model families. In many cases, however, the structure is unobserved and must be inferred, either as an end in itself or to assist in other estimation and prediction tasks. This paper addresses an important instance of the structure learning problem: the case when the data arise from a latent hierarchy. We take a direct nonparametric Bayesian approach, constructing a prior on tree-structured partitions of data that provides for unbounded width and depth while still allowing tractable posterior inference. Probabilistic approaches to latent hierarchies have been explored in a variety of domains. Unsu-pervised learning of densities and nested mixtures has received particular attention via finite-depth trees [ 1 ], diffusive branching processes [ 2 ] and hierarchical clustering [ 3 , 4 ]. Bayesian approaches to learning latent hierarchies have also been useful for semi-supervised learning [ 5 ], relational learning [ 6 ] and multi-task learning [ 7 ]. In the vision community, distributions over trees have been useful as priors for figure motion [8] and for discovering visual taxonomies [9].
 In this paper we develop a distribution over probability measures that imbues them with a natural hierarchy. These hierarchies have unbounded width and depth and the data may live at internal nodes on the tree. As the process is defined in terms of a distribution over probability measures and not as a distribution over data per se, data from this model are infinitely exchangeable; the probability of any set of data is not dependent on its ordering. Unlike other infinitely exchangeable models [ 2 , 4 ], a pseudo-time process is not required to describe the distribution on trees and it can be understood in terms of other popular Bayesian nonparametric models.
 Our new approach allows the components of an infinite mixture model to be interpreted as part of a diffusive evolutionary process. Such a process captures the natural structure of many data. For example, some scientific papers are considered seminal  X  they spawn new areas of research and cause new papers to be written. We might expect that within a text corpus of scientific documents, such papers would be the natural ancestors of more specialized papers that followed on from the new ideas. This motivates two desirable features of a distribution over hierarchies: 1) ancestor data (the Figure 1: a) Dirichlet process stick-breaking procedure, with a linear partitioning. b) Interleaving two stick- X  X rototypes X ) should be able to live at internal nodes in the tree, and 2) as the ancestor/descendant relationships are not known a priori , the data should be infinitely exchangeable. Stick-breaking processes based on the beta distribution have played a prominent role in the develop-ment of Bayesian nonparametric methods, most significantly with the constructive approach to the Dirichlet process (DP) due to Sethuraman [ 10 ]. A random probability measure G can be drawn from a DP with base measure  X H using a sequence of beta variates via: We can view this as taking a stick of unit length and breaking it at a random location. We call the left side of the stick  X  1 and then break the right side at a new place, calling the left side of this new break  X  2 . If we continue this process of  X  X eep the left piece and break the right piece again X  as in Fig. 1a, assigning each  X  i a random value drawn from H , we can view this is a random probability measure centered on H . The distribution over the sequence (  X  1 , X  2 ,  X  X  X  ) is a case of the GEM distribution [ 11 ], which also includes the Pitman-Yor process [ 12 ]. Note that in Eq. (1) the  X  i are i.i.d. from H ; in the current paper these parameters will be drawn according to a hierarchical process. The GEM construction provides a distribution over infinite partitions of the unit interval, with natural numbers as the index set as in Fig. 1a. In this paper, we extend this idea to create a distribution over infinite partitions that also possess a hierarchical graph topology. To do this, we will use finite-length sequences of natural numbers as our index set on the partitions. Borrowing notation from the P  X  olya tree (PT) construction [ 13 ], let = ( 1 , 2 ,  X  X  X  , K ) , denote a length-K sequence of positive integers, i.e., k  X  N + . We denote the zero-length string as =  X  and use | | to indicate the length of  X  X  sequence. These strings will index the nodes in the tree and | | will then be the depth of node . We interleave two stick-breaking procedures as in Fig. 1b. The first has beta variates  X   X  Be (1 , X  ( | | )) which determine the size of a given node X  X  partition as a function of depth. The second has beta variates  X   X  Be (1 , X  ) , which determine the branching probabilities. Interleaving these processes partitions the unit interval. The size of the partition associated with each is given by where i denotes the sequence that results from appending i onto the end of , and 0  X  indicates that could be constructed by appending onto 0 . When viewing these strings as identifying nodes on a tree, { i : i  X  1 , 2 ,  X  X  X } are the children of and { 0 : 0  X  } are the ancestors of . The {  X  } in Eq. (2) can be seen as products of several decisions on how to allocate mass to nodes and branches in the tree: the {  X  } determine the probability of a particular sequence of children and the  X  and (1  X   X  ) terms determine the proportion of mass allotted to versus nodes that are descendants of . We require that the {  X  } sum to one. The  X  -sticks have no effect upon this, but  X  (  X  ) : N  X  R + (the depth-varying parameter for the  X  -sticks) must satisfy P  X  j =1 ln(1+1 / X  ( j  X  1)) = +  X  (see [ 14 ]). This with  X  0 &gt; 0 , X   X  (0 , 1] . The decay parameter  X  allows a distribution over trees with most of the mass at an intermediate depth. This is the  X  (  X  ) we will assume throughout the remainder of the paper. An Urn-based View When a Bayesian nonparametric model induces partitions over data, it is sometimes possible to construct a Blackwell-MacQueen [ 15 ] type urn scheme that corresponds to sequentially generating data, while integrating out the underlying random measure. The  X  X hinese restaurant X  metaphor for the Dirichlet process is a popular example. In our model, we can use such an urn scheme to construct a treed partition over a finite set of data.
 The urn process can be seen as a path-reinforcing Bernoulli trip down the tree where each datum starts at the root and descends into children until it stops at some node. The first datum lives at the root node with probability 1 / (  X  (0)+1) , otherwise it descends and instantiates a new child. It stays at this new child with probability 1 / (  X  (1)+1) or descends again and so on. A later datum stays at node with probability ( N +1) / ( N + N  X  X  +  X  ( | | )+1) , where N is the number of previous data that stopped at , and N  X  X  is the number of previous data that came down this path of the tree but did not stop at , i.e., a sum over all descendants: N  X  X  = P  X  0 N 0 . If a datum descends to but does not stop then it chooses which child to descend to according to a Chinese restaurant process where the previous customers are only those data who have also descended to this point. That is, if it has reached node but will not stay there, it descends to existing child i with probability ( N i + N i  X  X  ) / ( N  X  X  +  X  ) and instantiates a new child with probability  X / ( N  X  X  +  X  ) . A particular path therefore becomes more likely according to its  X  X opularity X  with previous data. Note that a node can be a part of a popular path without having any data of its own. Fig. 2 shows the structures over fifty data drawn from this process with different hyperparameter settings. Note that the branch ordering in a realization of the urn scheme will not necessarily be the same as that of the size-biased ordering [ 16 ] of the partitions in Fig. 1b: the former is a tree over a finite set of data and the latter is over a random infinite partition. The urn view allows us to compare this model to other priors on infinite trees. One contribution of this model is that the data can live at internal nodes in the tree, but are nevertheless infinitely exchangeable. This is in contrast to the model in [ 8 ], for example, which is not infinitely exchangeable. The nested Chinese restaurant process (nCRP) [ 17 ] provides a distribution over trees of unbounded width and depth, but data correspond to paths of infinite length, requiring an additional distribution over depths that is not path-dependent. The P  X  olya tree [ 13 ] uses a recursive stick-breaking process to specify a distribution over nested partitions in a binary tree, however the data live at infinitely-deep leaf nodes. The marginal distribution on the topology of a Dirichlet diffusion tree [ 2 ] (and the clustering variant of Kingman X  X  coalescent [ 4 ]) provides path-reinforcement and infinite exchangeability, however it requires a pseudo-time hazard process and data do not live at internal nodes. One can view the stick-breaking construction of the Dirichlet process as generating an infinite partition and then labeling each cell i with parameter  X  i drawn i.i.d. from H . In a mixture model, data from the i th component are generated independently according to a distribution f ( x |  X  i ) , where x takes values in a sample space X . In our model, we continue to assume that the data are generated independently given the latent labeling, but to take advantage of the tree-structured partitioning of Section 2 an i.i.d. assumption on the node parameters is inappropriate. Rather, the distribution over the parameters at node , denoted  X  , should depend in an interesting way on its ancestors {  X  0 : 0  X  } . A natural way to specify such dependency is via a directed graphical model, with the requirement that edges must always point down the tree. An intuitive subclass of such graphical models are those in which a child is conditionally independent of all ancestors, given its parents and any global hyperparameters. This is the case we will focus on here, as it provides a useful view of the parameter-generation process as a  X  X iffusion down the tree X  via a Markov transition kernel that can be essentially any distribution with a location parameter. Coupling such a kernel, which we denote T (  X  i  X   X  ) , with a root-level prior p (  X   X  ) and the node-wise data distribution f ( x |  X  ) , we have a complete model for infinitely exchangeable tree-structured data on X . We now examine a few specific examples. Generalized Gaussian Diffusions If our data distribution f ( x |  X  ) is such that the parameters can be specified as a real-valued vector  X   X  R M , then we can use a Gaussian distribution to describe the parent-to-child transition kernel: T norm (  X  i  X   X  ) = N (  X  i |  X   X  ,  X ) , where  X   X  [0 , 1) . Such a kernel captures the simple idea that the child X  X  parameters are noisy versions of the parent X  X , as specified by the covariance matrix  X  , while  X  ensures that all parameters in the tree have a finite marginal variance. While this will not result in a conjugate model unless the data are themselves Gaussian, it has the simple property that each node X  X  parameter has a Gaussian prior that is specified by its parent. We present an application of this model in Section 5, where we model images as a distribution over binary vectors obtained by transforming a real-valued vector to (0 , 1) via the logistic function. Chained Dirichlet-Multinomial Distributions If each datum is a set of counts over M discrete outcomes, as in many finite topic models, a multinomial model for f ( x |  X  ) may be appropriate. In this case, X = N M , and  X  takes values in the ( M  X  1) -simplex. We can construct a parent-to-child transi-tion kernel via a Dirichlet distribution with concentration parameter  X  : T dir (  X  i  X   X  ) = Dir (  X  X  ) , using a symmetric Dirichlet for the root node, i.e.,  X   X   X  Dir (  X  1 ) .
 Hierarchical Dirichlet Processes A very general way to specify the distribution over data is to say that it is drawn from a random probability measure with a Dirichlet process prior. In our case, one flexible approach would be to model the data at node with a distribution G as in Eq. (1). This means that  X   X  G where G now corresponds to an infinite set of parameters. The hierarchical Dirichlet process (HDP) [ 18 ] provides a natural parent-to-child transition kernel for the tree-structured model, again with concentration parameter  X  : T hdp ( G i  X  G ) = DP (  X G ) . At the top level, we specify a global base measure H for the root node, i.e., G  X   X  H . One negative aspect of this transition kernel is that the G will have a tendency to collapse down onto a single atom. One remedy is to smooth the kernel with  X  as in the Gaussian case, i.e., T hdp ( G i  X  G ) = DP (  X  (  X  G + (1  X   X  ) H )) . We have so far defined a model for data that are generated from the parameters associated with the nodes of a random tree. Having seen N data and assuming a model f ( x |  X  ) as in the previous section, we wish to infer possible trees and model parameters. As in most complex probabilistic models, closed form inference is impossible and we instead generate posterior samples via Markov chain Monte Carlo (MCMC). To operate efficiently over a variety of regimes without tuning, we use slice sampling [ 19 ] extensively. This allows us to sample from the true posterior distribution over the finite quantities of interest despite our model containing an infinite number of parameters. The primary data structure in our Markov chain is the set of N strings describing the current assignments of data to nodes, which we denote { n } N n =1 . We represent the  X  -sticks and parameters  X  for all nodes that are traversed by the data in its current assignments, i.e., {  X  , X  :  X  n,  X  n } . We also represent all  X  -sticks in the  X  X ull X  of the tree that contains the data: if at some node one of the N data paths passes through child i , then we represent all the  X  -sticks in the set S Slice Sampling Data Assignments The primary challenge in inference with Bayesian nonpara-metric mixture models is often sampling from the posterior distribution over assignments, as it is frequently difficult to integrate over the infinity of unrepresented components. To avoid this difficulty, we use a slice sampling approach that can be viewed as a combination of the Dirichlet slice sampler of Walker [20] and the retrospective sampler of Papaspiliopolous and Roberts [21].
 Section 2 described a path-reinforcing process for generating data from the model. An alternative method is to draw a uniform variate u on (0 , 1) and break sticks until we know what  X  the u fell into. One can imagine throwing a dart at the top of Fig. 1b and considering which  X  it hits. We would draw the sticks and parameters from the prior, as needed, conditioning on the state instantiated from any previous draws and with parent-to-child transitions enforcing the prior downwards in the tree. The pseudocode function FIND -NODE ( u , ) with u  X  Uni (0 , 1) and =  X  draws such a sample. This representation leads to a slice sampling scheme on u that does not require any tuning parameters. To slice sample the assignment of the n th datum, currently assigned to n , we initialize our slice sampling bounds to (0 , 1) . We draw a new u from the bounds and use the FIND -NODE function to determine the associated from the currently-represented state, plus any additional state that must be drawn from the prior. We do a lexical comparison ( X  X tring-like X ) of the new and our current state n , to determine whether this new path corresponds to a u that is  X  X bove X  or  X  X elow X  our current state. This lexical comparison prevents us from having to represent the initial u n . We shrink the slice sampling bounds appropriately, depending on the comparison, until we find a u that satisfies the slice. This procedure is given in pseudocode as SAMP -ASSIGNMENT ( n ). After performing this procedure, we can discard any state that is not in the previously-mentioned hull of representation.
 Gibbs Sampling Stick Lengths Given the represented sticks and the current assignments of nodes to data, it is straightforward to resample the lengths of the sticks from the posterior beta distributions where N and N  X  X  are the path-based counts as described in Section 2.
 Gibbs Sampling the Ordering of the  X  -Sticks When using the stick-breaking representation of the Dirichlet process, it is crucial for mixing to sample over possible orderings of the sticks. In our model, we include such moves on the  X  -sticks. We iterate over each instantiated node and perform a Gibbs update of the ordering of its immediate children using its invariance under size-biased permutation (SBP) [ 16 ]. For a given node, the  X  -sticks provide a  X  X ocal X  set of weights that sum to one. We repeatedly draw without replacement from the discrete distribution implied by the weights and keep the ordering that results. Pitman [ 16 ] showed that distributions over sequences such as our  X  -sticks are invariant under such permutations and we can view the SIZE -BIASED -PERM ( ) procedure as a Metropolis X  X astings proposal with an acceptance ratio that is always one.
 Slice Sampling Stick-Breaking Hyperparameters Given all of the instantiated sticks, we slice sample from the conditional posterior distribution over the hyperparameters  X  0 ,  X  and  X  : where the products are over nodes in the aforementioned hull. We initialize the bounds of the slice sampler with the bounds of the top-hat prior. Figure 3: These figures show a subset of the tree learned from the 50,000 CIFAR-100 images. The top tree only Selecting a Single Tree We have so far described a procedure for generating posterior samples from the tree structures and associated stick-breaking processes. If our objective is to find a single tree, however, samples from the posterior distribution are unsatisfying. Following [ 17 ], we report a best single tree structure over the data by choosing the sample from our Markov chain that has the highest complete-data likelihood p ( { x n , n } N n =1 |{  X  } , {  X  } , X  0 , X , X  ) . We applied our model and MCMC inference to the problem of hierarchically clustering the CIFAR-100 image data set 1 . These data are a labeled subset of the 80 million tiny images data [ 22 ] with 50,000 32  X  32 color images. We did not use the labels in our clustering. We modeled the images via 256-dimensional binary features that had been previously extracted from each image (i.e., x n  X  X  0 , 1 } 256 ) using a deep neural network that had been trained for an image retrieval task [ 23 ]. We used a factored Bernoulli likelihood at each node, parameterized by a latent 256-dimensional real vector (i.e.,  X   X  R 256 ) that was transformed component-wise via the logistic function: The prior over the parameters of a child node was Gaussian with its parent X  X  value as the mean. The covariance of the prior (  X  in Section 3) was diagonal and inferred as part of the Markov chain. We placed independent Uni (0 . 01 , 1) priors on the elements of the diagonal. To efficiently learn the node parameters, we used Hamiltonian (hybrid) Monte Carlo (HMC) [ 24 ], taking 25 leapfrog HMC steps, with a randomized step size. We occasionally interleaved a slice sampling move for robustness. Figure 4: A subtree of documents from NIPS 1-12, inferred using 20 topics. Only nodes with at least 50 For the stick-breaking processes, we used  X  0  X  Uni (10 , 50) ,  X   X  Uni (0 . 05 , 0 . 8) , and  X   X  Uni (1 , 10) . Using Python on a single core of a modern workstation each MCMC iteration of the entire model (including slice sampled reassignment of all 50,000 images) requires approximately three minutes. Fig. 3 represents a part of the tree with the best complete-data log likelihood after 4000 such iterations. The tree provides a useful visualization of the data set, capturing broad variations in color at the higher levels of the tree, with lower branches varying in texture and shape. A larger version of this tree is provided in the supplementary material. We also used our approach in a bag-of-words topic model, applying it to 1740 papers from NIPS 1 X 12 2 . As in latent Dirichlet allocation (LDA) [ 25 ], we consider a topic to be a distribution over words and each document to be described by a distribution over topics. In LDA, each document has a unique topic distribution. In our model, however, each document lives at a node and that node has a unique topic distribution. Thus multiple documents share a distribution over topics if they inhabit the same node. Each node X  X  topic distribution is from a chained Dirichlet-multinomial as described in Section 3. The topics each have symmetric Dirichlet priors over their word distributions. This results in a different kind of topic model than that provided by the nested Chinese restaurant process. In the nCRP, each node corresponds to a topic and documents are spread across infinitely-long paths down the tree. Each word is drawn from a distribution over depths that is given by a GEM distribution. In the nCRP, it is not the documents that have the hierarchy, but the topics .
 We did two kinds of analyses. The first is a visualization as with the image data of the previous section, using all 1740 documents. The subtree in Fig. 4 shows the nodes that had at least fifty documents, along with the most common authors and words at that node. The normalized histogram in each box shows which of the twelve years are represented among the documents in that node. An Figure 5: Results of predictive performance comparison between latent Dirichlet allocation (LDA) and expanded version of this tree is provided in the supplementary material. Secondly, we quantitatively assessed the predictive performance of the model. We created ten random partitions of the NIPS corpus into 1200 training and 540 test documents. We then performed inference with different numbers of topics ( 10 , 20 ,..., 100 ) and evaluated the predictive perplexity of the held-out data using an empirical likelihood estimate taken from a mixture of multinomials (pseudo-documents of infinite length, see, e.g. [ 26 ]) with 100,000 components. As Fig. 5a shows, our model improves in performance over standard LDA for smaller numbers of topics. This improvement appears to be due to the constraints on possible topic distributions that are imposed by the diffusion. For larger numbers of topics, however, it may be that these constraints become a hindrance and the model may be allocating predictive mass to regions where it is not warranted. In absolute terms, more topics did not appear to improve predictive performance for LDA or the tree-structured model. Both models performed best with fewer than fifty topics and the best tree model outperformed the best LDA model on all folds, as shown in Fig. 5b.
 The MCMC inference procedure we used to train our model was as follows: first, we ran Gibbs sampling of a standard LDA topic model for 1000 iterations. We then burned in the tree inference for 500 iterations with fixed word-topic associations. We then allowed the word-topic associations to vary and burned in for an additional 500 iterations, before drawing 5000 samples from the full posterior. For the comparison, we burned in LDA for 1000 iterations and then drew 5000 samples from the posterior [ 27 ]. For both models we thinned the samples by a factor of 50. The mixing of the topic model seems to be somewhat sensitive to the initialization of the  X  parameter in the chained Dirichlet-multinomial and we initialized this parameter to be the same as the number of topics. We have presented a model for a distribution over random measures that also constructs a hierarchy, with the goal of constructing a general-purpose prior on tree-structured data. Our approach is novel in that it combines infinite exchangeability with a representation that allows data to live at internal nodes on the tree, without a hazard rate process. We have developed a practical inference approach based on Markov chain Monte Carlo and demonstrated it on two real-world data sets in different domains. The imposition of structure on the parameters of an infinite mixture model is an increasingly important topic. In this light, our notion of evolutionary diffusion down a tree sits within the larger class of models that construct dependencies between distributions on random measures [28, 29, 18]. Acknowledgements The authors wish to thank Alex Krizhevsky for providing the image feature data. We also thank Kurt Miller, Iain Murray, Hanna Wallach, and Sinead Williamson for valuable discussions, and Yee Whye Teh for suggesting Gibbs moves based on size-biased permutation. RPA is a Junior Fellow of the Canadian Institute for Advanced Reserch.
