 Recommender systems perform much better on users for which they have more information. This gives rise to a problem of sat-isfying users new to a system. The problem is even more acute considering that some of these hard to profile new users judge the unfamiliar system by its ability to immediately provide them with satisfying recommendations, and may be the quickest to abandon the system when disappointed. Rapid profiling of new users is of-ten achieved through a bootstrapping process -a kind of an initial interview -that elicits users to provide their opinions on certain carefully chosen items or categories. This work offers a new boot-strapping method, which is based on a concrete optimization goal, thereby handily outperforming known approaches in our tests. H.2.8 [ Database Management ]: Database Applications X  Data Min-ing Algorithms collaborative filtering, new user, recommender systems, user cold start
Modern consumers are inundated with choices. Electronic re-tailers and content providers offer huge selections of products, with unprecedented opportunities to meet a variety of special needs and tastes. Matching consumers with the most appropriate products is not trivial, yet is key to enhancing user satisfaction and loyalty. This motivates the study of recommender systems, which analyze patterns of user interest in items or products to provide personalized recommendations of items that will suit a user X  X  taste.
One particular challenge that recommender systems face is han-dling new users; this is known as the user cold start problem .The quality of recommendations strongly depends on the amount of data gathered from the user, making it difficult to generate reason-able recommendations to users new to the system. In order to quan-tify this point, Fig. 1 shows how the error on Netflix test data de-creases as users provide more ratings. Users that have vested many ratings in the system, can enjoy error rates around 0.85, whereas new users, with just a few known ratings, are served with a sig-nificantly higher error rate (around 1). Yet, new users are crucial for the recommendation environment, and providing them with a good experience is essential to growing the user base of the sys-tem. Pleasing these new users is all the more challenging, as they often judge the system X  X  value based on their first few experiences. This is particularly true for systems based on explicit user feedback, where users are required to actively provide ratings to the system in order to get useful suggestions. The essence of bootstrapping a recommender system is to promote this interaction, encouraging users to invest in a low-effort initial interaction that will lead them to an immediately rewarding experience. Figure 1: The test error rate vs. number of train ratings per user on the Netflix data. Lower y -axis values represent more accurate predictions. The x -axis describes the exact number of ratings taken for each user. When the x value equals k ,we are considering only users that gave at least k ratings. For each such user, we sort the ratings in chronological order and take the first k ratings into account. Results are computed by the factorized item-item model [5].

This paper introduces a method for eliciting information from new users by asking their feedback on a few deliberately chosen items. The method involves creating a seed set of items based on optimizing a formally defined cost function, thereby handily out-performing previous approaches.
We are given ratings for m users within user set U = { 1 ,...,m and n items within item set I = { 1 ,...,n } . We reserve special indexing letters to distinguish users from items: for users u, v ,and for items i, j . A rating r ui indicates the preference by user u of item i , where high values mean stronger preference. For example, values can be integers ranging from 1 (star) indicating no interest to 5 (stars) indicating a strong interest. Usually the data is sparse and the vast majority of ratings are unknown. For example, in the Netflix data, 99% of the possible ratings are missing because a user typically rates only a small portion of the movies. We distinguish predicted ratings from known ones, by using the notation  X  r the predicted value of r ui . The set of users rating item i (in the training set) is denoted by R( i ) .

Our test bed is a large movie rating dataset released by Net-flix as the basis of a well publicized competition [1]. The dataset contains more than 100 million date-stamped ratings performed by about 480,000 anonymous Netflix customers on 17,770 movies between Nov 11, 1999 and Dec 31, 2005. Ratings are integers ranging between 1 and 5. We evaluate our methods on a sepa-rate test set ( T ) designed by Netflix, which contains over 2.8 mil-lion ratings (also known as the  X  X ualifying set X ). The quality of the results is measured by their root mean squared error (RMSE)
A popular approach to building recommender systems is Col-laborative Filtering (CF), a term coined by the developers of the first recommender system -Tapestry [2]. CF relies only on past user behavior, e.g., their previous transactions or product ratings. It analyzes relationships between users and interdependencies among products, in order to identify new user-item associations. A major appeal of CF is that it is domain free, yet it can address aspects of the data that are often elusive and difficult to profile.
The two primary areas of CF are neighborhood methods and la-tent factor models . Neighborhood methods compute the relation-ships between items or, alternatively, among users. The item-item approach [7, 10] evaluates the preference of a user to an item based on ratings of  X  X eighboring X  items by the same user. An item X  X  neighbors are items that tend to be scored similarly when rated by the same user. Latent factor models are an alternative approach that explains ratings by characterizing both items and users on factors inferred from the pattern of ratings. One of the most successful re-alizations of latent factor models is based on matrix factorization , e.g., [6].
When bootstrapping a recommender system, each user profile starts with no information, and gradually accumulates more rat-ings. This greatly favors CF methods that can handle incremental user input, i.e. methods that can easily accommodate new ratings entered by the user, without requiring to recompute the underlying model or any other major adaptation. Indeed, not all CF methods have this quality. For example, methods that require explicit repre-sentation of user-related parameters are less suitable, as any newly received user ratings would require modifying those parameters. Accordingly, methods that compute user-user correlations are less appropriate, because they require a constant recomputation of those correlations as the system accumulates user interactions. Similarly, the most common forms of the aforementioned matrix factoriza-tion method are less desirable, as they rely on associating a user factor vector with each user, which must change with the growing number of user ratings. On the other hand, some of the more popu-lar CF methods do not require a parameterization of user-dependent values. Among the best known examples is the item-item neighbor-hood approach, whose parameters depend solely on the items in the system. Item characteristics are usually stable and barely affected by the few ratings added by a user, thereby making the underlying item-item model robust to incremental growth of user profiles. Past efforts on item-item models focused on computing the correlations between items [7, 10]. Later works achieved significantly better performance by a more principled modeling approach. In particu-lar, we concentrate on the factorized item-item approach [5], which combines close to the best known accuracy with significant space and time-complexity improvements. This allows us to compare our methods to one of the best performing ones.
Relatively few prior works deal with handling of users new to the recommender system. An early attempt was [4]. A comprehensive study was conducted by the GroupLens team [8], and was later enhanced [9]. The main theme of these works is the construction of a specialized item set on which new users will be asked to provide their ratings. We refer further to these works in the next section.
Learning the preferences of new users can be efficiently achie-ved by a short interview during which they are asked to rate a few, carefully selected items of a seed set . One might intuitively expect the items in such a seed set to have certain properties, leading to various selection criteria which we discuss in the followings.
Popularity. A prime guideline is that the seed set should be bi-ased toward familiar items, since asking users to rate obscure items is mostly futile. Thus, a reasonable strategy is to ask the user about the most popular items, i.e., those that have received the most rat-ings in the past [8]. In the following we dub this approach Popu-larity .

Contention. Items in the seed set should not only be familiar to the users, but also indicative of their tendencies. After all, finding that a user liked an item that is also liked by everyone else, provides less insight than discovering that a user likes a more controversial item. Two common measures to quantify the contention associated with an item are the variance and entropy of its ratings [4, 8, 9]. For example, a seed set can be populated with the items of highest entropy ( Entropy ).

However, as identified in [8], maximizing a contention-related criterion with disregard to item popularity is unwise. There is lit-tle point in presenting highly controversial, yet lesser known items, which are highly indicative only with respect to a narrow group of users that are familiar with them. Furthermore, it has been claimed [8] that the widest spread in rating pattern (indicating contention) tends to be associated with those items receiving fewer ratings, while popular items are less controversial. In this sense, contention is negatively correlated with popularity.

Therefore, contention is only useful when combined with popu-larity. Among top performing methods in [9] are hybrids of pop-ularity and entropy such as Entropy0 and HELF . Another measure amplifies variance by multiplying it with the square root of the item popularity: | R( i ) | X  Var( i ) 1 . This contention measure will be henceforth dubbed Var .

Coverage. Items are useful when they possess predictive power
See a post in The Netflix Prize Forum on that mea-sure  X  http://www.netflixprize.com/community/ viewtopic.php?id=164&amp;p=1 on other items. Controversial items are not necessarily best at this. For example, within the Netflix movie rating dataset, most con-tention metrics identify Napoleon Dynamite as a highly controver-sial movie, arising much disagreement among users. Yet, it is well known that ratings of Napoleon Dynamite are very weakly corre-lated with ratings of other movies. This is an example of a movie which, despite being ranked high on the contention and familiarity axes, is less useful within the seed set, as it is not very instructive on the perceived quality of other items. We tried measuring the predic-tive ability of an item in different ways. The one we picked, which we call coverage ,isdefinedas: coverage( i )= j n ij . Here, n denotes the number of users that rated both items i and j . The intu-ition is that CF systems infer patterns across co-rated items. Hence, items that are more heavily co-rated than others allow systems to better understand users. Note that this measure also accounts for popularity, as it is biased toward items receiving many ratings.
The approaches discussed so far use various criteria to select a small seed set of items to be presented to a new user. Most of these criteria were suggested in prior works, and usually involve a combination of contention and popularity of the shown items. Yet, we would like to criticize certain aspects of these approaches: 1. Arbitrariness of selection criteria. The criteria presented 2. Independence of selected items. All above criteria score the To resolve these issues, we devise a principled approach without a need to balance between conflicting criteria.
The ultimate goal of seed set construction is to maximize user satisfaction due to the subsequent generated recommendations. The literature quantifies user satisfaction by various formal measures. Most common are convenient error metrics like the aforementioned RMSE and the closely related MAE (mean absolute error). Other useful measures evaluate the accuracy of the top-K suggestion, such as recall, precision, and area under the ROC curve. (See [3] for a discussion on measuring performance of recommender systems.) In what follows, the exact optimization metric is not important, as long as it can be computed efficiently from the test set.
Formally, let A denote the prediction algorithm, and fix the train set. Algorithm A is parameterized by the k -item seed set S .That is, it is making predictions of user preferences by considering, for each individual user, only her train ratings of the items in S .The performance measure (on the train set) is denoted by the function (we assume, without loss of generality, that the goal is minimizing this measure). We seek a seed item set S such that:
Concretely, this work considers the prediction algorithm A be the factorized item-item model [5]. The cost function that we minimize is RMSE (on train set). The seed set S is incrementally grown by a greedy algorithm, which iteratively adds to S the item: argmin i  X  X  X  S F ( A ( S  X  X  i } )) .

We call this method GreedyExtend . In comparison to the ap-proaches described in the previous section, GreedyExtend explic-itly accounts for the end goal of optimizing prediction accuracy during the construction of the seed set. It adopts a well defined and user-meaningful optimization criterion, rather than balancing and patching together multiple construction guidelines. Desired properties of the seed items  X  X ontention, popularity, coverage and list-diversity X  are a side outcome of the selection process, to the extent they actually improve the utility of the set.
In order to evaluate the performance of the selection criteria listed above, we tested them on the Netflix test data. Efficient ex-perimentation is facilitated by the good performance of the afore-mentioned factorized item-item model, which allows to quickly process new user ratings without needing to recompute the model.
For each of the discussed item ordering criteria, we ordered all movies in the Netflix data, and picked each of the first 200 prefixes (of length 1 to 200), as a set of movies to be presented to new users. For each of the 200 resulting seed sets, we predict test ratings for every single user, while accounting only for that user X  X  train ratings that intersect with the seed set. Error rates of the different criteria are depicted in Fig. 2. As expected, prediction accuracy improves (RMSE drops) as seed sets increase in size. However, the different ordering criteria fare differently. Figure 2: The test error rate vs. number of displayed items (=size of seed set), for various methods of selecting seed set items. Methods that disregard item popularity (Random and Entropy) significantly lag in performance. GreedyExtend de-livers the best performing seed sets by guiding the set creation process with a suitable cost function. Note that the legend or-ders methods by their performance. rank Movie title RMSE Seed-set size 1 The Royal Tenenbaums 0.99642 3 2 Miss Congeniality 0.99210 4 3 Pearl Harbor 0.98897 8 4 Lost in Translation 0.98625 11 5 Sweet Home Alabama 0.98383 13 6 Pulp Fiction 0.98171 15 7 The Day After Tomorrow 0.97992 18 8 Independence Day 0.97845 20 9 Maid in Manhattan 0.97694 23 10 Pretty Woman 0.97566 26 11 Gone in 60 Seconds 0.97449 28 12 Being John Malkovich 0.97344 29 13 Mr. Deeds 0.97253 30 14 Kill Bill: Vol. 1 0.97166 31 15 How to Lose a Guy in 10 days 0.97082 33 Table 1: The 15-movie seed set produced by GreedyExtend on the Netflix data, along with the test error achieved by elicit-ing ratings on each prefix of the seed set. The right-most col-umn compares GreedyExtend X  X  performance to that of its clos-est competitor -Var . The column reports the size of seed sets that Var requires in order to match GreedyExtend X  X  RMSE in each line.

Our proposed GreedyExtend method, which is the only one based on a formal cost function, consistently improves test set prediction accuracy over other methods.

As for the other methods, pure entropy, which disregards pop-ularity, performs only slightly better than a random choice, and markedly worse than the other criteria. This confirms the claims [8] on the necessity to integrate popularity with contention. While all other orderings perform much better, we observe that Coverage is under-performing the measures based on contention. This shows the value of emphasizing the more controversial items, which Cov-erage disregards. The two entropy based orderings  X  Entropy0 and HELF X  perform in line with the simple Popularity ordering. Finally, the Var ordering is the best performer among contention-based methods.

While we believe running time is not a crucial factor in a one-time preparation of a seed set, we note that GreedyExtend is signif-icantly slower to run compared to the ordering criteria mentioned in Section 4. On the Netflix data, it took 8 hours for our hard-ware to compute the seed sets of sizes 1 to 200. This is compared to an almost instantaneous execution for the other ordering crite-ria. One can adopt several strategies to accelerate running time, if necessary. First, the greedy set extension operation is highly paral-lelizable. Second, unlikely items can be pruned from the candidate item set I . For example, we empirically observed that the method avoids selecting unpopular items, which constitute the majority of the items, so their exclusion from the process significantly speeds run time.

Table 1 reports the actual ordered list of top 15 seed movies selected by GreedyExtend, together with the RMSE values corre-sponding to each prefix of the list. One familiar with the movies would recognize that they actually conform to the design criteria suggested in the previous section, despite those criteria having not been explicitly employed here. The listed movies are indeed well familiar, most are controversial and constitute a quite diverse list. Interestingly, a movie uncorrelated with others, yet popular and controversial, such as Napoleon Dynamite, did not make it at all into the top-200 list (unlike in previously described methods). For comparison we also report the sizes of seed sets produced by Var that yield comparable error rates. Note that by eliciting feedback for 10 movies, GreedyExtend achieves RMSE=0.97566, a bar that its closest competitor, Var, meets only after presenting as many as 26 movies.
Introducing a new user into a recommender system should re-quire a low effort bootstrapping process, after which the user can immediately appreciate the value provided by the system. Sys-tem designers trying to impress their new users, who might be the most judgmental ones, should look at methods providing best pre-diction accuracy at minimal distraction to the user. This usually proceeds by conducting a short interview with the user, where she is asked to evaluate certain products or categories. The prior few works dealing with the design of such an interview concentrated on several, possibly conflicting properties expected by the evaluated items. This work has shown that accuracy of the process signifi-cantly improves when it is driven, from its very beginning, by op-timizing a natural cost function. To get a feeling of the achieved improvement, an error rate that could have been previously achi-eved after soliciting 30 user evaluations, becomes achievable with just 13 user evaluations. [1] J. Bennett and S. Lanning. The Netflix Prize. Proc KDD Cup [2] D. Goldberg, D. Nichols, B. M. Oki and D. Terry. Using [3] J. Herlocker, J. Konstan, L. Terveen and J. Riedl. Evaluating [4] A. Kohrs and B. Merialdo. Improving Collaborative Filtering [5] Y. Koren. Factor in the Neighbors: Scalable and Accurate [6] Y. Koren, R. Bell and C. Volinsky. Matrix Factorization [7] G. Linden, B. Smith and J. York. Amazon.com [8] A. M. Rashid , I. Albert , D. Cosley , S. K. Lam , S. M. [9] A. M. Rashid, G. Karypis and J. Riedl. Learning Preferences [10] B. Sarwar , G. Karypis , J. Konstan and J. Riedl. Item-based
