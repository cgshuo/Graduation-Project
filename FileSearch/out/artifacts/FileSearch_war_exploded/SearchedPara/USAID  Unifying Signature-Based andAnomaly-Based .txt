 In general, there exist two approaches for intrusion detection: signature-based (a.k.a. misuse detection) and anomaly-based intrusion detection. In principle, signature-based intrusion detection (SID) works reliably on well-known intru-sions, but it is incapable of detecting new intrusions . In addition, it is also limited by several practical problems: signature updating bottleneck , intrusion variation detection (Rubin et al.[11]), and too many false alarms (Julisch[5]). search focus as it is a useful alternative to SID, being capable of detecting novel intrusions. Besides that it can not identify intrusions, AID suffers from higher false alarm rate ,the difficulty in determining whether the anomalies are caused by intrusions (Li et al.[8]), concept drifting problem ,and mimicry attacks (Wag-ner et al.[12]). Most importantly, the computational cost of intrusion detection must be reduced considerably before it can be usefully employed in practical sys-tems. This is more so as the size of the audit trails keep on growing. tion may lie in the fact that only partial knowledge is used in each approach of intrusion detection. For example, SID only uses the knowledge about well-known intrusions (e.g., Snort[10]). For AID, it is the knowledge about the normal be-haviors of the computing resources (e.g., LERAD[9], MADAM ID[7]). In our research, we try to use all the available knowledge about the behaviors ( intru-sive and normal ) in the historical data to detect intrusions.
 retical basis for intrusion detection. The basis then allows us to systematically analyze the detection performance to identify root causes for the problems in SID and AID. Secondly, the insights gained from the analysis naturally leads us to a new technique, named as USAID, which U nifies S ID and AID .
 in section 2. Then, the existing intrusion detection approaches are analyzed systematically in section 3. Section 4 describes USAID. Experimental results showing the effectiveness of USAID and related work are presented in section 5 and section 6 respectively. Lastly, we conclude the paper and layout the future work. In our theoretical basis for intrusion detection, we build behavior models like SID and AID, which use a feature vector FV = { F 1 ,F 2 ,...,F m } , where F i is a feature in the feature set. In general, a feature F i can be categorized into nominal , discrete or continuous one. For example,  X  name  X  is nominal,  X  TCP port number  X  is discrete and  X  SYN rate within 2 seconds  X  is continuous. A feature vector can contain any number of nominal, discrete, and/or continuous features. Besides that, we assume that there are training audit trails , which are constituted with labeled normal audit trails and intrusive audit trails. 2.1 Definitions For any feature F , there is a meaningful domain Dom ( F ) called the feature space. Any value occurring in the audit trails is a feature value v F . With respect to the training audit tails, a feature value v F will be labeled as normal , suspicious or anomalous . Specifically, if it only occurs in the normal audit trails, it is normal . If it only occurs in the anomalous audit trails, for example, in the intrusion signatures, it is anomalous . Otherwise, it is labeled as  X  X uspicious X  . We will refer to  X  X ormal X ,  X  X uspicious X , or  X  X nomalous X  as the NSA label of the feature value v , denoted as L ( v F )  X  X   X  N ,  X  S ,  X  A } using the first letter of the label. Feature Ranges. For any discrete/continuous feature F , the interval between v each nominal feature value is also referred to as a feature range. Thus, R F  X  Dom ( F ). The concept of NSA labels can be extended to the feature range: L ( R F )= X  N  X  X  X  v F ( v F  X  R F  X  L ( v F )= X  N ) L ( R F )= X  A  X  X  X  v F ( v F  X  R F  X  L ( v F )= X  A ) Where, all the feature values occur in the training audit trails.
 and i = j ). Furthermore, using the following rules, we can partition the feature space Dom ( F ) into three feature subspaces: N ( F ), S ( F )and A ( F ). We also define  X  ( F )= N ( F )  X  S ( F )  X  A ( F ) so that  X  ( F ) is the collection of all feature ranges found in the audit trails.
 Definition 1 (compound feature). A compound feature F 12 is an ordered pair { F 1 ,F 2 } ,and  X  ( F 12 ) is a subset of the cartesian product of  X  ( F 1 ) and  X  ( F 2 ) , such that each element in  X  ( F 12 ) actually represents at least one element in the audit trails. For the sake of expression, F 12 = F 1  X  F 2 .
 Intuitively, similar to its component features, a feature range of F 12 has an NSA label with respect to its representive feature instance(s), and the com-pound feature space can also be partitioned into three feature subspaces, i.e.,  X  ( F 12 )= N ( F 12 )  X  S ( F 12 )  X  A ( F 12 ). Note that the suspicious compound fea-ture ranges can potentially shrink with respect to component feature ranges as the combinations of two  X  X uspicious X  feature ranges may be  X  X ormal X  or  X  X noma-lous X .
 similar behaviours as any of its component atomic features. Therefore, we can treat the compound feature as an atomic one to build higher order compound features. Using this recursive procedure, the feature vector FV for intrusion detection can be converted into an equivalent n-order compound feature F 1 ...n . In USAID, each compound feature range of F 1 ...n is defined as a behavior signature in the behavior models for intrusion detection. 2.2 An Illustrative Example Let us assume that FV = { F 1 ,F 2 ,F 3 } . The example instances of the feature vector are listed below in Table 1. The feature ranges for every feature are listed as follows. For saving space, we will often use  X  X  X  and  X  X  X  respectively to denote the statuses  X  X ormal X  and  X  X ntrusion X .
 The feature subspaces are: N ( F 123 )= { R 1 F and A ( F 123 )= { R 3 F In our analysis, we will focus on a compound feature F since the feature vector for intrusion detection can be compounded into a higher-order compound feature. In the i deal scenario, which assumes hypothetical complete knowledge, the three feature subspaces are determined without any misclassification: N i ( F ), A i ( F ) and S i ( F ). Similarly, in the r eal scenario where we possess only partial knowledge of them, three feature subspaces are N r ( F ), A r ( F )and S r ( F ). rect labels), there are two defects in the behavior model: (1) model inaccuracy and (2) model incompleteness . We use the following subsets of feature ranges to quantify the inaccuracy: NA 1 ( F ), NS 1 ( F ), SN 1 ( F ), SA 1 ( F ), AN 1 ( F )and AS space, the second letter represents the ideal one, and the subscript  X 1 X  indicates that it is caused by  X  X odel inaccuracy X . As the behavior model is incomplete, there are some unknown feature ranges: N f ( F ), A f ( F )and S f ( F ). The influence of incompleteness in the behavior model is quantified as: NS 2 ( F )and AS 2 ( F ), where the subscript  X 2 X  indcates that is it caused by  X  X odel inaccuracy X . Then, where,  X + X  and  X - X  denote set union and difference operations respectively. 3.1 Performance Analysis In this subsection, we quantify and analyze the detection performance based on the detection results and the principles laid out so far. The detection performance is mainly represented by the detection rate and false alarm rate in the detection phase. In our discussion, we will assume that a fraction  X  of the feature ranges labeled as  X  X uspicious X  will be detected as  X  X nomalous X .
 Signature-Based Intrusion Detection. In SID, only A r ( F ) is known (a.k.a. the intrusion signature base). The behaviors which do not match A r ( F )are regarded as  X  X ormal X  behaviors. Therefore, its detection performance is, Where, | ... | represents the size of a set of feature ranges.
 incapability to detect new intrusions as well as intrusion variations and the signature updating problem are due to the limited size and quality of A r ( F ). AS Anomaly-Based Intrusion Detection. In AID, N r ( F i ) is known beforehand in the normal run of a process, and the behaviors that violate N r ( F i ) are regarded as  X  X nomalous X , S r ( F i )=  X  . Therefore, its detection performance is, Obviously, the higher false alarm rate is largely rooted in N f ( F )and S f ( F ). Mimicry attacks try to utilized NS 1 ( F )and NS 2 ( F ). Concept drifting prob-lem is to enlarge NS 1 ( F ), NS 2 ( F )and NA 1 ( F ). Conversely, anomaly context identification tries to shrink the above sets as well as S f ( F ).
 SID and AID. In our research, we try to utilized all available knowledege instead of the partial knowledge used in SID (i.e., A r ( F )) and AID (i.e., N r ( F )). Unifying Signature-Based and Anomaly-Based Intrusion Detection.
 In USAID, all three real feature subspaces are known in advance: N r ( F ), S r ( F ) and A r ( F ). Except the known feature ranges in all three feature subspaces, other feature ranges are detected as  X  X uspicious X  in USAID. As before, we can deduce the detection performance of USAID as follows.
 It is clear that USAID will achieve similar detection rate as AID. Like AID, if the behavior model is accurate and complete, DR = 100% and FAR =0.Onthe other hand, other than detecting anomalies, USAID can identify the intrusions in A r ( F ) as in SID. In summary, even though it is still limited by the quality issue of the behavior model, USAID provides advantages of both SID and AID. In this section, we apply USAID for intrusion detection. The architecture of USAID consists of three modules as indicated in Figure 1. The first module extracts the three feature subspaces of every feature. Module 2 is to construct the signature base. The last module incorporates the detection mechanism. 4.1 Feature Subspaces Extraction Step 1: Feature Value Collection. In the labeled training audit trails, the feature values are collected for every feature. The statuses ( normal and/or in-trusions ) of every feature value are also collected into its status list. Based on its status list, every feature value is assigned an NSA label. Note that this step is applied to nominal, discrete and continuous features, but the following two steps are only applicable to discrete and continuous features.
 Step 2: Feature Value Clustering. The objective of this step is to form initial feature ranges for every feature by clustering the neighboring feature values. For a discrete feature, two feature values x 1 and x 2 are neighboring if | x neighboring if | x 1  X  x 2 | X   X  . If several neighboring feature values have the same NSA label, they will be combined to form an initial feature range . As a special case, if, for a feature value, its neighbors have different NSA labels from itself, it forms an initial feature range itself. Every initial feature range thus formed inherits the NSA label of the feature values falling within it.
 Step 3: Feature Range Generalization. Under most scenarios, the initial feature ranges will not cover all of the feature space. Any outside feature subspace is named as an uncovered subspace . Comparing to the neighboring definition of feature values, two feature ranges are neighboring if there is no other feature range(s) between them. Then, an uncovered subspace between two neighboring feature ranges is processed as follows: if the two feature ranges have the same NSA label, a new feature range will be formed to cover the two feature ranges as well as the uncovered subspace; otherwise, the uncovered space is divided equally and allocated to these two defined feature ranges. The NSA labels of the initial feature ranges will be inherited by the newly extended or combined feature ranges. Ultimately, all the known feature space of every feature will be covered by well-defined feature ranges. 4.2 Building Behavior Signatures Initially, the NSA signature base (i.e., the behavior models) is empty. The fol-lowing procedures are performed resursively on each feature instance. First, we construct a signature by replacing the feature values in the instance with respec-tive feature ranges. Then, the signature is inserted into the NSA signature base with the status of the feature instance. Finally, based on the accumulated status list, every signature is assigned an NSA label. 4.3 Detection Mechanisms Via Signatures We first extract a feature instance from the test audit trails at a time, and the feature value of every feature is replaced by its corresponding feature range to construct a temporary signature. We then try to search for the temporary signature in the NSA signature base, and if found, the current instance is assigned the same status list as that of the stored signature. Otherwise, the detection result is  X  X nomaly X . We have chosen a typical dataset from KDD CUP 1999 contest, in which every record is an instance of a specific feature vector (Table 2). The dataset meets the requirements of USAID: labeled audit trails and a feature vector . The sizes of the dataset are as follows: training dataset: 4898431 instances, test dataset: 311029 instances . For a detailed description of the datasets, please refer to [1]. In addition, as the precision for continuous features in our experimental datasets is 0.01, we set the neighboring threshold  X  =0 . 01. 5.1 Experimental Results First, let us talk about the dataset quality. In the training dataset, there are several illegal records (e.g. instance 4817100). In the test dataset, we found several instances (whose indices are 136489 and 136497) with illegal combination between  X  TCP  X  protocol type and  X  ICMP  X  service. Therefore, they are discarded in our experiments. Moreover, there is an intrusion  X  spy  X  that is not documented properly (in instances 1381226, 1381227).
 Feature Ranges of Every Feature. In our experiments, the average numbers of normal, suspicious and anomalous feature ranges of all features in the feature vector are N : S : A = 23 : 18 : 12. Given the relatively large number of  X  X uspicious X  feature ranges, it is clear that only one feature from our selected feature vector is not enough for intrusion detection.
 The NSA Signature Base. In it, the numbers of the normal, suspicious and anomalous signatures are N : S : A = 60371 : 58 : 2779. Even though some sus-picious signatures still exist, the detection capability has been improved much in comparison to any single feature. The existence of suspicious signatures indicates that the features in our experiments are not enough to detect all known intru-sions. Note that the high ratio of N and A, namely N : A = 60371 : 2779  X  21 . 7 is quite significant since it indicates that the detection speed of SID will be faster. In addition, the total of possible signatures due to feature ranges of all 41 features are 8 . 38  X  10 33 . In contrast, our signature base is compact enough. We further sense that searching future intrusion signatures via negative selection algorithm [4] from 8 . 38  X  10 33 signatures is a mission impossible. Signature Variations of a Behavior. At the same time, we observed that most in-trusions cause more than one signature. For example, for portsweep, the number of signatures is 941; ipsweep, 72; satan, 389. The observation indicates that the intrusion variations do exist to a significant extent. In the NSA signature base, some signatures are shared by several intrusions, such as portsweep and nep-tune. Even though the number of shared signatures is small (Figure 2.(A)), this phenomenon shows that, under some scenarios (e.g., inadequate no. of features), it is difficult to identify some intrusions correctly. If the response strategies for the intrusions with overlapped signatures are much different, then generating responses to such intrusions may lead to disastrous results.
 Shared Signatures among Behaviors. Also in Figure 2.(A), we enumerate the numbers of signatures shared between different intrusion categories. The normal category will share many signatures with other intrusion categories. This is the main source of false alarms or false negatives in intrusion detection. In this table, the signatures of R2L intrusions are not shared with other intrusion categories, whereas only  X  X robe X  intrusions have shared signatures with each other category. One possible reason for this phenomenon lies in the proportions of signatures in every category, that is, normal : probe : DOS : U 2 R : R 2 L = 60432 : 1661 : 1255 : 65 : 42. In addition, the major principles of  X  X robe X  intrusions are similar to each other [6], that X  X  why the shared  X  X robe X  signatures are significant in Figure 2.(A). The strategies behind probe and DOS are similar, but it is different from the one behind U2R and R2L. Therefore, probe and DOS can be classified in one class, and U2R and R2L in another class [6]. The differences between these two classes explain why there are few signatures shared by them. Detection Results from the Test Dataset. We eliminate two new intru-sions, namely, snmpgetattack and mailbomb, from the detection results. This is because the information in the feature vector is not enough to detect these two intrusions.
 Detection Performance. Quantitatively, the false alarm rate is 1.45%, the detec-tion rate for known intrusions 99.78%, the detection rate for most new intrusions 98.18%. We also evaluate the USAID performance in comparison with the par-ticipants of KDD X 99 Classifier Learning Contest[1], in which every entry will be assigned a detection cost, and an average cost per entry is calculated for com-parison. The lower the average cost per entry is, the higher rank the classifier. the first 5 columns constitute the confusion matrix, and the last column in-cludes the numbers of detected anomalies. Its horizontal dimension is the pre-dicted class of every test example, and the vertical dimension is its actual class. In the performance comparisons, the detected anomalies will be processed in two ways. First, these anomalies are classified correctly to their actual intrusion categories. For example, the number of correctly predicated entries of  X  X robe X  is 2341+1818=4159. In such case, the performance of USAID is scored 0.1355, which is much better than the 1st rank of KDD X 99, 0.2331. Secondly, under the worst scenario, these anomalies are classified incorrectly into the intru-sion categories with highest cost. For instance, as cost(R2L,probe)=4 is high-est in row  X  X 2L X , the anomalies detected from actual  X  X 2L X  will be detected as category  X  X ormal X . The performance in the worst scenario is scored 0.3283, which is ranked 19th among all the participants. Note that almost half of the R2L intrusions, which are detected poorly in KDD X 99, are detected as anoma-lies in USAID. In summary, USAID is expected to achieve better performance than all the participants of KDD X 99 if the detected anomalies are categorized correctly. In USAID, two intrusion detection approaches are unified and their respective problems can be solved partially. The research work in [2] also shows the ef-fectiveness of this combination, in which an algorithm ( similar to the negative selection algorithm in[4] ) is proposed to generate the artificial anomalies. An in-trusion detection system is then built on the synthetic datasets. Actually, it only relies on the partial knowledge as well, and it lacks flexibility to fine-tune the model online. Other obvious advantages of USAID over [2] are the mechanisms for intrusion identification and anomaly context identification a compound feature in our general feature vector, USAID is a multiple classifier ensembler [3]. In this aspect, USAID is similar to the research work in [3], in which one classifier is used to detect known intrusions, and another classifier tries to classify the new intrusions. However, [3] depends on the assumption that the first classifier can detect known intrusions accurately. That X  X  not true since there are significant intrusion variations as shown in our experiments. In this paper, we proposed a theoretical basis for intrusion detection, in which we unified signature-based and anomaly-based intrusion detection and system-atically analyzed the hard problems faced by the researchers on intrusion detec-tion. Our experimental results have also shown that the detection performance of USAID are encouraging. Specifically, most new and known intrusions are de-tected in USAID, and the false alarm rate is 1.451%. In our future work, we will continue research on our theoretical basis for intrusion detection.
