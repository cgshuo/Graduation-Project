
University of California, Los Angeles Using Markov random fields (MRFs) one can capture global stat istical properties in large scale probabilistic networks while only explicitly modeling the interactions of neighboring sites. First in-troduced in statistical physics, MRFs and related models su ch as Boltzmann machines have proved particularly successful in computer vision and machile lea rning tasks such as image segmentation, signal recovery, texture modeling, classification, and uns upervised learning [1, 3, 5]. Drawing ran-dom samples from MRFs and juxtaposing them with real data all ows one to directly assess the model quality. Sampling of MRFs also plays an important role withi n algorithms for model parameter fit-ting [7], signal estimation, and in image analysis for textu re synthesis or inpainting [16,19,37]. The simplest but typically very slow way to draw random samples f rom MRFs is through single-site Gibbs sampling, a Markov chain Monte-Carlo (MCMC) algorith m in which one visits each node in the network and stochastically updates its state given the s tates of its neighbors [5]. Gaussian Markov random fields (GMRFs) are an important MRF cl ass describing continuous vari-ables linked by quadratic potentials [3,22,29,33]  X  see Sec . 2. They are very useful both for modeling inherently Gaussian data and as building blocks for constru cting more complex models. In this pa-per we study a technique which allows drawing exact samples f rom a GMRF in a single shot by first perturbing it and then computing the least energy configurat ion of the perturbed model. The pertur-bation involved amounts to independently injecting noise t o each of the Gaussian factors/potentials in a fully distributed manner, as discussed in detail in Sec. 3. This reduction of sampling to quadratic energy minimization allows us to employ as black-box GMRF si mulator any existing algorithm for MAP computation which is effective for a particular Gaussia n graphical model. The reliability of the most likely solution in a Gaussian mod el is characterized by the marginal vari-ances. Marginal variances also arise in computations withi n non-linear sparse Bayesian learning and compressed sensing models [11, 26, 32]. However, their comp utation can be very challenging and a host of sophisticated techniques have been developed for t his purpose, which often only apply to restricted classes of models [12, 24, 25, 28]. Being able to e fficiently sample from a GMRF makes it practical to employ the generic sample-based estimator f or computing Gaussian variances, as dis-cussed in Sec. 4. This estimator, whose accuracy is independ ent of the problem size, is particularly attractive if only relatively rough variance estimates suf fice, as is often the case in practice. Gaussian models have proven inadequate for image modeling a s they fail to capture important as-pects of natural image statistics such as the heavy tails in m arginal histograms of linear filter re-sponses. Nevertheless, much richer statistical image tool s can be built if we also incorporate into our models latent variables or allow nonlinear interactions be tween multiple Gaussian fields and thus the GMRF sampling technique we describe here is very useful with in this wider setting [10,16,19,34]. In Sec. 5 we discuss the integration of our GMRF sampling algo rithm in a block-Gibbs sampling context, where the conditionally Gaussian continuous vari ables and the conditionally independent latent variables are sampled alternately. The most straigh tforward way to capture the heavy tailed histograms of natural images is to model each filter response with a Gaussian mixture expert, thus using a single discrete assignment variable at each factor [ 16, 23]. However, our efficient GMRF algorithm can also be used in conjunction with Gaussian scal e mixture (GSM) models for which the latent scale variable is continuous [2]; we demonstrate thi s in the context of Bayesian signal restora-tion by sampling from the posterior distribution under a tot al variation (TV) prior, employing the GSM characterization of the Laplacian density. Further, ou r sampling technique also applies when the latent variables are distributed, with each hidden vari able affecting multiple experts. An inter-esting case we examine is the recently proposed factored Gau ssian restricted Boltzmann machine (GRBM) of [18], which takes into account residual correlati ons among visible units by modeling them as a multivariate GMRF, conditional on the distributed state of an adjacent layer of discrete hid-den units. We show that we can effectively replace the hybrid Monte Carlo sampler used by [18] with a block-Gibbs sampler in which the visible conditionally Ga ussian units are sampled collectively by local perturbations, potentially allowing extension of th e current patch-based model to a full-image factored GRBM, as has been recently done for the fields of inde pendent experts model [19,23]. Our GMRF sampling algorithm relies on a property of Gaussian densities (see Sec. 3) which, in a somewhat different form, has appeared before in the statist ics literature [21,22]. However, [21,22] emphasize direct matrix factorization methods for solving the linear system arising in computing the Gaussian mean, which cannot handle the large models we consi der here and do not discuss models with hidden variables. Variations of the sampling techniqu e we study here have been also used in the image modeling work of [16] and very recently of [23]. Howeve r the sampling technique in these papers is used as a tool and not studied by itself. Apart from h ighlighting the power and versatility of the efficient GMRF sampling algorithm and drawing the machin e learning community X  X  attention to it, our main novel contributions in this paper are: (1) Our in terpretation of the Gaussian sampling al-gorithm as local factor perturbation followed by mode compu tation, which highlights its distributed nature and implies that any Gaussian mean computation routi ne can be equally effectively employed for GMRF sampling; (2) the application of the efficient sampl ing algorithm in rapid sampling and variance estimation of very large Gaussian models; and (3) t he demonstration that, in the presence of hidden variables, it can be effectively integrated in a bl ock-Gibbs sampler not only in discrete but also in continuous GSM models and in conjunction not only with local but also with distributed latent assignment representations. 2.1 The linear Gaussian model We are working in the context of linear Gaussian models [20], in which a hidden vector x  X  R N is assumed to follow a prior distribution P ( x ) and noisy linear measurements y  X  R M of it are drawn with likelihood P ( y | x ) . Specifically: where N ( x ;  X  ,  X  ) = | 2  X   X  |  X  1 / 2 exp  X  1 2 ( x  X   X  ) T  X   X  1 ( x  X   X  ) denotes the multivariate Gaus-sian density on x with mean  X  and covariance  X  . It is convenient to express the prior and likelihood Gaussian densities on x in Eq. (1) in information form; the respective parameters ar e We recall that the information form of the Gaussian density N I ( x ; k , J )  X  exp  X  1 2 x T Jx + k T x employs the precision matrix J and the potential vector k [13]. If J is invertible, then the standard and information representations are equivalent, with  X  = J  X  1 k and  X  = J  X  1 , but the information form with J symmetric positive semidefinite is also convenient for desc ribing degenerate Gaussian densities. Further, the precision matrix directly reveals dependencies between subsets of variables in the network: x i and x j are conditionally independent, given the values of the rema ining components of x , iff J i,j = 0 , while, in general,  X  i,j 6 = 0 ; this implies that J is typically much sparser than  X  for GMRF models, as further discussed in Sec. 2.2.
 By Bayes X  rule the posterior distribution of x given y is the product of the prior and likelihood terms and also has Gaussian density We assume J = J x + J y | x to be invertible, although we allow for singular J x and/or J y | x ; in other words, the prior and likelihood jointly define a normalizabl e Gaussian density on x , although each of them on its own may leave a subspace of x unconstrained. 2.2 Gaussian Markov random fields The K rows of G = [ g T 1 ; . . . ; g T K ] and the M rows of H = [ h T 1 ; . . . ; h T M ] can be seen as two sets of length-N linear filters. The respective filter responses Gx and Hx determine the prior and { g k } and { h m } and further assume that any two filter responses are conditio nally independent given ( c 1 ; . . . ; c M ) . Then the posterior factorizes as a product of L Gaussian experts where the variances are  X  l =  X  p ,l , l = 1 . . . K , for the factors that come from the prior term and  X  l =  X  n ,l  X  K , l = K +1 . . . K + M , for those that come from the likelihood term; the correspon ding means are l = p ,l and l = y l  X  K  X  c l  X  K , respectively. Comparing with Eq. (3), we see that the posterior Gaussian information parameters split additive ly as J = P L l =1 J l and k = P L l =1 k l . The individual Gaussian factors have potential vectors k l = f l  X   X  1 l l and rank-one precision matrices J l = f l  X   X  1 l f T l . Since J is invertible, L  X  N . We see that there is a one-to-one correspondence between factors and filters; moreover, the ( i, j ) entry of J l is non-zero iff both i and j entries of f are non-zero. If the filter has T l non-zero elements, then the corresponding Gaussian factor will couple the T l variables in the clique x [ l ] . The resulting GMRF is depicted in a factor graph form in Fig. 1(a). It is straightforward to jointly model condition ally dependent filter responses by letting  X  p or  X  n have block diagonal structure, yielding multivariate Gaus sian factors in Eq. (4). 2.3 Inference: Efficiently computing the posterior mean Conceptually, the Gaussian posterior distribution is full y characterized by the posterior mean  X  and covariance matrix  X  , which are given in closed form in Eq. (3):  X  is the solution of a set of linear equations whose system matrix is the N  X  N precision matrix J , while  X  = J  X  1 . However, naively computing these quantities can be prohibitively expensive when working with high-dimensional models, requiring O ( N 3 ) computation and O ( N 2 ) space. For example, a typical 1 MP image model involves N = 10 6 variables; the corresponding symmetric covariance matrix  X  is generally dense and occupies as much space as about 5  X  10 5 equally-sized images.
 Thankfully, for the GMRF models mostly used in practice, the re exist powerful inference algo-rithms which avoid explicitly inverting the system matrix J . In certain special cases direct methods Figure 1: (a) The factor graph for the posterior GMRF contain s the union f 1: L of prior and likeli-hood factors/filters. An edge between a filter and a site means that the corresponding coefficient is non-zero. The variables connected to each factor comprise a clique of the GMRF. (b) Filterbank implementation of matrix-vector multiplication Jx arising in CG (  X   X  is the spatial mirror of  X  ). are applicable for computing the mode, marginal variances, and samples from the posterior. For example, spatially homogeneous GMRFs give rise to a block-c irculant precision matrix and exact computations can be carried out in O ( N log N ) complexity with DFT-based techniques [10]. Ex-act inference can also be carried out in chain or tree-struct ured GMRFs using O ( N ) Kalman filter equations which correspond to belief propagation (BP) upda tes recursively in time or scale [36]. A related direct approach which in the context of GMRFs has be en studied in detail by [21, 22] relies on the Cholesky factorization of the precision matri x by efficient sparse matrix techniques, which typically re-order the variables in x so as to minimize the bandwidth W of J . The resulting algorithm has O ( W 2 N ) speed and O ( W N ) space complexity, which is still quite expensive for very large scale 2-D lattice image models, since the bandwid th W increases linearly with the spatial extent of the image and the support of the filters.
 More generally, for large scale and arbitrarily structured GMRFs one needs to resort to iterative techniques such as conjugate gradients, multigrid, or loop y BP in order to approximately solve the linear system in Eq. (3) and recover the most likely solut ion  X  . Conjugate gradients (CG) [6] are generally applicable in our setup since the system matri x is positive definite. Each CG iteration involves a single matrix-vector multiplication Jx . By Sec. 2.2, this essentially amounts to computing sending messages from the variables to the factors and back i n the diagram of Fig. 1(a). The GMRFs arising in image modeling are typically defined on the image r esponses to a bank of linear filters {  X   X  } ,  X  = 1 . . . , B ; the spatial translation of each filter kernel  X   X  induces a subset of factors. In this context, the matrix-vector multiplication Jx in CG corresponds to convolutions and element-wise multiplications, as shown in the filterbank diagram of Fig. 1 (b). The time complexity per iteration is thus low, typically O ( N ) or O ( N log N ) , provided that the filter kernels  X   X  have small spatial support or correspond to wavelet or Fourier atoms for which f ast discrete transforms exist, while computations can also be carried out in the GPU. The memory ov erhead is also minimal, O ( N ) , as CG employs only 3 or 4 auxiliary length-N vectors. The convergence rate of CG is largely problem-dependent, but in many cases a relatively small number of ite rations suffice to bring us close enough to the solution, especially if an effective preconditioner is used [6]. Multigrid algorithms also apply in certain of the GMRF models we consider, especially those r elated to physics-based variational energy and PDE formulations [29, 31]. When multigrid applies , as in the example of Sec. 3, it recovers the solution after a fixed number of iterations (ind ependent of the problem size) and has optimal O ( N ) time and space complexity. Loopy BP is a powerful distribute d iterative method for computing  X  which is guaranteed to converge for certain GMRF classes [13 ,33]. Unlike direct methods, the iterative techniques discussed in Sec. 2.3 have been typically restricted to computing the posterior mode  X  and considered less suited to posterior sampling or varianc e computation (but see Sec. 4). However, as the following resu lt shows, exact sampling from a linear Gaussian model can be reduced to computing the mode of a Gauss ian model with identical precision matrix J but randomly perturbed potential vector  X  k , and thus the powerful iterative methods for recovering the mean can be used unmodified for sampling in lar ge scale GMRFs. Specifically: Algorithm. A sample x s from the posterior distribution P ( x | y ) = N ( x ;  X  ,  X  ) of Eq. (3) can be drawn using the following procedure: (1) Perturb the prior m ean filter responses  X   X  p  X  X  (  X  p ,  X  p ) . (2) Perturb the measurements  X  y  X  X  ( y ,  X  n ) . (3) Use the procedure for computing the posterior mode keeping the same system matrix J , only replacing  X  p and y with their perturbed versions: x Indeed, x s is a Gaussian random vector, as linear combination of Gaussi ans, and has the desired mean E { x s } =  X  and covariance E { ( x s  X   X  )( x s  X   X  ) T } = J  X  1 =  X  , as can readily be veri-fied. Clearly, solving the corresponding linear system appr oximately will only yield an approximate sample. The reduction above implies that posterior samplin g under the linear Gaussian model is computationally as hard as mode computation, provided that the structure of  X  p and  X  n allows efficient sampling from the corresponding distributions, u sing, e.g., the direct methods of Sec. 2.3. This algorithm is central to our paper; variations of it have appeared previously [16,22,23]. The sampling algorithm takes a particularly simple and intu itive form for the GMRFs discussed in Sec. 2.2. In this case  X  p and  X  n are diagonal and thus for sampling we perturb independently the factor means  X  l  X  X  ( l ,  X  l ) , l = 1 . . . L , followed by finding the mode of the so perturbed GMRF in Eq. (4). The perturbation can be equivalently seen in the i nformation parameterization as injecting operation carried out independently at each factor of the di agram in Fig. 1(a).
 To demonstrate the power of this algorithm, we show in Fig. 2 a n image inpainting example in which we fill in the occluded parts of an 498  X  495 image under a 2-D thin-membrane prior GMRF model [12,29,31], in which the Gaussian factors are induced by the first-order spatial derivative filters  X  1 = [  X  1 1 ] and  X  2 = [  X  1 1 ] T . The shared variance parameter  X  l for the experts has been matched to the variance of the image derivative histogram. The presence of randomly placed measurements makes the problem non-stationary and thus Fourier domain techniq ues are not applicable. Finding the posterior mean of this model amounts to solving a quadratic e nergy minimization problem in which the non-occluded pixels are clamped to their observed value s and corresponds to a Laplace PDE problem with non-homogeneous regularization, which can be tackled very efficiently with multigrid techniques [31]. To transform this efficient MAP computatio n technique into a powerful sampling algorithm for the thin-membrane GMRF, it suffices to inject n oise to the factors, only perturbing the linear system X  X  right hand side. Using a multigrid solve r originally developed for solving PDE problems, we can draw about 4 posterior samples per second fr om the 2-D thin-membrane model of Fig. 2, which is particularly impressive given its size; t he multilevel Gibbs sampling technique of [30] is the only other algorithm that could potentially ac hieve such speed in a similar setup, yet it cannot produce exact single-shot samples as our algorithm c an. Figure 2: Image inpainting by exact sampling from the poster ior under a 2-D thin-membrane prior GMRF model, conditional on the image values at the known site s. From left to right, the masked image (big occluded areas plus 50% missing pixels), the post erior mean, a posterior sample obtained by our perturbed GMRF sampling algorithm, and the sample-ba sed estimate of the posterior standard deviation (square root of the variance) using 20 samples (im age values are between 0 and 1). It is often desirable not only to compute the mode  X  but also recover aspects of the covariance structure in the posterior distribution. As we have discuss ed in Sec. 2.1, for very large models the fully-dense covariance matrix  X  is impractical to compute or store; however, we might be inte rested in certain of its elements. For example, the diagonal of  X  contains the variance of each variable and thus, along with the mean, fully describes the posterior marginal densities [29]. Marginal vari-ances also need to be computed in Gaussian subproblems that a rise in the context of non-Gaussian sparse Bayesian learning and relevance vector machine mode ls used for regression, classification, and experimental design [11, 26, 32]. For many of these model s variance estimation is the main computational bottleneck in applications involving large scale datasets.
 A number of techniques have been proposed for posterior vari ance estimation. One approach has been to employ modified conjugate gradient algorithms which allow forming variance estimates in parallel to computing the posterior mode when solving the li near system in Eq. (3) [15, 24, 27]. These techniques utilize the close connection between conj ugate gradients and the Lanczos method for determining eigensystems [6, 15] but unfortunately exh ibit erratic numerical behavior in prac-tice, especially when applied to large scale problems: loss of orthogonality due to finite numerical precision requires that one holds in memory the entire seque nce of Lanczos vectors and periodically reorthogonalize them as the iteration progresses, signific antly increasing the memory and time com-plexity relative to ordinary CG; the variance estimates typ ically converge much slower than mean estimates; one often has limited freedom in initializing th e iteration and/or selecting the precondi-tioner. We refer to [25] for further information.
 It is well known that belief propagation computes exact vari ances in tree-structured GMRFs [36]. However, in graphs with cycles its loopy version typically u nderestimates the marginal variances since it overcounts the evidence, even when it converges to t he correct means [13,33]. The variance estimator of [28] is only applicable to GMRFs for which just a small number of edges violates the graph X  X  tree structure. The method in [12] relies on a low-ra nk approximation of the N  X  N unit matrix, carefully adapted to the problem covariance struct ure, also employing a wavelet hierarchy for models exhibiting long-range dependencies. One then ne eds to solve as many linear systems as is the approximation rank, which in turn increases with th e model size ( [12] reports a rank of 448 for a relatively smooth model with about 10 6 variables). This technique is thus still relatively expensive and not necessarily generally applicable.
 The ability to efficiently sample from the Gaussian posterio r distribution using the algorithm of Sec. 3 immediately suggests the following Monte Carlo estim ator of the posterior covariance matrix If only the posterior variances are required, one will obvio usly just evaluate and retain the diagonal of the outer-products in the sum; any other selected element s of  X   X  can similarly be obtained. Clearly, the proposed estimator is unbiased. Its relative variance e stimation error follows from the properties of the  X  2 distribution and is r =  X (  X   X  i,i ) /  X  i,i = p 2 /S . The error drops quite slowly with the number of samples ( S = 2 /r 2 samples are required to reach a desired relative error r ), so the technique is best suited if rough variance estimates suffice , which is often the case in practical applications [26]; e.g., 50 samples suffice to reduce r to 20% . A desirable property of the estimator is that its accuracy is independent of the problem size N , in contrast to most alternative techniques. The proposed variance estimation technique can thus be read ily applied to every GMRF at a cost of S times that of computing  X  . We show in Fig. 2 the result of applying the proposed varianc e estimator for the thin-membrane GMRF example considered in Sec. 2.3; within only 20 samples (computed in 5 sec.) the qualitative structure of the varian ce in the model has been captured. Following the intuition behind Gaussian sampling by local p erturbations, one could try to inject noise to the local potentials and find the mode of the perturbe d model, even in the presence of non-quadratic MRF factors. Although such a randomization proce ss is interesting on its own right and deserves further study, it is not feasible to design it in a wa y that leads to single shot algorithms for exact sampling of non-Gaussian MRFs.
 Without completely abandoning the Gaussian realm, we can ge t versatile models in which some hidden variables q control the mean and/or variance of the Gaussian factors. Co nditional on the values of these hidden variables, the data are still Gaussia n where we have dropped the dependence on the measurements y for simplicity. Sampling from this model can be carried out efficiently (but not in a single s hot any more) by alternately block sampling from P ( x | q ) and P ( q | x ) , which typically mixes rapidly and is much more efficient tha n single-site Gibbs sampling [35]. For large models this is fe asible because, given the hidden vari-ables, we can update the visible units collectively using th e GMRF sampling by local perturba-tions algorithm, similarly to [16, 23]. We assume that block sampling of the hidden units given the visible variables is also feasible, by considering thei r conditional distribution independent or tree-structured [16]. One typically employs one discrete h idden variable q l per factor f l , leading to mixture of Gaussian local experts for which the joint dist ribution of visible and hidden units is turns off the smoothness constraint enforced by the factor f l by assigning a large variance  X  l,j to it when an image edge is detected.
 The block-Gibbs sampler leads to a rapidly mixing Markov cha in which after a few burn-in itera-P ( x , q ) . Summarizing the sample sequence into a unique estimate  X  x should be problem dependent. If we strive for minimizing the estimation X  X  mean square err or as typically is the case in image de-noising, our goal should be to induce the posterior mean from the sample sequence [23]. Apart from the standard sample-based posterior mean estimator  X  x S = 1 /S P S s =1 x s , we can alternatively es-timate the posterior mean with the Rao-Blackwellized (RB) e stimator  X  x RB = 1 /S P S s =1 E { x | q s } [16], which offers increased accuracy but requires finding t he means of the conditionally Gaussian MRFs P ( x | q ) , typically doubling the cost per step. Beyond MMSE, in appli cations such as image inpainting or texture synthesis, the posterior mean can be o verly smooth and selecting a single sam-ple from the simulation as the solution can be visually more p lausible [8], as can be appreciated by comparing the MMSE and sample reconstructions of the textur ed areas in the inpainting example of Fig. 2. Figure 3: Signal restoration under a total variation prior m odel and alternative estimation criteria. The heavy tailed histograms of natural image filter response s are often conveniently approximated by kurtotic continuous parametric distributions [10,19,35] . We can still resort to block Gibbs sampling for efficiently exploring the posterior distribution of the signal x if each expert can be represented as a continuous Gaussian scale mixture (GSM) [2], as has been do ne before for Student-t experts [35]. Motivated by [14, 23], we show here how this can lead to a novel Bayesian treatment of signal restoration under a total variation (TV) prior P ( x )  X  Q N  X  1 l =1 L ( X  x l ;  X  ) , which imposes an L1 penalty on the signal diferrences  X  x l = x l  X  x l +1 . We rely on the hierarchical characterization of the Laplacian density L ( z ;  X  ) = 1 / (2  X  ) exp(  X  X  z | / X  ) as a GSM in which the variance follows an GSM nature of this representation and assuming a Gaussian me asurement model, the conditionally Gaussian visible variables are easy to sample. Further, the latent variances v l conditionally decouple generalized inverse Gaussian distribution for which stand ard sampling routines exist. The derivation above carries over to the 2-D TV model, with the gradient magn itude at each pixel replacing |  X  x l | . We demonstrate our Bayesian TV restoration method in a signa l denoising experiment illustrated in Fig. 3. We synthesized a length-1000 signal by integrating L aplacian noise (  X  = 1 / 8 ), also adding jumps of height 5 at four locations (outliers), and subseque ntly degraded it by adding Gaussian noise (with variance 1). We depict the standard TV-MAP restoratio n result, as well as plausible solutions extracted from a 10-step block-Gibbs sampling run with our G SM-based Bayesian algorithm: the 10-th sample itself, and the two MMSE estimates outlined abo ve (sample mean and RB). As ex-pected, the two mean estimators are best in terms of PSNR (wit h the RB one slightly superior). The standard TV-MAP estimator captures the edges more sharp ly but has lower PSNR score and produces staircase artifacts. Although the random sample p erforms the worst in terms of PSNR, it resembles most closely the qualitative properties of the or iginal signal, capturing its fine structure. These findings shed new light in the critical view of [14] on MA P-based denoising.
 We must emphasize that the block Gibbs sampling strategy out lined above in conjunction with our GMRF sampling by local perturbations algorithm is equally w ell applicable when the latent variables are distributed, with each hidden variable affecting multi ple experts, as illustrated in Fig. 4(a). This situation arises in the context of unsupervised learning of hierarchical models applied on real-valued data, where it is natural to use a Gaussian restricted Boltzm ann machine (GRBM) in the first layer of the hierarchy. Training GRBMs with contrastive divergence [7] requires drawing random samples from the model. Sampling the visible layer given the layer of discrete hidden variables is easy if there are no sideways connections between the continuous vi sible units, as assumed in [9]. To take into account residual correlations among the visible units , the authors of the factored GRBM in [18] drop the conditional independence assumption, but resort t o difficult to tune hybrid Monte Carlo (HMC) for sampling. Employing our Gaussian sampling by loca l perturbations scheme we can efficiently jointly sample the correlated visible units, wh ich allows us to still use the more efficient block-Gibbs sampler in training the model of [18]. To verify this, we have accordingly replaced the sampling module in the publicly available implementati on of [18], and have closely followed their setup leaving their model otherwise unchanged. For co nditionally Gaussian sampling of the correlated visible units we have used our local perturbatio n algorithm, coupled with 5 iterations of conjugate gradients running on the GPU. Contrastive diverg ence training was done on the dataset accompanying their code, which comprises 10240 16  X  16 color patches randomly extracted from the Berkeley dataset and statistically whitened. The recep tive fields learned by this procedure are depicted in Fig. 4(b) and look qualitative the same with thos e reported in [18], while computation time was reduced by a factor of two. Besides this moderate com putation gain, the main interest in perturbed Gaussian sampling in this setup lies in its scal ability which offers the potential to move beyond the patch-based representation and sample from whole-image factored GRBM models, similarly to what has been recently achieved in [23] for the fi eld of independent experts model [19]. Figure 4: (a) Each hidden unit can control a single factor (su ch as the q 1 above) or it can affect multiple experts, resulting to models with distributed lat ent representations. (b) The visible-to-factor filters arising in the factored GRBM model of [18], as learned using block Gibbs sampling. Acknowledgments This work was supported by the NSF award 0917141 and the AFOSR grant 9550-08-1-0489.
