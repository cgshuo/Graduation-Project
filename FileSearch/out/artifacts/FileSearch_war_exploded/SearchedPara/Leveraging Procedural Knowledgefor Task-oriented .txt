 Many search engine users attempt to satisfy an information need by issuing multiple queries, with the expectation that each result will contribute some portion of the required information. Previ-ous research has shown that structured or semi-structured descrip-tive knowledge bases (such as Wikipedia) can be used to improve search quality and experience for general or entity-centric queries. However, such resources do not have sufficient coverage of proce-dural knowledge, i.e. what actions should be performed and what factors should be considered to achieve some goal; such procedu-ral knowledge is crucial when responding to task-oriented search queries. This paper provides a first attempt to bridge the gap be-tween two evolving research areas: development of procedural knowl-edge bases (such as wikiHow) and task-oriented search. We in-vestigate whether task-oriented search can benefit from existing procedural knowledge ( search task suggestion ) and whether auto-matic procedural knowledge construction can benefit from users X  search activities ( automatic procedural knowledge base construc-tion ). We propose to create a three-way parallel corpus of queries, query contexts, and task descriptions, and reduce both problems to sequence labeling tasks. We propose a set of textual features and structural features to identify key search phrases from task descrip-tions, and then adapt similar features to extract wikiHow-style pro-cedural knowledge descriptions from search queries and relevant text snippets. We compare our proposed solution with baseline al-gorithms, commercial search engines, and the (manually-curated) wikiHow procedural knowledge; experimental results show an im-provement of +0.28 to +0.41 in terms of Precision@8 and mean average precision (MAP).
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval procedural knowledge base; search log; query suggestion; wiki-how; search intent
Many search engine users attempt to satisfy an information need by issuing multiple queries, with the expectation that each sub-task will contribute some portion of the required information example, to organize a conference (task), the organizer needs to look for information related to choosing a hotel, identifying and deciding among banquet options, recruiting volunteers, selecting and contacting publishers, etc (subtasks). For each specific task, users must gather more detailed information. For example, when choosing a hotel, the organizer must consider the number and size of conference rooms, potential arrangements for meal catering and menu planning, whether or not a discounted rate will be available, etc. There is a huge demand for search engines to better assist their users in achieving their intended goals for such ad hoc tasks.
Researchers have studied problems related to search intent anal-ysis for general queries; for example, how to identify search intents [13, 11, 37, 25, 9, 19, 24, 28], how to suggest search queries to the user [3, 12, 32, 22, 41], how to rank results that cover diverse aspects of the task, etc. Most studies have so far mostly relied on queries and search logs, search result texts, behavioral information, etc. to improve search quality for entity-centric queries [15]. Re-cently, research has shown that structured knowledge bases (such as Wikidata or formerly FreeBase 2 ) or semi-structured knowledge bases (such as Wikipedia 3 ) can be used to improve search quality and experience [14, 6, 26, 43]. However, such knowledge bases fo-cus on representing descriptive knowledge (also known as declar-ative or propositional knowledge), i.e. the knowledge of the at-tributes and features of things, and do not have sufficient coverage of procedural knowledge used for task-based problem solving [2, 18]. In the past decade, hundreds of thousands of how-to guides have been created by the community and/or professional editors on sites such as wikiHow 4 or eHow 5 , which provide procedural knowledge in a semi-structured text format. There have also been attempts to define ontologies [7] to represent procedural knowl-edge, and algorithms have been proposed to automatically con-struct structured procedural knowledge bases such as PAA [1] from semi-structured procedural knowledge [34, 23, 1, 36]. However, to the best of our knowledge, structured or semi-structured procedu-ral knowledge has not been studied in the context of task-oriented search as a means to improve search quality and experience.
This paper provides a first attempt to bridge the gap between the two evolving research areas: procedural knowledge base and task-oriented search. We investigate whether task-oriented search can benefit from an existing procedural knowledge base and whether automatic procedural knowledge construction can benefit from users X  search activities.

We attempt to leverage the style guidelines for writing semi-structured procedural knowledge, such as the wikiHow guide which indicates that an action-oriented instruction beginning with a verb is required at the beginning of each procedural step, but we also attempt to process articles that do not fully comply with the guide. We accordingly propose a set of textual features and struc-tural features to identify query spans from each task description, and then adapt similar features to extract wikiHow-style procedural knowledge descriptions from search queries and relevant text snip-pets. We compare our proposed solution with baseline algorithms as well as commercial search engines and the manually-curated procedural knowledge base wikiHow for both problems; experi-mental results show an improvement of +0.28 to +0.41 in terms of Precision@8 and mean average precision (MAP). Our work can directly benefit search task mining problems such as the NTCIR-11 IMine TaskMine task 7 [29]; automatically extracted procedural knowledge can also provide decision processes for retrieval-based decision support systems such as QUADS [42].
In this section, we review related work in search intent analy-sis and task-oriented search, for which we summarize applications focusing on query suggestion. We then survey prior work on pro-cedural knowledge acquisition and its application, focusing on on-tology definition for procedural knowledge and algorithms to auto-matically extract semi-structured procedural knowledge from text.
Broder [8] and Rose and Levinson [38] propose to categorize search queries into informational , navigational , and transactional and relate them to a more fine-grained goal hierarchy . Marchionini [30] analyzed the user X  X  high-level  X  X eed to know X  and introduced the concept of exploratory search as an extension of the traditional lookup . Researchers have also proposed to predict both high-level and specific search intents from textual and behavioral information such as Web page contents [13], search queries [11], click logs [37, 25, 9], mouse movements and scrolling [19], and results of indi-vidual user questionnaires [24, 28]. Identifying searchers X  intents and incorporating them into the search process can contribute to search query suggestion and prediction [3, 12, 32, 22, 41] and se-lection of exact responses (such as various Google search special features) [40]. The suggested query type depends on the original search query and the predicted intent. E.g., when a user searches for an entity, search engines may suggest other relevant entities [6, 5], actions paired with the entity [27], or attributes [14], mostly using declarative knowledge bases such as Freebase or Wikipedia. However, whether a task-oriented search can achieve an analogous benefit from a procedural knowledge base has not been studied.
In the area of search intent analysis, Hassan et al. [20, 21] stud-ied the complex search task , a search behavior that is usually ap-plied to task-oriented search, using search queries. In comparison, our work focuses specifically on task-oriented search, and ignores other types of search (such as browsing different attributes of an object), which allows us to take the advantage of existing proce-dural knowledge to more reliably support search tasks (when com-pared to the use of general search logs). We also study to how to build a procedural knowledge base with a full text representation of task-oriented knowledge. The work by Weber et al. [40] on ex-tracting tips from Yahoo! Answers to address how-to Web queries also incorporates task-oriented question and tip extraction from un-structured question-answer texts, whereas our work deals with only complex task-related queries and focuses on identifying queryable snippets from more structured procedural knowledge. Moreover, we also study how to construct procedural knowledge bases from search queries and result snippets, which can be extended to facili-tate automatic question answering for community-based QA.
More recently, the IMine Subtopic Mining subtask [29] and the pilot subtask TaskMine at NTCIR focuses on identifying subtasks for a given complex task, and is highly relevant to our work. For a given task (e.g.  X  X ose weight X ), possible outputs are expected to be subtask steps (e.g.,  X  X o physical exercise X ,  X  X easure calorie intake X ). We make a similar assumption that the given queries are focused on seeking information (subtasks) for complex tasks.
Cognitive psychology defines  X  X rocedural knowledge X  (or im-perative knowledge) as the knowledge exercised in the performance of some task [2, 18]. The Semantic Web community has attempted to formally define ontologies to represent procedural knowledge [7], which usually include an instruction (or action sequence) and a purpose (or goal ). Also defined in such ontologies are the rela-tions between procedural knowledge elements, at different levels of granularity, such as has-step , has-goal [34] or is-achieved-by [17].
Several automatic procedural knowledge base construction ap-proaches have also been proposed to extract instructions from semi-structured texts, e.g. eHow or wikiHow articles [34, 23, 1, 36], recipes [36], community-based QA [4], etc. Most approaches take advantage of structural information (e.g. HTML tags [4], enumer-ation indicators [16, 23]), and define rules or templates to extract textual content. In a separate step, NLP tools are applied to extract relations and normalize each goal and action to its ontological form, to support linking to other ontological resources. Researchers have also studied how to identify script knowledge [39] which focuses on the temporal ordering of events [10].

In contrast, our approach takes advantage of the writing style of semi-structured procedural texts and proposes a set of structural and textual features for identifying procedural knowledge; the im-plemented approach can be optimized with a supervised learning method. Moreover, beyond the conventional use of small-scale pro-cedural knowledge in AI planning [33] or NLP [4], this paper stud-ies the problem of how to apply an online large-scale procedural knowledge base to complex task search problems.
In this section, we first give our formal definition of procedu-ral knowledge base , and then introduce the two main problems we will study in this paper: search task suggestion using procedural knowledge base (STS) and automatic procedural knowledge base construction from search activities (APKBC) .

Many knowledge bases such as Wikipedia or Wikidata that have been widely utilized contain a huge amount of descriptive knowl-edge . Procedural knowledge [2, 18], also known as know-how , is the knowledge exercised in the accomplishment of a task, i.e. how to do things, which is usually acquired by experience and consid-ered tacit and not easily shared, compared to descriptive knowl-edge . However, shared explicit procedural knowledge lays a foun-dation for efficiently coordinated action, which is often referred to as best practices or business rules within communities or organiza-tions. As wikiHow and many similar how-to Web sites allow users to easily share procedural knowledge, semi-structured large-scale procedural knowledge bases have become available. People have also attempted to construct structured procedural knowledge bases by defining ontologies or incorporating other existing representa-tions such as PDDL 8 [34, 23, 1, 36]. Since we notice that many of the proposed definitions use different terminologies, e.g. goal vs. target vs. purpose , instruction vs. action sequence , step vs. action , etc, although their meanings are identical, we first formally define the terminology related to procedural knowledge base.
 Definition 1 (Task) . A task t is represented by a short and concise summary and a detailed explanation for clarification.
 Definition 2 (Is-achieved-by relation) . A task is connected with an ordered list of n tasks s 1 , . . . , s n (referred to as subtasks ) that contribute to the task t by is-achieved-by relations r ( t; s ) . A weight w ( t; s ) may also associate with each relation r ( t; s ) . We assume the weights associated with the outgoing relations should sum to one, i.e. Definition 3 (Procedural knowledge graph) . Procedural knowl-edge graph G = ( T; R ) is the set of all tasks T and all is-achieved-by relations R .
 Definition 4 (Plan, goal, and primitive tasks) . A plan is a subgraph of G , which is a tree with root being the goal and the leaves being the primitive tasks .

A semi-structured procedural knowledge base such as wikiHow has an identical structure, but it allows users to more easily view and edit; each article corresponds to a task, whose summary is given in the title and whose explanation is detailed in the intro-duction section. Each step (or substep) also corresponds to a task. The is-achieved-by relations have three representation forms: num-bered steps in each article page (first-level subtasks), bulleted sub-steps under each step description (second-level subtasks), and free links 9 directed from a step to another task page. Steps that have no substeps or outgoing free links are considered primitive tasks . Current semi-structured procedural knowledge bases do not allow editors to explicitly specify the the relation strength between tasks, or the importance of each subtask 10 . Therefore, we assume each relation shares an equal weight with all other outgoing relations, i.e. w ( t; s ) = 1 = od ( t ) , where od is the out-degree of the task t .
Procedural knowledge captures how people decompose a com-plex task into a mixture of several mental actions (e.g.  X  X hoose X ,  X  X etermine X , etc.) and/or physical actions (e.g.  X  X isit X ,  X  X tore X , etc.). When users turn to search engines for information seek-ing and problem solving, we investigate how existing procedural knowledge can be leveraged to help understand a user X  X  search in-tent and suggest sub search tasks to users.
 Problem 1 ( Search task suggestion using procedural knowl-edge, STS ) . Given a procedural knowledge graph G and a task-oriented search task q , we aim to (a) identify the task t user intends to accomplish, and then (b) retrieve a list of k sub-tasks s 1 ; : : : ; s k 2 T and also (c) suggest the corresponding sub search tasks p 1 ; : : : ; p k .

A search task is usually represented by a search query or a key phrase [29, 20, 21]. Similarly, we assume both the input search task q and the suggested search tasks p 1 ; : : : ; p k are queryable phrases. As a result, the problem can be straightforwardly solved by sug-gesting relevant (sub)queries for task-oriented queries. In contrast to the entities and attributes in a descriptive knowledge base, which are usually words or phrases and can used directly in query sugges-tion, summaries and explanations in a procedural knowledge usu-ally contain richer action and condition information. Therefore, the problem poses three challenges. First, we need to identify the task t 2 T the user intends to accomplish. If the query corresponding to the search task exactly matches the summary of a task t (exact string match or after lexical transformations), then we can directly return t . However, in most cases when an exact match does not exist, we need to incorporate additional information such as the search result page of the query and the full description of each task, and rank candidate tasks using a retrieval method. Second, we need to choose the tasks s 1 ; : : : ; s k 2 T to suggest and then prioritize them based on the knowledge G containing relations V . Intuitively, we can choose the task t  X  X  top-k subtasks ordered by the weight of the is-achieved-by relation. Finally, we need to extract queryable phrases from each task description, and the algorithm should thus learn how searchers have been trained to formulate queries.
Although community-centric procedural knowledge bases may have collected hundreds of thousands of task descriptions, users still face ad hoc situations (tasks) that are not covered by an existing procedural knowledge base. However, we hypothesize that other searchers may have faced similar situations in the past and have al-ready interacted with search engines to attempt a solution, which means we may discover implicitly expressed procedural knowl-edge from users X  raw search activities (e.g. search logs) as well as those aggregated from many searchers (e.g. suggested queries). Therefore, we investigate whether we can automatically construct a procedural knowledge base like wikiHow using search queries and relevant documents returned from search engines.
 Problem 2 ( Automatic procedural knowledge base construc-tion from search activities, APKBC ) . Given a task t , we aim to (a) identify a search task q , and (b) collect k related search tasks p ; : : : ; p k , and then (c) identify n ( k ) search tasks p to generate n tasks s 1 ; : : : ; s n that can be performed to accomplish the task t with text description.

Related search tasks can be identified from users X  search activi-ties [37, 25, 9] and/or suggested by search engines [3, 12, 32, 22, 41]. Depending on the search intent, related actions, attributes, en-tities, or any form of reformulation of the original search task may be considered related, which however do not necessarily embed Figure 1: Subproblems in search task suggestion using procedu-ral knowledge problem (brown solid arrows and brown labels) and automatic procedural knowledge base construction from search ac-tivities problem (purple dashed arrows and purple labels) procedural knowledge. Therefore, the first challenge in extracting procedural knowledge is to identify the is-achieved-by relations, i.e., which of them correspond to the tasks that can achieve the goal. A textual description is a less ambiguous representation of a task in a procedural knowledge base, which can be accessed both by hu-mans and by automatic decision support systems that require pre-defined natural language decision processes as input (e.g. QUADS [42]). Our second challenge is therefore to automatically generate text descriptions for procedural knowledge which attempt to con-form to a common writing style. We summarize the interactions between task-oriented search and procedural knowledge base con-struction in Figure 1.
In this section, we propose a supervised learning framework, which requires zero extra manual annotation, and instead takes ad-vantage of available artifacts; users X  search activities and manually curated procedural knowledge are used to train models for both STS and APKBC problems. In particular, existing search logs can reveal how searchers have been trained to formulate queries, but they don X  X  necessarily or sufficiently cover how to search for pro-cedural knowledge. On the other hand, existing procedural knowl-edge bases indicate how to accomplish tasks in a very compre-hensive manner, but are not necessarily optimized for interactive search. We first describe how to build a three-way parallel cor-pus for training, and then define linguistic and structural features for training sequence labeling models for both problems. Then, we present detailed procedures for suggesting search tasks using pro-cedural knowledge and constructing procedural knowledge bases from search logs.
We create a three-way parallel corpus that contains triples f X  q; t; c  X  X  with a query q , a matching task t , and a context c of q and t , which is a passage of text that describes the task. We build the three-way parallel corpus in an iterative manner.

We first identify the exact matching pairs of searchers X  issued search tasks and task summaries in the procedural knowledge base, which guarantees precision in the first step, although we expect that many lexically distinct but semantically identical pairs may exist that will not be extracted. To achieve this, we may scan through the entire search query log to find each query q that matches the de-scription of the task t . We then retrieve the context c by extracting the textual content from the top documents relevant to the descrip-tion of the task t . We add the triple  X  q; t; c  X  to the parallel corpus. In addition, we collect related queries by combining the user-issued queries from the same session and the list of queries suggested by the search engine for the next iteration.

If we have no large query log, we may also manually create task-oriented search tasks for the tasks in the procedural knowl-edge base. Specifically, we use the summary of the task t to form a search query q and issue it to the search engine to extract the con-text c . Since these search queries are  X  X rtificially X  created, and do not necessarily represent how searchers normally formulate queries for these tasks in similar situations, we exclude the triple that contains the original query q from the parallel corpus, which is only used for search engines to suggest real queries for the next iteration.

Once we collect a list of related queries for q , we compare them with the known subtasks of t , i.e. the tasks connected by is-achieved-by relations in the procedural knowledge base. For each related query p , we first find the subtasks s 1 ; : : : ; s p in either its summary or explanation, and retrieve its context d . We then add all f X  p; s i ; d  X  X  i 2 [1 ;n ] to the parallel corpus. As we assume that all necessary subtasks have been detailed in the pro-cedural knowledge base, we consider a related query irrelevant to accomplishing the task if it is not mentioned in any of the subtasks, and it is excluded in the parallel corpus and discarded for future iterations, which solves Problem 2(b). We also ignore subtasks that are not matched to any related query, a situation that may arise from a lack of evidence from query logs, or truncation due to lim-ited space for displaying suggested queries on a result page. We continue the iterative process.

Furthermore, we annotate the occurrence of the query q in the task t  X  X  description. We also similarly annotate the occurrence of the summary and explanation of the task t in the context c by find-ing the contiguous sequence of words (a passage) from the context that is most relevant to the task summary or explanation. An exam-ple is shown in Figure 2. For training a sequence labeling model, we employ the BIO encoding, i.e., each token in the task descrip-tion (or context) that begins each occurrence of the query phrase (or the task summary or explanation) is labeled as B Q (or the tokens that continue in the query phrase (or the task summary or explanation) are labeled as I Q (or I TS , I TE ), and all other tokens in the task (or the context) are labeled as O .
We use the constructed parallel corpus, which contains triples of queries, task representations, and contexts, to train supervised mod-els that can automatically create task-oriented queries for tasks, as well as task descriptions for task-oriented search tasks. We view both problems as a word sequence extraction problem, which is usually reduced to a sequence labeling task. Therefore, we de-fine the features by taking advantage of the writing style prevalent in semi-structured procedural knowledge bases, and then apply a common statistical model for sequence labeling (such as Condi-tional Random Fields (CRF)), which also enables us to process ar-ticles that do not fully comply with a style guide.

We define both syntactic features and structural (positional) fea-tures that are motivated by the writing guide as follows:
Location (LOC). As suggested in the wikiHow writing guide, a task should provide both  X  X kimmable information that readers can quickly understand X  in the title of the article and in the beginning sentence of each step, and  X  X ich informative articles for more pa-tient users X  in the article X  X  introduction and in the sentences which follow in the detailed explanation of each step. Therefore, we de-fine features to capture the location of each word.

Part of speech (POS). Since both the article title and the first sentence in each step are required to begin with a verb in bare in-finitive form, we also extract the fine-grained part-of-speech value for each token.

Parsing (PAR). To further understand the task facets such as occurrences of subsidiary resources (e.g. a target object) or con-straints (e.g. a duration), we include features extracted from de-pendency parsing, named entity extraction, and chunking.
Word and context. As in other natural language sequence la-beling tasks, we also define features including the token X  X  surface form, its stem form, its TF-IDF score, as well as the word features and the part-of-speech tags of the previous and the next words in the task summary or explanation.

We extract the same set of features (except the location feature) from both the context c ( X c ) and task description t ( X triple in the parallel corpus, which together with the corresponding annotated BIO label ( Y c and Y t ) comprises a training instance for sequence labeling. We train a query construction model M Q the training sets ( X t ; Y t ) , a task summary construction model M and a task explanation construction model M TE using ( X c
To construct search queries for an unseen task t , we first extract the features x t from the task description, and then apply the query construction model M Q to extract each search query candidate q i.e. to identify each word sequence w i = w i 1 : : : w i the corresponding y t i labeled as B Q I Q : : : I Q , where l the length of w i , and y t is the optimal label sequence, i.e., where j t j represents the length of the task t  X  X  description. Similarly, to construct task descriptions for an unseen query q , we first retrieve the context c and extract the features x c from the context, and then apply the task description construction model M c to extract each word sequence labeled as B TS I TS : : : I TS and B TE I TE summary and the explanation of a task t i respectively from opti-mizing the following equation:
Given a task-oriented search task represented by query q , we first retrieve a list of candidate tasks from the procedural knowl-edge base that mention the query q in either the summary or the explanation. When more than one task is returned from the proce-dural knowledge base, we need to determine which task is the best fit for the user X  X  search intent. Therefore, we leverage the query construction model M Q to estimate the likelihood of the j -th oc-currence of the query q in a retrieved task t i  X  X  description (i.e. a word sequence w ij = w ij 1 : : : w ij l , where l is the length of query q ). We select the i -th task that contains the j -th word sequence which maximizes the conditional probability, i.e.
Once we have identified the task t , we identify the tasks in the procedural knowledge base for future query suggestion using a few heuristic strategies. First, we may select only the first-level sub-tasks and order the weight of the is-achieved-by relation in de-scending order. We may also select both the first-level and second-level subtasks and use the weight of each first-level subtask or the weight of each second-level subtask multiplied by the weight of its corresponding first-level subtask.

When a list of subtask candidates s 1 ; : : : ; s n are retrieved from the existing procedural knowledge base, we apply the query con-struction model M Q again, for each subtask s i  X  X  summary and ex-planation to identify each word sequence w ij labeled as B Q I Q using Eq. 1. Among the extracted query candidates f w ij g each subtask s i , we choose the query p i that maximizes the likeli-dates p 1 ; : : : ; p n corresponding to the subtask candidates are glob-ally ranked by multiplying the weight obtained from subtask re-trieval with the likelihood estimation; finally, we select the top-k queries. This process guarantees that the queries are extracted from distinct subtask candidates, which can lead to the most coverage of all necessary subtasks. Alternatively, a diversity-aware ranking approach can also be applied to global rank all query candidates f w ij g j . A plan can be gradually implemented and detailed when we iteratively apply this approach.
Given a task description t , we directly apply the query construc-tion model M Q to extract a task-oriented search query q using Eq. 1, and then (similar to the process in building the parallel corpus) we identify the the queries related to q in both search logs and sug-gested queries. The entire search session (reformulation of queries, click activities, etc.) can be analyzed to determine how a specific user works to accomplish a task; therefore, by mining the search session data, we should be able to model how users typically ac-complish tasks. As a result, the search engine should be able to correctly suggest to the user related tasks, rather than related enti-ties or attributes. Although this assumption may not always hold in real-world search scenarios, it allows us to consider how related tasks can be further explored to extract subtasks for t , as long as the task description construction model can identify any passage that can be used for the task summary or explanation. In Section 5, we observe whether or to what extent this assumption holds for actual search scenarios.
F or each related query p i , we collect its context by extracting relevant document snippets from search engines, and apply the task description construction model M T to extract the most likely sum-mary candidate w i and explanation candidate v i for task s cording to Eq. 2, among all extracted summary candidates and explanation candidates for s i . We select the top-n tasks ordered by the estimated likelihood for their summaries.
In this section, we describe the experiments conducted to eval-uate the effectiveness of the approach proposed in Section 4. We first introduce the data sets for the experiments, and then we de-scribe the experiment settings. The experimental results are pre-sented in three subsections for the constructed three-way parallel corpus and the two problems we introduced in Section 3; we also analyze the limitations of the current query suggestion approaches for task-oriented search, and the coverage and quality of current procedural knowledge bases for search suggestion. The data and code used in this section are available to download 11 .
We used two publicly available data sets (an English wikiHow data dump and the AOL search log) and public search engines (Google and Bing) to collect the suggested queries and contexts. English wikiHow data dump. We crawled an English wiki-How data dump using a modified version of the WikiTeam tool which contains 198,163 non-redirect articles within namespace  X 0 X  (Main) 13 . We also filtered out the articles that are marked as (incomplete and need more information) or have no  X  X ntroduction X  or  X  X teps X , which results in a collection of 149,975 valid articles. We performed minimal pre-processing to reformat the articles to the MediaWiki format 14 , without changing any textual content.
In particular, we first extracted the top-level task summary and explanation after identifying the title and the introduction of each article. We then located the  X  X teps X  section and extracted the enu-merated or bulleted items to build a local subtask hierarchy. Next, we built a procedural knowledge graph by creating nodes repre-senting all the top-level tasks and their subtasks and establishing relations based on both the task-subtask relation as well as internal links. Applying this approach, the constructed procedural knowl-edge graph contains a total of 1,488,587 tasks and 1,439,217 rela-tions, where 265,509 of the tasks are non-primitive, and 100,605 relations come from internal links. We built a Lucene 15 index for all task descriptions, which supports retrieval of candidate tasks for the search task suggestion problem described in Section 4.3. AOL search query log. We also used the publicly available AOL search query log, which consists of over 21M Web queries (over 10M unique queries) collected over three months in 2006 from AOL [35]. We downcased the query strings and task sum-maries, and removed all non-alphanumeric characters. We identi-fied that 867 unique queries (corresponding to 9,847 new queries from 23,099 lines) match some task summary in our constructed procedural knowledge base. In the experiment, we identified 639 unique queries that have at least two tokens (corresponding to 3,086 new queries from 7,019 lines), which tend to be less ambiguous and more likely task-oriented. We could estimate that each task-oriented search task is repeated 4.8 times on average, compared to 2.1 times for all queries, supporting the intuition that common task-oriented searches tasks are more often encountered than gen-eral search tasks, providing further motivation for our study. The 639 unique queries correspond to 619 tasks if punctuation marks and whitespaces are ignored.

To retrieve the related queries issued by users in the same ses-sion from the query log, we first identified related query candidates by collecting the queries that were issued by the same user within 30 minutes after they issued each matching query. In this way, we collected 33,548 query candidates (31,955 unique queries, 50 per query). We did not use other commonly adopted session detection heuristics such as edit distance or cosine similarity between query pairs, since a task-oriented search query may not share words in common with related search queries. For example, we identified a case where  X  X isit niagara falls X  16 is followed by another query  X  X ap of northeast X  in the AOL search log,  X  X esign a living room X  is followed by another query  X  X hoosing colors X , etc. We instead counted on the three-way parallel corpus construction process to correctly identify which of the query candidates are relevant to ac-complishing the task.

Queries suggested by search engines. To create  X  X rtificial X  search tasks, we randomly sampled 1,000 non-primitive tasks from the constructed procedural knowledge that do not appear in the query log. We limited the total number of tasks in building the parallel corpus partly due to our limited access to commercial search engines, and also because in a preliminary experiment we found that performance of the sequence labeling model on the test set had become stable. We merged them with the 639 identified queries from AOL search query log, using commercial search en-gines (Google and Bing) to build the parallel corpus, which cor-respond to 1,619 tasks. We constructed each query without using additional operators such as quotes, plus signs, minus sign, etc., unless quotes appear in the summary. Google may suggest up to eight queries at the bottom of the search page for each search task and Bing may suggest up to sixteen queries within the search result column and on the right side of the results page. We collected a total of 9,906 queries suggested by Google (6.11 per query on av-erage, 8 maximum) and 9,715 suggested queries by Bing (5.99 per query on average, 13 maximum).

We notice that both Google and Bing are able to display proce-dures directly on the result page from wikiHow (only Bing) or other procedural data sources (Google), which allows users to grasp the knowledge without an extra click, but hardly helps users or auto-matic decision support systems identify how to explore more re-lated information from the Web. We also found that both commer-cial search engines tend to suggest and display related queries for short queries, as they are more likely topically broad and semanti-cally ambiguous. However, for task-oriented search, the complex-ity of describing the task does not necessarily reflect the difficulty of accomplishing the task, in terms of which aspects to consider and the steps to perform. In Section 5.4, we further evaluate whether commercial search engines are effective for task suggestion.
Context extracted from search engines. We used Goolge to collect the contexts of the queries both in the parallel corpus (used for model training and testing) and those that are potentially use-ful for new procedural knowledge discard auxiliary words such as pronouns, articles, etc. when they formulate queries, retaining only nouns, verbs, adjectives, etc., we also plan to study whether we can e xtract queryable key words from non-continuous text spans.overy (used for end-to-end APKBC evaluation). We first extracted URLs for context candidates from their first search result pages, and ex-cluded Web documents from the wikihow.com domain to increase the reliability and generalizability of the trained model, and also excluded google.com domains and URLs that have only domain names without subpaths, which are usually navigational search re-sults. Then, we used the Boilerpipe 18 to extract textual content from the HTML documents. For queries in the parallel corpus, we finally downloaded 7,440 context documents and successfully processed 7,437 documents using Boilerpipe, and for end-to-end evaluation, we downloaded and extracted 3,512 context documents.
We describe the experiment settings, including the tools we used to extract features and learn the sequence labeling models, the eval-uation methods, and baseline methods.

Evaluation methods. We conducted two types of evaluation for both problems. We first evaluate the performance of proposed se-quence labeling approach on the query construction and task de-scription construction tasks using the automatically labeled paral-lel corpus. Then, based on manual judgment, we evaluate our pro-posed end-to-end solution for both STS and APKBC problems, and compare with commercial search engine suggestions and the cur-rent wikiHow knowledge base, respectively.

We performed 10-fold cross validation on the constructed paral-lel corpus, and report average performance results. We extracted text spans from the predicted labels and used precision, recall and F-1, averaged over the performance on all test instances (macro-averaged) and averaged on each task then across all tasksalso (micro-averaged), to compare the proposed approach with base-line methods. We also employed two ROUGE scores (F-1 based ROUGE-2 and ROUGE-S4 19 ), after we removed common English stop words 20 and constructed an array of words in lemma forms.
In the end-to-end comparison, we randomly sampled 50 triples from the parallel corpus and we applied both our approach for search task suggestion and commercial search engine services to produce up to eight related queries. We manually judged whether each search query can obtain a search result that can help achieve the user X  X  goal. Similarly, we applied our procedural knowledge base construction approach to generate up to eight procedural de-scriptions and merged it with the wikiHow  X  X teps". We manu-ally judged whether each subtask summary and explanation that the system produced can be considered a valid  X  X tep X  description for wikiHow. Given that this judgment is binary, we report the macro-averaged and micro-averaged Precision@8, and MAP aver-aged over all 50 test instances.

Baseline methods. For the query construction and task descrip-tion construction tasks, we compare the proposed approach CRF with other supervised and unsupervised approaches. We varied the classifier to evaluate HMM , a Hidden Markov Model based se-quence labeling approach based on surface forms, LR , a logistic re-gression classifier, and an SVM classifier. The latter two are trained on the same set of features to classify B , I , and O without the se-quence labeling assumption. We also compared performance with models that use only one category of the proposed features ( W/ ), all but one category ( W/O ), and only local or context features. Finally, Table 1: Averaged number and percentage of related queries sug-gested by Google or Bing or issued by the users subsequently in the same session that are mentioned by a task description in wikiHow we compare with a key word extraction method based on TF-IDF ( TFIDF ), where we try to maximize the macro-averaged F-1 score by tuning the TF-IDF score threshold when determining whether a word is selected as a key word.

Feature extractors and learners. We used Stanford CoreNLP to extract sentences, tokens, stems, POS tags, dependency labels, chunks (noun phrases and verb phrases), and named entities using the default configuration. We used MALLET 22 [31] to learn and test the sequence labeling models (CRF and HMM) with sparse weighting method, LibLinear 23 for training and testing LR and SVM models. As the Web documents tend to be noisy, we prepro-cessed the texts by inserting periods at certain places to improve the parsing performance. Details can be found in the source code.
Including the 639 queries that have already been associated with some tasks in the procedural knowledge base, we identified that a total of 1,182 query-task description pairs (from 1,146 distinct descriptions) among the suggested queries can be found in the sub-task descriptions of the task corresponding to the original query. We annotated the context with each corresponding task summary and explanation. Specifically, we segmented the context into sen-tences and selected those that contain all the tokens in the task sum-mary or at least 70% of the tokens in the task explanation as task description embedding sentences, and annotated the minimal span that contains all overlapping tokens.

To further understand how many of the related queries are aligned to a task description, we show in Table 1 the averaged num-ber and percentage of related queries suggested by Google or Bing or issued by users subsequently in the same session that are men-tioned within a task description.  X  X ew words X  refer to the subset of words that are in the suggested queries but not in the original queries. First, we can see that the queries identified in the same session do not have a higher quality (1.131%  X  4.639%) in related task suggestion, which may be due to an over-simplified session detection algorithm. In comparison, related queries collected from commercial search engines tend to be more relevant (0.727%  X  9.522%). Moreover, if we focus on the new words, we identify many more (from 0.727%  X  1.162% to 8.487%  X  9.522%) tasks that are aligned with the queries suggested by the search engines.
We note that if a query cannot be aligned with a task descrip-tion using our proposed construction method, this does not guaran-tee that the query is not semantically identical (e.g. a paraphrase) of any task description. In fact, as motivated in Section 4.1, the surface form matching approach can simplify the parallel corpus construction process while still guarantee the parallelity.
We first compare the query construction result between the pro-posed approach and the baseline methods in Table 2. We also con-ducted a t-test to calculate the significance level of each baseline method compared against the proposed approach (CRF). We can see the proposed CRF-based sequence labeling approach can sig-nificantly outperform other baseline classifiers (at a significance level of 0.05 in terms of F-1 and 0.1 in terms of ROUGE).
The performance gap between the sequence labeling approaches and the independent classification-based approaches such as LR and SVM suggests that the query construction problem is similar to other sequence labeling problems such as named entity detec-tion or supervised key word extraction. With only surface form features, HMM performs worse than CRF. All the supervised ap-proaches, which take advantage of the constructed parallel corpus, can outperform the unsupervised approach (TF-IDF).
 When we test each feature category (POS, Parsing, Location, Word, Local, and Context) independently, we can see none of them can beat the proposed approach with a confidence level of 0.99. We also see that word features contribute the most to performance, which can achieve around 90% of the performance in terms of F-1 and 95% in terms of ROUGE when all the features are used. Both F-1 and ROUGE scores drop when we remove any feature category from the feature list, and they drop the most when Word features are removed, which implies that all the features have positively con-tributed to the query construction task.

To better understand what non-Word features most likely pro-mote a word to become a B Q or I Q , we shows the top-5 features for O ! I Q , B Q ! I Q , I Q ! I Q transitions in Table 3. We see that the whole query phrase is very likely extracted from the summary part of a description (sum) due to its clarity and conciseness. We also see that both singular nouns (NN or NNP) and verbs in either VB or VBP forms are selected as indicators for beginning a query, and verb phase (VP) is a useful feature when deciding whether to continue a query.
 Table 4: End-to-end evaluation results for search task suggestion (STS) and title (TI) and explanation (EX) generation for automatic procedural knowledge base construction (APKBC). PROP/P. corre-sponds to the proposed approach and WH corresponds to the wiki-How knowledge.
 W e then evaluated the end-to-end performance of the proposed STS solution using the query construction model. We present the results in Table 4 and show examples in Table 5. We see the pro-posed solution can outperform the commercial search engines by +0.35 to +0.41 in terms of Precision@8 and +0.28 to +0.32 in terms of MAP. We see that our proposed method, which is tai-lored for task-oriented search, can provide unique insights when suggesting tasks, compared to current general-purpose commer-cial search engines, which have been designed for entity-centric search and tend to suggest queries by appending keywords such as  X  X roduct X ,  X  X mage X ,  X  X ogo X ,  X  X nline X ,  X  X ree X , etc. We discov-ered three types of errors from our system: (1) Some suggested queries are ambiguous when presented by themselves (e.g.  X  X ix it X ,  X  X nstall X ). (2) duplicated queries are suggested from different subtasks. (3) the manually created knowledge sometimes still con-tains a few non-instructional  X  X teps X . We could further improve the proposed approach by conducting a co-reference analysis or other approaches to incorporate original queries and contexts in the sug-gested queries. We could also collectively rank the candidates to avoid suggesting duplicated tasks. T able 5: Comparison of top 4 suggested queries produced by the proposed STS system and commercial search engines for example tasks.
We compare the results of our proposed approach with the base-line methods for task description (title and explanation) construc-tion in Table 2. We first see that all the methods produce much lower F-1 and ROUGE scores for APKBC than those for STS, which suggests that task description generation is a difficult task, esp. explanation construction. Nevertheless, we can see that the proposed approach significantly outperforms all other classifiers in terms of F-1 and ROUGE metrics (at a significance level of 0 : 1 ).
When comparing with the results from using each feature cate-gory, we see that not a single category can reach the same perfor-mance level as the proposed approach, which again implies that all the feature categories have contributed to the overall perfor-mance in task description construction problem. Comparing with using all but one feature categories, we can find that Word features are crucial for summary generation but not explanation generation, whereas POS and Parsing features are crucial for explanation gen-eration but not summary generation. We also observe the non-Word features that contribute the most in Table 3. Although the perfor-mance does not match that of the query construction approach, we can still find interesting patterns in the results. First, we can see that nouns and verbs are still crucial in constructing task descriptions, and verbs (VB and VBP) rather than nouns are more likely selected as the beginning of a summary. To begin the task explanation, the word is expected to have a  X  X egin X  indicator and/or a dependency type of nsubj, meaning the nominal subject of a sentence. Verb phrases are also important in identifying a task description.
Finally, we evaluated our proposed end-to-end APKBC solu-tion, and compared it with wikiHow articles in Table 4. Due to the difficulty of the task description construction problem and lack of high-quality task-oriented search query candidates, we find that the automatic approach performs much worse than manual curation in building a brand new procedural knowledge base from scratch; nevertheless, we still find that the proposed approach is able to dis-cover relevant subtasks that are not covered in the current wikiHow article, i.e. it can be used to improve the coverage and quality of existing procedural knowledge bases. Some examples are shown in Table 6. For example, for the task  X  X ign up for airbnb X , one of the suggested queries  X  X ign up for airbnb coupon X  implies a coupon may be an important subsidiary resource of a task that the current wikiHow article does not know. For this particular task, a statement with the detailed coupon info is able to be extracted to describe the concrete task related to the resource. Even though this result may be from an advertisement Web page, it still delivers the freshest information that can hardly be added and updated instantly in man-ually created procedural knowledge bases.

We also summarize the errors into three major categories: (1) ambiguous task representations. For example, the  X  X ind a match X  article on wikiHow 24 describes how to  X  X ook for a partner X , whereas search engines try to disambiguate the search intent by also sug-gesting queries in the context of data analysis such as  X  X ind a match in excel X ,  X  X ind a match spreadsheet X , etc., (2) duplicated descrip-tions, and (3) quality of the text extracted from noisy Web docu-ments. Moreover, as we note that not all wikiHow articles exactly follow the writing guide, the proposed approach can be used es-timate the quality of each existing task description, similar to Eq. 3, and report potentially suspicious low-quality articles or spurious steps such as  X  X ou X  X e done. X , which violate the writing guidelines.
This paper provides a first attempt to bridge the gap between the two evolving research areas: construction of procedural knowledge bases and task-oriented search. We investigated two problems: search task suggestion using procedural knowledge and automatic procedural knowledge base construction from search activities . We proposed the creation of a three-way parallel corpus of queries, query contexts, and task descriptions, and reduced both problems to sequence labeling tasks. We proposed a supervised framework to identify the queryable phrases from task descriptions and extract procedural knowledge from search queries and relevant text snip-pets. We compared our proposed solution with baseline algorithms, and used commercial search engines and manually-curated proce-dural knowledge for both problems; experimental results show an improvement of +0.28 to +0.41 in terms of Precision@8 and MAP.
As a next step, we intend to conduct a user study to estimate to what extent the users can find good tips for real-world information needs. We also plan to further study how to properly rank the candi-date queries and task descriptions, and also study the APKBC prob-lem by leveraging a supervised natural language generation frame-work in addition to the proposed sequence labeling approach.
