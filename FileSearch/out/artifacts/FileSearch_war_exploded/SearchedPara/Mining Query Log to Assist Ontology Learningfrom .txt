 Over the course of the last four decades of the twentieth century, use of databases grew in all enterprises. The Internet revolution in the late 1990s sharply increased direct user access to databases. Most or ganizations implemented a variety of services and information applications which make use of databases as their back-end. Today, with the emergence of the Semantic Web, the Internet is on the verge of a new revolution. A key ingredient of this is the ability to make the databases available semantically, that is, to find an automated and meaningful way of expressing the structure and semantics of the databases.

Ontologies play a key role in the Semantic Web. They are widely used to for-malize the conceptualization of a domain. O X  X eary [1] defines an ontology as  X  X n explicit specification of a conceptualizat ion X . This knowledge-based specification typically describes a taxonomy of the relationships that defines the knowledge. Within the context of knowledge-management systems, ontologies are  X  X pecifi-cations of discourse in the form of a shared vocabulary X . Informally, [2] remarks that an ontology usually provides some help into describing facts, beliefs, hy-potheses, and predictions about the world in general, or in a limited domain, if that is what is needed.

Ontology learning greatly facilitates the construction of ontologies. Commer-cial relational DBMSs are tailored to efficiently support fixed format data models in what is known as data management. In other words, the underlying conceptu-alization is usually obscure and implicit. In many user scenarios, the documen-tation of the database design is always missing or difficult to understand. Con-sequently manual construction of ontologies is laborious and error-prone, which is quite unpractical to follow. For this reason, a number of (semi-)automatic ontology learning approaches have been developed.

A quite straightforward way is to do a simple mapping between database terms and ontological terms as described in [3], with table mapping to class, table column to property, value of table column to literal or resource, foreign key to object property and table row to i nstance of class. [4] describes a naive approach, that is, to generate a node for every row of every relation. To that node, attach property arcs (one per column in the relation/field in the table) with the field content as the property value. In general, these approaches expose an RDF(or DAML) description of the relational database, not the conceptual entities which the relational description is attempting to capture.

To be improved, some work has contri buted to the learning process from relational database to ontology. [5], [6], [7], [8], [9] are typical ones of them. They generally analyze the features that schema definition can convey, and their ways vary diversely. As a result of this merit, a more reasonable and sophisticated ontology can be learned. However, the ontology is still rough or general, for all concepts/relationships are about upper level. Actually database contains deeper semantics since it is the main knowledge base that we are using today. To our best knowledge, no work has further researched the deeper semantic of database.
In this paper, we propose a novel learning approach to go close to the chal-lenge. The novel point of our approach is that we expand the ontology to the lower level by exploiting the data dimension, specifically, mining user query log. For example, if a query to the  X  X erson X  table with the where clause  X  X ge &gt; 60 and gender= X  X ale X  X  is frequently executed by user, one may agree that a concept  X  X lder man X  should be elicited to be the sub concept of  X  X erson X . Additionally, we propose a set of rules for schema extraction that provide the input for our theme.

The rest of this paper is organized as follows. In section 2 we give an com-parative study on relational database and ontology. In section 3 we discuss the related work. In section 4 we introduce our approach in detail. The usability analysis is given in section 5. Finally, we conclude this paper in section 6. It is widely accepted that there is a concep tual model hiding behind the relational database schema, for it is originally transformed from an ER graph. We also call the process of ontology learning from database, from the database community X  X  point of view, reverse engineering. To extract the ontology out from the relational database, we must be aware of the differ ences between them. The ontology differs from relational database schema in that [7]:  X  In the type system, there are no basi c types; everything is a concept  X  In the concept, there are no attributes and dependencies; everything is a  X  Concepts and properties can be organized into inheritance hierarchies A number of previous work has contributed to the field of ontology learning from database. The first challenge is to deal with the denormalization of the database schema. [10] assumes that the schema input is at least 3NF. However, it is still possible that one relation scheme corresponds to more than one object, or several relation schemes correspond to a single object, or inclusion dependency may give rise to an additional object. The author proposes several algorithms to manage such denormalization of the schema, followed by a method for transfor-mating a relational schema into a conceptual schema employing the algorithms mentioned above. [11] deals with schemas that are not normalized in 3NF. A se-ries of algorithms are proposed to identify inclusion dependencies and functional dependencies by analyzing user equi-join queries, which then are applied with another algorithm to construct a conceptual schema. [12] endeavors to discover objects that are implicitly embedded in a given dataset when schema definition is missing using data mining methods. Moreover, it uses some heuristic rules to classify objects into entities and relationships.

The second branch of related work is c oncerned with how schema-level in-formation is used to extract a conceptual schema or ontology. [5] employs a set of mapping rules to identify concept, property and inheritance, respectively. The rules are quite similar to the second phase metioned in [12], but it is more considerate to the not-so frequent condition that may arise in database design. [6] adds a novel part in. It refines the learned ontology based on user queries. The query may give rise to additional objects and relationships. [9] uses a term rewriting system to produce the correspondence between the relational elements and the object-oriented elements. [13] can be regarded as a complementary to [5]. Besides the mentioned facts, aggregations are identified as well.
The above methods mostly ignore the fact that the database extension, namely the data tuples, have a certain reflectio n on the object architecture that is em-bedded and can not be extracted by traditional methods like the ones mentioned in the above paragraph. [7] analyzes the data co-relation as well as the the key and attribute co-relation from an integrated point of view. Almost every com-bination of the three co-relationships are enumerated and analyzed, and each corresponds to a rule to generate the ob ject conceptualization. This method could finely enrich the result. However, not every combination seems quite rea-sonable. In other words, some redundant combinations may collapse the result. [14] utilizes the data tuples in database to extract the concealed is-a inheritance relationship through analyzing null value. However, this method only considers a single feature that leads to its limitation on the learning process. Moreover, null value is not always available for this purpose due to the diversity of database design.
Generally, the existing approaches propose many effective ways towards learn-ing an ontology out of a relational database. But they focus on how ontology can be extracted from the schema X  X  lev el of view which is the very reason that limits the ontology detailedness. In this paper we start from a new angle of view to address the problem, namely mining user query log, which will expand the ontology to much lower level. It should also be noted that our approach can be integrated into any global process of ontology learning method from database. In this section we introduce a novel approach to glorify the ontology by mining user query log. The overall learning process has two main steps, depicted in Figure 1.

The first step is to identify concept, property and inheritance. We name it schema extraction, which will be intr oduced in a moderate manner in section 4.2. We introduce this step for it serves as the basis for our theme. It produces the upper level ontology which is the input for the next step. The second step is query log mining which is the backbone of this paper. In this step more detailed concepts and properties are identified, and the ontology is expanded to a much lower level.

Before going into the details, we make an assumption that all schemes are of 3NF. If not, [10] and [11] have given a very sophisticated solution to cope with such demornalization. 4.1 An Introductory Example In the following, we refer to a simplified real example that models some univer-sity to go through the description of our approach. The example is depicted in Figure.2.

The notation  X  &lt;&lt;  X  in Figure.2 denotes an inclusion dependency .Aninclusion where r i [Y] is the projection of the table R i on a subset of its attributes Y. 4.2 Schema Extraction The schema extraction proces s concerns the identification of concepts, properties and inheritances.

Before proceeding, we assume that we ha ve captured basic information from a relational schema (consider relations, attributes, attribute types, primary keys, foreign keys/inclusion dependencies). These information can be obtained from data dictionary or by applying the introduced reverse engineering methods. These information will be used as the main input of the next sub steps.
The schema extraction pro cess has three sub steps, i.e . concept identification, inheritance identification and property identification. The rules for them are listed in Table.1, Table.2 and Table.3 respectively.

For concept identification, the general idea is that each relation is converted into a concept, except for the relations th at represent many-to-many relation-ship(Rule.3 in Table.1). While creating concepts, properties and inheritances may also be created according to differen t preconditions. Inheritance is gener-ated when the key of two relations are equal, two kinds of inheritance rules are applied considering the condition of data extension overlapping. Property has two kinds, namely object property and datatype property. Object property is generally introduced by inclusion dependency. Datatype property is generally introduced by attribute.

When considerin g two attributes a m in R i and a n in R j for equality, the two candidate attributes should first have the same datatype. Then we consider r [ a m ] and r j [ a n ] , if their overlap rate exceed a cer tain threshold(e.g. 0.5), we could regard them as equal.

Figure.3 shows the ontology generated after applying these rules for our uni-versity example given in 4.1. Notice the rectangle represents concept, dash line represents property, nested rect angle represents inheritance. 4.3 Mining User Query Log To now, the upper ontology have been generated. Now we introduce how the ontology is expanded by mining user query log. First we give some definitions which will be used. Definition: An Atomic Constraint AC on relation R i is an expression in the form ( a m operator value), where a m is an attribute of R i , operator includes all operators that can be used in where clause of a sql query. We say a m is the Axis of AC. For example, (age &lt; 18), (gender =  X  X ale X ) and (studentID in(1,2,3)) are three atomic constraints on relation  X  X TUDENT X .  X  X ge X  is the axis of the atomic constraint (age &lt; 18).
 Definition: A Constraint CO on relation R i is an OR concatenation of one or more atomic constraints on relation R i . For example, (gender =  X  X ale X ) and ((age &lt; 18) OR (age &gt; 25)) are two constraints on relation  X  X TUDENT X . Definition: A Candidate CA on relation R i is an AND concatenation of one or more constraints on relation R i . For example, (gender =  X  X ale X ) and ((gen-der =  X  X ale X ) AND ((age &lt; 18) OR (age &gt; 25))) are two candidates on relation  X  X TUDENT X . ( X  X ender X ) and ( X  X ender X , X  X ge X ) are Axis Set of the two sample candidates respectively.
 Definition: Two candidates CA 1 and CA 2 are Vertical Related if CA 1 and CA 2 have at least one constraint in common. For example, ((gender =  X  X ale X ) AND (age &lt; 18)) and ((gender =  X  X ale X ) AND (city =  X  X aerbin X )) are mutually vertical related for they have constraint (gender =  X  X ale X ) in common. Definition: Two candidates CA 1 and CA 2 are Horizontal Related if CA 1 and CA 2 have the same axis set. For example, ((gender =  X  X ale X ) AND (age &lt; 18)) and ((gender =  X  X emale X ) AND (age &gt; 25)) are mutually horizontal related for they have the same axis set ( X  X ge X , X  X ender X ). Probability calculation Our mining method takes a large set of user historical queries, i.e. query log, as input. We first extract all candidates out from the query set. Meanwhile, we get the occurrence frequency  X  (CA) for each candidate CA. Then we produce a probability value indicating how likely it is a concept for each candidate by the following formula.
 P ( CA i )= Where the first sum iterates on the candidate set V in which each element is verti-cal related with CA i , and the second sum iterates on the candidate set H in which each element is horizontal related with CA i .  X  ( CA i ,CA v )and  X  ( CA i ,CA h )are weight of the related candidate, they are defined as follow. Where | CA i | denotes the number of constraint contained in CA i , const is a constant larger than 1.
The third factor that impacts the probability is the prior value. Currently the two factors that determine the prior value are data types of the involved attributes(co( CA i )) and the number of constraints in this candidate(depicted in Formula (  X  X  X  X  X  X  )). The reason why data type impact the result is based on the fact that discrete value appears preferab le to continuous value, boolean appears preferable to enumer ation, etc. So we set up the preference sequence as: boolean &gt; enumeration &gt; string &gt; datatime &gt; number &gt; other datatype.
Notice that the probability depicts how likely a candidate on relation R i should be extracted as an sub concept of the concept the R i corresponds. There are alternatives to select the candidates that are to be conceptualized. We may either set a threshold or refer to user to choose top N based on user X  X  preference. For our example introduced in section 4.1, Table.4 shows a part of the resulting selected candidates together with their probabilities on relation  X  X TUDENT X . Lattice building The preceding calculation gives us a lis t of candidates (now we name them con-cepts) which should be merged into the ontology. However, we still do not know the hierarchy relationship among them, i.e. the sub-super concept relationship. For example, it is obviously in Table.4 concept 5 should be sub concept of 2 and 3. But only such intuitive judgement does not suffice to construct a sound hier-archy since there is implicit concept tha t should be modeled. For example, also in Table.4, intuitively both 7 and 8 should be sub concept of 3 and 9. However, it is more appropriate to generate concept (gender =  X  X EMALE X ) AND (dept =  X  X C12 X ) as sub concept of 3,9 and super concept of 7,8. For this purpose, we introduce FCA into our context.

Formal Concept Analysis (FCA) is introduced by WILLE [15], and [16] offers more extensive information. FCA starts with a formal context K which is defined as a triple K:=(G,M,I) where G is a set of objects, M is a set of attributes, and I is a binary relation between G and M (namely, I  X  G  X  M ). For a g  X  G and m  X  M , (g, m)  X  I means that g has an attribute m .The derivation operators are defined as below: and dually,
A formal concept (or briefly concept ) of a formal context ( G, M, I )isa pair (X, Y), where X is a subset of G , Y is a subset of M , and they satisfy the conditions: X  X = Y and Y  X = X . X is called the extent of the concept and Y is called the intent of the concept.

The partial order(i.e. subconcept-su perconcept relationship) between con-cepts is defined as below:
A concept lattice of K which is denoted by B(G, M, I) is always a set of concept of K and partial order  X  between concepts. With a formal context K, we can build the corresponding concept l attice according to a certain algorithm.
We borrow the idea of FCA to help us build the concept hierarchy among the selected candidates, we name these candidates S .Wesetuptheformalcontext K with G = { CA | CA  X  S } , M = { CO | CO  X  CA , CA  X  S } , I = { (CA,CO) | CO  X  CA , CA  X  S } . In our experiment, we employed the classical Ganter X  X  algorithm [17], which is the core algorithm in the FCA project TOSCANA[18], to build the concept lattice. Figure.4 shows the hier archy result for the concepts in Table.4 on relation  X  X TUDENT X  after applying the algorithm.

Note that this is a sub hierarchy for  X  X TUDENT X  and is merged into the upper ontology generated by schema extract ion. Every concept in Figure.3 has a sub hierarchy like this. Therefore, we are able to obtain a detailed and lower level ontology. Firstly, one may ask how far our approach would interfere with the daily working of legacy database applications, especially in enterprise scenario. Some tradi-tional work migrate data records to class instances while creating the ontology. This requires the program to maintain the consistency between ontology in-stances and data records all the time, which obviously depresses the efficiency of the database server. Our approach does not migrate data and thus it does not require daily interruption with legacy database application. Instead, we enable user to access ontology instances by building a mapping layer between the on-tology classes/properties and database terms. Figure.5 shows an example how class BoyStudent is mapped to database terms. By this merit ontology users can access instances transparen tly, and data consistency is by its nature maintained.
Secondly, one may argue that how our approach can apply since the query log may be unavailable in many application scenarios. This is really one crucial question we are facing. Our answer is to log query for a certain period of time. For our demonstrate example on a real university application database, we collected queries for two months for the experiment.
 Finally, note that our approach is to help ontology learning from database. The query log mining method can be integrated into any global process of this context. By applying these methods, we can build a bridge between ontology and legacy database and thus bring the legacy systems into semantic space. Many on-researching ontological techniques can be applied, and many potential semantic applications can be derived to facilitate the semantic vision. In this paper we present a novel approach that aims to glorify the ontology learning process by mining user query log. The general idea is based on the fact that user queries can mostly reflect the c onceptual recognition of the database in human X  X  mind. In addition, we introduced a schema extraction sub approach which serves as the input of the mining phase. Our approach can be integrated into any global process of ontology learning method from database. We examined our approach on a real application database and the result is admissive and promising.

To our best knowledge, currently there is a lack of formal measurement to check a learned ontology for validity and reliability in this context. In the future, we will pay attention to how such a measurement could be established. Besides, we will further tone up the mapping architecture between the ontology and database to guarantee a smooth and effic ient access to the potential semantic application on database, so as to make our work more effective and powerful.
